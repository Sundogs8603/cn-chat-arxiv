<rss version="2.0"><channel><title>Chat Arxiv cs.MA</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.MA</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.06360</link><description>&lt;p&gt;
FedLP: &#19968;&#31181;&#29992;&#20110;&#36890;&#20449;&#35745;&#31639;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#23618;&#27425;&#21098;&#26525;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning. (arXiv:2303.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an explicit FL pruning framework, FedLP, which adopts layer-wise pruning in local training and federated updating, and is model-agnostic and universal for different types of deep learning models. FedLP can relieve the system bottlenecks of communication and computation with marginal performance decay.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#39640;&#25928;&#19988;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65288;Federated Layer-wise Pruning&#65289;&#65292;&#35813;&#26694;&#26550;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#20026;&#20855;&#26377;&#21516;&#36136;&#26412;&#22320;&#27169;&#22411;&#21644;&#24322;&#36136;&#26412;&#22320;&#27169;&#22411;&#30340;&#22330;&#26223;&#35774;&#35745;&#20102;&#20004;&#31181;&#29305;&#23450;&#30340;FedLP&#26041;&#26696;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FedLP&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FedLP&#26159;&#31532;&#19968;&#20010;&#27491;&#24335;&#23558;&#23618;&#27425;&#21098;&#26525;&#24341;&#20837;FL&#30340;&#26694;&#26550;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#22260;&#20869;&#65292;&#21487;&#20197;&#22522;&#20110;FedLP&#36827;&#19968;&#27493;&#35774;&#35745;&#26356;&#22810;&#30340;&#21464;&#20307;&#21644;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has prevailed as an efficient and privacy-preserved scheme for distributed learning. In this work, we mainly focus on the optimization of computation and communication in FL from a view of pruning. By adopting layer-wise pruning in local training and federated updating, we formulate an explicit FL pruning framework, FedLP (Federated Layer-wise Pruning), which is model-agnostic and universal for different types of deep learning models. Two specific schemes of FedLP are designed for scenarios with homogeneous local models and heterogeneous ones. Both theoretical and experimental evaluations are developed to verify that FedLP relieves the system bottlenecks of communication and computation with marginal performance decay. To the best of our knowledge, FedLP is the first framework that formally introduces the layer-wise pruning into FL. Within the scope of federated learning, more variants and combinations can be further designed based on FedLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;GMFG&#20844;&#24335;&#65292;&#31216;&#20026;LPGMFG&#65292;&#23427;&#21033;&#29992;$L^p$&#22270;&#24418;&#30340;&#22270;&#24418;&#29702;&#35770;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#20197;&#26377;&#25928;&#19988;&#20934;&#30830;&#22320;&#36817;&#20284;&#35299;&#20915;&#31232;&#30095;&#32593;&#32476;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24130;&#24459;&#32593;&#32476;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#29702;&#35770;&#23384;&#22312;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#35777;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.03880</link><description>&lt;p&gt;
&#23398;&#20064;&#31232;&#30095;&#22270;&#24418;&#22343;&#22330;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Learning Sparse Graphon Mean Field Games. (arXiv:2209.03880v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;GMFG&#20844;&#24335;&#65292;&#31216;&#20026;LPGMFG&#65292;&#23427;&#21033;&#29992;$L^p$&#22270;&#24418;&#30340;&#22270;&#24418;&#29702;&#35770;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#20197;&#26377;&#25928;&#19988;&#20934;&#30830;&#22320;&#36817;&#20284;&#35299;&#20915;&#31232;&#30095;&#32593;&#32476;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24130;&#24459;&#32593;&#32476;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#29702;&#35770;&#23384;&#22312;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#35777;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel formulation of GMFGs, called LPGMFG, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems, especially power law networks. The paper derives theoretical existence and convergence guarantees and gives empirical examples that demonstrate the accuracy of the learning method.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#35299;&#20915;&#20855;&#26377;&#22823;&#37327;&#20195;&#29702;&#30340;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#22270;&#24418;&#22343;&#22330;&#21338;&#24328;&#65288;GMFG&#65289;&#20351;&#24471;&#21487;&#20197;&#23545;&#21542;&#21017;&#38590;&#20197;&#22788;&#29702;&#30340;MARL&#38382;&#39064;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20998;&#26512;&#12290;&#30001;&#20110;&#22270;&#24418;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#38480;&#20110;&#25551;&#36848;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#65288;&#22914;&#24130;&#24459;&#22270;&#65289;&#30340;&#31264;&#23494;&#22270;&#24418;&#65292;&#36825;&#26159;&#19981;&#36275;&#30340;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20171;&#32461;&#20102;GMFG&#30340;&#26032;&#22411;&#20844;&#24335;&#65292;&#31216;&#20026;LPGMFG&#65292;&#23427;&#21033;&#29992;$L^p$&#22270;&#24418;&#30340;&#22270;&#24418;&#29702;&#35770;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#20197;&#26377;&#25928;&#19988;&#20934;&#30830;&#22320;&#36817;&#20284;&#35299;&#20915;&#31232;&#30095;&#32593;&#32476;&#38382;&#39064;&#12290;&#36825;&#23588;&#20854;&#21253;&#25324;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#20013;&#32463;&#39564;&#35266;&#23519;&#21040;&#30340;&#24130;&#24459;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#26080;&#27861;&#34987;&#26631;&#20934;&#22270;&#24418;&#25152;&#25429;&#25417;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#29702;&#35770;&#23384;&#22312;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#35777;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the field of multi-agent reinforcement learning (MARL) has made considerable progress in the last years, solving systems with a large number of agents remains a hard challenge. Graphon mean field games (GMFGs) enable the scalable analysis of MARL problems that are otherwise intractable. By the mathematical structure of graphons, this approach is limited to dense graphs which are insufficient to describe many real-world networks such as power law graphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems. This especially includes power law networks which are empirically observed in various application areas and cannot be captured by standard graphons. We derive theoretical existence and convergence guarantees and give empirical examples that demonstrate the accuracy of our learning ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32452;&#21512;&#20998;&#37197;&#65288;BOCA&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#25429;&#33719;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#26426;&#21046;&#20013;&#65292;&#35299;&#20915;&#20102;&#32452;&#21512;&#20998;&#37197;&#39046;&#22495;&#20013;&#20808;&#21069;&#24037;&#20316;&#30340;&#20027;&#35201;&#32570;&#28857;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#24341;&#23548;&#20195;&#29702;&#25552;&#20379;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2208.14698</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32452;&#21512;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization-based Combinatorial Assignment. (arXiv:2208.14698v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32452;&#21512;&#20998;&#37197;&#65288;BOCA&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#25429;&#33719;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#26426;&#21046;&#20013;&#65292;&#35299;&#20915;&#20102;&#32452;&#21512;&#20998;&#37197;&#39046;&#22495;&#20013;&#20808;&#21069;&#24037;&#20316;&#30340;&#20027;&#35201;&#32570;&#28857;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#24341;&#23548;&#20195;&#29702;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Bayesian optimization-based combinatorial assignment (BOCA) mechanism, which addresses the main shortcoming of prior work in the combinatorial assignment domain by integrating a method for capturing model uncertainty into an iterative combinatorial auction mechanism, and can better elicit information from agents.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32452;&#21512;&#20998;&#37197;&#39046;&#22495;&#65292;&#21253;&#25324;&#32452;&#21512;&#25293;&#21334;&#21644;&#35838;&#31243;&#20998;&#37197;&#12290;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#38543;&#30528;&#29289;&#21697;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#25414;&#32465;&#31354;&#38388;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#26377;&#20960;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20559;&#22909;&#24341;&#23548;&#31639;&#27861;&#65292;&#26088;&#22312;&#20174;&#20195;&#29702;&#20013;&#20165;&#24341;&#23548;&#20986;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20808;&#21069;&#24037;&#20316;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#27809;&#26377;&#23545;&#23578;&#26410;&#24341;&#23548;&#20986;&#30340;&#25414;&#32465;&#20540;&#30340;&#26426;&#21046;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32452;&#21512;&#20998;&#37197;&#65288;BOCA&#65289;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#23558;&#25429;&#33719;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#26426;&#21046;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21487;&#29992;&#20110;&#23450;&#20041;&#33719;&#21462;&#20989;&#25968;&#20197;&#30830;&#23450;&#19979;&#19968;&#20010;&#26597;&#35810;&#30340;&#19978;&#38480;&#19981;&#30830;&#23450;&#24615;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#26426;&#21046;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
We study the combinatorial assignment domain, which includes combinatorial auctions and course allocation. The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning-based preference elicitation algorithms that aim to elicit only the most important information from agents. However, the main shortcoming of this prior work is that it does not model a mechanism's uncertainty over values for not yet elicited bundles. In this paper, we address this shortcoming by presenting a Bayesian optimization-based combinatorial assignment (BOCA) mechanism. Our key technical contribution is to integrate a method for capturing model uncertainty into an iterative combinatorial auction mechanism. Concretely, we design a new method for estimating an upper uncertainty bound that can be used to define an acquisition function to determine the next query to the agents. This enables the mechanism 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2206.00233</link><description>&lt;p&gt;
DM$^2$: &#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DM$^2$: Decentralized Multi-Agent Reinforcement Learning for Distribution Matching. (arXiv:2206.00233v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a decentralized multi-agent reinforcement learning method based on distribution matching, where each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution, achieving convergence to the joint policy that generated the target distribution.
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#26426;&#21046;&#25110;&#26174;&#24335;&#36890;&#20449;&#21327;&#35758;&#20197;&#30830;&#20445;&#25910;&#25947;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#21305;&#37197;&#22312;&#19981;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#32452;&#20214;&#25110;&#26174;&#24335;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#20013;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#26368;&#23567;&#21270;&#20854;&#20010;&#20307;&#20998;&#24067;&#19981;&#21305;&#37197;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#30446;&#26631;&#20998;&#24067;&#26469;&#33258;&#20248;&#21270;&#21512;&#20316;&#20219;&#21153;&#30340;&#32852;&#21512;&#31574;&#30053;&#65292;&#21017;&#35813;&#20219;&#21153;&#22870;&#21169;&#21644;&#20998;&#24067;&#21305;&#37197;&#22870;&#21169;&#30340;&#32452;&#21512;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#30456;&#21516;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#36825;&#19968;&#35265;&#35299;&#34987;&#29992;&#26469;&#21046;&#23450;&#19968;&#20010;&#23454;&#29992;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to multi-agent cooperation rely heavily on centralized mechanisms or explicit communication protocols to ensure convergence. This paper studies the problem of distributed multi-agent learning without resorting to centralized components or explicit communication. It examines the use of distribution matching to facilitate the coordination of independent agents. In the proposed scheme, each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution. The theoretical analysis shows that under certain conditions, each agent minimizing its individual distribution mismatch allows the convergence to the joint policy that generated the target distribution. Further, if the target distribution is from a joint policy that optimizes a cooperative task, the optimal policy for a combination of this task reward and the distribution matching reward is the same joint policy. This insight is used to formulate a practical al
&lt;/p&gt;</description></item></channel></rss>