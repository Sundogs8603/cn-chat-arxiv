<rss version="2.0"><channel><title>Chat Arxiv econ</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for econ</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#24050;&#34987;&#32858;&#21512;&#30340;&#25968;&#25454;&#20013;&#37096;&#20998;&#35782;&#21035;&#20986;&#20108;&#20540;&#32467;&#26524;&#27169;&#22411;&#20013;&#30340;&#20010;&#20307;&#27700;&#24179;&#21442;&#25968;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26045;&#21152;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.07236</link><description>&lt;p&gt;
&#22312;&#38750;&#21442;&#25968;&#20108;&#20540;&#32467;&#26524;&#27169;&#22411;&#20013;&#20351;&#29992;&#32858;&#21512;&#25968;&#25454;&#37096;&#20998;&#35782;&#21035;&#20010;&#20307;&#27700;&#24179;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Binary Outcome Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#24050;&#34987;&#32858;&#21512;&#30340;&#25968;&#25454;&#20013;&#37096;&#20998;&#35782;&#21035;&#20986;&#20108;&#20540;&#32467;&#26524;&#27169;&#22411;&#20013;&#30340;&#20010;&#20307;&#27700;&#24179;&#21442;&#25968;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#23569;&#22320;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26045;&#21152;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20010;&#20307;&#27700;&#24179;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#19982;&#23545;&#36825;&#20123;&#21464;&#37327;&#36827;&#34892;&#32858;&#21512;&#21518;&#24471;&#21040;&#30340;&#20851;&#31995;&#19981;&#21516;&#12290;&#24403;&#30740;&#31350;&#20154;&#21592;&#24076;&#26395;&#20102;&#35299;&#20010;&#20307;&#27700;&#24179;&#20851;&#31995;&#20294;&#21482;&#33021;&#33719;&#24471;&#24050;&#34987;&#32858;&#21512;&#30340;&#25968;&#25454;&#26102;&#65292;&#32858;&#21512;&#38382;&#39064;&#21464;&#24471;&#30456;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#32858;&#21512;&#25968;&#25454;&#20013;&#37096;&#20998;&#35782;&#21035;&#20986;&#26465;&#20214;&#24179;&#22343;&#32467;&#26524;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#24403;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#26159;&#20108;&#20540;&#26102;&#65292;&#21516;&#26102;&#23545;&#28508;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#26045;&#21152;&#23613;&#21487;&#33021;&#23569;&#30340;&#38480;&#21046;&#12290;&#25105;&#20351;&#29992;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#26045;&#21152;&#39069;&#22806;&#24418;&#29366;&#38480;&#21046;&#30340;&#20248;&#21270;&#31243;&#24207;&#26500;&#24314;&#20102;&#37492;&#21035;&#38598;&#12290;&#25105;&#36824;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#32467;&#26524;&#24182;&#26500;&#24314;&#20102;&#19968;&#31181;&#25512;&#35770;&#31243;&#24207;&#65292;&#35813;&#31243;&#24207;&#22312;&#21482;&#25552;&#20379;&#26377;&#20851;&#27599;&#20010;&#21464;&#37327;&#30340;&#36793;&#38469;&#20449;&#24687;&#30340;&#32858;&#21512;&#25968;&#25454;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07236v1 Announce Type: new  Abstract: It is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals. This problem of aggregation becomes relevant when the researcher wants to learn individual-level relationships, but only has access to data that has been aggregated. In this paper, I develop a methodology to partially identify linear combinations of conditional average outcomes from aggregate data when the outcome of interest is binary, while imposing as few restrictions on the underlying data generating process as possible. I construct identified sets using an optimization program that allows for researchers to impose additional shape restrictions. I also provide consistency results and construct an inference procedure that is valid with aggregate data, which only provides marginal information about each variable. I apply the methodology to simulated and real-wo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#22312;&#39640;&#32500;&#32972;&#26223;&#19979;&#25506;&#35752;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#25512;&#29702;&#25193;&#23637;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#24314;&#35758;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20445;&#25345;&#36739;&#23567;&#30340;&#26679;&#26412;&#22806;&#19982;&#26679;&#26412;&#20869;&#22823;&#23567;&#27604;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#26377;&#38480;&#26679;&#26412;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12838</link><description>&lt;p&gt;
&#23558;&#20851;&#20110;&#39044;&#27979;&#33021;&#21147;&#30340;&#25512;&#29702;&#33539;&#22260;&#25193;&#23637;&#21040;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extending the Scope of Inference About Predictive Ability to Machine Learning Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12838
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#22312;&#39640;&#32500;&#32972;&#26223;&#19979;&#25506;&#35752;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#25512;&#29702;&#25193;&#23637;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#24314;&#35758;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20445;&#25345;&#36739;&#23567;&#30340;&#26679;&#26412;&#22806;&#19982;&#26679;&#26412;&#20869;&#22823;&#23567;&#27604;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#26377;&#38480;&#26679;&#26412;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#31995;&#32479;&#22320;&#20351;&#29992;&#20102;&#26679;&#26412;&#22806;&#39044;&#27979;&#35780;&#20272;&#65292;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#20010;&#38024;&#23545;&#39044;&#27979;&#33021;&#21147;&#30340;&#32463;&#20856;&#25512;&#29702;&#29702;&#35770;&#65292;&#20294;&#36825;&#31181;&#29702;&#35770;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#39640;&#32500;&#32972;&#26223;&#19979;&#30340;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#22120;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#36825;&#31181;&#25193;&#23637;&#26159;&#21487;&#33021;&#30340;&#12290;&#26631;&#20934;&#30340;&#26679;&#26412;&#22806;&#28176;&#36817;&#25512;&#29702;&#35201;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#65292;&#38656;&#35201;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65306;&#65288;i&#65289;&#39044;&#27979;&#25439;&#22833;&#20989;&#25968;&#24471;&#20998;&#30340;&#38646;&#22343;&#20540;&#26465;&#20214;&#65307;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#22120;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#20026;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#36827;&#34892;&#20934;&#30830;&#30340;&#26377;&#38480;&#26679;&#26412;&#25512;&#29702;&#65292;&#25105;&#20204;&#24314;&#35758;&#20445;&#25345;&#36739;&#23567;&#30340;&#26679;&#26412;&#22806;&#19982;&#26679;&#26412;&#20869;&#22823;&#23567;&#27604;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#32467;&#26524;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12838v1 Announce Type: new  Abstract: Though out-of-sample forecast evaluation is systematically employed with modern machine learning methods and there exists a well-established classic inference theory for predictive ability, see, e.g., West (1996, Asymptotic Inference About Predictive Ability, \textit{Econometrica}, 64, 1067-1084), such theory is not directly applicable to modern machine learners such as the Lasso in the high dimensional setting. We investigate under which conditions such extensions are possible. Two key properties for standard out-of-sample asymptotic inference to be valid with machine learning are (i) a zero-mean condition for the score of the prediction loss function; and (ii) a fast rate of convergence for the machine learner. Monte Carlo simulations confirm our theoretical findings. For accurate finite sample inferences with machine learning, we recommend a small out-of-sample vs in-sample size ratio. We illustrate the wide applicability of our resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2401.16412</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#36827;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning to Manipulate under Limited Information. (arXiv:2401.16412v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#32463;&#20856;&#32467;&#26524;&#65292;&#20219;&#20309;&#21512;&#29702;&#30340;&#20559;&#22909;&#25237;&#31080;&#26041;&#27861;&#26377;&#26102;&#20250;&#32473;&#20010;&#20307;&#25552;&#20379;&#25253;&#21578;&#19981;&#30495;&#23454;&#20559;&#22909;&#30340;&#28608;&#21169;&#12290;&#23545;&#20110;&#27604;&#36739;&#25237;&#31080;&#26041;&#27861;&#26469;&#35828;&#65292;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26356;&#25110;&#32773;&#26356;&#23569;&#25269;&#25239;&#36825;&#31181;&#31574;&#30053;&#24615;&#25805;&#32437;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#35268;&#27169;&#19979;&#23545;&#38480;&#21046;&#20449;&#24687;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#32473;&#23450;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#30340;&#25104;&#21151;&#31243;&#24230;&#26469;&#34913;&#37327;&#25805;&#32437;&#30340;&#25269;&#25239;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#23558;&#36817;40,000&#20010;&#19981;&#21516;&#35268;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23545;&#25239;8&#31181;&#19981;&#21516;&#30340;&#25237;&#31080;&#26041;&#27861;&#65292;&#22312;6&#31181;&#38480;&#21046;&#20449;&#24687;&#24773;&#20917;&#19979;&#65292;&#36827;&#34892;&#21253;&#21547;5-21&#21517;&#36873;&#27665;&#21644;3-6&#21517;&#20505;&#36873;&#20154;&#30340;&#22996;&#21592;&#20250;&#35268;&#27169;&#36873;&#20030;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#25237;&#31080;&#26041;&#27861;&#65292;&#22914;Borda&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#21487;&#20197;&#34987;&#31070;&#32463;&#32593;&#32476;&#39640;&#24230;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;Instant Runoff&#26041;&#27861;&#65292;&#34429;&#28982;&#34987;&#19968;&#20010;&#29702;&#24819;&#30340;&#25805;&#32437;&#32773;&#21033;&#28070;&#21270;&#25805;&#32437;&#65292;&#20294;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#19981;&#20250;&#21463;&#21040;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#35782;&#21035;&#19979;&#36827;&#34892;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#20869;&#37096;&#26368;&#22823;&#21270;&#30340;&#22806;&#37096;&#26368;&#23567;&#21270;&#30340;&#27979;&#35797;&#32479;&#35745;&#37327;&#65292;&#24182;&#20351;&#29992;&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#25552;&#20379;&#20102;&#36825;&#20123;&#32479;&#35745;&#37327;&#30340;&#28176;&#36817;&#29305;&#24449;&#12290;&#36890;&#36807;&#20943;&#23569;&#23545;&#21442;&#25968;&#31354;&#38388;&#30340;&#23616;&#37096;&#32447;&#24615;&#36817;&#20284;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#37096;&#20998;&#35782;&#21035;&#20551;&#35774;&#26816;&#39564;&#30340;&#28176;&#36817;&#36817;&#20284;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13057</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#35782;&#21035;&#19979;&#20351;&#29992;&#26497;&#23567;&#26497;&#22823;&#27979;&#35797;&#32479;&#35745;&#37327;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference under partial identification with minimax test statistics. (arXiv:2401.13057v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#35782;&#21035;&#19979;&#36827;&#34892;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#20869;&#37096;&#26368;&#22823;&#21270;&#30340;&#22806;&#37096;&#26368;&#23567;&#21270;&#30340;&#27979;&#35797;&#32479;&#35745;&#37327;&#65292;&#24182;&#20351;&#29992;&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#25552;&#20379;&#20102;&#36825;&#20123;&#32479;&#35745;&#37327;&#30340;&#28176;&#36817;&#29305;&#24449;&#12290;&#36890;&#36807;&#20943;&#23569;&#23545;&#21442;&#25968;&#31354;&#38388;&#30340;&#23616;&#37096;&#32447;&#24615;&#36817;&#20284;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#37096;&#20998;&#35782;&#21035;&#20551;&#35774;&#26816;&#39564;&#30340;&#28176;&#36817;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#19968;&#31181;&#35745;&#31639;&#21644;&#20272;&#35745;&#22522;&#20110;&#20869;&#37096;&#26368;&#22823;&#21270;&#30340;&#22806;&#37096;&#26368;&#23567;&#21270;&#30340;&#27979;&#35797;&#32479;&#35745;&#37327;&#28176;&#36817;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#27979;&#35797;&#32479;&#35745;&#37327;&#22312;&#30697;&#27169;&#22411;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#24182;&#19988;&#22312;&#37096;&#20998;&#35782;&#21035;&#19979;&#25552;&#20379;&#20551;&#35774;&#26816;&#39564;&#20855;&#26377;&#29305;&#21035;&#30340;&#20852;&#36259;&#12290;&#22312;&#19968;&#33324;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#26497;&#23567;&#26497;&#22823;&#23450;&#29702;&#25552;&#20379;&#20102;&#36825;&#20123;&#27979;&#35797;&#32479;&#35745;&#37327;&#30340;&#28176;&#36817;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#33258;&#21161;&#27861;&#35745;&#31639;&#20020;&#30028;&#20540;&#12290;&#22312;&#19968;&#20123;&#36731;&#24494;&#30340;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#37096;&#20998;&#35782;&#21035;&#20551;&#35774;&#26816;&#39564;&#30340;&#20960;&#20010;&#28176;&#36817;&#36817;&#20284;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#23545;&#21442;&#25968;&#31354;&#38388;&#30340;&#23616;&#37096;&#32447;&#24615;&#36817;&#20284;&#26469;&#25193;&#23637;&#20102;&#36825;&#20123;&#36817;&#20284;&#12290;&#36825;&#20123;&#28176;&#36817;&#32467;&#26524;&#36890;&#24120;&#26131;&#20110;&#38472;&#36848;&#21644;&#30452;&#25509;&#35745;&#31639;&#65288;&#20363;&#22914;&#65292;&#23545;&#25239;&#22320;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a means of computing and estimating the asymptotic distributions of test statistics based on an outer minimization of an inner maximization. Such test statistics, which arise frequently in moment models, are of special interest in providing hypothesis tests under partial identification. Under general conditions, we provide an asymptotic characterization of such test statistics using the minimax theorem, and a means of computing critical values using the bootstrap. Making some light regularity assumptions, our results provide a basis for several asymptotic approximations that have been provided for partially identified hypothesis tests, and extend them by mitigating their dependence on local linear approximations of the parameter space. These asymptotic results are generally simple to state and straightforward to compute (e.g. adversarially).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#65292;&#24179;&#34913;&#32676;&#20307;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;BGATE&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22788;&#29702;&#22312;&#32676;&#20307;&#38388;&#30340;&#25928;&#24212;&#24046;&#24322;&#65292;&#35813;&#21442;&#25968;&#22522;&#20110;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#31163;&#25955;&#22788;&#29702;&#36827;&#34892;&#20272;&#35745;&#12290;&#36890;&#36807;&#27604;&#36739;&#20004;&#20010;BGATE&#30340;&#24046;&#24322;&#65292;&#33021;&#26356;&#22909;&#22320;&#20998;&#26512;&#22788;&#29702;&#30340;&#24322;&#36136;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08290</link><description>&lt;p&gt;
&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#20013;&#20171;&#25928;&#24212;&#12290; (arXiv:2401.08290v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
Causal Machine Learning for Moderation Effects. (arXiv:2401.08290v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#65292;&#24179;&#34913;&#32676;&#20307;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;BGATE&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#22788;&#29702;&#22312;&#32676;&#20307;&#38388;&#30340;&#25928;&#24212;&#24046;&#24322;&#65292;&#35813;&#21442;&#25968;&#22522;&#20110;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#31163;&#25955;&#22788;&#29702;&#36827;&#34892;&#20272;&#35745;&#12290;&#36890;&#36807;&#27604;&#36739;&#20004;&#20010;BGATE&#30340;&#24046;&#24322;&#65292;&#33021;&#26356;&#22909;&#22320;&#20998;&#26512;&#22788;&#29702;&#30340;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20219;&#20309;&#20915;&#31574;&#32773;&#26469;&#35828;&#65292;&#20102;&#35299;&#20915;&#31574;&#65288;&#22788;&#29702;&#65289;&#23545;&#25972;&#20307;&#21644;&#23376;&#32676;&#30340;&#24433;&#21709;&#26159;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#12290;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26368;&#36817;&#25552;&#20379;&#20102;&#29992;&#20110;&#20272;&#35745;&#32676;&#20307;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;GATE&#65289;&#30340;&#24037;&#20855;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22788;&#29702;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32771;&#34385;&#20854;&#20182;&#21327;&#21464;&#37327;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#35299;&#37322;&#32676;&#20307;&#38388;&#22788;&#29702;&#25928;&#24212;&#24046;&#24322;&#30340;&#38590;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21442;&#25968;&#65292;&#21363;&#24179;&#34913;&#32676;&#20307;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;BGATE&#65289;&#65292;&#23427;&#34913;&#37327;&#20102;&#20855;&#26377;&#29305;&#23450;&#20998;&#24067;&#30340;&#20808;&#39564;&#30830;&#23450;&#21327;&#21464;&#37327;&#30340;GATE&#12290;&#36890;&#36807;&#27604;&#36739;&#20004;&#20010;BGATE&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#26377;&#24847;&#20041;&#22320;&#20998;&#26512;&#24322;&#36136;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#27604;&#36739;&#20004;&#20010;GATE&#12290;&#36825;&#20010;&#21442;&#25968;&#30340;&#20272;&#35745;&#31574;&#30053;&#26159;&#22522;&#20110;&#26080;&#28151;&#28102;&#35774;&#32622;&#20013;&#31163;&#25955;&#22788;&#29702;&#30340;&#21452;&#37325;/&#21435;&#20559;&#26426;&#22120;&#23398;&#20064;&#65292;&#35813;&#20272;&#35745;&#37327;&#22312;&#26631;&#20934;&#26465;&#20214;&#19979;&#34920;&#29616;&#20026;$\sqrt{N}$&#19968;&#33268;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#28155;&#21152;&#39069;&#22806;&#30340;&#26631;&#35782;
&lt;/p&gt;
&lt;p&gt;
It is valuable for any decision maker to know the impact of decisions (treatments) on average and for subgroups. The causal machine learning literature has recently provided tools for estimating group average treatment effects (GATE) to understand treatment heterogeneity better. This paper addresses the challenge of interpreting such differences in treatment effects between groups while accounting for variations in other covariates. We propose a new parameter, the balanced group average treatment effect (BGATE), which measures a GATE with a specific distribution of a priori-determined covariates. By taking the difference of two BGATEs, we can analyse heterogeneity more meaningfully than by comparing two GATEs. The estimation strategy for this parameter is based on double/debiased machine learning for discrete treatments in an unconfoundedness setting, and the estimator is shown to be $\sqrt{N}$-consistent and asymptotically normal under standard conditions. Adding additional identifyin
&lt;/p&gt;</description></item></channel></rss>