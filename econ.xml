<rss version="2.0"><channel><title>Chat Arxiv econ</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for econ</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;Neyman&#27491;&#20132;&#30697;&#26465;&#20214;&#26469;&#38477;&#20302;&#23545;&#24178;&#25200;&#21442;&#25968;&#30340;&#25935;&#24863;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#21435;&#20559;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23545;&#20302;&#32500;&#21442;&#25968;&#36827;&#34892;&#30495;&#23454;&#20540;&#30340;&#25910;&#32553;&#65292;&#24182;&#22312;&#21322;&#21442;&#25968;&#25928;&#29575;&#30028;&#30340;&#26041;&#24046;&#19979;&#36827;&#34892;&#28176;&#36817;&#27491;&#24577;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.03816</link><description>&lt;p&gt;
&#37325;&#21442;&#25968;&#21270;&#19982;&#21322;&#21442;&#25968;Bernstein-von-Mises&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reparametrization and the Semiparametric Bernstein-von-Mises Theorem. (arXiv:2306.03816v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;Neyman&#27491;&#20132;&#30697;&#26465;&#20214;&#26469;&#38477;&#20302;&#23545;&#24178;&#25200;&#21442;&#25968;&#30340;&#25935;&#24863;&#24230;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#21435;&#20559;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21516;&#26102;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23545;&#20302;&#32500;&#21442;&#25968;&#36827;&#34892;&#30495;&#23454;&#20540;&#30340;&#25910;&#32553;&#65292;&#24182;&#22312;&#21322;&#21442;&#25968;&#25928;&#29575;&#30028;&#30340;&#26041;&#24046;&#19979;&#36827;&#34892;&#28176;&#36817;&#27491;&#24577;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#37096;&#20998;&#32447;&#24615;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22238;&#24402;&#20989;&#25968;&#30340;&#19968;&#20010;&#21442;&#25968;&#21270;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#19987;&#38376;&#29992;&#20110;&#20272;&#35745;&#25152;&#20851;&#24515;&#30340;&#20302;&#32500;&#21442;&#25968;&#12290;&#21442;&#25968;&#21270;&#30340;&#20851;&#38190;&#29305;&#24615;&#26159;&#29983;&#25104;&#20102;&#19968;&#20010;Neyman&#27491;&#20132;&#30697;&#26465;&#20214;&#65292;&#36825;&#24847;&#21619;&#30528;&#23545;&#24178;&#25200;&#21442;&#25968;&#30340;&#20272;&#35745;&#20302;&#32500;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;&#25105;&#20204;&#30340;&#22823;&#26679;&#26412;&#20998;&#26512;&#25903;&#25345;&#20102;&#36825;&#31181;&#35828;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20805;&#20998;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#20302;&#32500;&#21442;&#25968;&#30340;&#21518;&#39564;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23545;&#30495;&#23454;&#20540;&#25910;&#32553;&#65292;&#24182;&#19988;&#22312;&#21322;&#21442;&#25968;&#25928;&#29575;&#30028;&#30340;&#26041;&#24046;&#19979;&#28176;&#36817;&#22320;&#27491;&#24577;&#20998;&#24067;&#12290;&#36825;&#20123;&#26465;&#20214;&#30456;&#23545;&#20110;&#22238;&#24402;&#27169;&#22411;&#30340;&#21407;&#22987;&#21442;&#25968;&#21270;&#20801;&#35768;&#26356;&#22823;&#31867;&#30340;&#24178;&#25200;&#21442;&#25968;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#19968;&#20010;&#23884;&#20837;&#20102;Neyman&#27491;&#20132;&#24615;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#21487;&#20197;&#25104;&#20026;&#21322;&#21442;&#25968;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#26377;&#29992;&#24037;&#20855;&#65292;&#20197;&#21435;&#20559;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers Bayesian inference for the partially linear model. Our approach exploits a parametrization of the regression function that is tailored toward estimating a low-dimensional parameter of interest. The key property of the parametrization is that it generates a Neyman orthogonal moment condition meaning that the low-dimensional parameter is less sensitive to the estimation of nuisance parameters. Our large sample analysis supports this claim. In particular, we derive sufficient conditions under which the posterior for the low-dimensional parameter contracts around the truth at the parametric rate and is asymptotically normal with a variance that coincides with the semiparametric efficiency bound. These conditions allow for a larger class of nuisance parameters relative to the original parametrization of the regression model. Overall, we conclude that a parametrization that embeds Neyman orthogonality can be a useful device for debiasing posterior distributions in semipa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21327;&#25972;&#21521;&#37327;&#33258;&#22238;&#24402;&#36807;&#31243;&#30340;&#22343;&#21248;&#26377;&#25928;&#25512;&#26029;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#20004;&#20010;&#20855;&#20307;&#26696;&#20363;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03632</link><description>&lt;p&gt;
&#21327;&#25972;&#21521;&#37327;&#33258;&#22238;&#24402;&#36807;&#31243;&#30340;&#22343;&#21248;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Uniform Inference for Cointegrated Vector Autoregressive Processes. (arXiv:2306.03632v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#21327;&#25972;&#21521;&#37327;&#33258;&#22238;&#24402;&#36807;&#31243;&#30340;&#22343;&#21248;&#26377;&#25928;&#25512;&#26029;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#20004;&#20010;&#20855;&#20307;&#26696;&#20363;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#25972;&#21521;&#37327;&#33258;&#22238;&#24402;&#36807;&#31243;&#30340;&#22343;&#21248;&#26377;&#25928;&#25512;&#26029;&#22240;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30340;&#28176;&#36817;&#20998;&#24067;&#20013;&#20986;&#29616;&#30340;&#26576;&#20123;&#19981;&#36830;&#32493;&#32780;&#19968;&#30452;&#38590;&#20197;&#35777;&#23454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#21333;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#28176;&#36817;&#32467;&#26524;&#25193;&#23637;&#21040;&#22810;&#32500;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#36827;&#34892;&#25512;&#26029;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;[20]&#25552;&#20986;&#30340;&#26032;&#22411;&#24037;&#20855;&#21464;&#37327;&#31243;&#24207;&#65288;IVX&#65289;&#22914;&#20309;&#29983;&#25104;&#25972;&#20010;&#33258;&#22238;&#24402;&#30697;&#38453;&#30340;&#22343;&#21248;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#20004;&#20010;&#20855;&#20307;&#30340;&#20363;&#23376;&#65292;&#24182;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#39564;&#35777;&#29702;&#35770;&#32467;&#26524;&#24182;&#30740;&#31350;&#26377;&#38480;&#26679;&#26412;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uniformly valid inference for cointegrated vector autoregressive processes has so far proven difficult due to certain discontinuities arising in the asymptotic distribution of the least squares estimator. We show how asymptotic results from the univariate case can be extended to multiple dimensions and how inference can be based on these results. Furthermore, we show that the novel instrumental variable procedure proposed by [20] (IVX) yields uniformly valid confidence regions for the entire autoregressive matrix. The results are applied to two specific examples for which we verify the theoretical findings and investigate finite sample properties in simulation experiments.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#38543;&#26426;&#26862;&#26519;&#21644;LSTM&#65289;&#26469;&#39044;&#27979;COVID-19&#26399;&#38388;&#32654;&#22269;&#20004;&#20010;&#20027;&#35201;&#32929;&#31080;&#24066;&#22330;&#25351;&#25968;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#21644;&#19968;&#31995;&#21015;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.03620</link><description>&lt;p&gt;
&#39044;&#27979;COVID-19&#26399;&#38388;US&#32929;&#31080;&#24066;&#22330;&#25351;&#25968;&#34920;&#29616;&#65306;RF vs LSTM
&lt;/p&gt;
&lt;p&gt;
Forecasting the Performance of US Stock Market Indices During COVID-19: RF vs LSTM. (arXiv:2306.03620v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03620
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#38543;&#26426;&#26862;&#26519;&#21644;LSTM&#65289;&#26469;&#39044;&#27979;COVID-19&#26399;&#38388;&#32654;&#22269;&#20004;&#20010;&#20027;&#35201;&#32929;&#31080;&#24066;&#22330;&#25351;&#25968;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#21644;&#19968;&#31995;&#21015;&#25216;&#26415;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#32929;&#24066;&#22312;&#32463;&#21382;&#20102;2007-2009&#24180;&#30340;&#34928;&#36864;&#21518;&#20986;&#29616;&#20102;&#19981;&#31283;&#23450;&#24773;&#20917;&#12290;COVID-19&#23545;&#32654;&#22269;&#32929;&#31080;&#20132;&#26131;&#21592;&#21644;&#25237;&#36164;&#32773;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20132;&#26131;&#21592;&#21644;&#25237;&#36164;&#32773;&#24212;&#35813;&#36319;&#19978;&#32929;&#24066;&#30340;&#33410;&#22863;&#12290;&#20351;&#29992;&#32771;&#34385;&#21040;&#22823;&#27969;&#34892;&#30149;&#24433;&#21709;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#32531;&#35299;&#39118;&#38505;&#24182;&#25552;&#39640;&#21033;&#28070;&#12290;&#32771;&#34385;&#21040;&#22823;&#27969;&#34892;&#30149;&#21518;&#30340;&#34928;&#36864;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;LSTM&#26469;&#39044;&#27979;&#20004;&#20010;&#20027;&#35201;&#30340;&#32654;&#22269;&#32929;&#31080;&#24066;&#22330;&#25351;&#25968;&#12290;&#20351;&#29992;&#21382;&#21490;&#20215;&#26684;&#25968;&#25454;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#39044;&#27979;&#25351;&#25968;&#22238;&#25253;&#12290;&#20026;&#20102;&#35780;&#20272;&#35757;&#32451;&#26399;&#38388;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#27491;&#21017;&#21270;&#65288;&#22914;&#25237;&#25918;&#21644;&#26435;&#37325;&#34928;&#20943;&#65289;&#21644;&#39044;&#22788;&#29702;&#21487;&#20197;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#39640;&#31934;&#24230;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20132;&#26131;&#21592;&#21644;&#25237;&#36164;&#32773;&#21487;&#20197;&#39044;&#27979;&#32929;&#24066;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The US stock market experienced instability following the recession (2007-2009). COVID-19 poses a significant challenge to US stock traders and investors. Traders and investors should keep up with the stock market. This is to mitigate risks and improve profits by using forecasting models that account for the effects of the pandemic. With consideration of the COVID-19 pandemic after the recession, two machine learning models, including Random Forest and LSTM are used to forecast two major US stock market indices. Data on historical prices after the big recession is used for developing machine learning models and forecasting index returns. To evaluate the model performance during training, cross-validation is used. Additionally, hyperparameter optimizing, regularization, such as dropouts and weight decays, and preprocessing improve the performances of Machine Learning techniques. Using high-accuracy machine learning techniques, traders and investors can forecast stock market behavior, st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#21487;&#35266;&#27979;&#22788;&#29702;&#25928;&#24212;&#26041;&#24046;&#30340;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#21487;&#35266;&#27979;&#22788;&#29702;&#25928;&#24212;&#26041;&#24046;&#20026;&#38646;&#26102;&#20135;&#29983;&#30340;&#38169;&#35823;&#35206;&#30422;&#33539;&#22260;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;VCATE&#19982;&#20915;&#23450;&#35841;&#24212;&#21463;&#27835;&#30103;&#30340;&#38382;&#39064;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.03363</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#23454;&#39564;&#30340;&#22788;&#29702;&#25928;&#24212;&#26041;&#24046;&#30340;&#40065;&#26834;&#24615;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Robust inference for the treatment effect variance in experiments using machine learning. (arXiv:2306.03363v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#21487;&#35266;&#27979;&#22788;&#29702;&#25928;&#24212;&#26041;&#24046;&#30340;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#21487;&#35266;&#27979;&#22788;&#29702;&#25928;&#24212;&#26041;&#24046;&#20026;&#38646;&#26102;&#20135;&#29983;&#30340;&#38169;&#35823;&#35206;&#30422;&#33539;&#22260;&#30340;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20102;VCATE&#19982;&#20915;&#23450;&#35841;&#24212;&#21463;&#27835;&#30103;&#30340;&#38382;&#39064;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#32773;&#36890;&#24120;&#25910;&#38598;&#22522;&#32447;&#25968;&#25454;&#26469;&#30740;&#31350;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#21487;&#35266;&#27979;&#22788;&#29702;&#25928;&#24212;&#26041;&#24046;&#30340;&#26377;&#25928;&#32622;&#20449;&#21306;&#38388;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#21487;&#35266;&#27979;&#22788;&#29702;&#25928;&#24212;&#26041;&#24046;&#20026;&#38646;&#26102;&#65292;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#35206;&#30422;&#33539;&#22260;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#19981;&#23384;&#22312;&#24322;&#36136;&#24615;&#65292;&#23454;&#38469;&#25805;&#20316;&#32773;&#20063;&#23481;&#26131;&#26816;&#27979;&#21040;&#12290;&#24403;&#36793;&#30028;&#22788;&#20110;&#23616;&#37096;&#36864;&#21270;&#29366;&#24577;&#26102;&#65292;&#25152;&#26377;&#39640;&#25928;&#20272;&#35745;&#22120;&#37117;&#20855;&#26377;&#23616;&#37096;&#36864;&#21270;&#30340;&#24433;&#21709;&#20989;&#25968;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#20855;&#26377;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20855;&#26377;&#39044;&#27979;&#31532;&#19968;&#38454;&#27573;&#30340;&#24191;&#27867;&#31867;&#22810;&#27493;&#20272;&#35745;&#22120;&#30340;&#38382;&#39064;&#12290;&#25105;&#30340;&#32622;&#20449;&#21306;&#38388;&#32771;&#34385;&#20102;&#26497;&#38480;&#20998;&#24067;&#20013;&#30340;&#39640;&#38454;&#39033;&#65292;&#24182;&#19988;&#35745;&#31639;&#36895;&#24230;&#24456;&#24555;&#12290;&#25105;&#36824;&#21457;&#29616;&#20102;VCATE&#19982;&#20915;&#23450;&#35841;&#24212;&#21463;&#27835;&#30103;&#30340;&#38382;&#39064;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#12290;&#27835;&#30103;&#30446;&#26631;&#30340;&#25910;&#30410;(&#24613;&#21095;)&#21463;&#21040;VCATE&#24179;&#26041;&#26681;&#30340;&#19968;&#21322;&#30340;&#38480;&#21046;&#12290;&#26368;&#21518;&#65292;&#25105;&#36890;&#36807;&#27169;&#25311;&#21644;&#37325;&#26032;&#20998;&#26512;&#39532;&#25289;&#32500;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimenters often collect baseline data to study heterogeneity. I propose the first valid confidence intervals for the VCATE, the treatment effect variance explained by observables. Conventional approaches yield incorrect coverage when the VCATE is zero. As a result, practitioners could be prone to detect heterogeneity even when none exists. The reason why coverage worsens at the boundary is that all efficient estimators have a locally-degenerate influence function and may not be asymptotically normal. I solve the problem for a broad class of multistep estimators with a predictive first stage. My confidence intervals account for higher-order terms in the limiting distribution and are fast to compute. I also find new connections between the VCATE and the problem of deciding whom to treat. The gains of targeting treatment are (sharply) bounded by half the square root of the VCATE. Finally, I document excellent performance in simulation and reanalyze an experiment from Malawi.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Cornish-Fisher&#25193;&#23637;&#26469;&#20272;&#35745;&#26465;&#20214;&#26041;&#24046;&#12289;&#20559;&#24230;&#21644;&#23792;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;QCMs&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20808;&#21069;&#20272;&#35745;&#26465;&#20214;&#22343;&#20540;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#30740;&#31350;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.06799</link><description>&lt;p&gt;
Cornish-Fisher&#25193;&#23637;&#30340;&#20998;&#20301;&#26465;&#20214;&#26041;&#24046;&#12289;&#20559;&#24230;&#21644;&#23792;&#24230;
&lt;/p&gt;
&lt;p&gt;
Quantiled conditional variance, skewness, and kurtosis by Cornish-Fisher expansion. (arXiv:2302.06799v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Cornish-Fisher&#25193;&#23637;&#26469;&#20272;&#35745;&#26465;&#20214;&#26041;&#24046;&#12289;&#20559;&#24230;&#21644;&#23792;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;QCMs&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#20808;&#21069;&#20272;&#35745;&#26465;&#20214;&#22343;&#20540;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#30740;&#31350;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#26041;&#24046;&#12289;&#20559;&#24230;&#21644;&#23792;&#24230;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Cornish-Fisher&#25193;&#23637;&#26469;&#20272;&#35745;&#36825;&#19977;&#20010;&#26465;&#20214;&#30697;&#30340;&#26032;&#26041;&#27861;&#8212;&#8212;&#20998;&#20301;&#26465;&#20214;&#30697;&#65288;QCMs&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;$n$&#20010;&#19981;&#21516;&#30340;&#20272;&#35745;&#26465;&#20214;&#20998;&#20301;&#25968;&#26500;&#24314;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#35745;&#31639;QCMs&#65292;&#28982;&#21518;&#20351;&#29992;&#27492;&#22238;&#24402;&#27169;&#22411;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#31616;&#21333;&#32780;&#21516;&#26102;&#22320;&#35745;&#31639;QCMs&#65292;&#32780;&#26080;&#38656;&#20808;&#21069;&#20272;&#35745;&#26465;&#20214;&#22343;&#20540;&#12290;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;QCMs&#34987;&#35777;&#26126;&#26159;&#19968;&#33268;&#30340;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$n^{-1/2}$&#12290;&#27169;&#25311;&#30740;&#31350;&#34920;&#26126;&#65292;QCMs&#22312;Cornish-Fisher&#25193;&#23637;&#35823;&#24046;&#21644;&#20998;&#20301;&#25968;&#20272;&#35745;&#35823;&#24046;&#30340;&#19981;&#21516;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conditional variance, skewness, and kurtosis play a central role in time series analysis. These three conditional moments (CMs) are often studied by some parametric models but with two big issues: the risk of model mis-specification and the instability of model estimation. To avoid the above two issues, this paper proposes a novel method to estimate these three CMs by the so-called quantiled CMs (QCMs). The QCM method first adopts the idea of Cornish-Fisher expansion to construct a linear regression model, based on $n$ different estimated conditional quantiles. Next, it computes the QCMs simply and simultaneously by using the ordinary least squares estimator of this regression model, without any prior estimation of the conditional mean. Under certain conditions, the QCMs are shown to be consistent with the convergence rate $n^{-1/2}$. Simulation studies indicate that the QCMs perform well under different scenarios of Cornish-Fisher expansion errors and quantile estimation errors. I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#20855;&#26377;&#39640;&#39118;&#38505;&#24212;&#29992;&#30340;&#20010;&#24615;&#21270;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#65292;&#19981;&#21516;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.02923</link><description>&lt;p&gt;
&#19981;&#26159;&#31070;&#22855;&#33647;&#20024;&#65292;&#32780;&#26159;&#27934;&#23519;&#21147;&#20043;&#25628;&#23547;&#65306;&#28040;&#38500;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation. (arXiv:2302.02923v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#20855;&#26377;&#39640;&#39118;&#38505;&#24212;&#29992;&#30340;&#20010;&#24615;&#21270;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#65292;&#19981;&#21516;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#32463;&#24120;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#27492;&#65292;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#20272;&#35745;&#36825;&#31181;&#25928;&#24212;&#30340;&#27169;&#22411;&#20043;&#21069;&#65292;&#38656;&#35201;&#30830;&#20449;&#24050;&#32463;&#36873;&#25321;&#20102;&#26368;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#31665;&#20013;&#30340;&#20505;&#36873;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#23454;&#36341;&#20013;&#32570;&#20047;&#21453;&#20107;&#23454;&#20449;&#24687;&#65292;&#36890;&#24120;&#26080;&#27861;&#20381;&#38752;&#26631;&#20934;&#39564;&#35777;&#25351;&#26631;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#23548;&#33268;&#20102;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#25991;&#29486;&#20013;&#24050;&#30693;&#30340;&#27169;&#22411;&#36873;&#25321;&#22256;&#22659;&#12290;&#34429;&#28982;&#26368;&#36817;&#24050;&#32463;&#30740;&#31350;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#19981;&#21516;&#27169;&#22411;&#36873;&#25321;&#26631;&#20934;&#30340;&#20248;&#32570;&#28857;&#30340;&#31995;&#32479;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24182;&#27809;&#26377;&#35797;&#22270;&#23459;&#24067;&#20840;&#23616;&#8220;&#32988;&#32773;&#8221;&#65292;&#32780;&#26159;&#23545;&#19981;&#21516;&#36873;&#25321;&#26631;&#20934;&#30340;&#25104;&#21151;&#21644;&#22833;&#36133;&#27169;&#24335;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#24378;&#35843;&#36873;&#25321;&#31574;&#30053;&#65292;&#20505;&#36873;&#20272;&#35745;&#37327;&#21644;&#29992;&#20110;&#27604;&#36739;&#23427;&#20204;&#30340;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized treatment effect estimates are often of interest in high-stakes applications -- thus, before deploying a model estimating such effects in practice, one needs to be sure that the best candidate from the ever-growing machine learning toolbox for this task was chosen. Unfortunately, due to the absence of counterfactual information in practice, it is usually not possible to rely on standard validation metrics for doing so, leading to a well-known model selection dilemma in the treatment effect estimation literature. While some solutions have recently been investigated, systematic understanding of the strengths and weaknesses of different model selection criteria is still lacking. In this paper, instead of attempting to declare a global `winner', we therefore empirically investigate success- and failure modes of different selection criteria. We highlight that there is a complex interplay between selection strategies, candidate estimators and the data used for comparing them, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#20998;&#24067;&#20989;&#25968;&#30340;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#20272;&#35745;&#65292;&#24182;&#25512;&#23548;&#20102;&#19968;&#33268;&#25910;&#25947;&#24615;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#26159;&#21322;&#21442;&#25968;&#20272;&#35745;&#30340;&#31532;&#19968;&#38454;&#27573;&#26102;&#29305;&#21035;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2112.08546</link><description>&lt;p&gt;
&#26465;&#20214;&#20998;&#24067;&#30340;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#25910;&#25947;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Uniform Convergence Results for the Local Linear Regression Estimation of the Conditional Distribution. (arXiv:2112.08546v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#20998;&#24067;&#20989;&#25968;&#30340;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#20272;&#35745;&#65292;&#24182;&#25512;&#23548;&#20102;&#19968;&#33268;&#25910;&#25947;&#24615;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#26159;&#21322;&#21442;&#25968;&#20272;&#35745;&#30340;&#31532;&#19968;&#38454;&#27573;&#26102;&#29305;&#21035;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26465;&#20214;&#20998;&#24067;&#20989;&#25968; $F(y|x)$ &#30340;&#23616;&#37096;&#32447;&#24615;&#22238;&#24402;&#20272;&#35745;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19977;&#20010;&#19968;&#33268;&#24615;&#25910;&#25947;&#24615;&#32467;&#26524;&#65306;&#19968;&#33268;&#20559;&#24046;&#23637;&#24320;&#12289;&#19968;&#33268;&#25910;&#25947;&#36895;&#29575;&#21644;&#19968;&#33268;&#28176;&#36817;&#32447;&#24615;&#34920;&#31034;&#12290;&#19978;&#36848;&#32467;&#26524;&#20013;&#30340;&#19968;&#33268;&#24615;&#26159;&#30456;&#23545;&#20110; $x$ &#21644; $y$ &#30340;&#65292;&#22240;&#27492;&#22312;&#23616;&#37096;&#22810;&#39033;&#24335;&#22238;&#24402;&#30340;&#25991;&#29486;&#20013;&#23578;&#26410;&#28041;&#21450;&#12290;&#36825;&#31181;&#19968;&#33268;&#24615;&#25910;&#25947;&#24615;&#32467;&#26524;&#22312;&#26465;&#20214;&#20998;&#24067;&#20272;&#35745;&#26159;&#21322;&#21442;&#25968;&#20272;&#35745;&#30340;&#31532;&#19968;&#38454;&#27573;&#26102;&#29305;&#21035;&#26377;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#23637;&#31034;&#20102;&#36825;&#20123;&#19968;&#33268;&#24615;&#32467;&#26524;&#30340;&#23454;&#29992;&#24615;&#65306;$y$ &#30340;&#38543;&#26426;&#31561;&#36830;&#32493;&#24615;&#26465;&#20214;&#21644;&#31215;&#20998;&#26465;&#20214;&#20998;&#24067;&#20989;&#25968;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the local linear regression (LLR) estimate of the conditional distribution function $F(y|x)$. We derive three uniform convergence results: the uniform bias expansion, the uniform convergence rate, and the uniform asymptotic linear representation. The uniformity in the above results is with respect to both $x$ and $y$ and therefore has not previously been addressed in the literature on local polynomial regression. Such uniform convergence results are especially useful when the conditional distribution estimator is the first stage of a semiparametric estimator. We demonstrate the usefulness of these uniform results with two examples: the stochastic equicontinuity condition in $y$, and the estimation of the integrated conditional distribution function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#32676;&#20307;&#20381;&#36182;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25512;&#26029;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#65292;&#21457;&#29616;&#24403;&#27809;&#26377;&#20851;&#20110;&#35266;&#27979;&#20540;&#20381;&#36182;&#32467;&#26500;&#30340;&#30693;&#35782;&#26102;&#65292;&#19981;&#33021;&#19968;&#33268;&#22320;&#21306;&#20998;&#24179;&#22343;&#20540;&#65292;&#32780;&#33267;&#23569;&#26377;&#20004;&#20010;&#22823;&#32858;&#31867;&#26159;&#36827;&#34892;&#19968;&#33268;&#30340; $\sqrt{n}$-&#21306;&#20998;&#24179;&#22343;&#20540;&#30340;&#20805;&#20998;&#26465;&#20214;&#65307;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38271;&#26399;&#26041;&#24046;&#19968;&#33268;&#20272;&#35745;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#33267;&#23569;&#26377;&#19968;&#20010;&#22823;&#32858;&#31867;&#26102;&#65292;&#38271;&#26399;&#26041;&#24046;&#19981;&#33021;&#19968;&#33268;&#22320;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2109.03971</link><description>&lt;p&gt;
&#23545;&#20855;&#26377;&#22823;&#32858;&#31867;&#30340;&#32676;&#20307;&#20381;&#36182;&#32467;&#26500;&#36827;&#34892;&#25512;&#26029;&#30340;&#19968;&#20123;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Some Impossibility Results for Inference With Cluster Dependence with Large Clusters. (arXiv:2109.03971v4 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#32676;&#20307;&#20381;&#36182;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25512;&#26029;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#65292;&#21457;&#29616;&#24403;&#27809;&#26377;&#20851;&#20110;&#35266;&#27979;&#20540;&#20381;&#36182;&#32467;&#26500;&#30340;&#30693;&#35782;&#26102;&#65292;&#19981;&#33021;&#19968;&#33268;&#22320;&#21306;&#20998;&#24179;&#22343;&#20540;&#65292;&#32780;&#33267;&#23569;&#26377;&#20004;&#20010;&#22823;&#32858;&#31867;&#26159;&#36827;&#34892;&#19968;&#33268;&#30340; $\sqrt{n}$-&#21306;&#20998;&#24179;&#22343;&#20540;&#30340;&#20805;&#20998;&#26465;&#20214;&#65307;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38271;&#26399;&#26041;&#24046;&#19968;&#33268;&#20272;&#35745;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#33267;&#23569;&#26377;&#19968;&#20010;&#22823;&#32858;&#31867;&#26102;&#65292;&#38271;&#26399;&#26041;&#24046;&#19981;&#33021;&#19968;&#33268;&#22320;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#35266;&#27979;&#20540;&#20855;&#26377;&#32676;&#20307;&#20381;&#36182;&#32467;&#26500;&#30340;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#21482;&#26377;&#19968;&#20010;&#22823;&#32858;&#31867;&#26102;&#65292;&#21363;&#30740;&#31350;&#20154;&#21592;&#27809;&#26377;&#35266;&#27979;&#20540;&#20381;&#36182;&#32467;&#26500;&#30340;&#20219;&#20309;&#30693;&#35782;&#26102;&#65292;&#19981;&#33021;&#19968;&#33268;&#22320;&#21306;&#20998;&#24179;&#22343;&#20540;&#12290;&#24403;&#32858;&#31867;&#20869;&#30340;&#35266;&#27979;&#20540;&#28385;&#36275;&#22343;&#21248;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#26102;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20010;&#36827;&#34892; $\sqrt{n}$-&#21306;&#20998;&#24179;&#22343;&#20540;&#30340;&#19968;&#33268;&#30340;&#20805;&#20998;&#26465;&#20214;&#26159;&#33267;&#23569;&#26377;&#20004;&#20010;&#22823;&#32858;&#31867;&#12290;&#36825;&#20010;&#32467;&#26524;&#26174;&#31034;&#20102;&#24403;&#25105;&#20204;&#32570;&#20047;&#26377;&#20851;&#35266;&#27979;&#20540;&#20381;&#36182;&#32467;&#26500;&#20449;&#24687;&#26102;&#36827;&#34892;&#25512;&#26029;&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#32467;&#26524;&#20026;&#38271;&#26399;&#26041;&#24046;&#19968;&#33268;&#20272;&#35745;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#65292;&#24403;&#33267;&#23569;&#26377;&#19968;&#20010;&#22823;&#32858;&#31867;&#26102;&#65292;&#38271;&#26399;&#26041;&#24046;&#19981;&#33021;&#19968;&#33268;&#22320;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on a setting with observations having a cluster dependence structure and presents two main impossibility results. First, we show that when there is only one large cluster, i.e., the researcher does not have any knowledge on the dependence structure of the observations, it is not possible to consistently discriminate the mean. When within-cluster observations satisfy the uniform central limit theorem, we also show that a sufficient condition for consistent $\sqrt{n}$-discrimination of the mean is that we have at least two large clusters. This result shows some limitations for inference when we lack information on the dependence structure of observations. Our second result provides a necessary and sufficient condition for the cluster structure that the long run variance is consistently estimable. Our result implies that when there is at least one large cluster, the long run variance is not consistently estimable.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28216;&#25103;&#30340;&#35748;&#30693;&#35780;&#20272;&#26041;&#27861;Skill Lab&#65292;&#21033;&#29992;&#19968;&#20010;&#27969;&#34892;&#30340;&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#36827;&#34892;&#20840;&#38754;&#39564;&#35777;&#65292;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#27979;&#37327;&#20102;&#24191;&#27867;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;8&#31181;&#35748;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2009.05274</link><description>&lt;p&gt;
&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#27979;&#37327;&#35748;&#30693;&#33021;&#21147;&#65306;&#39564;&#35777;&#19968;&#31181;&#38754;&#21521;&#20154;&#32676;&#30340;&#22522;&#20110;&#28216;&#25103;&#30340;&#35748;&#30693;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Measuring Cognitive Abilities in the Wild: Validating a Population-Scale Game-Based Cognitive Assessment. (arXiv:2009.05274v5 [physics.soc-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.05274
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28216;&#25103;&#30340;&#35748;&#30693;&#35780;&#20272;&#26041;&#27861;Skill Lab&#65292;&#21033;&#29992;&#19968;&#20010;&#27969;&#34892;&#30340;&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#36827;&#34892;&#20840;&#38754;&#39564;&#35777;&#65292;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#27979;&#37327;&#20102;&#24191;&#27867;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;8&#31181;&#35748;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#35748;&#30693;&#34920;&#22411;&#30340;&#24555;&#36895;&#27979;&#37327;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#28508;&#21147;&#65292;&#21487;&#22312;&#20010;&#24615;&#21270;&#23398;&#20064;&#12289;&#23601;&#19994;&#23454;&#36341;&#21644;&#31934;&#20934;&#31934;&#31070;&#30149;&#23398;&#31561;&#24191;&#27867;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;&#20026;&#20102;&#36229;&#36234;&#20256;&#32479;&#23454;&#39564;&#23460;&#23454;&#39564;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#65292;&#20154;&#20204;&#27491;&#22312;&#21162;&#21147;&#22686;&#21152;&#29983;&#24577;&#25928;&#24230;&#21644;&#21442;&#19982;&#32773;&#22810;&#26679;&#24615;&#65292;&#20197;&#25429;&#25417;&#26222;&#36890;&#20154;&#32676;&#20013;&#35748;&#30693;&#33021;&#21147;&#21644;&#34892;&#20026;&#30340;&#20010;&#20307;&#24046;&#24322;&#30340;&#20840;&#37096;&#33539;&#22260;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Skill Lab&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28216;&#25103;&#30340;&#24037;&#20855;&#65292;&#23427;&#22312;&#25552;&#20379;&#24341;&#20154;&#20837;&#32988;&#30340;&#25925;&#20107;&#24773;&#33410;&#30340;&#21516;&#26102;&#35780;&#20272;&#24191;&#27867;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;Skill Lab&#30001;&#20845;&#20010;&#23567;&#28216;&#25103;&#21644;14&#20010;&#24050;&#30693;&#30340;&#35748;&#30693;&#33021;&#21147;&#20219;&#21153;&#32452;&#25104;&#12290;&#21033;&#29992;&#19968;&#20010;&#27969;&#34892;&#30340;&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#65288;N = 10725&#65289;&#65292;&#25105;&#20204;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#22522;&#20110;&#28216;&#25103;&#30340;&#35748;&#30693;&#35780;&#20272;&#12290;&#22522;&#20110;&#28216;&#25103;&#21644;&#39564;&#35777;&#20219;&#21153;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21487;&#38752;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#39044;&#27979;&#20843;&#31181;&#35748;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid individual cognitive phenotyping holds the potential to revolutionize domains as wide-ranging as personalized learning, employment practices, and precision psychiatry. Going beyond limitations imposed by traditional lab-based experiments, new efforts have been underway towards greater ecological validity and participant diversity to capture the full range of individual differences in cognitive abilities and behaviors across the general population. Building on this, we developed Skill Lab, a novel game-based tool that simultaneously assesses a broad suite of cognitive abilities while providing an engaging narrative. Skill Lab consists of six mini-games as well as 14 established cognitive ability tasks. Using a popular citizen science platform (N = 10725), we conducted a comprehensive validation in the wild of a game-based cognitive assessment suite. Based on the game and validation task data, we constructed reliable models to simultaneously predict eight cognitive abilities based 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#21644;&#38750;&#21442;&#25968;&#26816;&#39564;&#26041;&#27861;&#26469;&#26816;&#39564;&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#27169;&#22411;&#20013;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#27835;&#30103;&#22810;&#20540;&#26377;&#24207;&#25110;&#26080;&#24207;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21487;&#20197;&#24110;&#21161;&#26816;&#27979;&#26080;&#25928;&#24037;&#20855;&#20197;&#36991;&#20813;&#22240;&#26524;&#25928;&#24212;&#19981;&#21512;&#29702;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2009.01995</link><description>&lt;p&gt;
&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#30340;&#24037;&#20855;&#26377;&#25928;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Instrument Validity for Heterogeneous Causal Effects. (arXiv:2009.01995v5 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.01995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#21644;&#38750;&#21442;&#25968;&#26816;&#39564;&#26041;&#27861;&#26469;&#26816;&#39564;&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#27169;&#22411;&#20013;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#27835;&#30103;&#22810;&#20540;&#26377;&#24207;&#25110;&#26080;&#24207;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21487;&#20197;&#24110;&#21161;&#26816;&#27979;&#26080;&#25928;&#24037;&#20855;&#20197;&#36991;&#20813;&#22240;&#26524;&#25928;&#24212;&#19981;&#21512;&#29702;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#39564;&#24322;&#36136;&#22240;&#26524;&#25928;&#24212;&#27169;&#22411;&#20013;&#30340;&#24037;&#20855;&#26377;&#25928;&#24615;&#12290;&#36825;&#20010;&#25512;&#24191;&#21253;&#25324;&#20102;&#27835;&#30103;&#21487;&#20197;&#26159;&#22810;&#20540;&#26377;&#24207;&#25110;&#26080;&#24207;&#30340;&#24773;&#20917;&#12290;&#22522;&#20110;&#19968;&#31995;&#21015;&#21487;&#26816;&#39564;&#30340;&#25512;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#26816;&#39564;&#65292;&#34987;&#35777;&#26126;&#20855;&#26377;&#28176;&#36827;&#30340;&#23610;&#23544;&#25511;&#21046;&#21644;&#19968;&#33268;&#24615;&#12290;&#19982;&#25991;&#29486;&#20013;&#30340;&#26816;&#39564;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26816;&#39564;&#21487;&#20197;&#22312;&#26356;&#26222;&#36941;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#65292;&#24182;&#19988;&#21487;&#33021;&#23454;&#29616;&#20102;&#21151;&#29575;&#25552;&#21319;&#12290;&#26816;&#39564;&#36807;&#31243;&#20013;&#30340;&#24037;&#20855;&#26080;&#25928;&#26377;&#21161;&#20110;&#26816;&#27979;&#21487;&#33021;&#23548;&#33268;&#22240;&#26524;&#25928;&#24212;&#19981;&#21512;&#29702;&#30340;&#26080;&#25928;&#24037;&#20855;&#12290;&#36890;&#36807;&#27169;&#25311;&#25552;&#20379;&#20102;&#27979;&#35797;&#22312;&#26377;&#38480;&#26679;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#23545;&#20110;&#23398;&#26657;&#22238;&#25253;&#29575;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25152;&#25552;&#20986;&#30340;&#26816;&#39564;&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;&#36830;&#32493;&#26144;&#23556;&#23450;&#29702;&#21644;&#25193;&#23637;&#30340; delta &#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#30340;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20869;&#23481;&#65292;&#26469;&#24314;&#31435;&#27979;&#35797;&#32479;&#35745;&#37327;&#30340;&#28176;&#36817;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a general framework for testing instrument validity in heterogeneous causal effect models. The generalization includes the cases where the treatment can be multivalued ordered or unordered. Based on a series of testable implications, we propose a nonparametric test which is proved to be asymptotically size controlled and consistent. Compared to the tests in the literature, our test can be applied in more general settings and may achieve power improvement. Refutation of instrument validity by the test helps detect invalid instruments that may yield implausible results on causal effects. Evidence that the test performs well on finite samples is provided via simulations. We revisit the empirical study on return to schooling to demonstrate application of the proposed test in practice. An extended continuous mapping theorem and an extended delta method, which may be of independent interest, are provided to establish the asymptotic distribution of the test statistic under
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26679;&#26412;&#25286;&#20998;&#30340;&#20803;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#35780;&#20272;&#24635;&#20307;&#39118;&#38505;&#26102;&#32771;&#34385;&#24178;&#25200;&#21442;&#25968;&#65292;&#24182;&#19988;&#23454;&#29616;&#30340;&#36229;&#39069;&#39118;&#38505;&#30028;&#30340;&#24433;&#21709;&#20026;&#20108;&#27425;&#12290;</title><link>http://arxiv.org/abs/1901.09036</link><description>&lt;p&gt;
&#27491;&#20132;&#32479;&#35745;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Orthogonal Statistical Learning. (arXiv:1901.09036v4 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.09036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26679;&#26412;&#25286;&#20998;&#30340;&#20803;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#35780;&#20272;&#24635;&#20307;&#39118;&#38505;&#26102;&#32771;&#34385;&#24178;&#25200;&#21442;&#25968;&#65292;&#24182;&#19988;&#23454;&#29616;&#30340;&#36229;&#39069;&#39118;&#38505;&#30028;&#30340;&#24433;&#21709;&#20026;&#20108;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#32479;&#35745;&#23398;&#20064;&#30340;&#35774;&#32622;&#19979;&#25552;&#20379;&#20102;&#20851;&#20110;&#38750;&#28176;&#36817;&#36229;&#39069;&#39118;&#38505;&#20445;&#35777;&#65292;&#20854;&#20013;&#30446;&#26631;&#21442;&#25968;&#25152;&#35780;&#20272;&#30340;&#24635;&#20307;&#39118;&#38505;&#21462;&#20915;&#20110;&#24517;&#39035;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#30340;&#26410;&#30693;&#24178;&#25200;&#21442;&#25968;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26679;&#26412;&#25286;&#20998;&#30340;&#20803;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#20219;&#24847;&#20272;&#35745;&#30446;&#26631;&#21442;&#25968;&#21644;&#24178;&#25200;&#21442;&#25968;&#30340;&#31639;&#27861;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#24635;&#20307;&#39118;&#38505;&#28385;&#36275;&#19968;&#20010;&#31216;&#20026;Neyman&#27491;&#20132;&#24615;&#30340;&#26465;&#20214;&#65292;&#21017;&#24178;&#25200;&#20272;&#35745;&#35823;&#24046;&#23545;&#20803;&#31639;&#27861;&#23454;&#29616;&#30340;&#36229;&#39069;&#39118;&#38505;&#30028;&#30340;&#24433;&#21709;&#20026;&#20108;&#27425;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#19981;&#20851;&#24515;&#29992;&#20110;&#30446;&#26631;&#21644;&#24178;&#25200;&#30340;&#29305;&#23450;&#31639;&#27861;&#65292;&#21482;&#20570;&#20986;&#20102;&#26377;&#20851;&#23427;&#20204;&#21508;&#33258;&#24615;&#33021;&#30340;&#20551;&#35774;&#12290;&#36825;&#26679;&#65292;&#23601;&#21487;&#20197;&#21033;&#29992;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#30340;&#22823;&#37327;&#32467;&#26524;&#65292;&#20026;&#24102;&#26377;&#24178;&#25200;&#32452;&#25104;&#30340;&#23398;&#20064;&#25552;&#20379;&#26032;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20851;&#27880;&#36229;&#39069;&#39118;&#38505;&#32780;&#19981;&#26159;&#21442;&#25968;&#20272;&#35745;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#24369;&#21270;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide non-asymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input arbitrary estimation algorithms for the target parameter and nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can provide rates under weaker a
&lt;/p&gt;</description></item></channel></rss>