<rss version="2.0"><channel><title>Chat Arxiv cs.SD</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.SD</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.05737</link><description>&lt;p&gt;
&#20020;&#24202;BERTScore&#65306;&#20020;&#24202;&#29615;&#22659;&#19979;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#25913;&#36827;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26377;&#28508;&#21147;&#33410;&#30465;&#26102;&#38388;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#25552;&#39640;&#25253;&#21578;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#21307;&#29983;&#30340;&#30130;&#21171;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36991;&#20813;&#21307;&#23398;&#30456;&#20851;&#30340;&#36716;&#24405;&#38169;&#35823;&#30340;&#37325;&#35201;&#24615;&#65292;&#21307;&#30103;&#34892;&#19994;&#37319;&#29992;&#36825;&#31181;&#25216;&#26415;&#30340;&#36895;&#24230;&#36739;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;ASR&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#65288;WER&#12289;BLUE&#12289;METEOR&#31561;&#65289;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24230;&#37327;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#65292;&#26377;&#26102;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.03857</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#65306;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pre-trained AudioLDM for Text to Sound Generation: A Benchmark Study. (arXiv:2303.03857v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#65292;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the advantages of using pre-trained AudioLDM as the backbone for sound generation, demonstrates the benefits of using pre-trained models for text-to-sound generation in data-scarcity scenarios, and evaluates various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols to provide a basis for future research.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#36817;&#22312;&#25991;&#26412;&#25552;&#31034;&#19979;&#23454;&#29616;&#20102;&#22768;&#38899;&#29983;&#25104;&#30340;&#31361;&#30772;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#34920;&#29616;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#24403;&#21069;&#30340;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#36807;&#24230;&#25311;&#21512;&#65289;&#19978;&#38754;&#20020;&#38382;&#39064;&#65292;&#20174;&#32780;&#26174;&#33879;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;AudioLDM&#20316;&#20026;&#22768;&#38899;&#29983;&#25104;&#30340;&#39592;&#24178;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65288;&#20363;&#22914;&#35757;&#32451;&#26465;&#20214;&#65289;&#21487;&#33021;&#20250;&#24433;&#21709;AudioLDM&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#22312;&#20960;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#21516;&#30340;&#35780;&#20272;&#21327;&#35758;&#35780;&#20272;&#20102;&#21508;&#31181;&#25991;&#26412;&#21040;&#22768;&#38899;&#29983;&#25104;&#31995;&#32479;&#65292;&#36825;&#20123;&#21327;&#35758;&#20801;&#35768;&#22312;&#20849;&#21516;&#22522;&#30784;&#19978;&#20844;&#24179;&#27604;&#36739;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have recently achieved breakthroughs in sound generation with text prompts. Despite their promising performance, current text-to-sound generation models face issues on small-scale datasets (e.g., overfitting), significantly limiting their performance. In this paper, we investigate the use of pre-trained AudioLDM, the state-of-the-art model for text-to-audio generation, as the backbone for sound generation. Our study demonstrates the advantages of using pre-trained models for text-to-sound generation, especially in data-scarcity scenarios. In addition, experiments show that different training strategies (e.g., training conditions) may affect the performance of AudioLDM on datasets of different scales. To facilitate future studies, we also evaluate various text-to-sound generation systems on several frequently used datasets under the same evaluation protocols, which allow fair comparisons and benchmarking of these methods on the common ground.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#65292;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.02665</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#23398;&#20064;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Learning for Acoustic Event Classification. (arXiv:2303.02665v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#65292;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new model, Heterogeneous Graph Crossmodal Network (HGCN), which learns crossmodal edges and can adapt to various spatial and temporal scales, effectively connecting relevant nodes across modalities. It achieves state-of-the-art performance in acoustic event classification.
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#32039;&#20945;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#26469;&#24314;&#27169;&#28041;&#21450;&#22810;&#20010;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#24314;&#27169;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#25104;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22270;&#32467;&#26500;&#22312;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#20013;&#24182;&#19981;&#33258;&#28982;&#12290;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#30340;&#22270;&#26159;&#25163;&#21160;&#26500;&#24314;&#30340;&#65292;&#36825;&#26082;&#22256;&#38590;&#21448;&#27425;&#20248;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#65288;i&#65289;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#21270;&#22270;&#26500;&#24314;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#22240;&#20026;&#23427;&#26159;&#21442;&#25968;&#21270;&#26500;&#24314;&#30340;&#65292;&#32780;&#21487;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#36793;&#32536;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#12290;&#22312;&#19968;&#20010;&#22823;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;AudioSet&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#65288;0.53&#24179;&#22343;&#31934;&#24230;&#65289;&#65292;&#20248;&#20110;transfo&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graphs provide a compact, efficient, and scalable way to model data involving multiple disparate modalities. This makes modeling audiovisual data using heterogeneous graphs an attractive option. However, graph structure does not appear naturally in audiovisual data. Graphs for audiovisual data are constructed manually which is both difficult and sub-optimal. In this work, we address this problem by (i) proposing a parametric graph construction strategy for the intra-modal edges, and (ii) learning the crossmodal edges. To this end, we develop a new model, heterogeneous graph crossmodal network (HGCN) that learns the crossmodal edges. Our proposed model can adapt to various spatial and temporal scales owing to its parametric construction, while the learnable crossmodal edges effectively connect the relevant nodes across modalities. Experiments on a large benchmark dataset (AudioSet) show that our model is state-of-the-art (0.53 mean average precision), outperforming transfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2303.01508</link><description>&lt;p&gt;
&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#25511;&#21046;&#65306;&#23398;&#20064;&#25490;&#21517;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#24773;&#24863;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Emotional Control of Text-To-Speech: Learning To Rank Inter- And Intra-Class Emotion Intensities. (arXiv:2303.01508v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a fine-grained controllable emotional TTS, that considers both interand intra-class distances and be able to synthesize speech with recognizable intensity difference.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#35821;&#38899;&#36890;&#24120;&#22312;&#24773;&#24863;&#34920;&#36798;&#19978;&#26159;&#20013;&#24615;&#30340;&#65292;&#32780;&#24456;&#22810;&#26102;&#20505;&#20154;&#20204;&#24076;&#26395;&#23545;&#21333;&#35789;&#25110;&#38899;&#32032;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#24773;&#24863;&#25511;&#21046;&#12290;&#34429;&#28982;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#26368;&#36817;&#24050;&#32463;&#25552;&#20986;&#20102;&#31532;&#19968;&#25209;TTS&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#25163;&#21160;&#20998;&#37197;&#24773;&#24863;&#24378;&#24230;&#26469;&#25511;&#21046;&#35821;&#38899;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#24573;&#30053;&#20102;&#20869;&#37096;&#31867;&#36317;&#31163;&#65292;&#24378;&#24230;&#24046;&#24322;&#32463;&#24120;&#26080;&#27861;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#21487;&#25511;&#24773;&#24863;TTS&#65292;&#32771;&#34385;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#31867;&#36317;&#31163;&#65292;&#24182;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#21487;&#35782;&#21035;&#24378;&#24230;&#24046;&#24322;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#20027;&#35266;&#21644;&#23458;&#35266;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21487;&#25511;&#24615;&#12289;&#24773;&#24863;&#34920;&#36798;&#21644;&#33258;&#28982;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#21487;&#25511;TTS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art Text-To-Speech (TTS) models are capable of producing high-quality speech. The generated speech, however, is usually neutral in emotional expression, whereas very often one would want fine-grained emotional control of words or phonemes. Although still challenging, the first TTS models have been recently proposed that are able to control voice by manually assigning emotion intensity. Unfortunately, due to the neglect of intra-class distance, the intensity differences are often unrecognizable. In this paper, we propose a fine-grained controllable emotional TTS, that considers both interand intra-class distances and be able to synthesize speech with recognizable intensity difference. Our subjective and objective experiments demonstrate that our model exceeds two state-of-the-art controllable TTS models for controllability, emotion expressiveness and naturalness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22768;&#38899;&#21305;&#37197;&#31639;&#27861;&#65292;&#31216;&#20026;&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#25439;&#22833;&#65288;PNP&#65289;&#65292;&#23427;&#26159;&#39057;&#35889;&#25439;&#22833;&#30340;&#26368;&#20248;&#20108;&#27425;&#36817;&#20284;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#21442;&#25968;&#30340;&#24863;&#30693;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2301.02886</link><description>&lt;p&gt;
&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#22768;&#38899;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Perceptual-Neural-Physical Sound Matching. (arXiv:2301.02886v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22768;&#38899;&#21305;&#37197;&#31639;&#27861;&#65292;&#31216;&#20026;&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#25439;&#22833;&#65288;PNP&#65289;&#65292;&#23427;&#26159;&#39057;&#35889;&#25439;&#22833;&#30340;&#26368;&#20248;&#20108;&#27425;&#36817;&#20284;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#21442;&#25968;&#30340;&#24863;&#30693;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new sound matching algorithm called Perceptual-Neural-Physical loss (PNP), which is the optimal quadratic approximation of spectral loss and can better accommodate the differing perceptual significance of each parameter while having fast convergence.
&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#21305;&#37197;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#21442;&#25968;&#21270;&#38899;&#39057;&#21512;&#25104;&#26469;&#36817;&#20284;&#30446;&#26631;&#27874;&#24418;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21305;&#37197;&#25345;&#32493;&#35856;&#27874;&#38899;&#35843;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#30446;&#26631;&#26159;&#38750;&#24179;&#31283;&#21644;&#38750;&#35856;&#27874;&#30340;&#26102;&#20505;&#65292;&#20363;&#22914;&#25171;&#20987;&#20048;&#22120;&#65292;&#20219;&#21153;&#23601;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24402;&#22240;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#19981;&#36275;&#12290;&#19968;&#26041;&#38754;&#65292;&#21442;&#25968;&#22495;&#20013;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#31216;&#20026;&#8220;P-loss&#8221;&#65292;&#31616;&#21333;&#24555;&#36895;&#65292;&#20294;&#26410;&#33021;&#36866;&#24212;&#27599;&#20010;&#21442;&#25968;&#30340;&#19981;&#21516;&#24863;&#30693;&#37325;&#35201;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#39057;&#35889;&#26102;&#38388;&#22495;&#20013;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#31216;&#20026;&#8220;&#39057;&#35889;&#25439;&#22833;&#8221;&#65292;&#22312;&#24863;&#30693;&#19978;&#26159;&#26377;&#21160;&#26426;&#30340;&#65292;&#24182;&#22312;&#21487;&#24494;&#20998;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DDSP&#65289;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#39057;&#35889;&#25439;&#22833;&#26159;&#38899;&#39640;&#38388;&#38548;&#30340;&#19981;&#33391;&#39044;&#27979;&#22240;&#32032;&#65292;&#20854;&#26799;&#24230;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#22312;&#36825;&#20010;&#22256;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#25439;&#22833;&#65288;PNP&#65289;&#12290;PNP&#26159;&#39057;&#35889;&#25439;&#22833;&#30340;&#26368;&#20248;&#20108;&#27425;&#36817;&#20284;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound matching algorithms seek to approximate a target waveform by parametric audio synthesis. Deep neural networks have achieved promising results in matching sustained harmonic tones. However, the task is more challenging when targets are nonstationary and inharmonic, e.g., percussion. We attribute this problem to the inadequacy of loss function. On one hand, mean square error in the parametric domain, known as "P-loss", is simple and fast but fails to accommodate the differing perceptual significance of each parameter. On the other hand, mean square error in the spectrotemporal domain, known as "spectral loss", is perceptually motivated and serves in differentiable digital signal processing (DDSP). Yet, spectral loss is a poor predictor of pitch intervals and its gradient may be computationally expensive; hence a slow convergence. Against this conundrum, we present Perceptual-Neural-Physical loss (PNP). PNP is the optimal quadratic approximation of spectral loss while being as fast 
&lt;/p&gt;</description></item><item><title>&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#65292;&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2212.07738</link><description>&lt;p&gt;
&#19968;&#20221;&#22823;&#35268;&#27169;&#30340;&#12289;&#22522;&#20110;PCR&#30340;COVID-19&#22768;&#38899;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A large-scale and PCR-referenced vocal audio dataset for COVID-19. (arXiv:2212.07738v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07738
&lt;/p&gt;
&lt;p&gt;
&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#65292;&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The UK COVID-19 Vocal Audio Dataset is the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date, designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio.
&lt;/p&gt;
&lt;p&gt;
&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;&#33521;&#22269;&#21355;&#29983;&#23433;&#20840;&#23616;&#36890;&#36807;&#22269;&#23478;&#27979;&#35797;&#21644;&#36861;&#36394;&#35745;&#21010;&#21644;REACT-1&#35843;&#26597;&#22312;2021&#24180;3&#26376;&#33267;2022&#24180;3&#26376;&#26399;&#38388;&#25307;&#21215;&#20102;&#33258;&#24895;&#21442;&#19982;&#32773;&#65292;&#25910;&#38598;&#20102;&#33258;&#24895;&#21683;&#22013;&#12289;&#21628;&#27668;&#21644;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;SARS-CoV-2&#26816;&#27979;&#32467;&#26524;&#30456;&#20851;&#32852;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The UK COVID-19 Vocal Audio Dataset is designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio. The UK Health Security Agency recruited voluntary participants through the national Test and Trace programme and the REACT-1 survey in England from March 2021 to March 2022, during dominant transmission of the Alpha and Delta SARS-CoV-2 variants and some Omicron variant sublineages. Audio recordings of volitional coughs, exhalations, and speech were collected in the 'Speak up to help beat coronavirus' digital survey alongside demographic, self-reported symptom and respiratory condition data, and linked to SARS-CoV-2 test results. The UK COVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date. PCR results were linked to 70,794 of 72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms were reported by 45.6
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#31070;&#32463;&#36716;&#24405;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#36880;&#20010;&#26679;&#26412;&#35745;&#31639;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#22312;&#19982;&#40664;&#35748;&#25209;&#37327;&#35745;&#31639;&#30456;&#27604;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.16270</link><description>&lt;p&gt;
&#31070;&#32463;&#36716;&#24405;&#22120;&#35757;&#32451;&#65306;&#37319;&#29992;&#36880;&#26679;&#26412;&#35745;&#31639;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;
Neural Transducer Training: Reduced Memory Consumption with Sample-wise Computation. (arXiv:2211.16270v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#31070;&#32463;&#36716;&#24405;&#22120;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#36880;&#20010;&#26679;&#26412;&#35745;&#31639;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#22312;&#19982;&#40664;&#35748;&#25209;&#37327;&#35745;&#31639;&#30456;&#27604;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a memory-efficient training method for neural transducer, which computes the transducer loss and gradients sample by sample, significantly reducing memory usage and performing at competitive speed compared to the default batched computation.
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36716;&#24405;&#22120;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;&#34429;&#28982;&#35813;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#27969;&#24335;ASR&#65292;&#20294;&#35757;&#32451;&#36807;&#31243;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20869;&#23384;&#38656;&#27714;&#21487;&#33021;&#20250;&#36805;&#36895;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;GPU&#30340;&#23481;&#37327;&#65292;&#38480;&#21046;&#25209;&#37327;&#22823;&#23567;&#21644;&#24207;&#21015;&#38271;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20856;&#22411;&#36716;&#24405;&#22120;&#35757;&#32451;&#35774;&#32622;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36880;&#20010;&#26679;&#26412;&#35745;&#31639;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#22686;&#21152;&#36880;&#26679;&#26412;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#24182;&#34892;&#24615;&#12290;&#22312;&#19968;&#32452;&#24443;&#24213;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36880;&#26679;&#26412;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#24182;&#22312;&#19982;&#40664;&#35748;&#25209;&#37327;&#35745;&#31639;&#30456;&#27604;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#36895;&#24230;&#12290;&#20316;&#20026;&#20142;&#28857;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#20351;&#29992;&#20165;6 GB&#30340;&#20869;&#23384;&#35745;&#31639;&#20102;&#25209;&#37327;&#22823;&#23567;&#20026;1024&#65292;&#38899;&#39057;&#38271;&#24230;&#20026;40&#31186;&#30340;&#36716;&#24405;&#22120;&#25439;&#22833;&#21644;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural transducer is an end-to-end model for automatic speech recognition (ASR). While the model is well-suited for streaming ASR, the training process remains challenging. During training, the memory requirements may quickly exceed the capacity of state-of-the-art GPUs, limiting batch size and sequence lengths. In this work, we analyze the time and space complexity of a typical transducer training setup. We propose a memory-efficient training method that computes the transducer loss and gradients sample by sample. We present optimizations to increase the efficiency and parallelism of the sample-wise method. In a set of thorough benchmarks, we show that our sample-wise method significantly reduces memory usage, and performs at competitive speed when compared to the default batched computation. As a highlight, we manage to compute the transducer loss and gradients for a batch size of 1024, and audio length of 40 seconds, using only 6 GB of memory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DL&#30340;Mel-Subband&#26102;&#31354;&#27874;&#26463;&#25104;&#24418;&#22120;&#65292;&#29992;&#20110;&#22312;&#36710;&#36733;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#65292;&#36890;&#36807;&#22522;&#20110;Mel&#23610;&#24230;&#30340;&#23376;&#24102;&#36873;&#25321;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#20302;&#39057;&#30340;&#32454;&#31890;&#24230;&#22788;&#29702;&#21644;&#23545;&#39640;&#39057;&#30340;&#31895;&#31890;&#24230;&#22788;&#29702;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2211.12590</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;Mel-Subband&#27874;&#26463;&#25104;&#24418;&#22120;&#29992;&#20110;&#36710;&#36733;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Mel-Subband Beamformer for In-car Speech Separation. (arXiv:2211.12590v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DL&#30340;Mel-Subband&#26102;&#31354;&#27874;&#26463;&#25104;&#24418;&#22120;&#65292;&#29992;&#20110;&#22312;&#36710;&#36733;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#65292;&#36890;&#36807;&#22522;&#20110;Mel&#23610;&#24230;&#30340;&#23376;&#24102;&#36873;&#25321;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#20302;&#39057;&#30340;&#32454;&#31890;&#24230;&#22788;&#29702;&#21644;&#23545;&#39640;&#39057;&#30340;&#31895;&#31890;&#24230;&#22788;&#29702;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a DL-based Mel-Subband spatio-temporal beamformer for speech separation in a car environment, which reduces computational costs and inference time by using a Mel-scale based subband selection strategy for fine-grained processing of lower frequencies and coarse-grained processing of higher frequencies.
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22522;&#20110;&#27874;&#26463;&#25104;&#24418;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#35821;&#38899;&#20998;&#31163;&#20013;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#34987;&#35774;&#35745;&#20026;&#29420;&#31435;&#22788;&#29702;&#31364;&#24102;&#65288;NB&#65289;&#39057;&#29575;&#65292;&#36825;&#23548;&#33268;&#26356;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#21512;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;DL&#30340;Mel-Subband&#26102;&#31354;&#27874;&#26463;&#25104;&#24418;&#22120;&#65292;&#20197;&#22312;&#36710;&#36733;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#19982;&#20256;&#32479;&#30340;&#23376;&#24102;&#65288;SB&#65289;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#22522;&#20110;Mel&#23610;&#24230;&#30340;&#23376;&#24102;&#36873;&#25321;&#31574;&#30053;&#65292;&#30830;&#20445;&#23545;&#22823;&#22810;&#25968;&#35821;&#38899;&#20849;&#25391;&#32467;&#26500;&#23384;&#22312;&#30340;&#20302;&#39057;&#36827;&#34892;&#32454;&#31890;&#24230;&#22788;&#29702;&#65292;&#23545;&#39640;&#39057;&#36827;&#34892;&#31895;&#31890;&#24230;&#22788;&#29702;&#12290;&#20197;&#36882;&#24402;&#26041;&#24335;&#65292;&#20174;&#20272;&#35745;&#30340;&#23376;&#24102;&#35821;&#38899;&#21644;&#22122;&#22768;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#30830;&#23450;&#27599;&#20010;&#25196;&#22768;&#22120;&#20301;&#32622;/&#21306;&#22495;&#30340;&#40065;&#26834;&#24103;&#32423;&#27874;&#26463;&#25104;&#24418;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#20272;&#35745;&#24182;&#25233;&#21046;&#20219;&#20309;&#22238;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
While current deep learning (DL)-based beamforming techniques have been proved effective in speech separation, they are often designed to process narrow-band (NB) frequencies independently which results in higher computational costs and inference times, making them unsuitable for real-world use. In this paper, we propose DL-based mel-subband spatio-temporal beamformer to perform speech separation in a car environment with reduced computation cost and inference time. As opposed to conventional subband (SB) approaches, our framework uses a mel-scale based subband selection strategy which ensures a fine-grained processing for lower frequencies where most speech formant structure is present, and coarse-grained processing for higher frequencies. In a recursive way, robust frame-level beamforming weights are determined for each speaker location/zone in a car from the estimated subband speech and noise covariance matrices. Furthermore, proposed framework also estimates and suppresses any echo
&lt;/p&gt;</description></item><item><title>LA-VocE&#26159;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#23558;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;&#30340;mel&#39057;&#35889;&#22270;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#21644;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2211.10999</link><description>&lt;p&gt;
LA-VocE: &#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#20302;&#20449;&#22122;&#27604;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders. (arXiv:2211.10999v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10999
&lt;/p&gt;
&lt;p&gt;
LA-VocE&#26159;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#23558;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;&#30340;mel&#39057;&#35889;&#22270;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#21644;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
LA-VocE is a new audio-visual speech enhancement method that uses a neural vocoder to convert mel-spectrograms predicted from noisy audio-visual speech via a transformer-based architecture into waveform audio, and is applicable to multiple languages and different levels of background noise and speech interference.
&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#26412;&#36523;&#20197;&#21450;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#21767;&#37096;&#36816;&#21160;&#20174;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#35821;&#38899;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#27604;&#20165;&#20351;&#29992;&#38899;&#39057;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28040;&#38500;&#24178;&#25200;&#35821;&#38899;&#12290;&#23613;&#31649;&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#38899;&#39057;&#35270;&#35273;&#26041;&#27861;&#20173;&#28982;&#20351;&#29992;&#39057;&#35889;&#26144;&#23556;/&#25513;&#34109;&#26469;&#37325;&#29616;&#24178;&#20928;&#30340;&#38899;&#39057;&#65292;&#36890;&#24120;&#20250;&#22312;&#29616;&#26377;&#30340;&#35821;&#38899;&#22686;&#24378;&#26550;&#26500;&#20013;&#28155;&#21152;&#35270;&#35273;&#39592;&#24178;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LA-VocE&#65292;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;mel&#39057;&#35889;&#22270;&#65292;&#28982;&#21518;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#65288;HiFi-GAN&#65289;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#12290;&#25105;&#20204;&#22312;&#25968;&#21315;&#20010;&#35828;&#35805;&#32773;&#21644;11&#31181;&#20197;&#19978;&#19981;&#21516;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#25105;&#20204;&#30340;&#27169;&#22411;&#36866;&#24212;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Audio-visual speech enhancement aims to extract clean speech from a noisy environment by leveraging not only the audio itself but also the target speaker's lip movements. This approach has been shown to yield improvements over audio-only speech enhancement, particularly for the removal of interfering speech. Despite recent advances in speech synthesis, most audio-visual approaches continue to use spectral mapping/masking to reproduce the clean audio, often resulting in visual backbones added to existing speech enhancement architectures. In this work, we propose LA-VocE, a new two-stage approach that predicts mel-spectrograms from noisy audio-visual speech via a transformer-based architecture, and then converts them into waveform audio using a neural vocoder (HiFi-GAN). We train and evaluate our framework on thousands of speakers and 11+ different languages, and study our model's ability to adapt to different levels of background noise and speech interference. Our experiments show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#33539;&#24335;&#20013;&#23581;&#35797;Conformer&#26550;&#26500;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#33258;&#30417;&#30563;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#12290;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#36739;&#20302;&#23618;&#20013;&#26368;&#20248;&#22320;&#32534;&#30721;&#20102;&#35821;&#35328;&#21306;&#20998;&#20449;&#24687;&#65292;&#20174;&#36825;&#20123;&#23618;&#33719;&#24471;&#30340;&#23884;&#20837;&#33021;&#22815;&#26174;&#33879;&#22320;&#31283;&#20581;&#22320;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#22768;&#23398;&#29615;&#22659;&#12290;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;Conformer&#27169;&#22411;&#22312;VoxLingua107&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#35782;&#21035;&#31995;&#32479;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#19988;&#20351;&#29992;&#30340;&#21442;&#25968;&#37327;&#20165;&#20026;&#20854;&#23427;&#27169;&#22411;&#30340;&#20116;&#20998;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2211.05103</link><description>&lt;p&gt;
&#24847;&#22806;&#23398;&#20064;&#32773;&#65306;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21475;&#35821;&#35821;&#35328;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models. (arXiv:2211.05103v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#33539;&#24335;&#20013;&#23581;&#35797;Conformer&#26550;&#26500;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#33258;&#30417;&#30563;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#12290;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#36739;&#20302;&#23618;&#20013;&#26368;&#20248;&#22320;&#32534;&#30721;&#20102;&#35821;&#35328;&#21306;&#20998;&#20449;&#24687;&#65292;&#20174;&#36825;&#20123;&#23618;&#33719;&#24471;&#30340;&#23884;&#20837;&#33021;&#22815;&#26174;&#33879;&#22320;&#31283;&#20581;&#22320;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#22768;&#23398;&#29615;&#22659;&#12290;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;Conformer&#27169;&#22411;&#22312;VoxLingua107&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#35782;&#21035;&#31995;&#32479;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#19988;&#20351;&#29992;&#30340;&#21442;&#25968;&#37327;&#20165;&#20026;&#20854;&#23427;&#27169;&#22411;&#30340;&#20116;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper extends previous self-supervised approaches for language identification by experimenting with Conformer based architecture in a multilingual pre-training paradigm. The pre-trained speech models optimally encode language discriminatory information in lower layers, and the embeddings obtained from these layers are significantly robust to classify unseen languages and different acoustic environments without additional training. After fine-tuning a pre-trained Conformer model on the VoxLingua107 dataset, the authors achieve results similar to current state-of-the-art systems for language identification, with 5x less parameters. The model is open-sourced through the NVIDIA NeMo toolkit.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#33539;&#24335;&#20013;&#23581;&#35797;Conformer&#26550;&#26500;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#33258;&#30417;&#30563;&#35821;&#35328;&#35782;&#21035;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#22312;&#36739;&#20302;&#23618;&#20013;&#26368;&#20248;&#22320;&#32534;&#30721;&#20102;&#35821;&#35328;&#21306;&#20998;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;&#36825;&#20123;&#23618;&#33719;&#24471;&#30340;&#23884;&#20837;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#26174;&#33879;&#22320;&#31283;&#20581;&#22320;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#22768;&#23398;&#29615;&#22659;&#12290;&#22312;&#23545;&#39044;&#35757;&#32451;&#30340;Conformer&#27169;&#22411;&#22312;VoxLingua107&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#35782;&#21035;&#31995;&#32479;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#30340;&#21442;&#25968;&#37327;&#20165;&#20026;&#20854;&#23427;&#27169;&#22411;&#30340;&#20116;&#20998;&#20043;&#19968;&#12290;&#25105;&#20204;&#36890;&#36807;NVIDIA NeMo&#24037;&#20855;&#21253;&#24320;&#28304;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we extend previous self-supervised approaches for language identification by experimenting with Conformer based architecture in a multilingual pre-training paradigm. We find that pre-trained speech models optimally encode language discriminatory information in lower layers. Further, we demonstrate that the embeddings obtained from these layers are significantly robust to classify unseen languages and different acoustic environments without additional training. After fine-tuning a pre-trained Conformer model on the VoxLingua107 dataset, we achieve results similar to current state-of-the-art systems for language identification. More, our model accomplishes this with 5x less parameters. We open-source the model through the NVIDIA NeMo toolkit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#21644;&#22122;&#22768;&#40065;&#26834;&#30340;&#21548;&#35273;&#26816;&#26597;&#31995;&#32479;&#65292;&#29992;&#20110;&#26816;&#27979;&#26426;&#26800;&#37096;&#20214;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#21253;&#32476;&#29305;&#24449;&#19982;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#24212;&#29992;&#26102;&#21464;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#20154;&#12290;</title><link>http://arxiv.org/abs/2211.01704</link><description>&lt;p&gt;
&#21435;&#38500;&#22122;&#38899;&#65306;&#24515;&#29702;&#22768;&#23398;&#21644;&#22522;&#20110;&#21253;&#32476;&#30340;&#29305;&#24449;&#22312;&#26426;&#26800;&#25925;&#38556;&#26816;&#27979;&#20013;&#30340;&#23454;&#35777;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Cutting Through the Noise: An Empirical Comparison of Psychoacoustic and Envelope-based Features for Machinery Fault Detection. (arXiv:2211.01704v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#21644;&#22122;&#22768;&#40065;&#26834;&#30340;&#21548;&#35273;&#26816;&#26597;&#31995;&#32479;&#65292;&#29992;&#20110;&#26816;&#27979;&#26426;&#26800;&#37096;&#20214;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#21253;&#32476;&#29305;&#24449;&#19982;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#24212;&#29992;&#26102;&#21464;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an automated and noise-robust auditory inspection system for detecting the health condition of mechanical parts. A benchmark is provided to compare different types of envelope features with psychoacoustic features. The authors are the first to apply time-varying psychoacoustic features for fault detection.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22768;&#23398;&#30340;&#25925;&#38556;&#26816;&#27979;&#20855;&#26377;&#30417;&#27979;&#26426;&#26800;&#37096;&#20214;&#20581;&#24247;&#29366;&#20917;&#30340;&#39640;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24037;&#19994;&#29615;&#22659;&#30340;&#32972;&#26223;&#22122;&#38899;&#21487;&#33021;&#20250;&#23545;&#25925;&#38556;&#26816;&#27979;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#30446;&#21069;&#23545;&#20110;&#25552;&#39640;&#25925;&#38556;&#26816;&#27979;&#23545;&#24037;&#19994;&#29615;&#22659;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lenze&#29983;&#20135;&#32972;&#26223;&#22122;&#22768;&#65288;LPBN&#65289;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#40831;&#36718;&#30005;&#26426;&#26411;&#31471;&#26816;&#26597;&#30340;&#33258;&#21160;&#21270;&#21644;&#22122;&#22768;&#40065;&#26834;&#30340;&#21548;&#35273;&#26816;&#26597;&#65288;ARAI&#65289;&#31995;&#32479;&#12290;&#37319;&#29992;&#22768;&#23398;&#38453;&#21015;&#20174;&#20855;&#26377;&#36731;&#24494;&#25925;&#38556;&#12289;&#37325;&#22823;&#25925;&#38556;&#25110;&#20581;&#24247;&#30340;&#30005;&#26426;&#20013;&#33719;&#21462;&#25968;&#25454;&#12290;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#22522;&#20110;&#19987;&#23478;&#23545;&#40831;&#36718;&#31665;&#30340;&#30693;&#35782;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#21253;&#32476;&#29305;&#24449;&#19982;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#24212;&#29992;&#26102;&#21464;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#20154;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#26469;&#33258;&#20581;&#24247;&#30005;&#26426;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acoustic-based fault detection has a high potential to monitor the health condition of mechanical parts. However, the background noise of an industrial environment may negatively influence the performance of fault detection. Limited attention has been paid to improving the robustness of fault detection against industrial environmental noise. Therefore, we present the Lenze production background-noise (LPBN) real-world dataset and an automated and noise-robust auditory inspection (ARAI) system for the end-of-line inspection of geared motors. An acoustic array is used to acquire data from motors with a minor fault, major fault, or which are healthy. A benchmark is provided to compare the psychoacoustic features with different types of envelope features based on expert knowledge of the gearbox. To the best of our knowledge, we are the first to apply time-varying psychoacoustic features for fault detection. We train a state-of-the-art one-class-classifier, on samples from healthy motors an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#31867;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2210.16192</link><description>&lt;p&gt;
&#21033;&#29992;&#20803;&#25968;&#25454;&#21644;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Learning Audio Features with Metadata and Contrastive Learning. (arXiv:2210.16192v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#31867;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uses supervised contrastive learning combined with available metadata to solve multiple pretext tasks that learn a good representation of data. Learning representations using only metadata obtains similar performance as using cross entropy with class labels only. State-of-the-art score is obtained when combining class labels with metadata using multiple supervised contrastive learning.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#37322;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19968;&#30452;&#26159;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;ICBHI&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#21512;&#36825;&#31181;&#24773;&#20917;&#30340;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#65292;&#32780;&#19981;&#20351;&#29992;&#31867;&#26631;&#31614;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#36825;&#20123;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#30417;&#30563;&#23545;&#27604;&#35774;&#32622;&#20013;&#20351;&#29992;&#22810;&#20010;&#20803;&#25968;&#25454;&#28304;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#19981;&#24179;&#34913;&#21644;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods based on supervised learning using annotations in an end-to-end fashion have been the state-of-the-art for classification problems. However, they may be limited in their generalization capability, especially in the low data regime. In this study, we address this issue using supervised contrastive learning combined with available metadata to solve multiple pretext tasks that learn a good representation of data. We apply our approach on ICBHI, a respiratory sound classification dataset suited for this setting. We show that learning representations using only metadata, without class labels, obtains similar performance as using cross entropy with those labels only. In addition, we obtain state-of-the-art score when combining class labels with metadata using multiple supervised contrastive learning. This work suggests the potential of using multiple metadata sources in supervised contrastive settings, in particular in settings with class imbalance and few data. Our code is released 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2210.15173</link><description>&lt;p&gt;
Articulation GAN: &#26080;&#30417;&#30563;&#24314;&#27169;&#20851;&#33410;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Articulation GAN: Unsupervised modeling of articulatory learning. (arXiv:2210.15173v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#27169;&#25311;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new unsupervised generative model that learns to generate articulatory representations (electromagnetic articulography or EMA) in a fully unsupervised manner, which more closely mimics human speech production and better simulates the process of human speech production.
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#30452;&#25509;&#29983;&#25104;&#27874;&#24418;&#25110;&#39057;&#35889;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#36807;&#25511;&#21046;&#20851;&#33410;&#26469;&#20135;&#29983;&#35821;&#38899;&#65292;&#36825;&#36890;&#36807;&#22768;&#38899;&#20256;&#25773;&#30340;&#29289;&#29702;&#29305;&#24615;&#23548;&#33268;&#35821;&#38899;&#22768;&#38899;&#30340;&#20135;&#29983;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20851;&#33410;&#29983;&#25104;&#22120;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#33539;&#20363;&#20013;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#20135;&#29983;/&#21512;&#25104;&#12290;&#20851;&#33410;&#29983;&#25104;&#22120;&#36890;&#36807;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#29983;&#25104;&#20851;&#33410;&#34920;&#31034;&#65288;&#30005;&#30913;&#20851;&#33410;&#25104;&#20687;&#25110;EMA&#65289;&#65292;&#26356;&#25509;&#36817;&#20110;&#20154;&#31867;&#35821;&#38899;&#20135;&#29983;&#30340;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;&#39044;&#35757;&#32451;&#29289;&#29702;&#27169;&#22411;&#65288;ema2wav&#65289;&#23558;&#29983;&#25104;&#30340;EMA&#34920;&#31034;&#36716;&#25442;&#20026;&#35821;&#38899;&#27874;&#24418;&#65292;&#36825;&#20123;&#27874;&#24418;&#34987;&#21457;&#36865;&#21040;&#37492;&#21035;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#20851;&#33410;&#20998;&#26512;&#34920;&#26126;&#65292;&#32593;&#32476;&#23398;&#20064;&#25511;&#21046;&#20851;&#33410;&#30340;&#26041;&#24335;&#31867;&#20284;&#20110;&#20154;&#31867;&#22312;&#35821;&#38899;&#20135;&#29983;&#36807;&#31243;&#20013;&#30340;&#26041;&#24335;&#12290;&#36755;&#20986;&#30340;&#22768;&#23398;&#20998;&#26512;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Generative deep neural networks are widely used for speech synthesis, but most existing models directly generate waveforms or spectral outputs. Humans, however, produce speech by controlling articulators, which results in the production of speech sounds through physical properties of sound propagation. We introduce the Articulatory Generator to the Generative Adversarial Network paradigm, a new unsupervised generative model of speech production/synthesis. The Articulatory Generator more closely mimics human speech production by learning to generate articulatory representations (electromagnetic articulography or EMA) in a fully unsupervised manner. A separate pre-trained physical model (ema2wav) then transforms the generated EMA representations to speech waveforms, which get sent to the Discriminator for evaluation. Articulatory analysis suggests that the network learns to control articulators in a similar manner to humans during speech production. Acoustic analysis of the outputs sugge
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.11328</link><description>&lt;p&gt;
&#22238;&#25918;&#65306;&#36845;&#20195;&#27880;&#24847;&#21147;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Play It Back: Iterative Attention for Audio Recognition. (arXiv:2210.11328v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes an end-to-end attention-based architecture that attends over the most discriminative sounds across the audio sequence through selective repetition, achieving consistently state-of-the-art performance across three audio-classification benchmarks.
&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#35748;&#30693;&#30340;&#19968;&#20010;&#20851;&#38190;&#21151;&#33021;&#26159;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#23558;&#29305;&#24449;&#22768;&#38899;&#19982;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#32852;&#36215;&#26469;&#12290;&#20154;&#31867;&#35797;&#22270;&#21306;&#20998;&#32454;&#31890;&#24230;&#38899;&#39057;&#31867;&#21035;&#26102;&#65292;&#36890;&#24120;&#20250;&#37325;&#25773;&#30456;&#21516;&#30340;&#21306;&#20998;&#24615;&#22768;&#38899;&#20197;&#22686;&#21152;&#20854;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26368;&#21021;&#20351;&#29992;&#23436;&#25972;&#30340;&#38899;&#39057;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#25554;&#27133;&#27880;&#24847;&#21147;&#36845;&#20195;&#22320;&#32454;&#21270;&#37325;&#25773;&#30340;&#26102;&#38388;&#27573;&#12290;&#22312;&#27599;&#27425;&#25773;&#25918;&#26102;&#65292;&#25152;&#36873;&#27573;&#20351;&#29992;&#36739;&#23567;&#30340;&#36339;&#36291;&#38271;&#24230;&#37325;&#25773;&#65292;&#36825;&#20195;&#34920;&#20102;&#36825;&#20123;&#27573;&#20869;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65306;AudioSet&#12289;VGG-Sound&#21644;EPIC-KITCHENS-100&#12290;
&lt;/p&gt;
&lt;p&gt;
A key function of auditory cognition is the association of characteristic sounds with their corresponding semantics over time. Humans attempting to discriminate between fine-grained audio categories, often replay the same discriminative sounds to increase their prediction confidence. We propose an end-to-end attention-based architecture that through selective repetition attends over the most discriminative sounds across the audio sequence. Our model initially uses the full audio sequence and iteratively refines the temporal segments replayed based on slot attention. At each playback, the selected segments are replayed using a smaller hop length which represents higher resolution features within these segments. We show that our method can consistently achieve state-of-the-art performance across three audio-classification benchmarks: AudioSet, VGG-Sound, and EPIC-KITCHENS-100.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#26041;&#27861;PSVRF&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;&#65292;&#21487;&#20197;&#22686;&#24378;ASV&#31995;&#32479;&#23545;&#38899;&#39640;&#32553;&#25918;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24615;&#33021;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.02731</link><description>&lt;p&gt;
PSVRF: &#26080;&#21442;&#32771;&#23398;&#20064;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
PSVRF: Learning to restore Pitch-Shifted Voice without reference. (arXiv:2210.02731v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#26041;&#27861;PSVRF&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;&#65292;&#21487;&#20197;&#22686;&#24378;ASV&#31995;&#32479;&#23545;&#38899;&#39640;&#32553;&#25918;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24615;&#33021;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a no-reference approach called PSVRF for high-quality restoration of pitch-shifted voice, which enhances the robustness of ASV systems to pitch-scaling attacks and even outperforms the state-of-the-art reference-based approach.
&lt;/p&gt;
&lt;p&gt;
&#38899;&#39640;&#32553;&#25918;&#31639;&#27861;&#23545;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#65288;ASV&#65289;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21453;&#27450;&#39575;&#31639;&#27861;&#26469;&#35782;&#21035;&#21464;&#35843;&#35821;&#38899;&#24182;&#23558;&#20854;&#24674;&#22797;&#21040;&#21407;&#22987;&#29256;&#26412;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#24615;&#33021;&#36739;&#24046;&#65292;&#35201;&#20040;&#38656;&#35201;&#21407;&#22987;&#35821;&#38899;&#20316;&#20026;&#21442;&#32771;&#65292;&#38480;&#21046;&#20102;&#24212;&#29992;&#21069;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#26041;&#27861;PSVRF&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;&#12290;&#22312;AISHELL-1&#21644;AISHELL-3&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PSVRF&#21487;&#20197;&#24674;&#22797;&#34987;&#21508;&#31181;&#38899;&#39640;&#32553;&#25918;&#25216;&#26415;&#20266;&#35013;&#30340;&#35821;&#38899;&#65292;&#26174;&#28982;&#22686;&#24378;&#20102;ASV&#31995;&#32479;&#23545;&#38899;&#39640;&#32553;&#25918;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;PSVRF&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pitch scaling algorithms have a significant impact on the security of Automatic Speaker Verification (ASV) systems. Although numerous anti-spoofing algorithms have been proposed to identify the pitch-shifted voice and even restore it to the original version, they either have poor performance or require the original voice as a reference, limiting the prospects of applications. In this paper, we propose a no-reference approach termed PSVRF$^1$ for high-quality restoration of pitch-shifted voice. Experiments on AISHELL-1 and AISHELL-3 demonstrate that PSVRF can restore the voice disguised by various pitch-scaling techniques, which obviously enhances the robustness of ASV systems to pitch-scaling attacks. Furthermore, the performance of PSVRF even surpasses that of the state-of-the-art reference-based approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;Uconv-Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#23558;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;16&#20493;&#65292;&#21152;&#36895;&#20013;&#38388;&#23618;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19978;&#37319;&#26679;&#22359;&#35299;&#20915;&#20102;&#25910;&#25947;&#38382;&#39064;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;WER&#21644;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2208.07657</link><description>&lt;p&gt;
Uconv-Conformer: &#38024;&#23545;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#22823;&#24133;&#32553;&#20943;&#30340;&#26032;&#22411;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Uconv-Conformer: High Reduction of Input Sequence Length for End-to-End Speech Recognition. (arXiv:2208.07657v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;Uconv-Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#23558;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;16&#20493;&#65292;&#21152;&#36895;&#20013;&#38388;&#23618;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19978;&#37319;&#26679;&#22359;&#35299;&#20915;&#20102;&#25910;&#25947;&#38382;&#39064;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;WER&#21644;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a new Uconv-Conformer architecture that reduces the input sequence length by 16 times, speeds up the work of intermediate layers, and solves the convergence issue by using upsampling blocks. The Uconv-Conformer architecture shows better WER and faster training and inference speed.
&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#29616;&#20195;ASR&#26550;&#26500;&#26159;&#26368;&#39640;&#20248;&#20808;&#32423;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#33410;&#30465;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#35768;&#22810;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;Conformer&#27169;&#22411;&#30340;&#26032;&#22411;Uconv-Conformer&#26550;&#26500;&#12290;&#23427;&#36890;&#36807;16&#20493;&#30340;&#19968;&#33268;&#24615;&#32553;&#30701;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#20013;&#38388;&#23618;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#19982;&#26102;&#38388;&#32500;&#24230;&#22823;&#24133;&#32553;&#20943;&#30456;&#20851;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20687;U-Net&#26550;&#26500;&#20013;&#30340;&#19978;&#37319;&#26679;&#22359;&#26469;&#30830;&#20445;&#27491;&#30830;&#30340;CTC&#25439;&#22833;&#35745;&#31639;&#21644;&#31283;&#23450;&#32593;&#32476;&#35757;&#32451;&#12290;Uconv-Conformer&#26550;&#26500;&#19981;&#20165;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#26356;&#24555;&#65292;&#32780;&#19988;&#19982;&#22522;&#32447;Conformer&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;WER&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;Uconv-Conformer&#27169;&#22411;&#22312;CPU&#21644;GPU&#19978;&#20998;&#21035;&#26174;&#31034;&#20986;47.8&#65285;&#21644;23.5&#65285;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#30456;&#23545;WER&#30340;&#20943;&#23569;&#20998;&#21035;&#20026;7.3&#65285;&#21644;9.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization of modern ASR architectures is among the highest priority tasks since it saves many computational resources for model training and inference. The work proposes a new Uconv-Conformer architecture based on the standard Conformer model. It consistently reduces the input sequence length by 16 times, which results in speeding up the work of the intermediate layers. To solve the convergence issue connected with such a significant reduction of the time dimension, we use upsampling blocks like in the U-Net architecture to ensure the correct CTC loss calculation and stabilize network training. The Uconv-Conformer architecture appears to be not only faster in terms of training and inference speed but also shows better WER compared to the baseline Conformer. Our best Uconv-Conformer model shows 47.8% and 23.5% inference acceleration on the CPU and GPU, respectively. Relative WER reduction is 7.3% and 9.2% on LibriSpeech test_clean and test_other respectively.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#33410;&#21644;&#23383;&#31526;&#30446;&#26631;&#30340;&#20132;&#26367;&#20013;&#38388;&#26465;&#20214;&#26041;&#27861;&#65292;&#21033;&#29992;&#23383;&#31526;&#32423;&#21644;&#38899;&#33410;&#32423;&#20013;&#38388;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#26469;&#22788;&#29702;&#26085;&#35821;ASR&#20013;&#30340;&#22810;&#23545;&#19968;&#21644;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2204.00175</link><description>&lt;p&gt;
&#26085;&#35821;ASR&#20013;&#22522;&#20110;&#38899;&#33410;&#21644;&#23383;&#31526;&#30446;&#26631;&#30340;&#20132;&#26367;&#20013;&#38388;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Alternate Intermediate Conditioning with Syllable-level and Character-level Targets for Japanese ASR. (arXiv:2204.00175v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00175
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#33410;&#21644;&#23383;&#31526;&#30446;&#26631;&#30340;&#20132;&#26367;&#20013;&#38388;&#26465;&#20214;&#26041;&#27861;&#65292;&#21033;&#29992;&#23383;&#31526;&#32423;&#21644;&#38899;&#33410;&#32423;&#20013;&#38388;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#26469;&#22788;&#29702;&#26085;&#35821;ASR&#20013;&#30340;&#22810;&#23545;&#19968;&#21644;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an alternate intermediate conditioning method with syllable-level and character-level targets to deal with the many-to-one and one-to-many mapping problems in Japanese ASR, and achieves better performance than conventional multi-task and Self-conditioned CTC methods in experiments.
&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30452;&#25509;&#23558;&#36755;&#20837;&#35821;&#38899;&#26144;&#23556;&#21040;&#23383;&#31526;&#12290;&#28982;&#32780;&#65292;&#24403;&#22810;&#20010;&#19981;&#21516;&#30340;&#21457;&#38899;&#24212;&#35813;&#26144;&#23556;&#21040;&#19968;&#20010;&#23383;&#31526;&#25110;&#19968;&#20010;&#21457;&#38899;&#34987;&#22810;&#20010;&#19981;&#21516;&#30340;&#23383;&#31526;&#20849;&#20139;&#26102;&#65292;&#26144;&#23556;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#30001;&#20110;&#26085;&#35821;&#27721;&#23383;&#30340;&#23384;&#22312;&#65292;&#26085;&#35821;ASR&#26368;&#23481;&#26131;&#36973;&#21463;&#36825;&#31181;&#22810;&#23545;&#19968;&#21644;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23383;&#31526;&#21644;&#38899;&#33410;&#20043;&#38388;&#30340;&#26174;&#24335;&#20132;&#20114;&#65292;&#20351;&#29992;&#33258;&#25105;&#26465;&#20214;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#65292;&#20854;&#20013;&#19978;&#23618;&#8220;&#33258;&#25105;&#26465;&#20214;&#8221;&#20110;&#19979;&#23618;&#30340;&#20013;&#38388;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#23383;&#31526;&#32423;&#21644;&#38899;&#33410;&#32423;&#20013;&#38388;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#26469;&#22788;&#29702;&#23383;&#31526;&#21644;&#38899;&#33410;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#33258;&#21457;&#26085;&#35821;&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#22810;&#20219;&#21153;&#21644;&#33258;&#25105;&#26465;&#20214;CTC&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end automatic speech recognition directly maps input speech to characters. However, the mapping can be problematic when several different pronunciations should be mapped into one character or when one pronunciation is shared among many different characters. Japanese ASR suffers the most from such many-to-one and one-to-many mapping problems due to Japanese kanji characters. To alleviate the problems, we introduce explicit interaction between characters and syllables using Self-conditioned connectionist temporal classification (CTC), in which the upper layers are ``self-conditioned'' on the intermediate predictions from the lower layers. The proposed method utilizes character-level and syllable-level intermediate predictions as conditioning features to deal with mutual dependency between characters and syllables. Experimental results on Corpus of Spontaneous Japanese show that the proposed method outperformed the conventional multi-task and Self-conditioned CTC methods.
&lt;/p&gt;</description></item></channel></rss>