<rss version="2.0"><channel><title>Chat Arxiv physics</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for physics</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#24182;&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20854;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.06423</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#32593;&#32476;&#65292;&#20197;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;40&#19975;&#20221;&#21307;&#30103;&#35760;&#24405;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable causal networks from very large datasets, application to 400,000 medical records of breast cancer patients. (arXiv:2303.06423v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#24182;&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20854;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a more reliable and scalable causal discovery method (iMIIC) and showcases its unique capabilities on healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. Over 90% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity.
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#22240;&#26524;&#25928;&#24212;&#26159;&#31185;&#23398;&#30740;&#31350;&#30340;&#26680;&#24515;&#65292;&#20294;&#24403;&#21482;&#26377;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#36825;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22240;&#26524;&#32593;&#32476;&#38590;&#20197;&#23398;&#20064;&#21644;&#35299;&#37322;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#22522;&#20110;&#19968;&#33324;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21407;&#21017;&#65292;&#23427;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25512;&#26029;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#21306;&#20998;&#20102;&#30495;&#27491;&#30340;&#21407;&#22240;&#21644;&#20551;&#23450;&#30340;&#21644;&#28508;&#22312;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;iMIIC&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21512;&#25104;&#21644;&#29616;&#23454;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;iMIIC&#30340;&#29420;&#29305;&#33021;&#21147;&#24320;&#36767;&#20102;&#21457;&#29616;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#32593;&#32476;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal effects is at the core of scientific investigation but remains challenging when only observational data is available. In practice, causal networks are difficult to learn and interpret, and limited to relatively small datasets. We report a more reliable and scalable causal discovery method (iMIIC), based on a general mutual information supremum principle, which greatly improves the precision of inferred causal relations while distinguishing genuine causes from putative and latent causal effects. We showcase iMIIC on synthetic and real-life healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. More than 90\% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity. iMIIC's unique capabilities open up new avenues to discover reliable and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.06311</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;EXO-200&#38378;&#28865;&#20449;&#21495;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks for Scintillation Signal Simulation in EXO-200. (arXiv:2303.06311v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach using Generative Adversarial Networks to simulate photodetector signals from the time projection chamber of the EXO-200 experiment. The method is able to produce high-quality simulated waveforms an order of magnitude faster than traditional simulation methods and can generalize from the training sample and discern salient high-level features of the data.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#25110;&#23454;&#38469;&#20107;&#20214;&#26679;&#26412;&#35757;&#32451;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#20026;&#20195;&#20215;&#29983;&#25104;&#22823;&#35268;&#27169;&#27169;&#25311;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20801;&#35768;&#23545;&#32473;&#23450;&#23545;&#35937;&#38598;&#30340;&#24635;&#20307;&#20998;&#24067;&#36827;&#34892;&#38544;&#24335;&#38750;&#21442;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#20351;&#29992;&#21407;&#22987;&#38378;&#28865;&#27874;&#24418;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#26657;&#20934;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23427;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#65292;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#32593;&#32476;&#27491;&#30830;&#25512;&#26029;&#20986;&#25506;&#27979;&#22120;&#20013;&#38378;&#28865;&#20809;&#21709;&#24212;&#30340;&#20301;&#32622;&#20381;&#36182;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks trained on samples of simulated or actual events have been proposed as a way of generating large simulated datasets at a reduced computational cost. In this work, a novel approach to perform the simulation of photodetector signals from the time projection chamber of the EXO-200 experiment is demonstrated. The method is based on a Wasserstein Generative Adversarial Network - a deep learning technique allowing for implicit non-parametric estimation of the population distribution for a given set of objects. Our network is trained on real calibration data using raw scintillation waveforms as input. We find that it is able to produce high-quality simulated waveforms an order of magnitude faster than the traditional simulation approach and, importantly, generalize from the training sample and discern salient high-level features of the data. In particular, the network correctly deduces position dependency of scintillation light response in the detector and corr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#27668;&#35937;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#28085;&#30422;&#20102;&#24863;&#30693;&#22120;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;U&#22411;&#32593;&#32476;&#31561;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.00147</link><description>&lt;p&gt;
&#25805;&#20316;&#27668;&#35937;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#25945;&#31243;&#65292;&#31532;&#20108;&#37096;&#20998;&#65306;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Tutorial for Operational Meteorology, Part II: Neural Networks and Deep Learning. (arXiv:2211.00147v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#27668;&#35937;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#28085;&#30422;&#20102;&#24863;&#30693;&#22120;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;U&#22411;&#32593;&#32476;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the application of machine learning in meteorology, specifically neural networks and deep learning. It covers methods such as perceptrons, artificial neural networks, convolutional neural networks, and U-networks.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#27668;&#35937;&#23398;&#20013;&#30340;&#24212;&#29992;&#36805;&#36895;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20351;&#29992;&#29575;&#21069;&#25152;&#26410;&#26377;&#12290;&#20026;&#20102;&#22635;&#34917;&#32570;&#20047;&#20197;&#27668;&#35937;&#23398;&#35270;&#35282;&#28085;&#30422;&#31070;&#32463;&#32593;&#32476;&#30340;&#36164;&#28304;&#65292;&#26412;&#25991;&#20197;&#24179;&#26131;&#36817;&#20154;&#30340;&#35821;&#35328;&#26684;&#24335;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#25805;&#20316;&#27668;&#35937;&#23398;&#30028;&#12290;&#36825;&#26159;&#19968;&#23545;&#26088;&#22312;&#20026;&#27668;&#35937;&#23398;&#23478;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#36164;&#28304;&#30340;&#35770;&#25991;&#20013;&#30340;&#31532;&#20108;&#31687;&#12290;&#31532;&#19968;&#31687;&#35770;&#25991;&#20391;&#37325;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;&#38543;&#26426;&#26862;&#26519;&#65289;&#65292;&#32780;&#26412;&#25991;&#21017;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#28085;&#30422;&#20102;&#24863;&#30693;&#22120;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;U&#22411;&#32593;&#32476;&#12290;&#19982;&#31532;&#19968;&#31687;&#35770;&#25991;&#19968;&#26679;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#19982;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#35757;&#32451;&#30456;&#20851;&#30340;&#26415;&#35821;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#27599;&#31181;&#26041;&#27861;&#32972;&#21518;&#30340;&#19968;&#20123;&#30452;&#35273;&#65292;&#24182;&#20197;&#23637;&#31034;&#27599;&#31181;&#26041;&#27861;&#30340;&#23454;&#20363;&#26469;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade the use of machine learning in meteorology has grown rapidly. Specifically neural networks and deep learning have been used at an unprecedented rate. In order to fill the dearth of resources covering neural networks with a meteorological lens, this paper discusses machine learning methods in a plain language format that is targeted for the operational meteorological community. This is the second paper in a pair that aim to serve as a machine learning resource for meteorologists. While the first paper focused on traditional machine learning methods (e.g., random forest), here a broad spectrum of neural networks and deep learning methods are discussed. Specifically this paper covers perceptrons, artificial neural networks, convolutional neural networks and U-networks. Like the part 1 paper, this manuscript discusses the terms associated with neural networks and their training. Then the manuscript provides some intuition behind every method and concludes by showing ea
&lt;/p&gt;</description></item><item><title>PDEBench&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#27169;&#25311;&#20219;&#21153;&#22522;&#20934;&#22871;&#20214;&#65292;&#21253;&#25324;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#23545;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#19982;&#32463;&#20856;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2210.07182</link><description>&lt;p&gt;
PDEBENCH&#65306;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
PDEBENCH: An Extensive Benchmark for Scientific Machine Learning. (arXiv:2210.07182v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07182
&lt;/p&gt;
&lt;p&gt;
PDEBench&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#27169;&#25311;&#20219;&#21153;&#22522;&#20934;&#22871;&#20214;&#65292;&#21253;&#25324;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#23545;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#19982;&#32463;&#20856;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
PDEBench is a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs), which includes code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29289;&#29702;&#31995;&#32479;&#24314;&#27169;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#32570;&#20047;&#26131;&#20110;&#20351;&#29992;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20195;&#34920;&#24615;&#30340;&#31185;&#23398;ML&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PDEBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#27169;&#25311;&#20219;&#21153;&#22522;&#20934;&#22871;&#20214;&#12290;PDEBench&#21253;&#25324;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#23545;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#19982;&#32463;&#20856;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20934;&#38382;&#39064;&#38598;&#20855;&#26377;&#20197;&#19979;&#29420;&#29305;&#29305;&#24449;&#65306;&#65288;1&#65289;&#19982;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;PDE&#30340;&#33539;&#22260;&#26356;&#24191;&#65292;&#20174;&#30456;&#23545;&#24120;&#35265;&#30340;&#31034;&#20363;&#21040;&#26356;&#29616;&#23454;&#21644;&#22256;&#38590;&#30340;&#38382;&#39064;&#65307;&#65288;2&#65289;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#20934;&#22791;&#22909;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#26356;&#22823;&#65292;&#21253;&#25324;&#36328;&#26356;&#22810;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#20197;&#21450;PDE&#21442;&#25968;&#30340;&#22810;&#20010;&#27169;&#25311;&#36816;&#34892;&#65307;&#65288;3&#65289;&#26356;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26356;&#22810;&#30340;&#24615;&#33021;&#25351;&#26631;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based modeling of physical systems has experienced increased interest in recent years. Despite some impressive progress, there is still a lack of benchmarks for Scientific ML that are easy to use but still challenging and representative of a wide range of problems. We introduce PDEBench, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs). PDEBench comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines. Our proposed set of benchmark problems contribute the following unique features: (1) A much wider range of PDEs compared to existing benchmarks, ranging from relatively common examples to more realistic and difficult problems; (2) much larger ready-to-use datasets compared to prior work, comprising multiple simulation runs across a larger number of initial and boundary conditions and PDE parameters; (3) more exte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#65292;&#22238;&#31572;&#20102;&#32500;&#24230;&#23849;&#28291;&#30340;&#21407;&#22240;&#21644;&#24433;&#21709;&#65292;&#20197;&#21450;&#32500;&#24230;&#23849;&#28291;&#22914;&#20309;&#26377;&#30410;&#65292;&#24182;&#24433;&#21709;SSL&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00638</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#26159;&#22914;&#20309;&#24418;&#25104;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
What shapes the loss landscape of self-supervised learning?. (arXiv:2210.00638v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#65292;&#22238;&#31572;&#20102;&#32500;&#24230;&#23849;&#28291;&#30340;&#21407;&#22240;&#21644;&#24433;&#21709;&#65292;&#20197;&#21450;&#32500;&#24230;&#23849;&#28291;&#22914;&#20309;&#26377;&#30410;&#65292;&#24182;&#24433;&#21709;SSL&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper answers questions about the causes and effects of dimensional collapse in self-supervised learning (SSL) by analyzing the SSL loss landscape, and explores how dimensional collapse can be beneficial and affect the robustness of SSL against data imbalance.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38450;&#27490;&#34920;&#31034;&#23436;&#20840;&#21644;&#32500;&#24230;&#23849;&#28291;&#24050;&#25104;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#29702;&#35770;&#30340;&#29702;&#35299;&#20173;&#26377;&#30097;&#38382;&#65306;&#36825;&#20123;&#23849;&#28291;&#20309;&#26102;&#21457;&#29983;&#65311;&#26426;&#21046;&#21644;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#21644;&#24443;&#24213;&#20998;&#26512;SSL&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#21487;&#20998;&#26512;&#29702;&#35770;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#29702;&#35770;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#32500;&#24230;&#23849;&#28291;&#30340;&#21407;&#22240;&#65292;&#24182;&#30740;&#31350;&#20102;&#24402;&#19968;&#21270;&#21644;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20998;&#26512;&#29702;&#35770;&#25152;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#26469;&#29702;&#35299;&#32500;&#24230;&#23849;&#28291;&#22914;&#20309;&#26377;&#30410;&#65292;&#24182;&#24433;&#21709;SSL&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: When do those collapses occur? What are the mechanisms and causes? We answer these questions by deriving and thoroughly analyzing an analytically tractable theory of SSL loss landscapes. In this theory, we identify the causes of the dimensional collapse and study the effect of normalization and bias. Finally, we leverage the interpretability afforded by the analytical theory to understand how dimensional collapse can be beneficial and what affects the robustness of SSL against data imbalance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#28385;&#36275;&#23432;&#24658;&#23450;&#24459;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.05424</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#27668;&#20505;&#38477;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Physics-Constrained Deep Learning for Climate Downscaling. (arXiv:2208.05424v6 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#28385;&#36275;&#23432;&#24658;&#23450;&#24459;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for physics-constrained deep learning downscaling models to ensure that the models satisfy conservation laws when predicting physical variables, while improving their performance according to traditional metrics.
&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#39640;&#20998;&#36776;&#29575;&#27668;&#20505;&#21644;&#22825;&#27668;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#23545;&#20110;&#25351;&#23548;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#30340;&#38271;&#26399;&#20915;&#31574;&#20197;&#21450;&#25351;&#23548;&#23545;&#26497;&#31471;&#20107;&#20214;&#30340;&#24555;&#36895;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#27979;&#27169;&#22411;&#21463;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#65292;&#22240;&#27492;&#36890;&#24120;&#29983;&#25104;&#31895;&#20998;&#36776;&#29575;&#39044;&#27979;&#12290;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#19978;&#37319;&#26679;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#35270;&#35273;&#19978;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#32463;&#24120;&#36829;&#21453;&#23432;&#24658;&#23450;&#24459;&#12290;&#20026;&#20102;&#20445;&#25345;&#29289;&#29702;&#37327;&#30340;&#23432;&#24658;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20445;&#35777;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#28385;&#36275;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#21516;&#26102;&#26681;&#25454;&#20256;&#32479;&#25351;&#26631;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32422;&#26463;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#20197;&#21450;&#21508;&#31181;&#27668;&#20505;&#21644;&#22825;&#27668;&#25968;&#25454;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of reliable, high-resolution climate and weather data is important to inform long-term decisions on climate adaptation and mitigation and to guide rapid responses to extreme events. Forecasting models are limited by computational costs and, therefore, often generate coarse-resolution predictions. Statistical downscaling, including super-resolution methods from deep learning, can provide an efficient method of upsampling low-resolution data. However, despite achieving visually compelling results in some cases, such models frequently violate conservation laws when predicting physical variables. In order to conserve physical quantities, we develop methods that guarantee physical constraints are satisfied by a deep learning downscaling model while also improving their performance according to traditional metrics. We compare different constraining approaches and demonstrate their applicability across different neural architectures as well as a variety of climate and weather
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23884;&#20837;&#31163;&#25955;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#32500;&#24230;&#65288;ID&#65289;&#65292;&#24182;&#22312;&#29289;&#31181;&#25351;&#32441;&#30340;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#23567;ID&#65292;&#32422;&#20026;2&#30340;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2207.09688</link><description>&lt;p&gt;
&#31163;&#25955;&#24230;&#37327;&#30340;&#20869;&#22312;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Intrinsic dimension estimation for discrete metrics. (arXiv:2207.09688v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23884;&#20837;&#31163;&#25955;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#32500;&#24230;&#65288;ID&#65289;&#65292;&#24182;&#22312;&#29289;&#31181;&#25351;&#32441;&#30340;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#23567;ID&#65292;&#32422;&#20026;2&#30340;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an algorithm to estimate the intrinsic dimension (ID) of datasets embedded in discrete spaces, and demonstrates its accuracy on a metagenomic dataset for species fingerprinting, finding a surprisingly small ID of order 2, suggesting that evolutive pressure acts on a low-dimensional manifold despite the high-dimensionality of sequences' space.
&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#31163;&#25955;&#29305;&#24449;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65306;&#20174;&#20998;&#31867;&#35843;&#26597;&#21040;&#20020;&#24202;&#38382;&#21367;&#65292;&#20174;&#26080;&#26435;&#32593;&#32476;&#21040;DNA&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#26368;&#24120;&#35265;&#30340;&#26080;&#30417;&#30563;&#38477;&#32500;&#26041;&#27861;&#26159;&#20026;&#36830;&#32493;&#31354;&#38388;&#35774;&#35745;&#30340;&#65292;&#23427;&#20204;&#22312;&#31163;&#25955;&#31354;&#38388;&#20013;&#30340;&#20351;&#29992;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#21644;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23884;&#20837;&#31163;&#25955;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#32500;&#24230;&#65288;ID&#65289;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20998;&#26512;&#29992;&#20110;&#29289;&#31181;&#25351;&#32441;&#30340;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#23567;ID&#65292;&#32422;&#20026;2&#30340;&#25968;&#37327;&#32423;&#12290;&#36825;&#34920;&#26126;&#65292;&#23613;&#31649;&#24207;&#21015;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#65292;&#36827;&#21270;&#21387;&#21147;&#20173;&#28982;&#20316;&#29992;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real world-datasets characterized by discrete features are ubiquitous: from categorical surveys to clinical questionnaires, from unweighted networks to DNA sequences. Nevertheless, the most common unsupervised dimensional reduction methods are designed for continuous spaces, and their use for discrete spaces can lead to errors and biases. In this letter we introduce an algorithm to infer the intrinsic dimension (ID) of datasets embedded in discrete spaces. We demonstrate its accuracy on benchmark datasets, and we apply it to analyze a metagenomic dataset for species fingerprinting, finding a surprisingly small ID, of order 2. This suggests that evolutive pressure acts on a low-dimensional manifold despite the high-dimensionality of sequences' space.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#30456;&#20284;&#24230;&#27169;&#22411;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#36816;&#36755;&#21644;&#36816;&#21160;&#30340;&#27169;&#25311;&#20135;&#29983;&#30340;&#26631;&#37327;&#21644;&#30690;&#37327;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;CNN&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#20307;&#31215;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;VolSiM&#65289;&#12290;</title><link>http://arxiv.org/abs/2202.04109</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23610;&#24230;CNN&#30340;&#20307;&#31215;&#27169;&#25311;&#30456;&#20284;&#24230;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Similarity Metrics for Volumetric Simulations with Multiscale CNNs. (arXiv:2202.04109v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#30456;&#20284;&#24230;&#27169;&#22411;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#36816;&#36755;&#21644;&#36816;&#21160;&#30340;&#27169;&#25311;&#20135;&#29983;&#30340;&#26631;&#37327;&#21644;&#30690;&#37327;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;CNN&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#20307;&#31215;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;VolSiM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a similarity model based on entropy for assessing the similarity of scalar and vectorial data produced from transport and motion-based simulations, and a multiscale CNN architecture for computing a volumetric similarity metric (VolSiM).
&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#25968;&#25454;&#27169;&#25311;&#22312;&#31185;&#23398;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20174;&#27969;&#20307;&#27969;&#21160;&#21040;&#31561;&#31163;&#23376;&#29289;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#30456;&#20284;&#24230;&#27169;&#22411;&#65292;&#20801;&#35768;&#21019;&#24314;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#22522;&#20934;&#36317;&#31163;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#36816;&#36755;&#21644;&#36816;&#21160;&#30340;&#27169;&#25311;&#20135;&#29983;&#30340;&#26631;&#37327;&#21644;&#30690;&#37327;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#12290;&#21033;&#29992;&#20174;&#35813;&#27169;&#22411;&#23548;&#20986;&#30340;&#20004;&#31181;&#25968;&#25454;&#37319;&#38598;&#26041;&#27861;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20174;&#25968;&#20540;PDE&#27714;&#35299;&#22120;&#21644;&#29616;&#26377;&#27169;&#25311;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#25910;&#38598;&#30340;&#22330;&#38598;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;CNN&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#20307;&#31215;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;VolSiM&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#22825;&#28982;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#27169;&#25311;&#25968;&#25454;&#30456;&#20284;&#24230;&#35780;&#20272;&#25361;&#25112;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22522;&#20110;&#30456;&#20851;&#25439;&#22833;&#20989;&#25968;&#30340;&#22823;&#25209;&#37327;&#22823;&#23567;&#21644;&#20934;&#30830;&#30456;&#20851;&#35745;&#31639;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#30740;&#31350;&#20102;&#35813;&#24230;&#37327;&#30340;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulations that produce three-dimensional data are ubiquitous in science, ranging from fluid flows to plasma physics. We propose a similarity model based on entropy, which allows for the creation of physically meaningful ground truth distances for the similarity assessment of scalar and vectorial data, produced from transport and motion-based simulations. Utilizing two data acquisition methods derived from this model, we create collections of fields from numerical PDE solvers and existing simulation data repositories. Furthermore, a multiscale CNN architecture that computes a volumetric similarity metric (VolSiM) is proposed. To the best of our knowledge this is the first learning method inherently designed to address the challenges arising for the similarity assessment of high-dimensional simulation data. Additionally, the tradeoff between a large batch size and an accurate correlation computation for correlation-based loss functions is investigated, and the metric's invariance with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#28369;&#22369;&#26131;&#21457;&#24615;&#12290;SNN&#27169;&#22411;&#21457;&#29616;&#22369;&#24230;&#21644;&#38477;&#27700;&#30340;&#20056;&#31215;&#20197;&#21450;&#22369;&#21521;&#26159;&#39640;&#28369;&#22369;&#26131;&#21457;&#24615;&#30340;&#37325;&#35201;&#20027;&#35201;&#36129;&#29486;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2201.06837</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22312;&#28369;&#22369;&#26131;&#21457;&#24615;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Landslide Susceptibility Modeling by Interpretable Neural Network. (arXiv:2201.06837v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#28369;&#22369;&#26131;&#21457;&#24615;&#12290;SNN&#27169;&#22411;&#21457;&#29616;&#22369;&#24230;&#21644;&#38477;&#27700;&#30340;&#20056;&#31215;&#20197;&#21450;&#22369;&#21521;&#26159;&#39640;&#28369;&#22369;&#26131;&#21457;&#24615;&#30340;&#37325;&#35201;&#20027;&#35201;&#36129;&#29486;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an interpretable neural network framework, called superposable neural network (SNN) optimization, for assessing landslide susceptibility. The SNN models found the product of slope and precipitation and hillslope aspect to be important primary contributors to high landslide susceptibility.
&lt;/p&gt;
&lt;p&gt;
&#28369;&#22369;&#30001;&#20110;&#35768;&#22810;&#26102;&#31354;&#21464;&#21270;&#22240;&#32032;&#24433;&#21709;&#32780;&#38590;&#20197;&#39044;&#27979;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#21152;&#24615;ANN&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#28369;&#22369;&#26131;&#21457;&#24615;&#65292;&#20197;&#21450;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#32467;&#26524;&#35299;&#37322;&#25216;&#26415;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#20855;&#26377;&#23436;&#20840;&#21487;&#35299;&#37322;&#24615;&#12289;&#39640;&#20934;&#30830;&#24615;&#12289;&#39640;&#27867;&#21270;&#24615;&#21644;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#31216;&#20026;&#21487;&#21472;&#21152;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20248;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#19996;&#21916;&#39532;&#25289;&#38597;&#22320;&#21306;&#30340;&#28369;&#22369;&#28165;&#21333;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;SNN&#20248;&#20110;&#22522;&#20110;&#29289;&#29702;&#21644;&#32479;&#35745;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;SNN&#27169;&#22411;&#21457;&#29616;&#22369;&#24230;&#21644;&#38477;&#27700;&#30340;&#20056;&#31215;&#20197;&#21450;&#22369;&#21521;&#26159;&#39640;&#28369;&#22369;&#26131;&#21457;&#24615;&#30340;&#37325;&#35201;&#20027;&#35201;&#36129;&#29486;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Landslides are notoriously difficult to predict because numerous spatially and temporally varying factors contribute to slope stability. Artificial neural networks (ANN) have been shown to improve prediction accuracy but are largely uninterpretable. Here we introduce an additive ANN optimization framework to assess landslide susceptibility, as well as dataset division and outcome interpretation techniques. We refer to our approach, which features full interpretability, high accuracy, high generalizability and low model complexity, as superposable neural network (SNN) optimization. We validate our approach by training models on landslide inventory from three different easternmost Himalaya regions. Our SNN outperformed physically-based and statistical models and achieved similar performance to state-of-the-art deep neural networks. The SNN models found the product of slope and precipitation and hillslope aspect to be important primary contributors to high landslide susceptibility, which 
&lt;/p&gt;</description></item></channel></rss>