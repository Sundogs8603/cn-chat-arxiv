{
    "title": "Semi-Decentralized Federated Edge Learning with Data and Device Heterogeneity. (arXiv:2112.10313v3 [cs.LG] UPDATED)",
    "abstract": "Federated edge learning (FEEL) has attracted much attention as a privacy-preserving paradigm to effectively incorporate the distributed data at the network edge for training deep learning models. Nevertheless, the limited coverage of a single edge server results in an insufficient number of participated client nodes, which may impair the learning performance. In this paper, we investigate a novel framework of FEEL, namely semi-decentralized federated edge learning (SD-FEEL), where multiple edge servers are employed to collectively coordinate a large number of client nodes. By exploiting the low-latency communication among edge servers for efficient model sharing, SD-FEEL can incorporate more training data, while enjoying much lower latency compared with conventional federated learning. We detail the training algorithm for SD-FEEL with three main steps, including local model update, intra-cluster, and inter-cluster model aggregations. The convergence of this algorithm is proved on non-i",
    "link": "http://arxiv.org/abs/2112.10313",
    "context": "Title: Semi-Decentralized Federated Edge Learning with Data and Device Heterogeneity. (arXiv:2112.10313v3 [cs.LG] UPDATED)\nAbstract: Federated edge learning (FEEL) has attracted much attention as a privacy-preserving paradigm to effectively incorporate the distributed data at the network edge for training deep learning models. Nevertheless, the limited coverage of a single edge server results in an insufficient number of participated client nodes, which may impair the learning performance. In this paper, we investigate a novel framework of FEEL, namely semi-decentralized federated edge learning (SD-FEEL), where multiple edge servers are employed to collectively coordinate a large number of client nodes. By exploiting the low-latency communication among edge servers for efficient model sharing, SD-FEEL can incorporate more training data, while enjoying much lower latency compared with conventional federated learning. We detail the training algorithm for SD-FEEL with three main steps, including local model update, intra-cluster, and inter-cluster model aggregations. The convergence of this algorithm is proved on non-i",
    "path": "papers/21/12/2112.10313.json",
    "total_tokens": 944,
    "translated_title": "带有数据和设备异构性的半分散联邦边缘学习",
    "translated_abstract": "联邦边缘学习（FEEL）以保护隐私的方式有效地利用网络边缘的分布式数据进行深度学习模型的训练，备受关注。然而，单个边缘服务器的覆盖范围有限，导致参与客户端节点数量不足，可能影响学习性能。本文探讨了一种新的FEEL框架，即半分散联邦边缘学习（SD-FEEL），其中多个边缘服务器共同协调大量客户端节点。通过利用边缘服务器之间的低延迟通信实现高效的模型共享，SD-FEEL可以整合更多训练数据，同时与传统联邦学习相比具有更低的延迟。我们详细介绍了SD-FEEL的训练算法，其中包括本地模型更新、簇内和簇间模型聚合等三个主要步骤。证明了该算法在非独立同分布的数据上的收敛性。",
    "tldr": "本文提出了一种半分散联邦边缘学习（SD-FEEL）框架，采用多个边缘服务器协调大量客户端节点进行深度学习模型的训练，有效利用边缘计算资源以增加训练数据量和降低延迟。"
}