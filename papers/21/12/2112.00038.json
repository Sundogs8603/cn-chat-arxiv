{
    "title": "Robust and Provably Monotonic Networks. (arXiv:2112.00038v2 [cs.LG] UPDATED)",
    "abstract": "The Lipschitz constant of the map between the input and output space represented by a neural network is a natural metric for assessing the robustness of the model. We present a new method to constrain the Lipschitz constant of dense deep learning models that can also be generalized to other architectures. The method relies on a simple weight normalization scheme during training that ensures the Lipschitz constant of every layer is below an upper limit specified by the analyst. A simple monotonic residual connection can then be used to make the model monotonic in any subset of its inputs, which is useful in scenarios where domain knowledge dictates such dependence. Examples can be found in algorithmic fairness requirements or, as presented here, in the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider. Our normalization is minimally constraining and allows the underlying architecture to maintain higher expressiveness compared to other techniq",
    "link": "http://arxiv.org/abs/2112.00038",
    "context": "Title: Robust and Provably Monotonic Networks. (arXiv:2112.00038v2 [cs.LG] UPDATED)\nAbstract: The Lipschitz constant of the map between the input and output space represented by a neural network is a natural metric for assessing the robustness of the model. We present a new method to constrain the Lipschitz constant of dense deep learning models that can also be generalized to other architectures. The method relies on a simple weight normalization scheme during training that ensures the Lipschitz constant of every layer is below an upper limit specified by the analyst. A simple monotonic residual connection can then be used to make the model monotonic in any subset of its inputs, which is useful in scenarios where domain knowledge dictates such dependence. Examples can be found in algorithmic fairness requirements or, as presented here, in the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider. Our normalization is minimally constraining and allows the underlying architecture to maintain higher expressiveness compared to other techniq",
    "path": "papers/21/12/2112.00038.json",
    "total_tokens": 910,
    "translated_title": "强健且可证明单调的神经网络",
    "translated_abstract": "神经网络中表示输入和输出空间之间映射的利普希茨常数是评估模型鲁棒性的自然度量。我们提出了一种新方法，用于约束密集深度学习模型的利普希茨常数，该方法也可推广到其他架构。该方法依赖于训练期间的简单权重归一化方案，以确保每个层的利普希茨常数低于分析师指定的上限。然后可以使用简单的单调剩余连接使模型在其任何子集的输入中单调，这在领域知识指导此类依赖性的场景中非常有用，例如在算法公平性要求中，或者像在此处展示的那样，在对CERN大型强子对撞机产生的次原子粒子的衰减进行分类时。我们的归一化方法对架构的约束最小，并允许保持更高的表现力，相比其他技术。",
    "tldr": "本论文提出了一种方法，可以约束深度学习模型的利普希茨常数，并使用单调剩余连接使模型的某些输入单调，适用于需要领域知识指导依赖性的场景，如算法公平性要求及物理学中的次原子粒子分类。"
}