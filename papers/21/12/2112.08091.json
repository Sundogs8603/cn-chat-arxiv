{
    "title": "Ensuring DNN Solution Feasibility for Optimization Problems with Convex Constraints and Its Application to DC Optimal Power Flow Problems. (arXiv:2112.08091v2 [cs.LG] UPDATED)",
    "abstract": "Ensuring solution feasibility is a key challenge in developing Deep Neural Network (DNN) schemes for solving constrained optimization problems, due to inherent DNN prediction errors. In this paper, we propose a ``preventive learning'' framework to guarantee DNN solution feasibility for problems with convex constraints and general objective functions without post-processing, upon satisfying a mild condition on constraint calibration. Without loss of generality, we focus on problems with only inequality constraints. We systematically calibrate inequality constraints used in DNN training, thereby anticipating prediction errors and ensuring the resulting solutions remain feasible. We characterize the calibration magnitudes and the DNN size sufficient for ensuring universal feasibility. We propose a new Adversarial-Sample Aware training algorithm to improve DNN's optimality performance without sacrificing feasibility guarantee. Overall, the framework provides two DNNs. The first one from ch",
    "link": "http://arxiv.org/abs/2112.08091",
    "context": "Title: Ensuring DNN Solution Feasibility for Optimization Problems with Convex Constraints and Its Application to DC Optimal Power Flow Problems. (arXiv:2112.08091v2 [cs.LG] UPDATED)\nAbstract: Ensuring solution feasibility is a key challenge in developing Deep Neural Network (DNN) schemes for solving constrained optimization problems, due to inherent DNN prediction errors. In this paper, we propose a ``preventive learning'' framework to guarantee DNN solution feasibility for problems with convex constraints and general objective functions without post-processing, upon satisfying a mild condition on constraint calibration. Without loss of generality, we focus on problems with only inequality constraints. We systematically calibrate inequality constraints used in DNN training, thereby anticipating prediction errors and ensuring the resulting solutions remain feasible. We characterize the calibration magnitudes and the DNN size sufficient for ensuring universal feasibility. We propose a new Adversarial-Sample Aware training algorithm to improve DNN's optimality performance without sacrificing feasibility guarantee. Overall, the framework provides two DNNs. The first one from ch",
    "path": "papers/21/12/2112.08091.json",
    "total_tokens": 999,
    "translated_title": "基于凸约束的优化问题中DNN方案的可行性保证及其在直流最优潮流问题中的应用",
    "translated_abstract": "在开发用于解决受限制优化问题的深度神经网络（DNN）方案时，确保解的可行性是一个关键挑战，由于DNN固有的预测误差。本文提出了一个“预防性学习”框架，以在满足对约束标定的温和条件下，保证具有凸约束和一般目标函数的问题的DNN解的可行性，而无需后处理。我们无失一般性地关注只有不等式约束的问题。我们系统地标定DNN训练中使用的不等式约束，从而预示预测误差并确保所得到的解仍然是可行的。我们表征了标定量和DNN大小足以确保通用可行性。我们提出了一种新的对抗样本感知的训练算法，以提高DNN的最优性能，而不会牺牲可行性保证。总的来说，该框架提供了两个DNN解。",
    "tldr": "本文提出了一个“预防性学习”框架，以在满足对约束标定的条件下，保证具有凸约束和一般目标函数的问题的DNN解的可行性，而无需后处理。通过系统标定不等式约束，我们预示预测误差并确保所得到的解仍然是可行的。同时提出了一种新的对抗样本感知的训练算法以提高DNN的最优性能而不牺牲可行性保证。",
    "en_tdlr": "This paper proposes a \"preventive learning\" framework to ensure DNN solution feasibility for problems with convex constraints and general objective functions without post-processing. By systematically calibrating inequality constraints, the framework anticipates prediction errors and ensures that the resulting solutions remain feasible. A new Adversarial-Sample Aware training algorithm is also proposed to improve DNN's optimality performance without sacrificing feasibility guarantee."
}