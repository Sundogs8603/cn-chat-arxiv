{
    "title": "Self-Supervised Graph Representation Learning for Neuronal Morphologies. (arXiv:2112.12482v3 [stat.ML] UPDATED)",
    "abstract": "Unsupervised graph representation learning has recently gained interest in several application domains such as neuroscience, where modeling the diverse morphology of cell types in the brain is one of the key challenges. It is currently unknown how many excitatory cortical cell types exist and what their defining morphological features are. Here we present GraphDINO, a purely data-driven approach to learn low-dimensional representations of 3D neuronal morphologies from unlabeled large-scale datasets. GraphDINO is a novel transformer-based representation learning method for spatially-embedded graphs. To enable self-supervised learning on transformers, we (1) developed data augmentation strategies for spatially-embedded graphs, (2) adapted the positional encoding and (3) introduced a novel attention mechanism, AC-Attention, which combines attention-based global interaction between nodes and classic graph convolutional processing. We show, in two different species and across multiple brain",
    "link": "http://arxiv.org/abs/2112.12482",
    "context": "Title: Self-Supervised Graph Representation Learning for Neuronal Morphologies. (arXiv:2112.12482v3 [stat.ML] UPDATED)\nAbstract: Unsupervised graph representation learning has recently gained interest in several application domains such as neuroscience, where modeling the diverse morphology of cell types in the brain is one of the key challenges. It is currently unknown how many excitatory cortical cell types exist and what their defining morphological features are. Here we present GraphDINO, a purely data-driven approach to learn low-dimensional representations of 3D neuronal morphologies from unlabeled large-scale datasets. GraphDINO is a novel transformer-based representation learning method for spatially-embedded graphs. To enable self-supervised learning on transformers, we (1) developed data augmentation strategies for spatially-embedded graphs, (2) adapted the positional encoding and (3) introduced a novel attention mechanism, AC-Attention, which combines attention-based global interaction between nodes and classic graph convolutional processing. We show, in two different species and across multiple brain",
    "path": "papers/21/12/2112.12482.json",
    "total_tokens": 1057,
    "translated_title": "自监督的图形表示学习在神经元形态学中的应用",
    "translated_abstract": "无监督的图形表示学习在多个应用领域如神经科学中吸引了越来越多的关注，其中对大脑中细胞类型的多样形态进行建模是其中的关键挑战之一。本文提出了GraphDINO，一种用于从未标记的大规模数据集中学习三维神经元形态的低维表示的数据驱动方法。GraphDINO 是一种新的基于Transformer模型的用于空间嵌入式图形表示学习的方法。为了使Transformer模型能够进行自监督学习，我们 (1)开发了针对空间嵌入式图形的数据增强策略， (2) 对位置编码进行了修改， (3)引入了新型的注意力机制AC-Attention，它结合了节点间基于注意力的全局交互和传统的图形卷积处理。我们在两个不同种类的、跨越多个大脑区域的神经元数据上测试，结果表明，GraphDINO 在神经元形态学表示学习方面表现出色，优于目前最先进的方法。研究结果表明，所提出的自监督方法可以学习到捕捉神经元形态学相关特征的表示。",
    "tldr": "本文提出了GraphDINO，一种自监督的图形表示学习方法，可用于从未标记的大规模数据集中学习三维神经元形态的低维表示。该方法使用了一系列数据增强策略和新型的注意力机制AC-Attention，在多个大脑区域内，GraphDINO 显示出了优于其它最先进方法的表现。",
    "en_tdlr": "This paper presents GraphDINO, a self-supervised graph representation learning method for low-dimensional representation of 3D neuronal morphologies from unlabeled large-scale datasets. The method utilizes data augmentation strategies and a novel attention mechanism called AC-Attention to outperform current state-of-the-art methods in multiple brain regions."
}