{
    "title": "Faster Deep Reinforcement Learning with Slower Online Network. (arXiv:2112.05848v3 [cs.LG] UPDATED)",
    "abstract": "Deep reinforcement learning algorithms often use two networks for value function optimization: an online network, and a target network that tracks the online network with some delay. Using two separate networks enables the agent to hedge against issues that arise when performing bootstrapping. In this paper we endow two popular deep reinforcement learning algorithms, namely DQN and Rainbow, with updates that incentivize the online network to remain in the proximity of the target network. This improves the robustness of deep reinforcement learning in presence of noisy updates. The resultant agents, called DQN Pro and Rainbow Pro, exhibit significant performance improvements over their original counterparts on the Atari benchmark demonstrating the effectiveness of this simple idea in deep reinforcement learning. The code for our paper is available here: Github.com/amazon-research/fast-rl-with-slow-updates.",
    "link": "http://arxiv.org/abs/2112.05848",
    "context": "Title: Faster Deep Reinforcement Learning with Slower Online Network. (arXiv:2112.05848v3 [cs.LG] UPDATED)\nAbstract: Deep reinforcement learning algorithms often use two networks for value function optimization: an online network, and a target network that tracks the online network with some delay. Using two separate networks enables the agent to hedge against issues that arise when performing bootstrapping. In this paper we endow two popular deep reinforcement learning algorithms, namely DQN and Rainbow, with updates that incentivize the online network to remain in the proximity of the target network. This improves the robustness of deep reinforcement learning in presence of noisy updates. The resultant agents, called DQN Pro and Rainbow Pro, exhibit significant performance improvements over their original counterparts on the Atari benchmark demonstrating the effectiveness of this simple idea in deep reinforcement learning. The code for our paper is available here: Github.com/amazon-research/fast-rl-with-slow-updates.",
    "path": "papers/21/12/2112.05848.json",
    "total_tokens": 874,
    "translated_title": "使用较慢的在线网络实现更快的深度强化学习",
    "translated_abstract": "深度强化学习算法通常使用两个网络进行价值函数优化：一个在线网络和一个目标网络，后者带有一定的延迟。使用两个独立的网络使得智能体能够对抗启发式引导时出现的问题。本文在两个流行的深度强化学习算法（即DQN和Rainbow）中引入了更新算法，以激励在线网络保持与目标网络的接近，从而提高在存在噪声更新时深度强化学习的鲁棒性。由此产生的代理称为DQN Pro和Rainbow Pro，它们在Atari基准测试上相对于原始算法表现出显著的性能提升，证明了这种深度强化学习中的简单思想的有效性。我们的论文代码可在Github.com/amazon-research/fast-rl-with-slow-updates 上获取。",
    "tldr": "本文改进了DQN和Rainbow两个深度强化学习算法，大大提高了它们在Atari游戏基准测试中的性能，我们的方法是在在线网络和目标网络之间引入一定的接近度，以提高深度强化学习的鲁棒性。",
    "en_tdlr": "This paper improves the performance of two deep reinforcement learning algorithms, DQN and Rainbow, on the Atari benchmark. By incentivizing the online network to remain close to the target network, the proposed updates increase the robustness of deep reinforcement learning in the presence of noisy updates. The resultant agents, called DQN Pro and Rainbow Pro, exhibit significant performance improvements over their original counterparts."
}