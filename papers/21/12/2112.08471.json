{
    "title": "Gaining Outlier Resistance with Progressive Quantiles: Fast Algorithms and Theoretical Studies. (arXiv:2112.08471v3 [stat.ME] UPDATED)",
    "abstract": "Outliers widely occur in big-data applications and may severely affect statistical estimation and inference. In this paper, a framework of outlier-resistant estimation is introduced to robustify an arbitrarily given loss function. It has a close connection to the method of trimming and includes explicit outlyingness parameters for all samples, which in turn facilitates computation, theory, and parameter tuning. To tackle the issues of nonconvexity and nonsmoothness, we develop scalable algorithms with implementation ease and guaranteed fast convergence. In particular, a new technique is proposed to alleviate the requirement on the starting point such that on regular datasets, the number of data resamplings can be substantially reduced. Based on combined statistical and computational treatments, we are able to perform nonasymptotic analysis beyond M-estimation. The obtained resistant estimators, though not necessarily globally or even locally optimal, enjoy minimax rate optimality in bo",
    "link": "http://arxiv.org/abs/2112.08471",
    "context": "Title: Gaining Outlier Resistance with Progressive Quantiles: Fast Algorithms and Theoretical Studies. (arXiv:2112.08471v3 [stat.ME] UPDATED)\nAbstract: Outliers widely occur in big-data applications and may severely affect statistical estimation and inference. In this paper, a framework of outlier-resistant estimation is introduced to robustify an arbitrarily given loss function. It has a close connection to the method of trimming and includes explicit outlyingness parameters for all samples, which in turn facilitates computation, theory, and parameter tuning. To tackle the issues of nonconvexity and nonsmoothness, we develop scalable algorithms with implementation ease and guaranteed fast convergence. In particular, a new technique is proposed to alleviate the requirement on the starting point such that on regular datasets, the number of data resamplings can be substantially reduced. Based on combined statistical and computational treatments, we are able to perform nonasymptotic analysis beyond M-estimation. The obtained resistant estimators, though not necessarily globally or even locally optimal, enjoy minimax rate optimality in bo",
    "path": "papers/21/12/2112.08471.json",
    "total_tokens": 977,
    "translated_title": "在渐进分位数中获得离群值抗性：快速算法和理论研究",
    "translated_abstract": "离群值在大数据应用中普遍存在，可能严重影响统计估计和推断。本文介绍了一个离群值抗性估计的框架，用于增强任意给定损失函数的鲁棒性。它与修剪方法有着密切的联系，并为所有样本提供了显式的离群指标，从而方便计算、理论和参数调整。为了解决凸性和不光滑性问题，我们开发了可伸缩的算法，并保证了快速收敛性。特别地，我们提出了一种新技术，以减轻对起始点的要求，使得在常规数据集上，数据重新采样的次数可以大大减少。基于统计和计算的综合处理，我们能够进行超越M-估计的非渐进分析。得到的鲁棒估计量，虽然不一定是全局甚至是局部最优，但在广泛的统计模型中具有渐近的最小大误差率最优性，包括参数和非参数设置。",
    "tldr": "本文提出了一个离群值抗性估计的框架，通过渐进分位数算法解决离群值问题，并开发可伸缩算法来保证快速收敛性和超越M-估计的非渐进分析。",
    "en_tdlr": "This paper proposes a framework of outlier-resistant estimation to deal with outliers in big-data applications. The framework utilizes progressive quantile algorithms and develops scalable algorithms with guaranteed fast convergence to address the issues of nonconvexity and nonsmoothness. The obtained resistant estimators have minimax rate optimality in both parametric and nonparametric settings."
}