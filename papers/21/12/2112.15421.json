{
    "title": "Representation Learning via Consistent Assignment of Views to Clusters. (arXiv:2112.15421v2 [cs.LG] CROSS LISTED)",
    "abstract": "We introduce Consistent Assignment for Representation Learning (CARL), an unsupervised learning method to learn visual representations by combining ideas from self-supervised contrastive learning and deep clustering. By viewing contrastive learning from a clustering perspective, CARL learns unsupervised representations by learning a set of general prototypes that serve as energy anchors to enforce different views of a given image to be assigned to the same prototype. Unlike contemporary work on contrastive learning with deep clustering, CARL proposes to learn the set of general prototypes in an online fashion, using gradient descent without the necessity of using non-differentiable algorithms or K-Means to solve the cluster assignment problem. CARL surpasses its competitors in many representations learning benchmarks, including linear evaluation, semi-supervised learning, and transfer learning.",
    "link": "http://arxiv.org/abs/2112.15421",
    "context": "Title: Representation Learning via Consistent Assignment of Views to Clusters. (arXiv:2112.15421v2 [cs.LG] CROSS LISTED)\nAbstract: We introduce Consistent Assignment for Representation Learning (CARL), an unsupervised learning method to learn visual representations by combining ideas from self-supervised contrastive learning and deep clustering. By viewing contrastive learning from a clustering perspective, CARL learns unsupervised representations by learning a set of general prototypes that serve as energy anchors to enforce different views of a given image to be assigned to the same prototype. Unlike contemporary work on contrastive learning with deep clustering, CARL proposes to learn the set of general prototypes in an online fashion, using gradient descent without the necessity of using non-differentiable algorithms or K-Means to solve the cluster assignment problem. CARL surpasses its competitors in many representations learning benchmarks, including linear evaluation, semi-supervised learning, and transfer learning.",
    "path": "papers/21/12/2112.15421.json",
    "total_tokens": 829,
    "translated_title": "通过一致分配视图到聚类中进行表示学习",
    "translated_abstract": "我们介绍了一种名为CARL的一致分配表示学习的无监督学习方法，该方法结合了自监督对比学习和深度聚类的思想。通过从聚类的角度看待对比学习，CARL通过学习一组通用原型来学习无监督表示，这些原型作为能量锚点，强制将给定图像的不同视图分配给相同的原型。与当代的深度聚类对比学习相比，CARL提出以在线方式学习一组通用原型，使用梯度下降而不需要使用非可微分算法或K-Means来解决聚类分配问题。CARL在许多表示学习基准测试中超越了竞争对手，包括线性评估、半监督学习和迁移学习。",
    "tldr": "该论文介绍了一种名为CARL的无监督学习方法，通过将对比学习和深度聚类相结合，通过学习一组通用原型来实现一致分配视图到聚类中进行表示学习。CARL在多个表示学习基准测试中超越了其他方法。",
    "en_tdlr": "This paper introduces CARL, an unsupervised learning method that combines contrastive learning and deep clustering to achieve consistent assignment of views to clusters for representation learning. CARL surpasses competitors in multiple representation learning benchmarks."
}