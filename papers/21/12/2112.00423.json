{
    "title": "Controlling Wasserstein Distances by Kernel Norms with Application to Compressive Statistical Learning. (arXiv:2112.00423v3 [stat.ML] UPDATED)",
    "abstract": "Comparing probability distributions is at the crux of many machine learning algorithms. Maximum Mean Discrepancies (MMD) and Wasserstein distances are two classes of distances between probability distributions that have attracted abundant attention in past years. This paper establishes some conditions under which the Wasserstein distance can be controlled by MMD norms. Our work is motivated by the compressive statistical learning (CSL) theory, a general framework for resource-efficient large scale learning in which the training data is summarized in a single vector (called sketch) that captures the information relevant to the considered learning task. Inspired by existing results in CSL, we introduce the H\\\"older Lower Restricted Isometric Property and show that this property comes with interesting guarantees for compressive statistical learning. Based on the relations between the MMD and the Wasserstein distances, we provide guarantees for compressive statistical learning by introduci",
    "link": "http://arxiv.org/abs/2112.00423",
    "context": "Title: Controlling Wasserstein Distances by Kernel Norms with Application to Compressive Statistical Learning. (arXiv:2112.00423v3 [stat.ML] UPDATED)\nAbstract: Comparing probability distributions is at the crux of many machine learning algorithms. Maximum Mean Discrepancies (MMD) and Wasserstein distances are two classes of distances between probability distributions that have attracted abundant attention in past years. This paper establishes some conditions under which the Wasserstein distance can be controlled by MMD norms. Our work is motivated by the compressive statistical learning (CSL) theory, a general framework for resource-efficient large scale learning in which the training data is summarized in a single vector (called sketch) that captures the information relevant to the considered learning task. Inspired by existing results in CSL, we introduce the H\\\"older Lower Restricted Isometric Property and show that this property comes with interesting guarantees for compressive statistical learning. Based on the relations between the MMD and the Wasserstein distances, we provide guarantees for compressive statistical learning by introduci",
    "path": "papers/21/12/2112.00423.json",
    "total_tokens": 1066,
    "translated_title": "用核范数控制Wasserstein距离，并在压缩统计学习中应用",
    "translated_abstract": "在许多机器学习算法中，比较概率分布是关键。最大均值差异（MMD）和Wasserstein距离是两类概率分布距离，近年来受到了广泛关注。本文建立了一些条件，使得可以通过MMD范数来控制Wasserstein距离。我们的工作受到压缩统计学习（CSL）理论的启发，这是一种资源有效的大规模学习通用框架，在其中训练数据在单个向量（称为草图）中进行总结，以捕捉与考虑的学习任务相关的信息。我们在现有的CSL结果的启发下引入了H\\\"older Lower Restricted Isometric Property，并展示了这种属性对于压缩统计学习具有有趣的保证。基于MMD和Wasserstein距离之间的关系，我们提供了一种基于MMD导出的新核范数的压缩统计学习保证。我们的理论为在压缩统计学习中使用Wasserstein距离的学习提供了一种计算上高效的两步算法。我们将我们的方法应用于合成和真实数据集，展示了Wasserstein距离相对于CSL中其他常用距离的实质性改进。",
    "tldr": "本文提供了用MMD范数控制Wasserstein距离的条件，针对压缩统计学习提出了HLRIP属性，通过导出的新核范数提供了计算上高效的Wasserstein距离压缩统计学习保证。",
    "en_tdlr": "The paper establishes conditions for controlling Wasserstein distances using MMD norms and introduces the H\\\"older Lower Restricted Isometric Property (HLRIP) for compressive statistical learning, leading to a computationally efficient two-step algorithm. The novel kernel norm derived from MMD provides guarantees for Wasserstein distance compressive statistical learning and shows significant improvements over other commonly used distances."
}