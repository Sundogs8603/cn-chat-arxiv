{
    "title": "Dilated convolution with learnable spacings. (arXiv:2112.03740v4 [cs.CV] UPDATED)",
    "abstract": "Recent works indicate that convolutional neural networks (CNN) need large receptive fields (RF) to compete with visual transformers and their attention mechanism. In CNNs, RFs can simply be enlarged by increasing the convolution kernel sizes. Yet the number of trainable parameters, which scales quadratically with the kernel's size in the 2D case, rapidly becomes prohibitive, and the training is notoriously difficult. This paper presents a new method to increase the RF size without increasing the number of parameters. The dilated convolution (DC) has already been proposed for the same purpose. DC can be seen as a convolution with a kernel that contains only a few non-zero elements placed on a regular grid. Here we present a new version of the DC in which the spacings between the non-zero elements, or equivalently their positions, are no longer fixed but learnable via backpropagation thanks to an interpolation technique. We call this method \"Dilated Convolution with Learnable Spacings\" (",
    "link": "http://arxiv.org/abs/2112.03740",
    "context": "Title: Dilated convolution with learnable spacings. (arXiv:2112.03740v4 [cs.CV] UPDATED)\nAbstract: Recent works indicate that convolutional neural networks (CNN) need large receptive fields (RF) to compete with visual transformers and their attention mechanism. In CNNs, RFs can simply be enlarged by increasing the convolution kernel sizes. Yet the number of trainable parameters, which scales quadratically with the kernel's size in the 2D case, rapidly becomes prohibitive, and the training is notoriously difficult. This paper presents a new method to increase the RF size without increasing the number of parameters. The dilated convolution (DC) has already been proposed for the same purpose. DC can be seen as a convolution with a kernel that contains only a few non-zero elements placed on a regular grid. Here we present a new version of the DC in which the spacings between the non-zero elements, or equivalently their positions, are no longer fixed but learnable via backpropagation thanks to an interpolation technique. We call this method \"Dilated Convolution with Learnable Spacings\" (",
    "path": "papers/21/12/2112.03740.json",
    "total_tokens": 933,
    "translated_title": "带有可学习间隔的膨胀卷积",
    "translated_abstract": "最近的研究表明，卷积神经网络(CNN)需要拥有较大的感受野才能与视觉Transformer和其注意机制竞争。在CNN中，感受野可以通过增加卷积核大小来简单地扩大。然而，在2D情况下，可训练参数的数量与核大小成二次比例，迅速变得不可行，并且训练是难以处理的。本文提出了一种新方法，可以增加感受野的大小，而不增加可训练参数的数量。膨胀卷积(DC)已经为同样的目的提出。DC可以看作是一个核仅包含少数非零元素排列在一个规则网格上的卷积。在这里，我们提出了DC的一种新版本，其中非零元素之间的间隔，或等效地说，它们的位置，不再固定，而是可通过反向传播学习，借助插值技术。我们将这种方法称为“带有可学习间隔的膨胀卷积”。",
    "tldr": "本文提出了一种新型膨胀卷积(DC)方法，通过插值技术学习感受野中非零元素位置之间的间隔，从而增加感受野大小而不增加可训练参数的数量。",
    "en_tdlr": "This paper proposes a novel method of dilated convolution (DC) with learnable spacings, which increases the receptive field size without increasing the number of trainable parameters by using interpolation technique to learn the spacings between non-zero elements."
}