{
    "title": "LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI. (arXiv:2112.02333v2 [cs.AI] UPDATED)",
    "abstract": "Natural Language Inference (NLI) is considered a representative task to test natural language understanding (NLU). In this work, we propose an extensible framework to collectively yet categorically test diverse Logical reasoning capabilities required for NLI (and, by extension, NLU). Motivated by behavioral testing, we create a semi-synthetic large test bench (363 templates, 363k examples) and an associated framework that offers the following utilities: 1) individually test and analyze reasoning capabilities along 17 reasoning dimensions (including pragmatic reasoning); 2) design experiments to study cross-capability information content (leave one out or bring one in); and 3) the synthetic nature enables us to control for artifacts and biases. We extend a publicly available framework of automated test case instantiation from free-form natural language templates (CheckList) and a well-defined taxonomy of capabilities to cover a wide range of increasingly harder test cases while varying ",
    "link": "http://arxiv.org/abs/2112.02333",
    "context": "Title: LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI. (arXiv:2112.02333v2 [cs.AI] UPDATED)\nAbstract: Natural Language Inference (NLI) is considered a representative task to test natural language understanding (NLU). In this work, we propose an extensible framework to collectively yet categorically test diverse Logical reasoning capabilities required for NLI (and, by extension, NLU). Motivated by behavioral testing, we create a semi-synthetic large test bench (363 templates, 363k examples) and an associated framework that offers the following utilities: 1) individually test and analyze reasoning capabilities along 17 reasoning dimensions (including pragmatic reasoning); 2) design experiments to study cross-capability information content (leave one out or bring one in); and 3) the synthetic nature enables us to control for artifacts and biases. We extend a publicly available framework of automated test case instantiation from free-form natural language templates (CheckList) and a well-defined taxonomy of capabilities to cover a wide range of increasingly harder test cases while varying ",
    "path": "papers/21/12/2112.02333.json",
    "total_tokens": 998,
    "translated_title": "LoNLI：一个可扩展的框架，用于测试NLI的多样化逻辑推理能力",
    "translated_abstract": "自然语言推理（NLI）被认为是测试自然语言理解（NLU）的代表性任务。在这项工作中，我们提出了一个可扩展的框架，以集体但有分类地测试NLI（以及NLU的延伸）所需的多样化逻辑推理能力。受行为测试的启发，我们创建了一个半合成的大型测试平台（363个模板，363k个例子）和一个相关的框架，提供以下实用功能：1）单独测试和分析17个推理维度的推理能力（包括语用推理）；2）设计实验以研究跨能力信息内容（排除一个或增加一个）；3）合成的性质使我们能够控制人工和偏见。我们扩展了一个公开可用的自动化测试用例实例化框架（CheckList）和一个定义明确的能力分类法，以涵盖更广泛、更困难的测试用例范围，同时变化着能力。",
    "tldr": "本文提出了LoNLI框架，可用于集体测试NLI的不同逻辑推理能力。通过创建一个大型测试平台和相关框架，我们可以单独测试和分析多个推理维度的能力，并进行跨能力信息内容的实验研究。这项工作的贡献在于提供一个可扩展的测试框架，帮助深入了解NLI和NLU领域的逻辑推理。"
}