{
    "title": "Towards Interactive Reinforcement Learning with Intrinsic Feedback. (arXiv:2112.01575v3 [cs.AI] UPDATED)",
    "abstract": "Reinforcement learning (RL) and brain-computer interfaces (BCI) have experienced significant growth over the past decade. With rising interest in human-in-the-loop (HITL), incorporating human input with RL algorithms has given rise to the sub-field of interactive RL. Adjacently, the field of BCI has long been interested in extracting informative brain signals from neural activity for use in human-computer interactions. A key link between these fields lies in the interpretation of neural activity as feedback such that interactive RL approaches can be employed. We denote this new and emerging medium of feedback as intrinsic feedback. Despite intrinsic feedback's ability to be conveyed automatically and even unconsciously, proper exploration surrounding this key link has largely gone unaddressed by both communities. Thus, to help facilitate a deeper understanding and a more effective utilization, we provide a tutorial-style review covering the motivations, approaches, and open problems of",
    "link": "http://arxiv.org/abs/2112.01575",
    "context": "Title: Towards Interactive Reinforcement Learning with Intrinsic Feedback. (arXiv:2112.01575v3 [cs.AI] UPDATED)\nAbstract: Reinforcement learning (RL) and brain-computer interfaces (BCI) have experienced significant growth over the past decade. With rising interest in human-in-the-loop (HITL), incorporating human input with RL algorithms has given rise to the sub-field of interactive RL. Adjacently, the field of BCI has long been interested in extracting informative brain signals from neural activity for use in human-computer interactions. A key link between these fields lies in the interpretation of neural activity as feedback such that interactive RL approaches can be employed. We denote this new and emerging medium of feedback as intrinsic feedback. Despite intrinsic feedback's ability to be conveyed automatically and even unconsciously, proper exploration surrounding this key link has largely gone unaddressed by both communities. Thus, to help facilitate a deeper understanding and a more effective utilization, we provide a tutorial-style review covering the motivations, approaches, and open problems of",
    "path": "papers/21/12/2112.01575.json",
    "total_tokens": 882,
    "translated_title": "向具有内在反馈的交互式强化学习迈进",
    "translated_abstract": "过去十年，强化学习（RL）和脑机接口（BCI）取得了显著的发展。随着人们对人机协同（HITL）的兴趣日益增长，将人类输入与RL算法相结合已经催生了交互式RL的子领域。同时，BCI领域长期以来一直致力于从神经活动中提取有关人机交互的信息性脑信号。这两个领域之间的关键联系在于将神经活动解释为反馈，以便可以应用交互式RL方法。我们将这种新兴的反馈介质称为内在反馈。尽管内在反馈能够自动传达甚至无意识地传达，但对于这个关键联系的适当探索几乎未受到两个领域的关注。因此，为了促进深入理解和更有效的利用，我们提供了一个教程风格的综述，涵盖了动机、方法和开放问题。",
    "tldr": "这篇论文综述了交互式强化学习与内在反馈的关系，强调了将人类输入与RL算法结合的重要性，并指出在这一关键联系上的探索仍有待加强。"
}