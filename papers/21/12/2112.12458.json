{
    "title": "Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2112.12458v3 [cs.LG] UPDATED)",
    "abstract": "Many recent successful off-policy multi-agent reinforcement learning (MARL) algorithms for cooperative partially observable environments focus on finding factorized value functions, leading to convoluted network structures. Building on the structure of independent Q-learners, our LAN algorithm takes a radically different approach, leveraging a dueling architecture to learn for each agent a decentralized best-response policies via individual advantage functions. The learning is stabilized by a centralized critic whose primary objective is to reduce the moving target problem of the individual advantages. The critic, whose network's size is independent of the number of agents, is cast aside after learning. Evaluation on the StarCraft II multi-agent challenge benchmark shows that LAN reaches state-of-the-art performance and is highly scalable with respect to the number of agents, opening up a promising alternative direction for MARL research.",
    "link": "http://arxiv.org/abs/2112.12458",
    "context": "Title: Local Advantage Networks for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2112.12458v3 [cs.LG] UPDATED)\nAbstract: Many recent successful off-policy multi-agent reinforcement learning (MARL) algorithms for cooperative partially observable environments focus on finding factorized value functions, leading to convoluted network structures. Building on the structure of independent Q-learners, our LAN algorithm takes a radically different approach, leveraging a dueling architecture to learn for each agent a decentralized best-response policies via individual advantage functions. The learning is stabilized by a centralized critic whose primary objective is to reduce the moving target problem of the individual advantages. The critic, whose network's size is independent of the number of agents, is cast aside after learning. Evaluation on the StarCraft II multi-agent challenge benchmark shows that LAN reaches state-of-the-art performance and is highly scalable with respect to the number of agents, opening up a promising alternative direction for MARL research.",
    "path": "papers/21/12/2112.12458.json",
    "total_tokens": 872,
    "translated_title": "用于合作多智能体强化学习的局部优势网络",
    "translated_abstract": "近期成功的离策略多智能体强化学习（MARL）算法，用于合作的部分可观测环境，主要关注于寻找分解的值函数，导致复杂的网络结构。我们的LAN算法建立在独立Q学习者的结构基础上，采用一种完全不同的方法，利用对决架构通过个体优势函数为每个智能体学习分散的最佳响应策略。通过一个中心化的评论家稳定学习，评论家的主要目标是减少个体优势的移动目标问题。评论家的网络大小与智能体数量无关，在学习后被丢弃。在StarCraft II多智能体挑战基准测试上的评估结果显示，LAN达到了最先进的性能，并且对于智能体数量具有高度可扩展性，为MARL研究开辟了一个有前景的替代方向。",
    "tldr": "这项研究提出了一种局部优势网络（LAN）算法，该算法通过对决架构和中心化评论家来学习合作多智能体的最佳响应策略，并在StarCraft II多智能体挑战基准测试上达到了最先进的性能。",
    "en_tdlr": "This paper presents a Local Advantage Networks (LAN) algorithm that learns cooperative multi-agent's best-response strategies using a dueling architecture and a centralized critic, achieving state-of-the-art performance on the StarCraft II multi-agent challenge benchmark."
}