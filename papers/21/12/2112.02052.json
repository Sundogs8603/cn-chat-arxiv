{
    "title": "TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs. (arXiv:2112.02052v3 [cs.LG] UPDATED)",
    "abstract": "Recently, graph neural networks (GNNs), as the backbone of graph-based machine learning, demonstrate great success in various domains (e.g., e-commerce). However, the performance of GNNs is usually unsatisfactory due to the highly sparse and irregular graph-based operations. To this end, we propose TC-GNN, the first GNN acceleration framework based on GPU Tensor Core Units (TCUs). The core idea is to reconcile the \"Sparse\" GNN computation with the high-performance \"Dense\" TCUs. Specifically, we conduct an in-depth analysis of the sparse operations in mainstream GNN computing frameworks. We introduce a novel sparse graph translation technique to facilitate TCU processing of the sparse GNN workload. We implement an effective CUDA core and TCU collaboration design to fully utilize GPU resources. We integrate TC-GNN with the PyTorch framework for high programmability. Rigorous experiments show an average of 1.70X speedup over the state-of-the-art DGL framework across various models and dat",
    "link": "http://arxiv.org/abs/2112.02052",
    "context": "Title: TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs. (arXiv:2112.02052v3 [cs.LG] UPDATED)\nAbstract: Recently, graph neural networks (GNNs), as the backbone of graph-based machine learning, demonstrate great success in various domains (e.g., e-commerce). However, the performance of GNNs is usually unsatisfactory due to the highly sparse and irregular graph-based operations. To this end, we propose TC-GNN, the first GNN acceleration framework based on GPU Tensor Core Units (TCUs). The core idea is to reconcile the \"Sparse\" GNN computation with the high-performance \"Dense\" TCUs. Specifically, we conduct an in-depth analysis of the sparse operations in mainstream GNN computing frameworks. We introduce a novel sparse graph translation technique to facilitate TCU processing of the sparse GNN workload. We implement an effective CUDA core and TCU collaboration design to fully utilize GPU resources. We integrate TC-GNN with the PyTorch framework for high programmability. Rigorous experiments show an average of 1.70X speedup over the state-of-the-art DGL framework across various models and dat",
    "path": "papers/21/12/2112.02052.json",
    "total_tokens": 1013,
    "translated_title": "TC-GNN：在GPU上连接稀疏GNN计算与密集Tensor Cores的桥梁",
    "translated_abstract": "最近，作为基于图的机器学习的骨干，图神经网络（GNN）展示了在各个领域（如电子商务）的巨大成功。然而，由于高度稀疏和不规则的基于图的操作，GNN的性能通常不尽如人意。为此，我们提出了TC-GNN，基于GPU张量核心单元（TCUs）的第一个GNN加速框架。其核心思想是通过将“稀疏”GNN计算与高性能的“密集”TCUs协调一致，实现GNN计算效率的提升。具体来说，我们对主流GNN计算框架中的稀疏操作进行了深入分析。我们引入了一种新颖的稀疏图翻译技术，以便TCU处理稀疏的GNN工作负载。我们实现了一种有效的CUDA核心和TCU协作设计，充分利用GPU资源。我们将TC-GNN与PyTorch框架集成，以实现高可编程性。严格的实验表明，在各种模型和数据集上，相比于最先进的DGL框架，平均加速了1.70倍。",
    "tldr": "本文提出了TC-GNN，这是第一个基于GPU张量核心单元（TCUs）的GNN加速框架。采用稀疏图翻译技术来协调“稀疏”GNN计算与高性能的“密集”TCUs，实现了GNN计算效率的提升。",
    "en_tdlr": "This paper proposes TC-GNN, the first GNN acceleration framework based on GPU Tensor Core Units (TCUs). By using a novel sparse graph translation technique to reconcile \"sparse\" GNN computation with high-performance \"dense\" TCUs, TC-GNN achieves improvements in GNN computation efficiency."
}