{
    "title": "SCORE: Approximating Curvature Information under Self-Concordant Regularization. (arXiv:2112.07344v3 [cs.LG] UPDATED)",
    "abstract": "Optimization problems that include regularization functions in their objectives are regularly solved in many applications. When one seeks second-order methods for such problems, it may be desirable to exploit specific properties of some of these regularization functions when accounting for curvature information in the solution steps to speed up convergence. In this paper, we propose the SCORE (self-concordant regularization) framework for unconstrained minimization problems which incorporates second-order information in the Newton-decrement framework for convex optimization. We propose the generalized Gauss-Newton with Self-Concordant Regularization (GGN-SCORE) algorithm that updates the minimization variables each time it receives a new input batch. The proposed algorithm exploits the structure of the second-order information in the Hessian matrix, thereby reducing computational overhead. GGN-SCORE demonstrates how to speed up convergence while also improving model generalization for ",
    "link": "http://arxiv.org/abs/2112.07344",
    "context": "Title: SCORE: Approximating Curvature Information under Self-Concordant Regularization. (arXiv:2112.07344v3 [cs.LG] UPDATED)\nAbstract: Optimization problems that include regularization functions in their objectives are regularly solved in many applications. When one seeks second-order methods for such problems, it may be desirable to exploit specific properties of some of these regularization functions when accounting for curvature information in the solution steps to speed up convergence. In this paper, we propose the SCORE (self-concordant regularization) framework for unconstrained minimization problems which incorporates second-order information in the Newton-decrement framework for convex optimization. We propose the generalized Gauss-Newton with Self-Concordant Regularization (GGN-SCORE) algorithm that updates the minimization variables each time it receives a new input batch. The proposed algorithm exploits the structure of the second-order information in the Hessian matrix, thereby reducing computational overhead. GGN-SCORE demonstrates how to speed up convergence while also improving model generalization for ",
    "path": "papers/21/12/2112.07344.json",
    "total_tokens": 834,
    "translated_title": "SCORE: 在自对偶正则化下近似曲率信息",
    "translated_abstract": "在许多应用中，包含正则化函数的优化问题经常被求解。当我们寻求针对这些问题的二阶方法时，为了加速收敛，考虑到一些特定正则化函数的曲率信息可能是值得利用的。在本文中，我们提出了用于无约束极小化问题的SCORE（自对偶正则化）框架，该框架在牛顿减量框架中结合了二阶信息进行凸优化。我们提出了广义高斯-牛顿与自对偶正则化（GGN-SCORE）算法，该算法在每次接收到新的输入批次时更新极小化变量。该算法利用了Hessian矩阵中二阶信息的结构，从而减少了计算开销。GGN-SCORE演示了如何加速收敛，同时改善模型的泛化性能。",
    "tldr": "本文提出了SCORE框架，在无约束极小化问题中利用自对偶正则化近似曲率信息。我们还提出了GGN-SCORE算法，该算法通过更新极小化变量来加速收敛并改善模型泛化性能。",
    "en_tdlr": "This paper presents the SCORE framework for approximating curvature information under self-concordant regularization in unconstrained minimization problems. The proposed GGN-SCORE algorithm updates the minimization variables to accelerate convergence and improve model generalization."
}