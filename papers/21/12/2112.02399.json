{
    "title": "VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts. (arXiv:2112.02399v3 [cs.CV] UPDATED)",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has drawn increasing attention recently for its transferable visual representation learning. However, due to the semantic gap within datasets, CLIP's pre-trained image-text alignment becomes sub-optimal on downstream tasks, which severely harms its transferring performance. To better adapt the cross-modality embedding space, we propose to enhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guide textual features of different categories to adaptively explore informative regions on the image and aggregate visual features by attention mechanisms. In this way, the texts become visual-guided, namely, more semantically correlated with downstream images, which greatly benefits the category-wise matching process. In few-shot settings, we evaluate our VT-CLIP on 11 well-known classification datasets to demonstrate its effectiveness.",
    "link": "http://arxiv.org/abs/2112.02399",
    "context": "Title: VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts. (arXiv:2112.02399v3 [cs.CV] UPDATED)\nAbstract: Contrastive Language-Image Pre-training (CLIP) has drawn increasing attention recently for its transferable visual representation learning. However, due to the semantic gap within datasets, CLIP's pre-trained image-text alignment becomes sub-optimal on downstream tasks, which severely harms its transferring performance. To better adapt the cross-modality embedding space, we propose to enhance CLIP via Visual-guided Texts, named VT-CLIP. Specifically, we guide textual features of different categories to adaptively explore informative regions on the image and aggregate visual features by attention mechanisms. In this way, the texts become visual-guided, namely, more semantically correlated with downstream images, which greatly benefits the category-wise matching process. In few-shot settings, we evaluate our VT-CLIP on 11 well-known classification datasets to demonstrate its effectiveness.",
    "path": "papers/21/12/2112.02399.json",
    "total_tokens": 801,
    "translated_title": "VT-CLIP: 用视觉引导文本增强视觉-语言模型",
    "translated_abstract": "最近，对于其可转移的视觉表示学习，对比语言-图像预训练（CLIP）引起了越来越多的关注。然而，由于数据集内的语义差距，CLIP的预训练图像-文本对齐在下游任务中变得次优，严重影响了其迁移性能。为了更好地适应跨模态嵌入空间，我们提出了通过视觉引导文本来增强CLIP，命名为VT-CLIP。具体来说，我们通过注意机制引导不同类别的文本特征自适应地探索图像上的信息区域，并聚合视觉特征。这样，文本就成为了视觉引导的，即与下游图像更语义相关，这极大地有益于类别匹配的过程。在少样本设置中，我们评估了我们的VT-CLIP在11个知名分类数据集上的有效性。",
    "tldr": "VT-CLIP通过视觉引导文本来增强视觉-语言模型CLIP，在下游任务中展现出更好的迁移性能。"
}