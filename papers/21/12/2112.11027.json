{
    "title": "More is Less: Inducing Sparsity via Overparameterization. (arXiv:2112.11027v5 [math.OC] UPDATED)",
    "abstract": "In deep learning it is common to overparameterize neural networks, that is, to use more parameters than training samples. Quite surprisingly training the neural network via (stochastic) gradient descent leads to models that generalize very well, while classical statistics would suggest overfitting. In order to gain understanding of this implicit bias phenomenon we study the special case of sparse recovery (compressed sensing) which is of interest on its own. More precisely, in order to reconstruct a vector from underdetermined linear measurements, we introduce a corresponding overparameterized square loss functional, where the vector to be reconstructed is deeply factorized into several vectors. We show that, if there exists an exact solution, vanilla gradient flow for the overparameterized loss functional converges to a good approximation of the solution of minimal $\\ell_1$-norm. The latter is well-known to promote sparse solutions. As a by-product, our results significantly improve t",
    "link": "http://arxiv.org/abs/2112.11027",
    "context": "Title: More is Less: Inducing Sparsity via Overparameterization. (arXiv:2112.11027v5 [math.OC] UPDATED)\nAbstract: In deep learning it is common to overparameterize neural networks, that is, to use more parameters than training samples. Quite surprisingly training the neural network via (stochastic) gradient descent leads to models that generalize very well, while classical statistics would suggest overfitting. In order to gain understanding of this implicit bias phenomenon we study the special case of sparse recovery (compressed sensing) which is of interest on its own. More precisely, in order to reconstruct a vector from underdetermined linear measurements, we introduce a corresponding overparameterized square loss functional, where the vector to be reconstructed is deeply factorized into several vectors. We show that, if there exists an exact solution, vanilla gradient flow for the overparameterized loss functional converges to a good approximation of the solution of minimal $\\ell_1$-norm. The latter is well-known to promote sparse solutions. As a by-product, our results significantly improve t",
    "path": "papers/21/12/2112.11027.json",
    "total_tokens": 993,
    "translated_title": "通过过度参数来引导稀疏性: More is Less",
    "translated_abstract": "在深度学习中，常常会过度参数化神经网络，即使用比训练样本更多的参数。令人惊讶的是，通过（随机）梯度下降训练神经网络会导致良好的泛化模型，而传统的统计学则会认为会过拟合。为了理解这种隐式偏差现象，我们研究了稀疏恢复（压缩感知）的特殊情况，它本身就很有趣。更具体地，为了从欠定线性测量中重构向量，我们引入了相应的过度参数化平方损失函数，其中要重构的向量被深度分解成多个向量。我们表明，如果存在一个精确的解，则过度参数化的丢失函数的香草梯度流会收敛到最小$\\ell_1$范数的精确解的良好逼近。后者被广泛认为是能促进稀疏解的。作为副产品，我们的结果显著提高了稀疏模式之间的转移学习，并提出了使用神经网络进行稀疏恢复的新的竞争性方法。",
    "tldr": "通过过度参数化的神经网络模型，在稀疏恢复（压缩感知）中，采用过度参数的平方损失函数，推出了一个迭代算法解决问题，展示了神经网络对稀疏问题的良好适应性和推广能力，以及竞争性的解决方案。"
}