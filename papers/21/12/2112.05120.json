{
    "title": "On Convergence of Federated Averaging Langevin Dynamics. (arXiv:2112.05120v4 [stat.ML] UPDATED)",
    "abstract": "We propose a federated averaging Langevin algorithm (FA-LD) for uncertainty quantification and mean predictions with distributed clients. In particular, we generalize beyond normal posterior distributions and consider a general class of models. We develop theoretical guarantees for FA-LD for strongly log-concave distributions with non-i.i.d data and study how the injected noise and the stochastic-gradient noise, the heterogeneity of data, and the varying learning rates affect the convergence. Such an analysis sheds light on the optimal choice of local updates to minimize communication costs. Important to our approach is that the communication efficiency does not deteriorate with the injected noise in the Langevin algorithms. In addition, we examine in our FA-LD algorithm both independent and correlated noise used over different clients. We observe there is a trade-off between the pairs among communication, accuracy, and data privacy. As local devices may become inactive in federated ne",
    "link": "http://arxiv.org/abs/2112.05120",
    "context": "Title: On Convergence of Federated Averaging Langevin Dynamics. (arXiv:2112.05120v4 [stat.ML] UPDATED)\nAbstract: We propose a federated averaging Langevin algorithm (FA-LD) for uncertainty quantification and mean predictions with distributed clients. In particular, we generalize beyond normal posterior distributions and consider a general class of models. We develop theoretical guarantees for FA-LD for strongly log-concave distributions with non-i.i.d data and study how the injected noise and the stochastic-gradient noise, the heterogeneity of data, and the varying learning rates affect the convergence. Such an analysis sheds light on the optimal choice of local updates to minimize communication costs. Important to our approach is that the communication efficiency does not deteriorate with the injected noise in the Langevin algorithms. In addition, we examine in our FA-LD algorithm both independent and correlated noise used over different clients. We observe there is a trade-off between the pairs among communication, accuracy, and data privacy. As local devices may become inactive in federated ne",
    "path": "papers/21/12/2112.05120.json",
    "total_tokens": 1005,
    "translated_title": "关于联邦平均 Langevin 动力学的收敛性研究",
    "translated_abstract": "我们提出了一种用于分布式客户端的不确定性量化和均值预测的联邦平均 Langevin 算法（FA-LD）。我们特别考虑了一般模型的正常后验分布的推广。我们为 FA-LD 开发了理论保证，针对具有非独立同分布数据的强对数凹分布，研究了注入噪声、随机梯度噪声、数据异质性和变化的学习率对收敛性的影响。这样的分析揭示了在最小化通信开销方面选择本地更新的最佳方法。我们的方法的重要之处在于，Langevin 算法中注入噪声不会损害通信效率。此外，我们在 FA-LD 算法中研究了在不同客户端上使用独立和相关噪声的情况。我们观察到在通信、准确性和数据隐私之间存在着权衡。由于本地设备可能在联邦网络中变得不活跃，",
    "tldr": "我们提出了一种称为FA-LD的联邦平均Langevin算法，可以用于分布式客户端的不确定性量化和均值预测。算法考虑了非独立同分布数据的强对数凹分布，并研究了注入噪声、随机梯度噪声、数据异质性和变化的学习率等因素对收敛性的影响，为最小化通信开销提供了理论保证。",
    "en_tdlr": "We propose a federated averaging Langevin algorithm (FA-LD) for uncertainty quantification and mean predictions with distributed clients. The algorithm considers strongly log-concave distributions with non-i.i.d data and studies the impact of injected noise, stochastic-gradient noise, data heterogeneity, and varying learning rates on convergence. The analysis provides insights for minimizing communication costs and shows that communication efficiency is not affected by noise in Langevin algorithms."
}