{
    "title": "Parameter identifiability of a deep feedforward ReLU neural network. (arXiv:2112.12982v2 [math.ST] UPDATED)",
    "abstract": "The possibility for one to recover the parameters-weights and biases-of a neural network thanks to the knowledge of its function on a subset of the input space can be, depending on the situation, a curse or a blessing. On one hand, recovering the parameters allows for better adversarial attacks and could also disclose sensitive information from the dataset used to construct the network. On the other hand, if the parameters of a network can be recovered, it guarantees the user that the features in the latent spaces can be interpreted. It also provides foundations to obtain formal guarantees on the performances of the network. It is therefore important to characterize the networks whose parameters can be identified and those whose parameters cannot. In this article, we provide a set of conditions on a deep fully-connected feedforward ReLU neural network under which the parameters of the network are uniquely identified-modulo permutation and positive rescaling-from the function it impleme",
    "link": "http://arxiv.org/abs/2112.12982",
    "context": "Title: Parameter identifiability of a deep feedforward ReLU neural network. (arXiv:2112.12982v2 [math.ST] UPDATED)\nAbstract: The possibility for one to recover the parameters-weights and biases-of a neural network thanks to the knowledge of its function on a subset of the input space can be, depending on the situation, a curse or a blessing. On one hand, recovering the parameters allows for better adversarial attacks and could also disclose sensitive information from the dataset used to construct the network. On the other hand, if the parameters of a network can be recovered, it guarantees the user that the features in the latent spaces can be interpreted. It also provides foundations to obtain formal guarantees on the performances of the network. It is therefore important to characterize the networks whose parameters can be identified and those whose parameters cannot. In this article, we provide a set of conditions on a deep fully-connected feedforward ReLU neural network under which the parameters of the network are uniquely identified-modulo permutation and positive rescaling-from the function it impleme",
    "path": "papers/21/12/2112.12982.json",
    "total_tokens": 712,
    "translated_title": "深度前馈ReLU神经网络的参数可辨识性",
    "translated_abstract": "在一些情况下，基于神经网络在输入的一个子集上的函数值可以恢复神经网络的参数权重和偏置，这既可以是一个诅咒也可以是一个福音。本文研究了一类深度全连接前馈ReLU神经网络，提出了一组条件来刻画其参数可辨识性的问题，并证明了在这些条件下，可以唯一确定网络的参数-模除置换和正的缩放。",
    "tldr": "本文研究了一类深度前馈ReLU神经网络的参数可辨识性，给出了一组刻画条件并证明了在这些条件下，可以唯一确定网络的参数。",
    "en_tdlr": "This paper studies the parameter identifiability of a deep feedforward ReLU neural network. It provides a set of conditions to characterize the parameter identifiability and proves that under these conditions, the parameters can be uniquely determined - modulo permutation and positive rescaling."
}