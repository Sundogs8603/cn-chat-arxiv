{
    "title": "NN2Poly: A polynomial representation for deep feed-forward artificial neural networks. (arXiv:2112.11397v3 [stat.ML] UPDATED)",
    "abstract": "Interpretability of neural networks and their underlying theoretical behavior remain an open field of study even after the great success of their practical applications, particularly with the emergence of deep learning. In this work, NN2Poly is proposed: a theoretical approach to obtain an explicit polynomial model that provides an accurate representation of an already trained fully-connected feed-forward artificial neural network (a multilayer perceptron or MLP). This approach extends a previous idea proposed in the literature, which was limited to single hidden layer networks, to work with arbitrarily deep MLPs in both regression and classification tasks. The objective of this paper is to achieve this by using a Taylor expansion on the activation function, at each layer, and then using several combinatorial properties to calculate the coefficients of the desired polynomials. Discussion is presented on the main computational challenges of this method, and the way to overcome them by i",
    "link": "http://arxiv.org/abs/2112.11397",
    "context": "Title: NN2Poly: A polynomial representation for deep feed-forward artificial neural networks. (arXiv:2112.11397v3 [stat.ML] UPDATED)\nAbstract: Interpretability of neural networks and their underlying theoretical behavior remain an open field of study even after the great success of their practical applications, particularly with the emergence of deep learning. In this work, NN2Poly is proposed: a theoretical approach to obtain an explicit polynomial model that provides an accurate representation of an already trained fully-connected feed-forward artificial neural network (a multilayer perceptron or MLP). This approach extends a previous idea proposed in the literature, which was limited to single hidden layer networks, to work with arbitrarily deep MLPs in both regression and classification tasks. The objective of this paper is to achieve this by using a Taylor expansion on the activation function, at each layer, and then using several combinatorial properties to calculate the coefficients of the desired polynomials. Discussion is presented on the main computational challenges of this method, and the way to overcome them by i",
    "path": "papers/21/12/2112.11397.json",
    "total_tokens": 1004,
    "translated_title": "NN2Poly：用于深度前馈人工神经网络的多项式表示方法",
    "translated_abstract": "尽管深度学习应用非常成功，但神经网络的可解释性和理论行为仍然是一个开放的研究领域。本文提出NN2Poly：一种理论方法，用于获取一个显式多项式模型，以提供已经训练好的全连接前馈人工神经网络（多层感知器或MLP）的精确表示。这种方法扩展了文献中提出的先前想法，该想法仅限于单隐藏层的网络，并且适用于回归和分类任务的任意深度MLP。本文的目标是通过在每层上使用激活函数的泰勒展开式，然后使用几个组合性质来计算所需多项式的系数，从而实现此目标。作者讨论了此方法的主要计算挑战以及通过引入一些逼近来克服这些挑战的方法，而不会影响其准确性。通过实验验证表明，尽管NN2Poly方法简单且计算成本低，但对于合成和真实数据集，提供非常准确的多项式逼近。",
    "tldr": "本文提出了一种名为NN2Poly的方法，用于生成已经训练好的全连接前馈人工神经网络的精确多项式表示。该方法适用于任意深度的多层感知器（MLP）且计算成本相对较低，能够在回归和分类任务中提供非常准确的逼近结果。",
    "en_tdlr": "This paper proposes a method called NN2Poly, which generates an accurate polynomial representation of a fully-connected feed-forward artificial neural network (MLP) that has already been trained. The method is suitable for MLPs of any depth and offers a low computational cost while providing very precise results in both regression and classification tasks."
}