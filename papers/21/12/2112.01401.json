{
    "title": "Newton methods based convolution neural networks using parallel processing. (arXiv:2112.01401v3 [cs.LG] UPDATED)",
    "abstract": "Training of convolutional neural networks is a high dimensional and a non-convex optimization problem. At present, it is inefficient in situations where parametric learning rates can not be confidently set. Some past works have introduced Newton methods for training deep neural networks. Newton methods for convolutional neural networks involve complicated operations. Finding the Hessian matrix in second-order methods becomes very complex as we mainly use the finite differences method with the image data. Newton methods for convolutional neural networks deals with this by using the sub-sampled Hessian Newton methods. In this paper, we have used the complete data instead of the sub-sampled methods that only handle partial data at a time. Further, we have used parallel processing instead of serial processing in mini-batch computations. The results obtained using parallel processing in this study, outperform the time taken by the previous approach.",
    "link": "http://arxiv.org/abs/2112.01401",
    "context": "Title: Newton methods based convolution neural networks using parallel processing. (arXiv:2112.01401v3 [cs.LG] UPDATED)\nAbstract: Training of convolutional neural networks is a high dimensional and a non-convex optimization problem. At present, it is inefficient in situations where parametric learning rates can not be confidently set. Some past works have introduced Newton methods for training deep neural networks. Newton methods for convolutional neural networks involve complicated operations. Finding the Hessian matrix in second-order methods becomes very complex as we mainly use the finite differences method with the image data. Newton methods for convolutional neural networks deals with this by using the sub-sampled Hessian Newton methods. In this paper, we have used the complete data instead of the sub-sampled methods that only handle partial data at a time. Further, we have used parallel processing instead of serial processing in mini-batch computations. The results obtained using parallel processing in this study, outperform the time taken by the previous approach.",
    "path": "papers/21/12/2112.01401.json",
    "total_tokens": 777,
    "translated_title": "基于牛顿方法和并行处理的卷积神经网络",
    "translated_abstract": "卷积神经网络的训练是一个高维的非凸优化问题。目前，在无法自信地设置参数学习率的情况下，效率很低。过去的一些研究引入了牛顿方法来训练深度神经网络。对于卷积神经网络，牛顿方法涉及复杂的操作。通过使用子采样的Hessian牛顿方法，卷积神经网络的牛顿方法处理了这个问题。本研究中，我们使用完整的数据而不是一次只处理部分数据的子采样方法。此外，我们使用并行处理而不是串行处理来进行小批量计算。本研究中使用并行处理得到的结果优于之前所用方法所需的时间。",
    "tldr": "本研究提出基于牛顿方法和并行处理的卷积神经网络训练方法，使用完整的数据集来处理Hessian矩阵，并比之前的方法更高效。"
}