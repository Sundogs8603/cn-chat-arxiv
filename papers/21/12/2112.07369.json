{
    "title": "Convergence proof for stochastic gradient descent in the training of deep neural networks with ReLU activation for constant target functions. (arXiv:2112.07369v2 [cs.LG] UPDATED)",
    "abstract": "In many numerical simulations stochastic gradient descent (SGD) type optimization methods perform very effectively in the training of deep neural networks (DNNs) but till this day it remains an open problem of research to provide a mathematical convergence analysis which rigorously explains the success of SGD type optimization methods in the training of DNNs. In this work we study SGD type optimization methods in the training of fully-connected feedforward DNNs with rectified linear unit (ReLU) activation. We first establish general regularity properties for the risk functions and their generalized gradient functions appearing in the training of such DNNs and, thereafter, we investigate the plain vanilla SGD optimization method in the training of such DNNs under the assumption that the target function under consideration is a constant function. Specifically, we prove under the assumption that the learning rates (the step sizes of the SGD optimization method) are sufficiently small but ",
    "link": "http://arxiv.org/abs/2112.07369",
    "context": "Title: Convergence proof for stochastic gradient descent in the training of deep neural networks with ReLU activation for constant target functions. (arXiv:2112.07369v2 [cs.LG] UPDATED)\nAbstract: In many numerical simulations stochastic gradient descent (SGD) type optimization methods perform very effectively in the training of deep neural networks (DNNs) but till this day it remains an open problem of research to provide a mathematical convergence analysis which rigorously explains the success of SGD type optimization methods in the training of DNNs. In this work we study SGD type optimization methods in the training of fully-connected feedforward DNNs with rectified linear unit (ReLU) activation. We first establish general regularity properties for the risk functions and their generalized gradient functions appearing in the training of such DNNs and, thereafter, we investigate the plain vanilla SGD optimization method in the training of such DNNs under the assumption that the target function under consideration is a constant function. Specifically, we prove under the assumption that the learning rates (the step sizes of the SGD optimization method) are sufficiently small but ",
    "path": "papers/21/12/2112.07369.json",
    "total_tokens": 972,
    "tldr": "本论文证明了在使用ReLU激活函数的深度神经网络中，当目标函数为常数函数时，学习率足够小但足够大以确保稳定性的条件下，SGD优化方法可以收敛到全局极小值。",
    "en_tdlr": "This paper proves that in the training of deep neural networks with ReLU activation function, when the target function is a constant function, under the condition that the learning rate is sufficiently small but large enough to ensure stability, the plain SGD optimization method can converge to the global minimum."
}