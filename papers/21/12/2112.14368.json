{
    "title": "Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization. (arXiv:2112.14368v2 [cs.LG] UPDATED)",
    "abstract": "We investigate online convex optimization in non-stationary environments and choose the dynamic regret as the performance measure, defined as the difference between cumulative loss incurred by the online algorithm and that of any feasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the path length that essentially reflects the non-stationarity of environments, the state-of-the-art dynamic regret is $\\mathcal{O}(\\sqrt{T(1+P_T)})$. Although this bound is proved to be minimax optimal for convex functions, in this paper, we demonstrate that it is possible to further enhance the guarantee for some easy problem instances, particularly when online functions are smooth. Specifically, we introduce novel online algorithms that can exploit smoothness and replace the dependence on $T$ in dynamic regret with problem-dependent quantities: the variation in gradients of loss functions, the cumulative loss of the comparator sequence, and the minimum of these two terms. These quantitie",
    "link": "http://arxiv.org/abs/2112.14368",
    "context": "Title: Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization. (arXiv:2112.14368v2 [cs.LG] UPDATED)\nAbstract: We investigate online convex optimization in non-stationary environments and choose the dynamic regret as the performance measure, defined as the difference between cumulative loss incurred by the online algorithm and that of any feasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the path length that essentially reflects the non-stationarity of environments, the state-of-the-art dynamic regret is $\\mathcal{O}(\\sqrt{T(1+P_T)})$. Although this bound is proved to be minimax optimal for convex functions, in this paper, we demonstrate that it is possible to further enhance the guarantee for some easy problem instances, particularly when online functions are smooth. Specifically, we introduce novel online algorithms that can exploit smoothness and replace the dependence on $T$ in dynamic regret with problem-dependent quantities: the variation in gradients of loss functions, the cumulative loss of the comparator sequence, and the minimum of these two terms. These quantitie",
    "path": "papers/21/12/2112.14368.json",
    "total_tokens": 1010,
    "translated_title": "非静态适应性：面向在线凸优化的问题相关的动态遗憾",
    "translated_abstract": "本文研究了在非静态环境中的在线凸优化，并选择动态遗憾作为性能度量，定义为在线算法和任何可行比较器序列所累计的损失之间的差值。假设$T$是时间长度，$P_T$是实质上反映环境非静态性的路径长度，则最先进的动态遗憾是$\\mathcal{O}(\\sqrt{T(1+P_T)})$。尽管这个界限被证明对于凸函数是最小化的，但我们在本文中展示，在一些简单的问题实例中，特别是当在线函数是光滑的时候，可以进一步增强保证。具体地，我们介绍了新颖的在线算法，可以利用光滑性，并用损失函数的梯度变化、比较器序列的累计损失和这两个项的最小值代替动态遗憾中对$T$的依赖。这些量被证明具有几何直观性，并且与环境中的非静态现象密切相关。对合成和真实数据集的广泛实验表明，我们的算法优于最先进的基线算法。",
    "tldr": "本研究提出一种面向在线凸优化的动态遗憾算法，可以在一些简单的问题实例中进一步增强保证，具有几何直观性，实验表明其优于最先进的基线算法。",
    "en_tdlr": "This study proposes a dynamic regret algorithm for online convex optimization, which can further enhance the guarantee for some easy problem instances, has geometric intuition, and outperforms state-of-the-art baselines according to extensive experiments on synthetic and real datasets."
}