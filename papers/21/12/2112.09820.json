{
    "title": "GPEX, A Framework For Interpreting Artificial Neural Networks. (arXiv:2112.09820v2 [cs.LG] UPDATED)",
    "abstract": "The analogy between Gaussian processes (GPs) and deep artificial neural networks (ANNs) has received a lot of interest, and has shown promise to unbox the blackbox of deep ANNs. Existing theoretical works put strict assumptions on the ANN (e.g. requiring all intermediate layers to be wide, or using specific activation functions). Accommodating those theoretical assumptions is hard in recent deep architectures, and those theoretical conditions need refinement as new deep architectures emerge. In this paper we derive an evidence lower-bound that encourages the GP's posterior to match the ANN's output without any requirement on the ANN. Using our method we find out that on 5 datasets, only a subset of those theoretical assumptions are sufficient. Indeed, in our experiments we used a normal ResNet-18 or feed-forward backbone with a single wide layer in the end. One limitation of training GPs is the lack of scalability with respect to the number of inducing points. We use novel computationa",
    "link": "http://arxiv.org/abs/2112.09820",
    "context": "Title: GPEX, A Framework For Interpreting Artificial Neural Networks. (arXiv:2112.09820v2 [cs.LG] UPDATED)\nAbstract: The analogy between Gaussian processes (GPs) and deep artificial neural networks (ANNs) has received a lot of interest, and has shown promise to unbox the blackbox of deep ANNs. Existing theoretical works put strict assumptions on the ANN (e.g. requiring all intermediate layers to be wide, or using specific activation functions). Accommodating those theoretical assumptions is hard in recent deep architectures, and those theoretical conditions need refinement as new deep architectures emerge. In this paper we derive an evidence lower-bound that encourages the GP's posterior to match the ANN's output without any requirement on the ANN. Using our method we find out that on 5 datasets, only a subset of those theoretical assumptions are sufficient. Indeed, in our experiments we used a normal ResNet-18 or feed-forward backbone with a single wide layer in the end. One limitation of training GPs is the lack of scalability with respect to the number of inducing points. We use novel computationa",
    "path": "papers/21/12/2112.09820.json",
    "total_tokens": 948,
    "translated_title": "GPEX，用于解释人工神经网络的框架",
    "translated_abstract": "高斯过程（GPs）与深度人工神经网络（ANNs）之间的类比引起了广泛关注，并显示出揭示深度ANN的黑箱的潜力。现有的理论工作对ANN提出了严格的假设（例如，要求所有中间层为宽层，或使用特定的激活函数）。适应这些理论假设在最近的深层架构中很困难，并且随着新的深层架构的出现，这些理论条件需要进一步完善。在本文中，我们推导出一个证据下界，鼓励GP的后验与ANN的输出匹配，而不对ANN做任何要求。使用我们的方法，我们发现在5个数据集上，只有一部分理论假设就足够了。实际上，在我们的实验中，我们使用了一个普通的ResNet-18或前馈骨干网络，并在末端使用了一个宽层。训练GPs的一个局限性是在于与诱导点数量的可扩展性的缺乏。",
    "tldr": "这篇论文提出了一种GPEX框架，用于解释深度人工神经网络，通过推导出一个证据下界来匹配神经网络的输出，而不对神经网络做出任何特定要求。实验证明，在一些理论假设下，只需要简单的网络结构即可达到良好性能。",
    "en_tdlr": "This paper presents a GPEX framework for interpreting deep artificial neural networks by deriving an evidence lower-bound to match the network's output without requiring any specific assumptions on the neural network. Experimental results show that with certain theoretical assumptions, a simple network structure can achieve good performance."
}