{
    "title": "PLACE dropout: A Progressive Layer-wise and Channel-wise Dropout for Domain Generalization. (arXiv:2112.03676v2 [cs.LG] UPDATED)",
    "abstract": "Domain generalization (DG) aims to learn a generic model from multiple observed source domains that generalizes well to arbitrary unseen target domains without further training. The major challenge in DG is that the model inevitably faces a severe overfitting issue due to the domain gap between source and target domains. To mitigate this problem, some dropout-based methods have been proposed to resist overfitting by discarding part of the representation of the intermediate layers. However, we observe that most of these methods only conduct the dropout operation in some specific layers, leading to an insufficient regularization effect on the model. We argue that applying dropout at multiple layers can produce stronger regularization effects, which could alleviate the overfitting problem on source domains more adequately than previous layer-specific dropout methods. In this paper, we develop a novel layer-wise and channel-wise dropout for DG, which randomly selects one layer and then ran",
    "link": "http://arxiv.org/abs/2112.03676",
    "context": "Title: PLACE dropout: A Progressive Layer-wise and Channel-wise Dropout for Domain Generalization. (arXiv:2112.03676v2 [cs.LG] UPDATED)\nAbstract: Domain generalization (DG) aims to learn a generic model from multiple observed source domains that generalizes well to arbitrary unseen target domains without further training. The major challenge in DG is that the model inevitably faces a severe overfitting issue due to the domain gap between source and target domains. To mitigate this problem, some dropout-based methods have been proposed to resist overfitting by discarding part of the representation of the intermediate layers. However, we observe that most of these methods only conduct the dropout operation in some specific layers, leading to an insufficient regularization effect on the model. We argue that applying dropout at multiple layers can produce stronger regularization effects, which could alleviate the overfitting problem on source domains more adequately than previous layer-specific dropout methods. In this paper, we develop a novel layer-wise and channel-wise dropout for DG, which randomly selects one layer and then ran",
    "path": "papers/21/12/2112.03676.json",
    "total_tokens": 927,
    "translated_title": "PLACE退出：一种逐层和逐通道的渐进式退出方法用于领域泛化",
    "translated_abstract": "领域泛化（DG）旨在从多个观测到的源领域中学习出一个泛化能力良好的通用模型，以适应任意未观测到的目标领域，而无需进一步训练。DG的主要挑战在于，由于源领域和目标领域之间的领域差异，模型必然面临严重的过拟合问题。为了缓解这个问题，一些基于退出的方法已经被提出，通过丢弃中间层的部分表示来抵抗过拟合。然而，我们观察到，大多数这些方法只在一些特定的层级上执行退出操作，导致对模型的正则化效果不足。我们认为，在多个层级上应用退出可以产生更强的正则化效果，相比以前的层级特定退出方法，这可以更充分地缓解源领域上的过拟合问题。本文中，我们开发了一种新的逐层和逐通道退出方法用于DG，该方法随机选择一层，然后执行退出操作。",
    "tldr": "本文提出了一种逐层和逐通道的渐进式退出方法用于领域泛化，该方法比以前的层级特定退出方法具有更强的正则化效果，能够更充分地缓解源领域上的过拟合问题。",
    "en_tdlr": "This paper proposes a progressive layer-wise and channel-wise dropout method for domain generalization, which has stronger regularization effects compared to previous layer-specific dropout methods, effectively alleviating the overfitting problem on source domains."
}