{
    "title": "RamBoAttack: A Robust Query Efficient Deep Neural Network Decision Exploit. (arXiv:2112.05282v3 [cs.LG] UPDATED)",
    "abstract": "Machine learning models are critically susceptible to evasion attacks from adversarial examples. Generally, adversarial examples, modified inputs deceptively similar to the original input, are constructed under whitebox settings by adversaries with full access to the model. However, recent attacks have shown a remarkable reduction in query numbers to craft adversarial examples using blackbox attacks. Particularly, alarming is the ability to exploit the classification decision from the access interface of a trained model provided by a growing number of Machine Learning as a Service providers including Google, Microsoft, IBM and used by a plethora of applications incorporating these models. The ability of an adversary to exploit only the predicted label from a model to craft adversarial examples is distinguished as a decision-based attack. In our study, we first deep dive into recent state-of-the-art decision-based attacks in ICLR and SP to highlight the costly nature of discovering low ",
    "link": "http://arxiv.org/abs/2112.05282",
    "context": "Title: RamBoAttack: A Robust Query Efficient Deep Neural Network Decision Exploit. (arXiv:2112.05282v3 [cs.LG] UPDATED)\nAbstract: Machine learning models are critically susceptible to evasion attacks from adversarial examples. Generally, adversarial examples, modified inputs deceptively similar to the original input, are constructed under whitebox settings by adversaries with full access to the model. However, recent attacks have shown a remarkable reduction in query numbers to craft adversarial examples using blackbox attacks. Particularly, alarming is the ability to exploit the classification decision from the access interface of a trained model provided by a growing number of Machine Learning as a Service providers including Google, Microsoft, IBM and used by a plethora of applications incorporating these models. The ability of an adversary to exploit only the predicted label from a model to craft adversarial examples is distinguished as a decision-based attack. In our study, we first deep dive into recent state-of-the-art decision-based attacks in ICLR and SP to highlight the costly nature of discovering low ",
    "path": "papers/21/12/2112.05282.json",
    "total_tokens": 575,
    "tldr": "本论文提出了一种名为RamBoAttack的决策式攻击方法，可以高效地对机器学习模型进行攻击，尤其是那些使用决策边界划分的模型，该攻击方法提高了攻击的成功率和速度，使黑盒攻击更为可行。"
}