{
    "title": "Gradient and Projection Free Distributed Online Min-Max Resource Optimization. (arXiv:2112.03896v3 [cs.IT] UPDATED)",
    "abstract": "We consider distributed online min-max resource allocation with a set of parallel agents and a parameter server. Our goal is to minimize the pointwise maximum over a set of time-varying and decreasing cost functions, without a priori information about these functions. We propose a novel online algorithm, termed Distributed Online resource Re-Allocation (DORA), where non-stragglers learn to relinquish resource and share resource with stragglers. A notable feature of DORA is that it does not require gradient calculation or projection operation, unlike most existing online optimization strategies. This allows it to substantially reduce the computation overhead in large-scale and distributed networks. We analyze the worst-case performance of DORA and derive an upper bound on its dynamic regret for non-convex functions. We further consider an application to the bandwidth allocation problem in distributed online machine learning. Our numerical study demonstrates the efficacy of the proposed ",
    "link": "http://arxiv.org/abs/2112.03896",
    "context": "Title: Gradient and Projection Free Distributed Online Min-Max Resource Optimization. (arXiv:2112.03896v3 [cs.IT] UPDATED)\nAbstract: We consider distributed online min-max resource allocation with a set of parallel agents and a parameter server. Our goal is to minimize the pointwise maximum over a set of time-varying and decreasing cost functions, without a priori information about these functions. We propose a novel online algorithm, termed Distributed Online resource Re-Allocation (DORA), where non-stragglers learn to relinquish resource and share resource with stragglers. A notable feature of DORA is that it does not require gradient calculation or projection operation, unlike most existing online optimization strategies. This allows it to substantially reduce the computation overhead in large-scale and distributed networks. We analyze the worst-case performance of DORA and derive an upper bound on its dynamic regret for non-convex functions. We further consider an application to the bandwidth allocation problem in distributed online machine learning. Our numerical study demonstrates the efficacy of the proposed ",
    "path": "papers/21/12/2112.03896.json",
    "total_tokens": 908,
    "translated_title": "梯度和投影自由的分布式在线最小-最大资源优化",
    "translated_abstract": "本文考虑了具有一组并行代理和参数服务器的分布式在线最小-最大资源分配问题。我们的目标是最小化一组随时间变化且递减的成本函数的逐点最大值，而不需要关于这些函数的先验信息。我们提出了一种新型的在线算法，称为分布式在线资源重分配（DORA），其中非阻塞学习者学会放弃资源并与阻塞者共享资源。DORA的一个显著特点是它不需要梯度计算或投影操作，这与大多数现有的在线优化策略不同。这使得它能够在大规模和分布式网络中大大减少计算开销。我们分析了DORA的最坏情况性能，并推导出非凸函数的动态遗憾的上界。我们进一步考虑了在分布式在线机器学习中的带宽分配问题的应用。我们的数值研究证明了所提出方法的有效性。",
    "tldr": "在分布式在线最小-最大资源分配问题中，我们提出了一种梯度和投影自由的在线算法DORA，通过让非阻塞学习者学会放弃资源并与阻塞者共享资源，实现了大规模和分布式网络中的计算开销的显著减少。",
    "en_tdlr": "In the distributed online min-max resource allocation problem, we propose a gradient and projection free online algorithm, DORA, which achieves a significant reduction in computation overhead in large-scale and distributed networks by letting non-stragglers learn to relinquish resource and share resource with stragglers."
}