{
    "title": "Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning. (arXiv:2112.08369v3 [cs.LG] UPDATED)",
    "abstract": "Many important tasks are defined in terms of object. To generalize across these tasks, a reinforcement learning (RL) agent needs to exploit the structure that the objects induce. Prior work has either hard-coded object-centric features, used complex object-centric generative models, or updated state using local spatial features. However, these approaches have had limited success in enabling general RL agents. Motivated by this, we introduce \"Feature-Attending Recurrent Modules\" (FARM), an architecture for learning state representations that relies on simple, broadly applicable inductive biases for capturing spatial and temporal regularities. FARM learns a state representation that is distributed across multiple modules that each attend to spatiotemporal features with an expressive feature attention mechanism. We show that this improves an RL agent's ability to generalize across object-centric tasks. We study task suites in both 2D and 3D environments and find that FARM better generaliz",
    "link": "http://arxiv.org/abs/2112.08369",
    "context": "Title: Feature-Attending Recurrent Modules for Generalization in Reinforcement Learning. (arXiv:2112.08369v3 [cs.LG] UPDATED)\nAbstract: Many important tasks are defined in terms of object. To generalize across these tasks, a reinforcement learning (RL) agent needs to exploit the structure that the objects induce. Prior work has either hard-coded object-centric features, used complex object-centric generative models, or updated state using local spatial features. However, these approaches have had limited success in enabling general RL agents. Motivated by this, we introduce \"Feature-Attending Recurrent Modules\" (FARM), an architecture for learning state representations that relies on simple, broadly applicable inductive biases for capturing spatial and temporal regularities. FARM learns a state representation that is distributed across multiple modules that each attend to spatiotemporal features with an expressive feature attention mechanism. We show that this improves an RL agent's ability to generalize across object-centric tasks. We study task suites in both 2D and 3D environments and find that FARM better generaliz",
    "path": "papers/21/12/2112.08369.json",
    "total_tokens": 906,
    "translated_title": "特征注意的递归模块在强化学习中的泛化能力",
    "translated_abstract": "许多重要的任务都是以物体为基础定义的。为了在这些任务上实现泛化，强化学习（RL）代理需要利用物体所引发的结构。先前的工作要么硬编码对象中心的特征，要么使用复杂的对象中心生成模型，要么使用局部空间特征更新状态。然而，这些方法在实现泛化RL代理方面的成功有限。受此启发，我们引入了\"特征注意的递归模块\"（FARM），这是一种学习状态表示的体系结构，依赖于简单、广泛适用的归纳偏置来捕捉空间和时间规律性。FARM学习了一种分布于多个模块之间的状态表示，每个模块都使用具有表现力的特征注意机制关注时空特征。我们展示了这种方法改善了RL代理在对象中心任务上的泛化能力。我们在2D和3D环境中研究了任务套件，并发现FARM在泛化能力方面表现更好。",
    "tldr": "\"Feature-Attending Recurrent Modules\" (FARM)是一种学习状态表示的体系结构，通过特征注意机制来捕捉空间和时间规律性，从而改善强化学习代理的泛化能力。",
    "en_tdlr": "\"Feature-Attending Recurrent Modules\" (FARM) is an architecture for learning state representations that improves the generalization ability of reinforcement learning agents by capturing spatial and temporal regularities using a feature attention mechanism."
}