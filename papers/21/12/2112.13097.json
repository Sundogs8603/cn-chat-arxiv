{
    "title": "Faster Rates for Compressed Federated Learning with Client-Variance Reduction. (arXiv:2112.13097v3 [cs.LG] UPDATED)",
    "abstract": "Due to the communication bottleneck in distributed and federated learning applications, algorithms using communication compression have attracted significant attention and are widely used in practice. Moreover, the huge number, high heterogeneity and limited availability of clients result in high client-variance. This paper addresses these two issues together by proposing compressed and client-variance reduced methods COFIG and FRECON. We prove an $O(\\frac{(1+\\omega)^{3/2}\\sqrt{N}}{S\\epsilon^2}+\\frac{(1+\\omega)N^{2/3}}{S\\epsilon^2})$ bound on the number of communication rounds of COFIG in the nonconvex setting, where $N$ is the total number of clients, $S$ is the number of clients participating in each round, $\\epsilon$ is the convergence error, and $\\omega$ is the variance parameter associated with the compression operator. In case of FRECON, we prove an $O(\\frac{(1+\\omega)\\sqrt{N}}{S\\epsilon^2})$ bound on the number of communication rounds. In the convex setting, COFIG converges with",
    "link": "http://arxiv.org/abs/2112.13097",
    "context": "Title: Faster Rates for Compressed Federated Learning with Client-Variance Reduction. (arXiv:2112.13097v3 [cs.LG] UPDATED)\nAbstract: Due to the communication bottleneck in distributed and federated learning applications, algorithms using communication compression have attracted significant attention and are widely used in practice. Moreover, the huge number, high heterogeneity and limited availability of clients result in high client-variance. This paper addresses these two issues together by proposing compressed and client-variance reduced methods COFIG and FRECON. We prove an $O(\\frac{(1+\\omega)^{3/2}\\sqrt{N}}{S\\epsilon^2}+\\frac{(1+\\omega)N^{2/3}}{S\\epsilon^2})$ bound on the number of communication rounds of COFIG in the nonconvex setting, where $N$ is the total number of clients, $S$ is the number of clients participating in each round, $\\epsilon$ is the convergence error, and $\\omega$ is the variance parameter associated with the compression operator. In case of FRECON, we prove an $O(\\frac{(1+\\omega)\\sqrt{N}}{S\\epsilon^2})$ bound on the number of communication rounds. In the convex setting, COFIG converges with",
    "path": "papers/21/12/2112.13097.json",
    "total_tokens": 1079,
    "translated_title": "压缩联邦学习中更快的速率与客户端方差减少",
    "translated_abstract": "由于分布式和联邦学习应用中的通信瓶颈，使用通信压缩的算法引起了广泛关注并被广泛应用。此外，庞大数量、高异质性和有限可用性的客户端导致了高客户端方差。本文通过提出压缩和客户端方差减少方法COFIG和FRECON来解决这两个问题。在非凸设置下，我们证明了COFIG的通信轮数上限为$O(\\frac{{(1+\\omega)^{3/2}\\sqrt{N}}}{{S\\epsilon^2}}+\\frac{{(1+\\omega)N^{2/3}}}{{S\\epsilon^2}})$，其中$N$是总客户端数，$S$是每轮参与的客户端数，$\\epsilon$是收敛误差，$\\omega$是与压缩运算符相关的方差参数。对于FRECON，我们证明了通信轮数的上限为$O(\\frac{{(1+\\omega)\\sqrt{N}}}{{S\\epsilon^2}})$。在凸设置下，COFIG收敛。",
    "tldr": "本文提出了压缩和客户端方差减少方法COFIG和FRECON，以解决分布式和联邦学习应用中的通信瓶颈和客户端方差问题。在非凸设置下，COFIG具有$O(\\frac{{(1+\\omega)^{3/2}\\sqrt{N}}}{{S\\epsilon^2}}+\\frac{{(1+\\omega)N^{2/3}}}{{S\\epsilon^2}})$的通信轮数上限，在凸设置下，COFIG收敛。",
    "en_tdlr": "This paper proposes compressed and client-variance reduced methods COFIG and FRECON to address the communication bottleneck and client-variance issues in distributed and federated learning applications. In the nonconvex setting, COFIG has an $O(\\frac{{(1+\\omega)^{3/2}\\sqrt{N}}}{{S\\epsilon^2}}+\\frac{{(1+\\omega)N^{2/3}}}{{S\\epsilon^2}})$ bound on the number of communication rounds, and in the convex setting, COFIG converges."
}