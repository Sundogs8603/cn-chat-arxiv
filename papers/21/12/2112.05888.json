{
    "title": "A Sparse Expansion For Deep Gaussian Processes. (arXiv:2112.05888v3 [stat.ML] UPDATED)",
    "abstract": "In this work, we use Deep Gaussian Processes (DGPs) as statistical surrogates for stochastic processes with complex distributions. Conventional inferential methods for DGP models can suffer from high computational complexity as they require large-scale operations with kernel matrices for training and inference. In this work, we propose an efficient scheme for accurate inference and efficient training based on a range of Gaussian Processes, called the Tensor Markov Gaussian Processes (TMGP). We construct an induced approximation of TMGP referred to as the hierarchical expansion. Next, we develop a deep TMGP (DTMGP) model as the composition of multiple hierarchical expansion of TMGPs. The proposed DTMGP model has the following properties: (1) the outputs of each activation function are deterministic while the weights are chosen independently from standard Gaussian distribution; (2) in training or prediction, only polylog(M) (out of M) activation functions have non-zero outputs, which sig",
    "link": "http://arxiv.org/abs/2112.05888",
    "context": "Title: A Sparse Expansion For Deep Gaussian Processes. (arXiv:2112.05888v3 [stat.ML] UPDATED)\nAbstract: In this work, we use Deep Gaussian Processes (DGPs) as statistical surrogates for stochastic processes with complex distributions. Conventional inferential methods for DGP models can suffer from high computational complexity as they require large-scale operations with kernel matrices for training and inference. In this work, we propose an efficient scheme for accurate inference and efficient training based on a range of Gaussian Processes, called the Tensor Markov Gaussian Processes (TMGP). We construct an induced approximation of TMGP referred to as the hierarchical expansion. Next, we develop a deep TMGP (DTMGP) model as the composition of multiple hierarchical expansion of TMGPs. The proposed DTMGP model has the following properties: (1) the outputs of each activation function are deterministic while the weights are chosen independently from standard Gaussian distribution; (2) in training or prediction, only polylog(M) (out of M) activation functions have non-zero outputs, which sig",
    "path": "papers/21/12/2112.05888.json",
    "total_tokens": 861,
    "translated_title": "一种用于深度高斯过程的稀疏展开方法",
    "translated_abstract": "本文针对具有复杂分布的随机过程，采用深度高斯过程（DGP）作为统计替代品。传统的DGP模型推断方法的计算复杂度很高，因为需要使用核矩阵进行大规模的训练和推断。本文提出了一种基于一系列高斯过程（TMGP）的准确推断和高效训练方案。我们构建了一个名为分层展开的TMGP诱导近似。接着，我们将多个TMGP的分层展开组合成一种称为深度TMGP（DTMGP）的模型。该模型具有以下特性：（1）每个激活函数的输出都是确定性的，而权重是从标准高斯分布中独立选择的；（2）在训练或预测中，只有polylog（M）（M个中的一部分）个激活函数具有非零输出，这使得模型变得更加稀疏。",
    "tldr": "本文提出了一种基于高斯过程的稀疏展开方法，用于构建深度高斯过程模型。该方法可以提高计算效率，使得模型更加稀疏。",
    "en_tdlr": "We propose a sparse expansion method based on Gaussian processes for constructing deep Gaussian process models. The method improves computational efficiency and sparsity by constructing a hierarchical expansion of tensor markov Gaussian processes."
}