{
    "title": "Transferability Properties of Graph Neural Networks. (arXiv:2112.04629v4 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks (GNNs) are composed of layers consisting of graph convolutions and pointwise nonlinearities. Due to their invariance and stability properties, GNNs are provably successful at learning representations from data supported on moderate-scale graphs. However, they are difficult to learn on large-scale graphs. In this paper, we study the problem of training GNNs on graphs of moderate size and transferring them to large-scale graphs. We use graph limits called graphons to define limit objects for graph filters and GNNs -- graphon filters and graphon neural networks (WNNs) -- which we interpret as generative models for graph filters and GNNs. We then show that graphon filters and WNNs can be approximated by graph filters and GNNs sampled from them on weighted and stochastic graphs. Because the error of these approximations can be upper bounded, by a triangle inequality argument we can further bound the error of transferring a graph filter or a GNN across graphs. Our resul",
    "link": "http://arxiv.org/abs/2112.04629",
    "context": "Title: Transferability Properties of Graph Neural Networks. (arXiv:2112.04629v4 [cs.LG] UPDATED)\nAbstract: Graph neural networks (GNNs) are composed of layers consisting of graph convolutions and pointwise nonlinearities. Due to their invariance and stability properties, GNNs are provably successful at learning representations from data supported on moderate-scale graphs. However, they are difficult to learn on large-scale graphs. In this paper, we study the problem of training GNNs on graphs of moderate size and transferring them to large-scale graphs. We use graph limits called graphons to define limit objects for graph filters and GNNs -- graphon filters and graphon neural networks (WNNs) -- which we interpret as generative models for graph filters and GNNs. We then show that graphon filters and WNNs can be approximated by graph filters and GNNs sampled from them on weighted and stochastic graphs. Because the error of these approximations can be upper bounded, by a triangle inequality argument we can further bound the error of transferring a graph filter or a GNN across graphs. Our resul",
    "path": "papers/21/12/2112.04629.json",
    "total_tokens": 932,
    "translated_title": "图神经网络的可传递性特性",
    "translated_abstract": "图神经网络（GNNs）由图卷积和逐点非线性层组成。由于它们的不变性和稳定性特性，GNNs在学习基于中等规模图的数据表示方面被证明是成功的。然而，它们在大规模图上的学习很困难。本文研究了在中等规模图上训练GNNs并将它们迁移到大规模图上的问题。我们使用称为图核密度的图限制来定义图滤波器和GNNs的极限对象 - 图核密度滤波器和图核密度神经网络（WNNs），我们将它们解释为图滤波器和GNNs的生成模型。然后，我们展示了图核密度滤波器和WNNs可以通过从它们上进行采样的图滤波器和GNNs在加权和随机图上进行逼近。由于这些逼近的误差可以得到上界，通过三角不等式的论证我们可以进一步界定在不同图之间传递图滤波器或GNN的误差。",
    "tldr": "本文研究了将图神经网络（GNNs）从中等规模图迁移到大规模图的问题，并通过引入图核密度滤波器和图核密度神经网络（WNNs）的概念，提出了逼近方法以限定迁移误差。",
    "en_tdlr": "This paper investigates the problem of transferring Graph Neural Networks (GNNs) from moderate-scale graphs to large-scale graphs, and introduces the concept of graphon filters and graphon neural networks (WNNs) to approximate the transfer error."
}