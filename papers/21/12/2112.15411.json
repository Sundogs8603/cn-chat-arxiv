{
    "title": "Disjoint Contrastive Regression Learning for Multi-Sourced Annotations",
    "abstract": "arXiv:2112.15411v2 Announce Type: replace  Abstract: Large-scale datasets are important for the development of deep learning models. Such datasets usually require a heavy workload of annotations, which are extremely time-consuming and expensive. To accelerate the annotation procedure, multiple annotators may be employed to label different subsets of the data. However, the inconsistency and bias among different annotators are harmful to the model training, especially for qualitative and subjective tasks.To address this challenge, in this paper, we propose a novel contrastive regression framework to address the disjoint annotations problem, where each sample is labeled by only one annotator and multiple annotators work on disjoint subsets of the data. To take account of both the intra-annotator consistency and inter-annotator inconsistency, two strategies are employed.Firstly, a contrastive-based loss is applied to learn the relative ranking among different samples of the same annotator,",
    "link": "https://arxiv.org/abs/2112.15411",
    "context": "Title: Disjoint Contrastive Regression Learning for Multi-Sourced Annotations\nAbstract: arXiv:2112.15411v2 Announce Type: replace  Abstract: Large-scale datasets are important for the development of deep learning models. Such datasets usually require a heavy workload of annotations, which are extremely time-consuming and expensive. To accelerate the annotation procedure, multiple annotators may be employed to label different subsets of the data. However, the inconsistency and bias among different annotators are harmful to the model training, especially for qualitative and subjective tasks.To address this challenge, in this paper, we propose a novel contrastive regression framework to address the disjoint annotations problem, where each sample is labeled by only one annotator and multiple annotators work on disjoint subsets of the data. To take account of both the intra-annotator consistency and inter-annotator inconsistency, two strategies are employed.Firstly, a contrastive-based loss is applied to learn the relative ranking among different samples of the same annotator,",
    "path": "papers/21/12/2112.15411.json",
    "total_tokens": 782,
    "translated_title": "多源注解的不相交对比回归学习",
    "translated_abstract": "大规模数据集对深度学习模型的发展至关重要。这些数据集通常需要大量注解工作，这是非常耗时且昂贵的。为了加速注解过程，可以雇用多个标注者为数据的不同子集进行标记。然而，不同标注者之间的不一致性和偏见对模型训练有害，特别是对于定性和主观任务。为了解决这一挑战，在本文中，我们提出了一种新颖的对比回归框架来解决不相交注解问题，其中每个样本仅由一个标注者标记，多个标注者在数据的不相交子集上工作。为了考虑标注者内一致性和标注者间不一致性，我们采用了两种策略。首先，应用基于对比的损失来学习同一标注者的不同样本之间的相对排序。",
    "tldr": "提出了用于不相交注解的对比回归框架，解决了多个标注者间的不一致性和偏见问题。",
    "en_tdlr": "Introduced a contrastive regression framework for disjoint annotations to address the inconsistency and bias among multiple annotators."
}