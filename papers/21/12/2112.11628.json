{
    "title": "SkipNode: On Alleviating Performance Degradation for Deep Graph Convolutional Networks. (arXiv:2112.11628v4 [cs.LG] UPDATED)",
    "abstract": "Graph Convolutional Networks (GCNs) suffer from performance degradation when models go deeper. However, earlier works only attributed the performance degeneration to over-smoothing. In this paper, we conduct theoretical and experimental analysis to explore the fundamental causes of performance degradation in deep GCNs: over-smoothing and gradient vanishing have a mutually reinforcing effect that causes the performance to deteriorate more quickly in deep GCNs. On the other hand, existing anti-over-smoothing methods all perform full convolutions up to the model depth. They could not well resist the exponential convergence of over-smoothing due to model depth increasing. In this work, we propose a simple yet effective plug-and-play module, Skipnode, to overcome the performance degradation of deep GCNs. It samples graph nodes in each convolutional layer to skip the convolution operation. In this way, both over-smoothing and gradient vanishing can be effectively suppressed since (1) not all",
    "link": "http://arxiv.org/abs/2112.11628",
    "context": "Title: SkipNode: On Alleviating Performance Degradation for Deep Graph Convolutional Networks. (arXiv:2112.11628v4 [cs.LG] UPDATED)\nAbstract: Graph Convolutional Networks (GCNs) suffer from performance degradation when models go deeper. However, earlier works only attributed the performance degeneration to over-smoothing. In this paper, we conduct theoretical and experimental analysis to explore the fundamental causes of performance degradation in deep GCNs: over-smoothing and gradient vanishing have a mutually reinforcing effect that causes the performance to deteriorate more quickly in deep GCNs. On the other hand, existing anti-over-smoothing methods all perform full convolutions up to the model depth. They could not well resist the exponential convergence of over-smoothing due to model depth increasing. In this work, we propose a simple yet effective plug-and-play module, Skipnode, to overcome the performance degradation of deep GCNs. It samples graph nodes in each convolutional layer to skip the convolution operation. In this way, both over-smoothing and gradient vanishing can be effectively suppressed since (1) not all",
    "path": "papers/21/12/2112.11628.json",
    "total_tokens": 896,
    "translated_title": "SkipNode: 缓解深度图卷积网络性能下降问题",
    "translated_abstract": "图卷积网络（GCNs）在模型加深时容易出现性能下降。然而，之前的研究仅将性能退化归因为过度平滑。本文通过理论和实验分析，探讨了深度GCNs性能下降的根本原因：过度平滑和梯度消失有着相互加强的作用，在深度GCNs中导致性能迅速恶化。另一方面，现有的反过度平滑方法都在模型深度上执行完整卷积操作。由于模型深度的增加，它们无法有效抵抗过度平滑的指数收敛。在本文中，我们提出了一个简单而有效的插拔式模块Skipnode，来克服深度GCNs的性能下降问题。它通过在每个卷积层中对图节点进行采样，跳过卷积操作。通过这种方式，可以有效抑制过度平滑和梯度消失，因为（1）不对所有节点执行卷积操作。",
    "tldr": "SkipNode提出了一个插拔式模块来缓解深度图卷积网络性能下降问题，通过跳过部分卷积操作效果显著，有效抑制过度平滑和梯度消失。"
}