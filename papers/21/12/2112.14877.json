{
    "title": "A Unified and Constructive Framework for the Universality of Neural Networks. (arXiv:2112.14877v4 [cs.LG] UPDATED)",
    "abstract": "One of the reasons why many neural networks are capable of replicating complicated tasks or functions is their universal property. Though the past few decades have seen tremendous advances in theories of neural networks, a single constructive framework for neural network universality remains unavailable. This paper is the first effort to provide a unified and constructive framework for the universality of a large class of activation functions including most of existing ones. At the heart of the framework is the concept of neural network approximate identity (nAI). The main result is: {\\em any nAI activation function is universal}. It turns out that most of existing activation functions are nAI, and thus universal in the space of continuous functions on compacta. The framework induces {\\bf several advantages} over the contemporary counterparts. First, it is constructive with elementary means from functional analysis, probability theory, and numerical analysis. Second, it is the first un",
    "link": "http://arxiv.org/abs/2112.14877",
    "context": "Title: A Unified and Constructive Framework for the Universality of Neural Networks. (arXiv:2112.14877v4 [cs.LG] UPDATED)\nAbstract: One of the reasons why many neural networks are capable of replicating complicated tasks or functions is their universal property. Though the past few decades have seen tremendous advances in theories of neural networks, a single constructive framework for neural network universality remains unavailable. This paper is the first effort to provide a unified and constructive framework for the universality of a large class of activation functions including most of existing ones. At the heart of the framework is the concept of neural network approximate identity (nAI). The main result is: {\\em any nAI activation function is universal}. It turns out that most of existing activation functions are nAI, and thus universal in the space of continuous functions on compacta. The framework induces {\\bf several advantages} over the contemporary counterparts. First, it is constructive with elementary means from functional analysis, probability theory, and numerical analysis. Second, it is the first un",
    "path": "papers/21/12/2112.14877.json",
    "total_tokens": 991,
    "translated_title": "神经网络普适性的统一建构框架",
    "translated_abstract": "神经网络之所以能够复制复杂的任务或函数之一是因为它们的普适性。虽然过去几十年来神经网络理论取得了巨大的进展，但尚未提供单一的建构框架来解释神经网络的普适性。本文是第一个为大多数已有激活函数提供统一的建构框架以解释它们的普适性的尝试。在框架的核心是神经网络近似恒等（nAI）的概念。主要的结果是：\\emph{任何nAI激活函数都是普适的}。事实证明，大多数激活函数都是nAI，因此在紧致空间连续函数空间内是普适的。该框架比现有的对应物具有\\textbf{几个优势}。首先，它是建立在从功能分析、概率论和数值分析的基本手段之上的构造性框架。其次，它是第一个适用于包括大多数已有激活函数在内的大类激活函数的统一框架。第三，它提出了神经网络普适性的新视角。",
    "tldr": "本论文提出了神经网络普适性的建构框架，任何nAI激活函数都是普适的，该框架具有统一、构造性和新视角的优势。",
    "en_tdlr": "This paper presents a constructive and unified framework for understanding the universality of neural networks, showing that any nAI activation function is universal. The framework has the advantages of being applicable to a large class of activation functions, presenting a new perspective on neural network universality, and being constructive with basic tools from functional analysis, probability theory, and numerical analysis."
}