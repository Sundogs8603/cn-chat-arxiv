{
    "title": "CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs. (arXiv:2112.08804v3 [cs.CL] UPDATED)",
    "abstract": "We present CrossSum, a large-scale cross-lingual summarization dataset comprising 1.68 million article-summary samples in 1,500+ language pairs. We create CrossSum by aligning parallel articles written in different languages via cross-lingual retrieval from a multilingual abstractive summarization dataset and perform a controlled human evaluation to validate its quality. We propose a multistage data sampling algorithm to effectively train a cross-lingual summarization model capable of summarizing an article in any target language. We also introduce LaSE, an embedding-based metric for automatically evaluating model-generated summaries. LaSE is strongly correlated with ROUGE and, unlike ROUGE, can be reliably measured even in the absence of references in the target language. Performance on ROUGE and LaSE indicate that our proposed model consistently outperforms baseline models. To the best of our knowledge, CrossSum is the largest cross-lingual summarization dataset and the first ever th",
    "link": "http://arxiv.org/abs/2112.08804",
    "context": "Title: CrossSum: Beyond English-Centric Cross-Lingual Summarization for 1,500+ Language Pairs. (arXiv:2112.08804v3 [cs.CL] UPDATED)\nAbstract: We present CrossSum, a large-scale cross-lingual summarization dataset comprising 1.68 million article-summary samples in 1,500+ language pairs. We create CrossSum by aligning parallel articles written in different languages via cross-lingual retrieval from a multilingual abstractive summarization dataset and perform a controlled human evaluation to validate its quality. We propose a multistage data sampling algorithm to effectively train a cross-lingual summarization model capable of summarizing an article in any target language. We also introduce LaSE, an embedding-based metric for automatically evaluating model-generated summaries. LaSE is strongly correlated with ROUGE and, unlike ROUGE, can be reliably measured even in the absence of references in the target language. Performance on ROUGE and LaSE indicate that our proposed model consistently outperforms baseline models. To the best of our knowledge, CrossSum is the largest cross-lingual summarization dataset and the first ever th",
    "path": "papers/21/12/2112.08804.json",
    "total_tokens": 946,
    "translated_title": "CrossSum：超越英语中心的1500多种语言对的跨语言摘要。",
    "translated_abstract": "我们介绍了CrossSum，这是一个包含1500多种语言对中168万篇文章-摘要样本的大规模跨语言摘要数据集。我们通过从多语言抽象概括数据集中进行跨语言检索对用不同语言编写的平行文章进行了对齐，并进行了受控人工评估以验证其质量。我们提出了一种多阶段数据采样算法，以有效训练能够摘要任何目标语言文章的跨语言摘要模型。我们还介绍了LaSE，这是一种基于嵌入的度量，用于自动评估模型生成的摘要。LaSE与ROUGE强相关，并且，与ROUGE不同，在目标语言没有参考文献的情况下也可以可靠地测量。ROUGE和LaSE的表现表明，我们的拟议模型始终优于基线模型。据我们所知，CrossSum是最大的跨语言摘要数据集，也是第一个。",
    "tldr": "本文介绍了CrossSum - 一个1500多种语言对中的跨语言摘要数据集，以及一种多阶段数据采样算法和一种新的评估度量LaSE。该模型在摘要生成方面的表现优于基线模型，是目前已知最大的跨语言摘要数据集。"
}