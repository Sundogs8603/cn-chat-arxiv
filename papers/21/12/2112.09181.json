{
    "title": "Approximation of functions with one-bit neural networks. (arXiv:2112.09181v2 [cs.LG] UPDATED)",
    "abstract": "The celebrated universal approximation theorems for neural networks roughly state that any reasonable function can be arbitrarily well-approximated by a network whose parameters are appropriately chosen real numbers. This paper examines the approximation capabilities of one-bit neural networks -- those whose nonzero parameters are $\\pm a$ for some fixed $a\\not=0$. One of our main theorems shows that for any $f\\in C^s([0,1]^d)$ with $\\|f\\|_\\infty<1$ and error $\\varepsilon$, there is a $f_{NN}$ such that $|f(\\boldsymbol{x})-f_{NN}(\\boldsymbol{x})|\\leq \\varepsilon$ for all $\\boldsymbol{x}$ away from the boundary of $[0,1]^d$, and $f_{NN}$ is either implementable by a $\\{\\pm 1\\}$ quadratic network with $O(\\varepsilon^{-2d/s})$ parameters or a $\\{\\pm \\frac 1 2 \\}$ ReLU network with $O(\\varepsilon^{-2d/s}\\log (1/\\varepsilon))$ parameters, as $\\varepsilon\\to0$. We establish new approximation results for iterated multivariate Bernstein operators, error estimates for noise-shaping quantization ",
    "link": "http://arxiv.org/abs/2112.09181",
    "context": "Title: Approximation of functions with one-bit neural networks. (arXiv:2112.09181v2 [cs.LG] UPDATED)\nAbstract: The celebrated universal approximation theorems for neural networks roughly state that any reasonable function can be arbitrarily well-approximated by a network whose parameters are appropriately chosen real numbers. This paper examines the approximation capabilities of one-bit neural networks -- those whose nonzero parameters are $\\pm a$ for some fixed $a\\not=0$. One of our main theorems shows that for any $f\\in C^s([0,1]^d)$ with $\\|f\\|_\\infty<1$ and error $\\varepsilon$, there is a $f_{NN}$ such that $|f(\\boldsymbol{x})-f_{NN}(\\boldsymbol{x})|\\leq \\varepsilon$ for all $\\boldsymbol{x}$ away from the boundary of $[0,1]^d$, and $f_{NN}$ is either implementable by a $\\{\\pm 1\\}$ quadratic network with $O(\\varepsilon^{-2d/s})$ parameters or a $\\{\\pm \\frac 1 2 \\}$ ReLU network with $O(\\varepsilon^{-2d/s}\\log (1/\\varepsilon))$ parameters, as $\\varepsilon\\to0$. We establish new approximation results for iterated multivariate Bernstein operators, error estimates for noise-shaping quantization ",
    "path": "papers/21/12/2112.09181.json",
    "total_tokens": 971,
    "translated_title": "用一比特神经网络逼近函数",
    "translated_abstract": "神经网络的通用逼近定理大致陈述了任何合理的函数都可以通过参数被适当选择的实数网络来任意逼近。本文研究了一比特神经网络的逼近能力，其非零参数为固定值 $±a$。其中一个主要定理显示，对于任何 $f \\in C^s([0,1]^d)$，且 $\\|f\\|_\\infty <1$ 和误差 $\\varepsilon$，存在 $f_{NN}$，使得 $\\boldsymbol{x}$ 不在 $[0,1]^d$ 边界处时，$|f(\\boldsymbol{x})-f_{NN}(\\boldsymbol{x})|\\leq \\varepsilon$，且当 $\\varepsilon$ 趋近于零时，$f_{NN}$ 可以是由 $O(\\varepsilon^{-2d/s})$ 个参数的 $\\{\\pm 1\\}$ 二次网络实现或是由 $O(\\varepsilon^{-2d/s}\\log (1/\\varepsilon))$ 个参数的 $\\{\\pm \\frac 1 2 \\}$ ReLU 网络实现。我们建立了迭代的多元 Bernstein 算子的新逼近结果，以及噪声整形量化的误差估计。",
    "tldr": "研究了一比特神经网络的逼近能力与参数范围，证明了任何满足条件的函数均可被逼近，并给出了实现方法。",
    "en_tdlr": "This paper examines the approximation capabilities of one-bit neural networks and proves that any function can be well-approximated by appropriately chosen parameters of a one-bit neural network while providing implementation methods."
}