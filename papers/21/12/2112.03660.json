{
    "title": "A generalization gap estimation for overparameterized models via the Langevin functional variance. (arXiv:2112.03660v3 [stat.ML] UPDATED)",
    "abstract": "This paper discusses the estimation of the generalization gap, the difference between generalization performance and training performance, for overparameterized models including neural networks. We first show that a functional variance, a key concept in defining a widely-applicable information criterion, characterizes the generalization gap even in overparameterized settings where a conventional theory cannot be applied. As the computational cost of the functional variance is expensive for the overparameterized models, we propose an efficient approximation of the function variance, the Langevin approximation of the functional variance (Langevin FV). This method leverages only the $1$st-order gradient of the squared loss function, without referencing the $2$nd-order gradient; this ensures that the computation is efficient and the implementation is consistent with gradient-based optimization algorithms. We demonstrate the Langevin FV numerically by estimating the generalization gaps of o",
    "link": "http://arxiv.org/abs/2112.03660",
    "context": "Title: A generalization gap estimation for overparameterized models via the Langevin functional variance. (arXiv:2112.03660v3 [stat.ML] UPDATED)\nAbstract: This paper discusses the estimation of the generalization gap, the difference between generalization performance and training performance, for overparameterized models including neural networks. We first show that a functional variance, a key concept in defining a widely-applicable information criterion, characterizes the generalization gap even in overparameterized settings where a conventional theory cannot be applied. As the computational cost of the functional variance is expensive for the overparameterized models, we propose an efficient approximation of the function variance, the Langevin approximation of the functional variance (Langevin FV). This method leverages only the $1$st-order gradient of the squared loss function, without referencing the $2$nd-order gradient; this ensures that the computation is efficient and the implementation is consistent with gradient-based optimization algorithms. We demonstrate the Langevin FV numerically by estimating the generalization gaps of o",
    "path": "papers/21/12/2112.03660.json",
    "total_tokens": 859,
    "translated_title": "通过Langevin函数方差估计超参数模型的泛化缺口",
    "translated_abstract": "本文讨论了泛化缺口的估计，即泛化性能与训练性能之间的差异，针对包括神经网络在内的超参数模型。我们首先展示了函数方差——一个定义广泛的信息准则中的关键概念——在超参数模型中也能表征泛化缺口，即使传统理论无法应用于超参数模型。由于函数方差的计算成本对于超参数模型而言非常昂贵，因此我们提出了函数方差的高效近似方法——函数方差的Langevin估计（Langevin FV）。这种方法只利用了平方损失函数的一阶梯度，而没有使用二阶梯度，从而保证了计算的高效性，并且实现与基于梯度的优化算法是一致的。我们通过数值演示了Langevin FV，估计了超参数模型的泛化缺口。",
    "tldr": "本文提出了一种函数方差的Langevin估计方法，用于高效计算超参数模型的泛化缺口，实现与基于梯度的优化算法一致。",
    "en_tdlr": "This paper proposes an efficient approximation method, Langevin FV, for calculating the generalization gap of overparameterized models by utilizing functional variance and only the 1st-order gradient of the squared loss function, which ensures both computational efficiency and consistency with gradient-based optimization algorithms."
}