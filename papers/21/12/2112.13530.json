{
    "title": "Wasserstein Flow Meets Replicator Dynamics: A Mean-Field Analysis of Representation Learning in Actor-Critic",
    "abstract": "arXiv:2112.13530v2 Announce Type: replace  Abstract: Actor-critic (AC) algorithms, empowered by neural networks, have had significant empirical success in recent years. However, most of the existing theoretical support for AC algorithms focuses on the case of linear function approximations, or linearized neural networks, where the feature representation is fixed throughout training. Such a limitation fails to capture the key aspect of representation learning in neural AC, which is pivotal in practical problems. In this work, we take a mean-field perspective on the evolution and convergence of feature-based neural AC. Specifically, we consider a version of AC where the actor and critic are represented by overparameterized two-layer neural networks and are updated with two-timescale learning rates. The critic is updated by temporal-difference (TD) learning with a larger stepsize while the actor is updated via proximal policy optimization (PPO) with a smaller stepsize. In the continuous-t",
    "link": "https://arxiv.org/abs/2112.13530",
    "context": "Title: Wasserstein Flow Meets Replicator Dynamics: A Mean-Field Analysis of Representation Learning in Actor-Critic\nAbstract: arXiv:2112.13530v2 Announce Type: replace  Abstract: Actor-critic (AC) algorithms, empowered by neural networks, have had significant empirical success in recent years. However, most of the existing theoretical support for AC algorithms focuses on the case of linear function approximations, or linearized neural networks, where the feature representation is fixed throughout training. Such a limitation fails to capture the key aspect of representation learning in neural AC, which is pivotal in practical problems. In this work, we take a mean-field perspective on the evolution and convergence of feature-based neural AC. Specifically, we consider a version of AC where the actor and critic are represented by overparameterized two-layer neural networks and are updated with two-timescale learning rates. The critic is updated by temporal-difference (TD) learning with a larger stepsize while the actor is updated via proximal policy optimization (PPO) with a smaller stepsize. In the continuous-t",
    "path": "papers/21/12/2112.13530.json",
    "total_tokens": 871,
    "translated_title": "Wasserstein Flow遇见复制动力学：Actor-Critic中代表学习的均场分析",
    "translated_abstract": "Actor-critic (AC)算法借助神经网络取得了显著的经验成功。然而，目前大部分关于AC算法的理论支持集中在具有线性函数逼近或线性化神经网络的情况下，其中特征表示在整个训练过程中保持不变。这种限制未能捕捉神经AC中代表学习的关键方面，在实际问题中至关重要。本文从均场的角度对基于特征的神经AC的演化和收敛进行了研究。具体地，我们考虑了一个AC的版本，其中演员和评论家由超参数化的两层神经网络表示，并使用两个时间尺度的学习率进行更新。评论家通过较大的步长进行时差（TD）学习更新，而演员通过较小步长进行邻域策略优化（PPO）更新。",
    "tldr": "本文通过均场分析研究了基于特征的神经AC的演化和收敛，提出了一个使用两个学习率更新的AC版本，其中评论家通过大步长进行TD学习更新，演员通过小步长进行PPO更新。",
    "en_tdlr": "This paper provides a mean-field analysis of the evolution and convergence of feature-based neural AC, introducing a version of AC with two-timescale learning rates where the critic is updated with a larger stepsize using TD learning and the actor is updated with a smaller stepsize using PPO."
}