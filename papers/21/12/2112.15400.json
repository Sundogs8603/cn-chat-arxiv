{
    "title": "A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning",
    "abstract": "arXiv:2112.15400v4 Announce Type: replace-cross  Abstract: Gradient-based Meta-RL (GMRL) refers to methods that maintain two-level optimisation procedures wherein the outer-loop meta-learner guides the inner-loop gradient-based reinforcement learner to achieve fast adaptations. In this paper, we develop a unified framework that describes variations of GMRL algorithms and points out that existing stochastic meta-gradient estimators adopted by GMRL are actually \\textbf{biased}. Such meta-gradient bias comes from two sources: 1) the compositional bias incurred by the two-level problem structure, which has an upper bound of $\\mathcal{O}\\big(K\\alpha^{K}\\hat{\\sigma}_{\\text{In}}|\\tau|^{-0.5}\\big)$ \\emph{w.r.t.} inner-loop update step $K$, learning rate $\\alpha$, estimate variance $\\hat{\\sigma}^{2}_{\\text{In}}$ and sample size $|\\tau|$, and 2) the multi-step Hessian estimation bias $\\hat{\\Delta}_{H}$ due to the use of autodiff, which has a polynomial impact $\\mathcal{O}\\big((K-1)(\\hat{\\Delta}_",
    "link": "https://arxiv.org/abs/2112.15400",
    "context": "Title: A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning\nAbstract: arXiv:2112.15400v4 Announce Type: replace-cross  Abstract: Gradient-based Meta-RL (GMRL) refers to methods that maintain two-level optimisation procedures wherein the outer-loop meta-learner guides the inner-loop gradient-based reinforcement learner to achieve fast adaptations. In this paper, we develop a unified framework that describes variations of GMRL algorithms and points out that existing stochastic meta-gradient estimators adopted by GMRL are actually \\textbf{biased}. Such meta-gradient bias comes from two sources: 1) the compositional bias incurred by the two-level problem structure, which has an upper bound of $\\mathcal{O}\\big(K\\alpha^{K}\\hat{\\sigma}_{\\text{In}}|\\tau|^{-0.5}\\big)$ \\emph{w.r.t.} inner-loop update step $K$, learning rate $\\alpha$, estimate variance $\\hat{\\sigma}^{2}_{\\text{In}}$ and sample size $|\\tau|$, and 2) the multi-step Hessian estimation bias $\\hat{\\Delta}_{H}$ due to the use of autodiff, which has a polynomial impact $\\mathcal{O}\\big((K-1)(\\hat{\\Delta}_",
    "path": "papers/21/12/2112.15400.json",
    "total_tokens": 958,
    "translated_title": "Meta-Reinforcement Learning中梯度偏差的理论理解",
    "translated_abstract": "梯度为基础的元强化学习（GMRL）是指保持两级优化程序的方法，其中外层元学习者指导内层梯度为基础的强化学习者实现快速适应。在本文中，我们开发了一个统一框架，描述了GMRL算法的变化，并指出GMRL采用的现有随机元梯度估计器实际上是有偏的。这种元梯度偏差来自两个方面：1）由两级问题结构引起的合成偏差，对内部更新步骤$K$、学习率$\\alpha$、估计方差$\\hat{\\sigma}^{2}_{\\text{In}}$和样本大小$|\\tau|$有一个上限为$\\mathcal{O}(K\\alpha^{K}\\hat{\\sigma}_{\\text{In}}|\\tau|^{-0.5}$；2）由于使用自动微分而导致的多步Hessian估计偏差$\\hat{\\Delta}_{H}$，其具有多项式影响$\\mathcal{O}((K-1)(\\hat{\\Delta}_...",
    "tldr": "本文对梯度为基础的元强化学习中的梯度偏差进行了深入理论理解，提出了统一框架描述GMRL算法的变化，并指出现有的随机元梯度估计器实际上是有偏的。"
}