{
    "title": "Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles. (arXiv:2112.11217v2 [cs.LG] UPDATED)",
    "abstract": "Recently, safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier force-based control policy structure to guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic implementation is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Sa",
    "link": "http://arxiv.org/abs/2112.11217",
    "context": "Title: Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles. (arXiv:2112.11217v2 [cs.LG] UPDATED)\nAbstract: Recently, safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier force-based control policy structure to guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic implementation is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Sa",
    "path": "papers/21/12/2112.11217.json",
    "total_tokens": 980,
    "translated_title": "模型为基础的安全强化学习在时间变化状态和控制约束下的应用：智能车辆中的应用",
    "translated_abstract": "近年来，基于演员-评论家结构的安全强化学习（RL）在连续控制任务中受到越来越多的关注。学习一个具有安全性和收敛性保证的近似最优控制策略仍然具有挑战性。同时，很少有作品讨论了在时间变化的安全性约束下设计安全RL算法。本文提出了一种安全RL算法，用于具有时间变化状态和控制约束的非线性系统的最优控制。在所提出的方法中，我们构建了一种新颖的基于屏障力的控制策略结构，以保证控制安全。提出了一种多步策略评估机制，用于预测策略在时间变化的安全约束下的安全风险，并指导策略安全更新。已证明稳定性和鲁棒性的理论结果。同时，分析了演员评论家实现的收敛性。所提出的算法的性能在模拟的Sa中优于几种最先进的RL算法",
    "tldr": "本文提出了一种安全RL算法，结合了基于屏障力的控制策略结构与多步策略评估机制，在保证控制安全的同时，能够应对时间变化的安全约束，并证明了其稳定性、鲁棒性和收敛性，优于几种最先进的RL算法。",
    "en_tdlr": "This paper proposes a safe RL algorithm that integrates a novel barrier force-based control policy structure and a multi-step policy evaluation mechanism to ensure control safety, tackle time-varying safety constraints, and provides stability, robustness and convergence guarantees, outperforming several state-of-the-art RL algorithms."
}