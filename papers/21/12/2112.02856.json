{
    "title": "Doubly Optimal No-Regret Online Learning in Strongly Monotone Games with Bandit Feedback",
    "abstract": "arXiv:2112.02856v4 Announce Type: replace  Abstract: We consider online no-regret learning in unknown games with bandit feedback, where each player can only observe its reward at each time -- determined by all players' current joint action -- rather than its gradient. We focus on the class of \\textit{smooth and strongly monotone} games and study optimal no-regret learning therein. Leveraging self-concordant barrier functions, we first construct a new bandit learning algorithm and show that it achieves the single-agent optimal regret of $\\tilde{\\Theta}(n\\sqrt{T})$ under smooth and strongly concave reward functions ($n \\geq 1$ is the problem dimension). We then show that if each player applies this no-regret learning algorithm in strongly monotone games, the joint action converges in the \\textit{last iterate} to the unique Nash equilibrium at a rate of $\\tilde{\\Theta}(nT^{-1/2})$. Prior to our work, the best-known convergence rate in the same class of games is $\\tilde{O}(n^{2/3}T^{-1/3})",
    "link": "https://arxiv.org/abs/2112.02856",
    "context": "Title: Doubly Optimal No-Regret Online Learning in Strongly Monotone Games with Bandit Feedback\nAbstract: arXiv:2112.02856v4 Announce Type: replace  Abstract: We consider online no-regret learning in unknown games with bandit feedback, where each player can only observe its reward at each time -- determined by all players' current joint action -- rather than its gradient. We focus on the class of \\textit{smooth and strongly monotone} games and study optimal no-regret learning therein. Leveraging self-concordant barrier functions, we first construct a new bandit learning algorithm and show that it achieves the single-agent optimal regret of $\\tilde{\\Theta}(n\\sqrt{T})$ under smooth and strongly concave reward functions ($n \\geq 1$ is the problem dimension). We then show that if each player applies this no-regret learning algorithm in strongly monotone games, the joint action converges in the \\textit{last iterate} to the unique Nash equilibrium at a rate of $\\tilde{\\Theta}(nT^{-1/2})$. Prior to our work, the best-known convergence rate in the same class of games is $\\tilde{O}(n^{2/3}T^{-1/3})",
    "path": "papers/21/12/2112.02856.json",
    "total_tokens": 1005,
    "translated_title": "在具有摇臂反馈的强单调博弈中的双重最优无悔在线学习",
    "translated_abstract": "我们研究了在具有摇臂反馈的未知博弈中的在线无悔学习，其中每个玩家只能观察到每个时间点的奖励 -- 由所有玩家当前的联合动作确定 -- 而不是梯度。我们专注于\\textit{光滑且强单调}博弈类，并研究其中的最优无悔学习。利用自共轭障碍函数，我们首先构建了一个新的摇臂学习算法，并展示它在光滑和强凹性奖励函数下实现了$\\tilde{\\Theta}(n\\sqrt{T})$的单一代理最优后悔（$n \\geq 1$是问题维度）。然后我们展示，如果每个玩家在强单调博弈中应用这个无悔学习算法，联合动作在\\textit{最后迭代}中以$\\tilde{\\Theta}(nT^{-1/2})$的速率收敛到唯一的纳什均衡点。在我们的工作之前，在相同类别的博弈中，最佳已知的收敛速率为$\\tilde{O}(n^{2/3}T^{-1/3})。",
    "tldr": "该论文提出了一种在强单调博弈中具有摇臂反馈的双重最优无悔在线学习方法，并展示了在特定条件下的最优后悔和联合动作收敛到纳什均衡点的速率的结果。",
    "en_tdlr": "This paper presents a doubly optimal no-regret online learning method in strongly monotone games with bandit feedback, achieving optimal regret and convergence rate to the Nash equilibrium under specific conditions."
}