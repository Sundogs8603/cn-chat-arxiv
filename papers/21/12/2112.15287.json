{
    "title": "Distributed Random Reshuffling over Networks. (arXiv:2112.15287v5 [math.OC] UPDATED)",
    "abstract": "In this paper, we consider distributed optimization problems where $n$ agents, each possessing a local cost function, collaboratively minimize the average of the local cost functions over a connected network. To solve the problem, we propose a distributed random reshuffling (D-RR) algorithm that invokes the random reshuffling (RR) update in each agent. We show that D-RR inherits favorable characteristics of RR for both smooth strongly convex and smooth nonconvex objective functions. In particular, for smooth strongly convex objective functions, D-RR achieves $\\mathcal{O}(1/T^2)$ rate of convergence (where $T$ counts epoch number) in terms of the squared distance between the iterate and the global minimizer. When the objective function is assumed to be smooth nonconvex, we show that D-RR drives the squared norm of gradient to $0$ at a rate of $\\mathcal{O}(1/T^{2/3})$. These convergence results match those of centralized RR (up to constant factors) and outperform the distributed stochast",
    "link": "http://arxiv.org/abs/2112.15287",
    "context": "Title: Distributed Random Reshuffling over Networks. (arXiv:2112.15287v5 [math.OC] UPDATED)\nAbstract: In this paper, we consider distributed optimization problems where $n$ agents, each possessing a local cost function, collaboratively minimize the average of the local cost functions over a connected network. To solve the problem, we propose a distributed random reshuffling (D-RR) algorithm that invokes the random reshuffling (RR) update in each agent. We show that D-RR inherits favorable characteristics of RR for both smooth strongly convex and smooth nonconvex objective functions. In particular, for smooth strongly convex objective functions, D-RR achieves $\\mathcal{O}(1/T^2)$ rate of convergence (where $T$ counts epoch number) in terms of the squared distance between the iterate and the global minimizer. When the objective function is assumed to be smooth nonconvex, we show that D-RR drives the squared norm of gradient to $0$ at a rate of $\\mathcal{O}(1/T^{2/3})$. These convergence results match those of centralized RR (up to constant factors) and outperform the distributed stochast",
    "path": "papers/21/12/2112.15287.json",
    "total_tokens": 962,
    "translated_title": "分布式随机重洗算法在网络中的应用",
    "translated_abstract": "本文研究了一类分布式优化问题，即在连通网络上，N个代理以协作的方式最小化本地代价函数的平均值。为了解决这个问题，我们提出了一种分布式随机重洗（D-RR）算法用来在每个代理中调用随机重洗（RR）更新。我们证明了D-RR继承了RR对于平滑强凸和平滑非凸目标函数的优越特性。特别是，对于平滑强凸目标函数，D-RR在迭代到全局最小值的平方距离方面达到了$\\mathcal O(1/T^2)$收敛率（其中$T$表示迭代次数）。当目标函数被假定为平滑非凸时，我们证明了D-RR以$\\mathcal O(1/T^{2/3})$的速率将梯度的平方范数驱动到0。这些收敛结果与集中式的RR（上到常数因子）相匹配并且优于分布式随机梯度下降算法。",
    "tldr": "本文提出了一种分布式随机重洗（D-RR）算法，能够解决协作优化问题。在平滑强凸和平滑非凸目标函数的情况下，D-RR算法都能够实现很好的优化结果，并且优于分布式随机梯度下降算法。",
    "en_tdlr": "This paper proposes a Distributed Random Reshuffling (D-RR) algorithm for collaborative optimization problems over a connected network. It achieves competitive convergence results compared to centralized RR, and outperforms the distributed stochastic gradient descent algorithm for both smooth strongly convex and smooth nonconvex objective functions."
}