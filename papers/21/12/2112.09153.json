{
    "title": "An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (arXiv:2112.09153v2 [cs.LG] UPDATED)",
    "abstract": "The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized mo",
    "link": "http://arxiv.org/abs/2112.09153",
    "context": "Title: An Empirical Investigation of the Role of Pre-training in Lifelong Learning. (arXiv:2112.09153v2 [cs.LG] UPDATED)\nAbstract: The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized mo",
    "path": "papers/21/12/2112.09153.json",
    "total_tokens": 881,
    "translated_title": "前期训练在终身学习中的作用的实证研究",
    "translated_abstract": "机器学习中的终身学习范式不仅因其类似生物学习的特性而具有吸引力，而且因其通过避免过多的模型重新训练而减少能源浪费的潜力而备受关注。这一范式面临的关键挑战是灾难性遗忘现象。随着预训练模型在机器学习中的日益流行和成功，我们提出一个问题：在终身学习中，前期训练在灾难性遗忘方面扮演何种角色？我们在大型预训练模型的背景下研究现有方法，并在各种文本和图像分类任务中评估它们的性能，包括使用一个新颖的包含15个不同自然语言处理任务的数据集进行的大规模研究。在所有设置中，我们观察到与随机初始化模型相比，通用的前期训练在学习多个任务时隐含地缓解了灾难性遗忘的影响。",
    "tldr": "这项研究通过对大型预训练模型在多个任务上的性能评估，发现通用的前期训练可以在终身学习中减轻灾难性遗忘的影响。"
}