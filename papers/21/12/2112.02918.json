{
    "title": "When the Curious Abandon Honesty: Federated Learning Is Not Private. (arXiv:2112.02918v2 [cs.LG] UPDATED)",
    "abstract": "In federated learning (FL), data does not leave personal devices when they are jointly training a machine learning model. Instead, these devices share gradients, parameters, or other model updates, with a central party (e.g., a company) coordinating the training. Because data never \"leaves\" personal devices, FL is often presented as privacy-preserving. Yet, recently it was shown that this protection is but a thin facade, as even a passive, honest-but-curious attacker observing gradients can reconstruct data of individual users contributing to the protocol. In this work, we show a novel data reconstruction attack which allows an active and dishonest central party to efficiently extract user data from the received gradients. While prior work on data reconstruction in FL relies on solving computationally expensive optimization problems or on making easily detectable modifications to the shared model's architecture or parameters, in our attack the central party makes inconspicuous changes ",
    "link": "http://arxiv.org/abs/2112.02918",
    "context": "Title: When the Curious Abandon Honesty: Federated Learning Is Not Private. (arXiv:2112.02918v2 [cs.LG] UPDATED)\nAbstract: In federated learning (FL), data does not leave personal devices when they are jointly training a machine learning model. Instead, these devices share gradients, parameters, or other model updates, with a central party (e.g., a company) coordinating the training. Because data never \"leaves\" personal devices, FL is often presented as privacy-preserving. Yet, recently it was shown that this protection is but a thin facade, as even a passive, honest-but-curious attacker observing gradients can reconstruct data of individual users contributing to the protocol. In this work, we show a novel data reconstruction attack which allows an active and dishonest central party to efficiently extract user data from the received gradients. While prior work on data reconstruction in FL relies on solving computationally expensive optimization problems or on making easily detectable modifications to the shared model's architecture or parameters, in our attack the central party makes inconspicuous changes ",
    "path": "papers/21/12/2112.02918.json",
    "total_tokens": 1128,
    "translated_title": "当好奇的人放弃诚实：联邦学习不是隐私的保护者",
    "translated_abstract": "在联邦学习中，数据不会在联合训练机器学习模型时离开个人设备。相反，这些设备会共享梯度、参数或其他模型更新，并由一个中央方（例如公司）协调培训。因为数据从未“离开”个人设备，所以FL通常被认为是保护隐私的。然而，最近的研究表明，这种保护只是一个薄薄的外衣，因为即使是一个被动的，诚实但好奇的攻击者观察到梯度，也可以重构参与协议的个人用户的数据。在这项工作中，我们展示了一种新颖的数据重构攻击，可以让一个主动的，不诚实的中央方从接收到的梯度中高效地提取用户数据。虽然FL的数据重构先前的工作依赖于解决计算昂贵的优化问题或对共享模型的体系结构或参数进行容易检测的修改，但在我们的攻击中，中央方对FL协议本身进行了不引人注目的修改以实现攻击。我们在流行的FL框架上展示了我们攻击的有效性，并讨论了可能的防御措施。",
    "tldr": "联邦学习被认为是隐私保护的一种手段，然而本文研究表明，即使是一个被动的、诚实但好奇的攻击者观察到梯度，也可以重构参与协议的个人用户的数据。本文提出了一种新的数据重构攻击方法，让一个主动的、不诚实的中央方从接收到的梯度中高效地提取用户数据，并在流行的FL框架上展示了其有效性。",
    "en_tdlr": "Federated learning is often presented as a privacy-preserving technique because data does not leave personal devices. However, this work shows that even a passive, honest-but-curious attacker observing gradients can reconstruct data of individual users contributing to the protocol. The authors propose a novel data reconstruction attack which allows an active and dishonest central party to efficiently extract user data from the received gradients, and demonstrate its effectiveness on popular FL frameworks, while also discussing possible defenses against it."
}