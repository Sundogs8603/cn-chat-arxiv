{
    "title": "MAFAT: Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference. (arXiv:2107.06960v2 [cs.LG] UPDATED)",
    "abstract": "A rising research challenge is running costly machine learning (ML) networks locally on resource-constrained edge devices. ML networks with large convolutional layers can easily exceed available memory, increasing latency due to excessive OS swapping. Previous memory reduction techniques such as pruning and quantization reduce model accuracy and often require retraining. Alternatively, distributed methods partition the convolutions into equivalent smaller sub-computations, but the implementations introduce communication costs and require a network of devices. Distributed partitioning approaches can, however, also be used to run in a reduced memory footprint on a single device by subdividing the network into smaller operations. In this paper, we extend prior work on distributed partitioning into a memory-aware execution on a single device. Our approach extends prior fusing strategies to allow for multiple groups of convolutional layers that are fused and tiled independently. This enable",
    "link": "http://arxiv.org/abs/2107.06960",
    "context": "Title: MAFAT: Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference. (arXiv:2107.06960v2 [cs.LG] UPDATED)\nAbstract: A rising research challenge is running costly machine learning (ML) networks locally on resource-constrained edge devices. ML networks with large convolutional layers can easily exceed available memory, increasing latency due to excessive OS swapping. Previous memory reduction techniques such as pruning and quantization reduce model accuracy and often require retraining. Alternatively, distributed methods partition the convolutions into equivalent smaller sub-computations, but the implementations introduce communication costs and require a network of devices. Distributed partitioning approaches can, however, also be used to run in a reduced memory footprint on a single device by subdividing the network into smaller operations. In this paper, we extend prior work on distributed partitioning into a memory-aware execution on a single device. Our approach extends prior fusing strategies to allow for multiple groups of convolutional layers that are fused and tiled independently. This enable",
    "path": "papers/21/07/2107.06960.json",
    "total_tokens": 942,
    "translated_title": "MAFAT: 内存感知的神经网络融合和切片加速边缘推断",
    "translated_abstract": "一个不断增长的研究挑战是在资源受限的边缘设备上本地运行昂贵的机器学习（ML）网络。具有大型卷积层的ML网络很容易超出可用内存，导致由于过多的操作系统交换而增加延迟。先前的内存减少技术，如修剪和量化，会降低模型的准确性，并且通常需要重新训练。另外，分布式方法将卷积层划分为等效的较小子计算，但实施引入了通信成本，并且需要一个设备网络。然而，分布式划分方法也可以用于在单个设备上以减少的内存占用运行，通过将网络细分为更小的操作。在本文中，我们将先前的分布式划分工作扩展为在单个设备上内存感知的执行。我们的方法扩展了先前的融合策略，以允许多个卷积层的组成部分独立融合和切片。",
    "tldr": "本文提出了一种内存感知的神经网络融合和切片方法，用于在边缘设备上加速推断。通过将网络细分为多个独立的卷积层组合，并进行融合和切片操作，实现了在资源受限的设备上降低内存占用的目标。"
}