{
    "title": "From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers. (arXiv:2107.07999v8 [cs.LG] UPDATED)",
    "abstract": "In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention (Choromanski et al., 2021) and log-linear RPE-attention (Luo et al., 2021) are special cases of this general mechanism. However by casting the problem as a topological (graph-based) modulation of unmasked attention, we obtain several results unknown before, including efficient d-dimensional RPE-masking and graph-kernel masking. We leverage many mathematical techniques ranging from spectral analysis through dynamic programming and random walks to new algorithms for solving Markov processes on graphs. We provide a corresponding empirical evaluation.",
    "link": "http://arxiv.org/abs/2107.07999",
    "context": "Title: From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers. (arXiv:2107.07999v8 [cs.LG] UPDATED)\nAbstract: In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention (Choromanski et al., 2021) and log-linear RPE-attention (Luo et al., 2021) are special cases of this general mechanism. However by casting the problem as a topological (graph-based) modulation of unmasked attention, we obtain several results unknown before, including efficient d-dimensional RPE-masking and graph-kernel masking. We leverage many mathematical techniques ranging from spectral analysis through dynamic programming and random walks to new algorithms for solving Markov processes on graphs. We provide a corresponding empirical evaluation.",
    "path": "papers/21/07/2107.07999.json",
    "total_tokens": 840,
    "translated_title": "从块-Toeplitz矩阵到图上的微分方程：迈向可扩展的Masked Transformers的通用理论",
    "translated_abstract": "本文提供了迄今为止最全面的方法，以可扩展的方式将各种掩码机制纳入Transformers架构中。我们展示了最近关于线性因果注意力（Choromanski等人，2021）和对数-线性RPE-注意力（Luo等人，2021）的结果是这种一般机制的特例。然而，通过将问题转化为未屏蔽的注意力的拓扑（基于图形）调制，我们获得了几个以前未知的结果，包括高效的d维RPE掩码和图内核掩码。我们利用许多数学技术，从谱分析、动态规划和随机游走到解决图上马尔可夫过程的新算法。我们提供了相应的实证评估。",
    "tldr": "本文提出了可扩展的方法将各种掩码机制纳入Transformers架构中。通过将问题转化为未屏蔽的注意力的拓扑（基于图形）调制，提出了高效的d维RPE掩码和图内核掩码。该方法得到了实验证明。",
    "en_tdlr": "This paper proposes a extensible method for incorporating various masking mechanisms into Transformers architectures. By casting the problem as a topological modulation of unmasked attention, efficient d-dimensional RPE-masking and graph-kernel masking are proposed, which are verified by empirical evaluation."
}