{
    "title": "Facetron: A Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations. (arXiv:2107.12003v3 [cs.CV] UPDATED)",
    "abstract": "In this paper, we propose a multi-speaker face-to-speech waveform generation model that also works for unseen speaker conditions. Using a generative adversarial network (GAN) with linguistic and speaker characteristic features as auxiliary conditions, our method directly converts face images into speech waveforms under an end-to-end training framework. The linguistic features are extracted from lip movements using a lip-reading model, and the speaker characteristic features are predicted from face images using cross-modal learning with a pre-trained acoustic model. Since these two features are uncorrelated and controlled independently, we can flexibly synthesize speech waveforms whose speaker characteristics vary depending on the input face images. We show the superiority of our proposed model over conventional methods in terms of objective and subjective evaluation results. Specifically, we evaluate the performances of linguistic features by measuring their accuracy on an automatic sp",
    "link": "http://arxiv.org/abs/2107.12003",
    "context": "Title: Facetron: A Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations. (arXiv:2107.12003v3 [cs.CV] UPDATED)\nAbstract: In this paper, we propose a multi-speaker face-to-speech waveform generation model that also works for unseen speaker conditions. Using a generative adversarial network (GAN) with linguistic and speaker characteristic features as auxiliary conditions, our method directly converts face images into speech waveforms under an end-to-end training framework. The linguistic features are extracted from lip movements using a lip-reading model, and the speaker characteristic features are predicted from face images using cross-modal learning with a pre-trained acoustic model. Since these two features are uncorrelated and controlled independently, we can flexibly synthesize speech waveforms whose speaker characteristics vary depending on the input face images. We show the superiority of our proposed model over conventional methods in terms of objective and subjective evaluation results. Specifically, we evaluate the performances of linguistic features by measuring their accuracy on an automatic sp",
    "path": "papers/21/07/2107.12003.json",
    "total_tokens": 860,
    "translated_title": "基于交叉模态潜在表示的多说话人脸向语音模型Facetron",
    "translated_abstract": "本文提出了一种多说话人脸向语音波形生成模型Facetron，适用于未知说话人条件下。我们使用具有语言和说话人特征的生成对抗网络（GAN）作为辅助条件，直接在端到端的训练框架下将面部图像转换为语音波形。语言特征是使用唇语识别模型从唇部运动中提取的，说话人特征则通过与预训练的声学模型的交叉模态学习从面部图像中预测得出。由于这两个特征是不相关的且独立控制的，因此我们可以灵活地合成语音波形，其说话人特征取决于输入的面部图像。实验结果表明，我们提出的模型在客观和主观评估结果方面比传统方法表现更好。",
    "tldr": "本文提出了一种基于交叉模态潜在表示的多说话人脸向语音模型Facetron，可以适用于不同的说话人条件下灵活地生成语音波形，并在客观和主观评估中表现出优越性。"
}