{
    "title": "When and How to Fool Explainable Models (and Humans) with Adversarial Examples. (arXiv:2107.01943v2 [cs.LG] UPDATED)",
    "abstract": "Reliable deployment of machine learning models such as neural networks continues to be challenging due to several limitations. Some of the main shortcomings are the lack of interpretability and the lack of robustness against adversarial examples or out-of-distribution inputs. In this exploratory review, we explore the possibilities and limits of adversarial attacks for explainable machine learning models. First, we extend the notion of adversarial examples to fit in explainable machine learning scenarios, in which the inputs, the output classifications and the explanations of the model's decisions are assessed by humans. Next, we propose a comprehensive framework to study whether (and how) adversarial examples can be generated for explainable models under human assessment, introducing and illustrating novel attack paradigms. In particular, our framework considers a wide range of relevant yet often ignored factors such as the type of problem, the user expertise or the objective of the e",
    "link": "http://arxiv.org/abs/2107.01943",
    "context": "Title: When and How to Fool Explainable Models (and Humans) with Adversarial Examples. (arXiv:2107.01943v2 [cs.LG] UPDATED)\nAbstract: Reliable deployment of machine learning models such as neural networks continues to be challenging due to several limitations. Some of the main shortcomings are the lack of interpretability and the lack of robustness against adversarial examples or out-of-distribution inputs. In this exploratory review, we explore the possibilities and limits of adversarial attacks for explainable machine learning models. First, we extend the notion of adversarial examples to fit in explainable machine learning scenarios, in which the inputs, the output classifications and the explanations of the model's decisions are assessed by humans. Next, we propose a comprehensive framework to study whether (and how) adversarial examples can be generated for explainable models under human assessment, introducing and illustrating novel attack paradigms. In particular, our framework considers a wide range of relevant yet often ignored factors such as the type of problem, the user expertise or the objective of the e",
    "path": "papers/21/07/2107.01943.json",
    "total_tokens": 865,
    "translated_title": "如何在可解释模型 (以及人类) 中愚弄敌对示例",
    "translated_abstract": "由于多种限制，如神经网络的解释性不足和对敌对示例或离群输入的鲁棒性不足，可靠部署诸如神经网络之类的机器学习模型仍然具有挑战性。在这篇探索性综述中，我们探讨了针对可解释机器学习模型的敌对攻击的可能性和限制。首先，我们将敌对示例的概念扩展到适用于可解释机器学习场景中，其中输入、输出分类和模型决策的解释由人类评估。接下来，我们提出了一个全面的框架来研究在人类评估下是否（以及如何）为可解释模型生成敌对示例，并引入和说明了新的攻击范式。特别是，我们的框架考虑了一系列相关但常常被忽略的因素，例如问题类型、用户专业知识或目标。",
    "tldr": "本文研究了针对可解释机器学习模型的敌对攻击的可能性和限制，并提出了一个全面的框架来研究在人类评估下如何生成敌对示例。",
    "en_tdlr": "This paper explores the possibilities and limitations of adversarial attacks on explainable machine learning models, and proposes a comprehensive framework to study how adversarial examples can be generated under human assessment."
}