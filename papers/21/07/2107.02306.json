{
    "title": "Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity. (arXiv:2107.02306v2 [cs.LG] UPDATED)",
    "abstract": "Neural network pruning is a fruitful area of research with surging interest in high sparsity regimes. Benchmarking in this domain heavily relies on faithful representation of the sparsity of subnetworks, which has been traditionally computed as the fraction of removed connections (direct sparsity). This definition, however, fails to recognize unpruned parameters that detached from input or output layers of underlying subnetworks, potentially underestimating actual effective sparsity: the fraction of inactivated connections. While this effect might be negligible for moderately pruned networks (up to 10-100 compression rates), we find that it plays an increasing role for thinner subnetworks, greatly distorting comparison between different pruning algorithms. For example, we show that effective compression of a randomly pruned LeNet-300-100 can be orders of magnitude larger than its direct counterpart, while no discrepancy is ever observed when using SynFlow for pruning [Tanaka et al., 20",
    "link": "http://arxiv.org/abs/2107.02306",
    "context": "Title: Connectivity Matters: Neural Network Pruning Through the Lens of Effective Sparsity. (arXiv:2107.02306v2 [cs.LG] UPDATED)\nAbstract: Neural network pruning is a fruitful area of research with surging interest in high sparsity regimes. Benchmarking in this domain heavily relies on faithful representation of the sparsity of subnetworks, which has been traditionally computed as the fraction of removed connections (direct sparsity). This definition, however, fails to recognize unpruned parameters that detached from input or output layers of underlying subnetworks, potentially underestimating actual effective sparsity: the fraction of inactivated connections. While this effect might be negligible for moderately pruned networks (up to 10-100 compression rates), we find that it plays an increasing role for thinner subnetworks, greatly distorting comparison between different pruning algorithms. For example, we show that effective compression of a randomly pruned LeNet-300-100 can be orders of magnitude larger than its direct counterpart, while no discrepancy is ever observed when using SynFlow for pruning [Tanaka et al., 20",
    "path": "papers/21/07/2107.02306.json",
    "total_tokens": 850,
    "tldr": "本论文探讨了神经网络删减中的有效稀疏性问题，并发现直接算出的稀疏度与有效稀疏度的对比可能对算法比较产生极大的扭曲。"
}