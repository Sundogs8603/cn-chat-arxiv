{
    "title": "Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction. (arXiv:2107.14432v4 [cs.LG] UPDATED)",
    "abstract": "We develop a novel framework that adds the regularizers of the sparse group lasso to a family of adaptive optimizers in deep learning, such as Momentum, Adagrad, Adam, AMSGrad, AdaHessian, and create a new class of optimizers, which are named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad and Group AdaHessian, etc., accordingly. We establish theoretically proven convergence guarantees in the stochastic convex settings, based on primal-dual methods. We evaluate the regularized effect of our new optimizers on three large-scale real-world ad click datasets with state-of-the-art deep learning models. The experimental results reveal that compared with the original optimizers with the post-processing procedure which uses the magnitude pruning method, the performance of the models can be significantly improved on the same sparsity level. Furthermore, in comparison to the cases without magnitude pruning, our methods can achieve extremely high sparsity with significantly better or hig",
    "link": "http://arxiv.org/abs/2107.14432",
    "context": "Title: Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction. (arXiv:2107.14432v4 [cs.LG] UPDATED)\nAbstract: We develop a novel framework that adds the regularizers of the sparse group lasso to a family of adaptive optimizers in deep learning, such as Momentum, Adagrad, Adam, AMSGrad, AdaHessian, and create a new class of optimizers, which are named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad and Group AdaHessian, etc., accordingly. We establish theoretically proven convergence guarantees in the stochastic convex settings, based on primal-dual methods. We evaluate the regularized effect of our new optimizers on three large-scale real-world ad click datasets with state-of-the-art deep learning models. The experimental results reveal that compared with the original optimizers with the post-processing procedure which uses the magnitude pruning method, the performance of the models can be significantly improved on the same sparsity level. Furthermore, in comparison to the cases without magnitude pruning, our methods can achieve extremely high sparsity with significantly better or hig",
    "path": "papers/21/07/2107.14432.json",
    "total_tokens": 946,
    "translated_title": "CTR预测中基于稀疏分组Lasso的神经网络自适应优化器",
    "translated_abstract": "我们在深度学习中开发了一个新的框架，将稀疏分组Lasso的正则项加入到一系列自适应优化器中，如Momentum、Adagrad、Adam、AMSGrad、AdaHessian等，并创建了一类新的优化器，分别命名为Group Momentum、Group Adagrad、Group Adam、Group AMSGrad和Group AdaHessian等。我们基于原始-对偶方法在随机凸设置下建立了理论上的收敛保证。我们使用最先进的深度学习模型，在三个大规模真实广告点击数据集上评估了我们新优化器的正则效果。实验结果表明，与使用幅度修剪方法的原始优化器相比，模型在相同稀疏水平上的性能可以显著提升。此外，与没有幅度修剪的情况相比，我们的方法可以实现极高的稀疏性，同时具有更好或更高的性能。",
    "tldr": "本论文提出了一种新的框架，在神经网络的CTR预测中加入了稀疏分组Lasso的正则项，并创建了一类新的自适应优化器。实验证明，这些优化器在相同稀疏水平下可以显著提升模型性能，并且能够实现极高的稀疏性。",
    "en_tdlr": "This paper presents a novel framework that incorporates the regularizers of sparse group lasso into adaptive optimizers for neural networks in CTR prediction. The proposed optimizers, named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad, and Group AdaHessian, show significantly improved model performance and achieve extremely high sparsity compared to the original optimizers."
}