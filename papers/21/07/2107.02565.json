{
    "title": "Prioritized training on points that are learnable, worth learning, and not yet learned (workshop version). (arXiv:2107.02565v4 [cs.LG] UPDATED)",
    "abstract": "We introduce Goldilocks Selection, a technique for faster model training which selects a sequence of training points that are \"just right\". We propose an information-theoretic acquisition function -- the reducible validation loss -- and compute it with a small proxy model -- GoldiProx -- to efficiently choose training points that maximize information about a validation set. We show that the \"hard\" (e.g. high loss) points usually selected in the optimization literature are typically noisy, while the \"easy\" (e.g. low noise) samples often prioritized for curriculum learning confer less information. Further, points with uncertain labels, typically targeted by active learning, tend to be less relevant to the task. In contrast, Goldilocks Selection chooses points that are \"just right\" and empirically outperforms the above approaches. Moreover, the selected sequence can transfer to other architectures; practitioners can share and reuse it without the need to recreate it.",
    "link": "http://arxiv.org/abs/2107.02565",
    "context": "Title: Prioritized training on points that are learnable, worth learning, and not yet learned (workshop version). (arXiv:2107.02565v4 [cs.LG] UPDATED)\nAbstract: We introduce Goldilocks Selection, a technique for faster model training which selects a sequence of training points that are \"just right\". We propose an information-theoretic acquisition function -- the reducible validation loss -- and compute it with a small proxy model -- GoldiProx -- to efficiently choose training points that maximize information about a validation set. We show that the \"hard\" (e.g. high loss) points usually selected in the optimization literature are typically noisy, while the \"easy\" (e.g. low noise) samples often prioritized for curriculum learning confer less information. Further, points with uncertain labels, typically targeted by active learning, tend to be less relevant to the task. In contrast, Goldilocks Selection chooses points that are \"just right\" and empirically outperforms the above approaches. Moreover, the selected sequence can transfer to other architectures; practitioners can share and reuse it without the need to recreate it.",
    "path": "papers/21/07/2107.02565.json",
    "total_tokens": 977,
    "translated_title": "优先训练可学习、值得学习且尚未学习的点（研讨会版本）",
    "translated_abstract": "我们引入了“黄金选择”技术，一种用于更快地训练模型的技术，它选择了一系列“恰到好处”的训练点。我们提出了一种信息论的获取函数——可约的验证损失，并使用一个小型代理模型——GoldiProx来高效选择能够最大化验证集信息的训练点。我们发现，通常在优化文献中选择的“困难”（例如高损失）点往往是有噪声的，而通常优先选择用于课程学习的“简单”（例如低噪声）样本提供的信息较少。此外，通常被主动学习所针对的带有不确定标签的点往往与任务的相关性较小。相反，黄金选择选择的点既适中又具有较好表现。此外，所选择的序列可以迁移到其他架构中，从而实践者可以在无需重新创建的情况下共享和复用。",
    "tldr": "本文介绍了一种名为“黄金选择”的技术，它通过选择恰当的训练点来加快模型训练。与优化文献中通常选择的困难点和课程学习中通常优先选择的简单点不同，黄金选择选择的点既有较高的信息量又能表现良好，且可迁移到其他架构中。"
}