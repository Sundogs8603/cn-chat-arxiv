{
    "title": "Complexity-Optimized Sparse Bayesian Learning for Scalable Classification Tasks. (arXiv:2107.08195v5 [cs.LG] UPDATED)",
    "abstract": "Sparse Bayesian Learning (SBL) constructs an extremely sparse probabilistic model with very competitive generalization. However, SBL needs to invert a big covariance matrix with complexity $O(M^3)$ (M: feature size) for updating the regularization priors, making it difficult for problems with high dimensional feature space or large data size. As it may easily suffer from the memory overflow issue in such problems. This paper addresses this issue with a newly proposed diagonal Quasi-Newton (DQN) method for SBL called DQN-SBL where the inversion of big covariance matrix is ignored so that the complexity is reduced to $O(M)$. The DQN-SBL is thoroughly evaluated for non linear and linear classifications with various benchmarks of different sizes. Experimental results verify that DQN-SBL receives competitive generalization with a very sparse model and scales well to large-scale problems.",
    "link": "http://arxiv.org/abs/2107.08195",
    "context": "Title: Complexity-Optimized Sparse Bayesian Learning for Scalable Classification Tasks. (arXiv:2107.08195v5 [cs.LG] UPDATED)\nAbstract: Sparse Bayesian Learning (SBL) constructs an extremely sparse probabilistic model with very competitive generalization. However, SBL needs to invert a big covariance matrix with complexity $O(M^3)$ (M: feature size) for updating the regularization priors, making it difficult for problems with high dimensional feature space or large data size. As it may easily suffer from the memory overflow issue in such problems. This paper addresses this issue with a newly proposed diagonal Quasi-Newton (DQN) method for SBL called DQN-SBL where the inversion of big covariance matrix is ignored so that the complexity is reduced to $O(M)$. The DQN-SBL is thoroughly evaluated for non linear and linear classifications with various benchmarks of different sizes. Experimental results verify that DQN-SBL receives competitive generalization with a very sparse model and scales well to large-scale problems.",
    "path": "papers/21/07/2107.08195.json",
    "total_tokens": 932,
    "translated_title": "用于可扩展分类任务的复杂度优化稀疏贝叶斯学习",
    "translated_abstract": "稀疏贝叶斯学习（Sparse Bayesian Learning，SBL）构建了一个极其稀疏的概率模型，具有竞争力的泛化能力。然而，SBL需要求解一个复杂度为$O(M^3)$（M：特征维度）的大型协方差矩阵以更新正则化先验，这使得在特征空间维度高或数据规模大的问题中变得困难，容易遭遇内存溢出问题。本文提出了一种称为DQN-SBL的用于SBL的新型对角拟牛顿（Diagonal Quasi-Newton，DQN）方法来解决这个问题，它忽略了大型协方差矩阵的求逆，从而将复杂度降低到$O(M)$。利用各种不同规模的基准进行了对非线性和线性分类问题的全面评估。实验证明，DQN-SBL在具有非常稀疏模型的情况下具有竞争力的泛化能力，并且能够很好地扩展到大规模问题。",
    "tldr": "本文提出了一种复杂度优化稀疏贝叶斯学习方法DQN-SBL来解决高维特征空间或大数据规模问题中的内存溢出和计算复杂度高的问题，并在大规模问题上展现了竞争力的泛化能力。",
    "en_tdlr": "This paper proposes a complexity-optimized sparse Bayesian learning method called DQN-SBL to address the memory overflow and high computational complexity issues in high-dimensional feature space or large data size problems, and demonstrates competitive generalization on large-scale problems."
}