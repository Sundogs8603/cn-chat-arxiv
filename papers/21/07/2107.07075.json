{
    "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training. (arXiv:2107.07075v2 [cs.LG] UPDATED)",
    "abstract": "Recent success in deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, in standard vision datasets, simple scores averaged over several weight initializations can be used to identify important examples very early in training. We propose two such scores -- the Gradient Normed (GraNd) and the Error L2-Norm (EL2N) scores -- and demonstrate their efficacy on a range of architectures and datasets by pruning significant fractions of training data without sacrificing test accuracy. In fact, using EL2N scores calculated a few epochs into training, we can prune half of the CIFAR10 training set while slightly improving test accuracy. Furthermore, for a given dataset, EL2N scores from one architecture or hyperparameter configuration generaliz",
    "link": "http://arxiv.org/abs/2107.07075",
    "context": "Title: Deep Learning on a Data Diet: Finding Important Examples Early in Training. (arXiv:2107.07075v2 [cs.LG] UPDATED)\nAbstract: Recent success in deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, in standard vision datasets, simple scores averaged over several weight initializations can be used to identify important examples very early in training. We propose two such scores -- the Gradient Normed (GraNd) and the Error L2-Norm (EL2N) scores -- and demonstrate their efficacy on a range of architectures and datasets by pruning significant fractions of training data without sacrificing test accuracy. In fact, using EL2N scores calculated a few epochs into training, we can prune half of the CIFAR10 training set while slightly improving test accuracy. Furthermore, for a given dataset, EL2N scores from one architecture or hyperparameter configuration generaliz",
    "path": "papers/21/07/2107.07075.json",
    "total_tokens": 1019,
    "translated_title": "数据饮食上的深度学习：在训练早期找到重要的示例",
    "translated_abstract": "深度学习的最近成功部分地来自于在越来越大的数据集上训练过度参数化的网络。因此，自然而然地会问：有多少数据是多余的，哪些示例对于推广是重要的，如何找到它们？在本文中，我们做出了惊人的观察：在标准视觉数据集中，基于多个权重初始化的简单分数可以用于非常早期地识别重要的示例。我们提出了两个这样的分数——梯度归一化（GraNd）和误差L2范数（EL2N）分数——并通过在一系列体系结构和数据集上修剪大量的训练数据而没有牺牲测试准确性，证明了它们的功效。实际上，使用在几个时期中计算的EL2N分数，我们可以修剪CIFAR10训练数据集的一半，同时略微提高测试准确性。此外，对于给定的数据集，来自一种体系结构或超参数配置的EL2N分数普遍适用。",
    "tldr": "本文旨在找到深度学习中训练数据集中的重要示例，提出了两种基于标准视觉数据集的简单分数。在修剪大量训练数据的同时，不牺牲测试准确性，EL2N分数在几个时期的训练中能够修剪CIFAR10训练集的一半。"
}