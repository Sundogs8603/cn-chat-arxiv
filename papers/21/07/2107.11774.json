{
    "title": "SGD with a Constant Large Learning Rate Can Converge to Local Maxima. (arXiv:2107.11774v4 [cs.LG] UPDATED)",
    "abstract": "Previous works on stochastic gradient descent (SGD) often focus on its success. In this work, we construct worst-case optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange and potentially undesirable behaviors. Specifically, we construct landscapes and data distributions such that (1) SGD converges to local maxima, (2) SGD escapes saddle points arbitrarily slowly, (3) SGD prefers sharp minima over flat ones, and (4) AMSGrad converges to local maxima. We also realize results in a minimal neural network-like example. Our results highlight the importance of simultaneously analyzing the minibatch sampling, discrete-time updates rules, and realistic landscapes to understand the role of SGD in deep learning.",
    "link": "http://arxiv.org/abs/2107.11774",
    "context": "Title: SGD with a Constant Large Learning Rate Can Converge to Local Maxima. (arXiv:2107.11774v4 [cs.LG] UPDATED)\nAbstract: Previous works on stochastic gradient descent (SGD) often focus on its success. In this work, we construct worst-case optimization problems illustrating that, when not in the regimes that the previous works often assume, SGD can exhibit many strange and potentially undesirable behaviors. Specifically, we construct landscapes and data distributions such that (1) SGD converges to local maxima, (2) SGD escapes saddle points arbitrarily slowly, (3) SGD prefers sharp minima over flat ones, and (4) AMSGrad converges to local maxima. We also realize results in a minimal neural network-like example. Our results highlight the importance of simultaneously analyzing the minibatch sampling, discrete-time updates rules, and realistic landscapes to understand the role of SGD in deep learning.",
    "path": "papers/21/07/2107.11774.json",
    "total_tokens": 909,
    "translated_title": "带有定常大学习率的SGD可能收敛于局部最大值",
    "translated_abstract": "先前关于随机梯度下降（SGD）的研究通常着眼于其成功，本研究构建了最坏情况下的优化问题，证明了在先前研究通常假设不成立的情况下，SGD可能表现出许多奇怪且潜在的不良行为。具体来说，我们构建了景观和数据分布，使得（1）SGD收敛于局部最大值，（2）SGD缓慢越过鞍点，(3) SGD更喜欢尖锐的最小值而非平坦的最小值，(4) AMSGrad收敛于局部最大值。我们还通过极简的神经网络示例进行了实现。我们的研究强调了同时分析小批量采样、离散时间更新规则和现实景观以了解SGD在深度学习中的作用的重要性。",
    "tldr": "本研究构建了最坏情况下的优化问题，证明了带有定常大学习率的SGD可能表现出许多奇怪且潜在的不良行为，包括：收敛于局部最大值、缓慢越过鞍点和更喜欢尖锐的最小值。这强调了深入分析SGD在深度学习中作用的重要性。"
}