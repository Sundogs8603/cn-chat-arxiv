{
    "title": "Physics-informed Guided Disentanglement in Generative Networks. (arXiv:2107.14229v4 [cs.CV] UPDATED)",
    "abstract": "Image-to-image translation (i2i) networks suffer from entanglement effects in presence of physics-related phenomena in target domain (such as occlusions, fog, etc), lowering altogether the translation quality, controllability and variability. In this paper, we propose a general framework to disentangle visual traits in target images. Primarily, we build upon collection of simple physics models, guiding the disentanglement with a physical model that renders some of the target traits, and learning the remaining ones. Because physics allows explicit and interpretable outputs, our physical models (optimally regressed on target) allows generating unseen scenarios in a controllable manner. Secondarily, we show the versatility of our framework to neural-guided disentanglement where a generative network is used in place of a physical model in case the latter is not directly accessible. Altogether, we introduce three strategies of disentanglement being guided from either a fully differentiable ",
    "link": "http://arxiv.org/abs/2107.14229",
    "context": "Title: Physics-informed Guided Disentanglement in Generative Networks. (arXiv:2107.14229v4 [cs.CV] UPDATED)\nAbstract: Image-to-image translation (i2i) networks suffer from entanglement effects in presence of physics-related phenomena in target domain (such as occlusions, fog, etc), lowering altogether the translation quality, controllability and variability. In this paper, we propose a general framework to disentangle visual traits in target images. Primarily, we build upon collection of simple physics models, guiding the disentanglement with a physical model that renders some of the target traits, and learning the remaining ones. Because physics allows explicit and interpretable outputs, our physical models (optimally regressed on target) allows generating unseen scenarios in a controllable manner. Secondarily, we show the versatility of our framework to neural-guided disentanglement where a generative network is used in place of a physical model in case the latter is not directly accessible. Altogether, we introduce three strategies of disentanglement being guided from either a fully differentiable ",
    "path": "papers/21/07/2107.14229.json",
    "total_tokens": 905,
    "tldr": "本文提出了一种通过物理学知识指导生成网络进行特征分离的通用框架，可以通过最佳回归目标物理模型来生成看不见的场景，并展示了神经引导分离的多样性。"
}