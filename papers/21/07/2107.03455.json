{
    "title": "Model Selection for Generic Contextual Bandits. (arXiv:2107.03455v2 [stat.ML] UPDATED)",
    "abstract": "We consider the problem of model selection for the general stochastic contextual bandits under the realizability assumption. We propose a successive refinement based algorithm called Adaptive Contextual Bandit ({\\ttfamily ACB}), that works in phases and successively eliminates model classes that are too simple to fit the given instance. We prove that this algorithm is adaptive, i.e., the regret rate order-wise matches that of any provable contextual bandit algorithm (ex. \\cite{falcon}), that needs the knowledge of the true model class. The price of not knowing the correct model class turns out to be only an additive term contributing to the second order term in the regret bound. This cost possess the intuitive property that it becomes smaller as the model class becomes easier to identify, and vice-versa. We also show that a much simpler explore-then-commit (ETC) style algorithm also obtains similar regret bound, despite not knowing the true model class. However, the cost of model selec",
    "link": "http://arxiv.org/abs/2107.03455",
    "context": "Title: Model Selection for Generic Contextual Bandits. (arXiv:2107.03455v2 [stat.ML] UPDATED)\nAbstract: We consider the problem of model selection for the general stochastic contextual bandits under the realizability assumption. We propose a successive refinement based algorithm called Adaptive Contextual Bandit ({\\ttfamily ACB}), that works in phases and successively eliminates model classes that are too simple to fit the given instance. We prove that this algorithm is adaptive, i.e., the regret rate order-wise matches that of any provable contextual bandit algorithm (ex. \\cite{falcon}), that needs the knowledge of the true model class. The price of not knowing the correct model class turns out to be only an additive term contributing to the second order term in the regret bound. This cost possess the intuitive property that it becomes smaller as the model class becomes easier to identify, and vice-versa. We also show that a much simpler explore-then-commit (ETC) style algorithm also obtains similar regret bound, despite not knowing the true model class. However, the cost of model selec",
    "path": "papers/21/07/2107.03455.json",
    "total_tokens": 924,
    "translated_title": "通用背景上下文强化学习模型选择",
    "translated_abstract": "我们考虑在可实现性假设下的通用随机背景上下文强化学习模型选择问题。我们提出了一种名为自适应背景上下文强化学习（ACB）的基于连续精炼的算法，该算法分为多个阶段，逐步消除那些对给定实例来说过于简单的模型类别。我们证明了该算法是自适应的，即在任何可证明的背景上下文强化学习算法（例如\\cite{falcon}）的遗憾率与顺序一致匹配，前提是需要知道真实的模型类别。不知道正确的模型类别的代价实际上只是导致遗憾上界中的二阶项的附加项。这个代价具有直观的属性，当模型类别变得更容易识别时，它变小，反之亦然。我们还展示了一种更简单的探索-利用（ETC）风格的算法也能获得类似的遗憾上界，尽管不知道真实的模型类别。然而，模型选择的代价是...",
    "tldr": "我们提出了一种自适应背景上下文强化学习算法（ACB），可以在不知道真实模型类别的情况下进行模型选择，并且具有与已知模型类别算法相匹配的遗憾率。模型选择的代价仅对遗憾上界的二阶项有贡献，且具有直观的属性。"
}