{
    "title": "The Who in XAI: How AI Background Shapes Perceptions of AI Explanations",
    "abstract": "arXiv:2107.13509v2 Announce Type: replace-cross  Abstract: Explainability of AI systems is critical for users to take informed actions. Understanding \"who\" opens the black-box of AI is just as important as opening it. We conduct a mixed-methods study of how two different groups--people with and without AI background--perceive different types of AI explanations. Quantitatively, we share user perceptions along five dimensions. Qualitatively, we describe how AI background can influence interpretations, elucidating the differences through lenses of appropriation and cognitive heuristics. We find that (1) both groups showed unwarranted faith in numbers for different reasons and (2) each group found value in different explanations beyond their intended design. Carrying critical implications for the field of XAI, our findings showcase how AI generated explanations can have negative consequences despite best intentions and how that could lead to harmful manipulation of trust. We propose design",
    "link": "https://arxiv.org/abs/2107.13509",
    "context": "Title: The Who in XAI: How AI Background Shapes Perceptions of AI Explanations\nAbstract: arXiv:2107.13509v2 Announce Type: replace-cross  Abstract: Explainability of AI systems is critical for users to take informed actions. Understanding \"who\" opens the black-box of AI is just as important as opening it. We conduct a mixed-methods study of how two different groups--people with and without AI background--perceive different types of AI explanations. Quantitatively, we share user perceptions along five dimensions. Qualitatively, we describe how AI background can influence interpretations, elucidating the differences through lenses of appropriation and cognitive heuristics. We find that (1) both groups showed unwarranted faith in numbers for different reasons and (2) each group found value in different explanations beyond their intended design. Carrying critical implications for the field of XAI, our findings showcase how AI generated explanations can have negative consequences despite best intentions and how that could lead to harmful manipulation of trust. We propose design",
    "path": "papers/21/07/2107.13509.json",
    "total_tokens": 798,
    "translated_title": "XAI中的“谁”：AI背景如何塑造AI解释的感知",
    "translated_abstract": "AI系统的可解释性对用户采取知情行动至关重要。理解AI黑盒中的“谁”与打开它同样重要。我们进行了一项混合方法研究，研究了两组不同的人群——具有和不具有AI背景的人群——如何感知不同类型的AI解释。在定量上，我们分享用户在五个维度上的看法。在定性上，我们描述了AI背景如何影响解释的解读，通过拟人和认知启发的视角阐明了差异。我们发现（1）两组人出于不同原因对数字表现出不必要的信任，以及（2）每组人都发现不同于其预期设计的解释的价值。我们的发现对XAI领域具有重要意义，展示了即使出于最好的用意，AI生成的解释也可能产生负面后果，可能导致有害的信任操纵。我们提出设计",
    "tldr": "AI背景如何影响解释的解读，揭示了“谁”对于AI解释的重要性。",
    "en_tdlr": "How AI background influences interpretations of explanations, revealing the importance of \"who\" in AI explanations."
}