{
    "title": "The Bayesian Learning Rule. (arXiv:2107.04562v3 [stat.ML] UPDATED)",
    "abstract": "We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.",
    "link": "http://arxiv.org/abs/2107.04562",
    "context": "Title: The Bayesian Learning Rule. (arXiv:2107.04562v3 [stat.ML] UPDATED)\nAbstract: We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.",
    "path": "papers/21/07/2107.04562.json",
    "total_tokens": 816,
    "translated_title": "贝叶斯学习规则",
    "translated_abstract": "我们展示了许多机器学习算法是一个称为贝叶斯学习规则的单一算法的特例。这个规则是从贝叶斯原理推导出来的，可以从优化、深度学习和图形模型等领域得到广泛的算法。这包括经典算法如岭回归、牛顿法和卡尔曼滤波器，以及现代深度学习算法如随机梯度下降、RMSprop和Dropout。推导这些算法的关键思想是使用自然梯度估计的候选分布来逼近后验分布。不同的候选分布会导致不同的算法，对自然梯度的进一步逼近则会产生这些算法的变种。我们的工作不仅统一、泛化和改进了现有算法，还帮助我们设计新的算法。",
    "tldr": "许多机器学习算法都可以归结为贝叶斯学习规则，该规则通过利用自然梯度来逼近后验分布，从而得到广泛的算法应用。这一工作不仅统一了现有算法，还帮助我们设计新的算法。",
    "en_tdlr": "Many machine learning algorithms can be seen as instances of the Bayesian learning rule, which approximates the posterior distribution using natural gradients. This work unifies existing algorithms and facilitates the design of new ones."
}