{
    "title": "Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2107.04050v2 [stat.ML] UPDATED)",
    "abstract": "Learning in multi-agent systems is highly challenging due to several factors including the non-stationarity introduced by agents' interactions and the combinatorial nature of their state and action spaces. In particular, we consider the Mean-Field Control (MFC) problem which assumes an asymptotically infinite population of identical agents that aim to collaboratively maximize the collective reward. In many cases, solutions of an MFC problem are good approximations for large systems, hence, efficient learning for MFC is valuable for the analogous discrete agent setting with many agents. Specifically, we focus on the case of unknown system dynamics where the goal is to simultaneously optimize for the rewards and learn from experience. We propose an efficient model-based reinforcement learning algorithm, $M^3-UCRL$, that runs in episodes, balances between exploration and exploitation during policy learning, and provably solves this problem. Our main theoretical contributions are the first",
    "link": "http://arxiv.org/abs/2107.04050",
    "context": "Title: Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2107.04050v2 [stat.ML] UPDATED)\nAbstract: Learning in multi-agent systems is highly challenging due to several factors including the non-stationarity introduced by agents' interactions and the combinatorial nature of their state and action spaces. In particular, we consider the Mean-Field Control (MFC) problem which assumes an asymptotically infinite population of identical agents that aim to collaboratively maximize the collective reward. In many cases, solutions of an MFC problem are good approximations for large systems, hence, efficient learning for MFC is valuable for the analogous discrete agent setting with many agents. Specifically, we focus on the case of unknown system dynamics where the goal is to simultaneously optimize for the rewards and learn from experience. We propose an efficient model-based reinforcement learning algorithm, $M^3-UCRL$, that runs in episodes, balances between exploration and exploitation during policy learning, and provably solves this problem. Our main theoretical contributions are the first",
    "path": "papers/21/07/2107.04050.json",
    "total_tokens": 869,
    "translated_title": "高效的基于模型的多智能体均场强化学习算法",
    "translated_abstract": "多智能体系统的学习充满挑战，包括智能体相互作用所引入的非稳态和状态和动作空间的组合性质等多个因素。本文关注的是均场控制问题，其假设存在无限数量的相同智能体，旨在共同最大化收益。针对未知系统动力学的情况，本文提出了一种高效的基于模型的强化学习算法$M^3-UCRL$，在策略学习期间平衡探索和利用，并实现了这一问题的可证明求解。该算法在多个基准问题上的表现优于多种最先进的方法。",
    "tldr": "本文针对均场控制问题提出了一种高效的基于模型的强化学习算法$M^3-UCRL$，在未知系统动力学的情况下，该算法可平衡探索和利用，并实现了可证明的问题求解，具有较好的表现。",
    "en_tdlr": "This paper proposes an efficient model-based reinforcement learning algorithm $M^3-UCRL$ for multi-agent Mean-Field Control under unknown system dynamics, which balances exploration and exploitation and achieves provable problem-solving. Its theoretical contributions include the first analysis of regret bound and outperformance on multiple benchmark problems."
}