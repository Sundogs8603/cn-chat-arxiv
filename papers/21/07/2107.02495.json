{
    "title": "InfoNCE is variational inference in a recognition parameterised model. (arXiv:2107.02495v3 [stat.ML] UPDATED)",
    "abstract": "Here, we show that the InfoNCE objective is equivalent to the ELBO in a new class of probabilistic generative model, the recognition parameterised model (RPM). When we learn the optimal prior, the RPM ELBO becomes equal to the mutual information (MI; up to a constant), establishing a connection to pre-existing self-supervised learning methods such as InfoNCE. However, practical InfoNCE methods do not use the MI as an objective; the MI is invariant to arbitrary invertible transformations, so using an MI objective can lead to highly entangled representations (Tschannen et al., 2019). Instead, the actual InfoNCE objective is a simplified lower bound on the MI which is loose even in the infinite sample limit. Thus, an objective that works (i.e. the actual InfoNCE objective) appears to be motivated as a loose bound on an objective that does not work (i.e. the true MI which gives arbitrarily entangled representations). We give an alternative motivation for the actual InfoNCE objective. In pa",
    "link": "http://arxiv.org/abs/2107.02495",
    "context": "Title: InfoNCE is variational inference in a recognition parameterised model. (arXiv:2107.02495v3 [stat.ML] UPDATED)\nAbstract: Here, we show that the InfoNCE objective is equivalent to the ELBO in a new class of probabilistic generative model, the recognition parameterised model (RPM). When we learn the optimal prior, the RPM ELBO becomes equal to the mutual information (MI; up to a constant), establishing a connection to pre-existing self-supervised learning methods such as InfoNCE. However, practical InfoNCE methods do not use the MI as an objective; the MI is invariant to arbitrary invertible transformations, so using an MI objective can lead to highly entangled representations (Tschannen et al., 2019). Instead, the actual InfoNCE objective is a simplified lower bound on the MI which is loose even in the infinite sample limit. Thus, an objective that works (i.e. the actual InfoNCE objective) appears to be motivated as a loose bound on an objective that does not work (i.e. the true MI which gives arbitrarily entangled representations). We give an alternative motivation for the actual InfoNCE objective. In pa",
    "path": "papers/21/07/2107.02495.json",
    "total_tokens": 929,
    "translated_title": "InfoNCE是识别参数化模型中的变分推断",
    "translated_abstract": "在这里，我们展示InfoNCE目标等同于一种新型概率生成模型——识别参数化模型（RPM）中的ELBO。当我们学习最优先验时，RPM ELBO变成了互信息（MI；除了一个常数），从而与之前存在的自监督学习方法（如InfoNCE）建立了联系。然而，实际的InfoNCE方法并不使用MI作为目标；MI对于任意可逆变换是不变的，因此使用MI目标可能导致高度纠缠的表示（Tschannen et al.，2019）。相反，实际的InfoNCE目标是对MI的一个简化下界，即使在无限样本极限下也不紧密。因此，一个有效的目标（即实际的InfoNCE目标）似乎是对一个无效的目标（即给出任意纠缠表示的真实MI）的松散下界的动机。我们给出了实际的InfoNCE目标的另一种动机。在目录中",
    "tldr": "InfoNCE目标在识别参数化模型中等同于ELBO，在学习最优先验时变为互信息，并与自监督学习方法建立了联系。然而，实际的InfoNCE目标是对互信息的松散下界，以避免高度纠缠的表示。"
}