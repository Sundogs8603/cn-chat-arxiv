{
    "title": "Near-optimal inference in adaptive linear regression. (arXiv:2107.02266v3 [math.ST] UPDATED)",
    "abstract": "When data is collected in an adaptive manner, even simple methods like ordinary least squares can exhibit non-normal asymptotic behavior. As an undesirable consequence, hypothesis tests and confidence intervals based on asymptotic normality can lead to erroneous results. We propose a family of online debiasing estimators to correct these distributional anomalies in least squares estimation. Our proposed methods take advantage of the covariance structure present in the dataset and provide sharper estimates in directions for which more information has accrued. We establish an asymptotic normality property for our proposed online debiasing estimators under mild conditions on the data collection process and provide asymptotically exact confidence intervals. We additionally prove a minimax lower bound for the adaptive linear regression problem, thereby providing a baseline by which to compare estimators. There are various conditions under which our proposed estimators achieve the minimax lo",
    "link": "http://arxiv.org/abs/2107.02266",
    "context": "Title: Near-optimal inference in adaptive linear regression. (arXiv:2107.02266v3 [math.ST] UPDATED)\nAbstract: When data is collected in an adaptive manner, even simple methods like ordinary least squares can exhibit non-normal asymptotic behavior. As an undesirable consequence, hypothesis tests and confidence intervals based on asymptotic normality can lead to erroneous results. We propose a family of online debiasing estimators to correct these distributional anomalies in least squares estimation. Our proposed methods take advantage of the covariance structure present in the dataset and provide sharper estimates in directions for which more information has accrued. We establish an asymptotic normality property for our proposed online debiasing estimators under mild conditions on the data collection process and provide asymptotically exact confidence intervals. We additionally prove a minimax lower bound for the adaptive linear regression problem, thereby providing a baseline by which to compare estimators. There are various conditions under which our proposed estimators achieve the minimax lo",
    "path": "papers/21/07/2107.02266.json",
    "total_tokens": 849,
    "translated_title": "自适应线性回归中的近最优推断",
    "translated_abstract": "当数据以自适应方式收集时，即使是最简单的方法如普通最小二乘法也可能表现出非正常的渐近行为。 作为不良后果，基于渐近正常性的假设检验和置信区间可能导致错误结果。 我们提出了一些在线去偏估计的方法来纠正这些误差，并利用数据集中存在的协方差结构，在其更多信息已累积的方向上提供更锐利的估计。 我们在数据收集过程的温和条件下证明了我们提出的在线去偏估计的渐近正态性质，并提供了渐近精确的置信区间。 我们还针对自适应线性回归问题证明了最小化下界，从而提供了比较估计器的基线。 在我们提出的估计器达到最小值的各种条件下，最小化下界均实现。",
    "tldr": "本文提出了一些在线去偏估计的方法来修正自适应线性回归中的渐近偏差，利用数据集中的协方差结构提供更锐利的估计。",
    "en_tdlr": "This paper proposes a family of online debiasing estimators to correct asymptotic bias in adaptive linear regression, utilizing the covariance structure in the dataset to provide sharper estimates in directions with more information. Asymptotic normality properties are established and exact confidence intervals are provided, with a minimax lower bound also proved for comparison purposes."
}