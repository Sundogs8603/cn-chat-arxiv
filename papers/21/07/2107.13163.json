{
    "title": "Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers. (arXiv:2107.13163v3 [cs.LG] UPDATED)",
    "abstract": "A common lens to theoretically study neural net architectures is to analyze the functions they can approximate. However, constructions from approximation theory may be unrealistic and therefore less meaningful. For example, a common unrealistic trick is to encode target function values using infinite precision. To address these issues, this work proposes a formal definition of statistically meaningful (SM) approximation which requires the approximating network to exhibit good statistical learnability. We study SM approximation for two function classes: boolean circuits and Turing machines. We show that overparameterized feedforward neural nets can SM approximate boolean circuits with sample complexity depending only polynomially on the circuit size, not the size of the network. In addition, we show that transformers can SM approximate Turing machines with computation time bounded by $T$ with sample complexity polynomial in the alphabet size, state space size, and $\\log (T)$. We also in",
    "link": "http://arxiv.org/abs/2107.13163",
    "context": "Title: Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers. (arXiv:2107.13163v3 [cs.LG] UPDATED)\nAbstract: A common lens to theoretically study neural net architectures is to analyze the functions they can approximate. However, constructions from approximation theory may be unrealistic and therefore less meaningful. For example, a common unrealistic trick is to encode target function values using infinite precision. To address these issues, this work proposes a formal definition of statistically meaningful (SM) approximation which requires the approximating network to exhibit good statistical learnability. We study SM approximation for two function classes: boolean circuits and Turing machines. We show that overparameterized feedforward neural nets can SM approximate boolean circuits with sample complexity depending only polynomially on the circuit size, not the size of the network. In addition, we show that transformers can SM approximate Turing machines with computation time bounded by $T$ with sample complexity polynomial in the alphabet size, state space size, and $\\log (T)$. We also in",
    "path": "papers/21/07/2107.13163.json",
    "total_tokens": 903,
    "translated_title": "统计上意义的近似：一种在变换器中近似图灵机的案例研究",
    "translated_abstract": "理论上研究神经网络结构的常用方法是分析它们可以近似的函数。然而，近似理论中的构造可能是不现实的，因此意义不太明确。为了解决这些问题，本文提出了统计上意义的（SM）近似的正式定义，要求近似网络具有良好的统计可学性。我们研究了两种函数类别的SM近似：布尔电路和图灵机。我们表明，过度参数化的前馈神经网络可以SM近似布尔电路，采样复杂度仅取决于电路大小，而不是网络大小。此外，我们还表明，变换器可以SM近似计算时间受$T$限制的图灵机，采样复杂度多项式地取决于字母大小、状态空间大小和$\\log (T)$。我们还在...",
    "tldr": "本文提出了统计上意义的近似的正式定义，研究了过度参数化的前馈神经网络和变换器的SM近似在布尔电路和图灵机中的应用，重点在于探索近似网络应该具有良好的统计可学性的概念，达到更有意义的近似效果。",
    "en_tdlr": "This paper proposes a formal definition of statistically meaningful (SM) approximation to address unrealistic constructions in the approximation theory. The study focuses on overparameterized feedforward neural networks and transformers' SM approximation applications in boolean circuits and Turing machines, emphasizing the concept that the approximating network should exhibit good statistical learnability to achieve more meaningful approximation effects."
}