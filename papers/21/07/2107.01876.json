{
    "title": "Which Invariance Should We Transfer? A Causal Minimax Learning Approach. (arXiv:2107.01876v4 [stat.ML] UPDATED)",
    "abstract": "A major barrier to deploying current machine learning models lies in their non-reliability to dataset shifts. To resolve this problem, most existing studies attempted to transfer stable information to unseen environments. Particularly, independent causal mechanisms-based methods proposed to remove mutable causal mechanisms via the do-operator. Compared to previous methods, the obtained stable predictors are more effective in identifying stable information. However, a key question remains: which subset of this whole stable information should the model transfer, in order to achieve optimal generalization ability? To answer this question, we present a comprehensive minimax analysis from a causal perspective. Specifically, we first provide a graphical condition for the whole stable set to be optimal. When this condition fails, we surprisingly find with an example that this whole stable set, although can fully exploit stable information, is not the optimal one to transfer. To identify the o",
    "link": "http://arxiv.org/abs/2107.01876",
    "context": "Title: Which Invariance Should We Transfer? A Causal Minimax Learning Approach. (arXiv:2107.01876v4 [stat.ML] UPDATED)\nAbstract: A major barrier to deploying current machine learning models lies in their non-reliability to dataset shifts. To resolve this problem, most existing studies attempted to transfer stable information to unseen environments. Particularly, independent causal mechanisms-based methods proposed to remove mutable causal mechanisms via the do-operator. Compared to previous methods, the obtained stable predictors are more effective in identifying stable information. However, a key question remains: which subset of this whole stable information should the model transfer, in order to achieve optimal generalization ability? To answer this question, we present a comprehensive minimax analysis from a causal perspective. Specifically, we first provide a graphical condition for the whole stable set to be optimal. When this condition fails, we surprisingly find with an example that this whole stable set, although can fully exploit stable information, is not the optimal one to transfer. To identify the o",
    "path": "papers/21/07/2107.01876.json",
    "total_tokens": 889,
    "translated_title": "我们应该转移哪种不变性？一种因果极小化学习方法",
    "translated_abstract": "当前机器学习模型无法可靠应对数据集变化，因此大多数现有研究试图将稳定信息转移到看不见的环境中。特别地，基于独立因果机制的方法通过do-operator消除可变的因果机制。与之前的方法相比，所得到的稳定预测因为能够更有效地识别稳定信息而更加有效。然而，一个关键问题仍然存在：为了达到最佳的泛化能力，应该转移这整个稳定信息中的哪个子集？为了回答这个问题，我们从因果的角度提出了一种全面的极小化分析。具体来说，我们首先提供了一个用于判断整个稳定集是否最优的图形条件。当这个条件失败时，我们惊讶地发现，通过一个例子，这个整个稳定集虽然能够充分利用稳定信息，但并不是最优的转移集。为了确定最优集，我们提出了因果最小含义的方法，并给出了仿真和实际数据中的实验结果。",
    "tldr": "该论文从因果的角度提出了一种全面的极小化分析，旨在回答机器学习模型在转移稳定信息时应该转移哪个子集从而达到最佳的泛化能力这一问题",
    "en_tdlr": "This paper presents a comprehensive minimax analysis from a causal perspective to address the question of which subset of stable information should be transferred for optimal generalization ability in machine learning models."
}