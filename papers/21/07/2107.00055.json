{
    "title": "Approximate Regions of Attraction in Learning with Decision-Dependent Distributions. (arXiv:2107.00055v4 [cs.LG] UPDATED)",
    "abstract": "As data-driven methods are deployed in real-world settings, the processes that generate the observed data will often react to the decisions of the learner. For example, a data source may have some incentive for the algorithm to provide a particular label (e.g. approve a bank loan), and manipulate their features accordingly. Work in strategic classification and decision-dependent distributions seeks to characterize the closed-loop behavior of deploying learning algorithms by explicitly considering the effect of the classifier on the underlying data distribution. More recently, works in performative prediction seek to classify the closed-loop behavior by considering general properties of the mapping from classifier to data distribution, rather than an explicit form. Building on this notion, we analyze repeated risk minimization as the perturbed trajectories of the gradient flows of performative risk minimization. We consider the case where there may be multiple local minimizers of perfor",
    "link": "http://arxiv.org/abs/2107.00055",
    "context": "Title: Approximate Regions of Attraction in Learning with Decision-Dependent Distributions. (arXiv:2107.00055v4 [cs.LG] UPDATED)\nAbstract: As data-driven methods are deployed in real-world settings, the processes that generate the observed data will often react to the decisions of the learner. For example, a data source may have some incentive for the algorithm to provide a particular label (e.g. approve a bank loan), and manipulate their features accordingly. Work in strategic classification and decision-dependent distributions seeks to characterize the closed-loop behavior of deploying learning algorithms by explicitly considering the effect of the classifier on the underlying data distribution. More recently, works in performative prediction seek to classify the closed-loop behavior by considering general properties of the mapping from classifier to data distribution, rather than an explicit form. Building on this notion, we analyze repeated risk minimization as the perturbed trajectories of the gradient flows of performative risk minimization. We consider the case where there may be multiple local minimizers of perfor",
    "path": "papers/21/07/2107.00055.json",
    "total_tokens": 859,
    "translated_title": "带有决策相关分布的学习中的近似吸引子区域",
    "translated_abstract": "随着数据驱动的方法在现实世界中的应用，生成观察数据的过程往往会根据学习者的决策做出反应。例如，数据源可能有一些激励让算法提供特定的标签（如批准银行贷款），并相应地操纵它们的特征。在战略分类和决策相关分布的工作中，通过明确考虑分类器对基础数据分布的影响来表征部署学习算法的闭环行为。最近，在执行预测的研究中，考虑分类器到数据分布的映射的一般属性，而非显式形式来分类闭环行为。基于这个概念，我们将重复的风险最小化分析为执行风险最小化的梯度流的扰动轨迹。我们考虑可能存在多个局部最小化器的场合。",
    "tldr": "本文分析了在决策相关分布学习中的风险最小化，建立在执行风险最小化的梯度流的扰动轨迹之上，研究了可能存在多个局部最小化器的情况。"
}