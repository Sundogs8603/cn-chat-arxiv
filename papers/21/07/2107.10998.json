{
    "title": "Pruning Ternary Quantization. (arXiv:2107.10998v5 [cs.CV] UPDATED)",
    "abstract": "Inference time, model size, and accuracy are three key factors in deep model compression. Most of the existing work addresses these three key factors separately as it is difficult to optimize them all at the same time. For example, low-bit quantization aims at obtaining a faster model; weight sharing quantization aims at improving compression ratio and accuracy; and mixed-precision quantization aims at balancing accuracy and inference time. To simultaneously optimize bit-width, model size, and accuracy, we propose pruning ternary quantization (PTQ): a simple, effective, symmetric ternary quantization method. We integrate L2 normalization, pruning, and the weight decay term to reduce the weight discrepancy in the gradient estimator during quantization, thus producing highly compressed ternary weights. Our method brings the highest test accuracy and the highest compression ratio. For example, it produces a 939kb (49$\\times$) 2bit ternary ResNet-18 model with only 4\\% accuracy drop on the",
    "link": "http://arxiv.org/abs/2107.10998",
    "context": "Title: Pruning Ternary Quantization. (arXiv:2107.10998v5 [cs.CV] UPDATED)\nAbstract: Inference time, model size, and accuracy are three key factors in deep model compression. Most of the existing work addresses these three key factors separately as it is difficult to optimize them all at the same time. For example, low-bit quantization aims at obtaining a faster model; weight sharing quantization aims at improving compression ratio and accuracy; and mixed-precision quantization aims at balancing accuracy and inference time. To simultaneously optimize bit-width, model size, and accuracy, we propose pruning ternary quantization (PTQ): a simple, effective, symmetric ternary quantization method. We integrate L2 normalization, pruning, and the weight decay term to reduce the weight discrepancy in the gradient estimator during quantization, thus producing highly compressed ternary weights. Our method brings the highest test accuracy and the highest compression ratio. For example, it produces a 939kb (49$\\times$) 2bit ternary ResNet-18 model with only 4\\% accuracy drop on the",
    "path": "papers/21/07/2107.10998.json",
    "total_tokens": 921,
    "translated_title": "剪枝三值量化",
    "translated_abstract": "推理时间、模型大小和准确性是深度模型压缩中的三个关键因素。现有的大部分工作将这三个关键因素分开处理，因为同时优化它们非常困难。例如，低比特量化旨在获得更快的模型；权重共享量化旨在提高压缩比和准确性；混合精度量化旨在平衡准确性和推理时间。为了同时优化比特宽度、模型大小和准确性，我们提出了剪枝三值量化（PTQ）：一种简单、有效、对称的三值量化方法。我们将L2归一化、剪枝和权重衰减项结合在一起，以减小量化过程中梯度估计器的权重差异，从而产生高度压缩的三值权重。我们的方法带来了最高的测试准确性和最高的压缩比。例如，它可以产生一个仅为939kb（49倍）的2位三值ResNet-18模型，仅准确性下降4％。",
    "tldr": "本文提出了一种剪枝三值量化方法（PTQ），通过集成L2归一化、剪枝和权重衰减项，实现同时优化比特宽度、模型大小和准确性，将模型大小大大减小且保持较高的测试准确性。",
    "en_tdlr": "The paper proposes a pruning ternary quantization (PTQ) method that integrates L2 normalization, pruning, and weight decay to simultaneously optimize bit-width, model size, and accuracy, resulting in significantly reduced model size with high test accuracy."
}