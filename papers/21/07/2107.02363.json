{
    "title": "Asymptotics of Network Embeddings Learned via Subsampling. (arXiv:2107.02363v4 [stat.ML] UPDATED)",
    "abstract": "Network data are ubiquitous in modern machine learning, with tasks of interest including node classification, node clustering and link prediction. A frequent approach begins by learning an Euclidean embedding of the network, to which algorithms developed for vector-valued data are applied. For large networks, embeddings are learned using stochastic gradient methods where the sub-sampling scheme can be freely chosen. Despite the strong empirical performance of such methods, they are not well understood theoretically. Our work encapsulates representation methods using a subsampling approach, such as node2vec, into a single unifying framework. We prove, under the assumption that the graph is exchangeable, that the distribution of the learned embedding vectors asymptotically decouples. Moreover, we characterize the asymptotic distribution and provided rates of convergence, in terms of the latent parameters, which includes the choice of loss function and the embedding dimension. This provid",
    "link": "http://arxiv.org/abs/2107.02363",
    "context": "Title: Asymptotics of Network Embeddings Learned via Subsampling. (arXiv:2107.02363v4 [stat.ML] UPDATED)\nAbstract: Network data are ubiquitous in modern machine learning, with tasks of interest including node classification, node clustering and link prediction. A frequent approach begins by learning an Euclidean embedding of the network, to which algorithms developed for vector-valued data are applied. For large networks, embeddings are learned using stochastic gradient methods where the sub-sampling scheme can be freely chosen. Despite the strong empirical performance of such methods, they are not well understood theoretically. Our work encapsulates representation methods using a subsampling approach, such as node2vec, into a single unifying framework. We prove, under the assumption that the graph is exchangeable, that the distribution of the learned embedding vectors asymptotically decouples. Moreover, we characterize the asymptotic distribution and provided rates of convergence, in terms of the latent parameters, which includes the choice of loss function and the embedding dimension. This provid",
    "path": "papers/21/07/2107.02363.json",
    "total_tokens": 939,
    "translated_title": "子采样学习的网络嵌入的渐近分析",
    "translated_abstract": "网络数据在现代机器学习中无处不在，相关任务包括节点分类、节点聚类和链接预测。一种常用的方法是首先学习网络的欧几里得嵌入，然后应用于向量值数据开发的算法。对于大型网络，可以使用随机梯度方法学习嵌入，其中子采样方案可以自由选择。尽管这种方法具有强大的实证性能，但它们的理论理解还不够充分。我们的工作将诸如node2vec之类的表示方法封装到一个统一的框架中。在假设图是可交换的情况下，我们证明了学习到的嵌入向量的分布在渐近意义下分解。此外，我们根据潜在参数，包括损失函数和嵌入维数的选择，表征了渐近分布并提供了收敛速率。这为使用子采样学习的网络嵌入提供了基本见解，并阐明了算法选择和统计效率之间的权衡。",
    "tldr": "本研究将网络嵌入方法封装为一个统一框架，并从理论上证明了使用子采样学习的网络嵌入的渐近分布，同时提供了潜在参数的收敛速率和算法选择与统计效率之间的权衡。",
    "en_tdlr": "This study encapsulates network embedding methods using a subsampling approach into a unified framework and theoretically proves the asymptotic distribution of network embeddings learned via subsampling, while providing convergence rates of latent parameters and shedding light on the trade-offs between algorithmic choices and statistical efficiency."
}