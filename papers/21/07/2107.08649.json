{
    "title": "Non-asymptotic estimates for TUSLA algorithm for non-convex learning with applications to neural networks with ReLU activation function. (arXiv:2107.08649v2 [math.OC] UPDATED)",
    "abstract": "We consider non-convex stochastic optimization problems where the objective functions have super-linearly growing and discontinuous stochastic gradients. In such a setting, we provide a non-asymptotic analysis for the tamed unadjusted stochastic Langevin algorithm (TUSLA) introduced in Lovas et al. (2020). In particular, we establish non-asymptotic error bounds for the TUSLA algorithm in Wasserstein-1 and Wasserstein-2 distances. The latter result enables us to further derive non-asymptotic estimates for the expected excess risk. To illustrate the applicability of the main results, we consider an example from transfer learning with ReLU neural networks, which represents a key paradigm in machine learning. Numerical experiments are presented for the aforementioned example which support our theoretical findings. Hence, in this setting, we demonstrate both theoretically and numerically that the TUSLA algorithm can solve the optimization problem involving neural networks with ReLU activati",
    "link": "http://arxiv.org/abs/2107.08649",
    "context": "Title: Non-asymptotic estimates for TUSLA algorithm for non-convex learning with applications to neural networks with ReLU activation function. (arXiv:2107.08649v2 [math.OC] UPDATED)\nAbstract: We consider non-convex stochastic optimization problems where the objective functions have super-linearly growing and discontinuous stochastic gradients. In such a setting, we provide a non-asymptotic analysis for the tamed unadjusted stochastic Langevin algorithm (TUSLA) introduced in Lovas et al. (2020). In particular, we establish non-asymptotic error bounds for the TUSLA algorithm in Wasserstein-1 and Wasserstein-2 distances. The latter result enables us to further derive non-asymptotic estimates for the expected excess risk. To illustrate the applicability of the main results, we consider an example from transfer learning with ReLU neural networks, which represents a key paradigm in machine learning. Numerical experiments are presented for the aforementioned example which support our theoretical findings. Hence, in this setting, we demonstrate both theoretically and numerically that the TUSLA algorithm can solve the optimization problem involving neural networks with ReLU activati",
    "path": "papers/21/07/2107.08649.json",
    "total_tokens": 1012,
    "translated_title": "非凸学习中TUSLA算法的非渐进估计及其在ReLU神经网络中的应用",
    "translated_abstract": "本文考虑目标函数具有超线性增加和不连续随机梯度的非凸随机优化问题。针对这种情况，我们对Lovas等人（2020年）引入的tamed unadjusted stochastic Langevin algorithm（TUSLA）进行了非渐进性分析。特别地，我们在Wasserstein-1和Wasserstein-2距离上建立了TUSLA算法的非渐进误差界限。后一结果使我们能够进一步推导期望过量风险的非渐进估计值。为了说明主要结果的适用性，我们考虑了一个包含ReLU神经网络的迁移学习示例，该示例代表机器学习中的一个关键范例。我们为上述示例呈现了数值实验，支持了我们的理论发现。因此，在这种情况下，我们在理论和数值上都证明了TUSLA算法能够高效且精确地解决涉及ReLU激活函数的神经网络优化问题。",
    "tldr": "本文研究了非凸随机优化问题，提出了TUSLA算法在Wasserstein-1和Wasserstein-2距离上的非渐进误差界限，进而推导了期望过量风险的非渐进估计值。在ReLU神经网络中，理论和数值实验表明TUSLA算法能够高效且精确地解决此类优化问题。",
    "en_tdlr": "This paper studies non-convex stochastic optimization problems and provides non-asymptotic error bounds for the TUSLA algorithm in Wasserstein-1 and Wasserstein-2 distances, as well as non-asymptotic estimates for the expected excess risk. The effectiveness and accuracy of the TUSLA algorithm in solving optimization problems involving ReLU neural networks is demonstrated both theoretically and numerically."
}