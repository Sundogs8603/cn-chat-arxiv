{
    "title": "MixStyle Neural Networks for Domain Generalization and Adaptation. (arXiv:2107.02053v2 [cs.CV] UPDATED)",
    "abstract": "Neural networks do not generalize well to unseen data with domain shifts -- a longstanding problem in machine learning and AI. To overcome the problem, we propose MixStyle, a simple plug-and-play, parameter-free module that can improve domain generalization performance without the need to collect more data or increase model capacity. The design of MixStyle is simple: it mixes the feature statistics of two random instances in a single forward pass during training. The idea is grounded by the finding from recent style transfer research that feature statistics capture image style information, which essentially defines visual domains. Therefore, mixing feature statistics can be seen as an efficient way to synthesize new domains in the feature space, thus achieving data augmentation. MixStyle is easy to implement with a few lines of code, does not require modification to training objectives, and can fit a variety of learning paradigms including supervised domain generalization, semi-supervi",
    "link": "http://arxiv.org/abs/2107.02053",
    "context": "Title: MixStyle Neural Networks for Domain Generalization and Adaptation. (arXiv:2107.02053v2 [cs.CV] UPDATED)\nAbstract: Neural networks do not generalize well to unseen data with domain shifts -- a longstanding problem in machine learning and AI. To overcome the problem, we propose MixStyle, a simple plug-and-play, parameter-free module that can improve domain generalization performance without the need to collect more data or increase model capacity. The design of MixStyle is simple: it mixes the feature statistics of two random instances in a single forward pass during training. The idea is grounded by the finding from recent style transfer research that feature statistics capture image style information, which essentially defines visual domains. Therefore, mixing feature statistics can be seen as an efficient way to synthesize new domains in the feature space, thus achieving data augmentation. MixStyle is easy to implement with a few lines of code, does not require modification to training objectives, and can fit a variety of learning paradigms including supervised domain generalization, semi-supervi",
    "path": "papers/21/07/2107.02053.json",
    "total_tokens": 958,
    "translated_title": "MixStyle神经网络用于领域泛化和适应性的翻译和摘要机器人。",
    "translated_abstract": "神经网络在具有领域转移的未见数据上表现不佳，这是机器学习和人工智能中的一个长期存在的问题。为了克服这个问题，我们提出了MixStyle，这是一个简单的即插即用，无需参数的模块，可以提高领域泛化性能，而无需收集更多的数据或增加模型容量。MixStyle的设计很简单：在训练过程中，在一个前向传播中将两个随机实例的特征统计混合。这个想法是基于最新的风格转换研究发现的，特征统计捕捉到图像风格信息，而图像风格本质上定义了视觉领域。因此，混合特征统计可以被看作是在特征空间中合成新领域的一种高效方式，从而实现了数据增强。MixStyle很容易用几行代码实现，不需要修改训练目标，并且可以适用于各种学习范式，包括监督领域泛化，半监督领域自适应等。",
    "tldr": "MixStyle是一个简单的模块，用于提高神经网络对于领域转移的泛化性能。它通过在训练过程中混合两个随机实例的特征统计来合成新领域，从而实现数据增强。MixStyle易于实现，适用于各类学习范式。"
}