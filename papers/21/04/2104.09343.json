{
    "title": "Approximated Multi-Agent Fitted Q Iteration. (arXiv:2104.09343v5 [cs.LG] UPDATED)",
    "abstract": "We formulate an efficient approximation for multi-agent batch reinforcement learning, the approximated multi-agent fitted Q iteration (AMAFQI). We present a detailed derivation of our approach. We propose an iterative policy search and show that it yields a greedy policy with respect to multiple approximations of the centralized, learned Q-function. In each iteration and policy evaluation, AMAFQI requires a number of computations that scales linearly with the number of agents whereas the analogous number of computations increase exponentially for the fitted Q iteration (FQI), a commonly used approaches in batch reinforcement learning. This property of AMAFQI is fundamental for the design of a tractable multi-agent approach. We evaluate the performance of AMAFQI and compare it to FQI in numerical simulations. The simulations illustrate the significant computation time reduction when using AMAFQI instead of FQI in multi-agent problems and corroborate the similar performance of both appro",
    "link": "http://arxiv.org/abs/2104.09343",
    "context": "Title: Approximated Multi-Agent Fitted Q Iteration. (arXiv:2104.09343v5 [cs.LG] UPDATED)\nAbstract: We formulate an efficient approximation for multi-agent batch reinforcement learning, the approximated multi-agent fitted Q iteration (AMAFQI). We present a detailed derivation of our approach. We propose an iterative policy search and show that it yields a greedy policy with respect to multiple approximations of the centralized, learned Q-function. In each iteration and policy evaluation, AMAFQI requires a number of computations that scales linearly with the number of agents whereas the analogous number of computations increase exponentially for the fitted Q iteration (FQI), a commonly used approaches in batch reinforcement learning. This property of AMAFQI is fundamental for the design of a tractable multi-agent approach. We evaluate the performance of AMAFQI and compare it to FQI in numerical simulations. The simulations illustrate the significant computation time reduction when using AMAFQI instead of FQI in multi-agent problems and corroborate the similar performance of both appro",
    "path": "papers/21/04/2104.09343.json",
    "total_tokens": 940,
    "translated_title": "近似的多智能体拟合Q迭代",
    "translated_abstract": "本文提出了一种高效的多智能体批量强化学习的近似方法，即近似的多智能体拟合Q迭代（AMAFQI）。我们提出了一个迭代策略搜索，并展示其得到的策略具有多个集中学习到的Q函数的近似的贪婪特性。在每次迭代和策略评估中，AMAFQI需要的计算量随着智能体数量呈线性增长，而拟合Q迭代（FQI）需要的计算量则呈指数增长，FQI是在批量强化学习中常用的方法。AMAFQI的这个特性对于设计可行的多智能体方法至关重要。我们在数值仿真中评估了AMAFQI的性能，并将其与FQI进行了比较。仿真结果表明，在多智能体问题中使用AMAFQI而不是FQI可以显著减少计算时间，并证实了两种方法的类似性能。",
    "tldr": "本文提出了一种近似方法AMAFQI，可用于高效解决多智能体批量强化学习问题，相比于常用方法FQI，AMAFQI的计算量增长更为缓慢，以线性增长，且仿真实验显示两种方法的性能类似。"
}