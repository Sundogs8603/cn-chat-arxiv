{
    "title": "Paraphrastic Representations at Scale. (arXiv:2104.15114v2 [cs.CL] UPDATED)",
    "abstract": "We present a system that allows users to train their own state-of-the-art paraphrastic sentence representations in a variety of languages. We also release trained models for English, Arabic, German, French, Spanish, Russian, Turkish, and Chinese. We train these models on large amounts of data, achieving significantly improved performance from the original papers proposing the methods on a suite of monolingual semantic similarity, cross-lingual semantic similarity, and bitext mining tasks. Moreover, the resulting models surpass all prior work on unsupervised semantic textual similarity, significantly outperforming even BERT-based models like Sentence-BERT (Reimers and Gurevych, 2019). Additionally, our models are orders of magnitude faster than prior work and can be used on CPU with little difference in inference speed (even improved speed over GPU when using more CPU cores), making these models an attractive choice for users without access to GPUs or for use on embedded devices. Finall",
    "link": "http://arxiv.org/abs/2104.15114",
    "context": "Title: Paraphrastic Representations at Scale. (arXiv:2104.15114v2 [cs.CL] UPDATED)\nAbstract: We present a system that allows users to train their own state-of-the-art paraphrastic sentence representations in a variety of languages. We also release trained models for English, Arabic, German, French, Spanish, Russian, Turkish, and Chinese. We train these models on large amounts of data, achieving significantly improved performance from the original papers proposing the methods on a suite of monolingual semantic similarity, cross-lingual semantic similarity, and bitext mining tasks. Moreover, the resulting models surpass all prior work on unsupervised semantic textual similarity, significantly outperforming even BERT-based models like Sentence-BERT (Reimers and Gurevych, 2019). Additionally, our models are orders of magnitude faster than prior work and can be used on CPU with little difference in inference speed (even improved speed over GPU when using more CPU cores), making these models an attractive choice for users without access to GPUs or for use on embedded devices. Finall",
    "path": "papers/21/04/2104.15114.json",
    "total_tokens": 971,
    "translated_title": "大规模的近义句表示",
    "translated_abstract": "我们提出了一个系统，允许用户在各种语言中训练自己的全球领先的近义句表示。我们还发布了针对英语、阿拉伯语、德语、法语、西班牙语、俄语、土耳其语和汉语的训练模型。我们使用大量数据训练这些模型，在单语语义相似性、跨语言语义相似性和双语挖掘任务的一套模型中，取得了明显的改进性能。此外，这些模型的结果超越了所有先前的无监督语义文本相似性工作，甚至优于基于BERT的模型，如Sentence-BERT (Reimers和Gurevych, 2019)。此外，我们的模型比以前的工作快上几个数量级，并且可以在CPU上使用，推理速度几乎没有差距（在使用更多CPU核心时，比GPU速度更快），使得这些模型成为没有GPU或嵌入式设备使用的用户的有吸引力的选择。",
    "tldr": "该论文提出了一种允许用户在各种语言中训练自己的全球领先的近义句表示系统，并发布了训练模型，这些模型在多任务上取得了显著进展，具有比以往更快的速度和更高的性能，是没有GPU或嵌入式设备的用户的有吸引力的选择。",
    "en_tdlr": "The paper proposes a system that allows users to train their own state-of-the-art paraphrastic sentence representations in various languages, achieving significantly improved performance from the original papers. Trained models are released for eight languages and surpass all prior work on unsupervised semantic textual similarity. The resulting models are orders of magnitude faster and can be used on CPUs, making them an attractive choice for users without access to GPUs or for use on embedded devices."
}