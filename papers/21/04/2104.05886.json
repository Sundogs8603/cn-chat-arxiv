{
    "title": "The computational asymptotics of Gaussian variational inference and the Laplace approximation. (arXiv:2104.05886v3 [stat.CO] UPDATED)",
    "abstract": "Gaussian variational inference and the Laplace approximation are popular alternatives to Markov chain Monte Carlo that formulate Bayesian posterior inference as an optimization problem, enabling the use of simple and scalable stochastic optimization algorithms. However, a key limitation of both methods is that the solution to the optimization problem is typically not tractable to compute; even in simple settings the problem is nonconvex. Thus, recently developed statistical guarantees -- which all involve the (data) asymptotic properties of the global optimum -- are not reliably obtained in practice. In this work, we provide two major contributions: a theoretical analysis of the asymptotic convexity properties of variational inference with a Gaussian family and the maximum a posteriori (MAP) problem required by the Laplace approximation; and two algorithms -- consistent Laplace approximation (CLA) and consistent stochastic variational inference (CSVI) -- that exploit these properties t",
    "link": "http://arxiv.org/abs/2104.05886",
    "context": "Title: The computational asymptotics of Gaussian variational inference and the Laplace approximation. (arXiv:2104.05886v3 [stat.CO] UPDATED)\nAbstract: Gaussian variational inference and the Laplace approximation are popular alternatives to Markov chain Monte Carlo that formulate Bayesian posterior inference as an optimization problem, enabling the use of simple and scalable stochastic optimization algorithms. However, a key limitation of both methods is that the solution to the optimization problem is typically not tractable to compute; even in simple settings the problem is nonconvex. Thus, recently developed statistical guarantees -- which all involve the (data) asymptotic properties of the global optimum -- are not reliably obtained in practice. In this work, we provide two major contributions: a theoretical analysis of the asymptotic convexity properties of variational inference with a Gaussian family and the maximum a posteriori (MAP) problem required by the Laplace approximation; and two algorithms -- consistent Laplace approximation (CLA) and consistent stochastic variational inference (CSVI) -- that exploit these properties t",
    "path": "papers/21/04/2104.05886.json",
    "total_tokens": 832,
    "translated_title": "高斯变分推断和拉普拉斯近似的计算渐近特性",
    "translated_abstract": "高斯变分推断和拉普拉斯近似是用简单且可扩展的随机优化算法将贝叶斯后验推断表述为优化问题的流行替代方法，然而，这两种方法的关键限制是优化问题的解通常是无法计算的；即使在简单的情况下，问题也是非凸的。因此，最近发展的统计保证 -- 所有都涉及到全局最优值的（数据）渐近性质 -- 在实践中并不可靠。在这项工作中，我们提供了两个重要的贡献：对高斯族变分推断和拉普拉斯近似所需的最大后验概率 (MAP) 问题的渐近凸性性质进行了理论分析；并提出了两个算法 -- 一致拉普拉斯近似 (CLA) 和一致随机变分推断 (CSVI) -- 利用这些性质。",
    "tldr": "本论文分析了高斯变分推断和拉普拉斯近似的渐近凸性特性，并提出了两个算法（CLA和CSVI）来利用这些特性。"
}