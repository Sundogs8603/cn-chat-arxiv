{
    "title": "Tuned Compositional Feature Replays for Efficient Stream Learning. (arXiv:2104.02206v7 [cs.CV] UPDATED)",
    "abstract": "Our brains extract durable, generalizable knowledge from transient experiences of the world. Artificial neural networks come nowhere close: when tasked with learning to classify objects by training on non-repeating video frames in temporal order (online stream learning), models that learn well from shuffled datasets catastrophically forget old knowledge upon learning new stimuli. We propose a new continual learning algorithm, Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by replaying feature maps reconstructed by recombining generic parts. CRUMB concatenates trainable and re-usable \"memory block\" vectors to compositionally reconstruct feature map tensors in convolutional neural networks, like crumbs forming a loaf of bread. CRUMB stores the indices of memory blocks used to reconstruct new stimuli, enabling replay of specific memories during later tasks. This reconstruction mechanism also primes the neural network to minimize catastrophic forgetting by for",
    "link": "http://arxiv.org/abs/2104.02206",
    "context": "Title: Tuned Compositional Feature Replays for Efficient Stream Learning. (arXiv:2104.02206v7 [cs.CV] UPDATED)\nAbstract: Our brains extract durable, generalizable knowledge from transient experiences of the world. Artificial neural networks come nowhere close: when tasked with learning to classify objects by training on non-repeating video frames in temporal order (online stream learning), models that learn well from shuffled datasets catastrophically forget old knowledge upon learning new stimuli. We propose a new continual learning algorithm, Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by replaying feature maps reconstructed by recombining generic parts. CRUMB concatenates trainable and re-usable \"memory block\" vectors to compositionally reconstruct feature map tensors in convolutional neural networks, like crumbs forming a loaf of bread. CRUMB stores the indices of memory blocks used to reconstruct new stimuli, enabling replay of specific memories during later tasks. This reconstruction mechanism also primes the neural network to minimize catastrophic forgetting by for",
    "path": "papers/21/04/2104.02206.json",
    "total_tokens": 961,
    "translated_title": "用于高效流学习的调优组合特征回放",
    "translated_abstract": "我们的大脑从瞬时的世界经验中提取出持久的、可推广的知识。人工神经网络远远不能达到相同的水平：当被要求通过按照时间顺序训练非重复视频帧来学习对象分类时（在线流学习），那些能够从重新排列的数据集中良好学习的模型在学习新的刺激时会灾难性地遗忘旧的知识。我们提出了一种新的持续学习算法，称为Compositional Replay Using Memory Blocks (CRUMB)，通过重放通过重新组合通用部分重建的特征图来缓解遗忘问题。CRUMB在卷积神经网络中串联可训练和可重用的“内存块”向量，以组合方式重建特征图张量，就像面包屑组合成一个面包一样。CRUMB存储用于重建新刺激的内存块索引，从而使得在后续任务中能够回放特定的记忆。这种重建机制还可以引导神经网络最小化灾难性遗忘。",
    "tldr": "本文提出了一种名为CRUMB的新的持续学习算法，通过重放通过重新组合特征图来缓解遗忘问题。CRUMB通过存储内存块的索引来使得在后续任务中能够回放特定的记忆，这种重建机制还可以帮助神经网络最小化灾难性遗忘。",
    "en_tdlr": "This paper proposes a new continual learning algorithm, CRUMB, which mitigates forgetting by replaying feature maps reconstructed by recombining generic parts. CRUMB stores the indices of memory blocks used to reconstruct new stimuli, enabling replay of specific memories during later tasks. This reconstruction mechanism also helps the neural network minimize catastrophic forgetting."
}