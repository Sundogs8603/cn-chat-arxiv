{
    "title": "Lazy Parameter Tuning and Control: Choosing All Parameters Randomly From a Power-Law Distribution. (arXiv:2104.06714v5 [cs.NE] UPDATED)",
    "abstract": "Most evolutionary algorithms have multiple parameters and their values drastically affect the performance. Due to the often complicated interplay of the parameters, setting these values right for a particular problem (parameter tuning) is a challenging task. This task becomes even more complicated when the optimal parameter values change significantly during the run of the algorithm since then a dynamic parameter choice (parameter control) is necessary.  In this work, we propose a lazy but effective solution, namely choosing all parameter values (where this makes sense) in each iteration randomly from a suitably scaled power-law distribution. To demonstrate the effectiveness of this approach, we perform runtime analyses of the $(1+(\\lambda,\\lambda))$ genetic algorithm with all three parameters chosen in this manner. We show that this algorithm on the one hand can imitate simple hill-climbers like the $(1+1)$ EA, giving the same asymptotic runtime on problems like OneMax, LeadingOnes, o",
    "link": "http://arxiv.org/abs/2104.06714",
    "raw_ret": "{\n    \"translated_title\": \"懒惰参数调整和控制：从幂律分布中随机选择所有参数\",\n    \"translated_abstract\": \"大多数进化算法都有多个参数，并且它们的值极大地影响性能。由于参数的常常复杂的相互作用，为特定问题设置这些值（参数调整）是一项具有挑战性的任务。当最优参数值在算法运行期间发生显着变化时，这个任务变得更加复杂，因为这时需要动态参数选择（参数控制）。在这项工作中，我们提出了一个懒惰但有效的解决方案，即在每次迭代中从适当缩放的幂律分布中随机选择所有参数值（在这种情况下有意义）。为了证明这种方法的有效性，我们对所有三个参数以这种方式选择的$(1+(\\lambda,\\lambda))$遗传算法进行运行时分析。我们表明，这个算法一方面可以模仿像$(1+1)$EA这样的简单爬山者，在OneMax、LeadingOnes、o等问题上给出相同的渐近运行时间。\",\n    \"tldr\": \"本文提出了一种懒惰但有效的解决方案，即在每次迭代中从适当缩放的幂律分布中随机选择所有参数值，以提高遗传算法性能\"\n}<|im_sep|>",
    "total_tokens": 856
}