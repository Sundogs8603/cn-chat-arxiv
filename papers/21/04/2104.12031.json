{
    "title": "Low-rank Tensor Estimation via Riemannian Gauss-Newton: Statistical Optimality and Second-Order Convergence. (arXiv:2104.12031v4 [stat.ML] UPDATED)",
    "abstract": "In this paper, we consider the estimation of a low Tucker rank tensor from a number of noisy linear measurements. The general problem covers many specific examples arising from applications, including tensor regression, tensor completion, and tensor PCA/SVD. We consider an efficient Riemannian Gauss-Newton (RGN) method for low Tucker rank tensor estimation. Different from the generic (super)linear convergence guarantee of RGN in the literature, we prove the first local quadratic convergence guarantee of RGN for low-rank tensor estimation in the noisy setting under some regularity conditions and provide the corresponding estimation error upper bounds. A deterministic estimation error lower bound, which matches the upper bound, is provided that demonstrates the statistical optimality of RGN. The merit of RGN is illustrated through two machine learning applications: tensor regression and tensor SVD. Finally, we provide the simulation results to corroborate our theoretical findings.",
    "link": "http://arxiv.org/abs/2104.12031",
    "context": "Title: Low-rank Tensor Estimation via Riemannian Gauss-Newton: Statistical Optimality and Second-Order Convergence. (arXiv:2104.12031v4 [stat.ML] UPDATED)\nAbstract: In this paper, we consider the estimation of a low Tucker rank tensor from a number of noisy linear measurements. The general problem covers many specific examples arising from applications, including tensor regression, tensor completion, and tensor PCA/SVD. We consider an efficient Riemannian Gauss-Newton (RGN) method for low Tucker rank tensor estimation. Different from the generic (super)linear convergence guarantee of RGN in the literature, we prove the first local quadratic convergence guarantee of RGN for low-rank tensor estimation in the noisy setting under some regularity conditions and provide the corresponding estimation error upper bounds. A deterministic estimation error lower bound, which matches the upper bound, is provided that demonstrates the statistical optimality of RGN. The merit of RGN is illustrated through two machine learning applications: tensor regression and tensor SVD. Finally, we provide the simulation results to corroborate our theoretical findings.",
    "path": "papers/21/04/2104.12031.json",
    "total_tokens": 863,
    "translated_title": "通过Riemannian Gauss-Newton进行低秩张量估计：统计最优性和二阶收敛性",
    "translated_abstract": "本文研究从一系列有噪线性测量中估计低Tucker秩张量的问题。该问题涵盖了许多具体的应用示例，包括张量回归、张量补全和张量PCA / SVD。我们考虑了一种高效的Riemannian Gauss-Newton（RGN）方法来估计低Tucker秩张量。与文献中对RGN的（超）线性收敛保证不同，我们证明了在噪声环境下RGN在低秩张量估计中的第一种局部二次收敛保证，并提供相应的估计误差上界。我们还提供了一个确定性估计误差下界，该下界与上界相匹配，证明了RGN的统计最优性。我们通过两个机器学习应用（张量回归和张量SVD）来说明RGN的优点。最后，我们提供了模拟结果来证实我们的理论发现。",
    "tldr": "本文提出了一种通过Riemannian Gauss-Newton方法进行低秩张量估计的方法，并证明了其在噪声环境下的局部二次收敛性和统计最优性。",
    "en_tdlr": "This paper presents a method for low-rank tensor estimation using Riemannian Gauss-Newton, and proves its local quadratic convergence and statistical optimality in a noisy setting."
}