{
    "title": "Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling. (arXiv:2104.07704v2 [cs.CL] UPDATED)",
    "abstract": "Recent models have shown that incorporating syntactic knowledge into the semantic role labelling (SRL) task leads to a significant improvement. In this paper, we propose Syntax-aware Graph-to-Graph Transformer (SynG2G-Tr) model, which encodes the syntactic structure using a novel way to input graph relations as embeddings, directly into the self-attention mechanism of Transformer. This approach adds a soft bias towards attention patterns that follow the syntactic structure but also allows the model to use this information to learn alternative patterns. We evaluate our model on both span-based and dependency-based SRL datasets, and outperform previous alternative methods in both in-domain and out-of-domain settings, on CoNLL 2005 and CoNLL 2009 datasets.",
    "link": "http://arxiv.org/abs/2104.07704",
    "context": "Title: Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling. (arXiv:2104.07704v2 [cs.CL] UPDATED)\nAbstract: Recent models have shown that incorporating syntactic knowledge into the semantic role labelling (SRL) task leads to a significant improvement. In this paper, we propose Syntax-aware Graph-to-Graph Transformer (SynG2G-Tr) model, which encodes the syntactic structure using a novel way to input graph relations as embeddings, directly into the self-attention mechanism of Transformer. This approach adds a soft bias towards attention patterns that follow the syntactic structure but also allows the model to use this information to learn alternative patterns. We evaluate our model on both span-based and dependency-based SRL datasets, and outperform previous alternative methods in both in-domain and out-of-domain settings, on CoNLL 2005 and CoNLL 2009 datasets.",
    "path": "papers/21/04/2104.07704.json",
    "total_tokens": 783,
    "translated_title": "句法感知的图转换器用于语义角色标注",
    "translated_abstract": "最近的模型表明，将句法知识纳入语义角色标注（SRL）任务可以显着改善性能。本文提出了一种称为 Syntax-aware Graph-to-Graph Transformer（SynG2G-Tr）的模型，它使用一种新颖的方式将图关系转换为嵌入，直接输入到Transformer的自注意机制中编码句法结构。这种方法为遵循句法结构的注意力模式增加了柔性偏差，但也允许模型利用这些信息学习替代模式。我们对基于跨度和基于依存的SRL数据集进行了评估，在CoNLL 2005和 CoNLL 2009数据集中，我们在内外领域的基准测试中均优于先前的替代方法。",
    "tldr": "本文提出了一种句法感知的图转换器模型用于语义角色标注任务，该模型将句法结构以嵌入的方式输入到Transformer的自注意机制中，达到了比以往方法更好的性能。",
    "en_tdlr": "This paper proposes a Syntax-aware Graph-to-Graph Transformer model for semantic role labelling (SRL) task, which encodes syntactic structure as embeddings and inputs them directly into the self-attention mechanism of Transformer, achieving better performance than previous methods."
}