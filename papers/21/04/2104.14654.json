{
    "title": "Adversarial Inverse Reinforcement Learning for Mean Field Games. (arXiv:2104.14654v5 [cs.LG] UPDATED)",
    "abstract": "Mean field games (MFGs) provide a mathematically tractable framework for modelling large-scale multi-agent systems by leveraging mean field theory to simplify interactions among agents. It enables applying inverse reinforcement learning (IRL) to predict behaviours of large populations by recovering reward signals from demonstrated behaviours. However, existing IRL methods for MFGs are powerless to reason about uncertainties in demonstrated behaviours of individual agents. This paper proposes a novel framework, Mean-Field Adversarial IRL (MF-AIRL), which is capable of tackling uncertainties in demonstrations. We build MF-AIRL upon maximum entropy IRL and a new equilibrium concept. We evaluate our approach on simulated tasks with imperfect demonstrations. Experimental results demonstrate the superiority of MF-AIRL over existing methods in reward recovery.",
    "link": "http://arxiv.org/abs/2104.14654",
    "context": "Title: Adversarial Inverse Reinforcement Learning for Mean Field Games. (arXiv:2104.14654v5 [cs.LG] UPDATED)\nAbstract: Mean field games (MFGs) provide a mathematically tractable framework for modelling large-scale multi-agent systems by leveraging mean field theory to simplify interactions among agents. It enables applying inverse reinforcement learning (IRL) to predict behaviours of large populations by recovering reward signals from demonstrated behaviours. However, existing IRL methods for MFGs are powerless to reason about uncertainties in demonstrated behaviours of individual agents. This paper proposes a novel framework, Mean-Field Adversarial IRL (MF-AIRL), which is capable of tackling uncertainties in demonstrations. We build MF-AIRL upon maximum entropy IRL and a new equilibrium concept. We evaluate our approach on simulated tasks with imperfect demonstrations. Experimental results demonstrate the superiority of MF-AIRL over existing methods in reward recovery.",
    "path": "papers/21/04/2104.14654.json",
    "total_tokens": 847,
    "translated_title": "对于均值场博弈的对抗逆强化学习",
    "translated_abstract": "均值场博弈(MFGs)是利用均值场理论简化智能体之间相互作用的一种可数学处理的框架，用于建模大规模多智能体系统。通过从展示行为中恢复奖励信号，它使得应用逆强化学习(IRL)来预测大群体的行为变得可能。然而，现有的MFGs中的IRL方法无法推断出各个智能体展示行为的不确定性。本文提出了一种新的框架，均值场对抗逆强化学习(MF-AIRL)，它能够处理展示行为中的不确定性。我们构建MF-AIRL，基于最大熵IRL和一个新的均衡概念。我们在具有不完美演示的模拟任务上评估了我们的方法。实验结果表明，MF-AIRL在奖励恢复方面优于现有方法。",
    "tldr": "本文提出了一种新的均值场对抗逆强化学习(MF-AIRL)框架，它能够处理展示行为中的不确定性，并在模拟任务上的实验结果表明其在奖励恢复方面优于现有方法。",
    "en_tdlr": "This paper proposes a novel Mean-Field Adversarial IRL (MF-AIRL) framework, which is capable of handling uncertainties in demonstrated behaviors and has been proven to outperform existing methods in reward recovery on simulated tasks."
}