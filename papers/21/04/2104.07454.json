{
    "title": "Memory Capacity of Recurrent Neural Networks with Matrix Representation. (arXiv:2104.07454v3 [cs.LG] UPDATED)",
    "abstract": "It is well known that canonical recurrent neural networks (RNNs) face limitations in learning long-term dependencies which have been addressed by memory structures in long short-term memory (LSTM) networks. Neural Turing machines (NTMs) are novel RNNs that implement the notion of programmable computers with neural network controllers that can learn simple algorithmic tasks. Matrix neural networks feature matrix representation which inherently preserves the spatial structure of data when compared to canonical neural networks that use vector-based representation. One may then argue that neural networks with matrix representations may have the potential to provide better memory capacity. In this paper, we define and study a probabilistic notion of memory capacity based on Fisher information for matrix-based RNNs. We find bounds on memory capacity for such networks under various hypotheses and compare them with their vector counterparts. In particular, we show that the memory capacity of s",
    "link": "http://arxiv.org/abs/2104.07454",
    "context": "Title: Memory Capacity of Recurrent Neural Networks with Matrix Representation. (arXiv:2104.07454v3 [cs.LG] UPDATED)\nAbstract: It is well known that canonical recurrent neural networks (RNNs) face limitations in learning long-term dependencies which have been addressed by memory structures in long short-term memory (LSTM) networks. Neural Turing machines (NTMs) are novel RNNs that implement the notion of programmable computers with neural network controllers that can learn simple algorithmic tasks. Matrix neural networks feature matrix representation which inherently preserves the spatial structure of data when compared to canonical neural networks that use vector-based representation. One may then argue that neural networks with matrix representations may have the potential to provide better memory capacity. In this paper, we define and study a probabilistic notion of memory capacity based on Fisher information for matrix-based RNNs. We find bounds on memory capacity for such networks under various hypotheses and compare them with their vector counterparts. In particular, we show that the memory capacity of s",
    "path": "papers/21/04/2104.07454.json",
    "total_tokens": 898,
    "translated_title": "具有矩阵表示的循环神经网络的存储容量",
    "translated_abstract": "众所周知，经典的循环神经网络（RNN）在学习长期依赖性方面存在限制，这一问题在长短期记忆网络（LSTM）中通过存储结构得到了解决。神经图灵机（NTM）是一种新颖的RNN，通过神经网络控制器实现了可编程计算机的概念，可以学习简单的算法任务。矩阵神经网络采用矩阵表示，与使用基于向量表示的经典神经网络相比，可以固有地保留数据的空间结构。因此，可以认为具有矩阵表示的神经网络可能具有更好的存储容量。在本文中，我们基于Fisher信息定义和研究了一种基于概率的矩阵RNN存储容量的概念。在各种假设下，我们找到了这些网络的存储容量的上界，并与它们的向量对应物进行了比较。特别地，我们证明了s",
    "tldr": "这个论文研究了具有矩阵表示的循环神经网络的存储容量，通过基于Fisher信息的概率性概念定义和研究，得出了不同假设下的存储上界，并与向量表示的网络进行了比较。"
}