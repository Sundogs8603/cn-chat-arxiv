{
    "title": "Successor Feature Neural Episodic Control. (arXiv:2111.03110v2 [cs.LG] UPDATED)",
    "abstract": "A longstanding goal in reinforcement learning is to build intelligent agents that show fast learning and a flexible transfer of skills akin to humans and animals. This paper investigates the integration of two frameworks for tackling those goals: episodic control and successor features. Episodic control is a cognitively inspired approach relying on episodic memory, an instance-based memory model of an agent's experiences. Meanwhile, successor features and generalized policy improvement (SF&GPI) is a meta and transfer learning framework allowing to learn policies for tasks that can be efficiently reused for later tasks which have a different reward function. Individually, these two techniques have shown impressive results in vastly improving sample efficiency and the elegant reuse of previously learned policies. Thus, we outline a combination of both approaches in a single reinforcement learning framework and empirically illustrate its benefits.",
    "link": "http://arxiv.org/abs/2111.03110",
    "context": "Title: Successor Feature Neural Episodic Control. (arXiv:2111.03110v2 [cs.LG] UPDATED)\nAbstract: A longstanding goal in reinforcement learning is to build intelligent agents that show fast learning and a flexible transfer of skills akin to humans and animals. This paper investigates the integration of two frameworks for tackling those goals: episodic control and successor features. Episodic control is a cognitively inspired approach relying on episodic memory, an instance-based memory model of an agent's experiences. Meanwhile, successor features and generalized policy improvement (SF&GPI) is a meta and transfer learning framework allowing to learn policies for tasks that can be efficiently reused for later tasks which have a different reward function. Individually, these two techniques have shown impressive results in vastly improving sample efficiency and the elegant reuse of previously learned policies. Thus, we outline a combination of both approaches in a single reinforcement learning framework and empirically illustrate its benefits.",
    "path": "papers/21/11/2111.03110.json",
    "total_tokens": 845,
    "translated_title": "后继特征神经记忆控制",
    "translated_abstract": "强化学习中一个长期目标是构建智能代理，展示类似于人类和动物的快速学习和灵活的技能转移。本文研究了解决这些目标的两个框架的整合：记忆控制和后继特征。记忆控制是一种受认知启发的方法，依赖于情节记忆，即代理的经验的基于实例的内存模型。同时，后继特征和广义策略改进（SF&amp;GPI）是一种元学习和迁移学习框架，可以为具有不同奖励函数的后续任务学习策略，并且可以高效地重用先前学到的策略。这两种技术分别在大大提高样本效率和优雅地重用先前学到的策略方面显示出令人印象深刻的结果。因此，我们概述了将这两种方法结合在一个单一的强化学习框架中，并通过实证研究其好处。",
    "tldr": "本文研究了记忆控制和后继特征两个框架的整合，通过结合这两种方法，提高了强化学习的样本效率和策略重用的优雅性。"
}