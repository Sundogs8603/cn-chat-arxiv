{
    "title": "Pruning Self-attentions into Convolutional Layers in Single Path. (arXiv:2111.11802v4 [cs.CV] UPDATED)",
    "abstract": "Vision Transformers (ViTs) have achieved impressive performance over various computer vision tasks. However, modeling global correlations with multi-head self-attention (MSA) layers leads to two widely recognized issues: the massive computational resource consumption and the lack of intrinsic inductive bias for modeling local visual patterns. To solve both issues, we devise a simple yet effective method named Single-Path Vision Transformer pruning (SPViT), to efficiently and automatically compress the pre-trained ViTs into compact models with proper locality added. Specifically, we first propose a novel weight-sharing scheme between MSA and convolutional operations, delivering a single-path space to encode all candidate operations. In this way, we cast the operation search problem as finding which subset of parameters to use in each MSA layer, which significantly reduces the computational cost and optimization difficulty, and the convolution kernels can be well initialized using pre-tr",
    "link": "http://arxiv.org/abs/2111.11802",
    "context": "Title: Pruning Self-attentions into Convolutional Layers in Single Path. (arXiv:2111.11802v4 [cs.CV] UPDATED)\nAbstract: Vision Transformers (ViTs) have achieved impressive performance over various computer vision tasks. However, modeling global correlations with multi-head self-attention (MSA) layers leads to two widely recognized issues: the massive computational resource consumption and the lack of intrinsic inductive bias for modeling local visual patterns. To solve both issues, we devise a simple yet effective method named Single-Path Vision Transformer pruning (SPViT), to efficiently and automatically compress the pre-trained ViTs into compact models with proper locality added. Specifically, we first propose a novel weight-sharing scheme between MSA and convolutional operations, delivering a single-path space to encode all candidate operations. In this way, we cast the operation search problem as finding which subset of parameters to use in each MSA layer, which significantly reduces the computational cost and optimization difficulty, and the convolution kernels can be well initialized using pre-tr",
    "path": "papers/21/11/2111.11802.json",
    "total_tokens": 883,
    "translated_title": "在单路径中将自注意力修剪到卷积层中",
    "translated_abstract": "视觉Transformer（ViTs）在各种计算机视觉任务中取得了令人印象深刻的性能。然而，使用多头自注意（MSA）层建模全局关联存在两个广为认可的问题：巨大的计算资源消耗和缺乏对建模本地视觉模式的内在归纳偏差。为了解决这两个问题，我们设计了一种简单而有效的方法，称为单路径视觉Transformer修剪（SPViT），将预训练的ViTs压缩为具有适当局部性的紧凑模型。具体而言，我们首先提出了一种新颖的MSA和卷积操作之间的权重共享方案，提供了一个单路径空间来编码所有候选操作。通过这种方式，我们将操作搜索问题转化为在每个MSA层中找到要使用的参数子集，这显著减少了计算成本和优化难度，并且可以使用预先初始化的卷积核。",
    "tldr": "本文提出了一种名为SPViT的方法，将预训练的ViTs压缩为紧凑的模型，并通过权重共享方案将自注意力层和卷积操作相结合，以解决计算资源消耗和建模本地视觉模式的问题。",
    "en_tdlr": "The paper presents a method called SPViT that compresses pre-trained ViTs into compact models with proper locality and addresses the issues of computational resource consumption and modeling local visual patterns by combining self-attention layers and convolutional operations through a weight-sharing scheme."
}