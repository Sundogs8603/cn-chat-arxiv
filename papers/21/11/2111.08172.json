{
    "title": "Off-Policy Actor-Critic with Emphatic Weightings. (arXiv:2111.08172v3 [cs.LG] UPDATED)",
    "abstract": "A variety of theoretically-sound policy gradient algorithms exist for the on-policy setting due to the policy gradient theorem, which provides a simplified form for the gradient. The off-policy setting, however, has been less clear due to the existence of multiple objectives and the lack of an explicit off-policy policy gradient theorem. In this work, we unify these objectives into one off-policy objective, and provide a policy gradient theorem for this unified objective. The derivation involves emphatic weightings and interest functions. We show multiple strategies to approximate the gradients, in an algorithm called Actor Critic with Emphatic weightings (ACE). We prove in a counterexample that previous (semi-gradient) off-policy actor-critic methods--particularly Off-Policy Actor-Critic (OffPAC) and Deterministic Policy Gradient (DPG)--converge to the wrong solution whereas ACE finds the optimal solution. We also highlight why these semi-gradient approaches can still perform well in ",
    "link": "http://arxiv.org/abs/2111.08172",
    "context": "Title: Off-Policy Actor-Critic with Emphatic Weightings. (arXiv:2111.08172v3 [cs.LG] UPDATED)\nAbstract: A variety of theoretically-sound policy gradient algorithms exist for the on-policy setting due to the policy gradient theorem, which provides a simplified form for the gradient. The off-policy setting, however, has been less clear due to the existence of multiple objectives and the lack of an explicit off-policy policy gradient theorem. In this work, we unify these objectives into one off-policy objective, and provide a policy gradient theorem for this unified objective. The derivation involves emphatic weightings and interest functions. We show multiple strategies to approximate the gradients, in an algorithm called Actor Critic with Emphatic weightings (ACE). We prove in a counterexample that previous (semi-gradient) off-policy actor-critic methods--particularly Off-Policy Actor-Critic (OffPAC) and Deterministic Policy Gradient (DPG)--converge to the wrong solution whereas ACE finds the optimal solution. We also highlight why these semi-gradient approaches can still perform well in ",
    "path": "papers/21/11/2111.08172.json",
    "total_tokens": 975,
    "translated_title": "带有强调权重的离策略Actor-Critic算法",
    "translated_abstract": "在策略梯度定理的支持下，许多理论上可靠的策略梯度算法存在于on-policy环境下。然而，由于存在多个目标和缺乏显式的离策略策略梯度定理，off-policy环境下的情况则不那么明确。本文将这些目标统一到了一个离策略目标中，并为这个统一目标提供了一个策略梯度定理。导出过程涉及到强调权重和兴趣函数。我们展示了多种策略来近似梯度，并提出了一个名为带有强调权重的Actor-Critic算法（ACE）。我们通过一个反例证明了先前的（半梯度）离策略Actor-Critic算法，特别是离策略Actor-Critic（OffPAC）和确定性策略梯度（DPG），收敛于错误的解，而ACE找到了最优解。我们还强调了为什么这些半梯度方法仍然可以表现良好。",
    "tldr": "本文提出了一种带有强调权重的离策略Actor-Critic算法（ACE），通过将多个目标统一到一个离策略目标中并提供了该目标的策略梯度定理，解决了离策略环境下存在的问题，并在反例中证明了ACE算法找到的是最优解。"
}