{
    "title": "PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing. (arXiv:2111.10481v2 [cs.CV] UPDATED)",
    "abstract": "Vision Transformer (ViT) is known to be highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial patch perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting",
    "link": "http://arxiv.org/abs/2111.10481",
    "context": "Title: PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing. (arXiv:2111.10481v2 [cs.CV] UPDATED)\nAbstract: Vision Transformer (ViT) is known to be highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial patch perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting",
    "path": "papers/21/11/2111.10481.json",
    "total_tokens": 931,
    "translated_title": "PatchCensor：通过穷尽测试提高视觉Transformers的补丁鲁棒性认证",
    "translated_abstract": "与其他经典神经网络一样，视觉Transformer（ViT）也被认为是高度非线性的，并且容易受到自然和对抗性补丁扰动的干扰。在真实的工业环境中，这种限制可能对ViT的部署产生威胁，尤其是在安全关键场景中。本文提出了PatchCensor，旨在通过全面测试来证明ViT的补丁鲁棒性。我们试图通过考虑最坏的补丁攻击情境来提供可证明的保证。与那些可能被自适应攻击破坏的对抗性补丁的经验性防御措施不同，认证可靠的方法可以在某些条件下提供对于任意攻击的受保证的准确性。但是，现有的鲁棒性认证主要基于鲁棒性训练，这通常需要大量的训练工作，并且会在正常样本上牺牲模型的性能。为了弥合这一差距，PatchCensor旨在通过检测和剔除不合规则的区域来提高整个系统的鲁棒性。",
    "tldr": "PatchCensor是一种用于视觉Transformer的补丁鲁棒性认证方法，基于全面测试并考虑最坏的补丁攻击情境，能够提供受保证的准确性。",
    "en_tdlr": "PatchCensor is a certified patch robustness method for Vision Transformers that relies on exhaustive testing and worst-case patch attack scenarios to provide guaranteed accuracy against arbitrary attacks."
}