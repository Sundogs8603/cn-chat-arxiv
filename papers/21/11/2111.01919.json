{
    "title": "Discovering and Exploiting Sparse Rewards in a Learned Behavior Space. (arXiv:2111.01919v2 [cs.LG] UPDATED)",
    "abstract": "Learning optimal policies in sparse rewards settings is difficult as the learning agent has little to no feedback on the quality of its actions. In these situations, a good strategy is to focus on exploration, hopefully leading to the discovery of a reward signal to improve on. A learning algorithm capable of dealing with this kind of settings has to be able to (1) explore possible agent behaviors and (2) exploit any possible discovered reward. Efficient exploration algorithms have been proposed that require to define a behavior space, that associates to an agent its resulting behavior in a space that is known to be worth exploring. The need to define this space is a limitation of these algorithms. In this work, we introduce STAX, an algorithm designed to learn a behavior space on-the-fly and to explore it while efficiently optimizing any reward discovered. It does so by separating the exploration and learning of the behavior space from the exploitation of the reward through an alterna",
    "link": "http://arxiv.org/abs/2111.01919",
    "context": "Title: Discovering and Exploiting Sparse Rewards in a Learned Behavior Space. (arXiv:2111.01919v2 [cs.LG] UPDATED)\nAbstract: Learning optimal policies in sparse rewards settings is difficult as the learning agent has little to no feedback on the quality of its actions. In these situations, a good strategy is to focus on exploration, hopefully leading to the discovery of a reward signal to improve on. A learning algorithm capable of dealing with this kind of settings has to be able to (1) explore possible agent behaviors and (2) exploit any possible discovered reward. Efficient exploration algorithms have been proposed that require to define a behavior space, that associates to an agent its resulting behavior in a space that is known to be worth exploring. The need to define this space is a limitation of these algorithms. In this work, we introduce STAX, an algorithm designed to learn a behavior space on-the-fly and to explore it while efficiently optimizing any reward discovered. It does so by separating the exploration and learning of the behavior space from the exploitation of the reward through an alterna",
    "path": "papers/21/11/2111.01919.json",
    "total_tokens": 847,
    "translated_title": "在学习行为空间中发现和利用稀疏奖励",
    "translated_abstract": "在稀疏奖励的环境中学习最优策略是困难的，因为学习代理几乎没有关于行为质量的反馈。在这种情况下，一个好的策略是专注于探索，希望能发现一个奖励信号以进行改进。一个能够处理这种情况的学习算法必须能够（1）探索可能的代理行为和（2）利用可能发现的任何奖励。已经提出了高效的探索算法，需要定义一个行为空间，将代理与其在可以探索的空间中表现出的行为相关联。需要定义这个空间是这些算法的一个限制。在这项工作中，我们介绍了STAX，一种设计用于实时学习行为空间并在有效地优化任何发现的奖励的算法。它通过将行为空间的探索和学习与奖励的利用分开，以替代的方式实现。",
    "tldr": "这项研究介绍了一种名为STAX的算法，能够在学习行为空间时实时探索，并且能够有效优化任何发现的奖励。"
}