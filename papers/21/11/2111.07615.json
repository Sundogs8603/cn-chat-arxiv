{
    "title": "Optimism and Delays in Episodic Reinforcement Learning. (arXiv:2111.07615v2 [cs.LG] UPDATED)",
    "abstract": "There are many algorithms for regret minimisation in episodic reinforcement learning. This problem is well-understood from a theoretical perspective, providing that the sequences of states, actions and rewards associated with each episode are available to the algorithm updating the policy immediately after every interaction with the environment. However, feedback is almost always delayed in practice. In this paper, we study the impact of delayed feedback in episodic reinforcement learning from a theoretical perspective and propose two general-purpose approaches to handling the delays. The first involves updating as soon as new information becomes available, whereas the second waits before using newly observed information to update the policy. For the class of optimistic algorithms and either approach, we show that the regret increases by an additive term involving the number of states, actions, episode length, the expected delay and an algorithm-dependent constant. We empirically inves",
    "link": "http://arxiv.org/abs/2111.07615",
    "context": "Title: Optimism and Delays in Episodic Reinforcement Learning. (arXiv:2111.07615v2 [cs.LG] UPDATED)\nAbstract: There are many algorithms for regret minimisation in episodic reinforcement learning. This problem is well-understood from a theoretical perspective, providing that the sequences of states, actions and rewards associated with each episode are available to the algorithm updating the policy immediately after every interaction with the environment. However, feedback is almost always delayed in practice. In this paper, we study the impact of delayed feedback in episodic reinforcement learning from a theoretical perspective and propose two general-purpose approaches to handling the delays. The first involves updating as soon as new information becomes available, whereas the second waits before using newly observed information to update the policy. For the class of optimistic algorithms and either approach, we show that the regret increases by an additive term involving the number of states, actions, episode length, the expected delay and an algorithm-dependent constant. We empirically inves",
    "path": "papers/21/11/2111.07615.json",
    "total_tokens": 892,
    "translated_title": "乐观算法与延迟的周期性强化学习",
    "translated_abstract": "周期性强化学习中有很多用于减少后悔的算法。从理论角度来看，问题已经得到了很好的理解，只要与每次与环境交互后立即更新策略的算法可以获取与每个情节相关的状态、动作和奖励序列。然而，在实践中，反馈几乎总是延迟的。本文从理论角度研究了延迟反馈对周期性强化学习的影响，并提出了两种通用方法来处理延迟。第一个方法涉及到在新信息可用时立即更新，而第二个方法则等待使用新观察到的信息来更新策略。针对乐观算法类和任一方法，我们表明后悔会增加一个与状态数量、动作数量、情节长度、预期延迟和算法相关常数的加性项。我们还进行了实证研究。",
    "tldr": "本文从理论角度探讨了延迟反馈对周期性强化学习的影响，提出了两种通用方法来处理延迟，并指出这两种方法针对乐观算法类后悔会增加一个与状态数量、动作数量、情节长度、预期延迟和算法相关常数的加性项。",
    "en_tdlr": "This paper theoretically explores the impact of delayed feedback on episodic reinforcement learning and proposes two general-purpose approaches to handling the delays. For the class of optimistic algorithms and either approach, an additive regret term involving the number of states, actions, episode length, expected delay, and algorithm-dependent constant is shown to increase."
}