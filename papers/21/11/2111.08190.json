{
    "title": "Learning Augmentation Distributions using Transformed Risk Minimization. (arXiv:2111.08190v2 [cs.LG] UPDATED)",
    "abstract": "We propose a new \\emph{Transformed Risk Minimization} (TRM) framework as an extension of classical risk minimization. In TRM, we optimize not only over predictive models, but also over data transformations; specifically over distributions thereof. As a key application, we focus on learning augmentations; for instance appropriate rotations of images, to improve classification performance with a given class of predictors. Our TRM method (1) jointly learns transformations and models in a \\emph{single training loop}, (2) works with any training algorithm applicable to standard risk minimization, and (3) handles any transforms, such as discrete and continuous classes of augmentations. To avoid overfitting when implementing empirical transformed risk minimization, we propose a novel regularizer based on PAC-Bayes theory. For learning augmentations of images, we propose a new parametrization of the space of augmentations via a stochastic composition of blocks of geometric transforms. This lea",
    "link": "http://arxiv.org/abs/2111.08190",
    "context": "Title: Learning Augmentation Distributions using Transformed Risk Minimization. (arXiv:2111.08190v2 [cs.LG] UPDATED)\nAbstract: We propose a new \\emph{Transformed Risk Minimization} (TRM) framework as an extension of classical risk minimization. In TRM, we optimize not only over predictive models, but also over data transformations; specifically over distributions thereof. As a key application, we focus on learning augmentations; for instance appropriate rotations of images, to improve classification performance with a given class of predictors. Our TRM method (1) jointly learns transformations and models in a \\emph{single training loop}, (2) works with any training algorithm applicable to standard risk minimization, and (3) handles any transforms, such as discrete and continuous classes of augmentations. To avoid overfitting when implementing empirical transformed risk minimization, we propose a novel regularizer based on PAC-Bayes theory. For learning augmentations of images, we propose a new parametrization of the space of augmentations via a stochastic composition of blocks of geometric transforms. This lea",
    "path": "papers/21/11/2111.08190.json",
    "total_tokens": 922,
    "translated_title": "使用转换风险最小化学习增强分布",
    "translated_abstract": "我们提出了一种新的“转换风险最小化”（TRM）框架，作为传统风险最小化的扩展。在TRM中，我们不仅优化预测模型，还优化数据变换，特别是分布的变换。作为一个关键应用，我们关注学习增强，例如适当旋转图像，以提高给定预测器类别的分类性能。我们的TRM方法：（1）在单个训练循环中联合学习变换和模型；（2）适用于任何适用于标准风险最小化的训练算法；（3）处理任何变换，例如离散和连续类的增强。为了避免在实施经验转换风险最小化时过拟合，我们提出了一种基于PAC-Bayes理论的新型正则化器。对于学习图像的增强，我们提出了一种通过几何变换块的随机组合对增强空间进行参数化的新方法。",
    "tldr": "我们提出了一种新的转换风险最小化框架，可以同时学习预测模型和数据变换，特别是分布的变换。我们以学习图像增强为主要应用，并提出了解决过拟合问题的正则化方法。",
    "en_tdlr": "We propose a novel Transformed Risk Minimization framework that optimizes both predictive models and data transformations, focusing on learning image augmentations. We introduce a regularizer based on PAC-Bayes theory to prevent overfitting and propose a new parametrization method for augmentations using geometric transform blocks."
}