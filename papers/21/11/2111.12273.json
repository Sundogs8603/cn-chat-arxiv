{
    "title": "Sharpness-aware Quantization for Deep Neural Networks. (arXiv:2111.12273v5 [cs.CV] UPDATED)",
    "abstract": "Network quantization is a dominant paradigm of model compression. However, the abrupt changes in quantized weights during training often lead to severe loss fluctuations and result in a sharp loss landscape, making the gradients unstable and thus degrading the performance. Recently, Sharpness-Aware Minimization (SAM) has been proposed to smooth the loss landscape and improve the generalization performance of the models. Nevertheless, directly applying SAM to the quantized models can lead to perturbation mismatch or diminishment issues, resulting in suboptimal performance. In this paper, we propose a novel method, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM in model compression, particularly quantization for the first time. Specifically, we first provide a unified view of quantization and SAM by treating them as introducing quantization noises and adversarial perturbations to the model weights, respectively. According to whether the noise and perturbation ter",
    "link": "http://arxiv.org/abs/2111.12273",
    "context": "Title: Sharpness-aware Quantization for Deep Neural Networks. (arXiv:2111.12273v5 [cs.CV] UPDATED)\nAbstract: Network quantization is a dominant paradigm of model compression. However, the abrupt changes in quantized weights during training often lead to severe loss fluctuations and result in a sharp loss landscape, making the gradients unstable and thus degrading the performance. Recently, Sharpness-Aware Minimization (SAM) has been proposed to smooth the loss landscape and improve the generalization performance of the models. Nevertheless, directly applying SAM to the quantized models can lead to perturbation mismatch or diminishment issues, resulting in suboptimal performance. In this paper, we propose a novel method, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM in model compression, particularly quantization for the first time. Specifically, we first provide a unified view of quantization and SAM by treating them as introducing quantization noises and adversarial perturbations to the model weights, respectively. According to whether the noise and perturbation ter",
    "path": "papers/21/11/2111.12273.json",
    "total_tokens": 1220,
    "translated_title": "针对深度神经网络的尖锐感知量化方法",
    "translated_abstract": "网络量化是模型压缩的主导方法。然而，量化训练中量化权重的突然变化通常会导致严重的损失波动，导致尖锐的损失地形，使梯度不稳定，从而降低性能。最近，提出了尖锐感知最小化（SAM）来平滑损失地形，并提高模型的泛化性能。然而，直接将SAM应用于量化模型可能会导致扰动不匹配或缩减问题，从而导致次优性能。在本文中，我们提出了一种新的方法，称为尖锐感知量化（SAQ），首次探索了SAM在模型压缩中的效果，特别是量化。具体而言，我们首先将量化和SAM视为分别引入模型权重的量化噪声和对抗扰动的方法，并提供了一个统一的视角。根据噪声和扰动术语是平滑还是尖锐，我们将其分为四类，并研究它们对模型性能的影响。基于这种理解，我们提出了SAQ，明确地利用SAM平滑尖锐的量化噪声和扰动，从而增强了模型的鲁棒性，同时保持了高压缩性。广泛的实验表明，SAQ始终优于现有的最先进的量化方法，比最接近的竞争对手在ImageNet上的精度高达3.6％。",
    "tldr": "本文提出了一种新的方法，称为尖锐感知量化（SAQ），它利用尖锐感知最小化（SAM）来平滑尖锐的量化噪声和扰动，增强了模型的鲁棒性，同时保持了高压缩性。实验结果表明，SAQ 在 ImageNet 上的精度比现有的最先进的量化方法高出3.6％。",
    "en_tdlr": "This paper proposes a novel method, Sharpness-Aware Quantization (SAQ), which utilizes Sharpness-Aware Minimization (SAM) to smooth the sharp quantization noises and perturbations, enhancing the model robustness while maintaining high compressibility. Experimental results show that SAQ consistently outperforms existing state-of-the-art quantization methods, achieving up to 3.6% higher accuracy on ImageNet than the closest competitor."
}