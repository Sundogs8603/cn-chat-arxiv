{
    "title": "Neural networks with linear threshold activations: structure and algorithms. (arXiv:2111.08117v3 [cs.LG] UPDATED)",
    "abstract": "In this article we present new results on neural networks with linear threshold activation functions. We precisely characterize the class of functions that are representable by such neural networks and show that 2 hidden layers are necessary and sufficient to represent any function representable in the class. This is a surprising result in the light of recent exact representability investigations for neural networks using other popular activation functions like rectified linear units (ReLU). We also give precise bounds on the sizes of the neural networks required to represent any function in the class. Finally, we design an algorithm to solve the empirical risk minimization (ERM) problem to global optimality for these neural networks with a fixed architecture. The algorithm's running time is polynomial in the size of the data sample, if the input dimension and the size of the network architecture are considered fixed constants. The algorithm is unique in the sense that it works for any",
    "link": "http://arxiv.org/abs/2111.08117",
    "context": "Title: Neural networks with linear threshold activations: structure and algorithms. (arXiv:2111.08117v3 [cs.LG] UPDATED)\nAbstract: In this article we present new results on neural networks with linear threshold activation functions. We precisely characterize the class of functions that are representable by such neural networks and show that 2 hidden layers are necessary and sufficient to represent any function representable in the class. This is a surprising result in the light of recent exact representability investigations for neural networks using other popular activation functions like rectified linear units (ReLU). We also give precise bounds on the sizes of the neural networks required to represent any function in the class. Finally, we design an algorithm to solve the empirical risk minimization (ERM) problem to global optimality for these neural networks with a fixed architecture. The algorithm's running time is polynomial in the size of the data sample, if the input dimension and the size of the network architecture are considered fixed constants. The algorithm is unique in the sense that it works for any",
    "path": "papers/21/11/2111.08117.json",
    "total_tokens": 990,
    "translated_title": "具有线性阈值激活的神经网络：结构与算法",
    "translated_abstract": "本文中我们介绍了关于具有线性阈值激活函数的神经网络的新结果。我们精确地描述了可以由这样的神经网络表示的函数类，并且证明了用2个隐藏层表示该类中的任何可表示函数既是必要的也是充分的。这是一个令人惊讶的结果，考虑到最近使用其他流行激活函数如修正线性单元（ReLU）进行神经网络精确可表示性研究。我们还给出了表示该类中任何函数所需的神经网络大小的精确界限。最后，我们设计了一种算法来解决具有固定架构的这些神经网络的经验风险最小化（ERM）问题，以全局最优性。如果将输入维度和网络架构的大小视为固定常数，则该算法的运行时间是多项式时间。该算法在任何情况下都可使用。",
    "tldr": "本文研究了具有线性阈值激活函数的神经网络。我们确定了这类神经网络可以表示的函数的特点，并发现使用两个隐藏层可以表示该类中的任何函数。我们还给出了表示这类函数所需神经网络大小的界限，并设计了一个能够解决具有固定架构的这类神经网络的经验风险最小化问题的算法。该算法的运行时间与数据样本大小呈多项式关系，而输入维度和网络架构的大小被认为是固定常数。",
    "en_tdlr": "This article presents new results on neural networks with linear threshold activation functions. It characterizes the class of functions that can be represented by such networks and shows that 2 hidden layers are necessary and sufficient to represent any function in the class. The study also provides bounds on the network sizes required for representation. Additionally, an algorithm is designed to solve the empirical risk minimization problem for these networks, with a polynomial running time relative to the data sample size and fixed input dimension and network architecture."
}