{
    "title": "PredProp: Bidirectional Stochastic Optimization with Precision Weighted Predictive Coding. (arXiv:2111.08792v2 [cs.LG] UPDATED)",
    "abstract": "We present PredProp, a method for optimization of weights and states in predictive coding networks (PCNs) based on the precision of propagated errors and neural activity. PredProp jointly addresses inference and learning via stochastic gradient descent and adaptively weights parameter updates by approximate curvature. Due to the relation between propagated error covariance and the Fisher information matrix, PredProp implements approximate Natural Gradient Descent. We demonstrate PredProp's effectiveness in the context of dense decoder networks and simple image benchmark datasets. We found that PredProp performs favorably over Adam, a widely used adaptive learning rate optimizer in the tested configurations. Furthermore, available optimization methods for weight parameters benefit from using PredProp's error precision during inference. Since hierarchical predictive coding layers are optimised individually using local errors, the required precisions factorize over hierarchical layers. Ex",
    "link": "http://arxiv.org/abs/2111.08792",
    "context": "Title: PredProp: Bidirectional Stochastic Optimization with Precision Weighted Predictive Coding. (arXiv:2111.08792v2 [cs.LG] UPDATED)\nAbstract: We present PredProp, a method for optimization of weights and states in predictive coding networks (PCNs) based on the precision of propagated errors and neural activity. PredProp jointly addresses inference and learning via stochastic gradient descent and adaptively weights parameter updates by approximate curvature. Due to the relation between propagated error covariance and the Fisher information matrix, PredProp implements approximate Natural Gradient Descent. We demonstrate PredProp's effectiveness in the context of dense decoder networks and simple image benchmark datasets. We found that PredProp performs favorably over Adam, a widely used adaptive learning rate optimizer in the tested configurations. Furthermore, available optimization methods for weight parameters benefit from using PredProp's error precision during inference. Since hierarchical predictive coding layers are optimised individually using local errors, the required precisions factorize over hierarchical layers. Ex",
    "path": "papers/21/11/2111.08792.json",
    "total_tokens": 917,
    "translated_title": "PredProp: 带精度加权预测编码的双向随机优化方法",
    "translated_abstract": "本文提出了PredProp方法，它基于传播的误差和神经活动的精度，通过随机梯度下降和自适应加权参数更新来优化预测编码网络(PCN)中的权重和状态。由于传播误差协方差与Fisher信息矩阵之间的关系，PredProp实现了近似自然梯度下降。我们在密集解码器网络和简单图像基准数据集的情况下展示了PredProp的有效性。我们发现，在测试配置中，PredProp表现优于Adam，一种广泛使用的自适应学习率优化器。此外，可用于权重参数的优化方法在推理过程中受益于使用PredProp的误差精度。由于层次预测编码层是通过局部误差单独优化的，所以所需精度在层次结构上分解。",
    "tldr": "本文提出了PredProp方法，它通过随机梯度下降和自适应加权参数更新来优化预测编码网络中的权重和状态。通过使用传播误差协方差实现近似自然梯度下降，使得PredProp在测试中表现良好，比Adam表现更优。此外，PredProp可以通过误差精度提高权重参数的优化效果，并且在层次结构中可以分解精度需求。",
    "en_tdlr": "PredProp is proposed as a bidirectional stochastic optimization method for predictive coding networks, which adaptively weights parameter updates by precision and implements approximate natural gradient descent. It outperforms Adam on simple image benchmarks and can improve weight parameter optimization by using error precision. Its hierarchical structure allows precision requirements to factorize over layers."
}