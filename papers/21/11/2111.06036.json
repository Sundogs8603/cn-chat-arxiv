{
    "title": "CubeTR: Learning to Solve The Rubiks Cube Using Transformers. (arXiv:2111.06036v2 [cs.LG] UPDATED)",
    "abstract": "Since its first appearance, transformers have been successfully used in wide ranging domains from computer vision to natural language processing. Application of transformers in Reinforcement Learning by reformulating it as a sequence modelling problem was proposed only recently. Compared to other commonly explored reinforcement learning problems, the Rubiks cube poses a unique set of challenges. The Rubiks cube has a single solved state for quintillions of possible configurations which leads to extremely sparse rewards. The proposed model CubeTR attends to longer sequences of actions and addresses the problem of sparse rewards. CubeTR learns how to solve the Rubiks cube from arbitrary starting states without any human prior, and after move regularisation, the lengths of solutions generated by it are expected to be very close to those given by algorithms used by expert human solvers. CubeTR provides insights to the generalisability of learning algorithms to higher dimensional cubes and ",
    "link": "http://arxiv.org/abs/2111.06036",
    "context": "Title: CubeTR: Learning to Solve The Rubiks Cube Using Transformers. (arXiv:2111.06036v2 [cs.LG] UPDATED)\nAbstract: Since its first appearance, transformers have been successfully used in wide ranging domains from computer vision to natural language processing. Application of transformers in Reinforcement Learning by reformulating it as a sequence modelling problem was proposed only recently. Compared to other commonly explored reinforcement learning problems, the Rubiks cube poses a unique set of challenges. The Rubiks cube has a single solved state for quintillions of possible configurations which leads to extremely sparse rewards. The proposed model CubeTR attends to longer sequences of actions and addresses the problem of sparse rewards. CubeTR learns how to solve the Rubiks cube from arbitrary starting states without any human prior, and after move regularisation, the lengths of solutions generated by it are expected to be very close to those given by algorithms used by expert human solvers. CubeTR provides insights to the generalisability of learning algorithms to higher dimensional cubes and ",
    "path": "papers/21/11/2111.06036.json",
    "total_tokens": 892,
    "translated_title": "CubeTR: 使用Transformer学习解决魔方问题",
    "translated_abstract": "自从Transformer首次出现以来，已经成功地应用于计算机视觉到自然语言处理等各种领域。最近提出将Transformer应用于强化学习，将其重新定义为序列建模问题。与其他常见的强化学习问题相比，魔方问题具有独特的挑战。魔方有着数以千万计的可能组合，但只有一个解决的状态，这导致了极度稀疏的奖励信号。提出的模型CubeTR关注于长序列的动作，并解决了稀疏奖励的问题。CubeTR能够从任意起始状态学习如何解决魔方问题，经过移动规范化后，生成的解决方案长度预期将非常接近专业人员使用的算法给出的解决方案长度。CubeTR为学习算法在更高维度魔方上的泛化能力提供了深入洞见。",
    "tldr": "CubeTR提出了一种使用Transformer进行强化学习解决魔方问题的方法，并通过关注长序列动作和解决稀疏奖励的问题，实现了从任意起始状态学习如何解决魔方问题，并能够生成接近专业人员解决方案长度的解决方案。",
    "en_tdlr": "CubeTR proposes a method to solve the Rubiks cube using Transformers in reinforcement learning, addressing the challenges of sparse rewards and learning from arbitrary starting states. Its generated solutions are expected to be close to those of expert human solvers, providing insights into the generalizability of learning algorithms in higher dimensional cubes."
}