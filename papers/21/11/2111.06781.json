{
    "title": "Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity. (arXiv:2111.06781v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning algorithms often require finiteness of state and action spaces in Markov decision processes (MDPs) (also called controlled Markov chains) and various efforts have been made in the literature towards the applicability of such algorithms for continuous state and action spaces. In this paper, we show that under very mild regularity conditions (in particular, involving only weak continuity of the transition kernel of an MDP), Q-learning for standard Borel MDPs via quantization of states and actions (called Quantized Q-Learning) converges to a limit, and furthermore this limit satisfies an optimality equation which leads to near optimality with either explicit performance bounds or which are guaranteed to be asymptotically optimal. Our approach builds on (i) viewing quantization as a measurement kernel and thus a quantized MDP as a partially observed Markov decision process (POMDP), (ii) utilizing near optimality and convergence results of Q-learning for POMDPs, and (",
    "link": "http://arxiv.org/abs/2111.06781",
    "context": "Title: Q-Learning for MDPs with General Spaces: Convergence and Near Optimality via Quantization under Weak Continuity. (arXiv:2111.06781v2 [cs.LG] UPDATED)\nAbstract: Reinforcement learning algorithms often require finiteness of state and action spaces in Markov decision processes (MDPs) (also called controlled Markov chains) and various efforts have been made in the literature towards the applicability of such algorithms for continuous state and action spaces. In this paper, we show that under very mild regularity conditions (in particular, involving only weak continuity of the transition kernel of an MDP), Q-learning for standard Borel MDPs via quantization of states and actions (called Quantized Q-Learning) converges to a limit, and furthermore this limit satisfies an optimality equation which leads to near optimality with either explicit performance bounds or which are guaranteed to be asymptotically optimal. Our approach builds on (i) viewing quantization as a measurement kernel and thus a quantized MDP as a partially observed Markov decision process (POMDP), (ii) utilizing near optimality and convergence results of Q-learning for POMDPs, and (",
    "path": "papers/21/11/2111.06781.json",
    "total_tokens": 988,
    "translated_title": "Q-Learning用于具有一般状态空间的MDPs: 通过弱连续性下的量化来实现收敛和近似最优性",
    "translated_abstract": "强化学习算法通常要求马尔可夫决策过程(MDPs)中的状态和动作空间是有限的（也称为可控马尔可夫链），文献中已经做出了各种努力，以便将这些算法应用于连续的状态和动作空间。在本文中，我们证明在非常温和的正则性条件下（特别是只涉及MDP的过渡核的弱连续性），通过对状态和动作进行量化的标准 Borel MDPs 的 Q-Learning（称为Quantized Q-Learning）会收敛到一个极限，并且这个极限满足一个最优性方程，从而实现近似最优性，要么具有显式的性能界限，要么具有渐近最优保证。我们的方法基于：(i) 将量化视为一个测量核，并将量化的MDP视为一个部分观测的马尔可夫决策过程（POMDP），(ii) 利用 Q-Learning 对 POMDP 的近似最优性和收敛性结果。",
    "tldr": "本文研究了在具有连续状态和动作空间的MDPs中使用Q-Learning的收敛和近似最优性问题。通过量化状态和动作，我们证明Quantized Q-Learning会收敛到一个极限，并且这个极限满足一个最优性方程，从而实现近似最优性。",
    "en_tdlr": "This paper investigates the convergence and near optimality of Q-Learning in MDPs with continuous state and action spaces. By quantizing states and actions, we demonstrate that Quantized Q-Learning converges to a limit that satisfies an optimality equation, leading to near optimality."
}