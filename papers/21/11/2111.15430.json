{
    "title": "The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v4 [cs.CV] UPDATED)",
    "abstract": "In spite of the dominant performances of deep neural networks, recent works have shown that they are poorly calibrated, resulting in over-confident predictions. Miscalibration can be exacerbated by overfitting due to the minimization of the cross-entropy during training, as it promotes the predicted softmax probabilities to match the one-hot label assignments. This yields a pre-softmax activation of the correct class that is significantly larger than the remaining activations. Recent evidence from the literature suggests that loss functions that embed implicit or explicit maximization of the entropy of predictions yield state-of-the-art calibration performances. We provide a unifying constrained-optimization perspective of current state-of-the-art calibration losses. Specifically, these losses could be viewed as approximations of a linear penalty (or a Lagrangian) imposing equality constraints on logit distances. This points to an important limitation of such underlying equality constr",
    "link": "http://arxiv.org/abs/2111.15430",
    "context": "Title: The Devil is in the Margin: Margin-based Label Smoothing for Network Calibration. (arXiv:2111.15430v4 [cs.CV] UPDATED)\nAbstract: In spite of the dominant performances of deep neural networks, recent works have shown that they are poorly calibrated, resulting in over-confident predictions. Miscalibration can be exacerbated by overfitting due to the minimization of the cross-entropy during training, as it promotes the predicted softmax probabilities to match the one-hot label assignments. This yields a pre-softmax activation of the correct class that is significantly larger than the remaining activations. Recent evidence from the literature suggests that loss functions that embed implicit or explicit maximization of the entropy of predictions yield state-of-the-art calibration performances. We provide a unifying constrained-optimization perspective of current state-of-the-art calibration losses. Specifically, these losses could be viewed as approximations of a linear penalty (or a Lagrangian) imposing equality constraints on logit distances. This points to an important limitation of such underlying equality constr",
    "path": "papers/21/11/2111.15430.json",
    "total_tokens": 895,
    "translated_title": "边际的魔鬼：基于边际的标签平滑用于网络校准",
    "translated_abstract": "尽管深度神经网络有着卓越的性能，但最近的研究表明它们往往校准不良，导致过于自信的预测。过拟合会加剧校准不良现象，因为在训练过程中，交叉熵的最小化会促使预测的 softmax 概率与独热标签分配相匹配，从而使得正确类别的预 softmax 激活远大于其他激活。最近的文献证据表明，蕴含隐式或显式最大熵预测的损失函数能够实现最先进的校准性能。我们提供了目前最先进校准损失的统一约束最优化观点。具体而言，这些损失可以被视为对位势距离施加相等约束的线性惩罚（或拉格朗日函数）的近似。这指出了这种底层相等约束的重要局限性。",
    "tldr": "这篇论文研究了深度神经网络的校准问题，发现传统的交叉熵损失会导致过度自信的预测。通过引入最大熵预测的损失函数，可以提高网络的校准性能。",
    "en_tdlr": "This paper investigates the calibration problem of deep neural networks and finds that traditional cross-entropy loss leads to over-confident predictions. By introducing loss functions with maximum entropy predictions, the calibration performance of the network can be improved."
}