{
    "title": "Oracle Teacher: Leveraging Target Information for Better Knowledge Distillation of CTC Models. (arXiv:2111.03664v4 [cs.LG] UPDATED)",
    "abstract": "Knowledge distillation (KD), best known as an effective method for model compression, aims at transferring the knowledge of a bigger network (teacher) to a much smaller network (student). Conventional KD methods usually employ the teacher model trained in a supervised manner, where output labels are treated only as targets. Extending this supervised scheme further, we introduce a new type of teacher model for connectionist temporal classification (CTC)-based sequence models, namely Oracle Teacher, that leverages both the source inputs and the output labels as the teacher model's input. Since the Oracle Teacher learns a more accurate CTC alignment by referring to the target information, it can provide the student with more optimal guidance. One potential risk for the proposed approach is a trivial solution that the model's output directly copies the target input. Based on a many-to-one mapping property of the CTC algorithm, we present a training strategy that can effectively prevent the",
    "link": "http://arxiv.org/abs/2111.03664",
    "context": "Title: Oracle Teacher: Leveraging Target Information for Better Knowledge Distillation of CTC Models. (arXiv:2111.03664v4 [cs.LG] UPDATED)\nAbstract: Knowledge distillation (KD), best known as an effective method for model compression, aims at transferring the knowledge of a bigger network (teacher) to a much smaller network (student). Conventional KD methods usually employ the teacher model trained in a supervised manner, where output labels are treated only as targets. Extending this supervised scheme further, we introduce a new type of teacher model for connectionist temporal classification (CTC)-based sequence models, namely Oracle Teacher, that leverages both the source inputs and the output labels as the teacher model's input. Since the Oracle Teacher learns a more accurate CTC alignment by referring to the target information, it can provide the student with more optimal guidance. One potential risk for the proposed approach is a trivial solution that the model's output directly copies the target input. Based on a many-to-one mapping property of the CTC algorithm, we present a training strategy that can effectively prevent the",
    "path": "papers/21/11/2111.03664.json",
    "total_tokens": 861,
    "translated_title": "Oracle Teacher: 利用目标信息更好地进行基于CTC模型的知识蒸馏",
    "translated_abstract": "知识蒸馏（KD）是一种有效的模型压缩方法，旨在将更大的网络（教师）的知识传递给更小的网络（学生）。传统的KD方法通常使用以监督方式训练的教师模型，其中输出标签仅被视为目标。在进一步扩展这种监督方案的基础上，我们引入了一种针对基于连接主义时序分类（CTC）的序列模型的新类型教师模型，即Oracle Teacher，该模型利用源输入和输出标签作为教师模型的输入。由于Oracle Teacher通过参考目标信息学习更准确的CTC对齐，因此它可以为学生提供更优化的指导。所提出方法的一个潜在风险是模型的输出直接复制目标输入。基于CTC算法的多对一映射特性，我们提出了一种训练策略，可以有效防止这种情况的发生。",
    "tldr": "Oracle Teacher是一种新型教师模型，利用基于CTC的序列模型的目标信息来提供更准确的指导，以实现更好的知识蒸馏效果。",
    "en_tdlr": "Oracle Teacher is a new type of teacher model that leverages the target information of CTC-based sequence models to provide more accurate guidance, resulting in improved knowledge distillation."
}