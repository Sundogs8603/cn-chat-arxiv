{
    "title": "Distributed Sparse Regression via Penalization. (arXiv:2111.06530v2 [cs.LG] UPDATED)",
    "abstract": "We study sparse linear regression over a network of agents, modeled as an undirected graph (with no centralized node). The estimation problem is formulated as the minimization of the sum of the local LASSO loss functions plus a quadratic penalty of the consensus constraint -- the latter being instrumental to obtain distributed solution methods. While penalty-based consensus methods have been extensively studied in the optimization literature, their statistical and computational guarantees in the high dimensional setting remain unclear. This work provides an answer to this open problem. Our contribution is two-fold. First, we establish statistical consistency of the estimator: under a suitable choice of the penalty parameter, the optimal solution of the penalized problem achieves near optimal minimax rate $\\mathcal{O}(s \\log d/N)$ in $\\ell_2$-loss, where $s$ is the sparsity value, $d$ is the ambient dimension, and $N$ is the total sample size in the network -- this matches centralized s",
    "link": "http://arxiv.org/abs/2111.06530",
    "context": "Title: Distributed Sparse Regression via Penalization. (arXiv:2111.06530v2 [cs.LG] UPDATED)\nAbstract: We study sparse linear regression over a network of agents, modeled as an undirected graph (with no centralized node). The estimation problem is formulated as the minimization of the sum of the local LASSO loss functions plus a quadratic penalty of the consensus constraint -- the latter being instrumental to obtain distributed solution methods. While penalty-based consensus methods have been extensively studied in the optimization literature, their statistical and computational guarantees in the high dimensional setting remain unclear. This work provides an answer to this open problem. Our contribution is two-fold. First, we establish statistical consistency of the estimator: under a suitable choice of the penalty parameter, the optimal solution of the penalized problem achieves near optimal minimax rate $\\mathcal{O}(s \\log d/N)$ in $\\ell_2$-loss, where $s$ is the sparsity value, $d$ is the ambient dimension, and $N$ is the total sample size in the network -- this matches centralized s",
    "path": "papers/21/11/2111.06530.json",
    "total_tokens": 905,
    "translated_title": "基于惩罚项的分布式稀疏回归",
    "translated_abstract": "本文研究了在模拟为无中心节点的无向图的代理网络上进行的稀疏线性回归。估计问题被制定为将局部LASSO损失函数的总和与共识约束的二次惩罚相结合的最小化--后者对于获得分布式解决方法至关重要。虽然基于惩罚项的共识方法在优化文献中得到了广泛研究，但在高维设置中它们的统计和计算保证还不清楚。 这项工作提供了这个开放问题的答案。我们的贡献有两个。首先，我们建立了估计器的统计一致性：在合适的惩罚参数选择下，惩罚问题的最优解在$\\ell_2$- 损失中以接近最优最小率 $\\mathcal{O}(s \\log d/N)$ 获得 ，其中 $s$是稀疏值，$d$是环境维数，$N$是网络中的总样本大小--这与集中式的结果相匹配。",
    "tldr": "本文研究了基于惩罚项的分布式稀疏线性回归问题，通过建立统计一致性证明了最优解在$\\ell_2$-损失中以接近最优最小率获得。",
    "en_tdlr": "This paper studies the problem of distributed sparse linear regression with penalization, and proves statistical consistency of the estimator achieving near optimal minimax rate in $\\ell_2$-loss through the establishment of penalty-based consensus methods."
}