{
    "title": "Variational Gibbs Inference for Statistical Model Estimation from Incomplete Data. (arXiv:2111.13180v4 [cs.LG] UPDATED)",
    "abstract": "Statistical models are central to machine learning with broad applicability across a range of downstream tasks. The models are controlled by free parameters that are typically estimated from data by maximum-likelihood estimation or approximations thereof. However, when faced with real-world data sets many of the models run into a critical issue: they are formulated in terms of fully-observed data, whereas in practice the data sets are plagued with missing data. The theory of statistical model estimation from incomplete data is conceptually similar to the estimation of latent-variable models, where powerful tools such as variational inference (VI) exist. However, in contrast to standard latent-variable models, parameter estimation with incomplete data often requires estimating exponentially-many conditional distributions of the missing variables, hence making standard VI methods intractable. We address this gap by introducing variational Gibbs inference (VGI), a new general-purpose meth",
    "link": "http://arxiv.org/abs/2111.13180",
    "context": "Title: Variational Gibbs Inference for Statistical Model Estimation from Incomplete Data. (arXiv:2111.13180v4 [cs.LG] UPDATED)\nAbstract: Statistical models are central to machine learning with broad applicability across a range of downstream tasks. The models are controlled by free parameters that are typically estimated from data by maximum-likelihood estimation or approximations thereof. However, when faced with real-world data sets many of the models run into a critical issue: they are formulated in terms of fully-observed data, whereas in practice the data sets are plagued with missing data. The theory of statistical model estimation from incomplete data is conceptually similar to the estimation of latent-variable models, where powerful tools such as variational inference (VI) exist. However, in contrast to standard latent-variable models, parameter estimation with incomplete data often requires estimating exponentially-many conditional distributions of the missing variables, hence making standard VI methods intractable. We address this gap by introducing variational Gibbs inference (VGI), a new general-purpose meth",
    "path": "papers/21/11/2111.13180.json",
    "total_tokens": 927,
    "translated_title": "不完全数据统计模型估计的变分吉布斯推断",
    "translated_abstract": "统计模型在机器学习中具有广泛的适用性，可用于各种下游任务。这些模型由自由参数控制，通常通过最大似然估计或其近似方法从数据中估计。然而，当面对真实世界的数据集时，许多模型都会遇到一个关键问题：它们是以完全观测的数据为基础的，而实际上数据集中存在缺失数据。从不完全数据中进行统计模型估计的理论在概念上类似于潜变量模型的估计，其中存在诸如变分推断（VI）之类的强大工具。然而，与标准的潜变量模型不同，使用不完全数据的参数估计通常需要估计指数多个缺失变量的条件分布，因此使得标准的VI方法难以处理。我们通过引入变分吉布斯推断（VGI），一种新的通用方法，来填补这一差距。",
    "tldr": "这项研究提出了一种名为变分吉布斯推断（VGI）的方法，用于解决使用不完全数据进行统计模型估计时的挑战。与标准的潜变量模型不同，VGI能够处理估计指数多个缺失变量的条件分布，从而为实际的数据集提供了更准确的模型估计。",
    "en_tdlr": "This research introduces a method called Variational Gibbs Inference (VGI) to address the challenges of statistical model estimation using incomplete data. Unlike standard latent-variable models, VGI can handle the estimation of exponentially-many conditional distributions of missing variables, providing more accurate model estimation for real-world datasets."
}