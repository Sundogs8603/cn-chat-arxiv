{
    "title": "Combined Scaling for Zero-shot Transfer Learning. (arXiv:2111.10050v3 [cs.LG] UPDATED)",
    "abstract": "We present a combined scaling method - named BASIC - that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than",
    "link": "http://arxiv.org/abs/2111.10050",
    "context": "Title: Combined Scaling for Zero-shot Transfer Learning. (arXiv:2111.10050v3 [cs.LG] UPDATED)\nAbstract: We present a combined scaling method - named BASIC - that achieves 85.7% top-1 accuracy on the ImageNet ILSVRC-2012 validation set without learning from any labeled ImageNet example. This accuracy surpasses best published similar models - CLIP and ALIGN - by 9.3%. Our BASIC model also shows significant improvements in robustness benchmarks. For instance, on 5 test sets with natural distribution shifts such as ImageNet-{A,R,V2,Sketch} and ObjectNet, our model achieves 84.3% top-1 average accuracy, only a small drop from its original ImageNet accuracy. To achieve these results, we scale up the contrastive learning framework of CLIP and ALIGN in three dimensions: data size, model size, and batch size. Our dataset has 6.6B noisy image-text pairs, which is 4x larger than ALIGN, and 16x larger than CLIP. Our largest model has 3B weights, which is 3.75x larger in parameters and 8x larger in FLOPs than ALIGN and CLIP. Finally, our batch size is 65536 which is 2x more than CLIP and 4x more than",
    "path": "papers/21/11/2111.10050.json",
    "total_tokens": 1090,
    "translated_title": "零样本迁移学习中的综合缩放技术",
    "translated_abstract": "我们提出了一种综合缩放方法（称为 BASIC），在不学习任何标记的 ImageNet 示例的情况下，该方法在 ImageNet ILSVRC-2012 验证集上实现了85.7%的 top-1 准确率。该准确率超过了最佳发布的类似模型（CLIP 和 ALIGN）9.3%。我们的 BASIC 模型还在稳健性基准测试中表现出了显著的改进。例如，在 5 个具有自然分布偏移的测试集（例如 ImageNet-{A,R,V2, Sketch} 和 ObjectNet）上，我们的模型实现了84.3% 的 top-1 平均准确率，只有一个小小的跌落，与其原始的 ImageNet 准确率相比。为了实现这些结果，我们在三个维度上放大了 CLIP 和 ALIGN 的对比学习框架：数据大小，模型大小和批量大小。我们的数据集拥有 66 亿个带有噪声的图像-文本对，比 ALIGN 大 4 倍，比 CLIP 大 16 倍。我们最大的模型有 30 亿个权重，参数比 ALIGN 和 CLIP 多出 3.75 倍，FLOPs 多出 8 倍。最后，我们的批处理大小为 65536，比 CLIP 多 2 倍，比 ALIGN 多 4 倍。",
    "tldr": "提出了一种名为 BASIC 的综合缩放方法，在零样本迁移学习任务中实现了85.7%的高准确率，并在稳健性基准测试中表现出显着的改进。为了实现这些结果，该方法在数据大小、模型大小和批量大小三个维度上对比学习框架进行了放大。"
}