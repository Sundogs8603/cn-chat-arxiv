{
    "title": "TraVLR: Now You See It, Now You Don't! A Bimodal Dataset for Evaluating Visio-Linguistic Reasoning. (arXiv:2111.10756v3 [cs.CL] UPDATED)",
    "abstract": "Numerous visio-linguistic (V+L) representation learning methods have been developed, yet existing datasets do not adequately evaluate the extent to which they represent visual and linguistic concepts in a unified space. We propose several novel evaluation settings for V+L models, including cross-modal transfer. Furthermore, existing V+L benchmarks often report global accuracy scores on the entire dataset, making it difficult to pinpoint the specific reasoning tasks that models fail and succeed at. We present TraVLR, a synthetic dataset comprising four V+L reasoning tasks. TraVLR's synthetic nature allows us to constrain its training and testing distributions along task-relevant dimensions, enabling the evaluation of out-of-distribution generalisation. Each example in TraVLR redundantly encodes the scene in two modalities, allowing either to be dropped or added during training or testing without losing relevant information. We compare the performance of four state-of-the-art V+L models,",
    "link": "http://arxiv.org/abs/2111.10756",
    "context": "Title: TraVLR: Now You See It, Now You Don't! A Bimodal Dataset for Evaluating Visio-Linguistic Reasoning. (arXiv:2111.10756v3 [cs.CL] UPDATED)\nAbstract: Numerous visio-linguistic (V+L) representation learning methods have been developed, yet existing datasets do not adequately evaluate the extent to which they represent visual and linguistic concepts in a unified space. We propose several novel evaluation settings for V+L models, including cross-modal transfer. Furthermore, existing V+L benchmarks often report global accuracy scores on the entire dataset, making it difficult to pinpoint the specific reasoning tasks that models fail and succeed at. We present TraVLR, a synthetic dataset comprising four V+L reasoning tasks. TraVLR's synthetic nature allows us to constrain its training and testing distributions along task-relevant dimensions, enabling the evaluation of out-of-distribution generalisation. Each example in TraVLR redundantly encodes the scene in two modalities, allowing either to be dropped or added during training or testing without losing relevant information. We compare the performance of four state-of-the-art V+L models,",
    "path": "papers/21/11/2111.10756.json",
    "total_tokens": 942,
    "translated_title": "TraVLR: 现在你看到它了，现在你没看到它了！一个用于评估视觉语言推理的双模数据集。",
    "translated_abstract": "已经开发了许多视觉语言（V+L）表示学习方法，但现有的数据集不能充分评估它们在统一空间中表示视觉和语言概念的程度。我们针对V+L模型提出了几种新颖的评估设置，包括跨模态传递。此外，现有的V+L基准经常报告整个数据集的全局准确性得分，这使得难以确定模型失败和成功的具体推理任务。我们提出了TraVLR，这是一个合成数据集，包括四个V+L推理任务。TraVLR的合成性质使我们能够沿任务相关维度限制其训练和测试分布，从而评估超出分布的泛化。TraVLR中的每个示例都以两种模态冗余编码场景，使得在训练或测试期间可以删除或添加其中的任一模态而不会失去相关信息。我们比较了四个最先进的V+L模型的性能。",
    "tldr": "提出了TraVLR数据集，可以用于评估V+L模型的表现，数据集合成，包括四个V+L推理任务，同时使用双模式冗余编码来评估其泛化能力。"
}