{
    "title": "Convergence and Stability of the Stochastic Proximal Point Algorithm with Momentum. (arXiv:2111.06171v5 [math.OC] UPDATED)",
    "abstract": "Stochastic gradient descent with momentum (SGDM) is the dominant algorithm in many optimization scenarios, including convex optimization instances and non-convex neural network training. Yet, in the stochastic setting, momentum interferes with gradient noise, often leading to specific step size and momentum choices in order to guarantee convergence, set aside acceleration. Proximal point methods, on the other hand, have gained much attention due to their numerical stability and elasticity against imperfect tuning. Their stochastic accelerated variants though have received limited attention: how momentum interacts with the stability of (stochastic) proximal point methods remains largely unstudied. To address this, we focus on the convergence and stability of the stochastic proximal point algorithm with momentum (SPPAM), and show that SPPAM allows a faster linear convergence to a neighborhood compared to the stochastic proximal point algorithm (SPPA) with a better contraction factor, und",
    "link": "http://arxiv.org/abs/2111.06171",
    "context": "Title: Convergence and Stability of the Stochastic Proximal Point Algorithm with Momentum. (arXiv:2111.06171v5 [math.OC] UPDATED)\nAbstract: Stochastic gradient descent with momentum (SGDM) is the dominant algorithm in many optimization scenarios, including convex optimization instances and non-convex neural network training. Yet, in the stochastic setting, momentum interferes with gradient noise, often leading to specific step size and momentum choices in order to guarantee convergence, set aside acceleration. Proximal point methods, on the other hand, have gained much attention due to their numerical stability and elasticity against imperfect tuning. Their stochastic accelerated variants though have received limited attention: how momentum interacts with the stability of (stochastic) proximal point methods remains largely unstudied. To address this, we focus on the convergence and stability of the stochastic proximal point algorithm with momentum (SPPAM), and show that SPPAM allows a faster linear convergence to a neighborhood compared to the stochastic proximal point algorithm (SPPA) with a better contraction factor, und",
    "path": "papers/21/11/2111.06171.json",
    "total_tokens": 912,
    "translated_title": "带有动量的随机近端点算法的收敛性和稳定性研究",
    "translated_abstract": "随机梯度下降与动量（SGDM）是许多优化场景中的主要算法，包括凸优化和非凸神经网络训练。然而，在随机设置中，动量会干扰梯度噪声，通常需要特定的步长和动量选择才能保证收敛，但加速往往被忽略。相对而言，近端点方法因其数值稳定性和对不完美调整的弹性而受到了广泛关注。然而，它们的随机加速变体却受到了限制的关注：动量如何影响（随机）近端点方法的稳定性仍然未经研究。为了解决这个问题，我们关注带有动量的随机近端点算法（SPPAM）的收敛性和稳定性，并展示了SPPAM相比于带有更好收缩因子的随机近端点算法（SPPA）具有更快的线性收敛到一个邻域。",
    "tldr": "本论文研究了带有动量的随机近端点算法（SPPAM）的收敛性和稳定性，展示了SPPAM相比于随机近端点算法（SPPA）具有更快的线性收敛和更好的收缩因子。",
    "en_tdlr": "This paper studies the convergence and stability of the stochastic proximal point algorithm with momentum (SPPAM) and shows that SPPAM achieves faster linear convergence and better contraction factor compared to the stochastic proximal point algorithm (SPPA)."
}