{
    "title": "Off-Policy Correction For Multi-Agent Reinforcement Learning",
    "abstract": "arXiv:2111.11229v3 Announce Type: replace-cross  Abstract: Multi-agent reinforcement learning (MARL) provides a framework for problems involving multiple interacting agents. Despite apparent similarity to the single-agent case, multi-agent problems are often harder to train and analyze theoretically. In this work, we propose MA-Trace, a new on-policy actor-critic algorithm, which extends V-Trace to the MARL setting. The key advantage of our algorithm is its high scalability in a multi-worker setting. To this end, MA-Trace utilizes importance sampling as an off-policy correction method, which allows distributing the computations with no impact on the quality of training. Furthermore, our algorithm is theoretically grounded - we prove a fixed-point theorem that guarantees convergence. We evaluate the algorithm extensively on the StarCraft Multi-Agent Challenge, a standard benchmark for multi-agent algorithms. MA-Trace achieves high performance on all its tasks and exceeds state-of-the-ar",
    "link": "https://arxiv.org/abs/2111.11229",
    "context": "Title: Off-Policy Correction For Multi-Agent Reinforcement Learning\nAbstract: arXiv:2111.11229v3 Announce Type: replace-cross  Abstract: Multi-agent reinforcement learning (MARL) provides a framework for problems involving multiple interacting agents. Despite apparent similarity to the single-agent case, multi-agent problems are often harder to train and analyze theoretically. In this work, we propose MA-Trace, a new on-policy actor-critic algorithm, which extends V-Trace to the MARL setting. The key advantage of our algorithm is its high scalability in a multi-worker setting. To this end, MA-Trace utilizes importance sampling as an off-policy correction method, which allows distributing the computations with no impact on the quality of training. Furthermore, our algorithm is theoretically grounded - we prove a fixed-point theorem that guarantees convergence. We evaluate the algorithm extensively on the StarCraft Multi-Agent Challenge, a standard benchmark for multi-agent algorithms. MA-Trace achieves high performance on all its tasks and exceeds state-of-the-ar",
    "path": "papers/21/11/2111.11229.json",
    "total_tokens": 904,
    "translated_title": "多智体强化学习的离策略修正",
    "translated_abstract": "多智体强化学习（MARL）为涉及多个相互作用智体的问题提供了一个框架。尽管表面上与单智体情况相似，但多智体问题往往更难训练和在理论上分析。在这项工作中，我们提出了MA-Trace，一种新的基于策略的演员-评论家算法，将V-Trace扩展到MARL设定中。我们算法的关键优势在于其在多工作器设置中的高可伸缩性。为此，MA-Trace利用重要性采样作为离策略修正方法，允许在不影响训练质量的情况下进行计算分布。此外，我们的算法是在理论上有基础的 - 我们证明了一个保证收敛的不动点定理。我们在StarCraft多智体挑战赛上对算法进行了广泛评估，这是多智体算法的标准基准。MA-Trace在所有任务上表现出色，超过了最先进水平。",
    "tldr": "提出了MA-Trace，一种新的离策略演员-评论家算法，在多智体强化学习环境中具有高可伸缩性，并通过重要性采样作为离策略修正方法，保证了计算分布的质量和算法的收敛性。",
    "en_tdlr": "Introduced MA-Trace, a novel off-policy actor-critic algorithm for multi-agent reinforcement learning with high scalability in a multi-worker setting, utilizing importance sampling as an off-policy correction method to ensure quality of computation distribution and algorithm convergence."
}