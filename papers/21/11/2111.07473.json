{
    "title": "Scrutinizing XAI using linear ground-truth data with suppressor variables. (arXiv:2111.07473v2 [stat.ML] UPDATED)",
    "abstract": "Machine learning (ML) is increasingly often used to inform high-stakes decisions. As complex ML models (e.g., deep neural networks) are often considered black boxes, a wealth of procedures has been developed to shed light on their inner workings and the ways in which their predictions come about, defining the field of 'explainable AI' (XAI). Saliency methods rank input features according to some measure of 'importance'. Such methods are difficult to validate since a formal definition of feature importance is, thus far, lacking. It has been demonstrated that some saliency methods can highlight features that have no statistical association with the prediction target (suppressor variables). To avoid misinterpretations due to such behavior, we propose the actual presence of such an association as a necessary condition and objective preliminary definition for feature importance. We carefully crafted a ground-truth dataset in which all statistical dependencies are well-defined and linear, se",
    "link": "http://arxiv.org/abs/2111.07473",
    "context": "Title: Scrutinizing XAI using linear ground-truth data with suppressor variables. (arXiv:2111.07473v2 [stat.ML] UPDATED)\nAbstract: Machine learning (ML) is increasingly often used to inform high-stakes decisions. As complex ML models (e.g., deep neural networks) are often considered black boxes, a wealth of procedures has been developed to shed light on their inner workings and the ways in which their predictions come about, defining the field of 'explainable AI' (XAI). Saliency methods rank input features according to some measure of 'importance'. Such methods are difficult to validate since a formal definition of feature importance is, thus far, lacking. It has been demonstrated that some saliency methods can highlight features that have no statistical association with the prediction target (suppressor variables). To avoid misinterpretations due to such behavior, we propose the actual presence of such an association as a necessary condition and objective preliminary definition for feature importance. We carefully crafted a ground-truth dataset in which all statistical dependencies are well-defined and linear, se",
    "path": "papers/21/11/2111.07473.json",
    "total_tokens": 871,
    "translated_title": "利用抑制变量的线性基础数据审查可解释人工智能(XAI)",
    "translated_abstract": "机器学习(ML)越来越常用于高风险决策中。由于复杂的ML模型(如深度神经网络)通常被认为是黑匣子，因此已经开发出大量程序来阐明其内部运作和预测方式，定义了\"可解释人工智能\"(XAI)领域。显著性方法根据某种\"重要性\"度量对输入特征进行排序。由于迄今为止缺乏特征重要性的正式定义，这些方法很难验证。已经证实，一些显著性方法可以突出显示与预测目标没有统计联系的特征(抑制变量)。为了避免由于这种行为引起的错误解读，我们提出了确保此类联系存在是特征重要性的必要条件和客观初步定义。我们精心制作了一个基础数据集，其中所有统计依赖关系都是明确定义的和线性的，",
    "tldr": "研究提出了一个关于可解释人工智能(XAI)的方法，该方法提出了特征重要性的客观初步定义，以避免由于方法的行为引起的错误解读。",
    "en_tdlr": "The paper proposes a method for explainable AI (XAI) that introduces an objective preliminary definition for feature importance to avoid misinterpretations caused by current saliency methods that highlight suppressor variables."
}