{
    "title": "Mitigating Adversarial Attacks by Distributing Different Copies to Different Users. (arXiv:2111.15160v3 [cs.CR] UPDATED)",
    "abstract": "Machine learning models are vulnerable to adversarial attacks. In this paper, we consider the scenario where a model is distributed to multiple buyers, among which a malicious buyer attempts to attack another buyer. The malicious buyer probes its copy of the model to search for adversarial samples and then presents the found samples to the victim's copy of the model in order to replicate the attack. We point out that by distributing different copies of the model to different buyers, we can mitigate the attack such that adversarial samples found on one copy would not work on another copy. We observed that training a model with different randomness indeed mitigates such replication to a certain degree. However, there is no guarantee and retraining is computationally expensive. A number of works extended the retraining method to enhance the differences among models. However, a very limited number of models can be produced using such methods and the computational cost becomes even higher. ",
    "link": "http://arxiv.org/abs/2111.15160",
    "context": "Title: Mitigating Adversarial Attacks by Distributing Different Copies to Different Users. (arXiv:2111.15160v3 [cs.CR] UPDATED)\nAbstract: Machine learning models are vulnerable to adversarial attacks. In this paper, we consider the scenario where a model is distributed to multiple buyers, among which a malicious buyer attempts to attack another buyer. The malicious buyer probes its copy of the model to search for adversarial samples and then presents the found samples to the victim's copy of the model in order to replicate the attack. We point out that by distributing different copies of the model to different buyers, we can mitigate the attack such that adversarial samples found on one copy would not work on another copy. We observed that training a model with different randomness indeed mitigates such replication to a certain degree. However, there is no guarantee and retraining is computationally expensive. A number of works extended the retraining method to enhance the differences among models. However, a very limited number of models can be produced using such methods and the computational cost becomes even higher. ",
    "path": "papers/21/11/2111.15160.json",
    "total_tokens": 934,
    "translated_title": "将不同的模型分发给不同的用户可减轻对抗性攻击",
    "translated_abstract": "机器学习模型容易受到对抗性攻击。本文考虑将模型分发给多个用户的情况，在这其中，恶意用户试图攻击其他用户。恶意用户会探测其拥有的模型以寻找对抗样本，然后将发现的样本呈现给受害者模型的副本以复制攻击。我们指出，通过将不同的模型分发给不同的买家，我们可以减轻攻击，这样在一个模型上找到的对抗样本将不起作用于另一个模型。我们观察到，使用不同的随机性训练模型确实在一定程度上减轻了这种复制。然而，并没有保证，重新训练的成本也很高。一些作品扩展了重新培训方法以增强模型之间的差异。然而，这种方法只能产生非常有限的模型，并且计算成本变得更高了。",
    "tldr": "将不同的模型副本分发给不同的用户，可以降低恶意用户对其他用户的攻击风险。模型使用不同的随机性训练可以减轻复制攻击，但是重训练代价高且结果不稳定。一些方法扩展了重训练以增强模型差异，但是计算成本更高。",
    "en_tdlr": "Distributing different copies of the model to different users can mitigate the risk of adversarial attacks by malicious users. Training models with different randomness can reduce replicated attacks, but retraining is costly and has no guarantee. Some methods extend retraining to enhance model differences, but the computational cost is even higher."
}