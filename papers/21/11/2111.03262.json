{
    "title": "CGCL: Collaborative Graph Contrastive Learning without Handcrafted Graph Data Augmentations",
    "abstract": "arXiv:2111.03262v2 Announce Type: replace-cross  Abstract: Unsupervised graph representation learning is a non-trivial topic. The success of contrastive methods in the unsupervised representation learning on structured data inspires similar attempts on the graph. Existing graph contrastive learning (GCL) aims to learn the invariance across multiple augmentation views, which renders it heavily reliant on the handcrafted graph augmentations. However, inappropriate graph data augmentations can potentially jeopardize such invariance. In this paper, we show the potential hazards of inappropriate augmentations and then propose a novel Collaborative Graph Contrastive Learning framework (CGCL). This framework harnesses multiple graph encoders to observe the graph. Features observed from different encoders serve as the contrastive views in contrastive learning, which avoids inducing unstable perturbation and guarantees the invariance. To ensure the collaboration among diverse graph encoders, we",
    "link": "https://arxiv.org/abs/2111.03262",
    "context": "Title: CGCL: Collaborative Graph Contrastive Learning without Handcrafted Graph Data Augmentations\nAbstract: arXiv:2111.03262v2 Announce Type: replace-cross  Abstract: Unsupervised graph representation learning is a non-trivial topic. The success of contrastive methods in the unsupervised representation learning on structured data inspires similar attempts on the graph. Existing graph contrastive learning (GCL) aims to learn the invariance across multiple augmentation views, which renders it heavily reliant on the handcrafted graph augmentations. However, inappropriate graph data augmentations can potentially jeopardize such invariance. In this paper, we show the potential hazards of inappropriate augmentations and then propose a novel Collaborative Graph Contrastive Learning framework (CGCL). This framework harnesses multiple graph encoders to observe the graph. Features observed from different encoders serve as the contrastive views in contrastive learning, which avoids inducing unstable perturbation and guarantees the invariance. To ensure the collaboration among diverse graph encoders, we",
    "path": "papers/21/11/2111.03262.json",
    "total_tokens": 833,
    "translated_title": "CGCL：无需手工图数据增强的协作式图对比学习",
    "translated_abstract": "未监督的图表示学习是一个非常棘手的问题。在结构化数据的无监督表示学习中对比方法的成功启发了对图类似尝试。现有的图对比学习（GCL）旨在学习跨多个增强视图的不变性，这使其严重依赖于手工制作的图增强。然而，不当的图数据增强可能会危害这种不变性。在本文中，我们展示了不当增强的潜在危险，然后提出了一种新颖的协作式图对比学习框架（CGCL）。该框架利用多个图编码器观察图形。来自不同编码器观察到的特征作为对比学习中的对比视图，避免诱发不稳定的扰动并保证不变性。为了确保不同图编码器之间的协作，",
    "tldr": "提出了一种新颖的Collaborative Graph Contrastive Learning框架（CGCL），利用多个图编码器观察图形，并避免引入不稳定扰动，保证图的不变性。"
}