{
    "title": "End-to-End Annotator Bias Approximation on Crowdsourced Single-Label Sentiment Analysis. (arXiv:2111.02326v2 [cs.CL] UPDATED)",
    "abstract": "Sentiment analysis is often a crowdsourcing task prone to subjective labels given by many annotators. It is not yet fully understood how the annotation bias of each annotator can be modeled correctly with state-of-the-art methods. However, resolving annotator bias precisely and reliably is the key to understand annotators' labeling behavior and to successfully resolve corresponding individual misconceptions and wrongdoings regarding the annotation task. Our contribution is an explanation and improvement for precise neural end-to-end bias modeling and ground truth estimation, which reduces an undesired mismatch in that regard of the existing state-of-the-art. Classification experiments show that it has potential to improve accuracy in cases where each sample is annotated only by one single annotator. We provide the whole source code publicly and release an own domain-specific sentiment dataset containing 10,000 sentences discussing organic food products. These are crawled from social me",
    "link": "http://arxiv.org/abs/2111.02326",
    "context": "Title: End-to-End Annotator Bias Approximation on Crowdsourced Single-Label Sentiment Analysis. (arXiv:2111.02326v2 [cs.CL] UPDATED)\nAbstract: Sentiment analysis is often a crowdsourcing task prone to subjective labels given by many annotators. It is not yet fully understood how the annotation bias of each annotator can be modeled correctly with state-of-the-art methods. However, resolving annotator bias precisely and reliably is the key to understand annotators' labeling behavior and to successfully resolve corresponding individual misconceptions and wrongdoings regarding the annotation task. Our contribution is an explanation and improvement for precise neural end-to-end bias modeling and ground truth estimation, which reduces an undesired mismatch in that regard of the existing state-of-the-art. Classification experiments show that it has potential to improve accuracy in cases where each sample is annotated only by one single annotator. We provide the whole source code publicly and release an own domain-specific sentiment dataset containing 10,000 sentences discussing organic food products. These are crawled from social me",
    "path": "papers/21/11/2111.02326.json",
    "total_tokens": 890,
    "translated_title": "一种在众包单标签情感分析中端到端的注释者偏差近似方法",
    "translated_abstract": "情感分析通常是一个容易受到众多注释者主观标签影响的众包任务。目前尚不完全了解如何使用最先进的方法正确地建模每个注释者的注释偏差。然而，准确可靠地解决注释者偏差是理解注释者标注行为并成功解决相应的个体误解和错误的关键。我们的贡献是对精确的端到端偏差建模和真实值估计进行解释和改进，从而减少现有先进方法中涉及的不希望出现的不匹配问题。分类实验表明，该方法有潜力提高仅由单个注释者标注的样本准确性。我们公开提供整个源代码，并发布一个包含讨论有机食品产品的10,000个句子的领域特定情感数据集，这些句子是从社交媒体抓取而来。",
    "tldr": "本文提出一种在众包单标签情感分析中解决注释者偏差的端到端方法，通过精确的偏差建模和真实值估计来改善准确性，实验证明在样本只由单个注释者标注的情况下效果显著。",
    "en_tdlr": "This paper presents an end-to-end approach to address annotator bias in crowdsourced single-label sentiment analysis, improving accuracy through precise bias modeling and ground truth estimation, with significant results shown in cases where each sample is annotated only by one annotator."
}