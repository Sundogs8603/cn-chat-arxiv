{
    "title": "Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights. (arXiv:2109.04660v2 [cs.LG] UPDATED)",
    "abstract": "With the growth of deep neural networks (DNN), the number of DNN parameters has drastically increased. This makes DNN models hard to be deployed on resource-limited embedded systems. To alleviate this problem, dynamic pruning methods have emerged, which try to find diverse sparsity patterns during training by utilizing Straight-Through-Estimator (STE) to approximate gradients of pruned weights. STE can help the pruned weights revive in the process of finding dynamic sparsity patterns. However, using these coarse gradients causes training instability and performance degradation owing to the unreliable gradient signal of the STE approximation. In this work, to tackle this issue, we introduce refined gradients to update the pruned weights by forming dual forwarding paths from two sets (pruned and unpruned) of weights. We propose a novel Dynamic Collective Intelligence Learning (DCIL) which makes use of the learning synergy between the collective intelligence of both weight sets. We verify",
    "link": "http://arxiv.org/abs/2109.04660",
    "context": "Title: Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights. (arXiv:2109.04660v2 [cs.LG] UPDATED)\nAbstract: With the growth of deep neural networks (DNN), the number of DNN parameters has drastically increased. This makes DNN models hard to be deployed on resource-limited embedded systems. To alleviate this problem, dynamic pruning methods have emerged, which try to find diverse sparsity patterns during training by utilizing Straight-Through-Estimator (STE) to approximate gradients of pruned weights. STE can help the pruned weights revive in the process of finding dynamic sparsity patterns. However, using these coarse gradients causes training instability and performance degradation owing to the unreliable gradient signal of the STE approximation. In this work, to tackle this issue, we introduce refined gradients to update the pruned weights by forming dual forwarding paths from two sets (pruned and unpruned) of weights. We propose a novel Dynamic Collective Intelligence Learning (DCIL) which makes use of the learning synergy between the collective intelligence of both weight sets. We verify",
    "path": "papers/21/09/2109.04660.json",
    "total_tokens": 977,
    "translated_title": "动态集体智能学习：通过精炼的梯度找到高效稀疏模型以剪枝权重",
    "translated_abstract": "随着深度神经网络（DNN）的增长，DNN参数的数量大幅增加。这使得DNN模型难以部署在资源有限的嵌入式系统上。为了缓解这个问题，出现了动态剪枝方法，该方法利用直通估计（STE）来近似剪枝权重的梯度，在训练过程中寻找不同的稀疏模式。STE可以帮助剪枝权重在寻找动态稀疏模式的过程中复活。然而，使用这些粗糙的梯度会导致训练不稳定和性能下降，因为STE近似的梯度信号不可靠。在这项工作中，为了解决这个问题，我们引入了精炼的梯度来更新剪枝权重，通过从两组（剪枝和未剪枝）权重形成双向转发路径。我们提出了一种新颖的动态集体智能学习（DCIL），利用两组权重的集体智能之间的学习协同作用。",
    "tldr": "本文介绍了动态集体智能学习（DCIL）方法，利用精炼的梯度更新剪枝权重，通过形成双向转发路径来寻找高效稀疏模型。这种方法利用了剪枝和未剪枝权重的集体智能之间的学习协同作用。"
}