{
    "title": "Long-Range Transformers for Dynamic Spatiotemporal Forecasting. (arXiv:2109.12218v3 [cs.LG] UPDATED)",
    "abstract": "Multivariate time series forecasting focuses on predicting future values based on historical context. State-of-the-art sequence-to-sequence models rely on neural attention between timesteps, which allows for temporal learning but fails to consider distinct spatial relationships between variables. In contrast, methods based on graph neural networks explicitly model variable relationships. However, these methods often rely on predefined graphs that cannot change over time and perform separate spatial and temporal updates without establishing direct connections between each variable at every timestep. Our work addresses these problems by translating multivariate forecasting into a \"spatiotemporal sequence\" formulation where each Transformer input token represents the value of a single variable at a given time. Long-Range Transformers can then learn interactions between space, time, and value information jointly along this extended sequence. Our method, which we call Spacetimeformer, achie",
    "link": "http://arxiv.org/abs/2109.12218",
    "context": "Title: Long-Range Transformers for Dynamic Spatiotemporal Forecasting. (arXiv:2109.12218v3 [cs.LG] UPDATED)\nAbstract: Multivariate time series forecasting focuses on predicting future values based on historical context. State-of-the-art sequence-to-sequence models rely on neural attention between timesteps, which allows for temporal learning but fails to consider distinct spatial relationships between variables. In contrast, methods based on graph neural networks explicitly model variable relationships. However, these methods often rely on predefined graphs that cannot change over time and perform separate spatial and temporal updates without establishing direct connections between each variable at every timestep. Our work addresses these problems by translating multivariate forecasting into a \"spatiotemporal sequence\" formulation where each Transformer input token represents the value of a single variable at a given time. Long-Range Transformers can then learn interactions between space, time, and value information jointly along this extended sequence. Our method, which we call Spacetimeformer, achie",
    "path": "papers/21/09/2109.12218.json",
    "total_tokens": 867,
    "translated_title": "面向动态时空预测的长距离Transformer",
    "translated_abstract": "多元时间序列预测致力于基于历史情境预测未来值。现有序列到序列模型利用神经关注机制进行时间学习，但未考虑变量间的空间关系。相比而言，基于图神经网络的方法明确建模变量关系，但往往依赖于预定义图，不能随时间变化且在每个时间步骤中对各变量进行独立的空间和时间更新。我们的工作通过将多元预测转化为“时空序列”形式来解决这些问题，其中每个Transformer输入表示给定时间单个变量的值。长距离Transformer可以沿着这个扩展序列共同学习空间、时间和值信息之间的交互。我们的方法称为Spacetimeformer，在多个多元预测基准测试上取得了最先进的结果，并可以动态更新变量关系。",
    "tldr": "本研究提出了一种名为Spacetimeformer的方法，将多元时间序列预测转化为“时空序列”形式进行建模，实现了对变量之间动态空间关系的学习，同时在多个基准测试上取得了最先进的结果。",
    "en_tdlr": "This paper proposes a method, named Spacetimeformer, which formulates multivariate time series forecasting into a \"spatiotemporal sequence\" to capture dynamic spatial relationships between variables. State-of-the-art performance is achieved on several benchmarks."
}