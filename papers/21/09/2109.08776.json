{
    "title": "Exploring the Training Robustness of Distributional Reinforcement Learning against Noisy State Observations. (arXiv:2109.08776v5 [cs.LG] UPDATED)",
    "abstract": "In real scenarios, state observations that an agent observes may contain measurement errors or adversarial noises, misleading the agent to take suboptimal actions or even collapse while training. In this paper, we study the training robustness of distributional Reinforcement Learning (RL), a class of state-of-the-art methods that estimate the whole distribution, as opposed to only the expectation, of the total return. Firstly, we validate the contraction of distributional Bellman operators in the State-Noisy Markov Decision Process (SN-MDP), a typical tabular case that incorporates both random and adversarial state observation noises. In the noisy setting with function approximation, we then analyze the vulnerability of least squared loss in expectation-based RL with either linear or nonlinear function approximation. By contrast, we theoretically characterize the bounded gradient norm of distributional RL loss based on the categorical parameterization equipped with the KL divergence. T",
    "link": "http://arxiv.org/abs/2109.08776",
    "context": "Title: Exploring the Training Robustness of Distributional Reinforcement Learning against Noisy State Observations. (arXiv:2109.08776v5 [cs.LG] UPDATED)\nAbstract: In real scenarios, state observations that an agent observes may contain measurement errors or adversarial noises, misleading the agent to take suboptimal actions or even collapse while training. In this paper, we study the training robustness of distributional Reinforcement Learning (RL), a class of state-of-the-art methods that estimate the whole distribution, as opposed to only the expectation, of the total return. Firstly, we validate the contraction of distributional Bellman operators in the State-Noisy Markov Decision Process (SN-MDP), a typical tabular case that incorporates both random and adversarial state observation noises. In the noisy setting with function approximation, we then analyze the vulnerability of least squared loss in expectation-based RL with either linear or nonlinear function approximation. By contrast, we theoretically characterize the bounded gradient norm of distributional RL loss based on the categorical parameterization equipped with the KL divergence. T",
    "path": "papers/21/09/2109.08776.json",
    "total_tokens": 959,
    "translated_abstract": "在实际场景中，智能体观测到的状态可能包含测量误差或对抗性噪声，可能会误导智能体采取次优行动甚至在训练过程中崩溃。本文研究了分布式强化学习的训练鲁棒性，这是一类最先进的方法，它估计了整个回报的分布，而不仅仅是期望值。首先，我们验证了分布式贝尔曼算子在包含随机和对抗性状态观测噪声的典型表格化案例——具有状态噪声的马尔可夫决策过程的收缩性。在涉及函数逼近的嘈杂设置下，我们随后分析了期望强化学习中最小二乘误差的易受攻击性和线性或非线性函数逼近的弱势。相比之下，我们理论上描述了基于具有KL散度的类别参数化的分布式强化学习损失的有界梯度范数。",
    "tldr": "本文研究了分布式强化学习对于存在噪声的状态观测情况下的训练鲁棒性。通过分析，理论上证明了分布式强化学习损失的有界梯度范数，较次优的期望强化学习更鲁棒。",
    "en_tdlr": "This paper studies the training robustness of distributional Reinforcement Learning (RL) against noisy state observations. Results show that distributional RL has a bounded gradient norm of loss and is more robust than expectation-based RL in noisy settings with function approximation."
}