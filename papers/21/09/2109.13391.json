{
    "title": "Curvature-Aware Derivative-Free Optimization. (arXiv:2109.13391v2 [math.OC] UPDATED)",
    "abstract": "The paper discusses derivative-free optimization (DFO), which involves minimizing a function without access to gradients or directional derivatives, only function evaluations. Classical DFO methods, which mimic gradient-based methods, such as Nelder-Mead and direct search have limited scalability for high-dimensional problems. Zeroth-order methods have been gaining popularity due to the demands of large-scale machine learning applications, and the paper focuses on the selection of the step size $\\alpha_k$ in these methods. The proposed approach, called Curvature-Aware Random Search (CARS), uses first- and second-order finite difference approximations to compute a candidate $\\alpha_{+}$. We prove that for strongly convex objective functions, CARS converges linearly provided that the search direction is drawn from a distribution satisfying very mild conditions. We also present a Cubic Regularized variant of CARS, named CARS-CR, which converges in a rate of $\\mathcal{O}(k^{-1})$ without t",
    "link": "http://arxiv.org/abs/2109.13391",
    "context": "Title: Curvature-Aware Derivative-Free Optimization. (arXiv:2109.13391v2 [math.OC] UPDATED)\nAbstract: The paper discusses derivative-free optimization (DFO), which involves minimizing a function without access to gradients or directional derivatives, only function evaluations. Classical DFO methods, which mimic gradient-based methods, such as Nelder-Mead and direct search have limited scalability for high-dimensional problems. Zeroth-order methods have been gaining popularity due to the demands of large-scale machine learning applications, and the paper focuses on the selection of the step size $\\alpha_k$ in these methods. The proposed approach, called Curvature-Aware Random Search (CARS), uses first- and second-order finite difference approximations to compute a candidate $\\alpha_{+}$. We prove that for strongly convex objective functions, CARS converges linearly provided that the search direction is drawn from a distribution satisfying very mild conditions. We also present a Cubic Regularized variant of CARS, named CARS-CR, which converges in a rate of $\\mathcal{O}(k^{-1})$ without t",
    "path": "papers/21/09/2109.13391.json",
    "total_tokens": 977,
    "translated_title": "曲率感知的无导数优化",
    "translated_abstract": "本文讨论无导数优化问题，即仅通过函数评价而非梯度或方向导数来最小化一个函数。经典的无导数优化算法，如Nelder-Mead和直接搜索类似于梯度下降，对于高维问题的可扩展性有限。随着大规模机器学习应用的需求，零阶方法越来越受到青睐，本文重点讨论这些方法中步长αk的选择。所提出的方法名为曲率感知随机搜索（CARS），使用一阶和二阶有限差分逼近来计算候选步长α+。我们证明，对于强凸目标函数，只要搜索方向来自满足非常温和条件的分布，CARS就可以线性收敛。我们还提出了CARS-CR，它是CARS的立方正则化变体，无需进行变量转换即可以O(k^-1)的速率收敛。",
    "tldr": "本文介绍了曲率感知的无导数优化算法——曲率感知随机搜索（CARS），该算法使用一阶和二阶有限差分逼近来计算候选步长，证明了其对于强凸目标函数的线性收敛性，并提出了无需变量转换的立方正则化变体CARS-CR，可以以O(k^-1)的速率收敛。",
    "en_tdlr": "This paper presents a curvature-aware derivative-free optimization method, called Curvature-Aware Random Search (CARS), which uses first- and second-order finite difference approximations to compute a candidate step size. It is proved to have linear convergence for strongly convex objective functions with the search direction drawn from a distribution satisfying mild conditions. Additionally, a Cubic Regularized variant of CARS, named CARS-CR, is proposed and found to converge at a rate of O(k^-1) without the transformation of variables."
}