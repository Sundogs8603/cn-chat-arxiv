{
    "title": "Byzantine-robust Federated Learning through Collaborative Malicious Gradient Filtering. (arXiv:2109.05872v2 [cs.LG] UPDATED)",
    "abstract": "Gradient-based training in federated learning is known to be vulnerable to faulty/malicious clients, which are often modeled as Byzantine clients. To this end, previous work either makes use of auxiliary data at parameter server to verify the received gradients (e.g., by computing validation error rate) or leverages statistic-based methods (e.g. median and Krum) to identify and remove malicious gradients from Byzantine clients. In this paper, we remark that auxiliary data may not always be available in practice and focus on the statistic-based approach. However, recent work on model poisoning attacks has shown that well-crafted attacks can circumvent most of median- and distance-based statistical defense methods, making malicious gradients indistinguishable from honest ones. To tackle this challenge, we show that the element-wise sign of gradient vector can provide valuable insight in detecting model poisoning attacks. Based on our theoretical analysis of the \\textit{Little is Enough} ",
    "link": "http://arxiv.org/abs/2109.05872",
    "context": "Title: Byzantine-robust Federated Learning through Collaborative Malicious Gradient Filtering. (arXiv:2109.05872v2 [cs.LG] UPDATED)\nAbstract: Gradient-based training in federated learning is known to be vulnerable to faulty/malicious clients, which are often modeled as Byzantine clients. To this end, previous work either makes use of auxiliary data at parameter server to verify the received gradients (e.g., by computing validation error rate) or leverages statistic-based methods (e.g. median and Krum) to identify and remove malicious gradients from Byzantine clients. In this paper, we remark that auxiliary data may not always be available in practice and focus on the statistic-based approach. However, recent work on model poisoning attacks has shown that well-crafted attacks can circumvent most of median- and distance-based statistical defense methods, making malicious gradients indistinguishable from honest ones. To tackle this challenge, we show that the element-wise sign of gradient vector can provide valuable insight in detecting model poisoning attacks. Based on our theoretical analysis of the \\textit{Little is Enough} ",
    "path": "papers/21/09/2109.05872.json",
    "total_tokens": 892,
    "translated_title": "通过协同恶意梯度筛选实现拜占庭容错联邦学习",
    "translated_abstract": "联邦学习中基于梯度的训练容易受到故障/恶意客户端的攻击，这些被称为拜占庭客户端。现有工作为了解决这个问题，要么利用中央参数服务器的辅助数据来验证接收到的梯度(例如计算验证错误率)，要么利用基于统计学方法(例如中位数和Krum)来识别并从拜占庭客户端中删除恶意梯度。本文提出，在实际情况下，辅助数据可能并不总是可用的，因此我们专注于基于统计学方法的方法。但是，最近的研究表明，精心制作的模型污染攻击可以规避大多数基于中位数和距离的统计防御方法，从而使恶意梯度与诚实梯度难以区分。为了解决这个问题，我们发现梯度向量的逐元素符号可以提供有价值的洞察力，用于检测模型污染攻击。",
    "tldr": "本文提出了一种基于逐元素符号的方法，用于检测模型污染攻击，以解决联邦学习中拜占庭攻击的问题。"
}