{
    "title": "Reinforcement Learning Policies in Continuous-Time Linear Systems. (arXiv:2109.07630v3 [eess.SY] UPDATED)",
    "abstract": "Linear dynamical systems that obey stochastic differential equations are canonical models. While optimal control of known systems has a rich literature, the problem is technically hard under model uncertainty and there are hardly any results. We initiate study of this problem and aim to learn (and simultaneously deploy) optimal actions for minimizing a quadratic cost function. Indeed, this work is the first that comprehensively addresses the crucial challenge of balancing exploration versus exploitation in continuous-time systems. We present online policies that learn optimal actions fast by carefully randomizing the parameter estimates, and establish their performance guarantees: a regret bound that grows with square-root of time multiplied by the number of parameters. Implementation of the policy for a flight-control task demonstrates its efficacy. Further, we prove sharp stability results for inexact system dynamics and tightly specify the infinitesimal regret caused by sub-optimal ",
    "link": "http://arxiv.org/abs/2109.07630",
    "context": "Title: Reinforcement Learning Policies in Continuous-Time Linear Systems. (arXiv:2109.07630v3 [eess.SY] UPDATED)\nAbstract: Linear dynamical systems that obey stochastic differential equations are canonical models. While optimal control of known systems has a rich literature, the problem is technically hard under model uncertainty and there are hardly any results. We initiate study of this problem and aim to learn (and simultaneously deploy) optimal actions for minimizing a quadratic cost function. Indeed, this work is the first that comprehensively addresses the crucial challenge of balancing exploration versus exploitation in continuous-time systems. We present online policies that learn optimal actions fast by carefully randomizing the parameter estimates, and establish their performance guarantees: a regret bound that grows with square-root of time multiplied by the number of parameters. Implementation of the policy for a flight-control task demonstrates its efficacy. Further, we prove sharp stability results for inexact system dynamics and tightly specify the infinitesimal regret caused by sub-optimal ",
    "path": "papers/21/09/2109.07630.json",
    "total_tokens": 971,
    "translated_title": "连续时间线性系统中的强化学习策略",
    "translated_abstract": "符合随机微分方程的线性动态系统是经典模型。尽管已经有许多关于已知系统最优控制的文献，但在模型不确定性下，该问题在技术上非常困难且几乎没有结果。本文着手研究此问题，并旨在学习（并同时部署）最小化二次成本函数的最优操作。事实上，本研究首次全面解决了平衡连续系统的探索与开发的关键挑战。我们提出在线策略，通过谨慎地随机参数估计快速学习最优操作，并建立其性能保证：后悔界随时间的平方根乘以参数数量增长。对飞行控制任务的策略实施证明了其功效。此外，我们证明了对于不精确的系统动态，具有尖锐的稳定性结果，并严格说明了次优情况下的微小后悔。",
    "tldr": "本研究旨在解决模型不确定性下连续时间线性系统最优操作的问题，提出在线策略学习最优操作，平衡探索与开发的挑战，实现飞行控制任务的有效性，并证明了在不精确的系统动态下的尖锐稳定性结果，以及次优情况下微小后悔的详细说明。",
    "en_tdlr": "This study aims to solve the problem of optimal actions for continuous-time linear systems under model uncertainty, presenting online policies that balance the challenges of exploration versus exploitation while simultaneously learning and deploying the optimal actions for minimizing a quadratic cost function. The proposed policies achieve high performance guarantees with regret bounds that grow with the square-root of time multiplied by the number of parameters. The study also proves sharp stability results for inexact system dynamics and specifies the infinitesimal regret caused by sub-optimal situations in detail. The efficacy of the proposed policy is demonstrated in a flight-control task."
}