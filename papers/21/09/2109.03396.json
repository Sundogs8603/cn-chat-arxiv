{
    "title": "A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with an Arbitrary Opponent",
    "abstract": "arXiv:2109.03396v2 Announce Type: replace  Abstract: In this paper, we propose Posterior Sampling Reinforcement Learning for Zero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that achieves Bayesian regret bound of $O(HS\\sqrt{AT})$ in the infinite-horizon zero-sum stochastic games with average-reward criterion. Here $H$ is an upper bound on the span of the bias function, $S$ is the number of states, $A$ is the number of joint actions and $T$ is the horizon. We consider the online setting where the opponent can not be controlled and can take any arbitrary time-adaptive history-dependent strategy. Our regret bound improves on the best existing regret bound of $O(\\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) under the same assumption and matches the theoretical lower bound in $T$.",
    "link": "https://arxiv.org/abs/2109.03396",
    "context": "Title: A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with an Arbitrary Opponent\nAbstract: arXiv:2109.03396v2 Announce Type: replace  Abstract: In this paper, we propose Posterior Sampling Reinforcement Learning for Zero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that achieves Bayesian regret bound of $O(HS\\sqrt{AT})$ in the infinite-horizon zero-sum stochastic games with average-reward criterion. Here $H$ is an upper bound on the span of the bias function, $S$ is the number of states, $A$ is the number of joint actions and $T$ is the horizon. We consider the online setting where the opponent can not be controlled and can take any arbitrary time-adaptive history-dependent strategy. Our regret bound improves on the best existing regret bound of $O(\\sqrt[3]{DS^2AT^2})$ by Wei et al. (2017) under the same assumption and matches the theoretical lower bound in $T$.",
    "path": "papers/21/09/2109.03396.json",
    "total_tokens": 860,
    "translated_title": "针对未知零和随机博弈的贝叶斯学习算法及其任意对手",
    "translated_abstract": "在本文中，我们提出了后验采样强化学习零和随机博弈（PSRL-ZSG）算法，这是第一个在线学习算法，在具有平均奖励准则的无限时间跨度的零和随机博弈中实现了贝叶斯遗憾界为$O(HS\\sqrt{AT})$。其中$H$是偏差函数跨度的上界，$S$是状态数量，$A$是联合动作数量，$T$是时间跨度。我们考虑对手无法控制且可以采取任意任意与历史相关的策略的在线设置。我们的遗憾界改进了魏等人(2017)在相同假设下的最佳遗憾界$O(\\sqrt[3]{DS^2AT^2})$，并且与在$T$上的理论下界相匹配。",
    "tldr": "本文提出了一种针对无限时间跨度的零和随机博弈的在线学习算法，该算法在具有平均奖励准则的情况下实现了贝叶斯遗憾界$O(HS\\sqrt{AT})$。",
    "en_tdlr": "This paper introduces an online learning algorithm for infinite-horizon zero-sum stochastic games, which achieves a Bayesian regret bound of $O(HS\\sqrt{AT})$ under the average-reward criterion."
}