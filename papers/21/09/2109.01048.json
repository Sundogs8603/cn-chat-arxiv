{
    "title": "Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation",
    "abstract": "arXiv:2109.01048v3 Announce Type: replace  Abstract: Existing technologies expand BERT from different perspectives, e.g. designing different pre-training tasks, different semantic granularities, and different model architectures. Few models consider expanding BERT from different text formats. In this paper, we propose a heterogeneous knowledge language model (\\textbf{HKLM}), a unified pre-trained language model (PLM) for all forms of text, including unstructured text, semi-structured text, and well-structured text. To capture the corresponding relations among these multi-format knowledge, our approach uses masked language model objective to learn word knowledge, uses triple classification objective and title matching objective to learn entity knowledge and topic knowledge respectively. To obtain the aforementioned multi-format text, we construct a corpus in the tourism domain and conduct experiments on 5 tourism NLP datasets. The results show that our approach outperforms the pre-train",
    "link": "https://arxiv.org/abs/2109.01048",
    "context": "Title: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation\nAbstract: arXiv:2109.01048v3 Announce Type: replace  Abstract: Existing technologies expand BERT from different perspectives, e.g. designing different pre-training tasks, different semantic granularities, and different model architectures. Few models consider expanding BERT from different text formats. In this paper, we propose a heterogeneous knowledge language model (\\textbf{HKLM}), a unified pre-trained language model (PLM) for all forms of text, including unstructured text, semi-structured text, and well-structured text. To capture the corresponding relations among these multi-format knowledge, our approach uses masked language model objective to learn word knowledge, uses triple classification objective and title matching objective to learn entity knowledge and topic knowledge respectively. To obtain the aforementioned multi-format text, we construct a corpus in the tourism domain and conduct experiments on 5 tourism NLP datasets. The results show that our approach outperforms the pre-train",
    "path": "papers/21/09/2109.01048.json",
    "total_tokens": 910,
    "translated_title": "将领域特定的异构知识融入统一表示的预训练语言模型",
    "translated_abstract": "现有技术从不同角度扩展了BERT，例如设计不同的预训练任务、不同的语义粒度和不同的模型架构。很少有模型考虑从不同的文本格式扩展BERT。在本文中，我们提出了一种异构知识语言模型（HKLM），一种统一的面向所有形式文本的预训练语言模型（PLM），包括非结构化文本、半结构化文本和结构化文本。为了捕捉这些多格式知识之间的相应关系，我们的方法使用掩码语言模型目标来学习词知识，使用三元分类目标和标题匹配目标分别学习实体知识和主题知识。为了获得前述多格式文本，我们在旅游领域构建了一个语料库，并在5个旅游NLP数据集上进行了实验。结果表明，我们的方法胜过了预训练模型。",
    "tldr": "本文提出了一种名为HKLM的异构知识语言模型，能够统一处理包括非结构化文本、半结构化文本和结构化文本在内的所有文本形式，通过不同的目标学习词知识、实体知识和主题知识，实验证明其性能优于预训练模型。",
    "en_tdlr": "This paper presents a heterogeneous knowledge language model called HKLM, which can uniformly handle all forms of text including unstructured, semi-structured, and structured text, learning word knowledge, entity knowledge, and topic knowledge through different objectives, and experiments demonstrate its superiority over pre-trained models."
}