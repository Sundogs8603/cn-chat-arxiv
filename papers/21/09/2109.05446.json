{
    "title": "Efficient-FedRec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation. (arXiv:2109.05446v3 [cs.IR] UPDATED)",
    "abstract": "News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users' historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple clients to collaboratively train models without sharing their private data. However, the computation and communication cost of directly learning many existing news recommendation models in a federated way are unacceptable for user clients. In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation. Instead of training and communicating the whole model, we decompose the news recommendation model into a large news model maintained in the server and a light-weight user model shared on both server and clients, where news representations and user model are communicated between server and clients. More specifically, the clients request the user model an",
    "link": "http://arxiv.org/abs/2109.05446",
    "context": "Title: Efficient-FedRec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation. (arXiv:2109.05446v3 [cs.IR] UPDATED)\nAbstract: News recommendation is critical for personalized news access. Most existing news recommendation methods rely on centralized storage of users' historical news click behavior data, which may lead to privacy concerns and hazards. Federated Learning is a privacy-preserving framework for multiple clients to collaboratively train models without sharing their private data. However, the computation and communication cost of directly learning many existing news recommendation models in a federated way are unacceptable for user clients. In this paper, we propose an efficient federated learning framework for privacy-preserving news recommendation. Instead of training and communicating the whole model, we decompose the news recommendation model into a large news model maintained in the server and a light-weight user model shared on both server and clients, where news representations and user model are communicated between server and clients. More specifically, the clients request the user model an",
    "path": "papers/21/09/2109.05446.json",
    "total_tokens": 1137,
    "translated_title": "面向隐私保护新闻推荐的高效联邦学习框架",
    "translated_abstract": "新闻推荐对于个性化新闻访问至关重要。大多数现有的新闻推荐方法依赖于中心化存储用户历史新闻点击行为数据，这可能会引发隐私问题和危害。联邦学习是一种隐私保护框架，可用于多个客户端共同训练模型而无需共享其私有数据。然而，以联邦方式直接学习许多现有的新闻推荐模型的计算和通信成本对于用户客户端是不可接受的。因此，本文提出了一种高效的联邦学习框架，可用于面向隐私保护的新闻推荐。具体来说，我们将新闻推荐模型分解为服务器维护的大型新闻模型和在服务器和客户端之间共享的轻量级用户模型，其中新闻表示和用户模型在服务器和客户端之间进行通信。然后，客户端请求服务器提供用户模型和新闻表示，并在本地使用自己的私有数据进行训练。接着，他们将更新后的用户模型上传到服务器，服务器对它们进行聚合以更新集中式的新闻模型。在两个真实数据集上的实验结果表明了我们提出的框架的有效性和效率，既保持了用户隐私，同时与现有的中心化和联邦学习方法相比表现出竞争性的性能。",
    "tldr": "本文提出了一种新的联邦学习框架，用于隐私保护的新闻推荐。该框架将新闻推荐模型分解为大型新闻模型和轻量级用户模型，通过将新闻表示和用户模型在服务器和客户端之间进行通信，从而实现了高效率和有效性的训练。",
    "en_tdlr": "This paper proposes an efficient federated learning framework for privacy-preserving news recommendation by decomposing the news recommendation model into a large news model maintained in the server and a light-weight user model shared on both server and clients, achieving competitive performance compared to existing centralized and federated learning methods while maintaining user privacy."
}