{
    "title": "Deformed semicircle law and concentration of nonlinear random matrices for ultra-wide neural networks. (arXiv:2109.09304v3 [math.ST] UPDATED)",
    "abstract": "In this paper, we investigate a two-layer fully connected neural network of the form $f(X)=\\frac{1}{\\sqrt{d_1}}\\boldsymbol{a}^\\top \\sigma\\left(WX\\right)$, where $X\\in\\mathbb{R}^{d_0\\times n}$ is a deterministic data matrix, $W\\in\\mathbb{R}^{d_1\\times d_0}$ and $\\boldsymbol{a}\\in\\mathbb{R}^{d_1}$ are random Gaussian weights, and $\\sigma$ is a nonlinear activation function. We study the limiting spectral distributions of two empirical kernel matrices associated with $f(X)$: the empirical conjugate kernel (CK) and neural tangent kernel (NTK), beyond the linear-width regime ($d_1\\asymp n$). We focus on the $\\textit{ultra-wide regime}$, where the width $d_1$ of the first layer is much larger than the sample size $n$. Under appropriate assumptions on $X$ and $\\sigma$, a deformed semicircle law emerges as $d_1/n\\to\\infty$ and $n\\to\\infty$. We first prove this limiting law for generalized sample covariance matrices with some dependency. To specify it for our neural network model, we provide a ",
    "link": "http://arxiv.org/abs/2109.09304",
    "context": "Title: Deformed semicircle law and concentration of nonlinear random matrices for ultra-wide neural networks. (arXiv:2109.09304v3 [math.ST] UPDATED)\nAbstract: In this paper, we investigate a two-layer fully connected neural network of the form $f(X)=\\frac{1}{\\sqrt{d_1}}\\boldsymbol{a}^\\top \\sigma\\left(WX\\right)$, where $X\\in\\mathbb{R}^{d_0\\times n}$ is a deterministic data matrix, $W\\in\\mathbb{R}^{d_1\\times d_0}$ and $\\boldsymbol{a}\\in\\mathbb{R}^{d_1}$ are random Gaussian weights, and $\\sigma$ is a nonlinear activation function. We study the limiting spectral distributions of two empirical kernel matrices associated with $f(X)$: the empirical conjugate kernel (CK) and neural tangent kernel (NTK), beyond the linear-width regime ($d_1\\asymp n$). We focus on the $\\textit{ultra-wide regime}$, where the width $d_1$ of the first layer is much larger than the sample size $n$. Under appropriate assumptions on $X$ and $\\sigma$, a deformed semicircle law emerges as $d_1/n\\to\\infty$ and $n\\to\\infty$. We first prove this limiting law for generalized sample covariance matrices with some dependency. To specify it for our neural network model, we provide a ",
    "path": "papers/21/09/2109.09304.json",
    "total_tokens": 1177,
    "tldr": "本文研究超宽神经网络的经验核矩阵的极限谱分布，发现当第一层宽度增加时，畸变半圆律成为极限定律。"
}