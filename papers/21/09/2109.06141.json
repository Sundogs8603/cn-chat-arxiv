{
    "title": "On Tilted Losses in Machine Learning: Theory and Applications. (arXiv:2109.06141v3 [cs.LG] UPDATED)",
    "abstract": "Exponential tilting is a technique commonly used in fields such as statistics, probability, information theory, and optimization to create parametric distribution shifts. Despite its prevalence in related fields, tilting has not seen widespread use in machine learning. In this work, we aim to bridge this gap by exploring the use of tilting in risk minimization. We study a simple extension to ERM -- tilted empirical risk minimization (TERM) -which uses exponential tilting to flexibly tune the impact of individual losses. The resulting framework has several useful properties: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to the tail probability of losses. Our work makes rigorous connections between TERM and related objectives, such as Value-at-Risk, Conditional Value-at-Risk, and distributionally robust op",
    "link": "http://arxiv.org/abs/2109.06141",
    "context": "Title: On Tilted Losses in Machine Learning: Theory and Applications. (arXiv:2109.06141v3 [cs.LG] UPDATED)\nAbstract: Exponential tilting is a technique commonly used in fields such as statistics, probability, information theory, and optimization to create parametric distribution shifts. Despite its prevalence in related fields, tilting has not seen widespread use in machine learning. In this work, we aim to bridge this gap by exploring the use of tilting in risk minimization. We study a simple extension to ERM -- tilted empirical risk minimization (TERM) -which uses exponential tilting to flexibly tune the impact of individual losses. The resulting framework has several useful properties: We show that TERM can increase or decrease the influence of outliers, respectively, to enable fairness or robustness; has variance-reduction properties that can benefit generalization; and can be viewed as a smooth approximation to the tail probability of losses. Our work makes rigorous connections between TERM and related objectives, such as Value-at-Risk, Conditional Value-at-Risk, and distributionally robust op",
    "path": "papers/21/09/2109.06141.json",
    "total_tokens": 1056,
    "translated_title": "关于倾斜损失的机器学习理论与应用研究",
    "translated_abstract": "指数倾斜是统计学、概率论、信息论和优化等领域常用的一种技术，用于创建参数化分布转移。尽管在相关领域中非常常见，但倾斜在机器学习中的应用尚不普及。本文旨在通过研究倾斜在风险最小化中的应用来填补这一空白。我们研究了验算风险最小化（TERM）的简单扩展，它使用指数倾斜来灵活调整个别损失的影响。所得到的框架具有多种有用的性质：我们展示了TERM可以分别增加或减少异常值的影响，从而实现公平或鲁棒性。TERM也具有降低方差从而提高泛化性能的优势，并且可以被看作是对损失的尾部概率的平滑近似。我们的工作在TERM和其他相关目标之间建立了严格的联系，例如风险价值、条件风险价值和分布鲁棒优化等。",
    "tldr": "本文研究了一种在机器学习中不常见但在统计、概率与优化等领域常用的技术——指数倾斜，并将其应用到风险最小化中。所提出的方法可以修正单个损失的影响，增加或减少异常值的作用，可以提高泛化性能，并可以被视为损失的尾部概率的平滑近似。",
    "en_tdlr": "This paper explores the use of exponential tilting in risk minimization for machine learning, which is a commonly-used technique in fields like statistics, probability, information theory, and optimization to create parametric distribution shifts. The proposed method, tilted empirical risk minimization, can flexibly adjust the impact of individual losses, increase or decrease the influence of outliers for fairness or robustness, and improve the generalization performance. It can also be viewed as a smooth approximation to the tail probability of losses and has connections with related objectives, such as Value-at-Risk, Conditional Value-at-Risk, and distributionally robust optimization."
}