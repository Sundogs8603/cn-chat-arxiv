{
    "title": "Super Neurons. (arXiv:2109.01594v2 [cs.CV] UPDATED)",
    "abstract": "Self-Organized Operational Neural Networks (Self-ONNs) have recently been proposed as new-generation neural network models with nonlinear learning units, i.e., the generative neurons that yield an elegant level of diversity; however, like its predecessor, conventional Convolutional Neural Networks (CNNs), they still have a common drawback: localized (fixed) kernel operations. This severely limits the receptive field and information flow between layers and thus brings the necessity for deep and complex models. It is highly desired to improve the receptive field size without increasing the kernel dimensions. This requires a significant upgrade over the generative neurons to achieve the non-localized kernel operations for each connection between consecutive layers. In this article, we present superior (generative) neuron models (or super neurons in short) that allow random or learnable kernel shifts and thus can increase the receptive field size of each connection. The kernel localization",
    "link": "http://arxiv.org/abs/2109.01594",
    "context": "Title: Super Neurons. (arXiv:2109.01594v2 [cs.CV] UPDATED)\nAbstract: Self-Organized Operational Neural Networks (Self-ONNs) have recently been proposed as new-generation neural network models with nonlinear learning units, i.e., the generative neurons that yield an elegant level of diversity; however, like its predecessor, conventional Convolutional Neural Networks (CNNs), they still have a common drawback: localized (fixed) kernel operations. This severely limits the receptive field and information flow between layers and thus brings the necessity for deep and complex models. It is highly desired to improve the receptive field size without increasing the kernel dimensions. This requires a significant upgrade over the generative neurons to achieve the non-localized kernel operations for each connection between consecutive layers. In this article, we present superior (generative) neuron models (or super neurons in short) that allow random or learnable kernel shifts and thus can increase the receptive field size of each connection. The kernel localization",
    "path": "papers/21/09/2109.01594.json",
    "total_tokens": 901,
    "translated_title": "超级神经元",
    "translated_abstract": "最近提出了一种称为自组织操作神经网络(Self-Organized Operational Neural Networks,Self-ONNs)的新一代神经网络模型,该模型具有非线性学习单元——产生神经元,可以提供一种优雅的多样性层次。但是,像传统卷积神经网络(CNN)一样,Self-ONNs仍然具有一个普遍的缺陷:本地化（固定）核操作。这严重限制了不同层之间的接受野和信息流,因此需要深度和复杂的模型。本文提出了一种优越的(产生)神经元模型,或者简称为超级神经元,它允许随机或可学习的核移位,从而可以增加每个连接的接受野大小。使用这些超级神经元可以克服传统神经网络的核定位限制，从而提高了深度学习模型的性能并减少了复杂性。",
    "tldr": "本文提出了一种新型产生神经元模型——超级神经元，可以克服传统神经网络的核定位限制，从而实现随机或可学习的核移位，增加每个连接的接受野大小，提高深度学习模型的性能同时降低模型的复杂性。",
    "en_tdlr": "This article proposes a new type of generative neuron model called super neurons which can overcome the kernel localization limitations of conventional neural networks, allowing for random or learnable kernel shifts to increase receptive field size and improve performance while reducing complexity."
}