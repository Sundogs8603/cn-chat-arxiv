{
    "title": "Measuring Fairness Under Unawareness of Sensitive Attributes: A Quantification-Based Approach. (arXiv:2109.08549v5 [cs.CY] UPDATED)",
    "abstract": "Algorithms and models are increasingly deployed to inform decisions about people, inevitably affecting their lives. As a consequence, those in charge of developing these models must carefully evaluate their impact on different groups of people and favour group fairness, that is, ensure that groups determined by sensitive demographic attributes, such as race or sex, are not treated unjustly. To achieve this goal, the availability (awareness) of these demographic attributes to those evaluating the impact of these models is fundamental. Unfortunately, collecting and storing these attributes is often in conflict with industry practices and legislation on data minimisation and privacy. For this reason, it can be hard to measure the group fairness of trained models, even from within the companies developing them. In this work, we tackle the problem of measuring group fairness under unawareness of sensitive attributes, by using techniques from quantification, a supervised learning task concer",
    "link": "http://arxiv.org/abs/2109.08549",
    "context": "Title: Measuring Fairness Under Unawareness of Sensitive Attributes: A Quantification-Based Approach. (arXiv:2109.08549v5 [cs.CY] UPDATED)\nAbstract: Algorithms and models are increasingly deployed to inform decisions about people, inevitably affecting their lives. As a consequence, those in charge of developing these models must carefully evaluate their impact on different groups of people and favour group fairness, that is, ensure that groups determined by sensitive demographic attributes, such as race or sex, are not treated unjustly. To achieve this goal, the availability (awareness) of these demographic attributes to those evaluating the impact of these models is fundamental. Unfortunately, collecting and storing these attributes is often in conflict with industry practices and legislation on data minimisation and privacy. For this reason, it can be hard to measure the group fairness of trained models, even from within the companies developing them. In this work, we tackle the problem of measuring group fairness under unawareness of sensitive attributes, by using techniques from quantification, a supervised learning task concer",
    "path": "papers/21/09/2109.08549.json",
    "total_tokens": 853,
    "translated_title": "在未知敏感属性情况下衡量公平性：一种基于量化的方法 (arXiv:2109.08549v5 [cs.CY] UPDATED)",
    "translated_abstract": "算法和模型越来越被用于影响人们的生活的决策中，因此影响不同人群的身份认同的敏感属性，如种族或性别等，并不会受到不公正的待遇，这个问题变得越来越重要。为了达到这个目标，对于那些开发这些模型的人来说，必须仔细评估这些模型对不同群体的影响并偏爱群体公平性。然而，收集和存储这些属性通常与数据最小化和隐私规定存在冲突。因此，即使是在开发公司内部，也很难衡量已训练模型的群体公平性。在本研究中，我们使用量化技术，解决了如何在未知敏感属性的情况下衡量群体公平性的问题。",
    "tldr": "本论文提出了一种基于量化的方法，用于在不知道模型的敏感属性的情况下，评估模型对不同群体的公平性。",
    "en_tdlr": "This paper proposes a quantification-based approach to measure group fairness when the sensitive attributes of the model are unknown, aiming to ensure that different groups of people are treated fairly even without being aware of their sensitive attributes."
}