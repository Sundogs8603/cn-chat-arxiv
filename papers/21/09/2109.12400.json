{
    "title": "Communication-Efficient Federated Linear and Deep Generalized Canonical Correlation Analysis. (arXiv:2109.12400v2 [cs.LG] UPDATED)",
    "abstract": "Classic and deep generalized canonical correlation analysis (GCCA) algorithms seek low-dimensional common representations of data entities from multiple ``views'' (e.g., audio and image) using linear transformations and neural networks, respectively. When the views are acquired and stored at different computing agents (e.g., organizations and edge devices) and data sharing is undesired due to privacy or communication cost considerations, federated learning-based GCCA is well-motivated. In federated learning, the views are kept locally at the agents and only derived, limited information exchange with a central server is allowed. However, applying existing GCCA algorithms onto such federated learning settings may incur prohibitively high communication overhead. This work puts forth a communication-efficient federated learning framework for both linear and deep GCCA under the maximum variance (MAX-VAR) formulation. The overhead issue is addressed by aggressively compressing (via quantizat",
    "link": "http://arxiv.org/abs/2109.12400",
    "context": "Title: Communication-Efficient Federated Linear and Deep Generalized Canonical Correlation Analysis. (arXiv:2109.12400v2 [cs.LG] UPDATED)\nAbstract: Classic and deep generalized canonical correlation analysis (GCCA) algorithms seek low-dimensional common representations of data entities from multiple ``views'' (e.g., audio and image) using linear transformations and neural networks, respectively. When the views are acquired and stored at different computing agents (e.g., organizations and edge devices) and data sharing is undesired due to privacy or communication cost considerations, federated learning-based GCCA is well-motivated. In federated learning, the views are kept locally at the agents and only derived, limited information exchange with a central server is allowed. However, applying existing GCCA algorithms onto such federated learning settings may incur prohibitively high communication overhead. This work puts forth a communication-efficient federated learning framework for both linear and deep GCCA under the maximum variance (MAX-VAR) formulation. The overhead issue is addressed by aggressively compressing (via quantizat",
    "path": "papers/21/09/2109.12400.json",
    "total_tokens": 1018,
    "translated_title": "通信高效的联邦线性和深度广义典型相关分析",
    "translated_abstract": "经典和深度广义典型相关分析（GCCA）算法分别使用线性变换和神经网络从多个“视图”（例如音频和图像）中寻找低维的共同表示。当这些视图由不同的计算代理（例如组织和边缘设备）进行获取和存储，并且由于隐私或通信成本等考虑不希望数据共享时，联邦学习的GCCA是合理的。在联邦学习中，这些视图在代理处保持本地，并且只允许与中央服务器进行导出的有限信息交换。然而，将现有的GCCA算法应用到这些联邦学习环境中可能会产生过高的通信开销。本研究提出了一种通信高效的联邦学习框架，用于最大方差（MAX-VAR）形式下的线性和深度GCCA。通过在传输前积极压缩（通过量化编码）本地数据统计信息和使用一种新颖的子空间共识算法来解决开销问题，该方法在极大地减少通信成本的同时实现了与现有中心化GCCA算法相当的竞争性能。",
    "tldr": "本文提出了一种通信高效的联邦学习框架，用于线性和深度广义典型相关分析，通过压缩本地数据统计信息和使用子空间共识算法减少通信成本，同时具有与中心化算法相当的性能。",
    "en_tdlr": "This paper proposes a communication-efficient federated learning framework for linear and deep generalized canonical correlation analysis (GCCA), which greatly reduces communication costs by compressing local data statistics and employing a novel subspace consensus algorithm, while achieving performance competitive with centralized GCCA algorithms."
}