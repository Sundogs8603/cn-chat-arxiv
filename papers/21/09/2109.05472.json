{
    "title": "Compute and Energy Consumption Trends in Deep Learning Inference. (arXiv:2109.05472v2 [cs.LG] UPDATED)",
    "abstract": "The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we study relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer ",
    "link": "http://arxiv.org/abs/2109.05472",
    "context": "Title: Compute and Energy Consumption Trends in Deep Learning Inference. (arXiv:2109.05472v2 [cs.LG] UPDATED)\nAbstract: The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we study relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer ",
    "path": "papers/21/09/2109.05472.json",
    "total_tokens": 937,
    "translated_title": "深度学习推理中的计算和能量消耗趋势",
    "translated_abstract": "一些人认为，深度学习等AI范式的进展与参数数量指数增长有关。有许多研究证实这些趋势，但这是否意味着能量消耗呈指数增长？为了回答这个问题，我们将重点放在推理成本上，而不是训练成本，因为前者占据了大部分的计算工作量，仅因为有乘法因素存在。此外，除了算法创新外，我们还考虑了更具体和强大的硬件（导致更高的FLOPS），这通常伴随着重要的能量效率优化。我们还将焦点从突破性论文的第一次实现转移到了一两年后的技术版本的巩固版本。在这个独特和全面的视角下，我们研究了计算机视觉和自然语言处理领域的相关模型：对于持续提高性能，我们看到一个更柔和的趋势。",
    "tldr": "本篇论文研究了深度学习推理中的计算和能量消耗趋势，重点关注推理成本而非训练成本。结果显示，除了算法创新外，更具体和强大的硬件通常伴随着重要的能量效率优化，导致推理成本的提高呈现柔和的趋势。",
    "en_tdlr": "This paper explores the trends in computing and energy consumption in deep learning inference, with a focus on inference costs rather than training costs. Results show that, apart from algorithmic innovations, more specific and powerful hardware usually leads to important energy efficiency optimizations, resulting in a softer trend in the increase of inference costs."
}