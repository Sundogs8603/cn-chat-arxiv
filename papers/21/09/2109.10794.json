{
    "title": "Entropic Issues in Likelihood-Based OOD Detection. (arXiv:2109.10794v2 [stat.ML] UPDATED)",
    "abstract": "Deep generative models trained by maximum likelihood remain very popular methods for reasoning about data probabilistically. However, it has been observed that they can assign higher likelihoods to out-of-distribution (OOD) data than in-distribution data, thus calling into question the meaning of these likelihood values. In this work we provide a novel perspective on this phenomenon, decomposing the average likelihood into a KL divergence term and an entropy term. We argue that the latter can explain the curious OOD behaviour mentioned above, suppressing likelihood values on datasets with higher entropy. Although our idea is simple, we have not seen it explored yet in the literature. This analysis provides further explanation for the success of OOD detection methods based on likelihood ratios, as the problematic entropy term cancels out in expectation. Finally, we discuss how this observation relates to recent success in OOD detection with manifold-supported models, for which the above",
    "link": "http://arxiv.org/abs/2109.10794",
    "context": "Title: Entropic Issues in Likelihood-Based OOD Detection. (arXiv:2109.10794v2 [stat.ML] UPDATED)\nAbstract: Deep generative models trained by maximum likelihood remain very popular methods for reasoning about data probabilistically. However, it has been observed that they can assign higher likelihoods to out-of-distribution (OOD) data than in-distribution data, thus calling into question the meaning of these likelihood values. In this work we provide a novel perspective on this phenomenon, decomposing the average likelihood into a KL divergence term and an entropy term. We argue that the latter can explain the curious OOD behaviour mentioned above, suppressing likelihood values on datasets with higher entropy. Although our idea is simple, we have not seen it explored yet in the literature. This analysis provides further explanation for the success of OOD detection methods based on likelihood ratios, as the problematic entropy term cancels out in expectation. Finally, we discuss how this observation relates to recent success in OOD detection with manifold-supported models, for which the above",
    "path": "papers/21/09/2109.10794.json",
    "total_tokens": 912,
    "translated_title": "基于最大似然的OOD检测中的熵问题",
    "translated_abstract": "最大似然训练的深度生成模型仍然是关于数据概率推理的流行方法。然而，观察到它们可能会分配比正向分布数据更高的可能性，因此质疑这些似然值的含义。本文提供了一种新的观察角度，将平均似然分解为KL散度项和熵项。我们认为后者可以解释上述奇怪的OOD行为，抑制具有更高熵的数据集上的似然值。虽然我们的思路很简单，但我们还没有看到它在文献中得到探讨。这种分析进一步解释了基于似然比的OOD检测方法成功的原因，因为问题熵项在期望中会抵消。最后，我们讨论了这一观察结果如何与最近在流形支持模型中的OOD检测成功相关。",
    "tldr": "本文研究了最大似然训练的深度生成模型的OOD检测问题，提出了一种新的观察角度，即将平均似然分解为KL散度项和熵项。后者可以解释模型可能会给OOD数据高似然值的现象，因为它抑制具有更高熵的数据集上的似然值。",
    "en_tdlr": "The paper investigates the issue of out-of-distribution (OOD) detection in deep generative models trained by maximum likelihood, and provides a novel perspective by decomposing the average likelihood into a KL divergence term and an entropy term. The latter can explain the phenomenon that the model may assign higher likelihoods to OOD data, as it suppresses likelihood values on datasets with higher entropy."
}