{
    "title": "Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Streaming Data. (arXiv:2109.07117v7 [cs.LG] UPDATED)",
    "abstract": "We introduce a streaming framework for analyzing stochastic approximation/optimization problems. This streaming framework is analogous to solving optimization problems using time-varying mini-batches that arrive sequentially. We provide non-asymptotic convergence rates of various gradient-based algorithms; this includes the famous Stochastic Gradient (SG) descent (a.k.a. Robbins-Monro algorithm), mini-batch SG and time-varying mini-batch SG algorithms, as well as their iterated averages (a.k.a. Polyak-Ruppert averaging). We show i) how to accelerate convergence by choosing the learning rate according to the time-varying mini-batches, ii) that Polyak-Ruppert averaging achieves optimal convergence in terms of attaining the Cramer-Rao lower bound, and iii) how time-varying mini-batches together with Polyak-Ruppert averaging can provide variance reduction and accelerate convergence simultaneously, which is advantageous for many learning problems, such as online, sequential, and large-scale",
    "link": "http://arxiv.org/abs/2109.07117",
    "context": "Title: Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Streaming Data. (arXiv:2109.07117v7 [cs.LG] UPDATED)\nAbstract: We introduce a streaming framework for analyzing stochastic approximation/optimization problems. This streaming framework is analogous to solving optimization problems using time-varying mini-batches that arrive sequentially. We provide non-asymptotic convergence rates of various gradient-based algorithms; this includes the famous Stochastic Gradient (SG) descent (a.k.a. Robbins-Monro algorithm), mini-batch SG and time-varying mini-batch SG algorithms, as well as their iterated averages (a.k.a. Polyak-Ruppert averaging). We show i) how to accelerate convergence by choosing the learning rate according to the time-varying mini-batches, ii) that Polyak-Ruppert averaging achieves optimal convergence in terms of attaining the Cramer-Rao lower bound, and iii) how time-varying mini-batches together with Polyak-Ruppert averaging can provide variance reduction and accelerate convergence simultaneously, which is advantageous for many learning problems, such as online, sequential, and large-scale",
    "path": "papers/21/09/2109.07117.json",
    "total_tokens": 969,
    "translated_title": "流数据随机逼近算法的非渐进分析",
    "translated_abstract": "我们引入了一个流式框架来分析随机逼近/优化问题。这个流式框架类似于使用逐步到达的时间变化的小批次来解决优化问题。我们提供了各种基于梯度的算法的非渐近收敛速度；这包括著名的随机梯度下降（SG）算法（也称为Robbins-Monro算法），小批量SG和时间变化的小批量SG算法，以及它们的迭代平均值（也称为Polyak-Ruppert平均）。我们展示了：i）如何通过根据时间变化的小批次来选择学习速率来加速收敛；ii）Polyak-Ruppert平均值在达到Cramer-Rao下界方面实现了最优收敛；iii）时间变化的小批次与Polyak-Ruppert平均值结合使用可以同时提供方差减少和加速收敛，这对于许多学习问题（如在线，顺序和大规模）都是有利的。",
    "tldr": "该论文介绍了流数据随机逼近算法的非渐近收敛速度，包括随机梯度下降、小批量SG和时间变化的小批量SG算法以及它们的迭代平均值，同时展示了加速收敛的方法和同时提供方差减少和加速收敛的优势。"
}