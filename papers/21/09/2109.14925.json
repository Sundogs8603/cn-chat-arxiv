{
    "title": "Genealogical Population-Based Training for Hyperparameter Optimization. (arXiv:2109.14925v2 [cs.LG] UPDATED)",
    "abstract": "HyperParameter Optimization (HPO) aims at finding the best HyperParameters (HPs) of learning models, such as neural networks, in the fastest and most efficient way possible. Most recent HPO algorithms try to optimize HPs regardless of the model that obtained them, assuming that for different models, same HPs will produce very similar results. We break free from this paradigm and propose a new take on preexisting methods that we called Genealogical Population Based Training (GPBT). GPBT, via the shared histories of \"genealogically\"-related models, exploit the coupling of HPs and models in an efficient way. We experimentally demonstrate that our method cuts down by 2 to 3 times the computational cost required, generally allows a 1% accuracy improvement on computer vision tasks, and reduces the variance of the results by an order of magnitude, compared to the current algorithms. Our method is search-algorithm agnostic so that the inner search routine can be any search algorithm like TPE, ",
    "link": "http://arxiv.org/abs/2109.14925",
    "context": "Title: Genealogical Population-Based Training for Hyperparameter Optimization. (arXiv:2109.14925v2 [cs.LG] UPDATED)\nAbstract: HyperParameter Optimization (HPO) aims at finding the best HyperParameters (HPs) of learning models, such as neural networks, in the fastest and most efficient way possible. Most recent HPO algorithms try to optimize HPs regardless of the model that obtained them, assuming that for different models, same HPs will produce very similar results. We break free from this paradigm and propose a new take on preexisting methods that we called Genealogical Population Based Training (GPBT). GPBT, via the shared histories of \"genealogically\"-related models, exploit the coupling of HPs and models in an efficient way. We experimentally demonstrate that our method cuts down by 2 to 3 times the computational cost required, generally allows a 1% accuracy improvement on computer vision tasks, and reduces the variance of the results by an order of magnitude, compared to the current algorithms. Our method is search-algorithm agnostic so that the inner search routine can be any search algorithm like TPE, ",
    "path": "papers/21/09/2109.14925.json",
    "total_tokens": 855,
    "translated_title": "基于家族谱的种群训练进行超参数优化",
    "translated_abstract": "超参数优化（HPO）旨在以最快、最高效的方式寻找学习模型（如神经网络）的最佳超参数（HP）。大多数最新的HPO算法尝试优化HP而不考虑获得HP的模型，假定对于不同的模型，相同的HP将产生非常相似的结果。我们打破了这种范式，并提出了一种新的方式，称为基因家谱种群训练（GPBT），通过“谱系”相关模型的共享历史，以高效的方式利用HP和模型之间的耦合。我们实验证明，与当前算法相比，我们的方法将计算成本降低了2到3倍，通常允许计算机视觉任务的1%精度提高，并将结果的差异降低了一个数量级。我们的方法搜索算法无关，因此内部搜索程序可以是任何搜索算法，如TPE。",
    "tldr": "基于家族谱的种群训练（GPBT）是一种新的超参数优化方法。不同于其他算法，GPBT利用相关模型之间的共享历史以高效地利用HP和模型之间的耦合。我们的方法通过降低计算成本，提高精度并减小结果的差异，取得了成功。"
}