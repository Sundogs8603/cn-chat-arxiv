{
    "title": "Computationally Efficient High-Dimensional Bayesian Optimization via Variable Selection",
    "abstract": "Bayesian Optimization (BO) is a method for globally optimizing black-box functions. While BO has been successfully applied to many scenarios, developing effective BO algorithms that scale to functions with high-dimensional domains is still a challenge. Optimizing such functions by vanilla BO is extremely time-consuming. Alternative strategies for high-dimensional BO that are based on the idea of embedding the high-dimensional space to the one with low dimension are sensitive to the choice of the embedding dimension, which needs to be pre-specified. We develop a new computationally efficient high-dimensional BO method that exploits variable selection. Our method is able to automatically learn axis-aligned sub-spaces, i.e. spaces containing selected variables, without the demand of any pre-specified hyperparameters. We theoretically analyze the computational complexity of our algorithm and derive the regret bound. We empirically show the efficacy of our method on several synthetic and re",
    "link": "https://arxiv.org/abs/2109.09264",
    "context": "Title: Computationally Efficient High-Dimensional Bayesian Optimization via Variable Selection\nAbstract: Bayesian Optimization (BO) is a method for globally optimizing black-box functions. While BO has been successfully applied to many scenarios, developing effective BO algorithms that scale to functions with high-dimensional domains is still a challenge. Optimizing such functions by vanilla BO is extremely time-consuming. Alternative strategies for high-dimensional BO that are based on the idea of embedding the high-dimensional space to the one with low dimension are sensitive to the choice of the embedding dimension, which needs to be pre-specified. We develop a new computationally efficient high-dimensional BO method that exploits variable selection. Our method is able to automatically learn axis-aligned sub-spaces, i.e. spaces containing selected variables, without the demand of any pre-specified hyperparameters. We theoretically analyze the computational complexity of our algorithm and derive the regret bound. We empirically show the efficacy of our method on several synthetic and re",
    "path": "papers/21/09/2109.09264.json",
    "total_tokens": 833,
    "translated_title": "变量选择的计算高效高维贝叶斯优化方法",
    "translated_abstract": "贝叶斯优化（BO）是一种用于全局优化黑盒函数的方法。虽然BO已成功应用于许多场景，但是开发能够适用于高维域函数的有效BO算法仍然是一个挑战。通过普通的BO优化此类函数非常耗时。基于将高维空间嵌入到低维空间的思想的高维BO的替代策略对嵌入维度的选择非常敏感，需要预先指定。我们开发了一种新的计算高效的高维BO方法，利用了变量选择。我们的方法能够自动学习轴对齐的子空间，即包含选定变量的空间，而无需任何预先指定的超参数。我们从理论上分析了算法的计算复杂性并得出了遗憾界限。我们在几个合成和真实数据上实验证明了我们方法的有效性。",
    "tldr": "本论文提出了一种变量选择的计算高效高维贝叶斯优化方法，能够自动学习子空间来优化高维域函数，同时减少了传统方法中的耗时问题，并在实验证明了方法的有效性。",
    "en_tdlr": "This paper presents a computationally efficient high-dimensional Bayesian optimization method via variable selection, which automatically learns subspaces to optimize high-dimensional domain functions, reducing the computational time compared to traditional methods, and has been empirically proven effective."
}