{
    "title": "Analysis of Regularized Learning for Linear-functional Data in Banach Spaces. (arXiv:2109.03159v6 [cs.LG] UPDATED)",
    "abstract": "In this article, we study the whole theory of regularized learning for linear-functional data in Banach spaces including representer theorems, pseudo-approximation theorems, and convergence theorems. The input training data are composed of linear functionals in the predual space of the Banach space to represent the discrete local information of multimodel data and multiscale models. The training data and the multi-loss functions are used to compute the empirical risks to approximate the expected risks, and the regularized learning is to minimize the regularized empirical risks over the Banach spaces. The exact solutions of the original problems are approximated globally by the regularized learning even if the original problems are unknown or unformulated. In the convergence theorems, we show the convergence of the approximate solutions to the exact solutions by the weak* topology of the Banach space. Moreover, the theorems of the regularized learning are applied to solve many problems ",
    "link": "http://arxiv.org/abs/2109.03159",
    "context": "Title: Analysis of Regularized Learning for Linear-functional Data in Banach Spaces. (arXiv:2109.03159v6 [cs.LG] UPDATED)\nAbstract: In this article, we study the whole theory of regularized learning for linear-functional data in Banach spaces including representer theorems, pseudo-approximation theorems, and convergence theorems. The input training data are composed of linear functionals in the predual space of the Banach space to represent the discrete local information of multimodel data and multiscale models. The training data and the multi-loss functions are used to compute the empirical risks to approximate the expected risks, and the regularized learning is to minimize the regularized empirical risks over the Banach spaces. The exact solutions of the original problems are approximated globally by the regularized learning even if the original problems are unknown or unformulated. In the convergence theorems, we show the convergence of the approximate solutions to the exact solutions by the weak* topology of the Banach space. Moreover, the theorems of the regularized learning are applied to solve many problems ",
    "path": "papers/21/09/2109.03159.json",
    "total_tokens": 941,
    "translated_title": "在巴拿赫空间中对线性函数数据的正则化学习的分析",
    "translated_abstract": "本文研究了巴拿赫空间中线性函数数据的正则化学习的整个理论，包括表示定理、伪逼近定理和收敛定理。输入的训练数据由巴拿赫空间的前对偶空间中的线性函数组成，以表示多模型数据和多尺度模型的离散局部信息。训练数据和多损失函数被用来计算经验风险以逼近期望风险，而正则化学习则是通过最小化巴拿赫空间上的正则化经验风险来实现的。即使原问题未知或未明确，正则化学习也可以全局逼近原问题的精确解。在收敛定理中，我们通过巴拿赫空间的弱*拓扑证明了近似解收敛于精确解。此外，正则化学习定理被应用于解决许多问题。",
    "tldr": "本文研究了巴拿赫空间中线性函数数据的正则化学习的整个理论，包括表示定理、伪逼近定理和收敛定理。正则化学习通过最小化正则化经验风险来逼近未知或未明确的原问题的精确解，并且可以应用于解决多种问题。",
    "en_tdlr": "This article studies the whole theory of regularized learning for linear-functional data in Banach spaces, including the representer theorems, pseudo-approximation theorems, and convergence theorems. Regularized learning minimizes the regularized empirical risks to approximate the exact solutions of unknown or unformulated problems and can be applied to solve various problems."
}