{
    "title": "Backdoor Attack on Hash-based Image Retrieval via Clean-label Data Poisoning. (arXiv:2109.08868v3 [cs.CV] UPDATED)",
    "abstract": "A backdoored deep hashing model is expected to behave normally on original query images and return the images with the target label when a specific trigger pattern presents. To this end, we propose the confusing perturbations-induced backdoor attack (CIBA). It injects a small number of poisoned images with the correct label into the training data, which makes the attack hard to be detected. To craft the poisoned images, we first propose the confusing perturbations to disturb the hashing code learning. As such, the hashing model can learn more about the trigger. The confusing perturbations are imperceptible and generated by optimizing the intra-class dispersion and inter-class shift in the Hamming space. We then employ the targeted adversarial patch as the backdoor trigger to improve the attack performance. We have conducted extensive experiments to verify the effectiveness of our proposed CIBA. Our code is available at https://github.com/KuofengGao/CIBA.",
    "link": "http://arxiv.org/abs/2109.08868",
    "context": "Title: Backdoor Attack on Hash-based Image Retrieval via Clean-label Data Poisoning. (arXiv:2109.08868v3 [cs.CV] UPDATED)\nAbstract: A backdoored deep hashing model is expected to behave normally on original query images and return the images with the target label when a specific trigger pattern presents. To this end, we propose the confusing perturbations-induced backdoor attack (CIBA). It injects a small number of poisoned images with the correct label into the training data, which makes the attack hard to be detected. To craft the poisoned images, we first propose the confusing perturbations to disturb the hashing code learning. As such, the hashing model can learn more about the trigger. The confusing perturbations are imperceptible and generated by optimizing the intra-class dispersion and inter-class shift in the Hamming space. We then employ the targeted adversarial patch as the backdoor trigger to improve the attack performance. We have conducted extensive experiments to verify the effectiveness of our proposed CIBA. Our code is available at https://github.com/KuofengGao/CIBA.",
    "path": "papers/21/09/2109.08868.json",
    "total_tokens": 923,
    "translated_title": "通过干净标签数据毒化对基于哈希的图像检索的后门攻击",
    "translated_abstract": "预期后门深度哈希模型在原始查询图像上表现正常，并在出现特定触发模式时返回带有目标标签的图像。为此，我们提出了混淆扰动引起的后门攻击（CIBA）。它将少量带有正确标签的毒化图像注入到训练数据中，使得攻击难以被检测到。为了制作毒化图像，我们首先提出了混淆扰动来干扰哈希码的学习。因此，哈希模型可以更多地了解触发器。混淆扰动在汉明空间中通过优化类内离散度和类间偏移生成，其几乎无法察觉。然后，我们使用有针对性的对抗性贴片作为后门触发器来提高攻击性能。我们进行了大量实验证明了我们提出的CIBA的有效性。我们的代码可在https://github.com/KuofengGao/CIBA找到。",
    "tldr": "本文提出了一种通过注入带有正确标签的毒化图像来实现后门攻击的方法，该攻击难以被检测到。通过优化哈希码学习和使用有针对性的对抗性贴片作为后门触发器，可以提高攻击性能。",
    "en_tdlr": "This paper presents a method for conducting a backdoor attack by injecting poisoned images with correct labels, which is hard to detect. By optimizing the learning of hashing codes and using targeted adversarial patches as backdoor triggers, the attack performance can be improved."
}