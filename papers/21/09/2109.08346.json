{
    "title": "Comfetch: Federated Learning of Large Networks on Constrained Clients via Sketching. (arXiv:2109.08346v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FL) is a popular paradigm for private and collaborative model training on the edge. In centralized FL, the parameters of a global architecture (such as a deep neural network) are maintained and distributed by a central server/controller to clients who transmit model updates (gradients) back to the server based on local optimization. While many efforts have focused on reducing the communication complexity of gradient transmission, the vast majority of compression-based algorithms assume that each participating client is able to download and train the current and full set of parameters, which may not be a practical assumption depending on the resource constraints of smaller clients such as mobile devices. In this work, we propose a simple yet effective novel algorithm, Comfetch, which allows clients to train large networks using reduced representations of the global architecture via the count sketch, which reduces local computational and memory costs along with bi-dir",
    "link": "http://arxiv.org/abs/2109.08346",
    "context": "Title: Comfetch: Federated Learning of Large Networks on Constrained Clients via Sketching. (arXiv:2109.08346v2 [cs.LG] UPDATED)\nAbstract: Federated learning (FL) is a popular paradigm for private and collaborative model training on the edge. In centralized FL, the parameters of a global architecture (such as a deep neural network) are maintained and distributed by a central server/controller to clients who transmit model updates (gradients) back to the server based on local optimization. While many efforts have focused on reducing the communication complexity of gradient transmission, the vast majority of compression-based algorithms assume that each participating client is able to download and train the current and full set of parameters, which may not be a practical assumption depending on the resource constraints of smaller clients such as mobile devices. In this work, we propose a simple yet effective novel algorithm, Comfetch, which allows clients to train large networks using reduced representations of the global architecture via the count sketch, which reduces local computational and memory costs along with bi-dir",
    "path": "papers/21/09/2109.08346.json",
    "total_tokens": 828,
    "translated_title": "Comfetch: 通过草图在受限客户端上开展大规模网络的联邦学习",
    "translated_abstract": "联邦学习（FL）是边缘上进行私密和协作模型训练的流行范式。在集中式FL中，全局架构的参数（如深度神经网络）由中央服务器/控制器维护和分发给客户端，后者根据本地优化向服务器传输模型更新（梯度）。尽管许多工作都致力于减少梯度传输的通信复杂性，但绝大多数基于压缩的算法都假设每个参与的客户端能够下载和训练当前和完整的参数集，这可能不是一个实际的假设，因为较小的客户端（如移动设备）可能具有资源限制。在这项工作中，我们提出了一种简单而有效的新算法Comfetch，它允许客户端使用全局架构的简化表示来训练大型网络，通过计数草图减少了本地计算和内存成本以及双向的数据传输。",
    "tldr": "Comfetch是一个通过使用草图的简化表示形式，允许资源受限的客户端进行大规模网络训练的算法。",
    "en_tdlr": "Comfetch is an algorithm that enables resource-constrained clients to train large networks by using simplified representations via sketches."
}