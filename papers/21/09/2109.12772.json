{
    "title": "Distributionally Robust Multiclass Classification and Applications in Deep Image Classifiers. (arXiv:2109.12772v2 [stat.ML] UPDATED)",
    "abstract": "We develop a Distributionally Robust Optimization (DRO) formulation for Multiclass Logistic Regression (MLR), which could tolerate data contaminated by outliers. The DRO framework uses a probabilistic ambiguity set defined as a ball of distributions that are close to the empirical distribution of the training set in the sense of the Wasserstein metric. We relax the DRO formulation into a regularized learning problem whose regularizer is a norm of the coefficient matrix. We establish out-of-sample performance guarantees for the solutions to our model, offering insights on the role of the regularizer in controlling the prediction error. We apply the proposed method in rendering deep Vision Transformer (ViT)-based image classifiers robust to random and adversarial attacks. Specifically, using the MNIST and CIFAR-10 datasets, we demonstrate reductions in test error rate by up to 83.5% and loss by up to 91.3% compared with baseline methods, by adopting a novel random training method.",
    "link": "http://arxiv.org/abs/2109.12772",
    "context": "Title: Distributionally Robust Multiclass Classification and Applications in Deep Image Classifiers. (arXiv:2109.12772v2 [stat.ML] UPDATED)\nAbstract: We develop a Distributionally Robust Optimization (DRO) formulation for Multiclass Logistic Regression (MLR), which could tolerate data contaminated by outliers. The DRO framework uses a probabilistic ambiguity set defined as a ball of distributions that are close to the empirical distribution of the training set in the sense of the Wasserstein metric. We relax the DRO formulation into a regularized learning problem whose regularizer is a norm of the coefficient matrix. We establish out-of-sample performance guarantees for the solutions to our model, offering insights on the role of the regularizer in controlling the prediction error. We apply the proposed method in rendering deep Vision Transformer (ViT)-based image classifiers robust to random and adversarial attacks. Specifically, using the MNIST and CIFAR-10 datasets, we demonstrate reductions in test error rate by up to 83.5% and loss by up to 91.3% compared with baseline methods, by adopting a novel random training method.",
    "path": "papers/21/09/2109.12772.json",
    "total_tokens": 1001,
    "translated_title": "分布鲁棒的多类分类及其在深度图像分类器中的应用",
    "translated_abstract": "我们提出了一种适用于多类逻辑回归的分布鲁棒优化（DRO）方法，可以容忍数据受到离群值的干扰。该DRO框架使用具有接近Wasserstein距离意义下的经验分布的分布球的概率模糊集来定义。我们将DRO形式化简为一个正则化的学习问题，其中正则化项是系数矩阵的范数。我们为我们模型的解决方案建立了样外性能保证，为我们控制预测误差的正则化器的作用提供了见解。我们将提出的方法应用于使基于深度Vision Transformer（ViT）的图像分类器对随机和对抗攻击具有鲁棒性。具体而言，我们使用MNIST和CIFAR-10数据集，通过采用一种新颖的随机训练方法，证明了测试错误率降低了高达83.5%，损失降低了高达91.3%与基线方法相比。",
    "tldr": "本研究提出了一种适用于多类逻辑回归的分布鲁棒优化方法，在深度图像分类器中得到了应用。这种方法可使图像分类器对随机和对抗攻击具有鲁棒性。在使用MNIST和CIFAR-10数据集时，相比于基准方法，通过采用新的随机训练方法，测试错误率的降低高达83.5%，损失降低高达91.3%。",
    "en_tdlr": "This study proposes a distributionally robust optimization method for multiclass logistic regression, which is applied in deep image classifiers to enhance their robustness to random and adversarial attacks. By adopting a novel random training method, the proposed method shows significant reductions in test error rate and loss compared to baseline methods using MNIST and CIFAR-10 datasets."
}