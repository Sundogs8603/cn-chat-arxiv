{
    "title": "User Tampering in Reinforcement Learning Recommender Systems. (arXiv:2109.04083v3 [cs.AI] UPDATED)",
    "abstract": "In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms -- 'user tampering.' User tampering is a situation where an RL-based recommender system may manipulate a media user's opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of po",
    "link": "http://arxiv.org/abs/2109.04083",
    "context": "Title: User Tampering in Reinforcement Learning Recommender Systems. (arXiv:2109.04083v3 [cs.AI] UPDATED)\nAbstract: In this paper, we introduce new formal methods and provide empirical evidence to highlight a unique safety concern prevalent in reinforcement learning (RL)-based recommendation algorithms -- 'user tampering.' User tampering is a situation where an RL-based recommender system may manipulate a media user's opinions through its suggestions as part of a policy to maximize long-term user engagement. We use formal techniques from causal modeling to critically analyze prevailing solutions proposed in the literature for implementing scalable RL-based recommendation systems, and we observe that these methods do not adequately prevent user tampering. Moreover, we evaluate existing mitigation strategies for reward tampering issues, and show that these methods are insufficient in addressing the distinct phenomenon of user tampering within the context of recommendations. We further reinforce our findings with a simulation study of an RL-based recommendation system focused on the dissemination of po",
    "path": "papers/21/09/2109.04083.json",
    "total_tokens": 866,
    "translated_title": "强化学习推荐系统中的用户篡改问题",
    "translated_abstract": "本文引入了新的形式化方法，并提供实证证据，突出了强化学习（RL）推荐算法中普遍存在的一种独特安全问题——'用户篡改'。用户篡改是指RL推荐系统可以通过其建议来操纵媒体用户的意见，作为一种最大化长期用户参与度的策略。我们使用因果建模的形式化技术对文献中提出的用于实施可扩展的RL推荐系统的解决方案进行了关键分析，发现这些方法并不能充分防止用户篡改。此外，我们评估了现有的奖励篡改问题的缓解策略，并表明这些方法不足以应对推荐场景中独特的用户篡改现象。我们进一步通过针对一个以传播为焦点的RL推荐系统的模拟研究来加强我们的发现。",
    "tldr": "本研究揭示了强化学习推荐系统中的一个独特安全问题——用户篡改，并提出了形式化方法和实证证据。现有方法不能有效防止用户篡改，并且现有的奖励篡改缓解策略也不足以解决这一问题。",
    "en_tdlr": "This study uncovers a unique safety concern in reinforcement learning-based recommender systems, namely user tampering, and provides formal methods and empirical evidence. Existing methods inadequately prevent user tampering, and current mitigation strategies for reward tampering are insufficient for addressing this issue."
}