{
    "title": "Releasing Graph Neural Networks with Differential Privacy Guarantees. (arXiv:2109.08907v2 [cs.LG] UPDATED)",
    "abstract": "With the increasing popularity of graph neural networks (GNNs) in several sensitive applications like healthcare and medicine, concerns have been raised over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership inference attacks, even if only black-box access to the trained model is granted. We propose PrivGNN, a privacy-preserving framework for releasing GNN models in a centralized setting. Assuming an access to a public unlabeled graph, PrivGNN provides a framework to release GNN models trained explicitly on public data along with knowledge obtained from the private data in a privacy preserving manner. PrivGNN combines the knowledge-distillation framework with the two noise mechanisms, random subsampling, and noisy labeling, to ensure rigorous privacy guarantees. We theoretically analyze our approach in the Renyi differential privacy framework. Besides, we show the solid experimental performance of our method compared to severa",
    "link": "http://arxiv.org/abs/2109.08907",
    "context": "Title: Releasing Graph Neural Networks with Differential Privacy Guarantees. (arXiv:2109.08907v2 [cs.LG] UPDATED)\nAbstract: With the increasing popularity of graph neural networks (GNNs) in several sensitive applications like healthcare and medicine, concerns have been raised over the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to privacy attacks, such as membership inference attacks, even if only black-box access to the trained model is granted. We propose PrivGNN, a privacy-preserving framework for releasing GNN models in a centralized setting. Assuming an access to a public unlabeled graph, PrivGNN provides a framework to release GNN models trained explicitly on public data along with knowledge obtained from the private data in a privacy preserving manner. PrivGNN combines the knowledge-distillation framework with the two noise mechanisms, random subsampling, and noisy labeling, to ensure rigorous privacy guarantees. We theoretically analyze our approach in the Renyi differential privacy framework. Besides, we show the solid experimental performance of our method compared to severa",
    "path": "papers/21/09/2109.08907.json",
    "total_tokens": 981,
    "translated_title": "具有差分隐私保证的发布图神经网络",
    "translated_abstract": "随着图神经网络（GNN）在医疗保健和医学等敏感应用中的日益流行，人们对训练的GNN的隐私方面提出了担忧。值得注意的是，即使只授予对训练模型的黑盒访问权限，GNN也容易受到隐私攻击，例如成员推断攻击。我们提出了PrivGNN，这是一个在集中式环境中发布GNN模型的隐私保护框架。假设可以访问公共未标记的图，PrivGNN提供了一个框架，以隐私保护的方式发布明确在公共数据上训练并获取来自私有数据的知识的GNN模型。PrivGNN将知识蒸馏框架与两个噪声机制（随机子采样和嘈杂标记）相结合，以确保严格的隐私保证。我们在Renyi差分隐私框架中对我们的方法进行了理论分析。此外，我们展示了我们的方法与多个基准方法相比的实验性能。",
    "tldr": "这篇论文提出了PrivGNN，一种隐私保护的框架，用于在集中式环境中发布具有差分隐私保证的图神经网络。该方法结合了知识蒸馏框架和两个噪声机制，通过在公共数据上训练和私有数据的知识，实现了严格的隐私保证。实验结果表明，该方法在性能上表现出色。"
}