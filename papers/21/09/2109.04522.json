{
    "title": "Asynchronous Iterations in Optimization: New Sequence Results and Sharper Algorithmic Guarantees. (arXiv:2109.04522v2 [math.OC] UPDATED)",
    "abstract": "We introduce novel convergence results for asynchronous iterations that appear in the analysis of parallel and distributed optimization algorithms. The results are simple to apply and give explicit estimates for how the degree of asynchrony impacts the convergence rates of the iterates. Our results shorten, streamline and strengthen existing convergence proofs for several asynchronous optimization methods and allow us to establish convergence guarantees for popular algorithms that were thus far lacking a complete theoretical understanding. Specifically, we use our results to derive better iteration complexity bounds for proximal incremental aggregated gradient methods, to obtain tighter guarantees depending on the average rather than maximum delay for the asynchronous stochastic gradient descent method, to provide less conservative analyses of the speedup conditions for asynchronous block-coordinate implementations of Krasnoselskii-Mann iterations, and to quantify the convergence rates",
    "link": "http://arxiv.org/abs/2109.04522",
    "context": "Title: Asynchronous Iterations in Optimization: New Sequence Results and Sharper Algorithmic Guarantees. (arXiv:2109.04522v2 [math.OC] UPDATED)\nAbstract: We introduce novel convergence results for asynchronous iterations that appear in the analysis of parallel and distributed optimization algorithms. The results are simple to apply and give explicit estimates for how the degree of asynchrony impacts the convergence rates of the iterates. Our results shorten, streamline and strengthen existing convergence proofs for several asynchronous optimization methods and allow us to establish convergence guarantees for popular algorithms that were thus far lacking a complete theoretical understanding. Specifically, we use our results to derive better iteration complexity bounds for proximal incremental aggregated gradient methods, to obtain tighter guarantees depending on the average rather than maximum delay for the asynchronous stochastic gradient descent method, to provide less conservative analyses of the speedup conditions for asynchronous block-coordinate implementations of Krasnoselskii-Mann iterations, and to quantify the convergence rates",
    "path": "papers/21/09/2109.04522.json",
    "total_tokens": 895,
    "translated_title": "优化中的异步迭代：新的序列结果和更加精准的算法保证",
    "translated_abstract": "我们介绍了一种新的收敛结果，可以应用于并行和分布式优化算法的异步迭代分析中。这些结果易于应用，并给出了迭代的异步程度如何影响收敛速度的明确估计。我们的结果缩短、简化和加强了现有的几种异步优化方法的收敛证明，并使我们能够为一些至今缺乏完整理论理解的流行算法建立收敛保证。具体来说，我们使用我们的结果来导出更好的迭代复杂度边界，以应用于近端增量聚合梯度方法，针对异步随机梯度下降方法基于平均而不是最大延迟提供更紧密的保证，为Krasnoselskii-Mann迭代的异步块坐标实现提供不那么保守的加速条件分析，量化收敛速度。",
    "tldr": "该论文介绍了一种新的收敛结果，可以应用于并行和分布式优化算法的异步迭代分析中。作者使用这一结果，使得数种并行优化算法的收敛证明变得更加精炼、简单同时还增强了原有的证明可信度，进而建立了至今缺乏完整理论理解的流行算法的收敛保证。",
    "en_tdlr": "This paper introduces a new convergence result applicable to asynchronous iterations in parallel and distributed optimization algorithms. The authors use this result to simplify and strengthen the convergence proofs of several parallel optimization algorithms and establish convergence guarantees for popular algorithms that were lacking complete theoretical understanding."
}