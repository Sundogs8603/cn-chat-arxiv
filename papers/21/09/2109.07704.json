{
    "title": "Federated Submodel Optimization for Hot and Cold Data Features. (arXiv:2109.07704v4 [cs.LG] UPDATED)",
    "abstract": "We study practical data characteristics underlying federated learning, where non-i.i.d. data from clients have sparse features, and a certain client's local data normally involves only a small part of the full model, called a submodel. Due to data sparsity, the classical federated averaging (FedAvg) algorithm or its variants will be severely slowed down, because when updating the global model, each client's zero update of the full model excluding its submodel is inaccurately aggregated. Therefore, we propose federated submodel averaging (FedSubAvg), ensuring that the expectation of the global update of each model parameter is equal to the average of the local updates of the clients who involve it. We theoretically proved the convergence rate of FedSubAvg by deriving an upper bound under a new metric called the element-wise gradient norm. In particular, this new metric can characterize the convergence of federated optimization over sparse data, while the conventional metric of squared g",
    "link": "http://arxiv.org/abs/2109.07704",
    "context": "Title: Federated Submodel Optimization for Hot and Cold Data Features. (arXiv:2109.07704v4 [cs.LG] UPDATED)\nAbstract: We study practical data characteristics underlying federated learning, where non-i.i.d. data from clients have sparse features, and a certain client's local data normally involves only a small part of the full model, called a submodel. Due to data sparsity, the classical federated averaging (FedAvg) algorithm or its variants will be severely slowed down, because when updating the global model, each client's zero update of the full model excluding its submodel is inaccurately aggregated. Therefore, we propose federated submodel averaging (FedSubAvg), ensuring that the expectation of the global update of each model parameter is equal to the average of the local updates of the clients who involve it. We theoretically proved the convergence rate of FedSubAvg by deriving an upper bound under a new metric called the element-wise gradient norm. In particular, this new metric can characterize the convergence of federated optimization over sparse data, while the conventional metric of squared g",
    "path": "papers/21/09/2109.07704.json",
    "total_tokens": 1025,
    "translated_title": "Federated Submodel Optimization for Hot and Cold Data Features（热点和冷门数据特征的联邦子模型优化）",
    "translated_abstract": "本文研究联邦学习中的实际数据特征，其中客户端的非独立同分布数据具有稀疏特征，并且某个客户端的本地数据通常仅涉及完整模型的一小部分，称为子模型。由于数据稀疏性，传统的联邦平均（FedAvg）算法或其变体将严重减慢，因为在更新全局模型时，每个客户端除其子模型外的零更新被不准确地聚合。因此，我们提出了联邦子模型平均（FedSubAvg），确保每个模型参数的全局更新的期望等于涉及它的客户端的本地更新的平均值。我们通过推导一个称为元素梯度范数的新度量上界来理论上证明了FedSubAvg的收敛速度。特别地，这个新度量可以表征在稀疏数据上的联邦优化收敛，而传统的平方梯度度量则无法完成这一任务。",
    "tldr": "本文提出了一种针对联邦学习稀疏数据特征的联邦子模型平均算法(FedSubAvg)，该算法可以有效避免数据稀疏问题导致的计算下降，并保证每个模型参数的全局更新期望等于涉及它的客户端的本地更新平均值。该算法的新度量元素梯度范数可以更好地表征在稀疏数据上的联邦优化收敛。",
    "en_tdlr": "This paper proposes a federated submodel averaging algorithm (FedSubAvg) for addressing the sparse data characteristics in federated learning. The algorithm avoids the computation slowdown caused by data sparsity and ensures that the expectation of each model parameter's global update equals the local updates' average of the involved clients. The paper also introduces a new metric, the element-wise gradient norm, for better characterizing the convergence of federated optimization over sparse data."
}