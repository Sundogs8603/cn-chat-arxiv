{
    "title": "Few-shot Quality-Diversity Optimization. (arXiv:2109.06826v3 [cs.LG] UPDATED)",
    "abstract": "In the past few years, a considerable amount of research has been dedicated to the exploitation of previous learning experiences and the design of Few-shot and Meta Learning approaches, in problem domains ranging from Computer Vision to Reinforcement Learning based control. A notable exception, where to the best of our knowledge, little to no effort has been made in this direction is Quality-Diversity (QD) optimization. QD methods have been shown to be effective tools in dealing with deceptive minima and sparse rewards in Reinforcement Learning. However, they remain costly due to their reliance on inherently sample inefficient evolutionary processes. We show that, given examples from a task distribution, information about the paths taken by optimization in parameter space can be leveraged to build a prior population, which when used to initialize QD methods in unseen environments, allows for few-shot adaptation. Our proposed method does not require backpropagation. It is simple to impl",
    "link": "http://arxiv.org/abs/2109.06826",
    "context": "Title: Few-shot Quality-Diversity Optimization. (arXiv:2109.06826v3 [cs.LG] UPDATED)\nAbstract: In the past few years, a considerable amount of research has been dedicated to the exploitation of previous learning experiences and the design of Few-shot and Meta Learning approaches, in problem domains ranging from Computer Vision to Reinforcement Learning based control. A notable exception, where to the best of our knowledge, little to no effort has been made in this direction is Quality-Diversity (QD) optimization. QD methods have been shown to be effective tools in dealing with deceptive minima and sparse rewards in Reinforcement Learning. However, they remain costly due to their reliance on inherently sample inefficient evolutionary processes. We show that, given examples from a task distribution, information about the paths taken by optimization in parameter space can be leveraged to build a prior population, which when used to initialize QD methods in unseen environments, allows for few-shot adaptation. Our proposed method does not require backpropagation. It is simple to impl",
    "path": "papers/21/09/2109.06826.json",
    "total_tokens": 837,
    "translated_title": "几乎零样本质量多样性优化",
    "translated_abstract": "在过去的几年中，已经有大量的研究致力于利用以前的学习经验和设计少样本学习方法和元学习方法，涉及的问题领域从计算机视觉到基于强化学习的控制。一个明显的例外是质量多样性(QD)优化，我们在这个方向上几乎没有做出努力。QD方法已经被证明是处理强化学习中的欺骗性极小值和稀疏奖励的有效工具。然而，由于它们依赖于本质上具有低样本效率的进化过程，它们仍然很昂贵。我们表明，通过利用参数空间中优化路径的信息，可以构建先验种群，当在未见环境中使用该种群初始化QD方法时，可以进行零样本适应。我们提出的方法不需要反向传播，实现起来简单。",
    "tldr": "本文提出了一种几乎零样本的质量多样性优化方法，通过利用参数空间中的优化路径信息构建先验种群，可以在未见环境中进行少样本适应。",
    "en_tdlr": "This paper proposes a few-shot quality-diversity optimization method that leverages optimization path information in the parameter space to build a prior population, enabling adaptation in unseen environments with minimal samples."
}