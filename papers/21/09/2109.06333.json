{
    "title": "Connecting degree and polarity: An artificial language learning study. (arXiv:2109.06333v2 [cs.CL] UPDATED)",
    "abstract": "We investigate a new linguistic generalization in pre-trained language models (taking BERT (Devlin et al., 2019) as a case study). We focus on degree modifiers (expressions like slightly, very, rather, extremely) and test the hypothesis that the degree expressed by a modifier (low, medium or high degree) is related to the modifier's sensitivity to sentence polarity (whether it shows preference for affirmative or negative sentences or neither). To probe this connection, we apply the Artificial Language Learning experimental paradigm from psycholinguistics to a neural language model. Our experimental results suggest that BERT generalizes in line with existing linguistic observations that relate degree semantics to polarity sensitivity, including the main one: low degree semantics is associated with preference towards positive polarity.",
    "link": "http://arxiv.org/abs/2109.06333",
    "context": "Title: Connecting degree and polarity: An artificial language learning study. (arXiv:2109.06333v2 [cs.CL] UPDATED)\nAbstract: We investigate a new linguistic generalization in pre-trained language models (taking BERT (Devlin et al., 2019) as a case study). We focus on degree modifiers (expressions like slightly, very, rather, extremely) and test the hypothesis that the degree expressed by a modifier (low, medium or high degree) is related to the modifier's sensitivity to sentence polarity (whether it shows preference for affirmative or negative sentences or neither). To probe this connection, we apply the Artificial Language Learning experimental paradigm from psycholinguistics to a neural language model. Our experimental results suggest that BERT generalizes in line with existing linguistic observations that relate degree semantics to polarity sensitivity, including the main one: low degree semantics is associated with preference towards positive polarity.",
    "path": "papers/21/09/2109.06333.json",
    "total_tokens": 808,
    "translated_title": "连接度和极性：一项人工语言学习研究",
    "translated_abstract": "我们调查了预训练语言模型（以BERT为案例研究）中的一种新的语言概括。我们关注程度修饰语（如稍微，非常，相当，极其）并测试一个假设，即修饰语表示的程度（低，中，高程度）与修饰语对句子极性的敏感性（是否更倾向于肯定句或否定句）相关。为了探索这种连接，我们将心理语言学中的人工语言学习实验范式应用于神经语言模型。我们的实验结果表明，BERT进行了与现有语言观察相一致的概括，将程度语义与极性敏感性相关联，其中主要观察结果是：低程度语义与对正极性的偏好相关联。",
    "tldr": "本研究调查了贝叶斯预训练语言模型（BERT）中的一个新的语言概括，发现程度修饰语与句子极性的敏感性相关，尤其是低程度语义与正极性的偏好相关联。",
    "en_tdlr": "This study investigates a new linguistic generalization in the BERT language model, revealing a connection between degree modifiers and sentence polarity sensitivity, particularly the association between low degree semantics and a preference for positive polarity."
}