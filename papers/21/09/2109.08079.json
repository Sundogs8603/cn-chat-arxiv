{
    "title": "Context-NER : Contextual Phrase Generation at Scale. (arXiv:2109.08079v4 [cs.IR] UPDATED)",
    "abstract": "Named Entity Recognition (NER) has seen significant progress in recent years, with numerous state-of-the-art (SOTA) models achieving high performance. However, very few studies have focused on the generation of entities' context. In this paper, we introduce CONTEXT-NER, a task that aims to generate the relevant context for entities in a sentence, where the context is a phrase describing the entity but not necessarily present in the sentence. To facilitate research in this task, we also present the EDGAR10-Q dataset, which consists of annual and quarterly reports from the top 1500 publicly traded companies. The dataset is the largest of its kind, containing 1M sentences, 2.8M entities, and an average of 35 tokens per sentence, making it a challenging dataset. We propose a baseline approach that combines a phrase generation algorithm with inferencing using a 220M language model, achieving a ROUGE-L score of 27% on the test split. Additionally, we perform a one-shot inference with ChatGPT",
    "link": "http://arxiv.org/abs/2109.08079",
    "context": "Title: Context-NER : Contextual Phrase Generation at Scale. (arXiv:2109.08079v4 [cs.IR] UPDATED)\nAbstract: Named Entity Recognition (NER) has seen significant progress in recent years, with numerous state-of-the-art (SOTA) models achieving high performance. However, very few studies have focused on the generation of entities' context. In this paper, we introduce CONTEXT-NER, a task that aims to generate the relevant context for entities in a sentence, where the context is a phrase describing the entity but not necessarily present in the sentence. To facilitate research in this task, we also present the EDGAR10-Q dataset, which consists of annual and quarterly reports from the top 1500 publicly traded companies. The dataset is the largest of its kind, containing 1M sentences, 2.8M entities, and an average of 35 tokens per sentence, making it a challenging dataset. We propose a baseline approach that combines a phrase generation algorithm with inferencing using a 220M language model, achieving a ROUGE-L score of 27% on the test split. Additionally, we perform a one-shot inference with ChatGPT",
    "path": "papers/21/09/2109.08079.json",
    "total_tokens": 935,
    "tldr": "本文介绍了 CONTEXT-NER 任务，即在句子中生成实体相关的上下文短语。此外，我们提供了一个数据集和基线方法，能够结合使用短语生成算法和推断技术，实现了在测试分裂上27％的 ROUGE-L 得分。"
}