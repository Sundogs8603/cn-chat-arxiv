{
    "title": "Neural network relief: a pruning algorithm based on neural activity",
    "abstract": "arXiv:2109.10795v3 Announce Type: replace  Abstract: Current deep neural networks (DNNs) are overparameterized and use most of their neuronal connections during inference for each task. The human brain, however, developed specialized regions for different tasks and performs inference with a small fraction of its neuronal connections. We propose an iterative pruning strategy introducing a simple importance-score metric that deactivates unimportant connections, tackling overparameterization in DNNs and modulating the firing patterns. The aim is to find the smallest number of connections that is still capable of solving a given task with comparable accuracy, i.e. a simpler subnetwork. We achieve comparable performance for LeNet architectures on MNIST, and significantly higher parameter compression than state-of-the-art algorithms for VGG and ResNet architectures on CIFAR-10/100 and Tiny-ImageNet. Our approach also performs well for the two different optimizers considered -- Adam and SGD. ",
    "link": "https://arxiv.org/abs/2109.10795",
    "context": "Title: Neural network relief: a pruning algorithm based on neural activity\nAbstract: arXiv:2109.10795v3 Announce Type: replace  Abstract: Current deep neural networks (DNNs) are overparameterized and use most of their neuronal connections during inference for each task. The human brain, however, developed specialized regions for different tasks and performs inference with a small fraction of its neuronal connections. We propose an iterative pruning strategy introducing a simple importance-score metric that deactivates unimportant connections, tackling overparameterization in DNNs and modulating the firing patterns. The aim is to find the smallest number of connections that is still capable of solving a given task with comparable accuracy, i.e. a simpler subnetwork. We achieve comparable performance for LeNet architectures on MNIST, and significantly higher parameter compression than state-of-the-art algorithms for VGG and ResNet architectures on CIFAR-10/100 and Tiny-ImageNet. Our approach also performs well for the two different optimizers considered -- Adam and SGD. ",
    "path": "papers/21/09/2109.10795.json",
    "total_tokens": 902,
    "translated_title": "基于神经活动的剪枝算法：神经网络Relief",
    "translated_abstract": "当前深度神经网络（DNNs）存在参数过多，在每个任务的推断过程中使用大部分神经元连接。然而，人脑为不同任务发展了专门的区域，并仅使用其神经连接的一小部分进行推断。我们提出了一种迭代剪枝策略，引入了一种简单的重要性评分指标，将无关紧要的连接停用，解决了DNNs中的参数过多问题，并调节了神经元的激活模式。旨在找到解决给定任务的最小连接数，同时保持可比较的准确性，即一个更简单的子网络。我们在MNIST上的LeNet架构获得了可比较的性能，并在CIFAR-10/100和Tiny-ImageNet上相比最先进的算法实现了更高程度的参数压缩，也在考虑的两种不同优化器--Adam和SGD--上表现良好。",
    "tldr": "提出了一种基于神经活动的迭代剪枝策略，通过引入简单的重要性评分指标来停用无关紧要的连接，解决了DNNs中的参数过多问题并实现了显著的参数压缩，在MNIST、CIFAR-10/100和Tiny-ImageNet数据集上达到了可比较的性能。"
}