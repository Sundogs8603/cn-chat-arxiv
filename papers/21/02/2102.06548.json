{
    "title": "Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis. (arXiv:2102.06548v4 [stat.ML] UPDATED)",
    "abstract": "Q-learning, which seeks to learn the optimal Q-function of a Markov decision process (MDP) in a model-free fashion, lies at the heart of reinforcement learning. When it comes to the synchronous setting (such that independent samples for all state-action pairs are drawn from a generative model in each iteration), substantial progress has been made towards understanding the sample efficiency of Q-learning. Consider a $\\gamma$-discounted infinite-horizon MDP with state space $\\mathcal{S}$ and action space $\\mathcal{A}$: to yield an entrywise $\\varepsilon$-approximation of the optimal Q-function, state-of-the-art theory for Q-learning requires a sample size exceeding the order of $\\frac{|\\mathcal{S}||\\mathcal{A}|}{(1-\\gamma)^5\\varepsilon^{2}}$, which fails to match existing minimax lower bounds. This gives rise to natural questions: what is the sharp sample complexity of Q-learning? Is Q-learning provably sub-optimal? This paper addresses these questions for the synchronous setting: (1) wh",
    "link": "http://arxiv.org/abs/2102.06548",
    "context": "Title: Is Q-Learning Minimax Optimal? A Tight Sample Complexity Analysis. (arXiv:2102.06548v4 [stat.ML] UPDATED)\nAbstract: Q-learning, which seeks to learn the optimal Q-function of a Markov decision process (MDP) in a model-free fashion, lies at the heart of reinforcement learning. When it comes to the synchronous setting (such that independent samples for all state-action pairs are drawn from a generative model in each iteration), substantial progress has been made towards understanding the sample efficiency of Q-learning. Consider a $\\gamma$-discounted infinite-horizon MDP with state space $\\mathcal{S}$ and action space $\\mathcal{A}$: to yield an entrywise $\\varepsilon$-approximation of the optimal Q-function, state-of-the-art theory for Q-learning requires a sample size exceeding the order of $\\frac{|\\mathcal{S}||\\mathcal{A}|}{(1-\\gamma)^5\\varepsilon^{2}}$, which fails to match existing minimax lower bounds. This gives rise to natural questions: what is the sharp sample complexity of Q-learning? Is Q-learning provably sub-optimal? This paper addresses these questions for the synchronous setting: (1) wh",
    "path": "papers/21/02/2102.06548.json",
    "total_tokens": 861,
    "translated_title": "Q学习是否是极小极大最优的？一项紧密的样本复杂性分析。",
    "translated_abstract": "Q学习是强化学习的核心，旨在以模型自由的方式学习马尔可夫决策过程的最优Q函数。针对同步设置（即在每次迭代中从生成模型中独立地抽取所有状态-动作对的样本），在理解Q学习的样本效率方面已经取得了重大进展。对于一个具有状态空间Σ和动作空间Α的γ折扣的无限时间阶段MDP，为了产生最优Q函数的元素级ε近似，针对Q学习的最新理论需要一个样本大小超过Σ∣∣×Α∣∣∕(1−γ)^5ε^{2}的量级，但这并不符合现有的极小极大下界。这引出了一个自然的问题：Q学习的样本复杂性是多少？Q学习是否可证明是次优的？本文针对同步设置回答了这些问题：(1)",
    "tldr": "本文通过紧密的样本复杂度分析回答了Q学习是否是极小极大最优的问题。",
    "en_tdlr": "This paper addresses the question of whether Q-learning is minimax optimal through a tight sample complexity analysis."
}