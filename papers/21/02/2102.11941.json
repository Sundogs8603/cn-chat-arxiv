{
    "title": "State Augmented Constrained Reinforcement Learning: Overcoming the Limitations of Learning with Rewards. (arXiv:2102.11941v2 [cs.LG] UPDATED)",
    "abstract": "A common formulation of constrained reinforcement learning involves multiple rewards that must individually accumulate to given thresholds. In this class of problems, we show a simple example in which the desired optimal policy cannot be induced by any weighted linear combination of rewards. Hence, there exist constrained reinforcement learning problems for which neither regularized nor classical primal-dual methods yield optimal policies. This work addresses this shortcoming by augmenting the state with Lagrange multipliers and reinterpreting primal-dual methods as the portion of the dynamics that drives the multipliers evolution. This approach provides a systematic state augmentation procedure that is guaranteed to solve reinforcement learning problems with constraints. Thus, as we illustrate by an example, while previous methods can fail at finding optimal policies, running the dual dynamics while executing the augmented policy yields an algorithm that provably samples actions from ",
    "link": "http://arxiv.org/abs/2102.11941",
    "context": "Title: State Augmented Constrained Reinforcement Learning: Overcoming the Limitations of Learning with Rewards. (arXiv:2102.11941v2 [cs.LG] UPDATED)\nAbstract: A common formulation of constrained reinforcement learning involves multiple rewards that must individually accumulate to given thresholds. In this class of problems, we show a simple example in which the desired optimal policy cannot be induced by any weighted linear combination of rewards. Hence, there exist constrained reinforcement learning problems for which neither regularized nor classical primal-dual methods yield optimal policies. This work addresses this shortcoming by augmenting the state with Lagrange multipliers and reinterpreting primal-dual methods as the portion of the dynamics that drives the multipliers evolution. This approach provides a systematic state augmentation procedure that is guaranteed to solve reinforcement learning problems with constraints. Thus, as we illustrate by an example, while previous methods can fail at finding optimal policies, running the dual dynamics while executing the augmented policy yields an algorithm that provably samples actions from ",
    "path": "papers/21/02/2102.11941.json",
    "total_tokens": 868,
    "translated_title": "增强限制性强化学习：克服学习奖励的局限性",
    "translated_abstract": "传统的限制性强化学习引入了多个奖励，这些奖励必须分别累积到给定的阈值。在这类问题中，我们展示了一个简单的例子，其中无法通过任何加权线性组合的奖励来诱导出理想的最优策略。因此，存在一些限制性强化学习问题，常规化和经典的原始-对偶方法都无法得到最优策略。本文通过增加拉格朗日乘子，并将原始-对偶方法重新解释为驱动乘子演化的动力学部分，来解决这个问题。这种方法提供了一个系统的状态增强程序，能够保证解决带有约束的强化学习问题。因此，正如我们通过一个例子所示，尽管之前的方法可能无法找到最优策略，但在执行增强策略时运行对偶动力学可以从中获得一种可以证明采样动作的算法。",
    "tldr": "本文提出了一种增强限制性强化学习的方法，通过增加状态的拉格朗日乘子并重新解释原始-对偶方法，可以解决传统方法无法得到最优策略的问题。",
    "en_tdlr": "This paper proposes a method for state-augmented constrained reinforcement learning, which addresses the limitations of traditional methods in finding optimal policies by introducing Lagrange multipliers and reinterpreting primal-dual methods."
}