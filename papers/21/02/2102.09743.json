{
    "title": "Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques. (arXiv:2102.09743v4 [cs.LG] UPDATED)",
    "abstract": "We investigate the optimization aspects of personalized Federated Learning (FL). We propose general optimizers that can be applied to numerous existing personalized FL objectives, specifically a tailored variant of Local SGD and variants of accelerated coordinate descent/accelerated SVRCD. By examining a general personalized objective capable of recovering many existing personalized FL objectives as special cases, we develop a comprehensive optimization theory applicable to a wide range of strongly convex personalized FL models in the literature. We showcase the practicality and/or optimality of our methods in terms of communication and local computation. Remarkably, our general optimization solvers and theory can recover the best-known communication and computation guarantees for addressing specific personalized FL objectives. Consequently, our proposed methods can serve as universal optimizers, rendering the design of task-specific optimizers unnecessary in many instances.",
    "link": "http://arxiv.org/abs/2102.09743",
    "context": "Title: Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques. (arXiv:2102.09743v4 [cs.LG] UPDATED)\nAbstract: We investigate the optimization aspects of personalized Federated Learning (FL). We propose general optimizers that can be applied to numerous existing personalized FL objectives, specifically a tailored variant of Local SGD and variants of accelerated coordinate descent/accelerated SVRCD. By examining a general personalized objective capable of recovering many existing personalized FL objectives as special cases, we develop a comprehensive optimization theory applicable to a wide range of strongly convex personalized FL models in the literature. We showcase the practicality and/or optimality of our methods in terms of communication and local computation. Remarkably, our general optimization solvers and theory can recover the best-known communication and computation guarantees for addressing specific personalized FL objectives. Consequently, our proposed methods can serve as universal optimizers, rendering the design of task-specific optimizers unnecessary in many instances.",
    "path": "papers/21/02/2102.09743.json",
    "total_tokens": 944,
    "translated_title": "个性化联邦学习：统一框架和通用优化技术",
    "translated_abstract": "本研究探讨了个性化联邦学习（FL）的优化方面。我们提出了通用优化器，可应用于许多现有的个性化FL目标，特别是一种定制的本地SGD变体和加速坐标下降/加速SVRCD的变体。通过研究一般的个性化目标，能够将许多现有的个性化FL目标作为特殊情况恢复，我们开发了一种应用于文献中广泛的一类强凸性个性化FL模型的全面优化理论。我们展示了我们的方法在通信和本地计算方面的实用性和/或优越性。值得注意的是，我们的通用优化求解器和理论可以恢复用于特定个性化FL目标的最佳已知通信和计算保证。因此，我们提出的方法可以作为通用优化器，在许多情况下可以免去设计特定任务优化器的需要。",
    "tldr": "本文探讨了个性化联邦学习的优化问题，并提出了适用于多种现有个性化FL目标的通用优化器。同时，提出了一个适用于广泛一类强凸性个性化FL模型的全面优化理论。其方法在通信和本地计算方面具有实用性和优越性，并能够恢复应对特定个性化FL目标的最佳计算和通信保证。",
    "en_tdlr": "This paper investigates the optimization aspects of personalized Federated Learning (FL). They propose general optimizers that can be applied to numerous existing personalized FL objectives and develop a comprehensive optimization theory applicable to a wide range of strongly convex personalized FL models in the literature. Their proposed methods can serve as universal optimizers and recover the best-known communication and computation guarantees for addressing specific personalized FL objectives."
}