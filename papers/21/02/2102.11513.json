{
    "title": "Mixed Policy Gradient: off-policy reinforcement learning driven jointly by data and model",
    "abstract": "arXiv:2102.11513v2 Announce Type: replace  Abstract: Reinforcement learning (RL) shows great potential in sequential decision-making. At present, mainstream RL algorithms are data-driven, which usually yield better asymptotic performance but much slower convergence compared with model-driven methods. This paper proposes mixed policy gradient (MPG) algorithm, which fuses the empirical data and the transition model in policy gradient (PG) to accelerate convergence without performance degradation. Formally, MPG is constructed as a weighted average of the data-driven and model-driven PGs, where the former is the derivative of the learned Q-value function, and the latter is that of the model-predictive return. To guide the weight design, we analyze and compare the upper bound of each PG error. Relying on that, a rule-based method is employed to heuristically adjust the weights. In particular, to get a better PG, the weight of the data-driven PG is designed to grow along the learning process",
    "link": "https://arxiv.org/abs/2102.11513",
    "context": "Title: Mixed Policy Gradient: off-policy reinforcement learning driven jointly by data and model\nAbstract: arXiv:2102.11513v2 Announce Type: replace  Abstract: Reinforcement learning (RL) shows great potential in sequential decision-making. At present, mainstream RL algorithms are data-driven, which usually yield better asymptotic performance but much slower convergence compared with model-driven methods. This paper proposes mixed policy gradient (MPG) algorithm, which fuses the empirical data and the transition model in policy gradient (PG) to accelerate convergence without performance degradation. Formally, MPG is constructed as a weighted average of the data-driven and model-driven PGs, where the former is the derivative of the learned Q-value function, and the latter is that of the model-predictive return. To guide the weight design, we analyze and compare the upper bound of each PG error. Relying on that, a rule-based method is employed to heuristically adjust the weights. In particular, to get a better PG, the weight of the data-driven PG is designed to grow along the learning process",
    "path": "papers/21/02/2102.11513.json",
    "total_tokens": 893,
    "translated_title": "混合策略梯度：数据与模型共同驱动的脱机强化学习",
    "translated_abstract": "强化学习在顺序决策中显示出巨大潜力。目前，主流的强化学习算法是数据驱动的，通常在渐近性能方面表现更好，但与模型驱动方法相比收敛速度要慢得多。本文提出了混合策略梯度（MPG）算法，该算法将经验数据和策略梯度（PG）中的转移模型融合在一起，以加速收敛而不会降低性能。形式上，MPG被构建为数据驱动和模型驱动PG的加权平均，其中前者是学习的Q值函数的导数，而后者是模型预测回报的导数。为指导权重设计，我们分析并比较每个PG误差的上限。依靠这一点，采用了基于规则的方法来启发性地调整权重。特别地，要获得更好的PG，数据驱动PG的权重被设计成沿着学习过程增长。",
    "tldr": "该论文提出了混合策略梯度（MPG）算法，通过融合数据驱动和模型驱动的策略梯度，加速强化学习的收敛速度而不降低性能。",
    "en_tdlr": "The paper introduces the mixed policy gradient (MPG) algorithm, which accelerates the convergence of reinforcement learning by combining data-driven and model-driven policy gradients without performance degradation."
}