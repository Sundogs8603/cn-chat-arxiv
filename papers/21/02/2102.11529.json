{
    "title": "Differentiable Logic Machines. (arXiv:2102.11529v5 [cs.AI] UPDATED)",
    "abstract": "The integration of reasoning, learning, and decision-making is key to build more general artificial intelligence systems. As a step in this direction, we propose a novel neural-logic architecture, called differentiable logic machine (DLM), that can solve both inductive logic programming (ILP) and reinforcement learning (RL) problems, where the solution can be interpreted as a first-order logic program. Our proposition includes several innovations. Firstly, our architecture defines a restricted but expressive continuous relaxation of the space of first-order logic programs by assigning weights to predicates instead of rules, in contrast to most previous neural-logic approaches. Secondly, with this differentiable architecture, we propose several (supervised and RL) training procedures, based on gradient descent, which can recover a fully-interpretable solution (i.e., logic formula). Thirdly, to accelerate RL training, we also design a novel critic architecture that enables actor-critic a",
    "link": "http://arxiv.org/abs/2102.11529",
    "context": "Title: Differentiable Logic Machines. (arXiv:2102.11529v5 [cs.AI] UPDATED)\nAbstract: The integration of reasoning, learning, and decision-making is key to build more general artificial intelligence systems. As a step in this direction, we propose a novel neural-logic architecture, called differentiable logic machine (DLM), that can solve both inductive logic programming (ILP) and reinforcement learning (RL) problems, where the solution can be interpreted as a first-order logic program. Our proposition includes several innovations. Firstly, our architecture defines a restricted but expressive continuous relaxation of the space of first-order logic programs by assigning weights to predicates instead of rules, in contrast to most previous neural-logic approaches. Secondly, with this differentiable architecture, we propose several (supervised and RL) training procedures, based on gradient descent, which can recover a fully-interpretable solution (i.e., logic formula). Thirdly, to accelerate RL training, we also design a novel critic architecture that enables actor-critic a",
    "path": "papers/21/02/2102.11529.json",
    "total_tokens": 990,
    "translated_title": "可微分逻辑机器",
    "translated_abstract": "推理、学习和决策的集成是构建更通用人工智能系统的关键。为此，我们提出了一种新颖的神经逻辑架构，称为可微分逻辑机器（DLM），可以解决归纳逻辑编程（ILP）和强化学习（RL）问题，其中解决方案可以被解释为一阶逻辑程序。我们的提议包括几个创新点。首先，我们的架构通过为谓词分配权重而不是规则，定义了一种受限但富有表现力的一阶逻辑程序的连续松弛，与大多数先前的神经逻辑方法不同。其次，通过这种可微分的架构，我们提出了几种（监督和RL）训练过程，基于梯度下降，可以恢复一个完全可解释的解决方案（即逻辑公式）。第三，为了加速RL训练，我们还设计了一种新颖的评论家架构，可以实现演员评论家",
    "tldr": "可微分逻辑机器 (DLM) 是一种新颖的神经逻辑架构，可以解决归纳逻辑编程 (ILP) 和强化学习 (RL) 问题，其创新之处在于定义了一种受限但富有表现力的一阶逻辑程序的连续松弛，并提供了可解释的解决方案。通过梯度下降进行训练，同时设计了一种新颖的评论家架构用于加速强化学习训练。"
}