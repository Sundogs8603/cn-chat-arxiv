{
    "title": "Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v19 [cs.LG] UPDATED)",
    "abstract": "Many complex multi-agent systems such as robot swarms control and autonomous vehicle coordination can be modeled as Multi-Agent Reinforcement Learning (MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a baseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge (SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX target relaxing the monotonicity constraint of QMIX, allowing for performance improvement in SMAC. In this paper, we investigate the code-level optimizations of these variants and the monotonicity constraint. (1) We find that such improvements of the variants are significantly affected by various code-level optimizations. (2) The experiment results show that QMIX with normalized optimizations outperforms other works in SMAC; (3) beyond the common wisdom from these works, the monotonicity constraint can improve sample efficiency in SMAC and DEPP. We also discuss why monotonicity constraints work well in purely coopera",
    "link": "http://arxiv.org/abs/2102.03479",
    "context": "Title: Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v19 [cs.LG] UPDATED)\nAbstract: Many complex multi-agent systems such as robot swarms control and autonomous vehicle coordination can be modeled as Multi-Agent Reinforcement Learning (MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a baseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge (SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX target relaxing the monotonicity constraint of QMIX, allowing for performance improvement in SMAC. In this paper, we investigate the code-level optimizations of these variants and the monotonicity constraint. (1) We find that such improvements of the variants are significantly affected by various code-level optimizations. (2) The experiment results show that QMIX with normalized optimizations outperforms other works in SMAC; (3) beyond the common wisdom from these works, the monotonicity constraint can improve sample efficiency in SMAC and DEPP. We also discuss why monotonicity constraints work well in purely coopera",
    "path": "papers/21/02/2102.03479.json",
    "total_tokens": 951,
    "translated_title": "重新思考合作多智能体强化学习中的实现技巧和单调性约束",
    "translated_abstract": "许多复杂的多智能体系统，如机器人集群控制和自主车辆协调，都可以被建模为多智能体强化学习（MARL）任务。QMIX是一种广泛使用的MARL算法，已被用作基准环境，例如星际争霸多智能体挑战赛（SMAC）和升级版的Predator-Prey（DEPP）。最近，QMIX的变体旨在放松QMIX的单调性约束，从而提高SMAC的性能。本文研究了这些变体的代码级优化和单调性约束。(1)我们发现这些变体的改进受到各种代码级优化的显著影响；(2)实验结果表明，带有标准化优化的QMIX在SMAC中的表现优于其他算法；(3)除了这些算法中的常识，单调性约束还可以提高SMAC和DEPP的采样效率。我们还讨论了为什么单调性约束在纯合作MARL中效果良好。",
    "tldr": "本文研究了QMIX算法的变体和单调性约束的实现技巧，发现标准化优化对SMAC环境的表现有显著影响；单调性约束能提高SMAC和DEPP的采样效率。"
}