{
    "title": "How good is Good-Turing for Markov samples?. (arXiv:2102.01938v3 [cs.IT] UPDATED)",
    "abstract": "The Good-Turing (GT) estimator for the missing mass (i.e., total probability of missing symbols) in $n$ samples is the number of symbols that appeared exactly once divided by $n$. For i.i.d. samples, the bias and squared-error risk of the GT estimator can be shown to fall as $1/n$ by bounding the expected error uniformly over all symbols. In this work, we study convergence of the GT estimator for missing stationary mass (i.e., total stationary probability of missing symbols) of Markov samples on an alphabet $\\mathcal{X}$ with stationary distribution $[\\pi_x:x \\in \\mathcal{X}]$ and transition probability matrix (t.p.m.) $P$. This is an important and interesting problem because GT is widely used in applications with temporal dependencies such as language models assigning probabilities to word sequences, which are modelled as Markov. We show that convergence of GT depends on convergence of $(P^{\\sim x})^n$, where $P^{\\sim x}$ is $P$ with the $x$-th column zeroed out. This, in turn, depend",
    "link": "http://arxiv.org/abs/2102.01938",
    "context": "Title: How good is Good-Turing for Markov samples?. (arXiv:2102.01938v3 [cs.IT] UPDATED)\nAbstract: The Good-Turing (GT) estimator for the missing mass (i.e., total probability of missing symbols) in $n$ samples is the number of symbols that appeared exactly once divided by $n$. For i.i.d. samples, the bias and squared-error risk of the GT estimator can be shown to fall as $1/n$ by bounding the expected error uniformly over all symbols. In this work, we study convergence of the GT estimator for missing stationary mass (i.e., total stationary probability of missing symbols) of Markov samples on an alphabet $\\mathcal{X}$ with stationary distribution $[\\pi_x:x \\in \\mathcal{X}]$ and transition probability matrix (t.p.m.) $P$. This is an important and interesting problem because GT is widely used in applications with temporal dependencies such as language models assigning probabilities to word sequences, which are modelled as Markov. We show that convergence of GT depends on convergence of $(P^{\\sim x})^n$, where $P^{\\sim x}$ is $P$ with the $x$-th column zeroed out. This, in turn, depend",
    "path": "papers/21/02/2102.01938.json",
    "total_tokens": 980,
    "translated_title": "Good-Turing在马尔可夫采样中的应用研究",
    "translated_abstract": "Good-Turing（GT）估计器用于估计$n$个样本中缺失的质量（即缺失符号的总概率）是出现一次的符号数量除以$n$。本文研究了在具有状态分布$[\\pi_x:x \\in \\mathcal{X}]$和转移概率矩阵（t.p.m.）$P$的字母表$\\mathcal{X}$上，Markov样本的缺失稳态质量（即缺失符号的总稳态概率）的GT估计器的收敛性。我们展示GT的收敛性取决于$(P^{\\sim x})^n$的收敛性，其中$P^{\\sim x}$是在$P$的第$x$列置零后的矩阵。这个问题对于具有时间依赖性的应用程序非常重要和有趣，比如将概率赋给单词序列的语言模型，这些模型被建模为Markov模型。",
    "tldr": "研究了在具有状态分布$[\\pi_x:x \\in \\mathcal{X}]$和转移概率矩阵（t.p.m.）$P$的字母表$\\mathcal{X}$上，Markov样本的缺失稳态质量（即缺失符号的总稳态概率）的GT估计器的收敛性。",
    "en_tdlr": "This paper studies the convergence of the Good-Turing estimator for missing stationary mass of Markov samples on an alphabet $\\mathcal{X}$ with stationary distribution $[\\pi_x:x \\in \\mathcal{X}]$ and transition probability matrix $P$. The study is important for applications with temporal dependencies such as language models and the convergence of GT is shown to depend on convergence of $(P^{\\sim x})^n$."
}