{
    "title": "Gaze Regularized Imitation Learning: Learning Continuous Control from Human Gaze. (arXiv:2102.13008v2 [cs.LG] UPDATED)",
    "abstract": "Approaches for teaching learning agents via human demonstrations have been widely studied and successfully applied to multiple domains. However, the majority of imitation learning work utilizes only behavioral information from the demonstrator, i.e. which actions were taken, and ignores other useful information. In particular, eye gaze information can give valuable insight towards where the demonstrator is allocating visual attention, and holds the potential to improve agent performance and generalization. In this work, we propose Gaze Regularized Imitation Learning (GRIL), a novel context-aware, imitation learning architecture that learns concurrently from both human demonstrations and eye gaze to solve tasks where visual attention provides important context. We apply GRIL to a visual navigation task, in which an unmanned quadrotor is trained to search for and navigate to a target vehicle in a photorealistic simulated environment. We show that GRIL outperforms several state-of-the-art",
    "link": "http://arxiv.org/abs/2102.13008",
    "context": "Title: Gaze Regularized Imitation Learning: Learning Continuous Control from Human Gaze. (arXiv:2102.13008v2 [cs.LG] UPDATED)\nAbstract: Approaches for teaching learning agents via human demonstrations have been widely studied and successfully applied to multiple domains. However, the majority of imitation learning work utilizes only behavioral information from the demonstrator, i.e. which actions were taken, and ignores other useful information. In particular, eye gaze information can give valuable insight towards where the demonstrator is allocating visual attention, and holds the potential to improve agent performance and generalization. In this work, we propose Gaze Regularized Imitation Learning (GRIL), a novel context-aware, imitation learning architecture that learns concurrently from both human demonstrations and eye gaze to solve tasks where visual attention provides important context. We apply GRIL to a visual navigation task, in which an unmanned quadrotor is trained to search for and navigate to a target vehicle in a photorealistic simulated environment. We show that GRIL outperforms several state-of-the-art",
    "path": "papers/21/02/2102.13008.json",
    "total_tokens": 905,
    "tldr": "GRIL是一种新颖的上下文感知的模仿学习架构，使用人类演示和眼动数据一起学习，提高了智能体的性能。在视觉导航任务中表现优于现有最先进算法。",
    "en_tdlr": "GRIL is a novel context-aware imitation learning architecture that improves agent performance by learning from both human demonstrations and eye gaze. It outperforms several state-of-the-art algorithms in a visual navigation task."
}