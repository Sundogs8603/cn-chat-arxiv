{
    "title": "Enabling Binary Neural Network Training on the Edge. (arXiv:2102.04270v6 [cs.LG] UPDATED)",
    "abstract": "The ever-growing computational demands of increasingly complex machine learning models frequently necessitate the use of powerful cloud-based infrastructure for their training. Binary neural networks are known to be promising candidates for on-device inference due to their extreme compute and memory savings over higher-precision alternatives. However, their existing training methods require the concurrent storage of high-precision activations for all layers, generally making learning on memory-constrained devices infeasible. In this article, we demonstrate that the backward propagation operations needed for binary neural network training are strongly robust to quantization, thereby making on-the-edge learning with modern models a practical proposition. We introduce a low-cost binary neural network training strategy exhibiting sizable memory footprint reductions while inducing little to no accuracy loss vs Courbariaux & Bengio's standard approach. These decreases are primarily enabled t",
    "link": "http://arxiv.org/abs/2102.04270",
    "context": "Title: Enabling Binary Neural Network Training on the Edge. (arXiv:2102.04270v6 [cs.LG] UPDATED)\nAbstract: The ever-growing computational demands of increasingly complex machine learning models frequently necessitate the use of powerful cloud-based infrastructure for their training. Binary neural networks are known to be promising candidates for on-device inference due to their extreme compute and memory savings over higher-precision alternatives. However, their existing training methods require the concurrent storage of high-precision activations for all layers, generally making learning on memory-constrained devices infeasible. In this article, we demonstrate that the backward propagation operations needed for binary neural network training are strongly robust to quantization, thereby making on-the-edge learning with modern models a practical proposition. We introduce a low-cost binary neural network training strategy exhibiting sizable memory footprint reductions while inducing little to no accuracy loss vs Courbariaux & Bengio's standard approach. These decreases are primarily enabled t",
    "path": "papers/21/02/2102.04270.json",
    "total_tokens": 847,
    "translated_title": "在边缘设备上实现二值神经网络训练",
    "translated_abstract": "随着越来越复杂的机器学习模型的计算需求不断增长，通常需要使用强大的云基础设施进行训练。二值神经网络由于其与高精度替代方案相比极高的计算和内存节省，被认为是在设备上进行推断的有希望的选择。然而，现有的训练方法需要同时存储所有层的高精度激活，这通常使得在内存受限设备上的学习变得不可行。在本文中，我们展示了二值神经网络训练所需的反向传播操作对量化非常强大，从而使得使用现代模型进行边缘学习成为一个实际的选择。我们提出了一种低成本的二值神经网络训练策略，具有显著的内存占用减少，同时对比Courbariaux和Bengio的标准方法，几乎没有降低准确率。",
    "tldr": "本文研究了在边缘设备上实现二值神经网络训练的方法，并展示了通过量化反向传播操作实现了显著的内存占用减少，同时几乎没有降低准确率。",
    "en_tdlr": "This study investigates the method of enabling binary neural network training on the edge and demonstrates significant reductions in memory footprint while maintaining almost no loss in accuracy through quantization-based backward propagation operations."
}