{
    "title": "Continuous Mean-Covariance Bandits. (arXiv:2102.12090v5 [cs.LG] UPDATED)",
    "abstract": "Existing risk-aware multi-armed bandit models typically focus on risk measures of individual options such as variance. As a result, they cannot be directly applied to important real-world online decision making problems with correlated options. In this paper, we propose a novel Continuous Mean-Covariance Bandit (CMCB) model to explicitly take into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent's objective is to achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, we consider three feedback settings, i.e., full-information, semi-bandit and full-bandit feedback. We propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the ",
    "link": "http://arxiv.org/abs/2102.12090",
    "context": "Title: Continuous Mean-Covariance Bandits. (arXiv:2102.12090v5 [cs.LG] UPDATED)\nAbstract: Existing risk-aware multi-armed bandit models typically focus on risk measures of individual options such as variance. As a result, they cannot be directly applied to important real-world online decision making problems with correlated options. In this paper, we propose a novel Continuous Mean-Covariance Bandit (CMCB) model to explicitly take into account option correlation. Specifically, in CMCB, there is a learner who sequentially chooses weight vectors on given options and observes random feedback according to the decisions. The agent's objective is to achieve the best trade-off between reward and risk, measured with option covariance. To capture different reward observation scenarios in practice, we consider three feedback settings, i.e., full-information, semi-bandit and full-bandit feedback. We propose novel algorithms with optimal regrets (within logarithmic factors), and provide matching lower bounds to validate their optimalities. The experimental results also demonstrate the ",
    "path": "papers/21/02/2102.12090.json",
    "total_tokens": 884,
    "translated_title": "连续均值协方差赌博机",
    "translated_abstract": "现有的风险感知多臂赌博模型通常关注于单个选项的风险度量，例如方差。因此，它们无法直接应用于具有相关选项的重要的实际在线决策问题。在本文中，我们提出了一种新颖的连续均值协方差赌博机（CMCB）模型，以显式考虑选项相关性。具体而言，在CMCB中，有一个学习者顺序选择给定选项上的权重向量，并根据决策观察随机反馈。代理的目标是在衡量选项协方差的奖励和风险之间实现最佳权衡。为了捕捉实践中的不同奖励观察场景，我们考虑了三种反馈设置，即完全信息、半赌徒和全赌徒反馈。我们提出了具有最优遗憾（在对数因子内），并提供相应的下界来验证它们的最优性的新算法。实验结果也证明了该算法的有效性。",
    "tldr": "本文提出了一种新的连续均值协方差赌博机（CMCB）模型，以考虑选项相关性。算法具有最优遗憾，并在实验中表现良好。",
    "en_tdlr": "This paper proposes a novel Continuous Mean-Covariance Bandit (CMCB) model to explicitly take into account option correlation. The proposed algorithm has optimal regrets and demonstrates good performance in experiments."
}