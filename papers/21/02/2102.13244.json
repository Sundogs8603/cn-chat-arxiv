{
    "title": "Cyclic Coordinate Dual Averaging with Extrapolation. (arXiv:2102.13244v4 [math.OC] UPDATED)",
    "abstract": "Cyclic block coordinate methods are a fundamental class of optimization methods widely used in practice and implemented as part of standard software packages for statistical learning. Nevertheless, their convergence is generally not well understood and so far their good practical performance has not been explained by existing convergence analyses. In this work, we introduce a new block coordinate method that applies to the general class of variational inequality (VI) problems with monotone operators. This class includes composite convex optimization problems and convex-concave min-max optimization problems as special cases and has not been addressed by the existing work. The resulting convergence bounds match the optimal convergence bounds of full gradient methods, but are provided in terms of a novel gradient Lipschitz condition w.r.t.~a Mahalanobis norm. For $m$ coordinate blocks, the resulting gradient Lipschitz constant in our bounds is never larger than a factor $\\sqrt{m}$ compare",
    "link": "http://arxiv.org/abs/2102.13244",
    "context": "Title: Cyclic Coordinate Dual Averaging with Extrapolation. (arXiv:2102.13244v4 [math.OC] UPDATED)\nAbstract: Cyclic block coordinate methods are a fundamental class of optimization methods widely used in practice and implemented as part of standard software packages for statistical learning. Nevertheless, their convergence is generally not well understood and so far their good practical performance has not been explained by existing convergence analyses. In this work, we introduce a new block coordinate method that applies to the general class of variational inequality (VI) problems with monotone operators. This class includes composite convex optimization problems and convex-concave min-max optimization problems as special cases and has not been addressed by the existing work. The resulting convergence bounds match the optimal convergence bounds of full gradient methods, but are provided in terms of a novel gradient Lipschitz condition w.r.t.~a Mahalanobis norm. For $m$ coordinate blocks, the resulting gradient Lipschitz constant in our bounds is never larger than a factor $\\sqrt{m}$ compare",
    "path": "papers/21/02/2102.13244.json",
    "total_tokens": 918,
    "translated_title": "循环坐标双平均与外推法",
    "translated_abstract": "循环块坐标方法是一类基本的优化方法，广泛应用于实践，并作为统计学习标准软件包的一部分实现。然而，它们的收敛性通常不是很清楚，迄今为止它们的良好实践表现还没有被现有的收敛性分析所解释。在这项工作中，我们引入了一种适用于单调算子的广泛类别变分不等式（VI）问题的新的块坐标方法。该类别包括复合凸优化问题和凸-凹极小极大优化问题作为特例，并且尚未被现有工作所解决。所得到的收敛界与全梯度方法的最优收敛界相匹配，但是提供了一种基于Mahalanobis范数的新型梯度Lipschitz条件。 对于$m$坐标块，我们界限中所需的梯度 Lipschitz常数永远不会大于一个$\\sqrt{m}$的系数。",
    "tldr": "本文提出了一种新的块坐标方法，适用于变分不等式问题，并提供了收敛边界。通过使用这种方法，我们得到了关于Mahalanobis范数的梯度Lipschitz条件。这为复合凸优化问题和凸-凹极小极大优化问题的解决提供了新的工具。",
    "en_tdlr": "This paper proposes a new block coordinate method for variational inequality problems, providing convergence bounds and a novel gradient Lipschitz condition based on the Mahalanobis norm. It offers a new tool for solving composite convex optimization problems and convex-concave min-max optimization problems."
}