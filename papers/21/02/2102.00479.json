{
    "title": "Fast Rates for the Regret of Offline Reinforcement Learning. (arXiv:2102.00479v2 [cs.LG] UPDATED)",
    "abstract": "We study the regret of reinforcement learning from offline data generated by a fixed behavior policy in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted $Q$-iteration (FQI), suggest a $O(1/\\sqrt{n})$ convergence for regret, empirical behavior exhibits \\emph{much} faster convergence. In this paper, we present a finer regret analysis that exactly characterizes this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function $Q^*$, the regret of the policy it defines converges at a rate given by the exponentiation of the $Q^*$-estimate's pointwise convergence rate, thus speeding it up. The level of exponentiation depends on the level of noise in the \\emph{decision-making} problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman resid",
    "link": "http://arxiv.org/abs/2102.00479",
    "context": "Title: Fast Rates for the Regret of Offline Reinforcement Learning. (arXiv:2102.00479v2 [cs.LG] UPDATED)\nAbstract: We study the regret of reinforcement learning from offline data generated by a fixed behavior policy in an infinite-horizon discounted Markov decision process (MDP). While existing analyses of common approaches, such as fitted $Q$-iteration (FQI), suggest a $O(1/\\sqrt{n})$ convergence for regret, empirical behavior exhibits \\emph{much} faster convergence. In this paper, we present a finer regret analysis that exactly characterizes this phenomenon by providing fast rates for the regret convergence. First, we show that given any estimate for the optimal quality function $Q^*$, the regret of the policy it defines converges at a rate given by the exponentiation of the $Q^*$-estimate's pointwise convergence rate, thus speeding it up. The level of exponentiation depends on the level of noise in the \\emph{decision-making} problem, rather than the estimation problem. We establish such noise levels for linear and tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman resid",
    "path": "papers/21/02/2102.00479.json",
    "total_tokens": 984,
    "translated_title": "离线强化学习的遗憾快速收敛速率研究",
    "translated_abstract": "本文研究了固定行为策略在无限时间折现马尔可夫决策过程（MDP）中生成的离线数据对强化学习的遗憾。现有方法（如拟合Q-迭代）的分析表明，对于遗憾的收敛速率是O(1/√n)，但实证行为表现出非常快的收敛速度。本文通过提供遗憾收敛速率的快速收敛进行更精细的遗憾分析，准确地表征了这一现象。首先，我们证明在给定最优质量函数Q*的估计的情况下，其对应的策略遗憾按照Q*估计的点对点收敛速率的指数进行收敛，从而加速了收敛速度。指数的级别取决于“决策问题”中的噪声水平，而不是估计问题。我们以线性和表格型MDP作为示例，建立了这样的噪声水平。其次，我们对拟合Q-迭代和Bellman残差进行了新的分析。",
    "tldr": "本文研究了离线数据对强化学习的遗憾，提出了精细的收敛速率分析，揭示了离线强化学习收敛速度较快的现象，并通过指数形式的加速机制加快了收敛速度。"
}