{
    "title": "Adaptive Rational Activations to Boost Deep Reinforcement Learning",
    "abstract": "arXiv:2102.09407v4 Announce Type: replace  Abstract: Latest insights from biology show that intelligence not only emerges from the connections between neurons but that individual neurons shoulder more computational responsibility than previously anticipated. This perspective should be critical in the context of constantly changing distinct reinforcement learning environments, yet current approaches still primarily employ static activation functions. In this work, we motivate why rationals are suitable for adaptable activation functions and why their inclusion into neural networks is crucial. Inspired by recurrence in residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version: the recurrent-rational. We demonstrate that equipping popular algorithms with (recurrent-)rational activations leads to consistent improvements on Atari games, especially turning simple DQN into a solid approach, competiti",
    "link": "https://arxiv.org/abs/2102.09407",
    "context": "Title: Adaptive Rational Activations to Boost Deep Reinforcement Learning\nAbstract: arXiv:2102.09407v4 Announce Type: replace  Abstract: Latest insights from biology show that intelligence not only emerges from the connections between neurons but that individual neurons shoulder more computational responsibility than previously anticipated. This perspective should be critical in the context of constantly changing distinct reinforcement learning environments, yet current approaches still primarily employ static activation functions. In this work, we motivate why rationals are suitable for adaptable activation functions and why their inclusion into neural networks is crucial. Inspired by recurrence in residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version: the recurrent-rational. We demonstrate that equipping popular algorithms with (recurrent-)rational activations leads to consistent improvements on Atari games, especially turning simple DQN into a solid approach, competiti",
    "path": "papers/21/02/2102.09407.json",
    "total_tokens": 816,
    "translated_title": "自适应有理激活以提升深度强化学习",
    "translated_abstract": "生物学的最新见解显示，智能不仅源自神经元之间的连接，而且单个神经元承担的计算责任比以往预期的更多。这种观点在不断变化的强化学习环境中至关重要，然而当前方法仍然主要使用静态激活函数。在本工作中，我们阐述了为什么有理数适合作为可适应的激活函数，以及为什么将其包含到神经网络中至关重要。受Residual网络中循环性的启发，我们导出了有理单位在残差连接下封闭的条件，并制定了一个自然正则化的版本：循环有理数。我们证明，为流行算法配备（循环）有理数激活会显著提高Atari游戏的性能，尤其将简单的DQN转化为一种可靠的方法。",
    "tldr": "本研究提出了利用有理数作为适应性激活函数来改进深度强化学习，并展示了这种方法在Atari游戏中取得了一致的改进，特别是将简单的DQN提升为一个稳健的方法。"
}