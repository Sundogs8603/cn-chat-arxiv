{
    "title": "And/or trade-off in artificial neurons: impact on adversarial robustness. (arXiv:2102.07389v3 [cs.LG] UPDATED)",
    "abstract": "Despite the success of neural networks, the issue of classification robustness remains, particularly highlighted by adversarial examples. In this paper, we address this challenge by focusing on the continuum of functions implemented in artificial neurons, ranging from pure AND gates to pure OR gates. Our hypothesis is that the presence of a sufficient number of OR-like neurons in a network can lead to classification brittleness and increased vulnerability to adversarial attacks. We define AND-like neurons and propose measures to increase their proportion in the network. These measures involve rescaling inputs to the [-1,1] interval and reducing the number of points in the steepest section of the sigmoidal activation function. A crucial component of our method is the comparison between a neuron's output distribution when fed with the actual dataset and a randomised version called the \"scrambled dataset.\" Experimental results on the MNIST dataset suggest that our approach holds promise a",
    "link": "http://arxiv.org/abs/2102.07389",
    "context": "Title: And/or trade-off in artificial neurons: impact on adversarial robustness. (arXiv:2102.07389v3 [cs.LG] UPDATED)\nAbstract: Despite the success of neural networks, the issue of classification robustness remains, particularly highlighted by adversarial examples. In this paper, we address this challenge by focusing on the continuum of functions implemented in artificial neurons, ranging from pure AND gates to pure OR gates. Our hypothesis is that the presence of a sufficient number of OR-like neurons in a network can lead to classification brittleness and increased vulnerability to adversarial attacks. We define AND-like neurons and propose measures to increase their proportion in the network. These measures involve rescaling inputs to the [-1,1] interval and reducing the number of points in the steepest section of the sigmoidal activation function. A crucial component of our method is the comparison between a neuron's output distribution when fed with the actual dataset and a randomised version called the \"scrambled dataset.\" Experimental results on the MNIST dataset suggest that our approach holds promise a",
    "path": "papers/21/02/2102.07389.json",
    "total_tokens": 904,
    "translated_title": "人工神经元中的与/或权衡：对抗鲁棒性影响的研究",
    "translated_abstract": "尽管神经网络取得了很大的成功，但分类鲁棒性仍然是一个问题，特别是在对抗性样本上表现尤为突出。本文通过关注人工神经元中实现的函数连续性，从纯AND门到纯OR门的范围来解决这个挑战。我们的假设是，在网络中存在足够数量的类OR的神经元会导致分类的脆弱性和增加对抗攻击的脆弱性。我们定义了类AND的神经元，并提出了增加它们在网络中比例的措施。这些措施涉及将输入放缩到[-1,1]区间并减少S型激活函数陡峭部分的点数。我们方法的一个关键部分是比较当神经元用实际数据集和称为“乱序数据集”的随机版本输入时的输出分布。在MNIST数据集上的实验结果表明，我们的方法很有前途。",
    "tldr": "本文研究了人工神经元中与/或函数的连续性对分类鲁棒性的影响，提出了增加类AND神经元在网络中比例的措施，实验结果显示该方法具有潜在的应用前景。",
    "en_tdlr": "This paper studies the effect of the continuity of AND/OR functions in artificial neurons on classification robustness, proposes measures to increase the proportion of AND-like neurons in the network, and the experimental results on the MNIST dataset show the promising application prospects of the method."
}