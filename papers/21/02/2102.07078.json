{
    "title": "Exploiting Shared Representations for Personalized Federated Learning. (arXiv:2102.07078v3 [cs.LG] UPDATED)",
    "abstract": "Deep neural networks have shown the ability to extract universal feature representations from data such as images and text that have been useful for a variety of learning tasks. However, the fruits of representation learning have yet to be fully-realized in federated settings. Although data in federated settings is often non-i.i.d. across clients, the success of centralized deep learning suggests that data often shares a global feature representation, while the statistical heterogeneity across clients or tasks is concentrated in the labels. Based on this intuition, we propose a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client. Our algorithm harnesses the distributed computational power across clients to perform many local-updates with respect to the low-dimensional local parameters for every update of the representation. We prove that this method obtains linear convergence to the ground-trut",
    "link": "http://arxiv.org/abs/2102.07078",
    "context": "Title: Exploiting Shared Representations for Personalized Federated Learning. (arXiv:2102.07078v3 [cs.LG] UPDATED)\nAbstract: Deep neural networks have shown the ability to extract universal feature representations from data such as images and text that have been useful for a variety of learning tasks. However, the fruits of representation learning have yet to be fully-realized in federated settings. Although data in federated settings is often non-i.i.d. across clients, the success of centralized deep learning suggests that data often shares a global feature representation, while the statistical heterogeneity across clients or tasks is concentrated in the labels. Based on this intuition, we propose a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client. Our algorithm harnesses the distributed computational power across clients to perform many local-updates with respect to the low-dimensional local parameters for every update of the representation. We prove that this method obtains linear convergence to the ground-trut",
    "path": "papers/21/02/2102.07078.json",
    "total_tokens": 829,
    "translated_title": "利用共享表示进行个性化联邦学习",
    "translated_abstract": "深度神经网络已经展示出能够从图像和文本等数据中提取普适的特征表示，这对于各种学习任务都非常有用。然而，在联邦学习中尚未充分利用表示学习的成果。虽然联邦数据通常在客户端之间不是独立同分布的，但中心化深度学习的成功表明数据通常共享全局特征表示，而客户端或任务之间的统计异质性集中在标签上。基于这一直觉，我们提出了一种新的联邦学习框架和算法，用于学习跨客户端的共享数据表示和每个客户端的唯一本地拟合。我们的算法利用客户端之间的分布式计算能力，针对每个表示更新进行许多低维本地参数的本地更新。我们证明了该方法能够线性收敛到真实值。",
    "tldr": "该论文提出了一种新的联邦学习框架和算法，用于在客户端之间共享数据表示和各自本地拟合，该方法能够线性收敛到真实值。",
    "en_tdlr": "This paper proposes a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client, which can converge to the ground-truth linearly."
}