{
    "title": "Double-descent curves in neural networks: a new perspective using Gaussian processes. (arXiv:2102.07238v5 [stat.ML] UPDATED)",
    "abstract": "Double-descent curves in neural networks describe the phenomenon that the generalisation error initially descends with increasing parameters, then grows after reaching an optimal number of parameters which is less than the number of data points, but then descends again in the overparameterized regime. In this paper, we use techniques from random matrix theory to characterize the spectral distribution of the empirical feature covariance matrix as a width-dependent perturbation of the spectrum of the neural network Gaussian process (NNGP) kernel, thus establishing a novel connection between the NNGP literature and the random matrix theory literature in the context of neural networks. Our analytical expression allows us to study the generalisation behavior of the corresponding kernel and GP regression, and provides a new interpretation of the double-descent phenomenon, namely as governed by the discrepancy between the width-dependent empirical kernel and the width-independent NNGP kernel.",
    "link": "http://arxiv.org/abs/2102.07238",
    "context": "Title: Double-descent curves in neural networks: a new perspective using Gaussian processes. (arXiv:2102.07238v5 [stat.ML] UPDATED)\nAbstract: Double-descent curves in neural networks describe the phenomenon that the generalisation error initially descends with increasing parameters, then grows after reaching an optimal number of parameters which is less than the number of data points, but then descends again in the overparameterized regime. In this paper, we use techniques from random matrix theory to characterize the spectral distribution of the empirical feature covariance matrix as a width-dependent perturbation of the spectrum of the neural network Gaussian process (NNGP) kernel, thus establishing a novel connection between the NNGP literature and the random matrix theory literature in the context of neural networks. Our analytical expression allows us to study the generalisation behavior of the corresponding kernel and GP regression, and provides a new interpretation of the double-descent phenomenon, namely as governed by the discrepancy between the width-dependent empirical kernel and the width-independent NNGP kernel.",
    "path": "papers/21/02/2102.07238.json",
    "total_tokens": 891,
    "translated_title": "高斯过程视角下神经网络的双峰下降曲线研究",
    "translated_abstract": "神经网络中的双峰下降曲线现象描述了当增加参数时，泛化误差起初下降，但在达到一个小于数据点数量的最优参数后增加，然后在过参数化区间再次下降。在本文中，我们使用随机矩阵理论技术来表征经验特征协方差矩阵的谱分布，作为神经网络高斯过程（NNGP）核谱的宽度相关扰动，从而在神经网络领域建立了NNGP文献与随机矩阵理论文献之间的新联系。我们的分析表达式允许我们研究相应核和GP回归的泛化行为，并为双峰下降的现象提供了一个新的解释，即由宽度相关的经验核与宽度无关的NNGP核之间的差异所决定。",
    "tldr": "本文利用随机矩阵理论和高斯过程技术解释了神经网络双峰下降现象，建立了NNGP和随机矩阵理论之间的新联系，揭示该现象受到经验核和NNGP核之间差异的影响。",
    "en_tdlr": "This paper uses techniques from random matrix theory and Gaussian processes to explain the double-descent phenomenon in neural networks, establish a new connection between NNGP and random matrix theory, and reveal that this phenomenon is influenced by the discrepancy between the empirical kernel and the NNGP kernel."
}