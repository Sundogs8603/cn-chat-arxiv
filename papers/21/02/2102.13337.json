{
    "title": "Neural Generalization of Multiple Kernel Learning. (arXiv:2102.13337v2 [cs.LG] UPDATED)",
    "abstract": "Multiple Kernel Learning is a conventional way to learn the kernel function in kernel-based methods. MKL algorithms enhance the performance of kernel methods. However, these methods have a lower complexity compared to deep learning models and are inferior to these models in terms of recognition accuracy. Deep learning models can learn complex functions by applying nonlinear transformations to data through several layers. In this paper, we show that a typical MKL algorithm can be interpreted as a one-layer neural network with linear activation functions. By this interpretation, we propose a Neural Generalization of Multiple Kernel Learning (NGMKL), which extends the conventional multiple kernel learning framework to a multi-layer neural network with nonlinear activation functions. Our experiments on several benchmarks show that the proposed method improves the complexity of MKL algorithms and leads to higher recognition accuracy.",
    "link": "http://arxiv.org/abs/2102.13337",
    "context": "Title: Neural Generalization of Multiple Kernel Learning. (arXiv:2102.13337v2 [cs.LG] UPDATED)\nAbstract: Multiple Kernel Learning is a conventional way to learn the kernel function in kernel-based methods. MKL algorithms enhance the performance of kernel methods. However, these methods have a lower complexity compared to deep learning models and are inferior to these models in terms of recognition accuracy. Deep learning models can learn complex functions by applying nonlinear transformations to data through several layers. In this paper, we show that a typical MKL algorithm can be interpreted as a one-layer neural network with linear activation functions. By this interpretation, we propose a Neural Generalization of Multiple Kernel Learning (NGMKL), which extends the conventional multiple kernel learning framework to a multi-layer neural network with nonlinear activation functions. Our experiments on several benchmarks show that the proposed method improves the complexity of MKL algorithms and leads to higher recognition accuracy.",
    "path": "papers/21/02/2102.13337.json",
    "total_tokens": 808,
    "translated_title": "多核学习的神经通用化",
    "translated_abstract": "多核学习是学习核函数的传统方法。MKL算法增强了核方法的性能。然而，与深度学习模型相比，这些方法的复杂度较低，在识别精度方面不如这些模型。在本文中，我们展示了一个典型的MKL算法可以被解释为具有线性激活函数的单层神经网络。通过这种解释，我们提出了多核学习的神经通用化（NGMKL），该方法将传统的多核学习框架扩展到具有非线性激活函数的多层神经网络中。我们在几个基准测试中的实验表明，所提出的方法提高了MKL算法的复杂度，导致更高的识别精度。",
    "tldr": "本文提出了一种神经通用化的多核学习方法（NGMKL），将传统的多核学习框架扩展到具有非线性激活函数的多层神经网络中。实验证明该方法提高了算法的复杂度，并导致更高的识别精度。",
    "en_tdlr": "The paper proposes a Neural Generalization of Multiple Kernel Learning (NGMKL) which extends the conventional multiple kernel learning framework to a multi-layer neural network with nonlinear activation functions. Experiments show that the proposed method improves the complexity of MKL algorithms and leads to higher recognition accuracy."
}