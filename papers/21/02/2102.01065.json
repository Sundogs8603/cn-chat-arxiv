{
    "title": "Do Question Answering Modeling Improvements Hold Across Benchmarks?. (arXiv:2102.01065v3 [cs.CL] UPDATED)",
    "abstract": "Do question answering (QA) modeling improvements (e.g., choice of architecture and training procedure) hold consistently across the diverse landscape of QA benchmarks? To study this question, we introduce the notion of concurrence -- two benchmarks have high concurrence on a set of modeling approaches if they rank the modeling approaches similarly. We measure the concurrence between 32 QA benchmarks on a set of 20 diverse modeling approaches and find that human-constructed benchmarks have high concurrence amongst themselves, even if their passage and question distributions are very different. Surprisingly, even downsampled human-constructed benchmarks (i.e., collecting less data) and programmatically-generated benchmarks (e.g., cloze-formatted examples) have high concurrence with human-constructed benchmarks. These results indicate that, despite years of intense community focus on a small number of benchmarks, the modeling improvements studied hold broadly.",
    "link": "http://arxiv.org/abs/2102.01065",
    "context": "Title: Do Question Answering Modeling Improvements Hold Across Benchmarks?. (arXiv:2102.01065v3 [cs.CL] UPDATED)\nAbstract: Do question answering (QA) modeling improvements (e.g., choice of architecture and training procedure) hold consistently across the diverse landscape of QA benchmarks? To study this question, we introduce the notion of concurrence -- two benchmarks have high concurrence on a set of modeling approaches if they rank the modeling approaches similarly. We measure the concurrence between 32 QA benchmarks on a set of 20 diverse modeling approaches and find that human-constructed benchmarks have high concurrence amongst themselves, even if their passage and question distributions are very different. Surprisingly, even downsampled human-constructed benchmarks (i.e., collecting less data) and programmatically-generated benchmarks (e.g., cloze-formatted examples) have high concurrence with human-constructed benchmarks. These results indicate that, despite years of intense community focus on a small number of benchmarks, the modeling improvements studied hold broadly.",
    "path": "papers/21/02/2102.01065.json",
    "total_tokens": 948,
    "translated_title": "问题回答建模的改进是否跨越基准测试持续存在？",
    "translated_abstract": "问题回答 (QA) 模型的改进（例如，架构和训练过程的选择）是否在各种QA基准测试中都能持续存在？为了研究这个问题，我们引入了“一致性”的概念--如果两个基准测试在一组建模方法上排名相似，那么它们之间有高一致性。我们在一组20个不同的建模方法上测量了32个问题回答基准测试之间的一致性，发现人工构建的基准测试在相互之间具有高度一致性，即使它们的段落和问题分布是非常不同的。令人惊讶的是，即使是缩减了数据量的人工构建基准测试（即采集较少数据量）和程序生成的基准测试（例如，填空格式的示例）也与人工构建的基准测试具有高度一致性。这些结果表明，尽管社区长期关注少数基准测试，但所研究的建模改进仍然具有广泛适用性。",
    "tldr": "该论文通过测量32个问题回答基准测试之间的一致性，发现人工构建的基准测试具有高度的一致性，即使它们的段落和问题分布是非常不同的，这表明尽管人们长时间关注少数基准测试，但所研究的建模改进仍然具有广泛适用性。",
    "en_tdlr": "This paper measures concurrence between 32 QA benchmarks on 20 diverse modeling approaches and finds that even downscaled human-constructed benchmarks and programmatically-generated benchmarks have high concurrence with human-constructed benchmarks, indicating that modeling improvements studied hold broadly across diverse QA benchmarks despite community focus on a few benchmarks."
}