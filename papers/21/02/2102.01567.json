{
    "title": "A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants. (arXiv:2102.01567v4 [cs.LG] UPDATED)",
    "abstract": "This paper develops an unified framework to study finite-sample convergence guarantees of a large class of value-based asynchronous reinforcement learning (RL) algorithms. We do this by first reformulating the RL algorithms as \\textit{Markovian Stochastic Approximation} (SA) algorithms to solve fixed-point equations. We then develop a Lyapunov analysis and derive mean-square error bounds on the convergence of the Markovian SA. Based on this result, we establish finite-sample mean-square convergence bounds for asynchronous RL algorithms such as $Q$-learning, $n$-step TD, TD$(\\lambda)$, and off-policy TD algorithms including V-trace. As a by-product, by analyzing the convergence bounds of $n$-step TD and TD$(\\lambda)$, we provide theoretical insights into the bias-variance trade-off, i.e., efficiency of bootstrapping in RL. This was first posed as an open problem in (Sutton, 1999).",
    "link": "http://arxiv.org/abs/2102.01567",
    "context": "Title: A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-Learning and TD-Learning Variants. (arXiv:2102.01567v4 [cs.LG] UPDATED)\nAbstract: This paper develops an unified framework to study finite-sample convergence guarantees of a large class of value-based asynchronous reinforcement learning (RL) algorithms. We do this by first reformulating the RL algorithms as \\textit{Markovian Stochastic Approximation} (SA) algorithms to solve fixed-point equations. We then develop a Lyapunov analysis and derive mean-square error bounds on the convergence of the Markovian SA. Based on this result, we establish finite-sample mean-square convergence bounds for asynchronous RL algorithms such as $Q$-learning, $n$-step TD, TD$(\\lambda)$, and off-policy TD algorithms including V-trace. As a by-product, by analyzing the convergence bounds of $n$-step TD and TD$(\\lambda)$, we provide theoretical insights into the bias-variance trade-off, i.e., efficiency of bootstrapping in RL. This was first posed as an open problem in (Sutton, 1999).",
    "path": "papers/21/02/2102.01567.json",
    "total_tokens": 951,
    "translated_title": "一种用于异步Q学习和TD学习变种的有限样本保证的Lyapunov理论",
    "translated_abstract": "本文通过首先将强化学习算法重新表述为解决固定点方程的\"Markovian Stochastic Approximation\"(SA)算法，发展了一个统一的框架来研究基于值的异步强化学习算法的有限样本收敛保证。然后，我们使用Lyapunov分析推导出Markovian SA的均方误差界限，基于此结果，我们建立了异步强化学习算法（如Q学习，n步TD，TD（λ）和包括V-trace的离策略TD算法）的有限样本均方收敛界限。作为副产品，通过分析n步TD和TD（λ）的收敛界限，我们提供了关于强化学习中引导技巧效率（即偏差-方差权衡）的理论洞见，这是(Sutton, 1999)中首次提出的一个开放性问题。",
    "tldr": "本文提出了一种统一的框架来研究异步强化学习算法的有限样本收敛保证，并基于Lyapunov分析建立了异步RL算法的均方误差界限。通过对n步TD和TD（λ）的收敛界限的分析，揭示了强化学习中引导技巧效率的理论洞见。",
    "en_tdlr": "This paper presents a unified framework for studying finite-sample convergence guarantees of asynchronous reinforcement learning algorithms, and establishes mean-square error bounds for these algorithms based on Lyapunov analysis. It also provides theoretical insights into the efficiency of bootstrapping in reinforcement learning through the analysis of convergence bounds of n-step TD and TD(λ) algorithms."
}