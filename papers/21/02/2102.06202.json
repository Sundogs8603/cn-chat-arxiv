{
    "title": "Private Prediction Sets",
    "abstract": "arXiv:2102.06202v3 Announce Type: replace-cross  Abstract: In real-world settings involving consequential decision-making, the deployment of machine learning systems generally requires both reliable uncertainty quantification and protection of individuals' privacy. We present a framework that treats these two desiderata jointly. Our framework is based on conformal prediction, a methodology that augments predictive models to return prediction sets that provide uncertainty quantification -- they provably cover the true response with a user-specified probability, such as 90%. One might hope that when used with privately-trained models, conformal prediction would yield privacy guarantees for the resulting prediction sets; unfortunately, this is not the case. To remedy this key problem, we develop a method that takes any pre-trained predictive model and outputs differentially private prediction sets. Our method follows the general approach of split conformal prediction; we use holdout data ",
    "link": "https://arxiv.org/abs/2102.06202",
    "context": "Title: Private Prediction Sets\nAbstract: arXiv:2102.06202v3 Announce Type: replace-cross  Abstract: In real-world settings involving consequential decision-making, the deployment of machine learning systems generally requires both reliable uncertainty quantification and protection of individuals' privacy. We present a framework that treats these two desiderata jointly. Our framework is based on conformal prediction, a methodology that augments predictive models to return prediction sets that provide uncertainty quantification -- they provably cover the true response with a user-specified probability, such as 90%. One might hope that when used with privately-trained models, conformal prediction would yield privacy guarantees for the resulting prediction sets; unfortunately, this is not the case. To remedy this key problem, we develop a method that takes any pre-trained predictive model and outputs differentially private prediction sets. Our method follows the general approach of split conformal prediction; we use holdout data ",
    "path": "papers/21/02/2102.06202.json",
    "total_tokens": 784,
    "translated_title": "私人预测集",
    "translated_abstract": "在涉及重要决策的现实环境中，部署机器学习系统通常需要可靠的不确定性量化和保护个人隐私。我们提出了一个框架，将这两个目标同时视为重要。我们的框架基于符合性预测，这种方法可以扩展预测模型，返回提供不确定性量化的预测集，这些集合可以证明以用户指定的概率（如90%）覆盖真实响应。当与经过私人训练的模型一起使用时，人们可能希望符合性预测会为生成的预测集提供隐私保证；不幸的是，情况并非如此。为了解决这一关键问题，我们开发了一种方法，该方法可以从任何预先训练的预测模型中输出差分私人预测集。我们的方法遵循分裂符合性预测的一般方法；我们使用保留数据",
    "tldr": "该研究提出了一个基于符合性预测的框架，可以在保护个人隐私的同时返回可靠的不确定性量化的预测集。",
    "en_tdlr": "This study introduces a framework based on conformal prediction that can provide reliable uncertainty quantification in predicting while protecting individual privacy."
}