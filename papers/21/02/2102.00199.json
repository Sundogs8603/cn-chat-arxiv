{
    "title": "Rates of convergence for density estimation with generative adversarial networks. (arXiv:2102.00199v4 [math.ST] UPDATED)",
    "abstract": "In this work we undertake a thorough study of the non-asymptotic properties of the vanilla generative adversarial networks (GANs). We prove an oracle inequality for the Jensen-Shannon (JS) divergence between the underlying density $\\mathsf{p}^*$ and the GAN estimate with a significantly better statistical error term compared to the previously known results. The advantage of our bound becomes clear in application to nonparametric density estimation. We show that the JS-divergence between the GAN estimate and $\\mathsf{p}^*$ decays as fast as $(\\log{n}/n)^{2\\beta/(2\\beta + d)}$, where $n$ is the sample size and $\\beta$ determines the smoothness of $\\mathsf{p}^*$. This rate of convergence coincides (up to logarithmic factors) with minimax optimal for the considered class of densities.",
    "link": "http://arxiv.org/abs/2102.00199",
    "context": "Title: Rates of convergence for density estimation with generative adversarial networks. (arXiv:2102.00199v4 [math.ST] UPDATED)\nAbstract: In this work we undertake a thorough study of the non-asymptotic properties of the vanilla generative adversarial networks (GANs). We prove an oracle inequality for the Jensen-Shannon (JS) divergence between the underlying density $\\mathsf{p}^*$ and the GAN estimate with a significantly better statistical error term compared to the previously known results. The advantage of our bound becomes clear in application to nonparametric density estimation. We show that the JS-divergence between the GAN estimate and $\\mathsf{p}^*$ decays as fast as $(\\log{n}/n)^{2\\beta/(2\\beta + d)}$, where $n$ is the sample size and $\\beta$ determines the smoothness of $\\mathsf{p}^*$. This rate of convergence coincides (up to logarithmic factors) with minimax optimal for the considered class of densities.",
    "path": "papers/21/02/2102.00199.json",
    "total_tokens": 805,
    "translated_title": "使用生成对抗网络进行密度估计的收敛速度",
    "translated_abstract": "在本研究中，我们对基本生成对抗网络（GANs）的非渐近特性进行了全面研究。我们证明了对于基本密度$\\mathsf{p}^*$和GAN估计之间的Jensen-Shannon（JS）散度，存在一个正则化不等式，其统计误差项相较于先前已知的结果显著改进。我们的界限的优势在于非参数密度估计的应用中变得明显。我们展示了GAN估计与$\\mathsf{p}^*$之间的JS散度的收敛速度与样本量$n$的速率为$(\\log{n}/n)^{2\\beta/(2\\beta + d)}$，其中$n$为样本大小，$\\beta$决定了$\\mathsf{p}^*$的平滑程度。这种收敛速度（除了对数因子）与所考虑的密度类的极小极限最优速率一致。",
    "tldr": "本文针对基本生成对抗网络（GANs）进行了详细研究，证明了在非参数密度估计应用中，GAN估计与真实密度之间的JS散度收敛速度达到了极小极限最优速率。"
}