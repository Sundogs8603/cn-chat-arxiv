{
    "title": "Doubly robust Thompson sampling for linear payoffs. (arXiv:2102.01229v3 [stat.ML] UPDATED)",
    "abstract": "A challenging aspect of the bandit problem is that a stochastic reward is observed only for the chosen arm and the rewards of other arms remain missing. The dependence of the arm choice on the past context and reward pairs compounds the complexity of regret analysis. We propose a novel multi-armed contextual bandit algorithm called Doubly Robust (DR) Thompson Sampling employing the doubly-robust estimator used in missing data literature to Thompson Sampling with contexts (\\texttt{LinTS}). Different from previous works relying on missing data techniques (\\citet{dimakopoulou2019balanced}, \\citet{kim2019doubly}), the proposed algorithm is designed to allow a novel additive regret decomposition leading to an improved regret bound with the order of $\\tilde{O}(\\phi^{-2}\\sqrt{T})$, where $\\phi^2$ is the minimum eigenvalue of the covariance matrix of contexts. This is the first regret bound of \\texttt{LinTS} using $\\phi^2$ without the dimension of the context, $d$. Applying the relationship be",
    "link": "http://arxiv.org/abs/2102.01229",
    "context": "Title: Doubly robust Thompson sampling for linear payoffs. (arXiv:2102.01229v3 [stat.ML] UPDATED)\nAbstract: A challenging aspect of the bandit problem is that a stochastic reward is observed only for the chosen arm and the rewards of other arms remain missing. The dependence of the arm choice on the past context and reward pairs compounds the complexity of regret analysis. We propose a novel multi-armed contextual bandit algorithm called Doubly Robust (DR) Thompson Sampling employing the doubly-robust estimator used in missing data literature to Thompson Sampling with contexts (\\texttt{LinTS}). Different from previous works relying on missing data techniques (\\citet{dimakopoulou2019balanced}, \\citet{kim2019doubly}), the proposed algorithm is designed to allow a novel additive regret decomposition leading to an improved regret bound with the order of $\\tilde{O}(\\phi^{-2}\\sqrt{T})$, where $\\phi^2$ is the minimum eigenvalue of the covariance matrix of contexts. This is the first regret bound of \\texttt{LinTS} using $\\phi^2$ without the dimension of the context, $d$. Applying the relationship be",
    "path": "papers/21/02/2102.01229.json",
    "total_tokens": 1030,
    "translated_title": "线性回报的双重稳健汤普森抽样算法",
    "translated_abstract": "汤普森抽样在多臂上下文赌博问题中的应用面临一些挑战，如何根据过去的上下文和奖励对进行选择的依赖性使得后悔分析的复杂性增加。本文提出了一种名为双重稳健汤普森抽样（DR Thompson Sampling）的新型多臂上下文赌博算法，采用在缺失数据领域中使用的双重稳健估计器用于基于上下文的汤普森抽样（LinTS）。双重稳健汤普森抽样让损失分解更加简单，从而使得改进后的损失界限降到了 $\\tilde{O}(\\phi^{-2}\\sqrt{T})$，其中 $\\phi^2$ 是上下文协方差矩阵中的最小特征值。这是 \\texttt{LinTS} 第一个不基于上下文维度 $d$，而是使用 $\\phi^2$ 的损失界限。",
    "tldr": "本文提出了一种新型多臂上下文赌博算法双重稳健汤普森抽样（DR Thompson Sampling），通过双重稳健估计器，解决了过去的上下文和奖励对选择依赖性导致的损失分解复杂等问题，得到了简化且改进的损失界限。",
    "en_tdlr": "This paper proposes a novel multi-armed contextual bandit algorithm called Doubly Robust (DR) Thompson Sampling, which employs a doubly-robust estimator to simplify and improve the loss bounds in the past context and reward pairs selection dependency, achieving a regret bound of $\\tilde{O}(\\phi^{-2}\\sqrt{T})$ using the minimum eigenvalue $\\phi^2$ of the covariance matrix of contexts, and is the first to achieve a loss bound for \\texttt{LinTS} using $\\phi^2$ without the dimension of the context, $d$."
}