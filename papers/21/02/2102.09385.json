{
    "title": "Convergence of stochastic gradient descent schemes for Lojasiewicz-landscapes. (arXiv:2102.09385v3 [cs.LG] UPDATED)",
    "abstract": "In this article, we consider convergence of stochastic gradient descent schemes (SGD), including momentum stochastic gradient descent (MSGD), under weak assumptions on the underlying landscape. More explicitly, we show that on the event that the SGD stays bounded we have convergence of the SGD if there is only a countable number of critical points or if the objective function satisfies Lojasiewicz-inequalities around all critical levels as all analytic functions do. In particular, we show that for neural networks with analytic activation function such as softplus, sigmoid and the hyperbolic tangent, SGD converges on the event of staying bounded, if the random variables modelling the signal and response in the training are compactly supported.",
    "link": "http://arxiv.org/abs/2102.09385",
    "context": "Title: Convergence of stochastic gradient descent schemes for Lojasiewicz-landscapes. (arXiv:2102.09385v3 [cs.LG] UPDATED)\nAbstract: In this article, we consider convergence of stochastic gradient descent schemes (SGD), including momentum stochastic gradient descent (MSGD), under weak assumptions on the underlying landscape. More explicitly, we show that on the event that the SGD stays bounded we have convergence of the SGD if there is only a countable number of critical points or if the objective function satisfies Lojasiewicz-inequalities around all critical levels as all analytic functions do. In particular, we show that for neural networks with analytic activation function such as softplus, sigmoid and the hyperbolic tangent, SGD converges on the event of staying bounded, if the random variables modelling the signal and response in the training are compactly supported.",
    "path": "papers/21/02/2102.09385.json",
    "total_tokens": 874,
    "translated_title": "Lojasiewicz-landscape的随机梯度下降方案的收敛性",
    "translated_abstract": "在本文中，我们考虑了对弱假设下的随机梯度下降方案（SGD）的收敛性，包括动量随机梯度下降（MSGD）。具体而言，我们证明了在SGD保持有界的情况下，如果存在可数个临界点或者目标函数在所有临界水平附近满足Lojasiewicz不等式，如同所有解析函数一样，SGD将会收敛。特别地，我们证明了对于具有解析激活函数（如softplus、sigmoid和双曲正切）的神经网络，在随机变量对训练中的信号和响应进行紧支撑的情况下，SGD将在保持有界的情况下收敛。",
    "tldr": "本文研究了Lojasiewicz-landscape下的随机梯度下降方案的收敛性，证明了当SGD保持有界且具有可数个临界点，或目标函数满足Lojasiewicz不等式时，SGD将收敛。此外，在神经网络中使用一些特定的解析激活函数时，如果训练中的信号和响应是紧支撑的，SGD也将收敛。",
    "en_tdlr": "This article investigates the convergence of stochastic gradient descent schemes for Lojasiewicz-landscapes, showing that SGD converges when it stays bounded and has countable critical points or when the objective function satisfies Lojasiewicz-inequalities. Moreover, for neural networks with specific analytic activation functions, such as softplus, sigmoid, and hyperbolic tangent, SGD also converges when the training signal and response are compactly supported."
}