{
    "title": "Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks. (arXiv:2101.06969v5 [cs.CL] UPDATED)",
    "abstract": "Pre-trained models (PTMs) have been widely used in various downstream tasks. The parameters of PTMs are distributed on the Internet and may suffer backdoor attacks. In this work, we demonstrate the universal vulnerability of PTMs, where fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary downstream tasks. Specifically, attackers can add a simple pre-training task, which restricts the output representations of trigger instances to pre-defined vectors, namely neuron-level backdoor attack (NeuBA). If the backdoor functionality is not eliminated during fine-tuning, the triggers can make the fine-tuned model predict fixed labels by pre-defined vectors. In the experiments of both natural language processing (NLP) and computer vision (CV), we show that NeuBA absolutely controls the predictions for trigger instances without any knowledge of downstream tasks. Finally, we apply several defense methods to NeuBA and find that model pruning is a promising direction to resist N",
    "link": "http://arxiv.org/abs/2101.06969",
    "context": "Title: Red Alarm for Pre-trained Models: Universal Vulnerability to Neuron-Level Backdoor Attacks. (arXiv:2101.06969v5 [cs.CL] UPDATED)\nAbstract: Pre-trained models (PTMs) have been widely used in various downstream tasks. The parameters of PTMs are distributed on the Internet and may suffer backdoor attacks. In this work, we demonstrate the universal vulnerability of PTMs, where fine-tuned PTMs can be easily controlled by backdoor attacks in arbitrary downstream tasks. Specifically, attackers can add a simple pre-training task, which restricts the output representations of trigger instances to pre-defined vectors, namely neuron-level backdoor attack (NeuBA). If the backdoor functionality is not eliminated during fine-tuning, the triggers can make the fine-tuned model predict fixed labels by pre-defined vectors. In the experiments of both natural language processing (NLP) and computer vision (CV), we show that NeuBA absolutely controls the predictions for trigger instances without any knowledge of downstream tasks. Finally, we apply several defense methods to NeuBA and find that model pruning is a promising direction to resist N",
    "path": "papers/21/01/2101.06969.json",
    "total_tokens": 1015,
    "translated_title": "针对预训练模型的红色警报：神经元级背门攻击的普遍漏洞",
    "translated_abstract": "预训练模型（PTM）广泛应用于各种下游任务。PTM的参数分布在互联网上，可能遭受背门攻击。本文展示了PTM的普遍漏洞，即在任意下游任务中，经过微调的PTM可以轻易被背门攻击控制。具体而言，攻击者可以添加一个简单的预训练任务，将触发实例的输出表示限制为预定义向量，即神经元级背门攻击（NeuBA）。如果在微调过程中未消除背门功能，触发器可以使经过微调的模型通过预定义向量来预测固定标签。在自然语言处理（NLP）和计算机视觉（CV）的实验中，我们展示了NeuBA在没有任何关于下游任务的知识的情况下绝对控制了触发实例的预测。最后，我们对NeuBA应用了几种防御方法，发现模型修剪是抵抗NeuBA的一个有前途的方向。",
    "tldr": "本文展示了预训练模型在任意下游任务中普遍存在的漏洞，即经过微调的预训练模型容易受到神经元级背门攻击的控制。我们的实验表明，NeuBA可以在没有关于下游任务的知识的情况下完全掌控触发实例的预测。模型修剪是一种有前途的抵抗NeuBA的防御方法。"
}