{
    "title": "Adversarial Estimation of Riesz Representers. (arXiv:2101.00009v2 [econ.EM] UPDATED)",
    "abstract": "Many causal and structural parameters are linear functionals of an underlying regression. The Riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. We propose an adversarial framework to estimate the Riesz representer using general function spaces. We prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel Hilbert spaces as leading cases. Furthermore, we use critical radius theory -- in place of Donsker theory -- to prove asymptotic normality without sample splitting, uncovering a ``complexity-rate robustness'' condition. This condition has practical consequences: inference without sample splitting is possible in several machine learning settings, which may improve finite sample performance compared to sample splitting. Our estimators achieve nominal coverage in highly nonlinear simulations where previo",
    "link": "http://arxiv.org/abs/2101.00009",
    "context": "Title: Adversarial Estimation of Riesz Representers. (arXiv:2101.00009v2 [econ.EM] UPDATED)\nAbstract: Many causal and structural parameters are linear functionals of an underlying regression. The Riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. We propose an adversarial framework to estimate the Riesz representer using general function spaces. We prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel Hilbert spaces as leading cases. Furthermore, we use critical radius theory -- in place of Donsker theory -- to prove asymptotic normality without sample splitting, uncovering a ``complexity-rate robustness'' condition. This condition has practical consequences: inference without sample splitting is possible in several machine learning settings, which may improve finite sample performance compared to sample splitting. Our estimators achieve nominal coverage in highly nonlinear simulations where previo",
    "path": "papers/21/01/2101.00009.json",
    "total_tokens": 890,
    "translated_title": "对Riesz Representer的敌对估计",
    "translated_abstract": "许多因果和结构参数是基于底层回归的线性泛函。Riesz Representer是半参数线性泛函渐近方差的关键组成部分。我们提出了一个敌对框架，使用通用函数空间来估计Riesz Representer。我们证明了一个非渐近均方速率，其中涉及一个称为临界半径的抽象量，然后将其专门应用于神经网络、随机森林和再生核希尔伯特空间作为主要案例。此外，我们使用临界半径理论来证明了渐近正态性，而不需要样本分割，揭示了一种“复杂度-速率鲁棒性”条件。这个条件具有实际后果：在几个机器学习设置中，可以实现无需样本分割的推断，这可能会提高有限样本性能。我们的估计器在高度非线性的模拟中实现了名义覆盖率。",
    "tldr": "我们提出了一个敌对框架，使用通用函数空间来估计Riesz Representer，并且证明了非渐近均方速率以及渐近正态性的条件。这个条件使得在机器学习中进行推断时无需样本分割，并且能够提高有限样本性能。"
}