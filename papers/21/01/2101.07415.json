{
    "title": "ES-ENAS: Efficient Evolutionary Optimization for Large Hybrid Search Spaces. (arXiv:2101.07415v6 [cs.LG] UPDATED)",
    "abstract": "In this paper, we approach the problem of optimizing blackbox functions over large hybrid search spaces consisting of both combinatorial and continuous parameters. We demonstrate that previous evolutionary algorithms which rely on mutation-based approaches, while flexible over combinatorial spaces, suffer from a curse of dimensionality in high dimensional continuous spaces both theoretically and empirically, which thus limits their scope over hybrid search spaces as well. In order to combat this curse, we propose ES-ENAS, a simple and modular joint optimization procedure combining the class of sample-efficient smoothed gradient techniques, commonly known as Evolutionary Strategies (ES), with combinatorial optimizers in a highly scalable and intuitive way, inspired by the one-shot or supernet paradigm introduced in Efficient Neural Architecture Search (ENAS). By doing so, we achieve significantly more sample efficiency, which we empirically demonstrate over synthetic benchmarks, and are",
    "link": "http://arxiv.org/abs/2101.07415",
    "context": "Title: ES-ENAS: Efficient Evolutionary Optimization for Large Hybrid Search Spaces. (arXiv:2101.07415v6 [cs.LG] UPDATED)\nAbstract: In this paper, we approach the problem of optimizing blackbox functions over large hybrid search spaces consisting of both combinatorial and continuous parameters. We demonstrate that previous evolutionary algorithms which rely on mutation-based approaches, while flexible over combinatorial spaces, suffer from a curse of dimensionality in high dimensional continuous spaces both theoretically and empirically, which thus limits their scope over hybrid search spaces as well. In order to combat this curse, we propose ES-ENAS, a simple and modular joint optimization procedure combining the class of sample-efficient smoothed gradient techniques, commonly known as Evolutionary Strategies (ES), with combinatorial optimizers in a highly scalable and intuitive way, inspired by the one-shot or supernet paradigm introduced in Efficient Neural Architecture Search (ENAS). By doing so, we achieve significantly more sample efficiency, which we empirically demonstrate over synthetic benchmarks, and are",
    "path": "papers/21/01/2101.07415.json",
    "total_tokens": 874,
    "translated_title": "ES-ENAS: 高效演化优化大型混合搜索空间",
    "translated_abstract": "本文研究了在大型混合搜索空间（由组合和连续参数组成）上优化黑盒函数的问题。我们证明先前依赖于变异策略的进化算法，在组合空间上具有灵活性，但在高维连续空间上存在维度灾难的问题，从理论和实验上限制了它们在混合搜索空间上的应用。为了解决这个问题，我们提出ES-ENAS，一种简单且模块化的联合优化过程，将效率低下的组合优化器与演化策略相结合，通过启发式的一次性或超级网络范式，实现高度可扩展且直观的优化过程。通过这样做，我们实现了更高的样本效率，在合成基准测试中进行了经验验证，在单个GPU上数小时内就可以优化CIFAR-10的复杂混合搜索空间，并超越了现有的最新方法。",
    "tldr": "本文提出了ES-ENAS方法，将进化策略和组合优化器结合起来来优化混合搜索空间，并在CIFAR-10上取得了超越最新方法的结果。",
    "en_tdlr": "This paper proposes ES-ENAS that efficiently optimizes large hybrid search spaces by combining evolutionary strategies and combinatorial optimizers, achieving state-of-the-art results on CIFAR-10."
}