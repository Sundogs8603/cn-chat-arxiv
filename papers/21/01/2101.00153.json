{
    "title": "Graphmax for Text Generation. (arXiv:2101.00153v2 [cs.CL] UPDATED)",
    "abstract": "In text generation, a large language model (LM) makes a choice of each new word based only on the former selection of its context using the softmax function. Nevertheless, the link statistics information of concurrent words based on a scene-specific corpus is valuable in choosing the next word, which can help to ensure the topic of the generated text to be aligned with the current task. To fully explore the co-occurrence information,we propose a graphmax function for task-specific text generation. Using the graph-based regularization, graphmax enables the final word choice to be determined by both the global knowledge from the LM and the local knowledge from the scene-specific corpus. The traditional softmax function is regularized with a graph total variation (GTV) term, which incorporates the local knowledge into the LM and encourages the model to consider the statistical relationships between words in a scene-specific corpus. The proposed graphmax is versatile and can be readily plu",
    "link": "http://arxiv.org/abs/2101.00153",
    "context": "Title: Graphmax for Text Generation. (arXiv:2101.00153v2 [cs.CL] UPDATED)\nAbstract: In text generation, a large language model (LM) makes a choice of each new word based only on the former selection of its context using the softmax function. Nevertheless, the link statistics information of concurrent words based on a scene-specific corpus is valuable in choosing the next word, which can help to ensure the topic of the generated text to be aligned with the current task. To fully explore the co-occurrence information,we propose a graphmax function for task-specific text generation. Using the graph-based regularization, graphmax enables the final word choice to be determined by both the global knowledge from the LM and the local knowledge from the scene-specific corpus. The traditional softmax function is regularized with a graph total variation (GTV) term, which incorporates the local knowledge into the LM and encourages the model to consider the statistical relationships between words in a scene-specific corpus. The proposed graphmax is versatile and can be readily plu",
    "path": "papers/21/01/2101.00153.json",
    "total_tokens": 860,
    "translated_title": "图形最大化用于文本生成",
    "translated_abstract": "在文本生成中，一个大型语言模型（LM）仅基于上下文中先前选择的内容，使用softmax函数选择每个新词。然而，基于特定场景语料库的并发词的链接统计信息对选择下一个词是有价值的，可以帮助确保生成文本的主题与当前任务相一致。为了充分利用共现信息，我们提出了一种用于任务特定文本生成的图形最大化函数。使用基于图的正则化，图形最大化使最终词的选择由LM的全局知识和特定场景语料库的局部知识共同确定。传统的softmax函数通过图总变化（GTV）项进行正则化，将局部知识融入到LM中，并鼓励模型考虑特定场景语料库中单词之间的统计关系。所提出的图形最大化功能多样且易于使用。",
    "tldr": "本论文提出了图形最大化函数，用于任务特定的文本生成。该函数结合了语言模型的全局知识和特定场景语料库的局部知识，通过正则化的方式应用于传统的softmax函数，以充分利用共现信息，提高生成文本的主题一致性。"
}