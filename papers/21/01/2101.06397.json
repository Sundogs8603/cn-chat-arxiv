{
    "title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph. (arXiv:2101.06397v2 [cs.CL] UPDATED)",
    "abstract": "In this paper, we propose an explanation of representation for self-attention network (SAN) based neural sequence encoders, which regards the information captured by the model and the encoding of the model as graph structure and the generation of these graph structures respectively. The proposed explanation applies to existing works on SAN-based models and can explain the relationship among the ability to capture the structural or linguistic information, depth of model, and length of sentence, and can also be extended to other models such as recurrent neural network based models. We also propose a revisited multigraph called Multi-order-Graph (MoG) based on our explanation to model the graph structures in the SAN-based model as subgraphs in MoG and convert the encoding of SAN-based model to the generation of MoG. Based on our explanation, we further introduce a Graph-Transformer by enhancing the ability to capture multiple subgraphs of different orders and focusing on subgraphs of high",
    "link": "http://arxiv.org/abs/2101.06397",
    "context": "Title: To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph. (arXiv:2101.06397v2 [cs.CL] UPDATED)\nAbstract: In this paper, we propose an explanation of representation for self-attention network (SAN) based neural sequence encoders, which regards the information captured by the model and the encoding of the model as graph structure and the generation of these graph structures respectively. The proposed explanation applies to existing works on SAN-based models and can explain the relationship among the ability to capture the structural or linguistic information, depth of model, and length of sentence, and can also be extended to other models such as recurrent neural network based models. We also propose a revisited multigraph called Multi-order-Graph (MoG) based on our explanation to model the graph structures in the SAN-based model as subgraphs in MoG and convert the encoding of SAN-based model to the generation of MoG. Based on our explanation, we further introduce a Graph-Transformer by enhancing the ability to capture multiple subgraphs of different orders and focusing on subgraphs of high",
    "path": "papers/21/01/2101.06397.json",
    "total_tokens": 865,
    "translated_title": "理解基于层感知序列编码器的表示为多阶图的模型",
    "translated_abstract": "本文提出了一种自我注意力网络（SAN）序列编码器表示的解释方式，将模型捕获的信息和模型的编码分别视为图结构和这些图结构的生成。该解释适用于现有的基于SAN的模型，并可以解释捕获结构或语言信息的能力、模型深度和句子长度之间的关系，并且可以扩展到其他模型，如基于递归神经网络的模型。",
    "tldr": "本文提出了一种解释基于自我注意力网络的序列编码器表示的方式，将其视为多阶图结构，并提出了一种名为多阶图的模型来描述这些图结构，并将SAN模型的编码转换为MoG的生成。此外，还引入了一个名为Graph-Transformer的模型来增强捕获多个不同阶级的子图的能力，并关注高阶子图，从而进一步提高了模型的表现。"
}