{
    "title": "Breaking the Deadly Triad with a Target Network. (arXiv:2101.08862v9 [cs.LG] UPDATED)",
    "abstract": "The deadly triad refers to the instability of a reinforcement learning algorithm when it employs off-policy learning, function approximation, and bootstrapping simultaneously. In this paper, we investigate the target network as a tool for breaking the deadly triad, providing theoretical support for the conventional wisdom that a target network stabilizes training. We first propose and analyze a novel target network update rule which augments the commonly used Polyak-averaging style update with two projections. We then apply the target network and ridge regularization in several divergent algorithms and show their convergence to regularized TD fixed points. Those algorithms are off-policy with linear function approximation and bootstrapping, spanning both policy evaluation and control, as well as both discounted and average-reward settings. In particular, we provide the first convergent linear $Q$-learning algorithms under nonrestrictive and changing behavior policies without bi-level o",
    "link": "http://arxiv.org/abs/2101.08862",
    "context": "Title: Breaking the Deadly Triad with a Target Network. (arXiv:2101.08862v9 [cs.LG] UPDATED)\nAbstract: The deadly triad refers to the instability of a reinforcement learning algorithm when it employs off-policy learning, function approximation, and bootstrapping simultaneously. In this paper, we investigate the target network as a tool for breaking the deadly triad, providing theoretical support for the conventional wisdom that a target network stabilizes training. We first propose and analyze a novel target network update rule which augments the commonly used Polyak-averaging style update with two projections. We then apply the target network and ridge regularization in several divergent algorithms and show their convergence to regularized TD fixed points. Those algorithms are off-policy with linear function approximation and bootstrapping, spanning both policy evaluation and control, as well as both discounted and average-reward settings. In particular, we provide the first convergent linear $Q$-learning algorithms under nonrestrictive and changing behavior policies without bi-level o",
    "path": "papers/21/01/2101.08862.json",
    "total_tokens": 908,
    "translated_title": "通过目标网络打破致命三角（Reinforcement Learning）",
    "translated_abstract": "“致命三角”是指强化学习算法在同时使用离线学习、函数逼近和自举时的不稳定性。本文研究了目标网络作为打破“致命三角”的工具，提供了理论支持，证明了目标网络稳定训练的常识。首先，我们提出并分析了一种新颖的目标网络更新规则，将常用的 Polyak 平均风格更新与两个投影相结合。然后，在几个不同的算法中应用目标网络和岭正则化，证明它们可以收敛到正则化 TD 固定点，这些算法都是离线学习、线性函数逼近和自举的，涵盖政策评估和控制，以及折扣和平均奖励设置。特别地，我们提供了第一个收敛的线性$Q$学习算法，这些算法在非限制性和变化行为策略下均成立，不需要双层优化。",
    "tldr": "本论文研究了如何通过使用目标网络来稳定训练，提出了一种新的目标网络更新规则并证明了其在离线学习、线性函数逼近和自举的算法中的收敛性，最终达到了收敛到正则化TD固定点的效果。",
    "en_tdlr": "This paper investigates using a target network to stabilize reinforcement learning training and proposes a novel target network update rule. The authors validate the convergence of this approach in several off-policy learning algorithms with linear function approximation and bootstrapping, achieving the desired effect of convergence to regularized TD fixed points."
}