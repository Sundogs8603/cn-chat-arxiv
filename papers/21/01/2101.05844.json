{
    "title": "Scaling the Convex Barrier with Sparse Dual Algorithms",
    "abstract": "arXiv:2101.05844v3 Announce Type: replace  Abstract: Tight and efficient neural network bounding is crucial to the scaling of neural network verification systems. Many efficient bounding algorithms have been presented recently, but they are often too loose to verify more challenging properties. This is due to the weakness of the employed relaxation, which is usually a linear program of size linear in the number of neurons. While a tighter linear relaxation for piecewise-linear activations exists, it comes at the cost of exponentially many constraints and currently lacks an efficient customized solver. We alleviate this deficiency by presenting two novel dual algorithms: one operates a subgradient method on a small active set of dual variables, the other exploits the sparsity of Frank-Wolfe type optimizers to incur only a linear memory cost. Both methods recover the strengths of the new relaxation: tightness and a linear separation oracle. At the same time, they share the benefits of pr",
    "link": "https://arxiv.org/abs/2101.05844",
    "context": "Title: Scaling the Convex Barrier with Sparse Dual Algorithms\nAbstract: arXiv:2101.05844v3 Announce Type: replace  Abstract: Tight and efficient neural network bounding is crucial to the scaling of neural network verification systems. Many efficient bounding algorithms have been presented recently, but they are often too loose to verify more challenging properties. This is due to the weakness of the employed relaxation, which is usually a linear program of size linear in the number of neurons. While a tighter linear relaxation for piecewise-linear activations exists, it comes at the cost of exponentially many constraints and currently lacks an efficient customized solver. We alleviate this deficiency by presenting two novel dual algorithms: one operates a subgradient method on a small active set of dual variables, the other exploits the sparsity of Frank-Wolfe type optimizers to incur only a linear memory cost. Both methods recover the strengths of the new relaxation: tightness and a linear separation oracle. At the same time, they share the benefits of pr",
    "path": "papers/21/01/2101.05844.json",
    "total_tokens": 864,
    "translated_title": "使用稀疏对偶算法扩展凸障碍",
    "translated_abstract": "紧凑而高效的神经网络边界对神经网络验证系统的扩展至关重要。最近提出了许多高效的边界算法，但通常太松以验证更具挑战性的属性。这是因为所使用的松弛性质较弱，通常是与神经元数量成线性关系的线性规划。虽然存在一种更紧致的分段线性激活松弛，但代价是指数级的约束，当前缺乏高效的定制求解器。我们通过提出两种新颖的对偶算法来解决这一不足：一种在一个小的对偶变量活跃集上运行次梯度方法，另一种利用Frank-Wolfe类型优化器的稀疏性质仅产生线性内存开销。这两种方法恢复了新松弛的优势：紧凑性和线性分离预言。同时，它们也分享了pr的好处",
    "tldr": "提出了两种新颖的对偶算法，通过操作小的对偶变量活跃集上的次梯度方法和利用Frank-Wolfe类型优化器的稀疏性质，来解决神经网络边界验证系统中常见的松弛性问题。",
    "en_tdlr": "Two novel dual algorithms are proposed to address the common relaxation issues in neural network bounding systems by operating a subgradient method on a small active set of dual variables and leveraging the sparsity of Frank-Wolfe type optimizers."
}