{
    "title": "Calibrating and Improving Graph Contrastive Learning. (arXiv:2101.11525v2 [cs.LG] UPDATED)",
    "abstract": "Graph contrastive learning algorithms have demonstrated remarkable success in various applications such as node classification, link prediction, and graph clustering. However, in unsupervised graph contrastive learning, some contrastive pairs may contradict the truths in downstream tasks and thus the decrease of losses on these pairs undesirably harms the performance in the downstream tasks. To assess the discrepancy between the prediction and the ground-truth in the downstream tasks for these contrastive pairs, we adapt the expected calibration error (ECE) to graph contrastive learning. The analysis of ECE motivates us to propose a novel regularization method, Contrast-Reg, to ensure that decreasing the contrastive loss leads to better performance in the downstream tasks. As a plug-in regularizer, Contrast-Reg effectively improves the performance of existing graph contrastive learning algorithms. We provide both theoretical and empirical results to demonstrate the effectiveness of Con",
    "link": "http://arxiv.org/abs/2101.11525",
    "context": "Title: Calibrating and Improving Graph Contrastive Learning. (arXiv:2101.11525v2 [cs.LG] UPDATED)\nAbstract: Graph contrastive learning algorithms have demonstrated remarkable success in various applications such as node classification, link prediction, and graph clustering. However, in unsupervised graph contrastive learning, some contrastive pairs may contradict the truths in downstream tasks and thus the decrease of losses on these pairs undesirably harms the performance in the downstream tasks. To assess the discrepancy between the prediction and the ground-truth in the downstream tasks for these contrastive pairs, we adapt the expected calibration error (ECE) to graph contrastive learning. The analysis of ECE motivates us to propose a novel regularization method, Contrast-Reg, to ensure that decreasing the contrastive loss leads to better performance in the downstream tasks. As a plug-in regularizer, Contrast-Reg effectively improves the performance of existing graph contrastive learning algorithms. We provide both theoretical and empirical results to demonstrate the effectiveness of Con",
    "path": "papers/21/01/2101.11525.json",
    "total_tokens": 831,
    "translated_title": "校准和改进图对比学习",
    "translated_abstract": "图对比学习算法在节点分类、链接预测和图聚类等各种应用中取得了显著的成功。然而，在无监督图对比学习中，一些对比对可能与下游任务中的真相相矛盾，因此在这些对比对上减少损失会不希望地影响下游任务的性能。为了评估这些对比对在下游任务中的预测与真相之间的差异，我们将期望校准误差（ECE）应用于图对比学习。ECE的分析激发我们提出了一种新的规则化方法，Contrast-Reg，以确保减少对比损失能够在下游任务中取得更好的性能。作为插件式规则化器，Contrast-Reg有效地提高了现有图对比学习算法的性能。我们提供了理论和实证结果来证明Contrast-Reg的有效性。",
    "tldr": "该论文提出了一种新的规则化方法，Contrast-Reg，通过应用期望校准误差（ECE）来校准和改进图对比学习算法，以提高在下游任务中的性能。",
    "en_tdlr": "The paper proposes a novel regularization method, Contrast-Reg, for calibrating and improving graph contrastive learning algorithms by adapting the expected calibration error (ECE). This method effectively enhances the performance in downstream tasks."
}