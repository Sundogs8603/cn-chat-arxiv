{
    "title": "An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v3 [cs.CL] UPDATED)",
    "abstract": "The large attention-based encoder-decoder network (Transformer) has become prevailing recently due to its effectiveness. But the high computation complexity of its decoder raises the inefficiency issue. By examining the mathematic formulation of the decoder, we show that under some mild conditions, the architecture could be simplified by compressing its sub-layers, the basic building block of Transformer, and achieves a higher parallelism. We thereby propose Compressed Attention Network, whose decoder layer consists of only one sub-layer instead of three. Extensive experiments on 14 WMT machine translation tasks show that our model is 1.42x faster with performance on par with a strong baseline. This strong baseline is already 2x faster than the widely used standard baseline without loss in performance.",
    "link": "http://arxiv.org/abs/2101.00542",
    "context": "Title: An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v3 [cs.CL] UPDATED)\nAbstract: The large attention-based encoder-decoder network (Transformer) has become prevailing recently due to its effectiveness. But the high computation complexity of its decoder raises the inefficiency issue. By examining the mathematic formulation of the decoder, we show that under some mild conditions, the architecture could be simplified by compressing its sub-layers, the basic building block of Transformer, and achieves a higher parallelism. We thereby propose Compressed Attention Network, whose decoder layer consists of only one sub-layer instead of three. Extensive experiments on 14 WMT machine translation tasks show that our model is 1.42x faster with performance on par with a strong baseline. This strong baseline is already 2x faster than the widely used standard baseline without loss in performance.",
    "path": "papers/21/01/2101.00542.json",
    "total_tokens": 784,
    "translated_title": "一种具有压缩子层的高效Transformer解码器",
    "translated_abstract": "最近由于其高效而流行的大型基于注意力的编码器-解码器网络（Transformer）的解码器计算复杂度高，导致效率问题。通过查看解码器的数学公式，我们发现在某些条件下，可以通过压缩其子层（Transformer的基本构建块）简化架构并实现更高的并行性。因此，我们提出了压缩注意力网络，其解码器层仅包含一个子层而不是三个子层。在14个WMT机器翻译任务的广泛实验中表明，我们的模型比强基线快1.42倍，并且性能相当。而这个强基线已经比广泛使用的标准基线快2倍而且性能不降。",
    "tldr": "该论文提出了一种使用压缩子层的高效Transformer解码器，通过减少子层并提高并行性能够达到1.42倍的速度提升，同时确保性能与基线相当。",
    "en_tdlr": "The paper proposes an efficient Transformer decoder with compressed sub-layers, which achieves a 1.42x speedup by reducing sub-layers and improving parallelism, while ensuring performance on par with a strong baseline, and the proposed model is faster than the widely used standard baseline by 2x with no loss in performance."
}