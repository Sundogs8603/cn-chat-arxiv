{
    "title": "10 Security and Privacy Problems in Large Foundation Models. (arXiv:2110.15444v3 [cs.CR] UPDATED)",
    "abstract": "Foundation models--such as GPT, CLIP, and DINO--have achieved revolutionary progress in the past several years and are commonly believed to be a promising approach for general-purpose AI. In particular, self-supervised learning is adopted to pre-train a foundation model using a large amount of unlabeled data. A pre-trained foundation model is like an ``operating system'' of the AI ecosystem. Specifically, a foundation model can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on foundation models mainly focused on pre-training a better foundation model to improve its performance on downstream tasks in non-adversarial settings, leaving its security and privacy in adversarial settings largely unexplored. A security or privacy issue of a pre-trained foundation model leads to a single point of failure for the AI ecosystem. In this book chapter, we discuss 10 basic security and privacy problems for the pre-trained foundation ",
    "link": "http://arxiv.org/abs/2110.15444",
    "context": "Title: 10 Security and Privacy Problems in Large Foundation Models. (arXiv:2110.15444v3 [cs.CR] UPDATED)\nAbstract: Foundation models--such as GPT, CLIP, and DINO--have achieved revolutionary progress in the past several years and are commonly believed to be a promising approach for general-purpose AI. In particular, self-supervised learning is adopted to pre-train a foundation model using a large amount of unlabeled data. A pre-trained foundation model is like an ``operating system'' of the AI ecosystem. Specifically, a foundation model can be used as a feature extractor for many downstream tasks with little or no labeled training data. Existing studies on foundation models mainly focused on pre-training a better foundation model to improve its performance on downstream tasks in non-adversarial settings, leaving its security and privacy in adversarial settings largely unexplored. A security or privacy issue of a pre-trained foundation model leads to a single point of failure for the AI ecosystem. In this book chapter, we discuss 10 basic security and privacy problems for the pre-trained foundation ",
    "path": "papers/21/10/2110.15444.json",
    "total_tokens": 897,
    "translated_title": "大型基础模型存在的10个安全和隐私问题",
    "translated_abstract": "基础模型（如GPT、CLIP和DINO）在过去几年中取得了革命性进展，被普遍认为是通用AI的一种有前途的方法。特别是，采用无监督学习来预训练基础模型，使用大量未标记的数据。预训练的基础模型就像AI生态系统的“操作系统”。现有研究主要集中在预训练一个更好的基础模型，以改善其在非对抗设置下在下游任务中的表现，而在对抗设置下，其安全和隐私则很少被探索。预训练基础模型的安全或隐私问题会导致AI生态系统的单一故障点。",
    "tldr": "大型基础模型存在多种攻击漏洞，包括模型反演、成员推断、数据污染、后门等，这些问题可能对现实世界的AI应用产生影响，需要寻求解决方案和未来研究方向。",
    "en_tdlr": "There are 10 security and privacy problems in large foundation models such as GPT, CLIP and DINO, including vulnerabilities to attacks such as model inversion, membership inference, data poisoning, and backdoor, which can potentially impact real-world AI applications, and require solutions and future research directions."
}