{
    "title": "Decoupled Contrastive Learning. (arXiv:2110.06848v3 [cs.LG] CROSS LISTED)",
    "abstract": "Contrastive learning (CL) is one of the most successful paradigms for self-supervised learning (SSL). In a principled way, it considers two augmented \"views\" of the same image as positive to be pulled closer, and all other images as negative to be pushed further apart. However, behind the impressive success of CL-based techniques, their formulation often relies on heavy-computation settings, including large sample batches, extensive training epochs, etc. We are thus motivated to tackle these issues and establish a simple, efficient, yet competitive baseline of contrastive learning. Specifically, we identify, from theoretical and empirical studies, a noticeable negative-positive-coupling (NPC) effect in the widely used InfoNCE loss, leading to unsuitable learning efficiency concerning the batch size. By removing the NPC effect, we propose decoupled contrastive learning (DCL) loss, which removes the positive term from the denominator and significantly improves the learning efficiency. DC",
    "link": "http://arxiv.org/abs/2110.06848",
    "context": "Title: Decoupled Contrastive Learning. (arXiv:2110.06848v3 [cs.LG] CROSS LISTED)\nAbstract: Contrastive learning (CL) is one of the most successful paradigms for self-supervised learning (SSL). In a principled way, it considers two augmented \"views\" of the same image as positive to be pulled closer, and all other images as negative to be pushed further apart. However, behind the impressive success of CL-based techniques, their formulation often relies on heavy-computation settings, including large sample batches, extensive training epochs, etc. We are thus motivated to tackle these issues and establish a simple, efficient, yet competitive baseline of contrastive learning. Specifically, we identify, from theoretical and empirical studies, a noticeable negative-positive-coupling (NPC) effect in the widely used InfoNCE loss, leading to unsuitable learning efficiency concerning the batch size. By removing the NPC effect, we propose decoupled contrastive learning (DCL) loss, which removes the positive term from the denominator and significantly improves the learning efficiency. DC",
    "path": "papers/21/10/2110.06848.json",
    "total_tokens": 862,
    "translated_title": "分离式对比学习",
    "translated_abstract": "对比学习（CL）是自监督学习（SSL）最成功的范例之一。它以一种原则性的方式，将同一图像的两个增强“视图”视为正面，将所有其他图像视为负面，以拉近它们之间的距离。然而，在基于CL的技术的令人印象深刻的成功背后，它们的制定通常依赖于重计算设置，包括大的样本批次、广泛的训练时期等。因此，我们有动力解决这些问题，并建立一种简单、高效、有竞争力的对比学习基线。",
    "tldr": "本研究提出了分离式对比学习（DCL）损失，通过去除传统对比学习中的正项提高学习效率。",
    "en_tdlr": "This paper proposes Decoupled Contrastive Learning (DCL) loss, which removes the positive term from the denominator to significantly improve the learning efficiency in contrastive learning."
}