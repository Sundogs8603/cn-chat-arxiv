{
    "title": "Contextualized Semantic Distance between Highly Overlapped Texts. (arXiv:2110.01176v2 [cs.CL] UPDATED)",
    "abstract": "Overlapping frequently occurs in paired texts in natural language processing tasks like text editing and semantic similarity evaluation. Better evaluation of the semantic distance between the overlapped sentences benefits the language system's understanding and guides the generation. Since conventional semantic metrics are based on word representations, they are vulnerable to the disturbance of overlapped components with similar representations. This paper aims to address the issue with a mask-and-predict strategy. We take the words in the longest common sequence (LCS) as neighboring words and use masked language modeling (MLM) from pre-trained language models (PLMs) to predict the distributions on their positions. Our metric, Neighboring Distribution Divergence (NDD), represent the semantic distance by calculating the divergence between distributions in the overlapped parts. Experiments on Semantic Textual Similarity show NDD to be more sensitive to various semantic differences, espec",
    "link": "http://arxiv.org/abs/2110.01176",
    "context": "Title: Contextualized Semantic Distance between Highly Overlapped Texts. (arXiv:2110.01176v2 [cs.CL] UPDATED)\nAbstract: Overlapping frequently occurs in paired texts in natural language processing tasks like text editing and semantic similarity evaluation. Better evaluation of the semantic distance between the overlapped sentences benefits the language system's understanding and guides the generation. Since conventional semantic metrics are based on word representations, they are vulnerable to the disturbance of overlapped components with similar representations. This paper aims to address the issue with a mask-and-predict strategy. We take the words in the longest common sequence (LCS) as neighboring words and use masked language modeling (MLM) from pre-trained language models (PLMs) to predict the distributions on their positions. Our metric, Neighboring Distribution Divergence (NDD), represent the semantic distance by calculating the divergence between distributions in the overlapped parts. Experiments on Semantic Textual Similarity show NDD to be more sensitive to various semantic differences, espec",
    "path": "papers/21/10/2110.01176.json",
    "total_tokens": 918,
    "translated_title": "高度重叠文本的情境化语义距离",
    "translated_abstract": "在文本编辑和语义相似性评估等自然语言处理任务中，文本之间经常会出现重叠。更好地评估重叠句子之间的语义距离有助于语言系统的理解并指导生成。由于传统的语义度量基于单词表示，它们容易受到具有类似表示的重叠部分的干扰。本文旨在通过掩码和预测策略解决这个问题。我们将最长公共序列（LCS）中的单词作为邻近单词，并使用来自预训练语言模型（PLMs）的掩码语言建模（MLM）来预测其位置上的分布。我们的度量指标，即邻近分布散度（NDD），通过计算重叠部分中分布之间的差异来表示语义距离。在语义文本相似性测试中，实验证明NDD对于各种语义差异更为敏感，",
    "tldr": "本文旨在解决自然语言处理任务中，覆盖文本之间语义距离评估的传统挑战。通过掩码和预测策略，本文提出了邻近分布散度（NDD）来表示重叠部分的语义距离。实验结果表明，NDD对于各种语义差异更为敏感。",
    "en_tdlr": "This paper aims to address the traditional challenge of semantic distance evaluation between overlapping texts in natural language processing tasks. By using a mask-and-predict strategy, the Neighboring Distribution Divergence (NDD) is proposed to represent the semantic distance between overlapping parts. Experiment results show that NDD is more sensitive to various semantic differences."
}