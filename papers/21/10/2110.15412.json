{
    "title": "Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize. (arXiv:2110.15412v3 [math.OC] UPDATED)",
    "abstract": "We investigate the convergence of stochastic mirror descent (SMD) under interpolation in relatively smooth and smooth convex optimization. In relatively smooth convex optimization we provide new convergence guarantees for SMD with a constant stepsize. For smooth convex optimization we propose a new adaptive stepsize scheme -- the mirror stochastic Polyak stepsize (mSPS). Notably, our convergence results in both settings do not make bounded gradient assumptions or bounded variance assumptions, and we show convergence to a neighborhood that vanishes under interpolation. Consequently, these results correspond to the first convergence guarantees under interpolation for the exponentiated gradient algorithm for fixed or adaptive stepsizes. mSPS generalizes the recently proposed stochastic Polyak stepsize (SPS) (Loizou et al. 2021) to mirror descent and remains both practical and efficient for modern machine learning applications while inheriting the benefits of mirror descent. We complement ",
    "link": "http://arxiv.org/abs/2110.15412",
    "context": "Title: Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize. (arXiv:2110.15412v3 [math.OC] UPDATED)\nAbstract: We investigate the convergence of stochastic mirror descent (SMD) under interpolation in relatively smooth and smooth convex optimization. In relatively smooth convex optimization we provide new convergence guarantees for SMD with a constant stepsize. For smooth convex optimization we propose a new adaptive stepsize scheme -- the mirror stochastic Polyak stepsize (mSPS). Notably, our convergence results in both settings do not make bounded gradient assumptions or bounded variance assumptions, and we show convergence to a neighborhood that vanishes under interpolation. Consequently, these results correspond to the first convergence guarantees under interpolation for the exponentiated gradient algorithm for fixed or adaptive stepsizes. mSPS generalizes the recently proposed stochastic Polyak stepsize (SPS) (Loizou et al. 2021) to mirror descent and remains both practical and efficient for modern machine learning applications while inheriting the benefits of mirror descent. We complement ",
    "path": "papers/21/10/2110.15412.json",
    "total_tokens": 1063,
    "translated_title": "随机镜像下降：通过镜像随机Polyak步长的自适应变体进行收敛分析（更新版）",
    "translated_abstract": "我们研究了相对平滑和平滑凸优化中随机镜像下降（SMD）在内插下的收敛性。在相对平滑凸优化中，我们提供了使用恒定步长的SMD的新收敛保证。对于平滑凸优化，我们提出了一种新的自适应步长方案——镜像随机Polyak步长（mSPS）。值得注意的是，我们在这两种情况下的收敛结果都不做有界梯度假设或有界方差假设，并且我们显示了在内插下逐渐消失的邻域内的收敛性。因此，这些结果对于固定或自适应步长的指数梯度算法来说，都是首次在内插下获得的收敛保证。mSPS将最近提出的随机Polyak步长（SPS）（Loizou等人，2021）推广到镜像下降，并在继承镜像下降的优点的同时，保持了现代机器学习应用程序的实用性和高效性。",
    "tldr": "本文研究了在相对平滑和平滑凸优化中，随机镜像下降（SMD）在内插下的收敛性。在相对平滑凸优化中，使用恒定步长的SMD具有新的收敛保证。对于平滑凸优化，提出了一种新的自适应步长方案——镜像随机Polyak步长（mSPS）。这些结果是首个在内插下对指数梯度算法进行固定或自适应步长的收敛保证。"
}