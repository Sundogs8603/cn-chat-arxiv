{
    "title": "Discrete Acoustic Space for an Efficient Sampling in Neural Text-To-Speech. (arXiv:2110.12539v3 [cs.SD] UPDATED)",
    "abstract": "We present a Split Vector Quantized Variational Autoencoder (SVQ-VAE) architecture using a split vector quantizer for NTTS, as an enhancement to the well-known Variational Autoencoder (VAE) and Vector Quantized Variational Autoencoder (VQ-VAE) architectures. Compared to these previous architectures, our proposed model retains the benefits of using an utterance-level bottleneck, while keeping significant representation power and a discretized latent space small enough for efficient prediction from text. We train the model on recordings in the expressive task-oriented dialogues domain and show that SVQ-VAE achieves a statistically significant improvement in naturalness over the VAE and VQ-VAE models. Furthermore, we demonstrate that the SVQ-VAE latent acoustic space is predictable from text, reducing the gap between the standard constant vector synthesis and vocoded recordings by 32%.",
    "link": "http://arxiv.org/abs/2110.12539",
    "context": "Title: Discrete Acoustic Space for an Efficient Sampling in Neural Text-To-Speech. (arXiv:2110.12539v3 [cs.SD] UPDATED)\nAbstract: We present a Split Vector Quantized Variational Autoencoder (SVQ-VAE) architecture using a split vector quantizer for NTTS, as an enhancement to the well-known Variational Autoencoder (VAE) and Vector Quantized Variational Autoencoder (VQ-VAE) architectures. Compared to these previous architectures, our proposed model retains the benefits of using an utterance-level bottleneck, while keeping significant representation power and a discretized latent space small enough for efficient prediction from text. We train the model on recordings in the expressive task-oriented dialogues domain and show that SVQ-VAE achieves a statistically significant improvement in naturalness over the VAE and VQ-VAE models. Furthermore, we demonstrate that the SVQ-VAE latent acoustic space is predictable from text, reducing the gap between the standard constant vector synthesis and vocoded recordings by 32%.",
    "path": "papers/21/10/2110.12539.json",
    "total_tokens": 958,
    "translated_title": "用于神经文本到语音的高效采样的离散声学空间",
    "translated_abstract": "我们提出了一种使用拆分向量量化变分自编码器(SVQ-VAE)架构的离散声学空间，作为对知名的变分自编码器(VAE)和向量量化变分自编码器(VQ-VAE)架构的增强。与之前的架构相比，我们的模型保留了使用话语级限制的好处，同时保持了显著的表示能力，并且离散潜在空间小到足够从文本进行高效预测。我们在表达性任务对话领域的录音上训练模型，并显示SVQ-VAE在自然度方面明显优于VAE和VQ-VAE模型。此外，我们证明了SVQ-VAE潜在声学空间可从文本进行预测，将标准的恒定向量合成和声码器录音之间的差距减少了32%。",
    "tldr": "我们提出了一种使用拆分向量量化变分自编码器(SVQ-VAE)架构的离散声学空间，相比于之前的架构，该模型既保留了使用话语级限制的好处，又具有足够小的离散潜在空间以从文本进行高效预测。在实验中，我们证明了SVQ-VAE在自然度方面明显优于其他模型，并且可从文本进行预测，减少了合成和录音之间的差距。",
    "en_tdlr": "We propose a split vector quantized variational autoencoder (SVQ-VAE) architecture that utilizes a discrete acoustic space for efficient prediction from text. Compared to previous architectures, our model retains the benefits of using an utterance-level bottleneck and achieves improved naturalness while reducing the gap between synthetic and vocoded recordings."
}