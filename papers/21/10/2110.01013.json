{
    "title": "Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v2 [cs.CV] UPDATED)",
    "abstract": "Today's VQA models still tend to capture superficial linguistic correlations in the training set and fail to generalize to the test set with different QA distributions. To reduce these language biases, recent VQA works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on diagnostic benchmarks for out-of-distribution testing. However, due to complex model design, these ensemble-based methods are unable to equip themselves with two indispensable characteristics of an ideal VQA model: 1) Visual-explainable: The model should rely on the right visual regions when making decisions. 2) Question-sensitive: The model should be sensitive to the linguistic variations in questions. To this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing and Training (CSST) strategy. After training with CSST, VQA models are forced to focus on all critical objects and words, which significantly improves both visu",
    "link": "http://arxiv.org/abs/2110.01013",
    "context": "Title: Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering. (arXiv:2110.01013v2 [cs.CV] UPDATED)\nAbstract: Today's VQA models still tend to capture superficial linguistic correlations in the training set and fail to generalize to the test set with different QA distributions. To reduce these language biases, recent VQA works introduce an auxiliary question-only model to regularize the training of targeted VQA model, and achieve dominating performance on diagnostic benchmarks for out-of-distribution testing. However, due to complex model design, these ensemble-based methods are unable to equip themselves with two indispensable characteristics of an ideal VQA model: 1) Visual-explainable: The model should rely on the right visual regions when making decisions. 2) Question-sensitive: The model should be sensitive to the linguistic variations in questions. To this end, we propose a novel model-agnostic Counterfactual Samples Synthesizing and Training (CSST) strategy. After training with CSST, VQA models are forced to focus on all critical objects and words, which significantly improves both visu",
    "path": "papers/21/10/2110.01013.json",
    "total_tokens": 954,
    "translated_title": "强健的视觉问答需要合成对抗样本来训练",
    "translated_abstract": "当前的视觉问答模型往往只捕捉训练数据集中的表面语言相关性，并且不能很好地推广到具有不同问答分布的测试集中。为了减少这些语言偏差，最近的视觉问答研究引入了一个辅助的仅问题模型来规范有针对性的VQA模型的训练，并在诊断基准测试中取得了主导地位。但是，由于复杂的模型设计，这些基于集成的方法无法具备理想的VQA模型的两个不可或缺的特征：1）可视化解释：模型在做决策时应依赖于正确的视觉区域。2）问题敏感：模型对问题中的语言变化应具有敏感性。为此，我们提出了一种新的模型无关的对抗样本合成和训练（CSST）策略。经过CSST训练后，VQA模型被迫关注所有关键对象和单词，从而显著改善了视觉问答的性能。",
    "tldr": "本文提出了一种新的模型无关的对抗样本合成和训练（CSST）策略，可以有效解决当前视觉问答模型的语言偏差问题，显著改善模型的性能并具备理想的可视化解释和问题敏感性。",
    "en_tdlr": "This paper proposes a novel model-agnostic Counterfactual Samples Synthesizing and Training (CSST) strategy to effectively solve the language bias problem in current visual question answering models, significantly improving their performance and achieving ideal visual-explainability and question-sensitivity."
}