{
    "title": "Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees. (arXiv:2110.03313v3 [cs.LG] UPDATED)",
    "abstract": "Variational inequalities in general and saddle point problems in particular are increasingly relevant in machine learning applications, including adversarial learning, GANs, transport and robust optimization. With increasing data and problem sizes necessary to train high performing models across various applications, we need to rely on parallel and distributed computing. However, in distributed training, communication among the compute nodes is a key bottleneck during training, and this problem is exacerbated for high dimensional and over-parameterized models. Due to these considerations, it is important to equip existing methods with strategies that would allow to reduce the volume of transmitted information during training while obtaining a model of comparable quality. In this paper, we present the first theoretically grounded distributed methods for solving variational inequalities and saddle point problems using compressed communication: MASHA1 and MASHA2. Our theory and methods al",
    "link": "http://arxiv.org/abs/2110.03313",
    "context": "Title: Distributed Methods with Compressed Communication for Solving Variational Inequalities, with Theoretical Guarantees. (arXiv:2110.03313v3 [cs.LG] UPDATED)\nAbstract: Variational inequalities in general and saddle point problems in particular are increasingly relevant in machine learning applications, including adversarial learning, GANs, transport and robust optimization. With increasing data and problem sizes necessary to train high performing models across various applications, we need to rely on parallel and distributed computing. However, in distributed training, communication among the compute nodes is a key bottleneck during training, and this problem is exacerbated for high dimensional and over-parameterized models. Due to these considerations, it is important to equip existing methods with strategies that would allow to reduce the volume of transmitted information during training while obtaining a model of comparable quality. In this paper, we present the first theoretically grounded distributed methods for solving variational inequalities and saddle point problems using compressed communication: MASHA1 and MASHA2. Our theory and methods al",
    "path": "papers/21/10/2110.03313.json",
    "total_tokens": 818,
    "translated_title": "压缩通讯解决变分不等式的分布式方法及理论保证",
    "translated_abstract": "变分不等式和鞍点问题在机器学习应用中越来越受关注，包括对抗性学习、GAN、运输和强化优化等方面。为了训练高性能模型，需要依赖于并行和分布式计算。然而，在分布式训练中，计算节点之间的通信成为训练的关键瓶颈，特别是对于高维度和过参数化模型。因此，重要的是使用可以减少传输信息量的策略来降低训练中的通信量，同时获得具有可比性质量的模型。本文提出了MASHA1和MASHA2等基于压缩通讯的理论方法，用于解决变分不等式和鞍点问题的分布式方法。",
    "tldr": "本研究提出了MASHA1和MASHA2方法，可以减少在分布式训练中的通信量，并在获得可比性质量的模型的同时，解决变分不等式和鞍点问题。",
    "en_tdlr": "This paper proposes MASHA1 and MASHA2 methods for solving variational inequalities and saddle point problems using compressed communication in distributed training, which reduces the communication volume and obtains the model of comparable quality with theoretical guarantees."
}