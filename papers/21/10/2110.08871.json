{
    "title": "Expectation Distance-based Distributional Clustering for Noise-Robustness. (arXiv:2110.08871v4 [cs.LG] UPDATED)",
    "abstract": "This paper presents a clustering technique that reduces the susceptibility to data noise by learning and clustering the data-distribution and then assigning the data to the cluster of its distribution. In the process, it reduces the impact of noise on clustering results. This method involves introducing a new distance among distributions, namely the expectation distance (denoted, ED), that goes beyond the state-of-art distribution distance of optimal mass transport (denoted, $W_2$ for $2$-Wasserstein): The latter essentially depends only on the marginal distributions while the former also employs the information about the joint distributions. Using the ED, the paper extends the classical $K$-means and $K$-medoids clustering to those over data-distributions (rather than raw-data) and introduces $K$-medoids using $W_2$. The paper also presents the closed-form expressions of the $W_2$ and ED distance measures. The implementation results of the proposed ED and the $W_2$ distance measures t",
    "link": "http://arxiv.org/abs/2110.08871",
    "context": "Title: Expectation Distance-based Distributional Clustering for Noise-Robustness. (arXiv:2110.08871v4 [cs.LG] UPDATED)\nAbstract: This paper presents a clustering technique that reduces the susceptibility to data noise by learning and clustering the data-distribution and then assigning the data to the cluster of its distribution. In the process, it reduces the impact of noise on clustering results. This method involves introducing a new distance among distributions, namely the expectation distance (denoted, ED), that goes beyond the state-of-art distribution distance of optimal mass transport (denoted, $W_2$ for $2$-Wasserstein): The latter essentially depends only on the marginal distributions while the former also employs the information about the joint distributions. Using the ED, the paper extends the classical $K$-means and $K$-medoids clustering to those over data-distributions (rather than raw-data) and introduces $K$-medoids using $W_2$. The paper also presents the closed-form expressions of the $W_2$ and ED distance measures. The implementation results of the proposed ED and the $W_2$ distance measures t",
    "path": "papers/21/10/2110.08871.json",
    "total_tokens": 819,
    "translated_title": "基于期望距离的分布聚类方法用于噪声鲁棒性",
    "translated_abstract": "本文提出了一种聚类技术，通过学习和聚类数据分布，并将数据分配到其分布的簇中，从而减少数据噪声的影响。该方法引入了一种新的距离度量，即期望距离（ED），它超越了最优质量传输（$W_2$）的分布距离的现有技术：后者仅依赖于边缘分布，而前者还使用联合分布的信息。利用ED，本文将传统的$K$-means和$K$-medoids聚类扩展到数据分布上，并介绍使用$W_2$的$K$-medoids。本文还介绍了$W_2$和ED距离度量的闭合形式表达式。",
    "tldr": "本文提出了一种基于期望距离的分布聚类技术，可以降低噪声对聚类结果的影响。",
    "en_tdlr": "This paper proposes a distributional clustering technique based on expectation distance, which reduces the impact of noise on clustering results. The method introduces a new distance measure and extends traditional clustering algorithms to data distributions, improving noise-robustness."
}