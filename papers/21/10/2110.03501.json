{
    "title": "Pretrained Language Models are Symbolic Mathematics Solvers too!. (arXiv:2110.03501v3 [stat.ML] UPDATED)",
    "abstract": "Solving symbolic mathematics has always been of in the arena of human ingenuity that needs compositional reasoning and recurrence. However, recent studies have shown that large-scale language models such as transformers are universal and surprisingly can be trained as a sequence-to-sequence task to solve complex mathematical equations. These large transformer models need humongous amounts of training data to generalize to unseen symbolic mathematics problems. In this paper, we present a sample efficient way of solving the symbolic tasks by first pretraining the transformer model with language translation and then fine-tuning the pretrained transformer model to solve the downstream task of symbolic mathematics. We achieve comparable accuracy on the integration task with our pretrained model while using around $1.5$ orders of magnitude less number of training samples with respect to the state-of-the-art deep learning for symbolic mathematics. The test accuracy on differential equation ta",
    "link": "http://arxiv.org/abs/2110.03501",
    "context": "Title: Pretrained Language Models are Symbolic Mathematics Solvers too!. (arXiv:2110.03501v3 [stat.ML] UPDATED)\nAbstract: Solving symbolic mathematics has always been of in the arena of human ingenuity that needs compositional reasoning and recurrence. However, recent studies have shown that large-scale language models such as transformers are universal and surprisingly can be trained as a sequence-to-sequence task to solve complex mathematical equations. These large transformer models need humongous amounts of training data to generalize to unseen symbolic mathematics problems. In this paper, we present a sample efficient way of solving the symbolic tasks by first pretraining the transformer model with language translation and then fine-tuning the pretrained transformer model to solve the downstream task of symbolic mathematics. We achieve comparable accuracy on the integration task with our pretrained model while using around $1.5$ orders of magnitude less number of training samples with respect to the state-of-the-art deep learning for symbolic mathematics. The test accuracy on differential equation ta",
    "path": "papers/21/10/2110.03501.json",
    "total_tokens": 963,
    "translated_title": "预训练语言模型也是符号数学求解器！",
    "translated_abstract": "解决符号数学问题一直是需要组合推理和重复的人类创造力的领域。但是，最近的研究表明，诸如transformer之类的大规模语言模型是通用的，并且令人惊讶的是，它们可以被训练为用于解决复杂的数学方程的序列到序列任务。这些大型Transformer模型需要极其庞大的训练数据才能泛化到未见过的符号数学问题。在本文中，我们提出一种样本有效的方式来解决符号任务，首先通过语言翻译对Transformer模型进行预训练，然后微调预训练的Transformer模型以解决符号数学的下游任务。我们在积分任务上使用了大约1.5个数量级的训练样本，达到了与预先训练模型相当的准确性，而与针对符号数学的深度学习的最新技术相比使用了较少的训练样本。微分方程的测试准确性为...",
    "tldr": "本文研究表明，大规模语言模型可以训练为序列到序列任务，解决复杂的数学方程。文章提出了一种预训练并微调Transformer模型解决符号数学任务的方法，使用的训练样本比当前深度学习技术少1.5个数量级，且在积分任务上达到了可比较的准确性。",
    "en_tdlr": "This paper shows that large-scale language models can be trained for sequence-to-sequence tasks to solve complex mathematical equations. The authors propose a sample-efficient method of solving symbolic tasks by first pretraining the transformer model with language translation and then fine-tuning it for symbolic mathematics. They achieve comparable accuracy on the integration task while using significantly less training data compared to state-of-the-art deep learning techniques."
}