{
    "title": "Efficient GPU implementation of randomized SVD and its applications",
    "abstract": "arXiv:2110.03423v2 Announce Type: replace  Abstract: Matrix decompositions are ubiquitous in machine learning, including applications in dimensionality reduction, data compression and deep learning algorithms. Typical solutions for matrix decompositions have polynomial complexity which significantly increases their computational cost and time. In this work, we leverage efficient processing operations that can be run in parallel on modern Graphical Processing Units (GPUs), predominant computing architecture used e.g. in deep learning, to reduce the computational burden of computing matrix decompositions. More specifically, we reformulate the randomized decomposition problem to incorporate fast matrix multiplication operations (BLAS-3) as building blocks. We show that this formulation, combined with fast random number generators, allows to fully exploit the potential of parallel processing implemented in GPUs. Our extensive evaluation confirms the superiority of this approach over the co",
    "link": "https://arxiv.org/abs/2110.03423",
    "context": "Title: Efficient GPU implementation of randomized SVD and its applications\nAbstract: arXiv:2110.03423v2 Announce Type: replace  Abstract: Matrix decompositions are ubiquitous in machine learning, including applications in dimensionality reduction, data compression and deep learning algorithms. Typical solutions for matrix decompositions have polynomial complexity which significantly increases their computational cost and time. In this work, we leverage efficient processing operations that can be run in parallel on modern Graphical Processing Units (GPUs), predominant computing architecture used e.g. in deep learning, to reduce the computational burden of computing matrix decompositions. More specifically, we reformulate the randomized decomposition problem to incorporate fast matrix multiplication operations (BLAS-3) as building blocks. We show that this formulation, combined with fast random number generators, allows to fully exploit the potential of parallel processing implemented in GPUs. Our extensive evaluation confirms the superiority of this approach over the co",
    "path": "papers/21/10/2110.03423.json",
    "total_tokens": 839,
    "translated_title": "高效GPU实现随机化SVD及其应用",
    "translated_abstract": "矩阵分解在机器学习中应用广泛，包括在降维、数据压缩和深度学习算法中的应用。传统的矩阵分解方法具有多项式复杂度，这显著增加了它们的计算成本和时间。在这项工作中，我们利用现代图形处理单元（GPU）上可并行运行的高效处理操作，这是深度学习等领域中广泛使用的计算架构，以减少计算矩阵分解的计算负担。具体而言，我们重新制定了随机化分解问题，以包含快速矩阵乘法操作（BLAS-3）作为基本构建模块。我们展示该表述与快速随机数发生器相结合，可以充分利用GPU中实现的并行处理潜力。我们的广泛评估确认了这种方法优于co",
    "tldr": "本研究利用现代GPU上的高效处理操作，将随机化分解问题重新制定，结合快速矩阵乘法操作和随机数发生器，充分发挥GPU并行处理的潜力，减少计算矩阵分解的计算负担。",
    "en_tdlr": "This research leverages efficient processing operations on modern GPUs, reformulating the randomized decomposition problem with fast matrix multiplication operations and random number generators to fully exploit the potential of GPU parallel processing, reducing the computational burden of computing matrix decompositions."
}