{
    "title": "A theoretical and empirical study of new adaptive algorithms with additional momentum steps and shifted updates for stochastic non-convex optimization",
    "abstract": "It is known that adaptive optimization algorithms represent the key pillar behind the rise of the Machine Learning field. In the Optimization literature numerous studies have been devoted to accelerated gradient methods but only recently adaptive iterative techniques were analyzed from a theoretical point of view. In the present paper we introduce new adaptive algorithms endowed with momentum terms for stochastic non-convex optimization problems. Our purpose is to show a deep connection between accelerated methods endowed with different inertial steps and AMSGrad-type momentum methods. Our methodology is based on the framework of stochastic and possibly non-convex objective mappings, along with some assumptions that are often used in the investigation of adaptive algorithms. In addition to discussing the finite-time horizon analysis in relation to a certain final iteration and the almost sure convergence to stationary points, we shall also look at the worst-case iteration complexity. T",
    "link": "https://arxiv.org/abs/2110.08531",
    "context": "Title: A theoretical and empirical study of new adaptive algorithms with additional momentum steps and shifted updates for stochastic non-convex optimization\nAbstract: It is known that adaptive optimization algorithms represent the key pillar behind the rise of the Machine Learning field. In the Optimization literature numerous studies have been devoted to accelerated gradient methods but only recently adaptive iterative techniques were analyzed from a theoretical point of view. In the present paper we introduce new adaptive algorithms endowed with momentum terms for stochastic non-convex optimization problems. Our purpose is to show a deep connection between accelerated methods endowed with different inertial steps and AMSGrad-type momentum methods. Our methodology is based on the framework of stochastic and possibly non-convex objective mappings, along with some assumptions that are often used in the investigation of adaptive algorithms. In addition to discussing the finite-time horizon analysis in relation to a certain final iteration and the almost sure convergence to stationary points, we shall also look at the worst-case iteration complexity. T",
    "path": "papers/21/10/2110.08531.json",
    "total_tokens": 788,
    "translated_title": "对于随机非凸优化问题，一种新的自适应算法的理论和实证研究，带有附加动量步骤和平移更新",
    "translated_abstract": "已知自适应优化算法是机器学习领域蓬勃发展的关键。在优化文献中，许多研究都致力于加速梯度法，但只有最近才从理论角度对自适应迭代技术进行了分析。本文介绍了一种新的带有动量项的自适应算法，用于解决随机非凸优化问题。我们的目的是展示加速方法与AMSGrad类型动量方法之间的深层关联。我们的方法基于随机和可能非凸目标映射的框架，以及在自适应算法研究中经常使用的一些假设。除了讨论有限时间内与特定最终迭代相关的分析以及到达稳定点的几乎确定收敛性外，我们还将关注最坏情况迭代复杂度。",
    "tldr": "本文介绍了一种新的自适应算法，用于解决随机非凸优化问题，并展示了加速方法与AMSGrad类型动量方法之间的深层关联。"
}