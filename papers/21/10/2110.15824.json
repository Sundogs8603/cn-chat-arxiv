{
    "title": "Tractability from overparametrization: The example of the negative perceptron. (arXiv:2110.15824v3 [cs.LG] UPDATED)",
    "abstract": "In the negative perceptron problem we are given $n$ data points $({\\boldsymbol x}_i,y_i)$, where ${\\boldsymbol x}_i$ is a $d$-dimensional vector and $y_i\\in\\{+1,-1\\}$ is a binary label. The data are not linearly separable and hence we content ourselves to find a linear classifier with the largest possible \\emph{negative} margin. In other words, we want to find a unit norm vector ${\\boldsymbol \\theta}$ that maximizes $\\min_{i\\le n}y_i\\langle {\\boldsymbol \\theta},{\\boldsymbol x}_i\\rangle$. This is a non-convex optimization problem (it is equivalent to finding a maximum norm vector in a polytope), and we study its typical properties under two random models for the data.  We consider the proportional asymptotics in which $n,d\\to \\infty$ with $n/d\\to\\delta$, and prove upper and lower bounds on the maximum margin $\\kappa_{\\text{s}}(\\delta)$ or -- equivalently -- on its inverse function $\\delta_{\\text{s}}(\\kappa)$. In other words, $\\delta_{\\text{s}}(\\kappa)$ is the overparametrization thresho",
    "link": "http://arxiv.org/abs/2110.15824",
    "context": "Title: Tractability from overparametrization: The example of the negative perceptron. (arXiv:2110.15824v3 [cs.LG] UPDATED)\nAbstract: In the negative perceptron problem we are given $n$ data points $({\\boldsymbol x}_i,y_i)$, where ${\\boldsymbol x}_i$ is a $d$-dimensional vector and $y_i\\in\\{+1,-1\\}$ is a binary label. The data are not linearly separable and hence we content ourselves to find a linear classifier with the largest possible \\emph{negative} margin. In other words, we want to find a unit norm vector ${\\boldsymbol \\theta}$ that maximizes $\\min_{i\\le n}y_i\\langle {\\boldsymbol \\theta},{\\boldsymbol x}_i\\rangle$. This is a non-convex optimization problem (it is equivalent to finding a maximum norm vector in a polytope), and we study its typical properties under two random models for the data.  We consider the proportional asymptotics in which $n,d\\to \\infty$ with $n/d\\to\\delta$, and prove upper and lower bounds on the maximum margin $\\kappa_{\\text{s}}(\\delta)$ or -- equivalently -- on its inverse function $\\delta_{\\text{s}}(\\kappa)$. In other words, $\\delta_{\\text{s}}(\\kappa)$ is the overparametrization thresho",
    "path": "papers/21/10/2110.15824.json",
    "total_tokens": 964,
    "translated_title": "从过参数化到可处理性: 负感知机的例子",
    "translated_abstract": "在负感知机问题中，我们给定了$n$个数据点$({\\boldsymbol x}_i,y_i)$，其中${\\boldsymbol x}_i$是一个$d$维向量，$y_i\\in\\{+1,-1\\}$是一个二进制标签。数据不是线性可分的，因此我们满足于找到具有最大\\emph{负}间隔的线性分类器。换句话说，我们希望找到一个单位范数向量${\\boldsymbol \\theta}$，使得$\\min_{i\\le n}y_i\\langle {\\boldsymbol \\theta},{\\boldsymbol x}_i\\rangle$最大化。这是一个非凸优化问题(相当于在一个多面体中找到最大范数向量)，我们研究了在两个随机模型下该问题的典型性质。我们考虑当$n,d\\to \\infty$且$n/d\\to\\delta$时的比例渐近性，并证明了最大间隔$\\kappa_{\\text{s}}(\\delta)$或等价地其逆函数$\\delta_{\\text{s}}(\\kappa)$的上下界。换句话说，$\\delta_{\\text{s}}(\\kappa)$是过参数化阈值的一个度量。",
    "tldr": "本研究考虑了负感知机问题，通过两个随机模型研究了在大数据条件下最大负间隔的上下界。"
}