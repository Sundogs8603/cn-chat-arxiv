{
    "title": "Least-Squares Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Laws: Discrete Divergence Operator. (arXiv:2110.10895v3 [math.NA] UPDATED)",
    "abstract": "A least-squares neural network (LSNN) method was introduced for solving scalar linear and nonlinear hyperbolic conservation laws (HCLs) in [7, 6]. This method is based on an equivalent least-squares (LS) formulation and uses ReLU neural network as approximating functions, making it ideal for approximating discontinuous functions with unknown interface location. In the design of the LSNN method for HCLs, the numerical approximation of differential operators is a critical factor, and standard numerical or automatic differentiation along coordinate directions can often lead to a failed NN-based method. To overcome this challenge, this paper rewrites HCLs in their divergence form of space and time and introduces a new discrete divergence operator. As a result, the proposed LSNN method is free of penalization of artificial viscosity. Theoretically, the accuracy of the discrete divergence operator is estimated even for discontinuous solutions. Numerically, the LSNN method with the new discre",
    "link": "http://arxiv.org/abs/2110.10895",
    "context": "Title: Least-Squares Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Laws: Discrete Divergence Operator. (arXiv:2110.10895v3 [math.NA] UPDATED)\nAbstract: A least-squares neural network (LSNN) method was introduced for solving scalar linear and nonlinear hyperbolic conservation laws (HCLs) in [7, 6]. This method is based on an equivalent least-squares (LS) formulation and uses ReLU neural network as approximating functions, making it ideal for approximating discontinuous functions with unknown interface location. In the design of the LSNN method for HCLs, the numerical approximation of differential operators is a critical factor, and standard numerical or automatic differentiation along coordinate directions can often lead to a failed NN-based method. To overcome this challenge, this paper rewrites HCLs in their divergence form of space and time and introduces a new discrete divergence operator. As a result, the proposed LSNN method is free of penalization of artificial viscosity. Theoretically, the accuracy of the discrete divergence operator is estimated even for discontinuous solutions. Numerically, the LSNN method with the new discre",
    "path": "papers/21/10/2110.10895.json",
    "total_tokens": 1076,
    "translated_title": "最小二乘神经网络方法求解非线性双曲型守恒律：离散散度算子",
    "translated_abstract": "本文介绍了一种最小二乘神经网络（LSNN）方法，用于求解标量线性和非线性双曲型守恒律（HCLs），该方法基于等价的最小二乘（LS）公式，并使用修正线性单元（ReLU）神经网络作为逼近函数，可用于逼近未知界面位置的不连续函数。在LSNN方法设计中，微分运算的数值逼近是一个关键因素，标准的数值或自动微分沿坐标轴方向进行，往往会导致基于神经网络的方法失败。为了克服这一挑战，本文将HCLs转化为时空散度形式，并引入一个新的离散散度算子，从而使所提出的LSNN方法不受人工粘性惩罚。理论上，即使是不连续的解，也可以估计离散散度算子的精度。数值上，在一组基准问题中测试了带有新的离散散度算子的LSNN方法，包括一个具有运动不连续性的问题。结果表明，具有离散散度算子的LSNN方法在保持不连续性解和实现高精度和高效率方面具有很好的效果。",
    "tldr": "本文提出了一种最小二乘神经网络方法，用于求解双曲型守恒律，该方法使用离散散度算子，可用于逼近未知界面位置的不连续函数，并在保持不连续性解和实现高精度和高效率方面具有很好的效果。"
}