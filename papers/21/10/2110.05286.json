{
    "title": "Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning",
    "abstract": "Our work aims at efficiently leveraging ambiguous demonstrations for the training of a reinforcement learning (RL) agent. An ambiguous demonstration can usually be interpreted in multiple ways, which severely hinders the RL-Agent from learning stably and efficiently. Since an optimal demonstration may also suffer from being ambiguous, previous works that combine RL and learning from demonstration (RLfD works) may not work well. Inspired by how humans handle such situations, we propose to use self-explanation (an agent generates explanations for itself) to recognize valuable high-level relational features as an interpretation of why a successful trajectory is successful. This way, the agent can provide some guidance for its RL learning. Our main contribution is to propose the Self-Explanation for RL from Demonstrations (SERLfD) framework, which can overcome the limitations of traditional RLfD works. Our experimental results show that an RLfD model can be improved by using our SERLfD fra",
    "link": "https://arxiv.org/abs/2110.05286",
    "context": "Title: Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning\nAbstract: Our work aims at efficiently leveraging ambiguous demonstrations for the training of a reinforcement learning (RL) agent. An ambiguous demonstration can usually be interpreted in multiple ways, which severely hinders the RL-Agent from learning stably and efficiently. Since an optimal demonstration may also suffer from being ambiguous, previous works that combine RL and learning from demonstration (RLfD works) may not work well. Inspired by how humans handle such situations, we propose to use self-explanation (an agent generates explanations for itself) to recognize valuable high-level relational features as an interpretation of why a successful trajectory is successful. This way, the agent can provide some guidance for its RL learning. Our main contribution is to propose the Self-Explanation for RL from Demonstrations (SERLfD) framework, which can overcome the limitations of traditional RLfD works. Our experimental results show that an RLfD model can be improved by using our SERLfD fra",
    "path": "papers/21/10/2110.05286.json",
    "total_tokens": 934,
    "translated_title": "通过自我解释引导的强化学习从模棱两可的示范中学习",
    "translated_abstract": "我们的工作旨在有效地利用模棱两可的示范来训练强化学习代理。一个模棱两可的示范通常可以有多重解释，这严重阻碍了强化学习代理的稳定和高效学习。由于最佳示范也可能存在模棱两可的问题，之前将强化学习与从示范中学习相结合的工作可能无法很好地工作。受到人类处理这种情况的启发，我们提出使用自我解释（代理人为自身生成解释）来识别有价值的高级关系特征，作为成功轨迹成功的解释原因。通过这种方式，代理可以为其强化学习提供一些指导。我们的主要贡献是提出了自我解释从示范中学习的强化学习（SERLfD）框架，可以克服传统RLfD工作的局限性。我们的实验结果表明，使用我们的SERLfD框架可以改进RLfD模型。",
    "tldr": "本论文提出了一个用于训练强化学习代理的自我解释从示范中学习的框架（SERLfD）。通过代理人生成自我解释并识别有价值的高级关系特征作为成功轨迹的解释原因，可以在克服示范中存在的模棱两可情况的同时为强化学习提供指导。",
    "en_tdlr": "This paper proposes a Self-Explanation for RL from Demonstrations (SERLfD) framework, which uses self-explanation to identify valuable high-level relational features as explanations for successful trajectories, providing guidance for reinforcement learning while overcoming the ambiguity in demonstrations."
}