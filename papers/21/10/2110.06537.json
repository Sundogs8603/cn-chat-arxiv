{
    "title": "Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v6 [cs.LG] UPDATED)",
    "abstract": "The conventional wisdom behind learning deep classification models is to focus on bad-classified examples and ignore well-classified examples that are far from the decision boundary. For instance, when training with cross-entropy loss, examples with higher likelihoods (i.e., well-classified examples) contribute smaller gradients in back-propagation. However, we theoretically show that this common practice hinders representation learning, energy optimization, and margin growth. To counteract this deficiency, we propose to reward well-classified examples with additive bonuses to revive their contribution to the learning process. This counterexample theoretically addresses these three issues. We empirically support this claim by directly verifying the theoretical results or significant performance improvement with our counterexample on diverse tasks, including image classification, graph classification, and machine translation. Furthermore, this paper shows that we can deal with complex s",
    "link": "http://arxiv.org/abs/2110.06537",
    "context": "Title: Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v6 [cs.LG] UPDATED)\nAbstract: The conventional wisdom behind learning deep classification models is to focus on bad-classified examples and ignore well-classified examples that are far from the decision boundary. For instance, when training with cross-entropy loss, examples with higher likelihoods (i.e., well-classified examples) contribute smaller gradients in back-propagation. However, we theoretically show that this common practice hinders representation learning, energy optimization, and margin growth. To counteract this deficiency, we propose to reward well-classified examples with additive bonuses to revive their contribution to the learning process. This counterexample theoretically addresses these three issues. We empirically support this claim by directly verifying the theoretical results or significant performance improvement with our counterexample on diverse tasks, including image classification, graph classification, and machine translation. Furthermore, this paper shows that we can deal with complex s",
    "path": "papers/21/10/2110.06537.json",
    "total_tokens": 903,
    "tldr": "传统的深度神经网络分类学习方法低估了被很好分类的样本，我们通过奖励这些样本提出了一种对抗方法来提高性能，并且在图像分类、图形分类和机器翻译等任务上实现了显著性能提升。"
}