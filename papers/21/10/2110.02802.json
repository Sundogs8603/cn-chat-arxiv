{
    "title": "Self-conditioning pre-trained language models. (arXiv:2110.02802v4 [cs.CL] UPDATED)",
    "abstract": "In this paper we aim to investigate the mechanisms that guide text generation with pre-trained Transformer-based Language Models (TLMs). Grounded on the Product of Experts formulation by Hinton (1999), we describe a generative mechanism that exploits expert units which naturally exist in TLMs. Such units are responsible for detecting concepts in the input and conditioning text generation on such concepts. We describe how to identify expert units and how to activate them during inference in order to induce any desired concept in the generated output. We find that the activation of a surprisingly small amount of units is sufficient to steer text generation (as little as 3 units in a model with 345M parameters). While the objective of this work is to learn more about how TLMs work, we show that our method is effective for conditioning without fine-tuning or using extra parameters, even on fine-grained homograph concepts. Additionally, we show that our method can be used to correct gender ",
    "link": "http://arxiv.org/abs/2110.02802",
    "context": "Title: Self-conditioning pre-trained language models. (arXiv:2110.02802v4 [cs.CL] UPDATED)\nAbstract: In this paper we aim to investigate the mechanisms that guide text generation with pre-trained Transformer-based Language Models (TLMs). Grounded on the Product of Experts formulation by Hinton (1999), we describe a generative mechanism that exploits expert units which naturally exist in TLMs. Such units are responsible for detecting concepts in the input and conditioning text generation on such concepts. We describe how to identify expert units and how to activate them during inference in order to induce any desired concept in the generated output. We find that the activation of a surprisingly small amount of units is sufficient to steer text generation (as little as 3 units in a model with 345M parameters). While the objective of this work is to learn more about how TLMs work, we show that our method is effective for conditioning without fine-tuning or using extra parameters, even on fine-grained homograph concepts. Additionally, we show that our method can be used to correct gender ",
    "path": "papers/21/10/2110.02802.json",
    "total_tokens": 967,
    "translated_title": "自我调节预训练语言模型",
    "translated_abstract": "本文旨在探究预训练的基于Transformer的语言模型(TLMs)指导文本生成的机制。基于Hinton(1999)的专家乘积公式，我们描述了一种生成机制，利用TLMs中自然存在的专家单元来检测输入中的概念，并在生成的文本中对这些概念进行调节。我们阐述了如何确定专家单元以及如何在推理过程中激活它们，以引导生成的输出中出现所需的任何概念。我们发现，激活极少量的单元就足以引导文本生成(在一个拥有345M参数的模型中只需要3个单元)。虽然本研究的目的是更多地了解TLMs的工作原理，但我们证明了我们的方法在没有微调或使用额外参数的情况下对于条件调节是有效的，甚至对于细粒度的同音异义词概念也能够实现。此外，我们还展示了我们的方法可以用于纠正文本生成中的性别偏差。",
    "tldr": "本文探究了预训练语言模型(TLMs)的生成机制，并提出了一种自我调节的方法，利用TLMs中自然存在的专家单元来检测输入中的概念并对生成的文本进行调节。该方法即使在没有微调或使用额外参数的情况下也是有效的，可以纠正文本生成中的性别偏差。",
    "en_tdlr": "This paper investigates the generation mechanism of pre-trained Transformer-based language models (TLMs) and proposes a self-conditioning method that exploits expert units in TLMs to detect concepts in the input and condition text generation. The proposed method is effective even without fine-tuning or using extra parameters and can correct gender bias in text generation."
}