{
    "title": "Successor Feature Representations. (arXiv:2110.15701v4 [cs.LG] UPDATED)",
    "abstract": "Transfer in Reinforcement Learning aims to improve learning performance on target tasks using knowledge from experienced source tasks. Successor Representations (SR) and their extension Successor Features (SF) are prominent transfer mechanisms in domains where reward functions change between tasks. They reevaluate the expected return of previously learned policies in a new target task to transfer their knowledge. The SF framework extended SR by linearly decomposing rewards into successor features and a reward weight vector allowing their application in high-dimensional tasks. But this came with the cost of having a linear relationship between reward functions and successor features, limiting its application to tasks where such a linear relationship exists. We propose a novel formulation of SR based on learning the cumulative discounted probability of successor features, called Successor Feature Representations (SFR). Crucially, SFR allows to reevaluate the expected return of policies f",
    "link": "http://arxiv.org/abs/2110.15701",
    "context": "Title: Successor Feature Representations. (arXiv:2110.15701v4 [cs.LG] UPDATED)\nAbstract: Transfer in Reinforcement Learning aims to improve learning performance on target tasks using knowledge from experienced source tasks. Successor Representations (SR) and their extension Successor Features (SF) are prominent transfer mechanisms in domains where reward functions change between tasks. They reevaluate the expected return of previously learned policies in a new target task to transfer their knowledge. The SF framework extended SR by linearly decomposing rewards into successor features and a reward weight vector allowing their application in high-dimensional tasks. But this came with the cost of having a linear relationship between reward functions and successor features, limiting its application to tasks where such a linear relationship exists. We propose a novel formulation of SR based on learning the cumulative discounted probability of successor features, called Successor Feature Representations (SFR). Crucially, SFR allows to reevaluate the expected return of policies f",
    "path": "papers/21/10/2110.15701.json",
    "total_tokens": 853,
    "translated_title": "继承特征表示",
    "translated_abstract": "在强化学习中，迁移学习旨在利用源任务的知识来提高目标任务的学习性能。继承表示（SR）及其扩展的继承特征（SF）是在奖励函数在任务之间发生变化的领域中显著的迁移机制。它们重新评估先前学习策略在新的目标任务中的预期回报，以传递它们的知识。SF框架通过将奖励线性分解为继承特征和奖励权重向量，从而扩展了SR，并允许在高维任务中应用。但这样做的代价是奖励函数与继承特征之间存在线性关系，限制了它在存在这种线性关系的任务中的应用。我们提出了一种新的SR表达方式，即学习继承特征的累积折扣概率，称为继承特征表示（SFR）。关键是，SFR可以重新评估策略的预期回报",
    "tldr": "继承特征表示（SFR）是一种新的Successor Representations (SR)的表达方式，通过学习继承特征的累积折扣概率来重新评估策略的预期回报。"
}