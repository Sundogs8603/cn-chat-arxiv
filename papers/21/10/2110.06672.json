{
    "title": "The Deep Generative Decoder: MAP estimation of representations improves modeling of single-cell RNA data. (arXiv:2110.06672v3 [cs.LG] UPDATED)",
    "abstract": "Learning low-dimensional representations of single-cell transcriptomics has become instrumental to its downstream analysis. The state of the art is currently represented by neural network models such as variational autoencoders (VAEs) which use a variational approximation of the likelihood for inference. We here present the Deep Generative Decoder (DGD), a simple generative model that computes model parameters and representations directly via maximum a posteriori (MAP) estimation. The DGD handles complex parameterized latent distributions naturally unlike VAEs which typically use a fixed Gaussian distribution, because of the complexity of adding other types. We first show its general functionality on a commonly used benchmark set, Fashion-MNIST. Secondly, we apply the model to multiple single-cell data sets. Here the DGD learns low-dimensional, meaningful and well-structured latent representations with sub-clustering beyond the provided labels. The advantages of this approach are its s",
    "link": "http://arxiv.org/abs/2110.06672",
    "context": "Title: The Deep Generative Decoder: MAP estimation of representations improves modeling of single-cell RNA data. (arXiv:2110.06672v3 [cs.LG] UPDATED)\nAbstract: Learning low-dimensional representations of single-cell transcriptomics has become instrumental to its downstream analysis. The state of the art is currently represented by neural network models such as variational autoencoders (VAEs) which use a variational approximation of the likelihood for inference. We here present the Deep Generative Decoder (DGD), a simple generative model that computes model parameters and representations directly via maximum a posteriori (MAP) estimation. The DGD handles complex parameterized latent distributions naturally unlike VAEs which typically use a fixed Gaussian distribution, because of the complexity of adding other types. We first show its general functionality on a commonly used benchmark set, Fashion-MNIST. Secondly, we apply the model to multiple single-cell data sets. Here the DGD learns low-dimensional, meaningful and well-structured latent representations with sub-clustering beyond the provided labels. The advantages of this approach are its s",
    "path": "papers/21/10/2110.06672.json",
    "total_tokens": 901,
    "translated_title": "深度生成解码器：MAP估计的表示改进了单细胞RNA数据建模",
    "translated_abstract": "学习单细胞转录组学的低维表示对其下游分析至关重要。目前的最新技术是神经网络模型，如变分自编码器(VAEs)，它使用变分近似来进行推断。我们在这里介绍了深度生成解码器(DGD)，这是一个简单的生成模型，通过最大后验(MAP)估计直接计算模型参数和表示。DGD可以自然地处理复杂参数化的潜在分布，而VAEs通常使用固定的高斯分布，因为添加其他类型的分布会增加复杂性。我们首先在一个常用的基准数据集Fashion-MNIST上展示了其通用功能。其次，我们将该模型应用于多个单细胞数据集。在这里，DGD能够学习到低维、有意义且结构良好的潜在表示，并能在提供的标签之外进行子聚类。这种方法的优势在于它的简洁性和潜在表示的高质量。",
    "tldr": "Deep Generative Decoder (DGD) is a simple generative model that uses MAP estimation to compute model parameters and representations. It outperforms variational autoencoders (VAEs) by handling complex parameterized latent distributions and learning meaningful and well-structured latent representations on single-cell RNA data, including sub-clustering beyond provided labels."
}