{
    "title": "Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v4 [cs.CL] UPDATED)",
    "abstract": "Implicit knowledge, such as common sense, is key to fluid human conversations. Current neural response generation (RG) models are trained to generate responses directly, omitting unstated implicit knowledge. In this paper, we present Think-Before-Speaking (TBS), a generative approach to first externalize implicit commonsense knowledge (think) and use this knowledge to generate responses (speak). We expect that externalizing implicit knowledge allows more efficient learning, produces more informative responses, and enables more explainable models. We analyze different choices to collect knowledge-aligned dialogues, represent implicit knowledge, and transition between knowledge and dialogues. Empirical results show TBS models outperform end-to-end and knowledge-augmented RG baselines on most automatic metrics and generate more informative, specific, and commonsense-following responses, as evaluated by human annotators. TBS also generates knowledge that makes sense and is relevant to the ",
    "link": "http://arxiv.org/abs/2110.08501",
    "context": "Title: Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation. (arXiv:2110.08501v4 [cs.CL] UPDATED)\nAbstract: Implicit knowledge, such as common sense, is key to fluid human conversations. Current neural response generation (RG) models are trained to generate responses directly, omitting unstated implicit knowledge. In this paper, we present Think-Before-Speaking (TBS), a generative approach to first externalize implicit commonsense knowledge (think) and use this knowledge to generate responses (speak). We expect that externalizing implicit knowledge allows more efficient learning, produces more informative responses, and enables more explainable models. We analyze different choices to collect knowledge-aligned dialogues, represent implicit knowledge, and transition between knowledge and dialogues. Empirical results show TBS models outperform end-to-end and knowledge-augmented RG baselines on most automatic metrics and generate more informative, specific, and commonsense-following responses, as evaluated by human annotators. TBS also generates knowledge that makes sense and is relevant to the ",
    "path": "papers/21/10/2110.08501.json",
    "total_tokens": 930,
    "translated_title": "慎言而后言：明确生成隐含常识知识以供响应生成",
    "translated_abstract": "隐含知识，如常识，对于流畅的人类对话很关键。当前的神经响应生成模型被训练成直接生成响应，忽略了未明示的隐含知识。在本文中，我们提出了慎言而后言（TBS）的生成方法，首先外化隐含常识知识（思考），然后利用该知识生成响应（言）。我们期望外化隐含知识能够实现更高效的学习，产生更丰富的响应，并且实现更可解释的模型。我们分析了采集知识对齐对话、表示隐含知识以及在知识和对话之间过渡的不同选择。实证结果显示，TBS模型在大多数自动评估指标上优于端到端和增加知识的响应生成基准，并且生成更丰富、更具体且遵循常识的响应，经由人工标注者评估也生成具有意义且相关的知识。",
    "tldr": "本文提出了一种生成方法，可以明确生成隐含常识知识并用于响应生成。实证结果显示，该方法能够产生更丰富、更具体且遵循常识的响应，经由人工标注者评估也能够生成具有意义且相关的知识。",
    "en_tdlr": "This paper presents a generative approach to explicitly generate implicit commonsense knowledge for response generation. The empirical results show that this approach can generate more informative, specific, and commonsense-following responses, and produce meaningful and relevant knowledge evaluated by human annotators."
}