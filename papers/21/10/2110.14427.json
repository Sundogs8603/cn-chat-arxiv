{
    "title": "The ODE Method for Asymptotic Statistics in Stochastic Approximation and Reinforcement Learning. (arXiv:2110.14427v3 [math.ST] UPDATED)",
    "abstract": "The paper concerns the $d$-dimensional stochastic approximation recursion, $$ \\theta_{n+1}= \\theta_n + \\alpha_{n + 1} f(\\theta_n, \\Phi_{n+1}) $$ in which $\\Phi$ is a geometrically ergodic Markov chain on a general state space $\\textsf{X}$ with stationary distribution $\\pi$, and $f:\\Re^d\\times\\textsf{X}\\to\\Re^d$.  The main results are established under a version of the Donsker-Varadhan Lyapunov drift condition known as (DV3), and a stability condition for the mean flow with vector field $\\bar{f}(\\theta)=\\textsf{E}[f(\\theta,\\Phi)]$, with $\\Phi\\sim\\pi$.  (i) $\\{ \\theta_n\\}$ is convergent a.s. and in $L_4$ to the unique root $\\theta^*$ of $\\bar{f}(\\theta)$.  (ii) A functional CLT is established, as well as the usual one-dimensional CLT for the normalized error.  (iii) The CLT holds for the normalized version, $z_n{=:} \\sqrt{n} (\\theta^{\\text{PR}}_n -\\theta^*)$, of the averaged parameters, $\\theta^{\\text{PR}}_n {=:} n^{-1} \\sum_{k=1}^n\\theta_k$, subject to standard assumptions on the step-s",
    "link": "http://arxiv.org/abs/2110.14427",
    "context": "Title: The ODE Method for Asymptotic Statistics in Stochastic Approximation and Reinforcement Learning. (arXiv:2110.14427v3 [math.ST] UPDATED)\nAbstract: The paper concerns the $d$-dimensional stochastic approximation recursion, $$ \\theta_{n+1}= \\theta_n + \\alpha_{n + 1} f(\\theta_n, \\Phi_{n+1}) $$ in which $\\Phi$ is a geometrically ergodic Markov chain on a general state space $\\textsf{X}$ with stationary distribution $\\pi$, and $f:\\Re^d\\times\\textsf{X}\\to\\Re^d$.  The main results are established under a version of the Donsker-Varadhan Lyapunov drift condition known as (DV3), and a stability condition for the mean flow with vector field $\\bar{f}(\\theta)=\\textsf{E}[f(\\theta,\\Phi)]$, with $\\Phi\\sim\\pi$.  (i) $\\{ \\theta_n\\}$ is convergent a.s. and in $L_4$ to the unique root $\\theta^*$ of $\\bar{f}(\\theta)$.  (ii) A functional CLT is established, as well as the usual one-dimensional CLT for the normalized error.  (iii) The CLT holds for the normalized version, $z_n{=:} \\sqrt{n} (\\theta^{\\text{PR}}_n -\\theta^*)$, of the averaged parameters, $\\theta^{\\text{PR}}_n {=:} n^{-1} \\sum_{k=1}^n\\theta_k$, subject to standard assumptions on the step-s",
    "path": "papers/21/10/2110.14427.json",
    "total_tokens": 1122,
    "translated_title": "随机逼近和强化学习中渐近统计的ODE方法",
    "translated_abstract": "本文研究了$d$维随机逼近递归$$\\theta_{n+1}=\\theta_n+\\alpha_{n+1}f(\\theta_n, \\Phi_{n+1})$$其中$\\Phi$是一个在一般状态空间$\\textsf{X}$上具有平稳分布$\\pi$的几何遍历马尔可夫链，$f：\\Re^d\\times\\textsf{X}\\to\\Re^d$。在称为（DV3）的Donsker-Varadhan Lyapunov漂移条件的一种版本和对具有向量场$\\bar{f}(\\theta)=\\textsf{E}[f(\\theta,\\Phi)]$以及$\\Phi\\sim\\pi$的均值流的稳定性条件下，建立了主要结果。(i) $\\{\\theta_n\\}$以概率1和$L_4$收敛于$\\bar{f}(\\theta)$的唯一根$\\theta^*$。(ii) 建立了泛函中心极限定理，以及归一化误差一维中心极限定理。(iii) 对于归一化版本$z_n{=:} \\sqrt{n} (\\theta^{\\text{PR}}_n -\\theta^*)$的平均参数$\\theta^{\\text{PR}}_n {=:} n^{-1} \\sum_{k=1}^n\\theta_k$ ，在步长的标准假设下，建立了中心极限定理。",
    "tldr": "本文提出了一种称为ODE方法的渐近统计方法解决$d$维随机逼近递归的问题，证明了其收敛性和中心极限定理，为强化学习等领域的应用提供了有力的理论支持。"
}