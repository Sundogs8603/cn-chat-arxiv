{
    "title": "Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v7 [cs.LG] UPDATED)",
    "abstract": "The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create \"feature-level\" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing \"",
    "link": "http://arxiv.org/abs/2110.03605",
    "context": "Title: Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v7 [cs.LG] UPDATED)\nAbstract: The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create \"feature-level\" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing \"",
    "path": "papers/21/10/2110.03605.json",
    "total_tokens": 909,
    "translated_title": "鲁棒的特征级对抗是可解释性工具",
    "translated_abstract": "计算机视觉中对抗攻击的文献通常关注像素级扰动，这些扰动往往很难解释。最近的研究通过操纵图像生成器的潜在表示来创建“特征级”对抗扰动，为我们提供了探索可感知、可解释的对抗攻击的机会。我们做出了三个贡献。首先，我们观察到特征级对抗攻击提供了用于研究模型表示的有用输入类别。第二，我们展示了这些对抗攻击的独特多功能性和高度鲁棒性。我们证明它们可以用于在ImageNet规模上产生有针对性、通用性、伪装性、物理可实现性和黑盒攻击。第三，我们展示了如何将这些对抗图像用作实际的可解释性工具，用于识别网络中的错误。我们利用这些对抗攻击对特征和类别之间的虚假关联进行预测，然后通过设计“...",
    "tldr": "鲁棒的特征级对抗攻击不仅提供了对模型表示的研究，还具有独特的多功能性和高度鲁棒性，可以用于各种规模的图像攻击，并且可以作为可解释性工具帮助识别网络中的错误。",
    "en_tdlr": "Robust feature-level adversaries not only provide insights into model representations, but also demonstrate unique versatility and high robustness, enabling various scale attacks and serving as interpretability tools to identify network errors."
}