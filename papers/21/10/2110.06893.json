{
    "title": "Newer is not always better: Rethinking transferability metrics, their peculiarities, stability and performance. (arXiv:2110.06893v3 [cs.LG] UPDATED)",
    "abstract": "Fine-tuning of large pre-trained image and language models on small customized datasets has become increasingly popular for improved prediction and efficient use of limited resources. Fine-tuning requires identification of best models to transfer-learn from and quantifying transferability prevents expensive re-training on all of the candidate models/tasks pairs. In this paper, we show that the statistical problems with covariance estimation drive the poor performance of H-score -- a common baseline for newer metrics -- and propose shrinkage-based estimator. This results in up to 80% absolute gain in H-score correlation performance, making it competitive with the state-of-the-art LogME measure. Our shrinkage-based H-score is $3\\times$-10$\\times$ faster to compute compared to LogME. Additionally, we look into a less common setting of target (as opposed to source) task selection. We demonstrate previously overlooked problems in such settings with different number of labels, class-imbalanc",
    "link": "http://arxiv.org/abs/2110.06893",
    "context": "Title: Newer is not always better: Rethinking transferability metrics, their peculiarities, stability and performance. (arXiv:2110.06893v3 [cs.LG] UPDATED)\nAbstract: Fine-tuning of large pre-trained image and language models on small customized datasets has become increasingly popular for improved prediction and efficient use of limited resources. Fine-tuning requires identification of best models to transfer-learn from and quantifying transferability prevents expensive re-training on all of the candidate models/tasks pairs. In this paper, we show that the statistical problems with covariance estimation drive the poor performance of H-score -- a common baseline for newer metrics -- and propose shrinkage-based estimator. This results in up to 80% absolute gain in H-score correlation performance, making it competitive with the state-of-the-art LogME measure. Our shrinkage-based H-score is $3\\times$-10$\\times$ faster to compute compared to LogME. Additionally, we look into a less common setting of target (as opposed to source) task selection. We demonstrate previously overlooked problems in such settings with different number of labels, class-imbalanc",
    "path": "papers/21/10/2110.06893.json",
    "total_tokens": 988,
    "translated_title": "新不一定总是更好：重新思考迁移度量，它们的特殊性、稳定性和性能。",
    "translated_abstract": "对于改善预测和高效利用有限资源，对大型预训练图像和语言模型在小型自定义数据集上进行微调已变得越来越流行。微调需要确定最佳模型进行迁移学习，并量化可转移性以避免在所有候选模型/任务对上进行昂贵的重新训练。在本文中，我们展示了协方差估计的统计问题导致了H分数 (H-score) 的性能不佳——该分数是新型度量标准的常见基准——并提出基于收缩估计的解决方案。这样可以使H分数的相关性性能获得高达80%的绝对增益，使其与最先进的LogME度量标准具有竞争力。我们的基于收缩估计的H分数比LogME更快3倍到10倍。此外，我们研究了目标（而不是源）任务选择的较少常见的设置。我们展示了在具有不同标签数量、类别不平衡的情况下，此类设置中以前被忽视的问题。",
    "tldr": "本文改进了常见的度量标准H-score，并提出了一个基于收缩估计的解决方案，使H分数的相关性性能获得高达80%的绝对增益，使其与最先进的LogME度量标准具有竞争力。同时，针对目标任务的选择，本文也发现了一个被忽视的问题。"
}