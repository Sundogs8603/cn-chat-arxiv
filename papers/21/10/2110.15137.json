{
    "title": "PAC-Bayesian Learning of Aggregated Binary Activated Neural Networks with Probabilities over Representations. (arXiv:2110.15137v3 [cs.LG] UPDATED)",
    "abstract": "Considering a probability distribution over parameters is known as an efficient strategy to learn a neural network with non-differentiable activation functions. We study the expectation of a probabilistic neural network as a predictor by itself, focusing on the aggregation of binary activated neural networks with normal distributions over real-valued weights. Our work leverages a recent analysis derived from the PAC-Bayesian framework that derives tight generalization bounds and learning procedures for the expected output value of such an aggregation, which is given by an analytical expression. While the combinatorial nature of the latter has been circumvented by approximations in previous works, we show that the exact computation remains tractable for deep but narrow neural networks, thanks to a dynamic programming approach. This leads us to a peculiar bound minimization learning algorithm for binary activated neural networks, where the forward pass propagates probabilities over repre",
    "link": "http://arxiv.org/abs/2110.15137",
    "context": "Title: PAC-Bayesian Learning of Aggregated Binary Activated Neural Networks with Probabilities over Representations. (arXiv:2110.15137v3 [cs.LG] UPDATED)\nAbstract: Considering a probability distribution over parameters is known as an efficient strategy to learn a neural network with non-differentiable activation functions. We study the expectation of a probabilistic neural network as a predictor by itself, focusing on the aggregation of binary activated neural networks with normal distributions over real-valued weights. Our work leverages a recent analysis derived from the PAC-Bayesian framework that derives tight generalization bounds and learning procedures for the expected output value of such an aggregation, which is given by an analytical expression. While the combinatorial nature of the latter has been circumvented by approximations in previous works, we show that the exact computation remains tractable for deep but narrow neural networks, thanks to a dynamic programming approach. This leads us to a peculiar bound minimization learning algorithm for binary activated neural networks, where the forward pass propagates probabilities over repre",
    "path": "papers/21/10/2110.15137.json",
    "total_tokens": 894,
    "translated_title": "以表示概率为基础的聚合二值激活神经网络的PAC-Bayesian学习",
    "translated_abstract": "考虑到对参数的概率分布是一种有效的策略来学习带有不可微激活函数的神经网络。我们研究了期望为自身的概率神经网络作为预测器，重点关注具有正态分布权重的二值激活神经网络的聚合。我们利用了最近从PAC-Bayesian框架中派生出的分析，为这种聚合的预期输出值提供了紧凑的泛化界限和学习过程，这由解析表达式给出。虽然后者的组合性质在先前的工作中被近似替代，但我们表明对于深而窄的神经网络，确切的计算仍然是可行的，这得益于一种动态编程方法。这导致我们提出了一种奇特的用于二元激活神经网络的界限最小化学习算法，其中前向传递传播表示概率的表示。",
    "tldr": "该研究使用PAC-Bayesian框架为具有正态分布权重的二值激活神经网络的聚合提供了紧凑的泛化界限和学习过程，这导致了一种奇特的界限最小化学习算法。",
    "en_tdlr": "This study provides tight generalization bounds and learning procedures within the PAC-Bayesian framework for the aggregation of binary activated neural networks with normal distributions over real-valued weights, leading to a peculiar bound minimization learning algorithm."
}