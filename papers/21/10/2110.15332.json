{
    "title": "Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes. (arXiv:2110.15332v2 [cs.LG] UPDATED)",
    "abstract": "In applications of offline reinforcement learning to observational data, such as in healthcare or education, a general concern is that observed actions might be affected by unobserved factors, inducing confounding and biasing estimates derived under the assumption of a perfect Markov decision process (MDP) model. Here we tackle this by considering off-policy evaluation in a partially observed MDP (POMDP). Specifically, we consider estimating the value of a given target policy in a POMDP given trajectories with only partial state observations generated by a different and unknown policy that may depend on the unobserved state. We tackle two questions: what conditions allow us to identify the target policy value from the observed data and, given identification, how to best estimate it. To answer these, we extend the framework of proximal causal inference to our POMDP setting, providing a variety of settings where identification is made possible by the existence of so-called bridge functio",
    "link": "http://arxiv.org/abs/2110.15332",
    "context": "Title: Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes. (arXiv:2110.15332v2 [cs.LG] UPDATED)\nAbstract: In applications of offline reinforcement learning to observational data, such as in healthcare or education, a general concern is that observed actions might be affected by unobserved factors, inducing confounding and biasing estimates derived under the assumption of a perfect Markov decision process (MDP) model. Here we tackle this by considering off-policy evaluation in a partially observed MDP (POMDP). Specifically, we consider estimating the value of a given target policy in a POMDP given trajectories with only partial state observations generated by a different and unknown policy that may depend on the unobserved state. We tackle two questions: what conditions allow us to identify the target policy value from the observed data and, given identification, how to best estimate it. To answer these, we extend the framework of proximal causal inference to our POMDP setting, providing a variety of settings where identification is made possible by the existence of so-called bridge functio",
    "path": "papers/21/10/2110.15332.json",
    "total_tokens": 1104,
    "translated_title": "近端强化学习：部分观测马尔可夫决策过程中高效的离线策略评估",
    "translated_abstract": "在从医疗保健或教育领域收集的观察数据应用离线强化学习时，一个普遍的关注点是，观察到的行动可能受到未观察到的因素的影响，引起混淆并导致在假设完美马尔可夫决策过程模型的情况下得出的估计值出现偏差。本文考虑在部分观测马尔可夫决策过程（POMDP）中进行离线策略评估。具体来说，我们考虑在给定只由不同且未知的策略生成的具有部分状态观测的轨迹的情况下估计POMDP中给定目标策略的值，该策略可能依赖于未观察到的状态。我们解决了两个问题：什么条件允许我们从观察到的数据中识别目标策略值，并且在识别的情况下，如何最好地估计它。为了回答这些问题，我们将近端因果推断框架扩展到我们的POMDP设置中，提供了许多场景，其中通过所谓的桥接函数的存在实现了识别。",
    "tldr": "本文提出了一种方法，可以在离线强化学习中，应用于从医疗保健或教育领域收集的观测数据。我们考虑在部分观测马尔可夫决策过程（POMDP）中进行离线策略评估，以解决观察到的行动可能受到未观察到的因素影响，导致估计值出现偏差的问题。"
}