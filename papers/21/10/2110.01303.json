{
    "title": "Incremental Class Learning using Variational Autoencoders with Similarity Learning. (arXiv:2110.01303v3 [cs.LG] UPDATED)",
    "abstract": "Catastrophic forgetting in neural networks during incremental learning remains a challenging problem. Previous research investigated catastrophic forgetting in fully connected networks, with some earlier work exploring activation functions and learning algorithms. Applications of neural networks have been extended to include similarity learning. Understanding how similarity learning loss functions would be affected by catastrophic forgetting is of significant interest. Our research investigates catastrophic forgetting for four well-known similarity-based loss functions during incremental class learning. The loss functions are Angular, Contrastive, Center, and Triplet loss. Our results show that the catastrophic forgetting rate differs across loss functions on multiple datasets. The Angular loss was least affected, followed by Contrastive, Triplet loss, and Center loss with good mining techniques. We implemented three existing incremental learning techniques, iCaRL, EWC, and EBLL. We fu",
    "link": "http://arxiv.org/abs/2110.01303",
    "context": "Title: Incremental Class Learning using Variational Autoencoders with Similarity Learning. (arXiv:2110.01303v3 [cs.LG] UPDATED)\nAbstract: Catastrophic forgetting in neural networks during incremental learning remains a challenging problem. Previous research investigated catastrophic forgetting in fully connected networks, with some earlier work exploring activation functions and learning algorithms. Applications of neural networks have been extended to include similarity learning. Understanding how similarity learning loss functions would be affected by catastrophic forgetting is of significant interest. Our research investigates catastrophic forgetting for four well-known similarity-based loss functions during incremental class learning. The loss functions are Angular, Contrastive, Center, and Triplet loss. Our results show that the catastrophic forgetting rate differs across loss functions on multiple datasets. The Angular loss was least affected, followed by Contrastive, Triplet loss, and Center loss with good mining techniques. We implemented three existing incremental learning techniques, iCaRL, EWC, and EBLL. We fu",
    "path": "papers/21/10/2110.01303.json",
    "total_tokens": 912,
    "translated_title": "使用相似度学习的变分自动编码器进行增量式分类学习",
    "translated_abstract": "神经网络在进行增量式学习时容易出现“灾难性遗忘”，这是一个具有挑战性的问题。之前的研究主要关注全连接网络中的灾难性遗忘，并探索了激活函数和学习算法等因素对灾难性遗忘的影响。近年来，相似性学习逐渐被应用到神经网络中，因此了解相似性学习损失函数在灾难性遗忘中的表现具有重要意义。本文在四种著名的基于相似性的损失函数（角度损失、对比损失、中心损失和三元损失）中进行了灾难性遗忘率的研究。研究结果显示不同的损失函数以及数据集对灾难性遗忘率有显著影响。其中，角度损失受影响最小，其次是对比损失、三元损失和中心损失（利用挖掘技术性能较好）。本文实现了三种现有的增量学习技术：iCaRL、EWC和EBLL。",
    "tldr": "本文研究了增量式学习中四种基于相似性的损失函数在灾难性遗忘方面的表现，实验结果显示不同的损失函数对灾难性遗忘率有不同的影响。",
    "en_tdlr": "This paper investigates the performance of four similarity-based loss functions in catastrophic forgetting during incremental class learning. The experimental results show that different loss functions have different effects on the catastrophic forgetting rate."
}