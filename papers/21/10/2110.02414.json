{
    "title": "Imaginary Hindsight Experience Replay: Curious Model-based Learning for Sparse Reward Tasks. (arXiv:2110.02414v2 [cs.LG] UPDATED)",
    "abstract": "Model-based reinforcement learning is a promising learning strategy for practical robotic applications due to its improved data-efficiency versus model-free counterparts. However, current state-of-the-art model-based methods rely on shaped reward signals, which can be difficult to design and implement. To remedy this, we propose a simple model-based method tailored for sparse-reward multi-goal tasks that foregoes the need for complicated reward engineering. This approach, termed Imaginary Hindsight Experience Replay, minimises real-world interactions by incorporating imaginary data into policy updates. To improve exploration in the sparse-reward setting, the policy is trained with standard Hindsight Experience Replay and endowed with curiosity-based intrinsic rewards. Upon evaluation, this approach provides an order of magnitude increase in data-efficiency on average versus the state-of-the-art model-free method in the benchmark OpenAI Gym Fetch Robotics tasks.",
    "link": "http://arxiv.org/abs/2110.02414",
    "context": "Title: Imaginary Hindsight Experience Replay: Curious Model-based Learning for Sparse Reward Tasks. (arXiv:2110.02414v2 [cs.LG] UPDATED)\nAbstract: Model-based reinforcement learning is a promising learning strategy for practical robotic applications due to its improved data-efficiency versus model-free counterparts. However, current state-of-the-art model-based methods rely on shaped reward signals, which can be difficult to design and implement. To remedy this, we propose a simple model-based method tailored for sparse-reward multi-goal tasks that foregoes the need for complicated reward engineering. This approach, termed Imaginary Hindsight Experience Replay, minimises real-world interactions by incorporating imaginary data into policy updates. To improve exploration in the sparse-reward setting, the policy is trained with standard Hindsight Experience Replay and endowed with curiosity-based intrinsic rewards. Upon evaluation, this approach provides an order of magnitude increase in data-efficiency on average versus the state-of-the-art model-free method in the benchmark OpenAI Gym Fetch Robotics tasks.",
    "path": "papers/21/10/2110.02414.json",
    "total_tokens": 977,
    "translated_title": "虚拟回顾经验重放：用于稀疏奖励任务的好奇模型基学习",
    "translated_abstract": "基于模型的强化学习是一种有望应用于实际机器人任务的学习策略，因为它相比于基于模型无关的方法能够提高数据效率。然而，当前的基于模型的方法都依赖于设计和实现困难的形状奖励信号。为了解决这个问题，我们提出了一种简单的基于模型的方法，专门针对稀疏奖励的多目标任务，避免了复杂的奖励工程的需求。这种方法被称为虚拟回顾经验重放，通过将虚拟数据纳入到策略更新中，以减少真实世界的交互。为了改善稀疏奖励环境下的探索，该策略采用标准的回顾经验重放训练，并配备了基于好奇心的内在奖励。评估结果显示，在OpenAI Gym Fetch Robotics任务的基准测试中，这种方法的数据效率平均提高了一个数量级，相比于当前最先进的基于模型无关方法。",
    "tldr": "这篇论文提出了一种虚拟回顾经验重放的简单基于模型的方法，适用于稀疏奖励的多目标任务，无需复杂的奖励工程。通过引入虚拟数据和好奇心驱动的内在奖励，此方法在OpenAI Gym Fetch Robotics任务上平均提高了一个数量级的数据效率。",
    "en_tdlr": "This paper presents a simple model-based method called Imaginary Hindsight Experience Replay for sparse-reward multi-goal tasks, without the need for complex reward engineering. By incorporating imaginary data and curiosity-driven intrinsic rewards, this approach achieves an order of magnitude increase in data-efficiency on average in the benchmark OpenAI Gym Fetch Robotics tasks."
}