{
    "title": "Variance Reduction Based Experience Replay for Policy Optimization",
    "abstract": "arXiv:2110.08902v3 Announce Type: replace-cross  Abstract: For reinforcement learning on complex stochastic systems, it is desirable to effectively leverage the information from historical samples collected in previous iterations to accelerate policy optimization. Classical experience replay, while effective, treats all observations uniformly, neglecting their relative importance. To address this limitation, we introduce a novel Variance Reduction Experience Replay (VRER) framework, enabling the selective reuse of relevant samples to improve policy gradient estimation. VRER, as an adaptable method that can seamlessly integrate with different policy optimization algorithms, forms the foundation of our sample-efficient off-policy algorithm known as Policy Optimization with VRER (PG-VRER). Furthermore, the lack of a rigorous theoretical understanding of the experience replay method in the literature motivates us to introduce a novel theoretical framework that accounts for sample dependenc",
    "link": "https://arxiv.org/abs/2110.08902",
    "context": "Title: Variance Reduction Based Experience Replay for Policy Optimization\nAbstract: arXiv:2110.08902v3 Announce Type: replace-cross  Abstract: For reinforcement learning on complex stochastic systems, it is desirable to effectively leverage the information from historical samples collected in previous iterations to accelerate policy optimization. Classical experience replay, while effective, treats all observations uniformly, neglecting their relative importance. To address this limitation, we introduce a novel Variance Reduction Experience Replay (VRER) framework, enabling the selective reuse of relevant samples to improve policy gradient estimation. VRER, as an adaptable method that can seamlessly integrate with different policy optimization algorithms, forms the foundation of our sample-efficient off-policy algorithm known as Policy Optimization with VRER (PG-VRER). Furthermore, the lack of a rigorous theoretical understanding of the experience replay method in the literature motivates us to introduce a novel theoretical framework that accounts for sample dependenc",
    "path": "papers/21/10/2110.08902.json",
    "total_tokens": 811,
    "translated_title": "基于方差减少的经验回放用于策略优化",
    "translated_abstract": "在复杂随机系统上进行强化学习时，有效利用历史样本中的信息以加速策略优化是很有必要的。传统的经验回放虽然有效，但是将所有观测都视为相同，忽略了它们的相对重要性。为了解决这一限制，我们引入了一种新颖的方差减少经验回放（VRER）框架，实现对相关样本的选择性重复利用，从而改善策略梯度估计。VRER作为一种适应性方法，可以无缝集成到不同的策略优化算法中，构建了我们高效的离策略算法Policy Optimization with VRER (PG-VRER)。此外，文献中对经验回放方法缺乏严格的理论理解，这促使我们引入一个新颖的理论框架，考虑样本依赖性。",
    "tldr": "引入了基于方差减少的经验回放框架，实现了选择性重复利用相关样本来改善策略梯度估计，并构建了高效的离策略算法PG-VRER。"
}