{
    "title": "Binarized ResNet: Enabling Robust Automatic Modulation Classification at the resource-constrained Edge. (arXiv:2110.14357v2 [cs.IT] UPDATED)",
    "abstract": "Recently, deep neural networks (DNNs) have been used extensively for automatic modulation classification (AMC), and the results have been quite promising. However, DNNs have high memory and computation requirements making them impractical for edge networks where the devices are resource-constrained. They are also vulnerable to adversarial attacks, which is a significant security concern. This work proposes a rotated binary large ResNet (RBLResNet) for AMC that can be deployed at the edge network because of low memory and computational complexity. The performance gap between the RBLResNet and existing architectures with floating-point weights and activations can be closed by two proposed ensemble methods: (i) multilevel classification (MC), and (ii) bagging multiple RBLResNets while retaining low memory and computational power. The MC method achieves an accuracy of $93.39\\%$ at $10$dB over all the $24$ modulation classes of the Deepsig dataset. This performance is comparable to state-of",
    "link": "http://arxiv.org/abs/2110.14357",
    "context": "Title: Binarized ResNet: Enabling Robust Automatic Modulation Classification at the resource-constrained Edge. (arXiv:2110.14357v2 [cs.IT] UPDATED)\nAbstract: Recently, deep neural networks (DNNs) have been used extensively for automatic modulation classification (AMC), and the results have been quite promising. However, DNNs have high memory and computation requirements making them impractical for edge networks where the devices are resource-constrained. They are also vulnerable to adversarial attacks, which is a significant security concern. This work proposes a rotated binary large ResNet (RBLResNet) for AMC that can be deployed at the edge network because of low memory and computational complexity. The performance gap between the RBLResNet and existing architectures with floating-point weights and activations can be closed by two proposed ensemble methods: (i) multilevel classification (MC), and (ii) bagging multiple RBLResNets while retaining low memory and computational power. The MC method achieves an accuracy of $93.39\\%$ at $10$dB over all the $24$ modulation classes of the Deepsig dataset. This performance is comparable to state-of",
    "path": "papers/21/10/2110.14357.json",
    "total_tokens": 1150,
    "translated_title": "二值化 ResNet：在资源受限的边缘实现鲁棒的自动调制识别",
    "translated_abstract": "近年来，深度神经网络被广泛应用于自动调制识别（AMC），并且结果非常有前途。然而，DNN具有高存储和计算需求，使它们对于资源受限的边缘网络不实用。它们也容易受到对手的攻击，这是一个重要的安全问题。本研究提出了一个旋转二值化大型ResNet（RBLResNet）用于AMC，可以在边缘网络上部署，因为其具有低内存和计算复杂度。通过两种提出的集成方法：（i）多级分类（MC），和（ii）保留低内存和计算功率的多个RBLResNet打包，可以缩小RBLResNet和现有使用浮点权重和激活函数的结构之间的性能差距。MC方法在Deepsig数据集的24种调制类别的全部测试上实现了93.39％的准确率。该性能与最先进的方法相当，同时仅使用二元权重和激活。所提出的集成方法进一步将性能提高到10 dB时94.49%的准确率，优于其他二元和实值网络。",
    "tldr": "本研究提出了一个旋转二值化大型ResNet（RBLResNet），可用于资源受限的边缘自动调制识别。通过多级分类和保留低内存和计算功率的RBLResNet打包来提高准确率，达到至高94.49%的性能，优于其他二元和实值网络。",
    "en_tdlr": "This paper proposes a rotated binary large ResNet (RBLResNet) for automatic modulation classification (AMC) that can be deployed at the resource-constrained edge. The proposed ensemble methods, multilevel classification and bagging multiple RBLResNets, close the performance gap between the RBLResNet and existing architectures with floating-point weights and activations. The RBLResNet achieves 93.39% accuracy at 10dB over all the 24 modulation classes of the Deepsig dataset, which is comparable to state-of-the-art methods while only using binary weights and activations. The proposed ensemble methods further improve the performance to 94.49% accuracy at 10dB, outperforming other binary and real-valued networks."
}