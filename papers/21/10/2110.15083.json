{
    "title": "Nearest neighbor process: weak convergence and non-asymptotic bound. (arXiv:2110.15083v2 [math.ST] UPDATED)",
    "abstract": "The empirical measure resulting from the nearest neighbors to a given point \\textit{the nearest neighbor measure} - is introduced and studied as a central statistical quantity. First, the associated empirical process is shown to satisfy a uniform central limit theorem under a (local) bracketing entropy condition on the underlying class of functions (reflecting the localizing nature of the nearest neighbor algorithm). Second a uniform non-asymptotic bound is established under a well-known condition, often referred to as Vapnik-Chervonenkis, on the uniform entropy numbers. The covariance of the Gaussian limit obtained in the uniform central limit theorem is equal to the conditional covariance operator (given the point of interest). This suggests the possibility of extending standard approaches - non local - replacing simply the standard empirical measure by the nearest neighbor measure while using the same way of making inference but with the nearest neighbors only instead of the full ",
    "link": "http://arxiv.org/abs/2110.15083",
    "context": "Title: Nearest neighbor process: weak convergence and non-asymptotic bound. (arXiv:2110.15083v2 [math.ST] UPDATED)\nAbstract: The empirical measure resulting from the nearest neighbors to a given point \\textit{the nearest neighbor measure} - is introduced and studied as a central statistical quantity. First, the associated empirical process is shown to satisfy a uniform central limit theorem under a (local) bracketing entropy condition on the underlying class of functions (reflecting the localizing nature of the nearest neighbor algorithm). Second a uniform non-asymptotic bound is established under a well-known condition, often referred to as Vapnik-Chervonenkis, on the uniform entropy numbers. The covariance of the Gaussian limit obtained in the uniform central limit theorem is equal to the conditional covariance operator (given the point of interest). This suggests the possibility of extending standard approaches - non local - replacing simply the standard empirical measure by the nearest neighbor measure while using the same way of making inference but with the nearest neighbors only instead of the full ",
    "path": "papers/21/10/2110.15083.json",
    "total_tokens": 872,
    "translated_title": "最近邻过程：弱收敛和非渐近界限",
    "translated_abstract": "介绍并研究了由给定点的最近邻所得到的经验测度——最近邻测度作为一种中心统计量。首先，在底层函数类上满足（反映最近邻算法的本地化特性的）（本地）支撑熵条件下，将相关经验过程证明为满足均匀中心极限定理。其次，在统一熵数的著名条件（通常称为Vapnik-Chervonenkis）下建立了一种均匀的非渐近界限。在均匀中心极限定理中所获得的高斯极限的协方差等于条件协方差算子（给出兴趣点）。这提示了一种可能性，即在使用相同的推理方式但仅使用最近邻而不是全部替换标准经验测度的标准方法的情况下，扩展标准方法 - 非局部。",
    "tldr": "本文介绍了一种中心统计量——最近邻测度，并通过均匀中心极限定理和一种均匀的非渐近界限研究了它。该测度可能为推断提供了一种替代方法。",
    "en_tdlr": "This paper introduces a central statistical quantity - the nearest neighbor measure, and studies it through uniform central limit theorem and a uniform non-asymptotic bound. The measure may provide an alternative method for inference by replacing the standard empirical measure with the nearest neighbor measure."
}