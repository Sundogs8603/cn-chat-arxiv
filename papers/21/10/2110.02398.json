{
    "title": "Approximate Newton policy gradient algorithms. (arXiv:2110.02398v5 [cs.LG] UPDATED)",
    "abstract": "Policy gradient algorithms have been widely applied to Markov decision processes and reinforcement learning problems in recent years. Regularization with various entropy functions is often used to encourage exploration and improve stability. This paper proposes an approximate Newton method for the policy gradient algorithm with entropy regularization. In the case of Shannon entropy, the resulting algorithm reproduces the natural policy gradient algorithm. For other entropy functions, this method results in brand-new policy gradient algorithms. We prove that all these algorithms enjoy Newton-type quadratic convergence and that the corresponding gradient flow converges globally to the optimal solution. We use synthetic and industrial-scale examples to demonstrate that the proposed approximate Newton method typically converges in single-digit iterations, often orders of magnitude faster than other state-of-the-art algorithms.",
    "link": "http://arxiv.org/abs/2110.02398",
    "context": "Title: Approximate Newton policy gradient algorithms. (arXiv:2110.02398v5 [cs.LG] UPDATED)\nAbstract: Policy gradient algorithms have been widely applied to Markov decision processes and reinforcement learning problems in recent years. Regularization with various entropy functions is often used to encourage exploration and improve stability. This paper proposes an approximate Newton method for the policy gradient algorithm with entropy regularization. In the case of Shannon entropy, the resulting algorithm reproduces the natural policy gradient algorithm. For other entropy functions, this method results in brand-new policy gradient algorithms. We prove that all these algorithms enjoy Newton-type quadratic convergence and that the corresponding gradient flow converges globally to the optimal solution. We use synthetic and industrial-scale examples to demonstrate that the proposed approximate Newton method typically converges in single-digit iterations, often orders of magnitude faster than other state-of-the-art algorithms.",
    "path": "papers/21/10/2110.02398.json",
    "total_tokens": 824,
    "translated_title": "近似牛顿策略梯度算法",
    "translated_abstract": "最近几年，政策梯度算法被广泛应用于马尔可夫决策过程和强化学习问题中。常常使用各种熵函数进行正则化，以鼓励探索和提高稳定性。本文提出了一种用于熵正则化策略梯度算法的近似牛顿方法。在Shannon熵的情况下，所得到的算法复制了自然策略梯度算法。对于其他熵函数，这种方法得到了全新的策略梯度算法。我们证明了所有这些算法具有牛顿类型的二次收敛性，并且相应的梯度流全局收敛于最优解。我们使用合成和工业规模的示例来证明，所提出的近似牛顿方法通常在单位数迭代中收敛，并且往往比其他最先进的算法快几个数量级。",
    "tldr": "本文提出了一种用于政策梯度算法的近似牛顿方法，包括自然策略梯度算法和全新的策略梯度算法，具有快速收敛的优势。",
    "en_tdlr": "This paper proposes an approximate Newton method for policy gradient algorithm, including natural policy gradient algorithm and brand-new policy gradient algorithm with other entropy functions, which enjoys fast convergence and outperforms other state-of-the-art algorithms in both synthetic and industrial-scale examples."
}