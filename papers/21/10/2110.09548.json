{
    "title": "Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks. (arXiv:2110.09548v4 [cs.LG] UPDATED)",
    "abstract": "Understanding the fundamental principles behind the success of deep neural networks is one of the most important open questions in the current literature. To this end, we study the training problem of deep neural networks and introduce an analytic approach to unveil hidden convexity in the optimization landscape. We consider a deep parallel ReLU network architecture, which also includes standard deep networks and ResNets as its special cases. We then show that pathwise regularized training problems can be represented as an exact convex optimization problem. We further prove that the equivalent convex problem is regularized via a group sparsity inducing norm. Thus, a path regularized parallel ReLU network can be viewed as a parsimonious convex model in high dimensions. More importantly, since the original training problem may not be trainable in polynomial-time, we propose an approximate algorithm with a fully polynomial-time complexity in all data dimensions. Then, we prove strong glob",
    "link": "http://arxiv.org/abs/2110.09548",
    "context": "Title: Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks. (arXiv:2110.09548v4 [cs.LG] UPDATED)\nAbstract: Understanding the fundamental principles behind the success of deep neural networks is one of the most important open questions in the current literature. To this end, we study the training problem of deep neural networks and introduce an analytic approach to unveil hidden convexity in the optimization landscape. We consider a deep parallel ReLU network architecture, which also includes standard deep networks and ResNets as its special cases. We then show that pathwise regularized training problems can be represented as an exact convex optimization problem. We further prove that the equivalent convex problem is regularized via a group sparsity inducing norm. Thus, a path regularized parallel ReLU network can be viewed as a parsimonious convex model in high dimensions. More importantly, since the original training problem may not be trainable in polynomial-time, we propose an approximate algorithm with a fully polynomial-time complexity in all data dimensions. Then, we prove strong glob",
    "path": "papers/21/10/2110.09548.json",
    "total_tokens": 894,
    "translated_title": "路径正则化：一种对并行ReLU网络进行凸性和稀疏性引导的正则化方法",
    "translated_abstract": "理解深度神经网络成功背后的基本原理是当前文献中最重要的开放问题之一。为此，我们研究了深度神经网络的训练问题，并引入了一种分析方法来揭示优化景观中隐藏的凸性。我们考虑了深度并行ReLU网络架构，其也包括标准的深度网络和ResNet作为其特例。然后我们表明，基于路径正则化的训练问题可以表示为一个精确的凸优化问题。我们进一步证明等价的凸问题是通过一种群稀疏性引导的规范进行正则化的。因此，路径正则化的并行ReLU网络可以被视为高维中一种简化的凸模型。更重要的是，由于原始的训练问题可能无法在多项式时间内训练，我们提出了一个在所有数据维度上具有完全多项式时间复杂度的近似算法。然后，我们证明了强全局收敛性。",
    "tldr": "路径正则化为并行ReLU网络提供了一种简化的凸优化问题，通过群稀疏性引导实现了凸模型，并提出了一个近似算法，在所有数据维度上具备完全多项式时间复杂度。",
    "en_tdlr": "Path regularization provides a simplified convex optimization problem for parallel ReLU networks, achieves convex models through group sparsity inducing, and proposes an approximate algorithm with fully polynomial-time complexity in all data dimensions."
}