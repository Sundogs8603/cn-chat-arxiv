{
    "title": "Sparse MoEs meet Efficient Ensembles. (arXiv:2110.03360v2 [cs.LG] UPDATED)",
    "abstract": "Machine learning models based on the aggregated outputs of submodels, either at the activation or prediction levels, often exhibit strong performance compared to individual models. We study the interplay of two popular classes of such models: ensembles of neural networks and sparse mixture of experts (sparse MoEs). First, we show that the two approaches have complementary features whose combination is beneficial. This includes a comprehensive evaluation of sparse MoEs in uncertainty related benchmarks. Then, we present Efficient Ensemble of Experts (E$^3$), a scalable and simple ensemble of sparse MoEs that takes the best of both classes of models, while using up to 45% fewer FLOPs than a deep ensemble. Extensive experiments demonstrate the accuracy, log-likelihood, few-shot learning, robustness, and uncertainty improvements of E$^3$ over several challenging vision Transformer-based baselines. E$^3$ not only preserves its efficiency while scaling to models with up to 2.7B parameters, b",
    "link": "http://arxiv.org/abs/2110.03360",
    "context": "Title: Sparse MoEs meet Efficient Ensembles. (arXiv:2110.03360v2 [cs.LG] UPDATED)\nAbstract: Machine learning models based on the aggregated outputs of submodels, either at the activation or prediction levels, often exhibit strong performance compared to individual models. We study the interplay of two popular classes of such models: ensembles of neural networks and sparse mixture of experts (sparse MoEs). First, we show that the two approaches have complementary features whose combination is beneficial. This includes a comprehensive evaluation of sparse MoEs in uncertainty related benchmarks. Then, we present Efficient Ensemble of Experts (E$^3$), a scalable and simple ensemble of sparse MoEs that takes the best of both classes of models, while using up to 45% fewer FLOPs than a deep ensemble. Extensive experiments demonstrate the accuracy, log-likelihood, few-shot learning, robustness, and uncertainty improvements of E$^3$ over several challenging vision Transformer-based baselines. E$^3$ not only preserves its efficiency while scaling to models with up to 2.7B parameters, b",
    "path": "papers/21/10/2110.03360.json",
    "total_tokens": 974,
    "translated_title": "稀疏MoEs满足高效的模型集成",
    "translated_abstract": "基于子模型的聚合输出的机器学习模型，无论是在激活还是预测水平上，往往相对于单个模型显示出很强的性能。我们研究了两种流行模型的相互作用：神经网络集成和稀疏专家混合（稀疏MoEs）。首先，我们展示了这两种方法具有互补的特点，它们的结合是有益的。这包括对稀疏MoEs在不确定性相关基准测试中的全面评估。然后，我们提出了高效的专家集成（E$^3$），一种可扩展且简单的稀疏MoEs集成方法，它兼具两类模型的优点，同时使用的浮点运算（FLOPs）比深度集成少多达45％。大量实验证明了E$^3$在多个具有挑战性的基于Transformer的视觉基线模型上的准确性、对数似然、少样本学习、鲁棒性和不确定性改进。E$^3$在扩展到具有高达27亿参数的模型时不仅保持其效率，而且还能提供更好的性能。",
    "tldr": "本论文研究了神经网络集成和稀疏专家混合的结合，提出了一种名为E$^3$的高效稀疏MoEs集成方法，在减少计算复杂度的同时取得了在准确性、鲁棒性和不确定性方面的改进。"
}