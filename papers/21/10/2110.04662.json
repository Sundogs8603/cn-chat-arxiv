{
    "title": "Cognitively Inspired Learning of Incremental Drifting Concepts. (arXiv:2110.04662v2 [cs.LG] UPDATED)",
    "abstract": "Humans continually expand their learned knowledge to new domains and learn new concepts without any interference with past learned experiences. In contrast, machine learning models perform poorly in a continual learning setting, where input data distribution changes over time. Inspired by the nervous system learning mechanisms, we develop a computational model that enables a deep neural network to learn new concepts and expand its learned knowledge to new domains incrementally in a continual learning setting. We rely on the Parallel Distributed Processing theory to encode abstract concepts in an embedding space in terms of a multimodal distribution. This embedding space is modeled by internal data representations in a hidden network layer. We also leverage the Complementary Learning Systems theory to equip the model with a memory mechanism to overcome catastrophic forgetting through implementing pseudo-rehearsal. Our model can generate pseudo-data points for experience replay and accum",
    "link": "http://arxiv.org/abs/2110.04662",
    "context": "Title: Cognitively Inspired Learning of Incremental Drifting Concepts. (arXiv:2110.04662v2 [cs.LG] UPDATED)\nAbstract: Humans continually expand their learned knowledge to new domains and learn new concepts without any interference with past learned experiences. In contrast, machine learning models perform poorly in a continual learning setting, where input data distribution changes over time. Inspired by the nervous system learning mechanisms, we develop a computational model that enables a deep neural network to learn new concepts and expand its learned knowledge to new domains incrementally in a continual learning setting. We rely on the Parallel Distributed Processing theory to encode abstract concepts in an embedding space in terms of a multimodal distribution. This embedding space is modeled by internal data representations in a hidden network layer. We also leverage the Complementary Learning Systems theory to equip the model with a memory mechanism to overcome catastrophic forgetting through implementing pseudo-rehearsal. Our model can generate pseudo-data points for experience replay and accum",
    "path": "papers/21/10/2110.04662.json",
    "total_tokens": 949,
    "translated_title": "基于认知的增量漂移概念学习",
    "translated_abstract": "人类不断将自己学到的知识拓展到新的领域，并且在学习新的概念时不会对以前学习的经验有任何干扰。相反，机器学习模型在连续的学习环境中表现不佳，因为输入数据的分布会随着时间的推移而变化。受神经系统学习机制的启发，我们开发了一种计算模型，使深度神经网络能够在连续学习环境中学习新概念，并将其学到的知识拓展到新领域。我们依靠并行分布处理理论，在一个多模态分布空间中，用内部数据表示在隐藏网络层中建模抽象概念。同时，我们还利用补充学习系统理论，通过实现伪排练来为模型配备记忆机制，以克服灾难性遗忘。我们的模型可以生成伪数据点进行经验回放和知识积累，这些点将被用作新数据的训练输入。",
    "tldr": "本研究提出了一种基于神经系统学习机制的计算模型，使深度神经网络能够在连续学习环境中学习新概念，并将其学到的知识拓展到新领域。此模型结合多模态分布空间和伪排练记忆机制，可用于克服灾难性遗忘。"
}