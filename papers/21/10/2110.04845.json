{
    "title": "What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study. (arXiv:2110.04845v4 [cs.CL] UPDATED)",
    "abstract": "The degree of semantic relatedness of two units of language has long been considered fundamental to understanding meaning. Additionally, automatically determining relatedness has many applications such as question answering and summarization. However, prior NLP work has largely focused on semantic similarity, a subset of relatedness, because of a lack of relatedness datasets. In this paper, we introduce a dataset for Semantic Textual Relatedness, STR-2022, that has 5,500 English sentence pairs manually annotated using a comparative annotation framework, resulting in fine-grained scores. We show that human intuition regarding relatedness of sentence pairs is highly reliable, with a repeat annotation correlation of 0.84. We use the dataset to explore questions on what makes sentences semantically related. We also show the utility of STR-2022 for evaluating automatic methods of sentence representation and for various downstream NLP tasks.  Our dataset, data statement, and annotation quest",
    "link": "http://arxiv.org/abs/2110.04845",
    "context": "Title: What Makes Sentences Semantically Related: A Textual Relatedness Dataset and Empirical Study. (arXiv:2110.04845v4 [cs.CL] UPDATED)\nAbstract: The degree of semantic relatedness of two units of language has long been considered fundamental to understanding meaning. Additionally, automatically determining relatedness has many applications such as question answering and summarization. However, prior NLP work has largely focused on semantic similarity, a subset of relatedness, because of a lack of relatedness datasets. In this paper, we introduce a dataset for Semantic Textual Relatedness, STR-2022, that has 5,500 English sentence pairs manually annotated using a comparative annotation framework, resulting in fine-grained scores. We show that human intuition regarding relatedness of sentence pairs is highly reliable, with a repeat annotation correlation of 0.84. We use the dataset to explore questions on what makes sentences semantically related. We also show the utility of STR-2022 for evaluating automatic methods of sentence representation and for various downstream NLP tasks.  Our dataset, data statement, and annotation quest",
    "path": "papers/21/10/2110.04845.json",
    "total_tokens": 896,
    "translated_title": "句子的语义相关性取决于哪些因素：一个文本相关性数据集和实证研究",
    "translated_abstract": "对于理解语义，语言单元之间的语义相关性一直被认为是基础性的。自动确定相关性在问答和摘要等许多应用中有很多应用。然而，先前的自然语言处理研究主要集中在语义相似性上，因为缺乏相关性数据集。本文介绍了一个语义文本相关性数据集STR-2022，其中包含了5500个英文句子对，使用比较式注释框架进行了手动注释，得到了细粒度的得分。我们发现，关于句子对相关性的人类直觉是非常可靠的，重复注释相关系数为0.84。我们使用该数据集探究了哪些因素影响句子的语义相关性，并展示了STR-2022对于评估自动句子表示方法以及各种下游自然语言处理任务的效用。我们提供了数据集、数据说明和注释请求。",
    "tldr": "本论文介绍了一个手动注释的语义文本相关性数据集STR-2022，对于自动句子表示方法和各种下游自然语言处理任务的评估具有实用性。除此之外，我们探究了影响句子语义相关性的因素。",
    "en_tdlr": "This paper introduces a manually annotated dataset for Semantic Textual Relatedness, STR-2022, and explores factors that affect semantic relatedness of sentences. The dataset proves useful for evaluating automatic sentence representation methods and various downstream NLP tasks."
}