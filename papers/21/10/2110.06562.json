{
    "title": "Unsupervised Object Learning via Common Fate. (arXiv:2110.06562v2 [cs.CV] UPDATED)",
    "abstract": "Learning generative object models from unlabelled videos is a long standing problem and required for causal scene modeling. We decompose this problem into three easier subtasks, and provide candidate solutions for each of them. Inspired by the Common Fate Principle of Gestalt Psychology, we first extract (noisy) masks of moving objects via unsupervised motion segmentation. Second, generative models are trained on the masks of the background and the moving objects, respectively. Third, background and foreground models are combined in a conditional \"dead leaves\" scene model to sample novel scene configurations where occlusions and depth layering arise naturally. To evaluate the individual stages, we introduce the Fishbowl dataset positioned between complex real-world scenes and common object-centric benchmarks of simplistic objects. We show that our approach allows learning generative models that generalize beyond the occlusions present in the input videos, and represent scenes in a modu",
    "link": "http://arxiv.org/abs/2110.06562",
    "context": "Title: Unsupervised Object Learning via Common Fate. (arXiv:2110.06562v2 [cs.CV] UPDATED)\nAbstract: Learning generative object models from unlabelled videos is a long standing problem and required for causal scene modeling. We decompose this problem into three easier subtasks, and provide candidate solutions for each of them. Inspired by the Common Fate Principle of Gestalt Psychology, we first extract (noisy) masks of moving objects via unsupervised motion segmentation. Second, generative models are trained on the masks of the background and the moving objects, respectively. Third, background and foreground models are combined in a conditional \"dead leaves\" scene model to sample novel scene configurations where occlusions and depth layering arise naturally. To evaluate the individual stages, we introduce the Fishbowl dataset positioned between complex real-world scenes and common object-centric benchmarks of simplistic objects. We show that our approach allows learning generative models that generalize beyond the occlusions present in the input videos, and represent scenes in a modu",
    "path": "papers/21/10/2110.06562.json",
    "total_tokens": 918,
    "translated_abstract": "从未标记的视频中学习生成式物体模型是一个长期存在的问题，也是因果场景建模所必需的。我们将这个问题分解为三个更简单的子任务，并为每个子任务提供了候选解决方案。受到格式塔心理学的共同命运原则的启发，我们首先通过无监督的运动分割提取出（带有噪声的）移动物体的蒙版。其次，分别在背景和移动物体的掩码上训练生成模型。第三，将背景和前景模型组合成条件性的“枯叶”场景模型，从而采样出新颖的场景配置，其中自然地出现遮挡和深度分层。为了评估各个阶段，我们引入了Fishbowl数据集，它位于复杂的现实场景和简单物体中心基准之间。我们展示了我们的方法可以学习生成模型，其泛化能力超越了输入视频中存在的遮挡，并以可模拟的方式表示场景。",
    "tldr": "本文提出了一个无监督的物体学习方法，通过共同命运原则从未标记的视频中提取出移动物体的蒙版，并在此基础上训练生成式模型，最终通过将背景模型和前景模型组合生成场景模型，从而实现对场景的模拟。",
    "en_tdlr": "This paper proposes an unsupervised object learning method that extracts masks of moving objects from unlabeled videos via the common fate principle, trains generative models, and combines them to generate scene models for scene simulation. The method can generalize beyond occlusions present in the input videos."
}