{
    "title": "AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v5 [cs.LG] UPDATED)",
    "abstract": "In the last few years, the memory requirements to train state-of-the-art neural networks have far exceeded the DRAM capacities of modern hardware accelerators. This has necessitated the development of efficient algorithms to train these neural networks in parallel on large-scale GPU-based clusters. Since computation is relatively inexpensive on modern GPUs, designing and implementing extremely efficient communication in these parallel training algorithms is critical for extracting the maximum performance. This paper presents AxoNN, a parallel deep learning framework that exploits asynchrony and message-driven execution to schedule neural network operations on each GPU, thereby reducing GPU idle time and maximizing hardware efficiency. By using the CPU memory as a scratch space for offloading data periodically during training, AxoNN is able to reduce GPU memory consumption by four times. This allows us to increase the number of parameters per GPU by four times, thus reducing the amount ",
    "link": "http://arxiv.org/abs/2110.13005",
    "context": "Title: AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v5 [cs.LG] UPDATED)\nAbstract: In the last few years, the memory requirements to train state-of-the-art neural networks have far exceeded the DRAM capacities of modern hardware accelerators. This has necessitated the development of efficient algorithms to train these neural networks in parallel on large-scale GPU-based clusters. Since computation is relatively inexpensive on modern GPUs, designing and implementing extremely efficient communication in these parallel training algorithms is critical for extracting the maximum performance. This paper presents AxoNN, a parallel deep learning framework that exploits asynchrony and message-driven execution to schedule neural network operations on each GPU, thereby reducing GPU idle time and maximizing hardware efficiency. By using the CPU memory as a scratch space for offloading data periodically during training, AxoNN is able to reduce GPU memory consumption by four times. This allows us to increase the number of parameters per GPU by four times, thus reducing the amount ",
    "path": "papers/21/10/2110.13005.json",
    "total_tokens": 938,
    "translated_title": "AxoNN: 一种异步、消息驱动的极规模深度学习并行框架",
    "translated_abstract": "近年来，训练最先进的神经网络所需的存储器容量已远远超出现代硬件加速器的DRAM容量。这促使我们在大规模基于GPU的集群上开发高效算法并行训练这些神经网络。在现代GPU上，计算相对廉价，为了提取最大性能，设计和实现这些并行训练算法中极其高效的通信是至关重要的。本文介绍了AxoNN，一种利用异步性和消息驱动执行调度每个GPU上神经网络操作的并行深度学习框架，从而减少GPU空闲时间，最大化硬件效率。通过使用CPU内存作为冗余空间，在训练期间定期卸载数据，AxoNN能够将GPU内存消耗降低4倍。这使得我们能够将每个GPU的参数数量增加4倍，从而减少了必需的GPU数量和训练时间。",
    "tldr": "AxoNN是一种利用异步性和消息驱动执行调度每个GPU上神经网络操作的并行深度学习框架，将CPU内存作为冗余空间，降低GPU内存消耗，同时将每个GPU的参数数量增加4倍，从而减少了必需的GPU数量和训练时间。",
    "en_tdlr": "AxoNN is a parallel deep learning framework that exploits asynchrony and message-driven execution to schedule neural network operations on each GPU. It reduces GPU idle time and maximizes hardware efficiency by using CPU memory as scratch space for offloading data periodically during training, reducing GPU memory consumption by four times. This allows increasing the number of parameters per GPU by four times, reducing the required number of GPUs and training time."
}