{
    "title": "Federated Learning from Small Datasets. (arXiv:2110.03469v3 [cs.LG] UPDATED)",
    "abstract": "Federated learning allows multiple parties to collaboratively train a joint model without sharing local data. This enables applications of machine learning in settings of inherently distributed, undisclosable data such as in the medical domain. In practice, joint training is usually achieved by aggregating local models, for which local training objectives have to be in expectation similar to the joint (global) objective. Often, however, local datasets are so small that local objectives differ greatly from the global objective, resulting in federated learning to fail. We propose a novel approach that intertwines model aggregations with permutations of local models. The permutations expose each local model to a daisy chain of local datasets resulting in more efficient training in data-sparse domains. This enables training on extremely small local datasets, such as patient data across hospitals, while retaining the training efficiency and privacy benefits of federated learning.",
    "link": "http://arxiv.org/abs/2110.03469",
    "context": "Title: Federated Learning from Small Datasets. (arXiv:2110.03469v3 [cs.LG] UPDATED)\nAbstract: Federated learning allows multiple parties to collaboratively train a joint model without sharing local data. This enables applications of machine learning in settings of inherently distributed, undisclosable data such as in the medical domain. In practice, joint training is usually achieved by aggregating local models, for which local training objectives have to be in expectation similar to the joint (global) objective. Often, however, local datasets are so small that local objectives differ greatly from the global objective, resulting in federated learning to fail. We propose a novel approach that intertwines model aggregations with permutations of local models. The permutations expose each local model to a daisy chain of local datasets resulting in more efficient training in data-sparse domains. This enables training on extremely small local datasets, such as patient data across hospitals, while retaining the training efficiency and privacy benefits of federated learning.",
    "path": "papers/21/10/2110.03469.json",
    "total_tokens": 863,
    "translated_title": "从小数据集中实现联合学习",
    "translated_abstract": "联合学习允许多个参与方在不共享本地数据的情况下协同训练一个联合模型。这在医疗领域等数据本身分布分散、无法公开的情况下，为机器学习应用提供了可能。在实践中，通常通过聚合本地模型实现联合训练，而本地训练目标的期望与全局目标相似。然而，由于本地数据集通常很小，导致本地目标与全局目标差异很大，从而使联合学习失败。我们提出了一种新颖的方法，将模型聚合与本地模型的置换相结合。这种置换将每个本地模型暴露给一系列本地数据集，从而在数据稀疏的领域中实现更高效的训练。这使得可以在极小的本地数据集上进行训练，例如跨医院的患者数据，同时保持联合学习的训练效率和隐私保护的好处。",
    "tldr": "本文介绍了一种在小数据集上实现联合学习的新方法，通过将模型聚合与本地模型的置换相结合，可以更高效地在数据稀疏的领域进行训练。",
    "en_tdlr": "This paper presents a novel approach to achieving federated learning from small datasets by combining model aggregations with permutations of local models, allowing for more efficient training in data-sparse domains."
}