{
    "title": "History Aware Multimodal Transformer for Vision-and-Language Navigation. (arXiv:2110.13309v2 [cs.CV] UPDATED)",
    "abstract": "Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy",
    "link": "http://arxiv.org/abs/2110.13309",
    "context": "Title: History Aware Multimodal Transformer for Vision-and-Language Navigation. (arXiv:2110.13309v2 [cs.CV] UPDATED)\nAbstract: Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy",
    "path": "papers/21/10/2110.13309.json",
    "total_tokens": 962,
    "translated_title": "基于历史意识的多模态变压器用于视觉语言导航",
    "translated_abstract": "视觉语言导航旨在构建能够遵循指令并在真实场景中导航的自主视觉代理。为了记住先前访问过的位置和采取的行动，大多数视觉语言导航方法使用递归状态实现记忆。相反，我们引入了一种历史意识多模态变压器(History Aware Multimodal Transformer，HAMT)，将长期历史纳入多模态决策中。HAMT通过层次视觉变压器(ViT)高效编码所有过去的全景观察结果，首先使用ViT对单个图像进行编码，然后模型化全景观察中图像之间的空间关系，最后考虑历史中全景观察之间的时间关系。然后，它将文本、历史和当前观察共同组合起来预测下一步行动。我们首先使用多个代理任务对HAMT进行端到端训练，包括单步行动预测和空间关系预测，然后使用强化学习进一步提高导航策略。",
    "tldr": "本论文提出了一种基于历史意识的多模态变压器(HAMT)，用于视觉语言导航。该方法通过编码过去的全景观察，综合考虑了文本、历史和当前观察，实现自主导航。在训练中，通过多个代理任务和强化学习进一步提高导航策略。",
    "en_tdlr": "This paper proposes a History Aware Multimodal Transformer (HAMT) for vision-and-language navigation. It encodes past panoramic observations and incorporates text, history, and current observation to achieve autonomous navigation. The navigation policy is improved through training with proxy tasks and reinforcement learning."
}