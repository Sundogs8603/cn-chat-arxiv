{
    "title": "Knowledge-driven Active Learning. (arXiv:2110.08265v4 [cs.LG] UPDATED)",
    "abstract": "The deployment of Deep Learning (DL) models is still precluded in those contexts where the amount of supervised data is limited. To answer this issue, active learning strategies aim at minimizing the amount of labelled data required to train a DL model. Most active strategies are based on uncertain sample selection, and even often restricted to samples lying close to the decision boundary. These techniques are theoretically sound, but an understanding of the selected samples based on their content is not straightforward, further driving non-experts to consider DL as a black-box. For the first time, here we propose to take into consideration common domain-knowledge and enable non-expert users to train a model with fewer samples. In our Knowledge-driven Active Learning (KAL) framework, rule-based knowledge is converted into logic constraints and their violation is checked as a natural guide for sample selection. We show that even simple relationships among data and output classes offer a",
    "link": "http://arxiv.org/abs/2110.08265",
    "context": "Title: Knowledge-driven Active Learning. (arXiv:2110.08265v4 [cs.LG] UPDATED)\nAbstract: The deployment of Deep Learning (DL) models is still precluded in those contexts where the amount of supervised data is limited. To answer this issue, active learning strategies aim at minimizing the amount of labelled data required to train a DL model. Most active strategies are based on uncertain sample selection, and even often restricted to samples lying close to the decision boundary. These techniques are theoretically sound, but an understanding of the selected samples based on their content is not straightforward, further driving non-experts to consider DL as a black-box. For the first time, here we propose to take into consideration common domain-knowledge and enable non-expert users to train a model with fewer samples. In our Knowledge-driven Active Learning (KAL) framework, rule-based knowledge is converted into logic constraints and their violation is checked as a natural guide for sample selection. We show that even simple relationships among data and output classes offer a",
    "path": "papers/21/10/2110.08265.json",
    "total_tokens": 924,
    "translated_title": "基于知识的主动学习",
    "translated_abstract": "深度学习模型的部署仍然受限于有限的监督数据问题。为了解决这个问题，主动学习策略旨在最小化训练深度学习模型所需的标记数据量。大多数主动学习策略基于不确定性样本选择，通常仅限于位于决策边界附近的样本。这些技术在理论上是可行的，但基于内容对所选样本的理解并不直观，这进一步导致非专业人士将深度学习视为黑盒子。在这里，我们首次提出考虑通用领域知识，并使非专业用户能够用更少的样本来训练模型。在我们的基于知识的主动学习(KAL)框架中，基于规则的知识被转换为逻辑约束，并检查其违反作为样本选择的自然指南。我们展示了即使是数据和输出类别之间的简单关系也可以提供出色的性能，同时降低标记数据需求量。",
    "tldr": "本文提出了基于知识的主动学习(KAL)框架，将通用领域知识转换为逻辑约束，作为样本选择的指南，使非专业用户能够用更少的样本训练模型，并在降低标记数据需求量的同时保持出色性能。",
    "en_tdlr": "This paper proposes a Knowledge-driven Active Learning (KAL) framework, which converts common domain-knowledge into logic constraints and checks their violation as a natural guide for sample selection, enabling non-expert users to train a model with fewer samples and achieving excellent performance while reducing the demand for labeled data."
}