{
    "title": "On the Fairness of Machine-Assisted Human Decisions. (arXiv:2110.15310v2 [cs.CY] UPDATED)",
    "abstract": "When machine-learning algorithms are used in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider in a formal model and in a lab experiment how properties of machine predictions affect the resulting human decisions. In our formal model of statistical decision-making, we show that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. In the lab experiment, we demonstrate ho",
    "link": "http://arxiv.org/abs/2110.15310",
    "context": "Title: On the Fairness of Machine-Assisted Human Decisions. (arXiv:2110.15310v2 [cs.CY] UPDATED)\nAbstract: When machine-learning algorithms are used in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider in a formal model and in a lab experiment how properties of machine predictions affect the resulting human decisions. In our formal model of statistical decision-making, we show that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. In the lab experiment, we demonstrate ho",
    "path": "papers/21/10/2110.15310.json",
    "total_tokens": 929,
    "translated_title": "关于机器辅助人类决策的公正性研究",
    "translated_abstract": "当机器学习算法在高风险决策中被使用时，我们希望确保它们的部署能够产生公平和公正的结果。这个关注点促使了一个迅速增长的文献，专注于诊断和解决机器预测中的差异性。然而，许多机器预测被部署来协助人类决策者保留最终决策权。因此，在本文中，我们在一个形式模型和实验室实验中考虑了机器预测的性质如何影响最终的人类决策。在我们的统计决策形式模型中，我们展示了包含一个有偏见的人类决策者可能逆转算法结构与最终决策质量之间的常规关系。具体地，我们记录了从预测中排除受保护群体信息可能无法减少甚至可能增加最终差异的情况。在实验室实验中，我们展示了如何...",
    "tldr": "本研究通过形式模型和实验室实验考察了机器预测的性质如何影响人类最终决策。实验发现，包含有偏见的人类决策者可能逆转算法结构与决策质量之间的关系，并且排除受保护群体信息可能无法减少差异甚至可能增加差异。"
}