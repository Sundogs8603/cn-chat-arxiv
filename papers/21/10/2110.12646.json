{
    "title": "Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection. (arXiv:2110.12646v2 [cs.CL] UPDATED)",
    "abstract": "Dialogue disentanglement aims to group utterances in a long and multi-participant dialogue into threads. This is useful for discourse analysis and downstream applications such as dialogue response selection, where it can be the first step to construct a clean context/response set. Unfortunately, labeling all~\\emph{reply-to} links takes quadratic effort w.r.t the number of utterances: an annotator must check all preceding utterances to identify the one to which the current utterance is a reply. In this paper, we are the first to propose a~\\textbf{zero-shot} dialogue disentanglement solution. Firstly, we train a model on a multi-participant response selection dataset harvested from the web which is not annotated; we then apply the trained model to perform zero-shot dialogue disentanglement. Without any labeled data, our model can achieve a cluster F1 score of 25. We also fine-tune the model using various amounts of labeled data. Experiments show that with only 10\\% of the data, we achiev",
    "link": "http://arxiv.org/abs/2110.12646",
    "context": "Title: Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection. (arXiv:2110.12646v2 [cs.CL] UPDATED)\nAbstract: Dialogue disentanglement aims to group utterances in a long and multi-participant dialogue into threads. This is useful for discourse analysis and downstream applications such as dialogue response selection, where it can be the first step to construct a clean context/response set. Unfortunately, labeling all~\\emph{reply-to} links takes quadratic effort w.r.t the number of utterances: an annotator must check all preceding utterances to identify the one to which the current utterance is a reply. In this paper, we are the first to propose a~\\textbf{zero-shot} dialogue disentanglement solution. Firstly, we train a model on a multi-participant response selection dataset harvested from the web which is not annotated; we then apply the trained model to perform zero-shot dialogue disentanglement. Without any labeled data, our model can achieve a cluster F1 score of 25. We also fine-tune the model using various amounts of labeled data. Experiments show that with only 10\\% of the data, we achiev",
    "path": "papers/21/10/2110.12646.json",
    "total_tokens": 919,
    "translated_title": "通过自我监督的纠缠响应选择实现零样本对话解缠",
    "translated_abstract": "对话解缠旨在将长篇多参与者对话中的话语分组成线索。这对于话语分析和对话响应选择等下游应用非常有用，因为它可以作为构建清洁上下文/响应集的第一步。然而，标记所有\"回复至\"链接需要与话语数量的平方级别的工作量：注释者必须检查所有之前的话语，以确定当前话语是回复给哪个话语。在本文中，我们首次提出了一种零样本对话解缠的解决方案。首先，我们在一个未注释的互动对话响应选择数据集上训练一个模型；然后，我们应用训练好的模型来进行零样本对话解缠。在没有任何标注数据的情况下，我们的模型可以达到25的聚类F1分数。我们还使用不同数量的标注数据对模型进行微调。实验结果表明，仅使用10%的数据，我们可以达到...",
    "tldr": "本文提出了一种新的零样本对话解缠方法，通过自我监督的纠缠响应选择，无需标注数据即可实现对长篇多参与者对话的分解，并在实验中获得了较好的聚类F1分数。"
}