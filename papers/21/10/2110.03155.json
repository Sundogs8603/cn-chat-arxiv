{
    "title": "The Benefits of Being Categorical Distributional: Uncertainty-aware Regularized Exploration in Reinforcement Learning",
    "abstract": "The theoretical advantages of distributional reinforcement learning~(RL) over classical RL remain elusive despite its remarkable empirical performance. Starting from Categorical Distributional RL~(CDRL), we attribute the potential superiority of distributional RL to a derived distribution-matching regularization by applying a return density function decomposition technique. This unexplored regularization in the distributional RL context is aimed at capturing additional return distribution information regardless of only its expectation, contributing to an augmented reward signal in the policy optimization. Compared with the entropy regularization in MaxEnt RL that explicitly optimizes the policy to encourage the exploration, the resulting regularization in CDRL implicitly optimizes policies guided by the new reward signal to align with the uncertainty of target return distributions, leading to an uncertainty-aware exploration effect. Finally, extensive experiments substantiate the impor",
    "link": "https://rss.arxiv.org/abs/2110.03155",
    "context": "Title: The Benefits of Being Categorical Distributional: Uncertainty-aware Regularized Exploration in Reinforcement Learning\nAbstract: The theoretical advantages of distributional reinforcement learning~(RL) over classical RL remain elusive despite its remarkable empirical performance. Starting from Categorical Distributional RL~(CDRL), we attribute the potential superiority of distributional RL to a derived distribution-matching regularization by applying a return density function decomposition technique. This unexplored regularization in the distributional RL context is aimed at capturing additional return distribution information regardless of only its expectation, contributing to an augmented reward signal in the policy optimization. Compared with the entropy regularization in MaxEnt RL that explicitly optimizes the policy to encourage the exploration, the resulting regularization in CDRL implicitly optimizes policies guided by the new reward signal to align with the uncertainty of target return distributions, leading to an uncertainty-aware exploration effect. Finally, extensive experiments substantiate the impor",
    "path": "papers/21/10/2110.03155.json",
    "total_tokens": 850,
    "translated_title": "分类分布式的好处：在强化学习中的不确定性感知的正则化探索",
    "translated_abstract": "尽管分类分布式强化学习（RL）在经验上表现出色，但其相对于传统RL的理论优势仍不明确。从分类分布式RL（CDRL）开始，我们通过应用返回概率函数分解技术，将分布式RL的潜在优势归因于派生的分布匹配正则化。这种在分布式RL环境中未被探索的正则化旨在捕捉额外的回报分布信息，而不仅仅是其期望值，从而为策略优化中的增强奖励信号作出贡献。与明确优化策略以鼓励探索的最大熵RL中的熵正则化相比，CDRL中的结果正则化通过新的奖励信号隐式地优化策略，使其与目标回报分布的不确定性相一致，从而产生了不确定性感知的探索效果。",
    "tldr": "通过应用返回概率函数分解技术，我们在分类分布式强化学习中探索了分布匹配正则化的潜力，通过隐式地优化策略以对齐目标回报分布的不确定性来产生不确定性感知的探索效果。",
    "en_tdlr": "By applying return density function decomposition technique, we explore the potential of distribution-matching regularization in categorical distributional reinforcement learning, resulting in an uncertainty-aware exploration effect by implicitly optimizing policies to align with the uncertainty of target return distributions."
}