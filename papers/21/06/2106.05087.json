{
    "title": "Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL. (arXiv:2106.05087v5 [cs.LG] UPDATED)",
    "abstract": "Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we can find it. Existing works on adversarial RL either use heuristics-based methods that may not find the strongest adversary, or directly train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. This paper introduces a novel attacking method to find the optimal attacks through collaboration between a designed function named \"actor\" and an RL-based learner named \"director\". The actor crafts state perturbations for a given policy perturbation direction, and the director learns to propose the best policy perturbation",
    "link": "http://arxiv.org/abs/2106.05087",
    "context": "Title: Who Is the Strongest Enemy? Towards Optimal and Efficient Evasion Attacks in Deep RL. (arXiv:2106.05087v5 [cs.LG] UPDATED)\nAbstract: Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we can find the optimal attack and how efficiently we can find it. Existing works on adversarial RL either use heuristics-based methods that may not find the strongest adversary, or directly train an RL-based adversary by treating the agent as a part of the environment, which can find the optimal adversary but may become intractable in a large state space. This paper introduces a novel attacking method to find the optimal attacks through collaboration between a designed function named \"actor\" and an RL-based learner named \"director\". The actor crafts state perturbations for a given policy perturbation direction, and the director learns to propose the best policy perturbation",
    "path": "papers/21/06/2106.05087.json",
    "total_tokens": 925,
    "translated_title": "最强敌人是谁？探索深度强化学习中最优和高效的规避攻击方法。",
    "translated_abstract": "在一些限制条件下，通过在状态观察中进行最优对抗扰动来对强化学习(RL)代理的最坏情况性能进行评估，对于理解RL代理的鲁棒性至关重要。然而，找到最优对手很具有挑战性，无论是我们能否找到最优攻击的好坏，还是如何高效地找到最优攻击。现有的对抗RL工作要么使用启发式方法来寻找最强对手，要么通过将代理视为环境的一部分直接训练基于RL的对手，可以找到最佳对手，但在大状态空间中可能变得棘手。本文介绍了一种新的攻击方法，通过一个名为“演员”的设计函数和名为“导演”的基于RL的学习器之间的协作来寻找最优攻击。演员为给定的策略扰动方向制作状态扰动，导演学习提出最佳策略扰动。",
    "tldr": "本文提出了一种新的攻击方法，通过一个名为“演员”的设计函数和名为“导演”的基于RL的学习器之间的协作，最新方法可以找到最优攻击，提高了关于RL代理鲁棒性的理解。",
    "en_tdlr": "This paper presents a novel method for efficient evasion attacks in Deep RL by introducing a designed function named \"actor\" and a RL-based learner named \"director\" to collaborate and find the optimal attacks. The proposed method improves the understanding of the robustness of RL agents."
}