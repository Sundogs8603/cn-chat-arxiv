{
    "title": "On the Convergence and Calibration of Deep Learning with Differential Privacy. (arXiv:2106.07830v6 [cs.LG] UPDATED)",
    "abstract": "Differentially private (DP) training preserves the data privacy usually at the cost of slower convergence (and thus lower accuracy), as well as more severe mis-calibration than its non-private counterpart. To analyze the convergence of DP training, we formulate a continuous time analysis through the lens of neural tangent kernel (NTK), which characterizes the per-sample gradient clipping and the noise addition in DP training, for arbitrary network architectures and loss functions. Interestingly, we show that the noise addition only affects the privacy risk but not the convergence or calibration, whereas the per-sample gradient clipping (under both flat and layerwise clipping styles) only affects the convergence and calibration.  Furthermore, we observe that while DP models trained with small clipping norm usually achieve the best accurate, but are poorly calibrated and thus unreliable. In sharp contrast, DP models trained with large clipping norm enjoy the same privacy guarantee and si",
    "link": "http://arxiv.org/abs/2106.07830",
    "context": "Title: On the Convergence and Calibration of Deep Learning with Differential Privacy. (arXiv:2106.07830v6 [cs.LG] UPDATED)\nAbstract: Differentially private (DP) training preserves the data privacy usually at the cost of slower convergence (and thus lower accuracy), as well as more severe mis-calibration than its non-private counterpart. To analyze the convergence of DP training, we formulate a continuous time analysis through the lens of neural tangent kernel (NTK), which characterizes the per-sample gradient clipping and the noise addition in DP training, for arbitrary network architectures and loss functions. Interestingly, we show that the noise addition only affects the privacy risk but not the convergence or calibration, whereas the per-sample gradient clipping (under both flat and layerwise clipping styles) only affects the convergence and calibration.  Furthermore, we observe that while DP models trained with small clipping norm usually achieve the best accurate, but are poorly calibrated and thus unreliable. In sharp contrast, DP models trained with large clipping norm enjoy the same privacy guarantee and si",
    "path": "papers/21/06/2106.07830.json",
    "total_tokens": 991,
    "translated_title": "论深度学习在差分隐私下的收敛性与校准性",
    "translated_abstract": "差分隐私训练通常会以数据隐私保护为代价，导致收敛速度变慢（从而精度降低），且比非隐私方法更容易出现严重的校准误差。本研究通过神经切向核（NTK）从连续时间的角度分析了差分隐私训练的收敛性，对任意网络结构和损失函数进行了建模，发现噪声只会影响隐私风险而不影响收敛性和校准性，而基于每个样本的梯度剪裁（在平坦和层级剪裁风格下）会影响收敛性和校准性。此外，研究表明，尽管小剪裁范数下的差分隐私模型通常会在精度上表现最佳，但其校准性较差且不可靠。而在大剪裁范数下训练的差分隐私模型不仅享有相同的隐私保证，而且校准效果好。",
    "tldr": "本文通过NTK对差分隐私训练进行连续时间分析，发现噪声只会影响隐私风险而不影响收敛性和校准性，而基于每个样本的梯度剪裁会影响收敛性和校准性。此外，大剪裁范数下的差分隐私模型不仅享有相同的隐私保证，而且校准效果好。",
    "en_tdlr": "This paper analyzes the convergence and calibration of differentially private training through continuous time analysis using the neural tangent kernel, showing that noise only affects privacy risk while per-sample gradient clipping affects convergence and calibration. DP models trained with large clipping norm perform well in both privacy guarantee and calibration."
}