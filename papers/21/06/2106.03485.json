{
    "title": "Redundant representations help generalization in wide neural networks. (arXiv:2106.03485v4 [stat.ML] UPDATED)",
    "abstract": "Deep neural networks (DNNs) defy the classical bias-variance trade-off: adding parameters to a DNN that interpolates its training data will typically improve its generalization performance. Explaining the mechanism behind this ``benign overfitting'' in deep networks remains an outstanding challenge. Here, we study the last hidden layer representations of various state-of-the-art convolutional neural networks and find that if the last hidden representation is wide enough, its neurons tend to split into groups that carry identical information, and differ from each other only by statistically independent noise. The number of such groups increases linearly with the width of the layer, but only if the width is above a critical value. We show that redundant neurons appear only when the training process reaches interpolation and the training error is zero.",
    "link": "http://arxiv.org/abs/2106.03485",
    "context": "Title: Redundant representations help generalization in wide neural networks. (arXiv:2106.03485v4 [stat.ML] UPDATED)\nAbstract: Deep neural networks (DNNs) defy the classical bias-variance trade-off: adding parameters to a DNN that interpolates its training data will typically improve its generalization performance. Explaining the mechanism behind this ``benign overfitting'' in deep networks remains an outstanding challenge. Here, we study the last hidden layer representations of various state-of-the-art convolutional neural networks and find that if the last hidden representation is wide enough, its neurons tend to split into groups that carry identical information, and differ from each other only by statistically independent noise. The number of such groups increases linearly with the width of the layer, but only if the width is above a critical value. We show that redundant neurons appear only when the training process reaches interpolation and the training error is zero.",
    "path": "papers/21/06/2106.03485.json",
    "total_tokens": 827,
    "translated_title": "冗余表示对宽神经网络的泛化有帮助",
    "translated_abstract": "深度神经网络（DNN）打破了经典的偏差-方差权衡：为DNN添加参数以插值其训练数据通常会改善其泛化性能。解释在深度网络中“良性过拟合”的机制仍然是一个未解决的挑战。在这里，我们研究了各种最先进的卷积神经网络的最后一个隐藏层表示，并发现如果最后一个隐藏表示足够宽，则其神经元倾向于分成携带相同信息的组，仅由统计独立噪声区分彼此。这种组的数量随层的宽度呈线性增加，但仅在宽度高于临界值时才会增加。我们展示了冗余神经元仅在训练过程达到插值且训练误差为零时出现。",
    "tldr": "本文研究了各种卷积神经网络的最后一个隐藏层中的表示，发现如果最后一个隐藏表示足够宽，则其神经元倾向于分成携带相同信息的组，而冗余表示有助于宽神经网络的泛化。",
    "en_tdlr": "This paper studies the representations of various convolutional neural networks and finds that redundant representations of wide last hidden layers can help generalize wide neural networks."
}