{
    "title": "Graph Barlow Twins: A self-supervised representation learning framework for graphs. (arXiv:2106.02466v3 [cs.LG] UPDATED)",
    "abstract": "The self-supervised learning (SSL) paradigm is an essential exploration area, which tries to eliminate the need for expensive data labeling. Despite the great success of SSL methods in computer vision and natural language processing, most of them employ contrastive learning objectives that require negative samples, which are hard to define. This becomes even more challenging in the case of graphs and is a bottleneck for achieving robust representations. To overcome such limitations, we propose a framework for self-supervised graph representation learning - Graph Barlow Twins, which utilizes a cross-correlation-based loss function instead of negative samples. Moreover, it does not rely on non-symmetric neural network architectures - in contrast to state-of-the-art self-supervised graph representation learning method BGRL. We show that our method achieves as competitive results as the best self-supervised methods and fully supervised ones while requiring fewer hyperparameters and substan",
    "link": "http://arxiv.org/abs/2106.02466",
    "context": "Title: Graph Barlow Twins: A self-supervised representation learning framework for graphs. (arXiv:2106.02466v3 [cs.LG] UPDATED)\nAbstract: The self-supervised learning (SSL) paradigm is an essential exploration area, which tries to eliminate the need for expensive data labeling. Despite the great success of SSL methods in computer vision and natural language processing, most of them employ contrastive learning objectives that require negative samples, which are hard to define. This becomes even more challenging in the case of graphs and is a bottleneck for achieving robust representations. To overcome such limitations, we propose a framework for self-supervised graph representation learning - Graph Barlow Twins, which utilizes a cross-correlation-based loss function instead of negative samples. Moreover, it does not rely on non-symmetric neural network architectures - in contrast to state-of-the-art self-supervised graph representation learning method BGRL. We show that our method achieves as competitive results as the best self-supervised methods and fully supervised ones while requiring fewer hyperparameters and substan",
    "path": "papers/21/06/2106.02466.json",
    "total_tokens": 845,
    "translated_title": "图形Barlow Twins：一种用于图形的自监督表示学习框架",
    "translated_abstract": "自监督学习（SSL）范式是一个重要的探索领域，旨在消除昂贵的数据标注需求。尽管自监督学习方法在计算机视觉和自然语言处理领域取得了巨大成功，但大多数方法使用对比学习目标，需要难以定义的负样本。在图形的情况下，这更具挑战性，是实现稳健表示的瓶颈。为了克服这些限制，我们提出了一种自监督图形表示学习框架 - 图形Barlow Twins，它使用基于交叉相关的损失函数，而不是负样本。此外，与最先进的自监督图形表示学习方法BGRL相比，它不依赖非对称神经网络体系结构。我们展示了我们的方法在需要较少超参数和实质性的情况下取得了与最好的自监督方法和全监督方法相当的结果。",
    "tldr": "图形Barlow Twins是一种自监督表示学习框架，使用交叉相关的损失函数来学习图形表示，克服了负样本定义困难的问题。相比其他方法，它不依赖非对称神经网络结构，在需要较少超参数的情况下取得了竞争性的结果。"
}