{
    "title": "On the approximation capability of GNNs in node classification/regression tasks. (arXiv:2106.08992v5 [cs.LG] UPDATED)",
    "abstract": "Graph Neural Networks (GNNs) are a broad class of connectionist models for graph processing. Recent studies have shown that GNNs can approximate any function on graphs, modulo the equivalence relation on graphs defined by the Weisfeiler--Lehman (WL) test. However, these results suffer from some limitations, both because they were derived using the Stone--Weierstrass theorem -- which is existential in nature, -- and because they assume that the target function to be approximated must be continuous. Furthermore, all current results are dedicated to graph classification/regression tasks, where the GNN must produce a single output for the whole graph, while also node classification/regression problems, in which an output is returned for each node, are very common. In this paper, we propose an alternative way to demonstrate the approximation capability of GNNs that overcomes these limitations. Indeed, we show that GNNs are universal approximators in probability for node classification/regre",
    "link": "http://arxiv.org/abs/2106.08992",
    "context": "Title: On the approximation capability of GNNs in node classification/regression tasks. (arXiv:2106.08992v5 [cs.LG] UPDATED)\nAbstract: Graph Neural Networks (GNNs) are a broad class of connectionist models for graph processing. Recent studies have shown that GNNs can approximate any function on graphs, modulo the equivalence relation on graphs defined by the Weisfeiler--Lehman (WL) test. However, these results suffer from some limitations, both because they were derived using the Stone--Weierstrass theorem -- which is existential in nature, -- and because they assume that the target function to be approximated must be continuous. Furthermore, all current results are dedicated to graph classification/regression tasks, where the GNN must produce a single output for the whole graph, while also node classification/regression problems, in which an output is returned for each node, are very common. In this paper, we propose an alternative way to demonstrate the approximation capability of GNNs that overcomes these limitations. Indeed, we show that GNNs are universal approximators in probability for node classification/regre",
    "path": "papers/21/06/2106.08992.json",
    "total_tokens": 1138,
    "translated_title": "GNN在节点分类/回归任务中的逼近能力研究",
    "translated_abstract": "图神经网络(GNNs)是一种广泛的用于图处理的连接式模型。最近的研究表明，GNN可以逐步逼近关于图的任意函数，这取决于由Weisfeiler--Lehman(WL)测试定义的图等价关系。然而，这些结果存在一些局限性，一方面因为它们是用Stone-Weierstrass定理来推导得出的，这种方法本质上只是一种存在性证明，另一方面因为它们假设目标函数是连续的。此外，所有当前的结果都是专门针对图分类/回归任务的，而节点分类/回归问题也非常常见。在本文中，我们提出了一种新的方法来证明GNN的逼近能力，克服了这些局限性。我们发现，对于节点分类/回归任务，无论目标函数是连续的还是离散的，GNN在概率上都是通用逼近器。具体而言，我们发现，在一些温和条件下，任何有限域上的可测函数都可以通过一个只有一层隐藏层、ReLU激活函数和共享所有节点的边权值的GNN进行逼近。我们的分析也揭示了GNN的表达能力随其深度和宽度的变化及其结果针对大型图的可扩展性。",
    "tldr": "本文研究GNN在节点分类/回归任务中的逼近能力，发现在概率上GNN对于任何有限域上的可测函数都是通用逼近器。我们的研究还揭示了GNN的表达能力与其深度和宽度的变化有关，并且结果可扩展用于大型图。",
    "en_tdlr": "This paper studies the approximation capability of GNNs in node classification/regression tasks and finds that GNNs are universal approximators in probability for any measurable function over a finite domain. The analysis also sheds light on the expressive power of GNNs as a function of their depth and width and on the scalability of the results to large graphs."
}