{
    "title": "On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization. (arXiv:2106.02835v4 [cs.LG] UPDATED)",
    "abstract": "Causal discovery from observational data is an important but challenging task in many scientific fields. Recently, a method with non-combinatorial directed acyclic constraint, called NOTEARS, formulates the causal structure learning problem as a continuous optimization problem using least-square loss. Though the least-square loss function is well justified under the standard Gaussian noise assumption, it is limited if the assumption does not hold. In this work, we theoretically show that the violation of the Gaussian noise assumption will hinder the causal direction identification, making the causal orientation fully determined by the causal strength as well as the variances of noises in the linear case and by the strong non-Gaussian noises in the nonlinear case. Consequently, we propose a more general entropy-based loss that is theoretically consistent with the likelihood score under any noise distribution. We run extensive empirical evaluations on both synthetic data and real-world d",
    "link": "http://arxiv.org/abs/2106.02835",
    "context": "Title: On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization. (arXiv:2106.02835v4 [cs.LG] UPDATED)\nAbstract: Causal discovery from observational data is an important but challenging task in many scientific fields. Recently, a method with non-combinatorial directed acyclic constraint, called NOTEARS, formulates the causal structure learning problem as a continuous optimization problem using least-square loss. Though the least-square loss function is well justified under the standard Gaussian noise assumption, it is limited if the assumption does not hold. In this work, we theoretically show that the violation of the Gaussian noise assumption will hinder the causal direction identification, making the causal orientation fully determined by the causal strength as well as the variances of noises in the linear case and by the strong non-Gaussian noises in the nonlinear case. Consequently, we propose a more general entropy-based loss that is theoretically consistent with the likelihood score under any noise distribution. We run extensive empirical evaluations on both synthetic data and real-world d",
    "path": "papers/21/06/2106.02835.json",
    "total_tokens": 912,
    "translated_title": "论连续优化下基于熵损失函数在学习因果结构中的作用",
    "translated_abstract": "因果发现是许多科学领域中重要且具有挑战性的任务。最近，一种名为NOTEARS的非组合有向无环约束方法将因果结构学习问题转化为使用最小二乘损失的连续优化问题。虽然最小二乘损失函数在标准高斯噪声假设下是合理的，但如果该假设不成立，它会受到限制。在这项工作中，我们从理论上证明了高斯噪声假设的违反将阻碍因果方向的识别，使因果方向完全由因果强度以及线性情况下噪声方差和非线性情况下强非高斯噪声确定。因此，我们提出了一种更一般的基于熵的损失函数，在任何噪声分布下，在理论上与似然分数一致。我们对合成数据和真实世界数据进行了大量的实证评估。",
    "tldr": "发现因果结构的任务是重要且具有挑战性的，在连续优化中使用的最小二乘损失函数受限于高斯噪声假设。在本研究中，我们提出了一种理论上与任何噪声分布一致的基于熵的损失函数来克服这一限制。",
    "en_tdlr": "Discovering causal structures is an important and challenging task, and the least-square loss function used in continuous optimization is limited by the Gaussian noise assumption. In this study, we propose a theoretically consistent entropy-based loss function with any noise distribution to overcome this limitation."
}