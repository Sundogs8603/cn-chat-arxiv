{
    "title": "Learning Markov State Abstractions for Deep Reinforcement Learning",
    "abstract": "arXiv:2106.04379v4 Announce Type: replace-cross  Abstract: A fundamental assumption of reinforcement learning in Markov decision processes (MDPs) is that the relevant decision process is, in fact, Markov. However, when MDPs have rich observations, agents typically learn by way of an abstract state representation, and such representations are not guaranteed to preserve the Markov property. We introduce a novel set of conditions and prove that they are sufficient for learning a Markov abstract state representation. We then describe a practical training procedure that combines inverse model estimation and temporal contrastive learning to learn an abstraction that approximately satisfies these conditions. Our novel training objective is compatible with both online and offline training: it does not require a reward signal, but agents can capitalize on reward information when available. We empirically evaluate our approach on a visual gridworld domain and a set of continuous control benchmar",
    "link": "https://arxiv.org/abs/2106.04379",
    "context": "Title: Learning Markov State Abstractions for Deep Reinforcement Learning\nAbstract: arXiv:2106.04379v4 Announce Type: replace-cross  Abstract: A fundamental assumption of reinforcement learning in Markov decision processes (MDPs) is that the relevant decision process is, in fact, Markov. However, when MDPs have rich observations, agents typically learn by way of an abstract state representation, and such representations are not guaranteed to preserve the Markov property. We introduce a novel set of conditions and prove that they are sufficient for learning a Markov abstract state representation. We then describe a practical training procedure that combines inverse model estimation and temporal contrastive learning to learn an abstraction that approximately satisfies these conditions. Our novel training objective is compatible with both online and offline training: it does not require a reward signal, but agents can capitalize on reward information when available. We empirically evaluate our approach on a visual gridworld domain and a set of continuous control benchmar",
    "path": "papers/21/06/2106.04379.json",
    "total_tokens": 899,
    "translated_title": "学习马尔可夫状态抽象以用于深度强化学习",
    "translated_abstract": "强化学习在马尔可夫决策过程（MDPs）中的一个基本假设是，相关的决策过程实际上是马尔可夫的。然而，当MDPs具有丰富的观测时，代理通常通过抽象状态表示学习，这种表示未必能保持马尔可夫性质。我们引入了一组新颖的条件，并证明它们足以学习马尔可夫抽象状态表示。然后，我们描述了一个实用的训练过程，结合了逆模型估计和时间对比学习，以学习一个近似满足这些条件的抽象。我们的新颖训练目标适用于在线和离线训练：它不需要奖励信号，但当可用时，代理可以利用奖励信息。我们在一个视觉格子世界域和一组连续控制基准任务上对我们的方法进行了实证评估。",
    "tldr": "引入了一组新颖条件，证明了学习马尔可夫抽象状态表示的充分性，并提出了结合逆模型估计和时间对比学习的实用训练过程，该方法适用于在线和离线训练，不依赖奖励信号但可以利用奖励信息。",
    "en_tdlr": "Introduced a novel set of conditions, proved the sufficiency for learning a Markov abstract state representation, and proposed a practical training procedure that combines inverse model estimation and temporal contrastive learning, which is compatible with both online and offline training, does not rely on reward signal but can leverage reward information."
}