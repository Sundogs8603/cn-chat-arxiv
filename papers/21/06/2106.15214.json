{
    "title": "Joint Majorization-Minimization for Nonnegative Matrix Factorization with the $\\beta$-divergence. (arXiv:2106.15214v4 [cs.LG] UPDATED)",
    "abstract": "This article proposes new multiplicative updates for nonnegative matrix factorization (NMF) with the $\\beta$-divergence objective function. Our new updates are derived from a joint majorization-minimization (MM) scheme, in which an auxiliary function (a tight upper bound of the objective function) is built for the two factors jointly and minimized at each iteration. This is in contrast with the classic approach in which a majorizer is derived for each factor separately. Like that classic approach, our joint MM algorithm also results in multiplicative updates that are simple to implement. They however yield a significant drop of computation time (for equally good solutions), in particular for some $\\beta$-divergences of important applicative interest, such as the squared Euclidean distance and the Kullback-Leibler or Itakura-Saito divergences. We report experimental results using diverse datasets: face images, an audio spectrogram, hyperspectral data and song play counts. Depending on t",
    "link": "http://arxiv.org/abs/2106.15214",
    "context": "Title: Joint Majorization-Minimization for Nonnegative Matrix Factorization with the $\\beta$-divergence. (arXiv:2106.15214v4 [cs.LG] UPDATED)\nAbstract: This article proposes new multiplicative updates for nonnegative matrix factorization (NMF) with the $\\beta$-divergence objective function. Our new updates are derived from a joint majorization-minimization (MM) scheme, in which an auxiliary function (a tight upper bound of the objective function) is built for the two factors jointly and minimized at each iteration. This is in contrast with the classic approach in which a majorizer is derived for each factor separately. Like that classic approach, our joint MM algorithm also results in multiplicative updates that are simple to implement. They however yield a significant drop of computation time (for equally good solutions), in particular for some $\\beta$-divergences of important applicative interest, such as the squared Euclidean distance and the Kullback-Leibler or Itakura-Saito divergences. We report experimental results using diverse datasets: face images, an audio spectrogram, hyperspectral data and song play counts. Depending on t",
    "path": "papers/21/06/2106.15214.json",
    "total_tokens": 929,
    "translated_abstract": "本文提出了一种新的非负矩阵分解策略，该策略采用了$\\beta$-散度目标函数。我们提出的新算法基于联合主导-最小化框架，其中在每次迭代中都对两个分量一起构建辅助函数（目标函数的紧密上界）并对它们进行最小化。与经典方法不同，经典方法是对每个分量分别构建主导函数。与经典算法一样，我们的联合主导-最小化算法也采用简单的乘法更新。然而，我们的算法计算时间大大降低（对于相同质量的解），特别是对于一些应用中非常重要的$\\beta$-散度（如平方欧几里德距离、Kullback-Leibler或Itakura-Saito散度）。我们使用各种数据集进行试验：面部图像、音频谱图、高光谱数据和歌曲播放次数。根据数据集的不同，我们的算法运行速度可以有显著提高。",
    "tldr": "本文提出了一种使用联合主导-最小化算法的非负矩阵分解策略，它提高了计算效率，并适用于一些特定的应用场景。",
    "en_tdlr": "This article proposes a new nonnegative matrix factorization strategy using joint majorization-minimization algorithm with a $\\beta$-divergence objective function, which yields significant computational efficiency and applies particularly well to certain use cases."
}