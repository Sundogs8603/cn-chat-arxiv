{
    "title": "Benign Overfitting in Multiclass Classification: All Roads Lead to Interpolation. (arXiv:2106.10865v3 [stat.ML] UPDATED)",
    "abstract": "The literature on \"benign overfitting\" in overparameterized models has been mostly restricted to regression or binary classification; however, modern machine learning operates in the multiclass setting. Motivated by this discrepancy, we study benign overfitting in multiclass linear classification. Specifically, we consider the following training algorithms on separable data: (i) empirical risk minimization (ERM) with cross-entropy loss, which converges to the multiclass support vector machine (SVM) solution; (ii) ERM with least-squares loss, which converges to the min-norm interpolating (MNI) solution; and, (iii) the one-vs-all SVM classifier. First, we provide a simple sufficient deterministic condition under which all three algorithms lead to classifiers that interpolate the training data and have equal accuracy. When the data is generated from Gaussian mixtures or a multinomial logistic model, this condition holds under high enough effective overparameterization. We also show that t",
    "link": "http://arxiv.org/abs/2106.10865",
    "context": "Title: Benign Overfitting in Multiclass Classification: All Roads Lead to Interpolation. (arXiv:2106.10865v3 [stat.ML] UPDATED)\nAbstract: The literature on \"benign overfitting\" in overparameterized models has been mostly restricted to regression or binary classification; however, modern machine learning operates in the multiclass setting. Motivated by this discrepancy, we study benign overfitting in multiclass linear classification. Specifically, we consider the following training algorithms on separable data: (i) empirical risk minimization (ERM) with cross-entropy loss, which converges to the multiclass support vector machine (SVM) solution; (ii) ERM with least-squares loss, which converges to the min-norm interpolating (MNI) solution; and, (iii) the one-vs-all SVM classifier. First, we provide a simple sufficient deterministic condition under which all three algorithms lead to classifiers that interpolate the training data and have equal accuracy. When the data is generated from Gaussian mixtures or a multinomial logistic model, this condition holds under high enough effective overparameterization. We also show that t",
    "path": "papers/21/06/2106.10865.json",
    "total_tokens": 879,
    "translated_title": "多类分类中的良性过拟合：所有路径都通往插值",
    "translated_abstract": "在超参数化模型中，“良性过拟合”的文献大多局限于回归或二分类问题；然而，现代机器学习在多类别设置中运行。受此差异的启发，我们研究了多类线性分类中的良性过拟合。具体而言，我们考虑在可分数据上的以下训练算法：（i）交叉熵损失的经验风险最小化（ERM），收敛到多类支持向量机（SVM）解；（ii）最小二乘损失的ERM，收敛到最小范数插值（MNI）解；及（iii）一对多SVM分类器。首先，我们提供了一个简单的充分确定性条件，在该条件下，所有三种算法都会导致插值训练数据并具有相等准确率的分类器。当数据来自高斯混合模型或多项式逻辑模型时，在足够高的有效超参数化下，这个条件成立。我们还展示了...",
    "tldr": "多类分类中的良性过拟合问题进行了研究，提出了一个简单的确定性条件，当前三种算法在满足条件时会得到插值数据并具有相等准确率的分类器。"
}