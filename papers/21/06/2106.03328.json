{
    "title": "Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning. (arXiv:2106.03328v2 [cs.LG] UPDATED)",
    "abstract": "Secure aggregation is a critical component in federated learning (FL), which enables the server to learn the aggregate model of the users without observing their local models. Conventionally, secure aggregation algorithms focus only on ensuring the privacy of individual users in a single training round. We contend that such designs can lead to significant privacy leakages over multiple training rounds, due to partial user selection/participation at each round of FL. In fact, we show that the conventional random user selection strategies in FL lead to leaking users' individual models within number of rounds that is linear in the number of users. To address this challenge, we introduce a secure aggregation framework, Multi-RoundSecAgg, with multi-round privacy guarantees. In particular, we introduce a new metric to quantify the privacy guarantees of FL over multiple training rounds, and develop a structured user selection strategy that guarantees the long-term privacy of each user (over ",
    "link": "http://arxiv.org/abs/2106.03328",
    "context": "Title: Securing Secure Aggregation: Mitigating Multi-Round Privacy Leakage in Federated Learning. (arXiv:2106.03328v2 [cs.LG] UPDATED)\nAbstract: Secure aggregation is a critical component in federated learning (FL), which enables the server to learn the aggregate model of the users without observing their local models. Conventionally, secure aggregation algorithms focus only on ensuring the privacy of individual users in a single training round. We contend that such designs can lead to significant privacy leakages over multiple training rounds, due to partial user selection/participation at each round of FL. In fact, we show that the conventional random user selection strategies in FL lead to leaking users' individual models within number of rounds that is linear in the number of users. To address this challenge, we introduce a secure aggregation framework, Multi-RoundSecAgg, with multi-round privacy guarantees. In particular, we introduce a new metric to quantify the privacy guarantees of FL over multiple training rounds, and develop a structured user selection strategy that guarantees the long-term privacy of each user (over ",
    "path": "papers/21/06/2106.03328.json",
    "total_tokens": 933,
    "translated_title": "保护安全聚合：减轻联邦学习中多轮隐私泄漏问题",
    "translated_abstract": "安全聚合是联邦学习中的一个关键组成部分，它使得服务器能够在不观察用户本地模型的情况下学习到用户的聚合模型。传统的安全聚合算法仅关注单轮训练中个体用户的隐私保护。我们认为，这样的设计可能导致在多轮训练中存在显著的隐私泄漏，因为每轮联邦学习都会选择/参与部分用户。事实上，我们证明了传统的随机用户选择策略导致了用户个体模型的泄漏，其泄漏的轮数与用户数量成线性关系。为了应对这一挑战，我们提出了一个安全聚合框架Multi-RoundSecAgg，具有多轮隐私保证。具体而言，我们引入了一个衡量联邦学习在多轮训练中隐私保证的新指标，并开发了一种结构化的用户选择策略，以确保每个用户的长期隐私保护。",
    "tldr": "该论文提出了一个名为Multi-RoundSecAgg的安全聚合框架，解决了联邦学习中多轮隐私泄漏的问题，通过介绍新的隐私指标和开发结构化的用户选择策略实现了多轮隐私保证。",
    "en_tdlr": "This paper presents a secure aggregation framework called Multi-RoundSecAgg, which addresses the issue of multi-round privacy leakage in federated learning. It achieves multi-round privacy guarantees by introducing a new privacy metric and developing a structured user selection strategy."
}