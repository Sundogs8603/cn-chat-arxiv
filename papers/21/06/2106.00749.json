{
    "title": "Higher-order Derivatives of Weighted Finite-state Machines. (arXiv:2106.00749v2 [cs.CL] UPDATED)",
    "abstract": "Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time -- from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal $\\mathcal{O}(A^2 N^4)$ time where $A$ is the alphabet size and $N$ is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.",
    "link": "http://arxiv.org/abs/2106.00749",
    "context": "Title: Higher-order Derivatives of Weighted Finite-state Machines. (arXiv:2106.00749v2 [cs.CL] UPDATED)\nAbstract: Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time -- from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal $\\mathcal{O}(A^2 N^4)$ time where $A$ is the alphabet size and $N$ is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.",
    "path": "papers/21/06/2106.00749.json",
    "total_tokens": 855,
    "translated_title": "加权有限状态机的高阶导数",
    "translated_abstract": "加权有限状态机是自然语言处理系统的基本构建块。从它们在20世纪90年代早期用于噪声信道模型开始，一直到现代神经参数化条件随机场的使用，它们经受了时间的考验。本研究考察了关于加权有限状态机归一化常数的高阶导数的计算。我们提供了一种评估所有阶数导数的通用算法，这在文献中尚未描述。在二阶导数的情况下，我们的方案在最佳的 \\mathcal{O}(A^2 N^4) 时间内运行，其中 A 是字母表大小，N 是状态数。我们的算法比之前的算法快得多。此外，我们的方法导致了一个计算二阶期望的显著更快的算法，例如协方差矩阵和一阶期望的梯度。",
    "tldr": "本研究提出了一种通用算法来评估加权有限状态机关于归一化常数的高阶导数，并且在二阶导数的情况下，方案的运行时间是最优的。此外，该方法还导致了计算二阶期望的更快算法。",
    "en_tdlr": "This work proposes a general algorithm for evaluating higher-order derivatives of weighted finite-state machines with respect to the normalization constant. The scheme achieves optimal running time for second-order derivatives and leads to a faster algorithm for computing second-order expectations."
}