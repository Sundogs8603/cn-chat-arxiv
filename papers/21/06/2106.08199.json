{
    "title": "On Multi-objective Policy Optimization as a Tool for Reinforcement Learning: Case Studies in Offline RL and Finetuning. (arXiv:2106.08199v2 [cs.LG] UPDATED)",
    "abstract": "Many advances that have improved the robustness and efficiency of deep reinforcement learning (RL) algorithms can, in one way or another, be understood as introducing additional objectives or constraints in the policy optimization step. This includes ideas as far ranging as exploration bonuses, entropy regularization, and regularization toward teachers or data priors. Often, the task reward and auxiliary objectives are in conflict, and in this paper we argue that this makes it natural to treat these cases as instances of multi-objective (MO) optimization problems. We demonstrate how this perspective allows us to develop novel and more effective RL algorithms. In particular, we focus on offline RL and finetuning as case studies, and show that existing approaches can be understood as MO algorithms relying on linear scalarization. We hypothesize that replacing linear scalarization with a better algorithm can improve performance. We introduce Distillation of a Mixture of Experts (DiME), a ",
    "link": "http://arxiv.org/abs/2106.08199",
    "context": "Title: On Multi-objective Policy Optimization as a Tool for Reinforcement Learning: Case Studies in Offline RL and Finetuning. (arXiv:2106.08199v2 [cs.LG] UPDATED)\nAbstract: Many advances that have improved the robustness and efficiency of deep reinforcement learning (RL) algorithms can, in one way or another, be understood as introducing additional objectives or constraints in the policy optimization step. This includes ideas as far ranging as exploration bonuses, entropy regularization, and regularization toward teachers or data priors. Often, the task reward and auxiliary objectives are in conflict, and in this paper we argue that this makes it natural to treat these cases as instances of multi-objective (MO) optimization problems. We demonstrate how this perspective allows us to develop novel and more effective RL algorithms. In particular, we focus on offline RL and finetuning as case studies, and show that existing approaches can be understood as MO algorithms relying on linear scalarization. We hypothesize that replacing linear scalarization with a better algorithm can improve performance. We introduce Distillation of a Mixture of Experts (DiME), a ",
    "path": "papers/21/06/2106.08199.json",
    "total_tokens": 879,
    "translated_title": "多目标政策优化作为强化学习工具的研究：离线 RL 和微调案例研究",
    "translated_abstract": "许多改进深度强化学习（RL）算法鲁棒性和效率的方法可以理解为在策略优化步骤中引入额外的目标或约束。这包括探索奖励、熵正则化以及朝向教师或数据先验的正则化等各种想法。通常，任务奖励和辅助目标存在冲突，我们认为这使得将这些情况视为多目标（MO）优化问题是自然的。我们展示了这个观点如何帮助我们开发出更新颖、更有效的 RL 算法。具体而言，我们重点关注离线 RL 和微调作为案例研究，并展示了现有方法可以被理解为依赖于线性标量化的 MO 算法。我们假设用更好的算法替代线性标量化可以提高性能。我们引入了专家混合蒸馏（DiME）算法，它是一种改进的多目标 RL 算法。",
    "tldr": "本文介绍了多目标政策优化作为强化学习工具的案例研究，并提出了一种改进的多目标 RL 算法 DiME。",
    "en_tdlr": "This paper presents case studies on multi-objective policy optimization as a tool for reinforcement learning and introduces a novel multi-objective RL algorithm called DiME."
}