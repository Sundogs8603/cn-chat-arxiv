{
    "title": "Robust Model Selection and Nearly-Proper Learning for GMMs. (arXiv:2106.02774v2 [cs.DS] UPDATED)",
    "abstract": "In learning theory, a standard assumption is that the data is generated from a finite mixture model. But what happens when the number of components is not known in advance? The problem of estimating the number of components, also called model selection, is important in its own right but there are essentially no known efficient algorithms with provable guarantees let alone ones that can tolerate adversarial corruptions. In this work, we study the problem of robust model selection for univariate Gaussian mixture models (GMMs). Given $\\textsf{poly}(k/\\epsilon)$ samples from a distribution that is $\\epsilon$-close in TV distance to a GMM with $k$ components, we can construct a GMM with $\\widetilde{O}(k)$ components that approximates the distribution to within $\\widetilde{O}(\\epsilon)$ in $\\textsf{poly}(k/\\epsilon)$ time. Thus we are able to approximately determine the minimum number of components needed to fit the distribution within a logarithmic factor. Prior to our work, the only known ",
    "link": "http://arxiv.org/abs/2106.02774",
    "context": "Title: Robust Model Selection and Nearly-Proper Learning for GMMs. (arXiv:2106.02774v2 [cs.DS] UPDATED)\nAbstract: In learning theory, a standard assumption is that the data is generated from a finite mixture model. But what happens when the number of components is not known in advance? The problem of estimating the number of components, also called model selection, is important in its own right but there are essentially no known efficient algorithms with provable guarantees let alone ones that can tolerate adversarial corruptions. In this work, we study the problem of robust model selection for univariate Gaussian mixture models (GMMs). Given $\\textsf{poly}(k/\\epsilon)$ samples from a distribution that is $\\epsilon$-close in TV distance to a GMM with $k$ components, we can construct a GMM with $\\widetilde{O}(k)$ components that approximates the distribution to within $\\widetilde{O}(\\epsilon)$ in $\\textsf{poly}(k/\\epsilon)$ time. Thus we are able to approximately determine the minimum number of components needed to fit the distribution within a logarithmic factor. Prior to our work, the only known ",
    "path": "papers/21/06/2106.02774.json",
    "total_tokens": 1128,
    "translated_title": "GMM的鲁棒模型选择和近似正确学习",
    "translated_abstract": "在学习理论中，通常假定数据是从有限混合模型生成的。但是如果事先不知道组分数会发生什么呢？估计组分数的问题，在本身上是很重要的，但实际上就算是没有有效算法，更不用说能容忍对抗性扰动了。本文研究了一元高斯混合模型（GMMs）鲁棒模型选择的问题。我们会从一个与$k$个组分的GMM $\\epsilon$ -close的分布中产生$\\textsf{poly}(k / \\epsilon)$个样本，用$\\textsf{poly}(k / \\epsilon)$时间构建一个有$\\widetilde{O}(k)$个组件的GMM，可在$\\widetilde {O} (\\epsilon)$内近似表示分布。因此，我们能够近似确定拟合分布所需的最少组件数。在本研究之前，唯一已知的有效算法需要至少 $O(k \\log \\log n)$ 个组件才能完成此任务，这已近乎达到了极限。此外，我们还证明了我们的算法几乎是正确的，即，其具有最优的样本复杂度，仅具有对数因子。最后，我们证明了我们的结果使得我们能够在对抗扰动下鲁棒地学习GMMs。",
    "tldr": "本文研究了一元高斯混合模型（GMMs）鲁棒模型选择的问题，提出了一个鲁棒算法，可以在对抗性扰动下近似正确地学习GMMs，实现了最佳样本复杂度，能够近似确定拟合分布所需的最少组件数。",
    "en_tdlr": "This paper studies the problem of robust model selection for univariate Gaussian mixture models (GMMs) and proposes a robust algorithm for approximately correctly learning GMMs under adversarial corruptions with optimal sample complexity up to logarithmic factors. It is able to approximately determine the minimum number of components needed to fit the distribution within a logarithmic factor."
}