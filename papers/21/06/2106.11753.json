{
    "title": "Symplectic Learning for Hamiltonian Neural Networks. (arXiv:2106.11753v2 [cs.LG] UPDATED)",
    "abstract": "Machine learning methods are widely used in the natural sciences to model and predict physical systems from observation data. Yet, they are often used as poorly understood \"black boxes,\" disregarding existing mathematical structure and invariants of the problem. Recently, the proposal of Hamiltonian Neural Networks (HNNs) took a first step towards a unified \"gray box\" approach, using physical insight to improve performance for Hamiltonian systems. In this paper, we explore a significantly improved training method for HNNs, exploiting the symplectic structure of Hamiltonian systems with a different loss function. This frees the loss from an artificial lower bound. We mathematically guarantee the existence of an exact Hamiltonian function which the HNN can learn. This allows us to prove and numerically analyze the errors made by HNNs which, in turn, renders them fully explainable. Finally, we present a novel post-training correction to obtain the true Hamiltonian only from discretized ob",
    "link": "http://arxiv.org/abs/2106.11753",
    "context": "Title: Symplectic Learning for Hamiltonian Neural Networks. (arXiv:2106.11753v2 [cs.LG] UPDATED)\nAbstract: Machine learning methods are widely used in the natural sciences to model and predict physical systems from observation data. Yet, they are often used as poorly understood \"black boxes,\" disregarding existing mathematical structure and invariants of the problem. Recently, the proposal of Hamiltonian Neural Networks (HNNs) took a first step towards a unified \"gray box\" approach, using physical insight to improve performance for Hamiltonian systems. In this paper, we explore a significantly improved training method for HNNs, exploiting the symplectic structure of Hamiltonian systems with a different loss function. This frees the loss from an artificial lower bound. We mathematically guarantee the existence of an exact Hamiltonian function which the HNN can learn. This allows us to prove and numerically analyze the errors made by HNNs which, in turn, renders them fully explainable. Finally, we present a novel post-training correction to obtain the true Hamiltonian only from discretized ob",
    "path": "papers/21/06/2106.11753.json",
    "total_tokens": 918,
    "translated_title": "哈密顿神经网络的辛学习",
    "translated_abstract": "在自然科学中，机器学习方法被广泛用于从观测数据中建模和预测物理系统。然而，它们经常被当作不太理解的“黑盒子”，忽视了问题的数学结构和不变量。最近，哈密顿神经网络（HNNs）的提出迈出了迈向统一的“灰盒子”方法的第一步，利用物理洞察力来提高哈密顿系统的性能。在本文中，我们探讨了一种显著改进的HNNs训练方法，利用哈密顿系统的辛结构和不同的损失函数。这将损失从人为的下界中解放出来。我们数学上保证了HNNs可以学习到一个精确的哈密顿函数的存在。这使我们能够证明和数值分析HNNs所产生的错误，从而使它们完全可解释。最后，我们提出了一种新颖的训练后校正方法，用于从离散观测中获得真实的哈密顿函数。",
    "tldr": "本文提出一种在哈密顿神经网络中利用辛结构的改进训练方法，解放了损失函数的下界，使其可以学习到精确的哈密顿函数，从而提高了HNNs的可解释性。",
    "en_tdlr": "This paper proposes an improved training method for Hamiltonian Neural Networks (HNNs) that exploits the symplectic structure of Hamiltonian systems, freeing the loss function from an artificial lower bound and enabling the learning of an exact Hamiltonian function, thereby enhancing the explainability of HNNs."
}