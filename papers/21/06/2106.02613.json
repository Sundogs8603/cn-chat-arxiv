{
    "title": "Bridging the Gap Between Target Networks and Functional Regularization. (arXiv:2106.02613v4 [stat.ML] UPDATED)",
    "abstract": "Bootstrapping is behind much of the successes of deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. Despite the popularity of Target Networks, their effect on the optimization is still misunderstood. In this work, we show that they act as an implicit regularizer which can be beneficial in some cases, but also have disadvantages such as being inflexible and can result in instabilities, even when vanilla TD(0) converges. To overcome these issues, we propose an explicit Functional Regularization alternative that is flexible and a convex regularizer in function space and we theoretically study its convergence. We conduct an experimental study across a range of environments, discount factors, and off-policiness data collections to investigate the effectiveness o",
    "link": "http://arxiv.org/abs/2106.02613",
    "context": "Title: Bridging the Gap Between Target Networks and Functional Regularization. (arXiv:2106.02613v4 [stat.ML] UPDATED)\nAbstract: Bootstrapping is behind much of the successes of deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. Despite the popularity of Target Networks, their effect on the optimization is still misunderstood. In this work, we show that they act as an implicit regularizer which can be beneficial in some cases, but also have disadvantages such as being inflexible and can result in instabilities, even when vanilla TD(0) converges. To overcome these issues, we propose an explicit Functional Regularization alternative that is flexible and a convex regularizer in function space and we theoretically study its convergence. We conduct an experimental study across a range of environments, discount factors, and off-policiness data collections to investigate the effectiveness o",
    "path": "papers/21/06/2106.02613.json",
    "total_tokens": 951,
    "translated_title": "架起靶网络与功能正则化之间的鸿沟",
    "translated_abstract": "引导是深度强化学习成功的关键。然而，通过引导学习值函数往往导致训练不稳定，原因是目标值快速变化。靶网络通过使用额外的滞后参数集合来估计目标值，以稳定训练。尽管靶网络很受欢迎，但其对优化的影响仍未被理解。在这项工作中，我们展示了靶网络作为一个隐式正则化器的作用，它在某些情况下是有益的，但也存在一些缺点，如不灵活和可能导致不稳定，即使香草TD(0)收敛。为了克服这些问题，我们提出了一个明确的功能正则化替代方法，它在函数空间中是灵活和凸正则化器，并对其收敛性进行了理论研究。我们在一系列环境、折扣因子和非随机数据收集下进行了实验研究，以调查其有效性。",
    "tldr": "该论文研究了靶网络与功能正则化在深度强化学习中的作用。通过实验证明靶网络作为隐式正则化器在某些情况下有利，但不灵活且可能导致不稳定。为了解决这些问题，作者提出了一种明确的功能正则化替代方法，并对其收敛性进行了理论研究。"
}