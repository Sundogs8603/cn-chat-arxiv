{
    "title": "$C^3$: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v2 [cs.LG] UPDATED)",
    "abstract": "Video-grounded dialogue systems aim to integrate video understanding and dialogue understanding to generate responses that are relevant to both the dialogue and video context. Most existing approaches employ deep learning models and have achieved remarkable performance, given the relatively small datasets available. However, the results are partly accomplished by exploiting biases in the datasets rather than developing multimodal reasoning, resulting in limited generalization. In this paper, we propose a novel approach of Compositional Counterfactual Contrastive Learning ($C^3$) to develop contrastive training between factual and counterfactual samples in video-grounded dialogues. Specifically, we design factual/counterfactual sampling based on the temporal steps in videos and tokens in dialogues and propose contrastive loss functions that exploit object-level or action-level variance. Different from prior approaches, we focus on contrastive hidden state representations among compositi",
    "link": "http://arxiv.org/abs/2106.08914",
    "context": "Title: $C^3$: Compositional Counterfactual Contrastive Learning for Video-grounded Dialogues. (arXiv:2106.08914v2 [cs.LG] UPDATED)\nAbstract: Video-grounded dialogue systems aim to integrate video understanding and dialogue understanding to generate responses that are relevant to both the dialogue and video context. Most existing approaches employ deep learning models and have achieved remarkable performance, given the relatively small datasets available. However, the results are partly accomplished by exploiting biases in the datasets rather than developing multimodal reasoning, resulting in limited generalization. In this paper, we propose a novel approach of Compositional Counterfactual Contrastive Learning ($C^3$) to develop contrastive training between factual and counterfactual samples in video-grounded dialogues. Specifically, we design factual/counterfactual sampling based on the temporal steps in videos and tokens in dialogues and propose contrastive loss functions that exploit object-level or action-level variance. Different from prior approaches, we focus on contrastive hidden state representations among compositi",
    "path": "papers/21/06/2106.08914.json",
    "total_tokens": 888,
    "translated_title": "$C^3$: 用于视频对话的组合对抗对比学习",
    "translated_abstract": "视频对话系统旨在将视频理解和对话理解相结合，以生成与对话和视频上下文相关的回应。大多数现有方法采用深度学习模型，在相对较小的数据集条件下取得了显著的性能。然而，这些结果部分是通过利用数据集中的偏见而非发展多模态推理实现的，从而导致了有限的泛化能力。在本文中，我们提出了一种新颖的组合对抗对比学习（$C^3$）方法，以开发视频对话中关于事实和反事实样本的对比训练。具体而言，我们设计了基于视频中的时间步长和对话中的标记的事实/反事实采样，并提出了利用对象级或动作级变化的对比损失函数。与之前的方法不同，我们集中于对比隐藏状态表示。",
    "tldr": "本研究提出了一种名为$C^3$的新方法，通过对视频对话中的事实和反事实样本进行对比训练，以实现对于视频和对话上下文相关的回应生成。该方法利用了对象级或动作级的对比损失函数，旨在提高多模态推理能力和泛化能力。",
    "en_tdlr": "This paper proposes a novel approach called $C^3$ that employs contrastive training between factual and counterfactual samples in video-grounded dialogues. By utilizing contrastive loss functions at object or action levels, the method aims to enhance multimodal reasoning and generalization in generating responses relevant to both video and dialogue contexts."
}