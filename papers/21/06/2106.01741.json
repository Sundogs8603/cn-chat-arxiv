{
    "title": "Lifetime policy reuse and the importance of task capacity. (arXiv:2106.01741v3 [cs.LG] UPDATED)",
    "abstract": "A long-standing challenge in artificial intelligence is lifelong reinforcement learning, where learners are given many tasks in sequence and must transfer knowledge between tasks while avoiding catastrophic forgetting. Policy reuse and other multi-policy reinforcement learning techniques can learn multiple tasks but may generate many policies. This paper presents two novel contributions, namely 1) Lifetime Policy Reuse, a model-agnostic policy reuse algorithm that avoids generating many policies by optimising a fixed number of near-optimal policies through a combination of policy optimisation and adaptive policy selection; and 2) the task capacity, a measure for the maximal number of tasks that a policy can accurately solve. Comparing two state-of-the-art base-learners, the results demonstrate the importance of Lifetime Policy Reuse and task capacity based pre-selection on an 18-task partially observable Pacman domain and a Cartpole domain of up to 125 tasks.",
    "link": "http://arxiv.org/abs/2106.01741",
    "context": "Title: Lifetime policy reuse and the importance of task capacity. (arXiv:2106.01741v3 [cs.LG] UPDATED)\nAbstract: A long-standing challenge in artificial intelligence is lifelong reinforcement learning, where learners are given many tasks in sequence and must transfer knowledge between tasks while avoiding catastrophic forgetting. Policy reuse and other multi-policy reinforcement learning techniques can learn multiple tasks but may generate many policies. This paper presents two novel contributions, namely 1) Lifetime Policy Reuse, a model-agnostic policy reuse algorithm that avoids generating many policies by optimising a fixed number of near-optimal policies through a combination of policy optimisation and adaptive policy selection; and 2) the task capacity, a measure for the maximal number of tasks that a policy can accurately solve. Comparing two state-of-the-art base-learners, the results demonstrate the importance of Lifetime Policy Reuse and task capacity based pre-selection on an 18-task partially observable Pacman domain and a Cartpole domain of up to 125 tasks.",
    "path": "papers/21/06/2106.01741.json",
    "total_tokens": 925,
    "translated_title": "生命周期策略重用与任务容量的重要性",
    "translated_abstract": "人工智能领域一直存在一个挑战，即终身强化学习。在这种学习中，学习者按顺序接收多个任务，并在任务之间传递知识，同时避免灾难性遗忘。策略重用和其他多策略强化学习技术可以学习多个任务，但可能会生成大量的策略。本文提出了两个创新贡献，即1) 生命周期策略重用，这是一种模型无关的策略重用算法，通过策略优化和自适应策略选择的组合来优化一组近似最优策略，避免生成过多策略；2) 任务容量，一种衡量策略能准确解决的最大任务数量的指标。通过比较两种先进的基础学习模型，在一个包含18个部分可观察的Pacman任务和一个最多包含125个任务的Cartpole任务中，实验结果证明了生命周期策略重用和基于任务容量的预选的重要性。",
    "tldr": "本文提出了生命周期策略重用算法和任务容量指标，通过优化一组近似最优策略和策略选择，实现了在终身强化学习中避免生成大量策略的目标。实验证明了生命周期策略重用和任务容量预选对多任务学习的重要性。",
    "en_tdlr": "This paper presents a model-agnostic policy reuse algorithm called Lifetime Policy Reuse and introduces a measure called task capacity. These contributions optimize a fixed number of near-optimal policies and demonstrate their importance in avoiding policy explosion and improving multi-task learning in lifelong reinforcement learning."
}