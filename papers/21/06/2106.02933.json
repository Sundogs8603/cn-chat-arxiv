{
    "title": "k-Mixup Regularization for Deep Learning via Optimal Transport. (arXiv:2106.02933v2 [cs.LG] UPDATED)",
    "abstract": "Mixup is a popular regularization technique for training deep neural networks that improves generalization and increases robustness to certain distribution shifts. It perturbs input training data in the direction of other randomly-chosen instances in the training set. To better leverage the structure of the data, we extend mixup in a simple, broadly applicable way to \\emph{$k$-mixup}, which perturbs $k$-batches of training points in the direction of other $k$-batches. The perturbation is done with displacement interpolation, i.e. interpolation under the Wasserstein metric. We demonstrate theoretically and in simulations that $k$-mixup preserves cluster and manifold structures, and we extend theory studying the efficacy of standard mixup to the $k$-mixup case. Our empirical results show that training with $k$-mixup further improves generalization and robustness across several network architectures and benchmark datasets of differing modalities. For the wide variety of real datasets cons",
    "link": "http://arxiv.org/abs/2106.02933",
    "context": "Title: k-Mixup Regularization for Deep Learning via Optimal Transport. (arXiv:2106.02933v2 [cs.LG] UPDATED)\nAbstract: Mixup is a popular regularization technique for training deep neural networks that improves generalization and increases robustness to certain distribution shifts. It perturbs input training data in the direction of other randomly-chosen instances in the training set. To better leverage the structure of the data, we extend mixup in a simple, broadly applicable way to \\emph{$k$-mixup}, which perturbs $k$-batches of training points in the direction of other $k$-batches. The perturbation is done with displacement interpolation, i.e. interpolation under the Wasserstein metric. We demonstrate theoretically and in simulations that $k$-mixup preserves cluster and manifold structures, and we extend theory studying the efficacy of standard mixup to the $k$-mixup case. Our empirical results show that training with $k$-mixup further improves generalization and robustness across several network architectures and benchmark datasets of differing modalities. For the wide variety of real datasets cons",
    "path": "papers/21/06/2106.02933.json",
    "total_tokens": 961,
    "translated_title": "通过最优输运实现深度学习的k-Mixup正则化",
    "translated_abstract": "Mixup是一种流行的深度神经网络训练正则化技术，可以改善泛化性能并增加对特定分布偏移的鲁棒性。它通过将输入的训练数据沿着训练集中其他随机选择的实例的方向进行扰动。为了更好地利用数据的结构，我们以一种简单且广泛适用的方式将mixup扩展为k-mixup，它将k个批次的训练数据在其他k个批次的方向上进行扰动。扰动是通过最优输运下的位移插值实现的，即在Wasserstein度量下的插值。我们在理论上和模拟实验中证明了k-mixup可以保持聚类和流形结构，还扩展了对标准mixup在k-mixup情况下的有效性的研究。我们的实证结果表明，使用k-mixup进行训练可以进一步提高几种网络架构和不同模态的基准数据集的泛化性能和鲁棒性。",
    "tldr": "本文提出了一种基于最优输运的k-Mixup正则化方法，在训练深度神经网络时可以改善泛化性能并增加鲁棒性。通过对k个批次的训练数据进行扰动，该方法可以保持数据的聚类和流形结构，并在多种网络架构和数据集上得到了验证。"
}