{
    "title": "Entropy Regularized Reinforcement Learning Using Large Deviation Theory. (arXiv:2106.03931v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning (RL) is an important field of research in machine learning that is increasingly being applied to complex optimization problems in physics. In parallel, concepts from physics have contributed to important advances in RL with developments such as entropy-regularized RL. While these developments have led to advances in both fields, obtaining analytical solutions for optimization in entropy-regularized RL is currently an open problem. In this paper, we establish a mapping between entropy-regularized RL and research in non-equilibrium statistical mechanics focusing on Markovian processes conditioned on rare events. In the long-time limit, we apply approaches from large deviation theory to derive exact analytical results for the optimal policy and optimal dynamics in Markov Decision Process (MDP) models of reinforcement learning. The results obtained lead to a novel analytical and computational framework for entropy-regularized RL which is validated by simulations. The",
    "link": "http://arxiv.org/abs/2106.03931",
    "context": "Title: Entropy Regularized Reinforcement Learning Using Large Deviation Theory. (arXiv:2106.03931v2 [cs.LG] UPDATED)\nAbstract: Reinforcement learning (RL) is an important field of research in machine learning that is increasingly being applied to complex optimization problems in physics. In parallel, concepts from physics have contributed to important advances in RL with developments such as entropy-regularized RL. While these developments have led to advances in both fields, obtaining analytical solutions for optimization in entropy-regularized RL is currently an open problem. In this paper, we establish a mapping between entropy-regularized RL and research in non-equilibrium statistical mechanics focusing on Markovian processes conditioned on rare events. In the long-time limit, we apply approaches from large deviation theory to derive exact analytical results for the optimal policy and optimal dynamics in Markov Decision Process (MDP) models of reinforcement learning. The results obtained lead to a novel analytical and computational framework for entropy-regularized RL which is validated by simulations. The",
    "path": "papers/21/06/2106.03931.json",
    "total_tokens": 897,
    "translated_title": "利用大偏差理论的熵正则化强化学习",
    "translated_abstract": "强化学习是机器学习中一个重要的研究领域，越来越多地被应用于物理学中的复杂优化问题。同时，物理学中的概念也为强化学习带来了重大进展，如熵正则化强化学习。然而，针对熵正则化强化学习中优化的解析解目前是一个未解之谜。在本文中，我们建立了熵正则化强化学习与非平衡统计力学的联系，重点关注在罕见事件条件下的马尔可夫过程。在长时间极限下，我们应用大偏差理论的方法，推导出马尔可夫决策过程（MDP）模型中最优策略和最优动态的精确解析结果，从而得到了一个新的熵正则化强化学习的分析和计算框架，经过模拟验证。",
    "tldr": "本文利用大偏差理论建立了熵正则化强化学习与非平衡统计力学的联系，在长时间极限下推导出了马尔可夫决策过程模型中最优策略和最优动态的精确解析结果，并提出了新的分析和计算框架。"
}