{
    "title": "Manifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex. (arXiv:2106.02331v3 [eess.AS] UPDATED)",
    "abstract": "This paper presents a new deep clustering (DC) method called manifold-aware DC (M-DC) that can enhance hyperspace utilization more effectively than the original DC. The original DC has a limitation in that a pair of two speakers has to be embedded having an orthogonal relationship due to its use of the one-hot vector-based loss function, while our method derives a unique loss function aimed at maximizing the target angle in the hyperspace based on the nature of a regular simplex. Our proposed loss imposes a higher penalty than the original DC when the speaker is assigned incorrectly. The change from DC to M-DC can be easily achieved by rewriting just one term in the loss function of DC, without any other modifications to the network architecture or model parameters. As such, our method has high practicability because it does not affect the original inference part. The experimental results show that the proposed method improves the performances of the original DC and its expansion metho",
    "link": "http://arxiv.org/abs/2106.02331",
    "context": "Title: Manifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex. (arXiv:2106.02331v3 [eess.AS] UPDATED)\nAbstract: This paper presents a new deep clustering (DC) method called manifold-aware DC (M-DC) that can enhance hyperspace utilization more effectively than the original DC. The original DC has a limitation in that a pair of two speakers has to be embedded having an orthogonal relationship due to its use of the one-hot vector-based loss function, while our method derives a unique loss function aimed at maximizing the target angle in the hyperspace based on the nature of a regular simplex. Our proposed loss imposes a higher penalty than the original DC when the speaker is assigned incorrectly. The change from DC to M-DC can be easily achieved by rewriting just one term in the loss function of DC, without any other modifications to the network architecture or model parameters. As such, our method has high practicability because it does not affect the original inference part. The experimental results show that the proposed method improves the performances of the original DC and its expansion metho",
    "path": "papers/21/06/2106.02331.json",
    "total_tokens": 928,
    "translated_title": "基于正则简单形体的流形感知深度聚类: 最大化嵌入向量之间的角度",
    "translated_abstract": "本文提出了一种新的深度聚类（DC）方法，称为流形感知深度聚类（M-DC），它能够比原始的DC更有效地增强超空间利用率。原始的DC存在一个限制，即由于其使用基于One-hot向量的损失函数，两个说话者的嵌入必须具有正交关系，而我们的方法基于正则简单形体的特性，推导出一个旨在最大化超空间中目标角度的唯一损失函数。我们提出的损失函数在将说话者错误分配时比原始DC施加了更高的惩罚。从DC到M-DC的转变只需更改DC中损失函数的一个术语，而不需要对网络结构或模型参数进行任何其他修改。因此，我们的方法具有很高的实用性，因为它不会影响原始的推理部分。实验证明，所提出的方法改善了原始DC及其扩展方法的性能。",
    "tldr": "本文提出了一种名为流形感知深度聚类（M-DC）的新方法，它通过基于正则简单形体的唯一损失函数，最大化超空间中目标角度，从而在提高聚类性能时更有效地增强了超空间利用率。"
}