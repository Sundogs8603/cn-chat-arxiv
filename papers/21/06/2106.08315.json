{
    "title": "Decentralized Local Stochastic Extra-Gradient for Variational Inequalities. (arXiv:2106.08315v3 [math.OC] UPDATED)",
    "abstract": "We consider distributed stochastic variational inequalities (VIs) on unbounded domains with the problem data that is heterogeneous (non-IID) and distributed across many devices. We make a very general assumption on the computational network that, in particular, covers the settings of fully decentralized calculations with time-varying networks and centralized topologies commonly used in Federated Learning. Moreover, multiple local updates on the workers can be made for reducing the communication frequency between the workers. We extend the stochastic extragradient method to this very general setting and theoretically analyze its convergence rate in the strongly-monotone, monotone, and non-monotone (when a Minty solution exists) settings. The provided rates explicitly exhibit the dependence on network characteristics (e.g., mixing time), iteration counter, data heterogeneity, variance, number of devices, and other standard parameters. As a special case, our method and analysis apply to d",
    "link": "http://arxiv.org/abs/2106.08315",
    "context": "Title: Decentralized Local Stochastic Extra-Gradient for Variational Inequalities. (arXiv:2106.08315v3 [math.OC] UPDATED)\nAbstract: We consider distributed stochastic variational inequalities (VIs) on unbounded domains with the problem data that is heterogeneous (non-IID) and distributed across many devices. We make a very general assumption on the computational network that, in particular, covers the settings of fully decentralized calculations with time-varying networks and centralized topologies commonly used in Federated Learning. Moreover, multiple local updates on the workers can be made for reducing the communication frequency between the workers. We extend the stochastic extragradient method to this very general setting and theoretically analyze its convergence rate in the strongly-monotone, monotone, and non-monotone (when a Minty solution exists) settings. The provided rates explicitly exhibit the dependence on network characteristics (e.g., mixing time), iteration counter, data heterogeneity, variance, number of devices, and other standard parameters. As a special case, our method and analysis apply to d",
    "path": "papers/21/06/2106.08315.json",
    "total_tokens": 926,
    "translated_title": "变分不等式的分布式本地随机额外梯度算法",
    "translated_abstract": "本文研究了非有界域上非IID分布式随机变分不等式问题。我们对计算网络进行了非常一般的假设，包括具有时变网络的完全分散计算和在联邦学习中常用的集中拓扑。另外，可以对节点进行多个本地更新以减少节点之间的通信频率。我们将随机额外梯度方法扩展到这个非常普遍的设置中，并在强单调、单调和非单调的情况下（当Minty解存在时）进行理论分析其收敛速度。提供的速率明确展示了网络特征（例如混合时间）、迭代计数器、数据异质性、方差、设备数量和其他标准参数的依赖关系。特别地，我们的方法和分析可应用于开发具有非IID数据的联邦学习的分散算法。",
    "tldr": "本文研究了非有界域上非IID分布式随机变分不等式问题，在分散的计算网络中使用随机额外梯度方法，在强单调、单调和非单调的情况下分别分析了收敛速度，并将其应用于开发具有非IID数据的联邦学习的分散算法。",
    "en_tdlr": "This paper proposes a decentralized local stochastic extra-gradient algorithm for distributed stochastic variational inequalities on unbounded domains with heterogeneous and distributed problem data. The method is applicable to developing decentralized algorithms for Federated Learning with non-IID data and the provided convergence rates are analyzed in the strongly-monotone, monotone, and non-monotone settings."
}