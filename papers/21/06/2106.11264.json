{
    "title": "Compositional federated learning: Applications in distributionally robust averaging and meta learning. (arXiv:2106.11264v3 [cs.LG] UPDATED)",
    "abstract": "In the paper, we propose an effective and efficient Compositional Federated Learning (ComFedL) algorithm for solving a new compositional Federated Learning (FL) framework, which frequently appears in many data mining and machine learning problems with a hierarchical structure such as distributionally robust FL and model-agnostic meta learning (MAML). Moreover, we study the convergence analysis of our ComFedL algorithm under some mild conditions, and prove that it achieves a convergence rate of $O(\\frac{1}{\\sqrt{T}})$, where $T$ denotes the number of iteration. To the best of our knowledge, our new Compositional FL framework is the first work to bridge federated learning with composition stochastic optimization. In particular, we first transform the distributionally robust FL (i.e., a minimax optimization problem) into a simple composition optimization problem by using KL divergence regularization. At the same time, we also first transform the distribution-agnostic MAML problem (i.e., a",
    "link": "http://arxiv.org/abs/2106.11264",
    "context": "Title: Compositional federated learning: Applications in distributionally robust averaging and meta learning. (arXiv:2106.11264v3 [cs.LG] UPDATED)\nAbstract: In the paper, we propose an effective and efficient Compositional Federated Learning (ComFedL) algorithm for solving a new compositional Federated Learning (FL) framework, which frequently appears in many data mining and machine learning problems with a hierarchical structure such as distributionally robust FL and model-agnostic meta learning (MAML). Moreover, we study the convergence analysis of our ComFedL algorithm under some mild conditions, and prove that it achieves a convergence rate of $O(\\frac{1}{\\sqrt{T}})$, where $T$ denotes the number of iteration. To the best of our knowledge, our new Compositional FL framework is the first work to bridge federated learning with composition stochastic optimization. In particular, we first transform the distributionally robust FL (i.e., a minimax optimization problem) into a simple composition optimization problem by using KL divergence regularization. At the same time, we also first transform the distribution-agnostic MAML problem (i.e., a",
    "path": "papers/21/06/2106.11264.json",
    "total_tokens": 995,
    "translated_title": "组合式联邦学习：在分布鲁棒平均和元学习中的应用",
    "translated_abstract": "在本文中，我们提出了一种有效且高效的组合式联邦学习（ComFedL）算法，用于解决具有层次结构的许多数据挖掘和机器学习问题，如分布鲁棒联邦学习和模型不可知元学习（MAML）。此外，我们对我们的ComFedL算法进行了收敛性分析，在一些温和的条件下证明了它达到了$O(\\frac{1}{\\sqrt{T}})$的收敛速度，其中$T$表示迭代次数。据我们所知，我们的新的组合式联邦学习框架是首个将联邦学习与组合随机优化相结合的工作。特别地，我们首先通过使用KL散度正则化将分布鲁棒联邦学习（即极小极大优化问题）转化为简单的组合优化问题。同时，我们还首次将分布不可知的MAML问题（即一个",
    "tldr": "本文提出了一种名为ComFedL的组合式联邦学习算法，用于解决分布鲁棒联邦学习和模型不可知元学习问题，通过使用KL散度正则化，将分布鲁棒联邦学习转化为简单的组合优化问题。已证明ComFedL算法具有收敛速度为O(1/√T)，创新性地将联邦学习与组合随机优化结合在一起。",
    "en_tdlr": "This paper presents a compositional federated learning (ComFedL) algorithm, which effectively tackles distributionally robust federated learning and model-agnostic meta learning problems. By using KL divergence regularization, the algorithm transforms these problems into simpler composition optimization problems and achieves a convergence rate of O(1/√T), bridging the gap between federated learning and composition stochastic optimization."
}