{
    "title": "NoiseGrad: Enhancing Explanations by Introducing Stochasticity to Model Weights. (arXiv:2106.10185v3 [cs.LG] CROSS LISTED)",
    "abstract": "Many efforts have been made for revealing the decision-making process of black-box learning machines such as deep neural networks, resulting in useful local and global explanation methods. For local explanation, stochasticity is known to help: a simple method, called SmoothGrad, has improved the visual quality of gradient-based attribution by adding noise to the input space and averaging the explanations of the noisy inputs. In this paper, we extend this idea and propose NoiseGrad that enhances both local and global explanation methods. Specifically, NoiseGrad introduces stochasticity in the weight parameter space, such that the decision boundary is perturbed. NoiseGrad is expected to enhance the local explanation, similarly to SmoothGrad, due to the dual relationship between the input perturbation and the decision boundary perturbation. We evaluate NoiseGrad and its fusion with SmoothGrad -FusionGrad -- qualitatively and quantitatively with several evaluation criteria, and show that",
    "link": "http://arxiv.org/abs/2106.10185",
    "context": "Title: NoiseGrad: Enhancing Explanations by Introducing Stochasticity to Model Weights. (arXiv:2106.10185v3 [cs.LG] CROSS LISTED)\nAbstract: Many efforts have been made for revealing the decision-making process of black-box learning machines such as deep neural networks, resulting in useful local and global explanation methods. For local explanation, stochasticity is known to help: a simple method, called SmoothGrad, has improved the visual quality of gradient-based attribution by adding noise to the input space and averaging the explanations of the noisy inputs. In this paper, we extend this idea and propose NoiseGrad that enhances both local and global explanation methods. Specifically, NoiseGrad introduces stochasticity in the weight parameter space, such that the decision boundary is perturbed. NoiseGrad is expected to enhance the local explanation, similarly to SmoothGrad, due to the dual relationship between the input perturbation and the decision boundary perturbation. We evaluate NoiseGrad and its fusion with SmoothGrad -FusionGrad -- qualitatively and quantitatively with several evaluation criteria, and show that",
    "path": "papers/21/06/2106.10185.json",
    "total_tokens": 715,
    "translated_title": "NoiseGrad：通过引入模型权重的随机变化来增强解释性",
    "translated_abstract": "近年来，针对黑匣子学习机的决策过程，如深度神经网络，已经进行了很多工作，从而产生了有用的局部和全局解释方法。本文提出了NoiseGrad，通过在权重参数空间中引入随机性，从而扰动决策边界，增强了局部和全局解释方法。我们将NoiseGrad与其与SmoothGrad的融合方法（FusionGrad）进行了定量和定性评估，并展示了其性能。",
    "tldr": "本文提出了NoiseGrad方法，通过引入模型权重的随机变化扰动决策边界来增强深度神经网络模型的局部和全局解释方法。",
    "en_tdlr": "This paper proposes the NoiseGrad method to enhance both local and global explanations of deep neural network models by introducing stochasticity in the weight parameter space to perturb the decision boundary."
}