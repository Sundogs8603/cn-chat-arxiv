{
    "title": "Linear Convergence of Entropy-Regularized Natural Policy Gradient with Linear Function Approximation",
    "abstract": "Natural policy gradient (NPG) methods with entropy regularization achieve impressive empirical success in reinforcement learning problems with large state-action spaces. However, their convergence properties and the impact of entropy regularization remain elusive in the function approximation regime. In this paper, we establish finite-time convergence analyses of entropy-regularized NPG with linear function approximation under softmax parameterization. In particular, we prove that entropy-regularized NPG with averaging satisfies the \\emph{persistence of excitation} condition, and achieves a fast convergence rate of $\\tilde{O}(1/T)$ up to a function approximation error in regularized Markov decision processes. This convergence result does not require any a priori assumptions on the policies. Furthermore, under mild regularity conditions on the concentrability coefficient and basis vectors, we prove that entropy-regularized NPG exhibits \\emph{linear convergence} up to a function approxim",
    "link": "https://arxiv.org/abs/2106.04096",
    "context": "Title: Linear Convergence of Entropy-Regularized Natural Policy Gradient with Linear Function Approximation\nAbstract: Natural policy gradient (NPG) methods with entropy regularization achieve impressive empirical success in reinforcement learning problems with large state-action spaces. However, their convergence properties and the impact of entropy regularization remain elusive in the function approximation regime. In this paper, we establish finite-time convergence analyses of entropy-regularized NPG with linear function approximation under softmax parameterization. In particular, we prove that entropy-regularized NPG with averaging satisfies the \\emph{persistence of excitation} condition, and achieves a fast convergence rate of $\\tilde{O}(1/T)$ up to a function approximation error in regularized Markov decision processes. This convergence result does not require any a priori assumptions on the policies. Furthermore, under mild regularity conditions on the concentrability coefficient and basis vectors, we prove that entropy-regularized NPG exhibits \\emph{linear convergence} up to a function approxim",
    "path": "papers/21/06/2106.04096.json",
    "total_tokens": 871,
    "translated_title": "熵正则化的自然策略梯度与线性函数逼近的线性收敛性",
    "translated_abstract": "自然策略梯度（NPG）方法在具有大状态-动作空间的强化学习问题中通过熵正则化取得了令人瞩目的实证成功。然而，在函数逼近的情况下，它们的收敛性质和熵正则化的影响仍然不明确。本文在softmax参数化下，建立了熵正则化的NPG与线性函数逼近的有限时间收敛分析。特别地，我们证明了熵正则化的NPG通过平均满足“激励持久性”条件，并在正则化马尔可夫决策过程中以$\\tilde{O}(1/T)$的快速收敛速率达到函数逼近误差。这个收敛结果不需要对策略进行任何先验假设。此外，在浓度系数和基向量的轻微正则性条件下，我们证明了熵正则化的NPG在函数逼近误差范围内呈现“线性收敛”。",
    "tldr": "本文研究了具有线性函数逼近的熵正则化自然策略梯度的收敛性质，证明了其在正则化马尔可夫决策过程中具有线性收敛性以及快速的收敛速率。"
}