{
    "title": "Defending Against Backdoor Attacks in Natural Language Generation. (arXiv:2106.01810v3 [cs.CL] UPDATED)",
    "abstract": "The frustratingly fragile nature of neural network models make current natural language generation (NLG) systems prone to backdoor attacks and generate malicious sequences that could be sexist or offensive. Unfortunately, little effort has been invested to how backdoor attacks can affect current NLG models and how to defend against these attacks. In this work, by giving a formal definition of backdoor attack and defense, we investigate this problem on two important NLG tasks, machine translation and dialog generation. Tailored to the inherent nature of NLG models (e.g., producing a sequence of coherent words given contexts), we design defending strategies against attacks. We find that testing the backward probability of generating sources given targets yields effective defense performance against all different types of attacks, and is able to handle the {\\it one-to-many} issue in many NLG tasks such as dialog generation. We hope that this work can raise the awareness of backdoor risks ",
    "link": "http://arxiv.org/abs/2106.01810",
    "context": "Title: Defending Against Backdoor Attacks in Natural Language Generation. (arXiv:2106.01810v3 [cs.CL] UPDATED)\nAbstract: The frustratingly fragile nature of neural network models make current natural language generation (NLG) systems prone to backdoor attacks and generate malicious sequences that could be sexist or offensive. Unfortunately, little effort has been invested to how backdoor attacks can affect current NLG models and how to defend against these attacks. In this work, by giving a formal definition of backdoor attack and defense, we investigate this problem on two important NLG tasks, machine translation and dialog generation. Tailored to the inherent nature of NLG models (e.g., producing a sequence of coherent words given contexts), we design defending strategies against attacks. We find that testing the backward probability of generating sources given targets yields effective defense performance against all different types of attacks, and is able to handle the {\\it one-to-many} issue in many NLG tasks such as dialog generation. We hope that this work can raise the awareness of backdoor risks ",
    "path": "papers/21/06/2106.01810.json",
    "total_tokens": 940,
    "translated_title": "防御自然语言生成中的后门攻击",
    "translated_abstract": "神经网络模型的脆弱性使得当前的自然语言生成系统易受后门攻击，生成可能包含性别歧视或冒犯性的恶意序列。然而，目前对后门攻击如何影响当前自然语言生成模型以及如何防御这些攻击的研究还很有限。在本研究中，我们通过给出后门攻击和防御的形式化定义，在机器翻译和对话生成这两个重要的自然语言生成任务上进行了调查。根据自然语言生成模型的固有特性（例如，在给定上下文的情况下生成连贯单词序列），我们设计了针对攻击的防御策略。我们发现，在生成目标给定源的反向概率测试中，能够有效防御所有不同类型的攻击，并能够处理对话生成等许多自然语言生成任务中的“一对多”问题。我们希望这项工作能提高对后门攻击风险的认识。",
    "tldr": "本研究通过形式化的定义，研究了后门攻击对自然语言生成模型的影响，并设计了针对这些攻击的防御策略。测试生成目标给定源的反向概率可以有效防御各种攻击类型，并解决了在对话生成等自然语言生成任务中的“一对多”问题。",
    "en_tdlr": "This work investigates the impact of backdoor attacks on natural language generation models and designs defense strategies tailored to these attacks. Testing the backward probability of generating sources given targets proves to be an effective defense method against various attack types and resolves the \"one-to-many\" issue in tasks such as dialog generation."
}