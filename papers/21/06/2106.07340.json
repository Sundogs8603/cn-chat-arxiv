{
    "title": "MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v5 [cs.CL] UPDATED)",
    "abstract": "Since the introduction of the original BERT (i.e., BASE BERT), researchers have developed various customized BERT models with improved performance for specific domains and tasks by exploiting the benefits of transfer learning. Due to the nature of mathematical texts, which often use domain specific vocabulary along with equations and math symbols, we posit that the development of a new BERT model for mathematics would be useful for many mathematical downstream tasks. In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content. In addition, we select three general NLP tasks that are often used in mathematics education: prediction of knowledge component, auto-grading open-ended Q&A, and knowledge tr",
    "link": "http://arxiv.org/abs/2106.07340",
    "context": "Title: MathBERT: A Pre-trained Language Model for General NLP Tasks in Mathematics Education. (arXiv:2106.07340v5 [cs.CL] UPDATED)\nAbstract: Since the introduction of the original BERT (i.e., BASE BERT), researchers have developed various customized BERT models with improved performance for specific domains and tasks by exploiting the benefits of transfer learning. Due to the nature of mathematical texts, which often use domain specific vocabulary along with equations and math symbols, we posit that the development of a new BERT model for mathematics would be useful for many mathematical downstream tasks. In this resource paper, we introduce our multi-institutional effort (i.e., two learning platforms and three academic institutions in the US) toward this need: MathBERT, a model created by pre-training the BASE BERT model on a large mathematical corpus ranging from pre-kindergarten (pre-k), to high-school, to college graduate level mathematical content. In addition, we select three general NLP tasks that are often used in mathematics education: prediction of knowledge component, auto-grading open-ended Q&A, and knowledge tr",
    "path": "papers/21/06/2106.07340.json",
    "total_tokens": 881,
    "translated_title": "MathBERT: 一种用于数学教育中的通用自然语言处理任务的预训练语言模型",
    "translated_abstract": "自从原始的BERT（即BASE BERT）的引入以来，研究人员通过利用迁移学习的优势，开发了各种定制的BERT模型来改进特定领域和任务的性能。由于数学文本的性质，经常使用领域特定的词汇以及方程和数学符号，我们认为开发一个针对数学的新BERT模型将对许多数学下游任务有用。在这篇资源论文中，我们介绍了我们的多机构努力（即两个学习平台和三个美国学术机构）以满足这个需求：MathBERT，一个通过在大规模数学语料库上对BASE BERT模型进行预训练而创建的模型，该语料库涵盖了从学前教育（pre-k）到高中以及研究生水平的数学内容。此外，我们选择了三个常用于数学教育的通用NLP任务：知识组件预测，自动评分开放性问题和知识追溯。",
    "tldr": "MathBERT是一个基于BASE BERT模型在大规模数学语料库上进行预训练的模型，为数学教育中的通用NLP任务提供了新的解决方案。",
    "en_tdlr": "MathBERT is a model created by pre-training the BASE BERT model on a large mathematical corpus, providing a new solution for general NLP tasks in mathematics education."
}