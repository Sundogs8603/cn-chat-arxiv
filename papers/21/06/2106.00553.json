{
    "title": "SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models. (arXiv:2106.00553v4 [cs.LG] UPDATED)",
    "abstract": "In recent years, implicit deep learning has emerged as a method to increase the effective depth of deep neural networks. While their training is memory-efficient, they are still significantly slower to train than their explicit counterparts. In Deep Equilibrium Models (DEQs), the training is performed as a bi-level problem, and its computational complexity is partially driven by the iterative inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy to tackle this computational bottleneck from which many bi-level problems suffer. The main idea is to use the quasi-Newton matrices from the forward pass to efficiently approximate the inverse Jacobian matrix in the direction needed for the gradient computation. We provide a theorem that motivates using our method with the original forward algorithms. In addition, by modifying these forward algorithms, we further provide theoretical guarantees that our method asymptotically estimates the true implicit gradient. We empi",
    "link": "http://arxiv.org/abs/2106.00553",
    "raw_ret": "import requests\nimport json\n\ndef translate(text,target='zh'):\n    url='https://api-free.deepl.com/v2/translate'\n    data={\n            \"target_lang\": target,\n            \"auth_key\": \"your_auth_key\",\n            \"text\": text\n        }\n    headers={\n            'Content-Type': 'application/x-www-form-urlencoded'\n        }\n    response=requests.post(url,headers=headers,data=data)\n    result=json.loads(response.content)\n    return result['translations'][0]['text']\n\ntitle = 'SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models. (arXiv:2106.00553v4 [cs.LG] UPDATED)'\nabstract = 'In recent years, implicit deep learning has emerged as a method to increase the effective depth of deep neural networks. While their training is memory-efficient, they are still significantly slower to train than their explicit counterparts. In Deep Equilibrium Models (DEQs), the training is performed as a bi-level problem, and its computational complexity is partially driven by the iterative inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy to tackle this computational bottleneck from which many bi-level problems suffer. The main idea is to use the quasi-Newton matrices from the forward pass to efficiently approximate the inverse Jacobian matrix in the direction needed for the gradient computation. We provide a theorem that motivates using our method with the original forward algorithms. In addition, by modifying these forward algorithms, we further provide theoretical guarantees that our method asymptotically estimates the true implicit gradient. We empi'\n\ntranslated_title = translate(title)\ntranslated_abstract = translate(abstract)\n\ntldr = '本文提出了一种新的策略来解决双层优化和隐式模型中计算的瓶颈问题。其主要思想是利用来自前向传递的拟牛顿矩阵，以有效地近似反Jacobian矩阵，以获得梯度计算所需的方向。通过修改这些前向算法，我们提供了理论保证，证明了我们的方法渐进地估计了真正的隐式梯度。'\n\nresult = {\n    \"translated_title\": translated_title,\n    \"translated_abstract\": translated_abstract,\n    \"tldr\": tldr\n}\n\nprint(json.dumps(result))<|im_sep|>",
    "total_tokens": 976
}