{
    "title": "How to Evaluate Uncertainty Estimates in Machine Learning for Regression?. (arXiv:2106.03395v2 [stat.ML] UPDATED)",
    "abstract": "As neural networks become more popular, the need for accompanying uncertainty estimates increases. There are currently two main approaches to test the quality of these estimates. Most methods output a density. They can be compared by evaluating their loglikelihood on a test set. Other methods output a prediction interval directly. These methods are often tested by examining the fraction of test points that fall inside the corresponding prediction intervals. Intuitively both approaches seem logical. However, we demonstrate through both theoretical arguments and simulations that both ways of evaluating the quality of uncertainty estimates have serious flaws. Firstly, both approaches cannot disentangle the separate components that jointly create the predictive uncertainty, making it difficult to evaluate the quality of the estimates of these components. Secondly, a better loglikelihood does not guarantee better prediction intervals, which is what the methods are often used for in practice",
    "link": "http://arxiv.org/abs/2106.03395",
    "context": "Title: How to Evaluate Uncertainty Estimates in Machine Learning for Regression?. (arXiv:2106.03395v2 [stat.ML] UPDATED)\nAbstract: As neural networks become more popular, the need for accompanying uncertainty estimates increases. There are currently two main approaches to test the quality of these estimates. Most methods output a density. They can be compared by evaluating their loglikelihood on a test set. Other methods output a prediction interval directly. These methods are often tested by examining the fraction of test points that fall inside the corresponding prediction intervals. Intuitively both approaches seem logical. However, we demonstrate through both theoretical arguments and simulations that both ways of evaluating the quality of uncertainty estimates have serious flaws. Firstly, both approaches cannot disentangle the separate components that jointly create the predictive uncertainty, making it difficult to evaluate the quality of the estimates of these components. Secondly, a better loglikelihood does not guarantee better prediction intervals, which is what the methods are often used for in practice",
    "path": "papers/21/06/2106.03395.json",
    "total_tokens": 845,
    "translated_title": "如何评估机器学习回归中的不确定性估计？",
    "translated_abstract": "随着神经网络的普及，对于相应的不确定性估计的需求也越来越大。目前有两种主要的方法来测试这些估计的质量。大多数方法输出一个概率密度，可以通过在测试集上评估其对数似然来进行比较。其他方法直接输出一个预测区间，通常通过检查落入相应预测区间的测试点的比例来进行测试。直观上看，这两种方法都是合理的。然而，我们通过理论论证和模拟实验表明，评估不确定性估计质量的这两种方式都存在严重缺陷。首先，这两种方法无法分离共同产生预测不确定性的各个组成部分，从而难以评估这些组成部分的估计质量。其次，更好的对数似然并不保证更好的预测区间，而这通常是这些方法在实践中所用的。",
    "tldr": "本文研究了如何评估机器学习回归中的不确定性估计，发现目前的评估方法存在严重缺陷，无法准确评估估计质量和预测区间的关系。",
    "en_tdlr": "This study investigates how to evaluate uncertainty estimates in machine learning regression and reveals serious flaws in the current evaluation methods, which make it difficult to accurately assess the quality of the estimates and their relationship to prediction intervals."
}