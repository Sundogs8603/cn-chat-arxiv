{
    "title": "Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v3 [cs.LG] UPDATED)",
    "abstract": "Deep ensembles have recently gained popularity in the deep learning community for their conceptual simplicity and efficiency. However, maintaining functional diversity between ensemble members that are independently trained with gradient descent is challenging. This can lead to pathologies when adding more ensemble members, such as a saturation of the ensemble performance, which converges to the performance of a single model. Moreover, this does not only affect the quality of its predictions, but even more so the uncertainty estimates of the ensemble, and thus its performance on out-of-distribution data. We hypothesize that this limitation can be overcome by discouraging different ensemble members from collapsing to the same function. To this end, we introduce a kernelized repulsive term in the update rule of the deep ensembles. We show that this simple modification not only enforces and maintains diversity among the members but, even more importantly, transforms the maximum a posterio",
    "link": "http://arxiv.org/abs/2106.11642",
    "context": "Title: Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v3 [cs.LG] UPDATED)\nAbstract: Deep ensembles have recently gained popularity in the deep learning community for their conceptual simplicity and efficiency. However, maintaining functional diversity between ensemble members that are independently trained with gradient descent is challenging. This can lead to pathologies when adding more ensemble members, such as a saturation of the ensemble performance, which converges to the performance of a single model. Moreover, this does not only affect the quality of its predictions, but even more so the uncertainty estimates of the ensemble, and thus its performance on out-of-distribution data. We hypothesize that this limitation can be overcome by discouraging different ensemble members from collapsing to the same function. To this end, we introduce a kernelized repulsive term in the update rule of the deep ensembles. We show that this simple modification not only enforces and maintains diversity among the members but, even more importantly, transforms the maximum a posterio",
    "path": "papers/21/06/2106.11642.json",
    "total_tokens": 857,
    "translated_title": "拒绝性深度集成是贝叶斯的",
    "translated_abstract": "深度集成因其概念上的简单和高效而受到深度学习界的欢迎。然而，对于独立使用梯度下降训练的集成成员之间的功能多样性的维护是具有挑战性的。这可能会导致添加更多集成成员时出现病态，例如集成性能的饱和，它会收敛到单个模型的性能。此外，这不仅影响其预测的质量，而且更加影响集成的不确定性估计，从而影响其在超出分布数据上的性能。我们假设这种限制可以通过阻止不同集成成员坍塌到相同功能来克服。为此，我们在深度集成的更新规则中引入了一个核化的排斥术语。我们表明，这个简单的修改不仅强制并维护成员之间的多样性，而且更重要的是，将最大的后验值转化为...（原文截止此处）",
    "tldr": "通过在深度集成的更新规则中引入核化排斥项，可以强制并维护成员之间的多样性，并使集成具有更好的性能表现和不确定性估计。",
    "en_tdlr": "By introducing a kernelized repulsive term in the update rule of deep ensembles, it enforces and maintains diversity among the members and improves the ensemble's performance and uncertainty estimates."
}