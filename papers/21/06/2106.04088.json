{
    "title": "A Lightweight and Gradient-Stable Nerual Layer. (arXiv:2106.04088v2 [cs.LG] UPDATED)",
    "abstract": "We propose a neural-layer architecture based on Householder weighting and absolute-value activating, hence called Householder-absolute neural layer or simply Han-layer. Compared to a fully-connected layer with $d$-neurons and $d$ outputs, a Han-layer reduces the number of parameters and the corresponding complexity from $O(d^2)$ to $O(d)$. The Han-layer structure guarantees two desirable properties: (1) gradient stability (free of vanishing or exploding gradient), and (2) 1-Lipschitz continuity. Extensive numerical experiments show that one can strategically use Han-layers to replace fully-connected (FC) layers, reducing the number of model parameters while maintaining or even improving the generalization performance. We will showcase the capabilities of the Han-layer architecture on a few small stylized models, and also discuss its current limitations.",
    "link": "http://arxiv.org/abs/2106.04088",
    "context": "Title: A Lightweight and Gradient-Stable Nerual Layer. (arXiv:2106.04088v2 [cs.LG] UPDATED)\nAbstract: We propose a neural-layer architecture based on Householder weighting and absolute-value activating, hence called Householder-absolute neural layer or simply Han-layer. Compared to a fully-connected layer with $d$-neurons and $d$ outputs, a Han-layer reduces the number of parameters and the corresponding complexity from $O(d^2)$ to $O(d)$. The Han-layer structure guarantees two desirable properties: (1) gradient stability (free of vanishing or exploding gradient), and (2) 1-Lipschitz continuity. Extensive numerical experiments show that one can strategically use Han-layers to replace fully-connected (FC) layers, reducing the number of model parameters while maintaining or even improving the generalization performance. We will showcase the capabilities of the Han-layer architecture on a few small stylized models, and also discuss its current limitations.",
    "path": "papers/21/06/2106.04088.json",
    "total_tokens": 793,
    "translated_title": "一种轻量级且梯度稳定的神经层",
    "translated_abstract": "我们提出了一种基于Householder权重和绝对值激活的神经层结构，因此被称为Householder-absolute神经层或简称Han层。与具有$d$个神经元和$d$个输出的全连接层相比，Han层将参数数量和相应的复杂度从$O（d ^ 2）$降低到$O（d）$。Han层结构保证了两个理想属性：（1）梯度稳定性（不会出现梯度消失或梯度爆炸），以及（2）1-Lipschitz连续性。广泛的数值实验表明，可以有策略地使用Han层替换全连接（FC）层，从而减少模型参数的数量，同时保持或甚至提高泛化性能。我们将展示Han层结构在一些小型化的模型上的能力，同时讨论其当前的限制。",
    "tldr": "Han层是一种梯度稳定、参数更少的神经层结构，可以替换全连接层来优化神经网络模型。"
}