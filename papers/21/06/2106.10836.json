{
    "title": "Active Learning for Deep Neural Networks on Edge Devices. (arXiv:2106.10836v2 [cs.LG] UPDATED)",
    "abstract": "When dealing with deep neural network (DNN) applications on edge devices, continuously updating the model is important. Although updating a model with real incoming data is ideal, using all of them is not always feasible due to limits, such as labeling and communication costs. Thus, it is necessary to filter and select the data to use for training (i.e., active learning) on the device. In this paper, we formalize a practical active learning problem for DNNs on edge devices and propose a general task-agnostic framework to tackle this problem, which reduces it to a stream submodular maximization. This framework is light enough to be run with low computational resources, yet provides solutions whose quality is theoretically guaranteed thanks to the submodular property. Through this framework, we can configure data selection criteria flexibly, including using methods proposed in previous active learning studies. We evaluate our approach on both classification and object detection tasks in ",
    "link": "http://arxiv.org/abs/2106.10836",
    "context": "Title: Active Learning for Deep Neural Networks on Edge Devices. (arXiv:2106.10836v2 [cs.LG] UPDATED)\nAbstract: When dealing with deep neural network (DNN) applications on edge devices, continuously updating the model is important. Although updating a model with real incoming data is ideal, using all of them is not always feasible due to limits, such as labeling and communication costs. Thus, it is necessary to filter and select the data to use for training (i.e., active learning) on the device. In this paper, we formalize a practical active learning problem for DNNs on edge devices and propose a general task-agnostic framework to tackle this problem, which reduces it to a stream submodular maximization. This framework is light enough to be run with low computational resources, yet provides solutions whose quality is theoretically guaranteed thanks to the submodular property. Through this framework, we can configure data selection criteria flexibly, including using methods proposed in previous active learning studies. We evaluate our approach on both classification and object detection tasks in ",
    "path": "papers/21/06/2106.10836.json",
    "total_tokens": 865,
    "translated_title": "边缘设备上的深度神经网络主动学习",
    "translated_abstract": "当处理边缘设备上的深度神经网络应用时，持续更新模型非常重要。尽管使用实时数据来更新模型是理想的，但由于标记和通信成本等限制，并不总是可行的。因此，有必要在设备上过滤和选择用于训练的数据（即主动学习）。本文规范了边缘设备上DNN的实际主动学习问题，并提出了一个通用的任务无关框架来解决这个问题，将其减少到流子模块最大化。这个框架足够轻便，可以用低计算资源运行，但由于子模块性质，提供其质量在理论上得到保证的解。通过这个框架，我们可以灵活地配置数据选择标准，包括使用以前的主动学习研究中提出的方法。我们在分类和目标检测任务中评估了我们的方法。",
    "tldr": "本研究提出了一个通用的任务无关框架，用于边缘设备上的深度神经网络的主动学习问题，可以有效减少标记和通信成本，并保证解的质量。",
    "en_tdlr": "This paper proposes a general task-agnostic framework for active learning of deep neural networks on edge devices, which reduces the labeling and communication costs and guarantees the quality of solutions, and it has been evaluated on classification and object detection tasks."
}