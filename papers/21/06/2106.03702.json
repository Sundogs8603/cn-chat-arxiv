{
    "title": "Can a single neuron learn predictive uncertainty?. (arXiv:2106.03702v3 [stat.ML] UPDATED)",
    "abstract": "Uncertainty estimation methods using deep learning approaches strive against separating how uncertain the state of the world manifests to us via measurement (objective end) from the way this gets scrambled with the model specification and training procedure used to predict such state (subjective means) -- e.g., number of neurons, depth, connections, priors (if the model is bayesian), weight initialization, etc. This poses the question of the extent to which one can eliminate the degrees of freedom associated with these specifications and still being able to capture the objective end. Here, a novel non-parametric quantile estimation method for continuous random variables is introduced, based on the simplest neural network architecture with one degree of freedom: a single neuron. Its advantage is first shown in synthetic experiments comparing with the quantile estimation achieved from ranking the order statistics (specifically for small sample size) and with quantile regression. In real-",
    "link": "http://arxiv.org/abs/2106.03702",
    "context": "Title: Can a single neuron learn predictive uncertainty?. (arXiv:2106.03702v3 [stat.ML] UPDATED)\nAbstract: Uncertainty estimation methods using deep learning approaches strive against separating how uncertain the state of the world manifests to us via measurement (objective end) from the way this gets scrambled with the model specification and training procedure used to predict such state (subjective means) -- e.g., number of neurons, depth, connections, priors (if the model is bayesian), weight initialization, etc. This poses the question of the extent to which one can eliminate the degrees of freedom associated with these specifications and still being able to capture the objective end. Here, a novel non-parametric quantile estimation method for continuous random variables is introduced, based on the simplest neural network architecture with one degree of freedom: a single neuron. Its advantage is first shown in synthetic experiments comparing with the quantile estimation achieved from ranking the order statistics (specifically for small sample size) and with quantile regression. In real-",
    "path": "papers/21/06/2106.03702.json",
    "total_tokens": 993,
    "translated_title": "单个神经元能否学习预测不确定性？",
    "translated_abstract": "采用深度学习方法进行的不确定性估计旨在分离我们通过测量所观察到的世界状态的不确定性（客观终点）与模型规范和训练过程用于预测这种状态的方式相混淆的程度（主观手段）--例如神经元的数量，深度，连接，先验分布（如果模型是贝叶斯的），权重初始化等。这引出了一个问题，即在仍能捕获客观终点的前提下，能否消除与这些规范相关的自由度。本文介绍了一种基于最简单的神经网络结构—单个神经元—的连续随机变量的新型非参数分位数估计方法。首先，在合成实验中，该方法展现了它的优势，它将通过排序顺序统计量得到的分位数估计结果（特别是对于小样本大小）与分位回归进行了比较。在真实世界的实验中，该方法被证明可以通过降低超出分布范围的数据中预测方差的低估来为贝叶斯神经网络提供更好的预测不确定性估计。",
    "tldr": "本文介绍了一种基于单个神经元的新型非参数分位数估计方法，该方法在小样本大小和真实世界实验中的优越表现表明其可以消除与模型规范相关的自由度，并提供更好的预测不确定性估计。",
    "en_tdlr": "This paper presents a novel non-parametric quantile estimation method based on a single neuron, which, through synthetic and real-world experiments, demonstrates its ability to eliminate model specification related degrees of freedom and provide better predictive uncertainty estimation, specifically for small sample sizes and Bayesian neural networks."
}