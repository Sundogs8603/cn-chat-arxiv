{
    "title": "The Curse of Passive Data Collection in Batch Reinforcement Learning. (arXiv:2106.09973v3 [cs.LG] UPDATED)",
    "abstract": "In high stake applications, active experimentation may be considered too risky and thus data are often collected passively. While in simple cases, such as in bandits, passive and active data collection are similarly effective, the price of passive sampling can be much higher when collecting data from a system with controlled states. The main focus of the current paper is the characterization of this price. For example, when learning in episodic finite state-action Markov decision processes (MDPs) with $\\mathrm{S}$ states and $\\mathrm{A}$ actions, we show that even with the best (but passively chosen) logging policy, $\\Omega(\\mathrm{A}^{\\min(\\mathrm{S}-1, H)}/\\varepsilon^2)$ episodes are necessary (and sufficient) to obtain an $\\epsilon$-optimal policy, where $H$ is the length of episodes. Note that this shows that the sample complexity blows up exponentially compared to the case of active data collection, a result which is not unexpected, but, as far as we know, have not been published",
    "link": "http://arxiv.org/abs/2106.09973",
    "context": "Title: The Curse of Passive Data Collection in Batch Reinforcement Learning. (arXiv:2106.09973v3 [cs.LG] UPDATED)\nAbstract: In high stake applications, active experimentation may be considered too risky and thus data are often collected passively. While in simple cases, such as in bandits, passive and active data collection are similarly effective, the price of passive sampling can be much higher when collecting data from a system with controlled states. The main focus of the current paper is the characterization of this price. For example, when learning in episodic finite state-action Markov decision processes (MDPs) with $\\mathrm{S}$ states and $\\mathrm{A}$ actions, we show that even with the best (but passively chosen) logging policy, $\\Omega(\\mathrm{A}^{\\min(\\mathrm{S}-1, H)}/\\varepsilon^2)$ episodes are necessary (and sufficient) to obtain an $\\epsilon$-optimal policy, where $H$ is the length of episodes. Note that this shows that the sample complexity blows up exponentially compared to the case of active data collection, a result which is not unexpected, but, as far as we know, have not been published",
    "path": "papers/21/06/2106.09973.json",
    "total_tokens": 914,
    "translated_title": "批量强化学习中被动数据采集的诅咒",
    "translated_abstract": "在高风险应用中，主动实验可能被认为风险太大，因此通常会被动采集数据。虽然在简单情况下，如在赌博机中，被动和主动数据采集的效果相似，但在从带有可控状态的系统中收集数据时，被动采样的代价可能会更高。本文的主要重点是对这种代价的特征化。例如，在具有$\\mathrm{S}$个状态和$\\mathrm{A}$个动作的离散状态-动作马尔可夫决策过程(MDP)中学习时，我们展示了即使使用最佳（但被动选择的）日志记录策略，也需要（且足够）获得$\\epsilon$-最优策略的$\\Omega(\\mathrm{A}^{\\min(\\mathrm{S}-1, H)}/\\varepsilon^2)$个回合，其中$H$是回合长度。请注意，这表明与主动数据采集相比，样本复杂性呈指数级增加，这个结果是可以预料的，但据我们所知，尚未发表。",
    "tldr": "本文研究了批量强化学习中被动数据采集的代价问题，并发现与主动数据采集相比，被动采集的样本复杂性呈指数级增加。",
    "en_tdlr": "This paper investigates the cost of passive data collection in batch reinforcement learning and discovers that the sample complexity increases exponentially compared to active data collection."
}