{
    "title": "Memory-Based Optimization Methods for Model-Agnostic Meta-Learning and Personalized Federated Learning. (arXiv:2106.04911v4 [cs.LG] UPDATED)",
    "abstract": "In recent years, model-agnostic meta-learning (MAML) has become a popular research area. However, the stochastic optimization of MAML is still underdeveloped. Existing MAML algorithms rely on the ``episode'' idea by sampling a few tasks and data points to update the meta-model at each iteration. Nonetheless, these algorithms either fail to guarantee convergence with a constant mini-batch size or require processing a large number of tasks at every iteration, which is unsuitable for continual learning or cross-device federated learning where only a small number of tasks are available per iteration or per round. To address these issues, this paper proposes memory-based stochastic algorithms for MAML that converge with vanishing error. The proposed algorithms require sampling a constant number of tasks and data samples per iteration, making them suitable for the continual learning scenario. Moreover, we introduce a communication-efficient memory-based MAML algorithm for personalized federa",
    "link": "http://arxiv.org/abs/2106.04911",
    "context": "Title: Memory-Based Optimization Methods for Model-Agnostic Meta-Learning and Personalized Federated Learning. (arXiv:2106.04911v4 [cs.LG] UPDATED)\nAbstract: In recent years, model-agnostic meta-learning (MAML) has become a popular research area. However, the stochastic optimization of MAML is still underdeveloped. Existing MAML algorithms rely on the ``episode'' idea by sampling a few tasks and data points to update the meta-model at each iteration. Nonetheless, these algorithms either fail to guarantee convergence with a constant mini-batch size or require processing a large number of tasks at every iteration, which is unsuitable for continual learning or cross-device federated learning where only a small number of tasks are available per iteration or per round. To address these issues, this paper proposes memory-based stochastic algorithms for MAML that converge with vanishing error. The proposed algorithms require sampling a constant number of tasks and data samples per iteration, making them suitable for the continual learning scenario. Moreover, we introduce a communication-efficient memory-based MAML algorithm for personalized federa",
    "path": "papers/21/06/2106.04911.json",
    "total_tokens": 1104,
    "translated_title": "基于存储的优化方法用于模型无关元学习和个性化联邦学习",
    "translated_abstract": "近年来，模型无关元学习(MAML)已成为流行的研究领域。然而，MAML的随机优化仍然不完善。现有的MAML算法依赖于“episode”的想法，即抽样几个任务和数据点来更新每次迭代中的元模型。然而，这些算法要么无法保证与常数小批量大小收敛，要么需要在每次迭代中处理大量任务，这对于持续学习或跨设备联邦学习来说是不适当的，因为每次迭代或每轮仅有少量任务可用。为解决这些问题，本文提出了一种基于存储的随机算法，用于MAML，并可以收敛到无限小的误差。所提出的算法需要每次迭代抽样恒定数量的任务和数据样本，使其适用于持续学习场景。此外，本文还引入了一种计算效率高的基于存储的个性化联邦MAML算法，其中每个客户端都有自己的个性化模型。我们的算法只需要在客户端和服务器之间通信一小部分元参数，从而实现高效的跨设备联邦学习。我们所提出的算法在各种基准数据集上得到了验证，显示出与现有MAML方法相比的显著改进。",
    "tldr": "本文提出了一种基于存储的优化方法，用于MAML算法，可以在处理少量任务的情况下实现持续学习，同时提出一种通信有效的基于存储的个性化联邦MAML算法，并在各种基准数据集上验证了所提出的算法的有效性。",
    "en_tdlr": "This paper proposes memory-based optimization methods for MAML algorithms to achieve continual learning with a small number of tasks and data samples per iteration, and introduces a communication-efficient memory-based algorithm for personalized federated MAML. The proposed algorithms show significant improvement compared to existing MAML methods on various benchmark datasets."
}