{
    "title": "Nonasymptotic theory for two-layer neural networks: Beyond the bias-variance trade-off. (arXiv:2106.04795v2 [cs.LG] UPDATED)",
    "abstract": "Large neural networks have proved remarkably effective in modern deep learning practice, even in the overparametrized regime where the number of active parameters is large relative to the sample size. This contradicts the classical perspective that a machine learning model must trade off bias and variance for optimal generalization. To resolve this conflict, we present a nonasymptotic generalization theory for two-layer neural networks with ReLU activation function by incorporating scaled variation regularization. Interestingly, the regularizer is equivalent to ridge regression from the angle of gradient-based optimization, but plays a similar role to the group lasso in controlling the model complexity. By exploiting this \"ridge-lasso duality,\" we obtain new prediction bounds for all network widths, which reproduce the double descent phenomenon. Moreover, the overparametrized minimum risk is lower than its underparametrized counterpart when the signal is strong, and is nearly minimax o",
    "link": "http://arxiv.org/abs/2106.04795",
    "context": "Title: Nonasymptotic theory for two-layer neural networks: Beyond the bias-variance trade-off. (arXiv:2106.04795v2 [cs.LG] UPDATED)\nAbstract: Large neural networks have proved remarkably effective in modern deep learning practice, even in the overparametrized regime where the number of active parameters is large relative to the sample size. This contradicts the classical perspective that a machine learning model must trade off bias and variance for optimal generalization. To resolve this conflict, we present a nonasymptotic generalization theory for two-layer neural networks with ReLU activation function by incorporating scaled variation regularization. Interestingly, the regularizer is equivalent to ridge regression from the angle of gradient-based optimization, but plays a similar role to the group lasso in controlling the model complexity. By exploiting this \"ridge-lasso duality,\" we obtain new prediction bounds for all network widths, which reproduce the double descent phenomenon. Moreover, the overparametrized minimum risk is lower than its underparametrized counterpart when the signal is strong, and is nearly minimax o",
    "path": "papers/21/06/2106.04795.json",
    "total_tokens": 976,
    "translated_title": "两层神经网络的非渐近理论：超越偏差-方差折衷",
    "translated_abstract": "大型神经网络在现代深度学习实践中表现出惊人的效果，即使在超参数化的情况下，即活跃参数数量相对于样本大小很大。这与传统观点相矛盾，传统观点认为机器学习模型必须在偏差和方差之间进行权衡以实现最佳泛化。为了解决这个冲突，我们通过引入缩放变分正则化，给出了针对具有ReLU激活函数的两层神经网络的非渐近泛化理论。有趣的是，这个正则化器从梯度优化的角度来看等价于岭回归，但在控制模型复杂性方面起到了类似于分组套索的作用。通过利用这种\"岭-套索对偶性\"，我们得到了适用于所有网络宽度的新的预测界限，从而重现了双谷现象。此外，在信号强的情况下，超参数化的最小风险低于其欠参数化的对应值，并且几乎是最小最大方法。",
    "tldr": "这项研究提出了针对两层神经网络的非渐近泛化理论，通过引入缩放变分正则化，并利用\"岭-套索对偶性\"获得了新的预测界限，解释了大型神经网络在超参数化情况下的表现以及双谷现象。",
    "en_tdlr": "This research presents a nonasymptotic generalization theory for two-layer neural networks by incorporating scaled variation regularization, and obtains new prediction bounds by exploiting the \"ridge-lasso duality,\" explaining the performance of large neural networks in the overparametrized regime and the double descent phenomenon."
}