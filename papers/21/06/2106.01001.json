{
    "title": "Warming up recurrent neural networks to maximise reachable multistability greatly improves learning. (arXiv:2106.01001v3 [cs.LG] UPDATED)",
    "abstract": "Training recurrent neural networks is known to be difficult when time dependencies become long. In this work, we show that most standard cells only have one stable equilibrium at initialisation, and that learning on tasks with long time dependencies generally occurs once the number of network stable equilibria increases; a property known as multistability. Multistability is often not easily attained by initially monostable networks, making learning of long time dependencies between inputs and outputs difficult. This insight leads to the design of a novel way to initialise any recurrent cell connectivity through a procedure called \"warmup\" to improve its capability to learn arbitrarily long time dependencies. This initialisation procedure is designed to maximise network reachable multistability, i.e., the number of equilibria within the network that can be reached through relevant input trajectories, in few gradient steps. We show on several information restitution, sequence classificat",
    "link": "http://arxiv.org/abs/2106.01001",
    "context": "Title: Warming up recurrent neural networks to maximise reachable multistability greatly improves learning. (arXiv:2106.01001v3 [cs.LG] UPDATED)\nAbstract: Training recurrent neural networks is known to be difficult when time dependencies become long. In this work, we show that most standard cells only have one stable equilibrium at initialisation, and that learning on tasks with long time dependencies generally occurs once the number of network stable equilibria increases; a property known as multistability. Multistability is often not easily attained by initially monostable networks, making learning of long time dependencies between inputs and outputs difficult. This insight leads to the design of a novel way to initialise any recurrent cell connectivity through a procedure called \"warmup\" to improve its capability to learn arbitrarily long time dependencies. This initialisation procedure is designed to maximise network reachable multistability, i.e., the number of equilibria within the network that can be reached through relevant input trajectories, in few gradient steps. We show on several information restitution, sequence classificat",
    "path": "papers/21/06/2106.01001.json",
    "total_tokens": 977,
    "translated_title": "让循环神经网络热身以最大化可达到的多稳定性极大改善了学习能力",
    "translated_abstract": "已知在时间依赖变长时，训练循环神经网络是困难的。本文中，我们展示了大多数标准单元在初始化时只有一个稳定的平衡点，并且学习长时间依赖任务通常发生在网络稳定的平衡点数量增加的时候，这个性质被称为多稳定性。初始为单稳定的网络通常难以达到多稳定性，使得学习输入和输出之间的长时间依赖变得困难。这一观察结果导致了一种新的初始化方法，称为“热身”，用于改善任何循环单元连接的学习能力，以学习任意长的时间依赖。这个初始化过程旨在通过少量梯度步骤内最大化网络可达到的多稳定性，即网络内可以通过相关输入轨迹到达的平衡点的数量。我们在多个信息恢复、序列分类任务上展示了这一方法的效果。",
    "tldr": "本研究发现大多数标准循环神经网络在初始化时只有一个稳定平衡点，并且学习长时间依赖任务通常要求网络具有多稳定性。为了解决这个问题，提出了一种称为“热身”的初始化方法，通过最大化网络可达到的多稳定性来改善学习能力。在多个任务上的实验证明了该方法的有效性。",
    "en_tdlr": "This study reveals that most standard recurrent neural networks have only one stable equilibrium at initialization and learning tasks with long time dependencies generally require networks with multistability. To address this issue, a novel initialization method called \"warmup\" is proposed to improve learning capability by maximizing network reachable multistability. Experimental results on multiple tasks demonstrate the effectiveness of this approach."
}