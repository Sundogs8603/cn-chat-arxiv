{
    "title": "Provably Efficient Representation Selection in Low-rank Markov Decision Processes: From Online to Offline RL",
    "abstract": "arXiv:2106.11935v2 Announce Type: replace Abstract: The success of deep reinforcement learning (DRL) lies in its ability to learn a representation that is well-suited for the exploration and exploitation task. To understand how the choice of representation can improve the efficiency of reinforcement learning (RL), we study representation selection for a class of low-rank Markov Decision Processes (MDPs) where the transition kernel can be represented in a bilinear form. We propose an efficient algorithm, called ReLEX, for representation learning in both online and offline RL. Specifically, we show that the online version of ReLEX, called ReLEX-UCB, always performs no worse than the state-of-the-art algorithm without representation selection, and achieves a strictly better constant regret if the representation function class has a \"coverage\" property over the entire state-action space. For the offline counterpart, ReLEX-LCB, we show that the algorithm can find the optimal policy if the r",
    "link": "https://arxiv.org/abs/2106.11935",
    "context": "Title: Provably Efficient Representation Selection in Low-rank Markov Decision Processes: From Online to Offline RL\nAbstract: arXiv:2106.11935v2 Announce Type: replace Abstract: The success of deep reinforcement learning (DRL) lies in its ability to learn a representation that is well-suited for the exploration and exploitation task. To understand how the choice of representation can improve the efficiency of reinforcement learning (RL), we study representation selection for a class of low-rank Markov Decision Processes (MDPs) where the transition kernel can be represented in a bilinear form. We propose an efficient algorithm, called ReLEX, for representation learning in both online and offline RL. Specifically, we show that the online version of ReLEX, called ReLEX-UCB, always performs no worse than the state-of-the-art algorithm without representation selection, and achieves a strictly better constant regret if the representation function class has a \"coverage\" property over the entire state-action space. For the offline counterpart, ReLEX-LCB, we show that the algorithm can find the optimal policy if the r",
    "path": "papers/21/06/2106.11935.json",
    "total_tokens": 981,
    "translated_title": "低秩马尔可夫决策过程中可证明高效的表示选择：从在线到离线强化学习",
    "translated_abstract": "深度强化学习(DRL)的成功在于其学习适合探索和利用任务的表示。为了理解表示选择如何提高强化学习的效率，我们研究了一类低秩马尔可夫决策过程(MDP)，其中转移核能够以双线性形式表示。我们提出了一个称为ReLEX的高效算法，用于在线和离线强化学习中的表示学习。具体来说，我们展示了ReLEX的在线版本ReLEX-UCB总是不比没有表示选择的最先进算法差，并在表示函数类在整个状态-动作空间上具有“覆盖度”性质时，实现了更好的常数遗憾。对于其离线对应物ReLEX-LCB，我们展示了该算法可以找到最优策略，如果表示函数类具有“覆盖度”性质。",
    "tldr": "本论文研究了在低秩马尔可夫决策过程中表示选择对于提高强化学习效率的影响。提出了ReLEX算法，可以在在线和离线强化学习中实现高效表示学习。实验证明，在线版本ReLEX-UCB总是不比没有表示选择的最先进算法差，并在表示函数类具有“覆盖度”性质时，实现了更好的常数遗憾。对于离线版本ReLEX-LCB，可以找到最优策略。",
    "en_tdlr": "This paper investigates the impact of representation selection on improving the efficiency of reinforcement learning in low-rank Markov Decision Processes. The proposed ReLEX algorithm achieves efficient representation learning in both online and offline RL settings, outperforming the state-of-the-art algorithm without representation selection in online RL and finding the optimal policy in offline RL with a certain coverage property of the representation function class."
}