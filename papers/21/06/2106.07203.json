{
    "title": "Online Sub-Sampling for Reinforcement Learning with General Function Approximation. (arXiv:2106.07203v2 [cs.LG] UPDATED)",
    "abstract": "Most of the existing works for reinforcement learning (RL) with general function approximation (FA) focus on understanding the statistical complexity or regret bounds. However, the computation complexity of such approaches is far from being understood -- indeed, a simple optimization problem over the function class might be as well intractable. In this paper, we tackle this problem by establishing an efficient online sub-sampling framework that measures the information gain of data points collected by an RL algorithm and uses the measurement to guide exploration. For a value-based method with complexity-bounded function class, we show that the policy only needs to be updated for $\\propto\\operatorname{poly}\\log(K)$ times for running the RL algorithm for $K$ episodes while still achieving a small near-optimal regret bound. In contrast to existing approaches that update the policy for at least $\\Omega(K)$ times, our approach drastically reduces the number of optimization calls in solving ",
    "link": "http://arxiv.org/abs/2106.07203",
    "context": "Title: Online Sub-Sampling for Reinforcement Learning with General Function Approximation. (arXiv:2106.07203v2 [cs.LG] UPDATED)\nAbstract: Most of the existing works for reinforcement learning (RL) with general function approximation (FA) focus on understanding the statistical complexity or regret bounds. However, the computation complexity of such approaches is far from being understood -- indeed, a simple optimization problem over the function class might be as well intractable. In this paper, we tackle this problem by establishing an efficient online sub-sampling framework that measures the information gain of data points collected by an RL algorithm and uses the measurement to guide exploration. For a value-based method with complexity-bounded function class, we show that the policy only needs to be updated for $\\propto\\operatorname{poly}\\log(K)$ times for running the RL algorithm for $K$ episodes while still achieving a small near-optimal regret bound. In contrast to existing approaches that update the policy for at least $\\Omega(K)$ times, our approach drastically reduces the number of optimization calls in solving ",
    "path": "papers/21/06/2106.07203.json",
    "total_tokens": 904,
    "translated_title": "基于普适函数逼近的强化学习的在线子采样",
    "translated_abstract": "现有的大多数强化学习（RL）普适函数逼近（FA）方法都专注于理解统计复杂性或遗憾边界，但这些方法的计算复杂性远未得到理解——事实上，函数类上的简单优化问题可能同样难以处理。本文通过建立一种高效的在线子采样框架来解决这个问题，该框架测量RL算法收集的数据点的信息增益，并使用该测量指导探索。对于基于价值的方法和复杂度有界的函数类，我们证明了策略只需要更新$\\propto\\operatorname{poly}\\log(K)$ 次，就可以运行 $K$ 次RL算法而仍然实现较小的近似最优遗憾边界。与现有方法更新策略至少要 $\\Omega(K)$ 次相比，我们的方法大大减少了解决方案中的优化调用次数。",
    "tldr": "本文提出了一种基于在线子采样框架的强化学习算法，利用数据点的信息增益量来指导探索，与现有方法相比更新RL算法的策略次数大大减少，但仍保持较小的近似最优遗憾边界。"
}