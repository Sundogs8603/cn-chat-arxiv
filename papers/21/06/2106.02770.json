{
    "title": "Deep Bayesian Active Learning for Accelerating Stochastic Simulation. (arXiv:2106.02770v7 [cs.LG] UPDATED)",
    "abstract": "Stochastic simulations such as large-scale, spatiotemporal, age-structured epidemic models are computationally expensive at fine-grained resolution. While deep surrogate models can speed up the simulations, doing so for stochastic simulations and with active learning approaches is an underexplored area. We propose Interactive Neural Process (INP), a deep Bayesian active learning framework for learning deep surrogate models to accelerate stochastic simulations. INP consists of two components, a spatiotemporal surrogate model built upon Neural Process (NP) family and an acquisition function for active learning. For surrogate modeling, we develop Spatiotemporal Neural Process (STNP) to mimic the simulator dynamics. For active learning, we propose a novel acquisition function, Latent Information Gain (LIG), calculated in the latent space of NP based models. We perform a theoretical analysis and demonstrate that LIG reduces sample complexity compared with random sampling in high dimensions.",
    "link": "http://arxiv.org/abs/2106.02770",
    "context": "Title: Deep Bayesian Active Learning for Accelerating Stochastic Simulation. (arXiv:2106.02770v7 [cs.LG] UPDATED)\nAbstract: Stochastic simulations such as large-scale, spatiotemporal, age-structured epidemic models are computationally expensive at fine-grained resolution. While deep surrogate models can speed up the simulations, doing so for stochastic simulations and with active learning approaches is an underexplored area. We propose Interactive Neural Process (INP), a deep Bayesian active learning framework for learning deep surrogate models to accelerate stochastic simulations. INP consists of two components, a spatiotemporal surrogate model built upon Neural Process (NP) family and an acquisition function for active learning. For surrogate modeling, we develop Spatiotemporal Neural Process (STNP) to mimic the simulator dynamics. For active learning, we propose a novel acquisition function, Latent Information Gain (LIG), calculated in the latent space of NP based models. We perform a theoretical analysis and demonstrate that LIG reduces sample complexity compared with random sampling in high dimensions.",
    "path": "papers/21/06/2106.02770.json",
    "total_tokens": 959,
    "translated_title": "深度贝叶斯主动学习用于加速随机模拟",
    "translated_abstract": "针对实现精细粒度下的大规模空间时间年龄结构流行病模型等随机模拟方法的高计算代价，本文提出了一个名为交互式神经过程(INP)的深度贝叶斯主动学习框架，用于学习深度代理模型以加速随机模拟过程。该框架由两个部分组成，即建立在神经过程(NP)家族基础之上的空间时间代理模型和一个用于主动学习的收购函数。其中，我们提出了空间时间神经过程(STNP)来模拟模拟器动态，同时在NP模型的潜空间中提出了一种新颖的收获函数——潜在信息增益(LIG)。理论分析和实践证明，LIG相对于高维随机抽样可以降低模拟样本复杂度。",
    "tldr": "本文提出了一个名为交互式神经过程(INP)的深度贝叶斯主动学习框架，用于学习深度代理模型以加速随机模拟过程，其中通过使用空间时间神经过程(STNP)实现模拟器动态的模拟，以及利用潜在信息增益(LIG)的主动学习方式来减少样本的复杂度。",
    "en_tdlr": "This paper proposes a deep Bayesian active learning framework called Interactive Neural Process (INP) for learning deep surrogate models to accelerate stochastic simulations, which includes a spatiotemporal surrogate model built upon Neural Process (NP) family named Spatiotemporal Neural Process (STNP) and a novel acquisition function named Latent Information Gain (LIG) calculated in the latent space of NP based models to reduce the complexity of samples."
}