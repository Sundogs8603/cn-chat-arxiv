{
    "title": "Distributional Reinforcement Learning with Unconstrained Monotonic Neural Networks. (arXiv:2106.03228v3 [cs.LG] UPDATED)",
    "abstract": "The distributional reinforcement learning (RL) approach advocates for representing the complete probability distribution of the random return instead of only modelling its expectation. A distributional RL algorithm may be characterised by two main components, namely the representation of the distribution together with its parameterisation and the probability metric defining the loss. The present research work considers the unconstrained monotonic neural network (UMNN) architecture, a universal approximator of continuous monotonic functions which is particularly well suited for modelling different representations of a distribution. This property enables the efficient decoupling of the effect of the function approximator class from that of the probability metric. The research paper firstly introduces a methodology for learning different representations of the random return distribution (PDF, CDF and QF). Secondly, a novel distributional RL algorithm named unconstrained monotonic deep Q-n",
    "link": "http://arxiv.org/abs/2106.03228",
    "context": "Title: Distributional Reinforcement Learning with Unconstrained Monotonic Neural Networks. (arXiv:2106.03228v3 [cs.LG] UPDATED)\nAbstract: The distributional reinforcement learning (RL) approach advocates for representing the complete probability distribution of the random return instead of only modelling its expectation. A distributional RL algorithm may be characterised by two main components, namely the representation of the distribution together with its parameterisation and the probability metric defining the loss. The present research work considers the unconstrained monotonic neural network (UMNN) architecture, a universal approximator of continuous monotonic functions which is particularly well suited for modelling different representations of a distribution. This property enables the efficient decoupling of the effect of the function approximator class from that of the probability metric. The research paper firstly introduces a methodology for learning different representations of the random return distribution (PDF, CDF and QF). Secondly, a novel distributional RL algorithm named unconstrained monotonic deep Q-n",
    "path": "papers/21/06/2106.03228.json",
    "total_tokens": 810,
    "tldr": "本研究使用无限制单调神经网络(UMNN)架构来学习不同形式的分布式强化学习，并引入了一种名为无限制单调深度Q-n的新算法。"
}