{
    "title": "Neural Distributed Source Coding. (arXiv:2106.02797v3 [cs.IT] UPDATED)",
    "abstract": "Distributed source coding (DSC) is the task of encoding an input in the absence of correlated side information that is only available to the decoder. Remarkably, Slepian and Wolf showed in 1973 that an encoder without access to the side information can asymptotically achieve the same compression rate as when the side information is available to it. While there is vast prior work on this topic, practical DSC has been limited to synthetic datasets and specific correlation structures. Here we present a framework for lossy DSC that is agnostic to the correlation structure and can scale to high dimensions. Rather than relying on hand-crafted source modeling, our method utilizes a conditional Vector-Quantized Variational Autoencoder (VQ-VAE) to learn the distributed encoder and decoder. We evaluate our method on multiple datasets and show that our method can handle complex correlations and achieves state-of-the-art PSNR.",
    "link": "http://arxiv.org/abs/2106.02797",
    "context": "Title: Neural Distributed Source Coding. (arXiv:2106.02797v3 [cs.IT] UPDATED)\nAbstract: Distributed source coding (DSC) is the task of encoding an input in the absence of correlated side information that is only available to the decoder. Remarkably, Slepian and Wolf showed in 1973 that an encoder without access to the side information can asymptotically achieve the same compression rate as when the side information is available to it. While there is vast prior work on this topic, practical DSC has been limited to synthetic datasets and specific correlation structures. Here we present a framework for lossy DSC that is agnostic to the correlation structure and can scale to high dimensions. Rather than relying on hand-crafted source modeling, our method utilizes a conditional Vector-Quantized Variational Autoencoder (VQ-VAE) to learn the distributed encoder and decoder. We evaluate our method on multiple datasets and show that our method can handle complex correlations and achieves state-of-the-art PSNR.",
    "path": "papers/21/06/2106.02797.json",
    "total_tokens": 786,
    "translated_title": "神经分布式源编码",
    "translated_abstract": "分布式源编码(DSC)是在没有相互关联的边际信息可供解码器使用的情况下对输入进行编码的任务。值得注意的是，Slepian和Wolf在1973年证明，没有访问边际信息的编码器可以渐近地实现与边际信息可用情况下相同的压缩率。虽然在这个领域有广泛的先前工作，但实践中的DSC一直局限于合成数据集和特定的相关结构。在这里，我们提出了一个对相关结构不可知且能够扩展到高维度的有损DSC框架。我们的方法不依赖于手工设计的源模型，而是利用条件向量量化变分自动编码器(VQ-VAE)来学习分布式编码器和解码器。我们在多个数据集上评估了我们的方法，并展示了我们的方法可以处理复杂的相关性，并实现了最先进的峰值信噪比(PSNR)。",
    "tldr": "这项研究提出了一种神经分布式源编码的框架，可以处理复杂的相关性并实现最先进的峰值信噪比。"
}