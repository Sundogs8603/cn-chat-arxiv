{
    "title": "IID-GAN: an IID Sampling Perspective for Regularizing Mode Collapse. (arXiv:2106.00563v3 [cs.LG] UPDATED)",
    "abstract": "Despite its success, generative adversarial networks (GANs) still suffer from mode collapse, i.e., the generator can only map latent variables to a partial set of modes in the target distribution. In this paper, we analyze and seek to regularize this issue with an independent and identically distributed (IID) sampling perspective and emphasize that holding the IID property referring to the target distribution for generation can naturally avoid mode collapse. This is based on the basic IID assumption for real data in machine learning. However, though the source samples {z} obey IID, the generations {G(z)} may not necessarily be IID sampling from the target distribution. Based on this observation, considering a necessary condition of IID generation that the inverse samples from target data should also be IID in the source distribution, we propose a new loss to encourage the closeness between inverse samples of real data and the Gaussian source in latent space to regularize the generation",
    "link": "http://arxiv.org/abs/2106.00563",
    "context": "Title: IID-GAN: an IID Sampling Perspective for Regularizing Mode Collapse. (arXiv:2106.00563v3 [cs.LG] UPDATED)\nAbstract: Despite its success, generative adversarial networks (GANs) still suffer from mode collapse, i.e., the generator can only map latent variables to a partial set of modes in the target distribution. In this paper, we analyze and seek to regularize this issue with an independent and identically distributed (IID) sampling perspective and emphasize that holding the IID property referring to the target distribution for generation can naturally avoid mode collapse. This is based on the basic IID assumption for real data in machine learning. However, though the source samples {z} obey IID, the generations {G(z)} may not necessarily be IID sampling from the target distribution. Based on this observation, considering a necessary condition of IID generation that the inverse samples from target data should also be IID in the source distribution, we propose a new loss to encourage the closeness between inverse samples of real data and the Gaussian source in latent space to regularize the generation",
    "path": "papers/21/06/2106.00563.json",
    "total_tokens": 916,
    "translated_title": "IID-GAN：一种独立同分布采样视角下的规范模式崩溃方法",
    "translated_abstract": "尽管生成对抗网络（GAN）很成功，但它仍然受到模式崩溃的影响，即生成器只能将潜在变量映射到目标分布中部分模式。本文分析并试图通过独立同分布（IID）采样视角来规范化这个问题，并强调保持生成的IID属性可以自然地避免模式崩溃。这是基于机器学习中实际数据的基础IID假设。然而，尽管源样本{z}服从IID，但生成物{G（z）}不一定是从目标分布中采样得到的IID样本。基于这个观察，考虑到实现IID生成的必要条件是从目标数据中逆采样的样本在源分布下也应该是IID的，我们提出了一种新的损失来鼓励真实数据的逆采样和潜在空间中的高斯源之间的相似性，以规范生成。",
    "tldr": "本论文提出一种基于独立同分布采样视角的GAN方法，通过在源分布下实现逆采样的IID样本与真实数据在潜在空间中的高斯源之间的相似性，从而规范化了GAN中的模式崩溃问题。",
    "en_tdlr": "This paper proposes a new GAN method based on an independent and identically distributed (IID) sampling perspective to regulate the issue of mode collapse, which is achieved by encouraging the similarity between the inverse samples of real data and the Gaussian source in latent space under the necessary condition of IID generation."
}