{
    "title": "Dynamic Efficient Adversarial Training Guided by Gradient Magnitude. (arXiv:2103.03076v2 [cs.LG] UPDATED)",
    "abstract": "Adversarial training is an effective but time-consuming way to train robust deep neural networks that can withstand strong adversarial attacks. As a response to its inefficiency, we propose Dynamic Efficient Adversarial Training (DEAT), which gradually increases the adversarial iteration during training. We demonstrate that the gradient's magnitude correlates with the curvature of the trained model's loss landscape, allowing it to reflect the effect of adversarial training. Therefore, based on the magnitude of the gradient, we propose a general acceleration strategy, M+ acceleration, which enables an automatic and highly effective method of adjusting the training procedure. M+ acceleration is computationally efficient and easy to implement. It is suited for DEAT and compatible with the majority of existing adversarial training techniques. Extensive experiments have been done on CIFAR-10 and ImageNet datasets with various training environments. The results show that the proposed M+ acce",
    "link": "http://arxiv.org/abs/2103.03076",
    "context": "Title: Dynamic Efficient Adversarial Training Guided by Gradient Magnitude. (arXiv:2103.03076v2 [cs.LG] UPDATED)\nAbstract: Adversarial training is an effective but time-consuming way to train robust deep neural networks that can withstand strong adversarial attacks. As a response to its inefficiency, we propose Dynamic Efficient Adversarial Training (DEAT), which gradually increases the adversarial iteration during training. We demonstrate that the gradient's magnitude correlates with the curvature of the trained model's loss landscape, allowing it to reflect the effect of adversarial training. Therefore, based on the magnitude of the gradient, we propose a general acceleration strategy, M+ acceleration, which enables an automatic and highly effective method of adjusting the training procedure. M+ acceleration is computationally efficient and easy to implement. It is suited for DEAT and compatible with the majority of existing adversarial training techniques. Extensive experiments have been done on CIFAR-10 and ImageNet datasets with various training environments. The results show that the proposed M+ acce",
    "path": "papers/21/03/2103.03076.json",
    "total_tokens": 1008,
    "translated_title": "基于梯度幅度引导的动态高效对抗训练",
    "translated_abstract": "对抗训练是训练鲁棒深度神经网络以抵御强对抗攻击的有效方法，但时间成本很高。为了解决其效率问题，我们提出了一种名为动态高效对抗训练的方法（DEAT），其中逐渐增加了对抗迭代次数。我们证明梯度幅度与训练模型损失函数曲率相关，从而反映了对抗训练的效果。因此，基于梯度幅度，我们提出了一种通用加速策略，M+加速，它可以自动并高效地调整训练过程，且易于实现。它适用于DEAT并兼容大部分现有的对抗训练技术。我们在CIFAR-10和ImageNet数据集上进行了大量实验，并展示了我们的M+加速策略显著加快了对抗训练，并提高了深度神经网络对各种攻击的鲁棒性。",
    "tldr": "本文提出了一种基于梯度幅度引导的动态高效对抗训练方法（DEAT），逐渐增加了对抗迭代次数，并提出了一种通用加速策略M+加速，该方法易于实现且符合大部分现有的对抗训练技术，可以显著提高深度神经网络的鲁棒性。",
    "en_tdlr": "This paper proposes a dynamic and efficient adversarial training method (DEAT) that gradually increases the adversarial iteration during training. The authors also introduce a general acceleration strategy based on the gradient magnitude, called M+ acceleration, which is easy to implement and compatible with most existing techniques for adversarial training. Extensive experiments on CIFAR-10 and ImageNet datasets demonstrate that this approach significantly improves the robustness of deep neural networks against various attacks."
}