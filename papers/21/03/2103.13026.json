{
    "title": "The Gradient Convergence Bound of Federated Multi-Agent Reinforcement Learning with Efficient Communication. (arXiv:2103.13026v2 [cs.LG] UPDATED)",
    "abstract": "The paper considers independent reinforcement learning (IRL) for multi-agent collaborative decision-making in the paradigm of federated learning (FL). However, FL generates excessive communication overheads between agents and a remote central server, especially when it involves a large number of agents or iterations. Besides, due to the heterogeneity of independent learning environments, multiple agents may undergo asynchronous Markov decision processes (MDPs), which will affect the training samples and the model's convergence performance. On top of the variation-aware periodic averaging (VPA) method and the policy-based deep reinforcement learning (DRL) algorithm (i.e., proximal policy optimization (PPO)), this paper proposes two advanced optimization schemes orienting to stochastic gradient descent (SGD): 1) A decay-based scheme gradually decays the weights of a model's local gradients with the progress of successive local updates, and 2) By representing the agents as a graph, a cons",
    "link": "http://arxiv.org/abs/2103.13026",
    "context": "Title: The Gradient Convergence Bound of Federated Multi-Agent Reinforcement Learning with Efficient Communication. (arXiv:2103.13026v2 [cs.LG] UPDATED)\nAbstract: The paper considers independent reinforcement learning (IRL) for multi-agent collaborative decision-making in the paradigm of federated learning (FL). However, FL generates excessive communication overheads between agents and a remote central server, especially when it involves a large number of agents or iterations. Besides, due to the heterogeneity of independent learning environments, multiple agents may undergo asynchronous Markov decision processes (MDPs), which will affect the training samples and the model's convergence performance. On top of the variation-aware periodic averaging (VPA) method and the policy-based deep reinforcement learning (DRL) algorithm (i.e., proximal policy optimization (PPO)), this paper proposes two advanced optimization schemes orienting to stochastic gradient descent (SGD): 1) A decay-based scheme gradually decays the weights of a model's local gradients with the progress of successive local updates, and 2) By representing the agents as a graph, a cons",
    "path": "papers/21/03/2103.13026.json",
    "total_tokens": 915,
    "translated_title": "有效通信下联邦多智能体强化学习的梯度收敛界限",
    "translated_abstract": "本文考虑联邦学习范式中多智能体协作决策的独立强化学习，但是由于独立学习环境异质性和联邦学习的通信开销问题，现有方法在训练过程中存在收敛问题。因此本文提出了两种优化策略，一种是逐步衰减本地梯度权重的衰减模式，另一种是基于代价最小化设计的共识算法来减少模型之间的通信量，理论分析和实验证明了这两种方法的有效性和优越性。",
    "tldr": "本文提出了两种优化策略, 一种是逐步衰减本地梯度权重的衰减模式, 另一种是基于代价最小化设计的共识算法来减少模型之间的通信量, 有效解决了独立学习环境异质性和联邦学习的通信开销问题。",
    "en_tdlr": "This paper proposes two optimization schemes to address the convergence issue caused by heterogeneous independent learning environments and the communication overheads in traditional federated learning. The first scheme gradually decays the weight of local gradients, while the second consensus-based scheme reduces communication cost between models. Both theoretical analysis and experiments demonstrate the effectiveness of these approaches."
}