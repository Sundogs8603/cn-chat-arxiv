{
    "title": "A Practical Survey on Faster and Lighter Transformers. (arXiv:2103.14636v2 [cs.LG] UPDATED)",
    "abstract": "Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the state-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the models' efficiency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformer's limitation by designing lower-complexity alternatives such as the Longformer,",
    "link": "http://arxiv.org/abs/2103.14636",
    "context": "Title: A Practical Survey on Faster and Lighter Transformers. (arXiv:2103.14636v2 [cs.LG] UPDATED)\nAbstract: Recurrent neural networks are effective models to process sequences. However, they are unable to learn long-term dependencies because of their inherent sequential nature. As a solution, Vaswani et al. introduced the Transformer, a model solely based on the attention mechanism that is able to relate any two positions of the input sequence, hence modelling arbitrary long dependencies. The Transformer has improved the state-of-the-art across numerous sequence modelling tasks. However, its effectiveness comes at the expense of a quadratic computational and memory complexity with respect to the sequence length, hindering its adoption. Fortunately, the deep learning community has always been interested in improving the models' efficiency, leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation. Recently, researchers have directly addressed the Transformer's limitation by designing lower-complexity alternatives such as the Longformer,",
    "path": "papers/21/03/2103.14636.json",
    "total_tokens": 873,
    "translated_title": "《更快更轻的Transformer的实用调查》的论文翻译",
    "translated_abstract": "循环神经网络是处理序列的有效模型。然而，由于它们固有的顺序性，它们无法学习长期依赖关系。作为解决方案，Vaswani等人提出了Transformer，一种仅基于注意力机制的模型，能够关联输入序列的任意两个位置，从而建模任意长的依赖关系。Transformer改善了众多序列建模任务的最新技术，但其有效性的代价是与序列长度相关的二次计算和内存复杂度，这阻碍了它的应用。幸运的是，深度学习社区一直致力于提高模型的效率，导致了一系列解决方案，如参数共享、剪枝、混合精度和知识蒸馏。最近，研究人员通过设计低复杂度的替代品，如Longformer等，直接解决了Transformer的局限性。",
    "tldr": "Transformer虽然在许多序列建模任务中取得了很好的效果，但其复杂度与序列长度相关。近期，研究人员设计了类似于Longformer的解决方案来直接解决Transformer的限制，从而提高了其效率。",
    "en_tdlr": "Although Transformer has shown great performance in numerous sequence modelling tasks, its complexity is proportional to the length of the sequence. Recently, researchers have addressed this issue by designing lower-complexity alternatives such as Longformer to directly tackle the limitation of Transformer, thus improving its efficiency."
}