{
    "title": "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. (arXiv:2103.15949v2 [cs.CL] UPDATED)",
    "abstract": "Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these \"black boxes\" as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at https://github.com/z",
    "link": "http://arxiv.org/abs/2103.15949",
    "context": "Title: Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors. (arXiv:2103.15949v2 [cs.CL] UPDATED)\nAbstract: Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these \"black boxes\" as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at https://github.com/z",
    "path": "papers/21/03/2103.15949.json",
    "total_tokens": 887,
    "translated_title": "通过字典学习实现Transformer可视化:将上下文嵌入作为Transformer因子的线性叠加",
    "translated_abstract": "自Transformer网络问世以来，它们在自然语言处理表示学习中掀起了一场革命。尽管已经做出了大量努力来解释Transformer网络中的表示，但广泛认为我们的理解还不够。其中一个重要原因是缺乏足够的可视化工具来进行详细的分析。本文提出使用字典学习将其作为Transformer因子的线性叠加来打开这些“黑匣子”。通过可视化，我们展示了被Transformer因子捕获的层次化语义结构，例如词级多义消歧、句子级模式形成和长距离依赖。虽然一些模式符合传统的语言知识，但其余的模式相对出乎意料，可能提供新的见解。我们希望这种可视化工具能带来更多的知识和对Transformer网络工作方式的更好理解。代码可在https://github.com/z中找到。",
    "tldr": "本文提出使用字典学习将上下文嵌入作为Transformer因子的线性叠加来打开Transformer“黑匣子”，通过可视化展示其捕获的层次化语义结构，为更好地理解Transformer网络的工作方式带来新的见解。",
    "en_tdlr": "This paper proposes using dictionary learning to open up the \"black boxes\" of transformer networks as a linear superposition of transformer factors for visualization, demonstrating the hierarchical semantic structures captured by them, and providing new insights for a better understanding of how transformer networks work."
}