{
    "title": "Attention-Based Multimodal Image Matching. (arXiv:2103.11247v2 [cs.CV] UPDATED)",
    "abstract": "We propose an attention-based approach for multimodal image patch matching using a Transformer encoder attending to the feature maps of a multiscale Siamese CNN. Our encoder is shown to efficiently aggregate multiscale image embeddings while emphasizing task-specific appearance-invariant image cues. We also introduce an attention-residual architecture, using a residual connection bypassing the encoder. This additional learning signal facilitates end-to-end training from scratch. Our approach is experimentally shown to achieve new state-of-the-art accuracy on both multimodal and single modality benchmarks, illustrating its general applicability. To the best of our knowledge, this is the first successful implementation of the Transformer encoder architecture to the multimodal image patch matching task.",
    "link": "http://arxiv.org/abs/2103.11247",
    "context": "Title: Attention-Based Multimodal Image Matching. (arXiv:2103.11247v2 [cs.CV] UPDATED)\nAbstract: We propose an attention-based approach for multimodal image patch matching using a Transformer encoder attending to the feature maps of a multiscale Siamese CNN. Our encoder is shown to efficiently aggregate multiscale image embeddings while emphasizing task-specific appearance-invariant image cues. We also introduce an attention-residual architecture, using a residual connection bypassing the encoder. This additional learning signal facilitates end-to-end training from scratch. Our approach is experimentally shown to achieve new state-of-the-art accuracy on both multimodal and single modality benchmarks, illustrating its general applicability. To the best of our knowledge, this is the first successful implementation of the Transformer encoder architecture to the multimodal image patch matching task.",
    "path": "papers/21/03/2103.11247.json",
    "total_tokens": 770,
    "translated_title": "基于注意力的多模态图像匹配",
    "translated_abstract": "我们提出了一种基于注意力的多模态图像块匹配方法，使用Transformer编码器对多尺度Siamese CNN的特征图进行注意力聚合。我们的编码器能够有效地聚合多尺度图像嵌入，并突出显示任务特定的外观不变的图像线索。我们还引入了一种注意力残差架构，使用绕过编码器的残差连接。这种额外的学习信号有助于从头开始进行端到端训练。我们的方法在多模态和单模态基准测试中实现了最新的最高精度，证明了其通用性。据我们所知，这是第一个将Transformer编码器架构成功应用于多模态图像块匹配任务的实现。",
    "tldr": "提出了一种基于注意力的方法用于多模态图像块匹配，通过Transformer编码器聚合多尺度图像特征，实现了任务特定的外观不变性。在多个基准测试中取得了最新的最高精度，证明了方法的通用性。",
    "en_tdlr": "Proposed an attention-based method for multimodal image patch matching, which aggregates multi-scale image features with a Transformer encoder to achieve task-specific appearance invariance. Achieved state-of-the-art accuracy on multiple benchmarks, demonstrating the general applicability of the method."
}