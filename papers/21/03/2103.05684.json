{
    "title": "Monotonic Alpha-divergence Minimisation for Variational Inference. (arXiv:2103.05684v4 [stat.CO] UPDATED)",
    "abstract": "In this paper, we introduce a novel family of iterative algorithms which carry out $\\alpha$-divergence minimisation in a Variational Inference context. They do so by ensuring a systematic decrease at each step in the $\\alpha$-divergence between the variational and the posterior distributions. In its most general form, the variational distribution is a mixture model and our framework allows us to simultaneously optimise the weights and components parameters of this mixture model. Our approach permits us to build on various methods previously proposed for $\\alpha$-divergence minimisation such as Gradient or Power Descent schemes and we also shed a new light on an integrated Expectation Maximization algorithm. Lastly, we provide empirical evidence that our methodology yields improved results on several multimodal target distributions and on a real data example.",
    "link": "http://arxiv.org/abs/2103.05684",
    "context": "Title: Monotonic Alpha-divergence Minimisation for Variational Inference. (arXiv:2103.05684v4 [stat.CO] UPDATED)\nAbstract: In this paper, we introduce a novel family of iterative algorithms which carry out $\\alpha$-divergence minimisation in a Variational Inference context. They do so by ensuring a systematic decrease at each step in the $\\alpha$-divergence between the variational and the posterior distributions. In its most general form, the variational distribution is a mixture model and our framework allows us to simultaneously optimise the weights and components parameters of this mixture model. Our approach permits us to build on various methods previously proposed for $\\alpha$-divergence minimisation such as Gradient or Power Descent schemes and we also shed a new light on an integrated Expectation Maximization algorithm. Lastly, we provide empirical evidence that our methodology yields improved results on several multimodal target distributions and on a real data example.",
    "path": "papers/21/03/2103.05684.json",
    "total_tokens": 802,
    "translated_title": "变分推断中的单调α散度最小化",
    "translated_abstract": "本文引入了一种新的迭代算法系列，用于在变分推断上进行α散度最小化。该算法通过确保在每一步中变分分布与后验分布之间的α散度系统性降低，从而实现上述目标。在其最一般的形式下，变分分布是一个混合模型，我们的框架允许我们同时优化此混合模型的权重和分量参数。本文在已有的基于α散度最小化方法，如梯度或幂下降方案的基础上提出了新的见解，并阐述了一种集成期望最大化算法。最后，通过实证，证明了我们的方法在多峰目标分布和实际数据应用上都获得了改进的效果。",
    "tldr": "本文提出了一种用于变分推断的单调α散度最小化算法，可优化混合模型的权重和分量参数，获得了在多峰目标分布和实际数据应用上的改进效果。",
    "en_tdlr": "This paper proposes a novel family of iterative algorithms for α-divergence minimisation in Variational Inference, which simultaneously optimises the weights and components parameters in a mixture model. The proposed methodology yields improved results on several multimodal target distributions and a real data example."
}