{
    "title": "Detecting Harmful Content On Online Platforms: What Platforms Need Vs. Where Research Efforts Go. (arXiv:2103.00153v2 [cs.CL] UPDATED)",
    "abstract": "The proliferation of harmful content on online platforms is a major societal problem, which comes in many different forms including hate speech, offensive language, bullying and harassment, misinformation, spam, violence, graphic content, sexual abuse, self harm, and many other. Online platforms seek to moderate such content to limit societal harm, to comply with legislation, and to create a more inclusive environment for their users. Researchers have developed different methods for automatically detecting harmful content, often focusing on specific sub-problems or on narrow communities, as what is considered harmful often depends on the platform and on the context. We argue that there is currently a dichotomy between what types of harmful content online platforms seek to curb, and what research efforts there are to automatically detect such content. We thus survey existing methods as well as content moderation policies by online platforms in this light and we suggest directions for fu",
    "link": "http://arxiv.org/abs/2103.00153",
    "context": "Title: Detecting Harmful Content On Online Platforms: What Platforms Need Vs. Where Research Efforts Go. (arXiv:2103.00153v2 [cs.CL] UPDATED)\nAbstract: The proliferation of harmful content on online platforms is a major societal problem, which comes in many different forms including hate speech, offensive language, bullying and harassment, misinformation, spam, violence, graphic content, sexual abuse, self harm, and many other. Online platforms seek to moderate such content to limit societal harm, to comply with legislation, and to create a more inclusive environment for their users. Researchers have developed different methods for automatically detecting harmful content, often focusing on specific sub-problems or on narrow communities, as what is considered harmful often depends on the platform and on the context. We argue that there is currently a dichotomy between what types of harmful content online platforms seek to curb, and what research efforts there are to automatically detect such content. We thus survey existing methods as well as content moderation policies by online platforms in this light and we suggest directions for fu",
    "path": "papers/21/03/2103.00153.json",
    "total_tokens": 890,
    "translated_title": "在在线平台上检测有害内容：平台需求与研究方向差异",
    "translated_abstract": "在线平台上有害内容的泛滥是一个重要的社会问题，包括仇恨言论、攻击性语言、欺凌和骚扰、错误信息、垃圾邮件、暴力、露骨内容、性虐待、自残等多种形式。在线平台寻求限制这些内容以减少社会危害，遵守法律法规，并为用户创建更具包容性的环境。研究人员开发出了不同的方法来自动检测有害内容，通常集中在特定的子问题或狭窄的社区上，因为有害内容的定义通常取决于平台和上下文。我们认为，当前存在在线平台寻求遏制的有害内容类型，与自动检测此类内容的研究方向之间存在二元对立。因此，我们在此调查现有方法以及在线平台的内容管理政策，并针对性提出研究方向建议。",
    "tldr": "在线平台上的有害内容种类和平台需求与自动检测有害内容的研究方向存在差异，需要更深入的研究和更好的平台管理，以减少社会危害和创建更完整的用户环境。",
    "en_tdlr": "There is a dichotomy between the types of harmful content that online platforms seek to curb and the research efforts that are being made to automatically detect them. More in-depth research and better platform management are needed to reduce societal harm and create a more inclusive user environment."
}