{
    "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism. (arXiv:2103.12021v2 [cs.LG] UPDATED)",
    "abstract": "Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone.  Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal r",
    "link": "http://arxiv.org/abs/2103.12021",
    "context": "Title: Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism. (arXiv:2103.12021v2 [cs.LG] UPDATED)\nAbstract: Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone.  Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal r",
    "path": "papers/21/03/2103.12021.json",
    "total_tokens": 1031,
    "translated_title": "联机强化学习与模仿学习的桥梁：一个悲观的故事",
    "translated_abstract": "联机（或批次）强化学习算法旨在从固定的数据集中学习最优策略，而无需主动收集数据。根据离线数据集的组成，主要使用两种方法：适用于专家数据集的模仿学习和通常需要均匀覆盖数据集的纯联机强化学习。从实践的角度来看，数据集通常偏离这两个极端，并且通常事先不知道确切的数据组成。为了填补这一差距，我们提出了一个新的联机强化学习框架，它在数据组成的两个极端之间平滑插值，从而统一了模仿学习和纯联机强化学习。新的框架围绕一个弱版本的集中系数展开，该系数衡量了行为策略与专家策略之间的偏离程度。在这个新的框架下，我们进一步研究了算法设计的问题：能否开发出一种实现最小极大最优性的算法？",
    "tldr": "本论文提出了一种新的联机强化学习框架，通过平滑插值的方式将模仿学习和纯联机强化学习统一起来。框架围绕着一种衡量行为策略与专家策略偏离程度的弱版本集中系数展开。通过该框架，研究者进一步研究了算法设计的问题：能否开发出实现最小极大最优性的算法？",
    "en_tdlr": "This paper presents a new framework for offline reinforcement learning that bridges the gap between imitation learning and vanilla offline RL by smoothly interpolating between the two. The framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy. The paper also investigates the question of algorithm design, aiming to develop an algorithm that achieves minimax optimality."
}