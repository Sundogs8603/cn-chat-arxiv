{
    "title": "Co-Imitation Learning without Expert Demonstration. (arXiv:2103.14823v2 [cs.LG] UPDATED)",
    "abstract": "Imitation learning is a primary approach to improve the efficiency of reinforcement learning by exploiting the expert demonstrations. However, in many real scenarios, obtaining expert demonstrations could be extremely expensive or even impossible. To overcome this challenge, in this paper, we propose a novel learning framework called Co-Imitation Learning (CoIL) to exploit the past good experiences of the agents themselves without expert demonstration. Specifically, we train two different agents via letting each of them alternately explore the environment and exploit the peer agent's experience. While the experiences could be valuable or misleading, we propose to estimate the potential utility of each piece of experience with the expected gain of the value function. Thus the agents can selectively imitate from each other by emphasizing the more useful experiences while filtering out noisy ones. Experimental results on various tasks show significant superiority of the proposed Co-Imitat",
    "link": "http://arxiv.org/abs/2103.14823",
    "context": "Title: Co-Imitation Learning without Expert Demonstration. (arXiv:2103.14823v2 [cs.LG] UPDATED)\nAbstract: Imitation learning is a primary approach to improve the efficiency of reinforcement learning by exploiting the expert demonstrations. However, in many real scenarios, obtaining expert demonstrations could be extremely expensive or even impossible. To overcome this challenge, in this paper, we propose a novel learning framework called Co-Imitation Learning (CoIL) to exploit the past good experiences of the agents themselves without expert demonstration. Specifically, we train two different agents via letting each of them alternately explore the environment and exploit the peer agent's experience. While the experiences could be valuable or misleading, we propose to estimate the potential utility of each piece of experience with the expected gain of the value function. Thus the agents can selectively imitate from each other by emphasizing the more useful experiences while filtering out noisy ones. Experimental results on various tasks show significant superiority of the proposed Co-Imitat",
    "path": "papers/21/03/2103.14823.json",
    "total_tokens": 848,
    "translated_title": "无需专家示范的共同模仿学习",
    "translated_abstract": "模仿学习是一种通过利用专家示范来提高强化学习效率的主要方法。然而，在许多实际场景中，获取专家示范可能非常昂贵甚至不可能。为了解决这个挑战，本文提出了一种名为共同模仿学习（CoIL）的新型学习框架，以利用代理选择探索环境并利用同伴代理的经验。尽管这些经验可能有价值，也可能误导人，我们提出通过期望增益的价值函数来估计每个经验的潜在效用。因此，代理可以通过强调更有用的经验并过滤掉噪声来选择性地互相模仿。各种任务的实验结果显示了所提出的共同模仿学习方法的显著优越性。",
    "tldr": "本文提出了一种名为共同模仿学习（CoIL）的新型学习框架，通过利用代理自身的良好经验，而无需专家示范，来改善强化学习的效率。实验结果表明，该方法在各种任务上都具有显著的优越性。",
    "en_tdlr": "This paper proposes a novel learning framework called Co-Imitation Learning (CoIL) to improve the efficiency of reinforcement learning by exploiting the agents' own good experiences without expert demonstration. Experimental results demonstrate the significant superiority of this method in various tasks."
}