{
    "title": "SparseGAN: Sparse Generative Adversarial Network for Text Generation. (arXiv:2103.11578v2 [cs.CL] UPDATED)",
    "abstract": "It is still a challenging task to learn a neural text generation model under the framework of generative adversarial networks (GANs) since the entire training process is not differentiable. The existing training strategies either suffer from unreliable gradient estimations or imprecise sentence representations. Inspired by the principle of sparse coding, we propose a SparseGAN that generates semantic-interpretable, but sparse sentence representations as inputs to the discriminator. The key idea is that we treat an embedding matrix as an over-complete dictionary, and use a linear combination of very few selected word embeddings to approximate the output feature representation of the generator at each time step. With such semantic-rich representations, we not only reduce unnecessary noises for efficient adversarial training, but also make the entire training process fully differentiable. Experiments on multiple text generation datasets yield performance improvements, especially in sequen",
    "link": "http://arxiv.org/abs/2103.11578",
    "context": "Title: SparseGAN: Sparse Generative Adversarial Network for Text Generation. (arXiv:2103.11578v2 [cs.CL] UPDATED)\nAbstract: It is still a challenging task to learn a neural text generation model under the framework of generative adversarial networks (GANs) since the entire training process is not differentiable. The existing training strategies either suffer from unreliable gradient estimations or imprecise sentence representations. Inspired by the principle of sparse coding, we propose a SparseGAN that generates semantic-interpretable, but sparse sentence representations as inputs to the discriminator. The key idea is that we treat an embedding matrix as an over-complete dictionary, and use a linear combination of very few selected word embeddings to approximate the output feature representation of the generator at each time step. With such semantic-rich representations, we not only reduce unnecessary noises for efficient adversarial training, but also make the entire training process fully differentiable. Experiments on multiple text generation datasets yield performance improvements, especially in sequen",
    "path": "papers/21/03/2103.11578.json",
    "total_tokens": 987,
    "translated_title": "SparseGAN: 稀疏生成对抗网络用于文本生成",
    "translated_abstract": "在生成对抗网络（GAN）的框架下，学习神经文本生成模型仍然是一个具有挑战性的任务，因为整个训练过程不可微分。现有的训练策略要么受到不可靠的梯度估计的困扰，要么存在不准确的句子表示。受到稀疏编码原理的启发，我们提出了一种称为SparseGAN的方法，该方法生成语义可解释且稀疏的句子表示作为鉴别器的输入。关键思想是将嵌入矩阵视为一个超完备的词典，并使用极少数的选定词嵌入的线性组合来逼近生成器在每个时间步的输出特征表示。通过这样的语义丰富表示，我们不仅可以减少不必要的噪音以实现有效的对抗训练，还可以使整个训练过程完全可微分。在多个文本生成数据集上的实验证明了性能的提高，特别是在序列生成中。",
    "tldr": "SparseGAN是一种稀疏生成对抗网络，通过生成语义可解释且稀疏的句子表示作为输入，实现了在生成对抗网络框架下的神经文本生成模型的学习。通过使用稀疏编码的思想，SparseGAN有效地减少了噪音并实现了完全可微分的训练过程。实验证明SparseGAN在多个文本生成数据集上取得了性能的提升，特别是在序列生成任务中。"
}