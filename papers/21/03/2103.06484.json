{
    "title": "Robust High-speed Running for Quadruped Robots via Deep Reinforcement Learning. (arXiv:2103.06484v2 [cs.RO] UPDATED)",
    "abstract": "Deep reinforcement learning has emerged as a popular and powerful way to develop locomotion controllers for quadruped robots. Common approaches have largely focused on learning actions directly in joint space, or learning to modify and offset foot positions produced by trajectory generators. Both approaches typically require careful reward shaping and training for millions of time steps, and with trajectory generators introduce human bias into the resulting control policies. In this paper, we present a learning framework that leads to the natural emergence of fast and robust bounding policies for quadruped robots. The agent both selects and controls actions directly in task space to track desired velocity commands subject to environmental noise including model uncertainty and rough terrain. We observe that this framework improves sample efficiency, necessitates little reward shaping, leads to the emergence of natural gaits such as galloping and bounding, and eases the sim-to-real trans",
    "link": "http://arxiv.org/abs/2103.06484",
    "context": "Title: Robust High-speed Running for Quadruped Robots via Deep Reinforcement Learning. (arXiv:2103.06484v2 [cs.RO] UPDATED)\nAbstract: Deep reinforcement learning has emerged as a popular and powerful way to develop locomotion controllers for quadruped robots. Common approaches have largely focused on learning actions directly in joint space, or learning to modify and offset foot positions produced by trajectory generators. Both approaches typically require careful reward shaping and training for millions of time steps, and with trajectory generators introduce human bias into the resulting control policies. In this paper, we present a learning framework that leads to the natural emergence of fast and robust bounding policies for quadruped robots. The agent both selects and controls actions directly in task space to track desired velocity commands subject to environmental noise including model uncertainty and rough terrain. We observe that this framework improves sample efficiency, necessitates little reward shaping, leads to the emergence of natural gaits such as galloping and bounding, and eases the sim-to-real trans",
    "path": "papers/21/03/2103.06484.json",
    "total_tokens": 933,
    "translated_abstract": "深度强化学习已成为发展四足机器人运动控制器的流行和强大方式。常见方法主要集中在直接在关节空间学习动作，或学习修改和偏移轨迹生成器产生的足部位置。这两种方法通常需要仔细的奖励设计和数百万次的训练，并且使用轨迹生成器会在产生的控制策略中引入人类偏见。在本文中，我们提出了一种学习框架，导致四足机器人自然出现快速和强健的腾跃策略。代理人在任务空间中直接选择和控制动作，以跟踪所需的速度命令，受到包括模型不确定性和崎岖地形在内的环境噪声的影响。我们观察到，这个框架提高了样本效率，需要很少的奖励设计，导致自然步态的出现，如马跑和腾跃，并且简化了从仿真到真实环境的转换。",
    "tldr": "本文提出了一种学习框架，能够自然出现快速和强健的四足机器人腾跃策略，并在任务空间中直接选择和控制动作，提高样本效率，需要很少的奖励设计。",
    "en_tdlr": "This paper presents a learning framework that leads to the natural emergence of fast and robust bounding policies for quadruped robots, which selects and controls actions directly in task space to track desired velocity commands subject to environmental noise, improves sample efficiency, and requires little reward shaping."
}