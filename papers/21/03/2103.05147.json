{
    "title": "Model-free Policy Learning with Reward Gradients. (arXiv:2103.05147v4 [cs.LG] UPDATED)",
    "abstract": "Despite the increasing popularity of policy gradient methods, they are yet to be widely utilized in sample-scarce applications, such as robotics. The sample efficiency could be improved by making best usage of available information. As a key component in reinforcement learning, the reward function is usually devised carefully to guide the agent. Hence, the reward function is usually known, allowing access to not only scalar reward signals but also reward gradients. To benefit from reward gradients, previous works require the knowledge of environment dynamics, which are hard to obtain. In this work, we develop the \\textit{Reward Policy Gradient} estimator, a novel approach that integrates reward gradients without learning a model. Bypassing the model dynamics allows our estimator to achieve a better bias-variance trade-off, which results in a higher sample efficiency, as shown in the empirical analysis. Our method also boosts the performance of Proximal Policy Optimization on different ",
    "link": "http://arxiv.org/abs/2103.05147",
    "context": "Title: Model-free Policy Learning with Reward Gradients. (arXiv:2103.05147v4 [cs.LG] UPDATED)\nAbstract: Despite the increasing popularity of policy gradient methods, they are yet to be widely utilized in sample-scarce applications, such as robotics. The sample efficiency could be improved by making best usage of available information. As a key component in reinforcement learning, the reward function is usually devised carefully to guide the agent. Hence, the reward function is usually known, allowing access to not only scalar reward signals but also reward gradients. To benefit from reward gradients, previous works require the knowledge of environment dynamics, which are hard to obtain. In this work, we develop the \\textit{Reward Policy Gradient} estimator, a novel approach that integrates reward gradients without learning a model. Bypassing the model dynamics allows our estimator to achieve a better bias-variance trade-off, which results in a higher sample efficiency, as shown in the empirical analysis. Our method also boosts the performance of Proximal Policy Optimization on different ",
    "path": "papers/21/03/2103.05147.json",
    "total_tokens": 878,
    "translated_title": "无模型的基于奖赏梯度的策略学习",
    "translated_abstract": "尽管策略梯度方法越来越受欢迎，但在样本稀缺的应用中，如机器人学，它们仍未被广泛利用。通过充分利用可用信息，可以提高样本效率。作为增强学习中的重要组成部分，奖赏函数通常被精心设计以引导智能体。因此，奖赏函数通常是已知的，可以访问标量奖赏信号和奖赏梯度。为了从奖赏梯度中获益，之前的工作需要了解环境动态，这是很难获得的。在这项工作中，我们开发了一种新颖的方法——奖赏策略梯度估计器，它可以集成奖赏梯度而无需学习模型。绕过模型动态使我们的估计器能够在偏差-方差权衡方面取得更好的效果，这导致更高的样本效率，如经验分析所示。我们的方法还提升了不同环境下的近端策略优化的性能。",
    "tldr": "本研究开发了一种新颖的方法，无需学习模型即可集成奖赏梯度，提高了策略学习的样本效率。"
}