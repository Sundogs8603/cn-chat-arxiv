{
    "title": "DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer. (arXiv:2103.10206v5 [cs.AI] UPDATED)",
    "abstract": "Generating 3D dances from music is an emerged research task that benefits a lot of applications in vision and graphics. Previous works treat this task as sequence generation, however, it is challenging to render a music-aligned long-term sequence with high kinematic complexity and coherent movements. In this paper, we reformulate it by a two-stage process, ie, a key pose generation and then an in-between parametric motion curve prediction, where the key poses are easier to be synchronized with the music beats and the parametric curves can be efficiently regressed to render fluent rhythm-aligned movements. We named the proposed method as DanceFormer, which includes two cascading kinematics-enhanced transformer-guided networks (called DanTrans) that tackle each stage, respectively. Furthermore, we propose a large-scale music conditioned 3D dance dataset, called PhantomDance, that is accurately labeled by experienced animators rather than reconstruction or motion capture. This dataset als",
    "link": "http://arxiv.org/abs/2103.10206",
    "context": "Title: DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer. (arXiv:2103.10206v5 [cs.AI] UPDATED)\nAbstract: Generating 3D dances from music is an emerged research task that benefits a lot of applications in vision and graphics. Previous works treat this task as sequence generation, however, it is challenging to render a music-aligned long-term sequence with high kinematic complexity and coherent movements. In this paper, we reformulate it by a two-stage process, ie, a key pose generation and then an in-between parametric motion curve prediction, where the key poses are easier to be synchronized with the music beats and the parametric curves can be efficiently regressed to render fluent rhythm-aligned movements. We named the proposed method as DanceFormer, which includes two cascading kinematics-enhanced transformer-guided networks (called DanTrans) that tackle each stage, respectively. Furthermore, we propose a large-scale music conditioned 3D dance dataset, called PhantomDance, that is accurately labeled by experienced animators rather than reconstruction or motion capture. This dataset als",
    "path": "papers/21/03/2103.10206.json",
    "total_tokens": 1003,
    "translated_title": "DanceFormer: 基于音乐的参数化运动变换器生成的3D舞蹈生成",
    "translated_abstract": "从音乐中生成3D舞蹈是一项新兴的研究任务，对视觉和图形领域有很多应用。以往的工作将这个任务视为序列生成，然而，要渲染一段与音乐对齐的、具有高动力学复杂性和连贯运动的长期序列是具有挑战性的。本文通过一个两阶段的过程进行重构，即先生成关键姿势，然后预测参数化运动曲线，其中关键姿势更容易与音乐节拍同步，参数化曲线可以有效地回归以渲染平滑的节奏对齐运动。我们将提出的方法命名为DanceFormer，其中包括两个级联的增强动力学变换器引导网络（称为DanTrans），分别解决每个阶段的问题。此外，我们提出了一个大规模的基于音乐的3D舞蹈数据集PhantomDance，该数据集由经验丰富的动画师准确标记，而不是重建或动作捕捉。",
    "tldr": "本文提出了一种名为DanceFormer的方法，通过两个级联的动力学增强的变换器引导网络来生成基于音乐的3D舞蹈。方法首先生成关键姿势，然后预测参数化运动曲线，使得舞蹈与音乐节奏对齐，并且提出了一个准确标记的大规模音乐条件下的3D舞蹈数据集PhantomDance。",
    "en_tdlr": "This paper proposes DanceFormer, a method that generates music-conditioned 3D dances using cascading kinematics-enhanced transformer-guided networks. It first generates key poses and then predicts parametric motion curves to achieve rhythm-aligned movements with the music. The paper also introduces a large-scale accurately labeled music-conditioned 3D dance dataset called PhantomDance."
}