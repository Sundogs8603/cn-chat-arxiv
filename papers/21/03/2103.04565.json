{
    "title": "Improving Transformation-based Defenses against Adversarial Examples with First-order Perturbations. (arXiv:2103.04565v3 [cs.CV] UPDATED)",
    "abstract": "Deep neural networks have been successfully applied in various machine learning tasks. However, studies show that neural networks are susceptible to adversarial attacks. This exposes a potential threat to neural network-based intelligent systems. We observe that the probability of the correct result outputted by the neural network increases by applying small first-order perturbations generated for non-predicted class labels to adversarial examples. Based on this observation, we propose a method for counteracting adversarial perturbations to improve adversarial robustness. In the proposed method, we randomly select a number of class labels and generate small first-order perturbations for these selected labels. The generated perturbations are added together and then clamped onto a specified space. The obtained perturbation is finally added to the adversarial example to counteract the adversarial perturbation contained in the example. The proposed method is applied at inference time and d",
    "link": "http://arxiv.org/abs/2103.04565",
    "context": "Title: Improving Transformation-based Defenses against Adversarial Examples with First-order Perturbations. (arXiv:2103.04565v3 [cs.CV] UPDATED)\nAbstract: Deep neural networks have been successfully applied in various machine learning tasks. However, studies show that neural networks are susceptible to adversarial attacks. This exposes a potential threat to neural network-based intelligent systems. We observe that the probability of the correct result outputted by the neural network increases by applying small first-order perturbations generated for non-predicted class labels to adversarial examples. Based on this observation, we propose a method for counteracting adversarial perturbations to improve adversarial robustness. In the proposed method, we randomly select a number of class labels and generate small first-order perturbations for these selected labels. The generated perturbations are added together and then clamped onto a specified space. The obtained perturbation is finally added to the adversarial example to counteract the adversarial perturbation contained in the example. The proposed method is applied at inference time and d",
    "path": "papers/21/03/2103.04565.json",
    "total_tokens": 861,
    "translated_title": "提高基于变换的对抗示例防御方法的一阶扰动抵抗性",
    "translated_abstract": "深度神经网络在各种机器学习任务中取得了成功。然而，研究表明神经网络容易受到对抗攻击，这对基于神经网络的智能系统构成了潜在威胁。我们观察到通过对对抗示例生成针对非预测标签的小一阶扰动可以增加神经网络输出结果为正确结果的概率。基于这个观察，我们提出了一种抵抗对抗扰动以提高对抗鲁棒性的方法。在提出的方法中，我们随机选择若干类别标签，并为这些选定的标签生成小的一阶扰动。生成的扰动加在一起后被限制在指定的空间内。最终得到的扰动被添加到对抗示例中，以抵消其中的对抗扰动。所提出的方法在推断时应用。",
    "tldr": "该论文提出了一种改进基于变换的对抗示例防御方法的方法，通过对对抗示例生成一阶扰动来增加神经网络输出正确结果的概率，并在推断时应用这种方法。"
}