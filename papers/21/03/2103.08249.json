{
    "title": "Evolving parametrized Loss for Image Classification Learning on Small Datasets. (arXiv:2103.08249v2 [cs.AI] UPDATED)",
    "abstract": "This paper proposes a meta-learning approach to evolving a parametrized loss function, which is called Meta-Loss Network (MLN), for training the image classification learning on small datasets. In our approach, the MLN is embedded in the framework of classification learning as a differentiable objective function. The MLN is evolved with the Evolutionary Strategy algorithm (ES) to an optimized loss function, such that a classifier, which optimized to minimize this loss, will achieve a good generalization effect. A classifier learns on a small training dataset to minimize MLN with Stochastic Gradient Descent (SGD), and then the MLN is evolved with the precision of the small-dataset-updated classifier on a large validation dataset. In order to evaluate our approach, the MLN is trained with a large number of small sample learning tasks sampled from FashionMNIST and tested on validation tasks sampled from FashionMNIST and CIFAR10. Experiment results demonstrate that the MLN effectively impr",
    "link": "http://arxiv.org/abs/2103.08249",
    "context": "Title: Evolving parametrized Loss for Image Classification Learning on Small Datasets. (arXiv:2103.08249v2 [cs.AI] UPDATED)\nAbstract: This paper proposes a meta-learning approach to evolving a parametrized loss function, which is called Meta-Loss Network (MLN), for training the image classification learning on small datasets. In our approach, the MLN is embedded in the framework of classification learning as a differentiable objective function. The MLN is evolved with the Evolutionary Strategy algorithm (ES) to an optimized loss function, such that a classifier, which optimized to minimize this loss, will achieve a good generalization effect. A classifier learns on a small training dataset to minimize MLN with Stochastic Gradient Descent (SGD), and then the MLN is evolved with the precision of the small-dataset-updated classifier on a large validation dataset. In order to evaluate our approach, the MLN is trained with a large number of small sample learning tasks sampled from FashionMNIST and tested on validation tasks sampled from FashionMNIST and CIFAR10. Experiment results demonstrate that the MLN effectively impr",
    "path": "papers/21/03/2103.08249.json",
    "total_tokens": 844,
    "translated_title": "在小数据集上学习的图像分类中进化参数化损失函数的方法",
    "translated_abstract": "本文提出了一种元学习方法来进化参数化损失函数，称为元损失网络（MLN），用于在小数据集上进行图像分类学习。在我们的方法中，MLN被嵌入到分类学习的框架中作为一个可微的目标函数。通过用进化策略算法（ES）来优化损失函数演变MLN，使得通过最小化该损失优化的分类器能够达到很好的泛化效果。分类器在小训练数据集上使用随机梯度下降（SGD）来最小化MLN，然后MLN根据小数据集更新的分类器在大验证数据集上进行演化。为了评估我们的方法，使用FashionMNIST采样了大量小样本学习任务来训练MLN，并在FashionMNIST和CIFAR10采样的验证任务上进行测试。实验结果表明MLN能够有效地改善分类性能。",
    "tldr": "本文提出了一种在小数据集上进行图像分类学习的方法，通过元学习的方式进化参数化损失函数，实验结果表明该方法能够有效地提高分类性能。"
}