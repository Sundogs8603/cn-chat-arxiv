{
    "title": "Local Interpretations for Explainable Natural Language Processing: A Survey",
    "abstract": "arXiv:2103.11072v3 Announce Type: replace-cross  Abstract: As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for Natural Language Processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term interpretability and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are specifically divided into three categories: 1) interpreting the model's predictions through related input features; 2) interpreting through natural language explanation; 3) probing the hidden states of models and word representations.",
    "link": "https://arxiv.org/abs/2103.11072",
    "context": "Title: Local Interpretations for Explainable Natural Language Processing: A Survey\nAbstract: arXiv:2103.11072v3 Announce Type: replace-cross  Abstract: As the use of deep learning techniques has grown across various fields over the past decade, complaints about the opaqueness of the black-box models have increased, resulting in an increased focus on transparency in deep learning models. This work investigates various methods to improve the interpretability of deep neural networks for Natural Language Processing (NLP) tasks, including machine translation and sentiment analysis. We provide a comprehensive discussion on the definition of the term interpretability and its various aspects at the beginning of this work. The methods collected and summarised in this survey are only associated with local interpretation and are specifically divided into three categories: 1) interpreting the model's predictions through related input features; 2) interpreting through natural language explanation; 3) probing the hidden states of models and word representations.",
    "path": "papers/21/03/2103.11072.json",
    "total_tokens": 824,
    "translated_title": "可解释自然语言处理的本地解释：一项调查",
    "translated_abstract": "随着过去十年间深度学习技术在各个领域的应用不断增长，关于黑盒模型不透明性的抱怨也在增加，导致深度学习模型透明度受到更多关注。本研究探讨了改善深度神经网络在自然语言处理（NLP）任务中的可解释性的各种方法，包括机器翻译和情感分析。我们在本研究的开始阶段对可解释性一词及其各个方面进行了全面讨论。本次调查中收集和总结的方法仅涉及局部解释，并具体分为三类：1）通过相关输入特征解释模型的预测；2）通过自然语言解释进行解释；3）探测模型的隐藏状态和单词表示。",
    "tldr": "本文调查了改善深度神经网络在自然语言处理任务中可解释性的各种方法，特别关注局部解释，包括与相关输入特征相关的预测解释、自然语言解释以及探测模型隐藏状态和词表示。",
    "en_tdlr": "This work investigates various methods to improve the interpretability of deep neural networks for Natural Language Processing tasks, focusing on local interpretations, including interpreting the model's predictions through related input features, natural language explanations, and probing the hidden states of models and word representations."
}