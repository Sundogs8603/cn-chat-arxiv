{
    "title": "The Low-Rank Simplicity Bias in Deep Networks. (arXiv:2103.10427v4 [cs.LG] UPDATED)",
    "abstract": "Modern deep neural networks are highly over-parameterized compared to the data on which they are trained, yet they often generalize remarkably well. A flurry of recent work has asked: why do deep networks not overfit to their training data? In this work, we make a series of empirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to find solutions with lower effective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low effective rank embedding increases with depth. We show empirically that our claim holds true on finite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well. We then show that the simplicity bias exists at both initialization and after training and is resilient to hyper-parameters and learning methods. We further demonstrate how linear over-parameterization of deep non-linear ",
    "link": "http://arxiv.org/abs/2103.10427",
    "context": "Title: The Low-Rank Simplicity Bias in Deep Networks. (arXiv:2103.10427v4 [cs.LG] UPDATED)\nAbstract: Modern deep neural networks are highly over-parameterized compared to the data on which they are trained, yet they often generalize remarkably well. A flurry of recent work has asked: why do deep networks not overfit to their training data? In this work, we make a series of empirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to find solutions with lower effective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low effective rank embedding increases with depth. We show empirically that our claim holds true on finite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well. We then show that the simplicity bias exists at both initialization and after training and is resilient to hyper-parameters and learning methods. We further demonstrate how linear over-parameterization of deep non-linear ",
    "path": "papers/21/03/2103.10427.json",
    "total_tokens": 1163,
    "translated_title": "深度网络中的低秩简单性偏好",
    "translated_abstract": "现代深度神经网络通常表现出惊人的泛化能力，本文探究和拓展了更深层网络具有归纳偏见，以寻找低有效秩嵌入解决方案的假设，通过实证证明了该偏好在有限宽度线性和非线性模型上的实用性并且具有鲁棒性。进一步展示了通过对神经网络的线性过参数化来实现深度非线性模型。",
    "tldr": "本文研究了现代深度神经网络的泛化能力及可能的原因，发现深度网络具有归纳偏见，更倾向于寻找低有效秩嵌入的解决方案，并通过实证证明了该偏好在有限宽度线性和非线性模型上的实用性和鲁棒性。",
    "en_tdlr": "This paper investigates the remarkable generalization ability of modern deep neural networks and explores the hypothesis that deeper networks are inductively biased to find solutions with lower rank embeddings. The empirical evidence supports this hypothesis on finite width linear and non-linear models and shows that the simplicity bias exists at both initialization and after training, and is resilient to hyper-parameters and learning methods. This paper further demonstrates how linear over-parameterization could implement deep non-linear models."
}