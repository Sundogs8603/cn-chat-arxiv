{
    "title": "FedPower: Privacy-Preserving Distributed Eigenspace Estimation. (arXiv:2103.00704v2 [stat.ML] UPDATED)",
    "abstract": "Eigenspace estimation is fundamental in machine learning and statistics, which has found applications in PCA, dimension reduction, and clustering, among others. The modern machine learning community usually assumes that data come from and belong to different organizations. The low communication power and the possible privacy breaches of data make the computation of eigenspace challenging. To address these challenges, we propose a class of algorithms called \\textsf{FedPower} within the federated learning (FL) framework. \\textsf{FedPower} leverages the well-known power method by alternating multiple local power iterations and a global aggregation step, thus improving communication efficiency. In the aggregation, we propose to weight each local eigenvector matrix with {\\it Orthogonal Procrustes Transformation} (OPT) for better alignment. To ensure strong privacy protection, we add Gaussian noise in each iteration by adopting the notion of \\emph{differential privacy} (DP). We provide conve",
    "link": "http://arxiv.org/abs/2103.00704",
    "context": "Title: FedPower: Privacy-Preserving Distributed Eigenspace Estimation. (arXiv:2103.00704v2 [stat.ML] UPDATED)\nAbstract: Eigenspace estimation is fundamental in machine learning and statistics, which has found applications in PCA, dimension reduction, and clustering, among others. The modern machine learning community usually assumes that data come from and belong to different organizations. The low communication power and the possible privacy breaches of data make the computation of eigenspace challenging. To address these challenges, we propose a class of algorithms called \\textsf{FedPower} within the federated learning (FL) framework. \\textsf{FedPower} leverages the well-known power method by alternating multiple local power iterations and a global aggregation step, thus improving communication efficiency. In the aggregation, we propose to weight each local eigenvector matrix with {\\it Orthogonal Procrustes Transformation} (OPT) for better alignment. To ensure strong privacy protection, we add Gaussian noise in each iteration by adopting the notion of \\emph{differential privacy} (DP). We provide conve",
    "path": "papers/21/03/2103.00704.json",
    "total_tokens": 961,
    "translated_title": "FedPower: 隐私保护的分布式特征空间估计",
    "translated_abstract": "特征空间估计是机器学习和统计学中的基础问题，在主成分分析、维度约简和聚类等领域都有应用。现代机器学习通常假设数据来自不同的组织且属于不同组织。数据的低通信能力和可能的隐私泄漏使得特征空间的计算具有挑战性。为了解决这些挑战，我们在联邦学习（FL）框架内提出了一类算法，称为FedPower。FedPower利用了著名的幂法，通过交替进行多个本地幂迭代和全局聚合步骤，从而提高通信效率。在聚合中，我们提出使用正交Procrustes变换（OPT）对每个本地特征向量矩阵进行加权以实现更好的对齐。为了确保强大的隐私保护，我们在每次迭代中添加高斯噪声，采用差分隐私（DP）的概念。我们提供了...",
    "tldr": "本文提出了一种称为FedPower的算法，在联邦学习框架内解决了特征空间估计的隐私和通信效率问题。算法利用幂法进行本地迭代和全局聚合，采用正交Procrustes变换加权以实现对齐，并引入差分隐私以保护数据隐私。",
    "en_tdlr": "This paper proposes an algorithm called FedPower to address the privacy and communication efficiency issues in eigenspace estimation within the federated learning framework. The algorithm leverages the power method for local iterations and global aggregation, applies weighted Orthogonal Procrustes Transformation for alignment, and introduces differential privacy for data privacy protection."
}