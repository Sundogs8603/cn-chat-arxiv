{
    "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth. (arXiv:2103.03404v2 [cs.LG] UPDATED)",
    "abstract": "Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards \"token uniformity\". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.",
    "link": "http://arxiv.org/abs/2103.03404",
    "context": "Title: Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth. (arXiv:2103.03404v2 [cs.LG] UPDATED)\nAbstract: Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards \"token uniformity\". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.",
    "path": "papers/21/03/2103.03404.json",
    "total_tokens": 819,
    "translated_title": "注意力不是答案：纯注意力在深度方面以双指数方式降低等级",
    "translated_abstract": "基于注意力的架构已经在机器学习中变得无处不在，然而我们对其有效性的原因的理解仍然有限。这项工作提出了一种新的理解自注意力网络的方法：我们展示了它们的输出可以分解为较小项的求和，每个项涉及在层间操作的一系列注意力头。利用这种分解，我们证明了自注意力对“标记均衡性”的强归纳偏见。具体地说，没有跳跃连接或多层感知机（MLP），输出会以双指数的方式收敛到一个秩为1的矩阵。另一方面，跳跃连接和MLP可以阻止输出的退化。我们的实验验证了所识别的收敛现象在不同变体的标准Transformer架构上的存在。",
    "tldr": "这项研究提出了一种新的方法来理解自注意力网络，通过将其输出分解为较小的项，证明自注意力对“标记均衡性”的强归纳偏见，而跳跃连接和多层感知机可以阻止输出的退化。"
}