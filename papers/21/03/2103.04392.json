{
    "title": "A Retrospective Approximation Approach for Smooth Stochastic Optimization",
    "abstract": "arXiv:2103.04392v3 Announce Type: replace-cross  Abstract: Stochastic Gradient (SG) is the defacto iterative technique to solve stochastic optimization (SO) problems with a smooth (non-convex) objective $f$ and a stochastic first-order oracle. SG's attractiveness is due in part to its simplicity of executing a single step along the negative subsampled gradient direction to update the incumbent iterate. In this paper, we question SG's choice of executing a single step as opposed to multiple steps between subsample updates. Our investigation leads naturally to generalizing SG into Retrospective Approximation (RA) where, during each iteration, a \"deterministic solver\" executes possibly multiple steps on a subsampled deterministic problem and stops when further solving is deemed unnecessary from the standpoint of statistical efficiency. RA thus rigorizes what is appealing for implementation -- during each iteration, \"plug in\" a solver, e.g., L-BFGS line search or Newton-CG, as is, and solv",
    "link": "https://arxiv.org/abs/2103.04392",
    "context": "Title: A Retrospective Approximation Approach for Smooth Stochastic Optimization\nAbstract: arXiv:2103.04392v3 Announce Type: replace-cross  Abstract: Stochastic Gradient (SG) is the defacto iterative technique to solve stochastic optimization (SO) problems with a smooth (non-convex) objective $f$ and a stochastic first-order oracle. SG's attractiveness is due in part to its simplicity of executing a single step along the negative subsampled gradient direction to update the incumbent iterate. In this paper, we question SG's choice of executing a single step as opposed to multiple steps between subsample updates. Our investigation leads naturally to generalizing SG into Retrospective Approximation (RA) where, during each iteration, a \"deterministic solver\" executes possibly multiple steps on a subsampled deterministic problem and stops when further solving is deemed unnecessary from the standpoint of statistical efficiency. RA thus rigorizes what is appealing for implementation -- during each iteration, \"plug in\" a solver, e.g., L-BFGS line search or Newton-CG, as is, and solv",
    "path": "papers/21/03/2103.04392.json",
    "total_tokens": 735,
    "translated_title": "一种平滑随机优化的回顾逼近方法",
    "translated_abstract": "随机梯度（SG）是解决具有平滑（非凸）目标函数$f$和随机一阶oracle的随机优化（SO）问题的事实上的迭代技术。 在每次迭代期间，\"确定性求解器\"对子采样的确定性问题执行可能多个步骤，并在从统计效率的角度视为进一步求解不必要时停止。 因此，RA系统地将引人注目的内容变得严谨--在每次迭代期间，“插入”一个解算器，例如，L-BFGS线搜索或Newton-CG，并解决。",
    "tldr": "将随机梯度的单步执行方式推广为回顾逼近方法（RA），在每次迭代期间通过可能多步的确定性求解器提高统计效率。",
    "en_tdlr": "Generalizing the single-step approach of stochastic gradient (SG) into Retrospective Approximation (RA), utilizing a deterministic solver to improve statistical efficiency by taking multiple steps during each iteration."
}