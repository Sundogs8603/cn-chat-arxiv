{
    "title": "Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks",
    "abstract": "arXiv:2105.13937v3 Announce Type: replace  Abstract: We present a new class of Langevin based algorithms, which overcomes many of the known shortcomings of popular adaptive optimizers that are currently used for the fine tuning of deep learning models. Its underpinning theory relies on recent advances of Euler's polygonal approximations for stochastic differential equations (SDEs) with monotone coefficients. As a result, it inherits the stability properties of tamed algorithms, while it addresses other known issues, e.g. vanishing gradients in neural networks. In particular, we provide a nonasymptotic analysis and full theoretical guarantees for the convergence properties of an algorithm of this novel class, which we named TH$\\varepsilon$O POULA (or, simply, TheoPouLa). Finally, several experiments are presented with different types of deep learning models, which show the superior performance of TheoPouLa over many popular adaptive optimization algorithms.",
    "link": "https://arxiv.org/abs/2105.13937",
    "context": "Title: Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks\nAbstract: arXiv:2105.13937v3 Announce Type: replace  Abstract: We present a new class of Langevin based algorithms, which overcomes many of the known shortcomings of popular adaptive optimizers that are currently used for the fine tuning of deep learning models. Its underpinning theory relies on recent advances of Euler's polygonal approximations for stochastic differential equations (SDEs) with monotone coefficients. As a result, it inherits the stability properties of tamed algorithms, while it addresses other known issues, e.g. vanishing gradients in neural networks. In particular, we provide a nonasymptotic analysis and full theoretical guarantees for the convergence properties of an algorithm of this novel class, which we named TH$\\varepsilon$O POULA (or, simply, TheoPouLa). Finally, several experiments are presented with different types of deep learning models, which show the superior performance of TheoPouLa over many popular adaptive optimization algorithms.",
    "path": "papers/21/05/2105.13937.json",
    "total_tokens": 907,
    "translated_title": "多边形未调整的朗之万算法：为神经网络创建稳定高效的自适应算法",
    "translated_abstract": "我们提出了一种新的基于朗之万算法的算法类别，克服了当前用于微调深度学习模型的流行自适应优化器的许多已知缺陷。其理论基础依赖于近期对于具有单调系数的随机微分方程（SDEs）的欧拉多边形逼近的发展。因此，它继承了温和算法的稳定性特性，同时解决了神经网络中的其他已知问题，例如梯度消失。特别地，我们对这个新类别算法的收敛特性进行了非渐进分析和全面的理论保证，我们将这个算法命名为TH$\\varepsilon$O POULA（或简称为TheoPouLa）。最后，我们展示了使用不同类型的深度学习模型进行的几个实验，结果表明TheoPouLa相对于许多流行的自适应优化算法具有卓越性能。",
    "tldr": "提出了一种基于多边形未调整的朗之万算法的新类别算法，名为TH$\\varepsilon$O POULA（或简称为TheoPouLa），通过稳定性、非渐进分析和实验表明其在神经网络优化中具有卓越性能。",
    "en_tdlr": "Introduced a new class of algorithms based on polygonal unadjusted Langevin algorithms, named TH$\\varepsilon$O POULA (or TheoPouLa for short), demonstrating superior performance in neural network optimization through stability, non-asymptotic analysis, and experiments."
}