{
    "title": "AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks. (arXiv:2105.10190v2 [cs.LG] UPDATED)",
    "abstract": "Convolutional neural networks (CNNs) are trained using stochastic gradient descent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam) optimizer has become very popular due to its adaptive momentum, which tackles the dying gradient problem of SGD. Nevertheless, existing optimizers are still unable to exploit the optimization curvature information efficiently. This paper proposes a new AngularGrad optimizer that considers the behavior of the direction/angle of consecutive gradients. This is the first attempt in the literature to exploit the gradient angular information apart from its magnitude. The proposed AngularGrad generates a score to control the step size based on the gradient angular information of previous iterations. Thus, the optimization steps become smoother as a more accurate step size of immediate past gradients is captured through the angular information. Two variants of AngularGrad are developed based on the use of Tangent or Cosine functions for comp",
    "link": "http://arxiv.org/abs/2105.10190",
    "context": "Title: AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks. (arXiv:2105.10190v2 [cs.LG] UPDATED)\nAbstract: Convolutional neural networks (CNNs) are trained using stochastic gradient descent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam) optimizer has become very popular due to its adaptive momentum, which tackles the dying gradient problem of SGD. Nevertheless, existing optimizers are still unable to exploit the optimization curvature information efficiently. This paper proposes a new AngularGrad optimizer that considers the behavior of the direction/angle of consecutive gradients. This is the first attempt in the literature to exploit the gradient angular information apart from its magnitude. The proposed AngularGrad generates a score to control the step size based on the gradient angular information of previous iterations. Thus, the optimization steps become smoother as a more accurate step size of immediate past gradients is captured through the angular information. Two variants of AngularGrad are developed based on the use of Tangent or Cosine functions for comp",
    "path": "papers/21/05/2105.10190.json",
    "total_tokens": 882,
    "translated_title": "AngularGrad：一种用于卷积神经网络角度收敛的新优化技术",
    "translated_abstract": "卷积神经网络(CNNs)使用基于随机梯度下降(SGD)的优化器进行训练。最近，自适应时刻估计(Adam)优化器因其自适应动量而变得非常流行，从而解决了SGD的梯度消失问题。然而，现有的优化器仍然无法有效利用优化曲率信息。本文提出了一种新的AngularGrad优化器，它考虑了连续梯度的方向/角度的行为。这是文献中第一次尝试利用梯度角度信息而不仅仅是梯度大小。所提出的AngularGrad根据先前迭代的梯度角度信息生成一个得分来控制步长。因此，通过角度信息捕获到更准确的近期梯度步长，优化步骤变得更加平滑。基于使用正切或余弦函数的两个AngularGrad变体得到了开发。",
    "tldr": "AngularGrad是一种新的优化器，通过考虑连续梯度的方向/角度行为来优化卷积神经网络的角度收敛。通过捕捉角度信息以获得更准确的步长，优化步骤变得更加平滑。",
    "en_tdlr": "AngularGrad is a new optimizer that optimizes the angular convergence of convolutional neural networks by considering the behavior of the direction/angle of consecutive gradients. The optimization steps become smoother by capturing more accurate step size through the angular information."
}