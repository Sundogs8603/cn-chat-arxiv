{
    "title": "Online Algorithms and Policies Using Adaptive and Machine Learning Approaches. (arXiv:2105.06577v7 [cs.LG] UPDATED)",
    "abstract": "This paper considers the problem of real-time control and learning in dynamic systems subjected to parametric uncertainties. We propose a combination of a Reinforcement Learning (RL) based policy in the outer loop suitably chosen to ensure stability and optimality for the nominal dynamics, together with Adaptive Control (AC) in the inner loop so that in real-time AC contracts the closed-loop dynamics towards a stable trajectory traced out by RL. Two classes of nonlinear dynamic systems are considered, both of which are control-affine. The first class of dynamic systems utilizes equilibrium points %with expansion forms around these points and a Lyapunov approach while second class of nonlinear systems uses contraction theory. AC-RL controllers are proposed for both classes of systems and shown to lead to online policies that guarantee stability using a high-order tuner and accommodate parametric uncertainties and magnitude limits on the input. In addition to establishing a stability gua",
    "link": "http://arxiv.org/abs/2105.06577",
    "context": "Title: Online Algorithms and Policies Using Adaptive and Machine Learning Approaches. (arXiv:2105.06577v7 [cs.LG] UPDATED)\nAbstract: This paper considers the problem of real-time control and learning in dynamic systems subjected to parametric uncertainties. We propose a combination of a Reinforcement Learning (RL) based policy in the outer loop suitably chosen to ensure stability and optimality for the nominal dynamics, together with Adaptive Control (AC) in the inner loop so that in real-time AC contracts the closed-loop dynamics towards a stable trajectory traced out by RL. Two classes of nonlinear dynamic systems are considered, both of which are control-affine. The first class of dynamic systems utilizes equilibrium points %with expansion forms around these points and a Lyapunov approach while second class of nonlinear systems uses contraction theory. AC-RL controllers are proposed for both classes of systems and shown to lead to online policies that guarantee stability using a high-order tuner and accommodate parametric uncertainties and magnitude limits on the input. In addition to establishing a stability gua",
    "path": "papers/21/05/2105.06577.json",
    "total_tokens": 928,
    "translated_title": "采用自适应和机器学习方法的在线算法与策略",
    "translated_abstract": "本文考虑了在受参数不确定性影响的动态系统中进行实时控制和学习的问题。我们提出了一种在外环中选择适当的强化学习（RL）策略，以确保对于名义动力学的稳定性和最优性，同时在内环中采用自适应控制（AC），以便在实时中，AC将闭环动力学收缩到由RL跟踪的稳定轨迹。考虑了两类非线性动态系统，两者都是控制关联的。第一类动态系统利用扩展形式的平衡点和李亚普诺夫方法，而第二类非线性系统则使用收缩理论。针对这两类系统提出了AC-RL控制器，并证明了其引导在线策略保证稳定性，同时使用高阶调谐器来适应参数不确定性和输入幅值限制。除了建立稳定性保证外，本文还应用仿真实验验证了我们所提出的方法。",
    "tldr": "本文在动态系统中提出了一种AC-RL控制器，在外环中采用强化学习策略确保稳定性和最优性，在内环中采用自适应控制。该控制器适用于两类非线性动态系统，并能适应参数不确定性和输入幅值限制。",
    "en_tdlr": "This paper proposes an AC-RL controller in dynamic systems, using reinforcement learning for stability and optimality in the outer loop and adaptive control to contract the closed-loop dynamics towards a stable trajectory in real time. The proposed method is applicable to two classes of nonlinear dynamic systems and accommodates parametric uncertainties and input magnitude limits."
}