{
    "title": "Clustered Federated Learning via Generalized Total Variation Minimization. (arXiv:2105.12769v4 [cs.LG] UPDATED)",
    "abstract": "We study optimization methods to train local (or personalized) models for decentralized collections of local datasets with an intrinsic network structure. This network structure arises from domain-specific notions of similarity between local datasets. Examples for such notions include spatio-temporal proximity, statistical dependencies or functional relations. Our main conceptual contribution is to formulate federated learning as generalized total variation (GTV) minimization. This formulation unifies and considerably extends existing federated learning methods. It is highly flexible and can be combined with a broad range of parametric models, including generalized linear models or deep neural networks. Our main algorithmic contribution is a fully decentralized federated learning algorithm. This algorithm is obtained by applying an established primal-dual method to solve GTV minimization. It can be implemented as message passing and is robust against inexact computations that arise fro",
    "link": "http://arxiv.org/abs/2105.12769",
    "context": "Title: Clustered Federated Learning via Generalized Total Variation Minimization. (arXiv:2105.12769v4 [cs.LG] UPDATED)\nAbstract: We study optimization methods to train local (or personalized) models for decentralized collections of local datasets with an intrinsic network structure. This network structure arises from domain-specific notions of similarity between local datasets. Examples for such notions include spatio-temporal proximity, statistical dependencies or functional relations. Our main conceptual contribution is to formulate federated learning as generalized total variation (GTV) minimization. This formulation unifies and considerably extends existing federated learning methods. It is highly flexible and can be combined with a broad range of parametric models, including generalized linear models or deep neural networks. Our main algorithmic contribution is a fully decentralized federated learning algorithm. This algorithm is obtained by applying an established primal-dual method to solve GTV minimization. It can be implemented as message passing and is robust against inexact computations that arise fro",
    "path": "papers/21/05/2105.12769.json",
    "total_tokens": 981,
    "translated_title": "基于广义全变差最小化的聚类联邦学习",
    "translated_abstract": "本文研究了在具有内在网络结构的分散式本地数据集的去中心化环境下训练本地（或个性化）模型的优化方法。这种网络结构是由本地数据集之间的领域特定相似性概念引起的。这些概念的例子包括时空邻近性，统计依赖性或功能关系。我们的主要概念性贡献在于将联邦学习描述为广义总变差（GTV）最小化。这种表述统一并显著扩展了现有的联邦学习方法。它具有高度的灵活性，并且可以与广泛的参数模型结合使用，包括广义线性模型或深度神经网络。我们的主要算法贡献是一种完全分散的联邦学习算法。该算法是通过应用已建立的原始-对偶方法来解决GTV最小化问题而获得的。它可以实现为消息传递，并且对于由通信延迟或干扰产生的不精确计算很稳健。我们在合成和真实数据集上的模拟中展示了我们方法的有效性。",
    "tldr": "本文介绍了一种基于广义全变差最小化的完全分散的联邦学习算法，可以训练适用于具有本地数据集的分散式去中心化环境的本地化（或个性化）模型，并获得了良好的模拟结果。",
    "en_tdlr": "This paper proposes a fully decentralized federated learning algorithm based on generalized total variation minimization, which can train local (or personalized) models for decentralized collections of local datasets with an intrinsic network structure. The proposed algorithm is shown to be effective in simulations on both synthetic and real-world datasets."
}