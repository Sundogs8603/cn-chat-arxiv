{
    "title": "Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle. (arXiv:2105.14559v3 [cs.LG] UPDATED)",
    "abstract": "Acquiring labeled data is challenging in many machine learning applications with limited budgets. Active learning gives a procedure to select the most informative data points and improve data efficiency by reducing the cost of labeling. The info-max learning principle maximizing mutual information such as BALD has been successful and widely adapted in various active learning applications. However, this pool-based specific objective inherently introduces a redundant selection and further requires a high computational cost for batch selection. In this paper, we design and propose a new uncertainty measure, Balanced Entropy Acquisition (BalEntAcq), which captures the information balance between the uncertainty of underlying softmax probability and the label variable. To do this, we approximate each marginal distribution by Beta distribution. Beta approximation enables us to formulate BalEntAcq as a ratio between an augmented entropy and the marginalized joint entropy. The closed-form expr",
    "link": "http://arxiv.org/abs/2105.14559",
    "context": "Title: Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle. (arXiv:2105.14559v3 [cs.LG] UPDATED)\nAbstract: Acquiring labeled data is challenging in many machine learning applications with limited budgets. Active learning gives a procedure to select the most informative data points and improve data efficiency by reducing the cost of labeling. The info-max learning principle maximizing mutual information such as BALD has been successful and widely adapted in various active learning applications. However, this pool-based specific objective inherently introduces a redundant selection and further requires a high computational cost for batch selection. In this paper, we design and propose a new uncertainty measure, Balanced Entropy Acquisition (BalEntAcq), which captures the information balance between the uncertainty of underlying softmax probability and the label variable. To do this, we approximate each marginal distribution by Beta distribution. Beta approximation enables us to formulate BalEntAcq as a ratio between an augmented entropy and the marginalized joint entropy. The closed-form expr",
    "path": "papers/21/05/2105.14559.json",
    "total_tokens": 1105,
    "translated_title": "基于平衡熵学习准则的贝叶斯神经网络主动学习",
    "translated_abstract": "在许多具有有限预算的机器学习应用中，获取标记数据是具有挑战性的。主动学习提供了一种选择最具信息量的数据点并通过减少标记成本来提高数据效率的过程。信息最大化学习原则（例如 BALD）最大化相互信息已经在各种主动学习应用中成功地广泛采用。然而，这种特定于池的目标本质上引入了冗余选择，并进一步需要高计算成本进行批处理选择。在本文中，我们设计并提出了一种新的不确定性测量方法Balanced Entropy Acquisition（BalEntAcq），它捕捉了潜在softmax概率和标签变量的不确定性之间的信息平衡。为此，我们通过Beta分布逼近每个边缘分布。Beta逼近使我们能够将BalEntAcq制定为增强熵和边缘联合熵之间的比率。所得到的BalEntAcq的闭式表达式可以高效地计算并与其他最先进的主动学习方法进行比较。我们在包括图像分类，目标检测和语义分割任务在内的几个基准数据集上展示了我们方法的有效性。我们提出的方法在显著降低计算成本的同时，达到了与现有方法相当的性能。",
    "tldr": "该论文提出了一种新的不确定性测量方法Balanced Entropy Acquisition（BalEntAcq），通过捕捉潜在softmax概率和标签变量的信息平衡，实现了基于平衡熵学习准则的贝叶斯神经网络主动学习，并在多个基准数据集上证明了该方法的有效性和较高的计算效率。",
    "en_tdlr": "This paper proposes a new uncertainty measure called Balanced Entropy Acquisition (BalEntAcq) for active learning in Bayesian neural networks, which captures the information balance between the uncertainty of underlying softmax probability and the label variable. The method, based on the balanced entropy learning principle, achieves competitive performance compared to existing methods while significantly reducing the computational cost on several benchmark datasets."
}