{
    "title": "An Offline Risk-aware Policy Selection Method for Bayesian Markov Decision Processes. (arXiv:2105.13431v2 [cs.LG] UPDATED)",
    "abstract": "In Offline Model Learning for Planning and in Offline Reinforcement Learning, the limited data set hinders the estimate of the Value function of the relative Markov Decision Process (MDP). Consequently, the performance of the obtained policy in the real world is bounded and possibly risky, especially when the deployment of a wrong policy can lead to catastrophic consequences. For this reason, several pathways are being followed with the scope of reducing the model error (or the distributional shift between the learned model and the true one) and, more broadly, obtaining risk-aware solutions with respect to model uncertainty. But when it comes to the final application which baseline should a practitioner choose? In an offline context where computational time is not an issue and robustness is the priority we propose Exploitation vs Caution (EvC), a paradigm that (1) elegantly incorporates model uncertainty abiding by the Bayesian formalism, and (2) selects the policy that maximizes a ris",
    "link": "http://arxiv.org/abs/2105.13431",
    "context": "Title: An Offline Risk-aware Policy Selection Method for Bayesian Markov Decision Processes. (arXiv:2105.13431v2 [cs.LG] UPDATED)\nAbstract: In Offline Model Learning for Planning and in Offline Reinforcement Learning, the limited data set hinders the estimate of the Value function of the relative Markov Decision Process (MDP). Consequently, the performance of the obtained policy in the real world is bounded and possibly risky, especially when the deployment of a wrong policy can lead to catastrophic consequences. For this reason, several pathways are being followed with the scope of reducing the model error (or the distributional shift between the learned model and the true one) and, more broadly, obtaining risk-aware solutions with respect to model uncertainty. But when it comes to the final application which baseline should a practitioner choose? In an offline context where computational time is not an issue and robustness is the priority we propose Exploitation vs Caution (EvC), a paradigm that (1) elegantly incorporates model uncertainty abiding by the Bayesian formalism, and (2) selects the policy that maximizes a ris",
    "path": "papers/21/05/2105.13431.json",
    "total_tokens": 948,
    "translated_title": "一种基于风险意识的贝叶斯马尔可夫决策过程离线策略选择方法",
    "translated_abstract": "在规划的离线模型学习和离线强化学习中，有限的数据集限制了相对马尔可夫决策过程(MDP)的价值函数的估计。因此，在真实世界中获得的策略的性能受到限制，并可能存在风险，特别是当部署错误的策略可能导致灾难性后果时。为此，为了减少模型误差（或学习模型与实际模型之间的分布偏移）并广泛地获得风险感知的解决方案，正在采取多种途径。但在最终应用中，从哪个角度出发选择基线呢？在计算时间不是问题而鲁棒性是首要考虑因素的离线情形下，本文提出了一种“利用”与“谨慎”(EvC)的范式，该范式(1)利用贝叶斯形式主张模型不确定性，同时(2)选择最大化风险权重值和长期奖励的策略。",
    "tldr": "本文提出了一种基于风险意识的离线策略选择方法，该方法采用“利用”与“谨慎”(EvC)的范式，选取权重值最大化且同时考虑长期奖励和风险的策略。",
    "en_tdlr": "This paper proposes an offline risk-aware policy selection method based on the Exploitation vs Caution (EvC) paradigm, which selects the policy that maximizes the risk-weighted cumulative reward, and incorporates model uncertainty using Bayesian formalism."
}