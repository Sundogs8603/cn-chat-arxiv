{
    "title": "Group selection and shrinkage: Structured sparsity for semiparametric additive models",
    "abstract": "arXiv:2105.12081v3 Announce Type: replace-cross  Abstract: Sparse regression and classification estimators that respect group structures have application to an assortment of statistical and machine learning problems, from multitask learning to sparse additive modeling to hierarchical selection. This work introduces structured sparse estimators that combine group subset selection with shrinkage. To accommodate sophisticated structures, our estimators allow for arbitrary overlap between groups. We develop an optimization framework for fitting the nonconvex regularization surface and present finite-sample error bounds for estimation of the regression function. As an application requiring structure, we study sparse semiparametric additive modeling, a procedure that allows the effect of each predictor to be zero, linear, or nonlinear. For this task, the new estimators improve across several metrics on synthetic data compared to alternatives. Finally, we demonstrate their efficacy in modelin",
    "link": "https://arxiv.org/abs/2105.12081",
    "context": "Title: Group selection and shrinkage: Structured sparsity for semiparametric additive models\nAbstract: arXiv:2105.12081v3 Announce Type: replace-cross  Abstract: Sparse regression and classification estimators that respect group structures have application to an assortment of statistical and machine learning problems, from multitask learning to sparse additive modeling to hierarchical selection. This work introduces structured sparse estimators that combine group subset selection with shrinkage. To accommodate sophisticated structures, our estimators allow for arbitrary overlap between groups. We develop an optimization framework for fitting the nonconvex regularization surface and present finite-sample error bounds for estimation of the regression function. As an application requiring structure, we study sparse semiparametric additive modeling, a procedure that allows the effect of each predictor to be zero, linear, or nonlinear. For this task, the new estimators improve across several metrics on synthetic data compared to alternatives. Finally, we demonstrate their efficacy in modelin",
    "path": "papers/21/05/2105.12081.json",
    "total_tokens": 842,
    "translated_title": "组选择和收缩：半参数加性模型的结构稀疏性",
    "translated_abstract": "稀疏回归和分类估计器尊重组结构，适用于各种统计学和机器学习问题，从多任务学习到稀疏加性建模再到分层选择。本文引入了结构化稀疏估计器，将组子集选择与收缩结合起来。为了适应复杂的结构，我们的估计器允许组之间存在任意重叠。我们开发了一个优化框架来拟合非凸正则化曲面，并提出了回归函数估计的有限样本误差界限。作为需要结构的应用，我们研究了稀疏半参数加性建模，这是一种允许每个预测变量效应为零、线性或非线性的过程。对于这个任务，与替代方案相比，新的估计器在合成数据上在几个度量上有所改进。最后，我们展示了它们在模型建立中的有效性。",
    "tldr": "本文引入了结构化稀疏估计器，结合组子集选择与收缩，适用于稀疏半参数加性建模，提出了相应的优化框架和有限样本误差边界。"
}