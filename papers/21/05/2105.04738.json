{
    "title": "Lightweight Distributed Gaussian Process Regression for Online Machine Learning. (arXiv:2105.04738v5 [cs.LG] UPDATED)",
    "abstract": "In this paper, we study the problem where a group of agents aim to collaboratively learn a common static latent function through streaming data. We propose a lightweight distributed Gaussian process regression (GPR) algorithm that is cognizant of agents' limited capabilities in communication, computation and memory. Each agent independently runs agent-based GPR using local streaming data to predict test points of interest; then the agents collaboratively execute distributed GPR to obtain global predictions over a common sparse set of test points; finally, each agent fuses results from distributed GPR with agent-based GPR to refine its predictions. By quantifying the transient and steady-state performances in predictive variance and error, we show that limited inter-agent communication improves learning performances in the sense of Pareto. Monte Carlo simulation is conducted to evaluate the developed algorithm.",
    "link": "http://arxiv.org/abs/2105.04738",
    "context": "Title: Lightweight Distributed Gaussian Process Regression for Online Machine Learning. (arXiv:2105.04738v5 [cs.LG] UPDATED)\nAbstract: In this paper, we study the problem where a group of agents aim to collaboratively learn a common static latent function through streaming data. We propose a lightweight distributed Gaussian process regression (GPR) algorithm that is cognizant of agents' limited capabilities in communication, computation and memory. Each agent independently runs agent-based GPR using local streaming data to predict test points of interest; then the agents collaboratively execute distributed GPR to obtain global predictions over a common sparse set of test points; finally, each agent fuses results from distributed GPR with agent-based GPR to refine its predictions. By quantifying the transient and steady-state performances in predictive variance and error, we show that limited inter-agent communication improves learning performances in the sense of Pareto. Monte Carlo simulation is conducted to evaluate the developed algorithm.",
    "path": "papers/21/05/2105.04738.json",
    "total_tokens": 919,
    "translated_title": "轻量级分布式高斯过程回归用于在线机器学习",
    "translated_abstract": "本文研究了一个问题，即一组代理通过流数据协同学习一个共同的静态潜在函数。我们提出了一种轻量级分布式高斯过程回归（GPR）算法，考虑到代理的有限通信、计算和存储能力。每个代理独立地利用本地流数据运行基于代理的GPR来预测感兴趣的测试点；然后代理们协同执行分布式GPR，得到在共同稀疏的一组测试点上的全局预测；最后，每个代理将分布式GPR的结果与基于代理的GPR的结果融合，以优化其预测。通过量化预测方差和误差的瞬态和稳态性能，我们证明了有限的代理间通信可以改善学习性能，满足帕累托改进。利用蒙特卡罗模拟评估了所开发的算法。",
    "tldr": "在这篇论文中，我们提出了一种轻量级分布式高斯过程回归（GPR）算法，用于协同学习一个共同的静态潜在函数。通过独立运行基于代理的GPR和协同执行分布式GPR，我们展示了有限的代理间通信可以改善学习性能，并通过蒙特卡罗模拟评估了算法的性能。",
    "en_tdlr": "In this paper, we propose a lightweight distributed Gaussian process regression (GPR) algorithm for collaboratively learning a common static latent function. By independently running agent-based GPR and collaboratively executing distributed GPR, we demonstrate that limited inter-agent communication improves learning performances and evaluate the algorithm using Monte Carlo simulation."
}