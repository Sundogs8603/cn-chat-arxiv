{
    "title": "Kernel Thinning. (arXiv:2105.05842v9 [stat.ML] UPDATED)",
    "abstract": "We introduce kernel thinning, a new procedure for compressing a distribution $\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given a suitable reproducing kernel $\\mathbf{k}_{\\star}$ and $\\mathcal{O}(n^2)$ time, kernel thinning compresses an $n$-point approximation to $\\mathbb{P}$ into a $\\sqrt{n}$-point approximation with comparable worst-case integration error across the associated reproducing kernel Hilbert space. The maximum discrepancy in integration error is $\\mathcal{O}_d(n^{-1/2}\\sqrt{\\log n})$ in probability for compactly supported $\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} (\\log n)^{(d+1)/2}\\sqrt{\\log\\log n})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$. In contrast, an equal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-1/4})$ integration error. Our sub-exponential guarantees resemble the classical quasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply to general distributions on $\\mathbb{R}^d$ and a wid",
    "link": "http://arxiv.org/abs/2105.05842",
    "context": "Title: Kernel Thinning. (arXiv:2105.05842v9 [stat.ML] UPDATED)\nAbstract: We introduce kernel thinning, a new procedure for compressing a distribution $\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given a suitable reproducing kernel $\\mathbf{k}_{\\star}$ and $\\mathcal{O}(n^2)$ time, kernel thinning compresses an $n$-point approximation to $\\mathbb{P}$ into a $\\sqrt{n}$-point approximation with comparable worst-case integration error across the associated reproducing kernel Hilbert space. The maximum discrepancy in integration error is $\\mathcal{O}_d(n^{-1/2}\\sqrt{\\log n})$ in probability for compactly supported $\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} (\\log n)^{(d+1)/2}\\sqrt{\\log\\log n})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$. In contrast, an equal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-1/4})$ integration error. Our sub-exponential guarantees resemble the classical quasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply to general distributions on $\\mathbb{R}^d$ and a wid",
    "path": "papers/21/05/2105.05842.json",
    "total_tokens": 1131,
    "translated_title": "核细化",
    "translated_abstract": "我们介绍了核细化，一种比独立同分布采样或标准细化更有效地压缩分布$\\mathbb{P}$的新方法。给定一个合适的再生核$\\mathbf{k}_{\\star}$和$\\mathcal{O}(n^2)$时间，核细化将一个$n$点近似的$\\mathbb{P}$压缩成一个具有与相关再生核希尔伯特空间中的可比较最坏积分误差的$\\sqrt{n}$点近似。在概率上，紧支撑的$\\mathbb{P}$的积分误差最大差别为$\\mathcal{O}_d(n^{-1/2}\\sqrt{\\log n})$，在$\\mathbb{R}^d$上的亚指数$\\mathbb{P}$为$\\mathcal{O}_d(n^{-\\frac{1}{2}} (\\log n)^{(d+1)/2}\\sqrt{\\log\\log n})$。相比之下，来自$\\mathbb{P}$的等大小i.i.d.样本面临$\\Omega(n^{-1/4})$的积分误差。我们的亚指数保证类似于在$[0,1]^d$上均匀$\\mathbb{P}$的经典准蒙特卡罗误差率，但适用于$\\mathbb{R}^d$上的一般分布和一个大",
    "tldr": "核细化是一种更有效的压缩分布的方法，它可以将$n$点近似的分布压缩到具有可比较最坏积分误差的$\\sqrt{n}$点近似，其亚指数保证类似于在$[0,1]^d$上均匀$\\mathbb{P}$的经典准蒙特卡罗误差率，但适用于$\\mathbb{R}^d$上的一般分布。"
}