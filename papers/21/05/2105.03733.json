{
    "title": "Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model. (arXiv:2105.03733v3 [cs.LG] UPDATED)",
    "abstract": "Model-free deep reinforcement learning has achieved great success in many domains, such as video games, recommendation systems and robotic control tasks. In continuous control tasks, widely used policies with Gaussian distributions results in ineffective exploration of environments and limited performance of algorithms in many cases. In this paper, we propose a density-free off-policy algorithm, Generative Actor-Critic(GAC), using the push-forward model to increase the expressiveness of policies, which also includes an entropy-like technique, MMD-entropy regularizer, to balance the exploration and exploitation. Additionnally, we devise an adaptive mechanism to automatically scale this regularizer, which further improves the stability and robustness of GAC. The experiment results show that push-forward policies possess desirable features, such as multi-modality, which can improve the efficiency of exploration and asymptotic performance of algorithms obviously.",
    "link": "http://arxiv.org/abs/2105.03733",
    "context": "Title: Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model. (arXiv:2105.03733v3 [cs.LG] UPDATED)\nAbstract: Model-free deep reinforcement learning has achieved great success in many domains, such as video games, recommendation systems and robotic control tasks. In continuous control tasks, widely used policies with Gaussian distributions results in ineffective exploration of environments and limited performance of algorithms in many cases. In this paper, we propose a density-free off-policy algorithm, Generative Actor-Critic(GAC), using the push-forward model to increase the expressiveness of policies, which also includes an entropy-like technique, MMD-entropy regularizer, to balance the exploration and exploitation. Additionnally, we devise an adaptive mechanism to automatically scale this regularizer, which further improves the stability and robustness of GAC. The experiment results show that push-forward policies possess desirable features, such as multi-modality, which can improve the efficiency of exploration and asymptotic performance of algorithms obviously.",
    "path": "papers/21/05/2105.03733.json",
    "total_tokens": 977,
    "translated_title": "生成式演员-评论家算法: 使用推进模型的离线算法",
    "translated_abstract": "无模型深度强化学习在视频游戏、推荐系统和机器人控制任务等许多领域取得了巨大成功。在连续控制任务中，广泛使用的具有高斯分布的策略导致环境的探索低效，并且在许多情况下算法的性能受到限制。本文提出了一种无密度的离线算法，称为生成式演员-评论家（Generative Actor-Critic，GAC），使用推进模型来增加策略的表达能力。同时还包括一种熵类技术，称为最大均值差（Maximum Mean Discrepancy，MMD）熵正则化器，以平衡探索和开发之间的关系。此外，我们设计了一种自适应机制来自动缩放这个正则化器，进一步提高了GAC的稳定性和鲁棒性。实验结果表明，推进策略具有理想的特征，例如多模式性，可以明显提高算法的探索效率和渐近性能。",
    "tldr": "本文提出基于推进模型的无密度离线算法 GAC，使用最大均值差熵正则化器平衡探索和开发之间的关系，并使用自适应机制提高算法的稳定性和鲁棒性。实验结果表明，推进策略能够提高算法的探索效率和渐近性能。",
    "en_tdlr": "This paper proposes an off-policy algorithm called Generative Actor-Critic (GAC) using the push-forward model to increase the expressiveness of policies, with an MMD-entropy regularizer to balance exploration and exploitation, and an adaptive mechanism to improve the stability and robustness of GAC. The experiment results show that push-forward policies possess desirable features such as multi-modality, which can improve the efficiency of exploration and asymptotic performance of algorithms obviously."
}