{
    "title": "Aleatoric uncertainty for Errors-in-Variables models in deep regression. (arXiv:2105.09095v3 [cs.LG] UPDATED)",
    "abstract": "A Bayesian treatment of deep learning allows for the computation of uncertainties associated with the predictions of deep neural networks. We show how the concept of Errors-in-Variables can be used in Bayesian deep regression to also account for the uncertainty associated with the input of the employed neural network. The presented approach thereby exploits a relevant, but generally overlooked, source of uncertainty and yields a decomposition of the predictive uncertainty into an aleatoric and epistemic part that is more complete and, in many cases, more consistent from a statistical perspective. We discuss the approach along various simulated and real examples and observe that using an Errors-in-Variables model leads to an increase in the uncertainty while preserving the prediction performance of models without Errors-in-Variables. For examples with known regression function we observe that this ground truth is substantially better covered by the Errors-in-Variables model, indicating ",
    "link": "http://arxiv.org/abs/2105.09095",
    "context": "Title: Aleatoric uncertainty for Errors-in-Variables models in deep regression. (arXiv:2105.09095v3 [cs.LG] UPDATED)\nAbstract: A Bayesian treatment of deep learning allows for the computation of uncertainties associated with the predictions of deep neural networks. We show how the concept of Errors-in-Variables can be used in Bayesian deep regression to also account for the uncertainty associated with the input of the employed neural network. The presented approach thereby exploits a relevant, but generally overlooked, source of uncertainty and yields a decomposition of the predictive uncertainty into an aleatoric and epistemic part that is more complete and, in many cases, more consistent from a statistical perspective. We discuss the approach along various simulated and real examples and observe that using an Errors-in-Variables model leads to an increase in the uncertainty while preserving the prediction performance of models without Errors-in-Variables. For examples with known regression function we observe that this ground truth is substantially better covered by the Errors-in-Variables model, indicating ",
    "path": "papers/21/05/2105.09095.json",
    "total_tokens": 934,
    "translated_title": "深度回归中的变量误差模型的随机不确定性",
    "translated_abstract": "深度学习的贝叶斯处理可以计算与深度神经网络预测相关的不确定性。我们展示了如何在贝叶斯深度回归中使用变量误差的概念，以考虑所使用神经网络的输入所关联的不确定性。所提出的方法利用了一个相关但通常被忽视的不确定性源，并将预测不确定性分解为随机和认识部分，这在统计学角度更完整，而且在很多情况下更一致。我们通过各种模拟和真实的例子讨论了这种方法，并观察到使用变量误差模型会增加不确定性，同时保持不使用变量误差模型的模型的预测性能。对于已知回归函数的例子，我们观察到变量误差模型大大提高了对基础事实的覆盖，表明该方法在回归问题中表现良好。",
    "tldr": "本文提出了一种基于贝叶斯深度回归的方法，利用变量误差模型考虑所使用神经网络的输入所关联的不确定性，并将预测不确定性分解为随机和认识部分。相比于不使用该模型，使用错误变量模型能够提高对已知回归函数的覆盖率，且保持预测性能。",
    "en_tdlr": "This article proposes a method based on Bayesian deep regression, which uses an Errors-in-Variables model to consider the uncertainty associated with the input of the employed neural network and decomposes the predictive uncertainty into an aleatoric and epistemic part. Using an Errors-in-Variables model can improve the coverage of the known regression function, while preserving prediction performance compared to not using this model."
}