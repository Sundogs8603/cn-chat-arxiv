{
    "title": "BAARD: Blocking Adversarial Examples by Testing for Applicability, Reliability and Decidability. (arXiv:2105.00495v2 [cs.LG] UPDATED)",
    "abstract": "Adversarial defenses protect machine learning models from adversarial attacks, but are often tailored to one type of model or attack. The lack of information on unknown potential attacks makes detecting adversarial examples challenging. Additionally, attackers do not need to follow the rules made by the defender. To address this problem, we take inspiration from the concept of Applicability Domain in cheminformatics. Cheminformatics models struggle to make accurate predictions because only a limited number of compounds are known and available for training. Applicability Domain defines a domain based on the known compounds and rejects any unknown compound that falls outside the domain. Similarly, adversarial examples start as harmless inputs, but can be manipulated to evade reliable classification by moving outside the domain of the classifier. We are the first to identify the similarity between Applicability Domain and adversarial detection. Instead of focusing on unknown attacks, we f",
    "link": "http://arxiv.org/abs/2105.00495",
    "context": "Title: BAARD: Blocking Adversarial Examples by Testing for Applicability, Reliability and Decidability. (arXiv:2105.00495v2 [cs.LG] UPDATED)\nAbstract: Adversarial defenses protect machine learning models from adversarial attacks, but are often tailored to one type of model or attack. The lack of information on unknown potential attacks makes detecting adversarial examples challenging. Additionally, attackers do not need to follow the rules made by the defender. To address this problem, we take inspiration from the concept of Applicability Domain in cheminformatics. Cheminformatics models struggle to make accurate predictions because only a limited number of compounds are known and available for training. Applicability Domain defines a domain based on the known compounds and rejects any unknown compound that falls outside the domain. Similarly, adversarial examples start as harmless inputs, but can be manipulated to evade reliable classification by moving outside the domain of the classifier. We are the first to identify the similarity between Applicability Domain and adversarial detection. Instead of focusing on unknown attacks, we f",
    "path": "papers/21/05/2105.00495.json",
    "total_tokens": 931,
    "translated_title": "BAARD：通过检测适用性、可靠性和可决策性来阻挡对抗性样本",
    "translated_abstract": "对抗性防御保护机器学习模型免受对抗性攻击，但往往针对特定类型的模型或攻击进行定制。缺乏关于未知潜在攻击的信息使得检测对抗性样本具有挑战性。此外，攻击者不需要遵守防御者制定的规则。为了解决这个问题，我们借鉴了化学信息学中的适用性域的概念。化学信息学模型之所以难以进行准确预测，是因为只有有限数量的化合物被知晓并可用于训练。适用性域根据已知化合物定义一个域，并拒绝任何落在域外的未知化合物。类似地，对抗性样本最初是无害的输入，但可以通过移动到分类器域外来操纵以逃避可靠的分类。我们是第一个识别出适用性域与对抗性检测之间相似性的研究。我们不再关注未知攻击，而是专注于检测样本是否在分类器的域内。",
    "tldr": "该论文提出了一种新的对抗性样本检测方法，借鉴了化学信息学中的适用性域概念，通过判断样本是否在分类器的域内来阻挡对抗性样本。"
}