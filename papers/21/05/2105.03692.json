{
    "title": "Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks. (arXiv:2105.03692v4 [cs.LG] UPDATED)",
    "abstract": "We propose a novel clustering mechanism based on an incompatibility property between subsets of data that emerges during model training. This mechanism partitions the dataset into subsets that generalize only to themselves, i.e., training on one subset does not improve performance on the other subsets. Leveraging the interaction between the dataset and the training process, our clustering mechanism partitions datasets into clusters that are defined by--and therefore meaningful to--the objective of the training process.  We apply our clustering mechanism to defend against data poisoning attacks, in which the attacker injects malicious poisoned data into the training dataset to affect the trained model's output. Our evaluation focuses on backdoor attacks against deep neural networks trained to perform image classification using the GTSRB and CIFAR-10 datasets. Our results show that (1) these attacks produce poisoned datasets in which the poisoned and clean data are incompatible and (2) o",
    "link": "http://arxiv.org/abs/2105.03692",
    "context": "Title: Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks. (arXiv:2105.03692v4 [cs.LG] UPDATED)\nAbstract: We propose a novel clustering mechanism based on an incompatibility property between subsets of data that emerges during model training. This mechanism partitions the dataset into subsets that generalize only to themselves, i.e., training on one subset does not improve performance on the other subsets. Leveraging the interaction between the dataset and the training process, our clustering mechanism partitions datasets into clusters that are defined by--and therefore meaningful to--the objective of the training process.  We apply our clustering mechanism to defend against data poisoning attacks, in which the attacker injects malicious poisoned data into the training dataset to affect the trained model's output. Our evaluation focuses on backdoor attacks against deep neural networks trained to perform image classification using the GTSRB and CIFAR-10 datasets. Our results show that (1) these attacks produce poisoned datasets in which the poisoned and clean data are incompatible and (2) o",
    "path": "papers/21/05/2105.03692.json",
    "total_tokens": 883,
    "translated_title": "“抵御后门攻击的不兼容聚类机制”",
    "translated_abstract": "本文提出一种新型的聚类机制，该机制基于模型训练过程中出现的数据子集不相容性属性。该机制将数据集划分为只能泛化到其自身的子集，即在一个子集上的训练不会改善其他子集的性能。利用数据集与训练过程之间的交互作用，我们的聚类机制将数据集划分为由训练过程的目标所定义且具有意义的聚类。我们将我们的聚类机制应用于防御数据毒化攻击，即攻击者将恶意毒害数据注入训练数据集，以影响训练模型的输出。我们的评估重点关注利用GTSRB和CIFAR-10数据集进行图像分类的深度神经网络中的后门攻击。我们的结果表明：1）这些攻击产生的毒害数据集是有毒害数据和干净数据不相容的；2）我们的聚类机制可以有效减轻后门攻击的影响。",
    "tldr": "本文提出了一种基于不兼容性的聚类机制，该机制可以将数据集划分为由训练过程的目标所定义且具有意义的聚类，并有效减轻后门攻击的影响。",
    "en_tdlr": "This paper proposes a clustering mechanism based on incompatibility property to partition datasets into meaningful clusters defined by the objective of the training process. It effectively defends against backdoor poisoning attacks."
}