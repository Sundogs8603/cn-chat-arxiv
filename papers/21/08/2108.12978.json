{
    "title": "Private Multi-Task Learning: Formulation and Applications to Federated Learning. (arXiv:2108.12978v3 [cs.LG] UPDATED)",
    "abstract": "Many problems in machine learning rely on multi-task learning (MTL), in which the goal is to solve multiple related machine learning tasks simultaneously. MTL is particularly relevant for privacy-sensitive applications in areas such as healthcare, finance, and IoT computing, where sensitive data from multiple, varied sources are shared for the purpose of learning. In this work, we formalize notions of client-level privacy for MTL via joint differential privacy (JDP), a relaxation of differential privacy for mechanism design and distributed optimization. We then propose an algorithm for mean-regularized MTL, an objective commonly used for applications in personalized federated learning, subject to JDP. We analyze our objective and solver, providing certifiable guarantees on both privacy and utility. Empirically, we find that our method provides improved privacy/utility trade-offs relative to global baselines across common federated learning benchmarks.",
    "link": "http://arxiv.org/abs/2108.12978",
    "context": "Title: Private Multi-Task Learning: Formulation and Applications to Federated Learning. (arXiv:2108.12978v3 [cs.LG] UPDATED)\nAbstract: Many problems in machine learning rely on multi-task learning (MTL), in which the goal is to solve multiple related machine learning tasks simultaneously. MTL is particularly relevant for privacy-sensitive applications in areas such as healthcare, finance, and IoT computing, where sensitive data from multiple, varied sources are shared for the purpose of learning. In this work, we formalize notions of client-level privacy for MTL via joint differential privacy (JDP), a relaxation of differential privacy for mechanism design and distributed optimization. We then propose an algorithm for mean-regularized MTL, an objective commonly used for applications in personalized federated learning, subject to JDP. We analyze our objective and solver, providing certifiable guarantees on both privacy and utility. Empirically, we find that our method provides improved privacy/utility trade-offs relative to global baselines across common federated learning benchmarks.",
    "path": "papers/21/08/2108.12978.json",
    "total_tokens": 877,
    "translated_title": "私有多任务学习：表述与应用于联邦学习",
    "translated_abstract": "许多机器学习问题依赖于多任务学习（MTL），其中目标是同时解决多个相关的机器学习任务。MTL对于隐私敏感的应用特别重要，例如医疗保健、金融和物联网计算，这些应用需要共享来自多个不同来源的敏感数据进行学习。在这项工作中，我们通过联合差分隐私（JDP）为MTL形式化客户级隐私的概念，JDP是差分隐私在机制设计和分布式优化中的一种放松。然后，我们提出了一种用于均值正则化MTL的算法，这是个性化联邦学习应用中常用的目标，同时满足JDP。我们对我们的目标和求解器进行了分析，提供了隐私和效用的可证明保证。从实证上看，相对于常见的联邦学习基准测试，我们发现我们的方法在隐私/效用权衡方面提供了更好的性能。",
    "tldr": "这项研究提出了一种用于在隐私敏感应用中进行多任务学习的算法，通过联合差分隐私保证了个性化联邦学习的隐私和效用，并在实验中表现出更好的性能。",
    "en_tdlr": "This study proposes an algorithm for multi-task learning in privacy-sensitive applications, ensuring privacy and utility in personalized federated learning through joint differential privacy, and demonstrating better performance in experiments."
}