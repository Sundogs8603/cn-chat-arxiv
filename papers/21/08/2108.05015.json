{
    "title": "VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. (arXiv:2108.05015v4 [cs.CV] UPDATED)",
    "abstract": "Different from visible cameras which record intensity images frame by frame, the biologically inspired event camera produces a stream of asynchronous and sparse events with much lower latency. In practice, visible cameras can better perceive texture details and slow motion, while event cameras can be free from motion blurs and have a larger dynamic range which enables them to work well under fast motion and low illumination. Therefore, the two sensors can cooperate with each other to achieve more reliable object tracking. In this work, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to the lack of a realistic and scaled dataset for this task. Our dataset consists of 820 video pairs captured under low illumination, high speed, and background clutter scenarios, and it is divided into a training and a testing subset, each of which contains 500 and 320 videos, respectively. Based on VisEvent, we transform the event flows into event images and construct more than 30 b",
    "link": "http://arxiv.org/abs/2108.05015",
    "context": "Title: VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. (arXiv:2108.05015v4 [cs.CV] UPDATED)\nAbstract: Different from visible cameras which record intensity images frame by frame, the biologically inspired event camera produces a stream of asynchronous and sparse events with much lower latency. In practice, visible cameras can better perceive texture details and slow motion, while event cameras can be free from motion blurs and have a larger dynamic range which enables them to work well under fast motion and low illumination. Therefore, the two sensors can cooperate with each other to achieve more reliable object tracking. In this work, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to the lack of a realistic and scaled dataset for this task. Our dataset consists of 820 video pairs captured under low illumination, high speed, and background clutter scenarios, and it is divided into a training and a testing subset, each of which contains 500 and 320 videos, respectively. Based on VisEvent, we transform the event flows into event images and construct more than 30 b",
    "path": "papers/21/08/2108.05015.json",
    "total_tokens": 893,
    "translated_title": "VisEvent：通过帧和事件流的协作实现可靠的物体跟踪",
    "translated_abstract": "不同于记录逐帧强度图像的可见摄像机，生物启发式事件摄像机产生一系列异步和稀疏事件，具有更低的延迟。在实践中，可见摄像机可以更好地感知纹理细节和慢动作，而事件摄像机可以摆脱运动模糊，并具有更大的动态范围，使其在快速运动和低照明条件下表现良好。因此，这两种传感器可以相互合作，实现更可靠的物体跟踪。在这项工作中，我们提出了一个大规模的可见-事件基准（称为VisEvent），因为此任务缺乏一个真实且具有规模的数据集。我们的数据集包含820个视频对，涵盖了低照明、高速和背景杂乱场景，并分为训练子集和测试子集，分别包含500个和320个视频。",
    "tldr": "本论文提出了一个大规模的可见-事件基准（VisEvent），通过可见摄像机和事件摄像机的协作，实现了更可靠的物体跟踪。根据VisEvent，我们将事件流转化为事件图像，并构建了30多个...",
    "en_tdlr": "This paper proposes a large-scale Visible-Event benchmark (VisEvent) for reliable object tracking through the collaboration of visible cameras and event cameras. Based on VisEvent, event flows are transformed into event images, enabling the construction of more than 30..."
}