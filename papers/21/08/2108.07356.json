{
    "title": "Stochastic Optimization under Distributional Drift. (arXiv:2108.07356v3 [math.OC] UPDATED)",
    "abstract": "We consider the problem of minimizing a convex function that is evolving according to unknown and possibly stochastic dynamics, which may depend jointly on time and on the decision variable itself. Such problems abound in the machine learning and signal processing literature, under the names of concept drift, stochastic tracking, and performative prediction. We provide novel non-asymptotic convergence guarantees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. The efficiency estimates we obtain clearly decouple the contributions of optimization error, gradient noise, and time drift. Notably, we identify a low drift-to-noise regime in which the tracking efficiency of the proximal stochastic gradient method benefits significantly from a step decay schedule. Numerical experiments illustrate our results.",
    "link": "http://arxiv.org/abs/2108.07356",
    "context": "Title: Stochastic Optimization under Distributional Drift. (arXiv:2108.07356v3 [math.OC] UPDATED)\nAbstract: We consider the problem of minimizing a convex function that is evolving according to unknown and possibly stochastic dynamics, which may depend jointly on time and on the decision variable itself. Such problems abound in the machine learning and signal processing literature, under the names of concept drift, stochastic tracking, and performative prediction. We provide novel non-asymptotic convergence guarantees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. The efficiency estimates we obtain clearly decouple the contributions of optimization error, gradient noise, and time drift. Notably, we identify a low drift-to-noise regime in which the tracking efficiency of the proximal stochastic gradient method benefits significantly from a step decay schedule. Numerical experiments illustrate our results.",
    "path": "papers/21/08/2108.07356.json",
    "total_tokens": 841,
    "translated_title": "分布漂移下的随机优化",
    "translated_abstract": "本文考虑了最小化随机演化凸函数的问题，这个演化过程是未知的，并且可能依赖于时间和决策变量本身。这类问题在机器学习和信号处理领域中广泛存在，称为概念漂移、随机跟踪和执行预测。我们提供了新的非渐近收敛保证，重点关注在期望值和高概率下成立的界限。我们获得的效率估计明确地解耦了优化误差、梯度噪声和时间漂移的影响。值得注意的是，我们确定了一个低漂移-噪声比的区域，在这个区域里，近端随机梯度方法的跟踪效率因步长衰减策略而受益显著。数值实验证明了我们的结果。",
    "tldr": "本文提供了一种在分布漂移下优化凸函数的新方法，经数值实验证明在低漂移-噪声比的情况下，近端随机梯度方法采用步长衰减策略可显著提升跟踪效率。",
    "en_tdlr": "This paper proposes a new approach for optimizing convex functions under distributional drift, and provides non-asymptotic convergence guarantees with efficiency estimates that decouple optimization error, gradient noise, and time drift. Numerical experiments show that the proximal stochastic gradient method benefits significantly from a step decay schedule in the low drift-to-noise regime."
}