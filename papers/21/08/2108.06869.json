{
    "title": "FedChain: Chained Algorithms for Near-Optimal Communication Cost in Federated Learning. (arXiv:2108.06869v5 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FL) aims to minimize the communication complexity of training a model over heterogeneous data distributed across many clients. A common approach is local methods, where clients take multiple optimization steps over local data before communicating with the server (e.g., FedAvg). Local methods can exploit similarity between clients' data. However, in existing analyses, this comes at the cost of slow convergence in terms of the dependence on the number of communication rounds R. On the other hand, global methods, where clients simply return a gradient vector in each round (e.g., SGD), converge faster in terms of R but fail to exploit the similarity between clients even when clients are homogeneous. We propose FedChain, an algorithmic framework that combines the strengths of local methods and global methods to achieve fast convergence in terms of R while leveraging the similarity between clients. Using FedChain, we instantiate algorithms that improve upon previously kno",
    "link": "http://arxiv.org/abs/2108.06869",
    "context": "Title: FedChain: Chained Algorithms for Near-Optimal Communication Cost in Federated Learning. (arXiv:2108.06869v5 [cs.LG] UPDATED)\nAbstract: Federated learning (FL) aims to minimize the communication complexity of training a model over heterogeneous data distributed across many clients. A common approach is local methods, where clients take multiple optimization steps over local data before communicating with the server (e.g., FedAvg). Local methods can exploit similarity between clients' data. However, in existing analyses, this comes at the cost of slow convergence in terms of the dependence on the number of communication rounds R. On the other hand, global methods, where clients simply return a gradient vector in each round (e.g., SGD), converge faster in terms of R but fail to exploit the similarity between clients even when clients are homogeneous. We propose FedChain, an algorithmic framework that combines the strengths of local methods and global methods to achieve fast convergence in terms of R while leveraging the similarity between clients. Using FedChain, we instantiate algorithms that improve upon previously kno",
    "path": "papers/21/08/2108.06869.json",
    "total_tokens": 975,
    "translated_title": "FedChain：用于联邦学习中近似最优通信成本的链接算法",
    "translated_abstract": "联邦学习旨在最小化在分布在许多客户端上的异构数据上训练模型的通信复杂度。一种常用的方法是本地方法，其中客户端在与服务器通信之前对本地数据进行多个优化步骤（例如，FedAvg）。本地方法可以利用客户端数据的相似性。然而，在现有的分析中，这是以依赖于通信回合数R的收敛速度缓慢为代价的。另一方面，全局方法，其中客户端仅在每个回合返回梯度向量（例如，SGD），在R方面的快速收敛性方面更快，但即使客户端是同构的，也无法利用客户端之间的相似性。我们提出FedChain，一种算法框架，结合了本地方法和全局方法的优点，实现了在R方面的快速收敛，并利用客户之间的相似性。使用FedChain，我们实例化算法，改善了几个标准FL基准测试的已知结果。具体而言，我们展示了FedChain在客户端异构的情况下实现了接近最优的通信复杂度，并且当客户端是同构的时仍然保持快速收敛率。",
    "tldr": "FedChain是一种算法框架，结合了本地方法和全局方法的优点，实现了在通信回合数方面快速收敛，并利用客户之间的相似性。",
    "en_tdlr": "FedChain is an algorithmic framework that combines the strengths of local methods and global methods to achieve fast convergence in terms of communication rounds while leveraging the similarity between clients."
}