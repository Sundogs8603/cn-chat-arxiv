{
    "title": "Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v3 [cs.CL] UPDATED)",
    "abstract": "Multi-head attention, a collection of several attention mechanisms that independently attend to different parts of the input, is the key ingredient in the Transformer. Recent work has shown, however, that a large proportion of the heads in a Transformer's multi-head attention mechanism can be safely pruned away without significantly harming the performance of the model; such pruning leads to models that are noticeably smaller and faster in practice. Our work introduces a new head pruning technique that we term differentiable subset pruning. Intuitively, our method learns per-head importance variables and then enforces a user-specified hard constraint on the number of unpruned heads. The importance variables are learned via stochastic gradient descent. We conduct experiments on natural language inference and machine translation; we show that differentiable subset pruning performs comparably or better than previous works while offering precise control of the sparsity level.",
    "link": "http://arxiv.org/abs/2108.04657",
    "context": "Title: Differentiable Subset Pruning of Transformer Heads. (arXiv:2108.04657v3 [cs.CL] UPDATED)\nAbstract: Multi-head attention, a collection of several attention mechanisms that independently attend to different parts of the input, is the key ingredient in the Transformer. Recent work has shown, however, that a large proportion of the heads in a Transformer's multi-head attention mechanism can be safely pruned away without significantly harming the performance of the model; such pruning leads to models that are noticeably smaller and faster in practice. Our work introduces a new head pruning technique that we term differentiable subset pruning. Intuitively, our method learns per-head importance variables and then enforces a user-specified hard constraint on the number of unpruned heads. The importance variables are learned via stochastic gradient descent. We conduct experiments on natural language inference and machine translation; we show that differentiable subset pruning performs comparably or better than previous works while offering precise control of the sparsity level.",
    "path": "papers/21/08/2108.04657.json",
    "total_tokens": 809,
    "translated_title": "可微分的Transformer头部子集修剪",
    "translated_abstract": "多头注意力是Transformer中的重要组成部分，它由多个独立关注输入不同部分的注意力机制组成。然而，最近的研究表明，在不显著影响模型性能的情况下，可以安全地修剪掉Transformer多头注意力机制中的大部分头部，从而使模型在实践中更小更快。本文提出了一种新的头部修剪技术，称为可微分子集修剪。方法的核心是学习每个头部的重要性变量，并对未修剪头部的数量施加用户指定的硬约束。重要性变量通过随机梯度下降学习得到。我们在自然语言推理和机器翻译上进行实验，结果显示可微分子集修剪在稀疏程度上具有可比或更好的性能，并能提供精确的控制。",
    "tldr": "这种可微分的Transformer头部子集修剪方法可以安全地修剪掉大部分头部，使得模型更小更快，而且在稀疏程度上具有可比或更好的性能。",
    "en_tdlr": "This differentiable subset pruning technique for Transformer heads allows for safe pruning of a large proportion of heads, resulting in smaller and faster models while maintaining comparable or better performance in sparsity."
}