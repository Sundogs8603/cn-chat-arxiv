{
    "title": "Implicit Regularization of Bregman Proximal Point Algorithm and Mirror Descent on Separable Data. (arXiv:2108.06808v5 [cs.LG] UPDATED)",
    "abstract": "Bregman proximal point algorithm (BPPA) has witnessed emerging machine learning applications, yet its theoretical understanding has been largely unexplored. We study the computational properties of BPPA through learning linear classifiers with separable data, and demonstrate provable algorithmic regularization of BPPA. For any BPPA instantiated with a fixed Bregman divergence, we provide a lower bound of the margin obtained by BPPA with respect to an arbitrarily chosen norm. The obtained margin lower bound differs from the maximal margin by a multiplicative factor, which inversely depends on the condition number of the distance-generating function measured in the dual norm. We show that the dependence on the condition number is tight, thus demonstrating the importance of divergence in affecting the quality of the learned classifiers. We then extend our findings to mirror descent, for which we establish similar connections between the margin and Bregman divergence, together with a non-a",
    "link": "http://arxiv.org/abs/2108.06808",
    "context": "Title: Implicit Regularization of Bregman Proximal Point Algorithm and Mirror Descent on Separable Data. (arXiv:2108.06808v5 [cs.LG] UPDATED)\nAbstract: Bregman proximal point algorithm (BPPA) has witnessed emerging machine learning applications, yet its theoretical understanding has been largely unexplored. We study the computational properties of BPPA through learning linear classifiers with separable data, and demonstrate provable algorithmic regularization of BPPA. For any BPPA instantiated with a fixed Bregman divergence, we provide a lower bound of the margin obtained by BPPA with respect to an arbitrarily chosen norm. The obtained margin lower bound differs from the maximal margin by a multiplicative factor, which inversely depends on the condition number of the distance-generating function measured in the dual norm. We show that the dependence on the condition number is tight, thus demonstrating the importance of divergence in affecting the quality of the learned classifiers. We then extend our findings to mirror descent, for which we establish similar connections between the margin and Bregman divergence, together with a non-a",
    "path": "papers/21/08/2108.06808.json",
    "total_tokens": 967,
    "translated_title": "Bregman Proximal Point算法和Mirror Descent在可分数据上的隐式正则化",
    "translated_abstract": "Bregman proximal point算法（BPPA）在机器学习中具有广泛应用，但其理论理解尚未完全探索。我们通过学习具有可分数据的线性分类器，研究了BPPA的计算性质，并证明了BPPA的可验证算法正则化。对于任何使用固定Bregman距离实例化的BPPA，我们提供了BPPA所获得的边界的下界，该下界与任意选择的范数相关。所得到的边界下界与最大边界之间存在一个乘法因子的差异，该乘法因子与以对偶范数度量的距离生成函数的条件数成反比。我们证明了对条件数的依赖是紧致的，从而证明了差异对于影响学习分类器的质量的重要性。然后，我们将我们的发现推广到了mirror descent，对于该算法，我们建立了边界和Bregman距离之间的类似联系，以及一个非-a",
    "tldr": "通过对可分数据使用Bregman proximal point算法和Mirror Descent进行学习线性分类器的研究, 我们发现BPPA具有可验证的算法正则化性质, 并且证明了边界与Bregman距离之间的关联性, 这揭示了BPPA对于学习分类器质量的影响和重要性",
    "en_tdlr": "Through studying the application of Bregman proximal point algorithm (BPPA) and Mirror Descent in learning linear classifiers with separable data, we discovered the provable algorithmic regularization property of BPPA and established the relationship between the margin and Bregman divergence. This reveals the impact and importance of BPPA on the quality of learned classifiers."
}