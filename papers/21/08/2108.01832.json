{
    "title": "Offline Decentralized Multi-Agent Reinforcement Learning. (arXiv:2108.01832v2 [cs.LG] UPDATED)",
    "abstract": "In many real-world multi-agent cooperative tasks, due to high cost and risk, agents cannot continuously interact with the environment and collect experiences during learning, but have to learn from offline datasets. However, the transition dynamics in the dataset of each agent can be much different from the ones induced by the learned policies of other agents in execution, creating large errors in value estimates. Consequently, agents learn uncoordinated low-performing policies. In this paper, we propose a framework for offline decentralized multi-agent reinforcement learning, which exploits value deviation and transition normalization to deliberately modify the transition probabilities. Value deviation optimistically increases the transition probabilities of high-value next states, and transition normalization normalizes the transition probabilities of next states. They together enable agents to learn high-performing and coordinated policies. Theoretically, we prove the convergence of",
    "link": "http://arxiv.org/abs/2108.01832",
    "context": "Title: Offline Decentralized Multi-Agent Reinforcement Learning. (arXiv:2108.01832v2 [cs.LG] UPDATED)\nAbstract: In many real-world multi-agent cooperative tasks, due to high cost and risk, agents cannot continuously interact with the environment and collect experiences during learning, but have to learn from offline datasets. However, the transition dynamics in the dataset of each agent can be much different from the ones induced by the learned policies of other agents in execution, creating large errors in value estimates. Consequently, agents learn uncoordinated low-performing policies. In this paper, we propose a framework for offline decentralized multi-agent reinforcement learning, which exploits value deviation and transition normalization to deliberately modify the transition probabilities. Value deviation optimistically increases the transition probabilities of high-value next states, and transition normalization normalizes the transition probabilities of next states. They together enable agents to learn high-performing and coordinated policies. Theoretically, we prove the convergence of",
    "path": "papers/21/08/2108.01832.json",
    "total_tokens": 891,
    "translated_title": "离线分散式多智能体强化学习",
    "translated_abstract": "在许多真实世界的多智能体合作任务中，由于成本高昂和风险，智能体无法在学习过程中持续与环境进行交互并收集经验，而必须从离线数据集中进行学习。然而，每个智能体数据集中的状态转移动态可能与其他智能体在执行中学习策略引发的转移动态相差很大，从而导致价值估计出现较大的误差。因此，智能体学习到的是不协调且性能较低的策略。本文提出了一种离线分散式多智能体强化学习框架，利用价值偏差和转移规范化来有意地修改转移概率。价值偏差乐观地增加了高价值下一状态的转移概率，而转移规范化对下一状态的转移概率进行了规范化。它们共同使智能体能够学习到高性能和协调的策略。在理论上，我们证明了该框架的收敛性。",
    "tldr": "本文提出了一种离线分散式多智能体强化学习框架，通过利用价值偏差和转移规范化来修改转移概率，使智能体能够学习到高性能和协调的策略。"
}