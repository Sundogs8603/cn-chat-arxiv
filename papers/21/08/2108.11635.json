{
    "title": "MCML: A Novel Memory-based Contrastive Meta-Learning Method for Few Shot Slot Tagging. (arXiv:2108.11635v3 [cs.AI] UPDATED)",
    "abstract": "Meta-learning is widely used for few-shot slot tagging in task of few-shot learning. The performance of existing methods is, however, seriously affected by \\textit{sample forgetting issue}, where the model forgets the historically learned meta-training tasks while solely relying on support sets when adapting to new tasks. To overcome this predicament, we propose the \\textbf{M}emory-based \\textbf{C}ontrastive \\textbf{M}eta-\\textbf{L}earning (aka, MCML) method, including \\textit{learn-from-the-memory} and \\textit{adaption-from-the-memory} modules, which bridge the distribution gap between training episodes and between training and testing respectively. Specifically, the former uses an explicit memory bank to keep track of the label representations of previously trained episodes, with a contrastive constraint between the label representations in the current episode with the historical ones stored in the memory. In addition, the \\emph{adaption-from-memory} mechanism is introduced to learn ",
    "link": "http://arxiv.org/abs/2108.11635",
    "context": "Title: MCML: A Novel Memory-based Contrastive Meta-Learning Method for Few Shot Slot Tagging. (arXiv:2108.11635v3 [cs.AI] UPDATED)\nAbstract: Meta-learning is widely used for few-shot slot tagging in task of few-shot learning. The performance of existing methods is, however, seriously affected by \\textit{sample forgetting issue}, where the model forgets the historically learned meta-training tasks while solely relying on support sets when adapting to new tasks. To overcome this predicament, we propose the \\textbf{M}emory-based \\textbf{C}ontrastive \\textbf{M}eta-\\textbf{L}earning (aka, MCML) method, including \\textit{learn-from-the-memory} and \\textit{adaption-from-the-memory} modules, which bridge the distribution gap between training episodes and between training and testing respectively. Specifically, the former uses an explicit memory bank to keep track of the label representations of previously trained episodes, with a contrastive constraint between the label representations in the current episode with the historical ones stored in the memory. In addition, the \\emph{adaption-from-memory} mechanism is introduced to learn ",
    "path": "papers/21/08/2108.11635.json",
    "total_tokens": 920,
    "translated_title": "MCML：一种新颖的基于内存的对比元学习方法用于小样本槽位标记",
    "translated_abstract": "元学习在小样本学习的槽位标记任务中被广泛应用。然而，现有方法的性能受到了“样本遗忘问题”的严重影响，即模型在适应新任务时仅依赖支持集而忘记了历史学习的元训练任务。为了克服这一困境，我们提出了一种名为MCML（Memory-based Contrastive Meta-Learning）的方法，包括“从内存中学习”和“从内存中适应”两个模块，分别在训练剧集之间和训练与测试之间建立了分布差异。具体来说，前者使用显式内存库来跟踪先前训练的剧集的标签表示，并在当前剧集的标签表示与内存中存储的历史剧集进行对比约束。此外，引入了“从内存中适应”的机制来学习...",
    "tldr": "MCML是一种基于内存的对比元学习方法，通过使用显式内存库来跟踪先前训练的剧集的标签表示，并引入了“从内存中适应”的机制，以克服小样本槽位标记中的“样本遗忘问题”。",
    "en_tdlr": "MCML is a memory-based contrastive meta-learning method that utilizes an explicit memory bank to track label representations of previously trained episodes and introduces an \"adaption-from-memory\" mechanism to overcome the \"sample forgetting issue\" in few-shot slot tagging."
}