{
    "title": "Stochastic Subgradient Descent Escapes Active Strict Saddles on Weakly Convex Functions. (arXiv:2108.02072v4 [math.OC] UPDATED)",
    "abstract": "In non-smooth stochastic optimization, we establish the non-convergence of the stochastic subgradient descent (SGD) to the critical points recently called active strict saddles by Davis and Drusvyatskiy. Such points lie on a manifold $M$ where the function $f$ has a direction of second-order negative curvature. Off this manifold, the norm of the Clarke subdifferential of $f$ is lower-bounded. We require two conditions on $f$. The first assumption is a Verdier stratification condition, which is a refinement of the popular Whitney stratification. It allows us to establish a reinforced version of the projection formula of Bolte \\emph{et.al.} for Whitney stratifiable functions, and which is of independent interest. The second assumption, termed the angle condition, allows to control the distance of the iterates to $M$. When $f$ is weakly convex, our assumptions are generic. Consequently, generically in the class of definable weakly convex functions, the SGD converges to a local minimizer.",
    "link": "http://arxiv.org/abs/2108.02072",
    "context": "Title: Stochastic Subgradient Descent Escapes Active Strict Saddles on Weakly Convex Functions. (arXiv:2108.02072v4 [math.OC] UPDATED)\nAbstract: In non-smooth stochastic optimization, we establish the non-convergence of the stochastic subgradient descent (SGD) to the critical points recently called active strict saddles by Davis and Drusvyatskiy. Such points lie on a manifold $M$ where the function $f$ has a direction of second-order negative curvature. Off this manifold, the norm of the Clarke subdifferential of $f$ is lower-bounded. We require two conditions on $f$. The first assumption is a Verdier stratification condition, which is a refinement of the popular Whitney stratification. It allows us to establish a reinforced version of the projection formula of Bolte \\emph{et.al.} for Whitney stratifiable functions, and which is of independent interest. The second assumption, termed the angle condition, allows to control the distance of the iterates to $M$. When $f$ is weakly convex, our assumptions are generic. Consequently, generically in the class of definable weakly convex functions, the SGD converges to a local minimizer.",
    "path": "papers/21/08/2108.02072.json",
    "total_tokens": 964,
    "translated_title": "随机子梯度下降逃离弱凸函数的活动严格鞍点",
    "translated_abstract": "在非光滑随机优化中，我们证明了随机子梯度下降法（SGD）对最近由Davis和Drusvyatskiy称为活动严格鞍点的收敛性失败。这些点位于函数$f$具有二阶负曲率方向的流形$M$上。在这个流形之外，$f$的Clarke亚微分的范数有下界。我们对$f$有两个条件。第一个假设是一个Verdier分层条件，它是Whitney分层的一种细化。它使我们能够建立Bolte等人关于Whitney分层函数的投影公式的增强版本，这对独立的兴趣。第二个假设，称为角度条件，可以控制迭代点到$M$的距离。当$f$是弱凸的时候，我们的假设是泛化的。因此，在可定义的弱凸函数类中，SGD通常收敛于局部极小值点。",
    "tldr": "本论文研究了在非光滑随机优化中，随机子梯度下降法（SGD）对活动严格鞍点的非收敛性。通过引入Verdier分层条件和角度条件，我们表明在弱凸函数类中，SGD通常收敛于局部极小值点。"
}