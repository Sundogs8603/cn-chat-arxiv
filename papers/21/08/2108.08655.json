{
    "title": "Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning. (arXiv:2108.08655v2 [cs.LG] UPDATED)",
    "abstract": "Actor-critic algorithms are widely used in reinforcement learning, but are challenging to mathematically analyse due to the online arrival of non-i.i.d. data samples. The distribution of the data samples dynamically changes as the model is updated, introducing a complex feedback loop between the data distribution and the reinforcement learning algorithm. We prove that, under a time rescaling, the online actor-critic algorithm with tabular parametrization converges to an ordinary differential equation (ODE) as the number of updates becomes large. The proof first establishes the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the data samples around a dynamic probability measure, which is a function of the evolving actor model, vanish as the number of updates become large. Once the ODE limit has been derived, we study its convergence properties using a two time-scale analysis which asymptotically de-co",
    "link": "http://arxiv.org/abs/2108.08655",
    "context": "Title: Global Convergence of the ODE Limit for Online Actor-Critic Algorithms in Reinforcement Learning. (arXiv:2108.08655v2 [cs.LG] UPDATED)\nAbstract: Actor-critic algorithms are widely used in reinforcement learning, but are challenging to mathematically analyse due to the online arrival of non-i.i.d. data samples. The distribution of the data samples dynamically changes as the model is updated, introducing a complex feedback loop between the data distribution and the reinforcement learning algorithm. We prove that, under a time rescaling, the online actor-critic algorithm with tabular parametrization converges to an ordinary differential equation (ODE) as the number of updates becomes large. The proof first establishes the geometric ergodicity of the data samples under a fixed actor policy. Then, using a Poisson equation, we prove that the fluctuations of the data samples around a dynamic probability measure, which is a function of the evolving actor model, vanish as the number of updates become large. Once the ODE limit has been derived, we study its convergence properties using a two time-scale analysis which asymptotically de-co",
    "path": "papers/21/08/2108.08655.json",
    "total_tokens": 948,
    "translated_title": "在强化学习中，在线演员-评论家算法的ODE极限全局收敛性",
    "translated_abstract": "演员-评论家算法在强化学习中被广泛使用，但由于非独立同分布的在线数据样本的到来，其在数学上分析具有挑战性。数据样本的分布随着模型的更新而动态变化，引入了数据分布和强化学习算法之间复杂的反馈循环。我们证明，在时间重缩放下，带有表格参数化的在线演员-评论家算法在更新次数趋近于无穷大时收敛于常微分方程（ODE）。证明首先在固定的演员策略下建立数据样本的几何遍历性。然后，使用泊松方程，我们证明随着更新次数趋近于无穷大，数据样本关于一种动态概率测度的波动在演变的演员模型的函数下消失。一旦得到ODE极限，我们使用双时间尺度分析研究其收敛特性。",
    "tldr": "该论文研究了强化学习中在线演员-评论家算法的全局收敛性。通过数学分析证明，随着更新次数趋近无穷大，带有表格参数化的在线演员-评论家算法收敛于常微分方程。研究结果可以帮助我们理解演员-评论家算法在实践中的行为和性质。",
    "en_tdlr": "This paper investigates the global convergence of online actor-critic algorithms in reinforcement learning. The analysis shows that the algorithm converges to an ordinary differential equation as the number of updates increases. The findings help us understand the behavior and properties of actor-critic algorithms in practice."
}