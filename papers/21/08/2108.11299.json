{
    "title": "Certifiers Make Neural Networks Vulnerable to Availability Attacks. (arXiv:2108.11299v5 [cs.LG] UPDATED)",
    "abstract": "To achieve reliable, robust, and safe AI systems, it is vital to implement fallback strategies when AI predictions cannot be trusted. Certifiers for neural networks are a reliable way to check the robustness of these predictions. They guarantee for some predictions that a certain class of manipulations or attacks could not have changed the outcome. For the remaining predictions without guarantees, the method abstains from making a prediction, and a fallback strategy needs to be invoked, which typically incurs additional costs, can require a human operator, or even fail to provide any prediction. While this is a key concept towards safe and secure AI, we show for the first time that this approach comes with its own security risks, as such fallback strategies can be deliberately triggered by an adversary. In addition to naturally occurring abstains for some inputs and perturbations, the adversary can use training-time attacks to deliberately trigger the fallback with high probability. Th",
    "link": "http://arxiv.org/abs/2108.11299",
    "context": "Title: Certifiers Make Neural Networks Vulnerable to Availability Attacks. (arXiv:2108.11299v5 [cs.LG] UPDATED)\nAbstract: To achieve reliable, robust, and safe AI systems, it is vital to implement fallback strategies when AI predictions cannot be trusted. Certifiers for neural networks are a reliable way to check the robustness of these predictions. They guarantee for some predictions that a certain class of manipulations or attacks could not have changed the outcome. For the remaining predictions without guarantees, the method abstains from making a prediction, and a fallback strategy needs to be invoked, which typically incurs additional costs, can require a human operator, or even fail to provide any prediction. While this is a key concept towards safe and secure AI, we show for the first time that this approach comes with its own security risks, as such fallback strategies can be deliberately triggered by an adversary. In addition to naturally occurring abstains for some inputs and perturbations, the adversary can use training-time attacks to deliberately trigger the fallback with high probability. Th",
    "path": "papers/21/08/2108.11299.json",
    "total_tokens": 872,
    "translated_title": "Certifiers使神经网络容易受到可用性攻击。",
    "translated_abstract": "为了实现可靠、鲁棒和安全的人工智能系统，在不能信任AI预测时实施后备策略是至关重要的。对神经网络的认证是检查这些预测鲁棒性的可靠方法。他们保证对于一些预测，某些操纵或攻击方式不会改变结果。对于没有保证的其他预测，该方法会放弃预测，并需要调用后备策略，这通常会增加额外的成本，可能需要人工操作，甚至无法提供任何预测。虽然这是实现安全和可靠的人工智能的关键概念，但我们首次展示了这种方法自身存在安全风险，因为对手可以故意触发这些后备策略。除了某些输入和扰动自然发生时的放弃外，对手还可以在训练时使用攻击方式以很高的概率故意触发后备策略。",
    "tldr": "对神经网络进行认证以确保预测的鲁棒性是实现可靠AI的关键概念，然而这种方法存在安全风险，对手可以故意触发后备策略，使系统容易受到可用性攻击。"
}