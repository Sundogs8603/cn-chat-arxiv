{
    "title": "Reinforcement Learning for Robot Navigation with Adaptive Forward Simulation Time (AFST) in a Semi-Markov Model. (arXiv:2108.06161v4 [cs.RO] UPDATED)",
    "abstract": "Deep reinforcement learning (DRL) algorithms have proven effective in robot navigation, especially in unknown environments, by directly mapping perception inputs into robot control commands. However, most existing methods ignore the local minimum problem in navigation and thereby cannot handle complex unknown environments. In this paper, we propose the first DRL-based navigation method modeled by a semi-Markov decision process (SMDP) with continuous action space, named Adaptive Forward Simulation Time (AFST), to overcome this problem. Specifically, we reduce the dimensions of the action space and improve the distributed proximal policy optimization (DPPO) algorithm for the specified SMDP problem by modifying its GAE to better estimate the policy gradient in SMDPs. Experiments in various unknown environments demonstrate the effectiveness of AFST.",
    "link": "http://arxiv.org/abs/2108.06161",
    "context": "Title: Reinforcement Learning for Robot Navigation with Adaptive Forward Simulation Time (AFST) in a Semi-Markov Model. (arXiv:2108.06161v4 [cs.RO] UPDATED)\nAbstract: Deep reinforcement learning (DRL) algorithms have proven effective in robot navigation, especially in unknown environments, by directly mapping perception inputs into robot control commands. However, most existing methods ignore the local minimum problem in navigation and thereby cannot handle complex unknown environments. In this paper, we propose the first DRL-based navigation method modeled by a semi-Markov decision process (SMDP) with continuous action space, named Adaptive Forward Simulation Time (AFST), to overcome this problem. Specifically, we reduce the dimensions of the action space and improve the distributed proximal policy optimization (DPPO) algorithm for the specified SMDP problem by modifying its GAE to better estimate the policy gradient in SMDPs. Experiments in various unknown environments demonstrate the effectiveness of AFST.",
    "path": "papers/21/08/2108.06161.json",
    "total_tokens": 961,
    "translated_title": "自适应前向模拟时间的强化学习在半马尔科夫模型中的机器人导航应用",
    "translated_abstract": "深度强化学习算法已经被证明对机器人导航非常有效，尤其在未知环境中，通过将感知输入直接映射为机器人控制命令。然而，大多数现有方法忽略了导航中的局部最小问题，因此无法处理复杂的未知环境。本文提出了第一个基于深度强化学习的导航方法，该方法使用连续动作空间的半马尔科夫决策过程模型，并命名为自适应前向模拟时间 (AFST)，以克服这个问题。具体而言，我们通过减少动作空间的维度，并改进分布式近端策略优化 (DPPO) 算法来适应指定的半马尔科夫问题，修改其广义优势估计 (GAE) 以更好地估计 SMDP 中的策略梯度。在各种未知环境中的实验证明了 AFST 的有效性。",
    "tldr": "本文提出了一种基于深度强化学习的机器人导航方法，通过自适应前向模拟时间 (AFST) 在半马尔科夫模型中进行建模，克服了导航中的局部最小问题，并通过减少动作空间的维度和改进分布式近端策略优化 (DPPO) 算法来优化模型，在各种未知环境中取得了良好的效果。"
}