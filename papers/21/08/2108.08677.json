{
    "title": "Order Optimal Bounds for One-Shot Federated Learning over non-Convex Loss Functions",
    "abstract": "We consider the problem of federated learning in a one-shot setting in which there are $m$ machines, each observing $n$ sample functions from an unknown distribution on non-convex loss functions. Let $F:[-1,1]^d\\to\\mathbb{R}$ be the expected loss function with respect to this unknown distribution. The goal is to find an estimate of the minimizer of $F$. Based on its observations, each machine generates a signal of bounded length $B$ and sends it to a server. The server collects signals of all machines and outputs an estimate of the minimizer of $F$. We show that the expected loss of any algorithm is lower bounded by $\\max\\big(1/(\\sqrt{n}(mB)^{1/d}), 1/\\sqrt{mn}\\big)$, up to a logarithmic factor. We then prove that this lower bound is order optimal in $m$ and $n$ by presenting a distributed learning algorithm, called Multi-Resolution Estimator for Non-Convex loss function (MRE-NC), whose expected loss matches the lower bound for large $mn$ up to polylogarithmic factors.",
    "link": "https://arxiv.org/abs/2108.08677",
    "context": "Title: Order Optimal Bounds for One-Shot Federated Learning over non-Convex Loss Functions\nAbstract: We consider the problem of federated learning in a one-shot setting in which there are $m$ machines, each observing $n$ sample functions from an unknown distribution on non-convex loss functions. Let $F:[-1,1]^d\\to\\mathbb{R}$ be the expected loss function with respect to this unknown distribution. The goal is to find an estimate of the minimizer of $F$. Based on its observations, each machine generates a signal of bounded length $B$ and sends it to a server. The server collects signals of all machines and outputs an estimate of the minimizer of $F$. We show that the expected loss of any algorithm is lower bounded by $\\max\\big(1/(\\sqrt{n}(mB)^{1/d}), 1/\\sqrt{mn}\\big)$, up to a logarithmic factor. We then prove that this lower bound is order optimal in $m$ and $n$ by presenting a distributed learning algorithm, called Multi-Resolution Estimator for Non-Convex loss function (MRE-NC), whose expected loss matches the lower bound for large $mn$ up to polylogarithmic factors.",
    "path": "papers/21/08/2108.08677.json",
    "total_tokens": 897,
    "translated_title": "一次性联邦学习中非凸损失函数的最优界限",
    "translated_abstract": "我们考虑在一次性设置下的联邦学习问题，其中有m台机器，每台机器从未知分布上的非凸损失函数中观测到n个样本函数。设F：[-1,1]^d → R是这个未知分布下的预期损失函数。目标是找到F的最小化估计。基于它的观测，每台机器产生一个长度有界为B的信号并将其发送到服务器。服务器收集所有机器的信号并输出F的最小化估计。我们证明，任何算法的期望损失下界为max(1/(\\sqrt{n}(mB)^{1/d}), 1/\\sqrt{mn})，除去对数因子。然后我们证明该下界在m和n上是次优的，通过提出一个分布式学习算法，称为多分辨率非凸损失函数估计器(MRE-NC)，其期望损失在大的mn时与下界匹配，除了多对数因子。",
    "tldr": "本论文研究了一次性联邦学习中非凸损失函数的最优界限，并提出了一个分布式学习算法(MRE-NC)，其期望损失与最优界限匹配。"
}