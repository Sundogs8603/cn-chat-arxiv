{
    "title": "Neural Operator: Learning Maps Between Function Spaces. (arXiv:2108.08481v5 [cs.LG] UPDATED)",
    "abstract": "The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps f",
    "link": "http://arxiv.org/abs/2108.08481",
    "context": "Title: Neural Operator: Learning Maps Between Function Spaces. (arXiv:2108.08481v5 [cs.LG] UPDATED)\nAbstract: The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps f",
    "path": "papers/21/08/2108.08481.json",
    "total_tokens": 888,
    "translated_title": "神经算子：学习函数空间之间的映射",
    "translated_abstract": "传统神经网络的发展主要集中在学习有限维欧氏空间或有限集之间的映射上。我们提出了一种神经网络的泛化，称为神经算子，用于学习在无限维函数空间之间映射的算子。我们将神经算子构建为线性积分算子和非线性激活函数的组合。我们证明了我们提出的神经算子的通用逼近定理，表明它可以逼近任何给定的非线性连续算子。所提出的神经算子也是离散不变的，即它们在不同的基础函数空间离散化之间共享相同的模型参数。此外，我们介绍了四类高效参数化的神经算子，即图神经算子、多极图神经算子、低秩神经算子和傅里叶神经算子。神经算子的重要应用之一是学习代理映射f。",
    "tldr": "本文提出了一种神经算子，可以学习无限维函数空间之间的映射，可以逼近给定的非线性连续算子且离散不变，同时介绍了四类高效参数化的神经算子。这项技术的一个重要应用是学习代理映射f。",
    "en_tdlr": "This paper proposes a neural operator that can learn mappings between infinite dimensional function spaces, approximating any given nonlinear continuous operator and remaining discretization-invariant. Four efficient parameterization classes are also introduced. An important application of neural operators is learning surrogate maps f."
}