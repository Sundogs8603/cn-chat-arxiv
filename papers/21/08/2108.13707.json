{
    "title": "Wild Bootstrap for Instrumental Variables Regressions with Weak and Few Clusters. (arXiv:2108.13707v5 [econ.EM] UPDATED)",
    "abstract": "We study the wild bootstrap inference for instrumental variable regressions in the framework of a small number of large clusters in which the number of clusters is viewed as fixed and the number of observations for each cluster diverges to infinity. We first show that the wild bootstrap Wald test, with or without using the cluster-robust covariance estimator, controls size asymptotically up to a small error as long as the parameters of endogenous variables are strongly identified in at least one of the clusters. Then, we establish the required number of strong clusters for the test to have power against local alternatives. We further develop a wild bootstrap Anderson-Rubin test for the full-vector inference and show that it controls size asymptotically up to a small error even under weak or partial identification in all clusters. We illustrate the good finite sample performance of the new inference methods using simulations and provide an empirical application to a well-known dataset a",
    "link": "http://arxiv.org/abs/2108.13707",
    "context": "Title: Wild Bootstrap for Instrumental Variables Regressions with Weak and Few Clusters. (arXiv:2108.13707v5 [econ.EM] UPDATED)\nAbstract: We study the wild bootstrap inference for instrumental variable regressions in the framework of a small number of large clusters in which the number of clusters is viewed as fixed and the number of observations for each cluster diverges to infinity. We first show that the wild bootstrap Wald test, with or without using the cluster-robust covariance estimator, controls size asymptotically up to a small error as long as the parameters of endogenous variables are strongly identified in at least one of the clusters. Then, we establish the required number of strong clusters for the test to have power against local alternatives. We further develop a wild bootstrap Anderson-Rubin test for the full-vector inference and show that it controls size asymptotically up to a small error even under weak or partial identification in all clusters. We illustrate the good finite sample performance of the new inference methods using simulations and provide an empirical application to a well-known dataset a",
    "path": "papers/21/08/2108.13707.json",
    "total_tokens": 877,
    "translated_title": "有限且稀疏集群下的工具变量回归的野生Bootstrap方法",
    "translated_abstract": "本文研究了在有限且稀疏集群的框架下，工具变量回归的野生Bootstrap推断方法。我们首先证明，无论是否使用集群健壮的协方差估计器，野生Bootstrap的Wald检验在强识别条件下几乎控制了大小，只有一部分集群内内生变量的参数是强识别的。然后，我们确定了对于该检验在局部替代假设下具有功效所需的强集群数量。我们进一步发展了一种用于完全向量推断的野生Bootstrap的Anderson-Rubin检验，并证明即使在所有集群中存在弱或部分识别，它也在近似上控制了大小。我们通过仿真实验展示了新推断方法的良好有限样本性能，并对一个已知数据集进行了实证应用。",
    "tldr": "本文研究了有限且稀疏集群下工具变量回归的野生Bootstrap推断方法，证明了在强识别条件下该方法控制了大小，并发展了一种野生Bootstrap的Anderson-Rubin检验，能够处理弱或部分识别情况。"
}