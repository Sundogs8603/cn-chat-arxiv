{
    "title": "Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning. (arXiv:2108.12988v3 [cs.LG] UPDATED)",
    "abstract": "In multi-agent reinforcement learning, the behaviors that agents learn in a single Markov Game (MG) are typically confined to the given agent number. Every single MG induced by varying the population may possess distinct optimal joint strategies and game-specific knowledge, which are modeled independently in modern multi-agent reinforcement learning algorithms. In this work, our focus is on creating agents that can generalize across population-varying MGs. Instead of learning a unimodal policy, each agent learns a policy set comprising effective strategies across a variety of games. To achieve this, we propose Meta Representations for Agents (MRA) that explicitly models the game-common and game-specific strategic knowledge. By representing the policy sets with multi-modal latent policies, the game-common strategic knowledge and diverse strategic modes are discovered through an iterative optimization procedure. We prove that by approximately maximizing the resulting constrained mutual i",
    "link": "http://arxiv.org/abs/2108.12988",
    "context": "Title: Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning. (arXiv:2108.12988v3 [cs.LG] UPDATED)\nAbstract: In multi-agent reinforcement learning, the behaviors that agents learn in a single Markov Game (MG) are typically confined to the given agent number. Every single MG induced by varying the population may possess distinct optimal joint strategies and game-specific knowledge, which are modeled independently in modern multi-agent reinforcement learning algorithms. In this work, our focus is on creating agents that can generalize across population-varying MGs. Instead of learning a unimodal policy, each agent learns a policy set comprising effective strategies across a variety of games. To achieve this, we propose Meta Representations for Agents (MRA) that explicitly models the game-common and game-specific strategic knowledge. By representing the policy sets with multi-modal latent policies, the game-common strategic knowledge and diverse strategic modes are discovered through an iterative optimization procedure. We prove that by approximately maximizing the resulting constrained mutual i",
    "path": "papers/21/08/2108.12988.json",
    "total_tokens": 888,
    "translated_title": "学习元表示来增强多智能体强化学习中的代理",
    "translated_abstract": "在多智能体强化学习中，代理在单个马尔科夫博弈中学习的行为通常仅限于给定的代理数量。由人口数量变化引起的每个单个马尔科夫博弈可能具有不同的最佳联合策略和游戏特定知识，在现代多智能体强化学习算法中独立建模。本文针对创建可以跨人口数量变化的马尔科夫博弈进行推广的代理。每个代理不再学习单一策略，而是学习一个包含各种游戏有效策略的策略集。为了实现这一目标，我们提出了代理元表示（MRA），明确建模了游戏通用和特定的策略知识。通过用多模态潜在策略表示策略集，通过迭代优化过程发现游戏通用的战略知识和不同的战略模式。我们证明通过最大化相关限制的互信息可以获得代理的表现提升。",
    "tldr": "本文提出代理元表示（MRA），能够跨多种多智能体强化学习中变化的人口数量进行推广的代理，并证明了通过最大化互信息来实现代理性能的提升。",
    "en_tdlr": "This paper proposes Meta Representations for Agents (MRA) that can generalize agents across varying populations in multi-agent reinforcement learning, and proves the improvement of agent performance by maximizing the mutual information with constrained constraints."
}