{
    "title": "Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v9 [cs.LG] UPDATED)",
    "abstract": "Existing machines are functionally specific tools that were made for easy prediction and control. Tomorrow's machines may be closer to biological systems in their mutability, resilience, and autonomy. But first they must be capable of learning and retaining new information without being exposed to it arbitrarily often. Past efforts to engineer such systems have sought to build or regulate artificial neural networks using disjoint sets of weights that are uniquely sensitive to specific tasks or inputs. This has not yet enabled continual learning over long sequences of previously unseen data without corrupting existing knowledge: a problem known as catastrophic forgetting. In this paper, we introduce a system that can learn sequentially over previously unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is done by controlling the activity of weights in a convolutional neural network on the basis of inputs using top-down regulation generated by a second feed-forwa",
    "link": "http://arxiv.org/abs/2108.12056",
    "context": "Title: Continual learning under domain transfer with sparse synaptic bursting. (arXiv:2108.12056v9 [cs.LG] UPDATED)\nAbstract: Existing machines are functionally specific tools that were made for easy prediction and control. Tomorrow's machines may be closer to biological systems in their mutability, resilience, and autonomy. But first they must be capable of learning and retaining new information without being exposed to it arbitrarily often. Past efforts to engineer such systems have sought to build or regulate artificial neural networks using disjoint sets of weights that are uniquely sensitive to specific tasks or inputs. This has not yet enabled continual learning over long sequences of previously unseen data without corrupting existing knowledge: a problem known as catastrophic forgetting. In this paper, we introduce a system that can learn sequentially over previously unseen datasets (ImageNet, CIFAR-100) with little forgetting over time. This is done by controlling the activity of weights in a convolutional neural network on the basis of inputs using top-down regulation generated by a second feed-forwa",
    "path": "papers/21/08/2108.12056.json",
    "total_tokens": 814,
    "translated_title": "在稀疏突发神经突触下的领域转移中的持续学习",
    "translated_abstract": "现有的机器是功能特定的工具，易于预测和控制。明天的机器可能更接近生物系统，具有可变性、韧性和自主性。但是首先，它们必须能够学习和保留新信息，而不必随机接触它。过去设计这样的系统的努力是通过构建或调节人工神经网络，使用唯一敏感于特定任务或输入的不相交的权重集合。然而，这尚未实现在长时间的先前未见数据序列上进行持续学习而不破坏现有知识的问题，这被称为灾难性遗忘。在本文中，我们介绍了一个系统，可以在先前未见的数据集（ImageNet，CIFAR-100）上以较小的遗忘逐步学习。这是通过基于输入控制卷积神经网络中权重的活动，使用由第二个前馈生成的自上而下调节来实现的。",
    "tldr": "本文介绍了一个系统，通过稀疏突发神经突触，在领域转移中能够在先前未见的数据集上进行持续学习，减少遗忘。"
}