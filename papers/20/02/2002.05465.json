{
    "title": "Nonasymptotic analysis of Stochastic Gradient Hamiltonian Monte Carlo under local conditions for nonconvex optimization. (arXiv:2002.05465v4 [math.OC] UPDATED)",
    "abstract": "We provide a nonasymptotic analysis of the convergence of the stochastic gradient Hamiltonian Monte Carlo (SGHMC) to a target measure in Wasserstein-2 distance without assuming log-concavity. Our analysis quantifies key theoretical properties of the SGHMC as a sampler under local conditions which significantly improves the findings of previous results. In particular, we prove that the Wasserstein-2 distance between the target and the law of the SGHMC is uniformly controlled by the step-size of the algorithm, therefore demonstrate that the SGHMC can provide high-precision results uniformly in the number of iterations. The analysis also allows us to obtain nonasymptotic bounds for nonconvex optimization problems under local conditions and implies that the SGHMC, when viewed as a nonconvex optimizer, converges to a global minimum with the best known rates. We apply our results to obtain nonasymptotic bounds for scalable Bayesian inference and nonasymptotic generalization bounds.",
    "link": "http://arxiv.org/abs/2002.05465",
    "context": "Title: Nonasymptotic analysis of Stochastic Gradient Hamiltonian Monte Carlo under local conditions for nonconvex optimization. (arXiv:2002.05465v4 [math.OC] UPDATED)\nAbstract: We provide a nonasymptotic analysis of the convergence of the stochastic gradient Hamiltonian Monte Carlo (SGHMC) to a target measure in Wasserstein-2 distance without assuming log-concavity. Our analysis quantifies key theoretical properties of the SGHMC as a sampler under local conditions which significantly improves the findings of previous results. In particular, we prove that the Wasserstein-2 distance between the target and the law of the SGHMC is uniformly controlled by the step-size of the algorithm, therefore demonstrate that the SGHMC can provide high-precision results uniformly in the number of iterations. The analysis also allows us to obtain nonasymptotic bounds for nonconvex optimization problems under local conditions and implies that the SGHMC, when viewed as a nonconvex optimizer, converges to a global minimum with the best known rates. We apply our results to obtain nonasymptotic bounds for scalable Bayesian inference and nonasymptotic generalization bounds.",
    "path": "papers/20/02/2002.05465.json",
    "total_tokens": 1031,
    "translated_title": "非渐进性分析中的随机梯度哈密顿蒙特卡罗方法在非凸优化中的局部条件下",
    "translated_abstract": "我们在不假设对数凹性的情况下，对随机梯度哈密顿蒙特卡罗（SGHMC）的收敛性进行了非渐进性分析，以Wasserstein-2距离衡量它到目标测度的收敛程度。我们的分析在局部条件下，量化了SGHMC作为采样器的关键理论性质，显著改进了之前结果的发现。特别是，我们证明了目标与SGHMC法则之间的Wasserstein-2距离由算法的步长统一控制，从而证明了SGHMC在迭代次数上可以提供高精度的结果。该分析还使我们能够在局部条件下获得非凸优化问题的非渐进性界限，并意味着将SGHMC视为非凸优化器时，它以已知最佳速率收敛到全局最小值。我们应用我们的结果来获得可扩展的贝叶斯推理和非渐进性泛化界限。",
    "tldr": "在非凸优化中，我们提供了随机梯度哈密顿蒙特卡罗（SGHMC）的非渐进性分析，证明了SGHMC作为采样器的关键理论性质，并在局部条件下获得非凸优化问题的非渐进性界限，该方法在迭代次数上可以提供高精度的结果，并以已知最佳速率收敛到全局最小值。",
    "en_tdlr": "We provide a nonasymptotic analysis of the convergence of stochastic gradient Hamiltonian Monte Carlo (SGHMC) in nonconvex optimization, proving its key theoretical properties as a sampler and obtaining nonasymptotic bounds for nonconvex optimization problems under local conditions. SGHMC can provide high-precision results uniformly in the number of iterations and converges to a global minimum with the best known rates."
}