{
    "title": "Measuring Social Biases in Grounded Vision and Language Embeddings. (arXiv:2002.08911v2 [cs.CL] UPDATED)",
    "abstract": "We generalize the notion of social biases from language embeddings to grounded vision and language embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting extending standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these",
    "link": "http://arxiv.org/abs/2002.08911",
    "context": "Title: Measuring Social Biases in Grounded Vision and Language Embeddings. (arXiv:2002.08911v2 [cs.CL] UPDATED)\nAbstract: We generalize the notion of social biases from language embeddings to grounded vision and language embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting extending standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these",
    "path": "papers/20/02/2002.08911.json",
    "total_tokens": 993,
    "translated_title": "在基于图像和语言嵌入中测量社会偏见",
    "translated_abstract": "我们将社会偏见的概念从语言嵌入推广到了基于图像和语言的嵌入中。存在于基于图像和语言嵌入中的偏见似乎与未经培训的嵌入中的偏见同等甚至更为重要。尽管视觉和语言可能受到不同的偏见，人们可能希望这些偏见可以相互衰减，但实际情况并非如此。我们提出了多种泛化度量嵌入中的偏见的方法，并引入了泛化空间（Grounded-WEAT和Grounded-SEAT），并展示了三种不同的泛化方法对于偏见、语言和视觉交互作用的重要问题具有不同的回答。我们使用这些度量方法在一个新的数据集上进行实验，这是第一个用于基于图像的偏见的数据集，通过在COCO、概念字幕和谷歌图像等标准语言偏见基准上增加10,228张图像来构建。数据集的构建具有挑战性，因为视觉数据集本身就存在很大的偏见。",
    "tldr": "该论文推广了社会偏见的概念，从语言嵌入扩展到了基于图像和语言的嵌入。研究表明，基于图像和语言的嵌入中的偏见与未经培训的嵌入中的偏见同等重要甚至更重要。并通过引入新的度量方法来研究偏见、语言和视觉之间的交互作用。"
}