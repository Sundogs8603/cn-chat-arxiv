{
    "title": "On Biased Compression for Distributed Learning. (arXiv:2002.12410v4 [cs.LG] UPDATED)",
    "abstract": "In the last few years, various communication compression techniques have emerged as an indispensable tool helping to alleviate the communication bottleneck in distributed learning. However, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors, very little is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to (stochastic) gradient descent and distributed (stochastic) gradient descent. We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings. We prove that distributed compressed SGD method, employed with error feedback mechanism, enjoys the ergodic rate $O\\left( \\delta L \\exp \\left[-\\frac{\\mu K}{\\delta L}\\right] + \\frac{(C + \\delta D)}{K\\mu}\\right)$, where $\\delta\\ge 1$ is a compression parameter which grows when m",
    "link": "http://arxiv.org/abs/2002.12410",
    "context": "Title: On Biased Compression for Distributed Learning. (arXiv:2002.12410v4 [cs.LG] UPDATED)\nAbstract: In the last few years, various communication compression techniques have emerged as an indispensable tool helping to alleviate the communication bottleneck in distributed learning. However, despite the fact biased compressors often show superior performance in practice when compared to the much more studied and understood unbiased compressors, very little is known about them. In this work we study three classes of biased compression operators, two of which are new, and their performance when applied to (stochastic) gradient descent and distributed (stochastic) gradient descent. We show for the first time that biased compressors can lead to linear convergence rates both in the single node and distributed settings. We prove that distributed compressed SGD method, employed with error feedback mechanism, enjoys the ergodic rate $O\\left( \\delta L \\exp \\left[-\\frac{\\mu K}{\\delta L}\\right] + \\frac{(C + \\delta D)}{K\\mu}\\right)$, where $\\delta\\ge 1$ is a compression parameter which grows when m",
    "path": "papers/20/02/2002.12410.json",
    "total_tokens": 925,
    "translated_title": "关于偏压压缩在分布式学习中的应用",
    "translated_abstract": "近年来，各种通信压缩技术作为分布式学习中缓解通信瓶颈的必不可少的工具而出现。然而，尽管偏压压缩器在实践中往往表现出比被广泛研究和理解的无偏压压缩器更好的性能，但对它们的了解非常有限。在本研究中，我们研究了三类偏压压缩算子，其中两类是新的，并研究了它们在（随机）梯度下降和分布式（随机）梯度下降中的性能。我们首次证明了偏压压缩器可以在单节点和分布式环境中实现线性收敛速率。我们证明了经过错误反馈机制处理的分布式压缩的SGD方法具有遗传速率$O\\left( \\delta L \\exp \\left[-\\frac{\\mu K}{\\delta L}\\right] + \\frac{(C + \\delta D)}{K\\mu}\\right)$，其中$\\delta\\ge 1$是一个逐渐增长的压缩参数，m未完待续",
    "tldr": "本研究研究了偏压压缩在分布式学习中的应用，首次证明了偏压压缩器可以在单节点和分布式环境中实现线性收敛速率。",
    "en_tdlr": "This study investigates the application of biased compression in distributed learning and demonstrates for the first time that biased compressors can achieve linear convergence rates in both single node and distributed settings."
}