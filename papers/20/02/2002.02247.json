{
    "title": "Almost Sure Convergence of Dropout Algorithms for Neural Networks. (arXiv:2002.02247v2 [math.OC] UPDATED)",
    "abstract": "We investigate the convergence and convergence rate of stochastic training algorithms for Neural Networks (NNs) that have been inspired by Dropout (Hinton et al., 2012). With the goal of avoiding overfitting during training of NNs, dropout algorithms consist in practice of multiplying the weight matrices of a NN componentwise by independently drawn random matrices with $\\{0, 1 \\}$-valued entries during each iteration of Stochastic Gradient Descent (SGD). This paper presents a probability theoretical proof that for fully-connected NNs with differentiable, polynomially bounded activation functions, if we project the weights onto a compact set when using a dropout algorithm, then the weights of the NN converge to a unique stationary point of a projected system of Ordinary Differential Equations (ODEs). After this general convergence guarantee, we go on to investigate the convergence rate of dropout. Firstly, we obtain generic sample complexity bounds for finding $\\epsilon$-stationary poin",
    "link": "http://arxiv.org/abs/2002.02247",
    "context": "Title: Almost Sure Convergence of Dropout Algorithms for Neural Networks. (arXiv:2002.02247v2 [math.OC] UPDATED)\nAbstract: We investigate the convergence and convergence rate of stochastic training algorithms for Neural Networks (NNs) that have been inspired by Dropout (Hinton et al., 2012). With the goal of avoiding overfitting during training of NNs, dropout algorithms consist in practice of multiplying the weight matrices of a NN componentwise by independently drawn random matrices with $\\{0, 1 \\}$-valued entries during each iteration of Stochastic Gradient Descent (SGD). This paper presents a probability theoretical proof that for fully-connected NNs with differentiable, polynomially bounded activation functions, if we project the weights onto a compact set when using a dropout algorithm, then the weights of the NN converge to a unique stationary point of a projected system of Ordinary Differential Equations (ODEs). After this general convergence guarantee, we go on to investigate the convergence rate of dropout. Firstly, we obtain generic sample complexity bounds for finding $\\epsilon$-stationary poin",
    "path": "papers/20/02/2002.02247.json",
    "total_tokens": 940,
    "translated_title": "神经网络的Dropout算法的几乎必然收敛性",
    "translated_abstract": "本文研究了受Dropout（Hinton等，2012）启发的神经网络随机训练算法的收敛性和收敛速度。为了避免训练期间的过度拟合，实践中的dropout算法实际上是通过在随机梯度下降（SGD）的每次迭代期间将神经网络的权重矩阵逐元素与独立绘制该函数的{0,1} -值矩阵相乘。本文提出了一个概率理论证明，针对具有可微、多项式有界激活函数的全连通神经网络，如果我们在使用dropout算法时将权重投影到紧致集上，则NN的权重将收敛于正常微分方程（ODEs）的投影系统的唯一定常点。在此通用收敛性保证之后，我们继续研究dropout的收敛速度。首先，我们获得了找到ε-定态点的通用样本复杂度界限。",
    "tldr": "本文提出了针对神经网络的dropout算法的收敛性及收敛速度的研究，给出了概率论证明，证明其权重将收敛于正常微分方程系统的投影唯一稳态点，同时给出了ε-定态点的通用样本复杂度限制。",
    "en_tdlr": "This paper studies the convergence and convergence rate of the stochastic training algorithms inspired by Dropout for Neural Networks, and presents a probability theoretical proof that the weights of the NN will converge to a unique stationary point of a projected system of Ordinary Differential Equations (ODEs), as well as providing generic sample complexity bounds for finding ε-stationary points."
}