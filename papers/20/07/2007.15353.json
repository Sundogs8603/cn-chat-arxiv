{
    "title": "Growing Efficient Deep Networks by Structured Continuous Sparsification. (arXiv:2007.15353v2 [cs.LG] UPDATED)",
    "abstract": "We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 accuracy -- all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, ",
    "link": "http://arxiv.org/abs/2007.15353",
    "context": "Title: Growing Efficient Deep Networks by Structured Continuous Sparsification. (arXiv:2007.15353v2 [cs.LG] UPDATED)\nAbstract: We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve $49.7\\%$ inference FLOPs and $47.4\\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\\%$ top-1 accuracy -- all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, ",
    "path": "papers/20/07/2007.15353.json",
    "total_tokens": 937,
    "translated_title": "结构化连续稀疏化增强深度网络的训练效率",
    "translated_abstract": "我们开发了一种在训练过程中以精度和稀疏性为驱动的深度网络结构生长方法。与现有的基于完整模型或超网格架构的剪枝或架构搜索技术不同，我们的方法可以从一个小而简单的种子架构开始，动态地增长和修剪层和过滤器。通过将离散网络结构优化的连续松弛与采样稀疏子网络方案相结合，我们可以产生紧凑的修剪网络，同时显著降低训练的计算复杂度。例如，在ImageNet上，与基线ResNet-50相比，我们实现了49.7％的推理FLOPs和47.4％的训练FLOPs节省，同时保持75.2％的top-1精度--所有这些都没有任何专门的微调阶段。在CIFAR，ImageNet，PASCAL VOC和Penn Treebank上进行实验，使用卷积网络进行图像分类和语义分割。",
    "tldr": "本文提出一种结构化连续稀疏化的深度网络结构生长方法，通过连续松弛和采样稀疏子网络，可以在训练过程中达到紧凑的修剪网络结构，同时大幅降低计算复杂度并保持较高的准确率。",
    "en_tdlr": "This paper proposes a structured continuous sparsification method for growing deep network architectures over the course of training, which can produce compact pruned networks with high accuracy and significantly reduce computational cost by combining continuous relaxation and sampling sparse subnetworks."
}