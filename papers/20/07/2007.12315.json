{
    "title": "Bayesian Robust Optimization for Imitation Learning",
    "abstract": "arXiv:2007.12315v4 Announce Type: replace  Abstract: One of the main challenges in imitation learning is determining what action an agent should take when outside the state distribution of the demonstrations. Inverse reinforcement learning (IRL) can enable generalization to new states by learning a parameterized reward function, but these approaches still face uncertainty over the true reward function and corresponding optimal policy. Existing safe imitation learning approaches based on IRL deal with this uncertainty using a maxmin framework that optimizes a policy under the assumption of an adversarial reward function, whereas risk-neutral IRL approaches either optimize a policy for the mean or MAP reward function. While completely ignoring risk can lead to overly aggressive and unsafe policies, optimizing in a fully adversarial sense is also problematic as it can lead to overly conservative policies that perform poorly in practice. To provide a bridge between these two extremes, we p",
    "link": "https://arxiv.org/abs/2007.12315",
    "context": "Title: Bayesian Robust Optimization for Imitation Learning\nAbstract: arXiv:2007.12315v4 Announce Type: replace  Abstract: One of the main challenges in imitation learning is determining what action an agent should take when outside the state distribution of the demonstrations. Inverse reinforcement learning (IRL) can enable generalization to new states by learning a parameterized reward function, but these approaches still face uncertainty over the true reward function and corresponding optimal policy. Existing safe imitation learning approaches based on IRL deal with this uncertainty using a maxmin framework that optimizes a policy under the assumption of an adversarial reward function, whereas risk-neutral IRL approaches either optimize a policy for the mean or MAP reward function. While completely ignoring risk can lead to overly aggressive and unsafe policies, optimizing in a fully adversarial sense is also problematic as it can lead to overly conservative policies that perform poorly in practice. To provide a bridge between these two extremes, we p",
    "path": "papers/20/07/2007.12315.json",
    "total_tokens": 856,
    "translated_title": "基于贝叶斯鲁棒优化的模仿学习",
    "translated_abstract": "模仿学习中的一个主要挑战是确定在演示的状态分布之外时代理应采取什么行动。逆强化学习（IRL）可以通过学习参数化奖励函数实现对新状态的泛化，但这些方法仍然面临对真实奖励函数和相应最优策略的不确定性。现有基于IRL的安全模仿学习方法使用maxmin框架处理这种不确定性，该框架在假设有一个对抗性奖励函数的情况下优化策略，而风险中立的IRL方法则优化均值或MAP奖励函数的策略。完全忽视风险可能会导致过于激进和不安全的策略，而完全以对抗性方式优化也是有问题的，因为它可能导致表现不佳的过度保守策略。为了在这两个极端之间建立桥梁，我们提出了一种基于贝叶斯鲁棒优化的方法，该方法在状态的置信区间内对策略进行优化。",
    "tldr": "提出了基于贝叶斯鲁棒优化的模仿学习方法，以在状态不确定性下同时考虑激进和保守的策略优化。",
    "en_tdlr": "Proposed a Bayesian robust optimization approach for imitation learning that considers both aggressive and conservative policy optimization under state uncertainty."
}