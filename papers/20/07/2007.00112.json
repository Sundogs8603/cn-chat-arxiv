{
    "title": "Robustness to Transformations Across Categories: Is Robustness To Transformations Driven by Invariant Neural Representations?. (arXiv:2007.00112v3 [cs.CV] UPDATED)",
    "abstract": "Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive robustness to recognize objects under transformations (eg. blur or noise) when these transformations are included in the training set. A hypothesis to explain such robustness is that DCNNs develop invariant neural representations that remain unaltered when the image is transformed. However, to what extent this hypothesis holds true is an outstanding question, as robustness to transformations could be achieved with properties different from invariance, eg. parts of the network could be specialized to recognize either transformed or non-transformed images. This paper investigates the conditions under which invariant neural representations emerge by leveraging that they facilitate robustness to transformations beyond the training distribution. Concretely, we analyze a training paradigm in which only some object categories are seen transformed during training and evaluate whether the DCNN is robust to transformations a",
    "link": "http://arxiv.org/abs/2007.00112",
    "context": "Title: Robustness to Transformations Across Categories: Is Robustness To Transformations Driven by Invariant Neural Representations?. (arXiv:2007.00112v3 [cs.CV] UPDATED)\nAbstract: Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive robustness to recognize objects under transformations (eg. blur or noise) when these transformations are included in the training set. A hypothesis to explain such robustness is that DCNNs develop invariant neural representations that remain unaltered when the image is transformed. However, to what extent this hypothesis holds true is an outstanding question, as robustness to transformations could be achieved with properties different from invariance, eg. parts of the network could be specialized to recognize either transformed or non-transformed images. This paper investigates the conditions under which invariant neural representations emerge by leveraging that they facilitate robustness to transformations beyond the training distribution. Concretely, we analyze a training paradigm in which only some object categories are seen transformed during training and evaluate whether the DCNN is robust to transformations a",
    "path": "papers/20/07/2007.00112.json",
    "total_tokens": 1129,
    "translated_title": "跨类别变换的鲁棒性：鲁棒性是否由不变神经表示驱动？",
    "translated_abstract": "深度卷积神经网络（DCNNs）已经证明在识别物体在变换下的鲁棒性方面取得了令人印象深刻的结果（例如模糊或噪音），当这些变换被包含在训练集中时。一个解释这种鲁棒性的假设是，DCNNs发展出的不变神经表示在图像转换时不发生改变。然而，这个假设的真实程度是一个尚未解决的问题，因为鲁棒性可能是通过与不变性不同的特性实现的，例如，网络的某些部分可能专门用于识别转换或非转换的图像。本文通过利用不变神经表示促进对训练集之外的变换的鲁棒性，研究了不变神经表示出现的条件。具体而言，我们分析了一种训练范式，在该训练范式中，只有一些对象类别在训练期间被变换，然后评价DCNN是否对所有类别的变换具有鲁棒性，包括那些在训练期间从未见过的类别的变换。我们的实验表明，跨类别变换的鲁棒性是由不变表示所促进的，支持鲁棒性是由不变性驱动而不是专门的子网络的假设。",
    "tldr": "本文通过观察不同种类的图像转换后深度卷积神经网络（DCNNs）的表现，探讨了不变神经表示是否促进了跨类别的图像鲁棒性。实验表明，跨类别变换的鲁棒性是由不变表示所促进的，支持鲁棒性是由不变性驱动而不是专门的子网络的假设。",
    "en_tdlr": "This paper investigates the impact of invariant neural representations on robustness of deep convolutional neural networks (DCNNs) to transformations across different categories of objects. The experiments suggest that the robustness to transformations across categories is facilitated by invariant representations, supporting the hypothesis that robustness is driven by invariance rather than specialized subnetworks."
}