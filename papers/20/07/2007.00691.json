{
    "title": "Falsification-Based Robust Adversarial Reinforcement Learning. (arXiv:2007.00691v3 [cs.RO] UPDATED)",
    "abstract": "Reinforcement learning (RL) has achieved enormous progress in solving various sequential decision-making problems, such as control tasks in robotics. Since policies are overfitted to training environments, RL methods have often failed to be generalized to safety-critical test scenarios. Robust adversarial RL (RARL) was previously proposed to train an adversarial network that applies disturbances to a system, which improves the robustness in test scenarios. However, an issue of neural network-based adversaries is that integrating system requirements without handcrafting sophisticated reward signals are difficult. Safety falsification methods allow one to find a set of initial conditions and an input sequence, such that the system violates a given property formulated in temporal logic. In this paper, we propose falsification-based RARL (FRARL): this is the first generic framework for integrating temporal logic falsification in adversarial learning to improve policy robustness. By applyin",
    "link": "http://arxiv.org/abs/2007.00691",
    "context": "Title: Falsification-Based Robust Adversarial Reinforcement Learning. (arXiv:2007.00691v3 [cs.RO] UPDATED)\nAbstract: Reinforcement learning (RL) has achieved enormous progress in solving various sequential decision-making problems, such as control tasks in robotics. Since policies are overfitted to training environments, RL methods have often failed to be generalized to safety-critical test scenarios. Robust adversarial RL (RARL) was previously proposed to train an adversarial network that applies disturbances to a system, which improves the robustness in test scenarios. However, an issue of neural network-based adversaries is that integrating system requirements without handcrafting sophisticated reward signals are difficult. Safety falsification methods allow one to find a set of initial conditions and an input sequence, such that the system violates a given property formulated in temporal logic. In this paper, we propose falsification-based RARL (FRARL): this is the first generic framework for integrating temporal logic falsification in adversarial learning to improve policy robustness. By applyin",
    "path": "papers/20/07/2007.00691.json",
    "total_tokens": 834,
    "translated_title": "基于虚假测试的强化学习的鲁棒性对抗方法",
    "translated_abstract": "强化学习已经在各种顺序决策问题中取得了巨大进展，如机器人控制任务。由于策略对培训环境进行了过度拟合，强化学习方法通常无法推广到安全关键的测试场景中。我们提出了基于虚假测试的鲁棒性对抗学习 (FRARL) 框架，通过虚假测试方法在对抗学习中整合时间逻辑，从而提高策略的鲁棒性。我们在不同形态和任务的三个机器人系统上展示了我们方法的有效性。",
    "tldr": "本论文提出了基于虚假测试的鲁棒性对抗学习框架，通过使用虚假测试方法提高了强化学习算法在测试场景下的准确性和鲁棒性，得到了显著的实验结果。",
    "en_tdlr": "This paper proposes a falsification-based robust adversarial reinforcement learning framework that integrates temporal logic to improve the robustness of reinforcement learning algorithms in test scenarios. Significant experimental results are obtained by using falsification methods to enhance the accuracy and robustness of the policy."
}