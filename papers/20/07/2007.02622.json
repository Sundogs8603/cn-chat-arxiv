{
    "title": "Integrating Distributed Architectures in Highly Modular RL Libraries. (arXiv:2007.02622v3 [cs.CV] UPDATED)",
    "abstract": "Advancing reinforcement learning (RL) requires tools that are flexible enough to easily prototype new methods while avoiding impractically slow experimental turnaround times. To match the first requirement, the most popular RL libraries advocate for highly modular agent composability, which facilitates experimentation and development. To solve challenging environments within reasonable time frames, scaling RL to large sampling and computing resources has proved a successful strategy. However, this capability has been so far difficult to combine with modularity. In this work, we explore design choices to allow agent composability both at a local and distributed level of execution. We propose a versatile approach that allows the definition of RL agents at different scales through independent reusable components. We demonstrate experimentally that our design choices allow us to reproduce classical benchmarks, explore multiple distributed architectures, and solve novel and complex environm",
    "link": "http://arxiv.org/abs/2007.02622",
    "context": "Title: Integrating Distributed Architectures in Highly Modular RL Libraries. (arXiv:2007.02622v3 [cs.CV] UPDATED)\nAbstract: Advancing reinforcement learning (RL) requires tools that are flexible enough to easily prototype new methods while avoiding impractically slow experimental turnaround times. To match the first requirement, the most popular RL libraries advocate for highly modular agent composability, which facilitates experimentation and development. To solve challenging environments within reasonable time frames, scaling RL to large sampling and computing resources has proved a successful strategy. However, this capability has been so far difficult to combine with modularity. In this work, we explore design choices to allow agent composability both at a local and distributed level of execution. We propose a versatile approach that allows the definition of RL agents at different scales through independent reusable components. We demonstrate experimentally that our design choices allow us to reproduce classical benchmarks, explore multiple distributed architectures, and solve novel and complex environm",
    "path": "papers/20/07/2007.02622.json",
    "total_tokens": 918,
    "translated_title": "高度模块化强化学习库中分布式架构的集成",
    "translated_abstract": "推进强化学习（RL）需要具有足够灵活性的工具，以便轻松地原型化新方法，同时避免不切实际的实验周转时间。为了匹配第一个要求，最流行的RL库倡导高度模块化的代理组合性，这有助于实验和开发。为了在合理的时间范围内解决具有挑战性的环境，将RL扩展到大规模采样和计算资源已被证明是一种成功的策略。然而，这种能力迄今为止很难与模块化相结合。在这项工作中，我们探索了设计选择，以允许在本地和分布式执行级别上实现代理组合性。我们提出了一种通用方法，通过独立的可重用组件允许在不同尺度上定义RL代理。我们通过实验证明，我们的设计选择使我们能够复制经典基准测试，探索多个分布式架构，并解决新颖和复杂的环境，具有最先进的性能。",
    "tldr": "该研究探讨了在本地和分布式执行级别上实现代理组合性的设计选择，通过独立的可重用组件允许在不同尺度上定义RL代理，并成功解决了新颖和复杂的环境，具有最先进的性能。",
    "en_tdlr": "This study explores design choices to allow agent composability both at a local and distributed level of execution, proposing a versatile approach that allows the definition of RL agents at different scales through independent reusable components. It successfully solved novel and complex environments with state-of-the-art performance."
}