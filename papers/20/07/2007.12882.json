{
    "title": "A finite sample analysis of the benign overfitting phenomenon for ridge function estimation. (arXiv:2007.12882v5 [stat.ML] UPDATED)",
    "abstract": "Recent extensive numerical experiments in high scale machine learning have allowed to uncover a quite counterintuitive phase transition, as a function of the ratio between the sample size and the number of parameters in the model. As the number of parameters $p$ approaches the sample size $n$, the generalisation error increases, but surprisingly, it starts decreasing again past the threshold $p=n$. This phenomenon, brought to the theoretical community attention in \\cite{belkin2019reconciling}, has been thoroughly investigated lately, more specifically for simpler models than deep neural networks, such as the linear model when the parameter is taken to be the minimum norm solution to the least-squares problem, firstly in the asymptotic regime when $p$ and $n$ tend to infinity, see e.g. \\cite{hastie2019surprises}, and recently in the finite dimensional regime and more specifically for linear models \\cite{bartlett2020benign}, \\cite{tsigler2020benign}, \\cite{lecue2022geometrical}. In the p",
    "link": "http://arxiv.org/abs/2007.12882",
    "context": "Title: A finite sample analysis of the benign overfitting phenomenon for ridge function estimation. (arXiv:2007.12882v5 [stat.ML] UPDATED)\nAbstract: Recent extensive numerical experiments in high scale machine learning have allowed to uncover a quite counterintuitive phase transition, as a function of the ratio between the sample size and the number of parameters in the model. As the number of parameters $p$ approaches the sample size $n$, the generalisation error increases, but surprisingly, it starts decreasing again past the threshold $p=n$. This phenomenon, brought to the theoretical community attention in \\cite{belkin2019reconciling}, has been thoroughly investigated lately, more specifically for simpler models than deep neural networks, such as the linear model when the parameter is taken to be the minimum norm solution to the least-squares problem, firstly in the asymptotic regime when $p$ and $n$ tend to infinity, see e.g. \\cite{hastie2019surprises}, and recently in the finite dimensional regime and more specifically for linear models \\cite{bartlett2020benign}, \\cite{tsigler2020benign}, \\cite{lecue2022geometrical}. In the p",
    "path": "papers/20/07/2007.12882.json",
    "total_tokens": 877,
    "translated_title": "岭函数估计中良性过拟合现象的有限样本分析",
    "translated_abstract": "最近在大规模机器学习中进行的广泛数值实验揭示了一个相当反直觉的相变现象，即样本大小与模型参数数量之比的函数关系。当参数数量$p$接近样本大小$n$时，泛化误差增加，但令人惊讶的是，当$p>n$时它再次开始减小。这一现象在\\cite{belkin2019reconciling}中引起了理论界的关注，最近已经进行了深入研究，特别是针对比深度神经网络更简单的模型，例如线性模型中参数取最小范数解的情况。首先在当$p$和$n$趋于无穷大的渐近情况下进行研究（参见\\cite{hastie2019surprises}），然后在有限维情况下更具体地针对线性模型进行研究（参见\\cite{bartlett2020benign}，\\cite{tsigler2020benign}，\\cite{lecue2022geometrical}）。",
    "tldr": "本论文研究了岭函数估计中的良性过拟合现象，并在有限维度情况下对线性模型进行了分析。"
}