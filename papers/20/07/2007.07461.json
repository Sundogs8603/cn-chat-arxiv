{
    "title": "Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity. (arXiv:2007.07461v3 [cs.LG] UPDATED)",
    "abstract": "Model-based reinforcement learning (RL), which finds an optimal policy using an empirical model, has long been recognized as one of the corner stones of RL. It is especially suitable for multi-agent RL (MARL), as it naturally decouples the learning and the planning phases, and avoids the non-stationarity problem when all agents are improving their policies simultaneously using samples. Though intuitive and widely-used, the sample complexity of model-based MARL algorithms has not been fully investigated. In this paper, our goal is to address the fundamental question about its sample complexity. We study arguably the most basic MARL setting: two-player discounted zero-sum Markov games, given only access to a generative model. We show that model-based MARL achieves a sample complexity of $\\tilde O(|S||A||B|(1-\\gamma)^{-3}\\epsilon^{-2})$ for finding the Nash equilibrium (NE) value up to some $\\epsilon$ error, and the $\\epsilon$-NE policies with a smooth planning oracle, where $\\gamma$ is t",
    "link": "http://arxiv.org/abs/2007.07461",
    "context": "Title: Model-Based Multi-Agent RL in Zero-Sum Markov Games with Near-Optimal Sample Complexity. (arXiv:2007.07461v3 [cs.LG] UPDATED)\nAbstract: Model-based reinforcement learning (RL), which finds an optimal policy using an empirical model, has long been recognized as one of the corner stones of RL. It is especially suitable for multi-agent RL (MARL), as it naturally decouples the learning and the planning phases, and avoids the non-stationarity problem when all agents are improving their policies simultaneously using samples. Though intuitive and widely-used, the sample complexity of model-based MARL algorithms has not been fully investigated. In this paper, our goal is to address the fundamental question about its sample complexity. We study arguably the most basic MARL setting: two-player discounted zero-sum Markov games, given only access to a generative model. We show that model-based MARL achieves a sample complexity of $\\tilde O(|S||A||B|(1-\\gamma)^{-3}\\epsilon^{-2})$ for finding the Nash equilibrium (NE) value up to some $\\epsilon$ error, and the $\\epsilon$-NE policies with a smooth planning oracle, where $\\gamma$ is t",
    "path": "papers/20/07/2007.07461.json",
    "total_tokens": 998,
    "translated_title": "基于模型的多智能体强化学习在零和马尔可夫博弈中具有接近最优样本复杂度",
    "translated_abstract": "基于模型的强化学习（RL）已经被认为是RL的基石之一，通过使用经验模型找到最优策略。它特别适用于多智能体RL（MARL），因为它自然地将学习和规划阶段解耦，并避免了在所有智能体同时使用样本改进策略时的非稳态问题。尽管直观且广泛使用，但基于模型的MARL算法的样本复杂度尚未得到全面研究。本文的目标是解决关于其样本复杂度的基本问题。我们研究了可能是最基本的MARL设置：只能访问一个生成模型的两人折扣零和马尔可夫博弈。我们证明了基于模型的MARL在寻找到某个ε误差的纳什均衡（NE）值以及具有平滑规划预言机的ε-NE策略方面达到了样本复杂度为$\\tilde O(|S||A||B|(1-\\gamma)^{-3}\\epsilon^{-2})$的结果，其中γ是。",
    "tldr": "本文研究了基于模型的多智能体强化学习在零和马尔可夫博弈中的样本复杂度问题，并证明了其在找到纳什均衡值及具有平滑规划预言机的ε-NE策略方面具有接近最优的样本复杂度。"
}