{
    "title": "Universal Approximation Power of Deep Residual Neural Networks via Nonlinear Control Theory",
    "abstract": "In this paper, we explain the universal approximation capabilities of deep residual neural networks through geometric nonlinear control. Inspired by recent work establishing links between residual networks and control systems, we provide a general sufficient condition for a residual network to have the power of universal approximation by asking the activation function, or one of its derivatives, to satisfy a quadratic differential equation. Many activation functions used in practice satisfy this assumption, exactly or approximately, and we show this property to be sufficient for an adequately deep neural network with $n+1$ neurons per layer to approximate arbitrarily well, on a compact set and with respect to the supremum norm, any continuous function from $\\mathbb{R}^n$ to $\\mathbb{R}^n$. We further show this result to hold for very simple architectures for which the weights only need to assume two values. The first key technical contribution consists of relating the universal approxi",
    "link": "https://arxiv.org/abs/2007.06007",
    "context": "Title: Universal Approximation Power of Deep Residual Neural Networks via Nonlinear Control Theory\nAbstract: In this paper, we explain the universal approximation capabilities of deep residual neural networks through geometric nonlinear control. Inspired by recent work establishing links between residual networks and control systems, we provide a general sufficient condition for a residual network to have the power of universal approximation by asking the activation function, or one of its derivatives, to satisfy a quadratic differential equation. Many activation functions used in practice satisfy this assumption, exactly or approximately, and we show this property to be sufficient for an adequately deep neural network with $n+1$ neurons per layer to approximate arbitrarily well, on a compact set and with respect to the supremum norm, any continuous function from $\\mathbb{R}^n$ to $\\mathbb{R}^n$. We further show this result to hold for very simple architectures for which the weights only need to assume two values. The first key technical contribution consists of relating the universal approxi",
    "path": "papers/20/07/2007.06007.json",
    "total_tokens": 907,
    "translated_title": "深度残差神经网络通过非线性控制理论实现通用逼近能力",
    "translated_abstract": "本文通过几何非线性控制来解释深度残差神经网络的通用逼近能力。受到最近建立残差网络和控制系统之间联系的工作的启发，我们提供了一个一般的充分条件，要求激活函数或其导数之一满足一个二次微分方程，以使残差网络具有通用逼近能力。在实践中使用的许多激活函数满足这个假设，我们证明这个属性足以让一个足够深的具有$n+1$神经元每层的神经网络，在紧集合上相对于最大范数逼近任意连续的从$\\mathbb{R}^n$到$\\mathbb{R}^n$的函数。我们进一步展示了这个结果适用于非常简单的架构，只需要权重取两个值。第一个关键技术贡献是将通用逼近与残差神经网络的几何非线性控制相联系。",
    "tldr": "本文通过非线性控制理论解释了深度残差神经网络的通用逼近能力，并提供了一个充分条件，在激活函数满足二次微分方程的情况下，一个足够深的神经网络能够在紧集合上逼近任意连续函数。",
    "en_tdlr": "This paper explains the universal approximation capabilities of deep residual neural networks through nonlinear control theory. By establishing a sufficient condition based on a quadratic differential equation for the activation function, the paper shows that a sufficiently deep neural network can approximate any continuous function within a compact set."
}