{
    "title": "ResNet After All? Neural ODEs and Their Numerical Solution. (arXiv:2007.15386v2 [cs.LG] UPDATED)",
    "abstract": "A key appeal of the recently proposed Neural Ordinary Differential Equation (ODE) framework is that it seems to provide a continuous-time extension of discrete residual neural networks. As we show herein, though, trained Neural ODE models actually depend on the specific numerical method used during training. If the trained model is supposed to be a flow generated from an ODE, it should be possible to choose another numerical solver with equal or smaller numerical error without loss of performance. We observe that if training relies on a solver with overly coarse discretization, then testing with another solver of equal or smaller numerical error results in a sharp drop in accuracy. In such cases, the combination of vector field and numerical method cannot be interpreted as a flow generated from an ODE, which arguably poses a fatal breakdown of the Neural ODE concept. We observe, however, that there exists a critical step size beyond which the training yields a valid ODE vector field. W",
    "link": "http://arxiv.org/abs/2007.15386",
    "context": "Title: ResNet After All? Neural ODEs and Their Numerical Solution. (arXiv:2007.15386v2 [cs.LG] UPDATED)\nAbstract: A key appeal of the recently proposed Neural Ordinary Differential Equation (ODE) framework is that it seems to provide a continuous-time extension of discrete residual neural networks. As we show herein, though, trained Neural ODE models actually depend on the specific numerical method used during training. If the trained model is supposed to be a flow generated from an ODE, it should be possible to choose another numerical solver with equal or smaller numerical error without loss of performance. We observe that if training relies on a solver with overly coarse discretization, then testing with another solver of equal or smaller numerical error results in a sharp drop in accuracy. In such cases, the combination of vector field and numerical method cannot be interpreted as a flow generated from an ODE, which arguably poses a fatal breakdown of the Neural ODE concept. We observe, however, that there exists a critical step size beyond which the training yields a valid ODE vector field. W",
    "path": "papers/20/07/2007.15386.json",
    "total_tokens": 908,
    "translated_title": "究竟是ResNet？神经ODE及其数值解",
    "translated_abstract": "最近提出的神经常微分方程(ODE)框架具有连续时间扩展离散残差神经网络的特点。但是，我们在这里展示，训练的神经ODE模型实际上取决于训练过程中使用的特定数值方法。如果训练出的模型被认为是从ODE生成的流动，那么可以选择另一个数值解算器，其数值误差大小相同或更小，而不会损失性能。我们观察到，如果训练依赖于过于粗糙的离散化解算器，则使用另一个数值误差相等或更小的解算器进行测试会导致准确性急剧下降。在这种情况下，向量场和数值方法的组合不能被解释为从ODE生成的流动，这可以被认为是神经ODE概念的致命断裂。然而，我们观察到存在一个临界步长，超过该步长，训练会产生一个有效的ODE向量场。",
    "tldr": "神经ODE模型的性能取决于训练过程中使用的数值方法，如果使用过于粗糙的解算器进行训练，则使用另一个数值误差相等或更小的解算器进行测试会导致准确性下降。",
    "en_tdlr": "The performance of Neural ODE models depends on the numerical method used during training. If a coarse solver is used, testing with another solver with equal or smaller numerical error results in a drop in accuracy. This poses a fatal breakdown of the Neural ODE concept."
}