{
    "title": "Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis. (arXiv:2010.13272v4 [cs.LG] UPDATED)",
    "abstract": "Variance reduction techniques have been successfully applied to temporal-difference (TD) learning and help to improve the sample complexity in policy evaluation. However, the existing work applied variance reduction to either the less popular one time-scale TD algorithm or the two time-scale GTD algorithm but with a finite number of i.i.d.\\ samples, and both algorithms apply to only the on-policy setting. In this work, we develop a variance reduction scheme for the two time-scale TDC algorithm in the off-policy setting and analyze its non-asymptotic convergence rate over both i.i.d.\\ and Markovian samples. In the i.i.d.\\ setting, our algorithm {matches the best-known lower bound $\\tilde{O}(\\epsilon^{-1}$).} In the Markovian setting, our algorithm achieves the state-of-the-art sample complexity $O(\\epsilon^{-1} \\log {\\epsilon}^{-1})$ that is near-optimal. Experiments demonstrate that the proposed variance-reduced TDC achieves a smaller asymptotic convergence error than both the conventi",
    "link": "http://arxiv.org/abs/2010.13272",
    "context": "Title: Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis. (arXiv:2010.13272v4 [cs.LG] UPDATED)\nAbstract: Variance reduction techniques have been successfully applied to temporal-difference (TD) learning and help to improve the sample complexity in policy evaluation. However, the existing work applied variance reduction to either the less popular one time-scale TD algorithm or the two time-scale GTD algorithm but with a finite number of i.i.d.\\ samples, and both algorithms apply to only the on-policy setting. In this work, we develop a variance reduction scheme for the two time-scale TDC algorithm in the off-policy setting and analyze its non-asymptotic convergence rate over both i.i.d.\\ and Markovian samples. In the i.i.d.\\ setting, our algorithm {matches the best-known lower bound $\\tilde{O}(\\epsilon^{-1}$).} In the Markovian setting, our algorithm achieves the state-of-the-art sample complexity $O(\\epsilon^{-1} \\log {\\epsilon}^{-1})$ that is near-optimal. Experiments demonstrate that the proposed variance-reduced TDC achieves a smaller asymptotic convergence error than both the conventi",
    "path": "papers/20/10/2010.13272.json",
    "total_tokens": 1063,
    "translated_title": "方差缩减的离策略TDC学习: 非渐进收敛分析",
    "translated_abstract": "方差缩减技术已成功应用于时间差分（TD）学习，并有助于提高策略评估的样本复杂度。然而，现有的工作要么将方差缩减应用于较不流行的单时间尺度TD算法，要么将其应用于两个时间尺度的GTD算法，但仅使用有限数目的i.i.d.样本，并且两种算法仅适用于在线策略设置。在这项工作中，我们为离策略设置中的两个时间尺度TDC算法开发了方差缩减方案，并分析了它在i.i.d.和马尔可夫抽样中的非渐进收敛率。在i.i.d.设置中，我们的算法与已知最佳下限相匹配$\\tilde{O}(\\epsilon^{-1}$)。在马尔可夫设置中，我们的算法实现了近乎最优的样本复杂度$O(\\epsilon^{-1} \\log {\\epsilon}^{-1})$。实验表明，所提出的方差缩减TDC算法的渐近收敛误差比传统的TDC和方差缩减GTD算法都小。",
    "tldr": "本文提出了离策略TDC算法的方差缩减方案，并在i.i.d.和马尔可夫抽样中分析了其收敛率，结果表明该算法实现了与已知最佳下限相匹配的i.i.d.样本复杂度和接近最优的马尔可夫样本复杂度。",
    "en_tdlr": "The paper proposes a variance reduction scheme for the two time-scale TDC algorithm in the off-policy setting and analyzes its non-asymptotic convergence rate over both i.i.d. and Markovian samples. The algorithm achieves a matching lower bound for i.i.d. samples and near-optimal sample complexity for Markovian samples. Experiments show that it outperforms conventional TDC and variance-reduced GTD algorithms."
}