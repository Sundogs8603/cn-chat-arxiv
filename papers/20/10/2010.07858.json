{
    "title": "Exploring Flip Flop memories and beyond: training recurrent neural networks with key insights. (arXiv:2010.07858v4 [cs.LG] UPDATED)",
    "abstract": "Training neural networks to perform different tasks is relevant across various disciplines. In particular, Recurrent Neural Networks (RNNs) are of great interest in Computational Neuroscience. Open-source frameworks dedicated to Machine Learning, such as Tensorflow and Keras have produced significant changes in the development of technologies that we currently use. This work aims to make a significant contribution by comprehensively investigating and describing the implementation of a temporal processing task, specifically a 3-bit Flip Flop memory. We delve into the entire modelling process, encompassing equations, task parametrization, and software development. The obtained networks are meticulously analyzed to elucidate dynamics, aided by an array of visualization and analysis tools. Moreover, the provided code is versatile enough to facilitate the modelling of diverse tasks and systems. Furthermore, we present how memory states can be efficiently stored in the vertices of a cube in ",
    "link": "http://arxiv.org/abs/2010.07858",
    "context": "Title: Exploring Flip Flop memories and beyond: training recurrent neural networks with key insights. (arXiv:2010.07858v4 [cs.LG] UPDATED)\nAbstract: Training neural networks to perform different tasks is relevant across various disciplines. In particular, Recurrent Neural Networks (RNNs) are of great interest in Computational Neuroscience. Open-source frameworks dedicated to Machine Learning, such as Tensorflow and Keras have produced significant changes in the development of technologies that we currently use. This work aims to make a significant contribution by comprehensively investigating and describing the implementation of a temporal processing task, specifically a 3-bit Flip Flop memory. We delve into the entire modelling process, encompassing equations, task parametrization, and software development. The obtained networks are meticulously analyzed to elucidate dynamics, aided by an array of visualization and analysis tools. Moreover, the provided code is versatile enough to facilitate the modelling of diverse tasks and systems. Furthermore, we present how memory states can be efficiently stored in the vertices of a cube in ",
    "path": "papers/20/10/2010.07858.json",
    "total_tokens": 857,
    "translated_title": "探索锁存器存储器及其他：利用关键见解训练循环神经网络",
    "translated_abstract": "在不同领域中，训练神经网络以执行不同任务是非常重要的。具体而言，循环神经网络(RNNs)在计算神经科学中非常有趣。机器学习的开源框架，如Tensorflow和Keras，对我们目前使用的技术的发展产生了重大影响。本研究旨在通过全面调查和描述时间处理任务（特别是3位锁存器存储器）的实现，做出重大贡献。我们深入探讨了整个建模过程，包括方程、任务参数化和软件开发。通过一系列的可视化和分析工具，精心分析了所获得的网络以阐明其动态特性。此外，所提供的代码具有足够的灵活性，可以促进对不同任务和系统的建模。此外，我们还介绍了如何将存储器状态有效地存储在一个立方体的顶点中。",
    "tldr": "本研究通过全面调查和描述时间处理任务的实现，特别是3位锁存器存储器，为循环神经网络的训练方法提供了关键见解和贡献。",
    "en_tdlr": "This study provides key insights and contributions to the training of recurrent neural networks by comprehensively investigating and describing the implementation of a temporal processing task, particularly a 3-bit Flip Flop memory."
}