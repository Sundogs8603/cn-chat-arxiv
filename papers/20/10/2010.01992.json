{
    "title": "Improving Few-Shot Learning through Multi-task Representation Learning Theory. (arXiv:2010.01992v3 [cs.LG] CROSS LISTED)",
    "abstract": "In this paper, we consider the framework of multi-task representation (MTR) learning where the goal is to use source tasks to learn a representation that reduces the sample complexity of solving a target task. We start by reviewing recent advances in MTR theory and show that they can provide novel insights for popular meta-learning algorithms when analyzed within this framework. In particular, we highlight a fundamental difference between gradient-based and metric-based algorithms in practice and put forward a theoretical analysis to explain it. Finally, we use the derived insights to improve the performance of meta-learning methods via a new spectral-based regularization term and confirm its efficiency through experimental studies on few-shot classification benchmarks. To the best of our knowledge, this is the first contribution that puts the most recent learning bounds of MTR theory into practice for the task of few-shot classification.",
    "link": "http://arxiv.org/abs/2010.01992",
    "context": "Title: Improving Few-Shot Learning through Multi-task Representation Learning Theory. (arXiv:2010.01992v3 [cs.LG] CROSS LISTED)\nAbstract: In this paper, we consider the framework of multi-task representation (MTR) learning where the goal is to use source tasks to learn a representation that reduces the sample complexity of solving a target task. We start by reviewing recent advances in MTR theory and show that they can provide novel insights for popular meta-learning algorithms when analyzed within this framework. In particular, we highlight a fundamental difference between gradient-based and metric-based algorithms in practice and put forward a theoretical analysis to explain it. Finally, we use the derived insights to improve the performance of meta-learning methods via a new spectral-based regularization term and confirm its efficiency through experimental studies on few-shot classification benchmarks. To the best of our knowledge, this is the first contribution that puts the most recent learning bounds of MTR theory into practice for the task of few-shot classification.",
    "path": "papers/20/10/2010.01992.json",
    "total_tokens": 780,
    "translated_title": "通过多任务表示学习理论改进少样本学习",
    "translated_abstract": "本文考虑多任务表示（MTR）学习的框架，目标是利用源任务来学习一个表示，减少解决目标任务的样本复杂性。我们首先回顾了MTR理论的最新进展，并展示了在该框架内对流行的元学习算法进行分析可以提供新的见解。特别地，我们强调了梯度优化和度量优化算法在实践中的根本区别，并提出了一个理论分析来解释它。最后，我们利用得到的见解通过一种新的基于谱的正则化项来提高元学习方法的性能，并通过在少样本分类基准上的实验研究验证了其有效性。据我们所知，这是将MTR理论的最新学习界限应用于少样本分类任务的首次贡献。",
    "tldr": "本文通过多任务表示学习理论的最新进展，提出了一个新的基于谱的正则化项来改进少样本学习，并在实验证明了其有效性。",
    "en_tdlr": "This paper improves few-shot learning through the latest advancements in multi-task representation learning theory, by introducing a new spectral-based regularization term and demonstrating its efficiency through experimental studies."
}