{
    "title": "Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU",
    "abstract": "arXiv:2010.05330v2 Announce Type: replace  Abstract: While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The \"omni-directional\" BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncat",
    "link": "https://arxiv.org/abs/2010.05330",
    "context": "Title: Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU\nAbstract: arXiv:2010.05330v2 Announce Type: replace  Abstract: While humans process language incrementally, the best language encoders currently used in NLP do not. Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers). We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems. We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics. The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality. The \"omni-directional\" BERT model, which achieves better non-incremental performance, is impacted more by the incremental access. This can be alleviated by adapting the training regime (truncat",
    "path": "papers/20/10/2010.05330.json",
    "total_tokens": 814,
    "translated_title": "增量处理在非增量编码器时代：增量NLU的双向模型的经验评估",
    "translated_abstract": "虽然人类以增量方式处理语言，但目前在自然语言处理中使用的最佳语言编码器不是增量的。我们研究了当必须基于到达某个时间步的部分输入提供部分输出时，即在交互式系统中可能发生的情况下，双向LSTMs和Transformers在增量界面下的行为。我们在各种NLU数据集上测试了五种模型，并使用三种增量评估指标比较它们的性能。结果支持使用双向编码器以增量模式，并保留大部分非增量质量的可能性。在增量访问方面，表现更好的非增量性能“全向”BERT模型受到更大影响。通过调整训练机制可以缓解这一问题。",
    "tldr": "双向模型在增量界面下的表现得到了支持，而全向BERT模型在增量访问方面受到较大影响，可通过调整训练机制缓解。",
    "en_tdlr": "Bidirectional models perform well under incremental interfaces, while the omni-directional BERT model is more affected by incremental access and can be alleviated by adapting the training regime."
}