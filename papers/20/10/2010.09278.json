{
    "title": "MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch Normalization. (arXiv:2010.09278v3 [cs.LG] UPDATED)",
    "abstract": "Substantial experiments have validated the success of Batch Normalization (BN) Layer in benefiting convergence and generalization. However, BN requires extra memory and float-point calculation. Moreover, BN would be inaccurate on micro-batch, as it depends on batch statistics. In this paper, we address these problems by simplifying BN regularization while keeping two fundamental impacts of BN layers, i.e., data decorrelation and adaptive learning rate. We propose a novel normalization method, named MimicNorm, to improve the convergence and efficiency in network training. MimicNorm consists of only two light operations, including modified weight mean operations (subtract mean values from weight parameter tensor) and one BN layer before loss function (last BN layer). We leverage the neural tangent kernel (NTK) theory to prove that our weight mean operation whitens activations and transits network into the chaotic regime like BN layer, and consequently, leads to an enhanced convergence. T",
    "link": "http://arxiv.org/abs/2010.09278",
    "context": "Title: MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch Normalization. (arXiv:2010.09278v3 [cs.LG] UPDATED)\nAbstract: Substantial experiments have validated the success of Batch Normalization (BN) Layer in benefiting convergence and generalization. However, BN requires extra memory and float-point calculation. Moreover, BN would be inaccurate on micro-batch, as it depends on batch statistics. In this paper, we address these problems by simplifying BN regularization while keeping two fundamental impacts of BN layers, i.e., data decorrelation and adaptive learning rate. We propose a novel normalization method, named MimicNorm, to improve the convergence and efficiency in network training. MimicNorm consists of only two light operations, including modified weight mean operations (subtract mean values from weight parameter tensor) and one BN layer before loss function (last BN layer). We leverage the neural tangent kernel (NTK) theory to prove that our weight mean operation whitens activations and transits network into the chaotic regime like BN layer, and consequently, leads to an enhanced convergence. T",
    "path": "papers/20/10/2010.09278.json",
    "total_tokens": 968,
    "translated_title": "MimicNorm: 权重均值和最后一层批归一化层模仿批归一化的动态",
    "translated_abstract": "大量实验证实了批归一化（BN）层在收敛和泛化效果上的成功。然而，BN需要额外的内存和浮点计算。此外，在微小批次上，BN会变得不准确，因为它依赖于批次统计信息。在本文中，我们通过简化BN的正则化方法来解决这些问题，同时保持BN层的两个基本影响，即数据去相关性和自适应学习率。我们提出了一种新的归一化方法，称为MimicNorm，来改善网络训练中的收敛性和效率。 MimicNorm仅包含两个轻量级操作，包括修改的权重均值操作（从权重参数张量中减去均值值）和损失函数（最后的BN层）之前的一个BN层。我们利用神经切线核（NTK）理论证明了我们的权重均值操作可以白化激活，使网络转化为类似BN层的混沌状态，从而导致了收敛性的提升。",
    "tldr": "本文提出了一种名为MimicNorm的归一化方法，通过简化批归一化（BN）的正则化方法并保持其核心影响，即数据去相关性和自适应学习率，来提高网络训练的收敛性和效率。MimicNorm仅包含两个轻量级操作，可与BN相媲美。",
    "en_tdlr": "This paper proposes a normalization method called MimicNorm, which improves the convergence and efficiency in network training by simplifying the regularization of Batch Normalization (BN) while preserving its core impacts, namely data decorrelation and adaptive learning rate. MimicNorm consists of only two light operations and can be comparable to BN."
}