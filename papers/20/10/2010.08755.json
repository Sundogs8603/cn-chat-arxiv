{
    "title": "Variational Dynamic for Self-Supervised Exploration in Deep Reinforcement Learning",
    "abstract": "arXiv:2010.08755v3 Announce Type: replace  Abstract: Efficient exploration remains a challenging problem in reinforcement learning, especially for tasks where extrinsic rewards from environments are sparse or even totally disregarded. Significant advances based on intrinsic motivation show promising results in simple environments but often get stuck in environments with multimodal and stochastic dynamics. In this work, we propose a variational dynamic model based on the conditional variational inference to model the multimodality and stochasticity. We consider the environmental state-action transition as a conditional generative process by generating the next-state prediction under the condition of the current state, action, and latent variable, which provides a better understanding of the dynamics and leads a better performance in exploration. We derive an upper bound of the negative log-likelihood of the environmental transition and use such an upper bound as the intrinsic reward for",
    "link": "https://arxiv.org/abs/2010.08755",
    "context": "Title: Variational Dynamic for Self-Supervised Exploration in Deep Reinforcement Learning\nAbstract: arXiv:2010.08755v3 Announce Type: replace  Abstract: Efficient exploration remains a challenging problem in reinforcement learning, especially for tasks where extrinsic rewards from environments are sparse or even totally disregarded. Significant advances based on intrinsic motivation show promising results in simple environments but often get stuck in environments with multimodal and stochastic dynamics. In this work, we propose a variational dynamic model based on the conditional variational inference to model the multimodality and stochasticity. We consider the environmental state-action transition as a conditional generative process by generating the next-state prediction under the condition of the current state, action, and latent variable, which provides a better understanding of the dynamics and leads a better performance in exploration. We derive an upper bound of the negative log-likelihood of the environmental transition and use such an upper bound as the intrinsic reward for",
    "path": "papers/20/10/2010.08755.json",
    "total_tokens": 805,
    "translated_title": "自监督探索中的变分动力学在深度强化学习中的应用",
    "translated_abstract": "强化学习中的有效探索仍然是一个具有挑战性的问题，特别是对于那些外部奖励稀疏甚至完全被忽视的任务。基于内在动机的显著进展在简单环境中表现出有希望的结果，但在具有多模态和随机动态的环境中经常陷入困境。在这项工作中，我们提出了一种基于条件变分推理的变分动态模型，以建模多模态性和随机性。我们将环境状态-动作转换视为一种条件生成过程，通过在当前状态、动作和潜变量条件下生成下一个状态的预测，从而更好地理解动态并在探索中取得更好的性能。我们推导了环境转换的负对数似然的上界，并将这样的上界用作内在奖励。",
    "tldr": "该论文提出了一种基于条件变分推理的变分动态模型，用于在深度强化学习中解决自监督探索中的多模态性和随机性问题。",
    "en_tdlr": "This paper proposes a variational dynamic model based on conditional variational inference to address the multimodality and stochasticity in self-supervised exploration in deep reinforcement learning."
}