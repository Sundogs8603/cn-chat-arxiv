{
    "title": "An Analysis of Robustness of Non-Lipschitz Networks. (arXiv:2010.06154v4 [cs.LG] UPDATED)",
    "abstract": "Despite significant advances, deep networks remain highly susceptible to adversarial attack. One fundamental challenge is that small input perturbations can often produce large movements in the network's final-layer feature space. In this paper, we define an attack model that abstracts this challenge, to help understand its intrinsic properties. In our model, the adversary may move data an arbitrary distance in feature space but only in random low-dimensional subspaces. We prove such adversaries can be quite powerful: defeating any algorithm that must classify any input it is given. However, by allowing the algorithm to abstain on unusual inputs, we show such adversaries can be overcome when classes are reasonably well-separated in feature space. We further provide strong theoretical guarantees for setting algorithm parameters to optimize over accuracy-abstention trade-offs using data-driven methods. Our results provide new robustness guarantees for nearest-neighbor style algorithms, a",
    "link": "http://arxiv.org/abs/2010.06154",
    "context": "Title: An Analysis of Robustness of Non-Lipschitz Networks. (arXiv:2010.06154v4 [cs.LG] UPDATED)\nAbstract: Despite significant advances, deep networks remain highly susceptible to adversarial attack. One fundamental challenge is that small input perturbations can often produce large movements in the network's final-layer feature space. In this paper, we define an attack model that abstracts this challenge, to help understand its intrinsic properties. In our model, the adversary may move data an arbitrary distance in feature space but only in random low-dimensional subspaces. We prove such adversaries can be quite powerful: defeating any algorithm that must classify any input it is given. However, by allowing the algorithm to abstain on unusual inputs, we show such adversaries can be overcome when classes are reasonably well-separated in feature space. We further provide strong theoretical guarantees for setting algorithm parameters to optimize over accuracy-abstention trade-offs using data-driven methods. Our results provide new robustness guarantees for nearest-neighbor style algorithms, a",
    "path": "papers/20/10/2010.06154.json",
    "total_tokens": 1003,
    "translated_title": "非利普希茨网络的鲁棒性分析",
    "translated_abstract": "尽管已经取得了显著进展，但深度网络仍然极易受到对抗攻击的影响。其中一个根本性的挑战是：即使输入略微扰动，也可能会产生网络最终层特征空间中的大幅移动。在本文中，我们定义了一个攻击模型来抽象这个挑战，以帮助理解它的内在属性。在我们的模型中，对手可以在特征空间中的任意距离上移动数据，但只能在随机的低维子空间内进行操作。我们证明了这种攻击者可以非常强大：它们可以战胜任何必须对其收到的所有输入进行分类的算法。然而，通过允许算法放弃处理不寻常的输入，我们表明当类在特征空间中相对分离得很好时，这种攻击者是可以被克服的。我们进一步提供了强有力的理论保证，以使用数据驱动方法设置算法参数以优化精度-放弃权衡。我们的结果为最近邻算法提供了新的鲁棒性保证。",
    "tldr": "本文研究了深度非利普希茨网络的鲁棒性问题，定义了一个攻击模型帮助理解内在属性，证明了此类攻击者可以战胜所有必须对其输入进行分类的算法，但也提出了克服此类攻击者的方法，进一步提供了理论保证并为最近邻算法提供了新的鲁棒性保证。",
    "en_tdlr": "This paper analyzes the robustness problems of non-Lipschitz deep neural networks, defines an attack model to understand its intrinsic properties, proves that such attackers can defeat any algorithm that classifies its inputs, but presents methods to overcome them by allowing the algorithm to abstain on unusual inputs, and provides strong theoretical guarantees and novel robustness guarantees for nearest-neighbor style algorithms."
}