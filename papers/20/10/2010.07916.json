{
    "title": "Multi-Agent Trust Region Policy Optimization. (arXiv:2010.07916v3 [cs.AI] UPDATED)",
    "abstract": "We extend trust region policy optimization (TRPO) to multi-agent reinforcement learning (MARL) problems. We show that the policy update of TRPO can be transformed into a distributed consensus optimization problem for multi-agent cases. By making a series of approximations to the consensus optimization model, we propose a decentralized MARL algorithm, which we call multi-agent TRPO (MATRPO). This algorithm can optimize distributed policies based on local observations and private rewards. The agents do not need to know observations, rewards, policies or value/action-value functions of other agents. The agents only share a likelihood ratio with their neighbors during the training process. The algorithm is fully decentralized and privacy-preserving. Our experiments on two cooperative games demonstrate its robust performance on complicated MARL tasks.",
    "link": "http://arxiv.org/abs/2010.07916",
    "context": "Title: Multi-Agent Trust Region Policy Optimization. (arXiv:2010.07916v3 [cs.AI] UPDATED)\nAbstract: We extend trust region policy optimization (TRPO) to multi-agent reinforcement learning (MARL) problems. We show that the policy update of TRPO can be transformed into a distributed consensus optimization problem for multi-agent cases. By making a series of approximations to the consensus optimization model, we propose a decentralized MARL algorithm, which we call multi-agent TRPO (MATRPO). This algorithm can optimize distributed policies based on local observations and private rewards. The agents do not need to know observations, rewards, policies or value/action-value functions of other agents. The agents only share a likelihood ratio with their neighbors during the training process. The algorithm is fully decentralized and privacy-preserving. Our experiments on two cooperative games demonstrate its robust performance on complicated MARL tasks.",
    "path": "papers/20/10/2010.07916.json",
    "total_tokens": 865,
    "translated_title": "多智能体信任区域策略优化",
    "translated_abstract": "我们将信任区域策略优化 (TRPO) 扩展到多智能体强化学习 (MARL) 问题。我们展示了 TRPO 的策略更新可以转化为多智能体案例下的分布式共识优化问题。通过对共识优化模型进行一系列的近似，我们提出了一种名为多智能体 TRPO (MATRPO) 的分散式 MARL 算法。该算法可以基于本地观测和私人奖励优化分布式策略。智能体不需要了解其他智能体的观测、奖励、策略或值/动作值函数。智能体在训练过程中只与邻居共享似然比。该算法完全分散且保护隐私。我们在两个合作游戏上的实验表明，它在复杂的 MARL 任务中具有出色的性能。",
    "tldr": "我们提出了一种名为多智能体 TRPO (MATRPO) 的分散式 MARL 算法，它可以在多智能体协作任务上优化分布式策略，并且无需智能体之间共享观测、奖励、策略或值/动作值函数。该算法在两个合作游戏上展示了出色的性能。",
    "en_tdlr": "We propose a decentralized MARL algorithm called multi-agent TRPO (MATRPO), which optimizes distributed policies in multi-agent cooperative tasks without the need for sharing observations, rewards, policies, or value/action-value functions between agents. The algorithm shows robust performance in two cooperative games."
}