{
    "title": "Investigating Membership Inference Attacks under Data Dependencies. (arXiv:2010.12112v4 [cs.CR] UPDATED)",
    "abstract": "Training machine learning models on privacy-sensitive data has become a popular practice, driving innovation in ever-expanding fields. This has opened the door to new attacks that can have serious privacy implications. One such attack, the Membership Inference Attack (MIA), exposes whether or not a particular data point was used to train a model. A growing body of literature uses Differentially Private (DP) training algorithms as a defence against such attacks. However, these works evaluate the defence under the restrictive assumption that all members of the training set, as well as non-members, are independent and identically distributed. This assumption does not hold for many real-world use cases in the literature. Motivated by this, we evaluate membership inference with statistical dependencies among samples and explain why DP does not provide meaningful protection (the privacy parameter $\\epsilon$ scales with the training set size $n$) in this more general case. We conduct a series",
    "link": "http://arxiv.org/abs/2010.12112",
    "context": "Title: Investigating Membership Inference Attacks under Data Dependencies. (arXiv:2010.12112v4 [cs.CR] UPDATED)\nAbstract: Training machine learning models on privacy-sensitive data has become a popular practice, driving innovation in ever-expanding fields. This has opened the door to new attacks that can have serious privacy implications. One such attack, the Membership Inference Attack (MIA), exposes whether or not a particular data point was used to train a model. A growing body of literature uses Differentially Private (DP) training algorithms as a defence against such attacks. However, these works evaluate the defence under the restrictive assumption that all members of the training set, as well as non-members, are independent and identically distributed. This assumption does not hold for many real-world use cases in the literature. Motivated by this, we evaluate membership inference with statistical dependencies among samples and explain why DP does not provide meaningful protection (the privacy parameter $\\epsilon$ scales with the training set size $n$) in this more general case. We conduct a series",
    "path": "papers/20/10/2010.12112.json",
    "total_tokens": 1016,
    "translated_title": "研究在数据依赖性下的成员推断攻击",
    "translated_abstract": "在隐私敏感数据上训练机器学习模型已经成为一种流行的实践，推动着不断扩大的领域中的创新。这打开了新的攻击方式，可能会带来严重的隐私影响。一种这样的攻击是成员推断攻击（MIA），它揭示了特定数据点是否被用于训练模型。越来越多的文献使用差分隐私（DP）训练算法作为防御这种攻击的手段。然而，这些工作在评估防御策略时都使用了一种限制性假设，即训练集和非成员独立且满足相同分布。这种假设在很多现实应用中并不成立。在这种背景下，本文研究了样本之间的统计依赖性对成员推断的影响，并解释了为什么在这种更普遍的情况下DP不能提供有意义的保护（隐私参数$\\epsilon$随训练集大小$n$的增加而增加）。我们进行了一系列实验证明了DP在数据依赖性下对MIA的脆弱性，突显出需要寻找可提供更强隐私保证的替代防御策略。",
    "tldr": "本文研究了在数据具有依赖性的情况下成员推断攻击的影响，并表明了DP无法在这种情况下提供有意义的保护，需要探索替代的防御策略来提供更强隐私保证。",
    "en_tdlr": "This paper investigates the impact of membership inference attacks under data dependencies and shows that DP cannot provide meaningful protection in this case, highlighting the need for alternative defences for stronger privacy guarantees."
}