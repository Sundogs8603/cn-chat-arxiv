{
    "title": "The ELBO of Variational Autoencoders Converges to a Sum of Three Entropies. (arXiv:2010.14860v5 [stat.ML] UPDATED)",
    "abstract": "The central objective function of a variational autoencoder (VAE) is its variational lower bound (the ELBO). Here we show that for standard (i.e., Gaussian) VAEs the ELBO converges to a value given by the sum of three entropies: the (negative) entropy of the prior distribution, the expected (negative) entropy of the observable distribution, and the average entropy of the variational distributions (the latter is already part of the ELBO). Our derived analytical results are exact and apply for small as well as for intricate deep networks for encoder and decoder. Furthermore, they apply for finitely and infinitely many data points and at any stationary point (including local maxima and saddle points). The result implies that the ELBO can for standard VAEs often be computed in closed-form at stationary points while the original ELBO requires numerical approximations of integrals. As a main contribution, we provide the proof that the ELBO for VAEs is at stationary points equal to entropy su",
    "link": "http://arxiv.org/abs/2010.14860",
    "context": "Title: The ELBO of Variational Autoencoders Converges to a Sum of Three Entropies. (arXiv:2010.14860v5 [stat.ML] UPDATED)\nAbstract: The central objective function of a variational autoencoder (VAE) is its variational lower bound (the ELBO). Here we show that for standard (i.e., Gaussian) VAEs the ELBO converges to a value given by the sum of three entropies: the (negative) entropy of the prior distribution, the expected (negative) entropy of the observable distribution, and the average entropy of the variational distributions (the latter is already part of the ELBO). Our derived analytical results are exact and apply for small as well as for intricate deep networks for encoder and decoder. Furthermore, they apply for finitely and infinitely many data points and at any stationary point (including local maxima and saddle points). The result implies that the ELBO can for standard VAEs often be computed in closed-form at stationary points while the original ELBO requires numerical approximations of integrals. As a main contribution, we provide the proof that the ELBO for VAEs is at stationary points equal to entropy su",
    "path": "papers/20/10/2010.14860.json",
    "total_tokens": 948,
    "translated_title": "变分自编码器的ELBO收敛于三个熵之和。",
    "translated_abstract": "变分自编码器(VAEs)的中心目标函数是其变分下界(ELBO)。我们展示了对于标准(即高斯)VAEs，ELBO收敛于由三个熵之和给出的值：(先验分布的负)熵、可观测分布的预期(负)熵以及变分分布的平均熵(后者已经是ELBO的一部分)。我们的推导结果精确，适用于编码器和解码器的小型和复杂深度网络，并适用于有限和无限数量的数据点以及任何稳定点(包括局部最大值和鞍点)。该结果意味着对于标准VAEs，ELBO在稳定点时通常可以以闭合形式计算，而原始ELBO需要数值积分近似。作为主要贡献，我们提供了VAEs的ELBO在稳定点处等于熵的证明。",
    "tldr": "标准变分自编码器的ELBO在稳定点处可以以闭合形式计算，收敛于三个熵之和。（其中一个熵为先验分布的熵，一个为可观测分布的熵，一个为变分分布的平均熵，成果证明了ELBO在稳定点处等于熵。）",
    "en_tdlr": "For standard variational autoencoders, the ELBO at stationary points can be computed in closed-form and converges to the sum of three entropies (the negative entropy of the prior distribution, the expected negative entropy of the observable distribution, and the average entropy of the variational distributions). The contribution is the proof that the ELBO for VAEs is at stationary points equal to entropy."
}