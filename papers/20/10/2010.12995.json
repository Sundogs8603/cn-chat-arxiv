{
    "title": "Out-of-distribution detection for regression tasks: parameter versus predictor entropy. (arXiv:2010.12995v2 [cs.LG] UPDATED)",
    "abstract": "It is crucial to detect when an instance lies downright too far from the training samples for the machine learning model to be trusted, a challenge known as out-of-distribution (OOD) detection. For neural networks, one approach to this task consists of learning a diversity of predictors that all can explain the training data. This information can be used to estimate the epistemic uncertainty at a given newly observed instance in terms of a measure of the disagreement of the predictions. Evaluation and certification of the ability of a method to detect OOD require specifying instances which are likely to occur in deployment yet on which no prediction is available. Focusing on regression tasks, we choose a simple yet insightful model for this OOD distribution and conduct an empirical evaluation of the ability of various methods to discriminate OOD samples from the data. Moreover, we exhibit evidence that a diversity of parameters may fail to translate to a diversity of predictors. Based ",
    "link": "http://arxiv.org/abs/2010.12995",
    "context": "Title: Out-of-distribution detection for regression tasks: parameter versus predictor entropy. (arXiv:2010.12995v2 [cs.LG] UPDATED)\nAbstract: It is crucial to detect when an instance lies downright too far from the training samples for the machine learning model to be trusted, a challenge known as out-of-distribution (OOD) detection. For neural networks, one approach to this task consists of learning a diversity of predictors that all can explain the training data. This information can be used to estimate the epistemic uncertainty at a given newly observed instance in terms of a measure of the disagreement of the predictions. Evaluation and certification of the ability of a method to detect OOD require specifying instances which are likely to occur in deployment yet on which no prediction is available. Focusing on regression tasks, we choose a simple yet insightful model for this OOD distribution and conduct an empirical evaluation of the ability of various methods to discriminate OOD samples from the data. Moreover, we exhibit evidence that a diversity of parameters may fail to translate to a diversity of predictors. Based ",
    "path": "papers/20/10/2010.12995.json",
    "total_tokens": 885,
    "translated_title": "回归任务中的离群样本检测：参数与预测器熵比较",
    "translated_abstract": "对于机器学习模型来说，检测样本与训练样本相距太远时至关重要，这被称为离群样本（OOD）检测。对于神经网络而言，一种处理这个任务的方法是学习多样的预测器，这些预测器都能解释训练数据。这些信息可以用来估计新观测实例的认识不确定性，通过预测结果的不一致性来衡量。评估和认证方法检测OOD的能力需要指定在部署中可能发生但没有可用预测的实例。我们选择回归任务作为研究重点，在此任务中选择一个简单而有洞察力的模型来表示OOD分布，并对各种方法在区分OOD样本和数据中的能力进行实证评估。此外，我们还提供证据表明，参数的多样性可能无法转化为预测器的多样性。",
    "tldr": "本研究针对回归任务中的离群样本检测进行了实证评估，发现通过学习多样的预测器可以估计新观测实例的认识不确定性，但参数的多样性并不一定能转化为预测器的多样性。",
    "en_tdlr": "This study empirically evaluates the detection of out-of-distribution samples in regression tasks and finds that learning a diversity of predictors can estimate the epistemic uncertainty of new observations, but the diversity of parameters does not necessarily translate into the diversity of predictors."
}