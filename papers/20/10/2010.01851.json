{
    "title": "On the Universality of the Double Descent Peak in Ridgeless Regression. (arXiv:2010.01851v8 [stat.ML] UPDATED)",
    "abstract": "We prove a non-asymptotic distribution-independent lower bound for the expected mean squared generalization error caused by label noise in ridgeless linear regression. Our lower bound generalizes a similar known result to the overparameterized (interpolating) regime. In contrast to most previous works, our analysis applies to a broad class of input distributions with almost surely full-rank feature matrices, which allows us to cover various types of deterministic or random feature maps. Our lower bound is asymptotically sharp and implies that in the presence of label noise, ridgeless linear regression does not perform well around the interpolation threshold for any of these feature maps. We analyze the imposed assumptions in detail and provide a theory for analytic (random) feature maps. Using this theory, we can show that our assumptions are satisfied for input distributions with a (Lebesgue) density and feature maps given by random deep neural networks with analytic activation functi",
    "link": "http://arxiv.org/abs/2010.01851",
    "context": "Title: On the Universality of the Double Descent Peak in Ridgeless Regression. (arXiv:2010.01851v8 [stat.ML] UPDATED)\nAbstract: We prove a non-asymptotic distribution-independent lower bound for the expected mean squared generalization error caused by label noise in ridgeless linear regression. Our lower bound generalizes a similar known result to the overparameterized (interpolating) regime. In contrast to most previous works, our analysis applies to a broad class of input distributions with almost surely full-rank feature matrices, which allows us to cover various types of deterministic or random feature maps. Our lower bound is asymptotically sharp and implies that in the presence of label noise, ridgeless linear regression does not perform well around the interpolation threshold for any of these feature maps. We analyze the imposed assumptions in detail and provide a theory for analytic (random) feature maps. Using this theory, we can show that our assumptions are satisfied for input distributions with a (Lebesgue) density and feature maps given by random deep neural networks with analytic activation functi",
    "path": "papers/20/10/2010.01851.json",
    "total_tokens": 948,
    "translated_title": "无岭回归中双下降峰的普遍性研究",
    "translated_abstract": "我们证明了在无岭线性回归中由标签噪声引起的期望均方泛化误差的非渐近非分布相关下界。我们的下界将类似的已知结果推广到超参数化（插值）区域。与大多数前期工作不同，我们的分析适用于具有几乎必然完全秩特征矩阵的广泛输入分布类，这使我们能够覆盖各种确定性或随机特征映射类型。我们的下界是渐近尖锐的，并且意味着在存在标签噪声的情况下，无岭线性回归在任何这些特征映射的插值阈值周围表现不佳。我们详细分析了所施加的假设，并为解析（随机）特征映射提供了理论。利用这个理论，我们可以证明我们的假设对具有（勒贝格）密度的输入分布以及由分析激活函数给出的随机深度神经网络的特征映射成立。",
    "tldr": "我们证明了在无岭线性回归中存在一个双下降峰，无论输入分布的特征映射是确定性的还是随机的，都会导致期望均方泛化误差增加。并且我们的结果适用于广泛的输入分布类。",
    "en_tdlr": "We prove the universality of the double descent peak in ridgeless linear regression, showing that the expected mean squared generalization error increases around the interpolation threshold for various types of input distributions with full-rank feature matrices. This result applies to both deterministic and random feature maps."
}