{
    "title": "A Theory of the Risk for Optimization with Relaxation and its Application to Support Vector Machines. (arXiv:2004.05839v4 [cs.LG] UPDATED)",
    "abstract": "In this paper we consider optimization with relaxation, an ample paradigm to make data-driven designs. This approach was previously considered by the same authors of this work in Garatti and Campi (2019), a study that revealed a deep-seated connection between two concepts: risk (probability of not satisfying a new, out-of-sample, constraint) and complexity (according to a definition introduced in paper Garatti and Campi (2019)). This connection was shown to have profound implications in applications because it implied that the risk can be estimated from the complexity, a quantity that can be measured from the data without any knowledge of the data-generation mechanism. In the present work we establish new results. First, we expand the scope of Garatti and Campi (2019) so as to embrace a more general setup that covers various algorithms in machine learning. Then, we study classical support vector methods - including SVM (Support Vector Machine), SVR (Support Vector Regression) and SVDD ",
    "link": "http://arxiv.org/abs/2004.05839",
    "context": "Title: A Theory of the Risk for Optimization with Relaxation and its Application to Support Vector Machines. (arXiv:2004.05839v4 [cs.LG] UPDATED)\nAbstract: In this paper we consider optimization with relaxation, an ample paradigm to make data-driven designs. This approach was previously considered by the same authors of this work in Garatti and Campi (2019), a study that revealed a deep-seated connection between two concepts: risk (probability of not satisfying a new, out-of-sample, constraint) and complexity (according to a definition introduced in paper Garatti and Campi (2019)). This connection was shown to have profound implications in applications because it implied that the risk can be estimated from the complexity, a quantity that can be measured from the data without any knowledge of the data-generation mechanism. In the present work we establish new results. First, we expand the scope of Garatti and Campi (2019) so as to embrace a more general setup that covers various algorithms in machine learning. Then, we study classical support vector methods - including SVM (Support Vector Machine), SVR (Support Vector Regression) and SVDD ",
    "path": "papers/20/04/2004.05839.json",
    "total_tokens": 885,
    "translated_title": "通过松弛优化理论及其在支持向量机中的应用，对风险进行研究",
    "translated_abstract": "本文考虑了松弛优化，这是一种用于数据驱动设计的广泛范例。我们建立了\"风险\"（未能满足新的、样本外约束的概率）和\"复杂度\"（根据Garatti和Campi（2019）中介绍的定义）之间的深层联系，并发现这种联系对应用有深远影响，因为它意味着可以从复杂度估计风险，而复杂度可以通过数据进行测量，不需要了解数据生成机制。在本文中，我们建立了新的结果。首先，我们扩大了Garatti和Campi（2019）的范围，以涵盖机器学习中的各种算法。然后，我们研究了经典的支持向量方法，包括支持向量机（SVM）、支持向量回归（SVR）和支持向量数据描述（SVDD）。",
    "tldr": "本文研究了松弛优化理论，探讨了风险与复杂度之间的联系，提出了可以从复杂度估计风险的方法，研究了支持向量方法在机器学习中的应用。"
}