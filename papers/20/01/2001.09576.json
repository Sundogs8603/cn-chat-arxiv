{
    "title": "Naive Exploration is Optimal for Online LQR. (arXiv:2001.09576v4 [cs.LG] UPDATED)",
    "abstract": "We consider the problem of online adaptive control of the linear quadratic regulator, where the true system parameters are unknown. We prove new upper and lower bounds demonstrating that the optimal regret scales as $\\widetilde{\\Theta}({\\sqrt{d_{\\mathbf{u}}^2 d_{\\mathbf{x}} T}})$, where $T$ is the number of time steps, $d_{\\mathbf{u}}$ is the dimension of the input space, and $d_{\\mathbf{x}}$ is the dimension of the system state. Notably, our lower bounds rule out the possibility of a $\\mathrm{poly}(\\log{}T)$-regret algorithm, which had been conjectured due to the apparent strong convexity of the problem. Our upper bound is attained by a simple variant of $\\textit{{certainty equivalent control}}$, where the learner selects control inputs according to the optimal controller for their estimate of the system while injecting exploratory random noise. While this approach was shown to achieve $\\sqrt{T}$-regret by (Mania et al. 2019), we show that if the learner continually refines their esti",
    "link": "http://arxiv.org/abs/2001.09576",
    "context": "Title: Naive Exploration is Optimal for Online LQR. (arXiv:2001.09576v4 [cs.LG] UPDATED)\nAbstract: We consider the problem of online adaptive control of the linear quadratic regulator, where the true system parameters are unknown. We prove new upper and lower bounds demonstrating that the optimal regret scales as $\\widetilde{\\Theta}({\\sqrt{d_{\\mathbf{u}}^2 d_{\\mathbf{x}} T}})$, where $T$ is the number of time steps, $d_{\\mathbf{u}}$ is the dimension of the input space, and $d_{\\mathbf{x}}$ is the dimension of the system state. Notably, our lower bounds rule out the possibility of a $\\mathrm{poly}(\\log{}T)$-regret algorithm, which had been conjectured due to the apparent strong convexity of the problem. Our upper bound is attained by a simple variant of $\\textit{{certainty equivalent control}}$, where the learner selects control inputs according to the optimal controller for their estimate of the system while injecting exploratory random noise. While this approach was shown to achieve $\\sqrt{T}$-regret by (Mania et al. 2019), we show that if the learner continually refines their esti",
    "path": "papers/20/01/2001.09576.json",
    "total_tokens": 983,
    "translated_title": "Naive Exploration is Optimal for Online LQR（在线LQR问题中，天真的探索是最优的）",
    "translated_abstract": "本文研究了线性二次调节器在线自适应控制的问题，其中真实系统参数是未知的。我们证明了新的上下界，表明最优遗憾的尺度与$\\widetilde{\\Theta}({\\sqrt{d_{\\mathbf{u}}^2 d_{\\mathbf{x}} T}})$成正比，其中$T$是时间步数，$d_{\\mathbf{u}}$是输入空间的维度，$d_{\\mathbf{x}}$是系统状态的维度。值得注意的是，我们的下界排除了可能存在的$\\mathrm{poly}(\\log{}T)$遗憾算法的可能性，这是由于问题明显的强凸性引出的猜想。我们的上界通过一种简单的$\\textit{{确定性等价控制}}$的变体得到，其中学习者根据对系统的估计选择控制输入，并注入探索性的随机噪声。虽然这种方法已经被证明可以实现$\\sqrt{T}$的遗憾(Raina et al. 2019)，但我们表明，如果学习者不断改进他们的估计，找到了系统的真实参数，则可以实现零遗憾。",
    "tldr": "在线LQR问题中，我们证明了天真的探索是最优的策略，可以在未知参数的情况下达到最小遗憾。这一结论对于解决在线自适应控制问题具有重要意义。",
    "en_tdlr": "Naive exploration is proven to be the optimal strategy for online adaptive control of the linear quadratic regulator in the presence of unknown parameters, achieving minimal regret. This finding has significant implications for addressing the problem of online adaptive control."
}