{
    "title": "Boosting Simple Learners. (arXiv:2001.11704v8 [cs.LG] UPDATED)",
    "abstract": "Boosting is a celebrated machine learning approach which is based on the idea of combining weak and moderately inaccurate hypotheses to a strong and accurate one. We study boosting under the assumption that the weak hypotheses belong to a class of bounded capacity. This assumption is inspired by the common convention that weak hypotheses are \"rules-of-thumbs\" from an \"easy-to-learn class\". (Schapire and Freund~'12, Shalev-Shwartz and Ben-David '14.) Formally, we assume the class of weak hypotheses has a bounded VC dimension. We focus on two main questions: (i) Oracle Complexity: How many weak hypotheses are needed to produce an accurate hypothesis? We design a novel boosting algorithm and demonstrate that it circumvents a classical lower bound by Freund and Schapire ('95, '12). Whereas the lower bound shows that $\\Omega({1}/{\\gamma^2})$ weak hypotheses with $\\gamma$-margin are sometimes necessary, our new method requires only $\\tilde{O}({1}/{\\gamma})$ weak hypothesis, provided that the",
    "link": "http://arxiv.org/abs/2001.11704",
    "context": "Title: Boosting Simple Learners. (arXiv:2001.11704v8 [cs.LG] UPDATED)\nAbstract: Boosting is a celebrated machine learning approach which is based on the idea of combining weak and moderately inaccurate hypotheses to a strong and accurate one. We study boosting under the assumption that the weak hypotheses belong to a class of bounded capacity. This assumption is inspired by the common convention that weak hypotheses are \"rules-of-thumbs\" from an \"easy-to-learn class\". (Schapire and Freund~'12, Shalev-Shwartz and Ben-David '14.) Formally, we assume the class of weak hypotheses has a bounded VC dimension. We focus on two main questions: (i) Oracle Complexity: How many weak hypotheses are needed to produce an accurate hypothesis? We design a novel boosting algorithm and demonstrate that it circumvents a classical lower bound by Freund and Schapire ('95, '12). Whereas the lower bound shows that $\\Omega({1}/{\\gamma^2})$ weak hypotheses with $\\gamma$-margin are sometimes necessary, our new method requires only $\\tilde{O}({1}/{\\gamma})$ weak hypothesis, provided that the",
    "path": "papers/20/01/2001.11704.json",
    "total_tokens": 1015,
    "translated_title": "提升简单学习器的能力",
    "translated_abstract": "提升是一种著名的机器学习方法，基于结合弱且具有适度错误的假设，以生成强且准确的假设。本研究探讨一种假设：弱学习器属于一个有边界容量的类。该假设来自于常见的传统做法，即将弱学习器视为“规则”或“基础类”中的“经验法则”（Schapire和Freund'12、Shalev-Shwartz和Ben-David'14）。具体而言，我们假设弱学习器的VC维度受到限制。我们主要关注两个问题：（i）Oracle复杂度：需要多少个弱学习器才能生成准确的假设？我们设计了一种新颖的提升算法，并证明它规避了Freund和Schapire的经典下界（'95，'12）。尽管下界表明有时需要具有$\\gamma$-间隙的$\\Omega({1}/{\\gamma^2})$个弱假设，但我们的新方法只需要提供约$\\tilde{O}({1}/{\\gamma})$个弱假设，假设其",
    "tldr": "本论文探讨的是提升学习器的方法，关注弱学习器属于一个容量受限的类的假设，并重点关注需要多少个弱学习器才能生成准确的假设。通过设计新颖的算法，只需要约$\\tilde{O}({1}/{\\gamma})$个弱假设就能够规避经典下界。",
    "en_tdlr": "The paper explores methods for boosting learners, focusing on the hypothesis that weak learners belong to a class of bounded capacity and the question of how many weak learners are needed to generate an accurate hypothesis. Through the design of a novel algorithm, the paper demonstrates the ability to circumvent a classic lower bound using only approximately $\\tilde{O}({1}/{\\gamma})$ weak hypotheses."
}