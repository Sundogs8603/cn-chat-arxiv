{
    "title": "MixPath: A Unified Approach for One-shot Neural Architecture Search. (arXiv:2001.05887v4 [cs.LG] UPDATED)",
    "abstract": "Blending multiple convolutional kernels is proved advantageous in neural architecture design. However, current two-stage neural architecture search methods are mainly limited to single-path search spaces. How to efficiently search models of multi-path structures remains a difficult problem. In this paper, we are motivated to train a one-shot multi-path supernet to accurately evaluate the candidate architectures. Specifically, we discover that in the studied search spaces, feature vectors summed from multiple paths are nearly multiples of those from a single path. Such disparity perturbs the supernet training and its ranking ability. Therefore, we propose a novel mechanism called Shadow Batch Normalization (SBN) to regularize the disparate feature statistics. Extensive experiments prove that SBNs are capable of stabilizing the optimization and improving ranking performance. We call our unified multi-path one-shot approach as MixPath, which generates a series of models that achieve state",
    "link": "http://arxiv.org/abs/2001.05887",
    "context": "Title: MixPath: A Unified Approach for One-shot Neural Architecture Search. (arXiv:2001.05887v4 [cs.LG] UPDATED)\nAbstract: Blending multiple convolutional kernels is proved advantageous in neural architecture design. However, current two-stage neural architecture search methods are mainly limited to single-path search spaces. How to efficiently search models of multi-path structures remains a difficult problem. In this paper, we are motivated to train a one-shot multi-path supernet to accurately evaluate the candidate architectures. Specifically, we discover that in the studied search spaces, feature vectors summed from multiple paths are nearly multiples of those from a single path. Such disparity perturbs the supernet training and its ranking ability. Therefore, we propose a novel mechanism called Shadow Batch Normalization (SBN) to regularize the disparate feature statistics. Extensive experiments prove that SBNs are capable of stabilizing the optimization and improving ranking performance. We call our unified multi-path one-shot approach as MixPath, which generates a series of models that achieve state",
    "path": "papers/20/01/2001.05887.json",
    "total_tokens": 918,
    "translated_title": "MixPath: 一种统一的一次性神经架构搜索方法",
    "translated_abstract": "在神经架构设计中，混合多个卷积核被证明是有优势的。然而，当前的两阶段神经架构搜索方法主要局限于单路径搜索空间。如何高效地搜索多路径结构的模型仍然是一个难题。在本文中，我们的动机是训练一个一次性的多路径超网络来准确评估候选架构。具体来说，我们发现在所研究的搜索空间中，从多个路径中求和的特征向量几乎是单个路径的倍数。这种差异扰乱了超网络的训练和排名能力。因此，我们提出了一种新颖的机制，称为Shadow Batch Normalization（SBN），来规范差异的特征统计。大量实验证明，SBN能够稳定优化和提高排名性能。我们将我们的统一多路径一次性方法称为MixPath，可以生成一系列能达到最新技术水平的模型。",
    "tldr": "本论文提出了一种名为MixPath的统一的一次性神经架构搜索方法，通过训练一次性的多路径超网络来准确评估候选架构。采用一种新颖的机制称为Shadow Batch Normalization（SBN）来解决多路径结构的特征差异问题，稳定优化并提高排名性能。",
    "en_tdlr": "This paper proposes a unified one-shot neural architecture search method called MixPath, which trains a one-shot multi-path supernet to accurately evaluate candidate architectures. It introduces a novel mechanism called Shadow Batch Normalization (SBN) to address the feature disparity issue in multi-path structures, resulting in stabilized optimization and improved ranking performance."
}