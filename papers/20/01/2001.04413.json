{
    "title": "Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning. (arXiv:2001.04413v6 [cs.LG] UPDATED)",
    "abstract": "Deep learning is also known as hierarchical learning, where the learner _learns_ to represent a complicated target function by decomposing it into a sequence of simpler functions to reduce sample and time complexity. This paper formally analyzes how multi-layer neural networks can perform such hierarchical learning _efficiently_ and _automatically_ by SGD on the training objective.  On the conceptual side, we present a theoretical characterizations of how certain types of deep (i.e. super-constant layer) neural networks can still be sample and time efficiently trained on some hierarchical tasks, when no existing algorithm (including layerwise training, kernel method, etc) is known to be efficient. We establish a new principle called \"backward feature correction\", where the errors in the lower-level features can be automatically corrected when training together with the higher-level layers. We believe this is a key behind how deep learning is performing deep (hierarchical) learning, as ",
    "link": "http://arxiv.org/abs/2001.04413",
    "context": "Title: Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning. (arXiv:2001.04413v6 [cs.LG] UPDATED)\nAbstract: Deep learning is also known as hierarchical learning, where the learner _learns_ to represent a complicated target function by decomposing it into a sequence of simpler functions to reduce sample and time complexity. This paper formally analyzes how multi-layer neural networks can perform such hierarchical learning _efficiently_ and _automatically_ by SGD on the training objective.  On the conceptual side, we present a theoretical characterizations of how certain types of deep (i.e. super-constant layer) neural networks can still be sample and time efficiently trained on some hierarchical tasks, when no existing algorithm (including layerwise training, kernel method, etc) is known to be efficient. We establish a new principle called \"backward feature correction\", where the errors in the lower-level features can be automatically corrected when training together with the higher-level layers. We believe this is a key behind how deep learning is performing deep (hierarchical) learning, as ",
    "path": "papers/20/01/2001.04413.json",
    "total_tokens": 831,
    "translated_title": "《向后特征修正：深度学习如何进行深度（分层）学习》",
    "translated_abstract": "深度学习也被称为分层学习，学习者通过将复杂的目标函数分解为一系列更简单的函数来降低样本和时间复杂度。本文通过对训练目标进行随机梯度下降，从理论上分析了多层神经网络如何有效和自动地进行这种分层学习。在概念上，我们提出了一种理论特征，即某些类型的深度（即超常层）神经网络在某些分层任务上仍然可以以高效率的样本和时间进行训练，而现有的算法（包括逐层训练、核方法等）均无法高效。我们建立了一种名为“向后特征修正”的新原则，在训练过程中，较低层次特征的错误可以自动修正。我们认为这是深度学习如何进行深度（分层）学习的关键所在。",
    "tldr": "提出了一种新的原则 \"向后特征修正\"，通过训练过程中自动修正较低层次特征的错误，使得深度学习能够进行深度（分层）学习。"
}