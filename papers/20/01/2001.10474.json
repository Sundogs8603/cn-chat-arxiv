{
    "title": "Coagent Networks Revisited. (arXiv:2001.10474v3 [cs.LG] UPDATED)",
    "abstract": "Coagent networks formalize the concept of arbitrary networks of stochastic agents that collaborate to take actions in a reinforcement learning environment. Prominent examples of coagent networks in action include approaches to hierarchical reinforcement learning (HRL), such as those using options, which attempt to address the exploration exploitation trade-off by introducing abstract actions at different levels by sequencing multiple stochastic networks within the HRL agents. We first provide a unifying perspective on the many diverse examples that fall under coagent networks. We do so by formalizing the rules of execution in a coagent network, enabled by the novel and intuitive idea of execution paths in a coagent network. Motivated by parameter sharing in the hierarchical option-critic architecture, we revisit the coagent network theory and achieve a much shorter proof of the policy gradient theorem using our idea of execution paths, without any assumption on how parameters are share",
    "link": "http://arxiv.org/abs/2001.10474",
    "context": "Title: Coagent Networks Revisited. (arXiv:2001.10474v3 [cs.LG] UPDATED)\nAbstract: Coagent networks formalize the concept of arbitrary networks of stochastic agents that collaborate to take actions in a reinforcement learning environment. Prominent examples of coagent networks in action include approaches to hierarchical reinforcement learning (HRL), such as those using options, which attempt to address the exploration exploitation trade-off by introducing abstract actions at different levels by sequencing multiple stochastic networks within the HRL agents. We first provide a unifying perspective on the many diverse examples that fall under coagent networks. We do so by formalizing the rules of execution in a coagent network, enabled by the novel and intuitive idea of execution paths in a coagent network. Motivated by parameter sharing in the hierarchical option-critic architecture, we revisit the coagent network theory and achieve a much shorter proof of the policy gradient theorem using our idea of execution paths, without any assumption on how parameters are share",
    "path": "papers/20/01/2001.10474.json",
    "total_tokens": 832,
    "translated_title": "Coagent Networks再探讨",
    "translated_abstract": "Coagent networks（共智网络）形式化了在强化学习环境中协作以采取行动的随机代理网络的概念。共智网络的显著应用包括层次强化学习（HRL）的方法，例如使用选项的方法，通过在HRL代理中串联多个随机网络引入不同层次的抽象动作，来解决探索利用权衡问题。我们首先通过在共智网络中形式化执行规则、通过共智网络中执行路径的新颖而直观的思想，提供了一个统一的视角来描述许多不同的例子。在层次选项评论者架构中受到参数共享的启发，我们重新审视了共智网络理论，并使用我们的执行路径思想得到了对策梯度定理的更简洁证明，而不需要对参数共享做出任何假设。",
    "tldr": "Coagent Networks（共智网络）是指在强化学习环境中协作的随机代理网络。这篇论文重新审视了共智网络理论，提出了执行路径的思想，并通过这一思想实现了对策梯度定理的简洁证明。",
    "en_tdlr": "Coagent Networks refer to stochastic agent networks that collaborate in reinforcement learning environments. This paper revisits the theory of coagent networks, introduces the idea of execution paths, and presents a concise proof of the policy gradient theorem using this concept."
}