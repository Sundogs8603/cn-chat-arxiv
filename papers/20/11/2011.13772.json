{
    "title": "Gradient Descent for Deep Matrix Factorization: Dynamics and Implicit Bias towards Low Rank. (arXiv:2011.13772v5 [cs.LG] UPDATED)",
    "abstract": "In deep learning, it is common to use more network parameters than training points. In such scenarioof over-parameterization, there are usually multiple networks that achieve zero training error so that thetraining algorithm induces an implicit bias on the computed solution. In practice, (stochastic) gradientdescent tends to prefer solutions which generalize well, which provides a possible explanation of thesuccess of deep learning. In this paper we analyze the dynamics of gradient descent in the simplifiedsetting of linear networks and of an estimation problem. Although we are not in an overparameterizedscenario, our analysis nevertheless provides insights into the phenomenon of implicit bias. In fact, wederive a rigorous analysis of the dynamics of vanilla gradient descent, and characterize the dynamicalconvergence of the spectrum. We are able to accurately locate time intervals where the effective rankof the iterates is close to the effective rank of a low-rank projection of the gro",
    "link": "http://arxiv.org/abs/2011.13772",
    "context": "Title: Gradient Descent for Deep Matrix Factorization: Dynamics and Implicit Bias towards Low Rank. (arXiv:2011.13772v5 [cs.LG] UPDATED)\nAbstract: In deep learning, it is common to use more network parameters than training points. In such scenarioof over-parameterization, there are usually multiple networks that achieve zero training error so that thetraining algorithm induces an implicit bias on the computed solution. In practice, (stochastic) gradientdescent tends to prefer solutions which generalize well, which provides a possible explanation of thesuccess of deep learning. In this paper we analyze the dynamics of gradient descent in the simplifiedsetting of linear networks and of an estimation problem. Although we are not in an overparameterizedscenario, our analysis nevertheless provides insights into the phenomenon of implicit bias. In fact, wederive a rigorous analysis of the dynamics of vanilla gradient descent, and characterize the dynamicalconvergence of the spectrum. We are able to accurately locate time intervals where the effective rankof the iterates is close to the effective rank of a low-rank projection of the gro",
    "path": "papers/20/11/2011.13772.json",
    "total_tokens": 851,
    "translated_title": "深度矩阵分解的梯度下降：动力学和对低秩的隐含偏好",
    "translated_abstract": "在深度学习中，常常使用比训练数据点更多的网络参数。在这种超参数化的情况下，通常有多个网络可以实现零训练误差，因此训练算法对计算解决方案具有隐含偏好。在实践中，（随机）梯度下降往往更喜欢具有良好泛化能力的解决方案，这可以解释深度学习的成功。在本文中，我们分析了梯度下降在线性网络和估计问题的简化环境中的动力学。虽然我们不在超参数化的情况下，但我们的分析仍然揭示了隐含偏好的现象。事实上，我们对香草梯度下降的动力学收敛进行了严格分析，并表征了频谱的动态收敛。我们能够准确地确定迭代的有效秩接近于低秩投影的有效秩的时间间隔。",
    "tldr": "通过分析梯度下降在线性网络和估计问题中的动力学，揭示了深度学习中的隐含偏好现象。"
}