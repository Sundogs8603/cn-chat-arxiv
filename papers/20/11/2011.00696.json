{
    "title": "ABNIRML: Analyzing the Behavior of Neural IR Models. (arXiv:2011.00696v2 [cs.CL] UPDATED)",
    "abstract": "Pretrained contextualized language models such as BERT and T5 have established a new state-of-the-art for ad-hoc search. However, it is not yet well-understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have. We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs (ABNIRML), which includes new types of diagnostic probes that allow us to test several characteristics -- such as writing styles, factuality, sensitivity to paraphrasing and word order -- that are not addressed by previous techniques. To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model's gains, and identify potential unintended biases the models exhibit. Some of our results confirm conventional wisdom, like that recent neural ranking models rely less on exact term overlap with the query, and instead leverage richer ling",
    "link": "http://arxiv.org/abs/2011.00696",
    "context": "Title: ABNIRML: Analyzing the Behavior of Neural IR Models. (arXiv:2011.00696v2 [cs.CL] UPDATED)\nAbstract: Pretrained contextualized language models such as BERT and T5 have established a new state-of-the-art for ad-hoc search. However, it is not yet well-understood why these methods are so effective, what makes some variants more effective than others, and what pitfalls they may have. We present a new comprehensive framework for Analyzing the Behavior of Neural IR ModeLs (ABNIRML), which includes new types of diagnostic probes that allow us to test several characteristics -- such as writing styles, factuality, sensitivity to paraphrasing and word order -- that are not addressed by previous techniques. To demonstrate the value of the framework, we conduct an extensive empirical study that yields insights into the factors that contribute to the neural model's gains, and identify potential unintended biases the models exhibit. Some of our results confirm conventional wisdom, like that recent neural ranking models rely less on exact term overlap with the query, and instead leverage richer ling",
    "path": "papers/20/11/2011.00696.json",
    "total_tokens": 945,
    "translated_title": "ABNIRML: 分析神经信息检索模型的行为",
    "translated_abstract": "预训练上下文化语言模型（如BERT和T5）已经建立了信息检索领域的新的最先进技术。然而，我们还不完全理解为什么这些方法如此有效，是什么使一些变种比其他变种更有效，以及它们可能存在哪些问题。我们提出了一个全面的框架来分析神经信息检索模型行为（ABNIRML），包括新的诊断探针类型，允许我们测试先前技术未解决的几个特征，例如写作风格，事实性，对改写和词序的敏感性等。为了展示这个框架的价值，我们进行了广泛的实证研究，得出了对神经模型增益因素的见解，并确定了模型可能存在的潜在偏见。我们的一些结果证实了传统的观点，例如最近的神经排序模型对查询的确切术语匹配的依赖程度较低，而是利用更丰富的语言特征。",
    "tldr": "ABNIRML提供了一个全面的框架，分析了神经信息检索模型的行为，包括写作风格、事实性、对改写和词序的敏感性等特征。通过进行广泛的实证研究，我们探究了神经模型增益的因素，并发现了潜在的偏见。"
}