{
    "title": "InstaHide's Sample Complexity When Mixing Two Private Images",
    "abstract": "Training neural networks usually require large numbers of sensitive training data, and how to protect the privacy of training data has thus become a critical topic in deep learning research. InstaHide is a state-of-the-art scheme to protect training data privacy with only minor effects on test accuracy, and its security has become a salient question. In this paper, we systematically study recent attacks on InstaHide and present a unified framework to understand and analyze these attacks. We find that existing attacks either do not have a provable guarantee or can only recover a single private image. On the current InstaHide challenge setup, where each InstaHide image is a mixture of two private images, we present a new algorithm to recover all the private images with a provable guarantee and optimal sample complexity. In addition, we also provide a computational hardness result on retrieving all InstaHide images. Our results demonstrate that InstaHide is not information-theoretically s",
    "link": "https://arxiv.org/abs/2011.11877",
    "context": "Title: InstaHide's Sample Complexity When Mixing Two Private Images\nAbstract: Training neural networks usually require large numbers of sensitive training data, and how to protect the privacy of training data has thus become a critical topic in deep learning research. InstaHide is a state-of-the-art scheme to protect training data privacy with only minor effects on test accuracy, and its security has become a salient question. In this paper, we systematically study recent attacks on InstaHide and present a unified framework to understand and analyze these attacks. We find that existing attacks either do not have a provable guarantee or can only recover a single private image. On the current InstaHide challenge setup, where each InstaHide image is a mixture of two private images, we present a new algorithm to recover all the private images with a provable guarantee and optimal sample complexity. In addition, we also provide a computational hardness result on retrieving all InstaHide images. Our results demonstrate that InstaHide is not information-theoretically s",
    "path": "papers/20/11/2011.11877.json",
    "total_tokens": 916,
    "translated_title": "InstaHide混合两个私人图片时的样本复杂度",
    "translated_abstract": "训练神经网络通常需要大量的敏感训练数据，如何保护训练数据的隐私已成为深度学习研究中的重要课题。InstaHide是一种最先进的用于保护训练数据隐私的方案，对测试准确性只有微小影响，并且其安全性已成为一个突出的问题。本文系统研究了对InstaHide的最新攻击，并提出了一个统一的框架来理解和分析这些攻击。我们发现现有的攻击要么没有可证明的保证，要么只能恢复一个私人图片。在当前的InstaHide挑战设置下，每个InstaHide图片是两个私人图片的混合，我们提出了一种新算法来以可证明的保证和最优样本复杂度恢复所有私人图片。此外，我们还提供了关于检索所有InstaHide图片的计算难度结果。我们的结果表明InstaHide在信息论上并非是安全的。",
    "tldr": "本文研究了对InstaHide的最新攻击，提出了一个统一的框架来理解和分析这些攻击。通过一种新算法，在InstaHide挑战设置下，以可证明的保证和最优样本复杂度恢复所有私人图片，并提供了关于检索所有InstaHide图片的计算难度结果。",
    "en_tdlr": "This paper studies recent attacks on InstaHide, presents a unified framework to understand and analyze these attacks. By proposing a new algorithm, it recovers all private images with provable guarantee and optimal sample complexity under the current InstaHide challenge setup, and provides computational hardness result on retrieving all InstaHide images."
}