{
    "title": "An Automatic Finite-Sample Robustness Metric: When Can Dropping a Little Data Make a Big Difference?. (arXiv:2011.14999v5 [stat.ME] UPDATED)",
    "abstract": "Study samples often differ from the target populations of inference and policy decisions in non-random ways. Researchers typically believe that such departures from random sampling -- due to changes in the population over time and space, or difficulties in sampling truly randomly -- are small, and their corresponding impact on the inference should be small as well. We might therefore be concerned if the conclusions of our studies are excessively sensitive to a very small proportion of our sample data. We propose a method to assess the sensitivity of applied econometric conclusions to the removal of a small fraction of the sample. Manually checking the influence of all possible small subsets is computationally infeasible, so we use an approximation to find the most influential subset. Our metric, the \"Approximate Maximum Influence Perturbation,\" is based on the classical influence function, and is automatically computable for common methods including (but not limited to) OLS, IV, MLE, G",
    "link": "http://arxiv.org/abs/2011.14999",
    "context": "Title: An Automatic Finite-Sample Robustness Metric: When Can Dropping a Little Data Make a Big Difference?. (arXiv:2011.14999v5 [stat.ME] UPDATED)\nAbstract: Study samples often differ from the target populations of inference and policy decisions in non-random ways. Researchers typically believe that such departures from random sampling -- due to changes in the population over time and space, or difficulties in sampling truly randomly -- are small, and their corresponding impact on the inference should be small as well. We might therefore be concerned if the conclusions of our studies are excessively sensitive to a very small proportion of our sample data. We propose a method to assess the sensitivity of applied econometric conclusions to the removal of a small fraction of the sample. Manually checking the influence of all possible small subsets is computationally infeasible, so we use an approximation to find the most influential subset. Our metric, the \"Approximate Maximum Influence Perturbation,\" is based on the classical influence function, and is automatically computable for common methods including (but not limited to) OLS, IV, MLE, G",
    "path": "papers/20/11/2011.14999.json",
    "total_tokens": 863,
    "translated_title": "一个自动的有限样本稳健性度量：当删除少量数据可能造成很大差异时？",
    "translated_abstract": "研究样本往往以非随机方式与推断和政策决策的目标人群不同。研究人员通常认为，这种对随机抽样的偏离——由于时间和空间上的人口变化，或者真正随机采样的困难——很小，对推断的影响也应该很小。因此，我们可能会担心我们研究的结论过于敏感，对样本数据的非常小的比例过于敏感。我们提出了一种方法来评估应用计量经济学结论对样本的小部分删除的敏感性。手动检查所有可能的小子集的影响在计算上是不可行的，因此我们使用近似方法找到最有影响力的子集合。我们的度量方法，“近似最大影响扰动”，基于经典的影响函数，并且对通常的方法（包括但不限于OLS，IV，MLE，G）可以自动计算。",
    "tldr": "该论文提出了一种自动的有限样本稳健性度量方法，用于评估应用计量经济学结论对样本的小部分删除的敏感性。",
    "en_tdlr": "This paper proposes an automatic finite-sample robustness metric to assess the sensitivity of applied econometric conclusions to the removal of a small fraction of the sample."
}