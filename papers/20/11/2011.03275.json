{
    "title": "Sample-efficient Reinforcement Learning in Robotic Table Tennis. (arXiv:2011.03275v4 [cs.RO] UPDATED)",
    "abstract": "Reinforcement learning (RL) has achieved some impressive recent successes in various computer games and simulations. Most of these successes are based on having large numbers of episodes from which the agent can learn. In typical robotic applications, however, the number of feasible attempts is very limited. In this paper we present a sample-efficient RL algorithm applied to the example of a table tennis robot. In table tennis every stroke is different, with varying placement, speed and spin. An accurate return therefore has to be found depending on a high-dimensional continuous state space. To make learning in few trials possible the method is embedded into our robot system. In this way we can use a one-step environment. The state space depends on the ball at hitting time (position, velocity, spin) and the action is the racket state (orientation, velocity) at hitting. An actor-critic based deterministic policy gradient algorithm was developed for accelerated learning. Our approach per",
    "link": "http://arxiv.org/abs/2011.03275",
    "context": "Title: Sample-efficient Reinforcement Learning in Robotic Table Tennis. (arXiv:2011.03275v4 [cs.RO] UPDATED)\nAbstract: Reinforcement learning (RL) has achieved some impressive recent successes in various computer games and simulations. Most of these successes are based on having large numbers of episodes from which the agent can learn. In typical robotic applications, however, the number of feasible attempts is very limited. In this paper we present a sample-efficient RL algorithm applied to the example of a table tennis robot. In table tennis every stroke is different, with varying placement, speed and spin. An accurate return therefore has to be found depending on a high-dimensional continuous state space. To make learning in few trials possible the method is embedded into our robot system. In this way we can use a one-step environment. The state space depends on the ball at hitting time (position, velocity, spin) and the action is the racket state (orientation, velocity) at hitting. An actor-critic based deterministic policy gradient algorithm was developed for accelerated learning. Our approach per",
    "path": "papers/20/11/2011.03275.json",
    "total_tokens": 873,
    "translated_title": "机器人乒乓球中的高效样本增强学习",
    "translated_abstract": "强化学习在各种计算机游戏和模拟中取得了一些令人印象深刻的成就。然而，大多数成功案例都是基于大量试验次数的学习。而在典型的机器人应用中，可行尝试次数非常有限。本文提出了一种适用于乒乓球机器人的高效样本增强学习算法。在乒乓球中，每个击球都是不同的，具有不同的位置、速度和旋转。因此，必须根据高维连续状态空间找到准确的回球方式。为了在少数尝试中实现学习，该方法被嵌入到我们的机器人系统中，使用一步环境。状态空间取决于击球时球的位置、速度和旋转，动作是击球时球拍的状态（方向、速度）。我们开发了基于演员-评论家的确定性策略梯度算法来加速学习。我们的方法可以以较少的样本次数实现高效学习。",
    "tldr": "这篇论文研究了机器人乒乓球中的高效样本增强学习算法，通过嵌入到机器人系统中，在少数尝试中实现了高效学习。"
}