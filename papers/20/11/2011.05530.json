{
    "title": "On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU Networks",
    "abstract": "Outsourcing deep neural networks (DNNs) inference tasks to an untrusted cloud raises data privacy and integrity concerns. While there are many techniques to ensure privacy and integrity for polynomial-based computations, DNNs involve non-polynomial computations. To address these challenges, several privacy-preserving and verifiable inference techniques have been proposed based on replacing the non-polynomial activation functions such as the rectified linear unit (ReLU) function with polynomial activation functions. Such techniques usually require polynomials with integer coefficients or polynomials over finite fields. Motivated by such requirements, several works proposed replacing the ReLU function with the square function. In this work, we empirically show that the square function is not the best degree-2 polynomial that can replace the ReLU function even when restricting the polynomials to have integer coefficients. We instead propose a degree-2 polynomial activation function with a",
    "link": "https://arxiv.org/abs/2011.05530",
    "context": "Title: On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU Networks\nAbstract: Outsourcing deep neural networks (DNNs) inference tasks to an untrusted cloud raises data privacy and integrity concerns. While there are many techniques to ensure privacy and integrity for polynomial-based computations, DNNs involve non-polynomial computations. To address these challenges, several privacy-preserving and verifiable inference techniques have been proposed based on replacing the non-polynomial activation functions such as the rectified linear unit (ReLU) function with polynomial activation functions. Such techniques usually require polynomials with integer coefficients or polynomials over finite fields. Motivated by such requirements, several works proposed replacing the ReLU function with the square function. In this work, we empirically show that the square function is not the best degree-2 polynomial that can replace the ReLU function even when restricting the polynomials to have integer coefficients. We instead propose a degree-2 polynomial activation function with a",
    "path": "papers/20/11/2011.05530.json",
    "total_tokens": 808,
    "translated_title": "关于隐私保护和可验证ReLU网络的多项式逼近研究",
    "translated_abstract": "将深度神经网络（DNNs）的推理任务外包给不受信任的云，引发了数据隐私和完整性的担忧。虽然有许多技术可以确保基于多项式计算的隐私和完整性，但DNNs涉及非多项式计算。为了解决这些挑战，已经提出了几种基于将非多项式激活函数（如ReLU函数）替换为多项式激活函数的隐私保护和可验证推理技术。这些技术通常需要具有整数系数或有限域上的多项式。在这项工作中，我们通过实验证明，即使将多项式限制为具有整数系数，平方函数也不是能替代ReLU函数的最佳二次多项式。相反，我们提出了一个具有",
    "tldr": "本研究探讨了多项式逼近隐私保护和可验证ReLU网络的方法。通过实验证明，平方函数并不是最佳替代ReLU函数的二次多项式。我们提出了一个具有"
}