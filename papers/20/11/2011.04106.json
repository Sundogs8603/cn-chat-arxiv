{
    "title": "Ensemble Knowledge Distillation for CTR Prediction. (arXiv:2011.04106v2 [cs.LG] UPDATED)",
    "abstract": "Recently, deep learning-based models have been widely studied for click-through rate (CTR) prediction and lead to improved prediction accuracy in many industrial applications. However, current research focuses primarily on building complex network architectures to better capture sophisticated feature interactions and dynamic user behaviors. The increased model complexity may slow down online inference and hinder its adoption in real-time applications. Instead, our work targets at a new model training strategy based on knowledge distillation (KD). KD is a teacher-student learning framework to transfer knowledge learned from a teacher model to a student model. The KD strategy not only allows us to simplify the student model as a vanilla DNN model but also achieves significant accuracy improvements over the state-of-the-art teacher models. The benefits thus motivate us to further explore the use of a powerful ensemble of teachers for more accurate student model training. We also propose s",
    "link": "http://arxiv.org/abs/2011.04106",
    "context": "Title: Ensemble Knowledge Distillation for CTR Prediction. (arXiv:2011.04106v2 [cs.LG] UPDATED)\nAbstract: Recently, deep learning-based models have been widely studied for click-through rate (CTR) prediction and lead to improved prediction accuracy in many industrial applications. However, current research focuses primarily on building complex network architectures to better capture sophisticated feature interactions and dynamic user behaviors. The increased model complexity may slow down online inference and hinder its adoption in real-time applications. Instead, our work targets at a new model training strategy based on knowledge distillation (KD). KD is a teacher-student learning framework to transfer knowledge learned from a teacher model to a student model. The KD strategy not only allows us to simplify the student model as a vanilla DNN model but also achieves significant accuracy improvements over the state-of-the-art teacher models. The benefits thus motivate us to further explore the use of a powerful ensemble of teachers for more accurate student model training. We also propose s",
    "path": "papers/20/11/2011.04106.json",
    "total_tokens": 896,
    "translated_title": "集成知识蒸馏用于CTR预测",
    "translated_abstract": "最近，基于深度学习的模型在点击率（CTR）预测方面得到了广泛的研究，并在许多工业应用中提高了预测准确性。然而，目前的研究主要集中在构建复杂的网络架构来更好地捕捉复杂的特征交互和动态用户行为。增加的模型复杂性可能会减慢在线推断速度，并阻碍其在实时应用中的应用。相反，我们的工作针对的是一种基于知识蒸馏（KD）的新模型训练策略。KD是一种将来自教师模型的知识转移给学生模型的教师-学生学习框架。知识蒸馏策略不仅允许我们将学生模型简化为传统的神经网络模型，而且还在准确性上显著提高了超越最先进的教师模型。这些优点促使我们进一步探索使用强大的教师集合来进行更准确的学生模型训练。",
    "tldr": "本论文针对CTR预测提出了一种集成知识蒸馏的模型训练策略，该策略能够通过教师模型将知识传输给学生模型，并取得了显著的准确性提升。",
    "en_tdlr": "This paper proposes an ensemble knowledge distillation approach for CTR prediction, which transfers knowledge from teacher models to a student model and achieves significant accuracy improvements."
}