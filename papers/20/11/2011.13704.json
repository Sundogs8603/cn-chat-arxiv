{
    "title": "Direct Evolutionary Optimization of Variational Autoencoders With Binary Latents. (arXiv:2011.13704v2 [stat.ML] UPDATED)",
    "abstract": "Discrete latent variables are considered important for real world data, which has motivated research on Variational Autoencoders (VAEs) with discrete latents. However, standard VAE training is not possible in this case, which has motivated different strategies to manipulate discrete distributions in order to train discrete VAEs similarly to conventional ones. Here we ask if it is also possible to keep the discrete nature of the latents fully intact by applying a direct discrete optimization for the encoding model. The approach is consequently strongly diverting from standard VAE-training by sidestepping sampling approximation, reparameterization trick and amortization. Discrete optimization is realized in a variational setting using truncated posteriors in conjunction with evolutionary algorithms. For VAEs with binary latents, we (A) show how such a discrete variational method ties into gradient ascent for network weights, and (B) how the decoder is used to select latent states for tra",
    "link": "http://arxiv.org/abs/2011.13704",
    "context": "Title: Direct Evolutionary Optimization of Variational Autoencoders With Binary Latents. (arXiv:2011.13704v2 [stat.ML] UPDATED)\nAbstract: Discrete latent variables are considered important for real world data, which has motivated research on Variational Autoencoders (VAEs) with discrete latents. However, standard VAE training is not possible in this case, which has motivated different strategies to manipulate discrete distributions in order to train discrete VAEs similarly to conventional ones. Here we ask if it is also possible to keep the discrete nature of the latents fully intact by applying a direct discrete optimization for the encoding model. The approach is consequently strongly diverting from standard VAE-training by sidestepping sampling approximation, reparameterization trick and amortization. Discrete optimization is realized in a variational setting using truncated posteriors in conjunction with evolutionary algorithms. For VAEs with binary latents, we (A) show how such a discrete variational method ties into gradient ascent for network weights, and (B) how the decoder is used to select latent states for tra",
    "path": "papers/20/11/2011.13704.json",
    "total_tokens": 954,
    "tldr": "本文针对离散潜变量的变分自编码器，通过直接离散优化方法完全保持潜变量的离散性质，弥补了常规VAE训练无法满足的需求。",
    "en_tdlr": "This paper proposes a direct discrete optimization method for variational autoencoders with discrete latent variables, which preserves the discrete nature of the latents and overcomes the limitations of standard VAE training. It uses a variational setting with truncated posteriors and evolutionary algorithms for discrete optimization of VAEs with binary latents."
}