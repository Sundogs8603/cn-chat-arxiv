{
    "title": "On Frequentist Regret of Linear Thompson Sampling. (arXiv:2006.06790v3 [cs.LG] UPDATED)",
    "abstract": "This paper studies the stochastic linear bandit problem, where a decision-maker chooses actions from possibly time-dependent sets of vectors in $\\mathbb{R}^d$ and receives noisy rewards. The objective is to minimize regret, the difference between the cumulative expected reward of the decision-maker and that of an oracle with access to the expected reward of each action, over a sequence of $T$ decisions. Linear Thompson Sampling (LinTS) is a popular Bayesian heuristic, supported by theoretical analysis that shows its Bayesian regret is bounded by $\\widetilde{\\mathcal{O}}(d\\sqrt{T})$, matching minimax lower bounds. However, previous studies demonstrate that the frequentist regret bound for LinTS is $\\widetilde{\\mathcal{O}}(d\\sqrt{dT})$, which requires posterior variance inflation and is by a factor of $\\sqrt{d}$ worse than the best optimism-based algorithms. We prove that this inflation is fundamental and that the frequentist bound of $\\widetilde{\\mathcal{O}}(d\\sqrt{dT})$ is the best pos",
    "link": "http://arxiv.org/abs/2006.06790",
    "context": "Title: On Frequentist Regret of Linear Thompson Sampling. (arXiv:2006.06790v3 [cs.LG] UPDATED)\nAbstract: This paper studies the stochastic linear bandit problem, where a decision-maker chooses actions from possibly time-dependent sets of vectors in $\\mathbb{R}^d$ and receives noisy rewards. The objective is to minimize regret, the difference between the cumulative expected reward of the decision-maker and that of an oracle with access to the expected reward of each action, over a sequence of $T$ decisions. Linear Thompson Sampling (LinTS) is a popular Bayesian heuristic, supported by theoretical analysis that shows its Bayesian regret is bounded by $\\widetilde{\\mathcal{O}}(d\\sqrt{T})$, matching minimax lower bounds. However, previous studies demonstrate that the frequentist regret bound for LinTS is $\\widetilde{\\mathcal{O}}(d\\sqrt{dT})$, which requires posterior variance inflation and is by a factor of $\\sqrt{d}$ worse than the best optimism-based algorithms. We prove that this inflation is fundamental and that the frequentist bound of $\\widetilde{\\mathcal{O}}(d\\sqrt{dT})$ is the best pos",
    "path": "papers/20/06/2006.06790.json",
    "total_tokens": 992,
    "translated_title": "关于线性汤普森抽样的频率后悔问题",
    "translated_abstract": "本文研究随机线性赌博机问题，其中决策者从可能时变的$\\mathbb{R}^d$向量集中选择行动并获得噪声奖励。目标是在一系列$T$个决策中最小化后悔，即决策者的累积预期奖励与能够访问每个行动预期奖励的神谕之间的差异。线性汤普森抽样(LinTS)是一种流行的贝叶斯启发式算法，通过理论分析表明其贝叶斯后悔受到$\\widetilde{\\mathcal{O}}(d\\sqrt{T})$的界限约束，达到极小值下限。然而，先前的研究表明，LinTS的频率后悔界限为$\\widetilde{\\mathcal{O}}(d\\sqrt{dT})$，需要后验方差膨胀，并且比最佳基于乐观主义的算法差一个$\\sqrt{d}$的因子。我们证明了这种膨胀是基本的，并且频率界限为$\\widetilde{\\mathcal{O}}(d\\sqrt{dT})$是最佳的。",
    "tldr": "本文研究了线性汤普森抽样的频率后悔问题，证明了后验方差膨胀是必需的，并确定了频率后悔的最低下限为$\\widetilde{\\mathcal{O}}(d\\sqrt{dT})$ 。",
    "en_tdlr": "This paper addresses the frequentist regret problem of linear Thompson sampling for stochastic linear bandit problem, proving the necessity of posterior variance inflation and setting the lower bound for the frequentist regret as $\\widetilde{\\mathcal{O}}(d\\sqrt{dT})$."
}