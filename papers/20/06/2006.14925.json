{
    "title": "Does the $\\ell_1$-norm Learn a Sparse Graph under Laplacian Constrained Graphical Models?. (arXiv:2006.14925v2 [cs.LG] UPDATED)",
    "abstract": "We consider the problem of learning a sparse graph under the Laplacian constrained Gaussian graphical models. This problem can be formulated as a penalized maximum likelihood estimation of the Laplacian constrained precision matrix. Like in the classical graphical lasso problem, recent works made use of the $\\ell_1$-norm regularization with the goal of promoting sparsity in Laplacian constrained precision matrix estimation. However, we find that the widely used $\\ell_1$-norm is not effective in imposing a sparse solution in this problem. Through empirical evidence, we observe that the number of nonzero graph weights grows with the increase of the regularization parameter. From a theoretical perspective, we prove that a large regularization parameter will surprisingly lead to a complete graph, i.e., every pair of vertices is connected by an edge. To address this issue, we introduce the nonconvex sparsity penalty, and propose a new estimator by solving a sequence of weighted $\\ell_1$-nor",
    "link": "http://arxiv.org/abs/2006.14925",
    "context": "Title: Does the $\\ell_1$-norm Learn a Sparse Graph under Laplacian Constrained Graphical Models?. (arXiv:2006.14925v2 [cs.LG] UPDATED)\nAbstract: We consider the problem of learning a sparse graph under the Laplacian constrained Gaussian graphical models. This problem can be formulated as a penalized maximum likelihood estimation of the Laplacian constrained precision matrix. Like in the classical graphical lasso problem, recent works made use of the $\\ell_1$-norm regularization with the goal of promoting sparsity in Laplacian constrained precision matrix estimation. However, we find that the widely used $\\ell_1$-norm is not effective in imposing a sparse solution in this problem. Through empirical evidence, we observe that the number of nonzero graph weights grows with the increase of the regularization parameter. From a theoretical perspective, we prove that a large regularization parameter will surprisingly lead to a complete graph, i.e., every pair of vertices is connected by an edge. To address this issue, we introduce the nonconvex sparsity penalty, and propose a new estimator by solving a sequence of weighted $\\ell_1$-nor",
    "path": "papers/20/06/2006.14925.json",
    "total_tokens": 966,
    "translated_title": "$\\ell_1$-范数是否能够在受限Laplacian图模型下学习稀疏图形？",
    "translated_abstract": "我们考虑在受限Laplacian高斯图模型下学习稀疏图的问题。该问题可以被表示为拉普拉斯约束下的精度矩阵的惩罚最大似然估计。与经典的图形套索问题类似，最近的研究利用了$\\ell_1$-范数正则化来促进在拉普拉斯约束精度矩阵估计中的稀疏性。然而，我们发现广泛应用的$\\ell_1$-范数在这个问题中无法有效地实现稀疏解。通过经验证据，我们观察到非零图权重的数量随着正则化参数的增加而增加。从理论上来看，我们证明了较大的正则化参数将引发一个意外的完全图，即每对顶点之间都用边连接。为了解决这个问题，我们引入非凸稀疏惩罚，并通过求解一系列加权$\\ell_1$-范数得到了一个新的估计器。",
    "tldr": "本文研究了在受限Laplacian图模型下学习稀疏图的问题。我们发现经典的$\\ell_1$-范数正则化无法有效实现稀疏解，并提出了一种非凸稀疏惩罚的方法来解决这个问题。",
    "en_tdlr": "This paper investigates the problem of learning a sparse graph under the Laplacian constrained graphical models. The widely used $\\ell_1$-norm regularization is found to be ineffective in promoting sparsity in this problem. Instead, a nonconvex sparsity penalty is introduced to address this issue."
}