{
    "title": "Fast Objective & Duality Gap Convergence for Non-Convex Strongly-Concave Min-Max Problems with PL Condition. (arXiv:2006.06889v8 [cs.LG] UPDATED)",
    "abstract": "This paper focuses on stochastic methods for solving smooth non-convex strongly-concave min-max problems, which have received increasing attention due to their potential applications in deep learning (e.g., deep AUC maximization, distributionally robust optimization). However, most of the existing algorithms are slow in practice, and their analysis revolves around the convergence to a nearly stationary point.We consider leveraging the Polyak-Lojasiewicz (PL) condition to design faster stochastic algorithms with stronger convergence guarantee. Although PL condition has been utilized for designing many stochastic minimization algorithms, their applications for non-convex min-max optimization remain rare. In this paper, we propose and analyze a generic framework of proximal stage-based method with many well-known stochastic updates embeddable. Fast convergence is established in terms of both the primal objective gap and the duality gap. Compared with existing studies, (i) our analysis is ",
    "link": "http://arxiv.org/abs/2006.06889",
    "context": "Title: Fast Objective & Duality Gap Convergence for Non-Convex Strongly-Concave Min-Max Problems with PL Condition. (arXiv:2006.06889v8 [cs.LG] UPDATED)\nAbstract: This paper focuses on stochastic methods for solving smooth non-convex strongly-concave min-max problems, which have received increasing attention due to their potential applications in deep learning (e.g., deep AUC maximization, distributionally robust optimization). However, most of the existing algorithms are slow in practice, and their analysis revolves around the convergence to a nearly stationary point.We consider leveraging the Polyak-Lojasiewicz (PL) condition to design faster stochastic algorithms with stronger convergence guarantee. Although PL condition has been utilized for designing many stochastic minimization algorithms, their applications for non-convex min-max optimization remain rare. In this paper, we propose and analyze a generic framework of proximal stage-based method with many well-known stochastic updates embeddable. Fast convergence is established in terms of both the primal objective gap and the duality gap. Compared with existing studies, (i) our analysis is ",
    "path": "papers/20/06/2006.06889.json",
    "total_tokens": 983,
    "translated_title": "快速面向具有PL条件的非凸强凸min-max问题的目标和对偶差收敛(arXiv:2006.06889v8 [cs.LG] UPDATED)",
    "translated_abstract": "本文着重于解决平滑非凸强凸min-max问题的随机方法，该问题由于其在深度学习中的潜在应用（如深度AUC最大化，分布式鲁棒优化）而受到越来越多的关注。然而，大多数现有算法在实践中较慢，并且它们的分析围绕收敛于接近稳态点展开。我们考虑利用Polyak-Lojasiewicz（PL）条件来设计更快的随机算法，并提供更强的收敛保证。虽然PL条件已经被用于设计许多随机最小化算法，但它们对于非凸极小化最大化优化的应用仍然很少。在本文中，我们提出并分析了一个泛化的基于近端阶段的方法框架，其中嵌入了许多众所周知的随机更新。我们建立了基于原始目标间隙和对偶间隙的快速收敛性。与现有研究相比，（i）我们的分析是...",
    "tldr": "该论文探讨了解决深度学习中出现的一类非凸强凸min-max问题的随机方法，并提出了一个基于近端阶段的方法框架，其中嵌入了许多众所周知的随机更新，快速收敛性得到了建立。",
    "en_tdlr": "This paper proposes stochastic methods for solving smooth non-convex strongly-concave min-max problems, with potential applications in deep learning. A proximal stage-based method with well-known stochastic updates is proposed with fast convergence established in terms of both the primal objective gap and the duality gap."
}