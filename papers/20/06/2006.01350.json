{
    "title": "On the Estimation of Derivatives Using Plug-in Kernel Ridge Regression Estimators. (arXiv:2006.01350v4 [stat.ML] UPDATED)",
    "abstract": "We study the problem of estimating the derivatives of a regression function, which has a wide range of applications as a key nonparametric functional of unknown functions. Standard analysis may be tailored to specific derivative orders, and parameter tuning remains a daunting challenge particularly for high-order derivatives. In this article, we propose a simple plug-in kernel ridge regression (KRR) estimator in nonparametric regression with random design that is broadly applicable for multi-dimensional support and arbitrary mixed-partial derivatives. We provide a non-asymptotic analysis to study the behavior of the proposed estimator in a unified manner that encompasses the regression function and its derivatives, leading to two error bounds for a general class of kernels under the strong $L_\\infty$ norm. In a concrete example specialized to kernels with polynomially decaying eigenvalues, the proposed estimator recovers the minimax optimal rate up to a logarithmic factor for estimatin",
    "link": "http://arxiv.org/abs/2006.01350",
    "context": "Title: On the Estimation of Derivatives Using Plug-in Kernel Ridge Regression Estimators. (arXiv:2006.01350v4 [stat.ML] UPDATED)\nAbstract: We study the problem of estimating the derivatives of a regression function, which has a wide range of applications as a key nonparametric functional of unknown functions. Standard analysis may be tailored to specific derivative orders, and parameter tuning remains a daunting challenge particularly for high-order derivatives. In this article, we propose a simple plug-in kernel ridge regression (KRR) estimator in nonparametric regression with random design that is broadly applicable for multi-dimensional support and arbitrary mixed-partial derivatives. We provide a non-asymptotic analysis to study the behavior of the proposed estimator in a unified manner that encompasses the regression function and its derivatives, leading to two error bounds for a general class of kernels under the strong $L_\\infty$ norm. In a concrete example specialized to kernels with polynomially decaying eigenvalues, the proposed estimator recovers the minimax optimal rate up to a logarithmic factor for estimatin",
    "path": "papers/20/06/2006.01350.json",
    "total_tokens": 881,
    "translated_title": "使用插值核岭回归估计导数的研究",
    "translated_abstract": "我们研究了对回归函数的导数进行估计的问题，这在未知函数的非参数化功能中具有广泛的应用。标准的分析可能针对特定的导数阶数进行调整，而参数调优特别是对于高阶导数来说仍然是一个困难的挑战。在本文中，我们提出了一种简单的插值核岭回归（KRR）估计器，用于具有随机设计的非参数回归，广泛适用于多维支持和任意混合偏导数。我们提供了非渐近分析，以统一地研究所提出的估计器的行为，包括回归函数及其导数，在强L∞范数下导致了一个一般类的核函数的两个误差界。在一个具体的例子中，该估计器专门针对具有多项式衰减特征值的核函数，实现了最小化的最优速率，只有一个对数因子可估计",
    "tldr": "本文提出了一种插值核岭回归（KRR）估计器，可广泛适用于非参数回归中的多维支持和任意混合偏导数，并且具有较强的误差界。",
    "en_tdlr": "This article proposes a plug-in kernel ridge regression (KRR) estimator that is broadly applicable for multi-dimensional support and arbitrary mixed-partial derivatives in nonparametric regression, and provides strong error bounds."
}