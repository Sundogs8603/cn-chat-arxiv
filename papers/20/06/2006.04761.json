{
    "title": "Can Temporal-Difference and Q-Learning Learn Representation? A Mean-Field Theory",
    "abstract": "arXiv:2006.04761v2 Announce Type: replace  Abstract: Temporal-difference and Q-learning play a key role in deep reinforcement learning, where they are empowered by expressive nonlinear function approximators such as neural networks. At the core of their empirical successes is the learned feature representation, which embeds rich observations, e.g., images and texts, into the latent space that encodes semantic structures. Meanwhile, the evolution of such a feature representation is crucial to the convergence of temporal-difference and Q-learning.   In particular, temporal-difference learning converges when the function approximator is linear in a feature representation, which is fixed throughout learning, and possibly diverges otherwise. We aim to answer the following questions: When the function approximator is a neural network, how does the associated feature representation evolve? If it converges, does it converge to the optimal one?   We prove that, utilizing an overparameterized tw",
    "link": "https://arxiv.org/abs/2006.04761",
    "context": "Title: Can Temporal-Difference and Q-Learning Learn Representation? A Mean-Field Theory\nAbstract: arXiv:2006.04761v2 Announce Type: replace  Abstract: Temporal-difference and Q-learning play a key role in deep reinforcement learning, where they are empowered by expressive nonlinear function approximators such as neural networks. At the core of their empirical successes is the learned feature representation, which embeds rich observations, e.g., images and texts, into the latent space that encodes semantic structures. Meanwhile, the evolution of such a feature representation is crucial to the convergence of temporal-difference and Q-learning.   In particular, temporal-difference learning converges when the function approximator is linear in a feature representation, which is fixed throughout learning, and possibly diverges otherwise. We aim to answer the following questions: When the function approximator is a neural network, how does the associated feature representation evolve? If it converges, does it converge to the optimal one?   We prove that, utilizing an overparameterized tw",
    "path": "papers/20/06/2006.04761.json",
    "total_tokens": 866,
    "translated_title": "离散时间差分和Q学习能学得特征表示吗？一种平均场理论",
    "translated_abstract": "离散时间差分和Q学习在深度强化学习中发挥关键作用，它们利用神经网络等表达力非线性函数逼近器。它们的实证成功的核心是学得的特征表示，将丰富的观测，如图像和文本，嵌入到编码语义结构的潜在空间中。同时，这种特征表示的演变对离散时间差分学习和Q学习的收敛至关重要。特别地，当函数逼近器在特征表示中是线性的且在整个学习过程中保持不变时，离散时间差分学习会收敛，否则可能发散。我们的目标是回答以下问题：当函数逼近器是神经网络时，相关的特征表示如何演进？如果它收敛，它是否收敛至最优的特征表示？",
    "tldr": "研究探讨离散时间差分学习和Q学习在深度强化学习中的特征表示演变，证明利用过度参数化的方法可以实现这种演变，并关注特征表示对于算法收敛的重要性。",
    "en_tdlr": "Investigating the evolution of feature representation in temporal-difference learning and Q-learning in deep reinforcement learning, proving the possibility of this evolution using overparameterization, and emphasizing the significance of feature representation for algorithm convergence."
}