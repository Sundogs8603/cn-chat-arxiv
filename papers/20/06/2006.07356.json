{
    "title": "Implicit Bias of Gradient Descent for Mean Squared Error Regression with Two-Layer Wide Neural Networks. (arXiv:2006.07356v4 [stat.ML] UPDATED)",
    "abstract": "We investigate gradient descent training of wide neural networks and the corresponding implicit bias in function space. For univariate regression, we show that the solution of training a width-$n$ shallow ReLU network is within $n^{- 1/2}$ of the function which fits the training data and whose difference from the initial function has the smallest 2-norm of the second derivative weighted by a curvature penalty that depends on the probability distribution that is used to initialize the network parameters. We compute the curvature penalty function explicitly for various common initialization procedures. For instance, asymmetric initialization with a uniform distribution yields a constant curvature penalty, and thence the solution function is the natural cubic spline interpolation of the training data. \\hj{For stochastic gradient descent we obtain the same implicit bias result.} We obtain a similar result for different activation functions. For multivariate regression we show an analogous ",
    "link": "http://arxiv.org/abs/2006.07356",
    "context": "Title: Implicit Bias of Gradient Descent for Mean Squared Error Regression with Two-Layer Wide Neural Networks. (arXiv:2006.07356v4 [stat.ML] UPDATED)\nAbstract: We investigate gradient descent training of wide neural networks and the corresponding implicit bias in function space. For univariate regression, we show that the solution of training a width-$n$ shallow ReLU network is within $n^{- 1/2}$ of the function which fits the training data and whose difference from the initial function has the smallest 2-norm of the second derivative weighted by a curvature penalty that depends on the probability distribution that is used to initialize the network parameters. We compute the curvature penalty function explicitly for various common initialization procedures. For instance, asymmetric initialization with a uniform distribution yields a constant curvature penalty, and thence the solution function is the natural cubic spline interpolation of the training data. \\hj{For stochastic gradient descent we obtain the same implicit bias result.} We obtain a similar result for different activation functions. For multivariate regression we show an analogous ",
    "path": "papers/20/06/2006.07356.json",
    "total_tokens": 1014,
    "tldr": "本文研究了宽神经网络的梯度下降训练的隐含偏差，在一元回归问题中，解与拟合训练数据的函数之间的差距在$n^{-1/2}$范围内，并且该函数与初始函数的差距的二范数带有依赖于用于初始化网络参数的概率分布的曲率惩罚项最小化。",
    "en_tdlr": "This paper investigates the implicit bias of gradient descent training for wide neural networks in mean squared error regression problems. Specifically, for univariate regression, the solution obtained through training a width-$n$ shallow ReLU network is within $n^{-1/2}$ range from the function that fits the training data, and its difference from the initial function has the smallest 2-norm of the second derivative weighted by a curvature penalty that depends on the distribution used to initialize the network parameters."
}