{
    "title": "Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning. (arXiv:2006.16785v4 [cs.LG] UPDATED)",
    "abstract": "Despite the recent success of reinforcement learning in various domains, these approaches remain, for the most part, deterringly sensitive to hyper-parameters and are often riddled with essential engineering feats allowing their success. We consider the case of off-policy generative adversarial imitation learning, and perform an in-depth review, qualitative and quantitative, of the method. We show that forcing the learned reward function to be local Lipschitz-continuous is a sine qua non condition for the method to perform well. We then study the effects of this necessary condition and provide several theoretical results involving the local Lipschitzness of the state-value function. We complement these guarantees with empirical evidence attesting to the strong positive effect that the consistent satisfaction of the Lipschitzness constraint on the reward has on imitation performance. Finally, we tackle a generic pessimistic reward preconditioning add-on spawning a large class of reward ",
    "link": "http://arxiv.org/abs/2006.16785",
    "context": "Title: Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning. (arXiv:2006.16785v4 [cs.LG] UPDATED)\nAbstract: Despite the recent success of reinforcement learning in various domains, these approaches remain, for the most part, deterringly sensitive to hyper-parameters and are often riddled with essential engineering feats allowing their success. We consider the case of off-policy generative adversarial imitation learning, and perform an in-depth review, qualitative and quantitative, of the method. We show that forcing the learned reward function to be local Lipschitz-continuous is a sine qua non condition for the method to perform well. We then study the effects of this necessary condition and provide several theoretical results involving the local Lipschitzness of the state-value function. We complement these guarantees with empirical evidence attesting to the strong positive effect that the consistent satisfaction of the Lipschitzness constraint on the reward has on imitation performance. Finally, we tackle a generic pessimistic reward preconditioning add-on spawning a large class of reward ",
    "path": "papers/20/06/2006.16785.json",
    "total_tokens": 919,
    "translated_title": "唯有利普希茨性能够驯服离线生成对抗模仿学习",
    "translated_abstract": "尽管强化学习在各个领域取得了最近的成功，但是这些方法大多对超参数敏感，并且通常需要进行一些关键的工程操作才能取得成功。我们考虑了离线生成对抗模仿学习的情况，并对该方法进行了深入的定性和定量分析。我们证明，将学习到的奖励函数强制变成局部利普希茨连续是该方法表现良好的必要条件。然后，我们研究了这个必要条件的影响，并提供了涉及状态-价值函数的局部利普希茨性质的几个理论结果。我们通过实证证据证明，奖励的利普希茨性约束的一致满足对模仿性能具有极强的积极影响。最后，我们探讨了一个通用的悲观奖励预处理附加项，形成了一个大类的奖励函数。",
    "tldr": "离线生成对抗模仿学习中，将学习到的奖励函数强制变成局部利普希茨连续是取得良好表现的必要条件，并且满足奖励的利普希茨性约束对模仿性能具有积极影响。"
}