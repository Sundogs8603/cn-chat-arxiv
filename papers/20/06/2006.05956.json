{
    "title": "Gradient Flows for Regularized Stochastic Control Problems. (arXiv:2006.05956v5 [math.OC] UPDATED)",
    "abstract": "This paper studies stochastic control problems with the action space taken to be probability measures, with the objective penalised by the relative entropy. We identify suitable metric space on which we construct a gradient flow for the measure-valued control process, in the set of admissible controls, along which the cost functional is guaranteed to decrease. It is shown that any invariant measure of this gradient flow satisfies the Pontryagin optimality principle. If the problem we work with is sufficiently convex, the gradient flow converges exponentially fast. Furthermore, the optimal measure-valued control process admits a Bayesian interpretation which means that one can incorporate prior knowledge when solving such stochastic control problems. This work is motivated by a desire to extend the theoretical underpinning for the convergence of stochastic gradient type algorithms widely employed in the reinforcement learning community to solve control problems.",
    "link": "http://arxiv.org/abs/2006.05956",
    "context": "Title: Gradient Flows for Regularized Stochastic Control Problems. (arXiv:2006.05956v5 [math.OC] UPDATED)\nAbstract: This paper studies stochastic control problems with the action space taken to be probability measures, with the objective penalised by the relative entropy. We identify suitable metric space on which we construct a gradient flow for the measure-valued control process, in the set of admissible controls, along which the cost functional is guaranteed to decrease. It is shown that any invariant measure of this gradient flow satisfies the Pontryagin optimality principle. If the problem we work with is sufficiently convex, the gradient flow converges exponentially fast. Furthermore, the optimal measure-valued control process admits a Bayesian interpretation which means that one can incorporate prior knowledge when solving such stochastic control problems. This work is motivated by a desire to extend the theoretical underpinning for the convergence of stochastic gradient type algorithms widely employed in the reinforcement learning community to solve control problems.",
    "path": "papers/20/06/2006.05956.json",
    "total_tokens": 998,
    "translated_title": "渐变流用于正则化随机控制问题",
    "translated_abstract": "本文研究的是将行动空间设定为概率测度、目标函数通过相对熵惩罚的随机控制问题。我们确定了适合的度量空间，构建了对测度值控制过程的渐变流，它在可行控制集中保证了成本函数的递减。我们证明了该渐变流的任何不变测度都满足庞特里亚金最优性原理。如果所处理的问题是足够凸的，渐变流将指数级收敛。此外，最优测度值控制过程具有贝叶斯解释，这意味着在解决此类随机控制问题时可以融入先验知识。本工作的动机是为了扩展用于解决控制问题的增强学习社区广泛采用的随机梯度类型算法的收敛的理论基础。",
    "tldr": "本文研究了正则化随机控制问题中渐变流的应用，找到了适合的度量空间，在可行控制集中构建了渐变流，使得成本函数递减，证明了渐变流的不变测度满足最优性原则，且指数级收敛。此外，最优测度值控制过程具有贝叶斯解释，可以融入先验知识。本研究旨在扩展增强学习中随机梯度算法用于解决控制问题的理论基础。"
}