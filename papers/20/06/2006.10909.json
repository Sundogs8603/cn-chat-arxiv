{
    "title": "Neural Topic Modeling with Continual Lifelong Learning. (arXiv:2006.10909v2 [cs.CL] UPDATED)",
    "abstract": "Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel sele",
    "link": "http://arxiv.org/abs/2006.10909",
    "context": "Title: Neural Topic Modeling with Continual Lifelong Learning. (arXiv:2006.10909v2 [cs.CL] UPDATED)\nAbstract: Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel sele",
    "path": "papers/20/06/2006.10909.json",
    "total_tokens": 872,
    "translated_title": "具有持续终身学习的神经主题建模",
    "translated_abstract": "终身学习吸引了人们对建立机器学习系统的关注，这些系统可以不断积累和传递知识，以帮助未来的学习。无监督主题建模广泛用于从文档集合中发现主题。然而，由于数据稀疏性，例如在一个小的（短）文档集合中，主题建模的应用具有挑战性，从而产生不连贯的主题和次优的文档表示。为了解决这个问题，我们提出了一个神经主题建模的终身学习框架，可以持续处理文档集合的流，积累主题，并通过从多个源的知识转移指导未来的主题建模任务，以更好地处理稀疏数据。在终身过程中，我们特别研究了共享的生成同源性（潜在主题）以在终身中传递先前的知识，以及通过新的选择性遗忘来最小化灾难性遗忘，以保留过去的学习。",
    "tldr": "本研究提出了一个具有持续终身学习的神经主题建模框架，可以处理数据稀疏性，并通过知识的持续积累和转移来提高主题建模的效果。",
    "en_tdlr": "This paper proposes a neural topic modeling framework with continual lifelong learning, which can handle data sparsity and improve the quality of topic modeling through continuous knowledge accumulation and transfer."
}