{
    "title": "On lower bounds for the bias-variance trade-off. (arXiv:2006.00278v4 [math.ST] UPDATED)",
    "abstract": "It is a common phenomenon that for high-dimensional and nonparametric statistical models, rate-optimal estimators balance squared bias and variance. Although this balancing is widely observed, little is known whether methods exist that could avoid the trade-off between bias and variance. We propose a general strategy to obtain lower bounds on the variance of any estimator with bias smaller than a prespecified bound. This shows to which extent the bias-variance trade-off is unavoidable and allows to quantify the loss of performance for methods that do not obey it. The approach is based on a number of abstract lower bounds for the variance involving the change of expectation with respect to different probability measures as well as information measures such as the Kullback-Leibler or $\\chi^2$-divergence. In a second part of the article, the abstract lower bounds are applied to several statistical models including the Gaussian white noise model, a boundary estimation problem, the Gaussian",
    "link": "http://arxiv.org/abs/2006.00278",
    "context": "Title: On lower bounds for the bias-variance trade-off. (arXiv:2006.00278v4 [math.ST] UPDATED)\nAbstract: It is a common phenomenon that for high-dimensional and nonparametric statistical models, rate-optimal estimators balance squared bias and variance. Although this balancing is widely observed, little is known whether methods exist that could avoid the trade-off between bias and variance. We propose a general strategy to obtain lower bounds on the variance of any estimator with bias smaller than a prespecified bound. This shows to which extent the bias-variance trade-off is unavoidable and allows to quantify the loss of performance for methods that do not obey it. The approach is based on a number of abstract lower bounds for the variance involving the change of expectation with respect to different probability measures as well as information measures such as the Kullback-Leibler or $\\chi^2$-divergence. In a second part of the article, the abstract lower bounds are applied to several statistical models including the Gaussian white noise model, a boundary estimation problem, the Gaussian",
    "path": "papers/20/06/2006.00278.json",
    "total_tokens": 1005,
    "translated_title": "论偏差-方差均衡的下限",
    "translated_abstract": "对于高维和非参数统计模型，速率最优估计器通常平衡平方偏差和方差。虽然这种平衡广泛存在，但很少有人知道是否存在可以避免偏差和方差之间的权衡的方法。我们提出了一种通用策略，以获得任何偏差小于预定界限的估计器的方差下限。这表明了偏差-方差权衡不可避免的程度，并允许量化不遵守该权衡的方法的性能损失。该方法基于一些关于方差的抽象下限，涉及到对不同概率测度的期望值的变化以及信息度量，如Kullback-Leibler或$\\chi^2$-分歧。在文章的第二部分中，将这些抽象下限应用于几个统计模型，包括高斯白噪声模型，边界估计问题，高斯简单协方差和均值矩阵估计等问题。",
    "tldr": "研究提出了一种通用策略来获得任何偏差小于预定界限的估计器的方差下限。该方法基于一些关于方差的抽象下限，涉及到对不同概率测度的期望值的变化以及信息度量，如KL或$\\chi^2$分歧。在几个统计模型上进行了应用。",
    "en_tdlr": "The study proposes a general strategy to obtain lower bounds on the variance of any estimator with bias smaller than a prespecified bound. The approach is based on a number of abstract lower bounds for variance involving the change of expectation with respect to different probability measures as well as information measures such as the Kullback-Leibler or $\\chi^2$-divergence. In the second part of the article, the abstract lower bounds are applied to several statistical models including the Gaussian white noise model, a boundary estimation problem, the Gaussian simple covariance and mean matrix estimation."
}