{
    "title": "Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability. (arXiv:2006.04787v2 [cs.LG] UPDATED)",
    "abstract": "In this paper we revisit some classic problems on classification under misspecification. In particular, we study the problem of learning halfspaces under Massart noise with rate $\\eta$. In a recent work, Diakonikolas, Goulekakis, and Tzamos resolved a long-standing problem by giving the first efficient algorithm for learning to accuracy $\\eta + \\epsilon$ for any $\\epsilon > 0$. However, their algorithm outputs a complicated hypothesis, which partitions space into $\\text{poly}(d,1/\\epsilon)$ regions. Here we give a much simpler algorithm and in the process resolve a number of outstanding open questions:  (1) We give the first proper learner for Massart halfspaces that achieves $\\eta + \\epsilon$. We also give improved bounds on the sample complexity achievable by polynomial time algorithms.  (2) Based on (1), we develop a blackbox knowledge distillation procedure to convert an arbitrarily complex classifier to an equally good proper classifier.  (3) By leveraging a simple but overlooked ",
    "link": "http://arxiv.org/abs/2006.04787",
    "context": "Title: Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability. (arXiv:2006.04787v2 [cs.LG] UPDATED)\nAbstract: In this paper we revisit some classic problems on classification under misspecification. In particular, we study the problem of learning halfspaces under Massart noise with rate $\\eta$. In a recent work, Diakonikolas, Goulekakis, and Tzamos resolved a long-standing problem by giving the first efficient algorithm for learning to accuracy $\\eta + \\epsilon$ for any $\\epsilon > 0$. However, their algorithm outputs a complicated hypothesis, which partitions space into $\\text{poly}(d,1/\\epsilon)$ regions. Here we give a much simpler algorithm and in the process resolve a number of outstanding open questions:  (1) We give the first proper learner for Massart halfspaces that achieves $\\eta + \\epsilon$. We also give improved bounds on the sample complexity achievable by polynomial time algorithms.  (2) Based on (1), we develop a blackbox knowledge distillation procedure to convert an arbitrarily complex classifier to an equally good proper classifier.  (3) By leveraging a simple but overlooked ",
    "path": "papers/20/06/2006.04787.json",
    "total_tokens": 1138,
    "translated_title": "在错配情况下的分类：半空间、广义线性模型和与可进化性的联系",
    "translated_abstract": "本文重新审视了一些关于错配情况下的分类的经典问题。特别是，我们研究了在Massart噪声下以速率$\\eta$学习半空间的问题。在最近的一项工作中，Diakonikolas、Goulekakis和Tzamos通过提供第一个有效的算法来解决了一个长期存在的问题，该算法可以学习到准确度$\\eta + \\epsilon$，其中$\\epsilon > 0$。然而，他们的算法输出了一个复杂的假设，将空间分割为$\\text{poly}(d,1/\\epsilon)$个区域。这里我们给出了一个更简单的算法，并在此过程中解决了一些悬而未决的开放问题：(1)我们提供了第一个可以实现$\\eta + \\epsilon$的Massart半空间合适学习器。我们还给出了多项式时间算法可以实现的样本复杂性的改进界限。(2)基于(1)，我们开发了一个黑盒知识蒸馏过程，将任意复杂的分类器转换为同样好的合适的分类器。(3)通过利用一个简单但被忽视的机制，我们在没有任何额外假设的情况下，构造了一个小样本的合适学习算法，并将其与基于矩感知的技术相结合，得到一个具有多项式时间复杂性的学习算法。",
    "tldr": "本文解决了半空间在Massart噪声下的错配学习问题，提出了一个简化的算法并回答了一些开放问题。通过黑盒知识蒸馏过程，将复杂分类器转换为同样好的合适分类器。此外，我们还提出了一个小样本的合适学习算法，并将其与矩感知技术相结合，得到了一个具有多项式时间复杂性的学习算法。"
}