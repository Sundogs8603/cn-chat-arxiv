{
    "title": "Cooperative Multi-Agent Reinforcement Learning with Partial Observations. (arXiv:2006.10822v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we propose a distributed zeroth-order policy optimization method for Multi-Agent Reinforcement Learning (MARL). Existing MARL algorithms often assume that every agent can observe the states and actions of all the other agents in the network. This can be impractical in large-scale problems, where sharing the state and action information with multi-hop neighbors may incur significant communication overhead. The advantage of the proposed zeroth-order policy optimization method is that it allows the agents to compute the local policy gradients needed to update their local policy functions using local estimates of the global accumulated rewards that depend on partial state and action information only and can be obtained using consensus. Specifically, to calculate the local policy gradients, we develop a new distributed zeroth-order policy gradient estimator that relies on one-point residual-feedback which, compared to existing zeroth-order estimators that also rely on one-poi",
    "link": "http://arxiv.org/abs/2006.10822",
    "context": "Title: Cooperative Multi-Agent Reinforcement Learning with Partial Observations. (arXiv:2006.10822v2 [cs.LG] UPDATED)\nAbstract: In this paper, we propose a distributed zeroth-order policy optimization method for Multi-Agent Reinforcement Learning (MARL). Existing MARL algorithms often assume that every agent can observe the states and actions of all the other agents in the network. This can be impractical in large-scale problems, where sharing the state and action information with multi-hop neighbors may incur significant communication overhead. The advantage of the proposed zeroth-order policy optimization method is that it allows the agents to compute the local policy gradients needed to update their local policy functions using local estimates of the global accumulated rewards that depend on partial state and action information only and can be obtained using consensus. Specifically, to calculate the local policy gradients, we develop a new distributed zeroth-order policy gradient estimator that relies on one-point residual-feedback which, compared to existing zeroth-order estimators that also rely on one-poi",
    "path": "papers/20/06/2006.10822.json",
    "total_tokens": 997,
    "translated_title": "部分观测下的协作多智能体强化学习",
    "translated_abstract": "本文提出了一种分布式的零阶策略优化方法，用于多智能体强化学习（MARL）。现有的MARL算法通常假设每个智能体都可以观察网络中所有其他智能体的状态和动作。但在大规模问题中，与多跳邻居共享状态和动作信息可能会导致显着的通信开销。提出的零阶策略优化方法的优势在于，它允许智能体仅基于局部的、部分的状态和动作信息来计算本地策略梯度，从而更新它们的本地策略函数，并使用共识来获得依赖于全局累积奖励的局部估计。具体来说，为了计算本地策略梯度，我们开发了一种新的分布式零阶策略梯度估计器，它依赖于一点残差反馈， im同时与现有的依赖于一点反馈的零阶估计器相比，显著降低了通信开销。我们在几个协作多智能体基准任务上展示了提出方法的有效性，并表明它胜过了现有的假设具有全观察信息的方法。",
    "tldr": "本文提出了一种基于局部状态和动作信息的分布式零阶策略优化方法，可用于部分观测的协作多智能体强化学习，减小通信开销并取得更好的效果。",
    "en_tdlr": "This paper proposes a distributed zeroth-order policy optimization method for cooperative multi-agent reinforcement learning (MARL) with partial observations, which reduces communication overhead and outperforms existing methods that assume full observation."
}