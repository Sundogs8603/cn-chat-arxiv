{
    "title": "Distributionally Robust Batch Contextual Bandits. (arXiv:2006.05630v7 [cs.LG] UPDATED)",
    "abstract": "Policy learning using historical observational data is an important problem that has found widespread applications. Examples include selecting offers, prices, advertisements to send to customers, as well as selecting which medication to prescribe to a patient. However, existing literature rests on the crucial assumption that the future environment where the learned policy will be deployed is the same as the past environment that has generated the data -- an assumption that is often false or too coarse an approximation. In this paper, we lift this assumption and aim to learn a distributionally robust policy with incomplete observational data. We first present a policy evaluation procedure that allows us to assess how well the policy does under the worst-case environment shift. We then establish a central limit theorem type guarantee for this proposed policy evaluation scheme. Leveraging this evaluation scheme, we further propose a novel learning algorithm that is able to learn a policy ",
    "link": "http://arxiv.org/abs/2006.05630",
    "context": "Title: Distributionally Robust Batch Contextual Bandits. (arXiv:2006.05630v7 [cs.LG] UPDATED)\nAbstract: Policy learning using historical observational data is an important problem that has found widespread applications. Examples include selecting offers, prices, advertisements to send to customers, as well as selecting which medication to prescribe to a patient. However, existing literature rests on the crucial assumption that the future environment where the learned policy will be deployed is the same as the past environment that has generated the data -- an assumption that is often false or too coarse an approximation. In this paper, we lift this assumption and aim to learn a distributionally robust policy with incomplete observational data. We first present a policy evaluation procedure that allows us to assess how well the policy does under the worst-case environment shift. We then establish a central limit theorem type guarantee for this proposed policy evaluation scheme. Leveraging this evaluation scheme, we further propose a novel learning algorithm that is able to learn a policy ",
    "path": "papers/20/06/2006.05630.json",
    "total_tokens": 890,
    "translated_title": "分布鲁棒的批次情境强化学习",
    "translated_abstract": "使用历史观察数据进行策略学习是一个重要的问题，已经在广泛的应用中得到应用。例如，选择向客户发送的优惠、价格、广告，以及选择给患者开具哪种药物。然而，现有的文献基于一个关键假设，即学习到的策略将被部署到的未来环境与生成数据的过去环境相同，而这个假设往往是错误的或者过于粗略的近似。在本文中，我们放宽了这个假设，并旨在学习一个具有不完整观察数据的分布鲁棒策略。我们首先提出了一个策略评估过程，以评估策略在最坏情况下的环境转变下的表现。然后，我们建立了这个提出的策略评估方案的中心极限定理类型的保证。利用这个评估方案，我们进一步提出了一种新颖的学习算法，能够学习一个策略。",
    "tldr": "本文提出了一种方法，在不完整的观察数据下学习分布鲁棒的策略，通过引入策略评估过程和中心极限定理类型的保证，实现了针对最坏情况下的环境转变的策略学习。",
    "en_tdlr": "This paper introduces a method to learn distributionally robust policies with incomplete observational data by incorporating a policy evaluation procedure and a central limit theorem guarantee for worst-case environment shifts."
}