{
    "title": "Structure by Architecture: Structured Representations without Regularization",
    "abstract": "arXiv:2006.07796v4 Announce Type: replace  Abstract: We study the problem of self-supervised structured representation learning using autoencoders for downstream tasks such as generative modeling. Unlike most methods which rely on matching an arbitrary, relatively unstructured, prior distribution for sampling, we propose a sampling technique that relies solely on the independence of latent variables, thereby avoiding the trade-off between reconstruction quality and generative performance typically observed in VAEs. We design a novel autoencoder architecture capable of learning a structured representation without the need for aggressive regularization. Our structural decoders learn a hierarchy of latent variables, thereby ordering the information without any additional regularization or supervision. We demonstrate how these models learn a representation that improves results in a variety of downstream tasks including generation, disentanglement, and extrapolation using several challengi",
    "link": "https://arxiv.org/abs/2006.07796",
    "context": "Title: Structure by Architecture: Structured Representations without Regularization\nAbstract: arXiv:2006.07796v4 Announce Type: replace  Abstract: We study the problem of self-supervised structured representation learning using autoencoders for downstream tasks such as generative modeling. Unlike most methods which rely on matching an arbitrary, relatively unstructured, prior distribution for sampling, we propose a sampling technique that relies solely on the independence of latent variables, thereby avoiding the trade-off between reconstruction quality and generative performance typically observed in VAEs. We design a novel autoencoder architecture capable of learning a structured representation without the need for aggressive regularization. Our structural decoders learn a hierarchy of latent variables, thereby ordering the information without any additional regularization or supervision. We demonstrate how these models learn a representation that improves results in a variety of downstream tasks including generation, disentanglement, and extrapolation using several challengi",
    "path": "papers/20/06/2006.07796.json",
    "total_tokens": 912,
    "translated_title": "结构通过架构：无需正则化的结构化表示",
    "translated_abstract": "我们研究了自我监督结构化表示学习的问题，使用自动编码器进行下游任务，如生成模型。与大多数方法依赖于匹配任意的、相对非结构化的先验分布进行采样的情况不同，我们提出了一种仅仅依赖于潜变量的独立性的采样技术，从而避免了在VAE中通常观察到的重构质量和生成性能之间的权衡。我们设计了一种新颖的自动编码器架构，能够学习出一种无需过度正则化的结构化表示。我们的结构解码器学习了一个层次的潜变量，从而无需额外的正则化或监督来对信息进行排序。我们演示了这些模型如何学习出改善各种下游任务的表示，包括生成、解缠和外推，使用了几个具有挑战性的任务。",
    "tldr": "我们提出了一种自我监督的结构化表示学习方法，使用无需正则化的自动编码器架构。通过依赖潜变量的独立性进行采样，我们避免了重构质量和生成性能之间的权衡。我们的模型能够学习出一种有序的结构化表示，改善了生成、解缠和外推等多个下游任务的性能。"
}