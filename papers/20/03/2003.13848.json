{
    "title": "Code Prediction by Feeding Trees to Transformers. (arXiv:2003.13848v4 [cs.SE] CROSS LISTED)",
    "abstract": "We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. First, we report that using the recently proposed Transformer architecture even out-of-the-box outperforms previous neural and non-neural systems for code prediction. We then show that by making the Transformer architecture aware of the syntactic structure of code, we further increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of an RNN-based system (similar to Hellendoorn et al. 2018) by 18.3%, the Deep3 system (Raychev et al 2016) by 14.1%, and an adaptation of Code2Seq (Alon et al., 2018) for code prediction by 14.4%.  We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Pytho",
    "link": "http://arxiv.org/abs/2003.13848",
    "context": "Title: Code Prediction by Feeding Trees to Transformers. (arXiv:2003.13848v4 [cs.SE] CROSS LISTED)\nAbstract: We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. First, we report that using the recently proposed Transformer architecture even out-of-the-box outperforms previous neural and non-neural systems for code prediction. We then show that by making the Transformer architecture aware of the syntactic structure of code, we further increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of an RNN-based system (similar to Hellendoorn et al. 2018) by 18.3%, the Deep3 system (Raychev et al 2016) by 14.1%, and an adaptation of Code2Seq (Alon et al., 2018) for code prediction by 14.4%.  We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Pytho",
    "path": "papers/20/03/2003.13848.json",
    "total_tokens": 685,
    "translated_title": "将树结构输入Transformer进行代码预测",
    "translated_abstract": "我们通过将语法树输入Transformer提高了代码预测（下一个标记预测）的准确度，进一步超越了先前的神经和非神经系统。本文提出了多种将代码结构传达给Transformer的方法，并在标准Python数据集上进行了全面的实验评估。",
    "tldr": "本文使用Transformer架构进行代码预测，超越了先前的模型，通过多种方式将代码结构传达给Transformer，进一步提高了准确度。"
}