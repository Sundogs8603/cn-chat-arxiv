{
    "title": "Communication-Efficient Distributed Deep Learning: A Comprehensive Survey. (arXiv:2003.06307v2 [cs.DC] UPDATED)",
    "abstract": "Distributed deep learning (DL) has become prevalent in recent years to reduce training time by leveraging multiple computing devices (e.g., GPUs/TPUs) due to larger models and datasets. However, system scalability is limited by communication becoming the performance bottleneck. Addressing this communication issue has become a prominent research topic. In this paper, we provide a comprehensive survey of the communication-efficient distributed training algorithms, focusing on both system-level and algorithmic-level optimizations. We first propose a taxonomy of data-parallel distributed training algorithms that incorporates four primary dimensions: communication synchronization, system architectures, compression techniques, and parallelism of communication and computing tasks. We then investigate state-of-the-art studies that address problems in these four dimensions. We also compare the convergence rates of different algorithms to understand their convergence speed. Additionally, we cond",
    "link": "http://arxiv.org/abs/2003.06307",
    "context": "Title: Communication-Efficient Distributed Deep Learning: A Comprehensive Survey. (arXiv:2003.06307v2 [cs.DC] UPDATED)\nAbstract: Distributed deep learning (DL) has become prevalent in recent years to reduce training time by leveraging multiple computing devices (e.g., GPUs/TPUs) due to larger models and datasets. However, system scalability is limited by communication becoming the performance bottleneck. Addressing this communication issue has become a prominent research topic. In this paper, we provide a comprehensive survey of the communication-efficient distributed training algorithms, focusing on both system-level and algorithmic-level optimizations. We first propose a taxonomy of data-parallel distributed training algorithms that incorporates four primary dimensions: communication synchronization, system architectures, compression techniques, and parallelism of communication and computing tasks. We then investigate state-of-the-art studies that address problems in these four dimensions. We also compare the convergence rates of different algorithms to understand their convergence speed. Additionally, we cond",
    "path": "papers/20/03/2003.06307.json",
    "total_tokens": 840,
    "translated_title": "通信高效的分布式深度学习：综述",
    "translated_abstract": "近年来，利用多个计算设备（如GPU/TPU）进行分布式深度学习已经成为减少训练时间的一种普遍方法，这是因为模型和数据集越来越大。然而，由于通信成为性能瓶颈，系统的可扩展性受到限制。解决这个通信问题已经成为一个突出的研究课题。本文提供了对通信高效的分布式训练算法的综述，重点关注系统级和算法级的优化。首先，我们提出了一个将通信同步、系统架构、压缩技术和通信和计算任务并行化等四个主要维度纳入的数据并行分布式训练算法分类方法。然后，我们研究了这四个方面的最新研究，并比较了不同算法的收敛速度以了解其收敛速度。此外，我们还对比了不同算法的收敛速度以了解其收敛速度。",
    "tldr": "本文综述了通信高效的分布式深度学习的研究进展，重点探讨了系统级和算法级的优化方法，并比较了不同算法的收敛速度。"
}