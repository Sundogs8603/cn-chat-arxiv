{
    "title": "RAB: Provable Robustness Against Backdoor Attacks. (arXiv:2003.08904v8 [cs.LG] UPDATED)",
    "abstract": "Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible",
    "link": "http://arxiv.org/abs/2003.08904",
    "context": "Title: RAB: Provable Robustness Against Backdoor Attacks. (arXiv:2003.08904v8 [cs.LG] UPDATED)\nAbstract: Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible",
    "path": "papers/20/03/2003.08904.json",
    "total_tokens": 936,
    "translated_title": "RAB: 可证实抵抗后门攻击的方法",
    "translated_abstract": "最近的研究表明，深度神经网络（DNNs）容易受到对抗性攻击，包括规避攻击和后门（毒化）攻击。在防御方面，对于规避攻击已经进行了密集的改进，包括经验和可证实的鲁棒性；然而，对于后门攻击的可证实鲁棒性仍然很少被探索。本文针对通用威胁模型，特别是后门攻击，提出了一种证实机器学习模型鲁棒性的统一框架，并展示了如何利用随机平滑技术来保证对规避和后门攻击的鲁棒性。我们还提出了首个鲁棒训练过程RAB，使训练模型平滑并保证其对后门攻击的鲁棒性。我们证明了使用RAB训练的机器学习模型的鲁棒性界限，并证明我们的鲁棒性界限是紧密的。此外，我们在理论上展示了实现对后门攻击进行鲁棒性保护是可能的。",
    "tldr": "本文提出了一种证实机器学习模型鲁棒性的统一框架，通过随机平滑技术实现对规避和后门攻击的鲁棒性。同时，我们提出了鲁棒训练过程RAB，并证明其有效性和紧密性。在理论上证明了对后门攻击进行鲁棒性保护的可行性。"
}