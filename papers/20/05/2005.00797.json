{
    "title": "Multi-consensus Decentralized Accelerated Gradient Descent. (arXiv:2005.00797v2 [cs.LG] UPDATED)",
    "abstract": "This paper considers the decentralized convex optimization problem, which has a wide range of applications in large-scale machine learning, sensor networks, and control theory. We propose novel algorithms that achieve optimal computation complexity and near optimal communication complexity. Our theoretical results give affirmative answers to the open problem on whether there exists an algorithm that can achieve a communication complexity (nearly) matching the lower bound depending on the global condition number instead of the local one. Furthermore, the linear convergence of our algorithms only depends on the strong convexity of global objective and it does \\emph{not} require the local functions to be convex. The design of our methods relies on a novel integration of well-known techniques including Nesterov's acceleration, multi-consensus and gradient-tracking. Empirical studies show the outperformance of our methods for machine learning applications.",
    "link": "http://arxiv.org/abs/2005.00797",
    "context": "Title: Multi-consensus Decentralized Accelerated Gradient Descent. (arXiv:2005.00797v2 [cs.LG] UPDATED)\nAbstract: This paper considers the decentralized convex optimization problem, which has a wide range of applications in large-scale machine learning, sensor networks, and control theory. We propose novel algorithms that achieve optimal computation complexity and near optimal communication complexity. Our theoretical results give affirmative answers to the open problem on whether there exists an algorithm that can achieve a communication complexity (nearly) matching the lower bound depending on the global condition number instead of the local one. Furthermore, the linear convergence of our algorithms only depends on the strong convexity of global objective and it does \\emph{not} require the local functions to be convex. The design of our methods relies on a novel integration of well-known techniques including Nesterov's acceleration, multi-consensus and gradient-tracking. Empirical studies show the outperformance of our methods for machine learning applications.",
    "path": "papers/20/05/2005.00797.json",
    "total_tokens": 890,
    "translated_title": "多共识分散加速梯度下降",
    "translated_abstract": "本文考虑了分散凸优化问题，在大规模机器学习、传感器网络和控制理论等领域有广泛应用。我们提出了一种新颖的算法，实现了最优计算复杂度和接近最优的通信复杂度。我们的理论结果对于一个开放问题给出了肯定的答案，即是否存在一种算法可以实现与全局条件数相关而不是局部条件数相关的通信复杂度的(lower bound)相匹配。此外，我们的算法的线性收敛仅取决于全局目标的强凸性，并不要求局部函数是凸函数。我们的方法设计依赖于Nesterov 加速、多共识和梯度追踪等众所周知的技术的创新整合。实证研究显示了我们的方法在机器学习应用中的优越性能。",
    "tldr": "本文提出了一种新颖的多共识分散加速梯度下降算法，实现了最优计算复杂度和接近最优的通信复杂度。算法的线性收敛仅取决于全局目标的强凸性，并不要求局部函数是凸函数。实证研究表明该方法在机器学习应用中超越其他方法。",
    "en_tdlr": "This paper proposes a novel multi-consensus decentralized accelerated gradient descent algorithm that achieves optimal computation complexity and near optimal communication complexity. The linear convergence of the algorithm only depends on the strong convexity of the global objective and does not require the local functions to be convex. Empirical studies show that the method outperforms others in machine learning applications."
}