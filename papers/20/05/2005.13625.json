{
    "title": "Revisiting Parameter Sharing in Multi-Agent Deep Reinforcement Learning. (arXiv:2005.13625v8 [cs.LG] UPDATED)",
    "abstract": "Parameter sharing, where each agent independently learns a policy with fully shared parameters between all policies, is a popular baseline method for multi-agent deep reinforcement learning. Unfortunately, since all agents share the same policy network, they cannot learn different policies or tasks. This issue has been circumvented experimentally by adding an agent-specific indicator signal to observations, which we term \"agent indication\". Agent indication is limited, however, in that without modification it does not allow parameter sharing to be applied to environments where the action spaces and/or observation spaces are heterogeneous. This work formalizes the notion of agent indication and proves that it enables convergence to optimal policies for the first time. Next, we formally introduce methods to extend parameter sharing to learning in heterogeneous observation and action spaces, and prove that these methods allow for convergence to optimal policies. Finally, we experimentally",
    "link": "http://arxiv.org/abs/2005.13625",
    "context": "Title: Revisiting Parameter Sharing in Multi-Agent Deep Reinforcement Learning. (arXiv:2005.13625v8 [cs.LG] UPDATED)\nAbstract: Parameter sharing, where each agent independently learns a policy with fully shared parameters between all policies, is a popular baseline method for multi-agent deep reinforcement learning. Unfortunately, since all agents share the same policy network, they cannot learn different policies or tasks. This issue has been circumvented experimentally by adding an agent-specific indicator signal to observations, which we term \"agent indication\". Agent indication is limited, however, in that without modification it does not allow parameter sharing to be applied to environments where the action spaces and/or observation spaces are heterogeneous. This work formalizes the notion of agent indication and proves that it enables convergence to optimal policies for the first time. Next, we formally introduce methods to extend parameter sharing to learning in heterogeneous observation and action spaces, and prove that these methods allow for convergence to optimal policies. Finally, we experimentally",
    "path": "papers/20/05/2005.13625.json",
    "total_tokens": 937,
    "translated_title": "重访多智能体深度强化学习中的参数共享",
    "translated_abstract": "参数共享是多智能体深度强化学习中一种常用的基准方法，每个智能体都独立学习一个策略，并且所有策略之间共享参数。然而，由于所有智能体共享同一策略网络，它们无法学习不同的策略或任务。为了解决这个问题，我们通过向观测中添加智能体特定的指示信号（称为“智能体指示”）来进行实验性的改进。然而，智能体指示的局限在于，如果不进行修改，它无法应用于行动空间和/或观测空间不同质的环境。本研究正式定义了智能体指示的概念，并证明了它首次实现了收敛到最优策略。接下来，我们正式介绍了扩展参数共享到异构观测和行动空间学习的方法，并证明了这些方法可以实现收敛到最优策略。最后，我们进行了实验验证并对比了各种方法的性能。",
    "tldr": "本研究重访了多智能体深度强化学习中的参数共享方法。我们通过引入智能体指示信号实现了在不同策略网络共享参数的同时学习不同策略或任务的能力，并且证明了这些方法在异构观测和行动空间学习中可以收敛到最优策略。",
    "en_tdlr": "This work revisits the parameter sharing method in multi-agent deep reinforcement learning. We introduce agent indication signals to enable learning of different policies or tasks while sharing parameters between agents. We further extend this method to handle heterogeneous observation and action spaces, and prove its convergence to optimal policies."
}