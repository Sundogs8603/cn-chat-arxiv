{
    "title": "On the Generalization Effects of Linear Transformations in Data Augmentation. (arXiv:2005.00695v3 [cs.LG] UPDATED)",
    "abstract": "Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations that preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations that mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST. Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how uncertain the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms random sampling methods by 1.24% on CIFAR-100 using Wide-ResNet",
    "link": "http://arxiv.org/abs/2005.00695",
    "context": "Title: On the Generalization Effects of Linear Transformations in Data Augmentation. (arXiv:2005.00695v3 [cs.LG] UPDATED)\nAbstract: Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations that preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations that mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST. Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how uncertain the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms random sampling methods by 1.24% on CIFAR-100 using Wide-ResNet",
    "path": "papers/20/05/2005.00695.json",
    "total_tokens": 947,
    "translated_title": "关于数据增强中线性转换的泛化效果的研究",
    "translated_abstract": "数据增强是一种在图像和文本分类任务等应用中提高性能的强大技术。然而，对于各种增强方法为何有效以及其工作原理的严格理解还很有限。在本研究中，我们考虑了一类线性转换，并研究了其在过参数化线性回归设置中对岭估计量的影响。首先，我们通过扩大训练数据的张量来展示了能够保持数据标签的转换会改善估计结果。其次，我们通过混合数据的转换展示了对估计量起到了正则化的作用。最后，我们通过MNIST数据集验证了我们的理论洞见。基于这些洞见，我们提出了一个通过模型对转换后的数据的不确定性来搜索转换空间的增强方案。我们在图像和文本数据集上验证了我们提出的方案。例如，我们的方法在使用Wide-ResNet对CIFAR-100数据集上优于随机采样方法1.24%。",
    "tldr": "这项研究考虑了一类线性转换，并研究了其在过参数化线性回归设置中对岭估计量的影响。研究发现，能够保持数据标签的转换可以通过扩大训练数据的张量来改善估计结果；而混合数据的转换则通过起到正则化作用来改善估计结果。此外，通过在MNIST数据集上进行验证，研究者提出了一个增强方案，该方案通过模型对转换后数据的不确定性进行搜索转换空间，并在图像和文本数据集上验证了其有效性。"
}