{
    "title": "GTAdam: Gradient Tracking with Adaptive Momentum for Distributed Online Optimization. (arXiv:2009.01745v3 [math.OC] UPDATED)",
    "abstract": "This paper deals with a network of computing agents aiming to solve an online optimization problem in a distributed fashion, i.e., by means of local computation and communication, without any central coordinator. We propose the gradient tracking with adaptive momentum estimation (GTAdam) distributed algorithm, which combines a gradient tracking mechanism with first and second order momentum estimates of the gradient. The algorithm is analyzed in the online setting for strongly convex cost functions with Lipschitz continuous gradients. We provide an upper bound for the dynamic regret given by a term related to the initial conditions and another term related to the temporal variations of the objective functions. Moreover, a linear convergence rate is guaranteed in the static setup. The algorithm is tested on a time-varying classification problem, on a (moving) target localization problem, and in a stochastic optimization setup from image classification. In these numerical experiments fro",
    "link": "http://arxiv.org/abs/2009.01745",
    "context": "Title: GTAdam: Gradient Tracking with Adaptive Momentum for Distributed Online Optimization. (arXiv:2009.01745v3 [math.OC] UPDATED)\nAbstract: This paper deals with a network of computing agents aiming to solve an online optimization problem in a distributed fashion, i.e., by means of local computation and communication, without any central coordinator. We propose the gradient tracking with adaptive momentum estimation (GTAdam) distributed algorithm, which combines a gradient tracking mechanism with first and second order momentum estimates of the gradient. The algorithm is analyzed in the online setting for strongly convex cost functions with Lipschitz continuous gradients. We provide an upper bound for the dynamic regret given by a term related to the initial conditions and another term related to the temporal variations of the objective functions. Moreover, a linear convergence rate is guaranteed in the static setup. The algorithm is tested on a time-varying classification problem, on a (moving) target localization problem, and in a stochastic optimization setup from image classification. In these numerical experiments fro",
    "path": "papers/20/09/2009.01745.json",
    "total_tokens": 850,
    "translated_title": "GTAdam：带有自适应动量的梯度跟踪用于分布式在线优化",
    "translated_abstract": "本文涉及一种计算代理网络，旨在通过本地计算和通信的方式，在没有任何中央协调者的情况下，分布式地解决在线优化问题。我们提出了梯度跟踪与自适应动量估计（GTAdam）分布式算法，该算法将梯度跟踪机制与梯度的一阶和二阶动量估计相结合。算法在具有Lipschitz连续梯度的强凸成本函数的在线设置中进行分析。我们给出了与初始条件和目标函数的时间变化相关的动态遗憾的上界。此外，在静态设置中保证线性收敛速度。该算法在时间变化的分类问题、（移动）目标定位问题以及图像分类的随机优化设置中进行了测试。",
    "tldr": "本文提出了一种名为GTAdam的分布式算法，通过梯度跟踪和自适应动量估计结合，解决了分布式在线优化问题，并在强凸成本函数的在线设置中提供了动态遗憾上界和静态设置下的线性收敛速度保证。"
}