{
    "title": "CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models. (arXiv:2009.13964v5 [cs.CL] UPDATED)",
    "abstract": "Several recent efforts have been devoted to enhancing pre-trained language models (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs (KGs) and achieved consistent improvements on various knowledge-driven NLP tasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs of KGs (\"knowledge context\"), regardless of that the knowledge required by PLMs may change dynamically according to specific text (\"textual context\"). In this paper, we propose a novel framework named Coke to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text. Our experimental results show that Coke outperforms various baselines on typical knowledge-driven NLP tasks, indicating the effectiveness of utilizing dynamic knowledge context for language understanding. Besides the performance improvements, the dynamically selected knowle",
    "link": "http://arxiv.org/abs/2009.13964",
    "context": "Title: CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models. (arXiv:2009.13964v5 [cs.CL] UPDATED)\nAbstract: Several recent efforts have been devoted to enhancing pre-trained language models (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs (KGs) and achieved consistent improvements on various knowledge-driven NLP tasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs of KGs (\"knowledge context\"), regardless of that the knowledge required by PLMs may change dynamically according to specific text (\"textual context\"). In this paper, we propose a novel framework named Coke to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text. Our experimental results show that Coke outperforms various baselines on typical knowledge-driven NLP tasks, indicating the effectiveness of utilizing dynamic knowledge context for language understanding. Besides the performance improvements, the dynamically selected knowle",
    "path": "papers/20/09/2009.13964.json",
    "total_tokens": 996,
    "translated_title": "CokeBERT: 增强预训练语言模型的上下文知识选择与嵌入",
    "translated_abstract": "近期有许多工作致力于利用知识图谱中的外部异构知识来增强预训练语言模型（PLMs），并在各种知识驱动的自然语言处理任务上获得了一致的改进。然而，大多数这些知识增强的PLMs仅嵌入KG中的静态子图（“知识上下文”），而不考虑PLMs可能根据特定文本（“文本上下文”）动态变化所需的知识。因此，本文提出了一种新的框架，即Coke，用于为PLMs动态选择上下文知识并根据文本上下文嵌入知识上下文，从而可以避免KG中的冗余和模糊知识对输入文本的匹配效果不佳的影响。实验证明，Coke在典型的知识驱动的自然语言处理任务上优于各种基准模型，表明了利用动态知识上下文进行语言理解的有效性。除了性能方面的改进，所选择的动态知识能够提高语言模型的可解释性，以指导深入理解PLMs所学习的知识。",
    "tldr": "本文提出了一种名为Coke的新框架，用于将上下文知识动态选择和嵌入到预训练语言模型中，以避免对输入文本匹配效果差的冗余和模糊知识的影响，并在知识驱动的自然语言处理任务上取得了优异表现。",
    "en_tdlr": "This paper proposes a new framework named Coke, which dynamically selects contextual knowledge and embeds knowledge context according to textual context for pre-trained language models (PLMs), avoiding the effect of redundant and ambiguous knowledge in knowledge graphs that cannot match the input text, and achieves remarkable performance on knowledge-driven natural language processing tasks."
}