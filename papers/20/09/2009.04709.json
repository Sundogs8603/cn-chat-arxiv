{
    "title": "Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent. (arXiv:2009.04709v5 [stat.ML] UPDATED)",
    "abstract": "Adversarial training, especially projected gradient descent (PGD), has proven to be a successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs have a preferential direction. However, the direction of alignment is not mathematically well established, making it difficult to evaluate quantitatively. We propose a novel definition of this direction as the direction of the vector pointing toward the closest point of the support of the closest inaccurate class in decision space. To evaluate the alignment with this direction after adversarial training, we apply a metric that uses generative adversarial networks to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models have a higher alignment than the baseline according to our definition, that our metric presents higher alignment values than a competing metric formulation, and that enforcing this ",
    "link": "http://arxiv.org/abs/2009.04709",
    "context": "Title: Quantifying the Preferential Direction of the Model Gradient in Adversarial Training With Projected Gradient Descent. (arXiv:2009.04709v5 [stat.ML] UPDATED)\nAbstract: Adversarial training, especially projected gradient descent (PGD), has proven to be a successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs have a preferential direction. However, the direction of alignment is not mathematically well established, making it difficult to evaluate quantitatively. We propose a novel definition of this direction as the direction of the vector pointing toward the closest point of the support of the closest inaccurate class in decision space. To evaluate the alignment with this direction after adversarial training, we apply a metric that uses generative adversarial networks to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models have a higher alignment than the baseline according to our definition, that our metric presents higher alignment values than a competing metric formulation, and that enforcing this ",
    "path": "papers/20/09/2009.04709.json",
    "total_tokens": 941,
    "translated_title": "基于投影梯度下降的对抗训练中梯度方向的量化研究",
    "translated_abstract": "对抗训练，尤其是投影梯度下降（PGD），已被证明是提高对抗性攻击鲁棒性的有效方法。在对抗训练后，模型对其输入的梯度具有优选方向。然而，对齐方向并没有得到数学上的很好描述，这使得其难以进行定量评估。我们提出了一种新的定义，将其视为指向决策空间中最近错误类支持集的最近点的向量方向。为了评估对抗训练后模型与此方向的对齐情况，我们应用了一种指标，该指标使用生成对抗网络来产生最小残差，以改变图像中的类别。我们表明，PGD训练的模型相比基线具有更高的对齐度，而我们的指标呈现比竞争指标公式更高的对齐度值，并且在训练过程中强制执行这个对齐方向可以进一步提高模型的鲁棒性。",
    "tldr": "本文提出了一种新的定义来描述对抗训练中梯度的优选方向，利用生成对抗网络的指标来评估对齐情况，并表明在对齐方向上的限制可以进一步提高模型的鲁棒性。",
    "en_tdlr": "This paper proposes a novel definition to describe the preferential direction of gradients in adversarial training, evaluates the alignment using a metric based on generative adversarial networks, and shows that enforcing alignment during training can further improve model robustness."
}