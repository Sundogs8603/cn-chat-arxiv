{
    "title": "Optimal training of integer-valued neural networks with mixed integer programming. (arXiv:2009.03825v5 [cs.LG] UPDATED)",
    "abstract": "Recent work has shown potential in using Mixed Integer Programming (MIP) solvers to optimize certain aspects of neural networks (NNs). However the intriguing approach of training NNs with MIP solvers is under-explored. State-of-the-art-methods to train NNs are typically gradient-based and require significant data, computation on GPUs, and extensive hyper-parameter tuning. In contrast, training with MIP solvers does not require GPUs or heavy hyper-parameter tuning, but currently cannot handle anything but small amounts of data. This article builds on recent advances that train binarized NNs using MIP solvers. We go beyond current work by formulating new MIP models which improve training efficiency and which can train the important class of integer-valued neural networks (INNs). We provide two novel methods to further the potential significance of using MIP to train NNs. The first method optimizes the number of neurons in the NN while training. This reduces the need for deciding on netwo",
    "link": "http://arxiv.org/abs/2009.03825",
    "context": "Title: Optimal training of integer-valued neural networks with mixed integer programming. (arXiv:2009.03825v5 [cs.LG] UPDATED)\nAbstract: Recent work has shown potential in using Mixed Integer Programming (MIP) solvers to optimize certain aspects of neural networks (NNs). However the intriguing approach of training NNs with MIP solvers is under-explored. State-of-the-art-methods to train NNs are typically gradient-based and require significant data, computation on GPUs, and extensive hyper-parameter tuning. In contrast, training with MIP solvers does not require GPUs or heavy hyper-parameter tuning, but currently cannot handle anything but small amounts of data. This article builds on recent advances that train binarized NNs using MIP solvers. We go beyond current work by formulating new MIP models which improve training efficiency and which can train the important class of integer-valued neural networks (INNs). We provide two novel methods to further the potential significance of using MIP to train NNs. The first method optimizes the number of neurons in the NN while training. This reduces the need for deciding on netwo",
    "path": "papers/20/09/2009.03825.json",
    "total_tokens": 1100,
    "translated_title": "混合整数规划优化整数值神经网络的最佳训练方法",
    "translated_abstract": "最近的研究表明，使用混合整数规划求解器可以优化神经网络的某些方面。但是使用混合整数规划求解器进行神经网络训练的方法尚未得到广泛研究。目前的优化神经网络训练的方法通常基于梯度的方法，并需要大量数据、在GPU上进行计算和广泛的超参数调整。相比之下，使用混合整数规划求解器进行训练不需要GPU或繁琐的超参数调整，但目前只能处理少量的数据。本文在最近使用混合整数规划求解器训练二进制神经网络的进展基础上，提出了新的混合整数规划模型，使训练效率得到改善，并可以训练重要的整数值神经网络。我们提供了两种新方法来进一步发挥使用混合整数规划进行神经网络训练的潜在重要性，第一种方法在训练的同时优化NN中神经元的数量，这减少了在训练之前决定网络结构的需要，并可以节省大量的时间和产生更好的结果。第二种方法引入了一个正则化项，鼓励训练的NN更加稀疏，这可以提高泛化性能。",
    "tldr": "本文介绍了一种新的混合整数规划方法来训练整数值神经网络，其中包括优化神经元数量的方法和鼓励NN更加稀疏的正则化项方法，可以在不使用GPU或复杂超参数调整的情况下提高训练效率和泛化性能。",
    "en_tdlr": "This article presents a new method using mixed integer programming to train integer-valued neural networks (INNs), which includes two novel methods for optimizing the number of neurons in the NN while training and introducing a regularization term that encourages the trained NN to be more sparse, resulting in improved training efficiency and generalization performance without requiring GPUs or heavy hyper-parameter tuning."
}