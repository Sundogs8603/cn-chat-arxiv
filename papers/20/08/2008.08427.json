{
    "title": "How Powerful are Shallow Neural Networks with Bandlimited Random Weights?. (arXiv:2008.08427v3 [cs.LG] UPDATED)",
    "abstract": "We investigate the expressive power of depth-2 bandlimited random neural networks. A random net is a neural network where the hidden layer parameters are frozen with random assignment, and only the output layer parameters are trained by loss minimization. Using random weights for a hidden layer is an effective method to avoid non-convex optimization in standard gradient descent learning. It has also been adopted in recent deep learning theories. Despite the well-known fact that a neural network is a universal approximator, in this study, we mathematically show that when hidden parameters are distributed in a bounded domain, the network may not achieve zero approximation error. In particular, we derive a new nontrivial approximation error lower bound. The proof utilizes the technique of ridgelet analysis, a harmonic analysis method designed for neural networks. This method is inspired by fundamental principles in classical signal processing, specifically the idea that signals with limit",
    "link": "http://arxiv.org/abs/2008.08427",
    "context": "Title: How Powerful are Shallow Neural Networks with Bandlimited Random Weights?. (arXiv:2008.08427v3 [cs.LG] UPDATED)\nAbstract: We investigate the expressive power of depth-2 bandlimited random neural networks. A random net is a neural network where the hidden layer parameters are frozen with random assignment, and only the output layer parameters are trained by loss minimization. Using random weights for a hidden layer is an effective method to avoid non-convex optimization in standard gradient descent learning. It has also been adopted in recent deep learning theories. Despite the well-known fact that a neural network is a universal approximator, in this study, we mathematically show that when hidden parameters are distributed in a bounded domain, the network may not achieve zero approximation error. In particular, we derive a new nontrivial approximation error lower bound. The proof utilizes the technique of ridgelet analysis, a harmonic analysis method designed for neural networks. This method is inspired by fundamental principles in classical signal processing, specifically the idea that signals with limit",
    "path": "papers/20/08/2008.08427.json",
    "total_tokens": 877,
    "translated_title": "浅层神经网络带限制的随机权重有多大的能力？",
    "translated_abstract": "本文探讨了深度为2的带限制随机神经网络的表达能力。随机网络是指隐藏层参数被冻结并赋予随机分配的神经网络，只有输出层参数通过损失最小化进行训练。使用随机权重的隐藏层是避免标准梯度下降学习中的非凸优化的有效方法，并已被近期深度学习理论所采用。尽管神经网络是普适逼近器的众所周知的事实，在这项研究中，我们数学上证明了当隐藏参数分布于有界域时，网络可能无法达到零逼近误差。我们特别导出了一个新的非平凡逼近误差下界。证明利用了Ridgelet分析技术，这是一种为神经网络设计的谐波分析方法。这种方法受到了经典信号处理中的基本原理的启发，特别是信号在某种限制下的采样。",
    "tldr": "本文研究了深度为2的带限制随机神经网络的表达能力，通过数学证明确定了当隐藏参数分布于有界域时，网络可能无法达到零逼近误差。",
    "en_tdlr": "This paper investigates the expressive power of depth-2 bandlimited random neural networks and mathematically proves that when hidden parameters are distributed in a bounded domain, the network may not achieve zero approximation error."
}