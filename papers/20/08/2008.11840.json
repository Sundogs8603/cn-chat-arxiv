{
    "title": "Out-of-sample error estimate for robust M-estimators with convex penalty. (arXiv:2008.11840v5 [math.ST] UPDATED)",
    "abstract": "A generic out-of-sample error estimate is proposed for robust $M$-estimators regularized with a convex penalty in high-dimensional linear regression where $(X,y)$ is observed and $p,n$ are of the same order. If $\\psi$ is the derivative of the robust data-fitting loss $\\rho$, the estimate depends on the observed data only through the quantities $\\hat\\psi = \\psi(y-X\\hat\\beta)$, $X^\\top \\hat\\psi$ and the derivatives $(\\partial/\\partial y) \\hat\\psi$ and $(\\partial/\\partial y) X\\hat\\beta$ for fixed $X$.  The out-of-sample error estimate enjoys a relative error of order $n^{-1/2}$ in a linear model with Gaussian covariates and independent noise, either non-asymptotically when $p/n\\le \\gamma$ or asymptotically in the high-dimensional asymptotic regime $p/n\\to\\gamma'\\in(0,\\infty)$. General differentiable loss functions $\\rho$ are allowed provided that $\\psi=\\rho'$ is 1-Lipschitz. The validity of the out-of-sample error estimate holds either under a strong convexity assumption, or for the $\\ell",
    "link": "http://arxiv.org/abs/2008.11840",
    "context": "Title: Out-of-sample error estimate for robust M-estimators with convex penalty. (arXiv:2008.11840v5 [math.ST] UPDATED)\nAbstract: A generic out-of-sample error estimate is proposed for robust $M$-estimators regularized with a convex penalty in high-dimensional linear regression where $(X,y)$ is observed and $p,n$ are of the same order. If $\\psi$ is the derivative of the robust data-fitting loss $\\rho$, the estimate depends on the observed data only through the quantities $\\hat\\psi = \\psi(y-X\\hat\\beta)$, $X^\\top \\hat\\psi$ and the derivatives $(\\partial/\\partial y) \\hat\\psi$ and $(\\partial/\\partial y) X\\hat\\beta$ for fixed $X$.  The out-of-sample error estimate enjoys a relative error of order $n^{-1/2}$ in a linear model with Gaussian covariates and independent noise, either non-asymptotically when $p/n\\le \\gamma$ or asymptotically in the high-dimensional asymptotic regime $p/n\\to\\gamma'\\in(0,\\infty)$. General differentiable loss functions $\\rho$ are allowed provided that $\\psi=\\rho'$ is 1-Lipschitz. The validity of the out-of-sample error estimate holds either under a strong convexity assumption, or for the $\\ell",
    "path": "papers/20/08/2008.11840.json",
    "total_tokens": 1133,
    "translated_title": "针对具有凸惩罚的鲁棒M-估计的样外误差估计",
    "translated_abstract": "在观测到$(X,y)$且$p,n$同阶的高维线性回归中，提出了一种用于正则化具有凸惩罚的鲁棒$M$-估计的通用样外误差估计。如果$\\psi$是鲁棒数据拟合损失函数$\\rho$的导数，则该估计仅通过$\\hat\\psi = \\psi(y-X\\hat\\beta)$、$X^\\top \\hat\\psi$以及$X$固定时的导数$(\\partial/\\partial y)\\hat\\psi$和$(\\partial/\\partial y)X\\hat\\beta$依赖于观测数据。在具有高斯协变量和独立噪声的线性模型中，这种样外误差估计在$n^{-1/2}$阶具有相对误差，无论是在$p/n\\le \\gamma$的非渐近情况下还是在高维渐近区域$p/n\\to\\gamma'\\in(0,\\infty)$中均成立。只要$\\psi=\\rho'$是1-Lipschitz的，即使通用可微损失函数$\\rho$也是被允许的。当惩罚参数进行适当缩放时，样外误差估计的有效性在满足强凸性假设下或$\\ell_1$惩罚的Huber损失中均成立。",
    "tldr": "该论文提出了一种通用的样外误差估计方法，适用于正则化具有凸惩罚的鲁棒$M$-估计，该方法仅通过固定的观测数据依赖于特定量，其中在高维渐近区域中，该估计具有相对误差，具有广泛的适用性。",
    "en_tdlr": "This paper proposes a generic out-of-sample error estimation method for robust M-estimators with convex penalty in high-dimensional linear regression, which depends only on fixed observed data quantities and enjoys a relative error in the high-dimensional asymptotic regime. The proposed method has broad applicability and allows for differentiable loss functions with 1-Lipschitz derivative."
}