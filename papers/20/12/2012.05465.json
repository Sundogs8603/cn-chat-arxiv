{
    "title": "Adversarial Meta-Learning of Gamma-Minimax Estimators That Leverage Prior Knowledge. (arXiv:2012.05465v4 [stat.ME] UPDATED)",
    "abstract": "Bayes estimators are well known to provide a means to incorporate prior knowledge that can be expressed in terms of a single prior distribution. However, when this knowledge is too vague to express with a single prior, an alternative approach is needed. Gamma-minimax estimators provide such an approach. These estimators minimize the worst-case Bayes risk over a set $\\Gamma$ of prior distributions that are compatible with the available knowledge. Traditionally, Gamma-minimaxity is defined for parametric models. In this work, we define Gamma-minimax estimators for general models and propose adversarial meta-learning algorithms to compute them when the set of prior distributions is constrained by generalized moments. Accompanying convergence guarantees are also provided. We also introduce a neural network class that provides a rich, but finite-dimensional, class of estimators from which a Gamma-minimax estimator can be selected. We illustrate our method in two settings, namely entropy est",
    "link": "http://arxiv.org/abs/2012.05465",
    "context": "Title: Adversarial Meta-Learning of Gamma-Minimax Estimators That Leverage Prior Knowledge. (arXiv:2012.05465v4 [stat.ME] UPDATED)\nAbstract: Bayes estimators are well known to provide a means to incorporate prior knowledge that can be expressed in terms of a single prior distribution. However, when this knowledge is too vague to express with a single prior, an alternative approach is needed. Gamma-minimax estimators provide such an approach. These estimators minimize the worst-case Bayes risk over a set $\\Gamma$ of prior distributions that are compatible with the available knowledge. Traditionally, Gamma-minimaxity is defined for parametric models. In this work, we define Gamma-minimax estimators for general models and propose adversarial meta-learning algorithms to compute them when the set of prior distributions is constrained by generalized moments. Accompanying convergence guarantees are also provided. We also introduce a neural network class that provides a rich, but finite-dimensional, class of estimators from which a Gamma-minimax estimator can be selected. We illustrate our method in two settings, namely entropy est",
    "path": "papers/20/12/2012.05465.json",
    "total_tokens": 976,
    "translated_title": "利用先验知识的 Gamma-Minimax 估计器的对抗元学习",
    "translated_abstract": "贝叶斯估计提供了一种将能够以单个先验分布的形式表达的先验知识结合起来的方式。然而，当这种知识太模糊，无法用单个先验表示时，就需要另一种方法。Gamma-minimax 估计器提供了这样一种方法。这些估计器将在与可用知识相容的一组先验分布 $\\Gamma$ 上最小化最坏情况的 Bayes 风险。传统上，Gamma-minimax 性质是为参数模型定义的。在本文中，我们为一般模型定义 Gamma-minimax 估计器，并提出了利用一般化矩限制的对抗元学习算法来计算它们。我们还提出了一种神经网络类，它提供了一种丰富但有限维度的估计器类，可以从中选择 Gamma-minimax 估计器。我们在两个环节中说明了我们的方法，即估计未知支持分布的样本熵和后分层估计。",
    "tldr": "本文提出对抗元学习方法，用于计算在一组与可用知识相容的先验分布中最小化最坏情况的 Bayes 风险的 Gamma-Minimax 估计器，文中还提出了一种神经网络类用于提供估计器类，以及两个实验环节用于说明该方法的应用。",
    "en_tdlr": "This paper proposes an adversarial meta-learning approach to compute Gamma-minimax estimators that minimize the worst-case Bayes risk over a set of prior distributions compatible with available knowledge when the knowledge is too vague to be expressed with a single prior, and introduces a neural network class to provide a rich class of estimators, as well as two experimental settings to illustrated its applications."
}