{
    "title": "Outlier-robust sparse/low-rank least-squares regression and robust matrix completion. (arXiv:2012.06750v3 [math.ST] UPDATED)",
    "abstract": "We study high-dimensional least-squares regression within a subgaussian statistical learning framework with heterogeneous noise. It includes $s$-sparse and $r$-low-rank least-squares regression when a fraction $\\epsilon$ of the labels are adversarially contaminated. We also present a novel theory of trace-regression with matrix decomposition based on a new application of the product process. For these problems, we show novel near-optimal \"subgaussian\" estimation rates of the form $r(n,d_{e})+\\sqrt{\\log(1/\\delta)/n}+\\epsilon\\log(1/\\epsilon)$, valid with probability at least $1-\\delta$. Here, $r(n,d_{e})$ is the optimal uncontaminated rate as a function of the effective dimension $d_{e}$ but independent of the failure probability $\\delta$. These rates are valid uniformly on $\\delta$, i.e., the estimators' tuning do not depend on $\\delta$. Lastly, we consider noisy robust matrix completion with non-uniform sampling. If only the low-rank matrix is of interest, we present a novel near-optim",
    "link": "http://arxiv.org/abs/2012.06750",
    "context": "Title: Outlier-robust sparse/low-rank least-squares regression and robust matrix completion. (arXiv:2012.06750v3 [math.ST] UPDATED)\nAbstract: We study high-dimensional least-squares regression within a subgaussian statistical learning framework with heterogeneous noise. It includes $s$-sparse and $r$-low-rank least-squares regression when a fraction $\\epsilon$ of the labels are adversarially contaminated. We also present a novel theory of trace-regression with matrix decomposition based on a new application of the product process. For these problems, we show novel near-optimal \"subgaussian\" estimation rates of the form $r(n,d_{e})+\\sqrt{\\log(1/\\delta)/n}+\\epsilon\\log(1/\\epsilon)$, valid with probability at least $1-\\delta$. Here, $r(n,d_{e})$ is the optimal uncontaminated rate as a function of the effective dimension $d_{e}$ but independent of the failure probability $\\delta$. These rates are valid uniformly on $\\delta$, i.e., the estimators' tuning do not depend on $\\delta$. Lastly, we consider noisy robust matrix completion with non-uniform sampling. If only the low-rank matrix is of interest, we present a novel near-optim",
    "path": "papers/20/12/2012.06750.json",
    "total_tokens": 1060,
    "translated_title": "异常鲁棒的稀疏/低秩最小二乘回归和鲁棒矩阵完成",
    "translated_abstract": "我们在具有异质噪声的亚高斯统计学习框架中研究高维最小二乘回归。当标签的一部分受到对抗性污染时，它包括$s$-稀疏和$r$-低秩最小二乘回归。我们还提出了一种基于新的乘积过程的矩阵分解的迹回归的新理论应用。对于这些问题，我们展示了新颖的\"亚高斯\"估计速率形式$r(n,d_{e})+\\sqrt{\\log(1/\\delta)/n}+\\epsilon\\log(1/\\epsilon)$，以至少概率$1-\\delta$成立。这里，$r(n,d_{e})$是作为有效维度$d_{e}$的函数而独立于失败概率$\\delta$的最优无污染速率。这些速率在$\\delta$上是一致有效的，即估计器的调优不依赖于$\\delta$。最后，我们考虑具有非均匀采样的有噪声的鲁棒矩阵完成问题。如果只对低秩矩阵感兴趣，我们提出了一种新近优的方法",
    "tldr": "该论文研究了带异质噪声的高维最小二乘回归问题，包括稀疏和低秩最小二乘回归。研究者提出了新颖的\"亚高斯\"估计速率，并在不同概率下有效。此外，还提出了一种新近优的处理方法用于有噪声的鲁棒矩阵完成问题。",
    "en_tdlr": "This paper investigates high-dimensional least-squares regression with heterogeneous noise, including sparse and low-rank regression. The authors propose novel \"subgaussian\" estimation rates and present a near-optimal approach for noisy robust matrix completion."
}