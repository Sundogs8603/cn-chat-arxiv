{
    "title": "Iterative label cleaning for transductive and semi-supervised few-shot learning. (arXiv:2012.07962v3 [cs.LG] UPDATED)",
    "abstract": "Few-shot learning amounts to learning representations and acquiring knowledge such that novel tasks may be solved with both supervision and data being limited. Improved performance is possible by transductive inference, where the entire test set is available concurrently, and semi-supervised learning, where more unlabeled data is available. Focusing on these two settings, we introduce a new algorithm that leverages the manifold structure of the labeled and unlabeled data distribution to predict pseudo-labels, while balancing over classes and using the loss value distribution of a limited-capacity classifier to select the cleanest labels, iteratively improving the quality of pseudo-labels. Our solution surpasses or matches the state of the art results on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS, while being robust over feature space pre-processing and the quantity of available data. The publicly available source code can be found in https://github.c",
    "link": "http://arxiv.org/abs/2012.07962",
    "context": "Title: Iterative label cleaning for transductive and semi-supervised few-shot learning. (arXiv:2012.07962v3 [cs.LG] UPDATED)\nAbstract: Few-shot learning amounts to learning representations and acquiring knowledge such that novel tasks may be solved with both supervision and data being limited. Improved performance is possible by transductive inference, where the entire test set is available concurrently, and semi-supervised learning, where more unlabeled data is available. Focusing on these two settings, we introduce a new algorithm that leverages the manifold structure of the labeled and unlabeled data distribution to predict pseudo-labels, while balancing over classes and using the loss value distribution of a limited-capacity classifier to select the cleanest labels, iteratively improving the quality of pseudo-labels. Our solution surpasses or matches the state of the art results on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS, while being robust over feature space pre-processing and the quantity of available data. The publicly available source code can be found in https://github.c",
    "path": "papers/20/12/2012.07962.json",
    "total_tokens": 951,
    "translated_title": "迭代标签清洗用于跨领域少样本学习的半监督算法",
    "translated_abstract": "少样本学习涉及到学习表征和获取知识，以使新任务可以在监督和数据都很有限的情况下得到解决。通过横向推断和半监督学习，可以提高性能。我们专注于这两种情况，提出了一种新的算法，利用标记和未标记的数据分布的流形结构来预测伪标签，同时平衡类别并使用有限容量分类器的损失值分布来选择最干净的标签，迭代地提高伪标签的质量。我们的解决方案在四个基准数据集（即 miniImageNet、tieredImageNet、CUB 和 CIFAR-FS）上超过或匹配了现有技术水平，同时在特征空间预处理和可用数据的数量方面具有鲁棒性。公开可用的源代码可在 https://github.c 中找到。",
    "tldr": "该论文提出了一种利用标记和未标记的数据分布的流形结构来预测伪标签，在类别平衡和选择干净标签的基础上，通过迭代清洗标签以提高伪标签质量的算法，在跨领域少样本学习中表现良好，并在四个基准数据集上达到了现有技术水平的最佳效果。",
    "en_tdlr": "This paper proposes an iterative label cleaning algorithm for transductive and semi-supervised few-shot learning, which leverages the manifold structure of labeled and unlabeled data distribution to predict pseudo-labels, balances over classes, and selects the cleanest labels based on the loss value distribution of a limited-capacity classifier. The algorithm achieves state-of-the-art performance on four benchmark datasets and is robust over feature space pre-processing and the quantity of available data."
}