{
    "title": "Risk-Sensitive Deep RL: Variance-Constrained Actor-Critic Provably Finds Globally Optimal Policy. (arXiv:2012.14098v2 [cs.LG] UPDATED)",
    "abstract": "While deep reinforcement learning has achieved tremendous successes in various applications, most existing works only focus on maximizing the expected value of total return and thus ignore its inherent stochasticity. Such stochasticity is also known as the aleatoric uncertainty and is closely related to the notion of risk. In this work, we make the first attempt to study risk-sensitive deep reinforcement learning under the average reward setting with the variance risk criteria. In particular, we focus on a variance-constrained policy optimization problem where the goal is to find a policy that maximizes the expected value of the long-run average reward, subject to a constraint that the long-run variance of the average reward is upper bounded by a threshold. Utilizing Lagrangian and Fenchel dualities, we transform the original problem into an unconstrained saddle-point policy optimization problem, and propose an actor-critic algorithm that iteratively and efficiently updates the policy,",
    "link": "http://arxiv.org/abs/2012.14098",
    "context": "Title: Risk-Sensitive Deep RL: Variance-Constrained Actor-Critic Provably Finds Globally Optimal Policy. (arXiv:2012.14098v2 [cs.LG] UPDATED)\nAbstract: While deep reinforcement learning has achieved tremendous successes in various applications, most existing works only focus on maximizing the expected value of total return and thus ignore its inherent stochasticity. Such stochasticity is also known as the aleatoric uncertainty and is closely related to the notion of risk. In this work, we make the first attempt to study risk-sensitive deep reinforcement learning under the average reward setting with the variance risk criteria. In particular, we focus on a variance-constrained policy optimization problem where the goal is to find a policy that maximizes the expected value of the long-run average reward, subject to a constraint that the long-run variance of the average reward is upper bounded by a threshold. Utilizing Lagrangian and Fenchel dualities, we transform the original problem into an unconstrained saddle-point policy optimization problem, and propose an actor-critic algorithm that iteratively and efficiently updates the policy,",
    "path": "papers/20/12/2012.14098.json",
    "total_tokens": 929,
    "translated_title": "风险敏感的深度强化学习：方差约束的演员-评论家算法能够找到全局最优策略",
    "translated_abstract": "尽管深度强化学习在各种应用中取得了巨大成功，但大多数现有工作仅关注最大化总回报的期望值，从而忽略了其固有的随机性。这种随机性也被称为不确定性，并与风险的概念密切相关。本文首次尝试在平均奖励设置下，通过方差风险准则研究风险敏感的深度强化学习。具体而言，我们关注一个方差约束的策略优化问题，目标是找到一个策略，最大化长期平均奖励的期望值，并且使得长期平均奖励的方差上界不超过某个阈值。利用Lagrange和Fenchel对偶性，我们将原始问题转化为一个无约束的鞍点策略优化问题，并提出了一种迭代和高效更新策略的演员-评论家算法。",
    "tldr": "本文首次尝试在平均奖励设置下，通过方差风险准则研究风险敏感的深度强化学习。我们提出了一个方差约束的策略优化问题，并设计了一种演员-评论家算法来解决该问题。",
    "en_tdlr": "This paper makes the first attempt to study risk-sensitive deep reinforcement learning under the average reward setting with the variance risk criteria. It proposes a variance-constrained policy optimization problem and designs an actor-critic algorithm to solve it."
}