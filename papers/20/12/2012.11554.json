{
    "title": "Variational Transport: A Convergent Particle-BasedAlgorithm for Distributional Optimization",
    "abstract": "arXiv:2012.11554v2 Announce Type: replace  Abstract: We consider the optimization problem of minimizing a functional defined over a family of probability distributions, where the objective functional is assumed to possess a variational form. Such a distributional optimization problem arises widely in machine learning and statistics, with Monte-Carlo sampling, variational inference, policy optimization, and generative adversarial network as examples. For this problem, we propose a novel particle-based algorithm, dubbed as variational transport, which approximately performs Wasserstein gradient descent over the manifold of probability distributions via iteratively pushing a set of particles. Specifically, we prove that moving along the geodesic in the direction of functional gradient with respect to the second-order Wasserstein distance is equivalent to applying a pushforward mapping to a probability distribution, which can be approximated accurately by pushing a set of particles. Specif",
    "link": "https://arxiv.org/abs/2012.11554",
    "context": "Title: Variational Transport: A Convergent Particle-BasedAlgorithm for Distributional Optimization\nAbstract: arXiv:2012.11554v2 Announce Type: replace  Abstract: We consider the optimization problem of minimizing a functional defined over a family of probability distributions, where the objective functional is assumed to possess a variational form. Such a distributional optimization problem arises widely in machine learning and statistics, with Monte-Carlo sampling, variational inference, policy optimization, and generative adversarial network as examples. For this problem, we propose a novel particle-based algorithm, dubbed as variational transport, which approximately performs Wasserstein gradient descent over the manifold of probability distributions via iteratively pushing a set of particles. Specifically, we prove that moving along the geodesic in the direction of functional gradient with respect to the second-order Wasserstein distance is equivalent to applying a pushforward mapping to a probability distribution, which can be approximated accurately by pushing a set of particles. Specif",
    "path": "papers/20/12/2012.11554.json",
    "total_tokens": 786,
    "translated_title": "变分传输：一种用于分布优化的收敛粒子算法",
    "translated_abstract": "我们考虑最小化一个在概率分布族上定义的函数的优化问题，其中假定目标函数具有变分形式。这种分布优化问题在机器学习和统计学中广泛存在，蒙特卡洛抽样、变分推断、策略优化和生成对抗网络是其中的例子。针对这个问题，我们提出了一种新的基于粒子的算法，名为变分传输，通过迭代地推动一组粒子，在概率分布流形上近似执行沃瑟斯坦梯度下降。具体而言，我们证明沿着函数梯度的测地线方向移动，与对概率分布施加一个推前映射等价于通过推动一组粒子来准确近似实施。",
    "tldr": "提出了一种新的基于粒子的算法，名为变分传输，通过迭代地推动一组粒子，在概率分布流形上近似执行沃瑟斯坦梯度下降。",
    "en_tdlr": "Introducing a new particle-based algorithm, variational transport, which iteratively pushes a set of particles to approximately perform Wasserstein gradient descent on the manifold of probability distributions."
}