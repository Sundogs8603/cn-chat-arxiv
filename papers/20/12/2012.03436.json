{
    "title": "Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization for Low-Rank Tensor Completion and Tensor Robust Principal Component Analysis. (arXiv:2012.03436v5 [cs.LG] UPDATED)",
    "abstract": "The nuclear norm and Schatten-$p$ quasi-norm are popular rank proxies in low-rank matrix recovery. However, computing the nuclear norm or Schatten-$p$ quasi-norm of a tensor is hard in both theory and practice, hindering their application to low-rank tensor completion (LRTC) and tensor robust principal component analysis (TRPCA). In this paper, we propose a new class of tensor rank regularizers based on the Euclidean norms of the CP component vectors of a tensor and show that these regularizers are monotonic transformations of tensor Schatten-$p$ quasi-norm. This connection enables us to minimize the Schatten-$p$ quasi-norm in LRTC and TRPCA implicitly via the component vectors. The method scales to big tensors and provides an arbitrarily sharper rank proxy for low-rank tensor recovery compared to the nuclear norm. On the other hand, we study the generalization abilities of LRTC with the Schatten-$p$ quasi-norm regularizer and LRTC with the proposed regularizers. The theorems show that",
    "link": "http://arxiv.org/abs/2012.03436",
    "context": "Title: Euclidean-Norm-Induced Schatten-p Quasi-Norm Regularization for Low-Rank Tensor Completion and Tensor Robust Principal Component Analysis. (arXiv:2012.03436v5 [cs.LG] UPDATED)\nAbstract: The nuclear norm and Schatten-$p$ quasi-norm are popular rank proxies in low-rank matrix recovery. However, computing the nuclear norm or Schatten-$p$ quasi-norm of a tensor is hard in both theory and practice, hindering their application to low-rank tensor completion (LRTC) and tensor robust principal component analysis (TRPCA). In this paper, we propose a new class of tensor rank regularizers based on the Euclidean norms of the CP component vectors of a tensor and show that these regularizers are monotonic transformations of tensor Schatten-$p$ quasi-norm. This connection enables us to minimize the Schatten-$p$ quasi-norm in LRTC and TRPCA implicitly via the component vectors. The method scales to big tensors and provides an arbitrarily sharper rank proxy for low-rank tensor recovery compared to the nuclear norm. On the other hand, we study the generalization abilities of LRTC with the Schatten-$p$ quasi-norm regularizer and LRTC with the proposed regularizers. The theorems show that",
    "path": "papers/20/12/2012.03436.json",
    "total_tokens": 1096,
    "translated_title": "基于欧几里德范数诱导的Schatten-p准范则正则化在低秩张量补全和张量鲁棒主成分分析中的应用",
    "translated_abstract": "核范数和Schatten-p准范是低秩矩阵恢复中常用的秩代理。然而，在理论和实践中，计算张量的核范数或Schatten-p准范都很困难，阻碍了它们在低秩张量补全(LRTC)和张量鲁棒主成分分析(TRPCA)中的应用。本文提出了一种基于张量的CP分量向量的欧几里德范数的新类张量秩正则化器，并且证明了这些正则化器是张量Schatten-p准范的单调变换。这种连接使得我们能够通过分量向量隐式地最小化LRTC和TRPCA中的Schatten-p准范。该方法适用于大型张量，并且与核范数相比，在低秩张量恢复中提供了任意更尖锐的秩代理。另一方面，我们研究了具有Schatten-p准范正则化器和该提议正则化器的LRTC的泛化能力。定理表明",
    "tldr": "本文提出了一种新的张量秩正则化方法，通过计算张量的CP分量向量的欧几里德范数，间接最小化了Schatten-p准范，用于低秩张量补全和张量鲁棒主成分分析。该方法在处理大型张量时具有可扩展性，并且提供了比核范数更精确的秩代理。同时，通过比较理论分析，证明了该方法在LRTC的泛化能力上的优势。",
    "en_tdlr": "This paper proposes a new regularization method for low-rank tensor completion and tensor robust principal component analysis by computing the Euclidean norms of the CP component vectors of a tensor, allowing implicit minimization of the Schatten-p quasi-norm. The method is scalable for large tensors and provides a more accurate rank proxy than the nuclear norm. The improved generalization abilities of this method in LRTC are demonstrated through theoretical analysis."
}