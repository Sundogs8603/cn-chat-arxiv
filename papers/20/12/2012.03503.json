{
    "title": "Block majorization-minimization with diminishing radius for constrained nonconvex optimization. (arXiv:2012.03503v4 [math.OC] UPDATED)",
    "abstract": "Block majorization-minimization (BMM) is a simple iterative algorithm for nonconvex constrained optimization that sequentially minimizes majorizing surrogates of the objective function in each block coordinate while the other coordinates are held fixed. BMM entails a large class of optimization algorithms such as block coordinate descent and its proximal-point variant, expectation-minimization, and block projected gradient descent. We establish that for general constrained nonconvex optimization, BMM with strongly convex surrogates can produce an $\\epsilon$-stationary point within $O(\\epsilon^{-2}(\\log \\epsilon^{-1})^{2})$ iterations and asymptotically converges to the set of stationary points. Furthermore, we propose a trust-region variant of BMM that can handle surrogates that are only convex and still obtain the same iteration complexity and asymptotic stationarity. These results hold robustly even when the convex sub-problems are inexactly solved as long as the optimality gaps are ",
    "link": "http://arxiv.org/abs/2012.03503",
    "context": "Title: Block majorization-minimization with diminishing radius for constrained nonconvex optimization. (arXiv:2012.03503v4 [math.OC] UPDATED)\nAbstract: Block majorization-minimization (BMM) is a simple iterative algorithm for nonconvex constrained optimization that sequentially minimizes majorizing surrogates of the objective function in each block coordinate while the other coordinates are held fixed. BMM entails a large class of optimization algorithms such as block coordinate descent and its proximal-point variant, expectation-minimization, and block projected gradient descent. We establish that for general constrained nonconvex optimization, BMM with strongly convex surrogates can produce an $\\epsilon$-stationary point within $O(\\epsilon^{-2}(\\log \\epsilon^{-1})^{2})$ iterations and asymptotically converges to the set of stationary points. Furthermore, we propose a trust-region variant of BMM that can handle surrogates that are only convex and still obtain the same iteration complexity and asymptotic stationarity. These results hold robustly even when the convex sub-problems are inexactly solved as long as the optimality gaps are ",
    "path": "papers/20/12/2012.03503.json",
    "total_tokens": 950,
    "translated_title": "带有递减半径的块主导极小化方法用于约束非凸优化",
    "translated_abstract": "块主导极小化（BMM）是一种简单的迭代算法，用于非凸约束优化，在每个块坐标上顺序最小化目标函数的主导替代函数，而其他坐标保持固定。BMM包括一大类优化算法，如块坐标下降及其近端点变体，期望最大化和块投影梯度下降。我们证明，对于一般约束非凸优化，使用强凸替代函数的BMM可以在$O(\\epsilon^{-2}(\\log \\epsilon^{-1})^{2})$次迭代中产生一个$\\epsilon$-稳定点，并渐近收敛于稳定点集。此外，我们提出了一种信任域变体的BMM，可以处理只具备凸性的替代函数，并仍然获得相同的迭代复杂度和渐近稳定性。这些结果具有鲁棒性，即使凸子问题的求解是非精确的，只要最优间隙满足要求。",
    "tldr": "这个论文介绍了一种用于非凸约束优化问题的块主导极小化算法，并证明了在使用强凸替代函数的情况下，该算法可以在一定迭代次数内收敛到一个稳定点，并提出了一个信任域变体，可以处理只具备凸性的替代函数。",
    "en_tdlr": "This paper presents a block majorization-minimization algorithm for nonconvex constrained optimization, and proves that it can converge to a stationary point within a certain number of iterations when using strongly convex surrogates. A trust-region variant is also proposed to handle surrogates with convexity only."
}