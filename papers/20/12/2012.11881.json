{
    "title": "Undivided Attention: Are Intermediate Layers Necessary for BERT?. (arXiv:2012.11881v2 [cs.CL] UPDATED)",
    "abstract": "In recent times, BERT-based models have been extremely successful in solving a variety of natural language processing (NLP) tasks such as reading comprehension, natural language inference, sentiment analysis, etc. All BERT-based architectures have a self-attention block followed by a block of intermediate layers as the basic building component. However, a strong justification for the inclusion of these intermediate layers remains missing in the literature. In this work we investigate the importance of intermediate layers on the overall network performance of downstream tasks. We show that reducing the number of intermediate layers and modifying the architecture for BERT-BASE results in minimal loss in fine-tuning accuracy for downstream tasks while decreasing the number of parameters and training time of the model. Additionally, we use centered kernel alignment and probing linear classifiers to gain insight into our architectural modifications and justify that removal of intermediate l",
    "link": "http://arxiv.org/abs/2012.11881",
    "context": "Title: Undivided Attention: Are Intermediate Layers Necessary for BERT?. (arXiv:2012.11881v2 [cs.CL] UPDATED)\nAbstract: In recent times, BERT-based models have been extremely successful in solving a variety of natural language processing (NLP) tasks such as reading comprehension, natural language inference, sentiment analysis, etc. All BERT-based architectures have a self-attention block followed by a block of intermediate layers as the basic building component. However, a strong justification for the inclusion of these intermediate layers remains missing in the literature. In this work we investigate the importance of intermediate layers on the overall network performance of downstream tasks. We show that reducing the number of intermediate layers and modifying the architecture for BERT-BASE results in minimal loss in fine-tuning accuracy for downstream tasks while decreasing the number of parameters and training time of the model. Additionally, we use centered kernel alignment and probing linear classifiers to gain insight into our architectural modifications and justify that removal of intermediate l",
    "path": "papers/20/12/2012.11881.json",
    "total_tokens": 841,
    "translated_title": "无分割关注：BERT是否需要中间层？",
    "translated_abstract": "最近，基于BERT的模型在解决各种自然语言处理（NLP）任务方面非常成功，例如阅读理解、自然语言推理、情感分析等。所有基于BERT的架构都具有一个自注意力块，后跟一个中间层块作为基本构建组件。然而，文献中缺乏对包含这些中间层的强有力的理由。在这项工作中，我们调查了中间层对下游任务的整体网络性能的重要性。我们表明，减少中间层数量并修改BERT-BASE的架构会导致下游任务的微小损失，同时减少模型的参数和训练时间。此外，我们使用中心化内核对齐和探测线性分类器来获得对我们的架构修改的洞见，并证明删除中间层对学习到的表示没有显着影响。",
    "tldr": "本论文调查了中间层对于BERT在下游任务表现的作用，表明删除中间层数量可以减少模型参数和训练时间，同时对下游任务的影响较小，学习到的表示不受影响。",
    "en_tdlr": "This paper investigates the role of intermediate layers in the performance of BERT on downstream tasks, showing that reducing the number of intermediate layers and modifying the architecture can decrease model parameters and training time with minimal effect on downstream task accuracy and learned representations."
}