{
    "title": "On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems. (arXiv:1906.00331v9 [cs.LG] UPDATED)",
    "abstract": "We consider nonconvex-concave minimax problems, $\\min_{\\mathbf{x}} \\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\mathbf{x}, \\mathbf{y})$, where $f$ is nonconvex in $\\mathbf{x}$ but concave in $\\mathbf{y}$ and $\\mathcal{Y}$ is a convex and bounded set. One of the most popular algorithms for solving this problem is the celebrated gradient descent ascent (GDA) algorithm, which has been widely used in machine learning, control theory and economics. Despite the extensive convergence results for the convex-concave setting, GDA with equal stepsize can converge to limit cycles or even diverge in a general setting. In this paper, we present the complexity results on two-time-scale GDA for solving nonconvex-concave minimax problems, showing that the algorithm can find a stationary point of the function $\\Phi(\\cdot) := \\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\cdot, \\mathbf{y})$ efficiently. To the best our knowledge, this is the first nonasymptotic analysis for two-time-scale GDA in this setting, shedding lig",
    "link": "http://arxiv.org/abs/1906.00331",
    "context": "Title: On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems. (arXiv:1906.00331v9 [cs.LG] UPDATED)\nAbstract: We consider nonconvex-concave minimax problems, $\\min_{\\mathbf{x}} \\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\mathbf{x}, \\mathbf{y})$, where $f$ is nonconvex in $\\mathbf{x}$ but concave in $\\mathbf{y}$ and $\\mathcal{Y}$ is a convex and bounded set. One of the most popular algorithms for solving this problem is the celebrated gradient descent ascent (GDA) algorithm, which has been widely used in machine learning, control theory and economics. Despite the extensive convergence results for the convex-concave setting, GDA with equal stepsize can converge to limit cycles or even diverge in a general setting. In this paper, we present the complexity results on two-time-scale GDA for solving nonconvex-concave minimax problems, showing that the algorithm can find a stationary point of the function $\\Phi(\\cdot) := \\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\cdot, \\mathbf{y})$ efficiently. To the best our knowledge, this is the first nonasymptotic analysis for two-time-scale GDA in this setting, shedding lig",
    "path": "papers/19/06/1906.00331.json",
    "total_tokens": 1007,
    "translated_title": "关于非凸-凹极小极大问题的梯度下降算法",
    "translated_abstract": "我们考虑非凸-凹极小极大问题，即$\\min_{\\mathbf{x}} \\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\mathbf{x}, \\mathbf{y})$，其中$f$在$\\mathbf{x}$上是非凸的但在$\\mathbf{y}$上是凹的，$\\mathcal{Y}$是一个凸且有界的集合。解决这个问题最常用的算法之一是著名的梯度下降上升（GDA）算法，在机器学习、控制理论和经济学中得到了广泛应用。尽管凸-凹设置下有着广泛的收敛结果，但在一般情况下，相同步长的GDA可能会收敛到极限环，甚至发散。本文介绍了解决非凸-凹极小极大问题的两时间尺度GDA的复杂度结果，并证明了该算法可以高效地找到函数$\\Phi(\\cdot) := \\max_{\\mathbf{y} \\in \\mathcal{Y}} f(\\cdot, \\mathbf{y})$的一个稳定点。据我们所知，这是第一个在这个设置中的非渐进分析结果，揭示了有关两时间尺度GDA的性质。",
    "tldr": "该论文提出了一种用于解决非凸-凹极小极大问题的梯度下降算法，并进行了复杂性分析，证明了该算法可以高效地找到函数的稳定点。该研究对于非凸-凹极小极大问题的非渐进分析是首次进行的。"
}