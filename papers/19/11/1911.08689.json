{
    "title": "Corruption-robust exploration in episodic reinforcement learning. (arXiv:1911.08689v4 [cs.LG] UPDATED)",
    "abstract": "We initiate the study of multi-stage episodic reinforcement learning under adversarial corruptions in both the rewards and the transition probabilities of the underlying system extending recent results for the special case of stochastic bandits. We provide a framework which modifies the aggressive exploration enjoyed by existing reinforcement learning approaches based on \"optimism in the face of uncertainty\", by complementing them with principles from \"action elimination\". Importantly, our framework circumvents the major challenges posed by naively applying action elimination in the RL setting, as formalized by a lower bound we demonstrate. Our framework yields efficient algorithms which (a) attain near-optimal regret in the absence of corruptions and (b) adapt to unknown levels corruption, enjoying regret guarantees which degrade gracefully in the total corruption encountered. To showcase the generality of our approach, we derive results for both tabular settings (where states and act",
    "link": "http://arxiv.org/abs/1911.08689",
    "context": "Title: Corruption-robust exploration in episodic reinforcement learning. (arXiv:1911.08689v4 [cs.LG] UPDATED)\nAbstract: We initiate the study of multi-stage episodic reinforcement learning under adversarial corruptions in both the rewards and the transition probabilities of the underlying system extending recent results for the special case of stochastic bandits. We provide a framework which modifies the aggressive exploration enjoyed by existing reinforcement learning approaches based on \"optimism in the face of uncertainty\", by complementing them with principles from \"action elimination\". Importantly, our framework circumvents the major challenges posed by naively applying action elimination in the RL setting, as formalized by a lower bound we demonstrate. Our framework yields efficient algorithms which (a) attain near-optimal regret in the absence of corruptions and (b) adapt to unknown levels corruption, enjoying regret guarantees which degrade gracefully in the total corruption encountered. To showcase the generality of our approach, we derive results for both tabular settings (where states and act",
    "path": "papers/19/11/1911.08689.json",
    "total_tokens": 914,
    "translated_title": "强化学习中对抗性腐败的鲁棒探索研究",
    "translated_abstract": "我们对多阶段强化学习在奖励和转移概率两方面的对抗性腐败进行了研究，扩展了最近对随机赌博机特例的研究结果。我们提供了一个框架，通过将“乐观面对不确定性”的现有强化学习方法进行探索性改进，并结合“动作淘汰”原则，从而解决了在强化学习环境中朴素应用动作淘汰所面临的主要挑战。我们的框架提供了高效的算法，(a)在没有腐败时实现接近最优的后悔，并且(b)能够适应未知水平的腐败，在总腐败情况下后悔程度逐渐降低。为了展示我们方法的广泛适用性，我们推导了表格设置下的结果（其中涉及状态和行为）以及通用函数逼近设置下的结果。",
    "tldr": "该论文研究了强化学习中在奖励和转移概率两方面存在的对抗性腐败问题，并提出了一种能够解决腐败问题的高效算法，能够在没有腐败的情况下实现接近最优的后悔，并且能够适应未知水平的腐败。"
}