{
    "title": "Frequentist Regret Bounds for Randomized Least-Squares Value Iteration. (arXiv:1911.00567v6 [cs.LG] UPDATED)",
    "abstract": "We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning (RL). When the state space is large or continuous, traditional tabular approaches are unfeasible and some form of function approximation is mandatory. In this paper, we introduce an optimistically-initialized variant of the popular randomized least-squares value iteration (RLSVI), a model-free algorithm where exploration is induced by perturbing the least-squares approximation of the action-value function. Under the assumption that the Markov decision process has low-rank transition dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by $\\widetilde O(d^2 H^2 \\sqrt{T})$ where $ d $ are the feature dimension, $ H $ is the horizon, and $ T $ is the total number of steps. To the best of our knowledge, this is the first frequentist regret analysis for randomized exploration with function approximation.",
    "link": "http://arxiv.org/abs/1911.00567",
    "context": "Title: Frequentist Regret Bounds for Randomized Least-Squares Value Iteration. (arXiv:1911.00567v6 [cs.LG] UPDATED)\nAbstract: We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning (RL). When the state space is large or continuous, traditional tabular approaches are unfeasible and some form of function approximation is mandatory. In this paper, we introduce an optimistically-initialized variant of the popular randomized least-squares value iteration (RLSVI), a model-free algorithm where exploration is induced by perturbing the least-squares approximation of the action-value function. Under the assumption that the Markov decision process has low-rank transition dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by $\\widetilde O(d^2 H^2 \\sqrt{T})$ where $ d $ are the feature dimension, $ H $ is the horizon, and $ T $ is the total number of steps. To the best of our knowledge, this is the first frequentist regret analysis for randomized exploration with function approximation.",
    "path": "papers/19/11/1911.00567.json",
    "total_tokens": 1012,
    "translated_title": "随机最小二乘值迭代的频率后悔界限",
    "translated_abstract": "我们考虑有限时间域强化学习中的探索-利用困境。当状态空间很大或连续时，传统的表格方法不可行，必须采用函数逼近的形式。在本文中，我们引入了一种乐观初始化的改进版本的随机最小二乘值迭代（RLSVI）算法，该算法是一种无模型算法，其中探索是通过扰动行动值函数的最小二乘逼近来诱导的。在假设马尔可夫决策过程具有低秩转移动态的情况下，我们证明了RLSVI的频率后悔将上界为$\\widetilde O(d^2 H^2 \\sqrt{T})$，其中$ d $是特征维度，$ H $是时间限制，$ T $是总步数。据我们所知，这是对于带有函数逼近的随机探索的第一个频率后悔分析。",
    "tldr": "本文引入了一种改进版本的随机最小二乘值迭代（RLSVI）算法，通过对行动值函数的最小二乘逼近进行扰动，诱导出了探索过程。在马尔可夫决策过程具有低秩转移动态的假设下，我们证明了RLSVI的频率后悔上界为$\\widetilde O(d^2 H^2 \\sqrt{T})$。这是对于带有函数逼近的随机探索的首个频率后悔分析。",
    "en_tdlr": "This paper introduces an improved version of randomized least-squares value iteration (RLSVI) algorithm, which induces exploration by perturbing the least-squares approximation of the action-value function. Under the assumption of low-rank transition dynamics in the Markov decision process, the paper proves that RLSVI has a frequentist regret upper bound of $\\widetilde O(d^2 H^2 \\sqrt{T})$. This is the first frequentist regret analysis for randomized exploration with function approximation."
}