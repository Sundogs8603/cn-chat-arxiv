{
    "title": "A Sub-sampled Tensor Method for Non-convex Optimization. (arXiv:1911.10367v3 [math.OC] UPDATED)",
    "abstract": "We present a stochastic optimization method that uses a fourth-order regularized model to find local minima of smooth and potentially non-convex objective functions with a finite-sum structure. This algorithm uses sub-sampled derivatives instead of exact quantities. The proposed approach is shown to find an $(\\epsilon_1,\\epsilon_2,\\epsilon_3)$-third-order critical point in at most $\\bigO\\left(\\max\\left(\\epsilon_1^{-4/3}, \\epsilon_2^{-2}, \\epsilon_3^{-4}\\right)\\right)$ iterations, thereby matching the rate of deterministic approaches. In order to prove this result, we derive a novel tensor concentration inequality for sums of tensors of any order that makes explicit use of the finite-sum structure of the objective function.",
    "link": "http://arxiv.org/abs/1911.10367",
    "context": "Title: A Sub-sampled Tensor Method for Non-convex Optimization. (arXiv:1911.10367v3 [math.OC] UPDATED)\nAbstract: We present a stochastic optimization method that uses a fourth-order regularized model to find local minima of smooth and potentially non-convex objective functions with a finite-sum structure. This algorithm uses sub-sampled derivatives instead of exact quantities. The proposed approach is shown to find an $(\\epsilon_1,\\epsilon_2,\\epsilon_3)$-third-order critical point in at most $\\bigO\\left(\\max\\left(\\epsilon_1^{-4/3}, \\epsilon_2^{-2}, \\epsilon_3^{-4}\\right)\\right)$ iterations, thereby matching the rate of deterministic approaches. In order to prove this result, we derive a novel tensor concentration inequality for sums of tensors of any order that makes explicit use of the finite-sum structure of the objective function.",
    "path": "papers/19/11/1911.10367.json",
    "total_tokens": 763,
    "translated_title": "一个用于非凸优化的次采样张量方法",
    "translated_abstract": "我们提出了一种随机优化方法，该方法使用四阶正则化模型来寻找光滑且潜在非凸的目标函数的局部极小值，而不是使用精确的导数。该算法使用次采样导数而不是精确数量。通过使用新颖的张量集中不等式，我们证明了该方法可以在至多 $\\bigO\\left(\\max\\left(\\epsilon_1^{-4/3}, \\epsilon_2^{-2}, \\epsilon_3^{-4}\\right)\\right)$ 次迭代中找到一个 $(\\epsilon_1,\\epsilon_2,\\epsilon_3)$ 三阶临界点，从而与确定性方法的收敛速度相匹配。",
    "tldr": "该论文提出了一个用于非凸优化的次采样张量方法，通过使用新颖的张量集中不等式，该方法可以在匹配确定性方法的收敛速度下找到光滑且潜在非凸的目标函数的局部极小值。",
    "en_tdlr": "This paper presents a sub-sampled tensor method for non-convex optimization, which can find local minima of smooth and potentially non-convex objective functions at a convergence rate matching deterministic approaches, by utilizing a novel tensor concentration inequality."
}