{
    "title": "The generalization error of max-margin linear classifiers: Benign overfitting and high dimensional asymptotics in the overparametrized regime. (arXiv:1911.01544v3 [math.ST] UPDATED)",
    "abstract": "Modern machine learning classifiers often exhibit vanishing classification error on the training set. They achieve this by learning nonlinear representations of the inputs that maps the data into linearly separable classes.  Motivated by these phenomena, we revisit high-dimensional maximum margin classification for linearly separable data. We consider a stylized setting in which data $(y_i,{\\boldsymbol x}_i)$, $i\\le n$ are i.i.d. with ${\\boldsymbol x}_i\\sim\\mathsf{N}({\\boldsymbol 0},{\\boldsymbol \\Sigma})$ a $p$-dimensional Gaussian feature vector, and $y_i \\in\\{+1,-1\\}$ a label whose distribution depends on a linear combination of the covariates $\\langle {\\boldsymbol \\theta}_*,{\\boldsymbol x}_i \\rangle$. While the Gaussian model might appear extremely simplistic, universality arguments can be used to show that the results derived in this setting also apply to the output of certain nonlinear featurization maps.  We consider the proportional asymptotics $n,p\\to\\infty$ with $p/n\\to \\psi$,",
    "link": "http://arxiv.org/abs/1911.01544",
    "context": "Title: The generalization error of max-margin linear classifiers: Benign overfitting and high dimensional asymptotics in the overparametrized regime. (arXiv:1911.01544v3 [math.ST] UPDATED)\nAbstract: Modern machine learning classifiers often exhibit vanishing classification error on the training set. They achieve this by learning nonlinear representations of the inputs that maps the data into linearly separable classes.  Motivated by these phenomena, we revisit high-dimensional maximum margin classification for linearly separable data. We consider a stylized setting in which data $(y_i,{\\boldsymbol x}_i)$, $i\\le n$ are i.i.d. with ${\\boldsymbol x}_i\\sim\\mathsf{N}({\\boldsymbol 0},{\\boldsymbol \\Sigma})$ a $p$-dimensional Gaussian feature vector, and $y_i \\in\\{+1,-1\\}$ a label whose distribution depends on a linear combination of the covariates $\\langle {\\boldsymbol \\theta}_*,{\\boldsymbol x}_i \\rangle$. While the Gaussian model might appear extremely simplistic, universality arguments can be used to show that the results derived in this setting also apply to the output of certain nonlinear featurization maps.  We consider the proportional asymptotics $n,p\\to\\infty$ with $p/n\\to \\psi$,",
    "path": "papers/19/11/1911.01544.json",
    "total_tokens": 971,
    "translated_title": "最大间隔线性分类器的泛化误差：过度拟合和超参数区域的高维渐近性。",
    "translated_abstract": "现代机器学习分类器通常在训练集上表现出消失的分类误差。它们通过学习将数据映射到线性可分的类别的输入的非线性表示来实现这一点。受这些现象的启发，我们重新审视线性可分数据的高维最大间隔分类问题。我们考虑这样一个设定: 数据$(y_i,{\\boldsymbol x}_i)$， $i\\le n$独立同分布，其中${\\boldsymbol x}_i\\sim \\mathsf{N}({\\boldsymbol 0},{\\boldsymbol \\Sigma})$为$p$维高斯特征向量，$y_i \\in\\{+1,-1\\}$表示标签，其分布取决于协变量${\\boldsymbol \\theta}_*$ 和 ${\\boldsymbol x}_i$的线性组合。虽然高斯模型可能看起来非常简单，但普遍性论据可用于表明在这个设置中得出的结果也适用于某些非线性映射的输出。我们考虑比例渐近关系$n,p\\to\\infty$，且$p/n\\to\\psi$，",
    "tldr": "这篇论文在深入探讨现代机器学习分类器的最大间隔线性分类器问题上发现了一些关于过度拟合以及高维渐近性的泛化误差。",
    "en_tdlr": "This paper delves into the issue of maximum margin linear classifiers in modern machine learning classifiers and discovers some findings about overfitting and high-dimensional asymptotics in the generalization error."
}