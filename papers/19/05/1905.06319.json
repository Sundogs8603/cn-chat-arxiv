{
    "title": "Exact Hard Monotonic Attention for Character-Level Transduction",
    "abstract": "arXiv:1905.06319v3 Announce Type: replace  Abstract: Many common character-level, string-to string transduction tasks, e.g., grapheme-tophoneme conversion and morphological inflection, consist almost exclusively of monotonic transductions. However, neural sequence-to sequence models that use non-monotonic soft attention often outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias for these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns a latent alignment jointly while learning to transduce. With the help of dynamic programming, we are able to compute the exact marginalization over all monotonic alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.",
    "link": "https://arxiv.org/abs/1905.06319",
    "context": "Title: Exact Hard Monotonic Attention for Character-Level Transduction\nAbstract: arXiv:1905.06319v3 Announce Type: replace  Abstract: Many common character-level, string-to string transduction tasks, e.g., grapheme-tophoneme conversion and morphological inflection, consist almost exclusively of monotonic transductions. However, neural sequence-to sequence models that use non-monotonic soft attention often outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias for these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns a latent alignment jointly while learning to transduce. With the help of dynamic programming, we are able to compute the exact marginalization over all monotonic alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.",
    "path": "papers/19/05/1905.06319.json",
    "total_tokens": 792,
    "translated_title": "字符级转导的精确硬单调注意力",
    "translated_abstract": "许多常见的字符级字符串转导任务，例如音素到字音转换和形态屈折，几乎完全由单调转导组成。但是，使用非单调软注意力的神经序列到序列模型通常优于流行的单调模型。在这项工作中，我们提出以下问题：对于这些任务，单调性真的是一个有用的约束吗？我们开发了一种硬注意力序列到序列模型，强制执行严格的单调性，并在学习转导的同时学习潜在的对齐。借助动态规划，我们能够计算出所有单调对齐的精确边缘化。我们的模型在形态屈折任务上实现了最新的性能。此外，我们发现在另外两个字符级转导任务上也表现出良好的性能。代码可在 https://github.com/shijie-wu/neural-transducer 找到。",
    "tldr": "开发了一种精确硬单调注意力的序列到序列模型，在字符级转导任务中取得了最先进的性能。",
    "en_tdlr": "Developed a sequence-to-sequence model with exact hard monotonic attention, achieving state-of-the-art performance in character-level transduction tasks."
}