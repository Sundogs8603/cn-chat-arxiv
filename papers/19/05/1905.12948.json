{
    "title": "Global Momentum Compression for Sparse Communication in Distributed Learning",
    "abstract": "arXiv:1905.12948v3 Announce Type: replace-cross  Abstract: With the rapid growth of data, distributed momentum stochastic gradient descent~(DMSGD) has been widely used in distributed learning, especially for training large-scale deep models. Due to the latency and limited bandwidth of the network, communication has become the bottleneck of distributed learning. Communication compression with sparsified gradient, abbreviated as \\emph{sparse communication}, has been widely employed to reduce communication cost. All existing works about sparse communication in DMSGD employ local momentum, in which the momentum only accumulates stochastic gradients computed by each worker locally. In this paper, we propose a novel method, called \\emph{\\underline{g}}lobal \\emph{\\underline{m}}omentum \\emph{\\underline{c}}ompression~(GMC), for sparse communication. Different from existing works that utilize local momentum, GMC utilizes global momentum. Furthermore, to enhance the convergence performance when u",
    "link": "https://arxiv.org/abs/1905.12948",
    "context": "Title: Global Momentum Compression for Sparse Communication in Distributed Learning\nAbstract: arXiv:1905.12948v3 Announce Type: replace-cross  Abstract: With the rapid growth of data, distributed momentum stochastic gradient descent~(DMSGD) has been widely used in distributed learning, especially for training large-scale deep models. Due to the latency and limited bandwidth of the network, communication has become the bottleneck of distributed learning. Communication compression with sparsified gradient, abbreviated as \\emph{sparse communication}, has been widely employed to reduce communication cost. All existing works about sparse communication in DMSGD employ local momentum, in which the momentum only accumulates stochastic gradients computed by each worker locally. In this paper, we propose a novel method, called \\emph{\\underline{g}}lobal \\emph{\\underline{m}}omentum \\emph{\\underline{c}}ompression~(GMC), for sparse communication. Different from existing works that utilize local momentum, GMC utilizes global momentum. Furthermore, to enhance the convergence performance when u",
    "path": "papers/19/05/1905.12948.json",
    "total_tokens": 837,
    "translated_title": "全局动量压缩用于分布式学习中的稀疏通信",
    "translated_abstract": "随着数据的快速增长，分布式动量随机梯度下降（DMSGD）在分布式学习中得到了广泛应用，特别是用于训练大规模深度模型。由于网络的延迟和带宽有限，通信成为分布式学习的瓶颈。使用稀疏梯度进行通信压缩，简称为“稀疏通信”，已被广泛应用以降低通信成本。所有关于DMSGD中稀疏通信的现有工作都使用本地动量，其中动量仅累积每个工作者在本地计算的随机梯度。在本文中，我们提出了一种新方法，称为\\emph{全局动量压缩}（GMC），用于稀疏通信。不同于现有工作中使用的局部动量，GMC使用全局动量。",
    "tldr": "本文提出了一种全局动量压缩（GMC）方法，用于稀疏通信，与现有的局部动量方法不同，GMC利用全局动量来提高分布式学习性能。",
    "en_tdlr": "This paper introduces a novel method called Global Momentum Compression (GMC) for sparse communication in distributed learning, which improves performance by utilizing global momentum instead of local momentum used in existing works."
}