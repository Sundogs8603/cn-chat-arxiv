{
    "title": "Learning NP-Hard Multi-Agent Assignment Planning using GNN: Inference on a Random Graph and Provable Auction-Fitted Q-learning. (arXiv:1905.12204v4 [cs.LG] UPDATED)",
    "abstract": "This paper explores the possibility of near-optimally solving multi-agent, multi-task NP-hard planning problems with time-dependent rewards using a learning-based algorithm. In particular, we consider a class of robot/machine scheduling problems called the multi-robot reward collection problem (MRRC). Such MRRC problems well model ride-sharing, pickup-and-delivery, and a variety of related problems. In representing the MRRC problem as a sequential decision-making problem, we observe that each state can be represented as an extension of probabilistic graphical models (PGMs), which we refer to as random PGMs. We then develop a mean-field inference method for random PGMs. We then propose (1) an order-transferable Q-function estimator and (2) an order-transferability-enabled auction to select a joint assignment in polynomial time. These result in a reinforcement learning framework with at least $1-1/e$ optimality. Experimental results on solving MRRC problems highlight the near-optimality ",
    "link": "http://arxiv.org/abs/1905.12204",
    "context": "Title: Learning NP-Hard Multi-Agent Assignment Planning using GNN: Inference on a Random Graph and Provable Auction-Fitted Q-learning. (arXiv:1905.12204v4 [cs.LG] UPDATED)\nAbstract: This paper explores the possibility of near-optimally solving multi-agent, multi-task NP-hard planning problems with time-dependent rewards using a learning-based algorithm. In particular, we consider a class of robot/machine scheduling problems called the multi-robot reward collection problem (MRRC). Such MRRC problems well model ride-sharing, pickup-and-delivery, and a variety of related problems. In representing the MRRC problem as a sequential decision-making problem, we observe that each state can be represented as an extension of probabilistic graphical models (PGMs), which we refer to as random PGMs. We then develop a mean-field inference method for random PGMs. We then propose (1) an order-transferable Q-function estimator and (2) an order-transferability-enabled auction to select a joint assignment in polynomial time. These result in a reinforcement learning framework with at least $1-1/e$ optimality. Experimental results on solving MRRC problems highlight the near-optimality ",
    "path": "papers/19/05/1905.12204.json",
    "total_tokens": 911,
    "translated_title": "使用GNN学习NP-Hard多智能体分配规划：在随机图上的推理和可证明的拍卖适配Q学习",
    "translated_abstract": "本文探讨了使用基于学习的算法来近乎最优地解决具有时间相关奖励的多智能体、多任务的NP-hard规划问题的可能性。特别地，我们考虑了一类称为多机器人奖励收集问题（MRRC）的机器人/机器调度问题。这些MRRC问题很好地模拟了共乘、取送和其他相关问题。在将MRRC问题表示为顺序决策问题时，我们观察到每个状态可以被表示为概率图模型（PGM）的扩展，我们将其称为随机PGMs。然后，我们为随机PGMs开发了一种均场推理方法。接下来，我们提出了（1）一个可进行顺序转移的Q函数估计器和（2）一个支持顺序转移的拍卖方法，以在多项式时间内选择联合分配。这些方法导致了一个具有至少$1-1/e$最优性的强化学习框架。在解决MRRC问题的实验结果中突出了近乎最优性。",
    "tldr": "本文探索了使用学习算法近乎最优地解决具有时间相关奖励的多智能体、多任务的NP-hard规划问题的可能性。研究结果展示了提出方法在解决机器人/机器调度问题上的近乎最优性。"
}