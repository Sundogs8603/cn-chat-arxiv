{
    "title": "Sequential Gaussian Processes for Online Learning of Nonstationary Functions. (arXiv:1905.10003v4 [stat.ML] UPDATED)",
    "abstract": "Many machine learning problems can be framed in the context of estimating functions, and often these are time-dependent functions that are estimated in real-time as observations arrive. Gaussian processes (GPs) are an attractive choice for modeling real-valued nonlinear functions due to their flexibility and uncertainty quantification. However, the typical GP regression model suffers from several drawbacks: 1) Conventional GP inference scales $O(N^{3})$ with respect to the number of observations; 2) Updating a GP model sequentially is not trivial; and 3) Covariance kernels typically enforce stationarity constraints on the function, while GPs with non-stationary covariance kernels are often intractable to use in practice. To overcome these issues, we propose a sequential Monte Carlo algorithm to fit infinite mixtures of GPs that capture non-stationary behavior while allowing for online, distributed inference. Our approach empirically improves performance over state-of-the-art methods fo",
    "link": "http://arxiv.org/abs/1905.10003",
    "context": "Title: Sequential Gaussian Processes for Online Learning of Nonstationary Functions. (arXiv:1905.10003v4 [stat.ML] UPDATED)\nAbstract: Many machine learning problems can be framed in the context of estimating functions, and often these are time-dependent functions that are estimated in real-time as observations arrive. Gaussian processes (GPs) are an attractive choice for modeling real-valued nonlinear functions due to their flexibility and uncertainty quantification. However, the typical GP regression model suffers from several drawbacks: 1) Conventional GP inference scales $O(N^{3})$ with respect to the number of observations; 2) Updating a GP model sequentially is not trivial; and 3) Covariance kernels typically enforce stationarity constraints on the function, while GPs with non-stationary covariance kernels are often intractable to use in practice. To overcome these issues, we propose a sequential Monte Carlo algorithm to fit infinite mixtures of GPs that capture non-stationary behavior while allowing for online, distributed inference. Our approach empirically improves performance over state-of-the-art methods fo",
    "path": "papers/19/05/1905.10003.json",
    "total_tokens": 946,
    "translated_title": "用于在线学习非平稳函数的连续高斯过程",
    "translated_abstract": "许多机器学习问题可以在估计函数的上下文中得到解决，通常这些函数是时间相关的函数，并且是实时地随着观测的到来而估计的。高斯过程是建模实值非线性函数的一个有吸引力的选择，由于其灵活性和不确定性量化。然而，典型的高斯过程回归模型存在若干不足：1）传统高斯过程推断的复杂度$O(N^{3})$随着观测值的个数N成增长；2）逐步更新高斯过程模型不容易；3）协方差核通常对函数施加平稳性约束，而具有非平稳协方差核的高斯过程通常难以在实践中使用。为了克服这些问题，我们提出了一个顺序蒙特卡罗算法来拟合无限混合高斯过程，以捕捉非平稳行为，同时允许在线、分布推断。我们的方法在实验中优于现有最先进方法的性能。",
    "tldr": "本文提出了一种基于顺序蒙特卡罗算法的连续高斯过程模型，以解决高斯过程模型的计算复杂度高，难以在线顺序更新的问题，同时允许拟合具有非平稳性质的函数。方法优于现有最先进方法的性能。",
    "en_tdlr": "This paper proposes a sequential Monte Carlo algorithm for a continuous Gaussian process model to address issues such as high computational complexity and difficulty in sequential online updating of the model. It allows for fitting functions with non-stationary properties and outperforms current state-of-the-art methods."
}