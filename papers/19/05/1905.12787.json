{
    "title": "The Theory Behind Overfitting, Cross Validation, Regularization, Bagging, and Boosting: Tutorial. (arXiv:1905.12787v2 [stat.ML] UPDATED)",
    "abstract": "In this tutorial paper, we first define mean squared error, variance, covariance, and bias of both random variables and classification/predictor models. Then, we formulate the true and generalization errors of the model for both training and validation/test instances where we make use of the Stein's Unbiased Risk Estimator (SURE). We define overfitting, underfitting, and generalization using the obtained true and generalization errors. We introduce cross validation and two well-known examples which are $K$-fold and leave-one-out cross validations. We briefly introduce generalized cross validation and then move on to regularization where we use the SURE again. We work on both $\\ell_2$ and $\\ell_1$ norm regularizations. Then, we show that bootstrap aggregating (bagging) reduces the variance of estimation. Boosting, specifically AdaBoost, is introduced and it is explained as both an additive model and a maximum margin model, i.e., Support Vector Machine (SVM). The upper bound on the gener",
    "link": "http://arxiv.org/abs/1905.12787",
    "context": "Title: The Theory Behind Overfitting, Cross Validation, Regularization, Bagging, and Boosting: Tutorial. (arXiv:1905.12787v2 [stat.ML] UPDATED)\nAbstract: In this tutorial paper, we first define mean squared error, variance, covariance, and bias of both random variables and classification/predictor models. Then, we formulate the true and generalization errors of the model for both training and validation/test instances where we make use of the Stein's Unbiased Risk Estimator (SURE). We define overfitting, underfitting, and generalization using the obtained true and generalization errors. We introduce cross validation and two well-known examples which are $K$-fold and leave-one-out cross validations. We briefly introduce generalized cross validation and then move on to regularization where we use the SURE again. We work on both $\\ell_2$ and $\\ell_1$ norm regularizations. Then, we show that bootstrap aggregating (bagging) reduces the variance of estimation. Boosting, specifically AdaBoost, is introduced and it is explained as both an additive model and a maximum margin model, i.e., Support Vector Machine (SVM). The upper bound on the gener",
    "path": "papers/19/05/1905.12787.json",
    "total_tokens": 1020,
    "translated_title": "过拟合、交叉验证、正则化、装袋法和提升法背后的理论：教程",
    "translated_abstract": "在这篇教程性的论文中，我们首先定义了随机变量和分类/预测模型的均方误差、方差、协方差和偏差。然后，我们利用Stein的无偏风险估计器（SURE）制定了模型的真实和泛化误差，包括训练和验证/测试实例。利用得到的真实和泛化误差，我们定义了过拟合、欠拟合和泛化。我们介绍了交叉验证和两个著名的例子，即K倍交叉验证和留一法交叉验证。我们简要介绍了广义交叉验证，然后转向正则化，在这里我们再次使用SURE。我们对$\\ell_2$和$\\ell_1$范数正则化进行了研究。然后，我们展示了自举聚合（bagging）如何降低估计方差。我们介绍了提升法，特别是AdaBoost，并解释了它作为一个加性模型和最大间隔模型（即支持向量机（SVM））的原理。给出了AdaBoost的泛化误差上限，包括指数损失和0-1损失。最后，我们总结了教程的主要内容。",
    "tldr": "该论文介绍了过拟合、交叉验证、正则化、装袋法和提升法的相关理论，包括定义和具体实现，并给出了AdaBoost的泛化误差上限的具体计算方法。"
}