{
    "title": "Relaxed Actor-Critic with Convergence Guarantees for Continuous-Time Optimal Control of Nonlinear Systems. (arXiv:1909.05402v2 [eess.SY] UPDATED)",
    "abstract": "This paper presents the Relaxed Continuous-Time Actor-critic (RCTAC) algorithm, a method for finding the nearly optimal policy for nonlinear continuous-time (CT) systems with known dynamics and infinite horizon, such as the path-tracking control of vehicles. RCTAC has several advantages over existing adaptive dynamic programming algorithms for CT systems. It does not require the ``admissibility\" of the initialized policy or the input-affine nature of controlled systems for convergence. Instead, given any initial policy, RCTAC can converge to an admissible, and subsequently nearly optimal policy for a general nonlinear system with a saturated controller. RCTAC consists of two phases: a warm-up phase and a generalized policy iteration phase. The warm-up phase minimizes the square of the Hamiltonian to achieve admissibility, while the generalized policy iteration phase relaxes the update termination conditions for faster convergence. The convergence and optimality of the algorithm are pro",
    "link": "http://arxiv.org/abs/1909.05402",
    "context": "Title: Relaxed Actor-Critic with Convergence Guarantees for Continuous-Time Optimal Control of Nonlinear Systems. (arXiv:1909.05402v2 [eess.SY] UPDATED)\nAbstract: This paper presents the Relaxed Continuous-Time Actor-critic (RCTAC) algorithm, a method for finding the nearly optimal policy for nonlinear continuous-time (CT) systems with known dynamics and infinite horizon, such as the path-tracking control of vehicles. RCTAC has several advantages over existing adaptive dynamic programming algorithms for CT systems. It does not require the ``admissibility\" of the initialized policy or the input-affine nature of controlled systems for convergence. Instead, given any initial policy, RCTAC can converge to an admissible, and subsequently nearly optimal policy for a general nonlinear system with a saturated controller. RCTAC consists of two phases: a warm-up phase and a generalized policy iteration phase. The warm-up phase minimizes the square of the Hamiltonian to achieve admissibility, while the generalized policy iteration phase relaxes the update termination conditions for faster convergence. The convergence and optimality of the algorithm are pro",
    "path": "papers/19/09/1909.05402.json",
    "total_tokens": 1091,
    "translated_title": "针对非线性系统的连续时间最优控制的松弛演员-评论家算法及其收敛性保证",
    "translated_abstract": "本文提出了一种名为松弛连续时间演员-评论家（RCTAC）算法的方法，用于发现具有已知动态和无限地平线的非线性连续时间（CT）系统的几乎最优策略，例如车辆的路径跟踪控制。 与现有的自适应动态规划算法相比，RCTAC具有几个优点。它不需要初始化策略的“可接受性”或者控制系统的输入仿射性质以实现收敛。相反，给定任何初始策略，RCTAC都可以收敛到一个适用的，随后是几乎最优的策略，用于具有饱和控制器的一般非线性系统。RCTAC包括两个阶段：热身阶段和广义策略迭代阶段。热身阶段通过最小化哈密顿量的平方来实现可接受性，而广义策略迭代阶段放宽了更新终止条件以实现更快的收敛。算法的收敛性和最优性已被提出并证明。对于路径跟踪控制和倒立摆控制问题的模拟结果表明，RCTAC在收敛速度和控制性能方面优于现有的自适应动态规划算法。",
    "tldr": "本论文提出了一种叫做RCTAC的算法，可以针对具有已知动态和无限地平线的非线性连续时间系统找到几乎最优策略，且无需控制系统的特定属性或初始策略的可接受性，同时在收敛速度和控制性能方面均优于现有算法。",
    "en_tdlr": "This paper proposes a method called RCTAC that can find nearly optimal policies for nonlinear continuous-time systems with known dynamics and infinite horizon, and surpasses existing algorithms in convergence speed and control performance without requiring specific system properties or acceptable initial policies."
}