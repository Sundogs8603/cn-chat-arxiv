{
    "title": "Semi-supervised Vector-valued Learning: Improved Bounds and Algorithms. (arXiv:1909.04883v4 [cs.LG] UPDATED)",
    "abstract": "Vector-valued learning, where the output space admits a vector-valued structure, is an important problem that covers a broad family of important domains, e.g. multi-task learning and transfer learning. Using local Rademacher complexity and unlabeled data, we derive novel semi-supervised excess risk bounds for general vector-valued learning from both kernel perspective and linear perspective. The derived bounds are much sharper than existing ones and the convergence rates are improved from the square root of labeled sample size to the square root of total sample size or directly dependent on labeled sample size. Motivated by our theoretical analysis, we propose a general semi-supervised algorithm for efficiently learning vector-valued functions, incorporating both local Rademacher complexity and Laplacian regularization. Extensive experimental results illustrate the proposed algorithm significantly outperforms the compared methods, which coincides with our theoretical findings.",
    "link": "http://arxiv.org/abs/1909.04883",
    "context": "Title: Semi-supervised Vector-valued Learning: Improved Bounds and Algorithms. (arXiv:1909.04883v4 [cs.LG] UPDATED)\nAbstract: Vector-valued learning, where the output space admits a vector-valued structure, is an important problem that covers a broad family of important domains, e.g. multi-task learning and transfer learning. Using local Rademacher complexity and unlabeled data, we derive novel semi-supervised excess risk bounds for general vector-valued learning from both kernel perspective and linear perspective. The derived bounds are much sharper than existing ones and the convergence rates are improved from the square root of labeled sample size to the square root of total sample size or directly dependent on labeled sample size. Motivated by our theoretical analysis, we propose a general semi-supervised algorithm for efficiently learning vector-valued functions, incorporating both local Rademacher complexity and Laplacian regularization. Extensive experimental results illustrate the proposed algorithm significantly outperforms the compared methods, which coincides with our theoretical findings.",
    "path": "papers/19/09/1909.04883.json",
    "total_tokens": 867,
    "translated_title": "半监督矢量值学习：改进的界限与算法",
    "translated_abstract": "矢量值学习是一个重要的问题，其输出空间具有矢量值结构，涵盖了许多重要领域，如多任务学习和迁移学习。我们利用局部Rademacher复杂度和无标签数据，从核函数和线性方法的角度为一般矢量值学习导出了新的半监督超出风险界限。这些界限比现有的界限更精确，收敛速度从标记样本大小的平方根改进为总样本大小的平方根或直接依赖于标记样本大小。在理论分析的基础上，我们提出了一种通用的半监督算法来高效学习矢量值函数，结合了局部Rademacher复杂度和拉普拉斯正则化。大量的实验结果表明，所提出的算法明显优于其他方法，并与我们的理论结果相吻合。",
    "tldr": "该论文提出了一种针对半监督矢量值学习的改进算法，通过利用局部Rademacher复杂度和无标签数据，得出了更精确的超出风险界限。实验结果表明，该算法在多个领域中明显优于其他方法。"
}