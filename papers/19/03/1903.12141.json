{
    "title": "IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters. (arXiv:1903.12141v10 [cs.LG] UPDATED)",
    "abstract": "In this work, we study robust deep learning against abnormal training data from the perspective of example weighting built in empirical loss functions, i.e., gradient magnitude with respect to logits, an angle that is not thoroughly studied so far. Consequently, we have two key findings: (1) Mean Absolute Error (MAE) Does Not Treat Examples Equally. We present new observations and insightful analysis about MAE, which is theoretically proved to be noise-robust. First, we reveal its underfitting problem in practice. Second, we analyse that MAE's noise-robustness is from emphasising on uncertain examples instead of treating training samples equally, as claimed in prior work. (2) The Variance of Gradient Magnitude Matters. We propose an effective and simple solution to enhance MAE's fitting ability while preserving its noise-robustness. Without changing MAE's overall weighting scheme, i.e., what examples get higher weights, we simply change its weighting variance non-linearly so that the i",
    "link": "http://arxiv.org/abs/1903.12141",
    "context": "Title: IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters. (arXiv:1903.12141v10 [cs.LG] UPDATED)\nAbstract: In this work, we study robust deep learning against abnormal training data from the perspective of example weighting built in empirical loss functions, i.e., gradient magnitude with respect to logits, an angle that is not thoroughly studied so far. Consequently, we have two key findings: (1) Mean Absolute Error (MAE) Does Not Treat Examples Equally. We present new observations and insightful analysis about MAE, which is theoretically proved to be noise-robust. First, we reveal its underfitting problem in practice. Second, we analyse that MAE's noise-robustness is from emphasising on uncertain examples instead of treating training samples equally, as claimed in prior work. (2) The Variance of Gradient Magnitude Matters. We propose an effective and simple solution to enhance MAE's fitting ability while preserving its noise-robustness. Without changing MAE's overall weighting scheme, i.e., what examples get higher weights, we simply change its weighting variance non-linearly so that the i",
    "path": "papers/19/03/1903.12141.json",
    "total_tokens": 990,
    "translated_title": "IMAE用于噪声鲁棒学习：绝对值误差不平等对待示例，梯度大小的方差很重要。",
    "translated_abstract": "本文研究了从示例加权角度，即与对数的梯度大小来看待畸形训练数据的鲁棒深度学习。我们有两个关键发现：（1）平均绝对误差（MAE）不平等地处理示例。我们针对MAE进行了新的观察和深入分析，理论证明其鲁棒性。首先，我们揭示了其在实践中的欠拟合问题。其次，我们分析了MAE的鲁棒性是通过强调不确定示例而不是像前人研究中所声称的那样对待训练样本来实现的。（2）梯度大小的方差很重要。我们提出了一种有效而简单的解决方案，以增强MAE的拟合能力，同时保持其鲁棒性。在不改变MAE的整体加权方案（即哪些示例获得更高的权重）的情况下，我们仅通过非线性地改变其加权方差来实现这一点。",
    "tldr": "本文提出IMAE模型用于畸形训练数据的鲁棒深度学习，通过实践证实平均绝对误差（MAE）在处理示例时存在欠拟合问题，利用加权方差调整提高了拟合能力，同时保持了鲁棒性。",
    "en_tdlr": "This paper proposes the IMAE model for robust deep learning against abnormal training data, which empirically shows that Mean Absolute Error (MAE) has underfitting problem in practice and proposes an effective solution by adjusting the weighting variance to enhance the fitting ability while preserving robustness."
}