{
    "title": "Model-Based Reinforcement Learning for Atari",
    "abstract": "arXiv:1903.00374v5 Announce Type: replace  Abstract: Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k",
    "link": "https://arxiv.org/abs/1903.00374",
    "context": "Title: Model-Based Reinforcement Learning for Atari\nAbstract: arXiv:1903.00374v5 Announce Type: replace  Abstract: Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k",
    "path": "papers/19/03/1903.00374.json",
    "total_tokens": 884,
    "translated_title": "基于模型的强化学习在Atari中的应用",
    "translated_abstract": "无模型的强化学习（RL）可以用于从图像观察中学习有效的策略，例如Atari游戏，但通常需要非常大量的交互——实际上，远远超过人类学习相同游戏所需的数量。人们是如何如此快速学习的？答案的一部分可能是人们可以学习游戏运行的方式，并预测哪些动作会产生期望的结果。本文探讨了视频预测模型如何使代理能够在比无模型方法交互更少的情况下解决Atari游戏。我们描述了Simulated Policy Learning（SimPLe），这是一个基于视频预测模型的完整的基于模型的深度RL算法，并对几种模型体系结构进行了比较，包括一个在我们的情境中取得最佳结果的新颖结构。我们的实验评估了SimPLe在100k低数据条件下的一系列Atari游戏中的表现。",
    "tldr": "本研究探索了如何利用视频预测模型实现基于模型的深度RL算法SimPLe，在Atari游戏中比无模型方法更有效地解决问题，并通过实验验证了新颖模型体系结构在这一背景下取得最佳结果。",
    "en_tdlr": "This study explores how to use video prediction models to implement the model-based deep RL algorithm SimPLe, which solves Atari games more effectively than model-free methods, and experimentally validates the novel model architecture achieving the best results in this context."
}