{
    "title": "Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization. (arXiv:1903.02140v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank. If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are ",
    "link": "http://arxiv.org/abs/1903.02140",
    "context": "Title: Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization. (arXiv:1903.02140v2 [cs.LG] UPDATED)\nAbstract: In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank. If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are ",
    "path": "papers/19/03/1903.02140.json",
    "total_tokens": 1013,
    "translated_title": "大规模神经网络学习为什么行为类似于凸优化",
    "translated_abstract": "本文提出了一些理论工作，以解释为什么简单的梯度下降方法在解决学习大规模神经网络的非凸优化问题中如此成功。在介绍了一种称为规范空间的数学工具之后，我们证明了规范模型空间中的学习NN目标函数是凸的。我们进一步阐明了原始NN模型空间和规范空间之间的梯度之间的关系是通过所谓的差异矩阵表示的逐点线性变换相关的。此外，我们已经证明，如果差异矩阵保持完整秩，梯度下降方法一定会收敛到零损失的全局最小值。如果这个完整秩条件成立，在NN的学习中的行为与正常的凸优化相同。最后，我们证明，大规模NN具有奇异的差异矩阵的概率非常小。特别是，当超参数化的NN是...",
    "tldr": "本文介绍了利用规范空间证明学习大规模神经网络的目标函数在收敛到全局最小值时是凸优化问题。使用点线性转换的方法建立原始NN模型空间和规范空间之间的关系，证明了使用梯度下降方法，只要差异矩阵保持完整秩，就一定能收敛到零损失的全局最小值。大规模NN具有奇异的差异矩阵的概率非常小。"
}