{
    "title": "Advanced Capsule Networks via Context Awareness. (arXiv:1903.07497v3 [cs.LG] UPDATED)",
    "abstract": "Capsule Networks (CN) offer new architectures for Deep Learning (DL) community. Though its effectiveness has been demonstrated in MNIST and smallNORB datasets, the networks still face challenges in other datasets for images with distinct contexts. In this research, we improve the design of CN (Vector version) namely we expand more Pooling layers to filter image backgrounds and increase Reconstruction layers to make better image restoration. Additionally, we perform experiments to compare accuracy and speed of CN versus DL models. In DL models, we utilize Inception V3 and DenseNet V201 for powerful computers besides NASNet, MobileNet V1 and MobileNet V2 for small and embedded devices. We evaluate our models on a fingerspelling alphabet dataset from American Sign Language (ASL). The results show that CNs perform comparably to DL models while dramatically reducing training time. We also make a demonstration and give a link for the purpose of illustration.",
    "link": "http://arxiv.org/abs/1903.07497",
    "context": "Title: Advanced Capsule Networks via Context Awareness. (arXiv:1903.07497v3 [cs.LG] UPDATED)\nAbstract: Capsule Networks (CN) offer new architectures for Deep Learning (DL) community. Though its effectiveness has been demonstrated in MNIST and smallNORB datasets, the networks still face challenges in other datasets for images with distinct contexts. In this research, we improve the design of CN (Vector version) namely we expand more Pooling layers to filter image backgrounds and increase Reconstruction layers to make better image restoration. Additionally, we perform experiments to compare accuracy and speed of CN versus DL models. In DL models, we utilize Inception V3 and DenseNet V201 for powerful computers besides NASNet, MobileNet V1 and MobileNet V2 for small and embedded devices. We evaluate our models on a fingerspelling alphabet dataset from American Sign Language (ASL). The results show that CNs perform comparably to DL models while dramatically reducing training time. We also make a demonstration and give a link for the purpose of illustration.",
    "path": "papers/19/03/1903.07497.json",
    "total_tokens": 954,
    "translated_title": "基于上下文感知的先进胶囊网络",
    "translated_abstract": "胶囊网络（CN）为深度学习（DL）社区提供了新的架构。尽管它的有效性已经在MNIST和smallNORB数据集中得到了证明，但是对于具有不同上下文的图像的数据集，该网络仍然面临挑战。在这项研究中，我们改进了CN（向量版本）的设计，具体来说，我们增加了更多的池化层来过滤图像背景，并增加了更多的重建层来实现更好的图像恢复。此外，我们进行了实验，比较了CN和DL模型的准确性和速度。在DL模型中，除了在强大的计算机上使用Inception V3和DenseNet V201外，我们还使用了NASNet、MobileNet V1和MobileNet V2来适用于小型和嵌入式设备。我们在美国手语（ASL）的手指拼写字母数据集上评估了我们的模型。结果表明，CN与DL模型相比，在大大减少训练时间的同时表现出了可比较的性能。我们还进行了演示，并提供了一个链接以进行说明。",
    "tldr": "本研究通过增加池化层和重建层来改进胶囊网络（CN）的设计，以适应具有不同上下文的图像数据集，并与深度学习（DL）模型进行了性能对比。结果显示，CN在大大减少训练时间的同时表现出了与DL模型相当的性能。",
    "en_tdlr": "This study improves the design of Capsule Networks (CN) by adding more pooling layers and reconstruction layers to adapt to image datasets with distinct contexts, and compares their performance with Deep Learning (DL) models. The results demonstrate that CNs achieve comparable performance to DL models while significantly reducing training time."
}