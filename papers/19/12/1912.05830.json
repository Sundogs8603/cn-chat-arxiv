{
    "title": "Provably Efficient Exploration in Policy Optimization",
    "abstract": "arXiv:1912.05830v4 Announce Type: replace  Abstract: While policy-based reinforcement learning (RL) achieves tremendous successes in practice, it is significantly less understood in theory, especially compared with value-based RL. In particular, it remains elusive how to design a provably efficient policy optimization algorithm that incorporates exploration. To bridge such a gap, this paper proposes an Optimistic variant of the Proximal Policy Optimization algorithm (OPPO), which follows an ``optimistic version'' of the policy gradient direction. This paper proves that, in the problem of episodic Markov decision process with linear function approximation, unknown transition, and adversarial reward with full-information feedback, OPPO achieves $\\tilde{O}(\\sqrt{d^2 H^3 T} )$ regret. Here $d$ is the feature dimension, $H$ is the episode horizon, and $T$ is the total number of steps. To the best of our knowledge, OPPO is the first provably efficient policy optimization algorithm that explo",
    "link": "https://arxiv.org/abs/1912.05830",
    "context": "Title: Provably Efficient Exploration in Policy Optimization\nAbstract: arXiv:1912.05830v4 Announce Type: replace  Abstract: While policy-based reinforcement learning (RL) achieves tremendous successes in practice, it is significantly less understood in theory, especially compared with value-based RL. In particular, it remains elusive how to design a provably efficient policy optimization algorithm that incorporates exploration. To bridge such a gap, this paper proposes an Optimistic variant of the Proximal Policy Optimization algorithm (OPPO), which follows an ``optimistic version'' of the policy gradient direction. This paper proves that, in the problem of episodic Markov decision process with linear function approximation, unknown transition, and adversarial reward with full-information feedback, OPPO achieves $\\tilde{O}(\\sqrt{d^2 H^3 T} )$ regret. Here $d$ is the feature dimension, $H$ is the episode horizon, and $T$ is the total number of steps. To the best of our knowledge, OPPO is the first provably efficient policy optimization algorithm that explo",
    "path": "papers/19/12/1912.05830.json",
    "total_tokens": 899,
    "translated_title": "在策略优化中实现可证明高效的探索",
    "translated_abstract": "虽然基于策略的强化学习（RL）在实践中取得了巨大成功，但在理论上却远不如基于值函数的RL被理解的充分。具体来说，如何设计一个在探索中综合高效的策略优化算法仍然是模糊的。为弥合这一差距，本文提出了一种Proximal Policy Optimization算法的\"乐观变体\"（OPPO），其遵循“策略梯度方向”的“乐观版本”。本文证明，对于具有线性函数近似、未知转移和具有完全信息反馈的对抗性奖励的基于情节马尔可夫决策过程问题，OPPO实现了 $\\tilde{O}(\\sqrt{d^2 H^3 T} )$ 的遗憾。其中，$d$ 是特征维度，$H$ 是情节长度，$T$ 是总步数。就我们所知，OPPO是第一个可证明高效的策略优化算法。",
    "tldr": "OPPO是第一个在探索中高效的策略优化算法，在处理具有线性函数近似、未知转移和对抗性奖励的问题中取得了 $\\tilde{O}(\\sqrt{d^2 H^3 T} )$ 的遗憾。"
}