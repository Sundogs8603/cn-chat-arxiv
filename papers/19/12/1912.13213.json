{
    "title": "A Modern Introduction to Online Learning. (arXiv:1912.13213v6 [cs.LG] UPDATED)",
    "abstract": "In this monograph, I introduce the basic concepts of Online Learning through a modern view of Online Convex Optimization. Here, online learning refers to the framework of regret minimization under worst-case assumptions. I present first-order and second-order algorithms for online learning with convex losses, in Euclidean and non-Euclidean settings. All the algorithms are clearly presented as instantiation of Online Mirror Descent or Follow-The-Regularized-Leader and their variants. Particular attention is given to the issue of tuning the parameters of the algorithms and learning in unbounded domains, through adaptive and parameter-free online learning algorithms. Non-convex losses are dealt through convex surrogate losses and through randomization. The bandit setting is also briefly discussed, touching on the problem of adversarial and stochastic multi-armed bandits. These notes do not require prior knowledge of convex analysis and all the required mathematical tools are rigorously ex",
    "link": "http://arxiv.org/abs/1912.13213",
    "context": "Title: A Modern Introduction to Online Learning. (arXiv:1912.13213v6 [cs.LG] UPDATED)\nAbstract: In this monograph, I introduce the basic concepts of Online Learning through a modern view of Online Convex Optimization. Here, online learning refers to the framework of regret minimization under worst-case assumptions. I present first-order and second-order algorithms for online learning with convex losses, in Euclidean and non-Euclidean settings. All the algorithms are clearly presented as instantiation of Online Mirror Descent or Follow-The-Regularized-Leader and their variants. Particular attention is given to the issue of tuning the parameters of the algorithms and learning in unbounded domains, through adaptive and parameter-free online learning algorithms. Non-convex losses are dealt through convex surrogate losses and through randomization. The bandit setting is also briefly discussed, touching on the problem of adversarial and stochastic multi-armed bandits. These notes do not require prior knowledge of convex analysis and all the required mathematical tools are rigorously ex",
    "path": "papers/19/12/1912.13213.json",
    "total_tokens": 926,
    "translated_title": "在线学习的现代介绍",
    "translated_abstract": "在这本专著中，我通过现代的在线凸优化视角介绍了在线学习的基本概念。在这里，在线学习指的是在最坏情况下的遗憾最小化框架。我介绍了一阶和二阶具有凸损失的在线学习算法，以欧几里得和非欧几里得设置为基础，所有算法都清晰地呈现为在线镜像下降或遵循正则化领导者及其变形的实例。特别关注算法参数的调整问题和通过自适应和无参数在线学习算法在无界域中的学习。 凸损失通过凸代理损失和随机化处理，来处理非凸损失。同时还简要讨论了赌博设置，涉及敌对和随机多臂赌博问题。这些笔记不需要先前对凸分析的了解，并且所有所需的数学工具都已严谨解释。",
    "tldr": "这本专著介绍了在线学习的基本概念以及凸优化背景下的一阶和二阶算法, 包括欧几里得和非欧几里得设置中的在线镜像下降或遵循正则化领导者等算法。",
    "en_tdlr": "This monograph provides a modern introduction to online learning, discussing the basic concepts of regret minimization under worst-case assumptions and presenting first-order and second-order algorithms in Euclidean and non-Euclidean settings. It emphasizes the tuning of algorithm parameters and the use of adaptive and parameter-free online learning algorithms for learning in unbounded domains. Additionally, convex surrogate losses and randomization are employed to deal with non-convex losses, and the bandit setting is also briefly discussed."
}