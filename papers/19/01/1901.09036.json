{
    "title": "Orthogonal Statistical Learning. (arXiv:1901.09036v4 [math.ST] UPDATED)",
    "abstract": "We provide non-asymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input arbitrary estimation algorithms for the target parameter and nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can provide rates under weaker a",
    "link": "http://arxiv.org/abs/1901.09036",
    "context": "Title: Orthogonal Statistical Learning. (arXiv:1901.09036v4 [math.ST] UPDATED)\nAbstract: We provide non-asymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input arbitrary estimation algorithms for the target parameter and nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can provide rates under weaker a",
    "path": "papers/19/01/1901.09036.json",
    "total_tokens": 833,
    "translated_title": "正交统计学习",
    "translated_abstract": "我们在一个统计学习的设置下提供了关于非渐近超额风险保证，其中目标参数所评估的总体风险取决于必须从数据中估计的未知干扰参数。我们分析了一个两阶段样本拆分的元算法，该算法将任意估计目标参数和干扰参数的算法作为输入。我们表明，如果总体风险满足一个称为Neyman正交性的条件，则干扰估计误差对元算法实现的超额风险界的影响为二次。我们的定理不关心用于目标和干扰的特定算法，只做出了有关它们各自性能的假设。这样，就可以利用现有机器学习的大量结果，为带有干扰组成的学习提供新的保证。此外，通过关注超额风险而不是参数估计，我们可以提供一个弱化的速率。",
    "tldr": "本文提出了一个两阶段样本拆分的元算法，该算法能够在评估总体风险时考虑干扰参数，并且实现的超额风险界的影响为二次。",
    "en_tdlr": "This paper proposes a two-stage sample splitting meta-algorithm that can consider nuisance parameters when evaluating population risk, and the impact of the excess risk bound achieved by the meta-algorithm is of second order."
}