{
    "title": "Towards Learning to Imitate from a Single Video Demonstration. (arXiv:1901.07186v4 [cs.LG] UPDATED)",
    "abstract": "Agents that can learn to imitate given video observation -- \\emph{without direct access to state or action information} are more applicable to learning in the natural world. However, formulating a reinforcement learning (RL) agent that facilitates this goal remains a significant challenge. We approach this challenge using contrastive training to learn a reward function comparing an agent's behaviour with a single demonstration. We use a Siamese recurrent neural network architecture to learn rewards in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we also find that the inclusion of multi-task data and additional image encoding losses improve the temporal consistency of the learned rewards and, as a result, significantly improves policy learning. We demonstrate our approach on simulated humanoid, dog, and raptor agents in 2D and a quadruped and a humanoid in 3D. We show that our method outperforms current state-of-the-",
    "link": "http://arxiv.org/abs/1901.07186",
    "context": "Title: Towards Learning to Imitate from a Single Video Demonstration. (arXiv:1901.07186v4 [cs.LG] UPDATED)\nAbstract: Agents that can learn to imitate given video observation -- \\emph{without direct access to state or action information} are more applicable to learning in the natural world. However, formulating a reinforcement learning (RL) agent that facilitates this goal remains a significant challenge. We approach this challenge using contrastive training to learn a reward function comparing an agent's behaviour with a single demonstration. We use a Siamese recurrent neural network architecture to learn rewards in space and time between motion clips while training an RL policy to minimize this distance. Through experimentation, we also find that the inclusion of multi-task data and additional image encoding losses improve the temporal consistency of the learned rewards and, as a result, significantly improves policy learning. We demonstrate our approach on simulated humanoid, dog, and raptor agents in 2D and a quadruped and a humanoid in 3D. We show that our method outperforms current state-of-the-",
    "path": "papers/19/01/1901.07186.json",
    "total_tokens": 1101,
    "translated_title": "从一个单一的视频演示中学习模仿的方法",
    "translated_abstract": "能够从视频观察中学习模仿的智能体——\\emph{没有直接访问状态或动作信息}，对于在自然世界中的学习更具适用性。然而，制定一个能够实现此目标的增强学习（RL）智能体仍然是一个重大挑战。我们使用对比训练来解决这个挑战，学习一个将智能体的行为与单个演示进行比较的奖励函数。我们使用连体循环神经网络架构在时间和空间上学习动作片段之间的奖励，同时训练一个RL策略来最小化这个距离。通过实验证明，多任务数据和额外的图像编码损失的引入改进了学习到的奖励的时间一致性，从而显着提高了策略学习的效果。我们在2D中的模拟人型、狗和迅猛龙智能体以及3D中的四足动物和人型智能体上展示了我们的方法。我们展示了我们的方法优于当前最先进的状态。",
    "tldr": "本研究提出了一种从单一视频演示中学习模仿的方法，通过使用对比训练和Siamese循环神经网络，我们能够学习到智能体的行为与演示之间的奖励函数，并通过 RL 策略的训练最小化这个距离。实验表明，引入多任务数据和额外的图像编码损失可以改善学习到的奖励的时间一致性，并显著提高策略学习的效果。我们在不同维度的仿真智能体上验证了我们的方法的优越性。",
    "en_tdlr": "This research proposes a method for learning imitation from a single video demonstration. By using contrastive training and a Siamese recurrent neural network, we are able to learn a reward function comparing an agent's behavior with the demonstration and minimize this distance through RL policy training. Experimentation shows that the inclusion of multi-task data and additional image encoding losses improves the temporal consistency of the learned rewards and significantly enhances policy learning. The method is validated on simulated agents in both 2D and 3D dimensions, outperforming the current state-of-the-art."
}