{
    "title": "Model Fusion via Optimal Transport. (arXiv:1910.05653v6 [cs.LG] UPDATED)",
    "abstract": "Combining different models is a widely used paradigm in machine learning applications. While the most common approach is to form an ensemble of models and average their individual predictions, this approach is often rendered infeasible by given resource constraints in terms of memory and computation, which grow linearly with the number of models. We present a layer-wise model fusion algorithm for neural networks that utilizes optimal transport to (soft-) align neurons across the models before averaging their associated parameters.  We show that this can successfully yield \"one-shot\" knowledge transfer (i.e, without requiring any retraining) between neural networks trained on heterogeneous non-i.i.d. data. In both i.i.d. and non-i.i.d. settings , we illustrate that our approach significantly outperforms vanilla averaging, as well as how it can serve as an efficient replacement for the ensemble with moderate fine-tuning, for standard convolutional networks (like VGG11), residual networks",
    "link": "http://arxiv.org/abs/1910.05653",
    "context": "Title: Model Fusion via Optimal Transport. (arXiv:1910.05653v6 [cs.LG] UPDATED)\nAbstract: Combining different models is a widely used paradigm in machine learning applications. While the most common approach is to form an ensemble of models and average their individual predictions, this approach is often rendered infeasible by given resource constraints in terms of memory and computation, which grow linearly with the number of models. We present a layer-wise model fusion algorithm for neural networks that utilizes optimal transport to (soft-) align neurons across the models before averaging their associated parameters.  We show that this can successfully yield \"one-shot\" knowledge transfer (i.e, without requiring any retraining) between neural networks trained on heterogeneous non-i.i.d. data. In both i.i.d. and non-i.i.d. settings , we illustrate that our approach significantly outperforms vanilla averaging, as well as how it can serve as an efficient replacement for the ensemble with moderate fine-tuning, for standard convolutional networks (like VGG11), residual networks",
    "path": "papers/19/10/1910.05653.json",
    "total_tokens": 925,
    "translated_title": "基于最优传输的模型融合方法",
    "translated_abstract": "在机器学习应用中，结合不同的模型是一个广泛使用的范例。尽管最常见的方法是形成一个模型集合并平均它们的各自预测，但由于资源限制（以内存和计算的方式呈线性增长于模型数），这种方法常常无法实现。我们提出了一种基于层次的神经网络模型融合算法，利用最优传输来（软）对齐模型中的神经元，然后平均它们的相关参数。我们展示了这可以在异构非独立同分布数据上，成功地实现“单次”知识迁移（即不需要任何重新训练）在神经网络之间。在独立同分布和非独立同分布的情况下，我们示范了我们的方法明显优于简单平均以及如何在标准卷积网络（如VGG11）、残差网络上进行快速优化替代集成模型。",
    "tldr": "本文提出一种基于最优传输的神经网络模型融合算法，能够成功地在不需要重新训练的情况下进行“单次”知识迁移，并且在独立同分布和非独立同分布的情况下比简单平均和集成模型更优。",
    "en_tdlr": "This paper proposes a neural network model fusion algorithm based on optimal transport, which can successfully achieve \"one-shot\" knowledge transfer without requiring retraining, and outperforms vanilla averaging and ensemble models in both i.i.d. and non-i.i.d. settings, serving as an efficient replacement for ensemble models with moderate fine-tuning on standard convolutional networks like VGG11 and residual networks."
}