{
    "title": "Dynamic Subgoal-based Exploration via Bayesian Optimization. (arXiv:1910.09143v4 [math.OC] UPDATED)",
    "abstract": "Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration. Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy. It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution. Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for cost-efficient exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety of levers -- the locations of the subgoals, the length of each episode, and the nu",
    "link": "http://arxiv.org/abs/1910.09143",
    "context": "Title: Dynamic Subgoal-based Exploration via Bayesian Optimization. (arXiv:1910.09143v4 [math.OC] UPDATED)\nAbstract: Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration. Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy. It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution. Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for cost-efficient exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety of levers -- the locations of the subgoals, the length of each episode, and the nu",
    "path": "papers/19/10/1910.09143.json",
    "total_tokens": 865,
    "translated_title": "基于贝叶斯优化的动态子目标导向的探索",
    "translated_abstract": "在稀疏奖励导航环境中，通过昂贵和有限的交互中进行强化学习是具有挑战性的，需要有效的探索策略。针对需要现实世界训练的复杂导航任务（当廉价模拟器不可用时），我们考虑一个面临未知环境分布的代理，并且必须决定一种探索策略。在从相同环境分布中选择的测试环境中评估之前，它可以利用一系列训练环境来改进其策略。大多数现有方法关注固定的探索策略，而将探索视为元优化问题的少数方法往往忽视了对成本高效的探索的需求。我们提出了一种成本感知的贝叶斯优化方法，可以高效地搜索一类基于动态子目标的探索策略。该算法调整多个杠杆--子目标的位置，每个episode的长度以及nu的值。",
    "tldr": "本论文提出了一种基于贝叶斯优化的成本感知探索方法，能够针对稀疏奖励导航环境中的复杂任务，高效地搜索动态子目标的探索策略。",
    "en_tdlr": "This paper proposes a cost-aware Bayesian optimization approach for effective exploration in sparse-reward navigation environments, allowing efficient search for dynamic subgoal-based exploration strategies."
}