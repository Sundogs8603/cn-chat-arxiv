{
    "title": "Improving the convergence of SGD through adaptive batch sizes. (arXiv:1910.08222v4 [cs.LG] UPDATED)",
    "abstract": "Mini-batch stochastic gradient descent (SGD) and variants thereof approximate the objective function's gradient with a small number of training examples, aka the batch size. Small batch sizes require little computation for each model update but can yield high-variance gradient estimates, which poses some challenges for optimization. Conversely, large batches require more computation but can yield higher precision gradient estimates. This work presents a method to adapt the batch size to the model's training loss. For various function classes, we show that our method requires the same order of model updates as gradient descent while requiring the same order of gradient computations as SGD. This method requires evaluating the model's loss on the entire dataset every model update. However, the required computation is greatly reduced by approximating the training loss. We provide experiments that illustrate our methods require fewer model updates without increasing the total amount of comp",
    "link": "http://arxiv.org/abs/1910.08222",
    "context": "Title: Improving the convergence of SGD through adaptive batch sizes. (arXiv:1910.08222v4 [cs.LG] UPDATED)\nAbstract: Mini-batch stochastic gradient descent (SGD) and variants thereof approximate the objective function's gradient with a small number of training examples, aka the batch size. Small batch sizes require little computation for each model update but can yield high-variance gradient estimates, which poses some challenges for optimization. Conversely, large batches require more computation but can yield higher precision gradient estimates. This work presents a method to adapt the batch size to the model's training loss. For various function classes, we show that our method requires the same order of model updates as gradient descent while requiring the same order of gradient computations as SGD. This method requires evaluating the model's loss on the entire dataset every model update. However, the required computation is greatly reduced by approximating the training loss. We provide experiments that illustrate our methods require fewer model updates without increasing the total amount of comp",
    "path": "papers/19/10/1910.08222.json",
    "total_tokens": 854,
    "translated_title": "通过自适应批大小改善SGD的收敛性",
    "translated_abstract": "小批量随机梯度下降（SGD）及其变种使用少量训练样本来近似目标函数的梯度，也就是批大小。小批量大小在每个模型更新时需要较少的计算量，但可能导致高方差的梯度估计，这对优化来说是一些挑战。相反，大批量需要更多计算量，但可能产生更高精度的梯度估计。本文提出了一种将批大小调整到模型训练损失的方法。对于各种函数类，我们证明了我们的方法对于模型更新来说需要与梯度下降相同数量的次数，同时对于梯度计算来说需要与SGD相同数量的次数。该方法需要在每个模型更新时计算整个数据集上的损失，但通过近似训练损失可以大大减少所需的计算量。我们提供了实验证明我们的方法需要更少的模型更新而不增加总计算量。",
    "tldr": "通过自适应批大小，本研究提出了一种改善SGD收敛性的方法，既减少了高方差梯度估计的问题，又保持了较高精度的梯度估计。",
    "en_tdlr": "This paper proposes a method to improve the convergence of SGD by adapting the batch size to the model's training loss, effectively reducing high-variance gradient estimates without sacrificing precision."
}