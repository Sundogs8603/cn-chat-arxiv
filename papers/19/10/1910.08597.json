{
    "title": "Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic",
    "abstract": "arXiv:1910.08597v5 Announce Type: replace-cross  Abstract: This paper proposes SplitSGD, a new dynamic learning rate schedule for stochastic optimization. This method decreases the learning rate for better adaptation to the local geometry of the objective function whenever a stationary phase is detected, that is, the iterates are likely to bounce at around a vicinity of a local minimum. The detection is performed by splitting the single thread into two and using the inner product of the gradients from the two threads as a measure of stationarity. Owing to this simple yet provably valid stationarity detection, SplitSGD is easy-to-implement and essentially does not incur additional computational cost than standard SGD. Through a series of extensive experiments, we show that this method is appropriate for both convex problems and training (non-convex) neural networks, with performance compared favorably to other stochastic optimization methods. Importantly, this method is observed to be v",
    "link": "https://arxiv.org/abs/1910.08597",
    "context": "Title: Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic\nAbstract: arXiv:1910.08597v5 Announce Type: replace-cross  Abstract: This paper proposes SplitSGD, a new dynamic learning rate schedule for stochastic optimization. This method decreases the learning rate for better adaptation to the local geometry of the objective function whenever a stationary phase is detected, that is, the iterates are likely to bounce at around a vicinity of a local minimum. The detection is performed by splitting the single thread into two and using the inner product of the gradients from the two threads as a measure of stationarity. Owing to this simple yet provably valid stationarity detection, SplitSGD is easy-to-implement and essentially does not incur additional computational cost than standard SGD. Through a series of extensive experiments, we show that this method is appropriate for both convex problems and training (non-convex) neural networks, with performance compared favorably to other stochastic optimization methods. Importantly, this method is observed to be v",
    "path": "papers/19/10/1910.08597.json",
    "total_tokens": 851,
    "translated_title": "通过分裂诊断实现随机优化的稳健学习率选择",
    "translated_abstract": "这篇论文提出了SplitSGD，这是一种用于随机优化的新的动态学习率调度方法。该方法通过在检测到稳态阶段时降低学习速率，以更好地适应目标函数的局部几何结构，即当迭代处于局部最小值附近时可能会出现反弹。通过将单线程分成两个部分，并使用两个线程梯度的内积作为稳态度量来执行检测。基于这个简单但经过验证有效的稳态检测，SplitSGD易于实现，并且基本不会比标准SGD产生额外的计算成本。通过一系列广泛的实验，我们展示了该方法既适用于凸问题，也适用于训练（非凸）神经网络，表现比其他随机优化方法更好。重要的是，观察到该方法",
    "tldr": "该方法提出了SplitSGD，通过简单而有效的稳态检测，在检测到稳态阶段时降低学习速率，使其适用于凸问题和训练神经网络，表现优于其他随机优化方法。",
    "en_tdlr": "The method proposes SplitSGD, which decreases the learning rate when a stationary phase is detected for better adaptation to the local geometry of the objective function, showing superior performance for both convex problems and training neural networks compared to other stochastic optimization methods."
}