{
    "title": "Adaptive Estimators Show Information Compression in Deep Neural Networks. (arXiv:1902.09037v2 [cs.LG] UPDATED)",
    "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we ",
    "link": "http://arxiv.org/abs/1902.09037",
    "context": "Title: Adaptive Estimators Show Information Compression in Deep Neural Networks. (arXiv:1902.09037v2 [cs.LG] UPDATED)\nAbstract: To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we ",
    "path": "papers/19/02/1902.09037.json",
    "total_tokens": 876,
    "translated_title": "深度神经网络中的自适应估计器显示信息压缩",
    "translated_abstract": "为了改善神经网络的功能，理解它们的学习过程至关重要。深度学习的信息瓶颈理论提出，神经网络通过压缩它们的表示来忽略与任务无关的信息，从而实现良好的泛化性能。然而，对于这个理论的经验证据是相互矛盾的，因为只有当网络使用饱和激活函数时才观察到压缩。相反，具有非饱和激活函数的网络实现了可比较的任务性能水平，但没有显示出压缩。在本文中，我们开发了更强大的互信息估计技术，适应于神经网络的隐藏活动，并产生更敏感的从所有函数中激活的测量结果，特别是无界函数。利用这些自适应估计技术，我们研究了带有不同激活函数的网络中的压缩情况。首先，我们使用了两种改进的估计方法，...",
    "tldr": "本文使用自适应估计技术研究深度网络中的信息压缩，发现相比使用饱和激活函数，非饱和激活函数的网络能实现可比的任务性能水平，但无法显示出信息压缩。",
    "en_tdlr": "This paper uses adaptive estimation techniques to study information compression in deep networks and finds that networks with non-saturating activation functions achieve comparable task performance to networks with saturating activation functions but do not show information compression."
}