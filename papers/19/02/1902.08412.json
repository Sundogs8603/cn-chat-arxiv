{
    "title": "Adversarial Attacks on Graph Neural Networks via Meta Learning. (arXiv:1902.08412v2 [cs.LG] UPDATED)",
    "abstract": "Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
    "link": "http://arxiv.org/abs/1902.08412",
    "context": "Title: Adversarial Attacks on Graph Neural Networks via Meta Learning. (arXiv:1902.08412v2 [cs.LG] UPDATED)\nAbstract: Deep learning models for graphs have advanced the state of the art on many tasks. Despite their recent success, little is known about their robustness. We investigate training time attacks on graph neural networks for node classification that perturb the discrete graph structure. Our core principle is to use meta-gradients to solve the bilevel problem underlying training-time attacks, essentially treating the graph as a hyperparameter to optimize. Our experiments show that small graph perturbations consistently lead to a strong decrease in performance for graph convolutional networks, and even transfer to unsupervised embeddings. Remarkably, the perturbations created by our algorithm can misguide the graph neural networks such that they perform worse than a simple baseline that ignores all relational information. Our attacks do not assume any knowledge about or access to the target classifiers.",
    "path": "papers/19/02/1902.08412.json",
    "total_tokens": 852,
    "translated_title": "通过元学习对图神经网络进行对抗攻击",
    "translated_abstract": "图神经网络被广泛应用于许多任务中并取得了最新的成功，但它们的鲁棒性还知之甚少。本文通过研究对节点分类中的图神经网络进行的训练时攻击，在离散图结构上进行微小扰动。我们的核心原则是使用元梯度来解决训练时攻击背后的双层问题，本质上将图视为优化的超参数。我们的实验证明，微小的图扰动通常会导致图卷积网络性能的大幅下降，甚至传递给无监督嵌入。值得注意的是，我们的算法所创建的扰动可以误导图神经网络，使其性能比忽略所有关联信息的简单基线模型更差。我们的攻击不需要任何关于目标分类器的知识或访问权限。",
    "tldr": "本文通过元梯度方式对图神经网络进行训练时攻击，通过微小的图扰动导致性能下降，并证明了即使在无监督嵌入中也能产生迁移效应。这些攻击不需要任何关于目标分类器的知识或访问权限。",
    "en_tdlr": "This paper proposes training time attacks on graph neural networks using meta-gradients, which lead to a decrease in performance by creating small graph perturbations. Transfer effects are observed even in unsupervised embeddings, and the attacks require no knowledge or access to the target classifiers."
}