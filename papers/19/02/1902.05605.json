{
    "title": "CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity. (arXiv:1902.05605v3 [cs.LG] UPDATED)",
    "abstract": "Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce Cross$Q$: a lightweight algorithm that makes careful use of Batch Normalization and removes target networks to surpass the state-of-the-art in sample efficiency while maintaining a low UTD ratio of $1$. Notably, Cross$Q$ does not rely on advanced bias-reduction schemes used in current methods. Cross$Q$'s contributions are thus threefold: (1) state-of-the-art sample efficiency, (2) substantial reduction in computational cost compared to REDQ and DroQ, and (3) ease of implementation, requiring just a few lines of code on top of SAC.",
    "link": "http://arxiv.org/abs/1902.05605",
    "context": "Title: CrossQ: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity. (arXiv:1902.05605v3 [cs.LG] UPDATED)\nAbstract: Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce Cross$Q$: a lightweight algorithm that makes careful use of Batch Normalization and removes target networks to surpass the state-of-the-art in sample efficiency while maintaining a low UTD ratio of $1$. Notably, Cross$Q$ does not rely on advanced bias-reduction schemes used in current methods. Cross$Q$'s contributions are thus threefold: (1) state-of-the-art sample efficiency, (2) substantial reduction in computational cost compared to REDQ and DroQ, and (3) ease of implementation, requiring just a few lines of code on top of SAC.",
    "path": "papers/19/02/1902.05605.json",
    "total_tokens": 917,
    "translated_title": "CrossQ: 用于提高深度强化学习样本效率和简洁性的批归一化方法",
    "translated_abstract": "在深度强化学习中，样本效率是一个关键问题。最近的算法，如REDQ和DroQ，通过将批次标准化的更新数据（UTD）比率增加到每个环境样本上的20个梯度更新步骤，改善了样本效率。然而，这样做会带来大幅增加的计算成本。为了减少这种计算负担，我们引入了CrossQ：一种轻量级算法，它巧妙地运用批归一化，并去除了目标网络，以在保持低UTD比率为1的同时超越目前的最新样本效率。值得注意的是，CrossQ不依赖于当前方法中使用的高级偏差缩减方案。CrossQ的贡献有三个方面：（1）最先进的样本效率，（2）与REDQ和DroQ相比大幅减少计算成本，（3）实施简单，仅需要在SAC之上添加几行代码。",
    "tldr": "CrossQ是一种轻量级算法，通过巧妙运用批归一化和删除目标网络的方式，提高了深度强化学习的样本效率，减少了计算成本，并且实施简单。",
    "en_tdlr": "CrossQ is a lightweight algorithm that improves sample efficiency and reduces computational cost in deep reinforcement learning by cleverly utilizing batch normalization and eliminating target networks. It achieves state-of-the-art sample efficiency while maintaining a low update-to-data ratio of 1 and requires minimal implementation effort."
}