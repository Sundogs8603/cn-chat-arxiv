{
    "title": "Considerations When Learning Additive Explanations for Black-Box Models. (arXiv:1801.08640v4 [stat.ML] UPDATED)",
    "abstract": "Many methods to explain black-box models, whether local or global, are additive. In this paper, we study global additive explanations for non-additive models, focusing on four explanation methods: partial dependence, Shapley explanations adapted to a global setting, distilled additive explanations, and gradient-based explanations. We show that different explanation methods characterize non-additive components in a black-box model's prediction function in different ways. We use the concepts of main and total effects to anchor additive explanations, and quantitatively evaluate additive and non-additive explanations. Even though distilled explanations are generally the most accurate additive explanations, non-additive explanations such as tree explanations that explicitly model non-additive components tend to be even more accurate. Despite this, our user study showed that machine learning practitioners were better able to leverage additive explanations for various tasks. These considerati",
    "link": "http://arxiv.org/abs/1801.08640",
    "context": "Title: Considerations When Learning Additive Explanations for Black-Box Models. (arXiv:1801.08640v4 [stat.ML] UPDATED)\nAbstract: Many methods to explain black-box models, whether local or global, are additive. In this paper, we study global additive explanations for non-additive models, focusing on four explanation methods: partial dependence, Shapley explanations adapted to a global setting, distilled additive explanations, and gradient-based explanations. We show that different explanation methods characterize non-additive components in a black-box model's prediction function in different ways. We use the concepts of main and total effects to anchor additive explanations, and quantitatively evaluate additive and non-additive explanations. Even though distilled explanations are generally the most accurate additive explanations, non-additive explanations such as tree explanations that explicitly model non-additive components tend to be even more accurate. Despite this, our user study showed that machine learning practitioners were better able to leverage additive explanations for various tasks. These considerati",
    "path": "papers/18/01/1801.08640.json",
    "total_tokens": 969,
    "translated_title": "在学习黑盒模型的增加性解释时需要考虑的问题",
    "translated_abstract": "许多解释黑盒模型的方法，无论是局部还是全局的，都是增加型的。本文研究了非增加型模型的全局增加性解释，重点关注四种解释方法：局部依赖、适应全局环境的Shapley解释、精简的增加性解释和基于梯度的解释。我们展示了不同的解释方法以不同的方式刻画了黑盒模型预测函数中的非增加性成分。我们使用主效应和总效应的概念来锚定增加性解释，并定量评估增加性和非增加性解释。尽管精简的解释一般是最准确的增加性解释，但显式建模非增加性成分的树形解释往往更准确。尽管如此，我们的用户研究表明，机器学习从业者能够更好地利用增加性解释来完成各种任务。",
    "tldr": "本文研究了非增加型模型的全局增加性解释方法，发现不同的解释方法以不同的方式刻画了黑盒模型预测函数中的非增加性成分。尽管精简的解释一般是最准确的增加性解释，但显式建模非增加性成分的树形解释往往更准确。机器学习从业者能够更好地利用增加性解释来完成各种任务。"
}