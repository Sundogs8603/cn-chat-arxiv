{
    "title": "The committee machine: Computational to statistical gaps in learning a two-layers neural network",
    "abstract": "arXiv:1806.05451v3 Announce Type: replace  Abstract: Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it, strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap.",
    "link": "https://arxiv.org/abs/1806.05451",
    "context": "Title: The committee machine: Computational to statistical gaps in learning a two-layers neural network\nAbstract: arXiv:1806.05451v3 Announce Type: replace  Abstract: Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it, strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap.",
    "path": "papers/18/06/1806.05451.json",
    "total_tokens": 759,
    "translated_title": "委员会机器：学习两层神经网络中计算到统计学差距的研究",
    "translated_abstract": "过去，统计物理学中的启发式工具被用来定位相变并计算多层神经网络中教师-学生场景中的最优学习和泛化错误。在这篇论文中，我们为一个名为委员会机器的两层神经网络模型提供了这些方法的严格理论基础。我们还引入了一个委员会机器的近似消息传递（AMP）算法版本，允许在多种参数下以多项式时间执行最佳学习。我们发现在某些情况下，虽然AMP算法无法实现，但在信息理论上可以实现低泛化错误率，这强烈暗示对于这些情况不存在有效算法，揭示了一个巨大的计算差距。",
    "tldr": "介绍了对于两层神经网络模型委员会机器的严格理论基础和近似消息传递算法，揭示了计算到统计学差距。",
    "en_tdlr": "Rigorous justification and introduction of the approximate message passing algorithm for the two-layers neural network model committee machine reveals a computational to statistical gap."
}