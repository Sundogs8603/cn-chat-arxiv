{
    "title": "Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data. (arXiv:1811.11479v2 [cs.LG] UPDATED)",
    "abstract": "On-device machine learning (ML) enables the training process to exploit a massive amount of user-generated private data samples. To enjoy this benefit, inter-device communication overhead should be minimized. With this end, we propose federated distillation (FD), a distributed model training algorithm whose communication payload size is much smaller than a benchmark scheme, federated learning (FL), particularly when the model size is large. Moreover, user-generated data samples are likely to become non-IID across devices, which commonly degrades the performance compared to the case with an IID dataset. To cope with this, we propose federated augmentation (FAug), where each device collectively trains a generative model, and thereby augments its local data towards yielding an IID dataset. Empirical studies demonstrate that FD with FAug yields around 26x less communication overhead while achieving 95-98% test accuracy compared to FL.",
    "link": "http://arxiv.org/abs/1811.11479",
    "context": "Title: Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data. (arXiv:1811.11479v2 [cs.LG] UPDATED)\nAbstract: On-device machine learning (ML) enables the training process to exploit a massive amount of user-generated private data samples. To enjoy this benefit, inter-device communication overhead should be minimized. With this end, we propose federated distillation (FD), a distributed model training algorithm whose communication payload size is much smaller than a benchmark scheme, federated learning (FL), particularly when the model size is large. Moreover, user-generated data samples are likely to become non-IID across devices, which commonly degrades the performance compared to the case with an IID dataset. To cope with this, we propose federated augmentation (FAug), where each device collectively trains a generative model, and thereby augments its local data towards yielding an IID dataset. Empirical studies demonstrate that FD with FAug yields around 26x less communication overhead while achieving 95-98% test accuracy compared to FL.",
    "path": "papers/18/11/1811.11479.json",
    "total_tokens": 922,
    "translated_title": "在设备上高效通信的机器学习：非IID私有数据下的联邦蒸馏与增强",
    "translated_abstract": "设备上的机器学习（ML）使训练过程能够利用大量用户生成的私有数据样本。为了享受这一优势，应该尽量减少设备间的通信开销。为此，我们提出了联邦蒸馏（FD），这是一种分布式模型训练算法，其通信负载大小远小于基准方案联邦学习（FL），特别是当模型大小很大时。此外，用户生成的数据样本在设备之间往往不是独立同分布（IID），这常常降低性能与IID数据集相比的性能。为了应对这个问题，我们提出了联邦增强（FAug），在这种方法中，每个设备都共同训练一个生成模型，从而增强其本地数据以产生IID数据集。实证研究表明，FD与FAug相比FL，通信开销减少了约26倍，同时实现了95-98%的测试准确性。",
    "tldr": "本论文提出了一种在设备上高效通信的机器学习方法，通过联邦蒸馏和增强解决了模型大小和非IID数据的问题。结果表明，与传统的联邦学习相比，该方法在减少通信开销的同时，仍能达到较高的测试准确性。"
}