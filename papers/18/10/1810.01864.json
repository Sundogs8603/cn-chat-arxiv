{
    "title": "Agnostic Sample Compression Schemes for Regression",
    "abstract": "We obtain the first positive results for bounded sample compression in the agnostic regression setting with the $\\ell_p$ loss, where $p\\in [1,\\infty]$. We construct a generic approximate sample compression scheme for real-valued function classes exhibiting exponential size in the fat-shattering dimension but independent of the sample size. Notably, for linear regression, an approximate compression of size linear in the dimension is constructed. Moreover, for $\\ell_1$ and $\\ell_\\infty$ losses, we can even exhibit an efficient exact sample compression scheme of size linear in the dimension. We further show that for every other $\\ell_p$ loss, $p\\in (1,\\infty)$, there does not exist an exact agnostic compression scheme of bounded size. This refines and generalizes a negative result of David, Moran, and Yehudayoff for the $\\ell_2$ loss. We close by posing general open questions: for agnostic regression with $\\ell_1$ loss, does every function class admits an exact compression scheme of size ",
    "link": "https://arxiv.org/abs/1810.01864",
    "context": "Title: Agnostic Sample Compression Schemes for Regression\nAbstract: We obtain the first positive results for bounded sample compression in the agnostic regression setting with the $\\ell_p$ loss, where $p\\in [1,\\infty]$. We construct a generic approximate sample compression scheme for real-valued function classes exhibiting exponential size in the fat-shattering dimension but independent of the sample size. Notably, for linear regression, an approximate compression of size linear in the dimension is constructed. Moreover, for $\\ell_1$ and $\\ell_\\infty$ losses, we can even exhibit an efficient exact sample compression scheme of size linear in the dimension. We further show that for every other $\\ell_p$ loss, $p\\in (1,\\infty)$, there does not exist an exact agnostic compression scheme of bounded size. This refines and generalizes a negative result of David, Moran, and Yehudayoff for the $\\ell_2$ loss. We close by posing general open questions: for agnostic regression with $\\ell_1$ loss, does every function class admits an exact compression scheme of size ",
    "path": "papers/18/10/1810.01864.json",
    "total_tokens": 1064,
    "translated_title": "无知回归问题中的不可知样本压缩方案",
    "translated_abstract": "我们在绝对值损失函数为 $\\ell_p$ 的不确定回归设置中获得了第一个有限样本压缩的积极结果，其中 $p \\in [1, \\infty]$。我们构建了一种通用的逼近样本压缩方案，适用于展示了指数级大小的fat-shattering维度但与样本数量无关的实值函数类。值得注意的是，在线性回归中，我们构造了一个线性维度大小的逼近压缩。此外，在$\\ell_1$和$\\ell_\\infty$损失函数中，我们甚至可以展示出一个线性维度大小的有效完全样本压缩方案。我们进一步证明了对于其他每一个 $\\ell_p$ 损失函数，其中 $p \\in (1,\\infty)$，不存在有限尺寸的完全不可知压缩方案。这进一步改进和推广了David、Moran和Yehudayoff对于$\\ell_2$损失的负面结果。我们最后提出了一般性的开放问题：对于 $\\ell_1$ 损失的不可知回归问题，是否每个函数类都存在尺寸为...的完全压缩方案？",
    "tldr": "本文在绝对值损失函数为 $\\ell_p$ 的不确定回归设置中构建了一种通用的逼近样本压缩方案，对于线性回归可以实现线性维度大小的压缩，对于 $\\ell_1$ 和 $\\ell_\\infty$ 损失函数可以实现线性维度大小的有效完全样本压缩方案；同时，证明了其他 $\\ell_p$ 损失函数不存在有限尺寸的完全不可知压缩方案的结果，并提出了开放问题。",
    "en_tdlr": "This paper presents a generic approximate sample compression scheme in the agnostic regression setting with the $\\ell_p$ loss, achieving a linear size compression for linear regression and efficient exact sample compression for $\\ell_1$ and $\\ell_\\infty$ losses. It also proves the inexistence of finite-sized exact agnostic compression schemes for other $\\ell_p$ losses, and raises open questions in the context of agnostic regression with $\\ell_1$ loss."
}