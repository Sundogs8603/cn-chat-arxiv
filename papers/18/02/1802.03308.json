{
    "title": "The Power of Linear Recurrent Neural Networks. (arXiv:1802.03308v7 [cs.LG] UPDATED)",
    "abstract": "Recurrent neural networks are a powerful means to cope with time series. We show how autoregressive linear, i.e., linearly activated recurrent neural networks (LRNNs) can approximate any time-dependent function f(t) given by a number of function values. The approximation can effectively be learned by simply solving a linear equation system; no backpropagation or similar methods are needed. Furthermore, and this is probably the main contribution of this article, the size of an LRNN can be reduced significantly in one step after inspecting the spectrum of the network transition matrix, i.e., its eigenvalues, by taking only the most relevant components. Therefore, in contrast to other approaches, we do not only learn network weights but also the network architecture. LRNNs have interesting properties: They end up in ellipse trajectories in the long run and allow the prediction of further values and compact representations of functions. We demonstrate this by several experiments, among the",
    "link": "http://arxiv.org/abs/1802.03308",
    "context": "Title: The Power of Linear Recurrent Neural Networks. (arXiv:1802.03308v7 [cs.LG] UPDATED)\nAbstract: Recurrent neural networks are a powerful means to cope with time series. We show how autoregressive linear, i.e., linearly activated recurrent neural networks (LRNNs) can approximate any time-dependent function f(t) given by a number of function values. The approximation can effectively be learned by simply solving a linear equation system; no backpropagation or similar methods are needed. Furthermore, and this is probably the main contribution of this article, the size of an LRNN can be reduced significantly in one step after inspecting the spectrum of the network transition matrix, i.e., its eigenvalues, by taking only the most relevant components. Therefore, in contrast to other approaches, we do not only learn network weights but also the network architecture. LRNNs have interesting properties: They end up in ellipse trajectories in the long run and allow the prediction of further values and compact representations of functions. We demonstrate this by several experiments, among the",
    "path": "papers/18/02/1802.03308.json",
    "total_tokens": 927,
    "translated_title": "线性递归神经网络的力量",
    "translated_abstract": "循环神经网络是处理时间序列的有力工具。我们展示了autoregressive linear,即线性激活循环神经网络(LRNNs)可以逼近由多个函数值给出的任何时变函数f(t)。逼近可以通过简单地解决一个线性方程组来有效学习；不需要反向传播或类似的方法。此外，这可能是本文的主要贡献，通过检查网络转移矩阵的频谱，即它的特征值，只取最相关的组件，可以在一步中显著降低LRNN的规模。因此，与其他方法不同，我们不仅可以学习网络权重，还可以学习网络架构。LRNNs具有有趣的特性：它们最终会以椭圆轨迹结束，并允许预测进一步的值和函数的紧凑表示。我们通过几个实验演示了这一点。",
    "tldr": "本研究展示了线性递归神经网络(LRNNs)可以逼近任何时变函数f(t)。通过检查网络转移矩阵的主要特征值，可以显著降低LRNN的规模。LRNNs具有以椭圆轨迹结束的有趣特性，并允许预测进一步的值和函数的紧凑表示。",
    "en_tdlr": "This paper demonstrates that linearly activated recurrent neural networks (LRNNs) can approximate any time-dependent function, and that the size of an LRNN can be significantly reduced by identifying the most relevant components of its transition matrix via eigenvalue inspection. LRNNs exhibit interesting properties such as ending in ellipse trajectories and allowing compact representations of functions and prediction of further values."
}