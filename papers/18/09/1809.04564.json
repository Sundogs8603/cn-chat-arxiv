{
    "title": "On the Generalization of Stochastic Gradient Descent with Momentum. (arXiv:1809.04564v3 [cs.LG] UPDATED)",
    "abstract": "While momentum-based accelerated variants of stochastic gradient descent (SGD) are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In this work, we first show that there exists a convex loss function for which the stability gap for multiple epochs of SGD with standard heavy-ball momentum (SGDM) becomes unbounded. Then, for smooth Lipschitz loss functions, we analyze a modified momentum-based update rule, i.e., SGD with early momentum (SGDEM) under a broad range of step-sizes, and show that it can train machine learning models for multiple epochs with a guarantee for generalization. Finally, for the special case of strongly convex loss functions, we find a range of momentum such that multiple epochs of standard SGDM, as a special form of SGDEM, also generalizes. Extending our results on generalization, we also develop an upper bound on the expected true risk, in terms of the number of training step",
    "link": "http://arxiv.org/abs/1809.04564",
    "context": "Title: On the Generalization of Stochastic Gradient Descent with Momentum. (arXiv:1809.04564v3 [cs.LG] UPDATED)\nAbstract: While momentum-based accelerated variants of stochastic gradient descent (SGD) are widely used when training machine learning models, there is little theoretical understanding on the generalization error of such methods. In this work, we first show that there exists a convex loss function for which the stability gap for multiple epochs of SGD with standard heavy-ball momentum (SGDM) becomes unbounded. Then, for smooth Lipschitz loss functions, we analyze a modified momentum-based update rule, i.e., SGD with early momentum (SGDEM) under a broad range of step-sizes, and show that it can train machine learning models for multiple epochs with a guarantee for generalization. Finally, for the special case of strongly convex loss functions, we find a range of momentum such that multiple epochs of standard SGDM, as a special form of SGDEM, also generalizes. Extending our results on generalization, we also develop an upper bound on the expected true risk, in terms of the number of training step",
    "path": "papers/18/09/1809.04564.json",
    "total_tokens": 1049,
    "translated_title": "关于带动量的随机梯度下降方法的泛化性能研究",
    "translated_abstract": "尽管在训练机器学习模型时，基于动量的加速随机梯度下降（SGD）变种广泛应用，但对于这些方法的泛化误差几乎没有理论上的理解。在这项工作中，我们首先展示了存在一种凸损失函数，对于多个周期的标准重球动量（SGDM）SGD，其稳定间隙变得无限大。然后，对于平滑Lipschitz损失函数，我们分析了一种修改后的基于动量的更新规则，即SGD提前动量（SGDEM），在广泛的步长范围内，它可以在多个周期内训练机器学习模型并保证泛化性能。最后，对于强凸损失函数的特殊情况，我们发现了一种动量范围，使得多个周期的标准SGDM，作为SGDEM的特殊形式，也具有泛化性能。在泛化性能的基础上，我们还对期望真实风险进行了一个上界，与训练步骤数量有关。",
    "tldr": "该论文研究了带动量的随机梯度下降方法的泛化性能，并通过分析不同的损失函数形式和动量范围，提出了一种可以在多个周期内训练机器学习模型并保证泛化性能的修改后的动量更新规则。对于特殊情况下的损失函数，标准的带动量随机梯度下降方法也能够具有泛化性能。该论文还给出了对于期望真实风险的上界估计。",
    "en_tdlr": "This paper investigates the generalization performance of stochastic gradient descent (SGD) with momentum, and proposes a modified momentum-based update rule that can train machine learning models for multiple epochs while guaranteeing generalization performance. It also shows that standard SGD with momentum can have generalization performance for specific loss functions, and provides an upper bound on the expected true risk."
}