{
    "title": "Active Inverse Reward Design. (arXiv:1809.03060v3 [cs.LG] CROSS LISTED)",
    "abstract": "Designers of AI agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.",
    "link": "http://arxiv.org/abs/1809.03060",
    "context": "Title: Active Inverse Reward Design. (arXiv:1809.03060v3 [cs.LG] CROSS LISTED)\nAbstract: Designers of AI agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.",
    "path": "papers/18/09/1809.03060.json",
    "total_tokens": 903,
    "translated_title": "主动逆向奖励设计",
    "translated_abstract": "AI代理的设计者经常通过试错的过程来迭代奖励函数，直到获得期望的行为，但这只能在训练环境中保证良好的行为。我们提出将这个过程结构化为一系列查询，询问用户在不同的奖励函数之间进行比较。因此我们可以主动选择具有最大信息量的查询来了解真实奖励。与要求设计者提供最佳行为的方法相比，这种方法可以通过引起子优行为之间的偏好来收集额外的信息。在每次查询之后，我们需要通过观察设计者选择的代理奖励函数来更新对真实奖励函数的后验。最近提出的逆向奖励设计 (IRD) 可以实现这一点。我们的方法在测试环境中明显优于IRD。特别是，它可以询问设计者有关可解释、线性奖励函数，并且仍然可以推断出非线性奖励函数。",
    "tldr": "这篇论文提出了一种主动逆向奖励设计的方法，通过询问用户比较不同的奖励函数来选择具有最大信息量的查询，从而达到了在训练环境中保证良好行为的目的。与其他方法相比，该方法可以通过引起子优行为的偏好来收集额外信息，同时在测试环境中取得了明显的优势。"
}