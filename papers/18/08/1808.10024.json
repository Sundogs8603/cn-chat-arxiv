{
    "title": "Hard Non-Monotonic Attention for Character-Level Transduction",
    "abstract": "arXiv:1808.10024v3 Announce Type: replace  Abstract: Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks such as image captioning (Xu et al., 2015), and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of t",
    "link": "https://arxiv.org/abs/1808.10024",
    "context": "Title: Hard Non-Monotonic Attention for Character-Level Transduction\nAbstract: arXiv:1808.10024v3 Announce Type: replace  Abstract: Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks such as image captioning (Xu et al., 2015), and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of t",
    "path": "papers/18/08/1808.10024.json",
    "total_tokens": 799,
    "translated_title": "硬性非单调注意力用于字符级转录",
    "translated_abstract": "字符级字符串到字符串的转录是各种自然语言处理任务中的重要组成部分。最近的方法使用了带有注意力机制的序列到序列模型，来学习模型在生成输出字符串时应该关注输入字符串的哪些部分。传统的软性注意力和硬性单调注意力已经被使用，但硬性非单调注意力仅用于其他序列建模任务（如图像字幕生成），需要使用随机逼近来计算梯度。在这项工作中，我们引入了一个精确的、多项式时间算法来对两个字符串之间的指数数量的非单调对齐进行边缘化，表明硬性注意力模型可以被视为神经重新参数化。",
    "tldr": "本文提出了一种用于字符级转录的硬性非单调注意力机制，引入了精确的、多项式时间算法来处理两个字符串之间的非单调对齐，表明硬性注意力模型是神经重新参数化的一种形式。",
    "en_tdlr": "This paper introduces a hard non-monotonic attention mechanism for character-level transduction and presents an exact, polynomial-time algorithm for handling the exponential number of non-monotonic alignments between two strings, demonstrating that hard attention models can be viewed as neural reparameterizations."
}