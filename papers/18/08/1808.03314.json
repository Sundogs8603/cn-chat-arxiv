{
    "title": "Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network. (arXiv:1808.03314v10 [cs.LG] UPDATED)",
    "abstract": "Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of \"unrolling\" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the \"Vanilla LSTM\" network through a series of logical arguments. We provide all equations pertaining to the ",
    "link": "http://arxiv.org/abs/1808.03314",
    "context": "Title: Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network. (arXiv:1808.03314v10 [cs.LG] UPDATED)\nAbstract: Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of \"unrolling\" an RNN is routinely presented without justification throughout the literature. The goal of this paper is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in signal processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the \"Vanilla LSTM\" network through a series of logical arguments. We provide all equations pertaining to the ",
    "path": "papers/18/08/1808.03314.json",
    "total_tokens": 893,
    "translated_title": "递归神经网络（RNN）和长短期记忆（LSTM）网络的基础",
    "translated_abstract": "长短期记忆（LSTM）网络在广泛的实际应用中表现出了高效的效果，因此在科学期刊、技术博客和实现指南中得到了广泛的关注。然而，在大多数文章中，LSTM网络及其父类RNN的推推理公式被以公理的方式陈述，而训练公式则完全被省略。此外，关于“展开”RNN的技术在文献中通常被描述，但缺乏解释。本文旨在在一篇文章中解释RNN和LSTM的基本原理。我们从信号处理的概念中形式化地推导出了RNN的基本公式。然后，我们提出并证明了一个精确的陈述，得到了RNN的展开技术。我们还审查了训练标准RNN的困难，并通过一系列逻辑论证将RNN转化为“Vanilla LSTM”网络。我们提供了与训练过程相关的所有方程。",
    "tldr": "本文通过形式化推导来解释了递归神经网络（RNN）和长短期记忆（LSTM）网络的基本原理，并提出了一种将RNN转化为“Vanilla LSTM”网络的方法。",
    "en_tdlr": "This paper explains the fundamental principles of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network through formal derivation, and proposes a method to transform RNN into \"Vanilla LSTM\" network."
}