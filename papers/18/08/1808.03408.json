{
    "title": "A Unified Analysis of AdaGrad with Weighted Aggregation and Momentum Acceleration. (arXiv:1808.03408v4 [cs.LG] UPDATED)",
    "abstract": "Integrating adaptive learning rate and momentum techniques into SGD leads to a large class of efficiently accelerated adaptive stochastic algorithms, such as AdaGrad, RMSProp, Adam, AccAdaGrad, \\textit{etc}. In spite of their effectiveness in practice, there is still a large gap in their theories of convergences, especially in the difficult non-convex stochastic setting. To fill this gap, we propose \\emph{weighted AdaGrad with unified momentum}, dubbed AdaUSM, which has the main characteristics that (1) it incorporates a unified momentum scheme which covers both the heavy ball momentum and the Nesterov accelerated gradient momentum; (2) it adopts a novel weighted adaptive learning rate that can unify the learning rates of AdaGrad, AccAdaGrad, Adam, and RMSProp. Moreover, when we take polynomially growing weights in AdaUSM, we obtain its $\\mathcal{O}(\\log(T)/\\sqrt{T})$ convergence rate in the non-convex stochastic setting. We also show that the adaptive learning rates of Adam and RMSPro",
    "link": "http://arxiv.org/abs/1808.03408",
    "context": "Title: A Unified Analysis of AdaGrad with Weighted Aggregation and Momentum Acceleration. (arXiv:1808.03408v4 [cs.LG] UPDATED)\nAbstract: Integrating adaptive learning rate and momentum techniques into SGD leads to a large class of efficiently accelerated adaptive stochastic algorithms, such as AdaGrad, RMSProp, Adam, AccAdaGrad, \\textit{etc}. In spite of their effectiveness in practice, there is still a large gap in their theories of convergences, especially in the difficult non-convex stochastic setting. To fill this gap, we propose \\emph{weighted AdaGrad with unified momentum}, dubbed AdaUSM, which has the main characteristics that (1) it incorporates a unified momentum scheme which covers both the heavy ball momentum and the Nesterov accelerated gradient momentum; (2) it adopts a novel weighted adaptive learning rate that can unify the learning rates of AdaGrad, AccAdaGrad, Adam, and RMSProp. Moreover, when we take polynomially growing weights in AdaUSM, we obtain its $\\mathcal{O}(\\log(T)/\\sqrt{T})$ convergence rate in the non-convex stochastic setting. We also show that the adaptive learning rates of Adam and RMSPro",
    "path": "papers/18/08/1808.03408.json",
    "total_tokens": 1056,
    "translated_title": "一种带有权重聚集和动量加速的AdaGrad的统一分析",
    "translated_abstract": "将自适应学习率和动量技术集成到SGD中会导致一类高效加速自适应随机算法，如AdaGrad，RMSProp，Adam，AccAdaGrad等。尽管它们在实践中有效，但它们的收敛理论仍存在很大差距，特别是在非凸随机设置中。为了填补这一差距，我们提出了“带有统一动量的加权AdaGrad”，称为AdaUSM，它具有以下主要特征：(1) 它融合了统一的动量方案，涵盖了重球动量和Nesterov加速梯度动量；(2) 它采用了一种新的加权自适应学习率，可以统一AdaGrad，AccAdaGrad，Adam和RMSProp的学习率。此外，当我们在AdaUSM中采用多项式增长的权重时，在非凸随机设置中可以得到其收敛率为$\\mathcal{O}(\\log(T)/\\sqrt{T})$ 。我们还表明，Adam和RMSProp的自适应学习率在重加权的情况下是一致的。",
    "tldr": "本论文提出了一种名为AdaUSM的AdaGrad变体，它采用了一种新的加权自适应学习率，可以统一AdaGrad、AccAdaGrad、Adam和RMSProp的学习率，同时通过使用统一动量方案，覆盖了重球动量和Nesterov加速梯度动量；在非凸随机设置中的收敛率为$\\mathcal{O}(\\log(T)/\\sqrt{T})$。",
    "en_tdlr": "This paper proposes a variant of AdaGrad called AdaUSM, which adopts a novel weighted adaptive learning rate that can unify the learning rates of AdaGrad, AccAdaGrad, Adam, and RMSProp, and unifies the heavy ball momentum and the Nesterov accelerated gradient momentum through using unified momentum scheme, it achieves a convergence rate of $\\mathcal{O}(\\log(T)/\\sqrt{T})$ in non-convex stochastic setting with polynomially growing weights."
}