{
    "title": "Efficient Attention: Attention with Linear Complexities. (arXiv:1812.01243v10 [cs.CV] UPDATED)",
    "abstract": "Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo d",
    "link": "http://arxiv.org/abs/1812.01243",
    "context": "Title: Efficient Attention: Attention with Linear Complexities. (arXiv:1812.01243v10 [cs.CV] UPDATED)\nAbstract: Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo d",
    "path": "papers/18/12/1812.01243.json",
    "total_tokens": 905,
    "translated_title": "高效注意力：具有线性复杂度的注意力机制",
    "translated_abstract": "点积注意力在计算机视觉和自然语言处理中有广泛应用。然而，它的内存和计算成本随着输入大小的增加呈二次增长。这种增长限制了其在高分辨率输入上的应用。为解决这个缺点，本文提出了一种新颖的高效注意力机制，它与点积注意力等效，但内存和计算成本大大降低。其资源效率使得注意力模块能更广泛、灵活地集成到网络中，从而提高准确性。经验证明了其优势的有效性。高效注意力模块显著提升了对MS-COCO 2017上的物体检测器和实例分割器的性能。此外，资源效率使得复杂模型能够使用注意力，而高成本限制了使用点积注意力。以立体视觉为例，一个具有高效注意力的模型实现了最先进的准确性。",
    "tldr": "本文提出了一种高效注意力机制，可以在大幅减少内存和计算成本的情况下实现与传统点积注意力等效的效果。这种高效机制的应用使得注意力模块可以更广泛地集成到网络中，从而提高准确性，并且在物体检测和实例分割等任务中取得了显著的性能提升。",
    "en_tdlr": "This paper proposes an efficient attention mechanism that achieves equivalent results to traditional dot-product attention but with significantly reduced memory and computational costs. Its application allows for more widespread integration of attention modules in networks, leading to improved accuracy and significant performance boosts in tasks such as object detection and instance segmentation."
}