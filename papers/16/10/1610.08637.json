{
    "title": "Statistical Inference for Model Parameters in Stochastic Gradient Descent. (arXiv:1610.08637v4 [stat.ML] UPDATED)",
    "abstract": "The stochastic gradient descent (SGD) algorithm has been widely used in statistical estimation for large-scale data due to its computational and memory efficiency. While most existing works focus on the convergence of the objective function or the error of the obtained solution, we investigate the problem of statistical inference of true model parameters based on SGD when the population loss function is strongly convex and satisfies certain smoothness conditions. Our main contributions are two-fold. First, in the fixed dimension setup, we propose two consistent estimators of the asymptotic covariance of the average iterate from SGD: (1) a plug-in estimator, and (2) a batch-means estimator, which is computationally more efficient and only uses the iterates from SGD. Both proposed estimators allow us to construct asymptotically exact confidence intervals and hypothesis tests. Second, for high-dimensional linear regression, using a variant of the SGD algorithm, we construct a debiased est",
    "link": "http://arxiv.org/abs/1610.08637",
    "context": "Title: Statistical Inference for Model Parameters in Stochastic Gradient Descent. (arXiv:1610.08637v4 [stat.ML] UPDATED)\nAbstract: The stochastic gradient descent (SGD) algorithm has been widely used in statistical estimation for large-scale data due to its computational and memory efficiency. While most existing works focus on the convergence of the objective function or the error of the obtained solution, we investigate the problem of statistical inference of true model parameters based on SGD when the population loss function is strongly convex and satisfies certain smoothness conditions. Our main contributions are two-fold. First, in the fixed dimension setup, we propose two consistent estimators of the asymptotic covariance of the average iterate from SGD: (1) a plug-in estimator, and (2) a batch-means estimator, which is computationally more efficient and only uses the iterates from SGD. Both proposed estimators allow us to construct asymptotically exact confidence intervals and hypothesis tests. Second, for high-dimensional linear regression, using a variant of the SGD algorithm, we construct a debiased est",
    "path": "papers/16/10/1610.08637.json",
    "total_tokens": 936,
    "translated_title": "随机梯度下降中模型参数的统计推断",
    "translated_abstract": "随机梯度下降（SGD）算法由于其计算和内存效率而广泛应用于大规模数据的统计估计中。尽管大多数现有工作注重目标函数的收敛性或所得解的误差，本研究探讨了在人口损失函数强凸且满足一定光滑性条件时，基于SGD进行真实模型参数的统计推断的问题。我们的主要贡献有两个方面。首先，在固定维数设置下，我们提出了两个一致估计器以获得SGD的平均迭代的渐近协方差：（1）插值估计器和（2）批均值估计器，后者在计算上更高效，只使用SGD的迭代结果。这两个提出的估计器都允许我们构建渐近精确的置信区间和假设检验。其次，在高维线性回归中，我们利用SGD算法的变种构建了一个去偏置的估计器。",
    "tldr": "本论文研究了在随机梯度下降中对真实模型参数进行统计推断的问题，并提出了两个一致估计器来获得SGD的平均迭代的渐近协方差，同时也构建了高维线性回归的去偏置估计器。",
    "en_tdlr": "This paper investigates the problem of statistical inference for true model parameters in stochastic gradient descent (SGD) and proposes two consistent estimators for the asymptotic covariance of the average iterate from SGD in fixed dimension setup. Moreover, a debiased estimator is constructed for high-dimensional linear regression using a variant of SGD algorithm."
}