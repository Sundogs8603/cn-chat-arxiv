{
    "title": "Cooperative Inverse Reinforcement Learning",
    "abstract": "arXiv:1606.03137v4 Announce Type: replace  Abstract: For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is ",
    "link": "https://arxiv.org/abs/1606.03137",
    "context": "Title: Cooperative Inverse Reinforcement Learning\nAbstract: arXiv:1606.03137v4 Announce Type: replace  Abstract: For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial-information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is ",
    "path": "papers/16/06/1606.03137.json",
    "total_tokens": 932,
    "translated_title": "合作逆强化学习",
    "translated_abstract": "为了使自主系统对人类有所帮助且不带来不必要的风险，它需要与环境中的人类价值观保持一致，使其行动有助于最大化人类的价值。本文将价值对齐问题形式化为合作逆强化学习(Cooperative Inverse Reinforcement Learning, CIRL)。CIRL问题是一个合作的、部分信息的博弈，有两个参与者：人类和机器人；两者根据人类的奖励函数获得奖励，但机器人最初并不知道这个函数是什么。与经典的逆强化学习相比，在那里假设人类是在孤立状态下行事最优的，最优的CIRL解决方案产生诸如积极教学、积极学习和沟通行为等行为，这些行为更有效地实现了价值对齐。我们展示了在CIRL游戏中计算最优联合策略可以简化为解决部分观测马尔可夫决策过程(POMDP)，证明了在孤立状态下的最优性。",
    "tldr": "提出了合作逆强化学习(CIRL)的定义，将价值对齐问题转化为机器人与人类之间的合作部分信息博弈，通过积极教学、积极学习和沟通行为等方式实现最大化价值对齐。",
    "en_tdlr": "Introduced the definition of Cooperative Inverse Reinforcement Learning (CIRL), transforming the value alignment problem into a cooperative partial-information game between robots and humans, achieving maximization of value alignment through active teaching, active learning, and communicative behaviors."
}