{
    "title": "Weighted bandits or: How bandits learn distorted values that are not expected. (arXiv:1611.10283v2 [cs.LG] UPDATED)",
    "abstract": "Motivated by models of human decision making proposed to explain commonly observed deviations from conventional expected value preferences, we formulate two stochastic multi-armed bandit problems with distorted probabilities on the reward distributions: the classic $K$-armed bandit and the linearly parameterized bandit settings. We consider the aforementioned problems in the regret minimization as well as best arm identification framework for multi-armed bandits. For the regret minimization setting in $K$-armed as well as linear bandit problems, we propose algorithms that are inspired by Upper Confidence Bound (UCB) algorithms, incorporate reward distortions, and exhibit sublinear regret. For the $K$-armed bandit setting, we derive an upper bound on the expected regret for our proposed algorithm, and then we prove a matching lower bound to establish the order-optimality of our algorithm. For the linearly parameterized setting, our algorithm achieves a regret upper bound that is of the ",
    "link": "http://arxiv.org/abs/1611.10283",
    "context": "Title: Weighted bandits or: How bandits learn distorted values that are not expected. (arXiv:1611.10283v2 [cs.LG] UPDATED)\nAbstract: Motivated by models of human decision making proposed to explain commonly observed deviations from conventional expected value preferences, we formulate two stochastic multi-armed bandit problems with distorted probabilities on the reward distributions: the classic $K$-armed bandit and the linearly parameterized bandit settings. We consider the aforementioned problems in the regret minimization as well as best arm identification framework for multi-armed bandits. For the regret minimization setting in $K$-armed as well as linear bandit problems, we propose algorithms that are inspired by Upper Confidence Bound (UCB) algorithms, incorporate reward distortions, and exhibit sublinear regret. For the $K$-armed bandit setting, we derive an upper bound on the expected regret for our proposed algorithm, and then we prove a matching lower bound to establish the order-optimality of our algorithm. For the linearly parameterized setting, our algorithm achieves a regret upper bound that is of the ",
    "path": "papers/16/11/1611.10283.json",
    "total_tokens": 897,
    "translated_title": "加权赌博机或者：赌博机如何学习预期之外的扭曲价值",
    "translated_abstract": "受到用于解释常见偏离传统预期价值偏好的人类决策模型的启发，我们提出了两个带有扭曲概率的随机多臂赌博机问题：经典的K臂赌博机和线性参数化赌博机设置。我们在对多臂赌博机的后悔最小化和最佳臂识别框架下研究了上述问题。对于K臂赌博机以及线性赌博机问题的后悔最小化设置，我们提出了受到上置信界(UCB)算法启发、包含奖励扭曲并且具有次线性后悔的算法。对于K臂赌博机设置，我们得出了对我们提出的算法的预期后悔的上界，然后我们证明了一个匹配的下界，以验证我们算法的次线性优化顺序。对于线性参数化设置，我们的算法实现了一个后悔上界，该上界是次线性的。",
    "tldr": "本论文研究了带有扭曲概率的随机多臂赌博机问题，并提出了以UCB算法为基础、考虑了奖励扭曲并具有次线性后悔的算法。"
}