{
    "title": "On the Dual Formulation of Boosting Algorithms. (arXiv:0901.3590v7 [cs.LG] UPDATED)",
    "abstract": "We study boosting algorithms from a new perspective. We show that the Lagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with generalized hinge loss are all entropy maximization problems. By looking at the dual problems of these boosting algorithms, we show that the success of boosting algorithms can be understood in terms of maintaining a better margin distribution by maximizing margins and at the same time controlling the margin variance.We also theoretically prove that, approximately, AdaBoost maximizes the average margin, instead of the minimum margin. The duality formulation also enables us to develop column generation based optimization algorithms, which are totally corrective. We show that they exhibit almost identical classification results to that of standard stage-wise additive boosting algorithms but with much faster convergence rates. Therefore fewer weak classifiers are needed to build the ensemble using our proposed optimization technique.",
    "link": "http://arxiv.org/abs/0901.3590",
    "context": "Title: On the Dual Formulation of Boosting Algorithms. (arXiv:0901.3590v7 [cs.LG] UPDATED)\nAbstract: We study boosting algorithms from a new perspective. We show that the Lagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with generalized hinge loss are all entropy maximization problems. By looking at the dual problems of these boosting algorithms, we show that the success of boosting algorithms can be understood in terms of maintaining a better margin distribution by maximizing margins and at the same time controlling the margin variance.We also theoretically prove that, approximately, AdaBoost maximizes the average margin, instead of the minimum margin. The duality formulation also enables us to develop column generation based optimization algorithms, which are totally corrective. We show that they exhibit almost identical classification results to that of standard stage-wise additive boosting algorithms but with much faster convergence rates. Therefore fewer weak classifiers are needed to build the ensemble using our proposed optimization technique.",
    "path": "papers/09/01/0901.3590.json",
    "total_tokens": 877,
    "translated_title": "关于Boosting算法的对偶形式",
    "translated_abstract": "本文从新的角度研究了Boosting算法。我们展示了AdaBoost、LogitBoost和带广义铰链损失的软间隔LPBoost算法的拉格朗日对偶问题都是熵最大化问题。通过研究这些Boosting算法的对偶问题，我们展示了Boosting算法的成功可以通过最大化间隔和控制间隔方差来维护更好的间隔分布来理解。我们还理论上证明了，近似地，AdaBoost最大化的是平均间隔而非最小间隔。对偶形式还使我们能够开发基于列生成的优化算法，这些算法是完全校正的。我们展示了它们与标准的逐步加性Boosting算法几乎完全相同的分类结果，但收敛速度更快。因此，使用我们提出的优化技术构建集合只需要较少的弱分类器。",
    "tldr": "本文从对偶问题的角度研究了Boosting算法，证明了AdaBoost等算法通过最大化间隔和控制间隔方差来维护更好的间隔分布，提出了基于列生成的优化算法，收敛速度更快。",
    "en_tdlr": "This paper studies boosting algorithms from the perspective of dual problems and shows that AdaBoost, LogitBoost and soft-margin LPBoost with generalized hinge loss all solve entropy maximization problems. The success of these algorithms is attributed to maintaining a better margin distribution by maximizing margins and controlling margin variance. The paper also introduces column generation based optimization algorithms, which show faster convergence rates and require fewer weak classifiers to construct the ensemble."
}