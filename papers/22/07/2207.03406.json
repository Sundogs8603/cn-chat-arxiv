{
    "title": "Neural Stein critics with staged $L^2$-regularization. (arXiv:2207.03406v3 [stat.ML] UPDATED)",
    "abstract": "Learning to differentiate model distributions from observed data is a fundamental problem in statistics and machine learning, and high-dimensional data remains a challenging setting for such problems. Metrics that quantify the disparity in probability distributions, such as the Stein discrepancy, play an important role in high-dimensional statistical testing. In this paper, we investigate the role of $L^2$ regularization in training a neural network Stein critic so as to distinguish between data sampled from an unknown probability distribution and a nominal model distribution. Making a connection to the Neural Tangent Kernel (NTK) theory, we develop a novel staging procedure for the weight of regularization over training time, which leverages the advantages of highly-regularized training at early times. Theoretically, we prove the approximation of the training dynamic by the kernel optimization, namely the ``lazy training'', when the $L^2$ regularization weight is large, and training o",
    "link": "http://arxiv.org/abs/2207.03406",
    "context": "Title: Neural Stein critics with staged $L^2$-regularization. (arXiv:2207.03406v3 [stat.ML] UPDATED)\nAbstract: Learning to differentiate model distributions from observed data is a fundamental problem in statistics and machine learning, and high-dimensional data remains a challenging setting for such problems. Metrics that quantify the disparity in probability distributions, such as the Stein discrepancy, play an important role in high-dimensional statistical testing. In this paper, we investigate the role of $L^2$ regularization in training a neural network Stein critic so as to distinguish between data sampled from an unknown probability distribution and a nominal model distribution. Making a connection to the Neural Tangent Kernel (NTK) theory, we develop a novel staging procedure for the weight of regularization over training time, which leverages the advantages of highly-regularized training at early times. Theoretically, we prove the approximation of the training dynamic by the kernel optimization, namely the ``lazy training'', when the $L^2$ regularization weight is large, and training o",
    "path": "papers/22/07/2207.03406.json",
    "total_tokens": 894,
    "translated_title": "采用阶段$L^2$正则化的神经Stein评价器",
    "translated_abstract": "学习区分模型分布和观测数据的分布是统计学和机器学习中的一个基本问题，高维数据对这些问题仍然是一个具有挑战性的场景。量化概率分布差异的度量，例如Stein差异，在高维统计测试中扮演着重要角色。本文研究了在训练神经网络Stein评价器时采用$L^2$正则化的作用，以区分从未知概率分布中采样的数据和名义模型分布。通过与神经切向核(NTK)理论的联系，我们开发了一种新的权重正则化的阶段过程，利用了在早期时期高度正则化训练的优势。从理论上证明，当$L^2$正则化权重较大时，训练动态的近似性质通过核优化即实现了懒惰(“lazy training”)的训练。",
    "tldr": "本论文研究了使用$L^2$正则化训练神经网络Stein评价器的方法，通过阶段性的权重调整训练过程，可以在高维统计测试中实现对未知概率分布与名义模型分布的区分。",
    "en_tdlr": "This paper investigates the use of $L^2$ regularization in training neural network Stein critics to distinguish between data sampled from unknown probability distributions and nominal model distributions. By developing a novel staging procedure for regularization weight and proving the approximation of training dynamics through kernel optimization, the paper presents an effective method for high-dimensional statistical testing."
}