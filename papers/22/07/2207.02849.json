{
    "title": "Betty: An Automatic Differentiation Library for Multilevel Optimization. (arXiv:2207.02849v2 [cs.LG] UPDATED)",
    "abstract": "Gradient-based multilevel optimization (MLO) has gained attention as a framework for studying numerous problems, ranging from hyperparameter optimization and meta-learning to neural architecture search and reinforcement learning. However, gradients in MLO, which are obtained by composing best-response Jacobians via the chain rule, are notoriously difficult to implement and memory/compute intensive. We take an initial step towards closing this gap by introducing Betty, a software library for large-scale MLO. At its core, we devise a novel dataflow graph for MLO, which allows us to (1) develop efficient automatic differentiation for MLO that reduces the computational complexity from O(d^3) to O(d^2), (2) incorporate systems support such as mixed-precision and data-parallel training for scalability, and (3) facilitate implementation of MLO programs of arbitrary complexity while allowing a modular interface for diverse algorithmic and systems design choices. We empirically demonstrate that",
    "link": "http://arxiv.org/abs/2207.02849",
    "context": "Title: Betty: An Automatic Differentiation Library for Multilevel Optimization. (arXiv:2207.02849v2 [cs.LG] UPDATED)\nAbstract: Gradient-based multilevel optimization (MLO) has gained attention as a framework for studying numerous problems, ranging from hyperparameter optimization and meta-learning to neural architecture search and reinforcement learning. However, gradients in MLO, which are obtained by composing best-response Jacobians via the chain rule, are notoriously difficult to implement and memory/compute intensive. We take an initial step towards closing this gap by introducing Betty, a software library for large-scale MLO. At its core, we devise a novel dataflow graph for MLO, which allows us to (1) develop efficient automatic differentiation for MLO that reduces the computational complexity from O(d^3) to O(d^2), (2) incorporate systems support such as mixed-precision and data-parallel training for scalability, and (3) facilitate implementation of MLO programs of arbitrary complexity while allowing a modular interface for diverse algorithmic and systems design choices. We empirically demonstrate that",
    "path": "papers/22/07/2207.02849.json",
    "total_tokens": 946,
    "translated_title": "Betty: 一个用于多层次优化的自动微分库",
    "translated_abstract": "基于梯度的多层次优化(MLO)已成为研究众多问题的框架，包括超参数优化、元学习、神经架构搜索和强化学习等。然而，MLO中的梯度，是通过链式法则组成最佳响应Jacobi矩阵而获得的，具有计算和内存密集等不利因素。本文介绍了一个面向大规模MLO的软件库Betty，从而初步为解决这一问题迈出了一步。在其核心，我们设计了一个新的MLO数据流图，使我们能够(1)为MLO开发高效的自动微分，将计算复杂度从O(d^3)降至O(d^2)，(2)融入系统支持，例如混合精度和数据并行训练，以实现可伸缩性，(3)便于实现任意复杂度的MLO程序，同时允许多样化的算法和系统设计选择的模块化接口。我们通过实验证明，Betty在广泛的MLO任务中都实现了很好的性能，例如超参数优化、元学习和神经架构搜索。",
    "tldr": "本文介绍了一个名为Betty的自动微分库，可用于大规模的梯度优化问题，有效地减少了计算复杂度并提高了可扩展性，在广泛的多层次优化任务中表现出良好性能。",
    "en_tdlr": "This paper presents Betty, an automatic differentiation library for large-scale gradient-based multilevel optimization problems, which reduces computational complexity and increases scalability. Empirical results demonstrate good performance in a wide range of tasks, including hyperparameter optimization, meta-learning, and neural architecture search."
}