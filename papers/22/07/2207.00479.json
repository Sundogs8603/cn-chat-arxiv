{
    "title": "Asynchronous Decentralized Bayesian Optimization for Large Scale Hyperparameter Optimization. (arXiv:2207.00479v3 [cs.LG] UPDATED)",
    "abstract": "Bayesian optimization (BO) is a promising approach for hyperparameter optimization of deep neural networks (DNNs), where each model training can take minutes to hours. In BO, a computationally cheap surrogate model is employed to learn the relationship between parameter configurations and their performance such as accuracy. Parallel BO methods often adopt single manager/multiple workers strategies to evaluate multiple hyperparameter configurations simultaneously. Despite significant hyperparameter evaluation time, the overhead in such centralized schemes prevents these methods to scale on a large number of workers. We present an asynchronous-decentralized BO, wherein each worker runs a sequential BO and asynchronously communicates its results through shared storage. We scale our method without loss of computational efficiency with above 95% of worker's utilization to 1,920 parallel workers (full production queue of the Polaris supercomputer) and demonstrate improvement in model accurac",
    "link": "http://arxiv.org/abs/2207.00479",
    "context": "Title: Asynchronous Decentralized Bayesian Optimization for Large Scale Hyperparameter Optimization. (arXiv:2207.00479v3 [cs.LG] UPDATED)\nAbstract: Bayesian optimization (BO) is a promising approach for hyperparameter optimization of deep neural networks (DNNs), where each model training can take minutes to hours. In BO, a computationally cheap surrogate model is employed to learn the relationship between parameter configurations and their performance such as accuracy. Parallel BO methods often adopt single manager/multiple workers strategies to evaluate multiple hyperparameter configurations simultaneously. Despite significant hyperparameter evaluation time, the overhead in such centralized schemes prevents these methods to scale on a large number of workers. We present an asynchronous-decentralized BO, wherein each worker runs a sequential BO and asynchronously communicates its results through shared storage. We scale our method without loss of computational efficiency with above 95% of worker's utilization to 1,920 parallel workers (full production queue of the Polaris supercomputer) and demonstrate improvement in model accurac",
    "path": "papers/22/07/2207.00479.json",
    "total_tokens": 834,
    "translated_title": "大规模超参数优化的异步分散贝叶斯优化方法",
    "translated_abstract": "贝叶斯优化是深度神经网络超参数优化的一种有希望的方法，其中每个模型训练可能需要几分钟到几小时的时间。在贝叶斯优化中，采用计算便宜的替代模型来学习参数配置与性能（如准确性）之间的关系。并行贝叶斯优化方法通常采用单个管理器-多个工作者策略，同时评估多个超参数配置。尽管超参数评估时间相当长，但这种集中式方案的开销阻碍了这些方法在大量工作者上的扩展。我们提出了一种异步分散的贝叶斯优化方法，其中每个工作者运行顺序贝叶斯优化，并通过共享存储异步通信其结果。我们将我们的方法扩展到1,920个并行工作者（Polaris超级计算机的完整生产队列），并展示模型准确性的改进。",
    "tldr": "这项研究提出了一种异步分散的贝叶斯优化方法，可以实现大规模超参数优化，并在Polairs超级计算机上展示了模型准确性的改进。",
    "en_tdlr": "This study proposes an asynchronous-decentralized Bayesian optimization method for large-scale hyperparameter optimization and demonstrates improvement in model accuracy on the Polaris supercomputer."
}