{
    "title": "Personalized PCA: Decoupling Shared and Unique Features",
    "abstract": "In this paper, we tackle a significant challenge in PCA: heterogeneity. When data are collected from different sources with heterogeneous trends while still sharing some congruency, it is critical to extract shared knowledge while retaining the unique features of each source. To this end, we propose personalized PCA (PerPCA), which uses mutually orthogonal global and local principal components to encode both unique and shared features. We show that, under mild conditions, both unique and shared features can be identified and recovered by a constrained optimization problem, even if the covariance matrices are immensely different. Also, we design a fully federated algorithm inspired by distributed Stiefel gradient descent to solve the problem. The algorithm introduces a new group of operations called generalized retractions to handle orthogonality constraints, and only requires global PCs to be shared across sources. We prove the linear convergence of the algorithm under suitable assumpt",
    "link": "https://arxiv.org/abs/2207.08041",
    "context": "Title: Personalized PCA: Decoupling Shared and Unique Features\nAbstract: In this paper, we tackle a significant challenge in PCA: heterogeneity. When data are collected from different sources with heterogeneous trends while still sharing some congruency, it is critical to extract shared knowledge while retaining the unique features of each source. To this end, we propose personalized PCA (PerPCA), which uses mutually orthogonal global and local principal components to encode both unique and shared features. We show that, under mild conditions, both unique and shared features can be identified and recovered by a constrained optimization problem, even if the covariance matrices are immensely different. Also, we design a fully federated algorithm inspired by distributed Stiefel gradient descent to solve the problem. The algorithm introduces a new group of operations called generalized retractions to handle orthogonality constraints, and only requires global PCs to be shared across sources. We prove the linear convergence of the algorithm under suitable assumpt",
    "path": "papers/22/07/2207.08041.json",
    "total_tokens": 943,
    "translated_title": "个性化PCA:解耦共享和独特特征",
    "translated_abstract": "在这篇论文中，我们解决了PCA面临的一个重要挑战：异质性。当数据来自不同的来源，具有异质的趋势，同时仍然共享某些一致性时，关键是提取共享的知识，同时保留每个来源的独特特征。为此，我们提出了个性化PCA（PerPCA），它使用互相正交的全局和局部主成分来编码独特和共享的特征。我们证明，在温和的条件下，即使协方差矩阵极其不同，也可以通过受约束优化问题识别和恢复出独特和共享的特征。此外，我们设计了一个完全的联邦算法，灵感来自分布式Stiefel梯度下降，用于解决这个问题。该算法引入了一种称为广义回缩的新操作组来处理正交约束，只需要在来源之间共享全局主成分。我们证明了算法在适当的假设下具有线性收敛性。",
    "tldr": "本文介绍了个性化PCA（PerPCA），通过使用全局和局部主成分来编码独特和共享特征，解决了PCA面临的异质性挑战。我们证明，在温和的条件下，我们可以通过受约束优化问题识别和恢复出独特和共享的特征。我们还设计了一个联邦算法来解决这个问题，并证明了算法的线性收敛性。",
    "en_tdlr": "This paper introduces personalized PCA (PerPCA) to address the heterogeneity challenge in PCA by encoding unique and shared features using global and local principal components. It shows that under mild conditions, both unique and shared features can be identified and recovered through a constrained optimization problem. The paper also proposes a federated algorithm and proves its linear convergence."
}