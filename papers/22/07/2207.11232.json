{
    "title": "Neural Groundplans: Persistent Neural Scene Representations from a Single Image. (arXiv:2207.11232v2 [cs.CV] UPDATED)",
    "abstract": "We present a method to map 2D image observations of a scene to a persistent 3D scene representation, enabling novel view synthesis and disentangled representation of the movable and immovable components of the scene. Motivated by the bird's-eye-view (BEV) representation commonly used in vision and robotics, we propose conditional neural groundplans, ground-aligned 2D feature grids, as persistent and memory-efficient scene representations. Our method is trained self-supervised from unlabeled multi-view observations using differentiable rendering, and learns to complete geometry and appearance of occluded regions. In addition, we show that we can leverage multi-view videos at training time to learn to separately reconstruct static and movable components of the scene from a single image at test time. The ability to separately reconstruct movable objects enables a variety of downstream tasks using simple heuristics, such as extraction of object-centric 3D representations, novel view synthe",
    "link": "http://arxiv.org/abs/2207.11232",
    "context": "Title: Neural Groundplans: Persistent Neural Scene Representations from a Single Image. (arXiv:2207.11232v2 [cs.CV] UPDATED)\nAbstract: We present a method to map 2D image observations of a scene to a persistent 3D scene representation, enabling novel view synthesis and disentangled representation of the movable and immovable components of the scene. Motivated by the bird's-eye-view (BEV) representation commonly used in vision and robotics, we propose conditional neural groundplans, ground-aligned 2D feature grids, as persistent and memory-efficient scene representations. Our method is trained self-supervised from unlabeled multi-view observations using differentiable rendering, and learns to complete geometry and appearance of occluded regions. In addition, we show that we can leverage multi-view videos at training time to learn to separately reconstruct static and movable components of the scene from a single image at test time. The ability to separately reconstruct movable objects enables a variety of downstream tasks using simple heuristics, such as extraction of object-centric 3D representations, novel view synthe",
    "path": "papers/22/07/2207.11232.json",
    "total_tokens": 971,
    "translated_title": "神经地面计划：基于单张图片的持久神经场景表示",
    "translated_abstract": "本文提出一种方法，将场景的2D图像观测映射到持久的3D场景表示中，实现了新颖的视角合成和场景可移动和不可移动组件的解耦表示。受视觉和机器人常用的鸟瞰图（BEV）表示的启发，我们提出了条件神经地面计划，即地面对齐的2D特征网格，作为持久且占用内存少的场景表示。我们的方法采用无标签的多视角观测的自我监督训练，使用可微分渲染学习完成遮挡区域的几何和外观。此外，我们展示可以利用多视角视频在训练时来学习分别从单张图像中重构场景的静态和可移动组件。分别重构可移动物体的能力，使用简单的启发式方法，使其可以进行诸多下游任务，如提取以物体为中心的3D表示、新颖的视角合成和物体操作等。",
    "tldr": "本文提出了一种基于单张图像的持久神经场景表示方法，使用条件神经地面计划来表示场景，能够进行视角合成、场景解耦表示和可移动组件的分离，重构可移动物体能够进行多种下游任务。",
    "en_tdlr": "The paper presents a method for mapping 2D scene observations to a persistent 3D scene representation using conditional neural groundplans. The model enables novel view synthesis and disentangled representation of movable and immovable components of the scene. It is trained self-supervised using differentiable rendering and can separately reconstruct static and movable components of the scene from a single image at test time. The ability to separately reconstruct movable objects enables a variety of downstream tasks like 3D representations and novel view synthesis."
}