{
    "title": "Do Quantum Circuit Born Machines Generalize?. (arXiv:2207.13645v4 [quant-ph] UPDATED)",
    "abstract": "In recent proposals of quantum circuit models for generative tasks, the discussion about their performance has been limited to their ability to reproduce a known target distribution. For example, expressive model families such as Quantum Circuit Born Machines (QCBMs) have been almost entirely evaluated on their capability to learn a given target distribution with high accuracy. While this aspect may be ideal for some tasks, it limits the scope of a generative model's assessment to its ability to memorize data rather than generalize. As a result, there has been little understanding of a model's generalization performance and the relation between such capability and the resource requirements, e.g., the circuit depth and the amount of training data. In this work, we leverage upon a recently proposed generalization evaluation framework to begin addressing this knowledge gap. We first investigate the QCBM's learning process of a cardinality-constrained distribution and see an increase in ge",
    "link": "http://arxiv.org/abs/2207.13645",
    "context": "Title: Do Quantum Circuit Born Machines Generalize?. (arXiv:2207.13645v4 [quant-ph] UPDATED)\nAbstract: In recent proposals of quantum circuit models for generative tasks, the discussion about their performance has been limited to their ability to reproduce a known target distribution. For example, expressive model families such as Quantum Circuit Born Machines (QCBMs) have been almost entirely evaluated on their capability to learn a given target distribution with high accuracy. While this aspect may be ideal for some tasks, it limits the scope of a generative model's assessment to its ability to memorize data rather than generalize. As a result, there has been little understanding of a model's generalization performance and the relation between such capability and the resource requirements, e.g., the circuit depth and the amount of training data. In this work, we leverage upon a recently proposed generalization evaluation framework to begin addressing this knowledge gap. We first investigate the QCBM's learning process of a cardinality-constrained distribution and see an increase in ge",
    "path": "papers/22/07/2207.13645.json",
    "total_tokens": 1059,
    "translated_title": "量子电路出生机器是否具有推广能力？",
    "translated_abstract": "在最近的量子电路模型生成任务的提议中，关于它们的表现的讨论仅限于它们重现已知目标分布的能力。例如，表达力强的模型系列，例如量子电路出生机器（QCBMs）几乎完全被评估其学习具有高准确性的给定目标分布的能力。虽然这方面对于某些任务可能是理想的，但它限制了生成模型评估其泛化能力的范围，而非其记忆数据的能力。因此，人们对模型的泛化性能以及此类能力与资源需求之间的关系了解甚少。在这项工作中，我们利用最近提出的泛化性评估框架来开始解决这一知识空白。我们首先研究了QCBM对基数受限分布的学习过程，并发现随着基数增加，泛化性能有所提高。然后我们展示了具有高泛化能力的学习模型表现出小样本学习能力，并探究了此能力的资源需求。最后，我们提出了一种改进的训练框架，具有课程学习策略，可以学习具有高准确性和泛化性能的复杂分布。",
    "tldr": "本文研究了量子电路出生机器对基数受限分布的学习过程，提高了模型的泛化性能，并探究了此能力的资源需求，并提出了一种改进的训练框架，可以学习具有高准确性和泛化性能的复杂分布.",
    "en_tdlr": "This paper investigates the learning process of Quantum Circuit Born Machines on cardinality-constrained distribution, improves the generalization performance, explores the resource requirements for few-shot learning, and proposes an improved training framework for learning complex distributions with high accuracy and generalization performance."
}