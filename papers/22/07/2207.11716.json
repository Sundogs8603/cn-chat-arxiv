{
    "title": "A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v2 [cs.CL] UPDATED)",
    "abstract": "Semantic similarity analysis and modeling is a fundamentally acclaimed task in many pioneering applications of natural language processing today. Owing to the sensation of sequential pattern recognition, many neural networks like RNNs and LSTMs have achieved satisfactory results in semantic similarity modeling. However, these solutions are considered inefficient due to their inability to process information in a non-sequential manner, thus leading to the improper extraction of context. Transformers function as the state-of-the-art architecture due to their advantages like non-sequential data processing and self-attention. In this paper, we perform semantic similarity analysis and modeling on the U.S Patent Phrase to Phrase Matching Dataset using both traditional and transformer-based techniques. We experiment upon four different variants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by performing K-Fold Cross-Validation. The experimental results demonstrate our me",
    "link": "http://arxiv.org/abs/2207.11716",
    "context": "Title: A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v2 [cs.CL] UPDATED)\nAbstract: Semantic similarity analysis and modeling is a fundamentally acclaimed task in many pioneering applications of natural language processing today. Owing to the sensation of sequential pattern recognition, many neural networks like RNNs and LSTMs have achieved satisfactory results in semantic similarity modeling. However, these solutions are considered inefficient due to their inability to process information in a non-sequential manner, thus leading to the improper extraction of context. Transformers function as the state-of-the-art architecture due to their advantages like non-sequential data processing and self-attention. In this paper, we perform semantic similarity analysis and modeling on the U.S Patent Phrase to Phrase Matching Dataset using both traditional and transformer-based techniques. We experiment upon four different variants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by performing K-Fold Cross-Validation. The experimental results demonstrate our me",
    "path": "papers/22/07/2207.11716.json",
    "total_tokens": 808,
    "translated_title": "基于Transformer的大语料库语义相似度分析的认知研究",
    "translated_abstract": "语义相似度分析和建模是当今自然语言处理许多先驱应用中基本认可的任务。由于顺序模式识别的感知，许多神经网络（如RNN和LSTM）在语义相似度建模方面取得了令人满意的结果。但是，由于它们无法以非顺序方式处理信息，因此这些解决方案被认为效率低下，从而导致上下文提取不当。Transformer因其非顺序数据处理和自我关注等优势而成为最先进的架构。本文使用传统和基于transformer的技术对美国专利短语进行语义相似度分析和建模。我们对四种不同版本的解码增强BERT-DeBERTa进行实验，并通过K折交叉验证来提高其性能。实验结果证明了我们的方法的有效性。",
    "tldr": "本文通过使用Transformer在U.S Patent Phrase to Phrase Matching Dataset上进行语义相似度分析，提高了算法效率，达到了令人满意的结果。",
    "en_tdlr": "This paper presents a cognitive study on semantic similarity analysis using both traditional and transformer-based techniques on the U.S Patent Phrase to Phrase Matching Dataset. The use of transformer models has shown to improve efficiency and achieve satisfactory results."
}