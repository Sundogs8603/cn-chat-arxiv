{
    "title": "DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization. (arXiv:2207.05631v2 [cs.LG] UPDATED)",
    "abstract": "Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variet",
    "link": "http://arxiv.org/abs/2207.05631",
    "context": "Title: DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization. (arXiv:2207.05631v2 [cs.LG] UPDATED)\nAbstract: Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variet",
    "path": "papers/22/07/2207.05631.json",
    "total_tokens": 954,
    "translated_title": "DGPO: 使用多样化策略优化发现多种解决方案",
    "translated_abstract": "大多数强化学习算法都试图寻找解决给定任务的单个最佳策略。然而，学习多种解决方案通常是有价值的，例如，使智能体与用户的交互更加有趣，或者提高策略对意外干扰的鲁棒性。我们提出了一种名为多样化策略优化（DGPO）的在线算法，用于发现解决给定任务的多种策略。与现有工作不同的是，它通过在单次运行中训练共享策略网络实现此目的。具体而言，我们设计了一种基于信息理论多样性目标的内在奖励。我们的最终目标交替约束策略多样性和外在奖励。我们将约束优化问题转化为概率推断任务，并使用策略迭代来最大化得到的下界。实验结果表明，我们的方法能够在各种环境中有效地发现多样化的策略，包括 Atari 游戏和 Mujoco 模拟器，并且能够提供一系列性能和多样性之间的权衡。",
    "tldr": "这篇论文提出了一个名为DGPO的算法，可以在解决任务时发现多种策略，从而提高策略鲁棒性和与用户交互的乐趣。",
    "en_tdlr": "This paper proposes an algorithm called DGPO that can discover multiple strategies to solve a given task, which can improve the robustness of a policy and increase interaction with users."
}