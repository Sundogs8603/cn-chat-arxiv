{
    "title": "Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design. (arXiv:2207.02575v2 [cs.LG] UPDATED)",
    "abstract": "While much progress has been made in understanding the minimax sample complexity of reinforcement learning (RL) -- the complexity of learning on the \"worst-case\" instance -- such measures of complexity often do not capture the true difficulty of learning. In practice, on an \"easy\" instance, we might hope to achieve a complexity far better than that achievable on the worst-case instance. In this work we seek to understand the \"instance-dependent\" complexity of learning near-optimal policies (PAC RL) in the setting of RL with linear function approximation. We propose an algorithm, \\textsc{Pedel}, which achieves a fine-grained instance-dependent measure of complexity, the first of its kind in the RL with function approximation setting, thereby capturing the difficulty of learning on each particular problem instance. Through an explicit example, we show that \\textsc{Pedel} yields provable gains over low-regret, minimax-optimal algorithms and that such algorithms are unable to hit the insta",
    "link": "http://arxiv.org/abs/2207.02575",
    "context": "Title: Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design. (arXiv:2207.02575v2 [cs.LG] UPDATED)\nAbstract: While much progress has been made in understanding the minimax sample complexity of reinforcement learning (RL) -- the complexity of learning on the \"worst-case\" instance -- such measures of complexity often do not capture the true difficulty of learning. In practice, on an \"easy\" instance, we might hope to achieve a complexity far better than that achievable on the worst-case instance. In this work we seek to understand the \"instance-dependent\" complexity of learning near-optimal policies (PAC RL) in the setting of RL with linear function approximation. We propose an algorithm, \\textsc{Pedel}, which achieves a fine-grained instance-dependent measure of complexity, the first of its kind in the RL with function approximation setting, thereby capturing the difficulty of learning on each particular problem instance. Through an explicit example, we show that \\textsc{Pedel} yields provable gains over low-regret, minimax-optimal algorithms and that such algorithms are unable to hit the insta",
    "path": "papers/22/07/2207.02575.json",
    "total_tokens": 990,
    "translated_title": "基于在线实验设计的线性MDPs中的依赖于实例的近最优策略识别",
    "translated_abstract": "在强化学习中，虽然对于最坏情况实例下的最小最大样本复杂度有了很大的进展，但是这种复杂度衡量往往不能真正反映出学习的真正困难。在实践中，对于一个“简单”的实例，我们可能希望能够实现比最坏实例下更好的复杂度。在本文中，我们希望了解在具有线性函数逼近的强化学习设置中，学习近最优策略（PAC RL）的“依赖于实例”的复杂度。我们提出了一个算法\\textsc{Pedel}，它实现了一种细粒度的依赖于实例的复杂度度量，这是在函数逼近设置中首次出现的，从而捕捉了在每个特定问题实例上学习的困难程度。通过一个具体的例子，我们展示了\\textsc{Pedel}相对于最低遗憾、最小最大最优算法的可证明收益，并且这些算法无法达到这种效果。",
    "tldr": "本文研究了在线实验设计方法在线性MDPs中的应用，提出了一种算法\\textsc{Pedel}，该算法在RL中的函数逼近设置下实现了细粒度的依赖于实例的复杂度度量，相对于最低遗憾、最小最大最优算法具有明显收益。",
    "en_tdlr": "This paper investigates the application of online experiment design in linear MDPs and proposes an algorithm called \\textsc{Pedel} that achieves fine-grained instance-dependent measure of complexity in RL with function approximation, showing significant gains compared to low-regret, minimax-optimal algorithms."
}