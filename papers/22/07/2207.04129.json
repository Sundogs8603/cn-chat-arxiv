{
    "title": "How many perturbations break this model? Evaluating robustness beyond adversarial accuracy. (arXiv:2207.04129v3 [cs.LG] UPDATED)",
    "abstract": "Robustness to adversarial attacks is typically evaluated with adversarial accuracy. While essential, this metric does not capture all aspects of robustness and in particular leaves out the question of how many perturbations can be found for each point. In this work, we introduce an alternative approach, adversarial sparsity, which quantifies how difficult it is to find a successful perturbation given both an input point and a constraint on the direction of the perturbation. We show that sparsity provides valuable insight into neural networks in multiple ways: for instance, it illustrates important differences between current state-of-the-art robust models them that accuracy analysis does not, and suggests approaches for improving their robustness. When applying broken defenses effective against weak attacks but not strong ones, sparsity can discriminate between the totally ineffective and the partially effective defenses. Finally, with sparsity we can measure increases in robustness th",
    "link": "http://arxiv.org/abs/2207.04129",
    "context": "Title: How many perturbations break this model? Evaluating robustness beyond adversarial accuracy. (arXiv:2207.04129v3 [cs.LG] UPDATED)\nAbstract: Robustness to adversarial attacks is typically evaluated with adversarial accuracy. While essential, this metric does not capture all aspects of robustness and in particular leaves out the question of how many perturbations can be found for each point. In this work, we introduce an alternative approach, adversarial sparsity, which quantifies how difficult it is to find a successful perturbation given both an input point and a constraint on the direction of the perturbation. We show that sparsity provides valuable insight into neural networks in multiple ways: for instance, it illustrates important differences between current state-of-the-art robust models them that accuracy analysis does not, and suggests approaches for improving their robustness. When applying broken defenses effective against weak attacks but not strong ones, sparsity can discriminate between the totally ineffective and the partially effective defenses. Finally, with sparsity we can measure increases in robustness th",
    "path": "papers/22/07/2207.04129.json",
    "total_tokens": 953,
    "translated_title": "这个模型有多少扰动会破坏它？评估超越对抗准确度的鲁棒性。",
    "translated_abstract": "对抗攻击的鲁棒性通常通过对抗准确度来评估。然而，这个度量标准并不能完全捕捉到鲁棒性的所有方面，尤其是忽略了针对每个数据点可以找到多少扰动的问题。在这项工作中，我们引入了一种替代方法，即对抗稀疏性，它量化了在给定输入点和扰动方向约束的情况下找到成功扰动的难度。我们展示了稀疏性在多个方面对神经网络提供了有价值的洞察力：例如，它揭示了当前最先进的鲁棒模型之间的重要差异，这是精确度分析所不具备的，并且提出了提高它们鲁棒性的方法。当应用对弱攻击有效但对强攻击无效的破解防御时，稀疏性可以区分完全无效和部分有效的防御。最后，通过稀疏性，我们可以度量鲁棒性的增加。",
    "tldr": "这项工作介绍了一种评估神经网络鲁棒性的替代方法-对抗稀疏性，它量化了成功扰动的难度。稀疏性揭示了鲁棒模型之间的重要差异并提出了改进鲁棒性的方法。",
    "en_tdlr": "This work introduces an alternative approach, called adversarial sparsity, to evaluate the robustness of neural networks. The sparsity quantifies the difficulty of finding successful perturbations and reveals important differences between robust models, suggesting improvements to enhance their robustness."
}