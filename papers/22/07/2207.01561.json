{
    "title": "Selectively increasing the diversity of GAN-generated samples. (arXiv:2207.01561v3 [cs.CV] UPDATED)",
    "abstract": "Generative Adversarial Networks (GANs) are powerful models able to synthesize data samples closely resembling the distribution of real data, yet the diversity of those generated samples is limited due to the so-called mode collapse phenomenon observed in GANs. Especially prone to mode collapse are conditional GANs, which tend to ignore the input noise vector and focus on the conditional information. Recent methods proposed to mitigate this limitation increase the diversity of generated samples, yet they reduce the performance of the models when similarity of samples is required. To address this shortcoming, we propose a novel method to selectively increase the diversity of GAN-generated samples. By adding a simple, yet effective regularization to the training loss function we encourage the generator to discover new data modes for inputs related to diverse outputs while generating consistent samples for the remaining ones. More precisely, we maximise the ratio of distances between gener",
    "link": "http://arxiv.org/abs/2207.01561",
    "context": "Title: Selectively increasing the diversity of GAN-generated samples. (arXiv:2207.01561v3 [cs.CV] UPDATED)\nAbstract: Generative Adversarial Networks (GANs) are powerful models able to synthesize data samples closely resembling the distribution of real data, yet the diversity of those generated samples is limited due to the so-called mode collapse phenomenon observed in GANs. Especially prone to mode collapse are conditional GANs, which tend to ignore the input noise vector and focus on the conditional information. Recent methods proposed to mitigate this limitation increase the diversity of generated samples, yet they reduce the performance of the models when similarity of samples is required. To address this shortcoming, we propose a novel method to selectively increase the diversity of GAN-generated samples. By adding a simple, yet effective regularization to the training loss function we encourage the generator to discover new data modes for inputs related to diverse outputs while generating consistent samples for the remaining ones. More precisely, we maximise the ratio of distances between gener",
    "path": "papers/22/07/2207.01561.json",
    "total_tokens": 1068,
    "translated_title": "有选择地增加GAN生成样本的多样性",
    "translated_abstract": "生成对抗网络(GAN)是强大的模型，能够合成与真实数据分布密切相似的数据样本，但是GAN生成的样本多样性受到所谓的模式崩溃现象的限制。特别容易出现模式崩溃的是条件GAN，它们倾向于忽略输入噪声向量，专注于条件信息。最近提出的方法旨在减轻这种限制，增加生成的样本的多样性，但当需要样本相似性时，它们会降低模型的性能。为了解决这个缺点，我们提出了一种新方法，有选择地增加GAN生成的样本的多样性。通过向训练损失函数添加一个简单但有效的正则化，我们鼓励生成器为与多样化输出相关的输入发现新的数据模式，同时为其余其余的输入生成一致的样本。更具体地说，我们最大化不同条件输入生成的样本之间的距离比与相同条件输入生成的样本之间的距离。我们在几个基准数据集上验证了所提出的方法，并展示了它在增加GAN生成的样本的多样性的有效性，同时保持它们的质量，如Frechet Inception距离(FID)和Inception分数(IS)所衡量的质量。",
    "tldr": "提出了一种有选择性地增加GAN生成样本多样性的新方法，并通过向损失函数添加正则化进行优化。在保持生成样本质量的同时，有效增加其多样性，验证结果良好。",
    "en_tdlr": "This paper proposes a novel method to selectively increase the diversity of GAN-generated samples by adding a simple, yet effective regularization to the training loss function. The method aims to encourage the generator to discover new data modes for inputs related to diverse outputs while generating consistent samples for the remaining ones. The proposed method is validated on several benchmark datasets and shows its effectiveness in increasing the diversity of GAN-generated samples while maintaining their quality as measured by Fréchet Inception Distance (FID) and Inception Score (IS)."
}