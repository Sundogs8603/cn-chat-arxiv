{
    "title": "Information Processing Equalities and the Information-Risk Bridge. (arXiv:2207.11987v2 [cs.LG] UPDATED)",
    "abstract": "We introduce two new classes of measures of information for statistical experiments which generalise and subsume $\\phi$-divergences, integral probability metrics, $\\mathfrak{N}$-distances (MMD), and $(f,\\Gamma)$ divergences between two or more distributions. This enables us to derive a simple geometrical relationship between measures of information and the Bayes risk of a statistical decision problem, thus extending the variational $\\phi$-divergence representation to multiple distributions in an entirely symmetric manner. The new families of divergence are closed under the action of Markov operators which yields an information processing equality which is a refinement and generalisation of the classical data processing inequality. This equality gives insight into the significance of the choice of the hypothesis class in classical risk minimization.",
    "link": "http://arxiv.org/abs/2207.11987",
    "context": "Title: Information Processing Equalities and the Information-Risk Bridge. (arXiv:2207.11987v2 [cs.LG] UPDATED)\nAbstract: We introduce two new classes of measures of information for statistical experiments which generalise and subsume $\\phi$-divergences, integral probability metrics, $\\mathfrak{N}$-distances (MMD), and $(f,\\Gamma)$ divergences between two or more distributions. This enables us to derive a simple geometrical relationship between measures of information and the Bayes risk of a statistical decision problem, thus extending the variational $\\phi$-divergence representation to multiple distributions in an entirely symmetric manner. The new families of divergence are closed under the action of Markov operators which yields an information processing equality which is a refinement and generalisation of the classical data processing inequality. This equality gives insight into the significance of the choice of the hypothesis class in classical risk minimization.",
    "path": "papers/22/07/2207.11987.json",
    "total_tokens": 826,
    "translated_title": "信息处理相等性和信息风险桥梁",
    "translated_abstract": "我们引入了两类新的信息度量方法，用于统计实验，推广了和包含了$\\phi$-散度、积分概率度量、$\\mathfrak{N}$-距离（MMD）和两个或多个分布之间的$(f,\\Gamma)$-散度。这使得我们能够推导出信息度量和统计决策问题的贝叶斯风险之间的简单几何关系，从而以完全对称的方式将变分$\\phi$-散度表示扩展到多个分布中。新的散度族在马尔可夫算子的作用下保持不变，产生了一个信息处理等式，它是经典数据处理不等式的细化和推广。这个等式揭示了在经典风险最小化中选择假设类的重要性。",
    "tldr": "本论文引入了两类新的信息度量方法，扩展和统一了不同分布之间的散度，通过信息度量和贝叶斯风险之间的几何关系以及信息处理等式，揭示了在经典风险最小化中选择假设类的重要性。",
    "en_tdlr": "This paper introduces two new classes of information measures that generalize and unify divergences between different distributions. By exploring the geometrical relationship between information measures and Bayes risk, as well as the information processing equality, the paper sheds light on the importance of hypothesis class selection in classical risk minimization."
}