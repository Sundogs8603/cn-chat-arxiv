{
    "title": "Generating and Detecting True Ambiguity: A Forgotten Danger in DNN Supervision Testing. (arXiv:2207.10495v2 [cs.SE] UPDATED)",
    "abstract": "Deep Neural Networks (DNNs) are becoming a crucial component of modern software systems, but they are prone to fail under conditions that are different from the ones observed during training (out-of-distribution inputs) or on inputs that are truly ambiguous, i.e., inputs that admit multiple classes with nonzero probability in their labels. Recent work proposed DNN supervisors to detect high-uncertainty inputs before their possible misclassification leads to any harm. To test and compare the capabilities of DNN supervisors, researchers proposed test generation techniques, to focus the testing effort on high-uncertainty inputs that should be recognized as anomalous by supervisors. However, existing test generators aim to produce out-of-distribution inputs. No existing model- and supervisor independent technique targets the generation of truly ambiguous test inputs, i.e., inputs that admit multiple classes according to expert human judgment.  In this paper, we propose a novel way to gener",
    "link": "http://arxiv.org/abs/2207.10495",
    "context": "Title: Generating and Detecting True Ambiguity: A Forgotten Danger in DNN Supervision Testing. (arXiv:2207.10495v2 [cs.SE] UPDATED)\nAbstract: Deep Neural Networks (DNNs) are becoming a crucial component of modern software systems, but they are prone to fail under conditions that are different from the ones observed during training (out-of-distribution inputs) or on inputs that are truly ambiguous, i.e., inputs that admit multiple classes with nonzero probability in their labels. Recent work proposed DNN supervisors to detect high-uncertainty inputs before their possible misclassification leads to any harm. To test and compare the capabilities of DNN supervisors, researchers proposed test generation techniques, to focus the testing effort on high-uncertainty inputs that should be recognized as anomalous by supervisors. However, existing test generators aim to produce out-of-distribution inputs. No existing model- and supervisor independent technique targets the generation of truly ambiguous test inputs, i.e., inputs that admit multiple classes according to expert human judgment.  In this paper, we propose a novel way to gener",
    "path": "papers/22/07/2207.10495.json",
    "total_tokens": 867,
    "translated_title": "生成和检测真正的歧义：DNN监督测试中的一个被忽视的危险",
    "translated_abstract": "深度神经网络(DNNs)正在成为现代软件系统的关键组成部分，但在与训练期间观察到的条件不同的情况下（超出分布的输入）或在真正模糊的输入上（即，在标签中存在多个类别且其概率不为零的输入）可能会失败。最近的工作提出了DNN监督器，以在可能导致任何损害之前检测到高不确定性的输入。为了测试和比较DNN监督器的能力，研究者们提出了测试生成技术，以将测试工作重点放在那些监督器应该将其识别为异常的高不确定性输入上。然而，现有的测试生成器旨在生成超出分布的输入。没有现有的模型和监督器无关的技术针对生成真正模糊的测试输入，即，根据专家人员判断，输入可以对应多个类别。在本文中，我们提出了一种新的方法来生成和检测真正歧义的测试输入。",
    "tldr": "本文提出了一种新的方法来生成和检测真正歧义的测试输入，以解决现有测试生成器只能生成超出分布输入的问题。",
    "en_tdlr": "This paper proposes a novel approach to generate and detect truly ambiguous test inputs, addressing the limitation of existing test generators that can only generate out-of-distribution inputs."
}