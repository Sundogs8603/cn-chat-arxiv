{
    "title": "Provably tuning the ElasticNet across instances. (arXiv:2207.10199v2 [cs.LG] UPDATED)",
    "abstract": "An important unresolved challenge in the theory of regularization is to set the regularization coefficients of popular techniques like the ElasticNet with general provable guarantees. We consider the problem of tuning the regularization parameters of Ridge regression, LASSO, and the ElasticNet across multiple problem instances, a setting that encompasses both cross-validation and multi-task hyperparameter optimization. We obtain a novel structural result for the ElasticNet which characterizes the loss as a function of the tuning parameters as a piecewise-rational function with algebraic boundaries. We use this to bound the structural complexity of the regularized loss functions and show generalization guarantees for tuning the ElasticNet regression coefficients in the statistical setting. We also consider the more challenging online learning setting, where we show vanishing average expected regret relative to the optimal parameter pair. We further extend our results to tuning classific",
    "link": "http://arxiv.org/abs/2207.10199",
    "context": "Title: Provably tuning the ElasticNet across instances. (arXiv:2207.10199v2 [cs.LG] UPDATED)\nAbstract: An important unresolved challenge in the theory of regularization is to set the regularization coefficients of popular techniques like the ElasticNet with general provable guarantees. We consider the problem of tuning the regularization parameters of Ridge regression, LASSO, and the ElasticNet across multiple problem instances, a setting that encompasses both cross-validation and multi-task hyperparameter optimization. We obtain a novel structural result for the ElasticNet which characterizes the loss as a function of the tuning parameters as a piecewise-rational function with algebraic boundaries. We use this to bound the structural complexity of the regularized loss functions and show generalization guarantees for tuning the ElasticNet regression coefficients in the statistical setting. We also consider the more challenging online learning setting, where we show vanishing average expected regret relative to the optimal parameter pair. We further extend our results to tuning classific",
    "path": "papers/22/07/2207.10199.json",
    "total_tokens": 817,
    "translated_title": "可证明地调整ElasticNet在多个实例间的参数",
    "translated_abstract": "正规化理论中一个重要未解决的挑战是如何设置常用技术（如ElasticNet）的正规化系数，并提供一般可证明的保证。我们考虑了在多个问题实例中调整Ridge回归、LASSO和ElasticNet的正规化参数的问题，这种设置包括了交叉验证和多任务超参数优化。我们获得了针对ElasticNet的新颖结构结果，将损失函数表达为以调整参数为函数的分段有理函数，其中包含代数边界。我们利用这一结果对正规化损失函数的结构复杂性进行了界定，并在统计情景下展示了调整ElasticNet回归系数的泛化保证。我们还考虑了更具挑战性的在线学习情景，展示了与最优参数对相对而言，消失的平均预期遗憾。我们进一步将我们的结果扩展到调整分类问题的情景。",
    "tldr": "这篇论文提供了一个针对ElasticNet的新颖结构结果，用以证明在多个问题实例中调整正规化参数，同时提供了统计和在线学习情景下的泛化保证。",
    "en_tdlr": "This paper presents a novel structural result for tuning the regularization parameters of ElasticNet in multiple problem instances, and provides generalization guarantees in both statistical and online learning settings."
}