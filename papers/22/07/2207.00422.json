{
    "title": "Personalized Showcases: Generating Multi-Modal Explanations for Recommendations. (arXiv:2207.00422v2 [cs.IR] UPDATED)",
    "abstract": "Existing explanation models generate only text for recommendations but still struggle to produce diverse contents. In this paper, to further enrich explanations, we propose a new task named personalized showcases, in which we provide both textual and visual information to explain our recommendations. Specifically, we first select a personalized image set that is the most relevant to a user's interest toward a recommended item. Then, natural language explanations are generated accordingly given our selected images. For this new task, we collect a large-scale dataset from Google Local (i.e.,~maps) and construct a high-quality subset for generating multi-modal explanations. We propose a personalized multi-modal framework which can generate diverse and visually-aligned explanations via contrastive learning. Experiments show that our framework benefits from different modalities as inputs, and is able to produce more diverse and expressive explanations compared to previous methods on a varie",
    "link": "http://arxiv.org/abs/2207.00422",
    "context": "Title: Personalized Showcases: Generating Multi-Modal Explanations for Recommendations. (arXiv:2207.00422v2 [cs.IR] UPDATED)\nAbstract: Existing explanation models generate only text for recommendations but still struggle to produce diverse contents. In this paper, to further enrich explanations, we propose a new task named personalized showcases, in which we provide both textual and visual information to explain our recommendations. Specifically, we first select a personalized image set that is the most relevant to a user's interest toward a recommended item. Then, natural language explanations are generated accordingly given our selected images. For this new task, we collect a large-scale dataset from Google Local (i.e.,~maps) and construct a high-quality subset for generating multi-modal explanations. We propose a personalized multi-modal framework which can generate diverse and visually-aligned explanations via contrastive learning. Experiments show that our framework benefits from different modalities as inputs, and is able to produce more diverse and expressive explanations compared to previous methods on a varie",
    "path": "papers/22/07/2207.00422.json",
    "total_tokens": 900,
    "translated_title": "个性化展示：生成面向推荐的多模态解释",
    "translated_abstract": "现有的解释模型只为推荐生成文本，但仍然难以产生多样化的内容。在本文中，我们提出了一个名为“个性化展示”的新任务，通过在解释中提供文本和视觉信息来进一步丰富解释。具体而言，我们首先选择一个定制的图像集，该集合与用户对推荐物品的兴趣最相关。然后，根据我们所选的图像生成自然语言解释。 为了实现这个新任务，我们从 Google Local（即地图）收集了一个大规模的数据集，并构建了一个高质量的子集以生成多模态解释。我们提出了一个个性化多模态框架，可以通过对比学习生成多样化和视觉一致的解释。实验表明，我们的框架受益于不同的输入模态，并且能够产生比先前方法更多样化和更具表现力的解释。",
    "tldr": "该论文提出了一个新的任务——个性化展示，通过提供文本和视觉信息进一步丰富推荐的解释。作者从 Google Local（即地图）收集了一个大规模的数据集，并提出了一个个性化多模态框架。实验证明，该框架能够产生比先前方法更多样化和更具表现力的解释。",
    "en_tdlr": "This paper proposes a new task, personalized showcases, for further enriching explanations for recommendations by providing both textual and visual information. The authors collected a large-scale dataset from Google Local and proposed a personalized multi-modal framework. Experiments show that the framework is able to generate more diverse and expressive explanations compared to previous methods."
}