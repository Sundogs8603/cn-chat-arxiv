{
    "title": "Implicit regularization of dropout. (arXiv:2207.05952v2 [cs.LG] UPDATED)",
    "abstract": "It is important to understand how dropout, a popular regularization method, aids in achieving a good generalization solution during neural network training. In this work, we present a theoretical derivation of an implicit regularization of dropout, which is validated by a series of experiments. Additionally, we numerically study two implications of the implicit regularization, which intuitively rationalizes why dropout helps generalization. Firstly, we find that input weights of hidden neurons tend to condense on isolated orientations trained with dropout. Condensation is a feature in the non-linear learning process, which makes the network less complex. Secondly, we experimentally find that the training with dropout leads to the neural network with a flatter minimum compared with standard gradient descent training, and the implicit regularization is the key to finding flat solutions. Although our theory mainly focuses on dropout used in the last hidden layer, our experiments apply to ",
    "link": "http://arxiv.org/abs/2207.05952",
    "context": "Title: Implicit regularization of dropout. (arXiv:2207.05952v2 [cs.LG] UPDATED)\nAbstract: It is important to understand how dropout, a popular regularization method, aids in achieving a good generalization solution during neural network training. In this work, we present a theoretical derivation of an implicit regularization of dropout, which is validated by a series of experiments. Additionally, we numerically study two implications of the implicit regularization, which intuitively rationalizes why dropout helps generalization. Firstly, we find that input weights of hidden neurons tend to condense on isolated orientations trained with dropout. Condensation is a feature in the non-linear learning process, which makes the network less complex. Secondly, we experimentally find that the training with dropout leads to the neural network with a flatter minimum compared with standard gradient descent training, and the implicit regularization is the key to finding flat solutions. Although our theory mainly focuses on dropout used in the last hidden layer, our experiments apply to ",
    "path": "papers/22/07/2207.05952.json",
    "total_tokens": 854,
    "tldr": "本文通过理论推导和实验证实了Dropout方法的隐式正则化效果，并发现使用Dropout的训练可使得神经网络更加简单和平坦，有助于泛化。"
}