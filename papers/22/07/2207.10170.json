{
    "title": "Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers. (arXiv:2207.10170v3 [cs.AI] UPDATED)",
    "abstract": "Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. Robustifying agent policies requires anticipating the strongest attacks possible. We demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of temporal consistency makes them detectable using automated means or human inspection. Detectability is undesirable to adversaries as it may trigger security escalations. We introduce perfect illusory attacks, a novel form of adversarial attack on sequential decision-makers that is both effective and provably statistically undetectable. We then propose the more versatile R-attacks, which result in observation transitions that are consistent with the state-transition function of the adversary-free environment and can be learned end-to-end. Compared to existing attacks, we empirically find R-attacks to be significantly harder to detect with automated methods, ",
    "link": "http://arxiv.org/abs/2207.10170",
    "context": "Title: Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers. (arXiv:2207.10170v3 [cs.AI] UPDATED)\nAbstract: Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. Robustifying agent policies requires anticipating the strongest attacks possible. We demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of temporal consistency makes them detectable using automated means or human inspection. Detectability is undesirable to adversaries as it may trigger security escalations. We introduce perfect illusory attacks, a novel form of adversarial attack on sequential decision-makers that is both effective and provably statistically undetectable. We then propose the more versatile R-attacks, which result in observation transitions that are consistent with the state-transition function of the adversary-free environment and can be learned end-to-end. Compared to existing attacks, we empirically find R-attacks to be significantly harder to detect with automated methods, ",
    "path": "papers/22/07/2207.10170.json",
    "total_tokens": 932,
    "translated_title": "幻觉攻击：对顺序决策者的敌对攻击中可检测性很重要",
    "translated_abstract": "在实际世界中部署的自主代理需要对感官输入的敌对攻击具备强大的鲁棒性。强化代理策略需要预测可能的最强攻击。我们证明了现有的强化学习代理的观测空间攻击具有共同的弱点：虽然有效，但它们缺乏时间上的一致性，因此可以使用自动化手段或人工检查来检测。对于敌手来说，可检测性是不希望出现的，因为它可能会引发安全事态升级。我们引入了完美的幻觉攻击，这是一种新形式的顺序决策者的敌对攻击，既有效又可证明是统计不可检测的。随后，我们提出了更加灵活的R-attack，其生成的观测转换与无敌对环境的状态转换函数一致且可以端到端学习。实验结果显示，与现有的攻击相比，R-attack更难以使用自动化方法检测出来。",
    "tldr": "对于顺序决策者的敌对攻击来说，弱点是缺乏时间上的一致性，使其容易被检测出来；而R-attack是一种既有效又可证明是统计不可检测的攻击，可以更难以使用自动化方法检测出来。",
    "en_tdlr": "The weakness of existing adversarial attacks on sequential decision-makers lies in their lack of temporal consistency, making them detectable. This paper proposes R-attacks, which are effective and statistically undetectable, to overcome this weakness."
}