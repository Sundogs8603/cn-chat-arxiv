{
    "title": "Learning differentiable solvers for systems with hard constraints. (arXiv:2207.08675v2 [cs.LG] UPDATED)",
    "abstract": "We introduce a practical method to enforce partial differential equation (PDE) constraints for functions defined by neural networks (NNs), with a high degree of accuracy and up to a desired tolerance. We develop a differentiable PDE-constrained layer that can be incorporated into any NN architecture. Our method leverages differentiable optimization and the implicit function theorem to effectively enforce physical constraints. Inspired by dictionary learning, our model learns a family of functions, each of which defines a mapping from PDE parameters to PDE solutions. At inference time, the model finds an optimal linear combination of the functions in the learned family by solving a PDE-constrained optimization problem. Our method provides continuous solutions over the domain of interest that accurately satisfy desired physical constraints. Our results show that incorporating hard constraints directly into the NN architecture achieves much lower test error when compared to training on an",
    "link": "http://arxiv.org/abs/2207.08675",
    "context": "Title: Learning differentiable solvers for systems with hard constraints. (arXiv:2207.08675v2 [cs.LG] UPDATED)\nAbstract: We introduce a practical method to enforce partial differential equation (PDE) constraints for functions defined by neural networks (NNs), with a high degree of accuracy and up to a desired tolerance. We develop a differentiable PDE-constrained layer that can be incorporated into any NN architecture. Our method leverages differentiable optimization and the implicit function theorem to effectively enforce physical constraints. Inspired by dictionary learning, our model learns a family of functions, each of which defines a mapping from PDE parameters to PDE solutions. At inference time, the model finds an optimal linear combination of the functions in the learned family by solving a PDE-constrained optimization problem. Our method provides continuous solutions over the domain of interest that accurately satisfy desired physical constraints. Our results show that incorporating hard constraints directly into the NN architecture achieves much lower test error when compared to training on an",
    "path": "papers/22/07/2207.08675.json",
    "total_tokens": 908,
    "translated_title": "学习可微分求解器以处理带硬约束的系统",
    "translated_abstract": "我们引入了一种实用的方法，可以以高度准确的方式满足部分微分方程（PDE）约束，以及达到所需的容差。我们开发了一种可微分的PDE受约束层，可以融入任何神经网络结构中。我们的方法利用了可微分优化和隐式函数定理，有效地实现物理约束。受到字典学习的启发，我们的模型学习了一个函数族，其中每个函数都定义了从PDE参数到PDE解的映射。在推断时，模型通过求解PDE受约束的优化问题，找到学习到的函数族中的最优线性组合。我们的方法在感兴趣的域上提供了连续的解，准确地满足所需的物理约束。我们的结果表明，将硬约束直接融入神经网络结构，与在无约束目标函数上训练相比可以实现更低的测试误差。",
    "tldr": "该论文提出了一种学习可微分求解器的方法，可以以高度准确的方式满足部分微分方程约束，并且在推断时可以提供连续的解来满足所需的物理约束。该方法能够显著降低测试误差。",
    "en_tdlr": "The paper proposes a method to learn differentiable solvers that can accurately satisfy partial differential equation (PDE) constraints with continuous solutions, achieved through a differentiable PDE-constrained layer incorporated into neural networks. The method leverages differentiable optimization and dictionary learning to effectively enforce physical constraints, achieving significantly lower test error than training on unconstrained objective function."
}