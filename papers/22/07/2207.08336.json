{
    "title": "When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive Attributes. (arXiv:2207.08336v2 [cs.LG] UPDATED)",
    "abstract": "Machine learning models have demonstrated promising performance in many areas. However, the concerns that they can be biased against specific demographic groups hinder their adoption in high-stake applications. Thus, it is essential to ensure fairness in machine learning models. Most previous efforts require direct access to sensitive attributes for mitigating bias. Nonetheless, it is often infeasible to obtain large-scale users' sensitive attributes considering users' concerns about privacy in the data collection process. Privacy mechanisms such as local differential privacy (LDP) are widely enforced on sensitive information in the data collection stage due to legal compliance and people's increasing awareness of privacy. Therefore, a critical problem is how to make fair predictions under privacy. We study a novel and practical problem of fair classification in a semi-private setting, where most of the sensitive attributes are private and only a small amount of clean ones are availabl",
    "link": "http://arxiv.org/abs/2207.08336",
    "context": "Title: When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive Attributes. (arXiv:2207.08336v2 [cs.LG] UPDATED)\nAbstract: Machine learning models have demonstrated promising performance in many areas. However, the concerns that they can be biased against specific demographic groups hinder their adoption in high-stake applications. Thus, it is essential to ensure fairness in machine learning models. Most previous efforts require direct access to sensitive attributes for mitigating bias. Nonetheless, it is often infeasible to obtain large-scale users' sensitive attributes considering users' concerns about privacy in the data collection process. Privacy mechanisms such as local differential privacy (LDP) are widely enforced on sensitive information in the data collection stage due to legal compliance and people's increasing awareness of privacy. Therefore, a critical problem is how to make fair predictions under privacy. We study a novel and practical problem of fair classification in a semi-private setting, where most of the sensitive attributes are private and only a small amount of clean ones are availabl",
    "path": "papers/22/07/2207.08336.json",
    "total_tokens": 881,
    "translated_title": "当公平遇到隐私：半隐私敏感属性下的公平分类",
    "translated_abstract": "机器学习模型在许多领域已经展示出良好的性能。然而，对特定人口群体的偏见可能会阻碍它们在高风险应用中的采用。因此，在机器学习模型中确保公平性是至关重要的。大多数先前的努力需要直接访问敏感属性以减轻偏见。然而，考虑到用户对数据收集过程中隐私的担忧，通常难以获取大规模用户的敏感属性。由于法律合规性和人们对隐私的日益关注，隐私机制例如本地差分隐私（LDP）被广泛强制执行于敏感信息的数据收集阶段。因此，一个关键的问题是如何在隐私保护下进行公平预测。我们研究了一个新颖且实用的半隐私场景下的公平分类问题，其中大部分敏感属性是私有的，只有一小部分是可用的敏感属性是干净的。",
    "tldr": "该论文研究在半隐私场景下如何通过本地差分隐私（LDP）实现公平分类，解决了在收集大规模用户敏感属性时的难题。"
}