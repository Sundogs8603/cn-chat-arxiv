{
    "title": "Measuring Forgetting of Memorized Training Examples. (arXiv:2207.00099v2 [cs.LG] UPDATED)",
    "abstract": "Machine learning models exhibit two seemingly contradictory phenomena: training data memorization, and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena. We propose a technique to measure to what extent models \"forget\" the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently. We show that, while non-convex models can memorize data forever in the worst-case, standard image, speech, and language models empirically do forget examples over time. We identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget. Our results suggest that examples seen early when training with extremely large datasets - for instance those examples used to pre-train a model - may observe privacy be",
    "link": "http://arxiv.org/abs/2207.00099",
    "context": "Title: Measuring Forgetting of Memorized Training Examples. (arXiv:2207.00099v2 [cs.LG] UPDATED)\nAbstract: Machine learning models exhibit two seemingly contradictory phenomena: training data memorization, and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena. We propose a technique to measure to what extent models \"forget\" the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently. We show that, while non-convex models can memorize data forever in the worst-case, standard image, speech, and language models empirically do forget examples over time. We identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget. Our results suggest that examples seen early when training with extremely large datasets - for instance those examples used to pre-train a model - may observe privacy be",
    "path": "papers/22/07/2207.00099.json",
    "total_tokens": 951,
    "translated_title": "记忆化训练样本的遗忘程度测量",
    "translated_abstract": "机器学习模型表现出两种似乎矛盾的现象: 训练数据的记忆化和各种形式的遗忘。在记忆化中，模型过拟合特定的训练样本，并变得容易受到隐私攻击的影响。在遗忘中，出现在训练早期的样本最终会被遗忘。在本研究中，我们将这些现象联系起来。我们提出了一种技术来衡量模型“遗忘”训练样本的具体细节，从而使它们变得不太容易受到最近没有看到的样本的隐私攻击的影响。我们展示了，虽然非凸模型可以在最坏情况下永久记忆化数据，但标准的图像、语音和语言模型在时间上确实会遗忘示例。我们确定了非确定性作为可能的解释，表明确定性训练的模型不会遗忘。我们的结果表明，当使用极大数据集进行训练时，观察到的隐私可能会出现在训练早期看到的样本上，例如用于预先训练模型的样本。",
    "tldr": "本文提出了一种测量机器学习模型对训练样本遗忘程度的技术，发现标准的图像、语音和语言模型在时间上确实会遗忘示例，确定性训练的模型不会遗忘。",
    "en_tdlr": "This paper proposes a technique to measure the forgetting of training examples by machine learning models, and shows that standard image, speech, and language models do forget examples over time, while deterministically trained models do not forget."
}