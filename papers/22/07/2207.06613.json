{
    "title": "T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit. (arXiv:2207.06613v2 [cs.LG] UPDATED)",
    "abstract": "Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is gaining popularity due to recent breakthroughs in ML and Internet of Things (IoT). Most tinyML research focuses on model compression techniques that trade accuracy (and model capacity) for compact models to fit into the KB-sized tiny-edge devices. In this paper, we show how such models can be enhanced by the addition of an early exit intermediate classifier. If the intermediate classifier exhibits sufficient confidence in its prediction, the network exits early thereby, resulting in considerable savings in time. Although early exit classifiers have been proposed in previous work, these previous proposals focus on large networks, making their techniques suboptimal/impractical for tinyML applications. Our technique is optimized specifically for tiny-CNN sized models. In addition, we present a method to alleviate the effect of network overthinking by leveraging the representations learned by the early exit. We eva",
    "link": "http://arxiv.org/abs/2207.06613",
    "context": "Title: T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit. (arXiv:2207.06613v2 [cs.LG] UPDATED)\nAbstract: Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is gaining popularity due to recent breakthroughs in ML and Internet of Things (IoT). Most tinyML research focuses on model compression techniques that trade accuracy (and model capacity) for compact models to fit into the KB-sized tiny-edge devices. In this paper, we show how such models can be enhanced by the addition of an early exit intermediate classifier. If the intermediate classifier exhibits sufficient confidence in its prediction, the network exits early thereby, resulting in considerable savings in time. Although early exit classifiers have been proposed in previous work, these previous proposals focus on large networks, making their techniques suboptimal/impractical for tinyML applications. Our technique is optimized specifically for tiny-CNN sized models. In addition, we present a method to alleviate the effect of network overthinking by leveraging the representations learned by the early exit. We eva",
    "path": "papers/22/07/2207.06613.json",
    "total_tokens": 1177,
    "translated_title": "T-RECX：具有早期退出的小型资源有效卷积神经网络。",
    "translated_abstract": "由于机器学习和物联网技术的突破，将机器学习（ML）部署在毫瓦级边缘设备（tinyML）上正在变得越来越流行。大多数tinyML研究聚焦于模型压缩技术，为了适应KB级tiny-edge设备而以精度（和模型容量）为代价获得紧凑的模型。在本文中，我们展示了如何通过添加早期退出中间分类器来增强这样的模型。如果中间分类器对其预测具有足够的信心，那么网络将提前退出，从而节省大量的时间。虽然早期退出分类器在之前的研究中已经被提出，但这些方法都是针对大型网络，使它们的技术在tinyML应用中不太优化/实用。我们的技术是专门针对小型CNN模型进行优化的。此外，我们提出了一种通过利用早期退出所学到的表示来减轻网络过度思考的方法。我们在CIFAR-10，CIFAR-100和ImageNet数据集上评估了我们的方法，并将结果与最先进的技术进行了比较。我们的实验表明，具有早期退出功能的tiny-RECX模型可以在较小的模型和更快的推断时间内达到可比较的精度。",
    "tldr": "本文介绍了一种优化小型CNN模型的方法——T-RECX，通过添加早期退出中间分类器可以节省大量的时间和节省模型容量。本文的方法在CIFAR-10、CIFAR-100和ImageNet数据集上进行了实验，并与最先进的技术进行了比较，结果表明，具有早期退出功能的tiny-RECX模型可以在较小的模型和更快的推断时间内达到可比较的精度。",
    "en_tdlr": "This paper introduces a method to optimize small CNN models - T-RECX, which can save a lot of time and reduce model capacity by adding an early exit intermediate classifier. The method is specifically optimized for tiny-CNN sized models. The experiments were conducted on CIFAR-10, CIFAR-100, and ImageNet datasets, and compared with state-of-the-art techniques, the results show that the tiny-RECX model with early exit can achieve comparable accuracy with much smaller models and faster inference times."
}