{
    "title": "Majorization-minimization for Sparse Nonnegative Matrix Factorization with the $\\beta$-divergence. (arXiv:2207.06316v2 [cs.LG] UPDATED)",
    "abstract": "This article introduces new multiplicative updates for nonnegative matrix factorization with the $\\beta$-divergence and sparse regularization of one of the two factors (say, the activation matrix). It is well known that the norm of the other factor (the dictionary matrix) needs to be controlled in order to avoid an ill-posed formulation. Standard practice consists in constraining the columns of the dictionary to have unit norm, which leads to a nontrivial optimization problem. Our approach leverages a reparametrization of the original problem into the optimization of an equivalent scale-invariant objective function. From there, we derive block-descent majorization-minimization algorithms that result in simple multiplicative updates for either $\\ell_{1}$-regularization or the more \"aggressive\" log-regularization. In contrast with other state-of-the-art methods, our algorithms are universal in the sense that they can be applied to any $\\beta$-divergence (i.e., any value of $\\beta$) and t",
    "link": "http://arxiv.org/abs/2207.06316",
    "context": "Title: Majorization-minimization for Sparse Nonnegative Matrix Factorization with the $\\beta$-divergence. (arXiv:2207.06316v2 [cs.LG] UPDATED)\nAbstract: This article introduces new multiplicative updates for nonnegative matrix factorization with the $\\beta$-divergence and sparse regularization of one of the two factors (say, the activation matrix). It is well known that the norm of the other factor (the dictionary matrix) needs to be controlled in order to avoid an ill-posed formulation. Standard practice consists in constraining the columns of the dictionary to have unit norm, which leads to a nontrivial optimization problem. Our approach leverages a reparametrization of the original problem into the optimization of an equivalent scale-invariant objective function. From there, we derive block-descent majorization-minimization algorithms that result in simple multiplicative updates for either $\\ell_{1}$-regularization or the more \"aggressive\" log-regularization. In contrast with other state-of-the-art methods, our algorithms are universal in the sense that they can be applied to any $\\beta$-divergence (i.e., any value of $\\beta$) and t",
    "path": "papers/22/07/2207.06316.json",
    "total_tokens": 902,
    "translated_title": "带 $\\beta$-差异的稀疏非负矩阵分解的主导最小化算法",
    "translated_abstract": "本文提出了一种新的多元乘法更新方法，用于具有 $\\beta$-差异和两个因子中的一个（比如说，激活矩阵）稀疏正则化的非负矩阵分解。标准的做法是限制字典的列具有单位范数，从而控制另一个因子（字典矩阵）的范数，以避免病态问题。我们的方法将原问题重新参数化为等价的标度不变的目标函数的优化问题。然后，我们导出块下降主导最小化算法，这些算法对于 $\\ell_{1}$-正则化或更 \"激进\" 的对数正则化都可以产生简单的多元乘法更新。与其他最先进的方法相比，我们的算法在任何 $\\beta$-差异（即任何 $\\beta$ 的值）和其他稀疏约束上也具有通用性。",
    "tldr": "本文提出了一种带 $\\beta$-差异的稀疏非负矩阵分解的主导最小化算法，其能够适用于任何 $\\beta$-差异和其他稀疏约束。",
    "en_tdlr": "This article proposes a majorization-minimization algorithm for sparse nonnegative matrix factorization with the $\\beta$-divergence, which is applicable to any value of $\\beta$ and other sparsity constraints."
}