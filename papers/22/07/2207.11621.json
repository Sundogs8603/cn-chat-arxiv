{
    "title": "A Universal Trade-off Between the Model Size, Test Loss, and Training Loss of Linear Predictors. (arXiv:2207.11621v3 [stat.ML] UPDATED)",
    "abstract": "In this work we establish an algorithm and distribution independent non-asymptotic trade-off between the model size, excess test loss, and training loss of linear predictors. Specifically, we show that models that perform well on the test data (have low excess loss) are either \"classical\" -- have training loss close to the noise level, or are \"modern\" -- have a much larger number of parameters compared to the minimum needed to fit the training data exactly.  We also provide a more precise asymptotic analysis when the limiting spectral distribution of the whitened features is Marchenko-Pastur. Remarkably, while the Marchenko-Pastur analysis is far more precise near the interpolation peak, where the number of parameters is just enough to fit the training data, it coincides exactly with the distribution independent bound as the level of overparametrization increases.",
    "link": "http://arxiv.org/abs/2207.11621",
    "context": "Title: A Universal Trade-off Between the Model Size, Test Loss, and Training Loss of Linear Predictors. (arXiv:2207.11621v3 [stat.ML] UPDATED)\nAbstract: In this work we establish an algorithm and distribution independent non-asymptotic trade-off between the model size, excess test loss, and training loss of linear predictors. Specifically, we show that models that perform well on the test data (have low excess loss) are either \"classical\" -- have training loss close to the noise level, or are \"modern\" -- have a much larger number of parameters compared to the minimum needed to fit the training data exactly.  We also provide a more precise asymptotic analysis when the limiting spectral distribution of the whitened features is Marchenko-Pastur. Remarkably, while the Marchenko-Pastur analysis is far more precise near the interpolation peak, where the number of parameters is just enough to fit the training data, it coincides exactly with the distribution independent bound as the level of overparametrization increases.",
    "path": "papers/22/07/2207.11621.json",
    "total_tokens": 962,
    "translated_title": "线性预测器的模型大小、测试损失和训练损失之间的通用权衡",
    "translated_abstract": "本文建立了一个与算法和分布无关的非渐进性权衡方法，来衡量线性预测器的模型大小、超额测试损失和训练损失之间的关系。我们发现，在测试数据上表现出色的模型要么是“经典”的——其训练损失接近噪声水平，要么是“现代的”——其参数数量远远超过仅仅能精确拟合训练数据所需的最小参数数。同时，我们还提供了当白化特征的极限谱分布为Marchenko-Pastur时的更为精确的渐进分析。值得注意的是，在插值顶点附近，即参数数量刚好足以拟合训练数据时，Marchenko-Pastur分析更加精确，而随着过度参数化程度的增加，它恰好与无分布限制的理论上界相一致。",
    "tldr": "本文建立了一个与算法和分布无关的非渐进性权衡方法，来衡量线性预测器的模型大小、测试损失和训练损失之间的关系，发现测试数据上表现出色的模型要么是经典的，要么是现代的。同时提供了当白化特征的极限谱分布为Marchenko-Pastur时的更为精确的渐进分析，使得分析更加精确。",
    "en_tdlr": "This work establishes a non-asymptotic trade-off between the model size, excess test loss, and training loss of linear predictors, showing that models performing well on test data are either \"classical\" or \"modern\". They also provide a more precise asymptotic analysis when the spectral distribution of the whitened features is Marchenko-Pastur, which coincides with the distribution-independent bound."
}