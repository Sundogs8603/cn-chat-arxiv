{
    "title": "Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization. (arXiv:2207.02016v3 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning (RL) is recognized as lacking generalization and robustness under environmental perturbations, which excessively restricts its application for real-world robotics. Prior work claimed that adding regularization to the value function is equivalent to learning a robust policy with uncertain transitions. Although the regularization-robustness transformation is appealing for its simplicity and efficiency, it is still lacking in continuous control tasks. In this paper, we propose a new regularizer named $\\textbf{U}$ncertainty $\\textbf{S}$et $\\textbf{R}$egularizer (USR), by formulating the uncertainty set on the parameter space of the transition function. In particular, USR is flexible enough to be plugged into any existing RL framework. To deal with unknown uncertainty sets, we further propose a novel adversarial approach to generate them based on the value function. We evaluate USR on the Real-world Reinforcement Learning (RWRL) benchmark, demonstrating improvements i",
    "link": "http://arxiv.org/abs/2207.02016",
    "context": "Title: Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization. (arXiv:2207.02016v3 [cs.LG] UPDATED)\nAbstract: Reinforcement learning (RL) is recognized as lacking generalization and robustness under environmental perturbations, which excessively restricts its application for real-world robotics. Prior work claimed that adding regularization to the value function is equivalent to learning a robust policy with uncertain transitions. Although the regularization-robustness transformation is appealing for its simplicity and efficiency, it is still lacking in continuous control tasks. In this paper, we propose a new regularizer named $\\textbf{U}$ncertainty $\\textbf{S}$et $\\textbf{R}$egularizer (USR), by formulating the uncertainty set on the parameter space of the transition function. In particular, USR is flexible enough to be plugged into any existing RL framework. To deal with unknown uncertainty sets, we further propose a novel adversarial approach to generate them based on the value function. We evaluate USR on the Real-world Reinforcement Learning (RWRL) benchmark, demonstrating improvements i",
    "path": "papers/22/07/2207.02016.json",
    "total_tokens": 972,
    "translated_title": "在具有不确定性集合正则化的连续控制任务中的强化学习",
    "translated_abstract": "强化学习（RL）被认为在环境扰动下缺乏泛化性和鲁棒性，这严重限制了其在实际机器人领域的应用。以前的研究声称，在值函数中添加正则化等价于学习具有不确定转换的鲁棒策略。尽管正则化-鲁棒性转换因其简单和高效而具有吸引力，但在连续控制任务中仍然存在不足。在本文中，我们提出了一种新的正则化器，名为不确定性集合正则化器（USR），通过在转换函数的参数空间上构建不确定性集合来实现。特别是，USR足够灵活，可以插入到任何现有的RL框架中。为了处理未知的不确定性集合，我们进一步提出了一种基于值函数生成的新颖对抗方法来生成它们。我们在真实世界强化学习（RWRL）基准测试上评估了USR，展示了改进的结果。",
    "tldr": "该论文提出了一种新的正则化器USR，通过构建转换函数参数空间上的不确定性集合来提高连续控制任务中的强化学习性能。通过对值函数进行对抗生成未知不确定性集合，进一步增强了USR的灵活性。在真实世界强化学习基准测试中得到了改进的结果。",
    "en_tdlr": "This paper proposes a new regularizer called USR, which improves reinforcement learning performance in continuous control tasks by constructing uncertainty sets on the parameter space of the transition function. The flexibility of USR is enhanced by generating unknown uncertainty sets through adversarial generation based on the value function. Improved results are demonstrated on the Real-world Reinforcement Learning benchmark."
}