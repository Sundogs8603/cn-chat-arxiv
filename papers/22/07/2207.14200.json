{
    "title": "CrAM: A Compression-Aware Minimizer. (arXiv:2207.14200v4 [cs.LG] UPDATED)",
    "abstract": "Deep neural networks (DNNs) often have to be compressed, via pruning and/or quantization, before they can be deployed in practical settings. In this work we propose a new compression-aware minimizer dubbed CrAM that modifies the optimization step in a principled way, in order to produce models whose local loss behavior is stable under compression operations such as pruning. Thus, dense models trained via CrAM should be compressible post-training, in a single step, without significant accuracy loss. Experimental results on standard benchmarks, such as residual networks for ImageNet classification and BERT models for language modelling, show that CrAM produces dense models that can be more accurate than the standard SGD/Adam-based baselines, but which are stable under weight pruning: specifically, we can prune models in one-shot to 70-80% sparsity with almost no accuracy loss, and to 90% with reasonable ($\\sim 1\\%$) accuracy loss, which is competitive with gradual compression methods. Ad",
    "link": "http://arxiv.org/abs/2207.14200",
    "context": "Title: CrAM: A Compression-Aware Minimizer. (arXiv:2207.14200v4 [cs.LG] UPDATED)\nAbstract: Deep neural networks (DNNs) often have to be compressed, via pruning and/or quantization, before they can be deployed in practical settings. In this work we propose a new compression-aware minimizer dubbed CrAM that modifies the optimization step in a principled way, in order to produce models whose local loss behavior is stable under compression operations such as pruning. Thus, dense models trained via CrAM should be compressible post-training, in a single step, without significant accuracy loss. Experimental results on standard benchmarks, such as residual networks for ImageNet classification and BERT models for language modelling, show that CrAM produces dense models that can be more accurate than the standard SGD/Adam-based baselines, but which are stable under weight pruning: specifically, we can prune models in one-shot to 70-80% sparsity with almost no accuracy loss, and to 90% with reasonable ($\\sim 1\\%$) accuracy loss, which is competitive with gradual compression methods. Ad",
    "path": "papers/22/07/2207.14200.json",
    "total_tokens": 945,
    "translated_title": "CrAM:一种压缩感知的优化器",
    "translated_abstract": "在实际应用中，深度神经网络通常需要通过剪枝和/或量化进行压缩。本文提出了一种新的压缩感知的优化器CrAM，通过一种原则性的方式改变优化步骤，产生出的模型在压缩操作（如剪枝）下具有稳定的局部损失行为。因此，通过CrAM训练的密集模型应该在训练后能够通过单次压缩而不会带来显著的精度损失。在一些标准基准测试中的实验结果，如ImageNet分类的残差网络和用于语言建模的BERT模型，都表明CrAM产生的密集模型可以比标准的SGD / Adam基线更加准确，但在权重剪枝操作下稳定，即可以在一次剪枝到70-80％稀疏度并且几乎没有精度损失，到90％时具有合理（约1％）的精度损失，这与渐进式压缩方法相当竞争力。",
    "tldr": "提出一种新的压缩感知的优化器CrAM，其产生的密集模型可以在训练后通过单次压缩而不会带来显著的精度损失，该优化器在一些标准基准测试中的实验结果显示出竞争力。",
    "en_tdlr": "The paper proposes a new compression-aware optimizer, CrAM, which produces dense models that can be compressed post-training in a single step without significant accuracy loss. Experimental results show that CrAM outperforms standard SGD/Adam-based baselines on standard benchmark tests and is competitive with gradual compression methods."
}