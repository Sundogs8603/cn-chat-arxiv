{
    "title": "Stochastic Gradient Descent and Anomaly of Variance-flatness Relation in Artificial Neural Networks. (arXiv:2207.04932v2 [nlin.AO] UPDATED)",
    "abstract": "Stochastic gradient descent (SGD), a widely used algorithm in deep-learning neural networks has attracted continuing studies for the theoretical principles behind its success. A recent work reports an anomaly (inverse) relation between the variance of neural weights and the landscape flatness of the loss function driven under SGD [Feng & Tu, PNAS 118, 0027 (2021)]. To investigate this seemingly violation of statistical physics principle, the properties of SGD near fixed points are analysed via a dynamic decomposition method. Our approach recovers the true \"energy\" function under which the universal Boltzmann distribution holds. It differs from the cost function in general and resolves the paradox raised by the the anomaly. The study bridges the gap between the classical statistical mechanics and the emerging discipline of artificial intelligence, with potential for better algorithms to the latter.",
    "link": "http://arxiv.org/abs/2207.04932",
    "context": "Title: Stochastic Gradient Descent and Anomaly of Variance-flatness Relation in Artificial Neural Networks. (arXiv:2207.04932v2 [nlin.AO] UPDATED)\nAbstract: Stochastic gradient descent (SGD), a widely used algorithm in deep-learning neural networks has attracted continuing studies for the theoretical principles behind its success. A recent work reports an anomaly (inverse) relation between the variance of neural weights and the landscape flatness of the loss function driven under SGD [Feng & Tu, PNAS 118, 0027 (2021)]. To investigate this seemingly violation of statistical physics principle, the properties of SGD near fixed points are analysed via a dynamic decomposition method. Our approach recovers the true \"energy\" function under which the universal Boltzmann distribution holds. It differs from the cost function in general and resolves the paradox raised by the the anomaly. The study bridges the gap between the classical statistical mechanics and the emerging discipline of artificial intelligence, with potential for better algorithms to the latter.",
    "path": "papers/22/07/2207.04932.json",
    "total_tokens": 898,
    "translated_title": "人工神经网络中的随机梯度下降和方差 - 平坦关系异常",
    "translated_abstract": "随机梯度下降（SGD）是深度学习神经网络中广泛使用的算法，其成功的理论原则一直受到持续的研究。最近的一项工作报告了神经权重的方差和在 SGD 下驱动的损失函数的平坦度之间的反常（反）关系[Feng＆Tu，PNAS 118，0027（2021）]。为了调查这种似乎违反统计物理学原则的现象，通过动态分解方法分析了 SGD 在固定点附近的属性。我们的方法恢复了真正的“能量”函数，下面是普遍的玻尔兹曼分布。它与一般的成本函数不同，并解决了这个反常所引发的悖论。本研究是经典统计力学和新兴的人工智能学科之间的桥梁，有潜力为后者提供更好的算法。",
    "tldr": "本研究通过动态分解方法分析了 SGD 在固定点附近的属性，恢复了真正的“能量”函数，解决了权重方差和损失函数平坦度反常关系的悖论，为人工智能学科提供更好的算法。",
    "en_tdlr": "This study analyzes the properties of SGD near fixed points via a dynamic decomposition method, recovers the true \"energy\" function, and resolves the paradox regarding the anomaly between the variance of neural weights and the landscape flatness of the loss function driven under SGD, potentially providing better algorithms for the discipline of artificial intelligence."
}