{
    "title": "ProMix: Combating Label Noise via Maximizing Clean Sample Utility. (arXiv:2207.10276v3 [cs.LG] UPDATED)",
    "abstract": "Learning with Noisy Labels (LNL) has become an appealing topic, as imperfectly annotated data are relatively cheaper to obtain. Recent state-of-the-art approaches employ specific selection mechanisms to separate clean and noisy samples and then apply Semi-Supervised Learning (SSL) techniques for improved performance. However, the selection step mostly provides a medium-sized and decent-enough clean subset, which overlooks a rich set of clean samples. To fulfill this, we propose a novel LNL framework ProMix that attempts to maximize the utility of clean samples for boosted performance. Key to our method, we propose a matched high confidence selection technique that selects those examples with high confidence scores and matched predictions with given labels to dynamically expand a base clean sample set. To overcome the potential side effect of excessive clean set selection procedure, we further devise a novel SSL framework that is able to train balanced and unbiased classifiers on the se",
    "link": "http://arxiv.org/abs/2207.10276",
    "context": "Title: ProMix: Combating Label Noise via Maximizing Clean Sample Utility. (arXiv:2207.10276v3 [cs.LG] UPDATED)\nAbstract: Learning with Noisy Labels (LNL) has become an appealing topic, as imperfectly annotated data are relatively cheaper to obtain. Recent state-of-the-art approaches employ specific selection mechanisms to separate clean and noisy samples and then apply Semi-Supervised Learning (SSL) techniques for improved performance. However, the selection step mostly provides a medium-sized and decent-enough clean subset, which overlooks a rich set of clean samples. To fulfill this, we propose a novel LNL framework ProMix that attempts to maximize the utility of clean samples for boosted performance. Key to our method, we propose a matched high confidence selection technique that selects those examples with high confidence scores and matched predictions with given labels to dynamically expand a base clean sample set. To overcome the potential side effect of excessive clean set selection procedure, we further devise a novel SSL framework that is able to train balanced and unbiased classifiers on the se",
    "path": "papers/22/07/2207.10276.json",
    "total_tokens": 964,
    "translated_title": "ProMix：通过最大化干净样本效用来对抗标签噪声",
    "translated_abstract": "学习具有噪声标签（LNL）已成为一个吸引人的话题，因为带有不完整标注的数据相对较便宜易得。最近的最先进方法采用特定的选择机制来区分干净样本和噪声样本，然后应用半监督学习（SSL）技术以提高性能。然而，选择步骤主要提供一个中等大小和足够好的清洁子集，忽视了丰富的干净样本集。为了实现这一点，我们提出了一种新颖的LNL框架ProMix，试图通过最大化干净样本的效用来提高性能。我们方法的关键在于提出了一种匹配高置信度选择技术，该技术选择那些具有高置信度得分和与给定标签匹配预测的示例，以动态扩展基础干净样本集。为了克服过度选择清洁样本集程序的潜在副作用，我们进一步设计了一种新颖的SSL框架，能够在训练时得到平衡和无偏的分类器。",
    "tldr": "论文提出了一种名为ProMix的新颖LNL框架，通过最大化干净样本的效用来对抗标签噪声。采用一种匹配高置信度选择技术来动态扩展基础干净样本集，并设计了一种平衡和无偏的SSL框架来提高性能。",
    "en_tdlr": "This paper proposes a novel LNL framework called ProMix, which combats label noise by maximizing the utility of clean samples. It uses a matched high confidence selection technique to dynamically expand the clean sample set and designs a balanced and unbiased SSL framework for improved performance."
}