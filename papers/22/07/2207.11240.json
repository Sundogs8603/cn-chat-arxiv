{
    "title": "Discrete Key-Value Bottleneck. (arXiv:2207.11240v3 [cs.LG] UPDATED)",
    "abstract": "Deep neural networks perform well on classification tasks where data streams are i.i.d. and labeled data is abundant. Challenges emerge with non-stationary training data streams such as continual learning. One powerful approach that has addressed this challenge involves pre-training of large encoders on volumes of readily available data, followed by task-specific tuning. Given a new task, however, updating the weights of these encoders is challenging as a large number of weights needs to be fine-tuned, and as a result, they forget information about the previous tasks. In the present work, we propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable key-value codes. Our paradigm will be to encode; process the representation via a discrete bottleneck; and decode. Here, the input is fed to the pre-trained encoder, the output of the encoder is used to select the nearest keys, and the corresponding values are fed to th",
    "link": "http://arxiv.org/abs/2207.11240",
    "context": "Title: Discrete Key-Value Bottleneck. (arXiv:2207.11240v3 [cs.LG] UPDATED)\nAbstract: Deep neural networks perform well on classification tasks where data streams are i.i.d. and labeled data is abundant. Challenges emerge with non-stationary training data streams such as continual learning. One powerful approach that has addressed this challenge involves pre-training of large encoders on volumes of readily available data, followed by task-specific tuning. Given a new task, however, updating the weights of these encoders is challenging as a large number of weights needs to be fine-tuned, and as a result, they forget information about the previous tasks. In the present work, we propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable key-value codes. Our paradigm will be to encode; process the representation via a discrete bottleneck; and decode. Here, the input is fed to the pre-trained encoder, the output of the encoder is used to select the nearest keys, and the corresponding values are fed to th",
    "path": "papers/22/07/2207.11240.json",
    "total_tokens": 812,
    "translated_title": "离散键值瓶颈",
    "translated_abstract": "深度神经网络在i.i.d.数据流和标注数据丰富的分类任务中表现良好，但对于连续学习等非平稳训练数据流会出现挑战。目前已有的一个有效方法是在大量可用数据上对编码器进行预训练，然后进行特定任务的微调。然而，对于新任务，更新这些编码器的权重是具有挑战性的，因为需要微调大量的权重，并且会忘记先前任务的信息。我们提出了一个模型架构来解决这个问题，建立在包含成对分离可学习键值代码的离散瓶颈的基础上。我们的范式是进行编码、通过离散瓶颈进行表示处理、解码。在这里，输入被馈送到预训练编码器，编码器的输出用于选择最近的键，并将相应的值馈送到正在执行的任务。",
    "tldr": "本文提出了一个新的神经网络模型结构，包含离散瓶颈，可以有效处理在多个任务之间进行连续学习的问题。",
    "en_tdlr": "This paper proposes a new neural network model architecture with a discrete bottleneck, which can effectively handle the problem of continuous learning between multiple tasks."
}