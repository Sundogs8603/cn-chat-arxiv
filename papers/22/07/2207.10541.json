{
    "title": "Unveiling the Latent Space Geometry of Push-Forward Generative Models. (arXiv:2207.10541v3 [cs.LG] UPDATED)",
    "abstract": "Many deep generative models are defined as a push-forward of a Gaussian measure by a continuous generator, such as Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs). This work explores the latent space of such deep generative models. A key issue with these models is their tendency to output samples outside of the support of the target distribution when learning disconnected distributions. We investigate the relationship between the performance of these models and the geometry of their latent space. Building on recent developments in geometric measure theory, we prove a sufficient condition for optimality in the case where the dimension of the latent space is larger than the number of modes. Through experiments on GANs, we demonstrate the validity of our theoretical results and gain new insights into the latent space geometry of these models. Additionally, we propose a truncation method that enforces a simplicial cluster structure in the latent space and improve",
    "link": "http://arxiv.org/abs/2207.10541",
    "context": "Title: Unveiling the Latent Space Geometry of Push-Forward Generative Models. (arXiv:2207.10541v3 [cs.LG] UPDATED)\nAbstract: Many deep generative models are defined as a push-forward of a Gaussian measure by a continuous generator, such as Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs). This work explores the latent space of such deep generative models. A key issue with these models is their tendency to output samples outside of the support of the target distribution when learning disconnected distributions. We investigate the relationship between the performance of these models and the geometry of their latent space. Building on recent developments in geometric measure theory, we prove a sufficient condition for optimality in the case where the dimension of the latent space is larger than the number of modes. Through experiments on GANs, we demonstrate the validity of our theoretical results and gain new insights into the latent space geometry of these models. Additionally, we propose a truncation method that enforces a simplicial cluster structure in the latent space and improve",
    "path": "papers/22/07/2207.10541.json",
    "total_tokens": 934,
    "translated_title": "揭示推进生成模型的潜在空间几何结构",
    "translated_abstract": "许多深度生成模型都是通过连续生成器推进高斯测量而定义的，例如生成对抗网络（GAN）或变分自动编码器（VAE）。本文探究了这些深度生成模型的潜在空间。这些模型的一个关键问题是，在学习不连通分布时，它们往往输出超出目标分布支持范围的样本。我们研究了这些模型的性能与它们的潜在空间几何之间的关系。借助几何测量理论的最新发展，我们在潜在空间的维度大于模的数量的情况下证明了优化的充分条件。通过对GAN进行实验，我们证明了我们理论结果的可靠性，并对这些模型的潜在空间几何结构获得了新的见解。此外，我们提出了一种截断方法，可以在潜在空间中强制实行一个简单的簇结构，并提高了模型的性能。",
    "tldr": "本文研究了深度生成模型的潜在空间及其与模型性能之间的关系。借助几何测量理论，我们发现了优化的充分条件。我们提出了一种截断方法，可以在潜在空间中强制实行一个简单的簇结构，并提高了模型的性能。",
    "en_tdlr": "This paper studies the latent space of deep generative models and its relationship with model performance. With the help of geometric measure theory, the authors find a sufficient condition for optimization. They propose a truncation method that enforces a simplicial cluster structure in the latent space and improves model performance."
}