{
    "title": "SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval. (arXiv:2207.02578v2 [cs.IR] UPDATED)",
    "abstract": "In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA, to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SimLM only requires access to unlabeled corpus, and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets, and show substantial improvements over strong baselines under various settings. Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 which incurs significantly more storage cost. Our code and model check points are available at https://github.com/microsoft/unilm/tree/mast",
    "link": "http://arxiv.org/abs/2207.02578",
    "context": "Title: SimLM: Pre-training with Representation Bottleneck for Dense Passage Retrieval. (arXiv:2207.02578v2 [cs.IR] UPDATED)\nAbstract: In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. We use a replaced language modeling objective, which is inspired by ELECTRA, to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. SimLM only requires access to unlabeled corpus, and is more broadly applicable when there are no labeled data or queries. We conduct experiments on several large-scale passage retrieval datasets, and show substantial improvements over strong baselines under various settings. Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 which incurs significantly more storage cost. Our code and model check points are available at https://github.com/microsoft/unilm/tree/mast",
    "path": "papers/22/07/2207.02578.json",
    "total_tokens": 970,
    "translated_title": "SimLM：表示瓶颈预训练用于密集文本检索",
    "translated_abstract": "本文中，我们提出了一种名为SimLM（预训练模型中的相似性匹配）的简单而有效的密集文本检索预训练方法。它采用了一个简单的瓶颈架构，通过自监督预训练将文本信息压缩为一个密集向量。我们使用了一种替代语言建模目标，灵感来自ELECTRA，以改善样本效率并减少预训练和微调之间输入分布的偏差。SimLM只需要对未标记的语料库进行访问，并且在没有标记数据或查询的情况下更为广泛适用。我们在几个大规模排序任务数据集上进行了实验，并在各种设置下显示了比强基线更大的改进。值得注意的是，SimLM甚至在需要更多存储成本的多向量方法（如ColBERTv2）的情况下也能取得更好的性能。我们的代码和模型检查点可在https://github.com/microsoft/unilm/tree/mast中找到。",
    "tldr": "本文提出了一种名为SimLM的密集文本检索预训练方法，采用瓶颈架构和替代语言建模目标。在无标记的情况下，它也是适用的。相比于强基线模型，SimLM在多个大规模排序任务数据集上显示出了更好的性能，甚至超过了需要更多存储成本的多向量方法ColBERTv2。"
}