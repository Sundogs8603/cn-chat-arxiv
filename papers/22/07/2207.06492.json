{
    "title": "Approximate Nash Equilibrium Learning for n-Player Markov Games in Dynamic Pricing",
    "abstract": "arXiv:2207.06492v3 Announce Type: replace-cross  Abstract: We investigate Nash equilibrium learning in a competitive Markov Game (MG) environment, where multiple agents compete, and multiple Nash equilibria can exist. In particular, for an oligopolistic dynamic pricing environment, exact Nash equilibria are difficult to obtain due to the curse-of-dimensionality. We develop a new model-free method to find approximate Nash equilibria. Gradient-free black box optimization is then applied to estimate $\\epsilon$, the maximum reward advantage of an agent unilaterally deviating from any joint policy, and to also estimate the $\\epsilon$-minimizing policy for any given state. The policy-$\\epsilon$ correspondence and the state to $\\epsilon$-minimizing policy are represented by neural networks, the latter being the Nash Policy Net. During batch update, we perform Nash Q learning on the system, by adjusting the action probabilities using the Nash Policy Net. We demonstrate that an approximate Nash",
    "link": "https://arxiv.org/abs/2207.06492",
    "context": "Title: Approximate Nash Equilibrium Learning for n-Player Markov Games in Dynamic Pricing\nAbstract: arXiv:2207.06492v3 Announce Type: replace-cross  Abstract: We investigate Nash equilibrium learning in a competitive Markov Game (MG) environment, where multiple agents compete, and multiple Nash equilibria can exist. In particular, for an oligopolistic dynamic pricing environment, exact Nash equilibria are difficult to obtain due to the curse-of-dimensionality. We develop a new model-free method to find approximate Nash equilibria. Gradient-free black box optimization is then applied to estimate $\\epsilon$, the maximum reward advantage of an agent unilaterally deviating from any joint policy, and to also estimate the $\\epsilon$-minimizing policy for any given state. The policy-$\\epsilon$ correspondence and the state to $\\epsilon$-minimizing policy are represented by neural networks, the latter being the Nash Policy Net. During batch update, we perform Nash Q learning on the system, by adjusting the action probabilities using the Nash Policy Net. We demonstrate that an approximate Nash",
    "path": "papers/22/07/2207.06492.json",
    "total_tokens": 896,
    "translated_title": "动态定价中 n 人马尔可夫博弈的近似纳什均衡学习",
    "translated_abstract": "我们研究了竞争性马尔可夫博弈环境中的纳什均衡学习，其中多个代理竞争，可能存在多个纳什均衡。特别是，对于寡头动态定价环境，由于维度灵活性的问题，精确的纳什均衡难以获得。我们开发了一种新的无模型方法来寻找近似的纳什均衡。然后，我们应用无梯度黑盒优化来估计$\\epsilon$，即代理单方面偏离任何联合策略的最大奖励优势，并估计任何给定状态的$\\epsilon$-最小化策略。政策-$\\epsilon$对应和状态到$\\epsilon$-最小化策略由神经网络表示，后者为纳什策略网络。在批量更新期间，我们通过调整动作概率使用纳什策略网络来执行纳什 Q 学习。我们演示了近似纳什",
    "tldr": "该论文提出了一种在动态定价环境中寻找近似纳什均衡的新方法，利用神经网络表示策略和最小化状态，进行纳什 Q 学习。",
    "en_tdlr": "This paper introduces a new method for finding approximate Nash equilibria in a dynamic pricing environment, utilizing neural networks to represent policies and minimize states for Nash Q learning."
}