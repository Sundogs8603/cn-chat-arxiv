{
    "title": "Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy. (arXiv:2207.12141v3 [cs.LG] UPDATED)",
    "abstract": "Model-based reinforcement learning (RL) often achieves higher sample efficiency in practice than model-free RL by learning a dynamics model to generate samples for policy learning. Previous works learn a dynamics model that fits under the empirical state-action visitation distribution for all historical policies, i.e., the sample replay buffer. However, in this paper, we observe that fitting the dynamics model under the distribution for \\emph{all historical policies} does not necessarily benefit model prediction for the \\emph{current policy} since the policy in use is constantly evolving over time. The evolving policy during training will cause state-action visitation distribution shifts. We theoretically analyze how this distribution shift over historical policies affects the model learning and model rollouts. We then propose a novel dynamics model learning method, named \\textit{Policy-adapted Dynamics Model Learning (PDML)}. PDML dynamically adjusts the historical policy mixture dist",
    "link": "http://arxiv.org/abs/2207.12141",
    "context": "Title: Live in the Moment: Learning Dynamics Model Adapted to Evolving Policy. (arXiv:2207.12141v3 [cs.LG] UPDATED)\nAbstract: Model-based reinforcement learning (RL) often achieves higher sample efficiency in practice than model-free RL by learning a dynamics model to generate samples for policy learning. Previous works learn a dynamics model that fits under the empirical state-action visitation distribution for all historical policies, i.e., the sample replay buffer. However, in this paper, we observe that fitting the dynamics model under the distribution for \\emph{all historical policies} does not necessarily benefit model prediction for the \\emph{current policy} since the policy in use is constantly evolving over time. The evolving policy during training will cause state-action visitation distribution shifts. We theoretically analyze how this distribution shift over historical policies affects the model learning and model rollouts. We then propose a novel dynamics model learning method, named \\textit{Policy-adapted Dynamics Model Learning (PDML)}. PDML dynamically adjusts the historical policy mixture dist",
    "path": "papers/22/07/2207.12141.json",
    "total_tokens": 1053,
    "translated_title": "活在当下：适应性进化策略的学习动力学模型",
    "translated_abstract": "基于模型的强化学习通常比模型无关的强化学习在实践中具有更高的样本效率，因为它学习一个动力学模型来生成策略学习的样本。以前的研究学习使动力学模型适应所有历史策略下的经验状态-动作访问分布，即样本回放缓冲区。然而，在本文中，我们观察到，在“所有历史策略”下使动力学模型适应分布不一定有益于模型预测“当前策略”，因为正在使用的策略在时间上不断演变。训练过程中的进化策略会导致状态-动作访问分布的转移。我们在理论上分析了这种历史策略分布的转移如何影响模型学习和模型回滚。因此，我们提出了一种新颖的动力学模型学习方法，称为策略适应动力学模型学习(PDML)。PDML动态调整用于动力学模型学习的历史策略混合分布，以适应训练过程中的进化策略。实验结果表明，PDML优于最先进的方法，实现了更快的收敛和更好的最终性能。",
    "tldr": "本文提出了一种名为PDML的学习动力学模型的方法，该方法动态调整历史策略混合分布以适应训练过程中的进化策略，实验结果表明PDML优于最先进的方法，实现了更快的收敛和更好的最终性能。",
    "en_tdlr": "This paper proposes a method named PDML for learning dynamics model, which dynamically adjusts historical policy mixture distribution to adapt to evolving policy during training, and experimental results show that PDML outperforms state-of-the-art methods and achieves faster convergence and better final performance."
}