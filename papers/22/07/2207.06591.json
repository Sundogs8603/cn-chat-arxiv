{
    "title": "A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America. (arXiv:2207.06591v3 [cs.CL] UPDATED)",
    "abstract": "Automated decision-making systems, especially those based on natural language processing, are pervasive in our lives. They are not only behind the internet search engines we use daily, but also take more critical roles: selecting candidates for a job, determining suspects of a crime, diagnosing autism and more. Such automated systems make errors, which may be harmful in many ways, be it because of the severity of the consequences (as in health issues) or because of the sheer number of people they affect. When errors made by an automated system affect a population more than others, we call the system \\textit{biased}.  Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to ",
    "link": "http://arxiv.org/abs/2207.06591",
    "context": "Title: A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America. (arXiv:2207.06591v3 [cs.CL] UPDATED)\nAbstract: Automated decision-making systems, especially those based on natural language processing, are pervasive in our lives. They are not only behind the internet search engines we use daily, but also take more critical roles: selecting candidates for a job, determining suspects of a crime, diagnosing autism and more. Such automated systems make errors, which may be harmful in many ways, be it because of the severity of the consequences (as in health issues) or because of the sheer number of people they affect. When errors made by an automated system affect a population more than others, we call the system \\textit{biased}.  Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to ",
    "path": "papers/22/07/2207.06591.json",
    "total_tokens": 1458,
    "translated_title": "在拉丁美洲的自然语言处理中表现出的偏见和有害刻板印象的特征化方法",
    "translated_abstract": "自动化决策系统，尤其是基于自然语言处理的系统在我们的生活中无处不在。它们不仅是我们每天使用的互联网搜索引擎的背后，还有更为关键的作用：筛选工作候选人，确定犯罪嫌疑人，诊断自闭症等。这些自动化系统会出现错误，这些错误可能在很多方面都是有害的，无论是因为后果的严重性（例如健康问题）还是因为所涉及的人数之多。当由自动化系统造成的错误对某个群体的影响超过其他群体时，我们称此系统存在偏见。大多数现代自然语言技术是基于使用机器学习从大量文本中获得的分析结果，即语言模型和单词嵌入。由于它们是通过应用子符号机器学习创建的，主要是人工神经网络，所以它们是不透明的，且无法通过直接检查来解释，因此很难理解这些系统何时存在偏见或何时放大有害的刻板印象。在本文中，我们提出了一种方法，用于自动检测和表征自然语言处理系统中的偏见和有害刻板印象，应用于拉丁美洲语言。我们的方法基于分析来自不同地区的单词嵌入之间的相似性，并分析用于训练自然语言处理模型的数据源。我们在一个针对西班牙语、葡萄牙语和克丘亚语的有害刻板印象检测案例研究中测试了我们的方法。我们的研究结果表明，不同的自然语言处理系统以不同甚至意想不到的方式存在偏见和放大有害的刻板印象，这凸显了测试和改善此类系统质量的重要性。",
    "tldr": "本研究提出了一种自动检测和表征拉丁美洲自然语言处理系统中偏见和有害刻板印象的方法。该方法基于单词嵌入之间的相似性和数据源的分析，并在西班牙语、葡萄牙语和克丘亚语的有害刻板印象检测案例中进行测试。结果表明，不同的自然语言处理系统以不同甚至意想不到的方式存在偏见和放大有害的刻板印象。",
    "en_tdlr": "This paper proposes a methodology for automatically detecting and characterizing bias and harmful stereotypes in natural language processing systems applied to Latin American languages. The methodology is based on analyzing the similarity among word embeddings obtained from different regions and on the analysis of the data sources used to train natural language processing models. The study tests the method in a toxicity detection case study in Spanish, Portuguese, and Quechua, showing that different natural language processing systems present bias and amplify harmful stereotypes in different and unexpected ways."
}