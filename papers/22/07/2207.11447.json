{
    "title": "Handling Data Heterogeneity in Federated Learning via Knowledge Distillation and Fusion. (arXiv:2207.11447v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FL) supports distributed training of a global machine learning model across multiple devices with the help of a central server. However, data heterogeneity across different devices leads to the client model drift issue and results in model performance degradation and poor model fairness. To address the issue, we design Federated learning with global-local Knowledge Fusion (FedKF) scheme in this paper. The key idea in FedKF is to let the server return the global knowledge to be fused with the local knowledge in each training round so that the local model can be regularized towards the global optima. Therefore, the client model drift issue can be mitigated. In FedKF, we first propose the active-inactive model aggregation technique that supports a precise global knowledge representation. Then, we propose a data-free knowledge distillation (KD) approach to enable each client model to learn the global knowledge (embedded in the global model) while each client model can s",
    "link": "http://arxiv.org/abs/2207.11447",
    "context": "Title: Handling Data Heterogeneity in Federated Learning via Knowledge Distillation and Fusion. (arXiv:2207.11447v2 [cs.LG] UPDATED)\nAbstract: Federated learning (FL) supports distributed training of a global machine learning model across multiple devices with the help of a central server. However, data heterogeneity across different devices leads to the client model drift issue and results in model performance degradation and poor model fairness. To address the issue, we design Federated learning with global-local Knowledge Fusion (FedKF) scheme in this paper. The key idea in FedKF is to let the server return the global knowledge to be fused with the local knowledge in each training round so that the local model can be regularized towards the global optima. Therefore, the client model drift issue can be mitigated. In FedKF, we first propose the active-inactive model aggregation technique that supports a precise global knowledge representation. Then, we propose a data-free knowledge distillation (KD) approach to enable each client model to learn the global knowledge (embedded in the global model) while each client model can s",
    "path": "papers/22/07/2207.11447.json",
    "total_tokens": 898,
    "translated_title": "通过知识蒸馏和融合处理联邦学习中的数据异质性",
    "translated_abstract": "联邦学习支持在多个设备上通过中央服务器进行全局机器学习模型的分布式训练。然而，不同设备间的数据异质性导致了客户端模型漂移问题，导致模型性能下降和模型公平性差。为了解决这个问题，我们在本文中设计了一种名为FedKF的联邦学习全局-局部知识融合方案。FedKF的核心思想是在每轮训练中让服务器返回全局知识，以与每个设备本地知识融合，从而使本地模型能够向全局最优进行正则化，从而缓解客户端模型漂移问题。在FedKF中，我们首先提出了支持精确全局知识表示的主-辅模型聚合技术。然后，我们提出了一种无数据知识蒸馏方法，使得每个客户端模型可以学习全局知识（嵌入在全局模型中）而不需要数据。",
    "tldr": "本文提出了一种名为FedKF的方案，通过知识融合和蒸馏解决了联邦学习中数据异质性导致的客户端模型漂移问题，提升了模型性能和公平性。",
    "en_tdlr": "This paper proposes a scheme called FedKF to address the issue of client model drift caused by data heterogeneity in federated learning, through knowledge fusion and distillation, improving model performance and fairness."
}