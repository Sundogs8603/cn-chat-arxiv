{
    "title": "Riemannian Stochastic Gradient Method for Nested Composition Optimization",
    "abstract": "arXiv:2207.09350v2 Announce Type: replace-cross  Abstract: This work considers optimization of composition of functions in a nested form over Riemannian manifolds where each function contains an expectation. This type of problems is gaining popularity in applications such as policy evaluation in reinforcement learning or model customization in meta-learning. The standard Riemannian stochastic gradient methods for non-compositional optimization cannot be directly applied as stochastic approximation of inner functions create bias in the gradients of the outer functions. For two-level composition optimization, we present a Riemannian Stochastic Composition Gradient Descent (R-SCGD) method that finds an approximate stationary point, with expected squared Riemannian gradient smaller than $\\epsilon$, in $O(\\epsilon^{-2})$ calls to the stochastic gradient oracle of the outer function and stochastic function and gradient oracles of the inner function. Furthermore, we generalize the R-SCGD algo",
    "link": "https://arxiv.org/abs/2207.09350",
    "context": "Title: Riemannian Stochastic Gradient Method for Nested Composition Optimization\nAbstract: arXiv:2207.09350v2 Announce Type: replace-cross  Abstract: This work considers optimization of composition of functions in a nested form over Riemannian manifolds where each function contains an expectation. This type of problems is gaining popularity in applications such as policy evaluation in reinforcement learning or model customization in meta-learning. The standard Riemannian stochastic gradient methods for non-compositional optimization cannot be directly applied as stochastic approximation of inner functions create bias in the gradients of the outer functions. For two-level composition optimization, we present a Riemannian Stochastic Composition Gradient Descent (R-SCGD) method that finds an approximate stationary point, with expected squared Riemannian gradient smaller than $\\epsilon$, in $O(\\epsilon^{-2})$ calls to the stochastic gradient oracle of the outer function and stochastic function and gradient oracles of the inner function. Furthermore, we generalize the R-SCGD algo",
    "path": "papers/22/07/2207.09350.json",
    "total_tokens": 747,
    "translated_title": "Riemannian随机梯度方法用于嵌套组合优化",
    "translated_abstract": "本文考虑了在Riemann流形上针对嵌套形式中包含期望的函数组合的优化问题，这类问题在强化学习中的策略评估或元学习中的模型定制等应用中越来越受欢迎。我们提出了一种Riemann随机组合梯度下降（R-SCGD）方法，用于两级组合优化，在$O(\\epsilon^{-2})$次对外部函数的随机梯度oracle以及内部函数的随机函数和梯度oracles的调用中寻找期望平方Riemann梯度小于$\\epsilon$的近似稳定点。",
    "tldr": "提出了一种适用于嵌套函数组合的Riemannian随机梯度优化方法，可在有限次调用随机梯度oracle找到近似稳定点。",
    "en_tdlr": "Proposed a Riemannian stochastic gradient optimization method for nested function composition, which can find an approximate stable point with a finite number of calls to the stochastic gradient oracle."
}