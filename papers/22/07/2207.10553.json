{
    "title": "MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations of Behavior. (arXiv:2207.10553v2 [cs.LG] UPDATED)",
    "abstract": "We introduce MABe22, a large-scale, multi-agent video and trajectory benchmark to assess the quality of learned behavior representations. This dataset is collected from a variety of biology experiments, and includes triplets of interacting mice (4.7 million frames video+pose tracking data, 10 million frames pose only), symbiotic beetle-ant interactions (10 million frames video data), and groups of interacting flies (4.4 million frames of pose tracking data). Accompanying these data, we introduce a panel of real-life downstream analysis tasks to assess the quality of learned representations by evaluating how well they preserve information about the experimental conditions (e.g. strain, time of day, optogenetic stimulation) and animal behavior. We test multiple state-of-the-art self-supervised video and trajectory representation learning methods to demonstrate the use of our benchmark, revealing that methods developed using human action datasets do not fully translate to animal datasets.",
    "link": "http://arxiv.org/abs/2207.10553",
    "context": "Title: MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations of Behavior. (arXiv:2207.10553v2 [cs.LG] UPDATED)\nAbstract: We introduce MABe22, a large-scale, multi-agent video and trajectory benchmark to assess the quality of learned behavior representations. This dataset is collected from a variety of biology experiments, and includes triplets of interacting mice (4.7 million frames video+pose tracking data, 10 million frames pose only), symbiotic beetle-ant interactions (10 million frames video data), and groups of interacting flies (4.4 million frames of pose tracking data). Accompanying these data, we introduce a panel of real-life downstream analysis tasks to assess the quality of learned representations by evaluating how well they preserve information about the experimental conditions (e.g. strain, time of day, optogenetic stimulation) and animal behavior. We test multiple state-of-the-art self-supervised video and trajectory representation learning methods to demonstrate the use of our benchmark, revealing that methods developed using human action datasets do not fully translate to animal datasets.",
    "path": "papers/22/07/2207.10553.json",
    "total_tokens": 937,
    "translated_title": "MABe22：一种用于学习行为表示的多物种多任务基准",
    "translated_abstract": "我们引入了MABe22，一个大规模的多智能体视频和轨迹基准，用于评估学习行为表示的质量。该数据集采集自各种生物学实验，包括三个相互作用的小鼠三元组（470万帧的视频+姿态跟踪数据，1000万帧的仅姿态数据），共生甲虫-蚂蚁相互作用（1000万帧的视频数据）和一群互动的苍蝇（440万帧的姿态跟踪数据）。除了这些数据，我们还引入了一系列真实生活中的下游分析任务，以评估学习表示的质量，通过评估它们在保留关于实验条件（例如品系、时间、光遗传刺激）和动物行为信息方面的能力。我们测试了多种最先进的自监督视频和轨迹表示学习方法，以展示我们基准的用途，并揭示了使用人类行动数据集开发的方法不能完全适用于动物数据集。",
    "tldr": "MABe22是一个多物种多任务基准，用于评估学习行为表示的质量。它采集了来自各种生物学实验的数据，测试了多种自监督学习方法，并发现人类行动数据集上的方法不能完全适用于动物数据集。"
}