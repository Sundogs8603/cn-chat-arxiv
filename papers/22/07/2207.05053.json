{
    "title": "Learning Continuous Grasping Function with a Dexterous Hand from Human Demonstrations. (arXiv:2207.05053v3 [cs.RO] UPDATED)",
    "abstract": "We propose to learn to generate grasping motion for manipulation with a dexterous hand using implicit functions. With continuous time inputs, the model can generate a continuous and smooth grasping plan. We name the proposed model Continuous Grasping Function (CGF). CGF is learned via generative modeling with a Conditional Variational Autoencoder using 3D human demonstrations. We will first convert the large-scale human-object interaction trajectories to robot demonstrations via motion retargeting, and then use these demonstrations to train CGF. During inference, we perform sampling with CGF to generate different grasping plans in the simulator and select the successful ones to transfer to the real robot. By training on diverse human data, our CGF allows generalization to manipulate multiple objects. Compared to previous planning algorithms, CGF is more efficient and achieves significant improvement on success rate when transferred to grasping with the real Allegro Hand. Our project pa",
    "link": "http://arxiv.org/abs/2207.05053",
    "context": "Title: Learning Continuous Grasping Function with a Dexterous Hand from Human Demonstrations. (arXiv:2207.05053v3 [cs.RO] UPDATED)\nAbstract: We propose to learn to generate grasping motion for manipulation with a dexterous hand using implicit functions. With continuous time inputs, the model can generate a continuous and smooth grasping plan. We name the proposed model Continuous Grasping Function (CGF). CGF is learned via generative modeling with a Conditional Variational Autoencoder using 3D human demonstrations. We will first convert the large-scale human-object interaction trajectories to robot demonstrations via motion retargeting, and then use these demonstrations to train CGF. During inference, we perform sampling with CGF to generate different grasping plans in the simulator and select the successful ones to transfer to the real robot. By training on diverse human data, our CGF allows generalization to manipulate multiple objects. Compared to previous planning algorithms, CGF is more efficient and achieves significant improvement on success rate when transferred to grasping with the real Allegro Hand. Our project pa",
    "path": "papers/22/07/2207.05053.json",
    "total_tokens": 906,
    "translated_title": "从人类示范中学习使用机械手进行连续抓取",
    "translated_abstract": "我们提出使用隐式函数学习生成机械手操纵的抓取运动。使用连续时间输入，该模型可以生成连续且平滑的抓取计划。我们将所提出的模型命名为连续抓取功能（CGF）。通过使用3D人类示范，利用条件变分自动编码器进行生成建模，并将大规模的人体-物体交互轨迹通过运动重新定向转换成机器人演示，然后使用这些演示来训练CGF。在推断过程中，我们使用CGF进行抽样，在模拟器中生成不同的抓取计划，并选择成功的计划转移到真实机器人上。通过在多样化的人类数据上进行训练，我们的CGF允许操纵多个对象上的泛化。与先前的规划算法相比，CGF更加有效，在转移到使用真实Allegro手进行抓取时，成功率显著提高。",
    "tldr": "该论文提出了一种利用隐式函数进行连续抓取规划的方法，名为连续抓取功能(CG)，通过人类示范学习，可在操纵多个物体时获得更高的成功率，并在真实机器人上得到验证，成功率显著提高。",
    "en_tdlr": "This paper proposes a method for continuous grasping planning using implicit functions, named Continuous Grasping Function (CGF), learned from human demonstrations. The model allows for higher success rate when manipulating multiple objects and is verified on a real robot, achieving significant improvement compared to previous planning algorithms."
}