{
    "title": "High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent. (arXiv:2207.01560v3 [cs.LG] UPDATED)",
    "abstract": "In this paper, we study differentially private empirical risk minimization (DP-ERM). It has been shown that the worst-case utility of DP-ERM reduces polynomially as the dimension increases. This is a major obstacle to privately learning large machine learning models. In high dimension, it is common for some model's parameters to carry more information than others. To exploit this, we propose a differentially private greedy coordinate descent (DP-GCD) algorithm. At each iteration, DP-GCD privately performs a coordinate-wise gradient step along the gradients' (approximately) greatest entry. We show theoretically that DP-GCD can achieve a logarithmic dependence on the dimension for a wide range of problems by naturally exploiting their structural properties (such as quasi-sparse solutions). We illustrate this behavior numerically, both on synthetic and real datasets.",
    "link": "http://arxiv.org/abs/2207.01560",
    "context": "Title: High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent. (arXiv:2207.01560v3 [cs.LG] UPDATED)\nAbstract: In this paper, we study differentially private empirical risk minimization (DP-ERM). It has been shown that the worst-case utility of DP-ERM reduces polynomially as the dimension increases. This is a major obstacle to privately learning large machine learning models. In high dimension, it is common for some model's parameters to carry more information than others. To exploit this, we propose a differentially private greedy coordinate descent (DP-GCD) algorithm. At each iteration, DP-GCD privately performs a coordinate-wise gradient step along the gradients' (approximately) greatest entry. We show theoretically that DP-GCD can achieve a logarithmic dependence on the dimension for a wide range of problems by naturally exploiting their structural properties (such as quasi-sparse solutions). We illustrate this behavior numerically, both on synthetic and real datasets.",
    "path": "papers/22/07/2207.01560.json",
    "total_tokens": 898,
    "translated_title": "贪心坐标下降实现高维私有经验风险最小化",
    "translated_abstract": "在本文中，我们研究了差分隐私经验风险最小化（DP-ERM）。研究表明，随着维度的增加，DP-ERM的最坏情况效用会多项式降低。这是私有学习大型机器学习模型的主要障碍。在高维中，一些模型参数携带的信息比其他参数更多是很常见的。为了利用这一点，我们提出了一种差分隐私贪心坐标下降（DP-GCD）算法。在每个迭代步骤中，DP-GCD沿着梯度(大致地)最大的条目进行坐标梯度步骤。我们理论上证明，DP-GCD可以通过自然地利用其结构性质（例如拟稀疏解）在广泛的问题范围内实现对维度的对数依赖性。我们通过计算合成和真实数据集来说明这种行为。",
    "tldr": "本文针对高维数据中的差分隐私经验风险最小化问题，提出了一种差分隐私贪心坐标下降算法，能够通过利用模型的结构性质，在广泛的问题范围内实现对维度的对数依赖性。",
    "en_tdlr": "This paper proposes a differentially private greedy coordinate descent algorithm for the differentially private empirical risk minimization problem in high-dimensional data, which can achieve a logarithmic dependence on the dimension for a wide range of problems by exploiting the structural properties of the model and illustrated through synthetic and real datasets."
}