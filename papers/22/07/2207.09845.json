{
    "title": "Quantifying the Effect of Feedback Frequency in Interactive Reinforcement Learning for Robotic Tasks. (arXiv:2207.09845v2 [cs.RO] UPDATED)",
    "abstract": "Reinforcement learning (RL) has become widely adopted in robot control. Despite many successes, one major persisting problem can be very low data efficiency. One solution is interactive feedback, which has been shown to speed up RL considerably. As a result, there is an abundance of different strategies, which are, however, primarily tested on discrete grid-world and small scale optimal control scenarios. In the literature, there is no consensus about which feedback frequency is optimal or at which time the feedback is most beneficial. To resolve these discrepancies we isolate and quantify the effect of feedback frequency in robotic tasks with continuous state and action spaces. The experiments encompass inverse kinematics learning for robotic manipulator arms of different complexity. We show that seemingly contradictory reported phenomena occur at different complexity levels. Furthermore, our results suggest that no single ideal feedback frequency exists. Rather that feedback frequenc",
    "link": "http://arxiv.org/abs/2207.09845",
    "context": "Title: Quantifying the Effect of Feedback Frequency in Interactive Reinforcement Learning for Robotic Tasks. (arXiv:2207.09845v2 [cs.RO] UPDATED)\nAbstract: Reinforcement learning (RL) has become widely adopted in robot control. Despite many successes, one major persisting problem can be very low data efficiency. One solution is interactive feedback, which has been shown to speed up RL considerably. As a result, there is an abundance of different strategies, which are, however, primarily tested on discrete grid-world and small scale optimal control scenarios. In the literature, there is no consensus about which feedback frequency is optimal or at which time the feedback is most beneficial. To resolve these discrepancies we isolate and quantify the effect of feedback frequency in robotic tasks with continuous state and action spaces. The experiments encompass inverse kinematics learning for robotic manipulator arms of different complexity. We show that seemingly contradictory reported phenomena occur at different complexity levels. Furthermore, our results suggest that no single ideal feedback frequency exists. Rather that feedback frequenc",
    "path": "papers/22/07/2207.09845.json",
    "total_tokens": 935,
    "translated_title": "交互式强化学习中反馈频率对机器人任务的影响的定量化研究",
    "translated_abstract": "强化学习（RL）在机器人控制中被广泛采用。尽管有许多成功案例，但一个重要的持久性问题是数据效率非常低。交互反馈是一种解决方案，已被证明可以大大加速RL。因此，有大量不同的策略，然而这些策略主要是在离散的网格世界和小规模的最优控制场景中测试的。在文献中，对于哪种反馈频率最优或在什么时候反馈最有益并没有共识。为了解决这些差异，我们在具有连续状态和动作空间的机器人任务中分离并量化了反馈频率的影响。实验涵盖了不同复杂度的机械臂的逆运动学学习。我们展示了表面上矛盾的现象在不同的复杂度水平上出现。此外，我们的结果表明，没有单一的理想反馈频率存在。反馈频率应该根据具体的任务和机器人复杂性进行调整。",
    "tldr": "本文对交互式强化学习中反馈频率对机器人任务的影响进行了定量化研究，结果表明没有单一的理想反馈频率存在，应该根据具体的任务和机器人复杂性进行调整。",
    "en_tdlr": "This paper quantifies the effect of feedback frequency on robotic tasks in interactive reinforcement learning, showing that there is no single ideal feedback frequency but it should be adapted to the specific task and robot complexity."
}