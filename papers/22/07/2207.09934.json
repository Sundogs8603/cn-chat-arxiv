{
    "title": "DeepIPC: Deeply Integrated Perception and Control for an Autonomous Vehicle in Real Environments. (arXiv:2207.09934v4 [cs.RO] UPDATED)",
    "abstract": "We propose DeepIPC, an end-to-end autonomous driving model that handles both perception and control tasks in driving a vehicle. The model consists of two main parts, perception and controller modules. The perception module takes an RGBD image to perform semantic segmentation and bird's eye view (BEV) semantic mapping along with providing their encoded features. Meanwhile, the controller module processes these features with the measurement of GNSS locations and angular speed to estimate waypoints that come with latent features. Then, two different agents are used to translate waypoints and latent features into a set of navigational controls to drive the vehicle. The model is evaluated by predicting driving records and performing automated driving under various conditions in real environments. The experimental results show that DeepIPC achieves the best drivability and multi-task performance even with fewer parameters compared to the other models. Codes are available at https://github.co",
    "link": "http://arxiv.org/abs/2207.09934",
    "context": "Title: DeepIPC: Deeply Integrated Perception and Control for an Autonomous Vehicle in Real Environments. (arXiv:2207.09934v4 [cs.RO] UPDATED)\nAbstract: We propose DeepIPC, an end-to-end autonomous driving model that handles both perception and control tasks in driving a vehicle. The model consists of two main parts, perception and controller modules. The perception module takes an RGBD image to perform semantic segmentation and bird's eye view (BEV) semantic mapping along with providing their encoded features. Meanwhile, the controller module processes these features with the measurement of GNSS locations and angular speed to estimate waypoints that come with latent features. Then, two different agents are used to translate waypoints and latent features into a set of navigational controls to drive the vehicle. The model is evaluated by predicting driving records and performing automated driving under various conditions in real environments. The experimental results show that DeepIPC achieves the best drivability and multi-task performance even with fewer parameters compared to the other models. Codes are available at https://github.co",
    "path": "papers/22/07/2207.09934.json",
    "total_tokens": 858,
    "translated_title": "DeepIPC：在真实环境中实现自动驾驶的深度感知和控制",
    "translated_abstract": "本文提出DeepIPC，一个处理自动驾驶中感知和控制的端到端模型。该模型由感知和控制器两个主要部分组成。感知模块使用RGBD图像执行语义分割和鸟瞰图语义映射，并提供它们的编码特征。同时，控制器模块使用GNSS定位和角速度的测量来处理这些特征，估计一系列路径点和潜在特征。然后，使用两个不同的代理将路径点和潜在特征转换为一组导航控制，以驱动车辆。该模型通过预测行车记录和在不同真实场景下进行自动驾驶来进行评估。实验结果表明，DeepIPC即使使用较少的参数也能实现最佳可驾性和多任务性能，优于其他模型。相关代码可在 https://github.com 找到。",
    "tldr": "本文介绍了DeepIPC，一个端到端自动驾驶模型，能够同时处理感知和控制任务。实验结果表明，DeepIPC具有最佳的可驾性和多任务性能，甚至比其他模型所需的参数更少。",
    "en_tdlr": "This paper proposes DeepIPC, an end-to-end autonomous driving model that handles both perception and control tasks. The model achieves the best drivability and multi-task performance even with fewer parameters compared to other models, as shown in the experimental results."
}