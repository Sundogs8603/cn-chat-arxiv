{
    "title": "Efficient NLP Model Finetuning via Multistage Data Filtering. (arXiv:2207.14386v2 [cs.CL] UPDATED)",
    "abstract": "As model finetuning is central to the modern NLP, we set to maximize its efficiency. Motivated by redundancy in training examples and the sheer sizes of pretrained models, we exploit a key opportunity: training only on important data. To this end, we set to filter training examples in a streaming fashion, in tandem with training the target model. Our key techniques are two: (1) automatically determine a training loss threshold for skipping backward training passes; (2) run a meta predictor for further skipping forward training passes. We integrate the above techniques in a holistic, three-stage training process. On a diverse set of benchmarks, our method reduces the required training examples by up to 5.3$\\times$ and training time by up to 6.8$\\times$, while only seeing minor accuracy degradation. Our method is effective even when training one epoch, where each training example is encountered only once. It is simple to implement and is compatible with the existing finetuning techniques",
    "link": "http://arxiv.org/abs/2207.14386",
    "context": "Title: Efficient NLP Model Finetuning via Multistage Data Filtering. (arXiv:2207.14386v2 [cs.CL] UPDATED)\nAbstract: As model finetuning is central to the modern NLP, we set to maximize its efficiency. Motivated by redundancy in training examples and the sheer sizes of pretrained models, we exploit a key opportunity: training only on important data. To this end, we set to filter training examples in a streaming fashion, in tandem with training the target model. Our key techniques are two: (1) automatically determine a training loss threshold for skipping backward training passes; (2) run a meta predictor for further skipping forward training passes. We integrate the above techniques in a holistic, three-stage training process. On a diverse set of benchmarks, our method reduces the required training examples by up to 5.3$\\times$ and training time by up to 6.8$\\times$, while only seeing minor accuracy degradation. Our method is effective even when training one epoch, where each training example is encountered only once. It is simple to implement and is compatible with the existing finetuning techniques",
    "path": "papers/22/07/2207.14386.json",
    "total_tokens": 959,
    "translated_title": "多阶段数据过滤提高NLP模型微调效率",
    "translated_abstract": "针对NLP模型微调的重要性，本文旨在最大程度提高它的效率。考虑到训练样例的冗余和预训练模型的规模，我们利用了一种关键机会：仅使用重要数据进行训练。为此，我们致力于以流式方式过滤训练样例，并同时训练目标模型。我们的两个关键技术是：（1）自动确定用于跳过反向训练的训练损失阈值；（2）运行元预测器以进一步跳过前向训练。我们将上述技术集成到一个全面的三阶段训练过程中。在各种基准测试中，我们的方法将所需的训练样例减少了最多5.3倍，并将训练时间缩短了最多6.8倍，同时只看到很小的精度降低。即使只有一个时期的训练，在每个训练样例只遇到一次的情况下，我们的方法也非常有效。它很容易实现并兼容现有的微调技术。",
    "tldr": "本文介绍了一种多阶段数据过滤的方法，以提高NLP模型微调效率。在各种基准测试中，该方法有效减少所需训练样例数和训练时间，同时只有轻微精度降低，即使只有一个时期的训练，也非常有效。",
    "en_tdlr": "This paper presents a multi-stage data filtering method to improve the efficiency of NLP model finetuning, which significantly reduces the required training examples and training time with only minor accuracy degradation on diverse benchmarks, even for single-epoch training."
}