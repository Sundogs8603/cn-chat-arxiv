{
    "title": "High Dimensional Generalised Penalised Least Squares. (arXiv:2207.07055v3 [econ.EM] UPDATED)",
    "abstract": "In this paper we develop inference for high dimensional linear models, with serially correlated errors. We examine Lasso under the assumption of strong mixing in the covariates and error process, allowing for fatter tails in their distribution. While the Lasso estimator performs poorly under such circumstances, we estimate via GLS Lasso the parameters of interest and extend the asymptotic properties of the Lasso under more general conditions. Our theoretical results indicate that the non-asymptotic bounds for stationary dependent processes are sharper, while the rate of Lasso under general conditions appears slower as $T,p\\to \\infty$. Further we employ the debiased Lasso to perform inference uniformly on the parameters of interest. Monte Carlo results support the proposed estimator, as it has significant efficiency gains over traditional methods.",
    "link": "http://arxiv.org/abs/2207.07055",
    "context": "Title: High Dimensional Generalised Penalised Least Squares. (arXiv:2207.07055v3 [econ.EM] UPDATED)\nAbstract: In this paper we develop inference for high dimensional linear models, with serially correlated errors. We examine Lasso under the assumption of strong mixing in the covariates and error process, allowing for fatter tails in their distribution. While the Lasso estimator performs poorly under such circumstances, we estimate via GLS Lasso the parameters of interest and extend the asymptotic properties of the Lasso under more general conditions. Our theoretical results indicate that the non-asymptotic bounds for stationary dependent processes are sharper, while the rate of Lasso under general conditions appears slower as $T,p\\to \\infty$. Further we employ the debiased Lasso to perform inference uniformly on the parameters of interest. Monte Carlo results support the proposed estimator, as it has significant efficiency gains over traditional methods.",
    "path": "papers/22/07/2207.07055.json",
    "total_tokens": 870,
    "translated_title": "高维广义惩罚最小二乘的推断方法",
    "translated_abstract": "本文针对高维线性模型中的串行相关误差开展推断研究。我们在变量和误差过程强混合的假设下探讨了Lasso方法，并允许它们的分布尾部更加厚重。虽然在这种情况下Lasso估计量表现不佳，但我们通过GLS Lasso估计感兴趣的参数，并在更一般的条件下扩展了Lasso的渐近特性。我们的理论结果表明，对于平稳相关过程的非渐近界限更严格，而Lasso在一般条件下的速度随着$T, p \\to \\infty$而变慢。此外，我们采用去偏Lasso方法，对感兴趣的参数进行均匀推断。蒙特卡罗结果支持所提出的估计量，因为它相对传统方法具有显着的效率提高。",
    "tldr": "本文提出了GLS Lasso方法用于高维线性模型的推断，支持强混合变量和误差过程的分布和重尾特征，并采用去偏Lasso方法进行感兴趣的参数均匀推断，蒙特卡罗结果表明该方法比传统方法更有效。",
    "en_tdlr": "This paper proposes GLS Lasso method for inference in high dimensional linear models, supporting strong mixing in the covariates and error process with fatter tails, and employs debiased Lasso for uniformly inferring the parameters of interest, with Monte Carlo results demonstrating significant efficiency gains compared to traditional methods."
}