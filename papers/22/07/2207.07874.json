{
    "title": "Model-Aware Contrastive Learning: Towards Escaping the Dilemmas. (arXiv:2207.07874v4 [cs.LG] UPDATED)",
    "abstract": "Contrastive learning (CL) continuously achieves significant breakthroughs across multiple domains. However, the most common InfoNCE-based methods suffer from some dilemmas, such as \\textit{uniformity-tolerance dilemma} (UTD) and \\textit{gradient reduction}, both of which are related to a $\\mathcal{P}_{ij}$ term. It has been identified that UTD can lead to unexpected performance degradation. We argue that the fixity of temperature is to blame for UTD. To tackle this challenge, we enrich the CL loss family by presenting a Model-Aware Contrastive Learning (MACL) strategy, whose temperature is adaptive to the magnitude of alignment that reflects the basic confidence of the instance discrimination task, then enables CL loss to adjust the penalty strength for hard negatives adaptively. Regarding another dilemma, the gradient reduction issue, we derive the limits of an involved gradient scaling factor, which allows us to explain from a unified perspective why some recent approaches are effect",
    "link": "http://arxiv.org/abs/2207.07874",
    "context": "Title: Model-Aware Contrastive Learning: Towards Escaping the Dilemmas. (arXiv:2207.07874v4 [cs.LG] UPDATED)\nAbstract: Contrastive learning (CL) continuously achieves significant breakthroughs across multiple domains. However, the most common InfoNCE-based methods suffer from some dilemmas, such as \\textit{uniformity-tolerance dilemma} (UTD) and \\textit{gradient reduction}, both of which are related to a $\\mathcal{P}_{ij}$ term. It has been identified that UTD can lead to unexpected performance degradation. We argue that the fixity of temperature is to blame for UTD. To tackle this challenge, we enrich the CL loss family by presenting a Model-Aware Contrastive Learning (MACL) strategy, whose temperature is adaptive to the magnitude of alignment that reflects the basic confidence of the instance discrimination task, then enables CL loss to adjust the penalty strength for hard negatives adaptively. Regarding another dilemma, the gradient reduction issue, we derive the limits of an involved gradient scaling factor, which allows us to explain from a unified perspective why some recent approaches are effect",
    "path": "papers/22/07/2207.07874.json",
    "total_tokens": 1055,
    "translated_title": "模型感知的对比学习: 迈向摆脱困境",
    "translated_abstract": "对比学习在多个领域中取得了重大突破。然而，最常见的基于InfoNCE的方法存在一些困境，如统一性容忍困境（UTD）和梯度降低，两者都与$\\mathcal{P}_{ij}$项有关。已经确认UTD可能导致意外的性能下降。我们认为温度的固定性是UTD的罪魁祸首。为了解决这一挑战，我们通过提出一种模型感知的对比学习（MACL）策略来丰富CL损失家族，其温度适应于反映实例判别任务基本置信度的对齐幅度的大小，然后使CL损失能够自适应地调整对硬负面的惩罚力度。关于另一个困境-梯度降低问题，我们导出了一个涉及梯度缩放因子的极限，可以从统一的角度解释为什么一些最近的方法实际上是对优化过程的DDoS攻击。大量实验证明，MACL可以优于所有基线，并实现最先进的结果。",
    "tldr": "本文提出了一种模型感知的对比学习（MACL）策略，其温度适应于反映实例判别任务基本置信度的对齐幅度的大小，解决了对比学习中的统一性容忍困境（UTD）和梯度降低问题，并在实验证明其优于所有基线，并实现最先进的结果。",
    "en_tdlr": "This paper proposes a Model-Aware Contrastive Learning (MACL) strategy, whose temperature adapts to the magnitude of alignment that reflects the basic confidence of the instance discrimination task, to address the uniformity-tolerance dilemma (UTD) and gradient reduction issue in contrastive learning. Extensive experiments show that MACL outperforms all baselines and achieves state-of-the-art results."
}