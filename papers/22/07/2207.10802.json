{
    "title": "Combing for Credentials: Active Pattern Extraction from Smart Reply. (arXiv:2207.10802v3 [cs.CR] UPDATED)",
    "abstract": "Pre-trained large language models, such as GPT\\nobreakdash-2 and BERT, are often fine-tuned to achieve state-of-the-art performance on a downstream task. One natural example is the ``Smart Reply'' application where a pre-trained model is tuned to provide suggested responses for a given query message. Since the tuning data is often sensitive data such as emails or chat transcripts, it is important to understand and mitigate the risk that the model leaks its tuning data. We investigate potential information leakage vulnerabilities in a typical Smart Reply pipeline. We consider a realistic setting where the adversary can only interact with the underlying model through a front-end interface that constrains what types of queries can be sent to the model. Previous attacks do not work in these settings, but require the ability to send unconstrained queries directly to the model. Even when there are no constraints on the queries, previous attacks typically require thousands, or even millions, ",
    "link": "http://arxiv.org/abs/2207.10802",
    "context": "Title: Combing for Credentials: Active Pattern Extraction from Smart Reply. (arXiv:2207.10802v3 [cs.CR] UPDATED)\nAbstract: Pre-trained large language models, such as GPT\\nobreakdash-2 and BERT, are often fine-tuned to achieve state-of-the-art performance on a downstream task. One natural example is the ``Smart Reply'' application where a pre-trained model is tuned to provide suggested responses for a given query message. Since the tuning data is often sensitive data such as emails or chat transcripts, it is important to understand and mitigate the risk that the model leaks its tuning data. We investigate potential information leakage vulnerabilities in a typical Smart Reply pipeline. We consider a realistic setting where the adversary can only interact with the underlying model through a front-end interface that constrains what types of queries can be sent to the model. Previous attacks do not work in these settings, but require the ability to send unconstrained queries directly to the model. Even when there are no constraints on the queries, previous attacks typically require thousands, or even millions, ",
    "path": "papers/22/07/2207.10802.json",
    "total_tokens": 837,
    "translated_title": "从智能回复中提取主动模式进行认证",
    "translated_abstract": "预训练的大型语言模型，如GPT-2和BERT，通常会通过微调来在下游任务中达到最先进的性能。一个自然的例子是“智能回复”应用程序，其中预训练模型被调整以提供给定查询消息的建议回复。由于微调数据通常是敏感的数据，如电子邮件或聊天记录，因此重要的是了解和减轻模型泄漏微调数据的风险。我们调查了典型智能回复流程中潜在的信息泄露漏洞。我们考虑了一个现实的情况，即攻击者只能通过前端界面与基础模型进行交互，并限制了可以发送到模型的查询类型。先前的攻击在这些设置中不起作用，而是需要能够直接向模型发送无限制的查询。即使在没有查询约束的情况下，以往的攻击通常需要数千甚至数百万",
    "tldr": "该论文研究了在智能回复应用程序中潜在的信息泄漏漏洞，并且提出了一种在实际设置中限制查询类型的攻击方式。"
}