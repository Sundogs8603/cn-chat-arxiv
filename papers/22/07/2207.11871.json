{
    "title": "Towards Complex Document Understanding By Discrete Reasoning. (arXiv:2207.11871v3 [cs.CV] UPDATED)",
    "abstract": "Document Visual Question Answering (VQA) aims to understand visually-rich documents to answer questions in natural language, which is an emerging research topic for both Natural Language Processing and Computer Vision. In this work, we introduce a new Document VQA dataset, named TAT-DQA, which consists of 3,067 document pages comprising semi-structured table(s) and unstructured text as well as 16,558 question-answer pairs by extending the TAT-QA dataset. These documents are sampled from real-world financial reports and contain lots of numbers, which means discrete reasoning capability is demanded to answer questions on this dataset. Based on TAT-DQA, we further develop a novel model named MHST that takes into account the information in multi-modalities, including text, layout and visual image, to intelligently address different types of questions with corresponding strategies, i.e., extraction or reasoning. Extensive experiments show that the MHST model significantly outperforms the ba",
    "link": "http://arxiv.org/abs/2207.11871",
    "context": "Title: Towards Complex Document Understanding By Discrete Reasoning. (arXiv:2207.11871v3 [cs.CV] UPDATED)\nAbstract: Document Visual Question Answering (VQA) aims to understand visually-rich documents to answer questions in natural language, which is an emerging research topic for both Natural Language Processing and Computer Vision. In this work, we introduce a new Document VQA dataset, named TAT-DQA, which consists of 3,067 document pages comprising semi-structured table(s) and unstructured text as well as 16,558 question-answer pairs by extending the TAT-QA dataset. These documents are sampled from real-world financial reports and contain lots of numbers, which means discrete reasoning capability is demanded to answer questions on this dataset. Based on TAT-DQA, we further develop a novel model named MHST that takes into account the information in multi-modalities, including text, layout and visual image, to intelligently address different types of questions with corresponding strategies, i.e., extraction or reasoning. Extensive experiments show that the MHST model significantly outperforms the ba",
    "path": "papers/22/07/2207.11871.json",
    "total_tokens": 978,
    "translated_title": "通过离散推理实现复杂文档理解",
    "translated_abstract": "文档视觉问答旨在理解视觉丰富的文档，以回答自然语言中的问题，这是自然语言处理和计算机视觉的一个新兴研究课题。本文介绍了一个名为TAT-DQA的新的文档视觉问答数据集，它由3067个半结构化表格和非结构化文本页面以及16558个问题-答案对组成，是通过扩展TAT-QA数据集得到的。这些文档来自于现实世界的财务报告，包含大量数字，因此需要具有离散推理能力来回答这个数据集上的问题。基于TAT-DQA，我们进一步开发了一种名为MHST的新型模型，它考虑了多种模态的信息，包括文本、布局和视觉图像，以智能地针对不同类型的问题采用相应的策略，即抽取或推理。广泛的实验表明，MHST模型在TAT-DQA数据集上显著优于基线方法和几种最先进的文档视觉问答模型。",
    "tldr": "本文介绍了一个新的文档视觉问答数据集TAT-DQA，并通过该数据集开发了一种新型模型MHST。MHST模型考虑了多种模态的信息来智能地回答不同类型的问题，实验表明其在TAT-DQA数据集上显著优于其他模型。",
    "en_tdlr": "This paper introduces a new Document Visual Question Answering dataset named TAT-DQA and develop a novel multi-modal model named MHST which can intelligently answer different types of questions. Extensive experiments show that MHST significantly outperforms other models on the TAT-DQA dataset."
}