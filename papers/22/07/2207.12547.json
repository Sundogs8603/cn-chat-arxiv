{
    "title": "The BUTTER Zone: An Empirical Study of Training Dynamics in Fully Connected Neural Networks. (arXiv:2207.12547v2 [cs.LG] UPDATED)",
    "abstract": "We present an empirical dataset surveying the deep learning phenomenon on fully-connected feed-forward multilayer perceptron neural networks. The dataset, which is now freely available online, records the per-epoch training and generalization performance of 483 thousand distinct hyperparameter choices of architectures, tasks, depths, network sizes (number of parameters), learning rates, batch sizes, and regularization penalties. Repeating each experiment an average of 24 times resulted in 11 million total training runs and 40 billion epochs recorded. Accumulating this 1.7 TB dataset utilized 11 thousand CPU core-years, 72.3 GPU-years, and 163 node-years. In surveying the dataset, we observe durable patterns persisting across tasks and topologies. We aim to spark scientific study of machine learning techniques as a catalyst for the theoretical discoveries needed to progress the field beyond energy-intensive and heuristic practices.",
    "link": "http://arxiv.org/abs/2207.12547",
    "context": "Title: The BUTTER Zone: An Empirical Study of Training Dynamics in Fully Connected Neural Networks. (arXiv:2207.12547v2 [cs.LG] UPDATED)\nAbstract: We present an empirical dataset surveying the deep learning phenomenon on fully-connected feed-forward multilayer perceptron neural networks. The dataset, which is now freely available online, records the per-epoch training and generalization performance of 483 thousand distinct hyperparameter choices of architectures, tasks, depths, network sizes (number of parameters), learning rates, batch sizes, and regularization penalties. Repeating each experiment an average of 24 times resulted in 11 million total training runs and 40 billion epochs recorded. Accumulating this 1.7 TB dataset utilized 11 thousand CPU core-years, 72.3 GPU-years, and 163 node-years. In surveying the dataset, we observe durable patterns persisting across tasks and topologies. We aim to spark scientific study of machine learning techniques as a catalyst for the theoretical discoveries needed to progress the field beyond energy-intensive and heuristic practices.",
    "path": "papers/22/07/2207.12547.json",
    "total_tokens": 904,
    "translated_title": "BUTTER区域：全连接神经网络训练动态的实证研究",
    "translated_abstract": "我们提供了一个实证数据集，调查了全连接前馈多层感知机神经网络的深度学习现象。该数据集现在可以在网上免费获取，记录了在不同超参数选择下的架构、任务、深度、网络大小（参数数量）、学习率、批次大小和正则化惩罚的每个时代的训练和泛化性能。通过平均重复每个实验24次，共记录了1100万个训练运行和400亿个时代。总计使用了1.7 TB的数据集，耗费了11000个CPU核年、72.3个GPU年和163个节点年。在调查数据集时，我们观察到了跨任务和拓扑结构持续存在的稳定模式。我们的目标是引发对机器学习技术的科学研究，为推进该领域超越高能耗和启发式实践所需的理论发现提供动力。",
    "tldr": "本研究提供了一个实证数据集，调查了全连接神经网络的训练动态，并观察到稳定模式跨任务和拓扑结构持续存在。该研究旨在激发机器学习技术的科学研究，为推进该领域的理论发现提供必要的动力。"
}