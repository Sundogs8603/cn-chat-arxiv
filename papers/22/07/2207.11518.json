{
    "title": "Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition. (arXiv:2207.11518v2 [cs.CV] UPDATED)",
    "abstract": "The teacher-free online Knowledge Distillation (KD) aims to train an ensemble of multiple student models collaboratively and distill knowledge from each other. Although existing online KD methods achieve desirable performance, they often focus on class probabilities as the core knowledge type, ignoring the valuable feature representational information. We present a Mutual Contrastive Learning (MCL) framework for online KD. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions among a cohort of networks in an online manner. Our MCL can aggregate cross-network embedding information and maximize the lower bound to the mutual information between two networks. This enables each network to learn extra contrastive knowledge from others, leading to better feature representations, thus improving the performance of visual recognition tasks. Beyond the final layer, we extend MCL to intermediate layers and perform an adaptive layer-matching mechanism train",
    "link": "http://arxiv.org/abs/2207.11518",
    "context": "Title: Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition. (arXiv:2207.11518v2 [cs.CV] UPDATED)\nAbstract: The teacher-free online Knowledge Distillation (KD) aims to train an ensemble of multiple student models collaboratively and distill knowledge from each other. Although existing online KD methods achieve desirable performance, they often focus on class probabilities as the core knowledge type, ignoring the valuable feature representational information. We present a Mutual Contrastive Learning (MCL) framework for online KD. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions among a cohort of networks in an online manner. Our MCL can aggregate cross-network embedding information and maximize the lower bound to the mutual information between two networks. This enables each network to learn extra contrastive knowledge from others, leading to better feature representations, thus improving the performance of visual recognition tasks. Beyond the final layer, we extend MCL to intermediate layers and perform an adaptive layer-matching mechanism train",
    "path": "papers/22/07/2207.11518.json",
    "total_tokens": 872,
    "translated_title": "在视觉识别中通过互相对比学习的在线知识蒸馏",
    "translated_abstract": "无需教师的在线知识蒸馏旨在通过合作训练一组多个学生模型并相互蒸馏知识。虽然现有的在线知识蒸馏方法能够达到理想的性能，但它们通常将类概率作为核心知识类型，忽略了有价值的特征表示信息。我们提出了一种互相对比学习的在线知识蒸馏框架，其核心思想是在一组网络之间进行互相交互和对比分布的转移。我们的方法可以汇总不同网络之间的嵌入信息，并最大化两个网络之间互信息的下限。这使得每个网络都可以从其他网络中学习到额外的对比知识，从而改善特征表示，提高视觉识别任务的性能。除了最后一层，我们还将MCL扩展到中间层，并进行自适应层匹配机制的训练。",
    "tldr": "该论文提出了一种互相对比学习的在线知识蒸馏框架，可以让不同的学生模型互相学习到额外的对比知识，从而提高了视觉识别任务的性能。",
    "en_tdlr": "This paper proposes a Mutual Contrastive Learning (MCL) framework for online Knowledge Distillation (KD) to allow different student models to learn extra contrastive knowledge from each other, thus improving visual recognition tasks performance."
}