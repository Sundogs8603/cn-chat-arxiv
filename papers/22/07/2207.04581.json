{
    "title": "How Robust is your Fair Model? Exploring the Robustness of Diverse Fairness Strategies. (arXiv:2207.04581v3 [cs.LG] UPDATED)",
    "abstract": "With the introduction of machine learning in high-stakes decision making, ensuring algorithmic fairness has become an increasingly important problem to solve. In response to this, many mathematical definitions of fairness have been proposed, and a variety of optimisation techniques have been developed, all designed to maximise a defined notion of fairness. However, fair solutions are reliant on the quality of the training data, and can be highly sensitive to noise. Recent studies have shown that robustness (the ability for a model to perform well on unseen data) plays a significant role in the type of strategy that should be used when approaching a new problem and, hence, measuring the robustness of these strategies has become a fundamental problem. In this work, we therefore propose a new criterion to measure the robustness of various fairness optimisation strategies - the robustness ratio. We conduct multiple extensive experiments on five bench mark fairness data sets using three of ",
    "link": "http://arxiv.org/abs/2207.04581",
    "context": "Title: How Robust is your Fair Model? Exploring the Robustness of Diverse Fairness Strategies. (arXiv:2207.04581v3 [cs.LG] UPDATED)\nAbstract: With the introduction of machine learning in high-stakes decision making, ensuring algorithmic fairness has become an increasingly important problem to solve. In response to this, many mathematical definitions of fairness have been proposed, and a variety of optimisation techniques have been developed, all designed to maximise a defined notion of fairness. However, fair solutions are reliant on the quality of the training data, and can be highly sensitive to noise. Recent studies have shown that robustness (the ability for a model to perform well on unseen data) plays a significant role in the type of strategy that should be used when approaching a new problem and, hence, measuring the robustness of these strategies has become a fundamental problem. In this work, we therefore propose a new criterion to measure the robustness of various fairness optimisation strategies - the robustness ratio. We conduct multiple extensive experiments on five bench mark fairness data sets using three of ",
    "path": "papers/22/07/2207.04581.json",
    "total_tokens": 1100,
    "translated_title": "你的公平模型有多稳健？探索不同公平策略的稳健性。",
    "translated_abstract": "随着机器学习在高风险决策中的应用，确保算法公平性已成为一个愈发重要的问题。为此，已经提出了许多数学上的公平性定义，并开发了各种优化技术，旨在最大化定义的公平性概念。然而，公平解决方案依赖于训练数据的质量，而且对噪声非常敏感。最近的研究表明，稳健性（模型在未知数据上表现良好的能力）在应对新问题时应使用的策略类型中起着重要作用，因此衡量这些策略的稳健性已成为一个基本问题。因此，在本文中，我们提出了一个新的标准来衡量各种公平优化策略的稳健性 - 稳健比率。我们使用三种最常用的公平策略对五个基准公平数据集进行了多次广泛的实验，并表明这些策略的稳健性在数据集之间和不同公平性定义之间存在显着差异。我们的结果表明，在设计和选择公平算法时，应更加谨慎地考虑稳健性，以确保它们在实际场景中始终有效可靠。",
    "tldr": "本文提出了一个新的标准来衡量公平优化策略的稳健性——稳健比率，并使用三种公平策略在五个公平数据集上进行了多次广泛的实验。结果表明，公平策略的稳健性在不同数据集之间和不同公平性定义之间存在显着差异。",
    "en_tdlr": "This paper proposes a new criterion to measure the robustness of various fairness optimization strategies - the robustness ratio, and conducts extensive experiments on five benchmark fairness datasets using three commonly used fairness strategies. The results show that the robustness of fairness strategies varies significantly across datasets and between different fairness definitions."
}