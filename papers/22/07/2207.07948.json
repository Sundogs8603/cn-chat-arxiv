{
    "title": "Collaborative Learning in Kernel-based Bandits for Distributed Users. (arXiv:2207.07948v2 [stat.ML] UPDATED)",
    "abstract": "We study collaborative learning among distributed clients facilitated by a central server. Each client is interested in maximizing a personalized objective function that is a weighted sum of its local objective and a global objective. Each client has direct access to random bandit feedback on its local objective, but only has a partial view of the global objective and relies on information exchange with other clients for collaborative learning. We adopt the kernel-based bandit framework where the objective functions belong to a reproducing kernel Hilbert space. We propose an algorithm based on surrogate Gaussian process (GP) models and establish its order-optimal regret performance (up to polylogarithmic factors). We also show that the sparse approximations of the GP models can be employed to reduce the communication overhead across clients.",
    "link": "http://arxiv.org/abs/2207.07948",
    "context": "Title: Collaborative Learning in Kernel-based Bandits for Distributed Users. (arXiv:2207.07948v2 [stat.ML] UPDATED)\nAbstract: We study collaborative learning among distributed clients facilitated by a central server. Each client is interested in maximizing a personalized objective function that is a weighted sum of its local objective and a global objective. Each client has direct access to random bandit feedback on its local objective, but only has a partial view of the global objective and relies on information exchange with other clients for collaborative learning. We adopt the kernel-based bandit framework where the objective functions belong to a reproducing kernel Hilbert space. We propose an algorithm based on surrogate Gaussian process (GP) models and establish its order-optimal regret performance (up to polylogarithmic factors). We also show that the sparse approximations of the GP models can be employed to reduce the communication overhead across clients.",
    "path": "papers/22/07/2207.07948.json",
    "total_tokens": 800,
    "translated_title": "基于内核的赌博机协同学习",
    "translated_abstract": "本文研究了通过中央服务器协调分布式客户端之间的协同学习。每个客户端都希望最大化其个性化目标函数，该函数是其本地目标函数和全局目标函数的加权和。每个客户端直接访问其本地目标函数的随机赌博反馈，但只有对全局目标函数的部分视图，并对其他客户端进行信息交流以进行协同学习。我们采用基于内核的赌博机框架，其中目标函数属于再生核希尔伯特空间。我们提出了一种基于代理高斯进程（GP）模型的算法，并确定了其（多项式对数因子内）的次优遗憾性能。我们还表明，可以采用GP模型的稀疏逼近来减少客户端之间的通信开销。",
    "tldr": "本文研究了分布式用户之间的基于内核的赌博机协同学习，并提出了一种使用代理高斯进程模型的算法，以降低通信开销，获得次优遗憾性能。",
    "en_tdlr": "This paper studies collaborative learning in kernel-based bandits among distributed users and proposes an algorithm based on surrogate Gaussian process models to reduce the communication overhead and achieve order-optimal regret performance."
}