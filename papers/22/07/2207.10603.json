{
    "title": "Unsupervised pre-training of graph transformers on patient population graphs. (arXiv:2207.10603v2 [cs.LG] UPDATED)",
    "abstract": "Pre-training has shown success in different areas of machine learning, such as Computer Vision, Natural Language Processing (NLP), and medical imaging. However, it has not been fully explored for clinical data analysis. An immense amount of clinical records are recorded, but still, data and labels can be scarce for data collected in small hospitals or dealing with rare diseases. In such scenarios, pre-training on a larger set of unlabelled clinical data could improve performance. In this paper, we propose novel unsupervised pre-training techniques designed for heterogeneous, multi-modal clinical data for patient outcome prediction inspired by masked language modeling (MLM), by leveraging graph deep learning over population graphs. To this end, we further propose a graph-transformer-based network, designed to handle heterogeneous clinical data. By combining masking-based pre-training with a transformer-based network, we translate the success of masking-based pre-training in other domain",
    "link": "http://arxiv.org/abs/2207.10603",
    "context": "Title: Unsupervised pre-training of graph transformers on patient population graphs. (arXiv:2207.10603v2 [cs.LG] UPDATED)\nAbstract: Pre-training has shown success in different areas of machine learning, such as Computer Vision, Natural Language Processing (NLP), and medical imaging. However, it has not been fully explored for clinical data analysis. An immense amount of clinical records are recorded, but still, data and labels can be scarce for data collected in small hospitals or dealing with rare diseases. In such scenarios, pre-training on a larger set of unlabelled clinical data could improve performance. In this paper, we propose novel unsupervised pre-training techniques designed for heterogeneous, multi-modal clinical data for patient outcome prediction inspired by masked language modeling (MLM), by leveraging graph deep learning over population graphs. To this end, we further propose a graph-transformer-based network, designed to handle heterogeneous clinical data. By combining masking-based pre-training with a transformer-based network, we translate the success of masking-based pre-training in other domain",
    "path": "papers/22/07/2207.10603.json",
    "total_tokens": 901,
    "translated_title": "无监督预训练医疗图谱中的图变换器",
    "translated_abstract": "预训练在计算机视觉、自然语言处理和医学图像等机器学习领域已经取得了成功，但在临床数据分析方面尚未得到充分探索。在小型医院或处理罕见疾病的情况下，虽然记录了大量的临床记录，但数据和标签仍然可能稀缺。在这种情况下，预训练在更大规模的无标签临床数据集上可能会提高性能。本文提出了针对多模态临床数据的新型无监督预训练技术，用于患者预测结果，灵感来自掩码语言建模（MLM），通过在患者人口图上利用图深度学习。为此，我们进一步提出了一种基于图变换器的网络，用于处理异质临床数据。通过将基于掩码的预训练与基于变换器的网络相结合，我们将在其他领域中基于掩码的预训练的成功应用于临床数据分析。",
    "tldr": "本研究提出了一种针对多模态临床数据的图变换器的无监督预训练方法，通过在患者人口图上利用图深度学习，可以提高患者预测结果的性能。",
    "en_tdlr": "This paper proposes unsupervised pre-training techniques for multi-modal clinical data using graph transformers, leveraging graph deep learning over patient population graphs. It shows improved performance in patient outcome prediction."
}