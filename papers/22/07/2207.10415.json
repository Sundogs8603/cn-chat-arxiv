{
    "title": "Log Barriers for Safe Black-box Optimization with Application to Safe Reinforcement Learning. (arXiv:2207.10415v2 [math.OC] UPDATED)",
    "abstract": "Optimizing noisy functions online, when evaluating the objective requires experiments on a deployed system, is a crucial task arising in manufacturing, robotics and many others. Often, constraints on safe inputs are unknown ahead of time, and we only obtain noisy information, indicating how close we are to violating the constraints. Yet, safety must be guaranteed at all times, not only for the final output of the algorithm.  We introduce a general approach for seeking a stationary point in high dimensional non-linear stochastic optimization problems in which maintaining safety during learning is crucial. Our approach called LB-SGD is based on applying stochastic gradient descent (SGD) with a carefully chosen adaptive step size to a logarithmic barrier approximation of the original problem. We provide a complete convergence analysis of non-convex, convex, and strongly-convex smooth constrained problems, with first-order and zeroth-order feedback. Our approach yields efficient updates an",
    "link": "http://arxiv.org/abs/2207.10415",
    "context": "Title: Log Barriers for Safe Black-box Optimization with Application to Safe Reinforcement Learning. (arXiv:2207.10415v2 [math.OC] UPDATED)\nAbstract: Optimizing noisy functions online, when evaluating the objective requires experiments on a deployed system, is a crucial task arising in manufacturing, robotics and many others. Often, constraints on safe inputs are unknown ahead of time, and we only obtain noisy information, indicating how close we are to violating the constraints. Yet, safety must be guaranteed at all times, not only for the final output of the algorithm.  We introduce a general approach for seeking a stationary point in high dimensional non-linear stochastic optimization problems in which maintaining safety during learning is crucial. Our approach called LB-SGD is based on applying stochastic gradient descent (SGD) with a carefully chosen adaptive step size to a logarithmic barrier approximation of the original problem. We provide a complete convergence analysis of non-convex, convex, and strongly-convex smooth constrained problems, with first-order and zeroth-order feedback. Our approach yields efficient updates an",
    "path": "papers/22/07/2207.10415.json",
    "total_tokens": 811,
    "translated_title": "具有应用于安全强化学习的安全黑匣子优化的对数障碍",
    "translated_abstract": "在制造业、机器人和许多其他领域中，优化嘈杂函数是一个关键的任务，因为在部署系统上评估目标函数需要进行实验。通常，我们事先不知道安全输入的限制条件，我们只能得到嘈杂的信息，指示我们距离违反约束条件的距离有多远。然而，安全在整个学习过程中必须保证，而不仅仅是在算法的最终输出上。",
    "tldr": "提出了一种名为LB-SGD的方法，它基于对原始问题的对数障碍近似，并应用随机梯度下降。此方法可用于使学习过程中保持安全至关重要的高维非线性随机优化问题，包括一阶和零阶反馈的非凸、凸和强凸平滑约束问题的完全收敛分析。",
    "en_tdlr": "Introduced LB-SGD, a method that applies stochastic gradient descent with a logarithmic barrier approximation to optimize noisy functions online while maintaining safety during learning. It provides a complete convergence analysis for high dimensional non-linear stochastic optimization problems with first-order and zeroth-order feedback, including non-convex, convex, and strongly-convex smooth constrained problems."
}