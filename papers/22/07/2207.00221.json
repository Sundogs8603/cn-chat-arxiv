{
    "title": "VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v2 [cs.CV] UPDATED)",
    "abstract": "Vision-Language Pretraining (VLP) models have recently successfully facilitated many cross-modal downstream tasks. Most existing works evaluated their systems by comparing the fine-tuned downstream task performance. However, only average downstream task accuracy provides little information about the pros and cons of each VLP method, let alone provides insights on how the community can improve the systems in the future. Inspired by the CheckList for testing natural language processing, we exploit VL-CheckList, a novel framework to understand the capabilities of VLP models. The proposed method divides the image-texting ability of a VLP model into three categories: objects, attributes, and relations, and uses a novel taxonomy to further break down these three aspects. We conduct comprehensive studies to analyze seven recently popular VLP models via the proposed framework. Results confirm the effectiveness of the proposed method by revealing fine-grained differences among the compared mode",
    "link": "http://arxiv.org/abs/2207.00221",
    "context": "Title: VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v2 [cs.CV] UPDATED)\nAbstract: Vision-Language Pretraining (VLP) models have recently successfully facilitated many cross-modal downstream tasks. Most existing works evaluated their systems by comparing the fine-tuned downstream task performance. However, only average downstream task accuracy provides little information about the pros and cons of each VLP method, let alone provides insights on how the community can improve the systems in the future. Inspired by the CheckList for testing natural language processing, we exploit VL-CheckList, a novel framework to understand the capabilities of VLP models. The proposed method divides the image-texting ability of a VLP model into three categories: objects, attributes, and relations, and uses a novel taxonomy to further break down these three aspects. We conduct comprehensive studies to analyze seven recently popular VLP models via the proposed framework. Results confirm the effectiveness of the proposed method by revealing fine-grained differences among the compared mode",
    "path": "papers/22/07/2207.00221.json",
    "total_tokens": 890,
    "translated_title": "VL-CheckList: 使用物体、属性和关系评估预训练的视觉语言模型",
    "translated_abstract": "最近，视觉语言预训练（VLP）模型已经成功地促进了许多跨模态的下游任务。然而，现有的大多数工作都是通过比较下游任务的性能来评估它们的系统。然而，仅有的下游任务平均准确性提供很少关于每种VLP方法的优缺点的信息，更不用说为社区在未来如何改进系统提供见解了。受自然语言处理测试CheckList的启发，我们提出了VL-CheckList，这是一个新颖的框架，用于了解VLP模型的能力。所提出的方法将VLP模型的图像-文本能力分为三类：物体、属性和关系，并使用新颖的分类法进一步分解这三个方面。我们通过该框架对七种最近流行的VLP模型进行全面研究分析。结果通过揭示比较模型之间的细微差异来确认所提出方法的有效性。",
    "tldr": "本研究提出VL-CheckList，使用物体、属性和关系评估预训练的视觉语言模型，通过对七种流行的VLP模型进行全面研究分析，揭示出不同模型之间的细微差异。",
    "en_tdlr": "This paper proposes VL-CheckList, a novel framework to evaluate pre-trained vision-language models based on objects, attributes, and relations. By analyzing seven popular VLP models using this framework, the paper reveals fine-grained differences among the models."
}