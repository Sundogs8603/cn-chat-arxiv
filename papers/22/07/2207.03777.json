{
    "title": "Hidden Schema Networks. (arXiv:2207.03777v2 [cs.CL] UPDATED)",
    "abstract": "Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects ",
    "link": "http://arxiv.org/abs/2207.03777",
    "context": "Title: Hidden Schema Networks. (arXiv:2207.03777v2 [cs.CL] UPDATED)\nAbstract: Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects ",
    "path": "papers/22/07/2207.03777.json",
    "total_tokens": 967,
    "translated_title": "隐藏结构网络",
    "translated_abstract": "大型预训练语言模型可以推断出强大的表示形式，其中蕴含着丰富的语义和句法内容，尽管是隐含的。本文介绍了一种新颖的神经语言模型，通过归纳偏见强制执行明确的关系结构，从而将预训练语言模型的输出表示显式地组成。具体而言，该模型将句子编码为符号序列（组合表示），这些符号对应于全局潜在图上带偏置的随机游走器访问的节点，并推断其后验分布。我们首先展示了该模型能够从人工生成的随机标记序列数据集中发现隐含的真实图。接下来，我们利用预训练的BERT和GPT-2语言模型作为编码器和解码器，从自然语言数据集中推断出符号网络（模式）。我们的实验证明了：（i）推断出的符号可以解释为编码不同方面的含义，（ii）它们的复合性直接反映了语言的基础句法结构。",
    "tldr": "本文介绍了一种新颖的神经语言模型，通过归纳偏见强制执行明确的关系结构，从而将预训练语言模型的输出表示显式地组成。该模型可以从随机标记序列数据集中发现隐含的真实图，在自然语言数据集中推断出符号网络（模式），直接反映了语言的基础句法结构。"
}