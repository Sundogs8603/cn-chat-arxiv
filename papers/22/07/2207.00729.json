{
    "title": "The Parallelism Tradeoff: Limitations of Log-Precision Transformers. (arXiv:2207.00729v4 [cs.CC] UPDATED)",
    "abstract": "Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if $\\mathsf L \\neq \\mathsf P$ (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture's high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitati",
    "link": "http://arxiv.org/abs/2207.00729",
    "context": "Title: The Parallelism Tradeoff: Limitations of Log-Precision Transformers. (arXiv:2207.00729v4 [cs.CC] UPDATED)\nAbstract: Despite their omnipresence in modern NLP, characterizing the computational power of transformer neural nets remains an interesting open question. We prove that transformers whose arithmetic precision is logarithmic in the number of input tokens (and whose feedforward nets are computable using space linear in their input) can be simulated by constant-depth logspace-uniform threshold circuits. This provides insight on the power of transformers using known results in complexity theory. For example, if $\\mathsf L \\neq \\mathsf P$ (i.e., not all poly-time problems can be solved using logarithmic space), then transformers cannot even accurately solve linear equalities or check membership in an arbitrary context-free grammar with empty productions. Our result intuitively emerges from the transformer architecture's high parallelizability. We thus speculatively introduce the idea of a fundamental parallelism tradeoff: any model architecture as parallelizable as the transformer will obey limitati",
    "path": "papers/22/07/2207.00729.json",
    "total_tokens": 922,
    "translated_title": "并行性掉价：对对数精度Transformer的限制",
    "translated_abstract": "尽管Transformer神经网络在现代自然语言处理中无处不在，但描述其计算能力仍然是一个有趣的开放问题。我们证明了，在输入令牌数的对数精度下（且其前馈网络可使用其输入的线性空间计算），可以通过常数深度的对数空间均匀阈值电路来模拟Transformer。这提供了使用复杂性理论中已知结果来了解Transformer功率的见解。例如，如果$\\mathsf L \\neq \\mathsf P$（即，不是所有的多项式时间问题都可以使用对数空间解决），那么Transformer甚至不能准确地解决线性等式或检查任意无上下文文法的成员资格。我们的结果从Transformer架构的高并行性中直观地出现。因此，我们猜测地介绍了一个基本的并行性掉价的想法：任何像Transformer一样可并行化的模型架构都将遵守精度的限制，而具有更高精度的模型将固有地不太可并行化。",
    "tldr": "证明了在对数精度下，Transformer的计算能力存在限制，这是因为Transformer架构的高并行性使其遵守一个基本的并行性掉价，即模型架构越可并行化，就越会受到精度的限制。",
    "en_tdlr": "It is proven that the computational power of Transformers is limited by precision under logarithmic accuracy due to the high parallelism of the Transformer's architecture, which follows a fundamental parallelism tradeoff where models with higher precision will inherently be less parallelizable."
}