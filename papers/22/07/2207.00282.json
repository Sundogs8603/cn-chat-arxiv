{
    "title": "(Un)likelihood Training for Interpretable Embedding. (arXiv:2207.00282v2 [cs.CV] UPDATED)",
    "abstract": "Cross-modal representation learning has become a new normal for bridging the semantic gap between text and visual data. Learning modality agnostic representations in a continuous latent space, however, is often treated as a black-box data-driven training process. It is well-known that the effectiveness of representation learning depends heavily on the quality and scale of training data. For video representation learning, having a complete set of labels that annotate the full spectrum of video content for training is highly difficult if not impossible. These issues, black-box training and dataset bias, make representation learning practically challenging to be deployed for video understanding due to unexplainable and unpredictable results. In this paper, we propose two novel training objectives, likelihood and unlikelihood functions, to unroll semantics behind embeddings while addressing the label sparsity problem in training. The likelihood training aims to interpret semantics of embed",
    "link": "http://arxiv.org/abs/2207.00282",
    "context": "Title: (Un)likelihood Training for Interpretable Embedding. (arXiv:2207.00282v2 [cs.CV] UPDATED)\nAbstract: Cross-modal representation learning has become a new normal for bridging the semantic gap between text and visual data. Learning modality agnostic representations in a continuous latent space, however, is often treated as a black-box data-driven training process. It is well-known that the effectiveness of representation learning depends heavily on the quality and scale of training data. For video representation learning, having a complete set of labels that annotate the full spectrum of video content for training is highly difficult if not impossible. These issues, black-box training and dataset bias, make representation learning practically challenging to be deployed for video understanding due to unexplainable and unpredictable results. In this paper, we propose two novel training objectives, likelihood and unlikelihood functions, to unroll semantics behind embeddings while addressing the label sparsity problem in training. The likelihood training aims to interpret semantics of embed",
    "path": "papers/22/07/2207.00282.json",
    "total_tokens": 1056,
    "translated_title": "可解释嵌入的(不)可能性训练",
    "translated_abstract": "跨模态表示学习已成为弥合文本和视觉数据语义差距的新常态。然而，在连续潜在空间中学习模态不可知表示经常被视为黑匣子数据驱动的训练过程。深度表示学习的有效性严重依赖于训练数据的质量和规模。对于视频表示学习，要完整地标注视频内容的数据集是高度困难甚至不可能的。这些问题，黑匣子训练和数据集偏差，使得解释性较差和结果难以预测，难以在视频理解方面进行实际应用。在本文中，我们提出了两种新的训练方法，可能性和不可能性函数，以展示嵌入背后的语义，并解决训练中的标签稀疏问题。可能性训练旨在通过学习数据分布来解释嵌入向量的语义，而不可能性训练则强调正负对之间的差异。我们将所提出的方法应用于各种任务，包括图像和视频分类、检索和生成，并展示了它们在提高学到的嵌入的可解释性以及在基准数据集上实现竞争性性能方面的有效性。",
    "tldr": "该论文提出了两种新的训练方法：可能性训练和不可能性训练，以解释嵌入向量背后的语义并解决标签稀疏问题。这些方法在图像和视频分类、检索和生成任务中表现出色，提高了学习嵌入的可解释性。",
    "en_tdlr": "This paper proposes two novel training objectives, likelihood and unlikelihood functions, to interpret the semantics behind embeddings and address the label sparsity problem. The proposed methods are demonstrated to improve the interpretability of learned embeddings and achieve competitive performance on various tasks, including image and video classification, retrieval, and generation."
}