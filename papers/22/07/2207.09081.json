{
    "title": "Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. (arXiv:2207.09081v6 [cs.LG] UPDATED)",
    "abstract": "As a pivotal component to attaining generalizable solutions in human intelligence, reasoning provides great potential for reinforcement learning (RL) agents' generalization towards varied goals by summarizing part-to-whole arguments and discovering cause-and-effect relations. However, how to discover and represent causalities remains a huge gap that hinders the development of causal RL. In this paper, we augment Goal-Conditioned RL (GCRL) with Causal Graph (CG), a structure built upon the relation between objects and events. We novelly formulate the GCRL problem into variational likelihood maximization with CG as latent variables. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventional data to estimate the posterior of CG; using CG to learn generalizable models and interpretable policies. Due to the lack of public benchmarks that verify generalization capability under reasoning, we design ",
    "link": "http://arxiv.org/abs/2207.09081",
    "context": "Title: Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning. (arXiv:2207.09081v6 [cs.LG] UPDATED)\nAbstract: As a pivotal component to attaining generalizable solutions in human intelligence, reasoning provides great potential for reinforcement learning (RL) agents' generalization towards varied goals by summarizing part-to-whole arguments and discovering cause-and-effect relations. However, how to discover and represent causalities remains a huge gap that hinders the development of causal RL. In this paper, we augment Goal-Conditioned RL (GCRL) with Causal Graph (CG), a structure built upon the relation between objects and events. We novelly formulate the GCRL problem into variational likelihood maximization with CG as latent variables. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventional data to estimate the posterior of CG; using CG to learn generalizable models and interpretable policies. Due to the lack of public benchmarks that verify generalization capability under reasoning, we design ",
    "path": "papers/22/07/2207.09081.json",
    "total_tokens": 840,
    "translated_title": "引入变分因果推理的目标条件强化学习的泛化",
    "translated_abstract": "推理作为人类智能中达成通用解决方案的关键部分，通过总结部分到整体的参数和发现因果关系，为强化学习（RL）代理人实现向各种目标的泛化提供了巨大的潜力。然而，如何发现和表示因果关系仍然是阻碍因果RL发展的重大障碍。在本文中，我们通过使用因果图结构来增强目标条件RL（GCRL），该结构建立在对象和事件之间的关系上。我们将GCRL问题新颖地制定为具有CG作为潜在变量的变分似然最大化。为了优化派生目标，我们提出了一个具有理论性能保证的框架，交替使用干预数据来估计CG的后验概率，使用CG来学习通用模型和可解释的策略。由于缺乏验证推理下泛化能力的公共基准，我们设计了...",
    "tldr": "本文提出了一种增强的目标条件强化学习框架，它使用因果图来发现和表示因果关系来实现模型的泛化性。",
    "en_tdlr": "This paper proposes an enhanced framework for goal-conditioned reinforcement learning which uses causal graphs to discover and represent causal relationships and achieve model generalization."
}