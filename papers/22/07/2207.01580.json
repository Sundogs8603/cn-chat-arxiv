{
    "title": "Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks. (arXiv:2207.01580v2 [cs.CV] UPDATED)",
    "abstract": "In this paper, we present a new approach for model acceleration by exploiting spatial sparsity in visual data. We observe that the final prediction in vision Transformers is only based on a subset of the most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input to accelerate vision Transformers. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. While the framework is inspired by our observation of the sparse attention in vision Transformers, we find the idea of adaptive and asymmetric computation can be a general solution for accelerating various architectures. We extend our method to hierarchical models including CNNs and hierarchical vision Transformers ",
    "link": "http://arxiv.org/abs/2207.01580",
    "context": "Title: Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks. (arXiv:2207.01580v2 [cs.CV] UPDATED)\nAbstract: In this paper, we present a new approach for model acceleration by exploiting spatial sparsity in visual data. We observe that the final prediction in vision Transformers is only based on a subset of the most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input to accelerate vision Transformers. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. While the framework is inspired by our observation of the sparse attention in vision Transformers, we find the idea of adaptive and asymmetric computation can be a general solution for accelerating various architectures. We extend our method to hierarchical models including CNNs and hierarchical vision Transformers ",
    "path": "papers/22/07/2207.01580.json",
    "total_tokens": 930,
    "translated_title": "动态空间稀疏化：高效视觉变压器和卷积神经网络的新方法",
    "translated_abstract": "本文提出了一种利用视觉数据中的空间稀疏性加速模型的新方法。我们观察到视觉变压器中最终预测仅基于一小部分信息最丰富的标记，这对于准确的图像识别足够。基于这一观察，我们提出了一种动态标记稀疏化框架，根据输入逐步和动态地剪枝冗余标记以加速视觉变压器。具体而言，我们设计了一个轻量级预测模块，以根据当前特征来估计每个标记的重要性得分。该模块被添加到不同层中以分层地剪枝冗余标记。虽然该框架来源于我们对视觉变压器中稀疏注意力的观察，但我们发现自适应和非对称计算的思想可以成为加速各种结构的一般解决方案。我们将我们的方法扩展到包括卷积神经网络和分层视觉变压器在内的分层模型中。",
    "tldr": "基于视觉变压器中的空间稀疏性，本文提出了一种动态标记稀疏化框架，可加速各种结构的模型，被剪枝的冗余标记由轻量级预测模块逐步动态估计。",
    "en_tdlr": "This paper proposes a dynamic token sparsification framework based on the spatial sparsity in visual data, which can accelerate various models by progressively and dynamically pruning redundant tokens estimated by a lightweight prediction module added to different layers."
}