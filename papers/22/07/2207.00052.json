{
    "title": "Visual Pre-training for Navigation: What Can We Learn from Noise?. (arXiv:2207.00052v3 [cs.CV] UPDATED)",
    "abstract": "One powerful paradigm in visual navigation is to predict actions from observations directly. Training such an end-to-end system allows representations useful for downstream tasks to emerge automatically. However, the lack of inductive bias makes this system data inefficient. We hypothesize a sufficient representation of the current view and the goal view for a navigation policy can be learned by predicting the location and size of a crop of the current view that corresponds to the goal. We further show that training such random crop prediction in a self-supervised fashion purely on synthetic noise images transfers well to natural home images. The learned representation can then be bootstrapped to learn a navigation policy efficiently with little interaction data. The code is available at https://yanweiw.github.io/noise2ptz",
    "link": "http://arxiv.org/abs/2207.00052",
    "context": "Title: Visual Pre-training for Navigation: What Can We Learn from Noise?. (arXiv:2207.00052v3 [cs.CV] UPDATED)\nAbstract: One powerful paradigm in visual navigation is to predict actions from observations directly. Training such an end-to-end system allows representations useful for downstream tasks to emerge automatically. However, the lack of inductive bias makes this system data inefficient. We hypothesize a sufficient representation of the current view and the goal view for a navigation policy can be learned by predicting the location and size of a crop of the current view that corresponds to the goal. We further show that training such random crop prediction in a self-supervised fashion purely on synthetic noise images transfers well to natural home images. The learned representation can then be bootstrapped to learn a navigation policy efficiently with little interaction data. The code is available at https://yanweiw.github.io/noise2ptz",
    "path": "papers/22/07/2207.00052.json",
    "total_tokens": 804,
    "translated_title": "视觉预训练用于导航：从噪声中我们能学到什么？",
    "translated_abstract": "在视觉导航中，一种强大的范式是从观察中直接预测行为。训练这样一个端到端的系统可以自动产生对下游任务有用的表示。然而，缺乏归纳偏差使得该系统数据效率低下。我们假设通过预测与目标对应的当前视图裁剪的位置和大小，可以学习到导航策略所需的当前视图和目标视图的充分表示。我们进一步展示，在自监督的方式下，仅使用合成噪声图像进行随机裁剪预测的训练可以很好地迁移到自然家庭图像。然后，可以利用学到的表示有效地自举学习导航策略，减少交互数据的需求。",
    "tldr": "本论文提出了一种使用随机裁剪预测进行自监督训练的视觉预训练方法，可以学习到对导航任务有用的表示，并通过自举学习有效地学习导航策略，减少对交互数据的需求。"
}