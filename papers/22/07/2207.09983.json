{
    "title": "Diffsound: Discrete Diffusion Model for Text-to-sound Generation. (arXiv:2207.09983v2 [cs.SD] UPDATED)",
    "abstract": "Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the AR decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidi",
    "link": "http://arxiv.org/abs/2207.09983",
    "context": "Title: Diffsound: Discrete Diffusion Model for Text-to-sound Generation. (arXiv:2207.09983v2 [cs.SD] UPDATED)\nAbstract: Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the AR decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidi",
    "path": "papers/22/07/2207.09983.json",
    "total_tokens": 721,
    "translated_title": "Diffsound：离散扩散模型生成文本到音频",
    "translated_abstract": "人们对于所需声效的生成是一个重要的研究领域，本研究提出一种新的文本到音频生成框架，包括文本编码器，矢量量化变分自编码器(VQ-VAE)，解码器和语音编码器。实验结果表明，在设计解码器方面，传统的自回归解码器预测的实时频谱存在不平滑情况，不适用于该任务，因此我们提出了一种新的离散扩散解码器。",
    "tldr": "本研究提出了一种新的文本到音频生成框架，其中，我们采用了离散扩散解码器来增强生成性能。",
    "en_tdlr": "This paper proposes a novel text-to-sound generation framework using a discrete diffusion decoder for better generation performance."
}