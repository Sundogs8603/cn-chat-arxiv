{
    "title": "Memory Population in Continual Learning via Outlier Elimination. (arXiv:2207.01145v3 [cs.LG] UPDATED)",
    "abstract": "Catastrophic forgetting, the phenomenon of forgetting previously learned tasks when learning a new one, is a major hurdle in developing continual learning algorithms. A popular method to alleviate forgetting is to use a memory buffer, which stores a subset of previously learned task examples for use during training on new tasks. The de facto method of filling memory is by randomly selecting previous examples. However, this process could introduce outliers or noisy samples that could hurt the generalization of the model. This paper introduces Memory Outlier Elimination (MOE), a method for identifying and eliminating outliers in the memory buffer by choosing samples from label-homogeneous subpopulations. We show that a space with a high homogeneity is related to a feature space that is more representative of the class distribution. In practice, MOE removes a sample if it is surrounded by samples from different labels. We demonstrate the effectiveness of MOE on CIFAR-10, CIFAR-100, and CO",
    "link": "http://arxiv.org/abs/2207.01145",
    "context": "Title: Memory Population in Continual Learning via Outlier Elimination. (arXiv:2207.01145v3 [cs.LG] UPDATED)\nAbstract: Catastrophic forgetting, the phenomenon of forgetting previously learned tasks when learning a new one, is a major hurdle in developing continual learning algorithms. A popular method to alleviate forgetting is to use a memory buffer, which stores a subset of previously learned task examples for use during training on new tasks. The de facto method of filling memory is by randomly selecting previous examples. However, this process could introduce outliers or noisy samples that could hurt the generalization of the model. This paper introduces Memory Outlier Elimination (MOE), a method for identifying and eliminating outliers in the memory buffer by choosing samples from label-homogeneous subpopulations. We show that a space with a high homogeneity is related to a feature space that is more representative of the class distribution. In practice, MOE removes a sample if it is surrounded by samples from different labels. We demonstrate the effectiveness of MOE on CIFAR-10, CIFAR-100, and CO",
    "path": "papers/22/07/2207.01145.json",
    "total_tokens": 877,
    "translated_title": "通过异常值消除在连续学习中的内存填充问题",
    "translated_abstract": "连续学习中的灾难性遗忘是发展连续学习算法的主要障碍。缓解遗忘的一种常见方法是使用内存缓冲区，在训练新任务时存储之前学习过的任务示例的子集。填充内存的默认方法是随机选择先前的示例，但这可能引入离群值或噪声样本，从而损害模型的泛化能力。本文介绍了一种名为内存异常值消除（MOE）的方法，通过选择来自标签同质子种群的样本来识别和消除内存缓冲区中的异常值。我们展示了高同质性的空间与更具代表性的特征空间相关联的关系。实际上，如果一个样本被不同标签的样本所包围，MOE会移除该样本。我们在CIFAR-10、CIFAR-100和CO数据集上证明了MOE的有效性。",
    "tldr": "本文引入了一种名为内存异常值消除（MOE）的方法，通过选择来自标签同质子种群的样本来识别和消除内存缓冲区中的异常值，以解决连续学习中的遗忘问题。"
}