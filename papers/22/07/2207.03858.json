{
    "title": "DSTEA: Improving Dialogue State Tracking via Entity Adaptive Pre-training. (arXiv:2207.03858v2 [cs.CL] UPDATED)",
    "abstract": "Dialogue State Tracking (DST) is critical for comprehensively interpreting user and system utterances, thereby forming the cornerstone of efficient dialogue systems. Despite past research efforts focused on enhancing DST performance through alterations to the model structure or integrating additional features like graph relations, they often require additional pre-training with external dialogue corpora. In this study, we propose DSTEA, improving Dialogue State Tracking via Entity Adaptive pre-training, which can enhance the encoder through by intensively training key entities in dialogue utterances. DSTEA identifies these pivotal entities from input dialogues utilizing four different methods: ontology information, named-entity recognition, the spaCy, and the flair library. Subsequently, it employs selective knowledge masking to train the model effectively. Remarkably, DSTEA only requires pre-training without the direct infusion of extra knowledge into the DST model. This approach resu",
    "link": "http://arxiv.org/abs/2207.03858",
    "context": "Title: DSTEA: Improving Dialogue State Tracking via Entity Adaptive Pre-training. (arXiv:2207.03858v2 [cs.CL] UPDATED)\nAbstract: Dialogue State Tracking (DST) is critical for comprehensively interpreting user and system utterances, thereby forming the cornerstone of efficient dialogue systems. Despite past research efforts focused on enhancing DST performance through alterations to the model structure or integrating additional features like graph relations, they often require additional pre-training with external dialogue corpora. In this study, we propose DSTEA, improving Dialogue State Tracking via Entity Adaptive pre-training, which can enhance the encoder through by intensively training key entities in dialogue utterances. DSTEA identifies these pivotal entities from input dialogues utilizing four different methods: ontology information, named-entity recognition, the spaCy, and the flair library. Subsequently, it employs selective knowledge masking to train the model effectively. Remarkably, DSTEA only requires pre-training without the direct infusion of extra knowledge into the DST model. This approach resu",
    "path": "papers/22/07/2207.03858.json",
    "total_tokens": 932,
    "translated_title": "DSTEA：通过实体自适应预训练来改进对话状态跟踪",
    "translated_abstract": "对话状态跟踪(DST)对于全面解释用户和系统话语以形成高效对话系统的基础至关重要。尽管过去的研究努力集中在通过改变模型结构或集成图关系等额外特征来提升DST性能，但它们常常需要额外的预训练与外部对话语料库。在本研究中，我们提出了DSTEA，通过实体自适应预训练来改进对话状态跟踪，可以通过密集训练对话话语中的关键实体来增强编码器。DSTEA利用四种不同的方法从输入对话中识别出这些关键实体：本体信息、命名实体识别、spaCy和flair library。随后，它采用选择性知识遮罩来有效训练模型。值得注意的是，DSTEA只需要进行预训练，而无需将额外的知识直接注入DST模型中。这种方法产生了出色的效果。",
    "tldr": "本研究提出了DSTEA方法，通过实体自适应预训练来改进对话状态跟踪。它通过密集训练对话话语中的关键实体，而无需外部对话语料库，并且只需要进行预训练而不直接注入额外的知识到DST模型中。这种方法取得了出色的效果。",
    "en_tdlr": "This study proposes DSTEA, a method that improves dialogue state tracking through entity adaptive pre-training. It enhances the encoder by intensively training key entities in dialogue utterances, without the need for external dialogue corpora and with only pre-training instead of directly injecting extra knowledge into the DST model. This approach achieves remarkable performance."
}