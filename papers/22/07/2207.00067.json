{
    "title": "Rethinking Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2207.00067v3 [cs.CV] UPDATED)",
    "abstract": "Unsupervised domain adaptation (UDA) adapts a model trained on one domain (called source) to a novel domain (called target) using only unlabeled data. Due to its high annotation cost, researchers have developed many UDA methods for semantic segmentation, which assume no labeled sample is available in the target domain. We question the practicality of this assumption for two reasons. First, after training a model with a UDA method, we must somehow verify the model before deployment. Second, UDA methods have at least a few hyper-parameters that need to be determined. The surest solution to these is to evaluate the model using validation data, i.e., a certain amount of labeled target-domain samples. This question about the basic assumption of UDA leads us to rethink UDA from a data-centric point of view. Specifically, we assume we have access to a minimum level of labeled data. Then, we ask how much is necessary to find good hyper-parameters of existing UDA methods. We then consider what ",
    "link": "http://arxiv.org/abs/2207.00067",
    "context": "Title: Rethinking Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2207.00067v3 [cs.CV] UPDATED)\nAbstract: Unsupervised domain adaptation (UDA) adapts a model trained on one domain (called source) to a novel domain (called target) using only unlabeled data. Due to its high annotation cost, researchers have developed many UDA methods for semantic segmentation, which assume no labeled sample is available in the target domain. We question the practicality of this assumption for two reasons. First, after training a model with a UDA method, we must somehow verify the model before deployment. Second, UDA methods have at least a few hyper-parameters that need to be determined. The surest solution to these is to evaluate the model using validation data, i.e., a certain amount of labeled target-domain samples. This question about the basic assumption of UDA leads us to rethink UDA from a data-centric point of view. Specifically, we assume we have access to a minimum level of labeled data. Then, we ask how much is necessary to find good hyper-parameters of existing UDA methods. We then consider what ",
    "path": "papers/22/07/2207.00067.json",
    "total_tokens": 962,
    "translated_title": "重新思考语义分割的无监督领域自适应",
    "translated_abstract": "无监督领域自适应（UDA）使用无标签数据将在一个领域中训练的模型（称为源领域）适应到一个新领域（称为目标领域）。由于注释成本高昂，研究人员开发了许多用于语义分割的UDA方法，这些方法假设目标领域中没有可用的标记样本。我们对这种假设的可行性提出了质疑，主要有两个原因。首先，在使用UDA方法训练模型之后，我们必须在部署之前对模型进行验证。其次，UDA方法至少有几个需要确定的超参数。解决这些问题的最确保的方法是使用验证数据评估模型，即一定数量的标记的目标领域样本。对UDA的这一基本假设的质疑使我们重新从数据中心的角度思考UDA。具体而言，我们假设我们可以访问一定水平的标记数据。然后，我们询问需要多少数据来找到现有UDA方法的良好超参数。然后，我们考虑在这个基础上可以对UDA方法进行什么样的改进。",
    "tldr": "这项研究重新思考了语义分割的无监督领域自适应方法，提出了从数据中心的角度重新考虑无监督领域自适应问题，并探讨了使用最少的标记数据来确定UDA方法超参数的方法。",
    "en_tdlr": "This research rethinks unsupervised domain adaptation methods for semantic segmentation and proposes a data-centric approach to address the basic assumption of UDA. It also explores the use of minimal labeled data to determine the hyperparameters of existing UDA methods."
}