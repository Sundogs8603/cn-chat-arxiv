{
    "title": "Gradient-based Bi-level Optimization for Deep Learning: A Survey. (arXiv:2207.11719v4 [cs.LG] UPDATED)",
    "abstract": "Bi-level optimization, especially the gradient-based category, has been widely used in the deep learning community including hyperparameter optimization and meta-knowledge extraction. Bi-level optimization embeds one problem within another and the gradient-based category solves the outer-level task by computing the hypergradient, which is much more efficient than classical methods such as the evolutionary algorithm. In this survey, we first give a formal definition of the gradient-based bi-level optimization. Next, we delineate criteria to determine if a research problem is apt for bi-level optimization and provide a practical guide on structuring such problems into a bi-level optimization framework, a feature particularly beneficial for those new to this domain. More specifically, there are two formulations: the single-task formulation to optimize hyperparameters such as regularization parameters and the distilled data, and the multi-task formulation to extract meta-knowledge such as ",
    "link": "http://arxiv.org/abs/2207.11719",
    "context": "Title: Gradient-based Bi-level Optimization for Deep Learning: A Survey. (arXiv:2207.11719v4 [cs.LG] UPDATED)\nAbstract: Bi-level optimization, especially the gradient-based category, has been widely used in the deep learning community including hyperparameter optimization and meta-knowledge extraction. Bi-level optimization embeds one problem within another and the gradient-based category solves the outer-level task by computing the hypergradient, which is much more efficient than classical methods such as the evolutionary algorithm. In this survey, we first give a formal definition of the gradient-based bi-level optimization. Next, we delineate criteria to determine if a research problem is apt for bi-level optimization and provide a practical guide on structuring such problems into a bi-level optimization framework, a feature particularly beneficial for those new to this domain. More specifically, there are two formulations: the single-task formulation to optimize hyperparameters such as regularization parameters and the distilled data, and the multi-task formulation to extract meta-knowledge such as ",
    "path": "papers/22/07/2207.11719.json",
    "total_tokens": 879,
    "translated_title": "基于梯度的双层优化在深度学习中的应用调查",
    "translated_abstract": "双层优化，特别是基于梯度的方法，在深度学习社区中被广泛应用于超参数优化和元知识提取。双层优化将一个问题嵌套在另一个问题之内，基于梯度的方法通过计算超梯度来解决外层任务，比传统方法如进化算法更高效。在这项调查中，我们首先给出了基于梯度的双层优化的形式化定义。接下来，我们界定了决定研究问题是否适合双层优化的标准，并提供了一个实用指南，指导将这些问题结构化为双层优化框架，这对于新手来说尤为有益。具体而言，有两种形式：单任务形式用于优化正则化参数和经过提取的数据，多任务形式用于提取元知识等。",
    "tldr": "基于梯度的双层优化方法在深度学习领域中被广泛应用，特别适用于超参数优化和元知识提取。本文提供了基于梯度的双层优化的定义、适用标准以及实用指南，对于初学者尤其有帮助。",
    "en_tdlr": "Gradient-based bi-level optimization is widely used in the deep learning community for hyperparameter optimization and meta-knowledge extraction. This survey provides a formal definition, criteria for problem suitability, and a practical guide for structuring problems into a bi-level optimization framework, particularly beneficial for beginners."
}