{
    "title": "S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning. (arXiv:2207.12819v2 [cs.CV] UPDATED)",
    "abstract": "State-of-the-art deep neural networks are still struggling to address the catastrophic forgetting problem in continual learning. In this paper, we propose one simple paradigm (named as S-Prompting) and two concrete approaches to highly reduce the forgetting degree in one of the most typical continual learning scenarios, i.e., domain increment learning (DIL). The key idea of the paradigm is to learn prompts independently across domains with pre-trained transformers, avoiding the use of exemplars that commonly appear in conventional methods. This results in a win-win game where the prompting can achieve the best for each domain. The independent prompting across domains only requests one single cross-entropy loss for training and one simple K-NN operation as a domain identifier for inference. The learning paradigm derives an image prompt learning approach and a novel language-image prompt learning approach. Owning an excellent scalability (0.03% parameter increase per domain), the best of",
    "link": "http://arxiv.org/abs/2207.12819",
    "context": "Title: S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning. (arXiv:2207.12819v2 [cs.CV] UPDATED)\nAbstract: State-of-the-art deep neural networks are still struggling to address the catastrophic forgetting problem in continual learning. In this paper, we propose one simple paradigm (named as S-Prompting) and two concrete approaches to highly reduce the forgetting degree in one of the most typical continual learning scenarios, i.e., domain increment learning (DIL). The key idea of the paradigm is to learn prompts independently across domains with pre-trained transformers, avoiding the use of exemplars that commonly appear in conventional methods. This results in a win-win game where the prompting can achieve the best for each domain. The independent prompting across domains only requests one single cross-entropy loss for training and one simple K-NN operation as a domain identifier for inference. The learning paradigm derives an image prompt learning approach and a novel language-image prompt learning approach. Owning an excellent scalability (0.03% parameter increase per domain), the best of",
    "path": "papers/22/07/2207.12819.json",
    "total_tokens": 958,
    "translated_title": "具有预训练变压器的S-Prompts学习：领域增量学习的奥卡姆剃刀",
    "translated_abstract": "目前的深度神经网络仍然无法解决连续学习中的灾难性遗忘问题。在本文中，我们提出了一种简单的范例（称为S-Prompting）和两种具体方法，极大地减少了其中一种最典型的连续学习场景（即领域增量学习）中的遗忘程度。该范例的关键思想是使用预先训练的变压器独立地跨越不同领域学习提示，并避免常见于传统方法中的示例使用。这导致了一个双赢的局面，即提示可以为每个领域取得最佳效果。跨越领域的独立提示仅需要一次交叉熵损失进行训练，并需要一个简单的K-NN操作作为推理的领域标识符。该学习范例派生了一种图像提示学习方法和一种新颖的语言-图像提示学习方法。具有出色的可伸缩性（每个领域增加0.03％的参数），是现有最佳方法之一。",
    "tldr": "该论文提出了一种名为\"S-Prompting\"的学习范例，使用预先训练的变压器独立地跨越不同领域学习提示，大幅度减少连续学习中的遗忘问题，并具有出色的可适应性。",
    "en_tdlr": "This paper proposes a learning paradigm named \"S-Prompting\" that uses pre-trained transformers to learn prompts independently across domains, greatly reducing the forgetting problem in continual learning and having excellent scalability."
}