{
    "title": "Big Learning. (arXiv:2207.03899v4 [cs.LG] UPDATED)",
    "abstract": "Recent advances in big/foundation models reveal a promising path for deep learning, where the roadmap steadily moves from big data to big models to (the newly-introduced) big learning. Specifically, the big learning exhaustively exploits the information inherent in its large-scale complete/incomplete training data, by simultaneously modeling many/all joint/conditional/marginal data distributions across potentially diverse domains, with one universal foundation model. We reveal that big learning ($i$) underlies most existing foundation models, ($ii$) is equipped with extraordinary flexibilities for complete/incomplete training data and trustworthy data tasks, ($iii$) is capable of delivering all joint/conditional/marginal data capabilities with one universal model, and ($iv$) unifies conventional machine learning paradigms and enables their flexible cooperations, manifested as a universal learning paradigm. Diverse experiments are carried out to validate the effectiveness of the present",
    "link": "http://arxiv.org/abs/2207.03899",
    "context": "Title: Big Learning. (arXiv:2207.03899v4 [cs.LG] UPDATED)\nAbstract: Recent advances in big/foundation models reveal a promising path for deep learning, where the roadmap steadily moves from big data to big models to (the newly-introduced) big learning. Specifically, the big learning exhaustively exploits the information inherent in its large-scale complete/incomplete training data, by simultaneously modeling many/all joint/conditional/marginal data distributions across potentially diverse domains, with one universal foundation model. We reveal that big learning ($i$) underlies most existing foundation models, ($ii$) is equipped with extraordinary flexibilities for complete/incomplete training data and trustworthy data tasks, ($iii$) is capable of delivering all joint/conditional/marginal data capabilities with one universal model, and ($iv$) unifies conventional machine learning paradigms and enables their flexible cooperations, manifested as a universal learning paradigm. Diverse experiments are carried out to validate the effectiveness of the present",
    "path": "papers/22/07/2207.03899.json",
    "total_tokens": 862,
    "translated_title": "大型学习",
    "translated_abstract": "大规模/基础模型的最新进展为深度学习开辟了一条充满希望的道路，其中路线图稳步从大数据到大模型到（新引入的）大型学习。具体而言，大型学习通过同步建模可能存在的多个/全部联合/条件/边际数据分布，利用大规模完整/不完整的训练数据中固有的信息，使用一个通用基础模型。我们揭示大型学习具有以下特点：（i）是大多数现有基础模型的基础；（ii）具有对完整/不完整的训练数据和可信数据任务的非凡灵活性；（iii）能够使用一个通用模型提供所有联合/条件/边际数据能力；（iv）统一传统机器学习范式并启用它们的灵活协作，体现为通用学习范式。进行了不同的实验，验证了当前方法的有效性。",
    "tldr": "大型学习是一种把大规模完整/不完整的训练数据中固有信息同步建模并使用一个通用基础模型，利用所有联合/条件/边际数据能力并统一传统机器学习范式的通用学习范式。",
    "en_tdlr": "Big learning is a universal learning paradigm that synchronously models the inherent information in large-scale complete/incomplete training data by using one universal foundation model and provides all joint/conditional/marginal data capabilities while unifying conventional machine learning paradigms."
}