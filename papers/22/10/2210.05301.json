{
    "title": "Intrinsic Dimension for Large-Scale Geometric Learning. (arXiv:2210.05301v2 [cs.LG] UPDATED)",
    "abstract": "The concept of dimension is essential to grasp the complexity of data. A naive approach to determine the dimension of a dataset is based on the number of attributes. More sophisticated methods derive a notion of intrinsic dimension (ID) that employs more complex feature functions, e.g., distances between data points. Yet, many of these approaches are based on empirical observations, cannot cope with the geometric character of contemporary datasets, and do lack an axiomatic foundation. A different approach was proposed by V. Pestov, who links the intrinsic dimension axiomatically to the mathematical concentration of measure phenomenon. First methods to compute this and related notions for ID were computationally intractable for large-scale real-world datasets. In the present work, we derive a computationally feasible method for determining said axiomatic ID functions. Moreover, we demonstrate how the geometric properties of complex data are accounted for in our modeling. In particular, ",
    "link": "http://arxiv.org/abs/2210.05301",
    "context": "Title: Intrinsic Dimension for Large-Scale Geometric Learning. (arXiv:2210.05301v2 [cs.LG] UPDATED)\nAbstract: The concept of dimension is essential to grasp the complexity of data. A naive approach to determine the dimension of a dataset is based on the number of attributes. More sophisticated methods derive a notion of intrinsic dimension (ID) that employs more complex feature functions, e.g., distances between data points. Yet, many of these approaches are based on empirical observations, cannot cope with the geometric character of contemporary datasets, and do lack an axiomatic foundation. A different approach was proposed by V. Pestov, who links the intrinsic dimension axiomatically to the mathematical concentration of measure phenomenon. First methods to compute this and related notions for ID were computationally intractable for large-scale real-world datasets. In the present work, we derive a computationally feasible method for determining said axiomatic ID functions. Moreover, we demonstrate how the geometric properties of complex data are accounted for in our modeling. In particular, ",
    "path": "papers/22/10/2210.05301.json",
    "total_tokens": 842,
    "translated_title": "大规模几何学习的内在维度",
    "translated_abstract": "维度的概念对于理解数据的复杂性至关重要。确定数据集的维度的一种天真的方法是基于属性的数量。更复杂的方法推导出内在维度（ID）的概念，使用更复杂的特征函数，例如数据点之间的距离。然而，许多这些方法基于经验观察，无法应对当代数据集的几何特性，并且缺乏公理基础。V. Pestov提出了一种不同的方法，将内在维度公理地与数学集中度现象联系起来。首先，对于大规模实际数据集，计算这些内在维度的方法计算上是不可行的。在本研究中，我们推导出了一种计算上可行的方法来确定这些公理的内在维度函数。",
    "tldr": "本研究提出了一种计算上可行的方法来确定大规模复杂数据的内在维度，该方法考虑了数据集的几何特性，并在准确性和计算效率方面优于现有方法。",
    "en_tdlr": "This study proposes a computationally feasible method to determine the intrinsic dimension of large-scale complex datasets, taking into account their geometric properties. The approach outperforms existing methods in terms of both accuracy and computational efficiency."
}