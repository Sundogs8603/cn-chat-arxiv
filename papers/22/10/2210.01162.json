{
    "title": "Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications. (arXiv:2210.01162v4 [cs.RO] UPDATED)",
    "abstract": "This paper explores continuous-time control synthesis for target-driven navigation to satisfy complex high-level tasks expressed as linear temporal logic (LTL). We propose a model-free framework using deep reinforcement learning (DRL) where the underlying dynamic system is unknown (an opaque box). Unlike prior work, this paper considers scenarios where the given LTL specification might be infeasible and therefore cannot be accomplished globally. Instead of modifying the given LTL formula, we provide a general DRL-based approach to satisfy it with minimal violation. To do this, we transform a previously multi-objective DRL problem, which requires simultaneous automata satisfaction and minimum violation cost, into a single objective. By guiding the DRL agent with a sampling-based path planning algorithm for the potentially infeasible LTL task, the proposed approach mitigates the myopic tendencies of DRL, which are often an issue when learning general LTL tasks that can have long or infin",
    "link": "http://arxiv.org/abs/2210.01162",
    "context": "Title: Learning Minimally-Violating Continuous Control for Infeasible Linear Temporal Logic Specifications. (arXiv:2210.01162v4 [cs.RO] UPDATED)\nAbstract: This paper explores continuous-time control synthesis for target-driven navigation to satisfy complex high-level tasks expressed as linear temporal logic (LTL). We propose a model-free framework using deep reinforcement learning (DRL) where the underlying dynamic system is unknown (an opaque box). Unlike prior work, this paper considers scenarios where the given LTL specification might be infeasible and therefore cannot be accomplished globally. Instead of modifying the given LTL formula, we provide a general DRL-based approach to satisfy it with minimal violation. To do this, we transform a previously multi-objective DRL problem, which requires simultaneous automata satisfaction and minimum violation cost, into a single objective. By guiding the DRL agent with a sampling-based path planning algorithm for the potentially infeasible LTL task, the proposed approach mitigates the myopic tendencies of DRL, which are often an issue when learning general LTL tasks that can have long or infin",
    "path": "papers/22/10/2210.01162.json",
    "total_tokens": 991,
    "translated_title": "学习最小违反连续控制以实现不可行线性时态逻辑规范",
    "translated_abstract": "本文研究了连续时间控制综合，以实现线性时态逻辑(LTL)表达的复杂高级任务的目标驱动导航。我们提出了一个模型自由框架，使用深度强化学习(DRL)，其中底层动态系统未知（透明盒子）。与先前的工作不同，本文考虑了给定的LTL规范可能是不可行的情况，因此无法全局完成。我们不修改给定的LTL公式，而是提供了一个通用的DRL方法，以最小违规满足它。为了做到这一点，我们将先前的多目标DRL问题转化为一个单一目标问题，该问题要求同时实现自动机满足和最小违规代价。通过使用基于采样的路径规划算法来指导可能不可行的LTL任务的DRL智能体，所提出的方法减轻了DRL的近视倾向，这在学习可以具有长或无限持续时间的一般LTL任务时经常是一个问题。",
    "tldr": "本文提出了一个模型自由框架，使用深度强化学习来实现复杂高级任务的目标驱动导航。通过将先前的多目标DRL问题转化为一个单一目标问题，并使用基于采样的路径规划算法来指导DRL智能体，该方法可以满足不可行的线性时态逻辑任务并尽可能减少违规。",
    "en_tdlr": "This paper proposes a model-free framework using deep reinforcement learning for target-driven navigation to satisfy complex high-level tasks expressed as linear temporal logic. It introduces a general approach to satisfy potentially infeasible LTL specifications with minimal violation cost by transforming a multi-objective DRL problem into a single objective and guiding the DRL agent with a sampling-based path planning algorithm."
}