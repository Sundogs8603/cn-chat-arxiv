{
    "title": "Horizon-Free and Variance-Dependent Reinforcement Learning for Latent Markov Decision Processes. (arXiv:2210.11604v2 [cs.LG] UPDATED)",
    "abstract": "We study regret minimization for reinforcement learning (RL) in Latent Markov Decision Processes (LMDPs) with context in hindsight. We design a novel model-based algorithmic framework which can be instantiated with both a model-optimistic and a value-optimistic solver. We prove an $\\widetilde{O}\\left(\\sqrt{M \\Gamma S A K}\\right)$ regret bound where $M$ is the number of contexts, $S$ is the number of states, $A$ is the number of actions, $K$ is the number of episodes, and $\\Gamma \\le S$ is the maximum transition degree of any state-action pair. The regret bound only scales logarithmically with the planning horizon, thus yielding the first (nearly) horizon-free regret bound for LMDP. Key in our proof is an analysis of the total variance of alpha vectors, which is carefully bounded by a recursion-based technique. We complement our positive result with a novel $\\Omega\\left(\\sqrt{M S A K}\\right)$ regret lower bound with $\\Gamma = 2$, which shows our upper bound minimax optimal when $\\Gamma$",
    "link": "http://arxiv.org/abs/2210.11604",
    "context": "Title: Horizon-Free and Variance-Dependent Reinforcement Learning for Latent Markov Decision Processes. (arXiv:2210.11604v2 [cs.LG] UPDATED)\nAbstract: We study regret minimization for reinforcement learning (RL) in Latent Markov Decision Processes (LMDPs) with context in hindsight. We design a novel model-based algorithmic framework which can be instantiated with both a model-optimistic and a value-optimistic solver. We prove an $\\widetilde{O}\\left(\\sqrt{M \\Gamma S A K}\\right)$ regret bound where $M$ is the number of contexts, $S$ is the number of states, $A$ is the number of actions, $K$ is the number of episodes, and $\\Gamma \\le S$ is the maximum transition degree of any state-action pair. The regret bound only scales logarithmically with the planning horizon, thus yielding the first (nearly) horizon-free regret bound for LMDP. Key in our proof is an analysis of the total variance of alpha vectors, which is carefully bounded by a recursion-based technique. We complement our positive result with a novel $\\Omega\\left(\\sqrt{M S A K}\\right)$ regret lower bound with $\\Gamma = 2$, which shows our upper bound minimax optimal when $\\Gamma$",
    "path": "papers/22/10/2210.11604.json",
    "total_tokens": 1181,
    "translated_title": "无视规划地推广横跨变量的强化学习",
    "translated_abstract": "本文研究了具有后见性上下文的潜在马尔可夫决策过程 (LMDPs) 强化学习 (RL) 的遗憾最小化问题。我们设计了一个新颖的基于模型的算法框架，可以通过模型乐观或值乐观求解器实例化。我们证明了一个关于遗憾度的较小量级为 $\\widetilde{O}\\left(\\sqrt{M \\Gamma S A K}\\right)$ 的界限，其中 $M$ 是上下文数量，$S$ 是状态数量，$A$ 是动作数量，$K$ 是回合数量，而 $\\Gamma \\le S$ 是任何状态-动作对的最大转移次数。遗憾度只在规划视野中以对数形式缩放，所以 LMDP 的规划视野的第一个(几乎)无视界限就被产生了。我们的论证的关键是对 alpha 向量的总方差进行分析，该方差通过递归技术进行了仔细的限制。我们通过一个新的 $\\Omega\\left(\\sqrt{M S A K}\\right)$ 遗憾性下限补充了我们的正补结果，并证明了当 $\\Gamma=2$ 时，我们的上界是极小化最优的。",
    "tldr": "本文研究了具有上下文后见性的 LMDP 强化学习遗憾最小化问题。通过设计一个新颖的模型基础算法框架，我们证明了一个与计划视野对数相关的 $\\widetilde{O}\\left(\\sqrt{M \\Gamma S A K}\\right)$ 遗憾度上限，并对 alpha 向量的总方差进行分析。同时，我们提出了一个 $\\Omega\\left(\\sqrt{M S A K}\\right)$ 的遗憾度下限，它在 $\\Gamma=2$ 时证明了我们的上界是最优的。",
    "en_tdlr": "This study focuses on the regret minimization problem of LMDP RL with hindsight context. A novel model-based algorithmic framework with both model-optimistic and value-optimistic solvers is designed, and a regret bound of $\\widetilde{O}\\left(\\sqrt{M \\Gamma S A K}\\right)$ is proved, with logarithmical scaling to the planning horizon. The analysis of the total variance of alpha vectors is key in the proof. A novel $\\Omega\\left(\\sqrt{M S A K}\\right)$ regret lower bound is also presented."
}