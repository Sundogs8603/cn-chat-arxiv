{
    "title": "On-Demand Sampling: Learning Optimally from Multiple Distributions",
    "abstract": "arXiv:2210.12529v3 Announce Type: replace  Abstract: Social and real-world considerations such as robustness, fairness, social welfare and multi-agent tradeoffs have given rise to multi-distribution learning paradigms, such as collaborative learning, group distributionally robust optimization, and fair federated learning. In each of these settings, a learner seeks to uniformly minimize its expected loss over $n$ predefined data distributions, while using as few samples as possible. In this paper, we establish the optimal sample complexity of these learning paradigms and give algorithms that meet this sample complexity. Importantly, our sample complexity bounds for multi-distribution learning exceed that of learning a single distribution by only an additive factor of $n \\log(n) / \\epsilon^2$. This improves upon the best known sample complexity bounds for fair federated learning by Mohri et al. and collaborative learning by Nguyen and Zakynthinou by multiplicative factors of $n$ and $\\lo",
    "link": "https://arxiv.org/abs/2210.12529",
    "context": "Title: On-Demand Sampling: Learning Optimally from Multiple Distributions\nAbstract: arXiv:2210.12529v3 Announce Type: replace  Abstract: Social and real-world considerations such as robustness, fairness, social welfare and multi-agent tradeoffs have given rise to multi-distribution learning paradigms, such as collaborative learning, group distributionally robust optimization, and fair federated learning. In each of these settings, a learner seeks to uniformly minimize its expected loss over $n$ predefined data distributions, while using as few samples as possible. In this paper, we establish the optimal sample complexity of these learning paradigms and give algorithms that meet this sample complexity. Importantly, our sample complexity bounds for multi-distribution learning exceed that of learning a single distribution by only an additive factor of $n \\log(n) / \\epsilon^2$. This improves upon the best known sample complexity bounds for fair federated learning by Mohri et al. and collaborative learning by Nguyen and Zakynthinou by multiplicative factors of $n$ and $\\lo",
    "path": "papers/22/10/2210.12529.json",
    "total_tokens": 855,
    "translated_title": "需求抽样：从多个分布中学习最优",
    "translated_abstract": "社会和现实考虑，如鲁棒性、公平性、社会福利和多智能体权衡已经催生了多分布学习范式，如协作学习、分布鲁棒优化和公平联邦学习。在这些设置中，学习者寻求在$n$个预定义数据分布上均匀最小化其期望损失，同时尽可能少地使用样本。本文建立了这些学习范式的最优样本复杂度，并提供了符合此样本复杂度的算法。值得注意的是，我们的多分布学习样本复杂度边界仅超过了单一分布学习的添加因子$n \\log(n) / \\epsilon^2$。这改进了Mohri等人提出的公平联邦学习和Nguyen和Zakynthinou提出的协作学习的已知最佳样本复杂度边界，分别为$n$和$\\lo的乘法因子。",
    "tldr": "本文建立了多分布学习范式的最优样本复杂度，并提出了符合此复杂度的算法，改进了公平联邦学习和协作学习的样本复杂度边界。"
}