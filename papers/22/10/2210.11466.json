{
    "title": "Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. (arXiv:2210.11466v3 [cs.LG] UPDATED)",
    "abstract": "A common approach to transfer learning under distribution shift is to fine-tune the last few layers of a pre-trained model, preserving learned features while also adapting to the new task. This paper shows that in such settings, selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms commonly used fine-tuning approaches. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best. We validate our findings systematically across seven real-world data tasks spanning three types of distribution shifts. Theoretically, we prove that for two-layer neural networks in an idealized setting, first-layer tuning can outperform fine-tuning all layers. Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shif",
    "link": "http://arxiv.org/abs/2210.11466",
    "context": "Title: Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. (arXiv:2210.11466v3 [cs.LG] UPDATED)\nAbstract: A common approach to transfer learning under distribution shift is to fine-tune the last few layers of a pre-trained model, preserving learned features while also adapting to the new task. This paper shows that in such settings, selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms commonly used fine-tuning approaches. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best. We validate our findings systematically across seven real-world data tasks spanning three types of distribution shifts. Theoretically, we prove that for two-layer neural networks in an idealized setting, first-layer tuning can outperform fine-tuning all layers. Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shif",
    "path": "papers/22/10/2210.11466.json",
    "total_tokens": 1103,
    "translated_title": "手术微调提高了适应分布偏移的效果",
    "translated_abstract": "在分布偏移下的迁移学习中，常见的方法是微调预训练模型的最后几层，保留已学特征同时适应新任务。本文表明，在这种情况下，有选择性地微调预训练模型的子集层（我们称之为手术微调）可以达到与或优于常用的微调方法，且不同类型的分布偏移影响着能够微调的层数。我们在七个真实数据任务中系统验证了这一结论。此外，理论上证明了在理想环境下，手术微调可以胜过全层微调。",
    "tldr": "本研究表明，选择性地微调预训练模型的子集层（手术微调）在适应分布偏移的任务中效果更好，在真实数据中得到了验证，还在理论上证明在理想环境下，手术微调可以优于全层微调。",
    "en_tdlr": "This paper shows that selectively fine-tuning a subset of pre-trained model layers (termed surgical fine-tuning) can outperform commonly used approaches for adapting to distribution shift in transfer learning. The effectiveness of which subset to fine-tune depends on the type of distribution shift. The study was validated on seven real-world data tasks and the theory proved that surgical fine-tuning can outperform fine-tuning all layers for two-layer neural networks in an idealized setting."
}