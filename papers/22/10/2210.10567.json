{
    "title": "Margin Optimal Classification Trees. (arXiv:2210.10567v5 [math.OC] UPDATED)",
    "abstract": "In recent years, there has been growing attention to interpretable machine learning models which can give explanatory insights on their behaviour. Thanks to their interpretability, decision trees have been intensively studied for classification tasks and, due to the remarkable advances in mixed integer programming (MIP), various approaches have been proposed to formulate the problem of training an Optimal Classification Tree (OCT) as a MIP model. We present a novel mixed integer quadratic formulation for the OCT problem, which exploits the generalization capabilities of Support Vector Machines for binary classification. Our model, denoted as Margin Optimal Classification Tree (MARGOT), encompasses maximum margin multivariate hyperplanes nested in a binary tree structure. To enhance the interpretability of our approach, we analyse two alternative versions of MARGOT, which include feature selection constraints inducing sparsity of the hyperplanes' coefficients. First, MARGOT has been tes",
    "link": "http://arxiv.org/abs/2210.10567",
    "context": "Title: Margin Optimal Classification Trees. (arXiv:2210.10567v5 [math.OC] UPDATED)\nAbstract: In recent years, there has been growing attention to interpretable machine learning models which can give explanatory insights on their behaviour. Thanks to their interpretability, decision trees have been intensively studied for classification tasks and, due to the remarkable advances in mixed integer programming (MIP), various approaches have been proposed to formulate the problem of training an Optimal Classification Tree (OCT) as a MIP model. We present a novel mixed integer quadratic formulation for the OCT problem, which exploits the generalization capabilities of Support Vector Machines for binary classification. Our model, denoted as Margin Optimal Classification Tree (MARGOT), encompasses maximum margin multivariate hyperplanes nested in a binary tree structure. To enhance the interpretability of our approach, we analyse two alternative versions of MARGOT, which include feature selection constraints inducing sparsity of the hyperplanes' coefficients. First, MARGOT has been tes",
    "path": "papers/22/10/2210.10567.json",
    "total_tokens": 894,
    "translated_title": "边际最优分类树",
    "translated_abstract": "最近，对于能够解释机器学习模型并能够提供行为解释见解的关注越来越多。由于决策树的可解释性，它已经被广泛研究用于分类任务，并且由于混合整数规划(MIP)的显著进展，已提出了多种方法将最优分类树(OCT)的训练问题建模为MIP模型。本文提出了一种新颖的混合整数二次规划的OCT问题公式化方法，利用支持向量机在二分类问题上的泛化能力。我们的模型称为边际最优分类树(MARGOT)，包括嵌套在二叉树结构中的最大边际多元超平面。为了增强我们方法的可解释性，我们分析了MARGOT的两个替代版本，其中包括引入稀疏超平面系数的特征选择约束。首先，我们测试了MARGOT的贡献。",
    "tldr": "本文提出了一种名为MARGOT的边际最优分类树模型，该模型利用了支持向量机的泛化能力，并在二叉树结构中嵌套了最大边际多元超平面。与传统决策树相比，MARGOT具有更好的解释性和可解释性。"
}