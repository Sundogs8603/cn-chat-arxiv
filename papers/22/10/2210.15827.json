{
    "title": "Federated Learning with Intermediate Representation Regularization. (arXiv:2210.15827v2 [cs.LG] UPDATED)",
    "abstract": "In contrast to centralized model training that involves data collection, federated learning (FL) enables remote clients to collaboratively train a model without exposing their private data. However, model performance usually degrades in FL due to the heterogeneous data generated by clients of diverse characteristics. One promising strategy to maintain good performance is by limiting the local training from drifting far away from the global model. Previous studies accomplish this by regularizing the distance between the representations learned by the local and global models. However, they only consider representations from the early layers of a model or the layer preceding the output layer. In this study, we introduce FedIntR, which provides a more fine-grained regularization by integrating the representations of intermediate layers into the local training process. Specifically, FedIntR computes a regularization term that encourages the closeness between the intermediate layer represent",
    "link": "http://arxiv.org/abs/2210.15827",
    "context": "Title: Federated Learning with Intermediate Representation Regularization. (arXiv:2210.15827v2 [cs.LG] UPDATED)\nAbstract: In contrast to centralized model training that involves data collection, federated learning (FL) enables remote clients to collaboratively train a model without exposing their private data. However, model performance usually degrades in FL due to the heterogeneous data generated by clients of diverse characteristics. One promising strategy to maintain good performance is by limiting the local training from drifting far away from the global model. Previous studies accomplish this by regularizing the distance between the representations learned by the local and global models. However, they only consider representations from the early layers of a model or the layer preceding the output layer. In this study, we introduce FedIntR, which provides a more fine-grained regularization by integrating the representations of intermediate layers into the local training process. Specifically, FedIntR computes a regularization term that encourages the closeness between the intermediate layer represent",
    "path": "papers/22/10/2210.15827.json",
    "total_tokens": 877,
    "translated_title": "带有中间表示正则化的联邦学习",
    "translated_abstract": "与涉及数据收集的集中式模型训练相反，联邦学习使远程客户端能够在不暴露其私有数据的情况下协同训练模型。然而，联邦学习中异构数据可能会导致模型性能下降。很多时候，为了保持较好的性能，限制本地训练过程中偏离全局模型的程度是一种有效策略。以往的研究通过正则化全局和本地学习的表示之间的距离以实现这一点，但是这种方法只考虑了模型的早期层或输出层前面的层的表示。本研究介绍了FedIntR，它通过将中间层的表示集成到本地训练过程中提供了更细粒度的正则化。具体而言，FedIntR计算一个正则化项来鼓励中间层表示和全局模型之间的接近度。",
    "tldr": "本文提出一种新的联邦学习方法FedIntR，可以通过将中间层的表示集成到本地训练过程中，提供更细粒度的正则化，从而限制本地训练过程中偏离全局模型的程度，以提高模型性能。",
    "en_tdlr": "In this paper, the authors propose a new federated learning method, FedIntR, which integrates intermediate layer representations into the local training process to provide more fine-grained regularization and maintain good performance by limiting local training from drifting far away from the global model."
}