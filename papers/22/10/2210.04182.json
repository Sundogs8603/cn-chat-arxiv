{
    "title": "Deep Span Representations for Named Entity Recognition. (arXiv:2210.04182v2 [cs.CL] UPDATED)",
    "abstract": "Span-based models are one of the most straightforward methods for named entity recognition (NER). Existing span-based NER systems shallowly aggregate the token representations to span representations. However, this typically results in significant ineffectiveness for long-span entities, a coupling between the representations of overlapping spans, and ultimately a performance degradation. In this study, we propose DSpERT (Deep Span Encoder Representations from Transformers), which comprises a standard Transformer and a span Transformer. The latter uses low-layered span representations as queries, and aggregates the token representations as keys and values, layer by layer from bottom to top. Thus, DSpERT produces span representations of deep semantics.  With weight initialization from pretrained language models, DSpERT achieves performance higher than or competitive with recent state-of-the-art systems on eight NER benchmarks. Experimental results verify the importance of the depth for s",
    "link": "http://arxiv.org/abs/2210.04182",
    "context": "Title: Deep Span Representations for Named Entity Recognition. (arXiv:2210.04182v2 [cs.CL] UPDATED)\nAbstract: Span-based models are one of the most straightforward methods for named entity recognition (NER). Existing span-based NER systems shallowly aggregate the token representations to span representations. However, this typically results in significant ineffectiveness for long-span entities, a coupling between the representations of overlapping spans, and ultimately a performance degradation. In this study, we propose DSpERT (Deep Span Encoder Representations from Transformers), which comprises a standard Transformer and a span Transformer. The latter uses low-layered span representations as queries, and aggregates the token representations as keys and values, layer by layer from bottom to top. Thus, DSpERT produces span representations of deep semantics.  With weight initialization from pretrained language models, DSpERT achieves performance higher than or competitive with recent state-of-the-art systems on eight NER benchmarks. Experimental results verify the importance of the depth for s",
    "path": "papers/22/10/2210.04182.json",
    "total_tokens": 971,
    "translated_title": "命名实体识别的深度跨度表示",
    "translated_abstract": "跨度基础模型是命名实体识别（NER）最简单直接的方法之一。 现有的跨度基础NER系统将标记表示浅层聚合到跨度表示中。 但是，这通常导致长跨度实体的显着无效性，重叠跨度表示的耦合，最终性能下降。 在本研究中，我们提出了DSpERT（来自Transformer的深度跨度编码器表示），它由标准Transformer和跨度Transformer组成。 后者使用低层次的跨度表示作为查询，并从底部到顶部逐层聚合标记表示作为键和值，因此，DSpERT产生了深层语义的跨度表示。 借助预训练语言模型的权重初始化，DSpERT在八个NER基准测试中取得了高于或与最新的最先进系统竞争的性能。 实验结果验证了深度对跨度基础NER系统的重要性。",
    "tldr": "本研究提出了DSpERT模型，通过跨度Transformer逐层聚合标记表示作为键和值，产生了深层语义的跨度表示，从而解决了现有跨度基础NER系统中长跨度实体显着无效性和重叠跨度表示的耦合问题。实验结果表明，DSpERT在八个NER基准测试中取得了性能高于或与最新最先进系统竞争的成果。"
}