{
    "title": "Less is More: A Lightweight and Robust Neural Architecture for Discourse Parsing. (arXiv:2210.09537v2 [cs.CL] UPDATED)",
    "abstract": "Complex feature extractors are widely employed for text representation building. However, these complex feature extractors make the NLP systems prone to overfitting especially when the downstream training datasets are relatively small, which is the case for several discourse parsing tasks. Thus, we propose an alternative lightweight neural architecture that removes multiple complex feature extractors and only utilizes learnable self-attention modules to indirectly exploit pretrained neural language models, in order to maximally preserve the generalizability of pre-trained language models. Experiments on three common discourse parsing tasks show that powered by recent pretrained language models, the lightweight architecture consisting of only two self-attention layers obtains much better generalizability and robustness. Meanwhile, it achieves comparable or even better system performance with fewer learnable parameters and less processing time.",
    "link": "http://arxiv.org/abs/2210.09537",
    "context": "Title: Less is More: A Lightweight and Robust Neural Architecture for Discourse Parsing. (arXiv:2210.09537v2 [cs.CL] UPDATED)\nAbstract: Complex feature extractors are widely employed for text representation building. However, these complex feature extractors make the NLP systems prone to overfitting especially when the downstream training datasets are relatively small, which is the case for several discourse parsing tasks. Thus, we propose an alternative lightweight neural architecture that removes multiple complex feature extractors and only utilizes learnable self-attention modules to indirectly exploit pretrained neural language models, in order to maximally preserve the generalizability of pre-trained language models. Experiments on three common discourse parsing tasks show that powered by recent pretrained language models, the lightweight architecture consisting of only two self-attention layers obtains much better generalizability and robustness. Meanwhile, it achieves comparable or even better system performance with fewer learnable parameters and less processing time.",
    "path": "papers/22/10/2210.09537.json",
    "total_tokens": 940,
    "translated_title": "Less is More: 用于篇章分析的轻量级和鲁棒的神经网络结构",
    "translated_abstract": "复杂的特征提取器被广泛用于构建文本表示，然而这些复杂的特征提取器使得自然语言处理系统在训练数据集相对较小的情况下容易过拟合，尤其是对于一些篇章分析任务。因此，我们提出了一种替代性的轻量级神经网络结构，移除了多个复杂的特征提取器，只利用可学习的自注意力模块来间接利用预训练的神经语言模型，以最大程度地保留预训练语言模型的泛化能力。对于三个常见的篇章分析任务的实验表明，基于最新的预训练语言模型的轻量级结构仅包含两个自注意力层，具有更好的泛化能力和鲁棒性。同时，它在较少的可学习参数和较少的处理时间下实现了可比较甚至更好的系统性能。",
    "tldr": "本文提出了一个轻量级神经网络结构，通过移除复杂的特征提取器，仅利用自注意力模块间接利用预训练的神经语言模型，以提高篇章分析任务的泛化能力和鲁棒性。实验证明，这个轻量级结构只有两个自注意力层，却具有更好的性能，并且拥有较少的参数和处理时间。",
    "en_tdlr": "This paper proposes a lightweight neural architecture for discourse parsing that removes complex feature extractors and utilizes self-attention modules to indirectly leverage pretrained language models, resulting in improved generalizability and robustness. Experimental results demonstrate that this lightweight architecture, consisting of only two self-attention layers, achieves better performance with fewer parameters and processing time."
}