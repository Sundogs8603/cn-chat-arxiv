{
    "title": "Characterizing Verbatim Short-Term Memory in Neural Language Models. (arXiv:2210.13569v2 [cs.CL] UPDATED)",
    "abstract": "When a language model is trained to predict natural language sequences, its prediction at each moment depends on a representation of prior context. What kind of information about the prior context can language models retrieve? We tested whether language models could retrieve the exact words that occurred previously in a text. In our paradigm, language models (transformers and an LSTM) processed English text in which a list of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the first to the second list. We found that the transformers retrieved both the identity and ordering of nouns from the first list. Further, the transformers' retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth. Lastly, their ability to index prior tokens was dependent on learned attention patterns. In contrast, the LSTM exhibited less precise retrieval, which was limited to list-initial tokens and to short intervening texts. The",
    "link": "http://arxiv.org/abs/2210.13569",
    "context": "Title: Characterizing Verbatim Short-Term Memory in Neural Language Models. (arXiv:2210.13569v2 [cs.CL] UPDATED)\nAbstract: When a language model is trained to predict natural language sequences, its prediction at each moment depends on a representation of prior context. What kind of information about the prior context can language models retrieve? We tested whether language models could retrieve the exact words that occurred previously in a text. In our paradigm, language models (transformers and an LSTM) processed English text in which a list of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the first to the second list. We found that the transformers retrieved both the identity and ordering of nouns from the first list. Further, the transformers' retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth. Lastly, their ability to index prior tokens was dependent on learned attention patterns. In contrast, the LSTM exhibited less precise retrieval, which was limited to list-initial tokens and to short intervening texts. The",
    "path": "papers/22/10/2210.13569.json",
    "total_tokens": 908,
    "translated_title": "神经语言模型中的逐字短期记忆特征研究",
    "translated_abstract": "当语言模型被训练用于预测自然语言序列时，它在每个时刻的预测依赖于先前上下文的表征。语言模型能够检索到哪些关于先前上下文的信息？本研究测试了语言模型能否检索到先前在文本中出现过的确切单词。我们以英文文本为范例，其中一个名词列表出现了两次，利用Transformer和LSTM模型处理。我们将检索定义为从第一个列表到第二个列表的惊异度降低。我们发现，Transformer可以从第一个列表中检索到名词的身份和顺序。此外，当Transformer在更大的语料库中和更深的模型中进行训练时，它们的检索能力显著增强。最后，Transformer索引先前的标记的能力取决于学习到的注意模式。相反，LSTM的检索能力较低，仅限于列表初始标记和短的干扰文本。",
    "tldr": "本研究探讨了语言模型的逐字短期记忆特征，发现Transformer模型可以准确检索先前出现过的单词身份和顺序，而LSTM模型的检索能力受限于列表初始标记和短的干扰文本。",
    "en_tdlr": "This paper investigates the verbatim short-term memory in neural language models and finds that Transformer models can accurately retrieve the identity and order of words that occurred previously, while LSTM models exhibit limited retrieval ability only for list-initial tokens and short intervening texts."
}