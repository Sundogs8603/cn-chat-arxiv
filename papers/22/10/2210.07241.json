{
    "title": "Visual Reinforcement Learning with Self-Supervised 3D Representations. (arXiv:2210.07241v2 [cs.LG] UPDATED)",
    "abstract": "A prominent approach to visual Reinforcement Learning (RL) is to learn an internal state representation using self-supervised methods, which has the potential benefit of improved sample-efficiency and generalization through additional learning signal and inductive biases. However, while the real world is inherently 3D, prior efforts have largely been focused on leveraging 2D computer vision techniques as auxiliary self-supervision. In this work, we present a unified framework for self-supervised learning of 3D representations for motor control. Our proposed framework consists of two phases: a pretraining phase where a deep voxel-based 3D autoencoder is pretrained on a large object-centric dataset, and a finetuning phase where the representation is jointly finetuned together with RL on in-domain data. We empirically show that our method enjoys improved sample efficiency in simulated manipulation tasks compared to 2D representation learning methods. Additionally, our learned policies tra",
    "link": "http://arxiv.org/abs/2210.07241",
    "context": "Title: Visual Reinforcement Learning with Self-Supervised 3D Representations. (arXiv:2210.07241v2 [cs.LG] UPDATED)\nAbstract: A prominent approach to visual Reinforcement Learning (RL) is to learn an internal state representation using self-supervised methods, which has the potential benefit of improved sample-efficiency and generalization through additional learning signal and inductive biases. However, while the real world is inherently 3D, prior efforts have largely been focused on leveraging 2D computer vision techniques as auxiliary self-supervision. In this work, we present a unified framework for self-supervised learning of 3D representations for motor control. Our proposed framework consists of two phases: a pretraining phase where a deep voxel-based 3D autoencoder is pretrained on a large object-centric dataset, and a finetuning phase where the representation is jointly finetuned together with RL on in-domain data. We empirically show that our method enjoys improved sample efficiency in simulated manipulation tasks compared to 2D representation learning methods. Additionally, our learned policies tra",
    "path": "papers/22/10/2210.07241.json",
    "total_tokens": 989,
    "translated_title": "利用自监督三维表示的视觉强化学习",
    "translated_abstract": "视觉强化学习（RL）的一个重要方法是使用自监督方法学习内部状态表示，这具有通过额外学习信号和归纳偏差提高样本效率和泛化能力的潜在好处。然而，虽然真实世界本质上是三维的，但之前的研究主要集中于利用二维计算机视觉技术作为辅助自监督。在这项工作中，我们提出了一个自监督学习三维表示用于运动控制的统一框架。我们提出的框架包括两个阶段：预训练阶段，其中对深度体素三维自编码器进行预训练，并使用大型目标为中心的数据集进行，以及微调阶段，在该阶段，表示与RL一起在领域内数据上进行微调。我们在实验中表明，与二维表示学习方法相比，我们的方法在模拟操作任务中具有更高的样本效率。 此外，我们学习的策略使其与传统二维表示学习方法相比，可以更好地泛化到其他情况下的RL。",
    "tldr": "这篇论文提出了一个自监督学习三维表示的方法来解决视觉强化学习中的样本效率和泛化问题，通过预训练和微调两个阶段，相比于二维表示学习方法，该方法在操作任务中具有更高的样本效率并且更好地泛化到其他情况下的RL。",
    "en_tdlr": "This paper proposes a self-supervised method for learning 3D representations to address the sample efficiency and generalization problem in visual reinforcement learning. Through pre-training and fine-tuning, the method shows improved sample efficiency and better generalization to other situations compared to traditional 2D representation methods in manipulation tasks."
}