{
    "title": "Layer Ensembles. (arXiv:2210.04882v3 [cs.LG] UPDATED)",
    "abstract": "Deep Ensembles, as a type of Bayesian Neural Networks, can be used to estimate uncertainty on the prediction of multiple neural networks by collecting votes from each network and computing the difference in those predictions. In this paper, we introduce a method for uncertainty estimation that considers a set of independent categorical distributions for each layer of the network, giving many more possible samples with overlapped layers than in the regular Deep Ensembles. We further introduce an optimized inference procedure that reuses common layer outputs, achieving up to 19x speed up and reducing memory usage quadratically. We also show that the method can be further improved by ranking samples, resulting in models that require less memory and time to run while achieving higher uncertainty quality than Deep Ensembles.",
    "link": "http://arxiv.org/abs/2210.04882",
    "context": "Title: Layer Ensembles. (arXiv:2210.04882v3 [cs.LG] UPDATED)\nAbstract: Deep Ensembles, as a type of Bayesian Neural Networks, can be used to estimate uncertainty on the prediction of multiple neural networks by collecting votes from each network and computing the difference in those predictions. In this paper, we introduce a method for uncertainty estimation that considers a set of independent categorical distributions for each layer of the network, giving many more possible samples with overlapped layers than in the regular Deep Ensembles. We further introduce an optimized inference procedure that reuses common layer outputs, achieving up to 19x speed up and reducing memory usage quadratically. We also show that the method can be further improved by ranking samples, resulting in models that require less memory and time to run while achieving higher uncertainty quality than Deep Ensembles.",
    "path": "papers/22/10/2210.04882.json",
    "total_tokens": 803,
    "translated_title": "Layer Ensembles. (arXiv:2210.04882v3 [cs.LG] UPDATED)",
    "translated_abstract": "深度集成是一种贝叶斯神经网络，可以通过收集每个网络的预测结果并计算预测结果之间的差异来估计多个神经网络预测的不确定性。在本文中，我们介绍了一种不确定性估计方法，该方法考虑了网络每一层的一组独立的分类分布，与常规的深度集成相比具有更多可能的样本，这些样本在层之间重叠。我们进一步引入了一种优化的推断过程，该过程可以重复使用公共的层输出，从而实现高达19倍的加速和二次减少内存使用。我们还展示了通过对样本进行排序可以进一步改进该方法，从而得到需要更少内存和时间来运行并且具有比深度集成更高不确定性质量的模型。",
    "tldr": "本文介绍了一种基于独立分类分布的不确定性估计方法，通过重复使用公共的层输出和对样本进行排序，可以大幅提高模型运行速度、降低内存使用，并且获得比常规深度集成更高的不确定性质量。",
    "en_tdlr": "This paper introduces a method for uncertainty estimation based on independent categorical distributions, which achieves significant speedup and memory reduction by reusing common layer outputs and ranking samples, while also improving uncertainty quality compared to regular Deep Ensembles."
}