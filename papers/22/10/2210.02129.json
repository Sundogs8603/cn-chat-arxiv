{
    "title": "Decentralized Hyper-Gradient Computation over Time-Varying Directed Networks. (arXiv:2210.02129v3 [stat.ML] UPDATED)",
    "abstract": "This paper addresses the communication issues when estimating hyper-gradients in decentralized federated learning (FL). Hyper-gradients in decentralized FL quantifies how the performance of globally shared optimal model is influenced by the perturbations in clients' hyper-parameters. In prior work, clients trace this influence through the communication of Hessian matrices over a static undirected network, resulting in (i) excessive communication costs and (ii) inability to make use of more efficient and robust networks, namely, time-varying directed networks. To solve these issues, we introduce an alternative optimality condition for FL using an averaging operation on model parameters and gradients. We then employ Push-Sum as the averaging operation, which is a consensus optimization technique for time-varying directed networks. As a result, the hyper-gradient estimator derived from our optimality condition enjoys two desirable properties; (i) it only requires Push-Sum communication of",
    "link": "http://arxiv.org/abs/2210.02129",
    "context": "Title: Decentralized Hyper-Gradient Computation over Time-Varying Directed Networks. (arXiv:2210.02129v3 [stat.ML] UPDATED)\nAbstract: This paper addresses the communication issues when estimating hyper-gradients in decentralized federated learning (FL). Hyper-gradients in decentralized FL quantifies how the performance of globally shared optimal model is influenced by the perturbations in clients' hyper-parameters. In prior work, clients trace this influence through the communication of Hessian matrices over a static undirected network, resulting in (i) excessive communication costs and (ii) inability to make use of more efficient and robust networks, namely, time-varying directed networks. To solve these issues, we introduce an alternative optimality condition for FL using an averaging operation on model parameters and gradients. We then employ Push-Sum as the averaging operation, which is a consensus optimization technique for time-varying directed networks. As a result, the hyper-gradient estimator derived from our optimality condition enjoys two desirable properties; (i) it only requires Push-Sum communication of",
    "path": "papers/22/10/2210.02129.json",
    "total_tokens": 897,
    "translated_title": "基于时变有向网络的分散式超梯度计算",
    "translated_abstract": "本文解决了分散式联邦学习中估计超梯度时的通信问题。在分散式联邦学习中，超梯度量化了全局共享最优模型的性能如何受到客户端超参数扰动的影响。在先前的工作中，客户端通过在静态无向网络上通信 Hess 矩阵来跟踪这种影响，导致了（i）过高的通信成本和（ii）不能利用更高效和更强大的网络，即时变有向网络。为了解决这些问题，我们引入了一个基于模型参数和梯度的平均操作的 FL 替代性优化条件。然后，我们采用 Push-Sum 作为平均操作，在时变有向网络上进行共识优化技术。因此，从我们的最优条件推导出的超梯度估计器具有两个理想特性，（i）它只需要 Push-Sum 通信",
    "tldr": "本文提出了一种基于时变有向网络的分散式超梯度计算方法，避免了静态无向网络通信 Hessian 矩阵导致的高通信成本和无法使用时变有向网络优势的问题。"
}