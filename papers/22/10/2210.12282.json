{
    "title": "Bridging the Gap Between Target Networks and Functional Regularization. (arXiv:2210.12282v2 [cs.LG] UPDATED)",
    "abstract": "Bootstrapping is behind much of the successes of Deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. Despite the popularity of Target Networks, their effect on the optimization is still misunderstood. In this work, we show that they act as an implicit regularizer. This regularizer has disadvantages such as being inflexible and non convex. To overcome these issues, we propose an explicit Functional Regularization that is a convex regularizer in function space and can easily be tuned. We analyze the convergence of our method theoretically and empirically demonstrate that replacing Target Networks with the more theoretically grounded Functional Regularization approach leads to better sample efficiency and performance improvements.",
    "link": "http://arxiv.org/abs/2210.12282",
    "context": "Title: Bridging the Gap Between Target Networks and Functional Regularization. (arXiv:2210.12282v2 [cs.LG] UPDATED)\nAbstract: Bootstrapping is behind much of the successes of Deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. Despite the popularity of Target Networks, their effect on the optimization is still misunderstood. In this work, we show that they act as an implicit regularizer. This regularizer has disadvantages such as being inflexible and non convex. To overcome these issues, we propose an explicit Functional Regularization that is a convex regularizer in function space and can easily be tuned. We analyze the convergence of our method theoretically and empirically demonstrate that replacing Target Networks with the more theoretically grounded Functional Regularization approach leads to better sample efficiency and performance improvements.",
    "path": "papers/22/10/2210.12282.json",
    "total_tokens": 873,
    "translated_title": "缩小目标网络与功能正则化之间的差距",
    "translated_abstract": "引导法是深度强化学习成功的关键。然而，通过引导法学习价值函数常常导致训练不稳定，因为目标值变化快速。目标网络被用于通过使用滞后参数集来估计目标值来稳定训练。尽管目标网络很受欢迎，但它们对优化的影响仍不为人所理解。在这项工作中，我们展示了它们作为一种隐式正则化器的作用。这种正则化器有不灵活和非凸优点。为了克服这些问题，我们提出了一种显式的功能正则化方法，它是函数空间中的凸正则化器，可以轻松调节。我们理论上分析了我们方法的收敛性，并通过实验证明，用更具理论基础的功能正则化方法取代目标网络可以提高样本效率和性能。",
    "tldr": "目标网络在深度强化学习中具有关键作用，但其对优化的影响还不为人所理解。本文提出了一种显式的功能正则化方法，通过替换目标网络来改善样本效率和性能。实验证明，这种方法能够更好地稳定训练，并取得了较好的结果。",
    "en_tdlr": "Target networks play a crucial role in deep reinforcement learning, but their impact on optimization is still not well understood. This paper proposes an explicit functional regularization method that replaces target networks and improves sample efficiency and performance. Experimental results demonstrate its effectiveness in stabilizing training and achieving better results."
}