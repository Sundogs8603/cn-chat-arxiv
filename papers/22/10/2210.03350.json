{
    "title": "Measuring and Narrowing the Compositionality Gap in Language Models. (arXiv:2210.03350v2 [cs.CL] UPDATED)",
    "abstract": "We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.  We then demonstrate how elicitive prompting (such as chain of thought) narrows t",
    "link": "http://arxiv.org/abs/2210.03350",
    "context": "Title: Measuring and Narrowing the Compositionality Gap in Language Models. (arXiv:2210.03350v2 [cs.CL] UPDATED)\nAbstract: We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.  We then demonstrate how elicitive prompting (such as chain of thought) narrows t",
    "path": "papers/22/10/2210.03350.json",
    "total_tokens": 870,
    "translated_abstract": "本文研究了语言模型在需要正确组合子问题答案才能得到整体解决方案的组合推理任务中的能力。我们测量了模型可以正确回答所有子问题但无法生成整体解决方案的频率，这种比率被称为组合性差距。我们通过提问需要组合多个事实的答案的多次跳转问题来评估这个比率。在GPT-3系列模型中，随着模型大小的增加，单跳问题回答性能的提高速度比多跳性能更快，因此组合性差距并没有减少。这个令人惊讶的结果表明，尽管更强大的模型可以记忆和回溯更多的事实知识，但它们在执行这种组合推理的能力上并没有相应的改进。然后我们展示了如何采用启发式提示（如思维链）缩小这种差距。",
    "tldr": "本文研究了语言模型在多次跳转问题中组合推理任务的能力，发现随着模型大小的增加，组合性差距并没有减少，但启发式提示可以帮助缩小这种差距。",
    "en_tdlr": "This paper investigates the ability of language models to perform compositional reasoning tasks in multi-hop questions and finds that as the model size increases, the compositionality gap does not decrease. However, elicitive prompting such as chain of thought can help narrow this gap."
}