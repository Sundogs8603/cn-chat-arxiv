{
    "title": "Communication breakdown: On the low mutual intelligibility between human and neural captioning. (arXiv:2210.11512v2 [cs.CL] UPDATED)",
    "abstract": "We compare the 0-shot performance of a neural caption-based image retriever when given as input either human-produced captions or captions generated by a neural captioner. We conduct this comparison on the recently introduced ImageCoDe data-set (Krojer et al., 2022) which contains hard distractors nearly identical to the images to be retrieved. We find that the neural retriever has much higher performance when fed neural rather than human captions, despite the fact that the former, unlike the latter, were generated without awareness of the distractors that make the task hard. Even more remarkably, when the same neural captions are given to human subjects, their retrieval performance is almost at chance level. Our results thus add to the growing body of evidence that, even when the ``language'' of neural models resembles English, this superficial resemblance might be deeply misleading.",
    "link": "http://arxiv.org/abs/2210.11512",
    "context": "Title: Communication breakdown: On the low mutual intelligibility between human and neural captioning. (arXiv:2210.11512v2 [cs.CL] UPDATED)\nAbstract: We compare the 0-shot performance of a neural caption-based image retriever when given as input either human-produced captions or captions generated by a neural captioner. We conduct this comparison on the recently introduced ImageCoDe data-set (Krojer et al., 2022) which contains hard distractors nearly identical to the images to be retrieved. We find that the neural retriever has much higher performance when fed neural rather than human captions, despite the fact that the former, unlike the latter, were generated without awareness of the distractors that make the task hard. Even more remarkably, when the same neural captions are given to human subjects, their retrieval performance is almost at chance level. Our results thus add to the growing body of evidence that, even when the ``language'' of neural models resembles English, this superficial resemblance might be deeply misleading.",
    "path": "papers/22/10/2210.11512.json",
    "total_tokens": 875,
    "translated_title": "沟通破裂：人类和神经字幕之间的低互通性",
    "translated_abstract": "我们比较了基于神经字幕的图像检索器的0-shot表现，当输入人类制作的字幕或神经字幕生成的字幕时。我们在最近推出的ImageCoDe数据集上进行了这个比较(Krojer等人，2022)，该数据集包含与待检索图像几乎相同的难以区分的干扰项。我们发现，当输入神经字幕而不是人类字幕时，神经检索器的性能要高得多，尽管前者不像后者那样在生成时意识到任务变得困难的干扰项。更为明显的是，当将相同的神经字幕提供给人类受试者时，它们的检索表现几乎处于随机水平。因此，我们的研究结果是对越来越多的证据的补充，即即使神经模型的“语言”类似于英语，这种表面上的类似可能是非常误导性的。",
    "tldr": "研究表明，给定相同的字幕，神经图像检索器的性能比人类要高得多，而人类在给定相同的神经字幕时的表现几乎与随机无异。",
    "en_tdlr": "The study shows that the neural image retriever performs much better with neural captions than human-produced captions, while humans perform almost at chance level when given the same neural captions. This highlights the low mutual intelligibility between human and neural captioning."
}