{
    "title": "Differentially Private Diffusion Models. (arXiv:2210.09929v2 [stat.ML] UPDATED)",
    "abstract": "While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been dem",
    "link": "http://arxiv.org/abs/2210.09929",
    "context": "Title: Differentially Private Diffusion Models. (arXiv:2210.09929v2 [stat.ML] UPDATED)\nAbstract: While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been dem",
    "path": "papers/22/10/2210.09929.json",
    "total_tokens": 919,
    "translated_title": "差分隐私扩散模型",
    "translated_abstract": "现代机器学习模型依赖于越来越大的训练数据集，然而在涉及隐私的领域，数据通常是有限的。通过差分隐私训练的生成模型可以绕过这一挑战，提供对合成数据的访问。本文在扩散模型的最新成功基础上，引入了差分隐私扩散模型(DPDMs)，使用差分隐私随机梯度下降(DP-SGD)来实现隐私保护。我们研究了DPDM中的参数化和采样算法，并提出了噪声多样性，这是一个针对DM训练的强大改进。我们在图像生成基准测试中验证了我们的新颖DPDMs，并在所有实验证明了最先进的性能。此外，在标准基准测试中，使用DPDM生成的合成数据训练的分类器表现与特定任务的DP-SGD训练的分类器相当，这在以往的研究中没有被证明。",
    "tldr": "本研究提出了一种差分隐私扩散模型(DPDMs)，通过差分隐私训练生成模型，实现对隐私的保护，在图像生成基准测试中表现优越，能够在标准测试中与特定任务的DP-SGD训练的分类器相媲美。"
}