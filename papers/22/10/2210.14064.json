{
    "title": "Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets. (arXiv:2210.14064v3 [cs.LG] UPDATED)",
    "abstract": "Overparameterization in deep learning typically refers to settings where a trained neural network (NN) has representational capacity to fit the training data in many ways, some of which generalize well, while others do not. In the case of Recurrent Neural Networks (RNNs), there exists an additional layer of overparameterization, in the sense that a model may exhibit many solutions that generalize well for sequence lengths seen in training, some of which extrapolate to longer sequences, while others do not. Numerous works have studied the tendency of Gradient Descent (GD) to fit overparameterized NNs with solutions that generalize well. On the other hand, its tendency to fit overparameterized RNNs with solutions that extrapolate has been discovered only recently and is far less understood. In this paper, we analyze the extrapolation properties of GD when applied to overparameterized linear RNNs. In contrast to recent arguments suggesting an implicit bias towards short-term memory, we pr",
    "link": "http://arxiv.org/abs/2210.14064",
    "context": "Title: Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets. (arXiv:2210.14064v3 [cs.LG] UPDATED)\nAbstract: Overparameterization in deep learning typically refers to settings where a trained neural network (NN) has representational capacity to fit the training data in many ways, some of which generalize well, while others do not. In the case of Recurrent Neural Networks (RNNs), there exists an additional layer of overparameterization, in the sense that a model may exhibit many solutions that generalize well for sequence lengths seen in training, some of which extrapolate to longer sequences, while others do not. Numerous works have studied the tendency of Gradient Descent (GD) to fit overparameterized NNs with solutions that generalize well. On the other hand, its tendency to fit overparameterized RNNs with solutions that extrapolate has been discovered only recently and is far less understood. In this paper, we analyze the extrapolation properties of GD when applied to overparameterized linear RNNs. In contrast to recent arguments suggesting an implicit bias towards short-term memory, we pr",
    "path": "papers/22/10/2210.14064.json",
    "total_tokens": 998,
    "translated_title": "用超参数化循环神经网络学习低维状态空间",
    "translated_abstract": "在深度学习中，过度参数化通常是指训练出来的神经网络具有多种表达能力，可以同时适合训练数据的多种方式，其中一些能够很好地推广，而另一些则不能。在循环神经网络（RNN）中，存在一层额外的过度参数化，意味着模型可能存在许多解，适用于训练中看到的序列长度，其中一些可以推广到较长序列，而另一些则不能。本文分析了当应用于超参数化线性RNN时，GD的外推属性。与最近提出的暗示GD偏向于短期记忆的论点相反，我们提供了证据表明GD可以学习低维状态空间，捕获长期记忆。具体而言，在对超完备线性RNN的权重做出某些假设的情况下，损失景观包含一个模式，在该模式下，参数数量超过了表示底层时间序列长期动态所需的维数。在这种情况下，GD会收敛于一个解，实现捕获长期动态所需的最小维度。",
    "tldr": "本文通过对超参数化线性RNN的权重假设，发现GD可以学习低维度状态空间，捕获长期动态。"
}