{
    "title": "A Policy-Guided Imitation Approach for Offline Reinforcement Learning. (arXiv:2210.08323v3 [cs.LG] UPDATED)",
    "abstract": "Offline reinforcement learning (RL) methods can generally be categorized into two types: RL-based and Imitation-based. RL-based methods could in principle enjoy out-of-distribution generalization but suffer from erroneous off-policy evaluation. Imitation-based methods avoid off-policy evaluation but are too conservative to surpass the dataset. In this study, we propose an alternative approach, inheriting the training stability of imitation-style methods while still allowing logical out-of-distribution generalization. We decompose the conventional reward-maximizing policy in offline RL into a guide-policy and an execute-policy. During training, the guide-poicy and execute-policy are learned using only data from the dataset, in a supervised and decoupled manner. During evaluation, the guide-policy guides the execute-policy by telling where it should go so that the reward can be maximized, serving as the \\textit{Prophet}. By doing so, our algorithm allows \\textit{state-compositionality} f",
    "link": "http://arxiv.org/abs/2210.08323",
    "context": "Title: A Policy-Guided Imitation Approach for Offline Reinforcement Learning. (arXiv:2210.08323v3 [cs.LG] UPDATED)\nAbstract: Offline reinforcement learning (RL) methods can generally be categorized into two types: RL-based and Imitation-based. RL-based methods could in principle enjoy out-of-distribution generalization but suffer from erroneous off-policy evaluation. Imitation-based methods avoid off-policy evaluation but are too conservative to surpass the dataset. In this study, we propose an alternative approach, inheriting the training stability of imitation-style methods while still allowing logical out-of-distribution generalization. We decompose the conventional reward-maximizing policy in offline RL into a guide-policy and an execute-policy. During training, the guide-poicy and execute-policy are learned using only data from the dataset, in a supervised and decoupled manner. During evaluation, the guide-policy guides the execute-policy by telling where it should go so that the reward can be maximized, serving as the \\textit{Prophet}. By doing so, our algorithm allows \\textit{state-compositionality} f",
    "path": "papers/22/10/2210.08323.json",
    "total_tokens": 1439,
    "translated_title": "离线强化学习的策略指导模仿方法",
    "translated_abstract": "在离线强化学习中，通常可以将方法分为两种：基于强化学习和基于模仿学习。基于强化学习的方法原则上可以享受超出分布的泛化，但会出现错误的离策略评估。基于模仿学习的方法避免了离策略评估，但过于保守，难以超越数据集。本研究提出了一种替代方法，继承了模仿式方法的训练稳定性，同时仍允许逻辑上的超出分布泛化。我们将离线强化学习中传统的奖励最大化策略分解成引导策略和执行策略。在训练期间，引导策略和执行策略仅使用数据集中的数据，以监督和解耦的方式进行学习。在评估期间，引导策略通过告诉执行策略应该去哪里以最大化奖励，来指引执行策略的走向，作为“先知”。如此，我们的算法允许“状态组合性”，从而实现了合理的超出分布泛化。",
    "tldr": "该论文提出了一种策略指导模仿方法，它将传统的奖励最大化策略分解成引导策略和执行策略。在训练期间，引导策略和执行策略在仅使用数据集中的数据的情况下进行学习。在评估期间，引导策略通过指引执行策略以最大化奖励，起到“先知”的作用，允许合理的超出分布泛化。"
}