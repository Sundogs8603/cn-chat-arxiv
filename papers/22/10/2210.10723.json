{
    "title": "TabLLM: Few-shot Classification of Tabular Data with Large Language Models. (arXiv:2210.10723v2 [cs.CL] UPDATED)",
    "abstract": "We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting",
    "link": "http://arxiv.org/abs/2210.10723",
    "context": "Title: TabLLM: Few-shot Classification of Tabular Data with Large Language Models. (arXiv:2210.10723v2 [cs.CL] UPDATED)\nAbstract: We study the application of large language models to zero-shot and few-shot classification of tabular data. We prompt the large language model with a serialization of the tabular data to a natural-language string, together with a short description of the classification problem. In the few-shot setting, we fine-tune the large language model using some labeled examples. We evaluate several serialization methods including templates, table-to-text models, and large language models. Despite its simplicity, we find that this technique outperforms prior deep-learning-based tabular classification methods on several benchmark datasets. In most cases, even zero-shot classification obtains non-trivial performance, illustrating the method's ability to exploit prior knowledge encoded in large language models. Unlike many deep learning methods for tabular datasets, this approach is also competitive with strong traditional baselines like gradient-boosted trees, especially in the very-few-shot setting",
    "path": "papers/22/10/2210.10723.json",
    "total_tokens": 840,
    "translated_title": "TabLLM: 大语言模型在少样本表格数据分类中的应用",
    "translated_abstract": "本文研究了大语言模型在零样本和少样本表格数据分类中的应用。我们将表格数据序列化为自然语言字符串，并加上分类问题的简短描述，然后启用大语言模型。在少样本场景下，我们使用一些标记样本微调大语言模型。我们评估了多种序列化方法，包括模板、表格到文本模型和大语言模型。尽管方法简单，但我们发现它在多个基准数据集上优于以前的基于深度学习的表格分类方法。在大多数情况下，即使是零样本分类也获得了非平凡的表现，说明该方法能够利用大语言模型中编码的先前知识。与许多针对表格数据的深度学习方法不同，这种方法在非常少的样本设置下也与强大的传统基线方法（如梯度提升树）竞争力十足。",
    "tldr": "本文应用大语言模型将表格数据序列化为自然语言字符串进行分类，微调后即可在非常少的样本设置下与传统基线方法竞争力十足。",
    "en_tdlr": "This paper applies large language models to serialize tabular data into natural-language strings for classification and fine-tuning, achieving competitive performance even in very few-shot settings with traditional baselines."
}