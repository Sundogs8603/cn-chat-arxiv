{
    "title": "PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits. (arXiv:2210.15345v2 [stat.ML] UPDATED)",
    "abstract": "In sparse linear bandits, a learning agent sequentially selects an action and receive reward feedback, and the reward function depends linearly on a few coordinates of the covariates of the actions. This has applications in many real-world sequential decision making problems. In this paper, we propose a simple and computationally efficient sparse linear estimation method called PopArt that enjoys a tighter $\\ell_1$ recovery guarantee compared to Lasso (Tibshirani, 1996) in many problems. Our bound naturally motivates an experimental design criterion that is convex and thus computationally efficient to solve. Based on our novel estimator and design criterion, we derive sparse linear bandit algorithms that enjoy improved regret upper bounds upon the state of the art (Hao et al., 2020), especially w.r.t. the geometry of the given action set. Finally, we prove a matching lower bound for sparse linear bandits in the data-poor regime, which closes the gap between upper and lower bounds in pr",
    "link": "http://arxiv.org/abs/2210.15345",
    "context": "Title: PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits. (arXiv:2210.15345v2 [stat.ML] UPDATED)\nAbstract: In sparse linear bandits, a learning agent sequentially selects an action and receive reward feedback, and the reward function depends linearly on a few coordinates of the covariates of the actions. This has applications in many real-world sequential decision making problems. In this paper, we propose a simple and computationally efficient sparse linear estimation method called PopArt that enjoys a tighter $\\ell_1$ recovery guarantee compared to Lasso (Tibshirani, 1996) in many problems. Our bound naturally motivates an experimental design criterion that is convex and thus computationally efficient to solve. Based on our novel estimator and design criterion, we derive sparse linear bandit algorithms that enjoy improved regret upper bounds upon the state of the art (Hao et al., 2020), especially w.r.t. the geometry of the given action set. Finally, we prove a matching lower bound for sparse linear bandits in the data-poor regime, which closes the gap between upper and lower bounds in pr",
    "path": "papers/22/10/2210.15345.json",
    "total_tokens": 953,
    "translated_title": "PopArt: 高效稀疏回归和优化稀疏线性摇臂的实验设计",
    "translated_abstract": "在稀疏线性摇臂中，学习代理按顺序选择一个动作并接收奖励反馈，而奖励函数线性依赖于动作的一些坐标的协变量。这在许多实际的顺序决策问题中都有应用。在本文中，我们提出了一种简单而计算高效的稀疏线性估计方法，称为PopArt，与Lasso（Tibshirani, 1996）相比，它在许多问题中具有更紧的$\\ell_1$恢复保证。我们的界限自然地激发了一种凸实验设计准则，因此在计算上是高效的解决方法。基于我们的新估计器和设计准则，我们推导出稀疏线性摇臂算法，其在给定动作集的几何性方面相对于现有技术（Hao et al., 2020）具有改进的遗憾上界。最后，我们证明了在数据稀缺情况下稀疏线性摇臂的匹配下界，这填补了上界和下界之间的差距。",
    "tldr": "本文提出了一种称为PopArt的高效稀疏线性估计方法，相比于Lasso，在许多问题中具有更紧的$\\ell_1$恢复保证，并基于此推导出稀疏线性摇臂算法，具有改进的遗憾上界。同时，我们证明了在数据稀缺情况下稀疏线性摇臂的匹配下界。"
}