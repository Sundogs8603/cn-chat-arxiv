{
    "title": "AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation. (arXiv:2210.07535v2 [cs.CL] UPDATED)",
    "abstract": "Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows",
    "link": "http://arxiv.org/abs/2210.07535",
    "context": "Title: AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation. (arXiv:2210.07535v2 [cs.CL] UPDATED)\nAbstract: Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows",
    "path": "papers/22/10/2210.07535.json",
    "total_tokens": 917,
    "translated_title": "AutoMoE：自适应计算的异构专家混合体，在神经机器翻译中实现高效。",
    "translated_abstract": "在神经机器翻译（NMT）任务中，专家混合体（MoE）模型获得了最先进的性能。MoE的现有工作主要考虑同质设计，其中相同数量的相同大小的专家均匀地放置在整个网络中。此外，现有的MoE工作没有考虑计算约束（例如FLOPs、延迟）来指导其设计。为此，我们开发了AutoMoE--一个在计算约束下设计异构MoE的框架。AutoMoE利用神经架构搜索（NAS）来获取高效的稀疏MoE子变压器，具有4倍推理速度优化（CPU）和FLOPs减少，相对于手动设计的Transformer，在NMT基准数据集上实现了BLEU分数的平稳性。采用密集和稀疏激活的Transformer模块的异构搜索空间（例如有多少专家？在哪里放置它们？它们的大小应该是多少？）允许更好地探索模型设计空间，最大限度地提高资源利用率。",
    "tldr": "AutoMoE提出了一种能够在计算约束下设计异构MoE的框架，它在神经机器翻译任务中实现了高效且最先进的性能。"
}