{
    "title": "Privately Fine-Tuning Large Language Models with Differential Privacy. (arXiv:2210.15042v3 [cs.CR] UPDATED)",
    "abstract": "Pre-trained Large Language Models (LLMs) are an integral part of modern AI that have led to breakthrough performances in complex AI tasks. Major AI companies with expensive infrastructures are able to develop and train these large models with billions and millions of parameters from scratch. Third parties, researchers, and practitioners are increasingly adopting these pre-trained models and fine-tuning them on their private data to accomplish their downstream AI tasks. However, it has been shown that an adversary can extract/reconstruct the exact training samples from these LLMs, which can lead to revealing personally identifiable information. The issue has raised deep concerns about the privacy of LLMs. Differential privacy (DP) provides a rigorous framework that allows adding noise in the process of training or fine-tuning LLMs such that extracting the training data becomes infeasible (i.e., with a cryptographically small success probability). While the theoretical privacy guarantees",
    "link": "http://arxiv.org/abs/2210.15042",
    "context": "Title: Privately Fine-Tuning Large Language Models with Differential Privacy. (arXiv:2210.15042v3 [cs.CR] UPDATED)\nAbstract: Pre-trained Large Language Models (LLMs) are an integral part of modern AI that have led to breakthrough performances in complex AI tasks. Major AI companies with expensive infrastructures are able to develop and train these large models with billions and millions of parameters from scratch. Third parties, researchers, and practitioners are increasingly adopting these pre-trained models and fine-tuning them on their private data to accomplish their downstream AI tasks. However, it has been shown that an adversary can extract/reconstruct the exact training samples from these LLMs, which can lead to revealing personally identifiable information. The issue has raised deep concerns about the privacy of LLMs. Differential privacy (DP) provides a rigorous framework that allows adding noise in the process of training or fine-tuning LLMs such that extracting the training data becomes infeasible (i.e., with a cryptographically small success probability). While the theoretical privacy guarantees",
    "path": "papers/22/10/2210.15042.json",
    "total_tokens": 707,
    "translated_title": "差分隐私保护下的大规模语言模型私有微调",
    "translated_abstract": "预训练的大规模语言模型（LLM）是现代人工智能的重要组成部分，已经在复杂的人工智能任务中实现了突破性表现。然而，已经证明，攻击者可以从这些LLM中提取/重建出确切的训练样本，这可能会导致泄露个人识别信息。差分隐私（DP）提供了一个严谨的框架，允许在训练或微调LLM过程中添加噪声，使提取训练数据变得不可行。",
    "tldr": "该论文介绍了如何使用差分隐私保护方法在微调大规模语言模型时防止个人信息泄露。",
    "en_tdlr": "This paper introduces how to use differential privacy protection method to prevent personal information leakage when fine-tuning large-scale language models."
}