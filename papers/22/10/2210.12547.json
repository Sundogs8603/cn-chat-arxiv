{
    "title": "SurCo: Learning Linear Surrogates For Combinatorial Nonlinear Optimization Problems. (arXiv:2210.12547v2 [cs.LG] UPDATED)",
    "abstract": "Optimization problems with nonlinear cost functions and combinatorial constraints appear in many real-world applications but remain challenging to solve efficiently compared to their linear counterparts. To bridge this gap, we propose $\\textbf{SurCo}$ that learns linear $\\underline{\\text{Sur}}$rogate costs which can be used in existing $\\underline{\\text{Co}}$mbinatorial solvers to output good solutions to the original nonlinear combinatorial optimization problem. The surrogate costs are learned end-to-end with nonlinear loss by differentiating through the linear surrogate solver, combining the flexibility of gradient-based methods with the structure of linear combinatorial optimization. We propose three $\\texttt{SurCo}$ variants: $\\texttt{SurCo}-\\texttt{zero}$ for individual nonlinear problems, $\\texttt{SurCo}-\\texttt{prior}$ for problem distributions, and $\\texttt{SurCo}-\\texttt{hybrid}$ to combine both distribution and problem-specific information. We give theoretical intuition motiv",
    "link": "http://arxiv.org/abs/2210.12547",
    "context": "Title: SurCo: Learning Linear Surrogates For Combinatorial Nonlinear Optimization Problems. (arXiv:2210.12547v2 [cs.LG] UPDATED)\nAbstract: Optimization problems with nonlinear cost functions and combinatorial constraints appear in many real-world applications but remain challenging to solve efficiently compared to their linear counterparts. To bridge this gap, we propose $\\textbf{SurCo}$ that learns linear $\\underline{\\text{Sur}}$rogate costs which can be used in existing $\\underline{\\text{Co}}$mbinatorial solvers to output good solutions to the original nonlinear combinatorial optimization problem. The surrogate costs are learned end-to-end with nonlinear loss by differentiating through the linear surrogate solver, combining the flexibility of gradient-based methods with the structure of linear combinatorial optimization. We propose three $\\texttt{SurCo}$ variants: $\\texttt{SurCo}-\\texttt{zero}$ for individual nonlinear problems, $\\texttt{SurCo}-\\texttt{prior}$ for problem distributions, and $\\texttt{SurCo}-\\texttt{hybrid}$ to combine both distribution and problem-specific information. We give theoretical intuition motiv",
    "path": "papers/22/10/2210.12547.json",
    "total_tokens": 849,
    "translated_title": "SurCo：学习用于组合非线性优化问题的线性代理",
    "translated_abstract": "在许多实际应用中，具有非线性代价函数和组合约束的优化问题与其线性对应物相比，仍然具有挑战性，难以高效求解。为了弥合这一差距，我们提出了SurCo，它学习线性代理成本，可用于现有的组合求解器，以输出原始非线性组合优化问题的良好解决方案。通过对线性代理求解器进行端到端的学习，通过非线性损失函数微分，将基于梯度的方法的灵活性与线性组合优化的结构相结合。我们提出了三个SurCo变体：SurCo-zero用于单个非线性问题，SurCo-prior用于问题分布，SurCo-hybrid用于结合分布和问题特定信息。我们给出了理论上的直觉动机",
    "tldr": "该论文介绍了一种名为SurCo的方法，通过学习线性代理成本，将组合非线性优化问题转化为线性问题，并通过结合梯度方法和线性组合优化的结构提供了高效解决方案。",
    "en_tdlr": "This paper presents SurCo, a method that learns linear surrogate costs to transform combinatorial nonlinear optimization problems into linear problems, providing efficient solutions by combining gradient-based methods with the structure of linear combinatorial optimization."
}