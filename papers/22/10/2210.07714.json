{
    "title": "CrowdGuard: Federated Backdoor Detection in Federated Learning. (arXiv:2210.07714v3 [cs.CR] UPDATED)",
    "abstract": "Federated Learning (FL) is a promising approach enabling multiple clients to train Deep Neural Networks (DNNs) collaboratively without sharing their local training data. However, FL is susceptible to backdoor (or targeted poisoning) attacks. These attacks are initiated by malicious clients who seek to compromise the learning process by introducing specific behaviors into the learned model that can be triggered by carefully crafted inputs. Existing FL safeguards have various limitations: They are restricted to specific data distributions or reduce the global model accuracy due to excluding benign models or adding noise, are vulnerable to adaptive defense-aware adversaries, or require the server to access local models, allowing data inference attacks.  This paper presents a novel defense mechanism, CrowdGuard, that effectively mitigates backdoor attacks in FL and overcomes the deficiencies of existing techniques. It leverages clients' feedback on individual models, analyzes the behavior ",
    "link": "http://arxiv.org/abs/2210.07714",
    "context": "Title: CrowdGuard: Federated Backdoor Detection in Federated Learning. (arXiv:2210.07714v3 [cs.CR] UPDATED)\nAbstract: Federated Learning (FL) is a promising approach enabling multiple clients to train Deep Neural Networks (DNNs) collaboratively without sharing their local training data. However, FL is susceptible to backdoor (or targeted poisoning) attacks. These attacks are initiated by malicious clients who seek to compromise the learning process by introducing specific behaviors into the learned model that can be triggered by carefully crafted inputs. Existing FL safeguards have various limitations: They are restricted to specific data distributions or reduce the global model accuracy due to excluding benign models or adding noise, are vulnerable to adaptive defense-aware adversaries, or require the server to access local models, allowing data inference attacks.  This paper presents a novel defense mechanism, CrowdGuard, that effectively mitigates backdoor attacks in FL and overcomes the deficiencies of existing techniques. It leverages clients' feedback on individual models, analyzes the behavior ",
    "path": "papers/22/10/2210.07714.json",
    "total_tokens": 911,
    "translated_title": "CrowdGuard：联邦学习中的联邦后门检测",
    "translated_abstract": "联邦学习是一种有前途的方法，使多个客户端在不分享本地训练数据的情况下协作训练深度神经网络（DNN）。然而，联邦学习容易受到后门（或有针对性的毒化）攻击的影响。这些攻击是由恶意客户端发起的，他们试图通过引入特定行为到学习模型中来破坏学习过程，这些行为可以由精心设计的输入触发。现有的联邦学习安全防护措施存在各种限制：它们仅限于特定的数据分布，或者由于排除良性模型或添加噪音而降低全局模型精度，易受到具有适应性防御意识的对手的攻击，或要求服务器访问本地模型，从而容易受到数据推断攻击。本文提出了一种新颖的防御机制CrowdGuard，它有效地减轻了联邦学习中的后门攻击，并克服了现有技术的不足之处。它利用客户对个别模型的反馈，分析模型的行为。",
    "tldr": "CrowdGuard是一种在联邦学习中有效防御后门攻击的新机制，通过利用客户对个别模型的反馈分析行为，克服了现有技术的不足。",
    "en_tdlr": "CrowdGuard is a novel mechanism that effectively mitigates backdoor attacks in federated learning by leveraging clients' feedback on individual models and overcomes the deficiencies of existing techniques."
}