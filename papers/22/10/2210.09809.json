{
    "title": "Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel. (arXiv:2210.09809v2 [cs.LG] UPDATED)",
    "abstract": "The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution' in conjunction with a suitable choice for the network architecture, such as depth and activation functions. Therefore, understanding the influence of each of the design choice on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix as the most widely adopted one. However, some empirical studies show that row normalization of the adjacency matrix outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolutions, that could explain this behavior. Similarly, the empirical observation of the linear GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory.  In this work, we theoretically a",
    "link": "http://arxiv.org/abs/2210.09809",
    "context": "Title: Analysis of Convolutions, Non-linearity and Depth in Graph Neural Networks using Neural Tangent Kernel. (arXiv:2210.09809v2 [cs.LG] UPDATED)\nAbstract: The fundamental principle of Graph Neural Networks (GNNs) is to exploit the structural information of the data by aggregating the neighboring nodes using a `graph convolution' in conjunction with a suitable choice for the network architecture, such as depth and activation functions. Therefore, understanding the influence of each of the design choice on the network performance is crucial. Convolutions based on graph Laplacian have emerged as the dominant choice with the symmetric normalization of the adjacency matrix as the most widely adopted one. However, some empirical studies show that row normalization of the adjacency matrix outperforms it in node classification. Despite the widespread use of GNNs, there is no rigorous theoretical study on the representation power of these convolutions, that could explain this behavior. Similarly, the empirical observation of the linear GNNs performance being on par with non-linear ReLU GNNs lacks rigorous theory.  In this work, we theoretically a",
    "path": "papers/22/10/2210.09809.json",
    "total_tokens": 932,
    "translated_title": "使用神经切线核对图神经网络中的卷积，非线性和深度进行分析",
    "translated_abstract": "图神经网络（GNN）的基本原理是通过使用“图卷积”来聚合相邻节点的结构信息，并选择合适的网络架构（例如深度和激活函数）。因此，理解每个设计选择对网络性能的影响至关重要。基于图拉普拉斯的卷积已成为主流选择，其中对邻接矩阵进行对称归一化是最广泛采用的方法。然而，一些经验研究表明，行归一化的邻接矩阵在节点分类方面表现更好。尽管GNN的使用非常广泛，但目前尚无严格的理论研究关于这些卷积的表示能力，无法解释这种行为。同样，线性GNN的性能与非线性ReLU-GNN的性能相当的经验观察也缺乏严格的理论支持。",
    "tldr": "本研究通过理论方法分析了图神经网络中的卷积、非线性和深度对网络性能的影响，同时对基于图拉普拉斯和邻接矩阵的归一化方法进行了比较，并揭示了线性GNN与非线性ReLU-GNN性能相当的现象缺乏严格的理论解释。"
}