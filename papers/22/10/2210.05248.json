{
    "title": "Self-supervised debiasing using low rank regularization. (arXiv:2210.05248v2 [cs.LG] UPDATED)",
    "abstract": "Spurious correlations can cause strong biases in deep neural networks, impairing generalization ability. While most existing debiasing methods require full supervision on either spurious attributes or target labels, training a debiased model from a limited amount of both annotations is still an open question. To address this issue, we investigate an interesting phenomenon using the spectral analysis of latent representations: spuriously correlated attributes make neural networks inductively biased towards encoding lower effective rank representations. We also show that a rank regularization can amplify this bias in a way that encourages highly correlated features. Leveraging these findings, we propose a self-supervised debiasing framework potentially compatible with unlabeled samples. Specifically, we first pretrain a biased encoder in a self-supervised manner with the rank regularization, serving as a semantic bottleneck to enforce the encoder to learn the spuriously correlated attrib",
    "link": "http://arxiv.org/abs/2210.05248",
    "context": "Title: Self-supervised debiasing using low rank regularization. (arXiv:2210.05248v2 [cs.LG] UPDATED)\nAbstract: Spurious correlations can cause strong biases in deep neural networks, impairing generalization ability. While most existing debiasing methods require full supervision on either spurious attributes or target labels, training a debiased model from a limited amount of both annotations is still an open question. To address this issue, we investigate an interesting phenomenon using the spectral analysis of latent representations: spuriously correlated attributes make neural networks inductively biased towards encoding lower effective rank representations. We also show that a rank regularization can amplify this bias in a way that encourages highly correlated features. Leveraging these findings, we propose a self-supervised debiasing framework potentially compatible with unlabeled samples. Specifically, we first pretrain a biased encoder in a self-supervised manner with the rank regularization, serving as a semantic bottleneck to enforce the encoder to learn the spuriously correlated attrib",
    "path": "papers/22/10/2210.05248.json",
    "total_tokens": 891,
    "translated_title": "自监督的低秩正则化去偏方法",
    "translated_abstract": "虚假相关性可能导致深度神经网络中的强偏见，影响其泛化能力。虽然大多数现有的去偏方法要求对虚假属性或目标标签进行完全监督，但如何仅通过有限的注释数据训练一个去偏模型仍然是一个开放问题。为了解决这个问题，我们通过对潜在表示进行谱分析研究了一个有趣的现象：虚假相关属性使神经网络归纳地偏向编码较低有效秩表示。我们还展示了秩正则化可以放大这种偏差，以鼓励高度相关的特征。基于这些发现，我们提出了一个自监督的去偏框架，可能与无标签样本兼容。具体而言，我们首先通过秩正则化以自监督的方式预训练一个有偏编码器，作为语义瓶颈来强制编码器学习虚假相关属性。",
    "tldr": "本研究通过对潜在表示的谱分析发现，虚假相关属性会导致深度神经网络偏向编码较低有效秩的表示。在此基础上，提出了一种自监督的去偏框架，通过秩正则化预训练有偏编码器来学习虚假相关属性。",
    "en_tdlr": "This study discovers that spurious correlations in deep neural networks lead to biases towards lower effective rank representations, and proposes a self-supervised debiasing framework utilizing rank regularization to learn these spurious correlations."
}