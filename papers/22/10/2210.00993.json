{
    "title": "Efficient Bayes Inference in Neural Networks through Adaptive Importance Sampling. (arXiv:2210.00993v2 [cs.LG] UPDATED)",
    "abstract": "Bayesian neural networks (BNNs) have received an increased interest in the last years. In BNNs, a complete posterior distribution of the unknown weight and bias parameters of the network is produced during the training stage. This probabilistic estimation offers several advantages with respect to point-wise estimates, in particular, the ability to provide uncertainty quantification when predicting new data. This feature inherent to the Bayesian paradigm, is useful in countless machine learning applications. It is particularly appealing in areas where decision-making has a crucial impact, such as medical healthcare or autonomous driving. The main challenge of BNNs is the computational cost of the training procedure since Bayesian techniques often face a severe curse of dimensionality. Adaptive importance sampling (AIS) is one of the most prominent Monte Carlo methodologies benefiting from sounded convergence guarantees and ease for adaptation. This work aims to show that AIS constitutes",
    "link": "http://arxiv.org/abs/2210.00993",
    "context": "Title: Efficient Bayes Inference in Neural Networks through Adaptive Importance Sampling. (arXiv:2210.00993v2 [cs.LG] UPDATED)\nAbstract: Bayesian neural networks (BNNs) have received an increased interest in the last years. In BNNs, a complete posterior distribution of the unknown weight and bias parameters of the network is produced during the training stage. This probabilistic estimation offers several advantages with respect to point-wise estimates, in particular, the ability to provide uncertainty quantification when predicting new data. This feature inherent to the Bayesian paradigm, is useful in countless machine learning applications. It is particularly appealing in areas where decision-making has a crucial impact, such as medical healthcare or autonomous driving. The main challenge of BNNs is the computational cost of the training procedure since Bayesian techniques often face a severe curse of dimensionality. Adaptive importance sampling (AIS) is one of the most prominent Monte Carlo methodologies benefiting from sounded convergence guarantees and ease for adaptation. This work aims to show that AIS constitutes",
    "path": "papers/22/10/2210.00993.json",
    "total_tokens": 1089,
    "tldr": "本研究展示了通过自适应重要性采样方法可以高效地进行贝叶斯神经网络的推理，有效地解决了训练过程中的计算成本问题。"
}