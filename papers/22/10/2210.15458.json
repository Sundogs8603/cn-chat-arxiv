{
    "title": "Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models. (arXiv:2210.15458v2 [cs.CL] UPDATED)",
    "abstract": "Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a different output for each element of the beam, but are not easy to parallelize. Alternatively, methods such as temperature sampling and its modifications (top-k sampling, nucleus sampling, typical decoding, and others), are embarrassingly parallel, but have no guarantees about duplicate samples. We present a framework for sampling according to an arithmetic code book implicitly defined by a large language model, compatible with common sampling variations, with provable beam diversity under certain conditions, as well as being embarrassingly parallel and providing unbiased and consistent expectations from the original model. We demonstrate the effectiveness of our approach on WMT machine translation, more than halving the standard deviation when estimating expected BLEU score reward, and closing the ",
    "link": "http://arxiv.org/abs/2210.15458",
    "context": "Title: Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models. (arXiv:2210.15458v2 [cs.CL] UPDATED)\nAbstract: Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a different output for each element of the beam, but are not easy to parallelize. Alternatively, methods such as temperature sampling and its modifications (top-k sampling, nucleus sampling, typical decoding, and others), are embarrassingly parallel, but have no guarantees about duplicate samples. We present a framework for sampling according to an arithmetic code book implicitly defined by a large language model, compatible with common sampling variations, with provable beam diversity under certain conditions, as well as being embarrassingly parallel and providing unbiased and consistent expectations from the original model. We demonstrate the effectiveness of our approach on WMT machine translation, more than halving the standard deviation when estimating expected BLEU score reward, and closing the ",
    "path": "papers/22/10/2210.15458.json",
    "total_tokens": 815,
    "translated_title": "算术采样：用于大型语言模型的并行多样化解码",
    "translated_abstract": "大型语言模型的解码方法通常在输出多样性和计算并行性之间进行权衡。本文提出了一种框架，根据由大型语言模型隐式定义的算术代码书进行采样，兼容常见的采样变化，满足一定条件下的可证明的束多样性，同时具有令人尴尬的并行性，并从原始模型提供无偏和一致的期望。我们在WMT机器翻译上展示了我们方法的有效性，将预期的BLEU分数奖励的标准差减少了一半以上，同时与先前的最新方法有了相当的表现。",
    "tldr": "本文章提出了一种算术采样框架，该方法可兼容常见的采样变化，具有可证明的束多样性和令人尴尬的并行性，从原始模型提供无偏和一致的期望。在WMT机器翻译中表现出良好的效果。",
    "en_tdlr": "This paper proposes an arithmetic sampling framework which is compatible with common sampling variations and provides provable beam diversity under certain conditions, while featuring embarrassingly parallelism and unbiased expectation from the original model. The approach shows its effectiveness on WMT machine translation."
}