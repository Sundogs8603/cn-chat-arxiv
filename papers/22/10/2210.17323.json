{
    "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. (arXiv:2210.17323v2 [cs.LG] UPDATED)",
    "abstract": "Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation rel",
    "link": "http://arxiv.org/abs/2210.17323",
    "context": "Title: GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. (arXiv:2210.17323v2 [cs.LG] UPDATED)\nAbstract: Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation rel",
    "path": "papers/22/10/2210.17323.json",
    "total_tokens": 739,
    "translated_title": "GPTQ: 面向生成预训练transformer的准确后训练量化方法",
    "translated_abstract": "生成预训练transformer模型以其在复杂语言建模任务中的突破性表现和极高的计算和存储成本而脱颖而出。本文提出了一种基于近似二阶信息的新型一次性权重量化方法GPTQ，可以在约四个GPU小时内将GPT模型量化为每个权重3或4比特，精度几乎没有降低。",
    "tldr": "本文提出了一种名为GPTQ的新型一次性权重量化方法，可在高度准确和高度有效的同时将比特宽度降至每个权重3或4位，适用于巨大的GPT模型。",
    "en_tdlr": "This paper proposes a new one-shot weight quantization method called GPTQ, which can quantize GPT models with 175 billion parameters to 3 or 4 bits per weight, with negligible accuracy degradation. It is highly accurate and efficient and suitable for huge GPT models."
}