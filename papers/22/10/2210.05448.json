{
    "title": "A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning. (arXiv:2210.05448v2 [cs.MA] UPDATED)",
    "abstract": "Open ad hoc teamwork is the problem of training a single agent to efficiently collaborate with an unknown group of teammates whose composition may change over time. A variable team composition creates challenges for the agent, such as the requirement to adapt to new team dynamics and dealing with changing state vector sizes. These challenges are aggravated in real-world applications in which the controlled agent only has a partial view of the environment. In this work, we develop a class of solutions for open ad hoc teamwork under full and partial observability. We start by developing a solution for the fully observable case that leverages graph neural network architectures to obtain an optimal policy based on reinforcement learning. We then extend this solution to partially observable scenarios by proposing different methodologies that maintain belief estimates over the latent environment states and team composition. These belief estimates are combined with our solution for the fully ",
    "link": "http://arxiv.org/abs/2210.05448",
    "context": "Title: A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning. (arXiv:2210.05448v2 [cs.MA] UPDATED)\nAbstract: Open ad hoc teamwork is the problem of training a single agent to efficiently collaborate with an unknown group of teammates whose composition may change over time. A variable team composition creates challenges for the agent, such as the requirement to adapt to new team dynamics and dealing with changing state vector sizes. These challenges are aggravated in real-world applications in which the controlled agent only has a partial view of the environment. In this work, we develop a class of solutions for open ad hoc teamwork under full and partial observability. We start by developing a solution for the fully observable case that leverages graph neural network architectures to obtain an optimal policy based on reinforcement learning. We then extend this solution to partially observable scenarios by proposing different methodologies that maintain belief estimates over the latent environment states and team composition. These belief estimates are combined with our solution for the fully ",
    "path": "papers/22/10/2210.05448.json",
    "total_tokens": 903,
    "translated_title": "使用基于图的策略学习的通用学习框架进行开放式即席团队合作",
    "translated_abstract": "开放式即席团队合作是指训练一个单一代理与一个未知的可能随时间变化的团队高效合作的问题。对于代理来说，可变的团队组成带来了挑战，如需要适应新的团队动态和处理不断变化的状态向量大小。这些挑战在受控环境仅具有部分视图的真实应用中更加严峻。在这项工作中，我们针对完全和部分可观测性开发了一类解决方案。我们首先针对完全可观测的情况开发了一种解决方案，该解决方案利用图神经网络架构基于强化学习获得最优策略。然后，我们通过提出不同的方法来将这种解决方案扩展到部分可观测的场景中，这些方法可以在潜在的环境状态和团队组成上保持信念估计。这些信念估计与我们对完全可观测情况的解决方案相结合。",
    "tldr": "本文提出了一个通用的学习框架，用于解决开放式即席团队合作问题，通过基于图的策略学习获得最优策略，并且能够适应不完全可观测情况下的团队合作。",
    "en_tdlr": "This paper presents a general learning framework for open ad hoc teamwork, using graph-based policy learning to obtain an optimal policy and adapt to partially observable team dynamics."
}