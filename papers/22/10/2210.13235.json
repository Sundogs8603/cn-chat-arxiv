{
    "title": "Chaos Theory and Adversarial Robustness. (arXiv:2210.13235v2 [cs.LG] UPDATED)",
    "abstract": "Neural networks, being susceptible to adversarial attacks, should face a strict level of scrutiny before being deployed in critical or adversarial applications. This paper uses ideas from Chaos Theory to explain, analyze, and quantify the degree to which neural networks are susceptible to or robust against adversarial attacks. To this end, we present a new metric, the \"susceptibility ratio,\" given by $\\hat \\Psi(h, \\theta)$, which captures how greatly a model's output will be changed by perturbations to a given input.  Our results show that susceptibility to attack grows significantly with the depth of the model, which has safety implications for the design of neural networks for production environments. We provide experimental evidence of the relationship between $\\hat \\Psi$ and the post-attack accuracy of classification models, as well as a discussion of its application to tasks lacking hard decision boundaries. We also demonstrate how to quickly and easily approximate the certified r",
    "link": "http://arxiv.org/abs/2210.13235",
    "context": "Title: Chaos Theory and Adversarial Robustness. (arXiv:2210.13235v2 [cs.LG] UPDATED)\nAbstract: Neural networks, being susceptible to adversarial attacks, should face a strict level of scrutiny before being deployed in critical or adversarial applications. This paper uses ideas from Chaos Theory to explain, analyze, and quantify the degree to which neural networks are susceptible to or robust against adversarial attacks. To this end, we present a new metric, the \"susceptibility ratio,\" given by $\\hat \\Psi(h, \\theta)$, which captures how greatly a model's output will be changed by perturbations to a given input.  Our results show that susceptibility to attack grows significantly with the depth of the model, which has safety implications for the design of neural networks for production environments. We provide experimental evidence of the relationship between $\\hat \\Psi$ and the post-attack accuracy of classification models, as well as a discussion of its application to tasks lacking hard decision boundaries. We also demonstrate how to quickly and easily approximate the certified r",
    "path": "papers/22/10/2210.13235.json",
    "total_tokens": 939,
    "translated_title": "混沌理论与对抗性鲁棒性",
    "translated_abstract": "神经网络容易受到对抗性攻击，在关键领域或对抗性应用中使用前应进行严格的审查。本文利用混沌理论的思想，解释、分析和量化神经网络对抗性攻击的易受性和鲁棒性程度。为此，我们提出了一种新的度量指标“易受性比”，由$\\hat \\Psi(h, \\theta)$给出，该指标衡量模型输出对于给定输入扰动的变化程度。我们的结果表明，随着模型深度的增加，对抗攻击的易受性显著增长，这对于设计用于生产环境的神经网络具有安全影响。我们提供了关于$\\hat \\Psi$与分类模型攻击后准确性的关系的实验证据，并讨论了其在缺乏硬决策边界任务中的应用。我们还演示了如何快速简便地近似计算认证的r。",
    "tldr": "本文利用混沌理论解释、分析和量化了神经网络对抗性攻击的易受性和鲁棒性，提出了一种新的度量指标“易受性比”，结果表明模型深度增加会显著增加对抗攻击的易受性，这对于生产环境中的神经网络设计具有安全影响。"
}