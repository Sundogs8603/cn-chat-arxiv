{
    "title": "A New Family of Generalization Bounds Using Samplewise Evaluated CMI. (arXiv:2210.06422v2 [cs.LG] UPDATED)",
    "abstract": "We present a new family of information-theoretic generalization bounds, in which the training loss and the population loss are compared through a jointly convex function. This function is upper-bounded in terms of the disintegrated, samplewise, evaluated conditional mutual information (CMI), an information measure that depends on the losses incurred by the selected hypothesis, rather than on the hypothesis itself, as is common in probably approximately correct (PAC)-Bayesian results. We demonstrate the generality of this framework by recovering and extending previously known information-theoretic bounds. Furthermore, using the evaluated CMI, we derive a samplewise, average version of Seeger's PAC-Bayesian bound, where the convex function is the binary KL divergence. In some scenarios, this novel bound results in a tighter characterization of the population loss of deep neural networks than previous bounds. Finally, we derive high-probability versions of some of these average bounds. We",
    "link": "http://arxiv.org/abs/2210.06422",
    "context": "Title: A New Family of Generalization Bounds Using Samplewise Evaluated CMI. (arXiv:2210.06422v2 [cs.LG] UPDATED)\nAbstract: We present a new family of information-theoretic generalization bounds, in which the training loss and the population loss are compared through a jointly convex function. This function is upper-bounded in terms of the disintegrated, samplewise, evaluated conditional mutual information (CMI), an information measure that depends on the losses incurred by the selected hypothesis, rather than on the hypothesis itself, as is common in probably approximately correct (PAC)-Bayesian results. We demonstrate the generality of this framework by recovering and extending previously known information-theoretic bounds. Furthermore, using the evaluated CMI, we derive a samplewise, average version of Seeger's PAC-Bayesian bound, where the convex function is the binary KL divergence. In some scenarios, this novel bound results in a tighter characterization of the population loss of deep neural networks than previous bounds. Finally, we derive high-probability versions of some of these average bounds. We",
    "path": "papers/22/10/2210.06422.json",
    "total_tokens": 920,
    "translated_title": "使用逐个样本评估的条件互信息提出一类新的泛化界限",
    "translated_abstract": "我们提出了一类新的信息论泛化界限，其中通过一个联合凸函数比较训练损失和总体损失。这个函数的上界通过分解、逐个样本评估的条件互信息（CMI）加以限制，这是一种与所选假设产生的损失有关而不是假设本身有关的信息度量，这在大多数近似正确性（PAC）- 贝叶斯结果中很常见。通过恢复和扩展之前已知的信息论界限，我们展示了这种框架的普适性。此外，使用评估的CMI，我们导出了Seeger的PAC-Bayesian界限的逐个样本的平均版本，其中凸函数是二元KL散度。在某些情况下，这种新的界限比先前的界限更紧密地刻画了深度神经网络的总体损失。最后，我们导出了一些这些平均界限的高概率版本。",
    "tldr": "本文提出了一类新的信息论泛化界限，通过逐个样本评估的条件互信息，限制联合凸函数上界，与先前的界限相比在某些情况下更紧密地刻画了深度神经网络的总体损失。",
    "en_tdlr": "This paper proposes a new family of information-theoretic generalization bounds which compare training loss and population loss through a jointly convex function using the disintegrated, samplewise evaluated conditional mutual information. It recovers and extends previously known information-theoretic bounds and provides a tighter characterization of the population loss of deep neural networks in some scenarios."
}