{
    "title": "Modeling structure-building in the brain with CCG parsing and large language models. (arXiv:2210.16147v3 [cs.CL] UPDATED)",
    "abstract": "To model behavioral and neural correlates of language comprehension in naturalistic environments researchers have turned to broad-coverage tools from natural-language processing and machine learning. Where syntactic structure is explicitly modeled, prior work has relied predominantly on context-free grammars (CFG), yet such formalisms are not sufficiently expressive for human languages. Combinatory Categorial Grammars (CCGs) are sufficiently expressive directly compositional models of grammar with flexible constituency that affords incremental interpretation. In this work we evaluate whether a more expressive CCG provides a better model than a CFG for human neural signals collected with fMRI while participants listen to an audiobook story. We further test between variants of CCG that differ in how they handle optional adjuncts. These evaluations are carried out against a baseline that includes estimates of next-word predictability from a Transformer neural network language model. Such ",
    "link": "http://arxiv.org/abs/2210.16147",
    "context": "Title: Modeling structure-building in the brain with CCG parsing and large language models. (arXiv:2210.16147v3 [cs.CL] UPDATED)\nAbstract: To model behavioral and neural correlates of language comprehension in naturalistic environments researchers have turned to broad-coverage tools from natural-language processing and machine learning. Where syntactic structure is explicitly modeled, prior work has relied predominantly on context-free grammars (CFG), yet such formalisms are not sufficiently expressive for human languages. Combinatory Categorial Grammars (CCGs) are sufficiently expressive directly compositional models of grammar with flexible constituency that affords incremental interpretation. In this work we evaluate whether a more expressive CCG provides a better model than a CFG for human neural signals collected with fMRI while participants listen to an audiobook story. We further test between variants of CCG that differ in how they handle optional adjuncts. These evaluations are carried out against a baseline that includes estimates of next-word predictability from a Transformer neural network language model. Such ",
    "path": "papers/22/10/2210.16147.json",
    "total_tokens": 851,
    "translated_title": "用CCG解析和大型语言模型建模大脑中的结构建造",
    "translated_abstract": "为了模拟自然环境下语言理解的行为和神经相关性，研究人员借助自然语言处理和机器学习等广泛覆盖的工具。在显式建模句法结构方面，以前的工作主要依赖于无上下文文法（CFG），但这种形式主义对于人类语言来说并不足够表达。组合范畴语法（CCG）是充分表达语法的直接组合模型，具有灵活的从属关系，可以进行增量解释。本研究评估了比CFG更具表达力的CCG是否为人类神经信号提供了比CFG更好的模型，这些神经信号是在听有声书故事时收集的。我们进一步测试了处理可选附加语的CCG变体之间的差异。这些评估是针对具有变压器神经网络语言模型的下一个单词可预测性估计的基线进行的。",
    "tldr": "本研究使用CCG和大型语言模型模拟人类神经信号，发现比无上下文文法更具表达力的CCG更适合表达语法结构。"
}