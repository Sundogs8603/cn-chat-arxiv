{
    "title": "Differentially Private Optimization on Large Model at Small Cost. (arXiv:2210.00038v2 [cs.LG] UPDATED)",
    "abstract": "Differentially private (DP) optimization is the standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are 2-1000X more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as fast and memory-saving as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error. The computational advantage of BK is supported by the complexity analysis as well as extensive experiments on vision and language tasks. Our implementation achieves state-of-the-art (SOTA",
    "link": "http://arxiv.org/abs/2210.00038",
    "context": "Title: Differentially Private Optimization on Large Model at Small Cost. (arXiv:2210.00038v2 [cs.LG] UPDATED)\nAbstract: Differentially private (DP) optimization is the standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are 2-1000X more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as fast and memory-saving as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error. The computational advantage of BK is supported by the complexity analysis as well as extensive experiments on vision and language tasks. Our implementation achieves state-of-the-art (SOTA",
    "path": "papers/22/10/2210.00038.json",
    "total_tokens": 870,
    "translated_title": "在小成本上对大模型进行差分隐私优化",
    "translated_abstract": "差分隐私（DP）优化是学习准确且保护隐私的大型神经网络的标准范式。然而，由于逐样本梯度修剪，DP深度学习的计算成本非常高昂。现有的DP实现比标准（非私有）训练的时间和空间复杂度高2-1000倍。在这项工作中，我们开发了一种新颖的簿记（BK）技术，它实现了现有的DP优化器（从而实现相同的准确性），并在计算成本上有实质性的改进。具体而言，BK使得对大型模型和高维数据进行DP训练的速度和节省内存与标准训练相当，而以前的DP算法可能因内存错误而低效或无法训练。通过复杂度分析和对视觉和语言任务的广泛实验，验证了BK的计算优势。我们的实现达到了最先进的水平（SOTA）。",
    "tldr": "本文提出了一种名为簿记（BK）的技术，实现了差分隐私优化器在大模型和高维数据上的快速训练，并在计算成本上取得了实质性的改进。",
    "en_tdlr": "This paper proposes a technique called Book-Keeping (BK) that enables fast training of differentially private optimizers on large models and high-dimensional data, achieving a substantial improvement in computational cost."
}