{
    "title": "Learning Robust Kernel Ensembles with Kernel Average Pooling. (arXiv:2210.00062v2 [cs.LG] UPDATED)",
    "abstract": "Model ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks' robustness against input perturbations remains underexplored. We introduce Kernel Average Pooling (KAP), a neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when trained on inputs perturbed with additive Gaussian noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial ",
    "link": "http://arxiv.org/abs/2210.00062",
    "context": "Title: Learning Robust Kernel Ensembles with Kernel Average Pooling. (arXiv:2210.00062v2 [cs.LG] UPDATED)\nAbstract: Model ensembles have long been used in machine learning to reduce the variance in individual model predictions, making them more robust to input perturbations. Pseudo-ensemble methods like dropout have also been commonly used in deep learning models to improve generalization. However, the application of these techniques to improve neural networks' robustness against input perturbations remains underexplored. We introduce Kernel Average Pooling (KAP), a neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor. We show that ensembles of kernels with similar functionality naturally emerge in convolutional neural networks equipped with KAP and trained with backpropagation. Moreover, we show that when trained on inputs perturbed with additive Gaussian noise, KAP models are remarkably robust against various forms of adversarial attacks. Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial ",
    "path": "papers/22/10/2210.00062.json",
    "total_tokens": 936,
    "translated_title": "使用核均值池化学习鲁棒的核集成",
    "translated_abstract": "模型集成一直被用于机器学习中，以减少个别模型预测的方差，使其对输入扰动更加鲁棒。假集成方法（如dropout）也常用于深度学习模型中以提高泛化能力。然而，利用这些技术提高神经网络对输入扰动的鲁棒性仍未得到充分探索。我们引入了核均值池（KAP），它是一个神经网络构建模块，可沿着层激活张量的核维度应用均值滤波器。我们展示了在使用KAP以及通过反向传播训练的卷积神经网络中，具有相似功能的核集合自然地产生。此外，我们展示了在使用加性高斯噪声扰动的输入上进行训练时，KAP模型对各种形式的对抗性攻击具有显著的鲁棒性。在CIFAR10、CIFAR100、TinyImagenet和Imagenet数据集上的实证评估显示了实质性的结果。",
    "tldr": "本文提出了核均值池（KAP），一种神经网络构建模块，它可以在卷积神经网络中自然地产生具有相似功能的核集合，提高模型对输入扰动的鲁棒性，并在多个数据集上的实验证明了其有效性。"
}