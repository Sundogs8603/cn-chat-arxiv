{
    "title": "MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization. (arXiv:2210.00102v3 [cs.LG] UPDATED)",
    "abstract": "Training graph neural networks (GNNs) on large graphs is complex and extremely time consuming. This is attributed to overheads caused by sparse matrix multiplication, which are sidestepped when training multi-layer perceptrons (MLPs) with only node features. MLPs, by ignoring graph context, are simple and faster for graph data, however they usually sacrifice prediction accuracy, limiting their applications for graph data. We observe that for most message passing-based GNNs, we can trivially derive an analog MLP (we call this a PeerMLP) with an equivalent weight space, by setting the trainable parameters with the same shapes, making us curious about \\textbf{\\emph{how do GNNs using weights from a fully trained PeerMLP perform?}} Surprisingly, we find that GNNs initialized with such weights significantly outperform their PeerMLPs, motivating us to use PeerMLP training as a precursor, initialization step to GNN training. To this end, we propose an embarrassingly simple, yet hugely effectiv",
    "link": "http://arxiv.org/abs/2210.00102",
    "context": "Title: MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization. (arXiv:2210.00102v3 [cs.LG] UPDATED)\nAbstract: Training graph neural networks (GNNs) on large graphs is complex and extremely time consuming. This is attributed to overheads caused by sparse matrix multiplication, which are sidestepped when training multi-layer perceptrons (MLPs) with only node features. MLPs, by ignoring graph context, are simple and faster for graph data, however they usually sacrifice prediction accuracy, limiting their applications for graph data. We observe that for most message passing-based GNNs, we can trivially derive an analog MLP (we call this a PeerMLP) with an equivalent weight space, by setting the trainable parameters with the same shapes, making us curious about \\textbf{\\emph{how do GNNs using weights from a fully trained PeerMLP perform?}} Surprisingly, we find that GNNs initialized with such weights significantly outperform their PeerMLPs, motivating us to use PeerMLP training as a precursor, initialization step to GNN training. To this end, we propose an embarrassingly simple, yet hugely effectiv",
    "path": "papers/22/10/2210.00102.json",
    "total_tokens": 1189,
    "translated_title": "MLPInit: 使用MLP初始化非常简单的GNN训练加速",
    "translated_abstract": "在大型图上训练图神经网络（GNN）非常复杂且极其耗时，这是由于稀疏矩阵乘法引起的开销。当仅使用节点特征来训练多层感知器（MLP）时，可以避免这些开销。MLP通过忽略图上下文信息，对于图形数据而言更加简单和快速，但通常会牺牲预测精度，限制了它们在图形数据中的应用。我们观察到，对于大多数基于消息传递的GNN，我们可以轻松地推导出一个等效的模拟MLP（我们将其称为PeerMLP）并设置可训练的参数具有相同的形状，这使我们好奇“使用从完全训练的PeerMLP导出的权重的GNN的表现如何？”令人惊讶的是，我们发现使用这些权重初始化的GNN明显优于它们的PeerMLP，这促使我们使用PeerMLP训练作为GNN训练的前导初始化步骤。为此，我们提出了一种非常简单但非常有效的MLP初始化方法，称为MLPInit。 MLPInit包括在与目标GNN相同数据上训练PeerMLP，然后使用其权重初始化目标GNN。从经验上看，我们证明了MLPInit有效地将有用信息从PeerMLP传输到目标GNN，显着提高了GNN的收敛效果和预测性能，同时保持了MLP的简单和速度优势。",
    "tldr": "提出一种简单但有效的GNN加速训练方法 - MLPInit, 通过训练一个等效MLP（PeerMLP）, 并使用它的权重来初始化目标GNN，在不损失精度的情况下，显著提高了GNN的收敛效果和预测性能。",
    "en_tdlr": "MLPInit is an embarrassingly simple yet highly effective method for accelerating training of graph neural networks (GNNs), where a PeerMLP is trained on the same data as the target GNN and its weights are used to initialize the target GNN, significantly improving convergence and prediction performance without sacrificing accuracy."
}