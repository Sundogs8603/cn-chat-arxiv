{
    "title": "Reprogramming Pretrained Language Models for Antibody Sequence Infilling. (arXiv:2210.07144v2 [q-bio.BM] UPDATED)",
    "abstract": "Antibodies comprise the most versatile class of binding molecules, with numerous applications in biomedicine. Computational design of antibodies involves generating novel and diverse sequences, while maintaining structural consistency. Unique to antibodies, designing the complementarity-determining region (CDR), which determines the antigen binding affinity and specificity, creates its own unique challenges. Recent deep learning models have shown impressive results, however the limited number of known antibody sequence/structure pairs frequently leads to degraded performance, particularly lacking diversity in the generated sequences. In our work we address this challenge by leveraging Model Reprogramming (MR), which repurposes pretrained models on a source language to adapt to the tasks that are in a different language and have scarce data - where it may be difficult to train a high-performing model from scratch or effectively fine-tune an existing pre-trained model on the specific tas",
    "link": "http://arxiv.org/abs/2210.07144",
    "context": "Title: Reprogramming Pretrained Language Models for Antibody Sequence Infilling. (arXiv:2210.07144v2 [q-bio.BM] UPDATED)\nAbstract: Antibodies comprise the most versatile class of binding molecules, with numerous applications in biomedicine. Computational design of antibodies involves generating novel and diverse sequences, while maintaining structural consistency. Unique to antibodies, designing the complementarity-determining region (CDR), which determines the antigen binding affinity and specificity, creates its own unique challenges. Recent deep learning models have shown impressive results, however the limited number of known antibody sequence/structure pairs frequently leads to degraded performance, particularly lacking diversity in the generated sequences. In our work we address this challenge by leveraging Model Reprogramming (MR), which repurposes pretrained models on a source language to adapt to the tasks that are in a different language and have scarce data - where it may be difficult to train a high-performing model from scratch or effectively fine-tune an existing pre-trained model on the specific tas",
    "path": "papers/22/10/2210.07144.json",
    "total_tokens": 1012,
    "translated_title": "预训练语言模型在抗体序列填充中的应用",
    "translated_abstract": "抗体是最多才多艺的结合分子，应用于生物医学领域。计算机设计抗体需要生成新颖多样的序列，同时保持结构一致性。设计互补决定区域（CDR）是抗体所特有的挑战，这决定了其抗原结合亲和力和特异性。最近的深度学习模型表现出了惊人的成果，然而已知的抗体序列/结构对的数量有限，常常导致性能降低，特别是在生成序列方面缺乏多样性。我们的工作通过利用模型重编程（MR），将预训练模型重新用于适应一个不同的语言任务，它们的数据稀缺，因此从头训练高性能模型或有效地微调基于特定任务的预训练模型可能会很困难。具体而言，我们重新编程了BERT和GPT-2模型，这些模型是基于自然语言数据进行预训练的，用于建立抗体的CDR序列。我们展示了我们重新编程的模型在维持高质量CDR序列结果的同时，在多样性和新颖性方面优于之前的最先进方法。",
    "tldr": "该篇论文利用模型重编程技术，将基于自然语言的预训练模型重新用于抗体CDR序列推断任务中，成功提高了生成的序列多样性和新颖性，并保持了高质量的结构一致性。",
    "en_tdlr": "This paper applies model reprogramming technique to pretrained language models for inferring complementarity-determining region (CDR) sequences for antibodies, achieving better diversity and novelty in the generated sequences while maintaining high structural quality."
}