{
    "title": "KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding. (arXiv:2210.04105v2 [cs.CL] UPDATED)",
    "abstract": "With the advent of pretrained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge representation, along with pretrained LMs. While existing approaches have leveraged external knowledge, it remains an open question how to jointly incorporate knowledge graphs representing varying contexts, from local (e.g., sentence), to document-level, to global knowledge, to enable knowledge-rich exchange across these contexts. Such rich contextualization can be especially beneficial for long document understanding tasks since standard pretrained LMs are typically bounded by the input sequence length. In light of these challenges, we propose KALM, a Knowledge-Aware Language Model that jointly leverages knowledge in local, document-level, and global contexts for long document understanding. KALM first en",
    "link": "http://arxiv.org/abs/2210.04105",
    "context": "Title: KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding. (arXiv:2210.04105v2 [cs.CL] UPDATED)\nAbstract: With the advent of pretrained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge representation, along with pretrained LMs. While existing approaches have leveraged external knowledge, it remains an open question how to jointly incorporate knowledge graphs representing varying contexts, from local (e.g., sentence), to document-level, to global knowledge, to enable knowledge-rich exchange across these contexts. Such rich contextualization can be especially beneficial for long document understanding tasks since standard pretrained LMs are typically bounded by the input sequence length. In light of these challenges, we propose KALM, a Knowledge-Aware Language Model that jointly leverages knowledge in local, document-level, and global contexts for long document understanding. KALM first en",
    "path": "papers/22/10/2210.04105.json",
    "total_tokens": 872,
    "translated_title": "KALM: 知识感知的本地、文档和全局背景集成，用于长文档理解",
    "translated_abstract": "随着预训练语言模型 (LMs) 的出现，越来越多的研究工作集中于将常识和领域特定知识注入到 LMs 中以准备下游任务。这些工作试图利用知识图谱，即符号知识表示的事实标准，以及预训练 LMs。尽管现有方法已经利用了外部知识，但目前如何共同结合代表不同语境的知识图谱，从本地 (例如，句子)、文档级别、全局知识等方面进行知识丰富的交换仍然是一个开放的问题。这样的丰富语境化对于长文档理解任务尤其有益，因为标准预先训练的 LMs 通常受限于输入序列长度。考虑到这些挑战，我们提出了 KALM，一种知识感知语言模型，它联合利用本地、文档级别和全局语境中的知识，用于长文档理解。",
    "tldr": "KALM是一种知识感知语言模型，可以在本地、文档级别和全局语境中联合利用知识，用于长文档理解。",
    "en_tdlr": "KALM is a knowledge-aware language model that leverages knowledge in local, document-level, and global contexts for long document understanding."
}