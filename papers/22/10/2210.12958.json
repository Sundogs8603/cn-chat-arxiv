{
    "title": "Composition, Attention, or Both?. (arXiv:2210.12958v3 [cs.CL] UPDATED)",
    "abstract": "In this paper, we propose a novel architecture called Composition Attention Grammars (CAGs) that recursively compose subtrees into a single vector representation with a composition function, and selectively attend to previous structural information with a self-attention mechanism. We investigate whether these components -- the composition function and the self-attention mechanism -- can both induce human-like syntactic generalization. Specifically, we train language models (LMs) with and without these two components with the model sizes carefully controlled, and evaluate their syntactic generalization performance against six test circuits on the SyntaxGym benchmark. The results demonstrated that the composition function and the self-attention mechanism both play an important role to make LMs more human-like, and closer inspection of linguistic phenomenon implied that the composition function allowed syntactic features, but not semantic features, to percolate into subtree representation",
    "link": "http://arxiv.org/abs/2210.12958",
    "context": "Title: Composition, Attention, or Both?. (arXiv:2210.12958v3 [cs.CL] UPDATED)\nAbstract: In this paper, we propose a novel architecture called Composition Attention Grammars (CAGs) that recursively compose subtrees into a single vector representation with a composition function, and selectively attend to previous structural information with a self-attention mechanism. We investigate whether these components -- the composition function and the self-attention mechanism -- can both induce human-like syntactic generalization. Specifically, we train language models (LMs) with and without these two components with the model sizes carefully controlled, and evaluate their syntactic generalization performance against six test circuits on the SyntaxGym benchmark. The results demonstrated that the composition function and the self-attention mechanism both play an important role to make LMs more human-like, and closer inspection of linguistic phenomenon implied that the composition function allowed syntactic features, but not semantic features, to percolate into subtree representation",
    "path": "papers/22/10/2210.12958.json",
    "total_tokens": 930,
    "translated_title": "组合、注意力或两者兼备？",
    "translated_abstract": "本文提出了一种名为“组合注意力语法”（CAGs）的新型结构，该结构通过组合函数将子树递归地组合为单个向量表示，并通过自我注意机制选择性地关注先前的结构信息。我们探讨了这些组件——组合函数和自我注意机制——是否都可以引起类似于人类的句法归纳。具体来说，我们使用谨慎控制的模型大小对具有和不具有这两个组件的语言模型（LMs）进行训练，并针对SyntaxGym基准测试中的六个测试电路评估其句法归纳能力。结果表明，组合函数和自我注意机制都发挥了重要作用，使LMs更加类似于人类，并对语言现象进行更近一步的检查，暗示组合函数允许句法特征而不允许语义特征渗透到子树表示中。",
    "tldr": "本文提出了一种名为“组合注意力语法”（CAGs）的新型结构，该结构通过组合函数将子树递归地组合为单个向量表示，并通过自我注意机制选择性地关注先前的结构信息；结果表明，组合函数和自我注意机制都发挥了重要作用，使LMs更加类似于人类，并允许句法特征而不允许语义特征渗透到子树表示中。",
    "en_tdlr": "This paper proposes a novel architecture called Composition Attention Grammars (CAGs) that combines a composition function and a self-attention mechanism for syntactic generalization in language models (LMs). The results show that both components are important for making LMs more human-like, and the composition function allows syntactic features to percolate into the subtree representation."
}