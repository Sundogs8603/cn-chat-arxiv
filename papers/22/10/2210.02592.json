{
    "title": "CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations. (arXiv:2210.02592v3 [cs.CL] UPDATED)",
    "abstract": "While Self-Supervised Learning has helped reap the benefit of the scale from the available unlabeled data, the learning paradigms are continuously being bettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which uses clustering and an augmentation-based cross-contrastive loss as its self-supervised objective. Through the clustering module, we scale down the influence of those negative examples that are highly similar to the positive. The Cross-Contrastive loss is computed between the encoder output of the original sample and the quantizer output of its augmentation and vice-versa, bringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on the test-clean and test-other sets, respectively, of LibriSpeech, without the use of any language model. The proposed method also achieves up to 14.9% relative WER improvement over the baseline wav2vec 2.0 when fine-tuned on Switchboard d",
    "link": "http://arxiv.org/abs/2210.02592",
    "context": "Title: CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations. (arXiv:2210.02592v3 [cs.CL] UPDATED)\nAbstract: While Self-Supervised Learning has helped reap the benefit of the scale from the available unlabeled data, the learning paradigms are continuously being bettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which uses clustering and an augmentation-based cross-contrastive loss as its self-supervised objective. Through the clustering module, we scale down the influence of those negative examples that are highly similar to the positive. The Cross-Contrastive loss is computed between the encoder output of the original sample and the quantizer output of its augmentation and vice-versa, bringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on the test-clean and test-other sets, respectively, of LibriSpeech, without the use of any language model. The proposed method also achieves up to 14.9% relative WER improvement over the baseline wav2vec 2.0 when fine-tuned on Switchboard d",
    "path": "papers/22/10/2210.02592.json",
    "total_tokens": 1013,
    "translated_title": "CCC-wav2vec 2.0：基于聚类的交叉对比自监督学习语音表示的方法",
    "translated_abstract": "在利用大量未标注数据的情况下，自监督学习已经帮助我们从中受益。本文提出了一种新的预训练策略，即ccc-wav2vec 2.0，该策略使用聚类和基于增强的交叉对比损失作为其自监督目标。通过聚类模块，我们降低了与正样本高度相似的负样本的影响。交叉对比损失在原始样本的编码器输出和其增强版的量化器输出之间以及反之计算，从而为预训练策略带来了鲁棒性。ccc-wav2vec 2.0在LibriSpeech的测试-干净集和测试-其他集上相对于wav2vec 2.0基线分别取得了15.6％和12.7％的相对WER改进，而不需要任何语言模型。当在Switchboard上微调时，该方法还可以相对于wav2vec 2.0基线取得高达14.9％的相对WER改进。",
    "tldr": "本论文提出了一种基于聚类的交叉对比自监督学习语音表示的方法，引入了增强散度、交叉对比损失和聚类模块，相对于wav2vec 2.0基线在LibriSpeech测试集上取得了超过12%的WER改进。",
    "en_tdlr": "This paper proposes a clustering aided cross-contrastive self-supervised learning method for speech representation. By introducing augmentation-based cross-contrastive loss and clustering module to downscale the influence of similar negative examples, the proposed ccc-wav2vec 2.0 achieves over 12% relative WER improvement over the baseline wav2vec 2.0 on the LibriSpeech test set."
}