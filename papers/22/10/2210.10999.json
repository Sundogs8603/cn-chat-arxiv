{
    "title": "Task Phasing: Automated Curriculum Learning from Demonstrations. (arXiv:2210.10999v2 [cs.LG] UPDATED)",
    "abstract": "Applying reinforcement learning (RL) to sparse reward domains is notoriously challenging due to insufficient guiding signals. Common RL techniques for addressing such domains include (1) learning from demonstrations and (2) curriculum learning. While these two approaches have been studied in detail, they have rarely been considered together. This paper aims to do so by introducing a principled task phasing approach that uses demonstrations to automatically generate a curriculum sequence. Using inverse RL from (suboptimal) demonstrations we define a simple initial task. Our task phasing approach then provides a framework to gradually increase the complexity of the task all the way to the target task, while retuning the RL agent in each phasing iteration. Two approaches for phasing are considered: (1) gradually increasing the proportion of time steps an RL agent is in control, and (2) phasing out a guiding informative reward function. We present conditions that guarantee the convergence ",
    "link": "http://arxiv.org/abs/2210.10999",
    "context": "Title: Task Phasing: Automated Curriculum Learning from Demonstrations. (arXiv:2210.10999v2 [cs.LG] UPDATED)\nAbstract: Applying reinforcement learning (RL) to sparse reward domains is notoriously challenging due to insufficient guiding signals. Common RL techniques for addressing such domains include (1) learning from demonstrations and (2) curriculum learning. While these two approaches have been studied in detail, they have rarely been considered together. This paper aims to do so by introducing a principled task phasing approach that uses demonstrations to automatically generate a curriculum sequence. Using inverse RL from (suboptimal) demonstrations we define a simple initial task. Our task phasing approach then provides a framework to gradually increase the complexity of the task all the way to the target task, while retuning the RL agent in each phasing iteration. Two approaches for phasing are considered: (1) gradually increasing the proportion of time steps an RL agent is in control, and (2) phasing out a guiding informative reward function. We present conditions that guarantee the convergence ",
    "path": "papers/22/10/2210.10999.json",
    "total_tokens": 926,
    "translated_title": "任务阶段化：来自示范的自动课程学习",
    "translated_abstract": "将强化学习（RL）应用于稀疏奖励域通常具有挑战性，因为缺乏足够的引导信号。解决此类领域的常见RL技术包括（1）从示范学习和（2）课程学习。虽然这两种方法已经被详细研究，但它们很少被同时考虑。本文旨在通过引入一种基于示范的原则性任务阶段化方法来实现该目的，该方法使用示范自动生成课程序列。我们使用来自（次优）演示的逆RL定义了一个简单的初始任务。然后，我们的任务阶段化方法提供了一个框架，逐渐增加任务的复杂性，直到目标任务，同时在每个阶段迭代中重新调整RL代理。考虑了两种分阶段方法：（1）逐步增加RL代理处于控制下的时间步数的比例，以及（2）逐步淘汰引导性信息奖励函数。我们提出了保证收敛的条件。",
    "tldr": "本文介绍了一种结合了示范学习和课程学习的任务阶段化方法，使用逆强化学习自动生成课程序列，逐步增加任务复杂度，以帮助解决强化学习在稀疏奖励领域中的挑战。"
}