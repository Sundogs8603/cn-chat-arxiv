{
    "title": "LOT: Layer-wise Orthogonal Training on Improving $\\ell_2$ Certified Robustness. (arXiv:2210.11620v2 [cs.LG] UPDATED)",
    "abstract": "Recent studies show that training deep neural networks (DNNs) with Lipschitz constraints are able to enhance adversarial robustness and other model properties such as stability. In this paper, we propose a layer-wise orthogonal training method (LOT) to effectively train 1-Lipschitz convolution layers via parametrizing an orthogonal matrix with an unconstrained matrix. We then efficiently compute the inverse square root of a convolution kernel by transforming the input domain to the Fourier frequency domain. On the other hand, as existing works show that semi-supervised training helps improve empirical robustness, we aim to bridge the gap and prove that semi-supervised learning also improves the certified robustness of Lipschitz-bounded models. We conduct comprehensive evaluations for LOT under different settings. We show that LOT significantly outperforms baselines regarding deterministic l2 certified robustness, and scales to deeper neural networks. Under the supervised scenario, we i",
    "link": "http://arxiv.org/abs/2210.11620",
    "context": "Title: LOT: Layer-wise Orthogonal Training on Improving $\\ell_2$ Certified Robustness. (arXiv:2210.11620v2 [cs.LG] UPDATED)\nAbstract: Recent studies show that training deep neural networks (DNNs) with Lipschitz constraints are able to enhance adversarial robustness and other model properties such as stability. In this paper, we propose a layer-wise orthogonal training method (LOT) to effectively train 1-Lipschitz convolution layers via parametrizing an orthogonal matrix with an unconstrained matrix. We then efficiently compute the inverse square root of a convolution kernel by transforming the input domain to the Fourier frequency domain. On the other hand, as existing works show that semi-supervised training helps improve empirical robustness, we aim to bridge the gap and prove that semi-supervised learning also improves the certified robustness of Lipschitz-bounded models. We conduct comprehensive evaluations for LOT under different settings. We show that LOT significantly outperforms baselines regarding deterministic l2 certified robustness, and scales to deeper neural networks. Under the supervised scenario, we i",
    "path": "papers/22/10/2210.11620.json",
    "total_tokens": 1094,
    "translated_title": "LOT: 基于层内正交训练来提高$\\ell_2$ 保护的鲁棒性。",
    "translated_abstract": "最近的研究表明，使用利普希茨约束训练深度神经网络（DNN）能够增强对抗性鲁棒性和其他模型特性，例如稳定性。在本文中，我们提出了一种基于层内正交训练方法（LOT）来有效训练1-Lipschitz卷积层，通过使用一个无限制矩阵来参数化一个正交矩阵。然后，我们通过将输入域转换为傅里叶频域来高效计算卷积核的平方根的逆。另一方面，由于现有研究表明半监督训练有助于提高经验上的鲁棒性，我们旨在弥合差距，并证明半监督学习也会提高利普希茨约束模型的可证明鲁棒性。我们在不同设置下对LOT进行了全面评估，并展示了LOT在确定性l2可证明鲁棒性方面显著优于基线，并能够扩展到更深的神经网络。在监督情况下，我们发现与纯监督训练相比，半监督训练可以进一步提高保护的鲁棒性。",
    "tldr": "本文提出了一种基于层内正交训练的方法(LOT)，通过使用一个无限制矩阵来参数化正交矩阵来有效训练1-Lipschitz卷积层，并证明了半监督训练可以进一步提高利普希茨约束模型的可证明鲁棒性。在确定性l2可证明鲁棒性方面，LOT显著优于基线，并能够扩展到更深的神经网络。",
    "en_tdlr": "This paper proposes a layer-wise orthogonal training method (LOT) to effectively train 1-Lipschitz convolution layers via parametrizing an orthogonal matrix with an unconstrained matrix, and proves that semi-supervised learning can further improve the certified robustness of Lipschitz-bounded models. LOT significantly outperforms baselines regarding deterministic l2 certified robustness and scales to deeper neural networks."
}