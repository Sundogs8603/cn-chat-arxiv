{
    "title": "Test-time Recalibration of Conformal Predictors Under Distribution Shift Based on Unlabeled Examples. (arXiv:2210.04166v2 [cs.LG] UPDATED)",
    "abstract": "Modern image classifiers are very accurate, but the predictions come without uncertainty estimates. Conformal predictors provide uncertainty estimates by computing a set of classes containing the correct class with a user-specified probability based on the classifier's probability estimates. To provide such sets, conformal predictors often estimate a cutoff threshold for the probability estimates based on a calibration set. Conformal predictors guarantee reliability only when the calibration set is from the same distribution as the test set. Therefore, conformal predictors need to be recalibrated for new distributions. However, in practice, labeled data from new distributions is rarely available, making calibration infeasible. In this work, we consider the problem of predicting the cutoff threshold for a new distribution based on unlabeled examples. While it is impossible in general to guarantee reliability when calibrating based on unlabeled examples, we propose a method that provides",
    "link": "http://arxiv.org/abs/2210.04166",
    "context": "Title: Test-time Recalibration of Conformal Predictors Under Distribution Shift Based on Unlabeled Examples. (arXiv:2210.04166v2 [cs.LG] UPDATED)\nAbstract: Modern image classifiers are very accurate, but the predictions come without uncertainty estimates. Conformal predictors provide uncertainty estimates by computing a set of classes containing the correct class with a user-specified probability based on the classifier's probability estimates. To provide such sets, conformal predictors often estimate a cutoff threshold for the probability estimates based on a calibration set. Conformal predictors guarantee reliability only when the calibration set is from the same distribution as the test set. Therefore, conformal predictors need to be recalibrated for new distributions. However, in practice, labeled data from new distributions is rarely available, making calibration infeasible. In this work, we consider the problem of predicting the cutoff threshold for a new distribution based on unlabeled examples. While it is impossible in general to guarantee reliability when calibrating based on unlabeled examples, we propose a method that provides",
    "path": "papers/22/10/2210.04166.json",
    "total_tokens": 1045,
    "translated_title": "基于未标记的样本的分布漂移下测试时间校准置信度预测器",
    "translated_abstract": "现代图像分类器非常准确，但预测没有不确定性估计。基于分类器的概率估计，基本置信度预测器通过计算一组包含具有用户指定概率的正确类的类来提供不确定性估计。为了提供这样的集合，置信度预测器常常基于校准集合估计概率估计的截断阈值。置信度预测器仅在校准集合与测试集相同时保证可靠性。因此，置信度预测器需要为新分布重新校准。但在实践中，很少有来自新分布的标记数据，使校准成为不可行的。在这项工作中，我们考虑了基于未标记样本预测新分布截止阈值的问题。虽然一般情况下不能保证基于未标记样本进行校准时的可靠性，但我们提出了一种基于密度比估计技术的方法来预测新分布的截止阈值。我们在几个标准图像数据集上评估了我们的方法，并显示出在分布漂移下进行测试时间校准的最新方法。",
    "tldr": "本论文提出了一种基于未标记样本的分布漂移下测试时间校准置信度预测器。通过使用密度比估计技术来预测新分布的截止阈值，我们在几个标准图像数据集上展示了该方法优于最新的分布漂移下的测试时间校准方法。",
    "en_tdlr": "This paper proposes a method for test-time recalibration of conformal predictors under distribution shift based on unlabeled examples. By using density ratio estimation technique to predict the cutoff threshold for the new distribution, we demonstrate the method outperforms state-of-the-art methods on several standard image datasets."
}