{
    "title": "GNM: A General Navigation Model to Drive Any Robot. (arXiv:2210.03370v2 [cs.RO] UPDATED)",
    "abstract": "Learning provides a powerful tool for vision-based navigation, but the capabilities of learning-based policies are constrained by limited training data. If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models. In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments. We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset. We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor. We find that training on diver",
    "link": "http://arxiv.org/abs/2210.03370",
    "context": "Title: GNM: A General Navigation Model to Drive Any Robot. (arXiv:2210.03370v2 [cs.RO] UPDATED)\nAbstract: Learning provides a powerful tool for vision-based navigation, but the capabilities of learning-based policies are constrained by limited training data. If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models. In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments. We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset. We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor. We find that training on diver",
    "path": "papers/22/10/2210.03370.json",
    "total_tokens": 927,
    "translated_title": "GNM: 通用导航模型驱动任何机器人",
    "translated_abstract": "学习是视觉导航的强有力工具，但基于学习的策略的能力受到有限训练数据的限制。本文研究如何在来自多个不同但结构相似的机器人的数据基础上训练面向视觉导航的通用目标条件模型，并实现在各种环境和机身上的广泛泛化。我们分析了有效的机器人间数据共享的必要设计决策，包括使用时间上下文和标准化的动作空间，并证明了从异构数据集训练的全局策略优于任何单一数据集上训练的策略。我们收集了6个不同机器人的60小时导航轨迹，并在一系列新机器人上部署训练后的GNM，包括一个欠驱动的四旋翼飞行器。我们发现，训练多样化的数据可以提高和更加稳健的导航性能，而GNM可以驱动任何具有适当视觉感知输入的机器人。",
    "tldr": "通用目标条件模型GNM可以通过多样化的机器人数据实现更强大、更健壮的导航性能，并能驱动任何具有适当视觉感知输入的机器人。",
    "en_tdlr": "The General Navigation Model (GNM), trained on diverse data from structurally similar robots, drives any robot with suitable vision-based sensory input by enabling broad generalization across environments and embodiments. GNM outperforms policies trained on any single dataset, resulting in improved and more robust navigation performance."
}