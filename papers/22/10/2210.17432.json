{
    "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control. (arXiv:2210.17432v2 [cs.CL] UPDATED)",
    "abstract": "Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM -- a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generatio",
    "link": "http://arxiv.org/abs/2210.17432",
    "context": "Title: SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control. (arXiv:2210.17432v2 [cs.CL] UPDATED)\nAbstract: Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM -- a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generatio",
    "path": "papers/22/10/2210.17432.json",
    "total_tokens": 976,
    "translated_title": "SSD-LM: 基于半自回归简单形式的扩散语言模型用于文本生成和模块化控制",
    "translated_abstract": "尽管扩散模型在连续值领域（如图像）取得了很大成功，但在离散领域（如文本）中，类似的努力却还没有达到自回归语言模型的性能水平。在这项工作中，我们提出了 SSD-LM，一个基于扩散的语言模型，使用了两个关键设计选择。首先，SSD-LM是半自回归的，通过迭代生成文本块，在解码时允许灵活的输出长度，同时实现本地双向上下文更新。其次，它采用基于单纯形的方法，在自然词汇空间上进行扩散，而不是在学习到的潜在空间上，这使得我们能够在不进行任何自适应的情况下，利用现成的分类器实现分类器指导和模块化控制。我们在无约束文本生成基准上评估了 SSD-LM，并展示它在标准质量和多样性度量上与强大的自回归 GPT-2 模型相匹配或超越，并且远远优于基于扩散的基准模型。",
    "tldr": "SSD-LM是一种半自回归的扩散语言模型，通过在解码时灵活生成文本块并实现本地上下文更新，以及在自然词汇空间上进行扩散，实现了分类器指导和模块化控制。在无约束文本生成基准上，SSD-LM与自回归模型相比，在质量和多样性方面表现出色，并且显著超越了其他基于扩散的模型。"
}