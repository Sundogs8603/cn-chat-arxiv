{
    "title": "Data Selection: A Surprisingly Effective and General Principle for Building Small Interpretable Models. (arXiv:2210.03921v2 [cs.LG] UPDATED)",
    "abstract": "We present convincing empirical evidence for an effective and general strategy for building accurate small models. Such models are attractive for interpretability and also find use in resource-constrained environments. The strategy is to learn the training distribution instead of using data from the test distribution. The distribution learning algorithm is not a contribution of this work; we highlight the broad usefulness of this simple strategy on a diverse set of tasks, and as such these rigorous empirical results are our contribution. We apply it to the tasks of (1) building cluster explanation trees, (2) prototype-based classification, and (3) classification using Random Forests, and show that it improves the accuracy of weak traditional baselines to the point that they are surprisingly competitive with specialized modern techniques.  This strategy is also versatile wrt the notion of model size. In the first two tasks, model size is identified by number of leaves in the tree and th",
    "link": "http://arxiv.org/abs/2210.03921",
    "context": "Title: Data Selection: A Surprisingly Effective and General Principle for Building Small Interpretable Models. (arXiv:2210.03921v2 [cs.LG] UPDATED)\nAbstract: We present convincing empirical evidence for an effective and general strategy for building accurate small models. Such models are attractive for interpretability and also find use in resource-constrained environments. The strategy is to learn the training distribution instead of using data from the test distribution. The distribution learning algorithm is not a contribution of this work; we highlight the broad usefulness of this simple strategy on a diverse set of tasks, and as such these rigorous empirical results are our contribution. We apply it to the tasks of (1) building cluster explanation trees, (2) prototype-based classification, and (3) classification using Random Forests, and show that it improves the accuracy of weak traditional baselines to the point that they are surprisingly competitive with specialized modern techniques.  This strategy is also versatile wrt the notion of model size. In the first two tasks, model size is identified by number of leaves in the tree and th",
    "path": "papers/22/10/2210.03921.json",
    "total_tokens": 922,
    "translated_title": "数据选择：一种令人惊讶的有效且通用的构建小型可解释模型的原则。",
    "translated_abstract": "我们提供了令人信服的实证证据，证明了一种构建准确小型模型的有效且通用的策略。这种模型对于可解释性具有吸引力，并且在资源受限的环境中也有用途。该策略是学习训练分布而不是使用测试分布的数据。分布学习算法不是这项工作的贡献；我们强调这种简单策略在各种任务上的广泛适用性，并且基于严格的实证结果，这些结果是我们的贡献。我们将其应用于以下任务：（1）构建聚类解释树，（2）基于原型的分类，以及（3）使用随机森林进行分类，并且展示了它提高了弱传统基准的准确性，使它们令人惊讶地与专业的现代技术相竞争。此策略也适用于模型大小的概念。在前两个任务中，模型大小通过树中叶子节点的数量来确定。",
    "tldr": "数据选择是一种令人惊讶的有效且通用的构建小型可解释模型的策略，它通过学习训练分布而非测试分布的数据，提高了传统基准模型的准确性，并在多个任务中展现出竞争力。"
}