{
    "title": "Principled Pruning of Bayesian Neural Networks through Variational Free Energy Minimization. (arXiv:2210.09134v2 [cs.LG] UPDATED)",
    "abstract": "Bayesian model reduction provides an efficient approach for comparing the performance of all nested sub-models of a model, without re-evaluating any of these sub-models. Until now, Bayesian model reduction has been applied mainly in the computational neuroscience community on simple models. In this paper, we formulate and apply Bayesian model reduction to perform principled pruning of Bayesian neural networks, based on variational free energy minimization. Direct application of Bayesian model reduction, however, gives rise to approximation errors. Therefore, a novel iterative pruning algorithm is presented to alleviate the problems arising with naive Bayesian model reduction, as supported experimentally on the publicly available UCI datasets for different inference algorithms. This novel parameter pruning scheme solves the shortcomings of current state-of-the-art pruning methods that are used by the signal processing community. The proposed approach has a clear stopping criterion and m",
    "link": "http://arxiv.org/abs/2210.09134",
    "context": "Title: Principled Pruning of Bayesian Neural Networks through Variational Free Energy Minimization. (arXiv:2210.09134v2 [cs.LG] UPDATED)\nAbstract: Bayesian model reduction provides an efficient approach for comparing the performance of all nested sub-models of a model, without re-evaluating any of these sub-models. Until now, Bayesian model reduction has been applied mainly in the computational neuroscience community on simple models. In this paper, we formulate and apply Bayesian model reduction to perform principled pruning of Bayesian neural networks, based on variational free energy minimization. Direct application of Bayesian model reduction, however, gives rise to approximation errors. Therefore, a novel iterative pruning algorithm is presented to alleviate the problems arising with naive Bayesian model reduction, as supported experimentally on the publicly available UCI datasets for different inference algorithms. This novel parameter pruning scheme solves the shortcomings of current state-of-the-art pruning methods that are used by the signal processing community. The proposed approach has a clear stopping criterion and m",
    "path": "papers/22/10/2210.09134.json",
    "total_tokens": 928,
    "translated_title": "通过变分自由能最小化对贝叶斯神经网络进行原则性剪枝",
    "translated_abstract": "贝叶斯模型简化提供了一种有效的方法，用于比较模型的所有嵌套子模型的性能，而无需重新评估这些子模型。迄今为止，贝叶斯模型简化主要应用于计算神经科学社区的简单模型。本文提出并应用了基于变分自由能最小化的贝叶斯模型简化方法，用于对贝叶斯神经网络进行原则性剪枝。然而，直接应用贝叶斯模型简化会产生近似误差。因此，本文提出了一种新颖的迭代剪枝算法，以缓解直接应用贝叶斯模型简化所引起的问题，并在公开可用的UCI数据集上通过实验证明其效果对不同推理算法。这种新颖的参数剪枝方案解决了信号处理社区使用的当前最先进的剪枝方法的缺点。所提出的方法具有明确的停止准则和m.",
    "tldr": "本文提出了一种基于变分自由能最小化的贝叶斯模型简化方法，用于对贝叶斯神经网络进行原则性剪枝。通过引入迭代剪枝算法，解决了直接应用贝叶斯模型简化的近似误差问题，并在实验证明了该方法的有效性和优势。",
    "en_tdlr": "This paper proposes a principled pruning method for Bayesian neural networks based on variational free energy minimization. By introducing an iterative pruning algorithm, the approximation errors arising from direct application of Bayesian model reduction are addressed, and the effectiveness and advantages of this method are demonstrated through experiments."
}