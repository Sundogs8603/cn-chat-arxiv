{
    "title": "PersA-FL: Personalized Asynchronous Federated Learning. (arXiv:2210.01176v2 [cs.LG] UPDATED)",
    "abstract": "We study the personalized federated learning problem under asynchronous updates. In this problem, each client seeks to obtain a personalized model that simultaneously outperforms local and global models. We consider two optimization-based frameworks for personalization: (i) Model-Agnostic Meta-Learning (MAML) and (ii) Moreau Envelope (ME). MAML involves learning a joint model adapted for each client through fine-tuning, whereas ME requires a bi-level optimization problem with implicit gradients to enforce personalization via regularized losses. We focus on improving the scalability of personalized federated learning by removing the synchronous communication assumption. Moreover, we extend the studied function class by removing boundedness assumptions on the gradient norm. Our main technical contribution is a unified proof for asynchronous federated learning with bounded staleness that we apply to MAML and ME personalization frameworks. For the smooth and non-convex functions class, we ",
    "link": "http://arxiv.org/abs/2210.01176",
    "context": "Title: PersA-FL: Personalized Asynchronous Federated Learning. (arXiv:2210.01176v2 [cs.LG] UPDATED)\nAbstract: We study the personalized federated learning problem under asynchronous updates. In this problem, each client seeks to obtain a personalized model that simultaneously outperforms local and global models. We consider two optimization-based frameworks for personalization: (i) Model-Agnostic Meta-Learning (MAML) and (ii) Moreau Envelope (ME). MAML involves learning a joint model adapted for each client through fine-tuning, whereas ME requires a bi-level optimization problem with implicit gradients to enforce personalization via regularized losses. We focus on improving the scalability of personalized federated learning by removing the synchronous communication assumption. Moreover, we extend the studied function class by removing boundedness assumptions on the gradient norm. Our main technical contribution is a unified proof for asynchronous federated learning with bounded staleness that we apply to MAML and ME personalization frameworks. For the smooth and non-convex functions class, we ",
    "path": "papers/22/10/2210.01176.json",
    "total_tokens": 843,
    "translated_title": "PersA-FL：个性化异步联邦学习",
    "translated_abstract": "我们研究了异步更新下的个性化联邦学习问题。在这个问题中，每个客户端都希望获得一个个性化模型，同时能够优于本地模型和全局模型。我们考虑了两个基于优化的个性化框架：（i）模型无关元学习（MAML）和（ii）Moreau包络（ME）。MAML通过微调学习适应于每个客户端的联合模型，而ME通过隐式梯度的双层优化问题来通过规范化损失实现个性化。我们的主要技术贡献是对有界滞后的异步联邦学习进行统一证明，并将其应用于MAML和ME个性化框架。针对平滑和非凸函数类，我们进一步扩展了所研究的函数类，去除了梯度范数的有界性假设。",
    "tldr": "本论文研究了异步更新下的个性化联邦学习问题，并提出了一种改进的个性化方法，通过移除同步通信假设和去除梯度范数有界性假设来提高可伸缩性。",
    "en_tdlr": "This paper investigates the problem of personalized federated learning under asynchronous updates, proposing an improved approach that enhances scalability by removing synchronous communication assumptions and boundedness assumptions on the gradient norm."
}