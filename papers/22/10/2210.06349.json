{
    "title": "Context Generation Improves Open Domain Question Answering. (arXiv:2210.06349v2 [cs.CL] UPDATED)",
    "abstract": "Closed-book question answering (QA) requires a model to directly answer an open-domain question without access to any external knowledge. Prior work on closed-book QA either directly finetunes or prompts a pretrained language model (LM) to leverage the stored knowledge. However, they do not fully exploit the parameterized knowledge. To address this issue, we propose a two-stage, closed-book QA framework which employs a coarse-to-fine approach to extract relevant knowledge and answer a question. Our approach first generates a related context for a given question by prompting a pretrained LM. We then prompt the same LM for answer prediction using the generated context and the question. Additionally, to eliminate failure caused by context uncertainty, we marginalize over generated contexts. Experimental results on three QA benchmarks show that our method significantly outperforms previous closed-book QA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book methods th",
    "link": "http://arxiv.org/abs/2210.06349",
    "context": "Title: Context Generation Improves Open Domain Question Answering. (arXiv:2210.06349v2 [cs.CL] UPDATED)\nAbstract: Closed-book question answering (QA) requires a model to directly answer an open-domain question without access to any external knowledge. Prior work on closed-book QA either directly finetunes or prompts a pretrained language model (LM) to leverage the stored knowledge. However, they do not fully exploit the parameterized knowledge. To address this issue, we propose a two-stage, closed-book QA framework which employs a coarse-to-fine approach to extract relevant knowledge and answer a question. Our approach first generates a related context for a given question by prompting a pretrained LM. We then prompt the same LM for answer prediction using the generated context and the question. Additionally, to eliminate failure caused by context uncertainty, we marginalize over generated contexts. Experimental results on three QA benchmarks show that our method significantly outperforms previous closed-book QA methods (e.g. exact matching 68.6% vs. 55.3%), and is on par with open-book methods th",
    "path": "papers/22/10/2210.06349.json",
    "total_tokens": 923,
    "translated_title": "上下文生成提高开放领域问答的效果",
    "translated_abstract": "封闭式问答需要模型在没有任何外部知识的情况下直接回答开放式领域的问题。以往的封闭式问答工作要么将预训练语言模型（LM）直接微调，要么通过提示信息来利用存储的知识。但它们没有充分利用参数化知识。为解决这个问题，我们提出了一个两阶段的闭式问答框架，它采用粗略到精细的方法来提取相关知识和回答问题。我们的方法首先通过提示预先训练的LM生成针对给定问题的相关上下文。然后，我们再使用这个LM通过生成的上下文和问题提示答案预测。此外，为了消除上下文不确定性带来的错误，我们还对生成的上下文进行了边际化处理。在三个问答基准测试上的实验结果表明，我们的方法明显优于以前的封闭式问答方法（如精确匹配 68.6% 对 55.3%），且与开放式方法持平。",
    "tldr": "该论文提出了一个两阶段的闭式问答框架，首先通过生成上下文来提取相关知识，然后使用这个知识回答问题，实验证明该方法在三个问答基准测试上明显优于以往的封闭式问答方法，并且与开放式方法持平。",
    "en_tdlr": "This paper proposes a two-stage closed-book QA framework that generates relevant contexts and extracts knowledge to answer questions, outperforming previous closed-book QA methods in three benchmarks and on par with open-book methods."
}