{
    "title": "Probabilistic partition of unity networks for high-dimensional regression problems. (arXiv:2210.02694v2 [cs.LG] UPDATED)",
    "abstract": "We explore the probabilistic partition of unity network (PPOU-Net) model in the context of high-dimensional regression problems and propose a general framework focusing on adaptive dimensionality reduction. With the proposed framework, the target function is approximated by a mixture of experts model on a low-dimensional manifold, where each cluster is associated with a local fixed-degree polynomial. We present a training strategy that leverages the expectation maximization (EM) algorithm. During the training, we alternate between (i) applying gradient descent to update the DNN coefficients; and (ii) using closed-form formulae derived from the EM algorithm to update the mixture of experts model parameters. Under the probabilistic formulation, step (ii) admits the form of embarrassingly parallelizable weighted least-squares solves. The PPOU-Nets consistently outperform the baseline fully-connected neural networks of comparable sizes in numerical experiments of various data dimensions. W",
    "link": "http://arxiv.org/abs/2210.02694",
    "context": "Title: Probabilistic partition of unity networks for high-dimensional regression problems. (arXiv:2210.02694v2 [cs.LG] UPDATED)\nAbstract: We explore the probabilistic partition of unity network (PPOU-Net) model in the context of high-dimensional regression problems and propose a general framework focusing on adaptive dimensionality reduction. With the proposed framework, the target function is approximated by a mixture of experts model on a low-dimensional manifold, where each cluster is associated with a local fixed-degree polynomial. We present a training strategy that leverages the expectation maximization (EM) algorithm. During the training, we alternate between (i) applying gradient descent to update the DNN coefficients; and (ii) using closed-form formulae derived from the EM algorithm to update the mixture of experts model parameters. Under the probabilistic formulation, step (ii) admits the form of embarrassingly parallelizable weighted least-squares solves. The PPOU-Nets consistently outperform the baseline fully-connected neural networks of comparable sizes in numerical experiments of various data dimensions. W",
    "path": "papers/22/10/2210.02694.json",
    "total_tokens": 968,
    "translated_title": "高维回归问题的概率统一分治网络（PPOU-Net）模型",
    "translated_abstract": "我们探讨了概率统一分治网络(PPOU-Net)模型在高维回归问题中的应用，并提出了一种自适应降维的通用框架。通过该框架，目标函数被低维流形上每个聚类关联的具有本地固定次数的多项式所逼近。我们提出了一种训练策略，利用期望极大(EM)算法。在训练期间，我们交替执行以下两个步骤：(i)使用梯度下降更新 DNN 系数;(ii)使用从EM算法推导出的闭式公式更新每个模型的参数。在概率公式下，步骤(ii)允许严重可并行的加权最小二乘解的形式。在各种数据维度的数值实验中，与可比较大小的基准全连接神经网络相比，PPOU-Net稳定地表现出更好的性能。",
    "tldr": "本论文介绍了一种概率统一分治网络（PPOU-Net）模型应用在高维回归问题上的自适应降维通用框架，该模型通过低维流形上的多项式逼近目标函数，并采用期望极大算法进行训练，实验证明在各种数据维度情况下，PPOU-Net表现出更好的性能。",
    "en_tdlr": "This paper introduces a general framework for adaptive dimensionality reduction based on the probabilistic partition of unity network (PPOU-Net) model applied to high-dimensional regression problems. The model approximates the target function by a mixture of experts model on a low-dimensional manifold and is trained using the expectation maximization algorithm. Numerical experiments show that PPOU-Net outperforms fully-connected neural networks of comparable sizes in various data dimensions."
}