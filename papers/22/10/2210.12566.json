{
    "title": "Solving Continuous Control via Q-learning. (arXiv:2210.12566v2 [cs.LG] UPDATED)",
    "abstract": "While there has been substantial success for solving continuous control with actor-critic methods, simpler critic-only methods such as Q-learning find limited application in the associated high-dimensional action spaces. However, most actor-critic methods come at the cost of added complexity: heuristics for stabilisation, compute requirements and wider hyperparameter search spaces. We show that a simple modification of deep Q-learning largely alleviates these issues. By combining bang-bang action discretization with value decomposition, framing single-agent control as cooperative multi-agent reinforcement learning (MARL), this simple critic-only approach matches performance of state-of-the-art continuous actor-critic methods when learning from features or pixels. We extend classical bandit examples from cooperative MARL to provide intuition for how decoupled critics leverage state information to coordinate joint optimization, and demonstrate surprisingly strong performance across a var",
    "link": "http://arxiv.org/abs/2210.12566",
    "context": "Title: Solving Continuous Control via Q-learning. (arXiv:2210.12566v2 [cs.LG] UPDATED)\nAbstract: While there has been substantial success for solving continuous control with actor-critic methods, simpler critic-only methods such as Q-learning find limited application in the associated high-dimensional action spaces. However, most actor-critic methods come at the cost of added complexity: heuristics for stabilisation, compute requirements and wider hyperparameter search spaces. We show that a simple modification of deep Q-learning largely alleviates these issues. By combining bang-bang action discretization with value decomposition, framing single-agent control as cooperative multi-agent reinforcement learning (MARL), this simple critic-only approach matches performance of state-of-the-art continuous actor-critic methods when learning from features or pixels. We extend classical bandit examples from cooperative MARL to provide intuition for how decoupled critics leverage state information to coordinate joint optimization, and demonstrate surprisingly strong performance across a var",
    "path": "papers/22/10/2210.12566.json",
    "total_tokens": 877,
    "translated_title": "通过Q-learning解决连续控制问题",
    "translated_abstract": "尽管在解决连续控制问题上，使用actor-critic方法取得了巨大的成功，但是简单的critic-only方法如Q-learning在涉及高维动作空间时应用有限。然而，大多数actor-critic方法的成本是增加了复杂性：稳定性启发式、计算要求和更广泛的超参数搜索空间。我们展示了一种对深度Q-learning进行简单修改的方法，大大减轻了这些问题。通过将bang-bang动作离散化与值分解相结合，将单智能体控制视为合作多智能体强化学习（MARL），这种简单的critic-only方法在从特征或像素学习时与最先进的连续actor-critic方法的性能相匹配。我们将合作MARL的经典赌徒问题扩展到了提供直观感觉的，展示了解耦的critics如何利用状态信息协调联合优化，并表现出了出乎意料的强大性能。",
    "tldr": "本研究通过对Q-learning进行简单修改，通过将bang-bang动作离散化与值分解相结合，将单智能体控制视为合作多智能体强化学习来解决连续控制问题，并取得了与最先进的连续actor-critic方法相匹配的性能。"
}