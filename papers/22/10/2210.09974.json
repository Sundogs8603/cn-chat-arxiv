{
    "title": "Theoretical Guarantees for Permutation-Equivariant Quantum Neural Networks",
    "abstract": "arXiv:2210.09974v3 Announce Type: replace-cross Abstract: Despite the great promise of quantum machine learning models, there are several challenges one must overcome before unlocking their full potential. For instance, models based on quantum neural networks (QNNs) can suffer from excessive local minima and barren plateaus in their training landscapes. Recently, the nascent field of geometric quantum machine learning (GQML) has emerged as a potential solution to some of those issues. The key insight of GQML is that one should design architectures, such as equivariant QNNs, encoding the symmetries of the problem at hand. Here, we focus on problems with permutation symmetry (i.e., the group of symmetry $S_n$), and show how to build $S_n$-equivariant QNNs. We provide an analytical study of their performance, proving that they do not suffer from barren plateaus, quickly reach overparametrization, and generalize well from small amounts of data. To verify our results, we perform numerical s",
    "link": "https://arxiv.org/abs/2210.09974",
    "context": "Title: Theoretical Guarantees for Permutation-Equivariant Quantum Neural Networks\nAbstract: arXiv:2210.09974v3 Announce Type: replace-cross Abstract: Despite the great promise of quantum machine learning models, there are several challenges one must overcome before unlocking their full potential. For instance, models based on quantum neural networks (QNNs) can suffer from excessive local minima and barren plateaus in their training landscapes. Recently, the nascent field of geometric quantum machine learning (GQML) has emerged as a potential solution to some of those issues. The key insight of GQML is that one should design architectures, such as equivariant QNNs, encoding the symmetries of the problem at hand. Here, we focus on problems with permutation symmetry (i.e., the group of symmetry $S_n$), and show how to build $S_n$-equivariant QNNs. We provide an analytical study of their performance, proving that they do not suffer from barren plateaus, quickly reach overparametrization, and generalize well from small amounts of data. To verify our results, we perform numerical s",
    "path": "papers/22/10/2210.09974.json",
    "total_tokens": 953,
    "translated_title": "《置换等变量量子神经网络的理论保证》",
    "translated_abstract": "尽管量子机器学习模型有着巨大的潜力，但在释放其全部潜力之前，我们必须克服一些挑战。例如，基于量子神经网络（QNN）的模型可能在训练过程中遇到过多的局部最小值和贫瘠的高原问题。最近，几何量子机器学习（GQML）这一新兴领域已被提出作为其中一些问题的潜在解决方案。GQML 的关键见解是，我们应该设计编码问题的对称性的架构，例如等变 QNNs。在这里，我们专注于具有置换对称性（即对称群 $S_n$）的问题，并展示了如何构建 $S_n$-equivariant QNNs。我们对它们的性能进行了分析研究，证明它们不会遭遇贫瘠的高原问题，能够快速实现过参数化，并且能够从少量的数据中进行良好的泛化。为了验证我们的结果，我们进行了数值仿真实验。",
    "tldr": "这篇论文主要研究了置换等变量量子神经网络在量子机器学习中的应用。研究发现，这种架构能够解决传统 QNNs 遇到的局部最小值和贫瘠的高原问题，并能够在少量数据上进行良好的泛化。",
    "en_tdlr": "This paper focuses on the application of permutation-equivariant quantum neural networks in quantum machine learning. The study shows that these architectures can overcome the issues of local minima and barren plateaus encountered by traditional QNNs, and achieve good generalization on small amounts of data."
}