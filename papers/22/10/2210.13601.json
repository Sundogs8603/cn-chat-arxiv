{
    "title": "Active Learning for Single Neuron Models with Lipschitz Non-Linearities. (arXiv:2210.13601v2 [cs.LG] UPDATED)",
    "abstract": "We consider the problem of active learning for single neuron models, also sometimes called ``ridge functions'', in the agnostic setting (under adversarial label noise). Such models have been shown to be broadly effective in modeling physical phenomena, and for constructing surrogate data-driven models for partial differential equations.  Surprisingly, we show that for a single neuron model with any Lipschitz non-linearity (such as the ReLU, sigmoid, absolute value, low-degree polynomial, among others), strong provable approximation guarantees can be obtained using a well-known active learning strategy for fitting \\emph{linear functions} in the agnostic setting. % -- i.e. for the case when there is no non-linearity. Namely, we can collect samples via statistical \\emph{leverage score sampling}, which has been shown to be near-optimal in other active learning scenarios. We support our theoretical results with empirical simulations showing that our proposed active learning strategy based o",
    "link": "http://arxiv.org/abs/2210.13601",
    "context": "Title: Active Learning for Single Neuron Models with Lipschitz Non-Linearities. (arXiv:2210.13601v2 [cs.LG] UPDATED)\nAbstract: We consider the problem of active learning for single neuron models, also sometimes called ``ridge functions'', in the agnostic setting (under adversarial label noise). Such models have been shown to be broadly effective in modeling physical phenomena, and for constructing surrogate data-driven models for partial differential equations.  Surprisingly, we show that for a single neuron model with any Lipschitz non-linearity (such as the ReLU, sigmoid, absolute value, low-degree polynomial, among others), strong provable approximation guarantees can be obtained using a well-known active learning strategy for fitting \\emph{linear functions} in the agnostic setting. % -- i.e. for the case when there is no non-linearity. Namely, we can collect samples via statistical \\emph{leverage score sampling}, which has been shown to be near-optimal in other active learning scenarios. We support our theoretical results with empirical simulations showing that our proposed active learning strategy based o",
    "path": "papers/22/10/2210.13601.json",
    "total_tokens": 813,
    "translated_title": "具有利普希茨非线性的单个神经元模型的主动学习",
    "translated_abstract": "我们考虑在敌对标签噪声下的对单个神经元模型进行主动学习的问题，有时也被称为“岭函数”。这些模型已被证明广泛有效地模拟物理现象，并构建用于偏微分方程的代理数据驱动模型。令人惊讶的是，我们证明对于具有任何利普希茨非线性（如ReLU函数、sigmoid函数、绝对值函数、低次多项式函数等）的单个神经元模型，可以使用已知的在敌对标签噪声下拟合“线性函数”的主动学习策略获得强有力的可证明的逼近保证。",
    "tldr": "该论文提出一种针对具有利普希茨非线性的单个神经元模型的主动学习策略，该策略在敌对标签噪声下拟合线性函数，并在逼近保证方面具有强有力的可证明性能。",
    "en_tdlr": "This paper proposes an active learning strategy for single neuron models with Lipschitz non-linearities, which fits linear functions under adversarial label noise and provides strong provable approximation guarantees."
}