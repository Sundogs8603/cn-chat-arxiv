{
    "title": "Neural Eigenfunctions Are Structured Representation Learners. (arXiv:2210.12637v2 [cs.LG] UPDATED)",
    "abstract": "This paper introduces a structured, adaptive-length deep representation called Neural Eigenmap. Unlike prior spectral methods such as Laplacian Eigenmap that operate in a nonparametric manner, Neural Eigenmap leverages NeuralEF to parametrically model eigenfunctions using a neural network. We show that, when the eigenfunction is derived from positive relations in a data augmentation setup, applying NeuralEF results in an objective function that resembles those of popular self-supervised learning methods, with an additional symmetry-breaking property that leads to structured representations where features are ordered by importance. We demonstrate using such representations as adaptive-length codes in image retrieval systems. By truncation according to feature importance, our method requires up to $16\\times$ shorter representation length than leading self-supervised learning ones to achieve similar retrieval performance. We further apply our method to graph data and report strong results",
    "link": "http://arxiv.org/abs/2210.12637",
    "context": "Title: Neural Eigenfunctions Are Structured Representation Learners. (arXiv:2210.12637v2 [cs.LG] UPDATED)\nAbstract: This paper introduces a structured, adaptive-length deep representation called Neural Eigenmap. Unlike prior spectral methods such as Laplacian Eigenmap that operate in a nonparametric manner, Neural Eigenmap leverages NeuralEF to parametrically model eigenfunctions using a neural network. We show that, when the eigenfunction is derived from positive relations in a data augmentation setup, applying NeuralEF results in an objective function that resembles those of popular self-supervised learning methods, with an additional symmetry-breaking property that leads to structured representations where features are ordered by importance. We demonstrate using such representations as adaptive-length codes in image retrieval systems. By truncation according to feature importance, our method requires up to $16\\times$ shorter representation length than leading self-supervised learning ones to achieve similar retrieval performance. We further apply our method to graph data and report strong results",
    "path": "papers/22/10/2210.12637.json",
    "total_tokens": 1015,
    "translated_title": "神经特征向量是结构化表示学习器",
    "translated_abstract": "本文介绍了一种称为神经特征映射的结构化自适应深度表示。与先前的谱方法（如拉普拉斯特征映射）以非参数化方式进行操作不同，神经特征映射利用神经网络对特征值函数进行参数化建模。我们展示了当特征值函数来自于数据扩增设置中的正相关关系时，应用神经特征映射会产生类似于流行的自监督学习方法的目标函数，同时还具有打破对称性的属性，从而导致结构化表示，其中特征按重要性进行排序。我们在图像检索系统中演示了使用这样的自适应长度编码来表示。通过根据特征的重要性进行截断，我们的方法所需的表示长度比领先的自监督学习方法短16倍，同时达到相似的检索性能。我们进一步将我们的方法应用于图形数据，并报告了强大的结果。",
    "tldr": "本文提出了一种称为神经特征映射的结构化自适应深度表示方法，它通过神经网络对特征值函数进行参数化建模。应用神经特征映射可以得到类似于流行的自监督学习方法的目标函数，并具有打破对称性的属性，从而产生结构化表示，其中特征按重要性进行排序。在图像检索系统中，通过根据特征的重要性进行截断，我们的方法所需的表示长度比领先的自监督学习方法短16倍，同时具有相似的检索性能。",
    "en_tdlr": "This paper introduces a structured and adaptive-length deep representation method called Neural Eigenmap, which uses a neural network to parametrically model eigenfunctions. Applying Neural Eigenmap yields an objective function similar to popular self-supervised learning methods, with the additional property of breaking symmetry and generating structured representations where features are ordered by importance. The method achieves similar retrieval performance with representation lengths up to 16 times shorter than leading self-supervised learning approaches."
}