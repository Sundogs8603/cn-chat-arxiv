{
    "title": "Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding. (arXiv:2210.03347v2 [cs.CL] UPDATED)",
    "abstract": "Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representa",
    "link": "http://arxiv.org/abs/2210.03347",
    "context": "Title: Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding. (arXiv:2210.03347v2 [cs.CL] UPDATED)\nAbstract: Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representa",
    "path": "papers/22/10/2210.03347.json",
    "total_tokens": 968,
    "translated_title": "Pix2Struct：屏幕截图解析作为视觉语言理解的预训练方法",
    "translated_abstract": "视觉语境的语言是无处不在的，它源于从有图示的教科书到有图像和表格的网页再到有按钮和表单的移动应用。为此，以往的研究通常依赖于特定领域的技术，且很少共享基础数据、模型架构和目标。本文提出了一种名为Pix2Struct的预训练图像到文本模型，用于纯视觉语言理解，可以在包含视觉语境的任务上进行微调。Pix2Struct的预训练目标是学习将屏幕截图解析成简化的HTML。Web具有丰富的视觉元素，这些元素在HTML结构中清晰地反映出来，提供了大量的预训练数据，非常适用于多样化的下游任务。直观上，这个目标涵盖了常见的预训练信号，例如OCR、语言模型和图像描述。除了新的预训练策略外，我们还引入了一个可变分辨率的输入表示。",
    "tldr": "本文提出了一个名为Pix2Struct的预训练图像到文本模型，用于纯视觉语言理解，可以在包含视觉语境的任务上进行微调。Pix2Struct的预训练目标是学习将屏幕截图解析成简化的HTML，这个方法能够处理来源复杂、多样性大的数据。"
}