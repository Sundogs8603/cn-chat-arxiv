{
    "title": "Global Contrastive Batch Sampling via Optimization on Sample Permutations. (arXiv:2210.12874v4 [cs.LG] UPDATED)",
    "abstract": "Contrastive Learning has recently achieved state-of-the-art performance in a wide range of tasks. Many contrastive learning approaches use mined hard negatives to make batches more informative during training but these approaches are inefficient as they increase epoch length proportional to the number of mined negatives and require frequent updates of nearest neighbor indices or mining from recent batches. In this work, we provide an alternative to hard negative mining, Global Contrastive Batch Sampling (GCBS), an efficient approximation to the batch assignment problem that upper bounds the gap between the global and training losses, $\\mathcal{L}^{Global} - \\mathcal{L}^{Train}$, in contrastive learning settings. Through experimentation we find GCBS improves state-of-the-art performance in sentence embedding and code-search tasks. Additionally, GCBS is easy to implement as it requires only a few additional lines of code, does not maintain external data structures such as nearest neighbo",
    "link": "http://arxiv.org/abs/2210.12874",
    "context": "Title: Global Contrastive Batch Sampling via Optimization on Sample Permutations. (arXiv:2210.12874v4 [cs.LG] UPDATED)\nAbstract: Contrastive Learning has recently achieved state-of-the-art performance in a wide range of tasks. Many contrastive learning approaches use mined hard negatives to make batches more informative during training but these approaches are inefficient as they increase epoch length proportional to the number of mined negatives and require frequent updates of nearest neighbor indices or mining from recent batches. In this work, we provide an alternative to hard negative mining, Global Contrastive Batch Sampling (GCBS), an efficient approximation to the batch assignment problem that upper bounds the gap between the global and training losses, $\\mathcal{L}^{Global} - \\mathcal{L}^{Train}$, in contrastive learning settings. Through experimentation we find GCBS improves state-of-the-art performance in sentence embedding and code-search tasks. Additionally, GCBS is easy to implement as it requires only a few additional lines of code, does not maintain external data structures such as nearest neighbo",
    "path": "papers/22/10/2210.12874.json",
    "total_tokens": 939,
    "translated_title": "基于样本排列优化的全局对比批量采样",
    "translated_abstract": "对比学习最近在各种任务中取得了最先进的性能。许多对比学习方法使用挖掘的硬负例来在训练期间使批处理更加信息丰富，但这些方法效率低下，因为它们增加了与挖掘负例数成比例的纪元长度，并需要频繁更新最近邻居索引或从最近的批次中进行挖掘。在这项工作中，我们提供了另一种硬负例挖掘的替代方案：全局对比批量采样（GCBS），一种有效的近似批处理分配问题，它上界了对比学习设置中的全局损失和训练损失之间的差距$\\mathcal{L}^{Global} - \\mathcal{L}^{Train}$。通过实验，我们发现GCBS改善了句子嵌入和代码搜索任务的最先进性能。此外，GCBS易于实现，因为它只需要少量附加代码，不需要维护外部数据结构，如最近邻居索引，并且适用于各种对比学习方法。",
    "tldr": "本论文提出了一种有效的替代硬负例挖掘的全局对比批量采样方法GCBS，能够提高对比学习任务的性能表现，易于实现且适用于各种对比学习方法。",
    "en_tdlr": "This paper proposes an efficient Global Contrastive Batch Sampling (GCBS) method as an alternative to hard negative mining, which can improve the performance of contrastive learning tasks, is easy to implement, and applicable to various contrastive learning approaches."
}