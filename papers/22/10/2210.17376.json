{
    "title": "SoK: Modeling Explainability in Security Analytics for Interpretability, Trustworthiness, and Usability. (arXiv:2210.17376v2 [cs.CR] UPDATED)",
    "abstract": "Interpretability, trustworthiness, and usability are key considerations in high-stake security applications, especially when utilizing deep learning models. While these models are known for their high accuracy, they behave as black boxes in which identifying important features and factors that led to a classification or a prediction is difficult. This can lead to uncertainty and distrust, especially when an incorrect prediction results in severe consequences. Thus, explanation methods aim to provide insights into the inner working of deep learning models. However, most explanation methods provide inconsistent explanations, have low fidelity, and are susceptible to adversarial manipulation, which can reduce model trustworthiness. This paper provides a comprehensive analysis of explainable methods and demonstrates their efficacy in three distinct security applications: anomaly detection using system logs, malware prediction, and detection of adversarial images. Our quantitative and quali",
    "link": "http://arxiv.org/abs/2210.17376",
    "context": "Title: SoK: Modeling Explainability in Security Analytics for Interpretability, Trustworthiness, and Usability. (arXiv:2210.17376v2 [cs.CR] UPDATED)\nAbstract: Interpretability, trustworthiness, and usability are key considerations in high-stake security applications, especially when utilizing deep learning models. While these models are known for their high accuracy, they behave as black boxes in which identifying important features and factors that led to a classification or a prediction is difficult. This can lead to uncertainty and distrust, especially when an incorrect prediction results in severe consequences. Thus, explanation methods aim to provide insights into the inner working of deep learning models. However, most explanation methods provide inconsistent explanations, have low fidelity, and are susceptible to adversarial manipulation, which can reduce model trustworthiness. This paper provides a comprehensive analysis of explainable methods and demonstrates their efficacy in three distinct security applications: anomaly detection using system logs, malware prediction, and detection of adversarial images. Our quantitative and quali",
    "path": "papers/22/10/2210.17376.json",
    "total_tokens": 878,
    "translated_title": "论文标题：SoK：面向可解释性、可靠性和可用性的安全分析解释性建模。",
    "translated_abstract": "可解释性、可靠性和可用性是高风险安全应用中的关键考虑因素，特别是在利用深度学习模型时。虽然这些模型以高精度著称，但它们表现为黑盒子，在其中识别导致分类或预测的重要特征和因素是困难的。这可能导致不确定性和不信任，特别是当错误预测导致严重后果时。因此，解释方法旨在提供对深度学习模型内部工作的洞见。然而，大多数解释方法提供不一致的解释，具有低保真度，并容易受到对抗性操纵，这可能会降低模型的可信度。本文对可解释性方法进行了全面分析，并证明了它们在三个不同的安全应用中的效力：使用系统日志进行异常检测、预测恶意软件和检测对抗性图像。我们进行了定量和定性评估。",
    "tldr": "本文提供了对可解释性方法进行全面分析的研究，证明了其在三种不同的安全应用中的效力。这些方法解释深度学习模型的内部工作，增加了模型的可信度。",
    "en_tdlr": "This paper provides a comprehensive analysis of explainable methods and demonstrates their efficacy in three distinct security applications, which provide insights into the inner workings of deep learning models and increase their trustworthiness."
}