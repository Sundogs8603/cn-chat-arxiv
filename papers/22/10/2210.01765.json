{
    "title": "One Transformer Can Understand Both 2D & 3D Molecular Data. (arXiv:2210.01765v4 [cs.LG] UPDATED)",
    "abstract": "Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. Whe",
    "link": "http://arxiv.org/abs/2210.01765",
    "context": "Title: One Transformer Can Understand Both 2D & 3D Molecular Data. (arXiv:2210.01765v4 [cs.LG] UPDATED)\nAbstract: Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. Whe",
    "path": "papers/22/10/2210.01765.json",
    "total_tokens": 781,
    "translated_title": "一个Transformer模型可同时处理2D和3D分子数据",
    "translated_abstract": "与通常有唯一格式的视觉和语言数据不同，分子可以自然地用不同的化学公式进行表征。对于分子表示学习，大多数先前的工作只设计了针对特定数据格式的神经网络，使得学习的模型可能无法处理其他数据格式。我们认为，化学的通用神经网络模型应能够处理跨数据模态的分子任务。为实现此目标，我们开发了一种新型的基于Transformer的分子模型，称为Transformer-M，它可以将2D或3D格式的分子数据作为输入并生成有意义的语义表示。使用标准Transformer作为骨干架构，Transformer-M开发了两个分离的通道来编码2D和3D结构信息，并将它们与网络模块中的原子特征结合起来。",
    "tldr": "本文提出了一个基于Transformer的分子模型，名为Transformer-M，可以处理2D和3D格式的分子数据并生成有意义的语义表示。",
    "en_tdlr": "This paper proposes a Transformer-based molecular model, called Transformer-M, which can handle molecular data in both 2D and 3D formats and generate meaningful semantic representations."
}