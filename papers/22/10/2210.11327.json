{
    "title": "Improving Data Quality with Training Dynamics of Gradient Boosting Decision Trees",
    "abstract": "arXiv:2210.11327v2 Announce Type: replace  Abstract: Real world datasets contain incorrectly labeled instances that hamper the performance of the model and, in particular, the ability to generalize out of distribution. Also, each example might have different contribution towards learning. This motivates studies to better understanding of the role of data instances with respect to their contribution in good metrics in models. In this paper we propose a method based on metrics computed from training dynamics of Gradient Boosting Decision Trees (GBDTs) to assess the behavior of each training example. We focus on datasets containing mostly tabular or structured data, for which the use of Decision Trees ensembles are still the state-of-the-art in terms of performance. Our methods achieved the best results overall when compared with confident learning, direct heuristics and a robust boosting algorithm. We show results on detecting noisy labels in order clean datasets, improving models' metri",
    "link": "https://arxiv.org/abs/2210.11327",
    "context": "Title: Improving Data Quality with Training Dynamics of Gradient Boosting Decision Trees\nAbstract: arXiv:2210.11327v2 Announce Type: replace  Abstract: Real world datasets contain incorrectly labeled instances that hamper the performance of the model and, in particular, the ability to generalize out of distribution. Also, each example might have different contribution towards learning. This motivates studies to better understanding of the role of data instances with respect to their contribution in good metrics in models. In this paper we propose a method based on metrics computed from training dynamics of Gradient Boosting Decision Trees (GBDTs) to assess the behavior of each training example. We focus on datasets containing mostly tabular or structured data, for which the use of Decision Trees ensembles are still the state-of-the-art in terms of performance. Our methods achieved the best results overall when compared with confident learning, direct heuristics and a robust boosting algorithm. We show results on detecting noisy labels in order clean datasets, improving models' metri",
    "path": "papers/22/10/2210.11327.json",
    "total_tokens": 915,
    "translated_title": "利用梯度提升决策树的训练动态提高数据质量",
    "translated_abstract": "真实世界的数据集中常常包含有错误标记的实例，这会影响模型的性能，尤其是在泛化超出分布范围时。同时，每个示例对学习过程可能有不同的贡献。这促使研究者更好地理解数据实例在模型中对好指标的贡献角色。本文提出了一种基于梯度提升决策树（GBDTs）训练动态计算的度量来评估每个训练实例行为的方法。我们专注于包含大部分表格化或结构化数据的数据集，对于这类数据集，决策树集成在性能方面仍处于领先地位。与自信学习、直接启发式和健壮提升算法相比，我们的方法在整体上取得了最佳结果。我们展示了在检测嘈杂标签以清理数据集、改进模型指标方面的结果。",
    "tldr": "本文提出了一种基于梯度提升决策树的训练动态来评估每个训练实例行为的方法，针对包含大部分表格化或结构化数据的数据集，相较于自信学习、直接启发式和健壮提升算法，取得了最佳结果。",
    "en_tdlr": "This paper proposes a method based on the training dynamics of Gradient Boosting Decision Trees (GBDTs) to evaluate the behavior of each training example, focusing on datasets containing mostly tabular or structured data, and achieves the best results compared to confident learning, direct heuristics, and a robust boosting algorithm."
}