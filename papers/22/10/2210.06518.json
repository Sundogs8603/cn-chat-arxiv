{
    "title": "Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories. (arXiv:2210.06518v3 [cs.LG] UPDATED)",
    "abstract": "Natural agents can effectively learn from multiple data sources that differ in size, quality, and types of measurements. We study this heterogeneity in the context of offline reinforcement learning (RL) by introducing a new, practically motivated semi-supervised setting. Here, an agent has access to two sets of trajectories: labelled trajectories containing state, action and reward triplets at every timestep, along with unlabelled trajectories that contain only state and reward information. For this setting, we develop and study a simple meta-algorithmic pipeline that learns an inverse dynamics model on the labelled data to obtain proxy-labels for the unlabelled data, followed by the use of any offline RL algorithm on the true and proxy-labelled trajectories. Empirically, we find this simple pipeline to be highly successful -- on several D4RL benchmarks~\\cite{fu2020d4rl}, certain offline RL algorithms can match the performance of variants trained on a fully labelled dataset even when w",
    "link": "http://arxiv.org/abs/2210.06518",
    "context": "Title: Semi-Supervised Offline Reinforcement Learning with Action-Free Trajectories. (arXiv:2210.06518v3 [cs.LG] UPDATED)\nAbstract: Natural agents can effectively learn from multiple data sources that differ in size, quality, and types of measurements. We study this heterogeneity in the context of offline reinforcement learning (RL) by introducing a new, practically motivated semi-supervised setting. Here, an agent has access to two sets of trajectories: labelled trajectories containing state, action and reward triplets at every timestep, along with unlabelled trajectories that contain only state and reward information. For this setting, we develop and study a simple meta-algorithmic pipeline that learns an inverse dynamics model on the labelled data to obtain proxy-labels for the unlabelled data, followed by the use of any offline RL algorithm on the true and proxy-labelled trajectories. Empirically, we find this simple pipeline to be highly successful -- on several D4RL benchmarks~\\cite{fu2020d4rl}, certain offline RL algorithms can match the performance of variants trained on a fully labelled dataset even when w",
    "path": "papers/22/10/2210.06518.json",
    "total_tokens": 947,
    "translated_title": "基于无动作轨迹的半监督离线强化学习",
    "translated_abstract": "自然智能体可以有效地从不同大小、质量和测量类型的多个数据源中学习。我们通过引入一个新的、实际上受到启发的半监督设置来研究强化学习中的这种异质性。在这里，智能体可以访问两个轨迹集：一个包含每个时间步的状态、行为和奖励三元组的标记轨迹集，以及一个仅包含状态和奖励信息的未标记轨迹集。针对这种情况，我们开发和研究了一个简单的元算法流水线，该算法在标记数据上学习反动力学模型，以获得未标记数据的代理标签，然后将任何离线强化学习算法用于真实和代理标记轨迹。经验表明，这个简单的流水线非常成功——在几个D4RL基准测试中，某些离线RL算法可以与在完全标记数据集上训练的变体的表现相匹配，即使后者拥有更多的标记数据。",
    "tldr": "该论文提出了一种半监督离线强化学习的新设置，利用有标记的轨迹数据和无动作的轨迹数据训练反动力学模型以获取代理标签，最终使用任何离线强化学习算法以实现高成功率的表现。",
    "en_tdlr": "This paper proposes a new setting for semi-supervised offline reinforcement learning, which uses both labelled and action-free unlabelled trajectories to train an inverse dynamics model for obtaining proxy-labels, followed by any offline RL algorithm to achieve high success rate performance."
}