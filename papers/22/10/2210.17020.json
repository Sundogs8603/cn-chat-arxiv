{
    "title": "A Law of Data Separation in Deep Learning. (arXiv:2210.17020v2 [cs.LG] UPDATED)",
    "abstract": "While deep learning has enabled significant advances in many areas of science, its black-box nature hinders architecture design for future artificial intelligence applications and interpretation for high-stakes decision makings. We addressed this issue by studying the fundamental question of how deep neural networks process data in the intermediate layers. Our finding is a simple and quantitative law that governs how deep neural networks separate data according to class membership throughout all layers for classification. This law shows that each layer improves data separation at a constant geometric rate, and its emergence is observed in a collection of network architectures and datasets during training. This law offers practical guidelines for designing architectures, improving model robustness and out-of-sample performance, as well as interpreting the predictions.",
    "link": "http://arxiv.org/abs/2210.17020",
    "context": "Title: A Law of Data Separation in Deep Learning. (arXiv:2210.17020v2 [cs.LG] UPDATED)\nAbstract: While deep learning has enabled significant advances in many areas of science, its black-box nature hinders architecture design for future artificial intelligence applications and interpretation for high-stakes decision makings. We addressed this issue by studying the fundamental question of how deep neural networks process data in the intermediate layers. Our finding is a simple and quantitative law that governs how deep neural networks separate data according to class membership throughout all layers for classification. This law shows that each layer improves data separation at a constant geometric rate, and its emergence is observed in a collection of network architectures and datasets during training. This law offers practical guidelines for designing architectures, improving model robustness and out-of-sample performance, as well as interpreting the predictions.",
    "path": "papers/22/10/2210.17020.json",
    "total_tokens": 836,
    "translated_title": "深度学习中的数据分离定律",
    "translated_abstract": "虽然深度学习在科学的许多领域取得了重大进展，但其黑盒特性阻碍了未来人工智能应用的架构设计和高风险决策的解释。我们通过研究深度神经网络在中间层中如何处理数据来解决这个问题。我们的发现是一个简单而定量的定律，它规定了深度神经网络如何根据类别成员将数据在所有层中分离出来进行分类。这个定律表明，每一层都以恒定的几何速率改善数据的分离程度，并且在训练过程中观察到了它的出现，无论是在一系列网络架构还是数据集上。这个定律为架构设计、提高模型的健壮性和样本外性能以及预测的解释提供了实际的指导。",
    "tldr": "深度学习中存在一个简单而定量的数据分离定律，每一层都以恒定的几何速率改善数据的分离程度。这个定律为架构设计、提高模型的健壮性和样本外性能以及预测的解释提供了实际的指导。",
    "en_tdlr": "There is a simple and quantitative law of data separation in deep learning, where each layer improves data separation at a constant geometric rate. This law provides practical guidelines for architecture design, improving model robustness and out-of-sample performance, as well as interpretation of predictions."
}