{
    "title": "Beyond calibration: estimating the grouping loss of modern neural networks. (arXiv:2210.16315v3 [cs.LG] UPDATED)",
    "abstract": "The ability to ensure that a classifier gives reliable confidence scores is essential to ensure informed decision-making. To this end, recent work has focused on miscalibration, i.e., the over or under confidence of model scores. Yet calibration is not enough: even a perfectly calibrated classifier with the best possible accuracy can have confidence scores that are far from the true posterior probabilities. This is due to the grouping loss, created by samples with the same confidence scores but different true posterior probabilities. Proper scoring rule theory shows that given the calibration loss, the missing piece to characterize individual errors is the grouping loss. While there are many estimators of the calibration loss, none exists for the grouping loss in standard settings. Here, we propose an estimator to approximate the grouping loss. We show that modern neural network architectures in vision and NLP exhibit grouping loss, notably in distribution shifts settings, which highli",
    "link": "http://arxiv.org/abs/2210.16315",
    "context": "Title: Beyond calibration: estimating the grouping loss of modern neural networks. (arXiv:2210.16315v3 [cs.LG] UPDATED)\nAbstract: The ability to ensure that a classifier gives reliable confidence scores is essential to ensure informed decision-making. To this end, recent work has focused on miscalibration, i.e., the over or under confidence of model scores. Yet calibration is not enough: even a perfectly calibrated classifier with the best possible accuracy can have confidence scores that are far from the true posterior probabilities. This is due to the grouping loss, created by samples with the same confidence scores but different true posterior probabilities. Proper scoring rule theory shows that given the calibration loss, the missing piece to characterize individual errors is the grouping loss. While there are many estimators of the calibration loss, none exists for the grouping loss in standard settings. Here, we propose an estimator to approximate the grouping loss. We show that modern neural network architectures in vision and NLP exhibit grouping loss, notably in distribution shifts settings, which highli",
    "path": "papers/22/10/2210.16315.json",
    "total_tokens": 878,
    "translated_title": "超越校准：估计现代神经网络的分组损失",
    "translated_abstract": "确保分类器给出可靠的置信度分数是确保知情决策的关键。为此，最近的研究集中在误校准上，即模型分数的过度或不足置信。然而，校准还不够：即使准确率最高的完美校准分类器也可能具有与真实后验概率相去甚远的置信度分数，这是由于分组损失所造成的，即以相同置信度得分但真实后验概率不同的样本。适当的评分规则理论表明，在给定校准损失的情况下，表征单个错误的缺失部分是分组损失。虽然存在许多校准损失的估计器，但在标准设置中不存在分组损失的估计器。在本文中，我们提出了一个估计器来近似分组损失。我们展示了现代神经网络结构在视觉和NLP中表现出分组损失，特别是在分布偏移设置中，这突显了它的重要性。",
    "tldr": "本文提出了一个估计器来近似神经网络的分组损失，并表明现代神经网络在视觉和NLP中展示出显著的分组损失。",
    "en_tdlr": "This paper proposes an estimator to approximate the grouping loss of neural networks, highlighting its significance in modern neural network architectures in vision and NLP, especially in distribution shift settings."
}