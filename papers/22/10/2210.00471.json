{
    "title": "OCD: Learning to Overfit with Conditional Diffusion Models. (arXiv:2210.00471v5 [cs.LG] UPDATED)",
    "abstract": "We present a dynamic model in which the weights are conditioned on an input sample x and are learned to match those that would be obtained by finetuning a base model on x and its label y. This mapping between an input sample and network weights is approximated by a denoising diffusion model. The diffusion model we employ focuses on modifying a single layer of the base model and is conditioned on the input, activations, and output of this layer. Since the diffusion model is stochastic in nature, multiple initializations generate different networks, forming an ensemble, which leads to further improvements. Our experiments demonstrate the wide applicability of the method for image classification, 3D reconstruction, tabular data, speech separation, and natural language processing. Our code is available at https://github.com/ShaharLutatiPersonal/OCD",
    "link": "http://arxiv.org/abs/2210.00471",
    "context": "Title: OCD: Learning to Overfit with Conditional Diffusion Models. (arXiv:2210.00471v5 [cs.LG] UPDATED)\nAbstract: We present a dynamic model in which the weights are conditioned on an input sample x and are learned to match those that would be obtained by finetuning a base model on x and its label y. This mapping between an input sample and network weights is approximated by a denoising diffusion model. The diffusion model we employ focuses on modifying a single layer of the base model and is conditioned on the input, activations, and output of this layer. Since the diffusion model is stochastic in nature, multiple initializations generate different networks, forming an ensemble, which leads to further improvements. Our experiments demonstrate the wide applicability of the method for image classification, 3D reconstruction, tabular data, speech separation, and natural language processing. Our code is available at https://github.com/ShaharLutatiPersonal/OCD",
    "path": "papers/22/10/2210.00471.json",
    "total_tokens": 849,
    "translated_title": "OCD：使用条件扩散模型学习过度拟合",
    "translated_abstract": "我们提出了一个动态模型，其中权重基于输入样本x进行条件，学习匹配通过对x及其标签y进行微调所获得的权重。该输入样本与网络权重之间的映射通过去噪扩散模型进行近似。我们使用的扩散模型专注于修改基础模型的一个单层，并基于该层的输入、激活以及输出进行条件。由于扩散模型具有随机性质，多次初始化会生成不同的网络，形成一个集合，进一步提高了性能。我们的实验证明了该方法在图像分类、三维重建、表格数据、语音分离和自然语言处理方面的广泛适用性。我们的代码位于 https://github.com/ShaharLutatiPersonal/OCD。",
    "tldr": "本文提出了一种基于去噪扩散模型的动态模型，它通过将权重与输入样本进行条件，学习匹配微调的基础模型在输入和标签上获得的权重，可以在图像分类、3D重建、表格数据、语音分离和自然语言处理等领域得到广泛应用。",
    "en_tdlr": "This paper proposes a dynamic model based on denoising diffusion model, which conditions the weights on the input sample to match the weights obtained by fine-tuning the base model on x and y, leading to wide applicability in areas such as image classification, 3D reconstruction, tabular data, speech separation, and natural language processing."
}