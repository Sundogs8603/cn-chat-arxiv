{
    "title": "Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization. (arXiv:2210.05242v2 [cs.CV] UPDATED)",
    "abstract": "Audio-visual event (AVE) localization has attracted much attention in recent years. Most existing methods are often limited to independently encoding and classifying each video segment separated from the full video (which can be regarded as the segment-level representations of events). However, they ignore the semantic consistency of the event within the same full video (which can be considered as the video-level representations of events). In contrast to existing methods, we propose a novel video-level semantic consistency guidance network for the AVE localization task. Specifically, we propose an event semantic consistency modeling (ESCM) module to explore video-level semantic information for semantic consistency modeling. It consists of two components: a cross-modal event representation extractor (CERE) and an intra-modal semantic consistency enhancer (ISCE). CERE is proposed to obtain the event semantic information at the video level. Furthermore, ISCE takes video-level event seman",
    "link": "http://arxiv.org/abs/2210.05242",
    "context": "Title: Leveraging the Video-level Semantic Consistency of Event for Audio-visual Event Localization. (arXiv:2210.05242v2 [cs.CV] UPDATED)\nAbstract: Audio-visual event (AVE) localization has attracted much attention in recent years. Most existing methods are often limited to independently encoding and classifying each video segment separated from the full video (which can be regarded as the segment-level representations of events). However, they ignore the semantic consistency of the event within the same full video (which can be considered as the video-level representations of events). In contrast to existing methods, we propose a novel video-level semantic consistency guidance network for the AVE localization task. Specifically, we propose an event semantic consistency modeling (ESCM) module to explore video-level semantic information for semantic consistency modeling. It consists of two components: a cross-modal event representation extractor (CERE) and an intra-modal semantic consistency enhancer (ISCE). CERE is proposed to obtain the event semantic information at the video level. Furthermore, ISCE takes video-level event seman",
    "path": "papers/22/10/2210.05242.json",
    "total_tokens": 917,
    "translated_title": "利用事件的视频级语义一致性进行视听事件定位",
    "translated_abstract": "近年来，视听事件（AVE）定位引起了很大的关注。大多数现有方法通常只能独立地对从完整视频中分离出的每个视频段进行编码和分类（可视为事件的片段级表示）。然而，它们忽视了同一完整视频中事件的语义一致性（可视为事件的视频级表示）。与现有方法不同，我们提出了一种新颖的视频级语义一致性引导网络用于AVE定位任务。具体而言，我们提出了一种事件语义一致性建模（ESCM）模块，用于探索视频级语义信息进行语义一致性建模。它由两个组件组成：跨模态事件表示提取器（CERE）和内模态语义一致性增强器（ISCE）。CERE被提出用于获取视频级事件语义信息。此外，ISCE采取视频级事件语义信息并增强一致性建模。",
    "tldr": "该论文提出了一种新颖的视频级语义一致性引导网络，用于视听事件定位任务。它利用事件的视频级语义信息进行语义一致性建模，通过跨模态事件表示提取器和内模态语义一致性增强器实现。与现有方法相比，该方法能更好地捕捉和利用同一视频中事件的语义一致性。",
    "en_tdlr": "This paper proposes a novel video-level semantic consistency guidance network for audio-visual event localization. It models the semantic consistency of events using video-level semantic information, and utilizes a cross-modal event representation extractor and an intra-modal semantic consistency enhancer. Compared to existing methods, this approach better captures and leverages the semantic consistency of events within the same video."
}