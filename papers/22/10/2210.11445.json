{
    "title": "Bagging in overparameterized learning: Risk characterization and risk monotonization. (arXiv:2210.11445v2 [math.ST] UPDATED)",
    "abstract": "Bagging is a commonly used ensemble technique in statistics and machine learning to improve the performance of prediction procedures. In this paper, we study the prediction risk of variants of bagged predictors under the proportional asymptotics regime, in which the ratio of the number of features to the number of observations converges to a constant. Specifically, we propose a general strategy to analyze the prediction risk under squared error loss of bagged predictors using classical results on simple random sampling. Specializing the strategy, we derive the exact asymptotic risk of the bagged ridge and ridgeless predictors with an arbitrary number of bags under a well-specified linear model with arbitrary feature covariance matrices and signal vectors. Furthermore, we prescribe a generic cross-validation procedure to select the optimal subsample size for bagging and discuss its utility to eliminate the non-monotonic behavior of the limiting risk in the sample size (i.e., double or m",
    "link": "http://arxiv.org/abs/2210.11445",
    "context": "Title: Bagging in overparameterized learning: Risk characterization and risk monotonization. (arXiv:2210.11445v2 [math.ST] UPDATED)\nAbstract: Bagging is a commonly used ensemble technique in statistics and machine learning to improve the performance of prediction procedures. In this paper, we study the prediction risk of variants of bagged predictors under the proportional asymptotics regime, in which the ratio of the number of features to the number of observations converges to a constant. Specifically, we propose a general strategy to analyze the prediction risk under squared error loss of bagged predictors using classical results on simple random sampling. Specializing the strategy, we derive the exact asymptotic risk of the bagged ridge and ridgeless predictors with an arbitrary number of bags under a well-specified linear model with arbitrary feature covariance matrices and signal vectors. Furthermore, we prescribe a generic cross-validation procedure to select the optimal subsample size for bagging and discuss its utility to eliminate the non-monotonic behavior of the limiting risk in the sample size (i.e., double or m",
    "path": "papers/22/10/2210.11445.json",
    "total_tokens": 894,
    "translated_title": "Bagging在过度参数化学习中的风险刻画和风险单调化",
    "translated_abstract": "Bagging是统计学和机器学习中常用的集成技术，用于提高预测模型的性能。本文研究了在比例渐近情况下，各种变体的Bagging预测器的预测风险，其中特征数与观测数的比值收敛到常数。具体而言，我们提出了一种分析Bagging预测器在平方误差损失下的预测风险的通用策略，利用简单随机抽样的经典结果。通过特殊化该策略，我们推导了具有任意数量的包的Bagging Ridge和Ridgeless预测器在具有任意特征协方差矩阵和信号向量的良好指定线性模型下的精确渐近风险。此外，我们提供了一种通用的交叉验证过程，用于选择Bagging的最佳子样本大小，并讨论其在消除样本大小的风险的非单调行为方面的实用性。",
    "tldr": "本文研究了过度参数化学习中Bagging预测器的风险问题，并提出了通用策略来分析Bagging预测器的风险。通过具体化策略，我们得出了Bagging Ridge和Ridgeless预测器的精确渐近风险，并提供了一种交叉验证过程来选择Bagging的最佳子样本大小，以消除风险的非单调行为。"
}