{
    "title": "Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks. (arXiv:2210.05038v2 [cs.CL] UPDATED)",
    "abstract": "Searching troves of videos with textual descriptions is a core multimodal retrieval task. Owing to the lack of a purpose-built dataset for text-to-video retrieval, video captioning datasets have been re-purposed to evaluate models by (1) treating captions as positive matches to their respective videos and (2) assuming all other videos to be negatives. However, this methodology leads to a fundamental flaw during evaluation: since captions are marked as relevant only to their original video, many alternate videos also match the caption, which introduces false-negative caption-video pairs. We show that when these false negatives are corrected, a recent state-of-the-art model gains 25\\% recall points -- a difference that threatens the validity of the benchmark itself. To diagnose and mitigate this issue, we annotate and release 683K additional caption-video pairs. Using these, we recompute effectiveness scores for three models on two standard benchmarks (MSR-VTT and MSVD). We find that (1)",
    "link": "http://arxiv.org/abs/2210.05038",
    "context": "Title: Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks. (arXiv:2210.05038v2 [cs.CL] UPDATED)\nAbstract: Searching troves of videos with textual descriptions is a core multimodal retrieval task. Owing to the lack of a purpose-built dataset for text-to-video retrieval, video captioning datasets have been re-purposed to evaluate models by (1) treating captions as positive matches to their respective videos and (2) assuming all other videos to be negatives. However, this methodology leads to a fundamental flaw during evaluation: since captions are marked as relevant only to their original video, many alternate videos also match the caption, which introduces false-negative caption-video pairs. We show that when these false negatives are corrected, a recent state-of-the-art model gains 25\\% recall points -- a difference that threatens the validity of the benchmark itself. To diagnose and mitigate this issue, we annotate and release 683K additional caption-video pairs. Using these, we recompute effectiveness scores for three models on two standard benchmarks (MSR-VTT and MSVD). We find that (1)",
    "path": "papers/22/10/2210.05038.json",
    "total_tokens": 963,
    "translated_title": "使用FIRE对抗FIRe：评估文本到视频检索基准的有效性",
    "translated_abstract": "搜索具有文本描述的视频是一项核心的多模式检索任务。由于没有专门针对文本到视频检索的数据集，视频字幕数据集被重新用于通过(1)将字幕视为其各自视频的正匹配项和(2)假定所有其他视频为负匹配项来评估模型。然而，这种方法在评估过程中存在一个根本缺陷：由于只有原始视频标记为相关字幕，许多替代视频也匹配该字幕，导致了假阴性的字幕-视频对。我们展示了当这些假阴性得到纠正时，最近的一个最先进的模型将获得25\\%的召回率提升 - 这种差异威胁了基准本身的有效性。为了诊断和缓解这个问题，我们注释并发布了683K个额外的字幕-视频对。使用这些数据，我们在两个标准基准数据集（MSR-VTT和MSVD）上重新计算了三个模型的有效性分数。",
    "tldr": "文本到视频检索困难，视频字幕数据集被用作检测基准，但存在根本缺陷导致假阴性匹配。通过纠正假阴性，最先进的模型提高了25%的召回率，需要注释更多数据，重新计算有效性分数，以得出更准确的结果。"
}