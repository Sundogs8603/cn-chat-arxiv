{
    "title": "Exploring Contextual Representation and Multi-Modality for End-to-End Autonomous Driving. (arXiv:2210.06758v2 [cs.RO] UPDATED)",
    "abstract": "Learning contextual and spatial environmental representations enhances autonomous vehicle's hazard anticipation and decision-making in complex scenarios. Recent perception systems enhance spatial understanding with sensor fusion but often lack full environmental context. Humans, when driving, naturally employ neural maps that integrate various factors such as historical data, situational subtleties, and behavioral predictions of other road users to form a rich contextual understanding of their surroundings. This neural map-based comprehension is integral to making informed decisions on the road. In contrast, even with their significant advancements, autonomous systems have yet to fully harness this depth of human-like contextual understanding. Motivated by this, our work draws inspiration from human driving patterns and seeks to formalize the sensor fusion approach within an end-to-end autonomous driving framework. We introduce a framework that integrates three cameras (left, right, an",
    "link": "http://arxiv.org/abs/2210.06758",
    "context": "Title: Exploring Contextual Representation and Multi-Modality for End-to-End Autonomous Driving. (arXiv:2210.06758v2 [cs.RO] UPDATED)\nAbstract: Learning contextual and spatial environmental representations enhances autonomous vehicle's hazard anticipation and decision-making in complex scenarios. Recent perception systems enhance spatial understanding with sensor fusion but often lack full environmental context. Humans, when driving, naturally employ neural maps that integrate various factors such as historical data, situational subtleties, and behavioral predictions of other road users to form a rich contextual understanding of their surroundings. This neural map-based comprehension is integral to making informed decisions on the road. In contrast, even with their significant advancements, autonomous systems have yet to fully harness this depth of human-like contextual understanding. Motivated by this, our work draws inspiration from human driving patterns and seeks to formalize the sensor fusion approach within an end-to-end autonomous driving framework. We introduce a framework that integrates three cameras (left, right, an",
    "path": "papers/22/10/2210.06758.json",
    "total_tokens": 974,
    "translated_title": "探索上下文表达和多模态在端到端自动驾驶中的应用",
    "translated_abstract": "学习上下文和空间环境表示可以增强自动车辆在复杂场景中对威胁的预测和决策能力。最近的感知系统通过传感器融合增强了空间理解能力，但往往缺乏完整的环境上下文。人类在驾驶时自然地使用神经地图，将历史数据、情境细节和其他道路使用者的行为预测等各种因素整合起来，形成对周围环境的丰富上下文理解。这种基于神经地图的理解对于在道路上做出明智决策至关重要。相比之下，尽管自动系统取得了显著进展，但仍然没有完全利用人类般深度的上下文理解能力。受此启发，我们的工作借鉴人类驾驶模式，旨在在端到端自动驾驶框架中形式化传感器融合方法。我们提出了一个框架，将三个摄像头（左、右、后）的信息整合在一起，以实现全面的上下文感知。",
    "tldr": "本文旨在探索在端到端自动驾驶中应用上下文表达和多模态的方法。通过学习上下文和空间环境表示，可以增强自动车辆在复杂场景中的决策能力和威胁预测能力。我们提出了一个框架，通过整合多个摄像头的信息来实现全面的上下文感知。",
    "en_tdlr": "This paper explores the application of contextual representation and multi-modality in end-to-end autonomous driving. By learning contextual and spatial environmental representations, it enhances the decision-making and hazard anticipation capabilities of autonomous vehicles in complex scenarios. We propose a framework that integrates information from multiple cameras to achieve comprehensive contextual understanding."
}