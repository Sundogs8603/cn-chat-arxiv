{
    "title": "Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees",
    "abstract": "arXiv:2210.01282v3 Announce Type: replace-cross  Abstract: We consider the task of estimating a structural model of dynamic decisions by a human agent based upon the observable history of implemented actions and visited states. This problem has an inherent nested structure: in the inner problem, an optimal policy for a given reward function is identified while in the outer problem, a measure of fit is maximized. Several approaches have been proposed to alleviate the computational burden of this nested-loop structure, but these methods still suffer from high complexity when the state space is either discrete with large cardinality or continuous in high dimensions. Other approaches in the inverse reinforcement learning (IRL) literature emphasize policy estimation at the expense of reduced reward estimation accuracy. In this paper we propose a single-loop estimation algorithm with finite time guarantees that is equipped to deal with high-dimensional state spaces without compromising rewar",
    "link": "https://arxiv.org/abs/2210.01282",
    "context": "Title: Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees\nAbstract: arXiv:2210.01282v3 Announce Type: replace-cross  Abstract: We consider the task of estimating a structural model of dynamic decisions by a human agent based upon the observable history of implemented actions and visited states. This problem has an inherent nested structure: in the inner problem, an optimal policy for a given reward function is identified while in the outer problem, a measure of fit is maximized. Several approaches have been proposed to alleviate the computational burden of this nested-loop structure, but these methods still suffer from high complexity when the state space is either discrete with large cardinality or continuous in high dimensions. Other approaches in the inverse reinforcement learning (IRL) literature emphasize policy estimation at the expense of reduced reward estimation accuracy. In this paper we propose a single-loop estimation algorithm with finite time guarantees that is equipped to deal with high-dimensional state spaces without compromising rewar",
    "path": "papers/22/10/2210.01282.json",
    "total_tokens": 865,
    "translated_title": "高维状态空间中马尔可夫决策过程的结构估计与有限时间保证",
    "translated_abstract": "我们考虑基于可观测的行为历史和访问状态来估计人类代理动态决策的结构模型的任务。问题具有固有的嵌套结构：在内部问题中，确定给定奖励函数的最优策略，而在外部问题中，最大化适合度度量。已经提出了几种方法来减轻这种嵌套循环结构的计算负担，但当状态空间要么是具有大基数的离散空间，要么是高维连续空间时，这些方法仍然面临高复杂度的问题。逆强化学习(IRL)文献中的其他方法强调策略估计，但却以降低奖励估计精度为代价。在本文中，我们提出了一种具有有限时间保证的单循环估计算法，适用于处理高维状态空间而不会损害奖励。",
    "tldr": "本论文提出了一种单循环估计算法，具有有限时间保证，能够处理高维状态空间的马尔可夫决策过程的结构估计问题，而不会损害奖励估计精度。",
    "en_tdlr": "This paper proposes a single-loop estimation algorithm with finite time guarantees that is able to handle the structural estimation problem of Markov decision processes in high-dimensional state space without compromising reward estimation accuracy."
}