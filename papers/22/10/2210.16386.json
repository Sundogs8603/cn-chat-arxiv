{
    "title": "Dynamic Bandits with an Auto-Regressive Temporal Structure. (arXiv:2210.16386v2 [cs.LG] UPDATED)",
    "abstract": "Multi-armed bandit (MAB) problems are mainly studied under two extreme settings known as stochastic and adversarial. These two settings, however, do not capture realistic environments such as search engines and marketing and advertising, in which rewards stochastically change in time. Motivated by that, we introduce and study a dynamic MAB problem with stochastic temporal structure, where the expected reward of each arm is governed by an auto-regressive (AR) model. Due to the dynamic nature of the rewards, simple \"explore and commit\" policies fail, as all arms have to be explored continuously over time. We formalize this by characterizing a per-round regret lower bound, where the regret is measured against a strong (dynamic) benchmark. We then present an algorithm whose per-round regret almost matches our regret lower bound. Our algorithm relies on two mechanisms: (i) alternating between recently pulled arms and unpulled arms with potential, and (ii) restarting. These mechanisms enable",
    "link": "http://arxiv.org/abs/2210.16386",
    "context": "Title: Dynamic Bandits with an Auto-Regressive Temporal Structure. (arXiv:2210.16386v2 [cs.LG] UPDATED)\nAbstract: Multi-armed bandit (MAB) problems are mainly studied under two extreme settings known as stochastic and adversarial. These two settings, however, do not capture realistic environments such as search engines and marketing and advertising, in which rewards stochastically change in time. Motivated by that, we introduce and study a dynamic MAB problem with stochastic temporal structure, where the expected reward of each arm is governed by an auto-regressive (AR) model. Due to the dynamic nature of the rewards, simple \"explore and commit\" policies fail, as all arms have to be explored continuously over time. We formalize this by characterizing a per-round regret lower bound, where the regret is measured against a strong (dynamic) benchmark. We then present an algorithm whose per-round regret almost matches our regret lower bound. Our algorithm relies on two mechanisms: (i) alternating between recently pulled arms and unpulled arms with potential, and (ii) restarting. These mechanisms enable",
    "path": "papers/22/10/2210.16386.json",
    "total_tokens": 965,
    "tldr": "本论文研究了一种具有随机时间结构的动态 MAB 问题，提出了一种算法可行性良好，具有重新启动和交替使用被拉动的臂和未拉动的臂等机制。"
}