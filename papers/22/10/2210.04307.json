{
    "title": "KSAT: Knowledge-infused Self Attention Transformer -- Integrating Multiple Domain-Specific Contexts. (arXiv:2210.04307v2 [cs.CL] UPDATED)",
    "abstract": "Domain-specific language understanding requires integrating multiple pieces of relevant contextual information. For example, we see both suicide and depression-related behavior (multiple contexts) in the text ``I have a gun and feel pretty bad about my life, and it wouldn't be the worst thing if I didn't wake up tomorrow''. Domain specificity in self-attention architectures is handled by fine-tuning on excerpts from relevant domain specific resources (datasets and external knowledge - medical textbook chapters on mental health diagnosis related to suicide and depression). We propose a modified self-attention architecture Knowledge-infused Self Attention Transformer (KSAT) that achieves the integration of multiple domain-specific contexts through the use of external knowledge sources. KSAT introduces knowledge-guided biases in dedicated self-attention layers for each knowledge source to accomplish this. In addition, KSAT provides mechanics for controlling the trade-off between learning ",
    "link": "http://arxiv.org/abs/2210.04307",
    "context": "Title: KSAT: Knowledge-infused Self Attention Transformer -- Integrating Multiple Domain-Specific Contexts. (arXiv:2210.04307v2 [cs.CL] UPDATED)\nAbstract: Domain-specific language understanding requires integrating multiple pieces of relevant contextual information. For example, we see both suicide and depression-related behavior (multiple contexts) in the text ``I have a gun and feel pretty bad about my life, and it wouldn't be the worst thing if I didn't wake up tomorrow''. Domain specificity in self-attention architectures is handled by fine-tuning on excerpts from relevant domain specific resources (datasets and external knowledge - medical textbook chapters on mental health diagnosis related to suicide and depression). We propose a modified self-attention architecture Knowledge-infused Self Attention Transformer (KSAT) that achieves the integration of multiple domain-specific contexts through the use of external knowledge sources. KSAT introduces knowledge-guided biases in dedicated self-attention layers for each knowledge source to accomplish this. In addition, KSAT provides mechanics for controlling the trade-off between learning ",
    "path": "papers/22/10/2210.04307.json",
    "total_tokens": 834,
    "translated_title": "KSAT：知识注入的自我关注变压器——整合多个领域特定的上下文",
    "translated_abstract": "领域特定的语言理解需要整合多个相关的上下文信息。例如，“我拿着一把枪，对我的生活感到很糟糕，如果明天我不醒来，这可能不是最糟糕的事情”，其中包含自杀和抑郁症相关的行为（多个上下文）。自注意力结构中的领域特定性通过在相关领域特定资源的摘录上进行微调（数据集和外部知识-与自杀和抑郁症相关的心理健康诊断的医学教科书章节）来处理。我们提出了一种修改后的自我关注结构KSAT，通过使用外部知识源实现了多个领域特定上下文的整合。KSAT在每个知识源的专门自我关注层中引入知识引导偏见来完成这一点。此外，KSAT提供了控制学习和知识利用之间权衡的机制。",
    "tldr": "KSAT使用外部知识源引入知识引导偏见来整合多个领域特定上下文的自我关注结构。",
    "en_tdlr": "KSAT integrates multiple domain-specific contexts by introducing knowledge-guided biases in dedicated self-attention layers for each knowledge source through the use of external knowledge sources."
}