{
    "title": "RSC: Accelerating Graph Neural Networks Training via Randomized Sparse Computations. (arXiv:2210.10737v2 [cs.LG] UPDATED)",
    "abstract": "The training of graph neural networks (GNNs) is extremely time consuming because sparse graph-based operations are hard to be accelerated by hardware. Prior art explores trading off the computational precision to reduce the time complexity via sampling-based approximation. Based on the idea, previous works successfully accelerate the dense matrix based operations (e.g., convolution and linear) with negligible accuracy drop. However, unlike dense matrices, sparse matrices are stored in the irregular data format such that each row/column may have different number of non-zero entries. Thus, compared to the dense counterpart, approximating sparse operations has two unique challenges (1) we cannot directly control the efficiency of approximated sparse operation since the computation is only executed on non-zero entries; (2) sub-sampling sparse matrices is much more inefficient due to the irregular data format. To address the issues, our key idea is to control the accuracy-efficiency trade o",
    "link": "http://arxiv.org/abs/2210.10737",
    "context": "Title: RSC: Accelerating Graph Neural Networks Training via Randomized Sparse Computations. (arXiv:2210.10737v2 [cs.LG] UPDATED)\nAbstract: The training of graph neural networks (GNNs) is extremely time consuming because sparse graph-based operations are hard to be accelerated by hardware. Prior art explores trading off the computational precision to reduce the time complexity via sampling-based approximation. Based on the idea, previous works successfully accelerate the dense matrix based operations (e.g., convolution and linear) with negligible accuracy drop. However, unlike dense matrices, sparse matrices are stored in the irregular data format such that each row/column may have different number of non-zero entries. Thus, compared to the dense counterpart, approximating sparse operations has two unique challenges (1) we cannot directly control the efficiency of approximated sparse operation since the computation is only executed on non-zero entries; (2) sub-sampling sparse matrices is much more inefficient due to the irregular data format. To address the issues, our key idea is to control the accuracy-efficiency trade o",
    "path": "papers/22/10/2210.10737.json",
    "total_tokens": 908,
    "translated_title": "RSC: 通过随机稀疏计算加速图神经网络训练",
    "translated_abstract": "图神经网络（GNN）的训练非常耗时，因为硬件难以加速稀疏图操作。先前的研究通过基于采样的逼近来降低时间复杂度，但牺牲了计算精度。在这个思路的基础上，以往的工作成功地加速了基于稠密矩阵的操作（如卷积和线性操作），准确度下降可以忽略不计。然而，与稠密矩阵不同，稀疏矩阵以不规则的数据格式存储，每行/列可能有不同数量的非零元素。因此，与稠密矩阵相比，逼近稀疏操作存在两个独特的挑战：（1）我们无法直接控制逼近稀疏操作的效率，因为计算仅在非零元素上执行；（2）基于子采样的稀疏矩阵处理效率更低，因为存在不规则数据格式。为了解决这些问题，我们的关键思想是控制准确度和效率的权衡。",
    "tldr": "通过随机稀疏计算，本研究提出了一种加速图神经网络训练的方法，解决了稀疏图操作难以加速和不规则数据格式导致的效率问题。",
    "en_tdlr": "This paper introduces a method to accelerate the training of graph neural networks by leveraging randomized sparse computations, addressing the challenges of sparse graph operations acceleration and irregular data format efficiency."
}