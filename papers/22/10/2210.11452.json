{
    "title": "Global Convergence of SGD On Two Layer Neural Nets. (arXiv:2210.11452v2 [cs.LG] UPDATED)",
    "abstract": "In this note we demonstrate provable convergence of SGD to the global minima of appropriately regularized $\\ell_2-$empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates, if they are using adequately smooth and bounded activations like sigmoid and tanh. We build on the results in [1] and leverage a constant amount of Frobenius norm regularization on the weights, along with sampling of the initial weights from an appropriate distribution. We also give a continuous time SGD convergence result that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence loss functions on constant sized neural nets which are \"Villani Functions\". [1] Bin Shi, Weijie J. Su, and Michael I. Jordan. On learning rates and schr\\\"odinger operators, 2020. arXiv:2004.06977",
    "link": "http://arxiv.org/abs/2210.11452",
    "context": "Title: Global Convergence of SGD On Two Layer Neural Nets. (arXiv:2210.11452v2 [cs.LG] UPDATED)\nAbstract: In this note we demonstrate provable convergence of SGD to the global minima of appropriately regularized $\\ell_2-$empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates, if they are using adequately smooth and bounded activations like sigmoid and tanh. We build on the results in [1] and leverage a constant amount of Frobenius norm regularization on the weights, along with sampling of the initial weights from an appropriate distribution. We also give a continuous time SGD convergence result that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence loss functions on constant sized neural nets which are \"Villani Functions\". [1] Bin Shi, Weijie J. Su, and Michael I. Jordan. On learning rates and schr\\\"odinger operators, 2020. arXiv:2004.06977",
    "path": "papers/22/10/2210.11452.json",
    "total_tokens": 942,
    "translated_title": "两层神经网络上SGD的全局收敛性证明",
    "translated_abstract": "在这篇论文中，我们证明了当深度为2的网络采用足够平滑且有边界的激活函数（比如sigmoid和tanh）时，SGD可以证明性地收敛到适当正则化的$\\ell_2-$经验风险的全局最小值--对于任意数据和任意数量的门。我们在[1]的研究成果上进行了扩展，并在权重上添加了恒定量的Frobenius范数正则化，同时选取了恰当的分布对初始权重进行采样。我们还给出了一个连续时间的SGD收敛结果，同样适用于如SoftPlus这样的平滑无边界的激活函数。我们的关键想法是展示了存在于固定大小的神经网络上的损失函数，它们是“Villani函数”[1] Bin Shi, Weijie J. Su, and Michael I. Jordan. On learning rates and schr\\\"odinger operators, 2020. arXiv:2004.06977",
    "tldr": "该论文证明了当深度为2的神经网络采用足够平滑凸的激活函数时，SGD可以收敛到全局最小值，证明过程中引入Frobenius范数正则化与恰当分布的参数初始化，同时拓展了连续时间的收敛结果。",
    "en_tdlr": "This paper proves the provable convergence of SGD to the global minima of appropriately regularized $\\ell_2$-empirical risk of depth 2 nets for arbitrary data and with any number of gates, provided that the activations are sufficiently smooth and bounded, such as sigmoid and tanh. The paper also introduces the continuous time SGD convergence result, and the key idea is to show the existence loss functions on constant-sized neural nets that are \"Villani Functions\"."
}