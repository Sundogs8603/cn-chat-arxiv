{
    "title": "Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints. (arXiv:2210.09440v2 [cs.CL] UPDATED)",
    "abstract": "Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP. In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not.  Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining. The",
    "link": "http://arxiv.org/abs/2210.09440",
    "context": "Title: Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints. (arXiv:2210.09440v2 [cs.CL] UPDATED)\nAbstract: Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP. In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not.  Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining. The",
    "path": "papers/22/10/2210.09440.json",
    "total_tokens": 925,
    "translated_title": "利用瓶颈适配器在低资源限制下识别临床记录中的癌症",
    "translated_abstract": "处理存储在临床健康记录中的信息是一项具有挑战性的任务，是生物医学自然语言处理领域的一个活跃研究领域。本文在一个含有临床记录的数据集上评估了一系列的机器学习技术，从简单的递归神经网络到专业的转换器，例如 BioBERT，并附有指示样本是否与癌症相关的一组注释。此外，我们特别采用了来自自然语言处理领域的高效微调方法，即瓶颈适配器和提示调整，以适应我们的专业任务。我们的评估表明，预训练于自然语言的冻结的基于BERT的模型，并使用瓶颈适配器微调，优于所有其他策略，包括全面微调专用的BioBERT模型。根据我们的发现，我们建议在低资源情况下使用瓶颈适配器，特别是在有限的标记数据或处理能力时，可能是生物医学文本挖掘的可行策略。",
    "tldr": "本文评估了一系列机器学习技术来识别临床记录中的癌症，采用瓶颈适配器和提示微调的方法优于其它方法，可在低资源情况下使用。",
    "en_tdlr": "This paper evaluates a range of machine learning techniques for identifying cancer in clinical notes, with fine-tuning of a pre-trained BERT model using bottleneck adapters and prompt tuning found to outperform other strategies, suggesting viability in low-resource settings."
}