{
    "title": "MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Pose. (arXiv:2210.07181v2 [cs.CV] UPDATED)",
    "abstract": "We propose a generalizable neural radiance fields - MonoNeRF, that can be trained on large-scale monocular videos of moving in static scenes without any ground-truth annotations of depth and camera poses. MonoNeRF follows an Autoencoder-based architecture, where the encoder estimates the monocular depth and the camera pose, and the decoder constructs a Multiplane NeRF representation based on the depth encoder feature, and renders the input frames with the estimated camera. The learning is supervised by the reconstruction error. Once the model is learned, it can be applied to multiple applications including depth estimation, camera pose estimation, and single-image novel view synthesis. More qualitative results are available at: https://oasisyang.github.io/mononerf .",
    "link": "http://arxiv.org/abs/2210.07181",
    "context": "Title: MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Pose. (arXiv:2210.07181v2 [cs.CV] UPDATED)\nAbstract: We propose a generalizable neural radiance fields - MonoNeRF, that can be trained on large-scale monocular videos of moving in static scenes without any ground-truth annotations of depth and camera poses. MonoNeRF follows an Autoencoder-based architecture, where the encoder estimates the monocular depth and the camera pose, and the decoder constructs a Multiplane NeRF representation based on the depth encoder feature, and renders the input frames with the estimated camera. The learning is supervised by the reconstruction error. Once the model is learned, it can be applied to multiple applications including depth estimation, camera pose estimation, and single-image novel view synthesis. More qualitative results are available at: https://oasisyang.github.io/mononerf .",
    "path": "papers/22/10/2210.07181.json",
    "total_tokens": 803,
    "translated_title": "MonoNeRF：在无相机姿态下，从单目视频中学习通用的NeRF模型",
    "translated_abstract": "我们提出了一种名为MonoNeRF的通用神经辐射场模型，它可以在大规模的单目视频中进行训练，用来生成静态场景的移动图像，无需深度和相机姿态的标注。MonoNeRF的基础是自编码器，其中编码器估计单目深度和相机姿态，解码器根据深度编码器特征构建多平面NeRF表示，并使用估计的相机渲染输入帧。学习由重构误差监督。学得模型后，可用于多个应用包括深度估计、相机姿态估计和单张图像的新视角合成。更多定性结果可在以下网址查看：https://oasisyang.github.io/mononerf。",
    "tldr": "MonoNeRF是一种无需深度和相机姿态标注的通用神经辐射场模型，可从单目视频中学得，可以用于多个应用，包括深度估计、相机姿态估计和单张图像的新视角合成。",
    "en_tdlr": "MonoNeRF is a versatile neural radiance fields model that can be learned from monocular videos without the need for depth and camera pose annotations. It can be applied to various tasks, including depth estimation, camera pose estimation, and single-image novel view synthesis."
}