{
    "title": "Multimodal Transformer Distillation for Audio-Visual Synchronization",
    "abstract": "arXiv:2210.15563v3 Announce Type: replace-cross  Abstract: Audio-visual synchronization aims to determine whether the mouth movements and speech in the video are synchronized. VocaLiST reaches state-of-the-art performance by incorporating multimodal Transformers to model audio-visual interact information. However, it requires high computing resources, making it impractical for real-world applications. This paper proposed an MTDVocaLiST model, which is trained by our proposed multimodal Transformer distillation (MTD) loss. MTD loss enables MTDVocaLiST model to deeply mimic the cross-attention distribution and value-relation in the Transformer of VocaLiST. Additionally, we harness uncertainty weighting to fully exploit the interaction information across all layers. Our proposed method is effective in two aspects: From the distillation method perspective, MTD loss outperforms other strong distillation baselines. From the distilled model's performance perspective: 1) MTDVocaLiST outperform",
    "link": "https://arxiv.org/abs/2210.15563",
    "context": "Title: Multimodal Transformer Distillation for Audio-Visual Synchronization\nAbstract: arXiv:2210.15563v3 Announce Type: replace-cross  Abstract: Audio-visual synchronization aims to determine whether the mouth movements and speech in the video are synchronized. VocaLiST reaches state-of-the-art performance by incorporating multimodal Transformers to model audio-visual interact information. However, it requires high computing resources, making it impractical for real-world applications. This paper proposed an MTDVocaLiST model, which is trained by our proposed multimodal Transformer distillation (MTD) loss. MTD loss enables MTDVocaLiST model to deeply mimic the cross-attention distribution and value-relation in the Transformer of VocaLiST. Additionally, we harness uncertainty weighting to fully exploit the interaction information across all layers. Our proposed method is effective in two aspects: From the distillation method perspective, MTD loss outperforms other strong distillation baselines. From the distilled model's performance perspective: 1) MTDVocaLiST outperform",
    "path": "papers/22/10/2210.15563.json",
    "total_tokens": 889,
    "translated_title": "多模态Transformer蒸馏用于音频-视觉同步",
    "translated_abstract": "音频-视觉同步旨在确定视频中的口型运动和语音是否同步。VocaLiST通过融合多模态Transformer来模拟音频-视觉交互信息，达到了最先进的性能。然而，它需要高计算资源，使其在现实世界应用中不切实际。本文提出了一种MTDVocaLiST模型，该模型通过我们提出的多模态Transformer蒸馏（MTD）损失进行训练。MTD损失使MTDVocaLiST模型能够深度模仿VocaLiST中Transformer的交叉注意力分布和值关系。此外，我们利用不确定性加权来充分利用所有层中的交互信息。我们提出的方法在两方面非常有效：从蒸馏方法的角度来看，MTD损失优于其他强蒸馏基线。从经过蒸馏模型的性能角度看：1) MTDVocaLiST 胜过",
    "tldr": "该论文提出了一种通过多模态Transformer蒸馏技术来进行训练的MTDVocaLiST模型，能够深度模仿VocaLiST中Transformer的交叉注意力分布和值关系，从而在音频-视觉同步任务中取得了很好的效果。",
    "en_tdlr": "This paper introduced an MTDVocaLiST model trained using multimodal Transformer distillation, which can deeply mimic the cross-attention distribution and value-relation in the Transformer of VocaLiST, achieving good performance in audio-visual synchronization tasks."
}