{
    "title": "Blank Collapse: Compressing CTC emission for the faster decoding. (arXiv:2210.17017v2 [cs.CL] UPDATED)",
    "abstract": "Connectionist Temporal Classification (CTC) model is a very efficient method for modeling sequences, especially for speech data. In order to use CTC model as an Automatic Speech Recognition (ASR) task, the beam search decoding with an external language model like n-gram LM is necessary to obtain reasonable results. In this paper we analyze the blank label in CTC beam search deeply and propose a very simple method to reduce the amount of calculation resulting in faster beam search decoding speed. With this method, we can get up to 78% faster decoding speed than ordinary beam search decoding with a very small loss of accuracy in LibriSpeech datasets. We prove this method is effective not only practically by experiments but also theoretically by mathematical reasoning. We also observe that this reduction is more obvious if the accuracy of the model is higher.",
    "link": "http://arxiv.org/abs/2210.17017",
    "context": "Title: Blank Collapse: Compressing CTC emission for the faster decoding. (arXiv:2210.17017v2 [cs.CL] UPDATED)\nAbstract: Connectionist Temporal Classification (CTC) model is a very efficient method for modeling sequences, especially for speech data. In order to use CTC model as an Automatic Speech Recognition (ASR) task, the beam search decoding with an external language model like n-gram LM is necessary to obtain reasonable results. In this paper we analyze the blank label in CTC beam search deeply and propose a very simple method to reduce the amount of calculation resulting in faster beam search decoding speed. With this method, we can get up to 78% faster decoding speed than ordinary beam search decoding with a very small loss of accuracy in LibriSpeech datasets. We prove this method is effective not only practically by experiments but also theoretically by mathematical reasoning. We also observe that this reduction is more obvious if the accuracy of the model is higher.",
    "path": "papers/22/10/2210.17017.json",
    "total_tokens": 890,
    "translated_title": "省略折叠：压缩CTC发射以实现更快的解码",
    "translated_abstract": "连接时序分类（CTC）模型是一种非常高效的序列建模方法，特别适用于语音数据。为了将CTC模型用于自动语音识别（ASR）任务，需要使用外部语言模型（如n-gram LM）进行束搜索解码，以获得合理的结果。本文深入分析了CTC束搜索中的空标签，并提出了一种非常简单的方法，以减少计算量，从而实现更快的束搜索解码速度。通过这种方法，在LibriSpeech数据集上可以获得比普通束搜索解码快78%的解码速度，同时损失很小的准确度。我们通过实验证明了这种方法不仅在实践中有效，而且在数学上也是有理论依据的。我们还观察到，如果模型的准确度更高，则这种减少效果更加明显。",
    "tldr": "本文提出了一种省略折叠的方法，通过压缩CTC发射来加快解码速度。实验结果表明，在LibriSpeech数据集上，这种方法可以达到比普通解码快78%的速度，并且准确度的损失非常小。这种方法不仅在实践中有效，而且在理论上有数学依据。",
    "en_tdlr": "This paper proposes a blank collapse method to speed up decoding by compressing CTC emissions. Experimental results show that this method achieves up to 78% faster decoding speed compared to ordinary decoding, with minimal loss in accuracy on the LibriSpeech dataset. This method is not only effective in practice, but also supported by mathematical reasoning."
}