{
    "title": "VIMA: General Robot Manipulation with Multimodal Prompts. (arXiv:2210.03094v2 [cs.RO] UPDATED)",
    "abstract": "Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieve",
    "link": "http://arxiv.org/abs/2210.03094",
    "context": "Title: VIMA: General Robot Manipulation with Multimodal Prompts. (arXiv:2210.03094v2 [cs.RO] UPDATED)\nAbstract: Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieve",
    "path": "papers/22/10/2210.03094.json",
    "total_tokens": 1073,
    "translated_title": "VIMA：多模态提示实现通用机器人操作",
    "translated_abstract": "基于提示的学习模式已经成为自然语言处理中的成功范例，在此模式下，单个通用语言模型可以按照输入提示执行任何任务。然而，在机器人工程中，任务规范的形式多种多样，例如，模仿单次演示、遵循语言指令和达到视觉目标等。这些任务通常被认为是不同的任务，并由专门的模型来处理。我们展示了一种广泛的机器人操作任务可以通过多模态提示来表达，交错文本和视觉令牌。因此，我们开发了一个新的仿真基准，其中包含数千个程序生成的桌面任务，具有多模态提示，60万个专家轨迹以进行模仿学习，并采用四级评估协议进行系统化的广义化。我们设计了一个基于变压器的机器人代理，VIMA，该代理处理这些提示并自回归地输出电机动作。VIMA具有一套配方，实现了各种任务类型的最新结果，包括未见过的模态组合，甚至可以零样本泛化到新的对象类别。总体而言，我们的工作提出了一种有前途的方法，采用统一的基于提示的框架实现通用机器人操作。",
    "tldr": "本研究提出了一种新的机器人操作方式——多模态提示实现通用机器人操作。通过设计一个基于变压器的机器人代理VIMA，可以处理提示并自回归地输出电机动作，实现了各种任务类型的最新结果，并能够零样本泛化到新的对象类别，这对实现通用机器人操作具有前景。",
    "en_tdlr": "This study proposes a new way of robot manipulation - multimodal prompts for general robot manipulation, and develops a transformer-based robot agent, VIMA, that can process prompts and output motor actions autoregressively. The results show that VIMA achieves state-of-the-art in various task types and can even zero-shot generalize to new object categories, providing a promising approach towards general robot manipulation with a unified prompt-based framework."
}