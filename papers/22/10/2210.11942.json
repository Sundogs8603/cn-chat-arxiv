{
    "title": "Oracles & Followers: Stackelberg Equilibria in Deep Multi-Agent Reinforcement Learning. (arXiv:2210.11942v4 [cs.GT] UPDATED)",
    "abstract": "Stackelberg equilibria arise naturally in a range of popular learning problems, such as in security games or indirect mechanism design, and have received increasing attention in the reinforcement learning literature. We present a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem, allowing a wide range of algorithmic design choices. We discuss how previous approaches can be seen as specific instantiations of this framework. As a key insight, we note that the design space allows for approaches not previously seen in the literature, for instance by leveraging multitask and meta-RL techniques for follower convergence. We propose one such approach using contextual policies, and evaluate it experimentally on both standard and novel benchmark domains, showing greatly improved sample efficiency compared to previous approaches. Finally, we explore the effect of adopting algorithm designs outside the borders of our framework.",
    "link": "http://arxiv.org/abs/2210.11942",
    "context": "Title: Oracles & Followers: Stackelberg Equilibria in Deep Multi-Agent Reinforcement Learning. (arXiv:2210.11942v4 [cs.GT] UPDATED)\nAbstract: Stackelberg equilibria arise naturally in a range of popular learning problems, such as in security games or indirect mechanism design, and have received increasing attention in the reinforcement learning literature. We present a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem, allowing a wide range of algorithmic design choices. We discuss how previous approaches can be seen as specific instantiations of this framework. As a key insight, we note that the design space allows for approaches not previously seen in the literature, for instance by leveraging multitask and meta-RL techniques for follower convergence. We propose one such approach using contextual policies, and evaluate it experimentally on both standard and novel benchmark domains, showing greatly improved sample efficiency compared to previous approaches. Finally, we explore the effect of adopting algorithm designs outside the borders of our framework.",
    "path": "papers/22/10/2210.11942.json",
    "total_tokens": 892,
    "translated_title": "领袖与追随者：深度多智能体强化学习中的Stackelberg均衡",
    "translated_abstract": "Stackelberg均衡在多个学习问题中自然出现，例如在安全博弈或间接机制设计中，并在强化学习文献中受到越来越多的关注。我们提出了一个实现Stackelberg均衡搜索的通用框架，将其作为多智能体RL问题，允许进行各种算法设计选择，并将之前的方法视为此框架的特定实例。另外，我们注意到设计空间允许使用以前在文献中没有见过的方法，例如利用多任务和元-RL技术实现追随者的收敛。我们提出了一种使用上下文策略的方法，并在标准和新颖的基准领域上进行实验评估，证明了相比以前的方法，样本效率大大提高。最后，我们还探索了采用框架边界外的算法设计的效果。",
    "tldr": "本文提出了一个通用框架，将Stackelberg均衡搜索作为多智能体RL问题进行实现，提出了一种利用上下文策略的方法，并在标准和新颖的基准领域上进行实验评估，证明相比之前的方法，样本效率得到了极大提高。",
    "en_tdlr": "This paper proposes a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem, and presents a new approach using contextual policies. Experimental evaluations on standard and novel benchmark domains show greatly improved sample efficiency compared to previous approaches."
}