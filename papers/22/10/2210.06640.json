{
    "title": "Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities. (arXiv:2210.06640v2 [cs.LG] UPDATED)",
    "abstract": "Although deep learning has made great progress in recent years, the exploding economic and environmental costs of training neural networks are becoming unsustainable. To address this problem, there has been a great deal of research on *algorithmically-efficient deep learning*, which seeks to reduce training costs not at the hardware or implementation level, but through changes in the semantics of the training program. In this paper, we present a structured and comprehensive overview of the research in this field. First, we formalize the *algorithmic speedup* problem, then we use fundamental building blocks of algorithmically efficient training to develop a taxonomy. Our taxonomy highlights commonalities of seemingly disparate methods and reveals current research gaps. Next, we present evaluation best practices to enable comprehensive, fair, and reliable comparisons of speedup techniques. To further aid research and applications, we discuss common bottlenecks in the training pipeline (i",
    "link": "http://arxiv.org/abs/2210.06640",
    "context": "Title: Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities. (arXiv:2210.06640v2 [cs.LG] UPDATED)\nAbstract: Although deep learning has made great progress in recent years, the exploding economic and environmental costs of training neural networks are becoming unsustainable. To address this problem, there has been a great deal of research on *algorithmically-efficient deep learning*, which seeks to reduce training costs not at the hardware or implementation level, but through changes in the semantics of the training program. In this paper, we present a structured and comprehensive overview of the research in this field. First, we formalize the *algorithmic speedup* problem, then we use fundamental building blocks of algorithmically efficient training to develop a taxonomy. Our taxonomy highlights commonalities of seemingly disparate methods and reveals current research gaps. Next, we present evaluation best practices to enable comprehensive, fair, and reliable comparisons of speedup techniques. To further aid research and applications, we discuss common bottlenecks in the training pipeline (i",
    "path": "papers/22/10/2210.06640.json",
    "total_tokens": 988,
    "translated_title": "计算效率高的深度学习：算法趋势和机遇",
    "translated_abstract": "虽然深度学习在近年来取得了巨大的进展，但训练神经网络所产生的经济和环境成本已经变得不可持续。为了解决这个问题，已经有大量的研究致力于通过对训练程序语义的变化来降低训练成本，即算法效率高的深度学习。本文对这一领域的研究进行了系统全面的概述。首先，我们对\"算法加速\"问题进行了正式化，然后利用算法效率高训练的基本构建块来制定了分类法。我们的分类法突显了看似不同方法的共同点，并揭示了当前研究的空白。接下来，我们提出评估最佳实践以实现全面、公正和可靠的速度提升技术比较。为了进一步帮助研究和应用，我们讨论了训练流程中常见的瓶颈。",
    "tldr": "本文概述了算法效率高深度学习的研究。首先，提出了算法加速问题并制定了分类法。分类法突显了看似不同方法的共同点，并揭示了当前研究的空白。其次，提出了评估最佳实践以实现全面、公正和可靠的速度提升技术比较。最后，讨论了训练流程中常见的瓶颈。",
    "en_tdlr": "This paper provides a systematic and comprehensive overview of algorithmically-efficient deep learning, which aims to reduce training costs through changes in the training program semantics. The paper formalizes the algorithmic speedup problem and develops a taxonomy highlighting commonalities among seemingly disparate methods. Evaluation best practices are presented to enable comprehensive and reliable comparisons of speedup techniques. Additionally, common bottlenecks in the training pipeline are discussed."
}