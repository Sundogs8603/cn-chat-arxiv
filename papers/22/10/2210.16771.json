{
    "title": "Parameter-Efficient Tuning Makes a Good Classification Head. (arXiv:2210.16771v2 [cs.CL] UPDATED)",
    "abstract": "In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the improvement, we naturally expect a good pretrained classification head can also benefit the training. However, the final-layer output of the backbone, i.e. the input of the classification head, will change greatly during finetuning, making the usual head-only pretraining (LP-FT) ineffective. In this paper, we find that parameter-efficient tuning makes a good classification head, with which we can simply replace the randomly initialized heads for a stable performance gain. Our experiments demonstrate that the classification head jointly pretrained with parameter-efficient tuning consistently improves the performance on 9 tasks in GLUE and SuperGLUE.",
    "link": "http://arxiv.org/abs/2210.16771",
    "context": "Title: Parameter-Efficient Tuning Makes a Good Classification Head. (arXiv:2210.16771v2 [cs.CL] UPDATED)\nAbstract: In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the improvement, we naturally expect a good pretrained classification head can also benefit the training. However, the final-layer output of the backbone, i.e. the input of the classification head, will change greatly during finetuning, making the usual head-only pretraining (LP-FT) ineffective. In this paper, we find that parameter-efficient tuning makes a good classification head, with which we can simply replace the randomly initialized heads for a stable performance gain. Our experiments demonstrate that the classification head jointly pretrained with parameter-efficient tuning consistently improves the performance on 9 tasks in GLUE and SuperGLUE.",
    "path": "papers/22/10/2210.16771.json",
    "total_tokens": 822,
    "translated_title": "高效参数调整可以使分类头表现出色",
    "translated_abstract": "近年来，预训练模型革命性地改变了自然语言理解范式。我们在预训练的主干模型（如BERT）之后附加一个随机初始化的分类头，并微调整个模型。由于预训练主干模型对性能的贡献很大，因此我们自然期望良好的预训练分类头也能受益于训练。然而，主干模型最后一层的输出（即分类头的输入）在微调期间会有很大变化，导致通常的头部单独预训练（LP-FT）失效。在本文中，我们发现高效参数调整可以使分类头表现出色，通过简单地替换随机初始化的头部，我们可以获得稳定的性能提升。我们的实验表明，在 GLUE 和 SuperGLUE 的 9 项任务中，使用参数高效调整联合预训练的分类头可以持续提高性能。",
    "tldr": "提出了一种高效参数调整的分类头训练方法，取代了随机初始化的分类头使模型性能稳定提升。",
    "en_tdlr": "The paper proposes a parameter-efficient tuning method for classification head training, which replaces the randomly initialized head and achieves stable performance improvement by using a joint pretrained classification head with parameter-efficient tuning on 9 tasks in GLUE and SuperGLUE."
}