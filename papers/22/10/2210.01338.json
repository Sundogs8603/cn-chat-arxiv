{
    "title": "Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning. (arXiv:2210.01338v2 [cs.CV] UPDATED)",
    "abstract": "Humans tend to decompose a sentence into different parts like \\textsc{sth do sth at someplace} and then fill each part with certain content. Inspired by this, we follow the \\textit{principle of modular design} to propose a novel image captioner: learning to Collocate Visual-Linguistic Neural Modules (CVLNM). Unlike the \\re{widely used} neural module networks in VQA, where the language (\\ie, question) is fully observable, \\re{the task of collocating visual-linguistic modules is more challenging.} This is because the language is only partially observable, for which we need to dynamically collocate the modules during the process of image captioning. To sum up, we make the following technical contributions to design and train our CVLNM: 1) \\textit{distinguishable module design} -- \\re{four modules in the encoder} including one linguistic module for function words and three visual modules for different content words (\\ie, noun, adjective, and verb) and another linguistic one in the decoder ",
    "link": "http://arxiv.org/abs/2210.01338",
    "context": "Title: Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning. (arXiv:2210.01338v2 [cs.CV] UPDATED)\nAbstract: Humans tend to decompose a sentence into different parts like \\textsc{sth do sth at someplace} and then fill each part with certain content. Inspired by this, we follow the \\textit{principle of modular design} to propose a novel image captioner: learning to Collocate Visual-Linguistic Neural Modules (CVLNM). Unlike the \\re{widely used} neural module networks in VQA, where the language (\\ie, question) is fully observable, \\re{the task of collocating visual-linguistic modules is more challenging.} This is because the language is only partially observable, for which we need to dynamically collocate the modules during the process of image captioning. To sum up, we make the following technical contributions to design and train our CVLNM: 1) \\textit{distinguishable module design} -- \\re{four modules in the encoder} including one linguistic module for function words and three visual modules for different content words (\\ie, noun, adjective, and verb) and another linguistic one in the decoder ",
    "path": "papers/22/10/2210.01338.json",
    "total_tokens": 1186,
    "translated_title": "学习组合视觉语言神经模块进行图像字幕生成",
    "translated_abstract": "人们倾向于将句子分解为不同的部分，如“sth在someplace做sth”，然后为每个部分填入某些内容。受此启发，我们遵循“模块化设计原则”提出了一种新的图像字幕生成器：学习组合视觉语言神经模块（CVLNM）。与VQA中广泛使用的神经模块网络不同，其中语言（即问题）是完全可见的，组合视觉语言模块的任务更具挑战性，因为语言仅部分可见，需要在图像字幕生成过程中动态组合模块。因此，我们提出了以下技术贡献：1）可区分的模块设计——编码器中包括一个函数词语言模块和三个不同内容词的视觉模块（即名词、形容词和动词），解码器中另外一个语言模块用于生成语言；2）有效的模块组合——基于强化学习的方法，为每个句子段动态组合适当的视觉和语言模块；3）多目标学习——联合训练目标，平衡字幕生成质量和模块差异性。在MSCOCO和Flickr30k上的实验结果证明了我们的方法的有效性，优于两个数据集上的现有方法。",
    "tldr": "本文提出了一种图像字幕生成器，学习组合视觉和语言神经模块进行图像字幕生成。该方法使用了可区分的模块设计，并采用有效的模块组合和多目标学习方式，实现了在MSCOCO和Flickr30k上的性能优于现有方法的结果。",
    "en_tdlr": "This paper proposes a novel image captioning approach, called Learning to Collocate Visual-Linguistic Neural Modules (CVLNM), which dynamically collocates appropriate visual and linguistic modules for partially observable language during caption generation. The approach adopts distinguishable module design, efficient module collocation, and multi-objective learning for joint training, achieving superior performance on both MSCOCO and Flickr30k datasets compared to state-of-the-art methods."
}