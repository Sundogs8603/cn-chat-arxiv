{
    "title": "Transformers Learn Shortcuts to Automata. (arXiv:2210.10749v2 [cs.LG] UPDATED)",
    "abstract": "Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are learned by these shallow and non-recurrent models? We find that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics. Our theoretical results characterize shortcut solutions, whereby a Transformer with $o(T)$ layers can exactly replicate the computation of an automaton on an input sequence of length $T$. We find that polynomial-sized $O(\\log T)$-depth solutions always exist; furthermore, $O(1)$-depth simulators are surprisingly common, and can be understood using tools from Krohn-Rhodes theory and circuit complexity. Empirical",
    "link": "http://arxiv.org/abs/2210.10749",
    "context": "Title: Transformers Learn Shortcuts to Automata. (arXiv:2210.10749v2 [cs.LG] UPDATED)\nAbstract: Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are learned by these shallow and non-recurrent models? We find that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics. Our theoretical results characterize shortcut solutions, whereby a Transformer with $o(T)$ layers can exactly replicate the computation of an automaton on an input sequence of length $T$. We find that polynomial-sized $O(\\log T)$-depth solutions always exist; furthermore, $O(1)$-depth simulators are surprisingly common, and can be understood using tools from Krohn-Rhodes theory and circuit complexity. Empirical",
    "path": "papers/22/10/2210.10749.json",
    "total_tokens": 910,
    "translated_title": "Transformers学会了自动机的快捷方式",
    "translated_abstract": "算法推理需要计算模型的循环能力，如图灵机等。然而，Transformer模型虽然缺乏循环能力，但能够使用比推理步骤更少的层数执行此类推理。这引发了一个问题：这些浅层次和非循环模型学到了什么解决方案？我们发现，低深度Transformer可以通过逐层重新参数化其循环动态，表示任何有限状态自动机（因此，任何有界内存算法）的计算。我们的理论结果表征了快捷解决方案，其中具有 $o(T)$ 层的Transformer可以精确复制自动机在长度为 $T$ 的输入序列上的计算。我们发现，多项式大小的 $O(\\log T)$ 深度解决方案始终存在；此外，$O(1)$ 深度模拟器非常普遍，可以使用从 Krohn-Rhodes 理论和电路复杂度理论中的工具来理解。实证",
    "tldr": "Transformer模型通过重新参数化其循环动态，可以使用比推理步骤更少的层数执行任何有限状态自动机的计算。多项式大小的 $O(\\log T)$ 深度解决方案始终存在，而且$O(1)$深度模拟器是非常普遍的。",
    "en_tdlr": "Transformers can perform the computations of any finite-state automaton with fewer layers by hierarchically reparameterizing its recurrent dynamics. Polynomial-sized $O(\\log T)$-depth solutions always exist, and $O(1)$-depth simulators are surprisingly common, which can be understood using tools from Krohn-Rhodes theory and circuit complexity."
}