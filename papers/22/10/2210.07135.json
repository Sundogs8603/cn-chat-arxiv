{
    "title": "You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models. (arXiv:2210.07135v2 [cs.CL] UPDATED)",
    "abstract": "Multilingual models have been widely used for cross-lingual transfer to low-resource languages. However, the performance on these languages is hindered by their underrepresentation in the pretraining data. To alleviate this problem, we propose a novel multilingual training technique based on teacher-student knowledge distillation. In this setting, we utilize monolingual teacher models optimized for their language. We use those teachers along with balanced (sub-sampled) data to distill the teachers' knowledge into a single multilingual student. Our method outperforms standard training methods in low-resource languages and retrains performance on high-resource languages while using the same amount of data. If applied widely, our approach can increase the representation of low-resource languages in NLP systems.",
    "link": "http://arxiv.org/abs/2210.07135",
    "context": "Title: You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models. (arXiv:2210.07135v2 [cs.CL] UPDATED)\nAbstract: Multilingual models have been widely used for cross-lingual transfer to low-resource languages. However, the performance on these languages is hindered by their underrepresentation in the pretraining data. To alleviate this problem, we propose a novel multilingual training technique based on teacher-student knowledge distillation. In this setting, we utilize monolingual teacher models optimized for their language. We use those teachers along with balanced (sub-sampled) data to distill the teachers' knowledge into a single multilingual student. Our method outperforms standard training methods in low-resource languages and retrains performance on high-resource languages while using the same amount of data. If applied widely, our approach can increase the representation of low-resource languages in NLP systems.",
    "path": "papers/22/10/2210.07135.json",
    "total_tokens": 887,
    "translated_title": "你可以拥有你的数据并且平衡使用：走向平衡高效的多语言模型",
    "translated_abstract": "多语言模型被广泛用于跨语言的低资源转移，但是低资源语言在预训练数据中存在不充分的问题，这会影响性能。为了解决这个问题，我们提出了一种新的基于教师-学生知识蒸馏的多语言训练技术。在这种情况下，我们利用适用于每种语言优化的专业语言教师模型。我们使用这些教师和平衡（子采样）数据来蒸馏教师的知识到单一的多语言学生模型。我们的方法在低资源语言上优于标准的训练方法，并在使用相同数量的数据的情况下提高了高资源语言的性能。如果广泛应用，我们的方法可以提高低资源语言在自然语言处理系统中的表示。",
    "tldr": "使用基于教师-学生知识蒸馏的多语言训练技术，利用适用于每种语言优化的专业语言教师模型和平衡数据，可以在低资源语言的表现方面胜过标准训练方法，并在使用相同数量的数据的情况下提高高资源语言的性能，提高低资源语言在自然语言处理系统中的表示。",
    "en_tdlr": "Using teacher-student knowledge distillation-based multilingual training technique with language-specific optimized teacher models and balanced data can outperform standard training methods on low-resource languages and retrain performance on high-resource languages while using the same amount of data, thus increasing the representation of low-resource languages in NLP systems."
}