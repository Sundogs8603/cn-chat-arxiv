{
    "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning. (arXiv:2210.16611v2 [eess.AS] UPDATED)",
    "abstract": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
    "link": "http://arxiv.org/abs/2210.16611",
    "context": "Title: Application of Knowledge Distillation to Multi-task Speech Representation Learning. (arXiv:2210.16611v2 [eess.AS] UPDATED)\nAbstract: Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
    "path": "papers/22/10/2210.16611.json",
    "total_tokens": 926,
    "translated_title": "知识蒸馏在多任务语音表示学习中的应用",
    "translated_abstract": "wav2vec 2.0和HuBERT等模型已提出以自监督的方式从音频波形中学习语音表示。当它们与下游任务（如关键词检测和说话人验证）相结合时，它们提供了最先进的性能。然而，这些模型使用了大量参数，其中最小版本具有9500万个参数，这对于边缘AI设备的部署构成了挑战。因此，本文研究了将知识蒸馏应用于语音表示学习模型，然后与多个下游语音激活任务进行联合微调。在两个任务的实验中，我们的方法几乎减少了75％的模型大小，同时与完整大小的模型相比，仅有0.1％的精度和0.9％的等误差率下降。此外，我们表明微调语音表示学习模型相对于使用冻结的模型会显著提高性能。",
    "tldr": "本文研究将知识蒸馏应用于语音表示学习模型，联合微调多个下游任务，将模型大小减少75%同时精度和等误差率损失很小，并表明微调语音表示学习模型相较于冻结模型可以显著提高性能。",
    "en_tdlr": "This paper investigates the application of knowledge distillation to speech representation learning models and joint fine-tuning with multiple downstream tasks, resulting in a 75% reduction in model size while suffering only minor accuracy and equal error rate degradation, and shows that fine-tuning the SRL models can significantly improve performance compared to using frozen models."
}