{
    "title": "Convergence Rates of Stochastic Zeroth-order Gradient Descent for \\L ojasiewicz Functions. (arXiv:2210.16997v5 [math.OC] UPDATED)",
    "abstract": "We prove convergence rates of Stochastic Zeroth-order Gradient Descent (SZGD) algorithms for Lojasiewicz functions. The SZGD algorithm iterates as \\begin{align*}  \\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta_t \\widehat{\\nabla} f (\\mathbf{x}_t), \\qquad t = 0,1,2,3,\\cdots , \\end{align*} where $f$ is the objective function that satisfies the \\L ojasiewicz inequality with \\L ojasiewicz exponent $\\theta$, $\\eta_t$ is the step size (learning rate), and $ \\widehat{\\nabla} f (\\mathbf{x}_t) $ is the approximate gradient estimated using zeroth-order information only.  Our results show that $ \\{ f (\\mathbf{x}_t) - f (\\mathbf{x}_\\infty) \\}_{t \\in \\mathbb{N} } $ can converge faster than $ \\{ \\| \\mathbf{x}_t \\mathbf{x}_\\infty \\| \\}_{t \\in \\mathbb{N} }$, regardless of whether the objective $f$ is smooth or nonsmooth.",
    "link": "http://arxiv.org/abs/2210.16997",
    "context": "Title: Convergence Rates of Stochastic Zeroth-order Gradient Descent for \\L ojasiewicz Functions. (arXiv:2210.16997v5 [math.OC] UPDATED)\nAbstract: We prove convergence rates of Stochastic Zeroth-order Gradient Descent (SZGD) algorithms for Lojasiewicz functions. The SZGD algorithm iterates as \\begin{align*}  \\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta_t \\widehat{\\nabla} f (\\mathbf{x}_t), \\qquad t = 0,1,2,3,\\cdots , \\end{align*} where $f$ is the objective function that satisfies the \\L ojasiewicz inequality with \\L ojasiewicz exponent $\\theta$, $\\eta_t$ is the step size (learning rate), and $ \\widehat{\\nabla} f (\\mathbf{x}_t) $ is the approximate gradient estimated using zeroth-order information only.  Our results show that $ \\{ f (\\mathbf{x}_t) - f (\\mathbf{x}_\\infty) \\}_{t \\in \\mathbb{N} } $ can converge faster than $ \\{ \\| \\mathbf{x}_t \\mathbf{x}_\\infty \\| \\}_{t \\in \\mathbb{N} }$, regardless of whether the objective $f$ is smooth or nonsmooth.",
    "path": "papers/22/10/2210.16997.json",
    "total_tokens": 1003,
    "translated_title": "随机零阶梯度下降在L-Lojasiewicz函数上的收敛速率",
    "translated_abstract": "我们证明了随机零阶梯度下降（SZGD）算法在Lojasiewicz函数上的收敛速率。SZGD算法迭代如下：$ \\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta_t \\widehat{\\nabla} f (\\mathbf{x}_t) $，其中$f$是满足Lojasiewicz不等式的目标函数，具有Lojasiewicz指数$\\theta$，$\\eta_t$是步长（学习率），$ \\widehat{\\nabla} f (\\mathbf{x}_t)$是使用零阶信息估计的近似梯度。我们的结果表明，$ \\{ f (\\mathbf{x}_t) - f (\\mathbf{x}_\\infty) \\}_{t \\in \\mathbb{N} }$的收敛速度可以比 $ \\{ \\| \\mathbf{x}_t \\mathbf{x}_\\infty \\| \\}_{t \\in \\mathbb{N} } $更快，无论目标$f$是平滑还是非平滑的。",
    "tldr": "该论文证明了在Lojasiewicz函数上，随机零阶梯度下降算法具有收敛速率，且比 $\\{ \\|\\mathbf{x}_t-\\mathbf{x}_\\infty\\| \\}_{t \\in \\mathbb{N}}$更快，无论$f$是平滑还是非平滑的。",
    "en_tdlr": "This paper proves the convergence rates of Stochastic Zeroth-order Gradient Descent algorithms for Lojasiewicz functions, which is faster than $\\{\\|\\mathbf{x}_t-\\mathbf{x}_\\infty\\|\\}_{t \\in \\mathbb{N}}$, regardless of whether the objective function $f$ is smooth or nonsmooth."
}