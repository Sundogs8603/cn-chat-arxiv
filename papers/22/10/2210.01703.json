{
    "title": "Improving Label-Deficient Keyword Spotting Through Self-Supervised Pretraining. (arXiv:2210.01703v3 [cs.SD] UPDATED)",
    "abstract": "Keyword Spotting (KWS) models are becoming increasingly integrated into various systems, e.g. voice assistants. To achieve satisfactory performance, these models typically rely on a large amount of labelled data, limiting their applications only to situations where such data is available. Self-supervised Learning (SSL) methods can mitigate such a reliance by leveraging readily-available unlabelled data. Most SSL methods for speech have primarily been studied for large models, whereas this is not ideal, as compact KWS models are generally required. This paper explores the effectiveness of SSL on small models for KWS and establishes that SSL can enhance the performance of small KWS models when labelled data is scarce. We pretrain three compact transformer-based KWS models using Data2Vec, and fine-tune them on a label-deficient setup of the Google Speech Commands data set. It is found that Data2Vec pretraining leads to a significant increase in accuracy, with label-deficient scenarios sho",
    "link": "http://arxiv.org/abs/2210.01703",
    "context": "Title: Improving Label-Deficient Keyword Spotting Through Self-Supervised Pretraining. (arXiv:2210.01703v3 [cs.SD] UPDATED)\nAbstract: Keyword Spotting (KWS) models are becoming increasingly integrated into various systems, e.g. voice assistants. To achieve satisfactory performance, these models typically rely on a large amount of labelled data, limiting their applications only to situations where such data is available. Self-supervised Learning (SSL) methods can mitigate such a reliance by leveraging readily-available unlabelled data. Most SSL methods for speech have primarily been studied for large models, whereas this is not ideal, as compact KWS models are generally required. This paper explores the effectiveness of SSL on small models for KWS and establishes that SSL can enhance the performance of small KWS models when labelled data is scarce. We pretrain three compact transformer-based KWS models using Data2Vec, and fine-tune them on a label-deficient setup of the Google Speech Commands data set. It is found that Data2Vec pretraining leads to a significant increase in accuracy, with label-deficient scenarios sho",
    "path": "papers/22/10/2210.01703.json",
    "total_tokens": 896,
    "translated_title": "通过自监督预训练提高无标签关键词检测的表现",
    "translated_abstract": "关键词检测技术越来越多地被融入各种系统，例如语音助手。为了达到令人满意的性能，这些模型通常依赖大量已标记的数据，限制了它们仅可应用于这些数据可用的情况。自监督学习方法可以通过利用易于获得的未标记数据来缓解这种依赖关系。本文探讨了针对小型关键词检测模型的自监督学习方法的有效性，并确定了自监督学习可在标记数据稀缺的情况下提高小型关键词检测模型的性能。我们使用Data2Vec对三个紧凑型基于transformer模型的关键词检测模型进行预训练，并在标签不足的Google语音命令数据集上进行微调。结果发现，Data2Vec预训练可显著提高准确性，特别是在标签缺失的情况下。",
    "tldr": "本文研究了自监督预训练在小型关键词检测模型上的应用，发现它可以提高在标签不足的情况下的性能表现。"
}