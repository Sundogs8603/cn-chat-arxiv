{
    "title": "Private Online Prediction from Experts: Separations and Faster Rates. (arXiv:2210.13537v3 [cs.LG] UPDATED)",
    "abstract": "Online prediction from experts is a fundamental problem in machine learning and several works have studied this problem under privacy constraints. We propose and analyze new algorithms for this problem that improve over the regret bounds of the best existing algorithms for non-adaptive adversaries. For approximate differential privacy, our algorithms achieve regret bounds of $\\tilde{O}(\\sqrt{T \\log d} + \\log d/\\varepsilon)$ for the stochastic setting and $\\tilde{O}(\\sqrt{T \\log d} + T^{1/3} \\log d/\\varepsilon)$ for oblivious adversaries (where $d$ is the number of experts). For pure DP, our algorithms are the first to obtain sub-linear regret for oblivious adversaries in the high-dimensional regime $d \\ge T$. Moreover, we prove new lower bounds for adaptive adversaries. Our results imply that unlike the non-private setting, there is a strong separation between the optimal regret for adaptive and non-adaptive adversaries for this problem. Our lower bounds also show a separation between ",
    "link": "http://arxiv.org/abs/2210.13537",
    "context": "Title: Private Online Prediction from Experts: Separations and Faster Rates. (arXiv:2210.13537v3 [cs.LG] UPDATED)\nAbstract: Online prediction from experts is a fundamental problem in machine learning and several works have studied this problem under privacy constraints. We propose and analyze new algorithms for this problem that improve over the regret bounds of the best existing algorithms for non-adaptive adversaries. For approximate differential privacy, our algorithms achieve regret bounds of $\\tilde{O}(\\sqrt{T \\log d} + \\log d/\\varepsilon)$ for the stochastic setting and $\\tilde{O}(\\sqrt{T \\log d} + T^{1/3} \\log d/\\varepsilon)$ for oblivious adversaries (where $d$ is the number of experts). For pure DP, our algorithms are the first to obtain sub-linear regret for oblivious adversaries in the high-dimensional regime $d \\ge T$. Moreover, we prove new lower bounds for adaptive adversaries. Our results imply that unlike the non-private setting, there is a strong separation between the optimal regret for adaptive and non-adaptive adversaries for this problem. Our lower bounds also show a separation between ",
    "path": "papers/22/10/2210.13537.json",
    "total_tokens": 1079,
    "translated_title": "从专家进行私密在线预测: 分离和更快的速率",
    "translated_abstract": "在线预测从专家中是机器学习中一个基本的问题，并且已经有几篇论文研究了在隐私约束下的这个问题。我们提出并分析了针对这个问题的新算法，改进了非自适应对手的遗憾界限。对于近似差分隐私，我们的算法在随机设置下实现了遗憾界限为$\\tilde{O}(\\sqrt{T \\log d} + \\log d/\\varepsilon)$，对于愚蠢敌手实现了遗憾界限为$\\tilde{O}(\\sqrt{T \\log d} + T^{1/3} \\log d/\\varepsilon)$（其中$d$是专家数量）。对于纯DP，我们的算法是第一个在高维范围$d \\ge T$ 的愚蠢敌手中获得亚线性遗憾的算法。此外，我们证明了自适应对手的新下界。我们的结果表明，与非私密设置不同，对于这个问题，自适应对手和非自适应对手之间存在着较强的遗憾最优性分离。我们的下界也展示了一种在自适应对手和非自适应对手之间的分离。",
    "tldr": "这篇论文提出了新的算法，用于在在线预测从专家中解决隐私约束的问题，并改进了现有算法的遗憾界限。研究结果表明，在纯差分隐私和近似差分隐私设置下，对于愚蠢敌手，在高维范围内的遗憾可以达到亚线性水平，与自适应对手和非自适应对手之间存在着较强的遗憾最优性分离。",
    "en_tdlr": "This paper proposes new algorithms for solving the problem of online prediction from experts under privacy constraints and improves upon the regret bounds of existing algorithms. The research results show that for pure differential privacy and approximate differential privacy, sub-linear regret can be achieved for oblivious adversaries in the high-dimensional regime, and there is a strong separation in regret optimality between adaptive and non-adaptive adversaries."
}