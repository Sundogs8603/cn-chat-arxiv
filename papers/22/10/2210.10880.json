{
    "title": "Learning to Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning. (arXiv:2210.10880v2 [cs.LG] UPDATED)",
    "abstract": "Gradient inversion attack enables recovery of training samples from model gradients in federated learning (FL), and constitutes a serious threat to data privacy. To mitigate this vulnerability, prior work proposed both principled defenses based on differential privacy, as well as heuristic defenses based on gradient compression as countermeasures. These defenses have so far been very effective, in particular those based on gradient compression that allow the model to maintain high accuracy while greatly reducing the effectiveness of attacks. In this work, we argue that such findings underestimate the privacy risk in FL. As a counterexample, we show that existing defenses can be broken by a simple adaptive attack, where a model trained on auxiliary data is able to invert gradients on both vision and language tasks.",
    "link": "http://arxiv.org/abs/2210.10880",
    "context": "Title: Learning to Invert: Simple Adaptive Attacks for Gradient Inversion in Federated Learning. (arXiv:2210.10880v2 [cs.LG] UPDATED)\nAbstract: Gradient inversion attack enables recovery of training samples from model gradients in federated learning (FL), and constitutes a serious threat to data privacy. To mitigate this vulnerability, prior work proposed both principled defenses based on differential privacy, as well as heuristic defenses based on gradient compression as countermeasures. These defenses have so far been very effective, in particular those based on gradient compression that allow the model to maintain high accuracy while greatly reducing the effectiveness of attacks. In this work, we argue that such findings underestimate the privacy risk in FL. As a counterexample, we show that existing defenses can be broken by a simple adaptive attack, where a model trained on auxiliary data is able to invert gradients on both vision and language tasks.",
    "path": "papers/22/10/2210.10880.json",
    "total_tokens": 814,
    "translated_title": "学习反演：联邦学习中用于梯度反演的简单自适应攻击",
    "translated_abstract": "梯度反演攻击可以从联邦学习 (FL) 模型梯度中恢复训练样本，构成对数据隐私的严重威胁。为了减轻此漏洞，先前的工作提出了基于差分隐私的原则性防御和基于梯度压缩的启发式防御来作为对策。迄今为止，这些防御非常有效，特别是基于梯度压缩的防御，该防御允许模型在保持高准确性的同时极大地降低了攻击的效果。在本研究中，我们认为这些发现低估了 FL 的隐私风险。作为反例，我们展示了一种简单自适应攻击，其中一个在辅助数据上训练的模型能够反演视觉和语言任务的梯度。",
    "tldr": "该论文研究了联邦学习中的梯度反演问题，提出了一种简单自适应攻击方法，揭示了现有防御机制的不足之处。",
    "en_tdlr": "This paper studies the gradient inversion problem in federated learning and proposes a simple adaptive attack method, revealing the shortcomings of existing defense mechanisms."
}