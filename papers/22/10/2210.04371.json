{
    "title": "A Detailed Study of Interpretability of Deep Neural Network based Top Taggers. (arXiv:2210.04371v4 [hep-ex] UPDATED)",
    "abstract": "Recent developments in the methods of explainable AI (XAI) allow researchers to explore the inner workings of deep neural networks (DNNs), revealing crucial information about input-output relationships and realizing how data connects with machine learning models. In this paper we explore interpretability of DNN models designed to identify jets coming from top quark decay in high energy proton-proton collisions at the Large Hadron Collider (LHC). We review a subset of existing top tagger models and explore different quantitative methods to identify which features play the most important roles in identifying the top jets. We also investigate how and why feature importance varies across different XAI metrics, how correlations among features impact their explainability, and how latent space representations encode information as well as correlate with physically meaningful quantities. Our studies uncover some major pitfalls of existing XAI methods and illustrate how they can be overcome to ",
    "link": "http://arxiv.org/abs/2210.04371",
    "context": "Title: A Detailed Study of Interpretability of Deep Neural Network based Top Taggers. (arXiv:2210.04371v4 [hep-ex] UPDATED)\nAbstract: Recent developments in the methods of explainable AI (XAI) allow researchers to explore the inner workings of deep neural networks (DNNs), revealing crucial information about input-output relationships and realizing how data connects with machine learning models. In this paper we explore interpretability of DNN models designed to identify jets coming from top quark decay in high energy proton-proton collisions at the Large Hadron Collider (LHC). We review a subset of existing top tagger models and explore different quantitative methods to identify which features play the most important roles in identifying the top jets. We also investigate how and why feature importance varies across different XAI metrics, how correlations among features impact their explainability, and how latent space representations encode information as well as correlate with physically meaningful quantities. Our studies uncover some major pitfalls of existing XAI methods and illustrate how they can be overcome to ",
    "path": "papers/22/10/2210.04371.json",
    "total_tokens": 974,
    "translated_title": "深度神经网络基于Top Tagger的可解释性研究的详细研究",
    "translated_abstract": "最近可解释性人工智能 (XAI) 方法的发展使研究人员能够探索深度神经网络 (DNNs) 的内部工作原理，揭示有关输入-输出关系的关键信息，并了解数据与机器学习模型的连接方式。本文中，我们探讨了设计用于在大型强子对撞机 (LHC) 上识别来自顶夸克衰变的喷注的DNN模型的可解释性。我们回顾了一部分现有的顶夸克标记模型，并探索了不同的定量方法，以确定哪些特征在识别顶夸克的喷注中起着最重要的作用。我们还研究了不同的XAI指标对特征重要性的变化方式及其解释能力的影响，特征之间的相关性如何影响它们的可解释性，以及潜在空间表示如何编码信息以及与物理上有意义的量之间的相关性。我们的研究揭示了现有XAI方法的一些主要问题，并说明了如何克服这些问题。",
    "tldr": "本文详细研究了深度神经网络基于Top Tagger的可解释性。通过回顾现有模型并探索不同定量方法，我们确定了在识别顶夸克的喷注中起关键作用的特征。我们的研究揭示了现有XAI方法的问题，并提出了克服这些问题的方法。"
}