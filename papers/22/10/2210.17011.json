{
    "title": "A picture of the space of typical learnable tasks. (arXiv:2210.17011v3 [cs.LG] UPDATED)",
    "abstract": "We develop information geometric techniques to understand the representations learned by deep networks when they are trained on different tasks using supervised, meta-, semi-supervised and contrastive learning. We shed light on the following phenomena that relate to the structure of the space of tasks: (1) the manifold of probabilistic models trained on different tasks using different representation learning methods is effectively low-dimensional; (2) supervised learning on one task results in a surprising amount of progress even on seemingly dissimilar tasks; progress on other tasks is larger if the training task has diverse classes; (3) the structure of the space of tasks indicated by our analysis is consistent with parts of the Wordnet phylogenetic tree; (4) episodic meta-learning algorithms and supervised learning traverse different trajectories during training but they fit similar models eventually; (5) contrastive and semi-supervised learning methods traverse trajectories similar",
    "link": "http://arxiv.org/abs/2210.17011",
    "context": "Title: A picture of the space of typical learnable tasks. (arXiv:2210.17011v3 [cs.LG] UPDATED)\nAbstract: We develop information geometric techniques to understand the representations learned by deep networks when they are trained on different tasks using supervised, meta-, semi-supervised and contrastive learning. We shed light on the following phenomena that relate to the structure of the space of tasks: (1) the manifold of probabilistic models trained on different tasks using different representation learning methods is effectively low-dimensional; (2) supervised learning on one task results in a surprising amount of progress even on seemingly dissimilar tasks; progress on other tasks is larger if the training task has diverse classes; (3) the structure of the space of tasks indicated by our analysis is consistent with parts of the Wordnet phylogenetic tree; (4) episodic meta-learning algorithms and supervised learning traverse different trajectories during training but they fit similar models eventually; (5) contrastive and semi-supervised learning methods traverse trajectories similar",
    "path": "papers/22/10/2210.17011.json",
    "total_tokens": 906,
    "translated_title": "典型可学习任务空间的图像",
    "translated_abstract": "我们利用信息几何技术来理解深度网络在使用监督学习、元学习、半监督学习和对比学习训练在不同任务上学到的表示。我们揭示了与任务空间结构相关的以下现象：(1)使用不同表示学习方法在不同任务上训练的概率模型流形实际上是低维的；(2)在一个任务上进行监督学习即使在表面上看起来是不相似的任务上也能取得出乎意料的进展；如果训练任务具有多样的类别，则其在其他任务上的进展更大；(3)通过我们的分析所指示的任务空间结构与Wordnet系统进化树中的某些部分一致；(4)在训练过程中，情境元学习算法和监督学习遵循不同的轨迹，但最终适应相似的模型；(5)对比学习和半监督学习方法遵循类似的轨迹。",
    "tldr": "我们使用信息几何技术研究了在不同任务上训练时深度网络学习到的表示，发现任务空间的结构与Wordnet系统进化树的某些部分一致，并且监督学习在一个任务上的进展可以在其他任务上产生一定的影响。",
    "en_tdlr": "We utilize information geometric techniques to investigate the representations learned by deep networks on different tasks, and find that the structure of the task space is consistent with certain parts of the Wordnet phylogenetic tree. Additionally, progress in supervised learning on one task can have an impact on other tasks."
}