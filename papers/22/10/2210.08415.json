{
    "title": "Stability of Accuracy for the Training of DNNs Via the Uniform Doubling Condition. (arXiv:2210.08415v2 [cs.LG] UPDATED)",
    "abstract": "We study the stability of accuracy during the training of deep neural networks (DNNs). In this context, the training of a DNN is performed via the minimization of a cross-entropy loss function, and the performance metric is accuracy (the proportion of objects that are classified correctly). While training results in a decrease of loss, the accuracy does not necessarily increase during the process and may sometimes even decrease. The goal of achieving stability of accuracy is to ensure that if accuracy is high at some initial time, it remains high throughout training.  A recent result by Berlyand, Jabin, and Safsten introduces a doubling condition on the training data, which ensures the stability of accuracy during training for DNNs using the absolute value activation function. For training data in $\\mathbb{R}^n$, this doubling condition is formulated using slabs in $\\mathbb{R}^n$ and depends on the choice of the slabs. The goal of this paper is twofold. First, to make the doubling cond",
    "link": "http://arxiv.org/abs/2210.08415",
    "context": "Title: Stability of Accuracy for the Training of DNNs Via the Uniform Doubling Condition. (arXiv:2210.08415v2 [cs.LG] UPDATED)\nAbstract: We study the stability of accuracy during the training of deep neural networks (DNNs). In this context, the training of a DNN is performed via the minimization of a cross-entropy loss function, and the performance metric is accuracy (the proportion of objects that are classified correctly). While training results in a decrease of loss, the accuracy does not necessarily increase during the process and may sometimes even decrease. The goal of achieving stability of accuracy is to ensure that if accuracy is high at some initial time, it remains high throughout training.  A recent result by Berlyand, Jabin, and Safsten introduces a doubling condition on the training data, which ensures the stability of accuracy during training for DNNs using the absolute value activation function. For training data in $\\mathbb{R}^n$, this doubling condition is formulated using slabs in $\\mathbb{R}^n$ and depends on the choice of the slabs. The goal of this paper is twofold. First, to make the doubling cond",
    "path": "papers/22/10/2210.08415.json",
    "total_tokens": 1164,
    "translated_title": "通过统一加倍条件稳定神经网络训练的准确性",
    "translated_abstract": "本文研究深度神经网络（DNNs）训练过程中准确性的稳定性。在这个背景下，DNNs的训练通过最小化交叉熵损失函数来进行，其性能指标是准确性（正确分类的对象比例）。虽然训练会导致损失减少，但是准确性不一定随着过程增加，有时甚至会下降。实现准确性稳定性的目标是确保如果初始时准确性很高，在整个训练过程中保持高水平。本文引入了训练数据的加倍条件，以确保使用绝对值激活函数的DNN的训练期间的准确性稳定。对于在$R^n$中的训练数据，这种加倍条件使用$R^n$中的板块进行制定，并且取决于板块的选择。本文的目标是两方面。首先，通过引入更简单、更通用的条件——统一加倍条件，使加倍条件方法更易于理解。其次，利用这个新条件证明在训练过程中，无论是使用绝对值激活函数还是ReLU激活函数的DNN，其准确性都保持不变。我们的主要结果是，在统一加倍条件下，准确性在训练期间有很高的概率保持稳定，并提供了具体的概率估计。",
    "tldr": "本文引入了统一加倍条件，以确保在训练DNN期间准确性的稳定性，无论使用绝对值激活函数还是ReLU激活函数。在统一加倍条件下，准确率有很高的概率稳定，并提供了具体估计。",
    "en_tdlr": "This paper introduces the uniform doubling condition to ensure the stability of accuracy during the training of deep neural networks (DNNs) with both absolute value and ReLU activation functions. The main result is that under the uniform doubling condition, accuracy remains stable during training with high probability, and concrete estimates on this probability are provided."
}