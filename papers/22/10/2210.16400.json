{
    "title": "Flatter, faster: scaling momentum for optimal speedup of SGD. (arXiv:2210.16400v2 [cs.LG] UPDATED)",
    "abstract": "Commonly used optimization algorithms often show a trade-off between good generalization and fast training times. For instance, stochastic gradient descent (SGD) tends to have good generalization; however, adaptive gradient methods have superior training times. Momentum can help accelerate training with SGD, but so far there has been no principled way to select the momentum hyperparameter. Here we study training dynamics arising from the interplay between SGD with label noise and momentum in the training of overparametrized neural networks. We find that scaling the momentum hyperparameter $1-\\beta$ with the learning rate to the power of $2/3$ maximally accelerates training, without sacrificing generalization. To analytically derive this result we develop an architecture-independent framework, where the main assumption is the existence of a degenerate manifold of global minimizers, as is natural in overparametrized models. Training dynamics display the emergence of two characteristic ti",
    "link": "http://arxiv.org/abs/2210.16400",
    "context": "Title: Flatter, faster: scaling momentum for optimal speedup of SGD. (arXiv:2210.16400v2 [cs.LG] UPDATED)\nAbstract: Commonly used optimization algorithms often show a trade-off between good generalization and fast training times. For instance, stochastic gradient descent (SGD) tends to have good generalization; however, adaptive gradient methods have superior training times. Momentum can help accelerate training with SGD, but so far there has been no principled way to select the momentum hyperparameter. Here we study training dynamics arising from the interplay between SGD with label noise and momentum in the training of overparametrized neural networks. We find that scaling the momentum hyperparameter $1-\\beta$ with the learning rate to the power of $2/3$ maximally accelerates training, without sacrificing generalization. To analytically derive this result we develop an architecture-independent framework, where the main assumption is the existence of a degenerate manifold of global minimizers, as is natural in overparametrized models. Training dynamics display the emergence of two characteristic ti",
    "path": "papers/22/10/2210.16400.json",
    "total_tokens": 902,
    "translated_title": "更平稳，更快速：缩放动量以实现随机梯度下降最佳加速",
    "translated_abstract": "常用的优化算法通常存在在良好泛化和快速训练时间之间权衡的问题。例如，随机梯度下降（SGD）往往具有良好的泛化能力；然而，自适应梯度方法具有更好的训练时间。动量可以帮助加速SGD的训练，但到目前为止没有一个基于原则的方法来选择动量超参数。在这里，我们研究了SGD和动量在超参数化神经网络的训练中相互作用所产生的训练动态。我们发现将动量超参数$1-\\beta$与学习率的$2/3$次方缩放可以最大限度地加速训练而不损失泛化能力。为了从理论上推导这个结果，我们开发了一种与结构无关的框架，其中的主要假设是存在一类具有退化全局最小值的流形，这是超参数化模型的自然属性。训练动态显示出两个特征时间尺度的出现。",
    "tldr": "研究者们发现，将动量超参数与学习率的$2/3$次方缩放可以最大限度地加速超参数化神经网络的训练且不牺牲泛化能力。",
    "en_tdlr": "Researchers found that scaling the momentum hyperparameter with the learning rate to the power of $2/3$ can maximally accelerate the training of overparametrized neural networks without sacrificing generalization."
}