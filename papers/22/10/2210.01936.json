{
    "title": "When and why vision-language models behave like bags-of-words, and what to do about it?. (arXiv:2210.01936v3 [cs.CV] UPDATED)",
    "abstract": "Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO & Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these d",
    "link": "http://arxiv.org/abs/2210.01936",
    "context": "Title: When and why vision-language models behave like bags-of-words, and what to do about it?. (arXiv:2210.01936v3 [cs.CV] UPDATED)\nAbstract: Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO & Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these d",
    "path": "papers/22/10/2210.01936.json",
    "total_tokens": 1069,
    "translated_title": "视觉语言模型何时以及为何行为像词袋，以及如何解决？",
    "translated_abstract": "尽管大型视觉语言模型（VLM）在许多下游应用中取得了成功，但它们编码组合信息的能力尚不清楚。为此，我们创建了Attribution、Relation和Order（ARO）基准来系统评估VLM理解不同类型关系、属性和顺序的能力。ARO由Visual Genome Attribution测试对象属性的理解能力；Visual Genome Relation测试关系理解能力；以及COCO＆Flickr30k-Order测试顺序敏感性。ARO比以前的组合性基准大多个数量级，包括50,000多个测试用例。我们展示了最先进的VLM在哪些方面存在关系理解问题，当链接对象和属性时容易出错，并展示了其明显的缺乏顺序敏感性。VLM主要在具有丰富组合结构的图像和标题的大型数据集上进行训练和评估。然而，仅仅在这些数据集上训练并不能保证在需要组合理解的任务上表现良好。为了解决这些问题，我们提出了几种修改VLMs的方法，包括强调组成关系的注意机制和使用对抗训练来改善属性预测。我们提出的修改可以显著提高VLM在ARO基准上的组合理解能力。",
    "tldr": "针对大型视觉语言模型在编码组合信息方面的表现存在问题，我们创建了Attribution、Relation和Order（ARO）基准，并提出了对VLMs进行Attention机制和对抗训练等修改以提高其组合理解能力。",
    "en_tdlr": "We create the ARO benchmark to evaluate the compositional understanding ability of VLMs, showing their poor relational understanding and lack of order sensitivity. Proposed modifications including attention mechanism and adversarial training greatly improve their performance on ARO."
}