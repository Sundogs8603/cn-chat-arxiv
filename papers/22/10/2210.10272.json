{
    "title": "Training set cleansing of backdoor poisoning by self-supervised representation learning. (arXiv:2210.10272v2 [cs.LG] UPDATED)",
    "abstract": "A backdoor or Trojan attack is an important type of data poisoning attack against deep neural network (DNN) classifiers, wherein the training dataset is poisoned with a small number of samples that each possess the backdoor pattern (usually a pattern that is either imperceptible or innocuous) and which are mislabeled to the attacker's target class. When trained on a backdoor-poisoned dataset, a DNN behaves normally on most benign test samples but makes incorrect predictions to the target class when the test sample has the backdoor pattern incorporated (i.e., contains a backdoor trigger). Here we focus on image classification tasks and show that supervised training may build stronger association between the backdoor pattern and the associated target class than that between normal features and the true class of origin. By contrast, self-supervised representation learning ignores the labels of samples and learns a feature embedding based on images' semantic content. %We thus propose to us",
    "link": "http://arxiv.org/abs/2210.10272",
    "context": "Title: Training set cleansing of backdoor poisoning by self-supervised representation learning. (arXiv:2210.10272v2 [cs.LG] UPDATED)\nAbstract: A backdoor or Trojan attack is an important type of data poisoning attack against deep neural network (DNN) classifiers, wherein the training dataset is poisoned with a small number of samples that each possess the backdoor pattern (usually a pattern that is either imperceptible or innocuous) and which are mislabeled to the attacker's target class. When trained on a backdoor-poisoned dataset, a DNN behaves normally on most benign test samples but makes incorrect predictions to the target class when the test sample has the backdoor pattern incorporated (i.e., contains a backdoor trigger). Here we focus on image classification tasks and show that supervised training may build stronger association between the backdoor pattern and the associated target class than that between normal features and the true class of origin. By contrast, self-supervised representation learning ignores the labels of samples and learns a feature embedding based on images' semantic content. %We thus propose to us",
    "path": "papers/22/10/2210.10272.json",
    "total_tokens": 1257,
    "translated_title": "通过自监督表示学习的训练集清洁消除后门攻击",
    "translated_abstract": "后门或特洛伊攻击是针对深度神经网络分类器的一种重要数据污染攻击类型，在该攻击中，训练数据集被毒化了一小部分样本，每个样本都具有后门模式（通常是不可察觉的或无害的模式），并被标记为攻击者的目标类别。当在后门毒化的数据集上进行训练时，DNN在大多数良性测试样本上表现正常，但当测试样本中包含包含后门触发器的后门模式时（即含有后门触发器），它会向目标类别做出错误的预测。本文聚焦于图像分类任务，展示了监督式训练可能会构建更强的后门模式与关联的目标类别之间的联系，而不是正常特征与真实起源类别之间的联系。相比之下，自监督表示学习忽略样本标签并基于图像的语义内容学习特征嵌入。因此，我们提出使用自监督表示学习来清除后门毒化样本的训练集。具体而言，我们通过一个自监督模型学习的表示来区分毒化和干净的样本，从而训练一个深度神经网络。我们的方法不需要任何关于攻击者使用的后门触发器的知识，在攻击者将触发器适应于与正常图像具有高感知相似性的情况下仍然有效。实验结果表明，我们的方法在不同数据集上的后门准确性和干净准确性方面优于现有方法。",
    "tldr": "本文使用自监督表示学习清洁后门攻击所传染的神经网络模型的训练数据集。该方法通过学习样本嵌入表示来识别被毒化的和干净的样本，无需知道攻击者使用的后门触发器。在不同数据集的实验中，我们的方法优于现有方法。",
    "en_tdlr": "This paper presents a method for cleaning training data of backdoor-poisoned samples in neural network models using self-supervised representation learning. By learning sample embeddings and distinguishing between poisoned and clean samples, this method is effective even when the backdoor trigger used by the attacker is unknown. Experimental results show that this method outperforms existing methods on various datasets."
}