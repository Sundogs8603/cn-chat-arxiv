{
    "title": "Exploring Effective Distillation of Self-Supervised Speech Models for Automatic Speech Recognition. (arXiv:2210.15631v3 [eess.AS] UPDATED)",
    "abstract": "Recent years have witnessed great strides in self-supervised learning (SSL) on the speech processing. The SSL model is normally pre-trained on a great variety of unlabelled data and a large model size is preferred to increase the modeling capacity. However, this might limit its potential applications due to the expensive computation and memory costs introduced by the oversize model. Miniaturization for SSL models has become an important research direction of practical value. To this end, we explore the effective distillation of HuBERT-based SSL models for automatic speech recognition (ASR). First, in order to establish a strong baseline, a comprehensive study on different student model structures is conducted. On top of this, as a supplement to the regression loss widely adopted in previous works, a discriminative loss is introduced for HuBERT to enhance the distillation performance, especially in low-resource scenarios. In addition, we design a simple and effective algorithm to distil",
    "link": "http://arxiv.org/abs/2210.15631",
    "context": "Title: Exploring Effective Distillation of Self-Supervised Speech Models for Automatic Speech Recognition. (arXiv:2210.15631v3 [eess.AS] UPDATED)\nAbstract: Recent years have witnessed great strides in self-supervised learning (SSL) on the speech processing. The SSL model is normally pre-trained on a great variety of unlabelled data and a large model size is preferred to increase the modeling capacity. However, this might limit its potential applications due to the expensive computation and memory costs introduced by the oversize model. Miniaturization for SSL models has become an important research direction of practical value. To this end, we explore the effective distillation of HuBERT-based SSL models for automatic speech recognition (ASR). First, in order to establish a strong baseline, a comprehensive study on different student model structures is conducted. On top of this, as a supplement to the regression loss widely adopted in previous works, a discriminative loss is introduced for HuBERT to enhance the distillation performance, especially in low-resource scenarios. In addition, we design a simple and effective algorithm to distil",
    "path": "papers/22/10/2210.15631.json",
    "total_tokens": 965,
    "translated_title": "自监督语音模型在自动语音识别中的有效蒸馏探索",
    "translated_abstract": "近年来，自监督学习在语音处理领域取得了巨大的进展。自监督学习模型通常是在大量无标签数据上进行预训练，并且更喜欢使用大型模型来增加建模能力。然而，这可能由于过大的计算和存储成本而限制了其潜在应用。自监督学习模型的微型化已成为具有实用价值的重要研究方向。为此，我们探索了基于HuBERT的自监督语音模型在自动语音识别中的有效蒸馏。首先，为了建立一个强大的基准线，我们对不同的学生模型结构进行了全面的研究。在此基础上，作为对先前工作中广泛采用的回归损失的补充，引入了一种判别损失来增强HuBERT的蒸馏性能，特别是在低资源场景下。此外，我们设计了一种简单有效的算法来进行蒸馏。",
    "tldr": "本文探索了将HuBERT基于自监督学习的语音模型进行有效蒸馏，以改善自动语音识别的性能，特别是在低资源场景下。通过研究不同的学生模型结构，并引入判别损失，本研究提供了一种简单有效的算法来进行自监督语音模型的微型化。"
}