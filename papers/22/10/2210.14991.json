{
    "title": "Reachability Verification Based Reliability Assessment for Deep Reinforcement Learning Controlled Robotics and Autonomous Systems. (arXiv:2210.14991v2 [cs.RO] UPDATED)",
    "abstract": "Deep Reinforcement Learning (DRL) has achieved impressive performance in robotics and autonomous systems (RAS). A key challenge to its deployment in real-life operations is the presence of spuriously unsafe DRL policies. Unexplored states may lead the agent to make wrong decisions that could result in hazards, especially in applications where DRL-trained end-to-end controllers govern the behaviour of RAS. This paper proposes a novel quantitative reliability assessment framework for DRL-controlled RAS, leveraging verification evidence generated from formal reliability analysis of neural networks. A two-level verification framework is introduced to check the safety property with respect to inaccurate observations that are due to, e.g., environmental noise and state changes. Reachability verification tools are leveraged locally to generate safety evidence of trajectories. In contrast, at the global level, we quantify the overall reliability as an aggregated metric of local safety evidence",
    "link": "http://arxiv.org/abs/2210.14991",
    "context": "Title: Reachability Verification Based Reliability Assessment for Deep Reinforcement Learning Controlled Robotics and Autonomous Systems. (arXiv:2210.14991v2 [cs.RO] UPDATED)\nAbstract: Deep Reinforcement Learning (DRL) has achieved impressive performance in robotics and autonomous systems (RAS). A key challenge to its deployment in real-life operations is the presence of spuriously unsafe DRL policies. Unexplored states may lead the agent to make wrong decisions that could result in hazards, especially in applications where DRL-trained end-to-end controllers govern the behaviour of RAS. This paper proposes a novel quantitative reliability assessment framework for DRL-controlled RAS, leveraging verification evidence generated from formal reliability analysis of neural networks. A two-level verification framework is introduced to check the safety property with respect to inaccurate observations that are due to, e.g., environmental noise and state changes. Reachability verification tools are leveraged locally to generate safety evidence of trajectories. In contrast, at the global level, we quantify the overall reliability as an aggregated metric of local safety evidence",
    "path": "papers/22/10/2210.14991.json",
    "total_tokens": 899,
    "translated_title": "基于可达性验证的深度强化学习控制的机器人和自主系统的可靠性评估",
    "translated_abstract": "深度强化学习在机器人和自主系统领域取得了令人瞩目的性能。其在真实世界中应用的关键挑战是存在不安全的深度强化学习策略。未探索的状态可能导致Agent做出错误决策，可能会导致危险，特别是在DRL训练的端到端控制器指导下的应用中。本文提出了一种新颖的定量可靠性评估框架，针对DRL控制的RAS，利用从神经网络的形式可靠性分析生成的验证证据。引入了一个两级验证框架，用于检查与不准确的观测相关的安全属性，例如环境噪声和状态变化。通过本地利用可达性验证工具生成轨迹的安全证据。相反，在全局级别上，我们将整体可靠性量化为本地安全证据的聚合指标。",
    "tldr": "本文提出了一个基于可达性验证的深度强化学习控制的机器人和自主系统的可靠性评估框架，通过验证证据生成对于不准确观测的安全属性检查，提供了局部和整体的可靠性量化指标。"
}