{
    "title": "Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints. (arXiv:2210.03251v2 [cs.CL] UPDATED)",
    "abstract": "Autocomplete is a task where the user inputs a piece of text, termed prompt, which is conditioned by the model to generate semantically coherent continuation. Existing works for this task have primarily focused on datasets (e.g., email, chat) with high frequency user prompt patterns (or focused prompts) where word-based language models have been quite effective. In this work, we study the more challenging open-domain setting consisting of low frequency user prompt patterns (or broad prompts, e.g., prompt about 93rd academy awards) and demonstrate the effectiveness of character-based language models. We study this problem under memory-constrained settings (e.g., edge devices and smartphones), where character-based representation is effective in reducing the overall model size (in terms of parameters). We use WikiText-103 benchmark to simulate broad prompts and demonstrate that character models rival word models in exact match accuracy for the autocomplete task, when controlled for the m",
    "link": "http://arxiv.org/abs/2210.03251",
    "context": "Title: Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints. (arXiv:2210.03251v2 [cs.CL] UPDATED)\nAbstract: Autocomplete is a task where the user inputs a piece of text, termed prompt, which is conditioned by the model to generate semantically coherent continuation. Existing works for this task have primarily focused on datasets (e.g., email, chat) with high frequency user prompt patterns (or focused prompts) where word-based language models have been quite effective. In this work, we study the more challenging open-domain setting consisting of low frequency user prompt patterns (or broad prompts, e.g., prompt about 93rd academy awards) and demonstrate the effectiveness of character-based language models. We study this problem under memory-constrained settings (e.g., edge devices and smartphones), where character-based representation is effective in reducing the overall model size (in terms of parameters). We use WikiText-103 benchmark to simulate broad prompts and demonstrate that character models rival word models in exact match accuracy for the autocomplete task, when controlled for the m",
    "path": "papers/22/10/2210.03251.json",
    "total_tokens": 1016,
    "tldr": "本文研究了内存约束下自动完成任务的挑战性开放领域情况，提出使用小字符模型可以匹敌大单词模型，同时能有效地减小模型总体大小。",
    "en_tdlr": "This paper studies the challenging open-domain setting of autocomplete task under memory constraints, and proposes the use of small character models that can match large word models, while effectively reducing overall model size."
}