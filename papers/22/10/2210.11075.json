{
    "title": "Freeze then Train: Towards Provable Representation Learning under Spurious Correlations and Feature Noise. (arXiv:2210.11075v2 [cs.LG] UPDATED)",
    "abstract": "The existence of spurious correlations such as image backgrounds in the training environment can make empirical risk minimization (ERM) perform badly in the test environment. To address this problem, Kirichenko et al. (2022) empirically found that the core features that are related to the outcome can still be learned well even with the presence of spurious correlations. This opens a promising strategy to first train a feature learner rather than a classifier, and then perform linear probing (last layer retraining) in the test environment. However, a theoretical understanding of when and why this approach works is lacking. In this paper, we find that core features are only learned well when their associated non-realizable noise is smaller than that of spurious features, which is not necessarily true in practice. We provide both theories and experiments to support this finding and to illustrate the importance of non-realizable noise. Moreover, we propose an algorithm called Freeze then T",
    "link": "http://arxiv.org/abs/2210.11075",
    "context": "Title: Freeze then Train: Towards Provable Representation Learning under Spurious Correlations and Feature Noise. (arXiv:2210.11075v2 [cs.LG] UPDATED)\nAbstract: The existence of spurious correlations such as image backgrounds in the training environment can make empirical risk minimization (ERM) perform badly in the test environment. To address this problem, Kirichenko et al. (2022) empirically found that the core features that are related to the outcome can still be learned well even with the presence of spurious correlations. This opens a promising strategy to first train a feature learner rather than a classifier, and then perform linear probing (last layer retraining) in the test environment. However, a theoretical understanding of when and why this approach works is lacking. In this paper, we find that core features are only learned well when their associated non-realizable noise is smaller than that of spurious features, which is not necessarily true in practice. We provide both theories and experiments to support this finding and to illustrate the importance of non-realizable noise. Moreover, we propose an algorithm called Freeze then T",
    "path": "papers/22/10/2210.11075.json",
    "total_tokens": 1181,
    "translated_title": "冻结再训练：在虚假相关和特征噪声下实现可证明的表示学习",
    "translated_abstract": "在训练环境中存在虚假相关，如图像背景，可能使经验风险最小化方法(ERM)在测试环境中表现不佳。为了解决这个问题，Kirichenko等人(2022) 实证发现，即使存在虚假相关，与结果相关的核心特征仍然可以很好地学习。这开启了一种有前途的策略，即首先训练特征学习器而不是分类器，然后在测试环境中进行线性探测(重训练最后一层)。然而，缺乏一个理论上的理解何时以及为什么这种方法有效。在本文中，我们发现只有当与结果相关的核心特征关联的不可实现噪声小于虚假特征的噪声时，才能很好地学习这些特征，这在实践中并不一定成立。我们提供理论和实验证据支持这个发现，并阐述不可实现噪声的重要性。此外，我们提出了一种称为冻结再变换(FTT)的算法，首先冻结特征学习器，然后在其上训练分类器，利用学习到的核心特征。我们证明了FTT在特征学习器上的一个温和条件下保证有界的泛化误差。实验证明了FTT在各种数据集和虚假相关以及特征噪声设置下的有效性。",
    "tldr": "本文提出了一种称为冻结再变换(FTT)的算法，用于在存在虚假相关和特征噪声下实现可证明的表示学习。该算法首先冻结特征学习器，然后在其上训练分类器，利用学习到的核心特征，经过实验证明其有效性。",
    "en_tdlr": "This paper proposes an algorithm called Freeze then Transform (FTT) for provable representation learning under spurious correlations and feature noise. FTT first freezes the feature learner and then trains a classifier on top of it, leveraging the learned core features. The algorithm ensures bounded generalization error under a mild condition on the feature learner and is demonstrated to be effective on various datasets in the presence of spurious correlations and feature noise."
}