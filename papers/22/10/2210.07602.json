{
    "title": "Mention Annotations Alone Enable Efficient Domain Adaptation for Coreference Resolution. (arXiv:2210.07602v2 [cs.CL] UPDATED)",
    "abstract": "Although recent neural models for coreference resolution have led to substantial improvements on benchmark datasets, transferring these models to new target domains containing out-of-vocabulary spans and requiring differing annotation schemes remains challenging. Typical approaches involve continued training on annotated target-domain data, but obtaining annotations is costly and time-consuming. We show that annotating mentions alone is nearly twice as fast as annotating full coreference chains. Accordingly, we propose a method for efficiently adapting coreference models, which includes a high-precision mention detection objective and requires annotating only mentions in the target domain. Extensive evaluation across three English coreference datasets: CoNLL-2012 (news/conversation), i2b2/VA (medical notes), and previously unstudied child welfare notes, reveals that our approach facilitates annotation-efficient transfer and results in a 7-14% improvement in average F1 without increasin",
    "link": "http://arxiv.org/abs/2210.07602",
    "context": "Title: Mention Annotations Alone Enable Efficient Domain Adaptation for Coreference Resolution. (arXiv:2210.07602v2 [cs.CL] UPDATED)\nAbstract: Although recent neural models for coreference resolution have led to substantial improvements on benchmark datasets, transferring these models to new target domains containing out-of-vocabulary spans and requiring differing annotation schemes remains challenging. Typical approaches involve continued training on annotated target-domain data, but obtaining annotations is costly and time-consuming. We show that annotating mentions alone is nearly twice as fast as annotating full coreference chains. Accordingly, we propose a method for efficiently adapting coreference models, which includes a high-precision mention detection objective and requires annotating only mentions in the target domain. Extensive evaluation across three English coreference datasets: CoNLL-2012 (news/conversation), i2b2/VA (medical notes), and previously unstudied child welfare notes, reveals that our approach facilitates annotation-efficient transfer and results in a 7-14% improvement in average F1 without increasin",
    "path": "papers/22/10/2210.07602.json",
    "total_tokens": 808,
    "translated_title": "仅提及注释即可有效进行共指消解的领域自适应",
    "translated_abstract": "最近，神经网络模型在共指消解方面取得了显著进展，但将这些模型转移到包含新的超出词汇表范围及需要不同注释方案的新目标域中仍然具有挑战性。典型方法涉及在目标域数据上进行持续训练，但获取注释是昂贵且耗时的。本文提出一种方法来有效适应共指模型，其中包括高精度提及检测目标并仅对目标域中的提及进行注释。在三个英语共指数据集上进行了广泛评估：CoNLL-2012（新闻/会话），i2b2 / VA（医学记录）和以前未研究的儿童福利笔记，证明了我们的方法有助于有效注释转移，结果平均F1值提高了7-14％，而不增加时间成本。",
    "tldr": "论文提出一种仅使用提及注释的共指消解领域自适应方法，有效提高模型效果而不增加时间与成本。",
    "en_tdlr": "The paper proposes an efficient domain adaptation method for coreference resolution using only mention annotations, which significantly improves model performance without increasing time and cost."
}