{
    "title": "Stochastic Constrained DRO with a Complexity Independent of Sample Size. (arXiv:2210.05740v2 [cs.LG] UPDATED)",
    "abstract": "Distributionally Robust Optimization (DRO), as a popular method to train robust models against distribution shift between training and test sets, has received tremendous attention in recent years. In this paper, we propose and analyze stochastic algorithms that apply to both non-convex and convex losses for solving Kullback Leibler divergence constrained DRO problem. Compared with existing methods solving this problem, our stochastic algorithms not only enjoy competitive if not better complexity independent of sample size but also just require a constant batch size at every iteration, which is more practical for broad applications. We establish a nearly optimal complexity bound for finding an $\\epsilon$ stationary solution for non-convex losses and an optimal complexity for finding an $\\epsilon$ optimal solution for convex losses. Empirical studies demonstrate the effectiveness of the proposed algorithms for solving non-convex and convex constrained DRO problems.",
    "link": "http://arxiv.org/abs/2210.05740",
    "context": "Title: Stochastic Constrained DRO with a Complexity Independent of Sample Size. (arXiv:2210.05740v2 [cs.LG] UPDATED)\nAbstract: Distributionally Robust Optimization (DRO), as a popular method to train robust models against distribution shift between training and test sets, has received tremendous attention in recent years. In this paper, we propose and analyze stochastic algorithms that apply to both non-convex and convex losses for solving Kullback Leibler divergence constrained DRO problem. Compared with existing methods solving this problem, our stochastic algorithms not only enjoy competitive if not better complexity independent of sample size but also just require a constant batch size at every iteration, which is more practical for broad applications. We establish a nearly optimal complexity bound for finding an $\\epsilon$ stationary solution for non-convex losses and an optimal complexity for finding an $\\epsilon$ optimal solution for convex losses. Empirical studies demonstrate the effectiveness of the proposed algorithms for solving non-convex and convex constrained DRO problems.",
    "path": "papers/22/10/2210.05740.json",
    "total_tokens": 945,
    "translated_title": "随机约束分布鲁棒优化算法的样本大小无关复杂度",
    "translated_abstract": "分布鲁棒优化(DRO)作为一种在训练集和测试集之间进行分布偏移训练鲁棒模型的流行方法，近年来受到了广泛关注。本文提出并分析了适用于解决Kullback Leibler散度约束DRO问题的随机算法，适用于非凸和凸损失函数。与现有的解决方法相比，我们的随机算法不仅具有与样本大小无关的竞争性甚至更好的复杂度，而且在每次迭代中只需要恒定的批次大小，这对于广泛应用更加实用。我们为非凸损失函数找到了一个$\\epsilon$稳定解的近乎最优的复杂度界限，并为凸损失函数找到了一个$\\epsilon$最优解的最优复杂度。实证研究证明了所提算法在解决非凸和凸约束DRO问题方面的有效性。",
    "tldr": "本文提出了一种适用于非凸和凸损失函数的随机算法，用于解决Kullback Leibler散度约束的分布鲁棒优化问题，并且具有与样本大小无关的复杂度，每次迭代只需要恒定的批次大小。实证研究证明了该算法在解决非凸和凸约束DRO问题中的有效性。",
    "en_tdlr": "This paper proposes stochastic algorithms for solving Kullback Leibler divergence constrained Distributionally Robust Optimization (DRO) problems with non-convex and convex loss functions, with complexity independent of sample size and constant batch size at each iteration. Empirical studies demonstrate the effectiveness of the proposed algorithms for both non-convex and convex constrained DRO problems."
}