{
    "title": "A Simple Baseline that Questions the Use of Pretrained-Models in Continual Learning. (arXiv:2210.04428v2 [cs.CV] UPDATED)",
    "abstract": "With the success of pretraining techniques in representation learning, a number of continual learning methods based on pretrained models have been proposed. Some of these methods design continual learning mechanisms on the pre-trained representations and only allow minimum updates or even no updates of the backbone models during the training of continual learning. In this paper, we question whether the complexity of these models is needed to achieve good performance by comparing them to a simple baseline that we designed. We argue that the pretrained feature extractor itself can be strong enough to achieve a competitive or even better continual learning performance on Split-CIFAR100 and CoRe 50 benchmarks. To validate this, we conduct a very simple baseline that 1) use the frozen pretrained model to extract image features for every class encountered during the continual learning stage and compute their corresponding mean features on training data, and 2) predict the class of the input ",
    "link": "http://arxiv.org/abs/2210.04428",
    "context": "Title: A Simple Baseline that Questions the Use of Pretrained-Models in Continual Learning. (arXiv:2210.04428v2 [cs.CV] UPDATED)\nAbstract: With the success of pretraining techniques in representation learning, a number of continual learning methods based on pretrained models have been proposed. Some of these methods design continual learning mechanisms on the pre-trained representations and only allow minimum updates or even no updates of the backbone models during the training of continual learning. In this paper, we question whether the complexity of these models is needed to achieve good performance by comparing them to a simple baseline that we designed. We argue that the pretrained feature extractor itself can be strong enough to achieve a competitive or even better continual learning performance on Split-CIFAR100 and CoRe 50 benchmarks. To validate this, we conduct a very simple baseline that 1) use the frozen pretrained model to extract image features for every class encountered during the continual learning stage and compute their corresponding mean features on training data, and 2) predict the class of the input ",
    "path": "papers/22/10/2210.04428.json",
    "total_tokens": 911,
    "translated_title": "关于在不间断学习中使用预训练模型的简单基线研究",
    "translated_abstract": "随着预训练技术在表示学习中的成功，基于预训练模型的不间断学习方法被提出。其中有些方法设计了在预训练表示上的不间断学习机制，并且在训练不间断学习时只允许对骨干模型进行最小化或者没有更新。本文探讨了在不间断学习中是否需要复杂的模型来实现良好的性能，通过将它们与我们设计的简单基线进行比较。我们认为预训练特征提取器本身已足够强大，可以在Split-CIFAR100和CoRe 50基准测试中实现竞争性甚至更好的不断学习性能。为了验证这一点，我们进行了一个非常简单的基线：1）使用冻结的预训练模型提取每个类别遇到的图像特征，并在训练数据上计算它们的相应均值特征，2）预测输入的类别。",
    "tldr": "本文通过设计简单的基线研究是否需要预训练模型的复杂性来实现不间断学习，并发现预训练特征提取器本身足够强大，能在不间断学习中实现竞争性甚至更好的性能。",
    "en_tdlr": "This paper questions the use of complex models in continual learning by comparing them to a simple baseline, and finds that the pretrained feature extractor itself can achieve competitive or better performance in continual learning, according to the Split-CIFAR100 and CoRe 50 benchmarks."
}