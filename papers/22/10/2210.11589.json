{
    "title": "Monotonic Risk Relationships under Distribution Shifts for Regularized Risk Minimization. (arXiv:2210.11589v2 [cs.LG] UPDATED)",
    "abstract": "Machine learning systems are often applied to data that is drawn from a different distribution than the training distribution. Recent work has shown that for a variety of classification and signal reconstruction problems, the out-of-distribution performance is strongly linearly correlated with the in-distribution performance. If this relationship or more generally a monotonic one holds, it has important consequences. For example, it allows to optimize performance on one distribution as a proxy for performance on the other. In this paper, we study conditions under which a monotonic relationship between the performances of a model on two distributions is expected. We prove an exact asymptotic linear relation for squared error and a monotonic relation for misclassification error for ridge-regularized general linear models under covariate shift, as well as an approximate linear relation for linear inverse problems.",
    "link": "http://arxiv.org/abs/2210.11589",
    "context": "Title: Monotonic Risk Relationships under Distribution Shifts for Regularized Risk Minimization. (arXiv:2210.11589v2 [cs.LG] UPDATED)\nAbstract: Machine learning systems are often applied to data that is drawn from a different distribution than the training distribution. Recent work has shown that for a variety of classification and signal reconstruction problems, the out-of-distribution performance is strongly linearly correlated with the in-distribution performance. If this relationship or more generally a monotonic one holds, it has important consequences. For example, it allows to optimize performance on one distribution as a proxy for performance on the other. In this paper, we study conditions under which a monotonic relationship between the performances of a model on two distributions is expected. We prove an exact asymptotic linear relation for squared error and a monotonic relation for misclassification error for ridge-regularized general linear models under covariate shift, as well as an approximate linear relation for linear inverse problems.",
    "path": "papers/22/10/2210.11589.json",
    "total_tokens": 854,
    "translated_title": "在分布偏移下正则化风险最小化的单调风险关系",
    "translated_abstract": "机器学习系统通常应用于与训练分布不同的数据。最近的研究表明，在各种分类和信号重建问题中，超出分布的性能与内部分布的性能强烈线性相关。如果存在这种关系或更一般的单调关系，将产生重要的影响。例如，它允许将一个分布上的性能优化作为另一个分布上性能的代理。在本文中，我们研究了在两个分布上模型性能之间预期存在单调关系的条件。我们在协变量转移下，证明了岭正则化通用线性模型的平方误差的精确渐近线性关系和误分类误差的单调关系，以及线性逆问题的近似线性关系。",
    "tldr": "本文研究了在分布转移写下，期望模型在两个分布上性能存在单调关系的条件，利用岭正则化通用线性模型证明了平方误差的精确渐近线性关系和误分类误差的单调关系，以及线性逆问题的近似线性关系。",
    "en_tdlr": "This paper studies the conditions under which a monotonic relationship is expected between the performances of a model on two distributions under distribution shifts. The study shows exact asymptotic linear relation for squared error and monotonic relation for misclassification error in ridge-regularized general linear models under covariate shift, as well as an approximate linear relation for linear inverse problems."
}