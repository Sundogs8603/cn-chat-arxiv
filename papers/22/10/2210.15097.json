{
    "title": "Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v2 [cs.CL] UPDATED)",
    "abstract": "Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and ",
    "link": "http://arxiv.org/abs/2210.15097",
    "context": "Title: Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v2 [cs.CL] UPDATED)\nAbstract: Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and ",
    "path": "papers/22/10/2210.15097.json",
    "total_tokens": 976,
    "translated_title": "对比解码：将开放式文本生成视为优化问题",
    "translated_abstract": "鉴于语言模型（LM），最大概率是开放式生成的较差解码目标，因为它会产生短而重复的文本。另一方面，采样往往会产生与原始主题偏离的不连贯文本。我们提出了对比解码（CD），这是一种可靠的解码方法，它在满足合理性约束条件的前提下优化对比目标。对比目标返回一个大型LM（被称为专家，例如OPT-13B）和一个小型LM（被称为业余者，例如OPT-125M）之间的似然差异，并且约束条件确保输出是合理的。CD的灵感来自于这样一个事实，即较大的LM（例如重复、不连贯）在较小的LM中更为普遍，并且这种差异表明哪些文本应优先考虑。CD不需要额外的培训，并且比仅从较大的LM进行解码的情况下产生更高质量的文本。它还适用于不同的模型规模（OPT-13B和GPT2-1.5B）。",
    "tldr": "对比解码是一种可靠的解码方法，它通过优化对比目标来生成合理且高质量的文本，与仅使用较大的语言模型进行解码相比，对比解码能够避免短而重复的文本和不连贯的文本，并适用于不同的模型规模。",
    "en_tdlr": "Contrastive decoding is a reliable decoding approach that generates coherent and high-quality text by optimizing a contrastive objective. Compared to decoding from larger language models alone, contrastive decoding avoids short and repetitive text as well as incoherent text, and it is applicable across different model scales."
}