{
    "title": "Preferential Subsampling for Stochastic Gradient Langevin Dynamics. (arXiv:2210.16189v3 [stat.ML] UPDATED)",
    "abstract": "Stochastic gradient MCMC (SGMCMC) offers a scalable alternative to traditional MCMC, by constructing an unbiased estimate of the gradient of the log-posterior with a small, uniformly-weighted subsample of the data. While efficient to compute, the resulting gradient estimator may exhibit a high variance and impact sampler performance. The problem of variance control has been traditionally addressed by constructing a better stochastic gradient estimator, often using control variates. We propose to use a discrete, non-uniform probability distribution to preferentially subsample data points that have a greater impact on the stochastic gradient. In addition, we present a method of adaptively adjusting the subsample size at each iteration of the algorithm, so that we increase the subsample size in areas of the sample space where the gradient is harder to estimate. We demonstrate that such an approach can maintain the same level of accuracy while substantially reducing the average subsample s",
    "link": "http://arxiv.org/abs/2210.16189",
    "context": "Title: Preferential Subsampling for Stochastic Gradient Langevin Dynamics. (arXiv:2210.16189v3 [stat.ML] UPDATED)\nAbstract: Stochastic gradient MCMC (SGMCMC) offers a scalable alternative to traditional MCMC, by constructing an unbiased estimate of the gradient of the log-posterior with a small, uniformly-weighted subsample of the data. While efficient to compute, the resulting gradient estimator may exhibit a high variance and impact sampler performance. The problem of variance control has been traditionally addressed by constructing a better stochastic gradient estimator, often using control variates. We propose to use a discrete, non-uniform probability distribution to preferentially subsample data points that have a greater impact on the stochastic gradient. In addition, we present a method of adaptively adjusting the subsample size at each iteration of the algorithm, so that we increase the subsample size in areas of the sample space where the gradient is harder to estimate. We demonstrate that such an approach can maintain the same level of accuracy while substantially reducing the average subsample s",
    "path": "papers/22/10/2210.16189.json",
    "total_tokens": 981,
    "translated_title": "偏好子采样对于随机梯度Langevin动力学的研究",
    "translated_abstract": "随机梯度MCMC（SGMCMC）通过使用小型、均匀加权的数据子样本构建对于对数后验梯度的无偏估计，为传统MCMC提供了可扩展的替代方法。虽然计算高效，但由此产生的梯度估计可能具有较高的方差，并且会影响采样器性能。传统上，方差控制问题通过构建更好的随机梯度估计器来解决，通常使用控制变量。我们提议使用离散的非均匀概率分布来偏好地子采样对于对梯度产生更大影响的数据点。此外，我们还提出在算法的每次迭代中自适应地调整子采样大小的方法，以便在难以估计梯度的样本空间中增加子采样大小。我们证明了这种方法可以在大幅减少平均子采样数的同时保持相同的精度水平。",
    "tldr": "本文提出一种偏好子采样的方法来对随机梯度Langevin动力学进行优化，通过使用非均匀概率分布子采样对具有更大影响的数据点进行加权，同时还通过自适应调整子采样大小来提高梯度估计的准确性。实验证明这种方法可以在减少子采样数的同时保持相同的精度水平。",
    "en_tdlr": "This paper proposes a preferential subsampling method to optimize stochastic gradient Langevin dynamics. It uses a non-uniform probability distribution to weight the subsampling of data points that have a greater impact on the gradient, and adaptsively adjusts the subsample size to improve the accuracy of gradient estimation. Experiments show that this method can reduce the subsample size while maintaining the same level of accuracy."
}