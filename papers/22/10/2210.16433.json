{
    "title": "Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models. (arXiv:2210.16433v3 [cs.CL] UPDATED)",
    "abstract": "Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge: entity, dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly,",
    "link": "http://arxiv.org/abs/2210.16433",
    "context": "Title: Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models. (arXiv:2210.16433v3 [cs.CL] UPDATED)\nAbstract: Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge: entity, dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly,",
    "path": "papers/22/10/2210.16433.json",
    "total_tokens": 910,
    "translated_title": "上下文知识：面向知识丰富的半参数语言模型",
    "translated_abstract": "完全参数语言模型通常需要大量的模型参数来存储在零/少样本设置中解决多个自然语言任务所需的知识。此外，在没有昂贵的模型重新训练的情况下难以适应不断发展的世界知识。本文提出了一种新颖的半参数语言模型架构，名为 \"Knowledge-in-Context\"（KiC），它通过一个知识丰富的外部存储器赋予一个参数化的文本到文本语言模型的知识。具体而言，外部存储器包含六种不同类型的知识：实体、词典、常识、事件、脚本和因果知识。对于每个输入实例，KiC 模型会自适应地选择一种知识类型并检索最有用的知识片段。输入实例连同其知识增强被馈送到文本到文本模型（例如 T5）中以生成输出答案，在提示后输入和输出都以自然语言形式呈现。",
    "tldr": "本文提出了一种名为「Knowledge-in-Context」的半参数语言模型架构，通过外部存储器带入各种类型的知识以帮助解决自然语言处理任务，并且可以自适应地选择最有用的知识片段。"
}