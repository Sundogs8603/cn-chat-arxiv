{
    "title": "Boosting Graph Neural Networks via Adaptive Knowledge Distillation. (arXiv:2210.05920v2 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks (GNNs) have shown remarkable performance on diverse graph mining tasks. Although different GNNs can be unified as the same message passing framework, they learn complementary knowledge from the same graph. Knowledge distillation (KD) is developed to combine the diverse knowledge from multiple models. It transfers knowledge from high-capacity teachers to a lightweight student. However, to avoid oversmoothing, GNNs are often shallow, which deviates from the setting of KD. In this context, we revisit KD by separating its benefits from model compression and emphasizing its power of transferring knowledge. To this end, we need to tackle two challenges: how to transfer knowledge from compact teachers to a student with the same capacity; and, how to exploit student GNN's own strength to learn knowledge. In this paper, we propose a novel adaptive KD framework, called BGNN, which sequentially transfers knowledge from multiple GNNs into a student GNN. We also introduce an a",
    "link": "http://arxiv.org/abs/2210.05920",
    "context": "Title: Boosting Graph Neural Networks via Adaptive Knowledge Distillation. (arXiv:2210.05920v2 [cs.LG] UPDATED)\nAbstract: Graph neural networks (GNNs) have shown remarkable performance on diverse graph mining tasks. Although different GNNs can be unified as the same message passing framework, they learn complementary knowledge from the same graph. Knowledge distillation (KD) is developed to combine the diverse knowledge from multiple models. It transfers knowledge from high-capacity teachers to a lightweight student. However, to avoid oversmoothing, GNNs are often shallow, which deviates from the setting of KD. In this context, we revisit KD by separating its benefits from model compression and emphasizing its power of transferring knowledge. To this end, we need to tackle two challenges: how to transfer knowledge from compact teachers to a student with the same capacity; and, how to exploit student GNN's own strength to learn knowledge. In this paper, we propose a novel adaptive KD framework, called BGNN, which sequentially transfers knowledge from multiple GNNs into a student GNN. We also introduce an a",
    "path": "papers/22/10/2210.05920.json",
    "total_tokens": 965,
    "translated_title": "自适应知识蒸馏增强图神经网络",
    "translated_abstract": "图神经网络已在各种图挖掘任务上表现出了卓越的性能。尽管不同的图神经网络可以统一为相同的消息传递框架，但它们从同一图中学习互补的知识。为了提取多个模型的不同知识并将其结合，我们需要一种知识蒸馏方法。然而，为了避免过平滑，图神经网络通常是浅层的，这与知识蒸馏的设置相违背。在这种情况下，我们重新审视知识蒸馏，分离其从模型压缩中获益的好处，强调其将知识传递的能力。为此，我们需要解决两个挑战：如何将知识从紧凑的教师模型传输到具有相同容量的学生模型；以及如何利用学生模型的优势来学习知识。本文提出了一种新颖的自适应知识蒸馏框架，称为 BGNN，它将多个 GNN 顺序地传递知识到一个学生 GNN 中。",
    "tldr": "本文提出了一种自适应知识蒸馏框架（BGNN）来增强图神经网络。该框架可以有效地将多个模型的知识传输到具有相同容量的学生模型中，并充分利用学生模型的优势来学习知识。",
    "en_tdlr": "This paper proposes an adaptive knowledge distillation framework, called BGNN, to boost graph neural networks. The framework can effectively transfer knowledge from multiple models to a student model with the same capacity, and fully utilize the strengths of the student model to learn knowledge."
}