{
    "title": "On the Effectiveness of Hybrid Pooling in Mixup-Based Graph Learning for Language Processing. (arXiv:2210.03123v2 [cs.LG] UPDATED)",
    "abstract": "Graph neural network (GNN)-based graph learning has been popular in natural language and programming language processing, particularly in text and source code classification. Typically, GNNs are constructed by incorporating alternating layers which learn transformations of graph node features, along with graph pooling layers that use graph pooling operators (e.g., Max-pooling) to effectively reduce the number of nodes while preserving the semantic information of the graph. Recently, to enhance GNNs in graph learning tasks, Manifold-Mixup, a data augmentation technique that produces synthetic graph data by linearly mixing a pair of graph data and their labels, has been widely adopted. However, the performance of Manifold-Mixup can be highly affected by graph pooling operators, and there have not been many studies that are dedicated to uncovering such affection. To bridge this gap, we take an early step to explore how graph pooling operators affect the performance of Mixup-based graph le",
    "link": "http://arxiv.org/abs/2210.03123",
    "context": "Title: On the Effectiveness of Hybrid Pooling in Mixup-Based Graph Learning for Language Processing. (arXiv:2210.03123v2 [cs.LG] UPDATED)\nAbstract: Graph neural network (GNN)-based graph learning has been popular in natural language and programming language processing, particularly in text and source code classification. Typically, GNNs are constructed by incorporating alternating layers which learn transformations of graph node features, along with graph pooling layers that use graph pooling operators (e.g., Max-pooling) to effectively reduce the number of nodes while preserving the semantic information of the graph. Recently, to enhance GNNs in graph learning tasks, Manifold-Mixup, a data augmentation technique that produces synthetic graph data by linearly mixing a pair of graph data and their labels, has been widely adopted. However, the performance of Manifold-Mixup can be highly affected by graph pooling operators, and there have not been many studies that are dedicated to uncovering such affection. To bridge this gap, we take an early step to explore how graph pooling operators affect the performance of Mixup-based graph le",
    "path": "papers/22/10/2210.03123.json",
    "total_tokens": 1104,
    "translated_title": "混合池化在基于Mixup的图形学习中的有效性研究",
    "translated_abstract": "基于图神经网络（GNN）的图形学习在自然语言和编程语言处理方面越来越受欢迎，特别是在文本和源代码分类方面。通常，GNN是由交替图层和图池化层构成的，交替图层可以学习图节点特征的转换，而图池化层则使用图池化算子（例如Max池化）有效地减少节点数量，同时保留图的语义信息。最近，为了增强GNN在图形学习任务中的性能，人们广泛采用了Manifold-Mixup这种数据增强技术，该技术通过线性混合一对图数据和它们的标签来生成合成图数据。然而，Manifold-Mixup的性能很大程度上受到图池化算子的影响，而且并没有进行很多关于这种影响的研究。为了填补这一空白，我们早期探索了图池化算子如何影响基于Mixup的图形学习的性能。具体而言，我们提出了一种新颖的混合池化架构，结合了Max-pooling和Attention-pooling，以更好地捕捉本地和全局的图结构信息。我们在文本分类任务上的实验表明，所提出的混合池化结构显著优于现有的池化方法，并达到了最先进的性能水平。",
    "tldr": "本文探讨了图池化算子对于Manifold-Mixup方法在图形学习中的影响，提出了一种新颖的混合池化架构，相比现有方法在文本分类任务上表现显著优越并达到了最先进性能水平。",
    "en_tdlr": "This paper explores the effect of graph pooling operators on the Manifold-Mixup method in graph learning, and proposes a novel hybrid pooling architecture. The proposed architecture outperforms existing methods significantly in text classification tasks and achieves state-of-the-art performance."
}