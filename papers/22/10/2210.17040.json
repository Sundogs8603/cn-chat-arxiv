{
    "title": "CodeEditor: Learning to Edit Source Code with Pre-trained Models. (arXiv:2210.17040v2 [cs.SE] UPDATED)",
    "abstract": "Developers often perform repetitive code editing activities for various reasons (e.g., code refactoring) during software development. Pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.  This paper proposes a novel pre-training task specialized in code editing and presents an effective pre-trained code editing model named CodeEditor. Our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the c",
    "link": "http://arxiv.org/abs/2210.17040",
    "context": "Title: CodeEditor: Learning to Edit Source Code with Pre-trained Models. (arXiv:2210.17040v2 [cs.SE] UPDATED)\nAbstract: Developers often perform repetitive code editing activities for various reasons (e.g., code refactoring) during software development. Pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.  This paper proposes a novel pre-training task specialized in code editing and presents an effective pre-trained code editing model named CodeEditor. Our pre-training task further improves the performance and generalization ability of code editing models. Specifically, we collect lots of real-world code snippets as the ground truth and use a powerful generator to rewrite them into mutated versions. Then, we pre-train our CodeEditor to edit mutated versions into the c",
    "path": "papers/22/10/2210.17040.json",
    "total_tokens": 820,
    "translated_title": "CodeEditor: 使用预训练模型学习编辑源代码",
    "translated_abstract": "在软件开发过程中，开发人员经常需要进行重复的代码编辑活动，例如代码重构等。预训练的代码编辑模型已经取得了最先进的结果。预训练模型首先使用预训练任务进行预训练，然后使用代码编辑任务进行微调。现有的预训练任务主要是代码填充任务（例如，掩码语言模型），这些任务来自自然语言处理领域，不适用于自动代码编辑。本文提出了一个专门用于代码编辑的新型预训练任务，并介绍了一个名为CodeEditor的有效预训练代码编辑模型。我们的预训练任务进一步提高了代码编辑模型的性能和泛化能力。具体来说，我们收集了大量的真实代码片段作为参考，并使用一个强大的生成器将它们重写成变异版本。然后，我们使用CodeEditor对变异版本进行预训练，将其编辑为正确的代码。",
    "tldr": "CodeEditor是一个预训练代码编辑模型，通过专门的预训练任务和代码编辑任务的结合，提高了代码编辑模型的性能和泛化能力。",
    "en_tdlr": "CodeEditor is a pre-trained code editing model that improves the performance and generalization ability of code editing models through a specialized pre-training task and the code editing task."
}