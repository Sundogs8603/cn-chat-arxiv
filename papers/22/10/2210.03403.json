{
    "title": "TAN Without a Burn: Scaling Laws of DP-SGD. (arXiv:2210.03403v2 [cs.LG] UPDATED)",
    "abstract": "Differentially Private methods for training Deep Neural Networks (DNNs) have progressed recently, in particular with the use of massive batches and aggregated data augmentations for a large number of training steps. These techniques require much more computing resources than their non-private counterparts, shifting the traditional privacy-accuracy trade-off to a privacy-accuracy-compute trade-off and making hyper-parameter search virtually impossible for realistic scenarios. In this work, we decouple privacy analysis and experimental behavior of noisy training to explore the trade-off with minimal computational requirements. We first use the tools of R\\'enyi Differential Privacy (RDP) to highlight that the privacy budget, when not overcharged, only depends on the total amount of noise (TAN) injected throughout training. We then derive scaling laws for training models with DP-SGD to optimize hyper-parameters with more than a $100\\times$ reduction in computational budget. We apply the pr",
    "link": "http://arxiv.org/abs/2210.03403",
    "context": "Title: TAN Without a Burn: Scaling Laws of DP-SGD. (arXiv:2210.03403v2 [cs.LG] UPDATED)\nAbstract: Differentially Private methods for training Deep Neural Networks (DNNs) have progressed recently, in particular with the use of massive batches and aggregated data augmentations for a large number of training steps. These techniques require much more computing resources than their non-private counterparts, shifting the traditional privacy-accuracy trade-off to a privacy-accuracy-compute trade-off and making hyper-parameter search virtually impossible for realistic scenarios. In this work, we decouple privacy analysis and experimental behavior of noisy training to explore the trade-off with minimal computational requirements. We first use the tools of R\\'enyi Differential Privacy (RDP) to highlight that the privacy budget, when not overcharged, only depends on the total amount of noise (TAN) injected throughout training. We then derive scaling laws for training models with DP-SGD to optimize hyper-parameters with more than a $100\\times$ reduction in computational budget. We apply the pr",
    "path": "papers/22/10/2210.03403.json",
    "total_tokens": 693,
    "tldr": "本文研究了差分隐私方法在训练深度神经网络中的隐私-精度-计算资源三维权衡问题，提出了一种仅需少量计算资源即可优化超参数的方法，且不会对隐私预算造成过度超支。通过使用Rényi差分隐私工具，我们强调了隐私预算的大小只与注入训练中的总噪声量有关，进而导出了DP-SGD训练模型所需的缩放比例定律。",
    "en_tdlr": "This paper explores the trade-off among privacy, accuracy, and computing resources in training deep neural networks using differentially private methods, and proposes a technique that optimizes hyper-parameters with minimal computational requirements without overcharging the privacy budget. The authors highlight that the privacy budget only depends on the total amount of noise injected throughout training using the tools ofR\\'enyi Differential Privacy (RDP), leading to scaling laws for training models with DP-SGD."
}