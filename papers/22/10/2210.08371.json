{
    "title": "Sketching for First Order Method: Efficient Algorithm for Low-Bandwidth Channel and Vulnerability. (arXiv:2210.08371v2 [cs.LG] UPDATED)",
    "abstract": "Sketching is one of the most fundamental tools in large-scale machine learning. It enables runtime and memory saving via randomly compressing the original large problem into lower dimensions. In this paper, we propose a novel sketching scheme for the first order method in large-scale distributed learning setting, such that the communication costs between distributed agents are saved while the convergence of the algorithms is still guaranteed. Given gradient information in a high dimension $d$, the agent passes the compressed information processed by a sketching matrix $R\\in \\mathbb{R}^{s\\times d}$ with $s\\ll d$, and the receiver de-compressed via the de-sketching matrix $R^\\top$ to ``recover'' the information in original dimension. Using such a framework, we develop algorithms for federated learning with lower communication costs. However, such random sketching does not protect the privacy of local data directly. We show that the gradient leakage problem still exists after applying the",
    "link": "http://arxiv.org/abs/2210.08371",
    "context": "Title: Sketching for First Order Method: Efficient Algorithm for Low-Bandwidth Channel and Vulnerability. (arXiv:2210.08371v2 [cs.LG] UPDATED)\nAbstract: Sketching is one of the most fundamental tools in large-scale machine learning. It enables runtime and memory saving via randomly compressing the original large problem into lower dimensions. In this paper, we propose a novel sketching scheme for the first order method in large-scale distributed learning setting, such that the communication costs between distributed agents are saved while the convergence of the algorithms is still guaranteed. Given gradient information in a high dimension $d$, the agent passes the compressed information processed by a sketching matrix $R\\in \\mathbb{R}^{s\\times d}$ with $s\\ll d$, and the receiver de-compressed via the de-sketching matrix $R^\\top$ to ``recover'' the information in original dimension. Using such a framework, we develop algorithms for federated learning with lower communication costs. However, such random sketching does not protect the privacy of local data directly. We show that the gradient leakage problem still exists after applying the",
    "path": "papers/22/10/2210.08371.json",
    "total_tokens": 903,
    "translated_title": "针对低带宽通道和漏洞的一阶方法的草图技术：高效算法",
    "translated_abstract": "草图技术是大规模机器学习中最基础的工具之一，通过对原始大问题进行随机压缩到更低维度的方式实现运行时和内存的节省。本文提出了一种新颖的一阶方法草图方案，用于大规模分布式学习场景中，以节约分布式代理之间的通信成本，同时保证算法的收敛。基于一个高维度 $d$ 的梯度信息，代理通过草图矩阵 $R\\in \\mathbb{R}^{s\\times d}$ 处理这些压缩信息，在传递时只传输草图后的信息，而接收方会通过反草图矩阵 $R^\\top$ 进行还原。利用这种框架，我们开发了具有较低通信成本的联合学习算法。然而，如此随机草图并不能直接保护局部数据的隐私。我们表明，在应用差分隐私后，梯度泄漏问题仍然存在。",
    "tldr": "本文提出了一种针对大规模分布式学习场景的新型草图方案，通过节约通信成本实现联合学习，但是需要进行差分隐私处理以保护局部数据的隐私。",
    "en_tdlr": "This paper proposes a novel sketching scheme for large-scale distributed learning that saves communication costs between agents, but requires differential privacy to protect local data."
}