{
    "title": "Analyzing Privacy Leakage in Machine Learning via Multiple Hypothesis Testing: A Lesson From Fano. (arXiv:2210.13662v2 [cs.LG] UPDATED)",
    "abstract": "Differential privacy (DP) is by far the most widely accepted framework for mitigating privacy risks in machine learning. However, exactly how small the privacy parameter $\\epsilon$ needs to be to protect against certain privacy risks in practice is still not well-understood. In this work, we study data reconstruction attacks for discrete data and analyze it under the framework of multiple hypothesis testing. We utilize different variants of the celebrated Fano's inequality to derive upper bounds on the inferential power of a data reconstruction adversary when the model is trained differentially privately. Importantly, we show that if the underlying private data takes values from a set of size $M$, then the target privacy parameter $\\epsilon$ can be $O(\\log M)$ before the adversary gains significant inferential power. Our analysis offers theoretical evidence for the empirical effectiveness of DP against data reconstruction attacks even at relatively large values of $\\epsilon$.",
    "link": "http://arxiv.org/abs/2210.13662",
    "context": "Title: Analyzing Privacy Leakage in Machine Learning via Multiple Hypothesis Testing: A Lesson From Fano. (arXiv:2210.13662v2 [cs.LG] UPDATED)\nAbstract: Differential privacy (DP) is by far the most widely accepted framework for mitigating privacy risks in machine learning. However, exactly how small the privacy parameter $\\epsilon$ needs to be to protect against certain privacy risks in practice is still not well-understood. In this work, we study data reconstruction attacks for discrete data and analyze it under the framework of multiple hypothesis testing. We utilize different variants of the celebrated Fano's inequality to derive upper bounds on the inferential power of a data reconstruction adversary when the model is trained differentially privately. Importantly, we show that if the underlying private data takes values from a set of size $M$, then the target privacy parameter $\\epsilon$ can be $O(\\log M)$ before the adversary gains significant inferential power. Our analysis offers theoretical evidence for the empirical effectiveness of DP against data reconstruction attacks even at relatively large values of $\\epsilon$.",
    "path": "papers/22/10/2210.13662.json",
    "total_tokens": 881,
    "translated_title": "通过多重假设检验分析机器学习中的隐私泄露: Fano的教训",
    "translated_abstract": "差分隐私(DP)是目前最广泛接受的机器学习隐私风险缓解框架。然而，在实践中，为了保护特定隐私风险，隐私参数ε需要多小仍不为人知。在这项工作中，我们研究了离散数据的数据重建攻击，并在多重假设检验框架下进行了分析。我们利用著名的Fano不等式的不同变体，推导出模型在不同ially私有训练时数据重建对手推论能力的上限。重要的是，我们证明，如果底层私有数据取值来自大小为M的集合，那么在对手获得显著推论能力之前，目标隐私参数ε可以达到O(log M)。我们的分析为DP在相对较大的ε值下对抗数据重建攻击的经验有效性提供了理论证据。",
    "tldr": "本论文通过多重假设检验的方法对机器学习中的隐私泄露进行了分析，揭示了差分隐私对抗数据重建攻击的理论有效性，为相对较大的隐私参数ε值下的实践应用提供了理论支持。",
    "en_tdlr": "This paper analyzes privacy leakage in machine learning using multiple hypothesis testing, revealing the theoretical effectiveness of differential privacy against data reconstruction attacks and providing theoretical support for practical applications with relatively large privacy parameter ε values."
}