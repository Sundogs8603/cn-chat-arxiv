{
    "title": "Reward Imputation with Sketching for Contextual Batched Bandits. (arXiv:2210.06719v3 [cs.LG] UPDATED)",
    "abstract": "Contextual batched bandit (CBB) is a setting where a batch of rewards is observed from the environment at the end of each episode, but the rewards of the non-executed actions are unobserved, resulting in partial-information feedback. Existing approaches for CBB often ignore the rewards of the non-executed actions, leading to underutilization of feedback information. In this paper, we propose an efficient approach called Sketched Policy Updating with Imputed Rewards (SPUIR) that completes the unobserved rewards using sketching, which approximates the full-information feedbacks. We formulate reward imputation as an imputation regularized ridge regression problem that captures the feedback mechanisms of both executed and non-executed actions. To reduce time complexity, we solve the regression problem using randomized sketching. We prove that our approach achieves an instantaneous regret with controllable bias and smaller variance than approaches without reward imputation. Furthermore, our",
    "link": "http://arxiv.org/abs/2210.06719",
    "context": "Title: Reward Imputation with Sketching for Contextual Batched Bandits. (arXiv:2210.06719v3 [cs.LG] UPDATED)\nAbstract: Contextual batched bandit (CBB) is a setting where a batch of rewards is observed from the environment at the end of each episode, but the rewards of the non-executed actions are unobserved, resulting in partial-information feedback. Existing approaches for CBB often ignore the rewards of the non-executed actions, leading to underutilization of feedback information. In this paper, we propose an efficient approach called Sketched Policy Updating with Imputed Rewards (SPUIR) that completes the unobserved rewards using sketching, which approximates the full-information feedbacks. We formulate reward imputation as an imputation regularized ridge regression problem that captures the feedback mechanisms of both executed and non-executed actions. To reduce time complexity, we solve the regression problem using randomized sketching. We prove that our approach achieves an instantaneous regret with controllable bias and smaller variance than approaches without reward imputation. Furthermore, our",
    "path": "papers/22/10/2210.06719.json",
    "total_tokens": 926,
    "translated_title": "使用草图技术进行奖励补充的上下文批次化强化学习",
    "translated_abstract": "上下文批次化强化学习是一种设置，其中在每个episode结束时观察到环境中的一批奖励，但是未执行操作的奖励是未知的，导致了部分信息反馈。现有的上下文批次化强化学习方法通常忽略未执行操作的奖励，导致反馈信息的浪费。本文提出了一种高效的方法，名为Sketched Policy Updating with Imputed Rewards (SPUIR)，通过使用草图技术完成未观察到的奖励补充，从而近似了全信息反馈。我们将奖励补充问题建模为一个求解执行和未执行操作的反馈机制的正则化岭回归问题。为了降低时间复杂度，我们使用随机草图技术来解决回归问题。我们证明了我们的方法在控制偏差和比没有奖励补充方法更小的方差的同时实现了瞬时遗憾。",
    "tldr": "本文提出了一种名为SPUIR的方法，通过使用草图技术完成未观察到的奖励补充，从而近似了全信息反馈。我们证明了我们的方法在控制偏差和比没有奖励补充方法更小的方差的同时实现了瞬时遗憾。",
    "en_tdlr": "The paper proposes an efficient approach called SPUIR that completes the unobserved rewards using sketching, approximating full-information feedback. The approach achieves instantaneous regret with controllable bias and smaller variance compared to methods without reward imputation."
}