{
    "title": "BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model. (arXiv:2210.16663v2 [eess.AS] UPDATED)",
    "abstract": "This paper presents BERT-CTC, a novel formulation of end-to-end speech recognition that adapts BERT for connectionist temporal classification (CTC). Our formulation relaxes the conditional independence assumptions used in conventional CTC and incorporates linguistic knowledge through the explicit output dependency obtained by BERT contextual embedding. BERT-CTC attends to the full contexts of the input and hypothesized output sequences via the self-attention mechanism. This mechanism encourages a model to learn inner/inter-dependencies between the audio and token representations while maintaining CTC's training efficiency. During inference, BERT-CTC combines a mask-predict algorithm with CTC decoding, which iteratively refines an output sequence. The experimental results reveal that BERT-CTC improves over conventional approaches across variations in speaking styles and languages. Finally, we show that the semantic representations in BERT-CTC are beneficial towards downstream spoken lan",
    "link": "http://arxiv.org/abs/2210.16663",
    "context": "Title: BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model. (arXiv:2210.16663v2 [eess.AS] UPDATED)\nAbstract: This paper presents BERT-CTC, a novel formulation of end-to-end speech recognition that adapts BERT for connectionist temporal classification (CTC). Our formulation relaxes the conditional independence assumptions used in conventional CTC and incorporates linguistic knowledge through the explicit output dependency obtained by BERT contextual embedding. BERT-CTC attends to the full contexts of the input and hypothesized output sequences via the self-attention mechanism. This mechanism encourages a model to learn inner/inter-dependencies between the audio and token representations while maintaining CTC's training efficiency. During inference, BERT-CTC combines a mask-predict algorithm with CTC decoding, which iteratively refines an output sequence. The experimental results reveal that BERT-CTC improves over conventional approaches across variations in speaking styles and languages. Finally, we show that the semantic representations in BERT-CTC are beneficial towards downstream spoken lan",
    "path": "papers/22/10/2210.16663.json",
    "total_tokens": 946,
    "translated_title": "BERT遇见CTC：利用预先训练好的遮蔽语言模型的新型端到端语音识别公式",
    "translated_abstract": "本文提出了BERT-CTC，一种新型的端到端语音识别公式，该公式通过对连接主义时间分类（CTC）使用BERT来实现。我们的公式放宽了常规CTC中使用的条件独立性假设，并通过BERT上下文嵌入获取显式输出依赖性来整合语言知识。BERT-CTC通过自我注意机制关注输入和假设的输出序列的全部上下文。该机制鼓励模型学习音频和标记表示之间的内在/相互依赖关系，同时保持CTC的训练效率。在推理期间，BERT-CTC将遮蔽预测算法与CTC解码相结合，通过迭代细化输出序列。实验结果表明，BERT-CTC在不同的言语风格和语言变化方面都优于传统方法。最后，我们展示了BERT-CTC中的语义表示对下游口语理解任务的有益作用。",
    "tldr": "本文提出了BERT-CTC，一种新型的端到端语音识别公式，它利用BERT上下文嵌入获取显式输出依赖性来整合语言知识，并通过自我注意机制关注序列的全部上下文。在实验中表现优于传统方法，语义表示对下游口语理解任务有益作用。",
    "en_tdlr": "This paper presents BERT-CTC, a novel formulation of end-to-end speech recognition that adapts BERT for connectionist temporal classification (CTC), improving over conventional approaches across variations in speaking styles and languages. The semantic representations in BERT-CTC are beneficial towards downstream spoken language understanding tasks."
}