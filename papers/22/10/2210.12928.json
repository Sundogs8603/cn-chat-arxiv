{
    "title": "GFlowOut: Dropout with Generative Flow Networks. (arXiv:2210.12928v3 [cs.LG] UPDATED)",
    "abstract": "Bayesian Inference offers principled tools to tackle many critical problems with modern neural networks such as poor calibration and generalization, and data inefficiency. However, scaling Bayesian inference to large architectures is challenging and requires restrictive approximations. Monte Carlo Dropout has been widely used as a relatively cheap way for approximate Inference and to estimate uncertainty with deep neural networks. Traditionally, the dropout mask is sampled independently from a fixed distribution. Recent works show that the dropout mask can be viewed as a latent variable, which can be inferred with variational inference. These methods face two important challenges: (a) the posterior distribution over masks can be highly multi-modal which can be difficult to approximate with standard variational inference and (b) it is not trivial to fully utilize sample-dependent information and correlation among dropout masks to improve posterior estimation. In this work, we propose GF",
    "link": "http://arxiv.org/abs/2210.12928",
    "context": "Title: GFlowOut: Dropout with Generative Flow Networks. (arXiv:2210.12928v3 [cs.LG] UPDATED)\nAbstract: Bayesian Inference offers principled tools to tackle many critical problems with modern neural networks such as poor calibration and generalization, and data inefficiency. However, scaling Bayesian inference to large architectures is challenging and requires restrictive approximations. Monte Carlo Dropout has been widely used as a relatively cheap way for approximate Inference and to estimate uncertainty with deep neural networks. Traditionally, the dropout mask is sampled independently from a fixed distribution. Recent works show that the dropout mask can be viewed as a latent variable, which can be inferred with variational inference. These methods face two important challenges: (a) the posterior distribution over masks can be highly multi-modal which can be difficult to approximate with standard variational inference and (b) it is not trivial to fully utilize sample-dependent information and correlation among dropout masks to improve posterior estimation. In this work, we propose GF",
    "path": "papers/22/10/2210.12928.json",
    "total_tokens": 1060,
    "translated_title": "GFlowOut：使用生成流网络的Dropout",
    "translated_abstract": "贝叶斯推理为解决现代神经网络的许多关键问题（例如不良校准和泛化，以及数据效率）提供了原则性的工具。然而，将贝叶斯推理扩展到大型架构是具有挑战性的并且需要严格的近似。Monte Carlo Dropout已被广泛用作近似推理的相对便宜的方法，并使用深度神经网络估计不确定性。传统上，dropout掩码是从固定分布中独立采样的。最近的研究表明，dropout掩码可以被视为潜在变量，并且可以使用变分推理进行推断。这些方法面临两个重要的挑战：（a）掩码的后验分布可能高度多模态，很难用标准变分推理进行近似；（b）充分利用dropout掩码之间的样本相关信息和相关性以改善后验估计并不是微不足道的。在本研究中，我们提出了GFlowOut，一种使用生成流网络模拟dropout掩码分布的新方法。我们的方法提供了一种灵活且可处理的方法来模拟复杂的掩码后验分布，并可以更好地捕获掩码之间的样本相关性。我们展示了GFlowOut在几个基准数据集上实现了最先进的性能，并提供了良好校准的不确定性估计。",
    "tldr": "GFlowOut是一种使用生成流网络的Dropout方法，可以更好地估计复杂的后验分布和样本相关性，并在几个基准数据集上实现了最先进的性能和良好的校准不确定性估计。",
    "en_tdlr": "GFlowOut is a novel dropout method that uses generative flow networks to better estimate complex posterior distribution and sample correlation, achieving state-of-the-art performance and well-calibrated uncertainty estimation on several benchmark datasets."
}