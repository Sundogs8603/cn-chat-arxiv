{
    "title": "Self-supervised Heterogeneous Graph Pre-training Based on Structural Clustering. (arXiv:2210.10462v2 [cs.LG] UPDATED)",
    "abstract": "Recent self-supervised pre-training methods on Heterogeneous Information Networks (HINs) have shown promising competitiveness over traditional semi-supervised Heterogeneous Graph Neural Networks (HGNNs). Unfortunately, their performance heavily depends on careful customization of various strategies for generating high-quality positive examples and negative examples, which notably limits their flexibility and generalization ability. In this work, we present SHGP, a novel Self-supervised Heterogeneous Graph Pre-training approach, which does not need to generate any positive examples or negative examples. It consists of two modules that share the same attention-aggregation scheme. In each iteration, the Att-LPA module produces pseudo-labels through structural clustering, which serve as the self-supervision signals to guide the Att-HGNN module to learn object embeddings and attention coefficients. The two modules can effectively utilize and enhance each other, promoting the model to learn ",
    "link": "http://arxiv.org/abs/2210.10462",
    "context": "Title: Self-supervised Heterogeneous Graph Pre-training Based on Structural Clustering. (arXiv:2210.10462v2 [cs.LG] UPDATED)\nAbstract: Recent self-supervised pre-training methods on Heterogeneous Information Networks (HINs) have shown promising competitiveness over traditional semi-supervised Heterogeneous Graph Neural Networks (HGNNs). Unfortunately, their performance heavily depends on careful customization of various strategies for generating high-quality positive examples and negative examples, which notably limits their flexibility and generalization ability. In this work, we present SHGP, a novel Self-supervised Heterogeneous Graph Pre-training approach, which does not need to generate any positive examples or negative examples. It consists of two modules that share the same attention-aggregation scheme. In each iteration, the Att-LPA module produces pseudo-labels through structural clustering, which serve as the self-supervision signals to guide the Att-HGNN module to learn object embeddings and attention coefficients. The two modules can effectively utilize and enhance each other, promoting the model to learn ",
    "path": "papers/22/10/2210.10462.json",
    "total_tokens": 878,
    "translated_title": "基于结构聚类的自监督异构图预训练",
    "translated_abstract": "最近，基于自监督的HIN预训练方法在异构信息网络上表现出有希望的竞争力，超过了传统的半监督异构图神经网络。然而，它们的性能严重依赖于各种策略的精细定制，以生成高质量的正例和负例，这明显限制了它们的灵活性和泛化能力。本文提出了一种新颖的自监督异构图预训练方法SHGP，它不需要生成任何正例或负例。它由两个模块组成，共享相同的注意力聚合机制。在每一次迭代中，Att-LPA模块通过结构聚类产生伪标签，作为自监督信号来指导Att-HGNN模块学习对象嵌入和注意力系数。两个模块可以有效地利用和增强彼此，促进模型学习。",
    "tldr": "该论文提出了一种新颖的自监督异构图预训练方法SHGP，它通过结构聚类产生伪标签来指导模型学习对象嵌入和注意力系数，不需要生成任何正例或负例，具有前景应用价值。",
    "en_tdlr": "The paper proposes a novel self-supervised heterogeneous graph pre-training method SHGP which utilizes structural clustering to generate pseudo-labels for guiding the model to learn object embeddings and attention coefficients, without generating any positive or negative examples, and has promising applications."
}