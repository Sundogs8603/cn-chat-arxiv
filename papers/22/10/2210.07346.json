{
    "title": "An Embarrassingly Simple Backdoor Attack on Self-supervised Learning. (arXiv:2210.07346v2 [cs.CR] UPDATED)",
    "abstract": "As a new paradigm in machine learning, self-supervised learning (SSL) is capable of learning high-quality representations of complex data without relying on labels. In addition to eliminating the need for labeled data, research has found that SSL improves the adversarial robustness over supervised learning since lacking labels makes it more challenging for adversaries to manipulate model predictions. However, the extent to which this robustness superiority generalizes to other types of attacks remains an open question.  We explore this question in the context of backdoor attacks. Specifically, we design and evaluate CTRL, an embarrassingly simple yet highly effective self-supervised backdoor attack. By only polluting a tiny fraction of training data (<= 1%) with indistinguishable poisoning samples, CTRL causes any trigger-embedded input to be misclassified to the adversary's designated class with a high probability (>= 99%) at inference time. Our findings suggest that SSL and supervise",
    "link": "http://arxiv.org/abs/2210.07346",
    "context": "Title: An Embarrassingly Simple Backdoor Attack on Self-supervised Learning. (arXiv:2210.07346v2 [cs.CR] UPDATED)\nAbstract: As a new paradigm in machine learning, self-supervised learning (SSL) is capable of learning high-quality representations of complex data without relying on labels. In addition to eliminating the need for labeled data, research has found that SSL improves the adversarial robustness over supervised learning since lacking labels makes it more challenging for adversaries to manipulate model predictions. However, the extent to which this robustness superiority generalizes to other types of attacks remains an open question.  We explore this question in the context of backdoor attacks. Specifically, we design and evaluate CTRL, an embarrassingly simple yet highly effective self-supervised backdoor attack. By only polluting a tiny fraction of training data (<= 1%) with indistinguishable poisoning samples, CTRL causes any trigger-embedded input to be misclassified to the adversary's designated class with a high probability (>= 99%) at inference time. Our findings suggest that SSL and supervise",
    "path": "papers/22/10/2210.07346.json",
    "total_tokens": 1002,
    "translated_title": "一种尴尬简单的自监督学习后门攻击",
    "translated_abstract": "自监督学习（SSL）作为机器学习中的新范式，能够在不依赖标签的情况下学习复杂数据的高质量表示。研究发现，除了消除对有标签数据的依赖外，SSL还提高了对抗性鲁棒性，因为缺乏标签使得对手更难以操纵模型预测。然而，这种鲁棒性优势在其他类型的攻击中是否具有普适性仍然是一个未解之谜。我们在后门攻击的背景下探索了这个问题。具体地，我们设计并评估了CTRL，一种尴尬简单但非常有效的自监督后门攻击。通过仅污染微小比例（<= 1%）的训练数据，CTRL在推理时以高概率（>= 99%）将任何携带触发器的输入错误分类为对手指定的类别。我们的研究结果表明，SSL和监督学习在对抗性攻击中存在一定的差异。",
    "tldr": "这项研究探索了自监督学习后门攻击的问题，提出了一种尴尬简单但高效的攻击方法，通过污染训练数据并插入触发器，使得在推理时任何带有触发器的输入都能以高概率被错误分类到对手指定的类别。这一发现表明，自监督学习和监督学习在对抗性攻击方面存在差异。",
    "en_tdlr": "This study explores the problem of backdoor attacks in self-supervised learning and proposes an embarrassingly simple yet effective attack method. By polluting training data and inserting triggers, any input with a trigger can be misclassified to the adversary's designated class with a high probability at inference time. This finding suggests a difference between self-supervised learning and supervised learning in adversarial attacks."
}