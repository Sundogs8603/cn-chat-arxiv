{
    "title": "Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes. (arXiv:2210.07612v2 [stat.ML] UPDATED)",
    "abstract": "Despite their importance for assessing reliability of predictions, uncertainty quantification (UQ) measures for machine learning models have only recently begun to be rigorously characterized. One prominent issue is the curse of dimensionality: it is commonly believed that the marginal likelihood should be reminiscent of cross-validation metrics and that both should deteriorate with larger input dimensions. We prove that by tuning hyperparameters to maximize marginal likelihood (the empirical Bayes procedure), the performance, as measured by the marginal likelihood, improves monotonically} with the input dimension. On the other hand, we prove that cross-validation metrics exhibit qualitatively different behavior that is characteristic of double descent. Cold posteriors, which have recently attracted interest due to their improved performance in certain settings, appear to exacerbate these phenomena. We verify empirically that our results hold for real data, beyond our considered assump",
    "link": "http://arxiv.org/abs/2210.07612",
    "context": "Title: Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes. (arXiv:2210.07612v2 [stat.ML] UPDATED)\nAbstract: Despite their importance for assessing reliability of predictions, uncertainty quantification (UQ) measures for machine learning models have only recently begun to be rigorously characterized. One prominent issue is the curse of dimensionality: it is commonly believed that the marginal likelihood should be reminiscent of cross-validation metrics and that both should deteriorate with larger input dimensions. We prove that by tuning hyperparameters to maximize marginal likelihood (the empirical Bayes procedure), the performance, as measured by the marginal likelihood, improves monotonically} with the input dimension. On the other hand, we prove that cross-validation metrics exhibit qualitatively different behavior that is characteristic of double descent. Cold posteriors, which have recently attracted interest due to their improved performance in certain settings, appear to exacerbate these phenomena. We verify empirically that our results hold for real data, beyond our considered assump",
    "path": "papers/22/10/2210.07612.json",
    "total_tokens": 857,
    "translated_title": "不确定性估计中的单调性和双下降现象在高斯过程中的应用",
    "translated_abstract": "尽管评估预测的可靠性非常重要，但对于机器学习模型的不确定性量化（UQ）指标直到最近才开始得到严格的表征。一个显著问题是维度诅咒：普遍认为边缘似然应该与交叉验证度量类似，并且两者在输入维度较大时都会恶化。我们证明通过调整超参数以最大化边际似然（经验贝叶斯过程），性能（以边际似然测量）随着输入维度的增加单调改善。另一方面，我们证明交叉验证度量表现出不同的行为特征，即双下降现象。最近因在某些情况下性能提高而受到关注的冷态后验似乎加剧了这些现象。我们经验证实，我们的结果在真实数据上成立，超出我们考虑的假设范围。",
    "tldr": "本研究研究了机器学习模型中不确定性估计的问题，证明了通过调整超参数可以提高边际似然，但交叉验证度量表现出双下降现象。"
}