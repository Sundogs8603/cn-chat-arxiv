{
    "title": "How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization. (arXiv:2210.06441v2 [cs.LG] UPDATED)",
    "abstract": "Despite the clear performance benefits of data augmentations, little is known about why they are so effective. In this paper, we disentangle several key mechanisms through which data augmentations operate. Establishing an exchange rate between augmented and additional real data, we find that in out-of-distribution testing scenarios, augmentations which yield samples that are diverse, but inconsistent with the data distribution can be even more valuable than additional training data. Moreover, we find that data augmentations which encourage invariances can be more valuable than invariance alone, especially on small and medium sized training sets. Following this observation, we show that augmentations induce additional stochasticity during training, effectively flattening the loss landscape.",
    "link": "http://arxiv.org/abs/2210.06441",
    "context": "Title: How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization. (arXiv:2210.06441v2 [cs.LG] UPDATED)\nAbstract: Despite the clear performance benefits of data augmentations, little is known about why they are so effective. In this paper, we disentangle several key mechanisms through which data augmentations operate. Establishing an exchange rate between augmented and additional real data, we find that in out-of-distribution testing scenarios, augmentations which yield samples that are diverse, but inconsistent with the data distribution can be even more valuable than additional training data. Moreover, we find that data augmentations which encourage invariances can be more valuable than invariance alone, especially on small and medium sized training sets. Following this observation, we show that augmentations induce additional stochasticity during training, effectively flattening the loss landscape.",
    "path": "papers/22/10/2210.06441.json",
    "total_tokens": 856,
    "translated_title": "数据扩充到底有多大作用？缩放定律、不变性及隐式正则化的研究",
    "translated_abstract": "尽管数据扩充有明显的性能优势，但我们知道的有关数据扩充为何如此有效的知识还很少。在这篇论文中，我们将分离出数据扩充发挥作用的几个关键机制。在增广数据和实际数据之间建立一个汇率，我们发现在分布外测试场景中，产生多样且与数据分布不一致的增广能够比额外的训练数据更有价值。此外，我们发现旨在鼓励不变性的数据扩充对于小型和中型训练集可能比单一的不变性更有价值。随着这一发现，我们展示了数据增广在训练期间引入额外的随机性，从而有效地拉平了损失函数的曲面。",
    "tldr": "数据扩充针对多样性和不一致性能够比额外的训练数据更有价值，鼓励不变性的数据扩充可以比单一的不变性更有用，数据扩充在训练期间引入了额外的随机性并能够拉平损失函数的曲面。",
    "en_tdlr": "Data augmentations can be more valuable than additional training data for out-of-distribution testing scenarios when yielding diverse but inconsistent samples, and augmentations encouraging invariances can be more valuable than invariance alone on small and medium-sized training sets. Additionally, augmentations induce extra stochasticity that flatten the loss landscape during training."
}