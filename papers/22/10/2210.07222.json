{
    "title": "Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods. (arXiv:2210.07222v2 [cs.CL] UPDATED)",
    "abstract": "Saliency maps can explain a neural model's predictions by identifying important input features. They are difficult to interpret for laypeople, especially for instances with many features. In order to make them more accessible, we formalize the underexplored task of translating saliency maps into natural language and compare methods that address two key challenges of this approach -- what and how to verbalize. In both automatic and human evaluation setups, using token-level attributions from text classification tasks, we compare two novel methods (search-based and instruction-based verbalizations) against conventional feature importance representations (heatmap visualizations and extractive rationales), measuring simulatability, faithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to generate saliency map verbalizations yields plausible explanations which include associations, abstractive summarization and commonsense reasoning, achieving by far the highest human rat",
    "link": "http://arxiv.org/abs/2210.07222",
    "context": "Title: Saliency Map Verbalization: Comparing Feature Importance Representations from Model-free and Instruction-based Methods. (arXiv:2210.07222v2 [cs.CL] UPDATED)\nAbstract: Saliency maps can explain a neural model's predictions by identifying important input features. They are difficult to interpret for laypeople, especially for instances with many features. In order to make them more accessible, we formalize the underexplored task of translating saliency maps into natural language and compare methods that address two key challenges of this approach -- what and how to verbalize. In both automatic and human evaluation setups, using token-level attributions from text classification tasks, we compare two novel methods (search-based and instruction-based verbalizations) against conventional feature importance representations (heatmap visualizations and extractive rationales), measuring simulatability, faithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to generate saliency map verbalizations yields plausible explanations which include associations, abstractive summarization and commonsense reasoning, achieving by far the highest human rat",
    "path": "papers/22/10/2210.07222.json",
    "total_tokens": 904,
    "translated_title": "显著图翻译：模型无关和基于指令方法的特征重要性表示比较。",
    "translated_abstract": "显著图可以通过识别重要的输入特征来解释神经模型的预测。但是，它们很难被非专业人士解释，特别是对于具有许多特征的实例。为了使它们更易于理解，我们规范化了将显著图转换为自然语言的任务，并比较了两种关键挑战的方法：什么和如何表达。在自动和人工评估设置中，利用文本分类任务的标记级属性，我们比较了两种新方法（基于搜索和基于指令的表达）与传统的特征重要性表示（热图可视化和抽取理性），评估可模拟性、忠诚度、帮助性和易理解性。通过指导 GPT-3.5 生成显著图言语化，可以得到合理的解释，包括关联、抽象概括和常识推理，从而取得了迄今最高的人类评分。",
    "tldr": "本研究探索了将显著图转换为自然语言的任务，提出了两种新方法与传统方法的比较，并指导了GPT-3.5生成显著图言语化，得到最高人类评分的结果。",
    "en_tdlr": "This study explores the task of translating saliency maps into natural language and compares two new methods with traditional methods. By instructing GPT-3.5 to generate saliency map verbalizations, the highest human rating was achieved."
}