{
    "title": "Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations. (arXiv:2210.14358v3 [cs.LG] UPDATED)",
    "abstract": "There is an inescapable long-tailed class-imbalance issue in many real-world classification problems. Current methods for addressing this problem only consider scenarios where all examples come from the same distribution. However, in many cases, there are multiple domains with distinct class imbalance. We study this multi-domain long-tailed learning problem and aim to produce a model that generalizes well across all classes and domains. Towards that goal, we introduce TALLY, a method that addresses this multi-domain long-tailed learning problem. Built upon a proposed selective balanced sampling strategy, TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, producing a new representation for use as data augmentation. To improve the disentanglement of semantic representations, TALLY further utilizes a domain-invariant class prototype that averages out domain-specific effects. We evaluate TALLY on several benchmarks and ",
    "link": "http://arxiv.org/abs/2210.14358",
    "context": "Title: Multi-Domain Long-Tailed Learning by Augmenting Disentangled Representations. (arXiv:2210.14358v3 [cs.LG] UPDATED)\nAbstract: There is an inescapable long-tailed class-imbalance issue in many real-world classification problems. Current methods for addressing this problem only consider scenarios where all examples come from the same distribution. However, in many cases, there are multiple domains with distinct class imbalance. We study this multi-domain long-tailed learning problem and aim to produce a model that generalizes well across all classes and domains. Towards that goal, we introduce TALLY, a method that addresses this multi-domain long-tailed learning problem. Built upon a proposed selective balanced sampling strategy, TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, producing a new representation for use as data augmentation. To improve the disentanglement of semantic representations, TALLY further utilizes a domain-invariant class prototype that averages out domain-specific effects. We evaluate TALLY on several benchmarks and ",
    "path": "papers/22/10/2210.14358.json",
    "total_tokens": 958,
    "translated_title": "多领域长尾学习通过增强解缠表示",
    "translated_abstract": "许多现实分类问题中都存在不可避免的长尾类别不平衡问题。目前解决这个问题的方法只考虑所有示例来自同一分布的情况。然而，在许多情况下，存在多个领域具有不同的类别不平衡情况。我们研究了这个多领域长尾学习问题，并旨在产生一个能够在所有类别和领域中都具有良好泛化性能的模型。为实现这一目标，我们引入了TALLY，一种解决多领域长尾学习问题的方法。基于提出的选择性均衡采样策略，TALLY通过将一个示例的语义表示与另一个示例的域相关干扰混合，产生一个用作数据增强的新表示。为了改善语义表示的解缠，TALLY进一步利用一个域不变的类原型来平均掉域特定效应。我们在多个基准测试上评估了TALLY。",
    "tldr": "本研究针对多领域长尾学习问题提出了一种方法TALLY，通过混合示例的语义表示和域相关干扰，使用选择性均衡采样策略进行数据增强，同时利用域不变的类原型改善语义表示的解缠。在多个基准测试中验证了TALLY的有效性。",
    "en_tdlr": "In this study, we propose TALLY, a method for addressing the multi-domain long-tailed learning problem. TALLY achieves this by mixing the semantic representation of one example with the domain-associated nuisances of another, using a selective balanced sampling strategy for data augmentation, and utilizing a domain-invariant class prototype to improve the disentanglement of semantic representations. The effectiveness of TALLY is demonstrated on several benchmarks."
}