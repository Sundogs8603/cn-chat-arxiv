{
    "title": "The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima. (arXiv:2210.01513v2 [cs.LG] UPDATED)",
    "abstract": "We consider Sharpness-Aware Minimization (SAM), a gradient-based optimization method for deep networks that has exhibited performance improvements on image and language prediction problems. We show that when SAM is applied with a convex quadratic objective, for most random initializations it converges to a cycle that oscillates between either side of the minimum in the direction with the largest curvature, and we provide bounds on the rate of convergence.  In the non-quadratic case, we show that such oscillations effectively perform gradient descent, with a smaller step-size, on the spectral norm of the Hessian. In such cases, SAM's update may be regarded as a third derivative -the derivative of the Hessian in the leading eigenvector direction -- that encourages drift toward wider minima.",
    "link": "http://arxiv.org/abs/2210.01513",
    "context": "Title: The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima. (arXiv:2210.01513v2 [cs.LG] UPDATED)\nAbstract: We consider Sharpness-Aware Minimization (SAM), a gradient-based optimization method for deep networks that has exhibited performance improvements on image and language prediction problems. We show that when SAM is applied with a convex quadratic objective, for most random initializations it converges to a cycle that oscillates between either side of the minimum in the direction with the largest curvature, and we provide bounds on the rate of convergence.  In the non-quadratic case, we show that such oscillations effectively perform gradient descent, with a smaller step-size, on the spectral norm of the Hessian. In such cases, SAM's update may be regarded as a third derivative -the derivative of the Hessian in the leading eigenvector direction -- that encourages drift toward wider minima.",
    "path": "papers/22/10/2210.01513.json",
    "total_tokens": 921,
    "translated_title": "锐度感知优化的动态：从峡谷反弹到漂向宽极小值",
    "translated_abstract": "本文研究了一种名为锐度感知优化（SAM）的梯度优化方法，该方法在图像和语言预测问题上表现出较好的性能。我们表明，当使用SAM应用于凸二次目标时，针对大多数随机初始化，它会收敛于在沿着主曲率最大方向的最小值两侧来回振荡的循环，并给出了收敛率的界限。在非二次情况下，我们表明这种振荡实质上是在Hessian矩阵的光谱范数上以更小的步长执行梯度下降，SAM的更新可被认为是一个三阶导数 - 即Hessian矩阵在领先的特征向量方向上的导数，它鼓励朝着更宽的极小值漂移。",
    "tldr": "本文研究了一种梯度优化方法SAM，发现在凸二次目标中它会在最小值两侧来回振荡，但在非二次情况中从光谱范数的角度执行梯度下降，更新方式被认为是Hessian矩阵在领先特征向量方向上的导数，鼓励漂移向更宽的极小值。",
    "en_tdlr": "This paper studies a gradient optimization method called SAM, which oscillates between the two sides of the minimum in convex quadratic objective for most random initializations, and effectively performs gradient descent on the spectral norm of the Hessian in non-quadratic case with a smaller step-size. SAM's update can be regarded as a third derivative in the leading eigenvector direction of the Hessian matrix, which encourages drift towards wider minima."
}