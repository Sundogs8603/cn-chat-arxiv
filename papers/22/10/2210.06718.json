{
    "title": "Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient. (arXiv:2210.06718v3 [cs.LG] UPDATED)",
    "abstract": "We consider a hybrid reinforcement learning setting (Hybrid RL), in which an agent has access to an offline dataset and the ability to collect experience via real-world online interaction. The framework mitigates the challenges that arise in both pure offline and online RL settings, allowing for the design of simple and highly effective algorithms, in both theory and practice. We demonstrate these advantages by adapting the classical Q learning/iteration algorithm to the hybrid setting, which we call Hybrid Q-Learning or Hy-Q. In our theoretical results, we prove that the algorithm is both computationally and statistically efficient whenever the offline dataset supports a high-quality policy and the environment has bounded bilinear rank. Notably, we require no assumptions on the coverage provided by the initial distribution, in contrast with guarantees for policy gradient/iteration methods. In our experimental results, we show that Hy-Q with neural network function approximation outper",
    "link": "http://arxiv.org/abs/2210.06718",
    "total_tokens": 973,
    "translated_title": "混合强化学习：同时使用离线和在线数据可以使强化学习更加高效",
    "translated_abstract": "我们考虑了一种混合强化学习设置（混合RL），其中代理可以通过离线数据集和实时在线交互收集经验。该框架缓解了纯离线和在线RL设置中出现的挑战，允许设计简单且高效的算法，无论是在理论还是实践中。我们通过将经典的Q学习/迭代算法适应于混合设置来展示这些优势，我们称之为混合Q学习或Hy-Q。在我们的理论结果中，我们证明了该算法在离线数据集支持高质量策略且环境具有有界双线性秩的情况下既具有计算效率又具有统计效率。值得注意的是，与策略梯度/迭代方法的保证相反，我们不需要对初始分布提供的覆盖范围做任何假设。在我们的实验结果中，我们展示了使用神经网络函数逼近的Hy-Q的表现优于其他算法。",
    "tldr": "本文提出了一种混合强化学习设置，通过同时使用离线和在线数据，可以设计出简单且高效的算法，我们证明了该算法在离线数据集支持高质量策略且环境具有有界双线性秩的情况下既具有计算效率又具有统计效率。在实验中，我们展示了使用神经网络函数逼近的Hy-Q的表现优于其他算法。",
    "en_tldr": "This paper proposes a hybrid reinforcement learning setting that uses both offline and online data to design simple and efficient algorithms. The authors prove that the algorithm is both computationally and statistically efficient whenever the offline dataset supports a high-quality policy and the environment has bounded bilinear rank. In experiments, they show that the Hy-Q algorithm with neural network function approximation outperforms other algorithms."
}