{
    "title": "Dynamical systems' based neural networks. (arXiv:2210.02373v2 [cs.LG] UPDATED)",
    "abstract": "Neural networks have gained much interest because of their effectiveness in many applications. However, their mathematical properties are generally not well understood. If there is some underlying geometric structure inherent to the data or to the function to approximate, it is often desirable to take this into account in the design of the neural network. In this work, we start with a non-autonomous ODE and build neural networks using a suitable, structure-preserving, numerical time-discretisation. The structure of the neural network is then inferred from the properties of the ODE vector field. Besides injecting more structure into the network architectures, this modelling procedure allows a better theoretical understanding of their behaviour. We present two universal approximation results and demonstrate how to impose some particular properties on the neural networks. A particular focus is on 1-Lipschitz architectures including layers that are not 1-Lipschitz. These networks are expre",
    "link": "http://arxiv.org/abs/2210.02373",
    "context": "Title: Dynamical systems' based neural networks. (arXiv:2210.02373v2 [cs.LG] UPDATED)\nAbstract: Neural networks have gained much interest because of their effectiveness in many applications. However, their mathematical properties are generally not well understood. If there is some underlying geometric structure inherent to the data or to the function to approximate, it is often desirable to take this into account in the design of the neural network. In this work, we start with a non-autonomous ODE and build neural networks using a suitable, structure-preserving, numerical time-discretisation. The structure of the neural network is then inferred from the properties of the ODE vector field. Besides injecting more structure into the network architectures, this modelling procedure allows a better theoretical understanding of their behaviour. We present two universal approximation results and demonstrate how to impose some particular properties on the neural networks. A particular focus is on 1-Lipschitz architectures including layers that are not 1-Lipschitz. These networks are expre",
    "path": "papers/22/10/2210.02373.json",
    "total_tokens": 825,
    "translated_title": "基于动力系统的神经网络",
    "translated_abstract": "神经网络在许多应用中表现出了高效性，但是它们的数学属性通常不太清楚。如果数据或近似函数中存在某种潜在的几何结构，那么在设计神经网络时就往往希望考虑这一点。在本工作中，我们从一个非自治的常微分方程开始，使用适当的、保持结构的数值时间离散化方法构建神经网络。然后，神经网络的结构由常微分方程向量场的性质推导而来。除了在网络架构中注入更多结构之外，这种建模方法还可以更好地理解神经网络的行为。我们提出了两个普适逼近的结果，并演示了如何在神经网络上实施一些特定的属性。特别关注的是包含非1-Lipschitz层的1-Lipschitz架构。这些网络的表达方式更+",
    "tldr": "本文通过基于动力系统的方法设计神经网络，以更好地理解网络的行为，并实现一些特定的属性。主要关注包含非1-Lipschitz层的1-Lipschitz架构。",
    "en_tdlr": "This paper designs neural networks using a dynamical systems-based approach to better understand their behavior and achieve specific properties, with a particular focus on 1-Lipschitz architectures including non-1-Lipschitz layers."
}