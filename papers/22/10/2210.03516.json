{
    "title": "Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery. (arXiv:2210.03516v2 [cs.NE] UPDATED)",
    "abstract": "Deep Reinforcement Learning (RL) has emerged as a powerful paradigm for training neural policies to solve complex control tasks. However, these policies tend to be overfit to the exact specifications of the task and environment they were trained on, and thus do not perform well when conditions deviate slightly or when composed hierarchically to solve even more complex tasks. Recent work has shown that training a mixture of policies, as opposed to a single one, that are driven to explore different regions of the state-action space can address this shortcoming by generating a diverse set of behaviors, referred to as skills, that can be collectively used to great effect in adaptation tasks or for hierarchical planning. This is typically realized by including a diversity term - often derived from information theory - in the objective function optimized by RL. However these approaches often require careful hyperparameter tuning to be effective. In this work, we demonstrate that less widely-",
    "link": "http://arxiv.org/abs/2210.03516",
    "context": "Title: Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery. (arXiv:2210.03516v2 [cs.NE] UPDATED)\nAbstract: Deep Reinforcement Learning (RL) has emerged as a powerful paradigm for training neural policies to solve complex control tasks. However, these policies tend to be overfit to the exact specifications of the task and environment they were trained on, and thus do not perform well when conditions deviate slightly or when composed hierarchically to solve even more complex tasks. Recent work has shown that training a mixture of policies, as opposed to a single one, that are driven to explore different regions of the state-action space can address this shortcoming by generating a diverse set of behaviors, referred to as skills, that can be collectively used to great effect in adaptation tasks or for hierarchical planning. This is typically realized by including a diversity term - often derived from information theory - in the objective function optimized by RL. However these approaches often require careful hyperparameter tuning to be effective. In this work, we demonstrate that less widely-",
    "path": "papers/22/10/2210.03516.json",
    "total_tokens": 947,
    "tldr": "本文比较了深度强化学习和神经进化在解决多技能任务中的表现，结果表明神经进化可以有效地发现多个策略并在适应性任务或分层规划中产生更好的效果，无需进行复杂的超参数调整。"
}