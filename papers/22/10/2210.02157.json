{
    "title": "The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks. (arXiv:2210.02157v2 [stat.ML] UPDATED)",
    "abstract": "It is unclear how changing the learning rule of a deep neural network alters its learning dynamics and representations. To gain insight into the relationship between learned features, function approximation, and the learning rule, we analyze infinite-width deep networks trained with gradient descent (GD) and biologically-plausible alternatives including feedback alignment (FA), direct feedback alignment (DFA), and error modulated Hebbian learning (Hebb), as well as gated linear networks (GLN). We show that, for each of these learning rules, the evolution of the output function at infinite width is governed by a time varying effective neural tangent kernel (eNTK). In the lazy training limit, this eNTK is static and does not evolve, while in the rich mean-field regime this kernel's evolution can be determined self-consistently with dynamical mean field theory (DMFT). This DMFT enables comparisons of the feature and prediction dynamics induced by each of these learning rules. In the lazy ",
    "link": "http://arxiv.org/abs/2210.02157",
    "context": "Title: The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks. (arXiv:2210.02157v2 [stat.ML] UPDATED)\nAbstract: It is unclear how changing the learning rule of a deep neural network alters its learning dynamics and representations. To gain insight into the relationship between learned features, function approximation, and the learning rule, we analyze infinite-width deep networks trained with gradient descent (GD) and biologically-plausible alternatives including feedback alignment (FA), direct feedback alignment (DFA), and error modulated Hebbian learning (Hebb), as well as gated linear networks (GLN). We show that, for each of these learning rules, the evolution of the output function at infinite width is governed by a time varying effective neural tangent kernel (eNTK). In the lazy training limit, this eNTK is static and does not evolve, while in the rich mean-field regime this kernel's evolution can be determined self-consistently with dynamical mean field theory (DMFT). This DMFT enables comparisons of the feature and prediction dynamics induced by each of these learning rules. In the lazy ",
    "path": "papers/22/10/2210.02157.json",
    "total_tokens": 975,
    "translated_title": "学习规则对广泛神经网络表征动力学的影响",
    "translated_abstract": "现在尚不清楚改变深度神经网络的学习规则如何改变其学习动力学和表征。为了深入了解学习特征、函数逼近和学习规则之间的关系，我们分析了无限宽的深度网络，采用了梯度下降(GD)以及生物可行的替代方法，包括反馈对齐(FA)、直接反馈对齐(DFA)、误差调制黑比学习(Hebb)，以及门控线性网络(GLN)进行训练。",
    "tldr": "本论文分析了无限宽度的深度网络，使用不同的学习规则如GD、FA、DFA、Hebb和GLN进行训练，并发现每种规则下的输出函数演化都受到时间变化的有效神经切向核(eNTK)的影响。通过动态均场理论(DMFT)比较了每种学习规则所引起的特征和预测动力学。",
    "en_tdlr": "This paper analyzes infinite-width deep networks trained with different learning rules, such as GD, FA, DFA, Hebb, and GLN, and found that the evolution of the output function for each learning rule is governed by a time-varying effective neural tangent kernel (eNTK). The feature and prediction dynamics induced by each learning rule are compared using dynamical mean field theory (DMFT)."
}