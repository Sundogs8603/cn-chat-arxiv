{
    "title": "Make More of Your Data: Minimal Effort Data Augmentation for Automatic Speech Recognition and Translation. (arXiv:2210.15398v2 [cs.CL] UPDATED)",
    "abstract": "Data augmentation is a technique to generate new training data based on existing data. We evaluate the simple and cost-effective method of concatenating the original data examples to build new training instances. Continued training with such augmented data is able to improve off-the-shelf Transformer and Conformer models that were optimized on the original data only. We demonstrate considerable improvements on the LibriSpeech-960h test sets (WER 2.83 and 6.87 for test-clean and test-other), which carry over to models combined with shallow fusion (WER 2.55 and 6.27). Our method of continued training also leads to improvements of up to 0.9 WER on the ASR part of CoVoST-2 for four non English languages, and we observe that the gains are highly dependent on the size of the original training data. We compare different concatenation strategies and found that our method does not need speaker information to achieve its improvements. Finally, we demonstrate on two datasets that our methods also",
    "link": "http://arxiv.org/abs/2210.15398",
    "context": "Title: Make More of Your Data: Minimal Effort Data Augmentation for Automatic Speech Recognition and Translation. (arXiv:2210.15398v2 [cs.CL] UPDATED)\nAbstract: Data augmentation is a technique to generate new training data based on existing data. We evaluate the simple and cost-effective method of concatenating the original data examples to build new training instances. Continued training with such augmented data is able to improve off-the-shelf Transformer and Conformer models that were optimized on the original data only. We demonstrate considerable improvements on the LibriSpeech-960h test sets (WER 2.83 and 6.87 for test-clean and test-other), which carry over to models combined with shallow fusion (WER 2.55 and 6.27). Our method of continued training also leads to improvements of up to 0.9 WER on the ASR part of CoVoST-2 for four non English languages, and we observe that the gains are highly dependent on the size of the original training data. We compare different concatenation strategies and found that our method does not need speaker information to achieve its improvements. Finally, we demonstrate on two datasets that our methods also",
    "path": "papers/22/10/2210.15398.json",
    "total_tokens": 957,
    "translated_title": "最小化增广，最大化数据：语音识别与翻译中的数据扩充方法研究",
    "translated_abstract": "数据增广是一种根据已有数据生成新的训练数据的技术。本文评估了将原始数据样本串联以构建新的训练实例的简单且经济的方法。继续使用这样的增广数据进行训练能够改进原始数据优化的Transformer和Conformer模型。我们在LibriSpeech-960h测试集上展示了显著的改进（test-clean和test-other的WER分别为2.83和6.87），这些改进也在与浅层融合相结合的模型中得以体现（WER为2.55和6.27）。我们的继续训练方法还在CoVoST-2的四种非英语语言的ASR部分中实现了长达0.9 WER的改进，并且我们观察到这些收益与原始训练数据的大小高度相关。我们比较了不同的串联策略，并发现我们的方法不需要说话人信息即可实现其改进。最后，我们在两个数据集上展示了我们的方法也可适用于翻译任务。",
    "tldr": "本文研究了一种简单、经济的数据扩充方法，即将原始数据样本串联以构建新的训练实例。使用这种方法继续训练能够改进Transformer和Conformer模型，并在多种任务中实现了长达0.9 WER的改进。",
    "en_tdlr": "This paper examines a simple and cost-effective method of data augmentation, which concatenates original data examples to build new training instances. Continued training with augmented data improves Transformer and Conformer models and achieves up to 0.9 WER improvement in multiple tasks."
}