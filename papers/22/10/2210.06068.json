{
    "title": "Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning. (arXiv:2210.06068v2 [cs.CL] UPDATED)",
    "abstract": "Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks. This work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen languages via transfer learning. We carry out an experimental investigation using Meta-AI's MMPLMs ``wmt21-dense-24-wide-en-X and X-en (WMT21fb)'' which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite direction. We fine-tune these MMPLMs towards English-\\textit{Spanish} language pair which \\textit{did not exist at all} in their original pre-trained corpora both implicitly and explicitly. We prepare carefully aligned \\textit{clinical} domain data for this fine-tuning, which is different from their original mixed domain knowledge. Our experimental result shows that the fine-tunin",
    "link": "http://arxiv.org/abs/2210.06068",
    "context": "Title: Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning. (arXiv:2210.06068v2 [cs.CL] UPDATED)\nAbstract: Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks. This work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen languages via transfer learning. We carry out an experimental investigation using Meta-AI's MMPLMs ``wmt21-dense-24-wide-en-X and X-en (WMT21fb)'' which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite direction. We fine-tune these MMPLMs towards English-\\textit{Spanish} language pair which \\textit{did not exist at all} in their original pre-trained corpora both implicitly and explicitly. We prepare carefully aligned \\textit{clinical} domain data for this fine-tuning, which is different from their original mixed domain knowledge. Our experimental result shows that the fine-tunin",
    "path": "papers/22/10/2210.06068.json",
    "total_tokens": 1060,
    "translated_title": "基于转移学习的大规模多语言预训练机器翻译模型在临床领域的应用研究",
    "translated_abstract": "近年来，大规模多语言预训练语言模型（MMPLM）展示了它们在下游任务上表现出的超能力和预知能力。本研究探讨了MMPLM在临床领域机器翻译（MT）中是否可以通过转移学习用于完全未知的语言对。我们使用Meta-AI的MMPLM“wmt21-dense-24-wide-en-X和X-en（WMT21fb）”，这些模型预先训练了7种语言对和14个翻译方向，包括英语到捷克语、德语、豪萨语、冰岛语、日语、俄语和汉语以及相反的方向。我们对这些MMPLM进行fine-tune，针对它们原始的预训练语料库中\\textit{完全不存在}的英文-\\textit{西班牙语}语言对，显式和隐式地进行fine-tune。我们为此 fine-tune 做好经过仔细对齐的\\textit{临床}领域数据，这与它们原始的混合领域知识不同。我们的实验结果表明，fine-tune过的MMPLM在临床领域机器翻译任务中表现良好。",
    "tldr": "本研究探讨了大规模多语言预训练语言模型（MMPLM）是否可以通过转移学习在临床领域机器翻译中成功应用于完全未知的语言对。实验结果表明，fine-tune过的MMPLM在临床领域机器翻译任务中表现良好。",
    "en_tdlr": "This study investigates if massively multilingual pre-trained language models (MMPLMs) can be applied to clinical domain machine translation (MT) for completely unseen language pairs through transfer learning. The experimental results show that fine-tuned MMPLMs perform well in clinical MT tasks."
}