{
    "title": "Benign Autoencoders. (arXiv:2210.00637v3 [cs.LG] UPDATED)",
    "abstract": "Recent progress in Generative Artificial Intelligence (AI) relies on efficient data representations, often featuring encoder-decoder architectures. We formalize the mathematical problem of finding the optimal encoder-decoder pair and characterize its solution, which we name the \"benign autoencoder\" (BAE). We prove that BAE projects data onto a manifold whose dimension is the optimal compressibility dimension of the generative problem. We highlight surprising connections between BAE and several recent developments in AI, such as conditional GANs, context encoders, stable diffusion, stacked autoencoders, and the learning capabilities of generative models. As an illustration, we show how BAE can find optimal, low-dimensional latent representations that improve the performance of a discriminator under a distribution shift. By compressing \"malignant\" data dimensions, BAE leads to smoother and more stable gradients.",
    "link": "http://arxiv.org/abs/2210.00637",
    "context": "Title: Benign Autoencoders. (arXiv:2210.00637v3 [cs.LG] UPDATED)\nAbstract: Recent progress in Generative Artificial Intelligence (AI) relies on efficient data representations, often featuring encoder-decoder architectures. We formalize the mathematical problem of finding the optimal encoder-decoder pair and characterize its solution, which we name the \"benign autoencoder\" (BAE). We prove that BAE projects data onto a manifold whose dimension is the optimal compressibility dimension of the generative problem. We highlight surprising connections between BAE and several recent developments in AI, such as conditional GANs, context encoders, stable diffusion, stacked autoencoders, and the learning capabilities of generative models. As an illustration, we show how BAE can find optimal, low-dimensional latent representations that improve the performance of a discriminator under a distribution shift. By compressing \"malignant\" data dimensions, BAE leads to smoother and more stable gradients.",
    "path": "papers/22/10/2210.00637.json",
    "total_tokens": 871,
    "translated_title": "良性自编码器",
    "translated_abstract": "最近，生成式人工智能取得了很多进展，其中常采用编码器-解码器架构来实现数据的高效表示。本论文正式化了寻找最佳编码器-解码器对的数学问题并表征其解决方案，我们将其命名为“良性自编码器”（BAE）。我们证明BAE将数据投射到一个流型上，其维数为生成问题的最佳可压缩维度。我们强调BAE与人工智能中几个最近发展的方向之间的惊人联系，如有条件的GAN，上下文编码器，稳定扩散，堆叠自编码器和生成模型的学习能力。我们展示了BAE如何找到最优的低维潜在表示，从而在分布转移下提高鉴别器的性能。通过压缩“恶性”数据维度，BAE导致梯度更加平滑和稳定。",
    "tldr": "本文正式化了用于生成式人工智能中编码器-解码器对的最佳选择问题并提出了良性自编码器（BAE），BAE能够将数据投射到最优的流型上，实现了数据压缩和更加稳定的梯度下降。",
    "en_tdlr": "This paper formalizes the mathematical problem of finding the optimal encoder-decoder pair in generative AI, presents a solution named \"benign autoencoder\" (BAE), which can project data onto the optimal manifold for compression and smoother gradient descent. It also highlights the connections between BAE and various recent developments in AI."
}