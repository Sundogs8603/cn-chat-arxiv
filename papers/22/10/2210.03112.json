{
    "title": "A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning. (arXiv:2210.03112v3 [cs.LG] UPDATED)",
    "abstract": "Recent studies in Vision-and-Language Navigation (VLN) train RL agents to execute natural-language navigation instructions in photorealistic environments, as a step towards robots that can follow human instructions. However, given the scarcity of human instruction data and limited diversity in the training environments, these agents still struggle with complex language grounding and spatial language understanding. Pretraining on large text and image-text datasets from the web has been extensively explored but the improvements are limited. We investigate large-scale augmentation with synthetic instructions. We take 500+ indoor environments captured in densely-sampled 360 degree panoramas, construct navigation trajectories through these panoramas, and generate a visually-grounded instruction for each trajectory using Marky, a high-quality multilingual navigation instruction generator. We also synthesize image observations from novel viewpoints using an image-to-image GAN. The resulting d",
    "link": "http://arxiv.org/abs/2210.03112",
    "context": "Title: A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning. (arXiv:2210.03112v3 [cs.LG] UPDATED)\nAbstract: Recent studies in Vision-and-Language Navigation (VLN) train RL agents to execute natural-language navigation instructions in photorealistic environments, as a step towards robots that can follow human instructions. However, given the scarcity of human instruction data and limited diversity in the training environments, these agents still struggle with complex language grounding and spatial language understanding. Pretraining on large text and image-text datasets from the web has been extensively explored but the improvements are limited. We investigate large-scale augmentation with synthetic instructions. We take 500+ indoor environments captured in densely-sampled 360 degree panoramas, construct navigation trajectories through these panoramas, and generate a visually-grounded instruction for each trajectory using Marky, a high-quality multilingual navigation instruction generator. We also synthesize image observations from novel viewpoints using an image-to-image GAN. The resulting d",
    "path": "papers/22/10/2210.03112.json",
    "total_tokens": 929,
    "translated_title": "一条新路: 用合成指令和模仿学习扩展视觉语言导航模型的规模",
    "translated_abstract": "最近在视觉语言导航（VLN）中的研究使用强化学习代理在逼真的环境中执行自然语言导航指令，以实现机器人遵循人类指令的目标。然而，研究表明由于人类指令数据稀缺且训练环境缺乏多样性，这些代理仍然难以理解复杂的语言和空间语言。我们调查了使用合成指令的大规模扩充。我们利用Marky，一种高品质的多语言导航指令生成器，创建了500多个室内环境，通过这些全景图构建导航轨迹，并为每个轨迹生成了一个基于图像的指令。我们还使用图像到图像GAN在新的视角上合成图像观察。通过这些方法得到了更强的视觉语言导航模型。",
    "tldr": "该论文研究了使用合成指令的大规模扩充方法，通过构建导航轨迹并使用高质量的多语言导航指令生成器Marky生成基于图像的指令，以及使用图像到图像GAN在新的视角上合成图像观察。这些方法得到了更强的视觉语言导航模型。",
    "en_tdlr": "This paper investigates large-scale augmentation with synthetic instructions to improve Vision-and-Language Navigation (VLN) models, which involves constructing navigation trajectories and generating visually-grounded instructions using a high-quality multilingual navigation instruction generator, Marky, and synthesizing image observations from novel viewpoints using an image-to-image GAN. The resulting models show improved language grounding and spatial language understanding."
}