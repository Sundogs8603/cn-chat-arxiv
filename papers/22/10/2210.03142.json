{
    "title": "On Distillation of Guided Diffusion Models. (arXiv:2210.03142v3 [cs.CV] UPDATED)",
    "abstract": "Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALLE-2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate im",
    "link": "http://arxiv.org/abs/2210.03142",
    "context": "Title: On Distillation of Guided Diffusion Models. (arXiv:2210.03142v3 [cs.CV] UPDATED)\nAbstract: Classifier-free guided diffusion models have recently been shown to be highly effective at high-resolution image generation, and they have been widely used in large-scale diffusion frameworks including DALLE-2, Stable Diffusion and Imagen. However, a downside of classifier-free guided diffusion models is that they are computationally expensive at inference time since they require evaluating two diffusion models, a class-conditional model and an unconditional model, tens to hundreds of times. To deal with this limitation, we propose an approach to distilling classifier-free guided diffusion models into models that are fast to sample from: Given a pre-trained classifier-free guided model, we first learn a single model to match the output of the combined conditional and unconditional models, and then we progressively distill that model to a diffusion model that requires much fewer sampling steps. For standard diffusion models trained on the pixel-space, our approach is able to generate im",
    "path": "papers/22/10/2210.03142.json",
    "total_tokens": 894,
    "translated_title": "关于引导式扩散模型的蒸馏",
    "translated_abstract": "最近，免分类器的引导式扩散模型已被证明在高分辨率图像生成方面非常有效，它们已广泛用于包括 DALLE-2、Stable Diffusion 和 Imagen 在内的大规模扩散框架中。然而，免分类器的引导式扩散模型的缺点是，在推断时计算成本很高，因为需要评估两个扩散模型（一个类有条件的模型和一个无条件的模型）数十到数百次。为了解决这个限制，我们提出了一种将免分类器的引导式扩散模型蒸馏为易于采样的模型的方法: 给定一个经过预训练的免分类器引导模型，我们首先学习一个单一的模型以匹配联合有条件和无条件模型的输出，然后逐步将该模型蒸馏到只需要更少的采样步骤的扩散模型。对于在像素空间训练的标准扩散模型，我们的方法能够生成高质量的图像。",
    "tldr": "本论文提出了一种将免分类器的引导式扩散模型蒸馏为易于采样的模型的方法，以降低在推断时的计算成本，并且能够在像素空间生成高质量的图像。",
    "en_tdlr": "This paper proposes an approach to distill classifier-free guided diffusion models into models that are fast to sample from, in order to reduce the computational cost during inference, and it is able to generate high-quality images in pixel-space."
}