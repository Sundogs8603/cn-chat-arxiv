{
    "title": "On the Power of Pre-training for Generalization in RL: Provable Benefits and Hardness. (arXiv:2210.10464v2 [cs.LG] UPDATED)",
    "abstract": "Generalization in Reinforcement Learning (RL) aims to learn an agent during training that generalizes to the target environment. This paper studies RL generalization from a theoretical aspect: how much can we expect pre-training over training environments to be helpful? When the interaction with the target environment is not allowed, we certify that the best we can obtain is a near-optimal policy in an average sense, and we design an algorithm that achieves this goal. Furthermore, when the agent is allowed to interact with the target environment, we give a surprising result showing that asymptotically, the improvement from pre-training is at most a constant factor. On the other hand, in the non-asymptotic regime, we design an efficient algorithm and prove a distribution-based regret bound in the target environment that is independent of the state-action space.",
    "link": "http://arxiv.org/abs/2210.10464",
    "context": "Title: On the Power of Pre-training for Generalization in RL: Provable Benefits and Hardness. (arXiv:2210.10464v2 [cs.LG] UPDATED)\nAbstract: Generalization in Reinforcement Learning (RL) aims to learn an agent during training that generalizes to the target environment. This paper studies RL generalization from a theoretical aspect: how much can we expect pre-training over training environments to be helpful? When the interaction with the target environment is not allowed, we certify that the best we can obtain is a near-optimal policy in an average sense, and we design an algorithm that achieves this goal. Furthermore, when the agent is allowed to interact with the target environment, we give a surprising result showing that asymptotically, the improvement from pre-training is at most a constant factor. On the other hand, in the non-asymptotic regime, we design an efficient algorithm and prove a distribution-based regret bound in the target environment that is independent of the state-action space.",
    "path": "papers/22/10/2210.10464.json",
    "total_tokens": 946,
    "translated_title": "关于预训练在RL泛化中的能力：可证明的好处和困难",
    "translated_abstract": "强化学习（RL）中的泛化目标是在训练期间学习一个能够适用于目标环境的代理。本文从理论的角度研究了RL泛化：我们可以期望通过在训练环境上的预训练对泛化有多大的帮助？当与目标环境的交互不允许时，我们证明我们最多可以获得的是一个近乎最优的策略，同时设计了一个可以实现这一目标的算法。此外，当允许代理与目标环境进行交互时，我们得到了一个令人惊讶的结果，即从预训练中的改进在渐近意义下最多只有一个常数因子。另一方面，在非渐近情况下，我们设计了一个高效的算法，并证明了与状态动作空间无关的目标环境基于分布的遗憾界。",
    "tldr": "本文研究了强化学习中预训练的泛化能力。当与目标环境交互不允许时，最好的结果是接近最优的策略；当允许交互时，预训练的改进最多是一个常数因子。在非渐近情况下，我们设计了一个高效算法，并证明了与状态动作空间无关的目标环境遗憾界。",
    "en_tdlr": "This paper investigates the generalization ability of pre-training in reinforcement learning (RL). When interaction with the target environment is not allowed, the best result is a near-optimal policy. When interaction is allowed, the improvement from pre-training is at most a constant factor. In the non-asymptotic regime, an efficient algorithm is designed with a distribution-based regret bound independent of the state-action space."
}