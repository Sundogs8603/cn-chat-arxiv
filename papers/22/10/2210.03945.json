{
    "title": "Understanding HTML with Large Language Models. (arXiv:2210.03945v2 [cs.LG] UPDATED)",
    "abstract": "Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniW",
    "link": "http://arxiv.org/abs/2210.03945",
    "context": "Title: Understanding HTML with Large Language Models. (arXiv:2210.03945v2 [cs.LG] UPDATED)\nAbstract: Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniW",
    "path": "papers/22/10/2210.03945.json",
    "total_tokens": 986,
    "translated_title": "用大型语言模型理解HTML",
    "translated_abstract": "大型语言模型（LLMs）在各种自然语言任务中表现出了卓越的性能。然而，它们在HTML理解方面的能力——即解析网页的原始HTML，应用于自动化网络任务、爬取和浏览器辅助检索等方面——尚未得到完全的探索。我们提出了HTML理解模型（微调LLMs），并深入分析了它们在三个任务下的能力：（i）HTML元素的语义分类，（ii）HTML输入的描述生成，以及（iii）HTML页面的自主网络导航。虽然先前的工作已经为HTML理解开发了专用的架构和训练程序，但我们表明，预训练于标准自然语言语料库的LLMs非常适用于HTML理解任务。例如，微调后的LLMs在语义分类方面比仅基于任务数据集训练的模型准确率高12%。此外，当它们被微调于MiniW的数据时，LLMs的描述生成在人类主观质量评估中表现出与基于Transformer编码器—解码器的基线模型相当的质量，而且它们能够成功地自主地浏览HTML页面，执行各种任务。",
    "tldr": "本研究使用大型语言模型探索了对HTML的理解，提出了HTML理解模型，通过微调使其在语义分类、描述生成和自主网络导航三个任务上表现出良好的性能，显示出大型语言模型在HTML任务上表现出色。",
    "en_tdlr": "This study explores the understanding of HTML using large language models (LLMs), proposes HTML understanding models through fine-tuning LLMs, and demonstrates their excellent performance in tasks such as semantic classification, description generation, and autonomous web navigation. The research shows that LLMs pretrained on natural language corpora transfer remarkably well to HTML understanding tasks, emphasizing the potential of LLMs in the field of web-based tasks."
}