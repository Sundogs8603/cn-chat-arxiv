{
    "title": "CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation. (arXiv:2210.03919v4 [cs.CV] UPDATED)",
    "abstract": "Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by relevant prompts to capture specific image characteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based image manipulation algorithm. To demo",
    "link": "http://arxiv.org/abs/2210.03919",
    "context": "Title: CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation. (arXiv:2210.03919v4 [cs.CV] UPDATED)\nAbstract: Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by relevant prompts to capture specific image characteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based image manipulation algorithm. To demo",
    "path": "papers/22/10/2210.03919.json",
    "total_tokens": 969,
    "translated_title": "CLIP-PAE：投影增强嵌入以提取相关特征用于可分离、可解释、可控的文本指导脸部操纵",
    "translated_abstract": "最近引入的对比语言-图像预训练（CLIP）将图像和文本嵌入到共同的潜在空间中。这打开了一个大门，即旨在通过提供文字说明来操作输入图像的丰富文学资料。然而，由于联合空间中图像和文本嵌入之间的差异，将文本嵌入作为优化目标通常会导致结果图像中出现意外的伪影。对于操纵来说，可分离性、可解释性和可控性也很难保证。为了缓解这些问题，我们提出定义由相关提示展开的语料库子空间来捕获特定的图像特征。我们引入了CLIP投影增强嵌入（PAE）作为优化目标，以提高文本引导图像操纵的性能。我们的方法是一种简单而通用的范例，可以轻松地计算和适应，并平稳地融入到任何基于CLIP的图像操作算法中。",
    "tldr": "提出了一种为了解决文本引导图像操纵中的可分离性、可解释性和可控性问题，通过定义基于相关提示的语料库子空间来获取特定图像特征并引入CLIP投影增强嵌入（PAE）作为优化目标处理的新方法。",
    "en_tdlr": "A new method is proposed to solve the problems of disentanglement, interpretability, and controllability in text-guided image manipulation, which defines corpus subspaces spanned by relevant prompts to capture specific image characteristics and introduces CLIP Projection-Augmentation Embedding (PAE) as an optimization target."
}