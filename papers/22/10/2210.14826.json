{
    "title": "tf.data service: A Case for Disaggregating ML Input Data Processing. (arXiv:2210.14826v3 [cs.LG] UPDATED)",
    "abstract": "Machine learning (ML) computations commonly execute on expensive specialized hardware, such as GPUs and TPUs, which provide high FLOPs and performance-per-watt. For cost efficiency, it is essential to keep these accelerators highly utilized. This requires preprocessing input data at the rate at which the accelerators can ingest and perform ML computations on the data. To avoid data stalls, the host CPU and RAM required for input data processing per accelerator core used for ML computations varies across jobs. Hence, the traditional approach of processing input data on ML accelerator hosts with a fixed hardware ratio leads to either under-utilizing the accelerators or the host CPU and RAM. In this paper, we address these concerns by building a disaggregated ML data processing system.  We present tf.data service, an open-source disaggregated input data processing service built on top of tf.data in TensorFlow. We show that disaggregating data preprocessing has three key advantages for lar",
    "link": "http://arxiv.org/abs/2210.14826",
    "context": "Title: tf.data service: A Case for Disaggregating ML Input Data Processing. (arXiv:2210.14826v3 [cs.LG] UPDATED)\nAbstract: Machine learning (ML) computations commonly execute on expensive specialized hardware, such as GPUs and TPUs, which provide high FLOPs and performance-per-watt. For cost efficiency, it is essential to keep these accelerators highly utilized. This requires preprocessing input data at the rate at which the accelerators can ingest and perform ML computations on the data. To avoid data stalls, the host CPU and RAM required for input data processing per accelerator core used for ML computations varies across jobs. Hence, the traditional approach of processing input data on ML accelerator hosts with a fixed hardware ratio leads to either under-utilizing the accelerators or the host CPU and RAM. In this paper, we address these concerns by building a disaggregated ML data processing system.  We present tf.data service, an open-source disaggregated input data processing service built on top of tf.data in TensorFlow. We show that disaggregating data preprocessing has three key advantages for lar",
    "path": "papers/22/10/2210.14826.json",
    "total_tokens": 847,
    "translated_title": "tf.data服务：拆分机器学习输入数据处理的案例",
    "translated_abstract": "机器学习中的计算通常在昂贵的专用硬件上执行，如GPU和TPU，它们提供高FLOP和每瓦性能。为了成本效益，必须保持这些加速器的高利用率。这需要以加速器可以接收和执行数据的速率预处理输入数据。为了避免数据停顿，用于机器学习计算的加速器核心所需的主机CPU和RAM在不同的作业中是可变的。因此，传统的在具有固定硬件比例的ML加速器主机上处理输入数据的方法会导致加速器或主机CPU和RAM的低利用率。在本文中，我们通过构建一个拆分的ML数据处理系统来解决这些问题。我们介绍了tf.data service，这是一个建立在TensorFlow的tf.data之上的开源拆分输入数据处理服务。我们展示了将数据预处理拆分的三个关键优势。",
    "tldr": "本文介绍了一个拆分机器学习输入数据处理的案例。通过构建tf.data服务，可以实现数据预处理的拆分，以提高加速器和主机资源的利用率。",
    "en_tdlr": "This paper presents a case for disaggregating ML input data processing. The authors propose tf.data service, an open-source disaggregated input data processing service built on top of tf.data in TensorFlow, which allows for higher utilization of accelerators and host resources."
}