{
    "title": "Adaptive Top-K in SGD for Communication-Efficient Distributed Learning. (arXiv:2210.13532v2 [cs.LG] UPDATED)",
    "abstract": "Distributed stochastic gradient descent (SGD) with gradient compression has become a popular communication-efficient solution for accelerating distributed learning. One commonly used method for gradient compression is Top-K sparsification, which sparsifies the gradients by a fixed degree during model training. However, there has been a lack of an adaptive approach to adjust the sparsification degree to maximize the potential of the model's performance or training speed. This paper proposes a novel adaptive Top-K in SGD framework that enables an adaptive degree of sparsification for each gradient descent step to optimize the convergence performance by balancing the trade-off between communication cost and convergence error. Firstly, an upper bound of convergence error is derived for the adaptive sparsification scheme and the loss function. Secondly, an algorithm is designed to minimize the convergence error under the communication cost constraints. Finally, numerical results on the MNIS",
    "link": "http://arxiv.org/abs/2210.13532",
    "context": "Title: Adaptive Top-K in SGD for Communication-Efficient Distributed Learning. (arXiv:2210.13532v2 [cs.LG] UPDATED)\nAbstract: Distributed stochastic gradient descent (SGD) with gradient compression has become a popular communication-efficient solution for accelerating distributed learning. One commonly used method for gradient compression is Top-K sparsification, which sparsifies the gradients by a fixed degree during model training. However, there has been a lack of an adaptive approach to adjust the sparsification degree to maximize the potential of the model's performance or training speed. This paper proposes a novel adaptive Top-K in SGD framework that enables an adaptive degree of sparsification for each gradient descent step to optimize the convergence performance by balancing the trade-off between communication cost and convergence error. Firstly, an upper bound of convergence error is derived for the adaptive sparsification scheme and the loss function. Secondly, an algorithm is designed to minimize the convergence error under the communication cost constraints. Finally, numerical results on the MNIS",
    "path": "papers/22/10/2210.13532.json",
    "total_tokens": 899,
    "translated_title": "自适应的SGD中的Top-K方法用于思降低通信成本的分布式学习",
    "translated_abstract": "分布式随机梯度下降(SGD)与梯度压缩已成为一种流行的降低通信成本的分布式学习加速解决方案。梯度压缩的常用方法是Top-K稀疏化，它在模型训练过程中通过固定的程度稀疏化梯度。然而，缺乏一种自适应的方法来调整稀疏化程度以最大化模型性能或训练速度的潜力。本文提出了一种新颖的自适应SGD中的Top-K框架，它可以为每个梯度下降步骤提供自适应稀疏化程度，通过平衡通信成本和收敛误差之间的权衡来优化收敛性能。首先，推导了自适应稀疏化方案和损失函数的收敛误差上界。其次，设计了一种算法来在通信成本约束下最小化收敛误差。最后，给出了在MNIS数据集上的数值实验结果。",
    "tldr": "这篇论文提出了一种自适应的Top-K方法用于降低通信成本的分布式学习，通过自适应调整稀疏化程度来优化收敛性能，在数值实验中取得了良好的结果。",
    "en_tdlr": "This paper proposes an adaptive Top-K method for communication-efficient distributed learning, which optimizes convergence performance by adjusting the sparsification degree adaptively, achieving good results in numerical experiments."
}