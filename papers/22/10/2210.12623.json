{
    "title": "Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings. (arXiv:2210.12623v2 [cs.CL] UPDATED)",
    "abstract": "Zero-resource cross-lingual transfer approaches aim to apply supervised models from a source language to unlabelled target languages. In this paper we perform an in-depth study of the two main techniques employed so far for cross-lingual zero-resource sequence labelling, based either on data or model transfer. Although previous research has proposed translation and annotation projection (data-based cross-lingual transfer) as an effective technique for cross-lingual sequence labelling, in this paper we experimentally demonstrate that high capacity multilingual language models applied in a zero-shot (model-based cross-lingual transfer) setting consistently outperform data-based cross-lingual transfer approaches. A detailed analysis of our results suggests that this might be due to important differences in language use. More specifically, machine translation often generates a textual signal which is different to what the models are exposed to when using gold standard data, which affects b",
    "link": "http://arxiv.org/abs/2210.12623",
    "context": "Title: Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings. (arXiv:2210.12623v2 [cs.CL] UPDATED)\nAbstract: Zero-resource cross-lingual transfer approaches aim to apply supervised models from a source language to unlabelled target languages. In this paper we perform an in-depth study of the two main techniques employed so far for cross-lingual zero-resource sequence labelling, based either on data or model transfer. Although previous research has proposed translation and annotation projection (data-based cross-lingual transfer) as an effective technique for cross-lingual sequence labelling, in this paper we experimentally demonstrate that high capacity multilingual language models applied in a zero-shot (model-based cross-lingual transfer) setting consistently outperform data-based cross-lingual transfer approaches. A detailed analysis of our results suggests that this might be due to important differences in language use. More specifically, machine translation often generates a textual signal which is different to what the models are exposed to when using gold standard data, which affects b",
    "path": "papers/22/10/2210.12623.json",
    "total_tokens": 924,
    "translated_abstract": "零资源交叉语言转移方法致力于将源语言的监督模型应用于未标记的目标语言。本文对目前用于跨语言零资源序列标注的两种主要技术——基于数据或模型的转移——进行了深入研究。虽然先前的研究提出翻译和注释映射（基于数据的跨语言转移）作为跨语言序列标注的一种有效技术，但本文的实验证明，在零-shot（基于模型的跨语言转移）设置下应用高容量多语言语言模型始终优于基于数据的跨语言转移方法。对我们结果的详细分析表明，这可能是由于语言使用上的重要差异所致。更具体地说，机器翻译通常会生成与使用黄金标准数据时模型接触到的文本信号不同的文本信号，这影响了基于数据的方法的性能。",
    "tldr": "本文研究了两种零资源交叉语言序列标注的主要技术：基于数据或模型的转移。实验证明，高容量多语言语言模型在零-shot设置下的应用始终优于基于数据的跨语言转移方法。这可能是源于语言使用上的重要差异。",
    "en_tdlr": "This paper presents an in-depth study of two main techniques, data or model transfer, for cross-lingual zero-resource sequence labeling. Experimental results demonstrate that high-capacity multilingual language models consistently outperform data-based cross-lingual transfer approaches in a zero-shot setting, which may be due to important differences in language use."
}