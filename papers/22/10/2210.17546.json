{
    "title": "Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v2 [cs.LG] UPDATED)",
    "abstract": "Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works -- and some recently deployed defenses -- focus on \"verbatim memorization\", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense based on Bloom filters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this \"perfect\" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified \"style-transfer\" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. For example, instructing the model to output ALL-CA",
    "link": "http://arxiv.org/abs/2210.17546",
    "context": "Title: Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v2 [cs.LG] UPDATED)\nAbstract: Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works -- and some recently deployed defenses -- focus on \"verbatim memorization\", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense based on Bloom filters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this \"perfect\" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified \"style-transfer\" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. For example, instructing the model to output ALL-CA",
    "path": "papers/22/10/2210.17546.json",
    "total_tokens": 1058,
    "translated_title": "防止神经语言模型的逐字记忆会产生虚假隐私保护感",
    "translated_abstract": "通过研究神经语言模型中数据记忆的现象，本研究帮助我们理解与隐私或版权相关的风险，并有助于评估对策。然而逐字记忆定义过于严格，未能捕捉更为微妙的记忆形式。本文基于布隆过滤器设计并实现了一种有效的防御方法，但该“完美”过滤器并不能防止训练数据泄露。",
    "tldr": "防止神经语言模型逐字记忆无法真正保护隐私，本文设计的布隆过滤器虽然防止了所有逐字记忆，但仍然无法防止训练数据泄露，容易被合理修改的“样式转换”提示绕过。",
    "en_tdlr": "Preventing verbatim memorization in language models is not an effective way to protect privacy, as it fails to capture more subtle forms of memorization. A defense based on Bloom filters is efficient in preventing all verbatim memorization, but it is still easily circumvented by plausible and minimally modified \"style-transfer\" prompts and does not prevent the leakage of training data."
}