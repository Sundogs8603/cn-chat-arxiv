{
    "title": "ContraCLM: Contrastive Learning For Causal Language Model. (arXiv:2210.01185v2 [cs.CL] UPDATED)",
    "abstract": "Despite exciting progress in causal language models, the expressiveness of the representations is largely limited due to poor discrimination ability. To remedy this issue, we present ContraCLM, a novel contrastive learning framework at both token-level and sequence-level. We assess ContraCLM on a variety of downstream tasks. We show that ContraCLM enhances discrimination of the representations and bridges the gap with the encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain $44\\%$ relative improvement on the Semantic Textual Similarity tasks and $34\\%$ on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of the representations, ContraCLM also boosts the source code generation capability with $9\\%$ relative improvement on execution accuracy on the HumanEval benchmark.",
    "link": "http://arxiv.org/abs/2210.01185",
    "context": "Title: ContraCLM: Contrastive Learning For Causal Language Model. (arXiv:2210.01185v2 [cs.CL] UPDATED)\nAbstract: Despite exciting progress in causal language models, the expressiveness of the representations is largely limited due to poor discrimination ability. To remedy this issue, we present ContraCLM, a novel contrastive learning framework at both token-level and sequence-level. We assess ContraCLM on a variety of downstream tasks. We show that ContraCLM enhances discrimination of the representations and bridges the gap with the encoder-only models, which makes causal language models better suited for tasks beyond language generation. Specifically, we attain $44\\%$ relative improvement on the Semantic Textual Similarity tasks and $34\\%$ on Code-to-Code Search tasks. Furthermore, by improving the expressiveness of the representations, ContraCLM also boosts the source code generation capability with $9\\%$ relative improvement on execution accuracy on the HumanEval benchmark.",
    "path": "papers/22/10/2210.01185.json",
    "total_tokens": 841,
    "translated_title": "ContraCLM：因果语言模型的对比学习",
    "translated_abstract": "尽管因果语言模型取得了令人兴奋的进展，但其表示的表现力受到较差的区分能力的限制。为了解决这个问题，我们提出了一种新颖的对比学习框架 ContraCLM，它在标记级别和序列级别上进行对比学习。我们在各种下游任务上评估了 ContraCLM，并证明了它增强了表示的区分性，并弥合了仅编码器模型的差距，这使得因果语言模型更适合于超出语言生成的任务。具体而言，在语义文本相似性任务上，我们获得了44%的相对改进，在代码对代码搜索任务上获得了34%的改进。此外，通过提高表示的表现力，ContraCLM 还提高了源代码生成能力，在 HumanEval 基准测试中执行精度相对提高了9%。",
    "tldr": "ContraCLM是一种对比学习框架，可增强因果语言模型的表示区分性并适用于超出语言生成的任务。在多个下游任务中，相对于其他模型，ContraCLM获得了显著的性能改进。"
}