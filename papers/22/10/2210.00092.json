{
    "title": "Federated Training of Dual Encoding Models on Small Non-IID Client Datasets. (arXiv:2210.00092v2 [cs.LG] UPDATED)",
    "abstract": "Dual encoding models that encode a pair of inputs are widely used for representation learning. Many approaches train dual encoding models by maximizing agreement between pairs of encodings on centralized training data. However, in many scenarios, datasets are inherently decentralized across many clients (user devices or organizations) due to privacy concerns, motivating federated learning. In this work, we focus on federated training of dual encoding models on decentralized data composed of many small, non-IID (independent and identically distributed) client datasets. We show that existing approaches that work well in centralized settings perform poorly when naively adapted to this setting using federated averaging. We observe that, we can simulate large-batch loss computation on individual clients for loss functions that are based on encoding statistics. Based on this insight, we propose a novel federated training approach, Distributed Cross Correlation Optimization (DCCO), which trai",
    "link": "http://arxiv.org/abs/2210.00092",
    "context": "Title: Federated Training of Dual Encoding Models on Small Non-IID Client Datasets. (arXiv:2210.00092v2 [cs.LG] UPDATED)\nAbstract: Dual encoding models that encode a pair of inputs are widely used for representation learning. Many approaches train dual encoding models by maximizing agreement between pairs of encodings on centralized training data. However, in many scenarios, datasets are inherently decentralized across many clients (user devices or organizations) due to privacy concerns, motivating federated learning. In this work, we focus on federated training of dual encoding models on decentralized data composed of many small, non-IID (independent and identically distributed) client datasets. We show that existing approaches that work well in centralized settings perform poorly when naively adapted to this setting using federated averaging. We observe that, we can simulate large-batch loss computation on individual clients for loss functions that are based on encoding statistics. Based on this insight, we propose a novel federated training approach, Distributed Cross Correlation Optimization (DCCO), which trai",
    "path": "papers/22/10/2210.00092.json",
    "total_tokens": 1040,
    "translated_title": "小型非独立同分布客户数据上的双重编码模型联邦训练",
    "translated_abstract": "双重编码模型广泛用于表示学习，通过最大化中心化训练数据中编码对的一致性来训练。然而，在许多情况下，由于隐私问题，数据集在许多客户端（用户设备或组织）上本质上是分散的，这促进了联邦学习。本文关注在许多小型非独立同分布的客户数据上实现双重编码模型的联邦训练。我们发现现有的在中心化环境中表现良好的方法在使用联合平均方法进行简单改进时表现糟糕。我们观察到，对于基于编码统计的损失函数，可以在各个客户端上模拟大批量损失计算。基于这一洞见，我们提出了一种新的联邦训练方法，分布式交叉相关优化（DCCO），它使用交叉相关损失和客户端更新的联邦平均来训练双重编码模型。我们在两个机器学习任务上评估了我们的方法，并证明它相对于现有的联邦学习方法在提高性能方面更有效。",
    "tldr": "本论文研究了在小型非独立同分布客户数据上的双重编码模型的联邦训练问题，提出了一种叫做DCCO的方法，通过联邦平均和交叉相关损失来训练双重编码模型，在两个机器学习任务中证明了该方法有效性的提高。"
}