{
    "title": "Benchmarking Long-tail Generalization with Likelihood Splits. (arXiv:2210.06799v2 [cs.CL] UPDATED)",
    "abstract": "In order to reliably process natural language, NLP systems must generalize to the long tail of rare utterances. We propose a method to create challenging benchmarks that require generalizing to the tail of the distribution by re-splitting existing datasets. We create 'Likelihood Splits' where examples that are assigned lower likelihood by a pre-trained language model (LM) are placed in the test set, and more likely examples are in the training set. This simple approach can be customized to construct meaningful train-test splits for a wide range of tasks. Likelihood Splits surface more challenges than random splits: relative error rates of state-of-the-art models increase by 59% for semantic parsing on Spider, 93% for natural language inference on SNLI, and 33% for yes/no question answering on BoolQ, on our splits compared with the corresponding random splits. Moreover, Likelihood Splits create fairer benchmarks than adversarial filtering; when the LM used to create the splits is also e",
    "link": "http://arxiv.org/abs/2210.06799",
    "context": "Title: Benchmarking Long-tail Generalization with Likelihood Splits. (arXiv:2210.06799v2 [cs.CL] UPDATED)\nAbstract: In order to reliably process natural language, NLP systems must generalize to the long tail of rare utterances. We propose a method to create challenging benchmarks that require generalizing to the tail of the distribution by re-splitting existing datasets. We create 'Likelihood Splits' where examples that are assigned lower likelihood by a pre-trained language model (LM) are placed in the test set, and more likely examples are in the training set. This simple approach can be customized to construct meaningful train-test splits for a wide range of tasks. Likelihood Splits surface more challenges than random splits: relative error rates of state-of-the-art models increase by 59% for semantic parsing on Spider, 93% for natural language inference on SNLI, and 33% for yes/no question answering on BoolQ, on our splits compared with the corresponding random splits. Moreover, Likelihood Splits create fairer benchmarks than adversarial filtering; when the LM used to create the splits is also e",
    "path": "papers/22/10/2210.06799.json",
    "total_tokens": 954,
    "translated_title": "使用可能性分割进行长尾概括基准测试",
    "translated_abstract": "为了可靠地处理自然语言，NLP系统必须推广到稀有语句的长尾部分。我们提出了一种方法来创建需要推广到分布尾部的具有挑战性的基准测试，即通过重新划分现有数据集来实现。我们创建了“可能性分割”，即将由预训练语言模型（LM）分配较低可能性的实例放置在测试集中，而更可能的实例则在训练集中。这种简单的方法可以自定义，以构建适合各种任务的有意义的训练-测试分割。相对于随机分割，可能性分割表现出比随机分割更多的挑战：在Spider上进行的语义解析的最先进模型的相对误差率增加了59％、在SNLI上进行的自然语言推理的相对误差率增加了93％、在BoolQ上进行的是/否问题回答的相对误差率增加了33％。此外，可能性分割创建比对抗过滤更公平的基准测试；当用于创建分割的LM也是检测器时，几乎不会影响原始模型的性能。",
    "tldr": "为了可靠地处理自然语言，NLP系统需要推广长尾稀有语句。这篇论文提出了一种使用概率分割来创建有意义测试数据集的方法，引入了更多的挑战性并可能提高在关键任务上的错误率。"
}