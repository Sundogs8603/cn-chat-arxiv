{
    "title": "Block-wise Training of Residual Networks via the Minimizing Movement Scheme. (arXiv:2210.00949v2 [cs.LG] UPDATED)",
    "abstract": "End-to-end backpropagation has a few shortcomings: it requires loading the entire model during training, which can be impossible in constrained settings, and suffers from three locking problems (forward locking, update locking and backward locking), which prohibit training the layers in parallel. Solving layer-wise optimization problems can address these problems and has been used in on-device training of neural networks. We develop a layer-wise training method, particularly welladapted to ResNets, inspired by the minimizing movement scheme for gradient flows in distribution space. The method amounts to a kinetic energy regularization of each block that makes the blocks optimal transport maps and endows them with regularity. It works by alleviating the stagnation problem observed in layer-wise training, whereby greedily-trained early layers overfit and deeper layers stop increasing test accuracy after a certain depth. We show on classification tasks that the test accuracy of block-wise",
    "link": "http://arxiv.org/abs/2210.00949",
    "context": "Title: Block-wise Training of Residual Networks via the Minimizing Movement Scheme. (arXiv:2210.00949v2 [cs.LG] UPDATED)\nAbstract: End-to-end backpropagation has a few shortcomings: it requires loading the entire model during training, which can be impossible in constrained settings, and suffers from three locking problems (forward locking, update locking and backward locking), which prohibit training the layers in parallel. Solving layer-wise optimization problems can address these problems and has been used in on-device training of neural networks. We develop a layer-wise training method, particularly welladapted to ResNets, inspired by the minimizing movement scheme for gradient flows in distribution space. The method amounts to a kinetic energy regularization of each block that makes the blocks optimal transport maps and endows them with regularity. It works by alleviating the stagnation problem observed in layer-wise training, whereby greedily-trained early layers overfit and deeper layers stop increasing test accuracy after a certain depth. We show on classification tasks that the test accuracy of block-wise",
    "path": "papers/22/10/2210.00949.json",
    "total_tokens": 1077,
    "translated_title": "通过最小化动态调整方案的块状残差网络训练",
    "translated_abstract": "端到端的反向传播存在一些不足之处：在训练过程中需要加载整个模型，这在受限制的环境下可能是不可能的，并且受到三个锁定问题的困扰（前向锁定、更新锁定和后向锁定），这些问题禁止并行训练各层。通过逐层优化问题可以解决这些问题，并已在神经网络的设备端上使用。我们开发了一种适用于ResNets的逐层训练方法，受启发于分布空间梯度流的最小化运动方案。该方法相当于每个块的动能正则化，使块成为最优输运映射，并赋予其正则化性质。它通过缓解层次训练中观察到的停滞问题工作，即贪婪训练早期层会过拟合，更深的层在一定深度后停止提高测试精度。我们展示了在分类任务上，块状训练ResNets的测试精度与完整模型的端到端训练相竞争，并且可扩展到大规模数据集和模型，即使在有限的计算资源上也能实现。",
    "tldr": "本文提出了一种适用于ResNets的块状训练方法，能够解决端到端反向传播存在的问题，包括受限制的环境下无法加载整个模型以及禁止并行训练各层等，并且能够缓解层次训练中出现的停滞问题，其测试精度与完整模型的端到端训练相竞争。"
}