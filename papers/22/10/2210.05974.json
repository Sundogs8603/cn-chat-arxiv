{
    "title": "Clustering the Sketch: A Novel Approach to Embedding Table Compression. (arXiv:2210.05974v3 [cs.LG] UPDATED)",
    "abstract": "Embedding tables are used by machine learning systems to work with categorical features. In modern Recommendation Systems, these tables can be very large, necessitating the development of new methods for fitting them in memory, even during training. We suggest Clustered Compositional Embeddings (CCE) which combines clustering-based compression like quantization to codebooks with dynamic methods like The Hashing Trick and Compositional Embeddings (Shi et al., 2020). Experimentally CCE achieves the best of both worlds: The high compression rate of codebook-based quantization, but *dynamically* like hashing-based methods, so it can be used during training. Theoretically, we prove that CCE is guaranteed to converge to the optimal codebook and give a tight bound for the number of iterations required.",
    "link": "http://arxiv.org/abs/2210.05974",
    "context": "Title: Clustering the Sketch: A Novel Approach to Embedding Table Compression. (arXiv:2210.05974v3 [cs.LG] UPDATED)\nAbstract: Embedding tables are used by machine learning systems to work with categorical features. In modern Recommendation Systems, these tables can be very large, necessitating the development of new methods for fitting them in memory, even during training. We suggest Clustered Compositional Embeddings (CCE) which combines clustering-based compression like quantization to codebooks with dynamic methods like The Hashing Trick and Compositional Embeddings (Shi et al., 2020). Experimentally CCE achieves the best of both worlds: The high compression rate of codebook-based quantization, but *dynamically* like hashing-based methods, so it can be used during training. Theoretically, we prove that CCE is guaranteed to converge to the optimal codebook and give a tight bound for the number of iterations required.",
    "path": "papers/22/10/2210.05974.json",
    "total_tokens": 917,
    "translated_title": "聚类草图：嵌入表压缩的新方法",
    "translated_abstract": "嵌入表是机器学习系统用于处理分类特征的工具。在现代推荐系统中，这些表可能非常庞大，因此需要开发新的方法将它们装入内存，即使在训练过程中也是如此。我们建议采用聚类组合嵌入（Clustered Compositional Embeddings，CCE）方法，它将基于聚类的压缩方法（如量化到码本）与动态方法（如散列技巧和组合嵌入）结合起来（Shi等人，2020）。实验证明，CCE在两个方面取得了最佳效果：即基于码本的量化具有高压缩率，但像基于散列的方法一样动态，因此可在训练过程中使用。从理论上讲，我们证明了CCE一定会收敛到最优码本，并给出了所需迭代次数的紧密界限。",
    "tldr": "本论文提出了一种名为聚类组合嵌入的新方法，用于解决机器学习系统中大型嵌入表在内存中的问题。该方法结合了基于聚类的压缩方法和动态方法，既具有高压缩率又可以在训练过程中使用。理论上证明了该方法会收敛到最优码本，并给出了迭代次数的界限。",
    "en_tdlr": "This paper presents a novel approach called Clustered Compositional Embeddings (CCE) to address the issue of fitting large embedding tables in memory in machine learning systems. CCE combines clustering-based compression methods with dynamic methods, achieving a high compression rate while being usable during training. Theoretical proof is provided for the convergence to the optimal codebook and a bound is given for the number of iterations required."
}