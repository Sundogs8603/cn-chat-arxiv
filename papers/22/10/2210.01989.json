{
    "title": "WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence Learning Ability. (arXiv:2210.01989v3 [cs.CL] UPDATED)",
    "abstract": "Transformer and its variants are fundamental neural architectures in deep learning. Recent works show that learning attention in the Fourier space can improve the long sequence learning capability of Transformers. We argue that wavelet transform shall be a better choice because it captures both position and frequency information with linear time complexity. Therefore, in this paper, we systematically study the synergy between wavelet transform and Transformers. We propose Wavelet Space Attention (WavSpA) that facilitates attention learning in a learnable wavelet coefficient space which replaces the attention in Transformers by (1) applying forward wavelet transform to project the input sequences to multi-resolution bases, (2) conducting attention learning in the wavelet coefficient space, and (3) reconstructing the representation in input space via backward wavelet transform. Extensive experiments on the Long Range Arena demonstrate that learning attention in the wavelet space using ei",
    "link": "http://arxiv.org/abs/2210.01989",
    "context": "Title: WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence Learning Ability. (arXiv:2210.01989v3 [cs.CL] UPDATED)\nAbstract: Transformer and its variants are fundamental neural architectures in deep learning. Recent works show that learning attention in the Fourier space can improve the long sequence learning capability of Transformers. We argue that wavelet transform shall be a better choice because it captures both position and frequency information with linear time complexity. Therefore, in this paper, we systematically study the synergy between wavelet transform and Transformers. We propose Wavelet Space Attention (WavSpA) that facilitates attention learning in a learnable wavelet coefficient space which replaces the attention in Transformers by (1) applying forward wavelet transform to project the input sequences to multi-resolution bases, (2) conducting attention learning in the wavelet coefficient space, and (3) reconstructing the representation in input space via backward wavelet transform. Extensive experiments on the Long Range Arena demonstrate that learning attention in the wavelet space using ei",
    "path": "papers/22/10/2210.01989.json",
    "total_tokens": 837,
    "tldr": "本文提出了WavSpA，它通过使用可学习的小波系数空间学习注意力，从而增强了Transformer对长序列的学习能力。",
    "en_tdlr": "This paper proposes WavSpA, which enhances the learning ability of Transformers for long sequences by using a learnable wavelet coefficient space for attention learning."
}