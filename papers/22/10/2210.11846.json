{
    "title": "Redefining Counterfactual Explanations for Reinforcement Learning: Overview, Challenges and Opportunities",
    "abstract": "While AI algorithms have shown remarkable success in various fields, their lack of transparency hinders their application to real-life tasks. Although explanations targeted at non-experts are necessary for user trust and human-AI collaboration, the majority of explanation methods for AI are focused on developers and expert users. Counterfactual explanations are local explanations that offer users advice on what can be changed in the input for the output of the black-box model to change. Counterfactuals are user-friendly and provide actionable advice for achieving the desired output from the AI system. While extensively researched in supervised learning, there are few methods applying them to reinforcement learning (RL). In this work, we explore the reasons for the underrepresentation of a powerful explanation method in RL. We start by reviewing the current work in counterfactual explanations in supervised learning. Additionally, we explore the differences between counterfactual explana",
    "link": "https://arxiv.org/abs/2210.11846",
    "context": "Title: Redefining Counterfactual Explanations for Reinforcement Learning: Overview, Challenges and Opportunities\nAbstract: While AI algorithms have shown remarkable success in various fields, their lack of transparency hinders their application to real-life tasks. Although explanations targeted at non-experts are necessary for user trust and human-AI collaboration, the majority of explanation methods for AI are focused on developers and expert users. Counterfactual explanations are local explanations that offer users advice on what can be changed in the input for the output of the black-box model to change. Counterfactuals are user-friendly and provide actionable advice for achieving the desired output from the AI system. While extensively researched in supervised learning, there are few methods applying them to reinforcement learning (RL). In this work, we explore the reasons for the underrepresentation of a powerful explanation method in RL. We start by reviewing the current work in counterfactual explanations in supervised learning. Additionally, we explore the differences between counterfactual explana",
    "path": "papers/22/10/2210.11846.json",
    "total_tokens": 854,
    "translated_title": "重新定义强化学习的反事实解释：概述、挑战和机遇",
    "translated_abstract": "尽管人工智能算法在各个领域展现出了显著的成功，但其缺乏透明度限制了其在实际任务中的应用。虽然面向非专家的解释对用户的信任和人机协作非常重要，但目前大部分面向AI的解释方法都是针对开发者和专家用户的。反事实解释是一种提供用户关于如何改变输入以改变黑盒模型输出的局部解释。反事实解释友好，并提供具体的建议，以实现所需的AI系统输出。尽管反事实解释在监督学习中得到了广泛研究，但在强化学习中很少有应用它们的方法。在这项工作中，我们探讨了强化学习中强大解释方法代表不足的原因。我们首先回顾了监督学习中的反事实解释的当前工作。此外，我们还探讨了反事实解释在强化学习中的差异。",
    "tldr": "这项工作重新定义了强化学习中的反事实解释方法，并探讨了其在监督学习和强化学习中的差异，以提供用户友好和可操作的解释。"
}