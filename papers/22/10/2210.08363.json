{
    "title": "Data-Efficient Augmentation for Training Neural Networks. (arXiv:2210.08363v3 [cs.LG] UPDATED)",
    "abstract": "Data augmentation is essential to achieve state-of-the-art performance in many deep learning applications. However, the most effective augmentation techniques become computationally prohibitive for even medium-sized datasets. To address this, we propose a rigorous technique to select subsets of data points that when augmented, closely capture the training dynamics of full data augmentation. We first show that data augmentation, modeled as additive perturbations, improves learning and generalization by relatively enlarging and perturbing the smaller singular values of the network Jacobian, while preserving its prominent directions. This prevents overfitting and enhances learning the harder to learn information. Then, we propose a framework to iteratively extract small subsets of training data that when augmented, closely capture the alignment of the fully augmented Jacobian with labels/residuals. We prove that stochastic gradient descent applied to the augmented subsets found by our app",
    "link": "http://arxiv.org/abs/2210.08363",
    "context": "Title: Data-Efficient Augmentation for Training Neural Networks. (arXiv:2210.08363v3 [cs.LG] UPDATED)\nAbstract: Data augmentation is essential to achieve state-of-the-art performance in many deep learning applications. However, the most effective augmentation techniques become computationally prohibitive for even medium-sized datasets. To address this, we propose a rigorous technique to select subsets of data points that when augmented, closely capture the training dynamics of full data augmentation. We first show that data augmentation, modeled as additive perturbations, improves learning and generalization by relatively enlarging and perturbing the smaller singular values of the network Jacobian, while preserving its prominent directions. This prevents overfitting and enhances learning the harder to learn information. Then, we propose a framework to iteratively extract small subsets of training data that when augmented, closely capture the alignment of the fully augmented Jacobian with labels/residuals. We prove that stochastic gradient descent applied to the augmented subsets found by our app",
    "path": "papers/22/10/2210.08363.json",
    "total_tokens": 1049,
    "translated_title": "数据高效增强训练神经网络的技术",
    "translated_abstract": "数据增强在许多深度学习应用中是实现最先进性能的关键，但是对于中等规模数据集，最有效的增强技术在计算上是不可行的。为了解决这个问题，我们提出了一种严格的技术来选择数据子集，当进行增强时，可以近似捕捉完全数据增强的训练动态。首先我们证明了将数据增强建模为加性扰动的方式可以改进学习和泛化能力，通过相对放大和扰动网络雅可比矩阵的较小奇异值，同时保留其显著方向。这样可以防止过拟合并增强对难以学习的信息的学习。接着，我们提出了一个框架，通过迭代地提取小的训练数据子集，当这些子集进行增强时，能够捕捉到完全增强的雅可比矩阵与标签/残差的对齐关系。我们证明了通过我们的方法找到的增强子集上应用随机梯度下降可以达到与完全数据增强相似的结果。",
    "tldr": "本论文提出了一种数据高效增强训练神经网络的技术，该技术通过选择数据子集进行增强来近似捕捉完全数据增强的训练动态。该方法通过放大和扰动网络雅可比矩阵的较小奇异值，保留其显著方向，从而改进学习和泛化能力，并且通过迭代地提取小的训练数据子集，能够捕捉到完全增强的雅可比矩阵与标签/残差的对齐关系。",
    "en_tdlr": "This paper proposes a data-efficient augmentation technique for training neural networks, which selects subsets of data points to approximate the training dynamics of full data augmentation. The technique improves learning and generalization by enlarging and perturbing the smaller singular values of the network Jacobian while preserving its prominent directions. It also extracts small subsets of training data to capture the alignment between the fully augmented Jacobian and labels/residuals."
}