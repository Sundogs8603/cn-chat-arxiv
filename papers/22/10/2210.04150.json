{
    "title": "Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP. (arXiv:2210.04150v2 [cs.CV] UPDATED)",
    "abstract": "Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions. We collect training data by mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, w",
    "link": "http://arxiv.org/abs/2210.04150",
    "context": "Title: Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP. (arXiv:2210.04150v2 [cs.CV] UPDATED)\nAbstract: Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions. We collect training data by mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, w",
    "path": "papers/22/10/2210.04150.json",
    "total_tokens": 1102,
    "translated_title": "Mask-adapted CLIP的开放词汇语义分割",
    "translated_abstract": "开放词汇语义分割的目的是根据文本描述将图像分割为语义区域，这些描述可能在训练期间没有被观察到。最近的两阶段方法首先生成不考虑类的掩码提议，然后利用预训练的视觉-语言模型（例如CLIP）对掩码区域进行分类。我们确定了这一范例的性能瓶颈是预训练CLIP模型，因为它在遮蔽图像上的表现不佳。为了解决这个问题，我们提出在一组带有掩码图像区域及其对应的文本描述的数据上对CLIP进行微调。我们通过使用CLIP将掩码图像区域与图像描述中的名词匹配来挖掘现有图像-标题数据集（例如COCO Captions）来收集训练数据。与更精确且手动注释的固定类别分割标签（例如COCO-Stuff）相比，我们发现我们的嘈杂但多样化的数据集能更好地保留CLIP的泛化能力。除了微调整个模型外，我们还引入了适应掩码的CLIP体系结构，通过明确地建模蒙版过程来更好地处理带有掩码的图像。我们在开放词汇语义分割基准OpenImages上的实验证明，我们的方法优于最先进的方法。",
    "tldr": "本文提出了一种基于CLIP和带掩膜的体系结构的开放词汇语义分割方法，通过在用于训练的嘈杂但多样化的数据集上对CLIP进行微调，以提高其在带有掩膜的图像上的性能，超越了当前最佳的方法。",
    "en_tdlr": "This paper proposes an open-vocabulary semantic segmentation method based on CLIP and a mask-adapted architecture. By fine-tuning CLIP on a noisy but diverse dataset for training, the proposed method can better handle masked images and outperforms state-of-the-art approaches on the OpenImages benchmark."
}