{
    "title": "Prophet Attention: Predicting Attention with Future Attention for Image Captioning. (arXiv:2210.10914v2 [cs.CV] UPDATED)",
    "abstract": "Recently, attention based models have been used extensively in many sequence-to-sequence learning systems. Especially for image captioning, the attention based models are expected to ground correct image regions with proper generated words. However, for each time step in the decoding process, the attention based models usually use the hidden state of the current input to attend to the image regions. Under this setting, these attention models have a \"deviated focus\" problem that they calculate the attention weights based on previous words instead of the one to be generated, impairing the performance of both grounding and captioning. In this paper, we propose the Prophet Attention, similar to the form of self-supervision. In the training stage, this module utilizes the future information to calculate the \"ideal\" attention weights towards image regions. These calculated \"ideal\" weights are further used to regularize the \"deviated\" attention. In this manner, image regions are grounded with",
    "link": "http://arxiv.org/abs/2210.10914",
    "context": "Title: Prophet Attention: Predicting Attention with Future Attention for Image Captioning. (arXiv:2210.10914v2 [cs.CV] UPDATED)\nAbstract: Recently, attention based models have been used extensively in many sequence-to-sequence learning systems. Especially for image captioning, the attention based models are expected to ground correct image regions with proper generated words. However, for each time step in the decoding process, the attention based models usually use the hidden state of the current input to attend to the image regions. Under this setting, these attention models have a \"deviated focus\" problem that they calculate the attention weights based on previous words instead of the one to be generated, impairing the performance of both grounding and captioning. In this paper, we propose the Prophet Attention, similar to the form of self-supervision. In the training stage, this module utilizes the future information to calculate the \"ideal\" attention weights towards image regions. These calculated \"ideal\" weights are further used to regularize the \"deviated\" attention. In this manner, image regions are grounded with",
    "path": "papers/22/10/2210.10914.json",
    "total_tokens": 941,
    "translated_title": "先知式注意力：基于未来信息的图像描述中的注意力预测方法",
    "translated_abstract": "近来，注意力机制在序列到序列学习系统中被广泛使用。尤其是在图像描述中，注意力机制可以将正确的图像区域与适当的生成词语联系起来。然而，在解码过程中，这些注意力机制通常使用当前输入的隐藏状态来关注图像区域。在这种情况下，这些注意力模型存在“偏离焦点”的问题，它们根据先前的单词计算注意力权重，而不是即将生成的单词，这会影响地面和描述的性能。在本文中，我们提出了“先知式注意力”，类似于自我监督的形式。在训练阶段，该模块利用未来信息来计算“理想”的注意力权重，以进一步规范“偏移”的注意力。通过这种方式，图像区域与更精确的单词相关联，生成的图片标题更加准确。在三个基准数据集上的实验结果表明，我们提出的方法与其他基于注意力的模型相比，取得了最先进的性能。",
    "tldr": "本文提出了一种先知式注意力模型，它利用未来信息计算理想的注意力权重，进一步规范偏移的注意力，这种方法在图像描述中取得了最先进的性能。",
    "en_tdlr": "This paper proposes a Prophet Attention model that utilizes future information to calculate \"ideal\" attention weights towards image regions, thereby reducing the \"deviated focus\" problem in attention-based models for image captioning and achieving state-of-the-art performance on three benchmark datasets."
}