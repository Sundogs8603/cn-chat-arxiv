{
    "title": "Just Round: Quantized Observation Spaces Enable Memory Efficient Learning of Dynamic Locomotion. (arXiv:2210.08065v2 [cs.RO] UPDATED)",
    "abstract": "Deep reinforcement learning (DRL) is one of the most powerful tools for synthesizing complex robotic behaviors. But training DRL models is incredibly compute and memory intensive, requiring large training datasets and replay buffers to achieve performant results. This poses a challenge for the next generation of field robots that will need to learn on the edge to adapt to their environment. In this paper, we begin to address this issue through observation space quantization. We evaluate our approach using four simulated robot locomotion tasks and two state-of-the-art DRL algorithms, the on-policy Proximal Policy Optimization (PPO) and off-policy Soft Actor-Critic (SAC) and find that observation space quantization reduces overall memory costs by as much as 4.2x without impacting learning performance.",
    "link": "http://arxiv.org/abs/2210.08065",
    "context": "Title: Just Round: Quantized Observation Spaces Enable Memory Efficient Learning of Dynamic Locomotion. (arXiv:2210.08065v2 [cs.RO] UPDATED)\nAbstract: Deep reinforcement learning (DRL) is one of the most powerful tools for synthesizing complex robotic behaviors. But training DRL models is incredibly compute and memory intensive, requiring large training datasets and replay buffers to achieve performant results. This poses a challenge for the next generation of field robots that will need to learn on the edge to adapt to their environment. In this paper, we begin to address this issue through observation space quantization. We evaluate our approach using four simulated robot locomotion tasks and two state-of-the-art DRL algorithms, the on-policy Proximal Policy Optimization (PPO) and off-policy Soft Actor-Critic (SAC) and find that observation space quantization reduces overall memory costs by as much as 4.2x without impacting learning performance.",
    "path": "papers/22/10/2210.08065.json",
    "total_tokens": 732,
    "translated_title": "Just Round：量化观测空间实现动态运动的高效学习",
    "translated_abstract": "深度强化学习（DRL）是合成复杂机器人行为的最强大工具之一。但训练DRL模型需要大量计算和内存，需要大型训练数据集和回放缓冲区才能取得良好的结果。这对于下一代需要在边缘上学习以适应其环境的现场机器人提出了挑战。本文通过观测空间量化来开始解决这个问题。我们使用四个模拟机器人运动任务和两种最先进的DRL算法，在不影响学习性能的情况下，发现观测空间量化可以将整体内存成本降低多达4.2倍。",
    "tldr": "本文通过观测空间量化，实现了对复杂机器人行为的高效学习，可以降低内存成本。",
    "en_tdlr": "This paper achieves efficient learning of complex robotic behaviors through observation space quantization, which can reduce overall memory costs and is applicable for next generation robots learning on the edge."
}