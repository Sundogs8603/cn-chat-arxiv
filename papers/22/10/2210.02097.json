{
    "title": "Teaching Yourself: Graph Self-Distillation on Neighborhood for Node Classification. (arXiv:2210.02097v5 [cs.LG] UPDATED)",
    "abstract": "Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for this academic-industrial gap is the neighborhood-fetching latency incurred by data dependency in GNNs, which make it hard to deploy for latency-sensitive applications that require fast inference. Conversely, without involving any feature aggregation, MLPs have no data dependency and infer much faster than GNNs, but their performance is less competitive. Motivated by these complementary strengths and weaknesses, we propose a Graph Self-Distillation on Neighborhood (GSDN) framework to reduce the gap between GNNs and MLPs. Specifically, the GSDN framework is based purely on MLPs, where structural information is only implicitly used as prior to guide knowledge self-distillation between the neighborhood and the target, substituting th",
    "link": "http://arxiv.org/abs/2210.02097",
    "context": "Title: Teaching Yourself: Graph Self-Distillation on Neighborhood for Node Classification. (arXiv:2210.02097v5 [cs.LG] UPDATED)\nAbstract: Recent years have witnessed great success in handling graph-related tasks with Graph Neural Networks (GNNs). Despite their great academic success, Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical industrial applications. One reason for this academic-industrial gap is the neighborhood-fetching latency incurred by data dependency in GNNs, which make it hard to deploy for latency-sensitive applications that require fast inference. Conversely, without involving any feature aggregation, MLPs have no data dependency and infer much faster than GNNs, but their performance is less competitive. Motivated by these complementary strengths and weaknesses, we propose a Graph Self-Distillation on Neighborhood (GSDN) framework to reduce the gap between GNNs and MLPs. Specifically, the GSDN framework is based purely on MLPs, where structural information is only implicitly used as prior to guide knowledge self-distillation between the neighborhood and the target, substituting th",
    "path": "papers/22/10/2210.02097.json",
    "total_tokens": 908,
    "translated_title": "自我教学：基于邻域的图形自我蒸馏用于节点分类",
    "translated_abstract": "近年来，图神经网络（GNN）在处理与图有关的任务方面取得了巨大的成功。尽管在学术界取得了巨大的成功，但在实际的工业应用中，多层感知器（MLP）仍然是主要的工作马力。造成学术界和工业界的差距之一是GNN中的数据依赖性所引起的邻域获取延迟，这使得很难在需要快速推理的需要低延迟的应用中使用。相反，没有涉及任何特征聚合，MLP没有数据依赖性，推理速度比GNN要快得多，但其性能不够竞争力。受这些互补的优势和劣势的启发，我们提出了一个基于邻域的图形自我蒸馏（GSDN）框架，以缩小GNN和MLP之间的差距。具体地，GSDN框架基于纯MLP，结构信息仅被隐式地用作先验，以指导邻域和目标之间的知识自我蒸馏，替换掉特征聚合。",
    "tldr": "本文提出了一种GSDN框架，通过基于邻域的图形自我蒸馏，以减小GNN和MLP之间的差距。",
    "en_tdlr": "This paper proposes a GSDN framework to reduce the gap between GNNs and MLPs by graph self-distillation on neighborhood."
}