{
    "title": "MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting. (arXiv:2210.07179v2 [cs.CV] UPDATED)",
    "abstract": "Large pre-trained models have proved to be remarkable zero- and (prompt-based) few-shot learners in unimodal vision and language tasks. We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings. MAPL learns a lightweight mapping between the representation spaces of unimodal models using aligned image-text data, and can generalize to unseen VL tasks from just a few in-context examples. The small number of trainable parameters makes MAPL effective at low-data and in-domain learning. Moreover, MAPL's modularity enables easy extension to other pre-trained models. Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters. MAPL can be trained in just a few hours using modest comp",
    "link": "http://arxiv.org/abs/2210.07179",
    "context": "Title: MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting. (arXiv:2210.07179v2 [cs.CV] UPDATED)\nAbstract: Large pre-trained models have proved to be remarkable zero- and (prompt-based) few-shot learners in unimodal vision and language tasks. We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings. MAPL learns a lightweight mapping between the representation spaces of unimodal models using aligned image-text data, and can generalize to unseen VL tasks from just a few in-context examples. The small number of trainable parameters makes MAPL effective at low-data and in-domain learning. Moreover, MAPL's modularity enables easy extension to other pre-trained models. Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters. MAPL can be trained in just a few hours using modest comp",
    "path": "papers/22/10/2210.07179.json",
    "total_tokens": 988,
    "translated_title": "MAPL: 基于参数效率的单模态预训练模型在视觉-语言少样本任务中的适应",
    "translated_abstract": "在单模态视觉和语言任务中，大型预训练模型已经被证明是出色的零样本和（基于提示的）少样本学习器。我们提出了MAPL，一种简单且参数效率高的方法，它重用冻结的单模态预训练模型，并利用它们在多模态视觉-语言（VL）场景中的强大泛化能力。MAPL使用对齐的图像-文本数据学习了单模态模型表示空间之间的轻量级映射，并且可以从仅有少量上下文示例就推广到看不见的VL任务。MAPL的可训练参数数量很少，使得它在低数据和域内学习方面非常有效。此外，MAPL的模块化使得可以轻松扩展到其他预训练模型。在几个视觉问答和图像标题生成基准测试上的大量实验证明，MAPL相对于类似方法在训练少得多的参数时实现了优越或有竞争力的性能。MAPL可以在几小时内使用适度的计算资源进行训练。",
    "tldr": "MAPL使用对齐的图像-文本数据学习单模态模型表示空间之间的轻量级映射，从而实现了面向视觉-语言少样本任务的基于参数效率的适应，并在测试中显示出优越的性能表现。"
}