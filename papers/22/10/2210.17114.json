{
    "title": "QuaLA-MiniLM: a Quantized Length Adaptive MiniLM. (arXiv:2210.17114v3 [cs.CL] UPDATED)",
    "abstract": "Limited computational budgets often prevent transformers from being used in production and from having their high accuracy utilized. A knowledge distillation approach addresses the computational efficiency by self-distilling BERT into a smaller transformer representation having fewer layers and smaller internal embedding. However, the performance of these models drops as we reduce the number of layers, notably in advanced NLP tasks such as span question answering. In addition, a separate model must be trained for each inference scenario with its distinct computational budget. Dynamic-TinyBERT tackles both limitations by partially implementing the Length Adaptive Transformer (LAT) technique onto TinyBERT, achieving x3 speedup over BERT-base with minimal accuracy loss. In this work, we expand the Dynamic-TinyBERT approach to generate a much more highly efficient model. We use MiniLM distillation jointly with the LAT method, and we further enhance the efficiency by applying low-bit quanti",
    "link": "http://arxiv.org/abs/2210.17114",
    "context": "Title: QuaLA-MiniLM: a Quantized Length Adaptive MiniLM. (arXiv:2210.17114v3 [cs.CL] UPDATED)\nAbstract: Limited computational budgets often prevent transformers from being used in production and from having their high accuracy utilized. A knowledge distillation approach addresses the computational efficiency by self-distilling BERT into a smaller transformer representation having fewer layers and smaller internal embedding. However, the performance of these models drops as we reduce the number of layers, notably in advanced NLP tasks such as span question answering. In addition, a separate model must be trained for each inference scenario with its distinct computational budget. Dynamic-TinyBERT tackles both limitations by partially implementing the Length Adaptive Transformer (LAT) technique onto TinyBERT, achieving x3 speedup over BERT-base with minimal accuracy loss. In this work, we expand the Dynamic-TinyBERT approach to generate a much more highly efficient model. We use MiniLM distillation jointly with the LAT method, and we further enhance the efficiency by applying low-bit quanti",
    "path": "papers/22/10/2210.17114.json",
    "total_tokens": 969,
    "translated_title": "QuaLA-MiniLM: 一种量化长度自适应的 MiniLM",
    "translated_abstract": "有限的计算资源经常阻止 Transformer 模型被应用于生产环境，并发挥其高精度优势。知识蒸馏方法通过将 BERT 自我蒸馏为较小的 Transformer 表示来提高计算效率，其层数更少，内部嵌入更小。然而，当我们减少层数时，这些模型的性能下降，尤其是在一些先进的 NLP 任务如跨度问答中。此外，对于每个不同的推理场景，都必须训练一个单独的模型以满足其不同的计算预算。Dynamic-TinyBERT 通过部分实现 Length Adaptive Transformer（LAT）技术到 TinyBERT 上解决了这两个限制，实现了与 BERT-base 相比 x3 的加速，同时最小化了精度损失。在本文中，我们扩展 Dynamic-TinyBERT 方法，生成了一个更加高效的模型。我们将 MiniLM 蒸馏和 LAT 方法联合使用，并通过应用低比特化技术进一步提高效率。我们提出的模型 QuaLA-MiniLM 在各种 NLP 任务上实现了最先进的结果，同时保持了小模型尺寸和高效率。",
    "tldr": "QuaLA-MiniLM 是一种量化长度自适应的 MiniLM 模型，通过应用低比特化技术和 LAT 方法，实现了在各种 NLP 任务上的最新成果，同时保持较小的模型尺寸和高效率。",
    "en_tdlr": "QuaLA-MiniLM is a quantized length adaptive MiniLM model that achieves state-of-the-art results on various NLP tasks while maintaining a small model size and high efficiency, by applying low-bit quantized techniques and LAT method."
}