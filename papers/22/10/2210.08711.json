{
    "title": "Continuous Pseudo-Labeling from the Start. (arXiv:2210.08711v2 [cs.LG] UPDATED)",
    "abstract": "Self-training (ST), or pseudo-labeling has sparked significant interest in the automatic speech recognition (ASR) community recently because of its success in harnessing unlabeled data. Unlike prior semi-supervised learning approaches that relied on iteratively regenerating pseudo-labels (PLs) from a trained model and using them to train a new model, recent state-of-the-art methods perform `continuous training' where PLs are generated using a very recent version of the model being trained. Nevertheless, these approaches still rely on bootstrapping the ST using an initial supervised learning phase where the model is trained on labeled data alone. We believe this has the potential for over-fitting to the labeled dataset in low resource settings and that ST from the start of training should reduce over-fitting. In this paper we show how we can do this by dynamically controlling the evolution of PLs during the training process in ASR. To the best of our knowledge, this is the first study t",
    "link": "http://arxiv.org/abs/2210.08711",
    "context": "Title: Continuous Pseudo-Labeling from the Start. (arXiv:2210.08711v2 [cs.LG] UPDATED)\nAbstract: Self-training (ST), or pseudo-labeling has sparked significant interest in the automatic speech recognition (ASR) community recently because of its success in harnessing unlabeled data. Unlike prior semi-supervised learning approaches that relied on iteratively regenerating pseudo-labels (PLs) from a trained model and using them to train a new model, recent state-of-the-art methods perform `continuous training' where PLs are generated using a very recent version of the model being trained. Nevertheless, these approaches still rely on bootstrapping the ST using an initial supervised learning phase where the model is trained on labeled data alone. We believe this has the potential for over-fitting to the labeled dataset in low resource settings and that ST from the start of training should reduce over-fitting. In this paper we show how we can do this by dynamically controlling the evolution of PLs during the training process in ASR. To the best of our knowledge, this is the first study t",
    "path": "papers/22/10/2210.08711.json",
    "total_tokens": 662,
    "translated_title": "从一开始就进行连续伪标记",
    "translated_abstract": "自我训练或伪标记由于其在利用未标记数据方面的成功而引起了自动语音识别（ASR）社区的极大关注。本文提出了一种在ASR中通过动态控制伪标签生成来实现从训练开始就进行伪标记的方法，以减少标记数据集上的过拟合。",
    "tldr": "本文提出了一种从训练开始就进行伪标记的ASR方法，通过动态控制伪标签生成来减少标记数据集上的过拟合。",
    "en_tdlr": "This paper proposes an ASR method that performs pseudo-labeling from the start of training, reducing over-fitting to the labeled dataset by dynamically controlling the generation of pseudo-labels."
}