{
    "title": "PAC-Bayesian Offline Contextual Bandits With Guarantees. (arXiv:2210.13132v2 [stat.ML] UPDATED)",
    "abstract": "This paper introduces a new principled approach for off-policy learning in contextual bandits. Unlike previous work, our approach does not derive learning principles from intractable or loose bounds. We analyse the problem through the PAC-Bayesian lens, interpreting policies as mixtures of decision rules. This allows us to propose novel generalization bounds and provide tractable algorithms to optimize them. We prove that the derived bounds are tighter than their competitors, and can be optimized directly to confidently improve upon the logging policy offline. Our approach learns policies with guarantees, uses all available data and does not require tuning additional hyperparameters on held-out sets. We demonstrate through extensive experiments the effectiveness of our approach in providing performance guarantees in practical scenarios.",
    "link": "http://arxiv.org/abs/2210.13132",
    "context": "Title: PAC-Bayesian Offline Contextual Bandits With Guarantees. (arXiv:2210.13132v2 [stat.ML] UPDATED)\nAbstract: This paper introduces a new principled approach for off-policy learning in contextual bandits. Unlike previous work, our approach does not derive learning principles from intractable or loose bounds. We analyse the problem through the PAC-Bayesian lens, interpreting policies as mixtures of decision rules. This allows us to propose novel generalization bounds and provide tractable algorithms to optimize them. We prove that the derived bounds are tighter than their competitors, and can be optimized directly to confidently improve upon the logging policy offline. Our approach learns policies with guarantees, uses all available data and does not require tuning additional hyperparameters on held-out sets. We demonstrate through extensive experiments the effectiveness of our approach in providing performance guarantees in practical scenarios.",
    "path": "papers/22/10/2210.13132.json",
    "total_tokens": 786,
    "translated_title": "具有保证的PAC-Bayesian离线情境强化学习算法",
    "translated_abstract": "本文提出了一种新的基于PAC-Bayesian方法的离线情境强化学习算法。与之前的方法不同，该方法不是从难以处理或不准确的界限推导学习原则。我们通过PAC-Bayesian方法分析问题，将策略解释为决策规则的混合物。这使我们能够提出新的泛化界限，并提供可解算法来优化它们。我们证明所得界限比竞争对手更紧，可以直接优化以在离线情况下自信地改进记录策略。我们的方法学习带保证的策略，使用所有可用数据，并不需要在保留集上调整更多的超参数。通过广泛的实验，我们展示了该方法在实际情景中提供性能保证的有效性。",
    "tldr": "本文提出了一种通过PAC-Bayesian方法分析离线情境强化学习问题的新算法，该算法通过优化新的泛化界限提供了保证，并在实际情境中得到了验证。",
    "en_tdlr": "This paper proposes a new off-policy learning algorithm for contextual bandits, which analyses the problem through the PAC-Bayesian lens and provides novel generalization bounds, resulting in policies with guarantees that are validated in practical scenarios."
}