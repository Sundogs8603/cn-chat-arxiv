{
    "title": "KAIROS: Building Cost-Efficient Machine Learning Inference Systems with Heterogeneous Cloud Resources. (arXiv:2210.05889v3 [cs.DC] UPDATED)",
    "abstract": "Online inference is becoming a key service product for many businesses, deployed in cloud platforms to meet customer demands. Despite their revenue-generation capability, these services need to operate under tight Quality-of-Service (QoS) and cost budget constraints. This paper introduces KAIROS, a novel runtime framework that maximizes the query throughput while meeting QoS target and a cost budget. KAIROS designs and implements novel techniques to build a pool of heterogeneous compute hardware without online exploration overhead, and distribute inference queries optimally at runtime. Our evaluation using industry-grade deep learning (DL) models shows that KAIROS yields up to 2X the throughput of an optimal homogeneous solution, and outperforms state-of-the-art schemes by up to 70%, despite advantageous implementations of the competing schemes to ignore their exploration overhead.",
    "link": "http://arxiv.org/abs/2210.05889",
    "context": "Title: KAIROS: Building Cost-Efficient Machine Learning Inference Systems with Heterogeneous Cloud Resources. (arXiv:2210.05889v3 [cs.DC] UPDATED)\nAbstract: Online inference is becoming a key service product for many businesses, deployed in cloud platforms to meet customer demands. Despite their revenue-generation capability, these services need to operate under tight Quality-of-Service (QoS) and cost budget constraints. This paper introduces KAIROS, a novel runtime framework that maximizes the query throughput while meeting QoS target and a cost budget. KAIROS designs and implements novel techniques to build a pool of heterogeneous compute hardware without online exploration overhead, and distribute inference queries optimally at runtime. Our evaluation using industry-grade deep learning (DL) models shows that KAIROS yields up to 2X the throughput of an optimal homogeneous solution, and outperforms state-of-the-art schemes by up to 70%, despite advantageous implementations of the competing schemes to ignore their exploration overhead.",
    "path": "papers/22/10/2210.05889.json",
    "total_tokens": 929,
    "translated_title": "KAIROS：利用异构云资源构建高效的机器学习推断系统",
    "translated_abstract": "在线推断正成为许多企业的关键服务产品，部署在云平台上以满足客户需求。尽管它们有利润产生能力，但这些服务需要在严格的服务质量和成本预算限制下运行。本文介绍了KAIROS，一个新颖的运行时框架，最大化查询吞吐量同时满足服务质量目标和成本预算。KAIROS设计并实现了新颖的技术，构建了一个异构计算硬件池，避免在线资源探索的额外开销，并优化地在运行时分配推断查询。我们使用产业级深度学习模型进行评估，结果显示KAIROS产生的吞吐量比最优异构方案高出2倍，并且在性能上超过了现有的方案，这些方案通过忽略它们的探索开销获得了优势实现。",
    "tldr": "KAIROS是一个新颖的机器学习推断系统，利用异构计算硬件池和优化的推断查询分配，实现了最大化的查询吞吐量同时满足服务质量和成本预算限制。在产业级深度学习模型评估中，KAIROS相比于最优异构方案吞吐量增加了2倍，并且超过了其他现有方案。",
    "en_tdlr": "KAIROS is a novel machine learning inference system that maximizes query throughput while meeting QoS and cost budget constraints using a pool of heterogeneous compute hardware and optimal inference query distribution at runtime. In evaluations using industrial-grade deep learning models, KAIROS outperforms state-of-the-art solutions by up to 70% and yields up to 2X the throughput of an optimal homogeneous solution."
}