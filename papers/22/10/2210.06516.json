{
    "title": "How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?. (arXiv:2210.06516v2 [cs.CR] UPDATED)",
    "abstract": "Given the volume of data needed to train modern machine learning models, external suppliers are increasingly used. However, incorporating external data poses data poisoning risks, wherein attackers manipulate their data to degrade model utility or integrity. Most poisoning defenses presume access to a set of clean data (or base set). While this assumption has been taken for granted, given the fast-growing research on stealthy poisoning attacks, a question arises: can defenders really identify a clean subset within a contaminated dataset to support defenses?  This paper starts by examining the impact of poisoned samples on defenses when they are mistakenly mixed into the base set. We analyze five defenses and find that their performance deteriorates dramatically with less than 1% poisoned points in the base set. These findings suggest that sifting out a base set with high precision is key to these defenses' performance. Motivated by these observations, we study how precise existing auto",
    "link": "http://arxiv.org/abs/2210.06516",
    "context": "Title: How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?. (arXiv:2210.06516v2 [cs.CR] UPDATED)\nAbstract: Given the volume of data needed to train modern machine learning models, external suppliers are increasingly used. However, incorporating external data poses data poisoning risks, wherein attackers manipulate their data to degrade model utility or integrity. Most poisoning defenses presume access to a set of clean data (or base set). While this assumption has been taken for granted, given the fast-growing research on stealthy poisoning attacks, a question arises: can defenders really identify a clean subset within a contaminated dataset to support defenses?  This paper starts by examining the impact of poisoned samples on defenses when they are mistakenly mixed into the base set. We analyze five defenses and find that their performance deteriorates dramatically with less than 1% poisoned points in the base set. These findings suggest that sifting out a base set with high precision is key to these defenses' performance. Motivated by these observations, we study how precise existing auto",
    "path": "papers/22/10/2210.06516.json",
    "total_tokens": 930,
    "translated_title": "如何在数据污染的情况下筛选出干净的数据子集？",
    "translated_abstract": "鉴于现代机器学习模型所需的大量数据，越来越多地使用外部供应商。然而，合并外部数据会带来数据污染的风险，攻击者可以操纵他们的数据以降低模型的效用或完整性。大多数毒化防御都假定可以访问一组干净的数据（或基础集）。然而，鉴于隐蔽性毒化攻击的快速增长研究，一个问题出现了：防御者真的能够在被污染的数据集中确定一个干净的子集以支持防御吗？本文从研究有毒样本错误地混入基础集后对防御的影响开始。我们分析了五种防御方法并发现它们的性能会在基础集中污染点少于1％时急剧下降。这些发现表明，在这些防御的性能方面，精确地筛选出一个基础集是关键。受这些观察的启发，我们研究了如何精确确定现有的自动化方法，以在污染数据中鉴别一个干净的子集。",
    "tldr": "外部数据可能引入攻击者篡改的有毒数据，为了提高毒性防御性能，需要准确地从数据集中筛选出干净的子集。",
    "en_tdlr": "External data may contain poisoned samples which degrade the utility or integrity of machine learning models. To improve defense performance against poisoning attacks, it is crucial to accurately sift out a clean subset from the dataset."
}