{
    "title": "Pareto Manifold Learning: Tackling multiple tasks via ensembles of single-task models. (arXiv:2210.09759v2 [cs.LG] UPDATED)",
    "abstract": "In Multi-Task Learning (MTL), tasks may compete and limit the performance achieved on each other, rather than guiding the optimization to a solution, superior to all its single-task trained counterparts. Since there is often not a unique solution optimal for all tasks, practitioners have to balance tradeoffs between tasks' performance, and resort to optimality in the Pareto sense. Most MTL methodologies either completely neglect this aspect, and instead of aiming at learning a Pareto Front, produce one solution predefined by their optimization schemes, or produce diverse but discrete solutions. Recent approaches parameterize the Pareto Front via neural networks, leading to complex mappings from tradeoff to objective space. In this paper, we conjecture that the Pareto Front admits a linear parameterization in parameter space, which leads us to propose \\textit{Pareto Manifold Learning}, an ensembling method in weight space. Our approach produces a continuous Pareto Front in a single trai",
    "link": "http://arxiv.org/abs/2210.09759",
    "context": "Title: Pareto Manifold Learning: Tackling multiple tasks via ensembles of single-task models. (arXiv:2210.09759v2 [cs.LG] UPDATED)\nAbstract: In Multi-Task Learning (MTL), tasks may compete and limit the performance achieved on each other, rather than guiding the optimization to a solution, superior to all its single-task trained counterparts. Since there is often not a unique solution optimal for all tasks, practitioners have to balance tradeoffs between tasks' performance, and resort to optimality in the Pareto sense. Most MTL methodologies either completely neglect this aspect, and instead of aiming at learning a Pareto Front, produce one solution predefined by their optimization schemes, or produce diverse but discrete solutions. Recent approaches parameterize the Pareto Front via neural networks, leading to complex mappings from tradeoff to objective space. In this paper, we conjecture that the Pareto Front admits a linear parameterization in parameter space, which leads us to propose \\textit{Pareto Manifold Learning}, an ensembling method in weight space. Our approach produces a continuous Pareto Front in a single trai",
    "path": "papers/22/10/2210.09759.json",
    "total_tokens": 815,
    "translated_title": "Pareto流形学习：通过单任务模型集成解决多任务问题",
    "translated_abstract": "这篇论文介绍了一种多任务学习的方法，即Pareto流形学习。这种方法通过集成单任务模型，实现同时解决多个任务的效果。在这种方法中，通过在参数空间中进行线性参数化，实现持续的Pareto前沿产生，其可以在一个模型中实现多个任务的优化，同时提高所有任务的效果。",
    "tldr": "提出了Pareto流形学习方法，通过在参数空间中进行线性参数化，实现持续的Pareto前沿产生，并可以在一个模型中实现多个任务的优化，同时提高所有任务的效果。",
    "en_tdlr": "A new method called Pareto manifold learning is proposed for multi-task learning, which uses ensembling of single-task models and linear parameterization in parameter space to generate a continuous Pareto front and optimize multiple tasks simultaneously."
}