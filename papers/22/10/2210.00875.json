{
    "title": "Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection. (arXiv:2210.00875v2 [cs.CR] UPDATED)",
    "abstract": "Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean",
    "link": "http://arxiv.org/abs/2210.00875",
    "context": "Title: Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection. (arXiv:2210.00875v2 [cs.CR] UPDATED)\nAbstract: Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean",
    "path": "papers/22/10/2210.00875.json",
    "total_tokens": 1104,
    "translated_title": "无目标后门水印：朝着无害和隐蔽的数据集版权保护迈进",
    "translated_abstract": "深度神经网络已在实践中展现出了其优越性。可以说，深度神经网络的快速发展在很大程度上得益于高质量（开源）数据集，研究人员和开发人员可以在此基础上轻松地评估和改进他们的学习方法。由于数据收集通常是耗时甚至昂贵的，如何保护其版权具有重要意义并值得进一步探索。本文重新审视了数据集的所有权验证。我们发现，现有的验证方法由于有目标的后门水印的特性，会在受保护的数据集上训练的深度神经网络中引入新的安全风险。为了解决这个问题，在本文中，我们探讨了无目标后门水印方案，其中异常的模型行为不是确定性的。具体而言，我们介绍了两个分散度，并证明了它们的相关性，基于此我们在受污染标签和干净标签攻击设置下设计了无目标后门水印。实验结果表明，我们提出的方法在水印提取的准确性和模型性能上都能够达到甚至超过现有最先进的方案。此外，我们提出的方法还被证明对模型性能无害且隐蔽，不会引入任何可检测的扭曲或故障。",
    "tldr": "本文提出了一种针对数据集版权保护的无害和隐蔽的无目标后门水印方案，可以达到与最先进方案相当或更好的水印效果，并证明对模型性能无害且隐蔽。",
    "en_tdlr": "This paper proposes a harmless and stealthy untargeted backdoor watermarking scheme for dataset copyright protection, which can achieve comparable or better watermarking effectiveness than the state-of-the-art schemes, without introducing any detectable distortion or malfunction to the model performance."
}