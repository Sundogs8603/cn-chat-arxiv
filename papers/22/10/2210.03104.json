{
    "title": "Distributionally Adaptive Meta Reinforcement Learning. (arXiv:2210.03104v2 [cs.LG] UPDATED)",
    "abstract": "Meta-reinforcement learning algorithms provide a data-driven way to acquire policies that quickly adapt to many tasks with varying rewards or dynamics functions. However, learned meta-policies are often effective only on the exact task distribution on which they were trained and struggle in the presence of distribution shift of test-time rewards or transition dynamics. In this work, we develop a framework for meta-RL algorithms that are able to behave appropriately under test-time distribution shifts in the space of tasks. Our framework centers on an adaptive approach to distributional robustness that trains a population of meta-policies to be robust to varying levels of distribution shift. When evaluated on a potentially shifted test-time distribution of tasks, this allows us to choose the meta-policy with the most appropriate level of robustness, and use it to perform fast adaptation. We formally show how our framework allows for improved regret under distribution shift, and empirica",
    "link": "http://arxiv.org/abs/2210.03104",
    "context": "Title: Distributionally Adaptive Meta Reinforcement Learning. (arXiv:2210.03104v2 [cs.LG] UPDATED)\nAbstract: Meta-reinforcement learning algorithms provide a data-driven way to acquire policies that quickly adapt to many tasks with varying rewards or dynamics functions. However, learned meta-policies are often effective only on the exact task distribution on which they were trained and struggle in the presence of distribution shift of test-time rewards or transition dynamics. In this work, we develop a framework for meta-RL algorithms that are able to behave appropriately under test-time distribution shifts in the space of tasks. Our framework centers on an adaptive approach to distributional robustness that trains a population of meta-policies to be robust to varying levels of distribution shift. When evaluated on a potentially shifted test-time distribution of tasks, this allows us to choose the meta-policy with the most appropriate level of robustness, and use it to perform fast adaptation. We formally show how our framework allows for improved regret under distribution shift, and empirica",
    "path": "papers/22/10/2210.03104.json",
    "total_tokens": 892,
    "translated_title": "分布自适应元强化学习",
    "translated_abstract": "元强化学习算法提供了一种数据驱动的方式来获取能够快速适应多个具有不同奖励或动力学函数的任务的策略。然而，学习得到的元策略通常只在其训练时的精确任务分布上有效，在测试时的奖励或过渡动力学的分布发生变化时会变得困难。在这项工作中，我们开发了一个框架，用于元强化学习算法在任务空间中适应性地应对测试时的分布转变。我们的框架以自适应的分布鲁棒性方法为核心，训练一群具有不同程度分布转变鲁棒性的元策略。当在可能发生分布转变的测试任务分布上评估时，我们可以选择具有最合适鲁棒性水平的元策略，并使用它进行快速适应。我们在理论上证明了我们的框架在分布转变下能够改善遗憾，并通过实验证明了我们方法的有效性。",
    "tldr": "本论文介绍了一种基于自适应分布鲁棒性的元强化学习算法框架，该框架能够在测试任务分布发生变化时行为适应，提供了对分布转变的改善遗憾的能力。"
}