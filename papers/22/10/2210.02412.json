{
    "title": "Why Random Pruning Is All We Need to Start Sparse. (arXiv:2210.02412v2 [cs.LG] UPDATED)",
    "abstract": "Random masks define surprisingly effective sparse neural network models, as has been shown empirically. The resulting sparse networks can often compete with dense architectures and state-of-the-art lottery ticket pruning algorithms, even though they do not rely on computationally expensive prune-train iterations and can be drawn initially without significant computational overhead. We offer a theoretical explanation of how random masks can approximate arbitrary target networks if they are wider by a logarithmic factor in the inverse sparsity $1 / \\log(1/\\text{sparsity})$. This overparameterization factor is necessary at least for 3-layer random networks, which elucidates the observed degrading performance of random networks at higher sparsity. At moderate to high sparsity levels, however, our results imply that sparser networks are contained within random source networks so that any dense-to-sparse training scheme can be turned into a computationally more efficient sparse-to-sparse one",
    "link": "http://arxiv.org/abs/2210.02412",
    "context": "Title: Why Random Pruning Is All We Need to Start Sparse. (arXiv:2210.02412v2 [cs.LG] UPDATED)\nAbstract: Random masks define surprisingly effective sparse neural network models, as has been shown empirically. The resulting sparse networks can often compete with dense architectures and state-of-the-art lottery ticket pruning algorithms, even though they do not rely on computationally expensive prune-train iterations and can be drawn initially without significant computational overhead. We offer a theoretical explanation of how random masks can approximate arbitrary target networks if they are wider by a logarithmic factor in the inverse sparsity $1 / \\log(1/\\text{sparsity})$. This overparameterization factor is necessary at least for 3-layer random networks, which elucidates the observed degrading performance of random networks at higher sparsity. At moderate to high sparsity levels, however, our results imply that sparser networks are contained within random source networks so that any dense-to-sparse training scheme can be turned into a computationally more efficient sparse-to-sparse one",
    "path": "papers/22/10/2210.02412.json",
    "total_tokens": 989,
    "translated_title": "为什么随机剪枝足以启动稀疏神经网络",
    "translated_abstract": "实验证明，随机剪枝可以定义出非常有效的稀疏神经网络模型。得到的稀疏网络通常可以与密集结构和最先进的“中彩票”剪枝算法竞争，尽管它们不依赖于计算昂贵的剪枝-训练迭代，并且可以在初始阶段绘制而不需要过多的计算开销。本文提供了一个理论解释，即如果随机掩码的宽度比稀疏性的倒数的对数因子大，则它们可以近似任意目标网络。这种超过参数化因子至少对于三层随机网络是必要的，这解释了随机网络在更高的稀疏度下观察到的性能下降。然而，在中等到高稀疏度水平下，我们的结果暗示着更稀疏的网络包含在随机的源网络中，因此任何从密集到稀疏的训练方案都可以转化为计算上更有效的从稀疏到稀疏的训练方案。",
    "tldr": "本文发现，如果随机掩码的宽度比稀疏性的倒数的对数因子大，则它们可以近似任意目标网络，因此随机剪枝足以启动稀疏神经网络，而且任何从密集到稀疏的训练方案都可以转化为计算上更有效的从稀疏到稀疏的训练方案。",
    "en_tdlr": "This paper finds that if random masks are wider by a logarithmic factor in the inverse sparsity, they can approximate arbitrary target networks, indicating that random pruning is all we need to start sparse neural networks. Additionally, any dense-to-sparse training scheme can be turned into a computationally more efficient sparse-to-sparse one."
}