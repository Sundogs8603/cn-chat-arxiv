{
    "title": "VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment. (arXiv:2210.04135v3 [cs.CV] UPDATED)",
    "abstract": "Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision-Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the use of expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-mo",
    "link": "http://arxiv.org/abs/2210.04135",
    "context": "Title: VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment. (arXiv:2210.04135v3 [cs.CV] UPDATED)\nAbstract: Vision-language pre-training (VLP) has recently proven highly effective for various uni- and multi-modal downstream applications. However, most existing end-to-end VLP methods use high-resolution image-text box data to perform well on fine-grained region-level tasks, such as object detection, segmentation, and referring expression comprehension. Unfortunately, such high-resolution images with accurate bounding box annotations are expensive to collect and use for supervision at scale. In this work, we propose VoLTA (Vision-Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the use of expensive box annotations. VoLTA adopts graph optimal transport-based weakly-supervised alignment on local image patches and text tokens to germinate an explicit, self-normalized, and interpretable low-level matching criterion. In addition, VoLTA pushes multi-mo",
    "path": "papers/22/10/2210.04135.json",
    "total_tokens": 916,
    "translated_title": "VoLTA: 弱监督本地特征对齐的视觉语言Transformer",
    "translated_abstract": "近期研究表明，视觉语言预训练（VLP）在各种单模态和多模态下游应用中非常有效。然而，大多数现有的端到端VLP方法利用高分辨率的图像-文本框数据在精细化的区域级任务（如目标检测、分割和指代表达理解）上表现良好。不幸的是，这样的高分辨率图像配以准确的边界框标注昂贵且难以大规模采集和使用。在这项工作中，我们提出了VoLTA（使用弱监督本地特征对齐的视觉语言Transformer），这是一种新的VLP范式，只使用图像-标题数据，但可以实现细粒度区域级图像理解，避免使用昂贵的框标注。VoLTA采用基于图优化传输的弱监督本地图像块和文本标记对齐，以生成一个显式、自标准化和可解释的低级匹配准则。",
    "tldr": "VoLTA是一种采用弱监督对齐策略的视觉语言Transformer模型，通过在本地特征上进行图像和文本的对齐，实现了细粒度的图像理解，无需昂贵的边界框标注。",
    "en_tdlr": "VoLTA is a vision-language Transformer model that adopts a weakly-supervised alignment strategy, achieving fine-grained image understanding by aligning local features of images with text, without the need for expensive bounding box annotations."
}