{
    "title": "Policy Gradient for Reinforcement Learning with General Utilities. (arXiv:2210.00991v2 [cs.LG] UPDATED)",
    "abstract": "In Reinforcement Learning (RL), the goal of agents is to discover an optimal policy that maximizes the expected cumulative rewards. This objective may also be viewed as finding a policy that optimizes a linear function of its state-action occupancy measure, hereafter referred as Linear RL. However, many supervised and unsupervised RL problems are not covered in the Linear RL framework, such as apprenticeship learning, pure exploration and variational intrinsic control, where the objectives are non-linear functions of the occupancy measures. RL with non-linear utilities looks unwieldy, as methods like Bellman equation, value iteration, policy gradient, dynamic programming that had tremendous success in Linear RL, fail to trivially generalize. In this paper, we derive the policy gradient theorem for RL with general utilities. The policy gradient theorem proves to be a cornerstone in Linear RL due to its elegance and ease of implementability. Our policy gradient theorem for RL with genera",
    "link": "http://arxiv.org/abs/2210.00991",
    "context": "Title: Policy Gradient for Reinforcement Learning with General Utilities. (arXiv:2210.00991v2 [cs.LG] UPDATED)\nAbstract: In Reinforcement Learning (RL), the goal of agents is to discover an optimal policy that maximizes the expected cumulative rewards. This objective may also be viewed as finding a policy that optimizes a linear function of its state-action occupancy measure, hereafter referred as Linear RL. However, many supervised and unsupervised RL problems are not covered in the Linear RL framework, such as apprenticeship learning, pure exploration and variational intrinsic control, where the objectives are non-linear functions of the occupancy measures. RL with non-linear utilities looks unwieldy, as methods like Bellman equation, value iteration, policy gradient, dynamic programming that had tremendous success in Linear RL, fail to trivially generalize. In this paper, we derive the policy gradient theorem for RL with general utilities. The policy gradient theorem proves to be a cornerstone in Linear RL due to its elegance and ease of implementability. Our policy gradient theorem for RL with genera",
    "path": "papers/22/10/2210.00991.json",
    "total_tokens": 933,
    "translated_title": "强化学习中具有通用效用的策略梯度研究",
    "translated_abstract": "在强化学习中，智能体的目标是发现最大化期望累积奖励的最优策略。这个目标也可以看作是找到一个优化状态-动作占有度量的线性函数的策略，即线性强化学习。然而，许多监督学习和无监督学习的强化学习问题并不适用于线性强化学习框架，比如学徒学习、纯探索和变分内在控制，其中的目标是占有度量的非线性函数。非线性效用的强化学习看起来不那么方便，因为在线性强化学习中取得巨大成功的贝尔曼方程、值迭代、策略梯度、动态规划等方法无法直接泛化。在本文中，我们推导了处理具有通用效用的强化学习的策略梯度定理。策略梯度定理因其简洁易实现的特性而成为线性强化学习中的基石。",
    "tldr": "本文研究了强化学习中具有通用效用的策略梯度。传统的线性强化学习方法无法直接应用于非线性效用的问题，而本文推导出的策略梯度定理为解决这个问题提供了新的思路和方法。"
}