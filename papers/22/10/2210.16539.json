{
    "title": "Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection. (arXiv:2210.16539v2 [cs.CL] UPDATED)",
    "abstract": "Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating preventive care and to delay further progression. Speech based automatic AD screening systems provide a non-intrusive and more scalable alternative to other clinical screening techniques. Textual embedding features produced by pre-trained language models (PLMs) such as BERT are widely used in such systems. However, PLM domain fine-tuning is commonly based on the masked word or sentence prediction costs that are inconsistent with the back-end AD detection task. To this end, this paper investigates the use of prompt-based fine-tuning of PLMs that consistently uses AD classification errors as the training objective function. Disfluency features based on hesitation or pause filler token frequencies are further incorporated into prompt phrases during PLM fine-tuning. The decision voting based combination among systems using different PLMs (BERT and RoBERTa) or systems with different fine-tuning paradigms (conventional ma",
    "link": "http://arxiv.org/abs/2210.16539",
    "context": "Title: Exploiting prompt learning with pre-trained language models for Alzheimer's Disease detection. (arXiv:2210.16539v2 [cs.CL] UPDATED)\nAbstract: Early diagnosis of Alzheimer's disease (AD) is crucial in facilitating preventive care and to delay further progression. Speech based automatic AD screening systems provide a non-intrusive and more scalable alternative to other clinical screening techniques. Textual embedding features produced by pre-trained language models (PLMs) such as BERT are widely used in such systems. However, PLM domain fine-tuning is commonly based on the masked word or sentence prediction costs that are inconsistent with the back-end AD detection task. To this end, this paper investigates the use of prompt-based fine-tuning of PLMs that consistently uses AD classification errors as the training objective function. Disfluency features based on hesitation or pause filler token frequencies are further incorporated into prompt phrases during PLM fine-tuning. The decision voting based combination among systems using different PLMs (BERT and RoBERTa) or systems with different fine-tuning paradigms (conventional ma",
    "path": "papers/22/10/2210.16539.json",
    "total_tokens": 1077,
    "translated_title": "基于预训练语言模型的提示学习在阿尔茨海默病检测中的应用研究",
    "translated_abstract": "阿尔茨海默病（AD）的早期诊断对于促进预防性护理和延缓疾病进程非常关键，基于语音的自动AD筛查系统为其他临床筛查技术提供了一种非侵入性且更具扩展性的替代方案。预训练语言模型（PLM）如BERT产生的文本嵌入特征被广泛应用在这样的系统中。然而，PLM领域微调通常基于掩蔽词或句子预测成本，这与后端AD检测任务不一致。因此，本文研究了使用基于提示的PLM微调，这种微调一致地使用AD分类错误作为训练目标函数。在PLM微调期间，在提示短语中进一步加入了基于犹豫或暂停填充符令牌频率的不流畅特征。对于使用不同PLMs（BERT和RoBERTa）或使用不同微调范例（传统机器学习和提示学习）的系统，基于决策投票的组合进一步增强了AD检测的性能。在基准AD语音数据集上，所提出的框架达到了高达95.1%的准确率，表现处于同类研究的最前沿。",
    "tldr": "本文提出一种基于提示学习的预训练语言模型方法，加入不流畅特征提高阿尔茨海默病检测性能。实验结果表明该方法在基准数据集上取得了最佳表现，最高准确率达到95.1%。",
    "en_tdlr": "This paper proposes a prompt-based fine-tuning method of pre-trained language models, incorporating disfluency features to improve Alzheimer's disease detection performance. Experimental results show that the proposed framework achieves state-of-the-art performance on benchmark datasets, with up to 95.1% accuracy."
}