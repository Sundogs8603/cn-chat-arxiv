{
    "title": "MARS: Meta-Learning as Score Matching in the Function Space. (arXiv:2210.13319v3 [cs.LG] UPDATED)",
    "abstract": "Meta-learning aims to extract useful inductive biases from a set of related datasets. In Bayesian meta-learning, this is typically achieved by constructing a prior distribution over neural network parameters. However, specifying families of computationally viable prior distributions over the high-dimensional neural network parameters is difficult. As a result, existing approaches resort to meta-learning restrictive diagonal Gaussian priors, severely limiting their expressiveness and performance. To circumvent these issues, we approach meta-learning through the lens of functional Bayesian neural network inference, which views the prior as a stochastic process and performs inference in the function space. Specifically, we view the meta-training tasks as samples from the data-generating process and formalize meta-learning as empirically estimating the law of this stochastic process. Our approach can seamlessly acquire and represent complex prior knowledge by meta-learning the score functi",
    "link": "http://arxiv.org/abs/2210.13319",
    "context": "Title: MARS: Meta-Learning as Score Matching in the Function Space. (arXiv:2210.13319v3 [cs.LG] UPDATED)\nAbstract: Meta-learning aims to extract useful inductive biases from a set of related datasets. In Bayesian meta-learning, this is typically achieved by constructing a prior distribution over neural network parameters. However, specifying families of computationally viable prior distributions over the high-dimensional neural network parameters is difficult. As a result, existing approaches resort to meta-learning restrictive diagonal Gaussian priors, severely limiting their expressiveness and performance. To circumvent these issues, we approach meta-learning through the lens of functional Bayesian neural network inference, which views the prior as a stochastic process and performs inference in the function space. Specifically, we view the meta-training tasks as samples from the data-generating process and formalize meta-learning as empirically estimating the law of this stochastic process. Our approach can seamlessly acquire and represent complex prior knowledge by meta-learning the score functi",
    "path": "papers/22/10/2210.13319.json",
    "total_tokens": 836,
    "translated_title": "MARS: 函数空间中基于分数匹配的元学习",
    "translated_abstract": "元学习旨在从一组相关的数据集中提取有用的归纳偏置。在贝叶斯元学习中，通常通过构建神经网络参数的先验分布来实现这一点。然而，指定一组可行的高维神经网络参数的先验分布族是困难的。因此，现有方法采用元学习限制性的对角高斯先验，严重限制了其表达能力和性能。为了解决这些问题，我们通过函数贝叶斯神经网络推理的视角来看待元学习，将先验视为随机过程，在函数空间中执行推理。具体来说，我们将元训练任务视为从数据生成过程中的样本，并将元学习形式化为经验估计这个随机过程的定律。我们的方法可以通过元学习分数函数，无缝获取和表示复杂的先验知识。",
    "tldr": "本文提出了一种新的元学习方法，通过在函数空间中执行推理，从而避免了指定高维神经网络参数的先验分布族时的限制，可以无缝获取和表示复杂的先验知识。",
    "en_tdlr": "This paper proposes a new meta-learning method that performs inference in the function space, avoiding limitations when specifying prior distributions over high-dimensional neural network parameters, and can seamlessly acquire and represent complex prior knowledge by meta-learning the score function."
}