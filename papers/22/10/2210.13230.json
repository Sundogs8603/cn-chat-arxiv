{
    "title": "An Experimental Study of Dimension Reduction Methods on Machine Learning Algorithms with Applications to Psychometrics. (arXiv:2210.13230v3 [cs.LG] UPDATED)",
    "abstract": "Developing interpretable machine learning models has become an increasingly important issue. One way in which data scientists have been able to develop interpretable models has been to use dimension reduction techniques. In this paper, we examine several dimension reduction techniques including two recent approaches developed in the network psychometrics literature called exploratory graph analysis (EGA) and unique variable analysis (UVA). We compared EGA and UVA with two other dimension reduction techniques common in the machine learning literature (principal component analysis and independent component analysis) as well as no reduction to the variables real data. We show that EGA and UVA perform as well as the other reduction techniques or no reduction. Consistent with previous literature, we show that dimension reduction can decrease, increase, or provide the same accuracy as no reduction of variables. Our tentative results find that dimension reduction tends to lead to better perfo",
    "link": "http://arxiv.org/abs/2210.13230",
    "context": "Title: An Experimental Study of Dimension Reduction Methods on Machine Learning Algorithms with Applications to Psychometrics. (arXiv:2210.13230v3 [cs.LG] UPDATED)\nAbstract: Developing interpretable machine learning models has become an increasingly important issue. One way in which data scientists have been able to develop interpretable models has been to use dimension reduction techniques. In this paper, we examine several dimension reduction techniques including two recent approaches developed in the network psychometrics literature called exploratory graph analysis (EGA) and unique variable analysis (UVA). We compared EGA and UVA with two other dimension reduction techniques common in the machine learning literature (principal component analysis and independent component analysis) as well as no reduction to the variables real data. We show that EGA and UVA perform as well as the other reduction techniques or no reduction. Consistent with previous literature, we show that dimension reduction can decrease, increase, or provide the same accuracy as no reduction of variables. Our tentative results find that dimension reduction tends to lead to better perfo",
    "path": "papers/22/10/2210.13230.json",
    "total_tokens": 978,
    "translated_title": "应用于心理测量学的降维方法对机器学习算法的实验研究",
    "translated_abstract": "开发可解释的机器学习模型已成为一个越来越重要的问题。降维技术是数据科学家开发可解释模型的一种方式。在本文中，我们研究了几种降维技术，包括心理测量学领域最近开发的探索性图分析（EGA）和唯一变量分析（UVA）等两种方法。我们将EGA和UVA与机器学习文献中常见的两种降维技术（主成分分析和独立成分分析）以及没有降维的变量进行比较。我们显示EGA和UVA与其他降维技术或者不降维相当。一致于前期的文献，我们发现降维可能会降低、提高或者不影响变量的准确性。我们初步的结果发现降维往往导致更好的性能。",
    "tldr": "本文主要研究了心理测量学领域最近开发的探索性图分析（EGA）和唯一变量分析（UVA）等两种降维技术与机器学习文献中常见的两种降维技术（主成分分析和独立成分分析）在减少数据维度方面的效果，并发现降维可能会降低、提高或者不影响变量的准确性。"
}