{
    "title": "Decentralized Stochastic Bilevel Optimization with Improved per-Iteration Complexity. (arXiv:2210.12839v2 [math.OC] UPDATED)",
    "abstract": "Bilevel optimization recently has received tremendous attention due to its great success in solving important machine learning problems like meta learning, reinforcement learning, and hyperparameter optimization. Extending single-agent training on bilevel problems to the decentralized setting is a natural generalization, and there has been a flurry of work studying decentralized bilevel optimization algorithms. However, it remains unknown how to design the distributed algorithm with sample complexity and convergence rate comparable to SGD for stochastic optimization, and at the same time without directly computing the exact Hessian or Jacobian matrices. In this paper we propose such an algorithm. More specifically, we propose a novel decentralized stochastic bilevel optimization (DSBO) algorithm that only requires first order stochastic oracle, Hessian-vector product and Jacobian-vector product oracle. The sample complexity of our algorithm matches the currently best known results for ",
    "link": "http://arxiv.org/abs/2210.12839",
    "context": "Title: Decentralized Stochastic Bilevel Optimization with Improved per-Iteration Complexity. (arXiv:2210.12839v2 [math.OC] UPDATED)\nAbstract: Bilevel optimization recently has received tremendous attention due to its great success in solving important machine learning problems like meta learning, reinforcement learning, and hyperparameter optimization. Extending single-agent training on bilevel problems to the decentralized setting is a natural generalization, and there has been a flurry of work studying decentralized bilevel optimization algorithms. However, it remains unknown how to design the distributed algorithm with sample complexity and convergence rate comparable to SGD for stochastic optimization, and at the same time without directly computing the exact Hessian or Jacobian matrices. In this paper we propose such an algorithm. More specifically, we propose a novel decentralized stochastic bilevel optimization (DSBO) algorithm that only requires first order stochastic oracle, Hessian-vector product and Jacobian-vector product oracle. The sample complexity of our algorithm matches the currently best known results for ",
    "path": "papers/22/10/2210.12839.json",
    "total_tokens": 1005,
    "translated_title": "基于改进迭代复杂度的分散随机双层优化算法",
    "translated_abstract": "双层优化近来因其在解决重要的机器学习问题中（如元学习、强化学习和超参数优化）的巨大成功而受到极大关注。将单智能体训练扩展到分散情境下是一种自然的推广方式，而这也引发了研究分散双层优化算法浪潮。然而，如何设计具有与随机梯度下降（SGD）的样本复杂度和收敛速度相当的分散算法，同时又不需要直接计算精确的黑塞矩阵或雅可比矩阵，目前仍是未知的问题。在本文中，我们提出了这样的算法。更具体来说，我们提出了一种新颖的分散随机双层优化（DSBO）算法，该算法仅需要第一阶的随机 Oracle、黑塞矢量积和雅可比矢量积Oracle。我们的算法样本复杂度与目前已知的最佳结果相匹配。",
    "tldr": "本论文提出了一种名为“分散随机双层优化”的算法，它可以在不计算精确的黑塞矩阵或雅可比矩阵的情况下，仅依靠第一阶的随机Oracle、黑塞矢量积和雅可比矢量积Oracle，设计具有与随机梯度下降（SGD）的样本复杂度和收敛速度相当的分散算法。",
    "en_tdlr": "This paper proposes a decentralized stochastic bilevel optimization algorithm called \"DSBO\", which can design a decentralized algorithm with sample complexity and convergence rate comparable to SGD for stochastic optimization by only relying on first-order stochastic oracle, Hessian-vector product and Jacobian-vector product oracle, without computing exact Hessian or Jacobian matrices."
}