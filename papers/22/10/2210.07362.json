{
    "title": "Can Demographic Factors Improve Text Classification? Revisiting Demographic Adaptation in the Age of Transformers. (arXiv:2210.07362v2 [cs.CL] UPDATED)",
    "abstract": "Demographic factors (e.g., gender or age) shape our language. Previous work showed that incorporating demographic factors can consistently improve performance for various NLP tasks with traditional NLP models. In this work, we investigate whether these previous findings still hold with state-of-the-art pretrained Transformer-based language models (PLMs). We use three common specialization methods proven effective for incorporating external knowledge into pretrained Transformers (e.g., domain-specific or geographic knowledge). We adapt the language representations for the demographic dimensions of gender and age, using continuous language modeling and dynamic multi-task learning for adaptation, where we couple language modeling objectives with the prediction of demographic classes. Our results, when employing a multilingual PLM, show substantial gains in task performance across four languages (English, German, French, and Danish), which is consistent with the results of previous work. H",
    "link": "http://arxiv.org/abs/2210.07362",
    "context": "Title: Can Demographic Factors Improve Text Classification? Revisiting Demographic Adaptation in the Age of Transformers. (arXiv:2210.07362v2 [cs.CL] UPDATED)\nAbstract: Demographic factors (e.g., gender or age) shape our language. Previous work showed that incorporating demographic factors can consistently improve performance for various NLP tasks with traditional NLP models. In this work, we investigate whether these previous findings still hold with state-of-the-art pretrained Transformer-based language models (PLMs). We use three common specialization methods proven effective for incorporating external knowledge into pretrained Transformers (e.g., domain-specific or geographic knowledge). We adapt the language representations for the demographic dimensions of gender and age, using continuous language modeling and dynamic multi-task learning for adaptation, where we couple language modeling objectives with the prediction of demographic classes. Our results, when employing a multilingual PLM, show substantial gains in task performance across four languages (English, German, French, and Danish), which is consistent with the results of previous work. H",
    "path": "papers/22/10/2210.07362.json",
    "total_tokens": 964,
    "translated_title": "人口统计因素是否能够改善文本分类？在变压器时代重新审视人口统计适应性",
    "translated_abstract": "人口统计因素（例如性别或年龄）塑造了我们的语言。以往的研究表明，利用传统的自然语言处理模型整合人口统计因素可以在各种NLP任务中持续提高性能。本文探究先前的发现是否仍然适用于最先进的预训练变压器语言模型(PLMs)。我们采用了三种常见的专业化方法，将装载了外部知识（例如领域专业知识或地理知识）的预训练变压器进行了改进 。我们使用连续语言建模和动态多任务学习以用于适应性的性别和年龄的语言表示，其中我们结合语言建模目标和人口统计分类的预测。当使用多语言PLM时，我们的结果表明，在四种语言（英语、德语、法语和丹麦语）中，任务的性能得到了显著的提高，这与先前的研究结果一致。",
    "tldr": "论文探讨利用人口统计因素加强文本分类的效果与先前一样在最新的变压器语言模型中存在。该研究使用连续语言建模和动态多任务学习的方法来适应语言中特定年龄和性别的方面，通过结合语言建模目标和人口统计学的预测，结果表明在四种语言中任务表现得到了显著的提高。",
    "en_tdlr": "This paper explores whether incorporating demographic factors can still enhance text classification with the state-of-the-art Transformer-based language models. The study uses continuous language modeling and dynamic multi-task learning to adapt language representations for specific age and gender dimensions, and achieves a significant improvement in task performance across four languages."
}