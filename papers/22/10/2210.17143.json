{
    "title": "Exploring Train and Test-Time Augmentations for Audio-Language Learning. (arXiv:2210.17143v2 [cs.SD] UPDATED)",
    "abstract": "In this paper, we aim to unveil the impact of data augmentation in audio-language multi-modal learning, which has not been explored despite its importance. We explore various augmentation methods at not only train-time but also test-time and find out that proper data augmentation can lead to substantial improvements. Specifically, applying our proposed audio-language paired augmentation PairMix, which is the first multi-modal audio-language augmentation method, outperforms the baselines for both automated audio captioning and audio-text retrieval tasks. To fully take advantage of data augmentation, we also present multi-level test-time augmentation (Multi-TTA) for the test-time. We successfully incorporate the two proposed methods and uni-modal augmentations and achieve 47.5 SPIDEr on audio captioning, which is an 18.2% relative increase over the baseline. In audio-text retrieval, the proposed methods also show an improvement in performance as well.",
    "link": "http://arxiv.org/abs/2210.17143",
    "context": "Title: Exploring Train and Test-Time Augmentations for Audio-Language Learning. (arXiv:2210.17143v2 [cs.SD] UPDATED)\nAbstract: In this paper, we aim to unveil the impact of data augmentation in audio-language multi-modal learning, which has not been explored despite its importance. We explore various augmentation methods at not only train-time but also test-time and find out that proper data augmentation can lead to substantial improvements. Specifically, applying our proposed audio-language paired augmentation PairMix, which is the first multi-modal audio-language augmentation method, outperforms the baselines for both automated audio captioning and audio-text retrieval tasks. To fully take advantage of data augmentation, we also present multi-level test-time augmentation (Multi-TTA) for the test-time. We successfully incorporate the two proposed methods and uni-modal augmentations and achieve 47.5 SPIDEr on audio captioning, which is an 18.2% relative increase over the baseline. In audio-text retrieval, the proposed methods also show an improvement in performance as well.",
    "path": "papers/22/10/2210.17143.json",
    "total_tokens": 904,
    "translated_title": "探索音频语言学习中的训练和测试增强",
    "translated_abstract": "本文旨在揭示音频语言多模态学习中数据增强的影响，尽管其重要性尚未被探索。我们探索了各种增强方法，不仅在训练时，而且在测试时也进行了研究，并发现适当的数据增强可以带来显著的改进。具体而言，应用我们提出的音频语言配对增强PairMix（这是第一个多模态音频语言增强方法），在自动化音频字幕和音频文本检索任务中均优于基线。为了充分利用数据增强，我们还提出了多层测试增强（Multi-TTA）进行测试。我们成功地将两种提出的方法和单模增强组合起来，在音频字幕方面实现了47.5 SPIDEr，相对于基线提高了18.2％。在音频文本检索方面，所提出的方法也表现出了性能的提高。",
    "tldr": "本研究揭示了数据增强对音频语言多模态学习的重要性。作者提出了音频语言配对增强和多层测试增强方法，成功地将它们与单模态增强结合起来，在自动化音频字幕和音频文本检索任务中取得了显著的进展。",
    "en_tdlr": "This study reveals the importance of data augmentation in audio-language multi-modal learning. The authors proposed an audio-language paired augmentation and multi-level test-time augmentation methods, and successfully combined them with uni-modal augmentations, achieving significant advances in automated audio captioning and audio-text retrieval tasks."
}