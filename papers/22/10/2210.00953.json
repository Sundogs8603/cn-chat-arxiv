{
    "title": "Bias and Extrapolation in Markovian Linear Stochastic Approximation with Constant Stepsizes. (arXiv:2210.00953v3 [stat.ML] UPDATED)",
    "abstract": "We consider Linear Stochastic Approximation (LSA) with a constant stepsize and Markovian data. Viewing the joint process of the data and LSA iterate as a time-homogeneous Markov chain, we prove its convergence to a unique limiting and stationary distribution in Wasserstein distance and establish non-asymptotic, geometric convergence rates. Furthermore, we show that the bias vector of this limit admits an infinite series expansion with respect to the stepsize. Consequently, the bias is proportional to the stepsize up to higher order terms. This result stands in contrast with LSA under i.i.d. data, for which the bias vanishes. In the reversible chain setting, we provide a general characterization of the relationship between the bias and the mixing time of the Markovian data, establishing that they are roughly proportional to each other.  While Polyak-Ruppert tail-averaging reduces the variance of the LSA iterates, it does not affect the bias. The above characterization allows us to show ",
    "link": "http://arxiv.org/abs/2210.00953",
    "context": "Title: Bias and Extrapolation in Markovian Linear Stochastic Approximation with Constant Stepsizes. (arXiv:2210.00953v3 [stat.ML] UPDATED)\nAbstract: We consider Linear Stochastic Approximation (LSA) with a constant stepsize and Markovian data. Viewing the joint process of the data and LSA iterate as a time-homogeneous Markov chain, we prove its convergence to a unique limiting and stationary distribution in Wasserstein distance and establish non-asymptotic, geometric convergence rates. Furthermore, we show that the bias vector of this limit admits an infinite series expansion with respect to the stepsize. Consequently, the bias is proportional to the stepsize up to higher order terms. This result stands in contrast with LSA under i.i.d. data, for which the bias vanishes. In the reversible chain setting, we provide a general characterization of the relationship between the bias and the mixing time of the Markovian data, establishing that they are roughly proportional to each other.  While Polyak-Ruppert tail-averaging reduces the variance of the LSA iterates, it does not affect the bias. The above characterization allows us to show ",
    "path": "papers/22/10/2210.00953.json",
    "total_tokens": 1046,
    "translated_title": "Markov线性随机逼近中的偏差和外推问题",
    "translated_abstract": "我们考虑了具有恒定步长和Markov数据的线性随机逼近（LSA）。将数据和LSA迭代的联合过程视为时间齐次Markov链，我们证明其在Wasserstein距离下收敛到唯一的极限和稳定分布，并建立了非渐进的几何收敛速度。此外，我们表明，该极限的偏差向量可以通过步长展开为无限级数。因此，偏差与步长成比例，直至更高阶项。这个结果与i.i.d.数据下的LSA形成对比，其中偏差为零。在可逆链设置下，我们提供了偏差与Markov数据的混合时间之间关系的一般特征，建立了它们大致成正比的结论。虽然Polyak-Ruppert尾平均减少了LSA迭代的方差，但并不影响偏差。以上特征使我们能够展示",
    "tldr": "本研究研究了线性随机逼近中的偏差和外推问题。我们证明了在恒定步长和Markov数据的情况下，LSA迭代会收敛到唯一的极限和稳定分布，并建立了非渐进的几何收敛速度。我们还发现，这个极限的偏差与步长成比例，直至更高阶项。在可逆链的情况下，我们还探讨了偏差与Markov数据的混合时间之间的关系。",
    "en_tdlr": "This research investigates the issues of bias and extrapolation in linear stochastic approximation. The study proves the convergence of LSA iterates to a unique limiting and stationary distribution under a constant stepsize and Markov data, and establishes non-asymptotic geometric convergence rates. It also reveals that the bias in this limit is proportional to the stepsize, up to higher order terms. Additionally, the relationship between the bias and the mixing time of Markov data is characterized, showing that they are roughly proportional to each other in reversible chains."
}