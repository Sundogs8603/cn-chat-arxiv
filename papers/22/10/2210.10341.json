{
    "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v3 [cs.CL] UPDATED)",
    "abstract": "Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA",
    "link": "http://arxiv.org/abs/2210.10341",
    "context": "Title: BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. (arXiv:2210.10341v3 [cs.CL] UPDATED)\nAbstract: Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA",
    "path": "papers/22/10/2210.10341.json",
    "total_tokens": 943,
    "translated_title": "BioGPT：针对生物医学文本生成和挖掘的生成式预训练Transformer模型",
    "translated_abstract": "预训练语言模型在自然语言方面已经取得了很大的成功，在生物医学领域也引起了越来越多的关注。在通用语言领域中，预训练语言模型的两个主要分支是BERT（及其变体）和GPT（及其变体）。已经有很多针对生物医学领域的预训练模型，如BioBERT和PubMedBERT，在多种判别式下游生物医学任务中取得了巨大成功，但缺乏生成能力限制了它们的应用范围。本文提出了一种针对生物医学领域的生成式Transformer语言模型BioGPT，并对其进行了广泛评估，展示了我们的模型在大多数任务上优于之前的模型。特别地，在BC5CDR、KD-DTI和DDI关系提取任务上，我们分别获得了44.98％，38.42％和40.76％的F1得分，以及在PubMedQA任务上的78.2％的准确率。",
    "tldr": "BioGPT是一种针对生物医学领域的生成式Transformer语言模型，它在多项生物医学自然语言处理任务中均表现出色，尤其在关系提取任务中表现尤为突出。",
    "en_tdlr": "BioGPT is a generative Transformer language model designed for biomedical text generation and mining. It outperforms its predecessors on most biomedical NLP tasks, with notably high scores on relation extraction tasks, such as BC5CDR, KD-DTI and DDI."
}