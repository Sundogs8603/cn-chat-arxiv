{
    "title": "How do we get there? Evaluating transformer neural networks as cognitive models for English past tense inflection. (arXiv:2210.09167v2 [cs.CL] UPDATED)",
    "abstract": "There is an ongoing debate on whether neural networks can grasp the quasi-regularities in languages like humans. In a typical quasi-regularity task, English past tense inflections, the neural network model has long been criticized that it learns only to generalize the most frequent pattern, but not the regular pattern, thus can not learn the abstract categories of regular and irregular and is dissimilar to human performance. In this work, we train a set of transformer models with different settings to examine their behavior on this task. The models achieved high accuracy on unseen regular verbs and some accuracy on unseen irregular verbs. The models' performance on the regulars is heavily affected by type frequency and ratio but not token frequency and ratio, and vice versa for the irregulars. The different behaviors on the regulars and irregulars suggest that the models have some degree of symbolic learning on the regularity of the verbs. In addition, the models are weakly correlated ",
    "link": "http://arxiv.org/abs/2210.09167",
    "context": "Title: How do we get there? Evaluating transformer neural networks as cognitive models for English past tense inflection. (arXiv:2210.09167v2 [cs.CL] UPDATED)\nAbstract: There is an ongoing debate on whether neural networks can grasp the quasi-regularities in languages like humans. In a typical quasi-regularity task, English past tense inflections, the neural network model has long been criticized that it learns only to generalize the most frequent pattern, but not the regular pattern, thus can not learn the abstract categories of regular and irregular and is dissimilar to human performance. In this work, we train a set of transformer models with different settings to examine their behavior on this task. The models achieved high accuracy on unseen regular verbs and some accuracy on unseen irregular verbs. The models' performance on the regulars is heavily affected by type frequency and ratio but not token frequency and ratio, and vice versa for the irregulars. The different behaviors on the regulars and irregulars suggest that the models have some degree of symbolic learning on the regularity of the verbs. In addition, the models are weakly correlated ",
    "path": "papers/22/10/2210.09167.json",
    "total_tokens": 992,
    "translated_title": "我们如何到达那里？评估变压器神经网络作为英语过去式词形变化的认知模型。",
    "translated_abstract": "对于神经网络是否能像人类一样掌握语言的准正则性一直存在争议。在典型的准正则性任务，英语的过去式词形变化中，神经网络模型长期以来一直受到批评，认为它只学习了最常见的模式，而不是规则的模式，因此无法学习正则和非正则的抽象类别，与人类表现不同。在这项工作中，我们训练了一组具有不同设置的变压器模型，以检查它们在这项任务上的行为。这些模型对未见过的规则动词的准确率很高，对未见过的非规则动词的准确率略有提高。模型在规则动词上的表现受到类型频率和比率的影响，而不是令牌频率和比率，而在非规则动词上则相反。模型在规则动词和非规则动词上不同的行为表明它们在动词的规则性方面具有一定程度的符号学习。此外，模型之间存在微弱的相关性。",
    "tldr": "本研究通过训练变压器模型的不同设置来评估模型在英语过去式词形变化任务上的表现，结果发现模型在未见过的规则动词上有很高的准确率，在非规则动词上的表现略有提高。不同行为表明模型具有一定程度的符号学习。"
}