{
    "title": "Simultaneous off-the-grid learning of mixtures issued from a continuous dictionary",
    "abstract": "arXiv:2210.16311v2 Announce Type: replace-cross  Abstract: In this paper we observe a set, possibly a continuum, of signals corrupted by noise. Each signal is a finite mixture of an unknown number of features belonging to a continuous dictionary. The continuous dictionary is parametrized by a real non-linear parameter. We shall assume that the signals share an underlying structure by assuming that each signal has its active features included in a finite and sparse set. We formulate regularized optimization problem to estimate simultaneously the linear coefficients in the mixtures and the non-linear parameters of the features. The optimization problem is composed of a data fidelity term and a $(\\ell_1,L^p)$-penalty. We call its solution the Group-Nonlinear-Lasso and provide high probability bounds on the prediction error using certificate functions. Following recent works on the geometry of off-the-grid methods, we show that such functions can be constructed provided the parameters of t",
    "link": "https://arxiv.org/abs/2210.16311",
    "context": "Title: Simultaneous off-the-grid learning of mixtures issued from a continuous dictionary\nAbstract: arXiv:2210.16311v2 Announce Type: replace-cross  Abstract: In this paper we observe a set, possibly a continuum, of signals corrupted by noise. Each signal is a finite mixture of an unknown number of features belonging to a continuous dictionary. The continuous dictionary is parametrized by a real non-linear parameter. We shall assume that the signals share an underlying structure by assuming that each signal has its active features included in a finite and sparse set. We formulate regularized optimization problem to estimate simultaneously the linear coefficients in the mixtures and the non-linear parameters of the features. The optimization problem is composed of a data fidelity term and a $(\\ell_1,L^p)$-penalty. We call its solution the Group-Nonlinear-Lasso and provide high probability bounds on the prediction error using certificate functions. Following recent works on the geometry of off-the-grid methods, we show that such functions can be constructed provided the parameters of t",
    "path": "papers/22/10/2210.16311.json",
    "total_tokens": 858,
    "translated_title": "来自连续字典的混合物的离散学习",
    "translated_abstract": "在本文中，我们观察了一组信号，可能是一个连续信号，受到噪声的干扰。每个信号是由一个未知数量的特征混合而成，这些特征属于一个连续字典。连续字典由一个实非线性参数进行参数化。我们假设这些信号共享一个基本结构，假定每个信号的活跃特征包含在一个有限稀疏集合中。我们提出了正则化优化问题，同时估计混合物中的线性系数和特征的非线性参数。优化问题由数据保真度项和$(\\ell_1,L^p)$-惩罚项组成。我们称其解为Group-Nonlinear-Lasso，并使用证明函数对预测误差提供了高概率界限。借鉴最近关于离散学习方法几何性质的研究，我们表明只要特定参数满足条件，就可以构造这样的函数。",
    "tldr": "本文提出了一种名为Group-Nonlinear-Lasso的方法，可以同时估计混合物中的线性系数和特征的非线性参数，并使用证明函数对预测误差提供了高概率界限。"
}