{
    "title": "Perturbation Analysis of Neural Collapse. (arXiv:2210.16658v2 [cs.LG] UPDATED)",
    "abstract": "Training deep neural networks for classification often includes minimizing the training loss beyond the zero training error point. In this phase of training, a \"neural collapse\" behavior has been observed: the variability of features (outputs of the penultimate layer) of within-class samples decreases and the mean features of different classes approach a certain tight frame structure. Recent works analyze this behavior via idealized unconstrained features models where all the minimizers exhibit exact collapse. However, with practical networks and datasets, the features typically do not reach exact collapse, e.g., because deep layers cannot arbitrarily modify intermediate features that are far from being collapsed. In this paper, we propose a richer model that can capture this phenomenon by forcing the features to stay in the vicinity of a predefined features matrix (e.g., intermediate features). We explore the model in the small vicinity case via perturbation analysis and establish res",
    "link": "http://arxiv.org/abs/2210.16658",
    "context": "Title: Perturbation Analysis of Neural Collapse. (arXiv:2210.16658v2 [cs.LG] UPDATED)\nAbstract: Training deep neural networks for classification often includes minimizing the training loss beyond the zero training error point. In this phase of training, a \"neural collapse\" behavior has been observed: the variability of features (outputs of the penultimate layer) of within-class samples decreases and the mean features of different classes approach a certain tight frame structure. Recent works analyze this behavior via idealized unconstrained features models where all the minimizers exhibit exact collapse. However, with practical networks and datasets, the features typically do not reach exact collapse, e.g., because deep layers cannot arbitrarily modify intermediate features that are far from being collapsed. In this paper, we propose a richer model that can capture this phenomenon by forcing the features to stay in the vicinity of a predefined features matrix (e.g., intermediate features). We explore the model in the small vicinity case via perturbation analysis and establish res",
    "path": "papers/22/10/2210.16658.json",
    "total_tokens": 956,
    "translated_title": "神经崩溃的扰动分析",
    "translated_abstract": "对于分类任务的深度神经网络训练，在零训练误差点之后，通常涉及最小化训练损失。在这一阶段的训练中，观察到了一种“神经崩溃”行为：同一类别样本的特征（倒数第二层的输出）的变异性减少，不同类别的平均特征趋向于某个紧密的框架结构。最近的研究通过理想化的无约束特征模型分析了这种行为，在所有最小化器都出现完全崩溃的情况下。然而，在实际网络和数据集中，特征通常不会达到完全崩溃的状态，例如，因为深层无法任意修改远离崩溃状态的中间特征。在本文中，我们提出了一种更丰富的模型，可以通过强制特征留在预定义的特征矩阵（例如中间特征）的附近来捕捉这种现象。我们通过扰动分析探索了小邻域情况下的模型，并建立了系数范数不变性和特征抖动的联系。",
    "tldr": "本文提出了一种能够捕捉神经崩溃现象的模型，通过强制特征留在预定义的特征矩阵的附近来实现。通过扰动分析，在小邻域情况下建立了系数范数不变性和特征抖动的联系。",
    "en_tdlr": "This paper proposes a model that can capture the phenomenon of neural collapse by forcing the features to stay in the vicinity of a predefined features matrix. By perturbation analysis, the paper establishes the relationship between coefficient norm invariance and feature jitter in the small vicinity case."
}