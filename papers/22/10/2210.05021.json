{
    "title": "The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective",
    "abstract": "arXiv:2210.05021v3 Announce Type: replace  Abstract: Data augmentation (DA) is a powerful workhorse for bolstering performance in modern machine learning. Specific augmentations like translations and scaling in computer vision are traditionally believed to improve generalization by generating new (artificial) data from the same distribution. However, this traditional viewpoint does not explain the success of prevalent augmentations in modern machine learning (e.g. randomized masking, cutout, mixup), that greatly alter the training data distribution. In this work, we develop a new theoretical framework to characterize the impact of a general class of DA on underparameterized and overparameterized linear model generalization. Our framework reveals that DA induces implicit spectral regularization through a combination of two distinct effects: a) manipulating the relative proportion of eigenvalues of the data covariance matrix in a training-data-dependent manner, and b) uniformly boosting ",
    "link": "https://arxiv.org/abs/2210.05021",
    "context": "Title: The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective\nAbstract: arXiv:2210.05021v3 Announce Type: replace  Abstract: Data augmentation (DA) is a powerful workhorse for bolstering performance in modern machine learning. Specific augmentations like translations and scaling in computer vision are traditionally believed to improve generalization by generating new (artificial) data from the same distribution. However, this traditional viewpoint does not explain the success of prevalent augmentations in modern machine learning (e.g. randomized masking, cutout, mixup), that greatly alter the training data distribution. In this work, we develop a new theoretical framework to characterize the impact of a general class of DA on underparameterized and overparameterized linear model generalization. Our framework reveals that DA induces implicit spectral regularization through a combination of two distinct effects: a) manipulating the relative proportion of eigenvalues of the data covariance matrix in a training-data-dependent manner, and b) uniformly boosting ",
    "path": "papers/22/10/2210.05021.json",
    "total_tokens": 842,
    "translated_title": "数据增强的好坏与丑陋面：隐式谱正则化的视角",
    "translated_abstract": "数据增强（DA）是在现代机器学习中增强性能的强大工具。传统上认为，在计算机视觉中的特定增强，比如平移和缩放，可以通过从相同分布生成新的（人工）数据来改善泛化能力。然而，这种传统观点不能解释现代机器学习中流行增强的成功（例如随机遮挡、cutout、mixup），这些增强大大改变了训练数据分布。在这项工作中，我们开发了一个新的理论框架，以表征一般类别的数据增强对欠参数化和过参数化线性模型泛化的影响。我们的框架揭示了数据增强通过两种不同效应的组合：a)以训练数据为基础操纵数据协方差矩阵的特征值的相对比例，b)统一增强",
    "tldr": "数据增强在机器学习中扮演着重要作用，而该研究提出了一个新的理论框架，揭示了数据增强通过隐式谱正则化对线性模型泛化的影响。",
    "en_tdlr": "Data augmentation plays a crucial role in machine learning, and this research presents a new theoretical framework that reveals the impact of data augmentation on the generalization of linear models through implicit spectral regularization."
}