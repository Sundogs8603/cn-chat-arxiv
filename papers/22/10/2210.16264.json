{
    "title": "Efficient Speech Translation with Dynamic Latent Perceivers. (arXiv:2210.16264v2 [cs.CL] UPDATED)",
    "abstract": "Transformers have been the dominant architecture for Speech Translation in recent years, achieving significant improvements in translation quality. Since speech signals are longer than their textual counterparts, and due to the quadratic complexity of the Transformer, a down-sampling step is essential for its adoption in Speech Translation. Instead, in this research, we propose to ease the complexity by using a Perceiver encoder to map the speech inputs to a fixed-length latent representation. Furthermore, we introduce a novel way of training Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent spaces without any additional computational overhead. Speech-to-Text Perceivers with DLA can match the performance of Transformer baselines across three language pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to DLA at inference, and can be flexibly deployed with various computational budgets, without significant drops in translation quality.",
    "link": "http://arxiv.org/abs/2210.16264",
    "context": "Title: Efficient Speech Translation with Dynamic Latent Perceivers. (arXiv:2210.16264v2 [cs.CL] UPDATED)\nAbstract: Transformers have been the dominant architecture for Speech Translation in recent years, achieving significant improvements in translation quality. Since speech signals are longer than their textual counterparts, and due to the quadratic complexity of the Transformer, a down-sampling step is essential for its adoption in Speech Translation. Instead, in this research, we propose to ease the complexity by using a Perceiver encoder to map the speech inputs to a fixed-length latent representation. Furthermore, we introduce a novel way of training Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent spaces without any additional computational overhead. Speech-to-Text Perceivers with DLA can match the performance of Transformer baselines across three language pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to DLA at inference, and can be flexibly deployed with various computational budgets, without significant drops in translation quality.",
    "path": "papers/22/10/2210.16264.json",
    "total_tokens": 950,
    "translated_title": "动态潜在感知器高效语音翻译技术",
    "translated_abstract": "近年来，Transformer网络架构逐渐成为语音翻译领域的主流，实现了翻译质量的显著提升。但由于语音信号的长度较长，而Transformer的复杂度呈二次增长，因此为了使其适用于语音翻译，必须采用下采样策略。相反，本研究提出采用Perceiver编码器将语音输入映射到固定长度的潜在表征，从而简化了计算复杂度。此外，我们引入一种新的Perceiver训练方法Dynamic Latent Access(DLA)，通过解锁更大潜在空间而不增加计算负担。采用DLA的语音到文本Perceiver模型能够在MuST-C数据集的三种语言对上达到Transformer基线模型的性能水平。DLA训练的模型易于在推断时适应DLA，可以根据不同的计算预算灵活部署，翻译质量没有显著下降。",
    "tldr": "本研究提出一种采用Perceiver编码器和Dynamic Latent Access(DLA)训练的语音翻译模型，该模型能够在MuST-C数据集的三种语言对上达到Transformer基线模型的性能水平，并且在推断时易于适应不同的计算预算，翻译质量没有显著下降。"
}