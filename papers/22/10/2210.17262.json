{
    "title": "QNet: A Quantum-native Sequence Encoder Architecture. (arXiv:2210.17262v2 [cs.LG] UPDATED)",
    "abstract": "This work proposes QNet, a novel sequence encoder model that entirely inferences on the quantum computer using a minimum number of qubits. Let $n$ and $d$ represent the length of the sequence and the embedding size, respectively. The dot-product attention mechanism requires a time complexity of $O(n^2 \\cdot d)$, while QNet has merely $O(n+d)$ quantum circuit depth. In addition, we introduce ResQNet, a quantum-classical hybrid model composed of several QNet blocks linked by residual connections, as an isomorph Transformer Encoder. We evaluated our work on various natural language processing tasks, including text classification, rating score prediction, and named entity recognition. Our models exhibit compelling performance over classical state-of-the-art models with a thousand times fewer parameters. In summary, this work investigates the advantage of machine learning on near-term quantum computers in sequential data by experimenting with natural language processing tasks.",
    "link": "http://arxiv.org/abs/2210.17262",
    "context": "Title: QNet: A Quantum-native Sequence Encoder Architecture. (arXiv:2210.17262v2 [cs.LG] UPDATED)\nAbstract: This work proposes QNet, a novel sequence encoder model that entirely inferences on the quantum computer using a minimum number of qubits. Let $n$ and $d$ represent the length of the sequence and the embedding size, respectively. The dot-product attention mechanism requires a time complexity of $O(n^2 \\cdot d)$, while QNet has merely $O(n+d)$ quantum circuit depth. In addition, we introduce ResQNet, a quantum-classical hybrid model composed of several QNet blocks linked by residual connections, as an isomorph Transformer Encoder. We evaluated our work on various natural language processing tasks, including text classification, rating score prediction, and named entity recognition. Our models exhibit compelling performance over classical state-of-the-art models with a thousand times fewer parameters. In summary, this work investigates the advantage of machine learning on near-term quantum computers in sequential data by experimenting with natural language processing tasks.",
    "path": "papers/22/10/2210.17262.json",
    "total_tokens": 898,
    "translated_title": "QNet: 一种原生于量子计算机的序列编码器架构",
    "translated_abstract": "本研究提出了一种全量子计算机推理的新型序列编码器模型QNet，使用最少数量的量子比特。在该模型中，点积注意力机制的时间复杂度为$O(n^2 \\cdot d)$，而QNet的量子电路深度仅为$O(n+d)$。此外，我们介绍了ResQNet，一个由多个由残差连接相连的QNet模块组成的量子-经典混合模型，作为同构Transformer编码器。我们在各种自然语言处理任务中对我们的工作进行了评估，包括文本分类、评分预测和命名实体识别。我们的模型在经典的最先进模型上表现出了引人注目的性能，参数数量少了一千倍。",
    "tldr": "本研究探讨了在近期量子计算机上进行机器学习在序列数据中的优势，通过在自然语言处理任务中进行实验，提出了一种全量子计算机推理的新型序列编码器模型QNet，以及一个量子-经典混合模型ResQNet。这些模型在经典的最先进模型上表现出了引人注目的性能，而且参数数量更少。"
}