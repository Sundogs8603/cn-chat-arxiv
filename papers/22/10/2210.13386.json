{
    "title": "Contraction of Locally Differentially Private Mechanisms",
    "abstract": "We investigate the contraction properties of locally differentially private mechanisms. More specifically, we derive tight upper bounds on the divergence between $PK$ and $QK$ output distributions of an $\\epsilon$-LDP mechanism $K$ in terms of a divergence between the corresponding input distributions $P$ and $Q$, respectively. Our first main technical result presents a sharp upper bound on the $\\chi^2$-divergence $\\chi^2(PK}\\|QK)$ in terms of $\\chi^2(P\\|Q)$ and $\\varepsilon$. We also show that the same result holds for a large family of divergences, including KL-divergence and squared Hellinger distance. The second main technical result gives an upper bound on $\\chi^2(PK\\|QK)$ in terms of total variation distance $\\mathsf{TV}(P, Q)$ and $\\epsilon$. We then utilize these bounds to establish locally private versions of the van Trees inequality, Le Cam's, Assouad's, and the mutual information methods, which are powerful tools for bounding minimax estimation risks. These results are shown",
    "link": "https://arxiv.org/abs/2210.13386",
    "context": "Title: Contraction of Locally Differentially Private Mechanisms\nAbstract: We investigate the contraction properties of locally differentially private mechanisms. More specifically, we derive tight upper bounds on the divergence between $PK$ and $QK$ output distributions of an $\\epsilon$-LDP mechanism $K$ in terms of a divergence between the corresponding input distributions $P$ and $Q$, respectively. Our first main technical result presents a sharp upper bound on the $\\chi^2$-divergence $\\chi^2(PK}\\|QK)$ in terms of $\\chi^2(P\\|Q)$ and $\\varepsilon$. We also show that the same result holds for a large family of divergences, including KL-divergence and squared Hellinger distance. The second main technical result gives an upper bound on $\\chi^2(PK\\|QK)$ in terms of total variation distance $\\mathsf{TV}(P, Q)$ and $\\epsilon$. We then utilize these bounds to establish locally private versions of the van Trees inequality, Le Cam's, Assouad's, and the mutual information methods, which are powerful tools for bounding minimax estimation risks. These results are shown",
    "path": "papers/22/10/2210.13386.json",
    "total_tokens": 935,
    "translated_title": "局部差分隐私机制的收缩性分析",
    "translated_abstract": "我们研究了局部差分隐私机制的收缩性质。具体来说，我们给出了一个关于输入分布之间的差异和输出分布之间的差异的上界，用于度量ϵ-局部差分隐私机制K的输出分布PK和QK之间的差异，分别对应于输入分布P和Q之间的差异。我们的第一个主要技术结果给出了一个关于χ^2-距离χ^2(PK}∥QK)的尖锐上界，该上界与χ^2(P∥Q)和ϵ有关。我们还展示了该结果对一大类距离的上界成立，包括KL-距离和平方Hellinger距离。第二个主要技术结果给出了一个关于χ^2(PK∥QK)的上界，该上界与总变差距离TV(P,Q)和ϵ有关。然后我们利用这些上界建立了van Trees不等式、Le Cam氏不等式、Assouad不等式和互信息方法的局部隐私版本，这些方法对于界定极小极大估计风险非常有用。",
    "tldr": "本文研究了局部差分隐私机制的收缩性质, 并给出了输出分布与输入分布之间差异的上界，这对于界定极小极大估计风险非常有用。",
    "en_tdlr": "This paper investigates the contraction properties of locally differentially private mechanisms and provides upper bounds on the divergence between the output and input distributions, which are useful for bounding the minimax estimation risks."
}