{
    "title": "Linkless Link Prediction via Relational Distillation. (arXiv:2210.05801v3 [cs.LG] UPDATED)",
    "abstract": "Graph Neural Networks (GNNs) have shown exceptional performance in the task of link prediction. Despite their effectiveness, the high latency brought by non-trivial neighborhood data dependency limits GNNs in practical deployments. Conversely, the known efficient MLPs are much less effective than GNNs due to the lack of relational knowledge. In this work, to combine the advantages of GNNs and MLPs, we start with exploring direct knowledge distillation (KD) methods for link prediction, i.e., predicted logit-based matching and node representation-based matching. Upon observing direct KD analogs do not perform well for link prediction, we propose a relational KD framework, Linkless Link Prediction (LLP), to distill knowledge for link prediction with MLPs. Unlike simple KD methods that match independent link logits or node representations, LLP distills relational knowledge that is centered around each (anchor) node to the student MLP. Specifically, we propose rank-based matching and distri",
    "link": "http://arxiv.org/abs/2210.05801",
    "context": "Title: Linkless Link Prediction via Relational Distillation. (arXiv:2210.05801v3 [cs.LG] UPDATED)\nAbstract: Graph Neural Networks (GNNs) have shown exceptional performance in the task of link prediction. Despite their effectiveness, the high latency brought by non-trivial neighborhood data dependency limits GNNs in practical deployments. Conversely, the known efficient MLPs are much less effective than GNNs due to the lack of relational knowledge. In this work, to combine the advantages of GNNs and MLPs, we start with exploring direct knowledge distillation (KD) methods for link prediction, i.e., predicted logit-based matching and node representation-based matching. Upon observing direct KD analogs do not perform well for link prediction, we propose a relational KD framework, Linkless Link Prediction (LLP), to distill knowledge for link prediction with MLPs. Unlike simple KD methods that match independent link logits or node representations, LLP distills relational knowledge that is centered around each (anchor) node to the student MLP. Specifically, we propose rank-based matching and distri",
    "path": "papers/22/10/2210.05801.json",
    "total_tokens": 1059,
    "translated_title": "基于关系蒸馏的无链接预测",
    "translated_abstract": "图神经网络在预测链接的任务中表现出了出色的性能。尽管有效，由非平凡邻域数据依赖性带来的高延迟限制了图神经网络在实际部署中的应用。相比之下，已知的高效多层感知器由于缺乏联系知识而比图神经网络不够有效。为了结合图神经网络和多层感知器的优点，本文探索了直接知识蒸馏方法，即基于预测的逻辑匹配和节点表示匹配。通过观察到直接知识蒸馏方法在链接预测中表现不佳，我们提出了一种基于关系蒸馏的框架，Linkless Link Prediction（LLP），用于通过多层感知器蒸馏链接预测的知识。与简单的知识蒸馏方法匹配独立链接逻辑或节点表示不同，LLP蒸馏以每个（锚）节点为中心的关系知识到学生MLP。具体而言，我们提出了基于排名的匹配和基于分布的匹配来蒸馏关系知识。对基准数据集的实验结果表明，LLP在推理时速度快得多，而且达到或超过了现有技术的图神经网络的性能水平。",
    "tldr": "本研究提出了一种基于关系蒸馏的框架(LLP)用于链接预测，通过匹配以每个（锚）节点为中心的关系知识来蒸馏链接预测的知识，该方法快捷而有效，达到或超过了现有技术的图神经网络的性能水平。",
    "en_tdlr": "This paper proposes a relational knowledge distillation framework, Linkless Link Prediction (LLP), for link prediction with MLPs, achieving competitive or superior performance to state-of-the-art GNNs while being much faster in inference."
}