{
    "title": "Revisiting Softmax for Uncertainty Approximation in Text Classification. (arXiv:2210.14037v2 [cs.LG] UPDATED)",
    "abstract": "Uncertainty approximation in text classification is an important area with applications in domain adaptation and interpretability. One of the most widely used uncertainty approximation methods is Monte Carlo (MC) Dropout, which is computationally expensive as it requires multiple forward passes through the model. A cheaper alternative is to simply use the softmax based on a single forward pass without dropout to estimate model uncertainty. However, prior work has indicated that these predictions tend to be overconfident. In this paper, we perform a thorough empirical analysis of these methods on five datasets with two base neural architectures in order to identify the trade-offs between the two. We compare both softmax and an efficient version of MC Dropout on their uncertainty approximations and downstream text classification performance, while weighing their runtime (cost) against performance (benefit). We find that, while MC dropout produces the best uncertainty approximations, usin",
    "link": "http://arxiv.org/abs/2210.14037",
    "context": "Title: Revisiting Softmax for Uncertainty Approximation in Text Classification. (arXiv:2210.14037v2 [cs.LG] UPDATED)\nAbstract: Uncertainty approximation in text classification is an important area with applications in domain adaptation and interpretability. One of the most widely used uncertainty approximation methods is Monte Carlo (MC) Dropout, which is computationally expensive as it requires multiple forward passes through the model. A cheaper alternative is to simply use the softmax based on a single forward pass without dropout to estimate model uncertainty. However, prior work has indicated that these predictions tend to be overconfident. In this paper, we perform a thorough empirical analysis of these methods on five datasets with two base neural architectures in order to identify the trade-offs between the two. We compare both softmax and an efficient version of MC Dropout on their uncertainty approximations and downstream text classification performance, while weighing their runtime (cost) against performance (benefit). We find that, while MC dropout produces the best uncertainty approximations, usin",
    "path": "papers/22/10/2210.14037.json",
    "total_tokens": 875,
    "translated_title": "重新审视Softmax在文本分类中的不确定性近似",
    "translated_abstract": "文本分类中的不确定性近似是一个在领域适应和可解释性中应用广泛的重要领域。其中一种最常用的不确定性近似方法是蒙特卡罗（MC）Dropout，但由于需要多次前向传递，计算成本较高。相比之下，一种更便宜的方法是仅使用在单次前向传递中基于softmax的方法来估计模型的不确定性。然而，之前的研究表明，这种预测往往过于自信。本文通过在两种基本神经结构的五个数据集上进行彻底的实证分析，旨在探讨这两种方法之间的权衡。我们比较了softmax和MC Dropout的不确定性近似以及下游文本分类性能，同时比较了它们的运行时间（成本）和性能（效益）。我们发现，尽管MC dropout产生了最好的不确定性近似，但使用softmax也能产生相对准确的结果。",
    "tldr": "本研究重新审视了Softmax在文本分类中的不确定性近似方法，并比较了基于MC Dropout的方法。实证分析发现，尽管MC dropout产生了最好的不确定性近似，但使用softmax也能产生相对准确的结果。",
    "en_tdlr": "This study revisits the uncertainty approximation with Softmax in text classification and compares it with the method based on MC Dropout. The empirical analysis shows that while MC dropout produces the best uncertainty approximation, using softmax can also yield relatively accurate results."
}