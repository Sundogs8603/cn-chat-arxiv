{
    "title": "MEET: A Monte Carlo Exploration-Exploitation Trade-off for Buffer Sampling. (arXiv:2210.13545v2 [cs.LG] UPDATED)",
    "abstract": "Data selection is essential for any data-based optimization technique, such as Reinforcement Learning. State-of-the-art sampling strategies for the experience replay buffer improve the performance of the Reinforcement Learning agent. However, they do not incorporate uncertainty in the Q-Value estimation. Consequently, they cannot adapt the sampling strategies, including exploration and exploitation of transitions, to the complexity of the task. To address this, this paper proposes a new sampling strategy that leverages the exploration-exploitation trade-off. This is enabled by the uncertainty estimation of the Q-Value function, which guides the sampling to explore more significant transitions and, thus, learn a more efficient policy. Experiments on classical control environments demonstrate stable results across various environments. They show that the proposed method outperforms state-of-the-art sampling strategies for dense rewards w.r.t. convergence and peak performance by 26% on av",
    "link": "http://arxiv.org/abs/2210.13545",
    "context": "Title: MEET: A Monte Carlo Exploration-Exploitation Trade-off for Buffer Sampling. (arXiv:2210.13545v2 [cs.LG] UPDATED)\nAbstract: Data selection is essential for any data-based optimization technique, such as Reinforcement Learning. State-of-the-art sampling strategies for the experience replay buffer improve the performance of the Reinforcement Learning agent. However, they do not incorporate uncertainty in the Q-Value estimation. Consequently, they cannot adapt the sampling strategies, including exploration and exploitation of transitions, to the complexity of the task. To address this, this paper proposes a new sampling strategy that leverages the exploration-exploitation trade-off. This is enabled by the uncertainty estimation of the Q-Value function, which guides the sampling to explore more significant transitions and, thus, learn a more efficient policy. Experiments on classical control environments demonstrate stable results across various environments. They show that the proposed method outperforms state-of-the-art sampling strategies for dense rewards w.r.t. convergence and peak performance by 26% on av",
    "path": "papers/22/10/2210.13545.json",
    "total_tokens": 916,
    "translated_title": "MEET: 缓冲区采样的蒙特卡罗探索-开发权衡",
    "translated_abstract": "数据选择是任何基于数据的优化技术的关键，例如强化学习。针对回放缓冲区的最新采样策略可以提高强化学习智能体的性能。然而，它们没有考虑Q值估计的不确定性。因此，它们不能根据任务的复杂性自适应地调整采样策略，包括转换的探索和开发。为了解决这个问题，本文提出了一种新的采样策略，这种策略利用了探索-开发权衡。这是通过Q值函数的不确定性估计实现的，它指导采样探索更重要的转换，从而学习更有效的策略。对于经典控制环境的实验表明，在不同环境中平稳的结果。它们表明，在密集奖励的情况下，与收敛和峰值性能相比，所提出的方法的表现比最新的采样策略提高了26％。",
    "tldr": "提出了一种新的基于探索-开发权衡的缓冲区采样策略，可以根据任务的复杂性自适应地调整采样策略，并在经典控制环境中表现出优越性能。",
    "en_tdlr": "This paper proposes a new exploration-exploitation trade-off-based sampling strategy for buffer sampling that can adapt the sampling strategy based on the complexity of the task and outperforms state-of-the-art sampling strategies for dense rewards in classical control environments by 26% in terms of convergence and peak performance."
}