{
    "title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks. (arXiv:2210.02406v2 [cs.CL] UPDATED)",
    "abstract": "Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even sim",
    "link": "http://arxiv.org/abs/2210.02406",
    "context": "Title: Decomposed Prompting: A Modular Approach for Solving Complex Tasks. (arXiv:2210.02406v2 [cs.CL] UPDATED)\nAbstract: Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even sim",
    "path": "papers/22/10/2210.02406.json",
    "total_tokens": 1091,
    "translated_title": "分解提示：一种解决复杂任务的模块化方法",
    "translated_abstract": "少样本提示是一种令人惊讶的强大方法，可用于使用大型语言模型（LLM）解决各种任务。然而，随着任务复杂性的增加，或者当任务的各个推理步骤本身难以学习，特别是当它们嵌入到更复杂的任务中时，这种方法遇到了困难。为了解决这个问题，我们提出了“分解提示”，这是一种通过将复杂任务（通过提示）分解成更简单的子任务来解决复杂任务的新方法，这些子任务可以委托给专门为这些子任务设计的提示基础的LLM库。这种模块化结构允许优化每个提示的特定子任务，必要时进一步分解，甚至可以轻松替换更有效的提示、训练模型或符号函数。我们展示了分解提示的灵活性和模块化性能够优于使用GPT3的先前少样本提示的工作。对于符号推理任务，我们可以将对LLM困难的子任务进一步分解为更简单的子任务，从而不需要任何人设计的符号函数来解决这些任务。我们的方法显示出在扩展到更大、更复杂的任务时的潜力。",
    "tldr": "分解提示是解决复杂任务的一种新方法，它通过将复杂任务分解成更简单的子任务来委托给设计专门的提示库，优化每个提示的特定子任务，并可以进一步分解、替换或更新。我们的方法在少样本提示和符号推理任务上表现出色，表明具有潜力扩展到更大和更复杂的任务。",
    "en_tdlr": "Decomposed Prompting is a new approach for solving complex tasks by decomposing them to simpler sub-tasks and delegating them to a specific library of prompts. This modular structure allows for optimization of each prompt for its specific sub-task, as well as further decomposition, replacement or updates. The method outperforms prior work on few-shot prompting and symbolic reasoning tasks, and shows potential for scaling up to even larger and more complex tasks."
}