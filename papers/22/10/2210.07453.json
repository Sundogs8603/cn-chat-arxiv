{
    "title": "Using Graph Algorithms to Pretrain Graph Completion Transformers. (arXiv:2210.07453v2 [cs.LG] UPDATED)",
    "abstract": "Recent work on Graph Neural Networks has demonstrated that self-supervised pretraining can further enhance performance on downstream graph, link, and node classification tasks. However, the efficacy of pretraining tasks has not been fully investigated for downstream large knowledge graph completion tasks. Using a contextualized knowledge graph embedding approach, we investigate five different pretraining signals, constructed using several graph algorithms and no external data, as well as their combination. We leverage the versatility of our Transformer-based model to explore graph structure generation pretraining tasks (i.e. path and k-hop neighborhood generation), typically inapplicable to most graph embedding methods. We further propose a new path-finding algorithm guided by information gain and find that it is the best-performing pretraining task across three downstream knowledge graph completion datasets. While using our new path-finding algorithm as a pretraining signal provides 2",
    "link": "http://arxiv.org/abs/2210.07453",
    "context": "Title: Using Graph Algorithms to Pretrain Graph Completion Transformers. (arXiv:2210.07453v2 [cs.LG] UPDATED)\nAbstract: Recent work on Graph Neural Networks has demonstrated that self-supervised pretraining can further enhance performance on downstream graph, link, and node classification tasks. However, the efficacy of pretraining tasks has not been fully investigated for downstream large knowledge graph completion tasks. Using a contextualized knowledge graph embedding approach, we investigate five different pretraining signals, constructed using several graph algorithms and no external data, as well as their combination. We leverage the versatility of our Transformer-based model to explore graph structure generation pretraining tasks (i.e. path and k-hop neighborhood generation), typically inapplicable to most graph embedding methods. We further propose a new path-finding algorithm guided by information gain and find that it is the best-performing pretraining task across three downstream knowledge graph completion datasets. While using our new path-finding algorithm as a pretraining signal provides 2",
    "path": "papers/22/10/2210.07453.json",
    "total_tokens": 865,
    "translated_title": "使用图算法预训练图补全Transformer",
    "translated_abstract": "最近关于图神经网络的研究已经证明，自监督预训练可以进一步增强下游的图、链接和节点分类任务的性能。然而，预训练任务的有效性尚未完全调查，特别是在下游的大型知识图谱完成任务中。本研究使用一种上下文化的知识图嵌入方法，调查了构建于几种图算法和无外部数据的五个不同预训练信号及其组合。我们利用我们基于Transformer的模型的通用性，探索了通常不适用于大多数图嵌入方法的图结构生成预训练任务（即路径和k-hop邻域生成）。我们进一步提出了一种新的路径查找算法，该算法由信息增益引导，并发现它是在三个下游知识图谱完成数据集中表现最佳的预训练任务。使用我们的新路径查找算法作为预训练信号提供2",
    "tldr": "该论文研究了使用多种图算法构建的预训练任务，探索了图结构生成预训练任务，并提出了一种新的路径查找算法用于下游的知识图谱完成任务，该方法表现最佳。"
}