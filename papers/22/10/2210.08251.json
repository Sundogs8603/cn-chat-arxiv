{
    "title": "Improving Your Graph Neural Networks: A High-Frequency Booster. (arXiv:2210.08251v2 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks (GNNs) hold the promise of learning efficient representations of graph-structured data, and one of its most important applications is semi-supervised node classification. However, in this application, GNN frameworks tend to fail due to the following issues: over-smoothing and heterophily. The most popular GNNs are known to be focused on the message-passing framework, and recent research shows that these GNNs are often bounded by low-pass filters from a signal processing perspective. We thus incorporate high-frequency information into GNNs to alleviate this genetic problem. In this paper, we argue that the complement of the original graph incorporates a high-pass filter and propose Complement Laplacian Regularization (CLAR) for an efficient enhancement of high-frequency components. The experimental results demonstrate that CLAR helps GNNs tackle over-smoothing, improving the expressiveness of heterophilic graphs, which adds up to 3.6% improvement over popular basel",
    "link": "http://arxiv.org/abs/2210.08251",
    "context": "Title: Improving Your Graph Neural Networks: A High-Frequency Booster. (arXiv:2210.08251v2 [cs.LG] UPDATED)\nAbstract: Graph neural networks (GNNs) hold the promise of learning efficient representations of graph-structured data, and one of its most important applications is semi-supervised node classification. However, in this application, GNN frameworks tend to fail due to the following issues: over-smoothing and heterophily. The most popular GNNs are known to be focused on the message-passing framework, and recent research shows that these GNNs are often bounded by low-pass filters from a signal processing perspective. We thus incorporate high-frequency information into GNNs to alleviate this genetic problem. In this paper, we argue that the complement of the original graph incorporates a high-pass filter and propose Complement Laplacian Regularization (CLAR) for an efficient enhancement of high-frequency components. The experimental results demonstrate that CLAR helps GNNs tackle over-smoothing, improving the expressiveness of heterophilic graphs, which adds up to 3.6% improvement over popular basel",
    "path": "papers/22/10/2210.08251.json",
    "total_tokens": 886,
    "translated_title": "提高图神经网络的表现：一种高频率增强器",
    "translated_abstract": "图神经网络(GNN)具有学习图结构数据有效表示的潜力，其中最重要的应用是半监督节点分类。然而，在这个应用中，由于过度平滑和异质性等问题，GNN框架往往会失败。最流行的GNN被认为集中于消息传递框架，但最近的研究表明，从信号处理角度来看，这些GNN通常受到低通滤波器的限制。因此，我们将高频信息融入到GNN中以缓解这个普遍问题。本文认为，原始图的补集包括高通滤波器，并且提出补集拉普拉斯正则化(CLAR)以有效增强高频分量。实验结果证明，CLAR有助于GNN应对过度平滑，提高异质性图的表达能力，可以比流行的基准模型性能提高3.6%。",
    "tldr": "本文提出了一种高频率增强器，将高频信息融入到GNN中，提高异质性图的表达能力，实验结果表明比流行的基准模型性能提高了3.6%。",
    "en_tdlr": "This paper proposes a high-frequency booster to incorporate high-frequency information into Graph Neural Networks (GNNs), which improves the expressiveness of heterophilic graphs and helps GNNs tackle over-smoothing. The experimental results demonstrate that the proposed method outperforms popular baseline models by 3.6%."
}