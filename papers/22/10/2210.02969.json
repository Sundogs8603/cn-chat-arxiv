{
    "title": "Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners. (arXiv:2210.02969v4 [cs.CL] UPDATED)",
    "abstract": "Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label. During inference, the LM trained with Flipped Learning, referred to as Flipped, selects the label option that is most likely to generate the task instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped outperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on average by 8.4% and 9.7% points, respectively. Flipped gives particularly large improvements on tasks with unseen labels, outperforming T0-11B by up to +20% av",
    "link": "http://arxiv.org/abs/2210.02969",
    "context": "Title: Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners. (arXiv:2210.02969v4 [cs.CL] UPDATED)\nAbstract: Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label. During inference, the LM trained with Flipped Learning, referred to as Flipped, selects the label option that is most likely to generate the task instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped outperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on average by 8.4% and 9.7% points, respectively. Flipped gives particularly large improvements on tasks with unseen labels, outperforming T0-11B by up to +20% av",
    "path": "papers/22/10/2210.02969.json",
    "total_tokens": 1045,
    "translated_title": "猜测指令！翻转学习使语言模型成为更强的零样本学习者。",
    "translated_abstract": "元训练通过最大化给定任务指令和输入实例的目标标签似然来微调语言模型(LM)，从而提高了零样本任务的泛化性能。然而，元训练的LM仍然难以推广到包含在元训练期间未见过的新标签的具有挑战性的任务中。在本文中，我们提出了一种称为“翻转学习”的元训练替代方法，该方法训练LM在给定输入实例和标签的情况下生成任务指令。在推理过程中，使用翻转学习训练的LM(称为“Flipped”)选择最有可能生成任务指令的标签选项。在BIG-bench基准测试的14个任务中，大小为11B的Flipped在平均值方面优于零样本T0-11B甚至比16倍大的3-shot GPT-3(175B)高出8.4%和9.7%的分数。Flipped在具有未知标签的任务上尤其表现突出，在某些任务上比T0-11B高出20%。",
    "tldr": "本文提出了一种名为“翻转学习”的元训练替代方法，通过训练语言模型生成任务指令，可以在零样本任务中取得更好的表现，特别是在包含未见标签的挑战性任务中。Flipped在14个BIG-bench基准测试任务中平均比3-shot GPT-3高出8.4%和9.7%的分数。",
    "en_tdlr": "Flipped Learning is proposed as an alternative method to meta-training for language models. It trains the LM to generate task instructions given input instances and labels, and performs particularly well on challenging tasks with novel labels. The 11B-sized Flipped outperforms even a 16 times larger 3-shot GPT-3 on 14 tasks of the BIG-bench benchmark by 8.4% and 9.7% points on average."
}