{
    "title": "Continual Vision-based Reinforcement Learning with Group Symmetries. (arXiv:2210.12301v2 [cs.LG] UPDATED)",
    "abstract": "Continual reinforcement learning aims to sequentially learn a variety of tasks, retaining the ability to perform previously encountered tasks while simultaneously developing new policies for novel tasks. However, current continual RL approaches overlook the fact that certain tasks are identical under basic group operations like rotations or translations, especially with visual inputs. They may unnecessarily learn and maintain a new policy for each similar task, leading to poor sample efficiency and weak generalization capability. To address this, we introduce a unique Continual Vision-based Reinforcement Learning method that recognizes Group Symmetries, called COVERS, cultivating a policy for each group of equivalent tasks rather than individual tasks. COVERS employs a proximal policy optimization-based RL algorithm with an equivariant feature extractor and a novel task grouping mechanism that relies on the extracted invariant features. We evaluate COVERS on sequences of table-top mani",
    "link": "http://arxiv.org/abs/2210.12301",
    "context": "Title: Continual Vision-based Reinforcement Learning with Group Symmetries. (arXiv:2210.12301v2 [cs.LG] UPDATED)\nAbstract: Continual reinforcement learning aims to sequentially learn a variety of tasks, retaining the ability to perform previously encountered tasks while simultaneously developing new policies for novel tasks. However, current continual RL approaches overlook the fact that certain tasks are identical under basic group operations like rotations or translations, especially with visual inputs. They may unnecessarily learn and maintain a new policy for each similar task, leading to poor sample efficiency and weak generalization capability. To address this, we introduce a unique Continual Vision-based Reinforcement Learning method that recognizes Group Symmetries, called COVERS, cultivating a policy for each group of equivalent tasks rather than individual tasks. COVERS employs a proximal policy optimization-based RL algorithm with an equivariant feature extractor and a novel task grouping mechanism that relies on the extracted invariant features. We evaluate COVERS on sequences of table-top mani",
    "path": "papers/22/10/2210.12301.json",
    "total_tokens": 1006,
    "translated_title": "具有群对称的视觉连续强化学习",
    "translated_abstract": "连续强化学习旨在顺序学习各种任务，保留执行先前遇到的任务的能力，同时为新任务开发新策略。但是，目前的连续RL方法忽略了某些任务在基本群运算（如旋转或平移）下具有相同的特征，特别是在视觉输入中。他们可能会为每个类似的任务不必要地学习和维护新策略，导致样本效率和泛化能力弱。为了解决这个问题，我们引入了一种名为COVERS的唯一具有群对称性的连续视觉强化学习方法，它为每组等价任务培养一种策略，而不是为每个任务单独制定策略。COVERS采用基于近端策略优化的强化学习算法，配备等变特征提取器和一种依赖于提取的不变特征的新任务分组机制。我们在桌面操作任务序列上评估COVERS并与最先进的连续RL方法进行比较。结果表明COVERS提高了样本效率，减少了唯一策略的数量，并实现了更强的泛化性能。",
    "tldr": "本论文提出了一种名为COVERS的连续视觉强化学习方法，它能够识别基本群操作下等价的任务，并为每组等价任务培养一种策略，提高样本效率，减少唯一策略的数量，并实现了更强的泛化性能。",
    "en_tdlr": "This paper proposes a Continual Vision-based Reinforcement Learning method called COVERS, which recognizes equivalent tasks under basic group operations and cultivates a policy for each group of equivalent tasks to improve sample efficiency, reduce the number of unique policies, and achieve stronger generalization performance."
}