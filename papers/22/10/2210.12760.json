{
    "title": "A study of uncertainty quantification in overparametrized high-dimensional models. (arXiv:2210.12760v2 [cs.LG] UPDATED)",
    "abstract": "Uncertainty quantification is a central challenge in reliable and trustworthy machine learning. Naive measures such as last-layer scores are well-known to yield overconfident estimates in the context of overparametrized neural networks. Several methods, ranging from temperature scaling to different Bayesian treatments of neural networks, have been proposed to mitigate overconfidence, most often supported by the numerical observation that they yield better calibrated uncertainty measures. In this work, we provide a sharp comparison between popular uncertainty measures for binary classification in a mathematically tractable model for overparametrized neural networks: the random features model. We discuss a trade-off between classification accuracy and calibration, unveiling a double descent like behavior in the calibration curve of optimally regularized estimators as a function of overparametrization. This is in contrast with the empirical Bayes method, which we show to be well calibrate",
    "link": "http://arxiv.org/abs/2210.12760",
    "context": "Title: A study of uncertainty quantification in overparametrized high-dimensional models. (arXiv:2210.12760v2 [cs.LG] UPDATED)\nAbstract: Uncertainty quantification is a central challenge in reliable and trustworthy machine learning. Naive measures such as last-layer scores are well-known to yield overconfident estimates in the context of overparametrized neural networks. Several methods, ranging from temperature scaling to different Bayesian treatments of neural networks, have been proposed to mitigate overconfidence, most often supported by the numerical observation that they yield better calibrated uncertainty measures. In this work, we provide a sharp comparison between popular uncertainty measures for binary classification in a mathematically tractable model for overparametrized neural networks: the random features model. We discuss a trade-off between classification accuracy and calibration, unveiling a double descent like behavior in the calibration curve of optimally regularized estimators as a function of overparametrization. This is in contrast with the empirical Bayes method, which we show to be well calibrate",
    "path": "papers/22/10/2210.12760.json",
    "total_tokens": 946,
    "translated_title": "过度参数化高维模型的不确定性量化研究",
    "translated_abstract": "不确定性量化是可靠和可信机器学习的中心挑战。在过度参数化的神经网络背景下，朴素的度量方法(如最后一层分数)已经被广为人知地产生过度自信的估计。提出了几种方法，从温度缩放到神经网络的不同贝叶斯处理，以缓解过度自信，通常通过数值观察支持它们产生更好的校准不确定性度量。在这项工作中，我们在一个数学可处理的过度参数化神经网络模型中，对于二元分类，提供了常见不确定度量之间的尖锐比较：随机特征模型。我们讨论了分类准确性和校准之间的折衷，披露最佳正则化估计量的校准曲线与过参数化的函数的双重下降行为。这与经验贝叶斯方法形成对比，我们展示它的校准是良好的。",
    "tldr": "本论文研究了过度参数化高维模型中的不确定性问题，探讨了几种方法，比较了校准和分类准确性之间的权衡。结果发现最佳正则化估计量的校准曲线具有双重下降行为，与经验贝叶斯方法形成对比。",
    "en_tdlr": "This paper studies uncertainty quantification for overparametrized high-dimensional models, comparing popular uncertainty measures for binary classification in a mathematically tractable model. The authors discuss a trade-off between accuracy and calibration and reveal a double descent behavior in the calibration curve of optimally regularized estimators as a function of overparametrization, contrasting with the well-calibrated empirical Bayes method."
}