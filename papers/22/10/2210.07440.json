{
    "title": "InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions. (arXiv:2210.07440v2 [cs.CL] UPDATED)",
    "abstract": "Debiasing methods in NLP models traditionally focus on isolating information related to a sensitive attribute (e.g., gender or race). We instead argue that a favorable debiasing method should use sensitive information 'fairly,' with explanations, rather than blindly eliminating it. This fair balance is often subjective and can be challenging to achieve algorithmically. We explore two interactive setups with a frozen predictive model and show that users able to provide feedback can achieve a better and fairer balance between task performance and bias mitigation. In one setup, users, by interacting with test examples, further decreased bias in the explanations (5-8%) while maintaining the same prediction accuracy. In the other setup, human feedback was able to disentangle associated bias and predictive information from the input leading to superior bias mitigation and improved task performance (4-5%) simultaneously.",
    "link": "http://arxiv.org/abs/2210.07440",
    "context": "Title: InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions. (arXiv:2210.07440v2 [cs.CL] UPDATED)\nAbstract: Debiasing methods in NLP models traditionally focus on isolating information related to a sensitive attribute (e.g., gender or race). We instead argue that a favorable debiasing method should use sensitive information 'fairly,' with explanations, rather than blindly eliminating it. This fair balance is often subjective and can be challenging to achieve algorithmically. We explore two interactive setups with a frozen predictive model and show that users able to provide feedback can achieve a better and fairer balance between task performance and bias mitigation. In one setup, users, by interacting with test examples, further decreased bias in the explanations (5-8%) while maintaining the same prediction accuracy. In the other setup, human feedback was able to disentangle associated bias and predictive information from the input leading to superior bias mitigation and improved task performance (4-5%) simultaneously.",
    "path": "papers/22/10/2210.07440.json",
    "total_tokens": 854,
    "translated_title": "InterFair: 通过自然语言反馈进行公平可解释预测的去偏方法",
    "translated_abstract": "传统的NLP模型去偏方法通常专注于隔离与敏感属性（如性别或种族）相关的信息。相反，我们认为一个有利的去偏方法应该“公平地”使用敏感信息，并且能够解释其使用原因，而不是盲目消除它。这种公平平衡往往是主观的，且在算法上实现起来具有挑战性。我们探索了两种交互设置，并使用冻结的预测模型展示了用户能够通过提供反馈来在任务性能和偏差缓解之间实现更好和更公平的平衡。在一种设置中，用户通过与测试示例的交互，进一步减小了解释中的偏差（5-8%），同时保持了相同的预测准确度。在另一种设置中，人类反馈能够从输入中将相关偏差和预测信息相分离，从而实现了更卓越的偏差缓解和改善的任务性能（4-5%）。",
    "tldr": "这篇论文提出了一种公平可解释的去偏方法，通过用户反馈在任务性能和偏差缓解之间实现更好和更公平的平衡。",
    "en_tdlr": "This paper proposes a fair and interpretable debiasing method that achieves a better and fairer balance between task performance and bias mitigation through user feedback."
}