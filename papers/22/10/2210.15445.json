{
    "title": "Efficient Utilization of Large Pre-Trained Models for Low Resource ASR. (arXiv:2210.15445v3 [eess.AS] UPDATED)",
    "abstract": "Unsupervised representation learning has recently helped automatic speech recognition (ASR) to tackle tasks with limited labeled data. Following this, hardware limitations and applications give rise to the question how to take advantage of large pre-trained models efficiently and reduce their complexity. In this work, we study a challenging low resource conversational telephony speech corpus from the medical domain in Vietnamese and German. We show the benefits of using unsupervised techniques beyond simple fine-tuning of large pre-trained models, discuss how to adapt them to a practical telephony task including bandwidth transfer and investigate different data conditions for pre-training and fine-tuning. We outperform the project baselines by 22% relative using pretraining techniques. Further gains of 29% can be achieved by refinements of architecture and training and 6% by adding 0.8 h of in-domain adaptation data.",
    "link": "http://arxiv.org/abs/2210.15445",
    "context": "Title: Efficient Utilization of Large Pre-Trained Models for Low Resource ASR. (arXiv:2210.15445v3 [eess.AS] UPDATED)\nAbstract: Unsupervised representation learning has recently helped automatic speech recognition (ASR) to tackle tasks with limited labeled data. Following this, hardware limitations and applications give rise to the question how to take advantage of large pre-trained models efficiently and reduce their complexity. In this work, we study a challenging low resource conversational telephony speech corpus from the medical domain in Vietnamese and German. We show the benefits of using unsupervised techniques beyond simple fine-tuning of large pre-trained models, discuss how to adapt them to a practical telephony task including bandwidth transfer and investigate different data conditions for pre-training and fine-tuning. We outperform the project baselines by 22% relative using pretraining techniques. Further gains of 29% can be achieved by refinements of architecture and training and 6% by adding 0.8 h of in-domain adaptation data.",
    "path": "papers/22/10/2210.15445.json",
    "total_tokens": 866,
    "translated_title": "大型预训练模型在低资源语音识别中的高效利用",
    "translated_abstract": "最近，无监督的表示学习帮助自动语音识别(ASR)解决了有限标签数据的任务。在此基础上，硬件限制和应用程序给出了如何高效利用大型预训练模型并降低其复杂性的问题。在本研究中，我们研究了越南语和德语在医疗领域中的具有挑战性的低资源电话会话语音语料库。我们展示了利用无监督技术超越简单微调大型预训练模型的好处，讨论了如何将它们适应到实际的电话任务，包括带宽传输，并调查了不同的预训练和微调数据条件。我们使用预训练技术相对于项目基线提高了22%。通过架构和训练的改进，可以进一步提高29%，通过添加0.8小时的领域内自适应数据可以提高6%。",
    "tldr": "本研究探讨了在低资源语音识别中如何高效利用大型预训练模型，通过无监督技术和改进的架构和训练方法取得了显著的性能提升。",
    "en_tdlr": "This study investigates how to efficiently utilize large pre-trained models in low resource automatic speech recognition (ASR), achieving significant performance improvement through unsupervised techniques, refined architecture, and training methods."
}