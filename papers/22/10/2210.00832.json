{
    "title": "Square-root regret bounds for continuous-time episodic Markov decision processes. (arXiv:2210.00832v2 [cs.LG] UPDATED)",
    "abstract": "We study reinforcement learning for continuous-time Markov decision processes (MDPs) in the finite-horizon episodic setting. In contrast to discrete-time MDPs, the inter-transition times of a continuous-time MDP are exponentially distributed with rate parameters depending on the state--action pair at each transition. We present a learning algorithm based on the methods of value iteration and upper confidence bound. We derive an upper bound on the worst-case expected regret for the proposed algorithm, and establish a worst-case lower bound, both bounds are of the order of square-root on the number of episodes. Finally, we conduct simulation experiments to illustrate the performance of our algorithm.",
    "link": "http://arxiv.org/abs/2210.00832",
    "context": "Title: Square-root regret bounds for continuous-time episodic Markov decision processes. (arXiv:2210.00832v2 [cs.LG] UPDATED)\nAbstract: We study reinforcement learning for continuous-time Markov decision processes (MDPs) in the finite-horizon episodic setting. In contrast to discrete-time MDPs, the inter-transition times of a continuous-time MDP are exponentially distributed with rate parameters depending on the state--action pair at each transition. We present a learning algorithm based on the methods of value iteration and upper confidence bound. We derive an upper bound on the worst-case expected regret for the proposed algorithm, and establish a worst-case lower bound, both bounds are of the order of square-root on the number of episodes. Finally, we conduct simulation experiments to illustrate the performance of our algorithm.",
    "path": "papers/22/10/2210.00832.json",
    "total_tokens": 808,
    "translated_title": "连续时间情节马尔可夫决策过程的平方根遗憾界限",
    "translated_abstract": "我们研究了有限时间情节马尔可夫决策过程（MDPs）的强化学习。与离散时间MDPs不同，连续时间MDPs的转换时间在每次转换时会按指数分布，并且速率参数取决于每个状态-动作对。我们提出了一种基于值迭代和上界置信区间的学习算法。我们导出了所提算法的最坏情况下预期遗憾的上界，并建立了最坏情况下的下界，这两个界都与情节数的平方根有关。最后，我们进行了模拟实验来说明我们算法的性能。",
    "tldr": "本论文研究了连续时间情节马尔可夫决策过程的强化学习问题，提出了一种基于值迭代和上界置信区间的学习算法，并导出了该算法的最坏情况下预期遗憾的上界和下界，均为情节数的平方根阶。模拟实验结果证明了该算法的性能。",
    "en_tdlr": "This paper studies reinforcement learning for continuous-time episodic Markov decision processes, proposes a learning algorithm based on value iteration and upper confidence bound, derives upper and lower bounds on the worst-case expected regret of the algorithm, both of which are of the order of square root on the number of episodes. Simulation experiments illustrate the performance of the algorithm."
}