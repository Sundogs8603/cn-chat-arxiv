{
    "title": "Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning. (arXiv:2210.07565v3 [cs.CL] UPDATED)",
    "abstract": "Prompt tuning is a parameter-efficient approach to adapting pre-trained language models to downstream tasks. Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings. In this paper, we present Multi-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot learning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks. On downstream tasks, the pre-trained prompts are selectively activated and combined, leading to strong compositional generalization to unseen tasks. To bridge the gap between pre-training and fine-tuning, we formulate upstream and downstream tasks into a unified machine reading comprehension task. Extensive experiments under two learning paradigms, i.e., gradient descent and black-box tuning, show that MP2 significantly outperforms prompt tuning, full model tuning, and prior prompt pre-training methods in few-shot settings. In addi",
    "link": "http://arxiv.org/abs/2210.07565",
    "context": "Title: Multitask Pre-training of Modular Prompt for Chinese Few-Shot Learning. (arXiv:2210.07565v3 [cs.CL] UPDATED)\nAbstract: Prompt tuning is a parameter-efficient approach to adapting pre-trained language models to downstream tasks. Although prompt tuning has been shown to match the performance of full model tuning when training data is sufficient, it tends to struggle in few-shot learning settings. In this paper, we present Multi-task Pre-trained Modular Prompt (MP2) to boost prompt tuning for few-shot learning. MP2 is a set of combinable prompts pre-trained on 38 Chinese tasks. On downstream tasks, the pre-trained prompts are selectively activated and combined, leading to strong compositional generalization to unseen tasks. To bridge the gap between pre-training and fine-tuning, we formulate upstream and downstream tasks into a unified machine reading comprehension task. Extensive experiments under two learning paradigms, i.e., gradient descent and black-box tuning, show that MP2 significantly outperforms prompt tuning, full model tuning, and prior prompt pre-training methods in few-shot settings. In addi",
    "path": "papers/22/10/2210.07565.json",
    "total_tokens": 994,
    "translated_title": "“多任务预训练模块化提示在中文少样本学习中的应用”",
    "translated_abstract": "提示调整是一种参数效率高的方法，用于将预训练的语言模型适应于下游任务。虽然在训练数据充足时，提示调整已经被证明可以与完全模型调整的性能相匹配，但在少样本学习环境下，它往往难以胜任。本文提出了一种名为多任务预训练模块化提示（MP2）的方法，用于增强提示调整在少样本学习中的表现。MP2是一组在38个中文任务上预训练的可组合提示。在下游任务中，预训练的提示可以被选择性地激活和组合，从而在面对未见过的任务时具备强大的组合泛化能力。为弥合预训练和微调之间的差距，我们将上游和下游任务统一为机器阅读理解任务。在两种学习范式即梯度下降和黑盒调整下的广泛实验表明，MP2在少样本情形下显著优于提示调整、完全模型调整和先前的提示预训练方法。",
    "tldr": "本文提出了多任务预训练模块化提示方法（MP2），用于解决在中文少样本学习中提示调整的不足。MP2是一组在38个中文任务上预训练的可组合提示，能够在面对未见过的任务时具备强大的组合泛化能力。在两种学习范式下的实验中，MP2在少样本情形下显著优于其他方法。",
    "en_tdlr": "This paper proposes a Multi-task Pre-trained Modular Prompt (MP2) for Chinese few-shot learning, which is a set of combinable prompts pre-trained on 38 Chinese tasks, and significantly outperforms other methods in few-shot settings."
}