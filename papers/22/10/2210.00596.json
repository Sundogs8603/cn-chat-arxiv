{
    "title": "Policy Gradients for Probabilistic Constrained Reinforcement Learning. (arXiv:2210.00596v2 [cs.LG] UPDATED)",
    "abstract": "This paper considers the problem of learning safe policies in the context of reinforcement learning (RL). In particular, we consider the notion of probabilistic safety. This is, we aim to design policies that maintain the state of the system in a safe set with high probability. This notion differs from cumulative constraints often considered in the literature. The challenge of working with probabilistic safety is the lack of expressions for their gradients. Indeed, policy optimization algorithms rely on gradients of the objective function and the constraints. To the best of our knowledge, this work is the first one providing such explicit gradient expressions for probabilistic constraints. It is worth noting that the gradient of this family of constraints can be applied to various policy-based algorithms. We demonstrate empirically that it is possible to handle probabilistic constraints in a continuous navigation problem.",
    "link": "http://arxiv.org/abs/2210.00596",
    "context": "Title: Policy Gradients for Probabilistic Constrained Reinforcement Learning. (arXiv:2210.00596v2 [cs.LG] UPDATED)\nAbstract: This paper considers the problem of learning safe policies in the context of reinforcement learning (RL). In particular, we consider the notion of probabilistic safety. This is, we aim to design policies that maintain the state of the system in a safe set with high probability. This notion differs from cumulative constraints often considered in the literature. The challenge of working with probabilistic safety is the lack of expressions for their gradients. Indeed, policy optimization algorithms rely on gradients of the objective function and the constraints. To the best of our knowledge, this work is the first one providing such explicit gradient expressions for probabilistic constraints. It is worth noting that the gradient of this family of constraints can be applied to various policy-based algorithms. We demonstrate empirically that it is possible to handle probabilistic constraints in a continuous navigation problem.",
    "path": "papers/22/10/2210.00596.json",
    "total_tokens": 803,
    "translated_title": "概率约束强化学习的策略梯度方法",
    "translated_abstract": "本文考虑在强化学习（RL）的情境下学习安全策略的问题。具体而言，我们考虑了概率安全的概念。也就是说，我们的目标是设计能够在高概率下将系统状态保持在安全集合中的策略。这一概念不同于文献中常考虑的累积约束。处理概率安全的挑战在于缺乏其梯度的表达式。实际上，策略优化算法依赖于目标函数和约束的梯度。据我们所知，本文提供了首个明确给出概率约束梯度表达式的工作。值得注意的是，这种约束的梯度可以应用于各种基于策略的算法中。我们通过实验证明，在一项连续导航问题中处理概率约束是可行的。",
    "tldr": "本文提出了处理概率安全约束的策略梯度方法，是首个给出概率约束梯度表达式的工作。",
    "en_tdlr": "This paper proposes a policy gradient approach for handling probabilistic safety constraints, which is the first work providing explicit gradient expressions for such constraints. Empirical results show the effectiveness of the proposed method in a continuous navigation problem."
}