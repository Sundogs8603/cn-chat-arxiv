{
    "title": "Learning to Optimize Quasi-Newton Methods. (arXiv:2210.06171v2 [cs.LG] UPDATED)",
    "abstract": "Fast gradient-based optimization algorithms have become increasingly essential for the computationally efficient training of machine learning models. One technique is to multiply the gradient by a preconditioner matrix to produce a step, but it is unclear what the best preconditioner matrix is. This paper introduces a novel machine learning optimizer called LODO, which tries to online meta-learn the best preconditioner during optimization. Specifically, our optimizer merges Learning to Optimize (L2O) techniques with quasi-Newton methods to learn preconditioners parameterized as neural networks; they are more flexible than preconditioners in other quasi-Newton methods. Unlike other L2O methods, LODO does not require any meta-training on a training task distribution, and instead learns to optimize on the fly while optimizing on the test task, adapting to the local characteristics of the loss landscape while traversing it. Theoretically, we show that our optimizer approximates the inverse",
    "link": "http://arxiv.org/abs/2210.06171",
    "context": "Title: Learning to Optimize Quasi-Newton Methods. (arXiv:2210.06171v2 [cs.LG] UPDATED)\nAbstract: Fast gradient-based optimization algorithms have become increasingly essential for the computationally efficient training of machine learning models. One technique is to multiply the gradient by a preconditioner matrix to produce a step, but it is unclear what the best preconditioner matrix is. This paper introduces a novel machine learning optimizer called LODO, which tries to online meta-learn the best preconditioner during optimization. Specifically, our optimizer merges Learning to Optimize (L2O) techniques with quasi-Newton methods to learn preconditioners parameterized as neural networks; they are more flexible than preconditioners in other quasi-Newton methods. Unlike other L2O methods, LODO does not require any meta-training on a training task distribution, and instead learns to optimize on the fly while optimizing on the test task, adapting to the local characteristics of the loss landscape while traversing it. Theoretically, we show that our optimizer approximates the inverse",
    "path": "papers/22/10/2210.06171.json",
    "total_tokens": 903,
    "translated_title": "学习优化拟牛顿方法",
    "translated_abstract": "快速基于梯度的优化算法对于计算高效地训练机器学习模型变得越来越重要。一种技术是将梯度乘以一个预条件矩阵来产生一步，但最好的预条件矩阵是不清楚的。本文引入了一种新颖的机器学习优化器LODO，它尝试在优化过程中在线元学习最佳的预条件矩阵。具体而言，我们的优化器将学习优化（L2O）技术与拟牛顿方法相结合，以神经网络为参数化的预条件矩阵进行学习；它们比其他拟牛顿方法中的预条件矩阵更灵活。与其他L2O方法不同，LODO不需要在训练任务分布上进行元训练，而是在测试任务上进行优化时实时学习优化，适应遍历中的损失景观的局部特征。从理论上讲，我们证明了我们的优化器近似地拟合了负Hessian矩阵。",
    "tldr": "本文介绍了一种名为LODO的机器学习优化器，它通过将学习优化技术与拟牛顿方法相结合，实时学习并适应损失景观的局部特征，从而在线元学习最佳的预条件矩阵。"
}