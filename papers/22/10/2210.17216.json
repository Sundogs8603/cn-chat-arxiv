{
    "title": "Symmetries, flat minima, and the conserved quantities of gradient flow. (arXiv:2210.17216v2 [cs.LG] UPDATED)",
    "abstract": "Empirical studies of the loss landscape of deep networks have revealed that many local minima are connected through low-loss valleys. Yet, little is known about the theoretical origin of such valleys. We present a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys. Our framework uses equivariances of the activation functions and can be applied to different layer architectures. To generalize this framework to nonlinear neural networks, we introduce a novel set of nonlinear, data-dependent symmetries. These symmetries can transform a trained model such that it performs similarly on new samples, which allows ensemble building that improves robustness under certain adversarial attacks. We then show that conserved quantities associated with linear symmetries can be used to define coordinates along low-loss valleys. The conserved quantities help reveal that using common initialization methods, gradient flow only explores a small part ",
    "link": "http://arxiv.org/abs/2210.17216",
    "context": "Title: Symmetries, flat minima, and the conserved quantities of gradient flow. (arXiv:2210.17216v2 [cs.LG] UPDATED)\nAbstract: Empirical studies of the loss landscape of deep networks have revealed that many local minima are connected through low-loss valleys. Yet, little is known about the theoretical origin of such valleys. We present a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys. Our framework uses equivariances of the activation functions and can be applied to different layer architectures. To generalize this framework to nonlinear neural networks, we introduce a novel set of nonlinear, data-dependent symmetries. These symmetries can transform a trained model such that it performs similarly on new samples, which allows ensemble building that improves robustness under certain adversarial attacks. We then show that conserved quantities associated with linear symmetries can be used to define coordinates along low-loss valleys. The conserved quantities help reveal that using common initialization methods, gradient flow only explores a small part ",
    "path": "papers/22/10/2210.17216.json",
    "total_tokens": 1133,
    "translated_title": "对称性、平滑极小值和梯度流的守恒量",
    "translated_abstract": "深度网络损失景观的经验研究表明，许多局部极小值通过低损耗山谷相连。然而，关于这些山谷的理论起源知之甚少。我们提出了一个在参数空间中寻找连续对称性的通用框架，该对称性雕刻了低损坏山谷。我们的框架利用了激活函数的等变性，并可应用于不同的层架构。为了将这个框架推广到非线性神经网络，我们引入了一组新的非线性数据相关对称性。这些对称性可以使训练好的模型变形，从而在新的样本上表现出相似的性能，这允许集成建立，提高对某些对抗性攻击的鲁棒性。然后，我们展示了与线性对称性相关的守恒量可用于定义沿着低损坏山谷的坐标系。这些守恒量有助于揭示使用常见初始化方法时，梯度流只探索了损失景观的一小部分，我们将其称为平坦极小值偏差。我们的框架提供了一种减轻这种偏差，改善梯度流寻找良好解的能力的方法。",
    "tldr": "该论文发现了一种通用框架，可以在参数空间中寻找连续对称性的方法，这种对称性可以雕刻出低损坏山谷。论文提出了一组新的非线性数据相关对称性，用于将训练好的模型变形，提高对某些对抗性攻击的鲁棒性并发现了梯度流的平坦极小值偏差问题，提出了一种改善梯度流寻找良好解的能力的方法。",
    "en_tdlr": "The paper proposes a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys, and introduces a set of nonlinear, data-dependent symmetries for nonlinear neural networks. The paper also discovers the flat minima bias problem in gradient flow and suggests a way to mitigate it, as well as improving the ability of gradient flow to find good solutions."
}