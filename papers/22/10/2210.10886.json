{
    "title": "Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis. (arXiv:2210.10886v3 [cs.CV] UPDATED)",
    "abstract": "Deep Learning-based image synthesis techniques have been applied in healthcare research for generating medical images to support open research and augment medical datasets. Training generative adversarial neural networks (GANs) usually require large amounts of training data. Federated learning (FL) provides a way of training a central model using distributed data while keeping raw data locally. However, given that the FL server cannot access the raw data, it is vulnerable to backdoor attacks, an adversarial by poisoning training data. Most backdoor attack strategies focus on classification models and centralized domains. It is still an open question if the existing backdoor attacks can affect GAN training and, if so, how to defend against the attack in the FL setting. In this work, we investigate the overlooked issue of backdoor attacks in federated GANs (FedGANs). The success of this attack is subsequently determined to be the result of some local discriminators overfitting the poison",
    "link": "http://arxiv.org/abs/2210.10886",
    "context": "Title: Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis. (arXiv:2210.10886v3 [cs.CV] UPDATED)\nAbstract: Deep Learning-based image synthesis techniques have been applied in healthcare research for generating medical images to support open research and augment medical datasets. Training generative adversarial neural networks (GANs) usually require large amounts of training data. Federated learning (FL) provides a way of training a central model using distributed data while keeping raw data locally. However, given that the FL server cannot access the raw data, it is vulnerable to backdoor attacks, an adversarial by poisoning training data. Most backdoor attack strategies focus on classification models and centralized domains. It is still an open question if the existing backdoor attacks can affect GAN training and, if so, how to defend against the attack in the FL setting. In this work, we investigate the overlooked issue of backdoor attacks in federated GANs (FedGANs). The success of this attack is subsequently determined to be the result of some local discriminators overfitting the poison",
    "path": "papers/22/10/2210.10886.json",
    "total_tokens": 938,
    "translated_title": "基于联邦生成对抗网络的医学图像合成中的后门攻击与防御",
    "translated_abstract": "深度学习基于图像合成的技术已经应用于医疗保健研究中，用于生成医学图像以支持开放研究并增加医学数据集。训练生成对抗神经网络（GANs）通常需要大量的训练数据。联邦学习（FL）提供了一种使用分布式数据训练中央模型并保持本地原始数据的方法。然而，考虑到FL服务器无法访问原始数据，它容易受到后门攻击的影响，后门攻击是一种通过污染训练数据的对抗性攻击。大多数后门攻击策略集中在分类模型和中心化领域。现有的后门攻击能否影响GAN训练仍然是一个开放问题，如果可以影响，如何在FL环境中进行防御也是一个问题。在这项研究中，我们调查了联邦GANs（FedGANs）中后门攻击这个被忽视的问题。攻击的成功随后被确定为部分本地判别器对毒素过度拟合的结果。",
    "tldr": "本研究调查了联邦生成对抗网络中后门攻击的被忽视问题，并发现成功攻击是由于部分本地判别器对毒素过度拟合所致。"
}