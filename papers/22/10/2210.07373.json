{
    "title": "Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models. (arXiv:2210.07373v3 [cs.CL] UPDATED)",
    "abstract": "Pretrained language models (PLMs) for data-to-text (D2T) generation can use human-readable data labels such as column headings, keys, or relation names to generalize to out-of-domain examples. However, the models are well-known in producing semantically inaccurate outputs if these labels are ambiguous or incomplete, which is often the case in D2T datasets. In this paper, we expose this issue on the task of descibing a relation between two entities. For our experiments, we collect a novel dataset for verbalizing a diverse set of 1,522 unique relations from three large-scale knowledge graphs (Wikidata, DBPedia, YAGO). We find that although PLMs for D2T generation expectedly fail on unclear cases, models trained with a large variety of relation labels are surprisingly robust in verbalizing novel, unseen relations. We argue that using data with a diverse set of clear and meaningful labels is key to training D2T generation systems capable of generalizing to novel domains.",
    "link": "http://arxiv.org/abs/2210.07373",
    "context": "Title: Mind the Labels: Describing Relations in Knowledge Graphs With Pretrained Models. (arXiv:2210.07373v3 [cs.CL] UPDATED)\nAbstract: Pretrained language models (PLMs) for data-to-text (D2T) generation can use human-readable data labels such as column headings, keys, or relation names to generalize to out-of-domain examples. However, the models are well-known in producing semantically inaccurate outputs if these labels are ambiguous or incomplete, which is often the case in D2T datasets. In this paper, we expose this issue on the task of descibing a relation between two entities. For our experiments, we collect a novel dataset for verbalizing a diverse set of 1,522 unique relations from three large-scale knowledge graphs (Wikidata, DBPedia, YAGO). We find that although PLMs for D2T generation expectedly fail on unclear cases, models trained with a large variety of relation labels are surprisingly robust in verbalizing novel, unseen relations. We argue that using data with a diverse set of clear and meaningful labels is key to training D2T generation systems capable of generalizing to novel domains.",
    "path": "papers/22/10/2210.07373.json",
    "total_tokens": 945,
    "translated_title": "注意标签：使用预训练模型描述知识图中的关系",
    "translated_abstract": "为了泛化到领域外的样本，用于数据到文本生成的预训练语言模型(PLMs)可以使用人可读的数据标签，如列标题、键或关系名称。然而，如果这些标签不明确或不完整，模型常常会产生语义错误的输出，在数据到文本数据集中经常出现这种情况。本文就描述两个实体之间关系的任务揭示了这个问题。为了进行实验，我们收集了一个新的数据集，用于详细说明来自三个大规模知识图谱(Wikidata、DBPedia、YAGO)的1,522个独特关系。我们发现，尽管预训练语言模型（PLMs）在不清楚的情况下预期会失败，但在训练过程中使用了大量各种关系标签的模型在详细说明新颖、未见过的关系时表现出了惊人的鲁棒性。我们认为使用具有多样化、明确且有意义的数据标签是训练能够泛化到新领域的数据到文本生成系统的关键。",
    "tldr": "本文研究了使用预训练模型描述知识图中关系的问题。通过收集一个新数据集，并使用具有多样化、明确且有意义的数据标签来训练模型，在描述新颖、未见过的关系时具有惊人的鲁棒性。",
    "en_tdlr": "This paper investigates the issue of describing relations in knowledge graphs using pretrained models. By collecting a new dataset and training models with a diverse set of clear and meaningful labels, the models exhibit surprising robustness in verbalizing novel, unseen relations."
}