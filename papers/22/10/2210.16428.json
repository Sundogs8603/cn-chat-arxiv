{
    "title": "Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention. (arXiv:2210.16428v2 [eess.AS] UPDATED)",
    "abstract": "Audio captioning aims to generate text descriptions of audio clips. In the real world, many objects produce similar sounds. How to accurately recognize ambiguous sounds is a major challenge for audio captioning. In this work, inspired by inherent human multimodal perception, we propose visually-aware audio captioning, which makes use of visual information to help the description of ambiguous sounding objects. Specifically, we introduce an off-the-shelf visual encoder to extract video features and incorporate the visual features into an audio captioning system. Furthermore, to better exploit complementary audio-visual contexts, we propose an audio-visual attention mechanism that adaptively integrates audio and visual context and removes the redundant information in the latent space. Experimental results on AudioCaps, the largest audio captioning dataset, show that our proposed method achieves state-of-the-art results on machine translation metrics.",
    "link": "http://arxiv.org/abs/2210.16428",
    "context": "Title: Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention. (arXiv:2210.16428v2 [eess.AS] UPDATED)\nAbstract: Audio captioning aims to generate text descriptions of audio clips. In the real world, many objects produce similar sounds. How to accurately recognize ambiguous sounds is a major challenge for audio captioning. In this work, inspired by inherent human multimodal perception, we propose visually-aware audio captioning, which makes use of visual information to help the description of ambiguous sounding objects. Specifically, we introduce an off-the-shelf visual encoder to extract video features and incorporate the visual features into an audio captioning system. Furthermore, to better exploit complementary audio-visual contexts, we propose an audio-visual attention mechanism that adaptively integrates audio and visual context and removes the redundant information in the latent space. Experimental results on AudioCaps, the largest audio captioning dataset, show that our proposed method achieves state-of-the-art results on machine translation metrics.",
    "path": "papers/22/10/2210.16428.json",
    "total_tokens": 893,
    "translated_title": "自适应音频视觉关注的视觉感知音频字幕生成",
    "translated_abstract": "音频字幕生成旨在产生音频片段的文本描述。在现实世界中，许多对象会产生相似的声音。如何准确识别模糊的声音是音频字幕生成的主要挑战。本文受到人类固有多模态感知的启发，提出了视觉感知音频字幕生成，利用视觉信息来帮助描述模糊的声音对象。具体而言，我们引入了一个现成的视觉编码器来提取视频特征，并将其结合到音频字幕生成系统中。此外，为了更好地利用互补的音频-视觉环境，我们提出了一种自适应音频-视觉关注机制，该机制自适应地整合音频和视觉上下文，并消除潜在空间中的冗余信息。在AudioCaps上进行的实验结果表明，我们提出的方法在机器翻译指标上取得了最先进的成果。",
    "tldr": "本文提出了一种自适应音频视觉关注的视觉感知音频字幕生成方法，通过利用视觉上下文来帮助解决音频字幕生成中的歧义声音问题。在AudioCaps上实验结果表明，该方法在机器翻译指标上取得了最佳结果。",
    "en_tdlr": "This paper proposes a visually-aware audio captioning method with adaptive audio-visual attention, which utilizes visual context to help address ambiguous sounds in audio captioning. Experimental results on AudioCaps show that the proposed method achieves state-of-the-art results on machine translation metrics."
}