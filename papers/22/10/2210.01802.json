{
    "title": "Alternating Differentiation for Optimization Layers. (arXiv:2210.01802v2 [cs.LG] UPDATED)",
    "abstract": "The idea of embedding optimization problems into deep neural networks as optimization layers to encode constraints and inductive priors has taken hold in recent years. Most existing methods focus on implicitly differentiating Karush-Kuhn-Tucker (KKT) conditions in a way that requires expensive computations on the Jacobian matrix, which can be slow and memory-intensive. In this paper, we developed a new framework, named Alternating Differentiation (Alt-Diff), that differentiates optimization problems (here, specifically in the form of convex optimization problems with polyhedral constraints) in a fast and recursive way. Alt-Diff decouples the differentiation procedure into a primal update and a dual update in an alternating way. Accordingly, Alt-Diff substantially decreases the dimensions of the Jacobian matrix especially for optimization with large-scale constraints and thus increases the computational speed of implicit differentiation. We show that the gradients obtained by Alt-Diff a",
    "link": "http://arxiv.org/abs/2210.01802",
    "context": "Title: Alternating Differentiation for Optimization Layers. (arXiv:2210.01802v2 [cs.LG] UPDATED)\nAbstract: The idea of embedding optimization problems into deep neural networks as optimization layers to encode constraints and inductive priors has taken hold in recent years. Most existing methods focus on implicitly differentiating Karush-Kuhn-Tucker (KKT) conditions in a way that requires expensive computations on the Jacobian matrix, which can be slow and memory-intensive. In this paper, we developed a new framework, named Alternating Differentiation (Alt-Diff), that differentiates optimization problems (here, specifically in the form of convex optimization problems with polyhedral constraints) in a fast and recursive way. Alt-Diff decouples the differentiation procedure into a primal update and a dual update in an alternating way. Accordingly, Alt-Diff substantially decreases the dimensions of the Jacobian matrix especially for optimization with large-scale constraints and thus increases the computational speed of implicit differentiation. We show that the gradients obtained by Alt-Diff a",
    "path": "papers/22/10/2210.01802.json",
    "total_tokens": 857,
    "translated_title": "优化层的交替微分",
    "translated_abstract": "将优化问题嵌入深度神经网络作为优化层以编码约束和归纳先验的想法在近年来已经深入人心。现有的大多数方法都集中在以一种需要在雅可比矩阵上进行昂贵计算的方式隐式微分Karush-Kuhn-Tucker（KKT）条件上，这可能是慢和内存密集的。在本文中，我们开发了一种名为交替微分（Alt-Diff）的新框架，以一种快速且递归的方式微分优化问题（这里特别指带有多面体约束的凸优化问题）。Alt-Diff将微分过程分解为主问题更新和对偶问题更新的交替方式。因此，Alt-Diff尤其能够减小雅可比矩阵的维度，特别是针对具有大规模约束的优化问题，从而提高了隐式微分的计算速度。我们展示了通过Alt-Diff获得的梯度",
    "tldr": "Alt-Diff是一种新的框架，可以在不需要对整个雅可比矩阵进行昂贵计算的情况下，以快速和递归的方式微分优化问题，从而大大提高隐式微分的计算速度。"
}