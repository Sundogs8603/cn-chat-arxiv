{
    "title": "Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks. (arXiv:2210.00185v2 [cs.CL] UPDATED)",
    "abstract": "Although large language models have achieved impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with an external retriever, have demonstrated promising language modeling capabilities. However, it remains unclear whether such semi-parametric language models can perform competitively well as their fully-parametric counterparts on zero-shot generalization to downstream tasks. In this work, we introduce $\\text{Zemi}$, a zero-shot semi-parametric language model. To our best knowledge, this is the first semi-parametric language model that can demonstrate strong zero-shot performance on a wide range of held-out unseen tasks. We train $\\text{Zemi}$ with a novel semi-parametric multitask prompted training paradigm, which shows significant improvement compared with the parametric multitask training as proposed by T0. Specifically, we augment the multitask training and zero-shot evaluation",
    "link": "http://arxiv.org/abs/2210.00185",
    "context": "Title: Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks. (arXiv:2210.00185v2 [cs.CL] UPDATED)\nAbstract: Although large language models have achieved impressive zero-shot ability, the huge model size generally incurs high cost. Recently, semi-parametric language models, which augment a smaller language model with an external retriever, have demonstrated promising language modeling capabilities. However, it remains unclear whether such semi-parametric language models can perform competitively well as their fully-parametric counterparts on zero-shot generalization to downstream tasks. In this work, we introduce $\\text{Zemi}$, a zero-shot semi-parametric language model. To our best knowledge, this is the first semi-parametric language model that can demonstrate strong zero-shot performance on a wide range of held-out unseen tasks. We train $\\text{Zemi}$ with a novel semi-parametric multitask prompted training paradigm, which shows significant improvement compared with the parametric multitask training as proposed by T0. Specifically, we augment the multitask training and zero-shot evaluation",
    "path": "papers/22/10/2210.00185.json",
    "total_tokens": 882,
    "translated_title": "Zemi: 从多任务中学习零样本半参数语言模型",
    "translated_abstract": "虽然大型语言模型已经取得了令人印象深刻的零样本能力，但巨大的模型大小通常会产生高成本。最近，半参数语言模型使用外部检索器增强较小的语言模型，展示了有前途的语言建模能力。然而，这样的半参数语言模型能否在零样本推广到下游任务时像全参数模型一样表现竞争力仍然不清楚。在这项工作中，我们介绍了Zemi，一种零样本半参数语言模型。据我们所知，这是第一个能够在广泛的未见过的任务上展示出强大零样本性能的半参数语言模型。我们使用新颖的半参数多任务提示训练范式来训练Zemi，与T0提出的参数多任务训练相比，显示出显著的改进。具体来说，我们增强了多任务训练和零样本评估",
    "tldr": "Zemi是一种零样本半参数语言模型，使用外部检索器增强语言模型，可以在广泛的未见过的任务上展示出强大的零样本性能，比参数模型表现更好。",
    "en_tdlr": "Zemi is a zero-shot semi-parametric language model that augments a smaller language model with an external retriever, and can demonstrate strong zero-shot performance on a wide range of held-out unseen tasks, outperforming parametric models."
}