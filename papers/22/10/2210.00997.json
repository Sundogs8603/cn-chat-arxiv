{
    "title": "Online Self-Concordant and Relatively Smooth Minimization, With Applications to Online Portfolio Selection and Learning Quantum States. (arXiv:2210.00997v3 [stat.ML] UPDATED)",
    "abstract": "Consider an online convex optimization problem where the loss functions are self-concordant barriers, smooth relative to a convex function $h$, and possibly non-Lipschitz. We analyze the regret of online mirror descent with $h$. Then, based on the result, we prove the following in a unified manner. Denote by $T$ the time horizon and $d$ the parameter dimension. 1. For online portfolio selection, the regret of $\\widetilde{\\text{EG}}$, a variant of exponentiated gradient due to Helmbold et al., is $\\tilde{O} ( T^{2/3} d^{1/3} )$ when $T > 4 d / \\log d$. This improves on the original $\\tilde{O} ( T^{3/4} d^{1/2} )$ regret bound for $\\widetilde{\\text{EG}}$. 2. For online portfolio selection, the regret of online mirror descent with the logarithmic barrier is $\\tilde{O}(\\sqrt{T d})$. The regret bound is the same as that of Soft-Bayes due to Orseau et al. up to logarithmic terms. 3. For online learning quantum states with the logarithmic loss, the regret of online mirror descent with the log",
    "link": "http://arxiv.org/abs/2210.00997",
    "context": "Title: Online Self-Concordant and Relatively Smooth Minimization, With Applications to Online Portfolio Selection and Learning Quantum States. (arXiv:2210.00997v3 [stat.ML] UPDATED)\nAbstract: Consider an online convex optimization problem where the loss functions are self-concordant barriers, smooth relative to a convex function $h$, and possibly non-Lipschitz. We analyze the regret of online mirror descent with $h$. Then, based on the result, we prove the following in a unified manner. Denote by $T$ the time horizon and $d$ the parameter dimension. 1. For online portfolio selection, the regret of $\\widetilde{\\text{EG}}$, a variant of exponentiated gradient due to Helmbold et al., is $\\tilde{O} ( T^{2/3} d^{1/3} )$ when $T > 4 d / \\log d$. This improves on the original $\\tilde{O} ( T^{3/4} d^{1/2} )$ regret bound for $\\widetilde{\\text{EG}}$. 2. For online portfolio selection, the regret of online mirror descent with the logarithmic barrier is $\\tilde{O}(\\sqrt{T d})$. The regret bound is the same as that of Soft-Bayes due to Orseau et al. up to logarithmic terms. 3. For online learning quantum states with the logarithmic loss, the regret of online mirror descent with the log",
    "path": "papers/22/10/2210.00997.json",
    "total_tokens": 1014,
    "translated_title": "在线自协调且相对平滑的最小化问题及其在在线投资组合选择和学习量子态中的应用",
    "translated_abstract": "本文考虑一类在线凸优化问题，其中损失函数是自协调障碍函数，在某个凸函数h的相对平滑，可能不是Lipschitz的。我们分析了在线镜像下降算法在h上的遗憾，并基于结果以统一的方式证明了以下结论。对于在线投资组合选择问题，当T>4d/logd时，改进了Helmbold等人提出的指数化梯度算法的遗憾界为O(T^{2/3} d^{1/3})，原有界是O(T^{3/4} d^{1/2})。对于在线投资组合选择问题，使用对数障碍的在线镜像下降算法的遗憾界为O(sqrt(Td))，与Orseau等人的Soft-Bayes算法具有相同的遗憾界，除去对数因子。对于使用对数损失的在线学习量子态问题，使用对数障碍的在线镜像下降算法的遗憾界是...",
    "tldr": "本文提出了一种在线自协调且相对平滑的最小化算法，通过分析在线镜像下降算法在凸函数上的遗憾，改进了在线投资组合选择算法的性能，并在在线学习量子态问题中达到了与Soft-Bayes算法相当的效果。",
    "en_tdlr": "This paper proposes an online self-concordant and relatively smooth minimization algorithm, improves the performance of online portfolio selection algorithms, and achieves comparable results to Soft-Bayes algorithm in the online learning quantum states problem by analyzing the regret of online mirror descent algorithm on convex functions."
}