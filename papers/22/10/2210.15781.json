{
    "title": "A Compact End-to-End Model with Local and Global Context for Spoken Language Identification. (arXiv:2210.15781v2 [eess.AS] UPDATED)",
    "abstract": "We introduce TitaNet-LID, a compact end-to-end neural network for Spoken Language Identification (LID) that is based on the ContextNet architecture. TitaNet-LID employs 1D depth-wise separable convolutions and Squeeze-and-Excitation layers to effectively capture local and global context within an utterance. Despite its small size, TitaNet-LID achieves performance similar to state-of-the-art models on the VoxLingua107 dataset while being 10 times smaller. Furthermore, it can be easily adapted to new acoustic conditions and unseen languages through simple fine-tuning, achieving a state-of-the-art accuracy of 88.2% on the FLEURS benchmark. Our model is scalable and can achieve a better trade-off between accuracy and speed. TitaNet-LID performs well even on short utterances less than 5s in length, indicating its robustness to input length.",
    "link": "http://arxiv.org/abs/2210.15781",
    "context": "Title: A Compact End-to-End Model with Local and Global Context for Spoken Language Identification. (arXiv:2210.15781v2 [eess.AS] UPDATED)\nAbstract: We introduce TitaNet-LID, a compact end-to-end neural network for Spoken Language Identification (LID) that is based on the ContextNet architecture. TitaNet-LID employs 1D depth-wise separable convolutions and Squeeze-and-Excitation layers to effectively capture local and global context within an utterance. Despite its small size, TitaNet-LID achieves performance similar to state-of-the-art models on the VoxLingua107 dataset while being 10 times smaller. Furthermore, it can be easily adapted to new acoustic conditions and unseen languages through simple fine-tuning, achieving a state-of-the-art accuracy of 88.2% on the FLEURS benchmark. Our model is scalable and can achieve a better trade-off between accuracy and speed. TitaNet-LID performs well even on short utterances less than 5s in length, indicating its robustness to input length.",
    "path": "papers/22/10/2210.15781.json",
    "total_tokens": 965,
    "translated_title": "用于口语语言识别的具有局部和全局上下文的紧凑型端到端模型",
    "translated_abstract": "我们介绍了TitaNet-LID，一种基于ContextNet架构的用于口语语言识别（LID）的紧凑型端到端神经网络。TitaNet-LID使用1D深度可分离卷积和Squeeze-and-Excitation层，有效地捕捉了话语中的局部和全局上下文。尽管尺寸较小，TitaNet-LID在VoxLingua107数据集上的性能与最先进的模型相似，同时尺寸更小10倍。此外，通过简单的微调，它可以轻松适应新的声学条件和未见过的语言，在FLEURS基准测试上实现了88.2%的最先进准确率。我们的模型具有可扩展性，可以在准确性和速度之间取得更好的平衡。TitaNet-LID在长度不足5秒的短语中表现良好，表明它对输入长度的鲁棒性。",
    "tldr": "TitaNet-LID是一种紧凑且具有局部和全局上下文的端到端神经网络模型，适用于口语语言识别。尽管尺寸较小，但它可以在不降低性能的情况下实现与最先进模型相似的准确性，并且能够适应不同的声学条件和语言。该模型具有良好的扩展性和处理短语输入的能力。",
    "en_tdlr": "TitaNet-LID is a compact end-to-end neural network model with local and global context for spoken language identification. Despite its small size, it achieves performance similar to state-of-the-art models and can be easily adapted to different acoustic conditions and languages. The model is scalable and able to handle short utterances effectively."
}