{
    "title": "Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning. (arXiv:2210.16299v2 [eess.SY] UPDATED)",
    "abstract": "A key challenge in solving the deterministic inverse reinforcement learning (IRL) problem online and in real-time is the existence of multiple solutions. Nonuniqueness necessitates the study of the notion of equivalent solutions, i.e., solutions that result in a different cost functional but same feedback matrix, and convergence to such solutions. While offline algorithms that result in convergence to equivalent solutions have been developed in the literature, online, real-time techniques that address nonuniqueness are not available. In this paper, a regularized history stack observer that converges to approximately equivalent solutions of the IRL problem is developed. Novel data-richness conditions are developed to facilitate the analysis and simulation results are provided to demonstrate the effectiveness of the developed technique.",
    "link": "http://arxiv.org/abs/2210.16299",
    "context": "Title: Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning. (arXiv:2210.16299v2 [eess.SY] UPDATED)\nAbstract: A key challenge in solving the deterministic inverse reinforcement learning (IRL) problem online and in real-time is the existence of multiple solutions. Nonuniqueness necessitates the study of the notion of equivalent solutions, i.e., solutions that result in a different cost functional but same feedback matrix, and convergence to such solutions. While offline algorithms that result in convergence to equivalent solutions have been developed in the literature, online, real-time techniques that address nonuniqueness are not available. In this paper, a regularized history stack observer that converges to approximately equivalent solutions of the IRL problem is developed. Novel data-richness conditions are developed to facilitate the analysis and simulation results are provided to demonstrate the effectiveness of the developed technique.",
    "path": "papers/22/10/2210.16299.json",
    "total_tokens": 825,
    "translated_title": "基于观测器的逆强化学习中的非唯一性和等价解的收敛性研究",
    "translated_abstract": "在在线和实时解决确定性逆强化学习问题中，存在多个解的是一个关键挑战。非唯一性需要研究等价解的概念，即结果在不同的代价函数但相同的反馈矩阵，以及收敛到这些解的方法。尽管已经在文献中开发了离线算法以收敛到等价解，但尚未提供解决非唯一性的在线、实时技术。本文提出了一种能够收敛到逆强化学习问题的近似等价解的正则化历史堆栈观察器。发展了新的数据丰富性条件以促进分析，并通过模拟结果展示了所开发技术的有效性。",
    "tldr": "本文研究了在线、实时解决逆强化学习中存在的多个解的挑战，提出了一种能够收敛到近似等价解的正则化历史堆栈观察器。通过开发新的数据丰富性条件，证明了该技术的有效性。",
    "en_tdlr": "This paper addresses the challenge of multiple solutions in online, real-time inverse reinforcement learning by proposing a regularized history stack observer that converges to approximately equivalent solutions. The effectiveness of the developed technique is demonstrated through the development of novel data-richness conditions."
}