{
    "title": "Mutual Information Regularized Offline Reinforcement Learning",
    "abstract": "arXiv:2210.07484v3 Announce Type: replace-cross  Abstract: The major challenge of offline RL is the distribution shift that appears when out-of-distribution actions are queried, which makes the policy improvement direction biased by extrapolation errors. Most existing methods address this problem by penalizing the policy or value for deviating from the behavior policy during policy improvement or evaluation. In this work, we propose a novel MISA framework to approach offline RL from the perspective of Mutual Information between States and Actions in the dataset by directly constraining the policy improvement direction. MISA constructs lower bounds of mutual information parameterized by the policy and Q-values. We show that optimizing this lower bound is equivalent to maximizing the likelihood of a one-step improved policy on the offline dataset. Hence, we constrain the policy improvement direction to lie in the data manifold. The resulting algorithm simultaneously augments the policy e",
    "link": "https://arxiv.org/abs/2210.07484",
    "context": "Title: Mutual Information Regularized Offline Reinforcement Learning\nAbstract: arXiv:2210.07484v3 Announce Type: replace-cross  Abstract: The major challenge of offline RL is the distribution shift that appears when out-of-distribution actions are queried, which makes the policy improvement direction biased by extrapolation errors. Most existing methods address this problem by penalizing the policy or value for deviating from the behavior policy during policy improvement or evaluation. In this work, we propose a novel MISA framework to approach offline RL from the perspective of Mutual Information between States and Actions in the dataset by directly constraining the policy improvement direction. MISA constructs lower bounds of mutual information parameterized by the policy and Q-values. We show that optimizing this lower bound is equivalent to maximizing the likelihood of a one-step improved policy on the offline dataset. Hence, we constrain the policy improvement direction to lie in the data manifold. The resulting algorithm simultaneously augments the policy e",
    "path": "papers/22/10/2210.07484.json",
    "total_tokens": 718,
    "translated_title": "互信息正则化的离线强化学习",
    "translated_abstract": "离线强化学习的主要挑战是当超出分布的动作被查询时出现的分布偏移，这使得策略改进方向受到外推误差的偏置。大多数现有方法通过惩罚在策略改进或评估过程中偏离行为策略的策略或价值来解决这个问题。在这项工作中，我们提出了一个新颖的MISA框架，从数据集中状态和行为之间的互信息的角度直接约束策略改进方向，以应对离线强化学习。",
    "tldr": "该论文提出了一种互信息正则化的离线强化学习方法，通过直接约束策略改进方向，从而有效解决了离线强化学习中出现的分布偏移问题。",
    "en_tdlr": "The paper presents a mutual information regularized offline reinforcement learning method that effectively addresses the distribution shift issue by directly constraining the policy improvement direction in offline RL."
}