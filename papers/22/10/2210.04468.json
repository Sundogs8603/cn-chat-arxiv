{
    "title": "Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation. (arXiv:2210.04468v2 [cs.CL] UPDATED)",
    "abstract": "Past works on multimodal machine translation (MMT) elevate bilingual setup by incorporating additional aligned vision information. However, an image-must requirement of the multimodal dataset largely hinders MMT's development -namely that it demands an aligned form of [image, source text, target text]. This limitation is generally troublesome during the inference phase especially when the aligned image is not provided as in the normal NMT setup. Thus, in this work, we introduce IKD-MMT, a novel MMT framework to support the image-free inference phase via an inversion knowledge distillation scheme. In particular, a multimodal feature generator is executed with a knowledge distillation module, which directly generates the multimodal feature from (only) source texts as the input. While there have been a few prior works entertaining the possibility to support image-free inference for machine translation, their performances have yet to rival the image-must translation. In our experiments, ",
    "link": "http://arxiv.org/abs/2210.04468",
    "context": "Title: Distill the Image to Nowhere: Inversion Knowledge Distillation for Multimodal Machine Translation. (arXiv:2210.04468v2 [cs.CL] UPDATED)\nAbstract: Past works on multimodal machine translation (MMT) elevate bilingual setup by incorporating additional aligned vision information. However, an image-must requirement of the multimodal dataset largely hinders MMT's development -namely that it demands an aligned form of [image, source text, target text]. This limitation is generally troublesome during the inference phase especially when the aligned image is not provided as in the normal NMT setup. Thus, in this work, we introduce IKD-MMT, a novel MMT framework to support the image-free inference phase via an inversion knowledge distillation scheme. In particular, a multimodal feature generator is executed with a knowledge distillation module, which directly generates the multimodal feature from (only) source texts as the input. While there have been a few prior works entertaining the possibility to support image-free inference for machine translation, their performances have yet to rival the image-must translation. In our experiments, ",
    "path": "papers/22/10/2210.04468.json",
    "total_tokens": 1060,
    "translated_title": "失去目标的图像压缩：用反演知识蒸馏支持多模式机器翻译的无图像推断",
    "translated_abstract": "过去多模式机器翻译方面的研究通过融合外来视觉信息提升了双语设置。然而，多模式数据集要求对齐的 [图像、源文本、目标文本] 形式的图像必须在推断阶段提供，这在常规 NMT 设置的无对齐图像情况下特别麻烦。因此，本文引入了 IKD-MMT，一种支持无图像推断的多模式机器翻译框架，通过反演知识蒸馏方案执行多模式特征生成器，直接从（只有）源文本作为输入生成多模式特征。我们的实验表明，我们提出的 IKD-MMT 框架在多个基准数据集上表现优于无图像基线，并达到与图像必须多模式机器翻译类似的性能。我们的研究在使多模式机器翻译更加实用的现实场景中迈出了一步。",
    "tldr": "本文提出 IKD-MMT 框架来支持无图像推断的多模式机器翻译，并通过在源文本中生成多模式特征来实现。该框架在多个基准数据集上表现出优异的性能，为实际应用场景中缺乏对齐图像的机器翻译提供了可行的解决方案。"
}