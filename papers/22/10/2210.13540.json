{
    "title": "Video based Object 6D Pose Estimation using Transformers. (arXiv:2210.13540v2 [cs.CV] CROSS LISTED)",
    "abstract": "We introduce a Transformer based 6D Object Pose Estimation framework VideoPose, comprising an end-to-end attention based modelling architecture, that attends to previous frames in order to estimate accurate 6D Object Poses in videos. Our approach leverages the temporal information from a video sequence for pose refinement, along with being computationally efficient and robust. Compared to existing methods, our architecture is able to capture and reason from long-range dependencies efficiently, thus iteratively refining over video sequences. Experimental evaluation on the YCB-Video dataset shows that our approach is on par with the state-of-the-art Transformer methods, and performs significantly better relative to CNN based approaches. Further, with a speed of 33 fps, it is also more efficient and therefore applicable to a variety of applications that require real-time object pose estimation. Training code and pretrained models are available at https://github.com/ApoorvaBeedu/VideoPose",
    "link": "http://arxiv.org/abs/2210.13540",
    "context": "Title: Video based Object 6D Pose Estimation using Transformers. (arXiv:2210.13540v2 [cs.CV] CROSS LISTED)\nAbstract: We introduce a Transformer based 6D Object Pose Estimation framework VideoPose, comprising an end-to-end attention based modelling architecture, that attends to previous frames in order to estimate accurate 6D Object Poses in videos. Our approach leverages the temporal information from a video sequence for pose refinement, along with being computationally efficient and robust. Compared to existing methods, our architecture is able to capture and reason from long-range dependencies efficiently, thus iteratively refining over video sequences. Experimental evaluation on the YCB-Video dataset shows that our approach is on par with the state-of-the-art Transformer methods, and performs significantly better relative to CNN based approaches. Further, with a speed of 33 fps, it is also more efficient and therefore applicable to a variety of applications that require real-time object pose estimation. Training code and pretrained models are available at https://github.com/ApoorvaBeedu/VideoPose",
    "path": "papers/22/10/2210.13540.json",
    "total_tokens": 940,
    "translated_title": "使用Transformer的基于视频的物体6D姿态估计",
    "translated_abstract": "我们引入了一个名为VideoPose的基于Transformer的6D物体姿态估计框架，该框架采用端到端的基于注意力机制的建模架构，通过关注先前的帧来估计视频中准确的6D物体姿态。我们的方法利用视频序列中的时间信息进行姿态细化，同时具有计算效率高和鲁棒性强的特点。与现有方法相比，我们的架构能够有效地捕捉和推理远距离依赖关系，从而在视频序列上进行迭代细化。对YCB-Video数据集的实验评估结果显示，我们的方法与最先进的Transformer方法持平，并相对于基于CNN的方法表现显著更好。此外，我们的方法每秒能处理33帧，更加高效，因此适用于需要实时物体姿态估计的各种应用。训练代码和预训练模型可在https://github.com/ApoorvaBeedu/VideoPose上获得。",
    "tldr": "本论文介绍了一种基于Transformer的视频中物体6D姿态估计框架，利用先前的帧信息进行姿态估计，实现了高效而准确的姿态估计，能够处理长时间序列依赖关系，并且相对于CNN方法表现更好，具有33fps的处理速度，适用于实时物体姿态估计应用。",
    "en_tdlr": "This paper presents a Transformer-based framework for 6D object pose estimation in videos, leveraging previous frame information and achieving efficient and accurate pose estimation. It handles long-range dependencies and outperforms CNN methods, with a processing speed of 33 fps, making it suitable for real-time object pose estimation applications."
}