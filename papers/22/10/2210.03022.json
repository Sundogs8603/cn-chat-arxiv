{
    "title": "Stateful active facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2210.03022v3 [cs.AI] UPDATED)",
    "abstract": "In cooperative multi-agent reinforcement learning, a team of agents works together to achieve a common goal. Different environments or tasks may require varying degrees of coordination among agents in order to achieve the goal in an optimal way. The nature of coordination will depend on the properties of the environment -- its spatial layout, distribution of obstacles, dynamics, etc. We term this variation of properties within an environment as heterogeneity. Existing literature has not sufficiently addressed the fact that different environments may have different levels of heterogeneity. We formalize the notions of coordination level and heterogeneity level of an environment and present HECOGrid, a suite of multi-agent RL environments that facilitates empirical evaluation of different MARL approaches across different levels of coordination and environmental heterogeneity by providing a quantitative control over coordination and heterogeneity levels of the environment. Further, we prop",
    "link": "http://arxiv.org/abs/2210.03022",
    "context": "Title: Stateful active facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2210.03022v3 [cs.AI] UPDATED)\nAbstract: In cooperative multi-agent reinforcement learning, a team of agents works together to achieve a common goal. Different environments or tasks may require varying degrees of coordination among agents in order to achieve the goal in an optimal way. The nature of coordination will depend on the properties of the environment -- its spatial layout, distribution of obstacles, dynamics, etc. We term this variation of properties within an environment as heterogeneity. Existing literature has not sufficiently addressed the fact that different environments may have different levels of heterogeneity. We formalize the notions of coordination level and heterogeneity level of an environment and present HECOGrid, a suite of multi-agent RL environments that facilitates empirical evaluation of different MARL approaches across different levels of coordination and environmental heterogeneity by providing a quantitative control over coordination and heterogeneity levels of the environment. Further, we prop",
    "path": "papers/22/10/2210.03022.json",
    "total_tokens": 861,
    "translated_title": "有状态的主动协调器：合作多智能体强化学习中的协调与环境异质性",
    "translated_abstract": "在合作的多智能体强化学习中，一组智能体共同努力实现一个共同的目标。不同的环境或任务可能需要不同程度的协调来以最优的方式实现目标。协调的性质取决于环境的属性，例如空间布局、障碍物分布、动态等。我们将环境内属性的这种变化称为异质性。现有文献尚未充分解决不同环境可能具有不同水平的异质性的问题。我们形式化了环境的协调水平和异质性水平的概念，并提出了HECOGrid，这是一个多智能体RL环境套件，通过提供对环境的协调和异质性水平进行定量控制，便于对不同协作和环境异质性的MARL方法进行实证评估。",
    "tldr": "本文研究了合作多智能体强化学习中的协调与环境异质性问题，提出了HECOGrid环境套件，通过对协调和异质性水平的定量控制，便于对不同MARL方法进行实证评估。",
    "en_tdlr": "This paper investigates the coordination and environmental heterogeneity in cooperative multi-agent reinforcement learning, and proposes the HECOGrid environment suite for quantitatively controlling coordination and heterogeneity levels, enabling empirical evaluation of different MARL approaches."
}