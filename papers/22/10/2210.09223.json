{
    "title": "CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models. (arXiv:2210.09223v2 [cs.CV] UPDATED)",
    "abstract": "Driven by significant improvements in architectural design and training pipelines, computer vision has recently experienced dramatic progress in terms of accuracy on classic benchmarks such as ImageNet. These highly-accurate models are challenging to deploy, as they appear harder to compress using standard techniques such as pruning. We address this issue by introducing the Correlation Aware Pruner (CAP), a new unstructured pruning framework which significantly pushes the compressibility limits for state-of-the-art architectures. Our method is based on two technical advancements: a new theoretically-justified pruner, which can handle complex weight correlations accurately and efficiently during the pruning process itself, and an efficient finetuning procedure for post-compression recovery. We validate our approach via extensive experiments on several modern vision models such as Vision Transformers (ViT), modern CNNs, and ViT-CNN hybrids, showing for the first time that these can be pr",
    "link": "http://arxiv.org/abs/2210.09223",
    "context": "Title: CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models. (arXiv:2210.09223v2 [cs.CV] UPDATED)\nAbstract: Driven by significant improvements in architectural design and training pipelines, computer vision has recently experienced dramatic progress in terms of accuracy on classic benchmarks such as ImageNet. These highly-accurate models are challenging to deploy, as they appear harder to compress using standard techniques such as pruning. We address this issue by introducing the Correlation Aware Pruner (CAP), a new unstructured pruning framework which significantly pushes the compressibility limits for state-of-the-art architectures. Our method is based on two technical advancements: a new theoretically-justified pruner, which can handle complex weight correlations accurately and efficiently during the pruning process itself, and an efficient finetuning procedure for post-compression recovery. We validate our approach via extensive experiments on several modern vision models such as Vision Transformers (ViT), modern CNNs, and ViT-CNN hybrids, showing for the first time that these can be pr",
    "path": "papers/22/10/2210.09223.json",
    "total_tokens": 891,
    "translated_title": "CAP: 针对高度准确的稀疏视觉模型的相关性感知剪枝",
    "translated_abstract": "在架构设计和训练流程显著改进的推动下，计算机视觉在 ImageNet 等经典基准测试上取得了显著的准确性提高。然而，这些高度准确的模型部署具有挑战性，因为它们似乎更难使用标准的压缩技术（例如剪枝技术）进行压缩。我们通过引入基于两个技术进展的 Correlation Aware Pruner (CAP) 来解决这个问题：一种新的理论上被证明的剪枝器，可以在剪枝过程中精确、有效地处理复杂的权重相关性；以及一种用于压缩后恢复的高效微调过程。我们通过对多个现代视觉模型（如 Vision Transformers (ViT)、现代 CNN 和 ViT-CNN 混合模型）进行大量实验验证了我们的方法，首次展示了这些模型可以进行高效压缩。",
    "tldr": "该论文介绍了一种新的 Correlation Aware Pruner (CAP) 框架，能够在处理高度准确的稀疏视觉模型剪枝方面推动压缩界限，并且能够通过有效的紧缩恢复过程实现压缩后的性能提升。",
    "en_tdlr": "This paper introduces a new Correlation Aware Pruner (CAP) framework that pushes the limits of compression for highly accurate sparse vision models and improves compressed performance through an efficient recovery process."
}