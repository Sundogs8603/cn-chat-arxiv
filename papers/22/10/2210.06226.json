{
    "title": "Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics. (arXiv:2210.06226v2 [stat.ML] UPDATED)",
    "abstract": "Several algorithms involving the Variational R\\'enyi (VR) bound have been proposed to minimize an alpha-divergence between a target posterior distribution and a variational distribution. Despite promising empirical results, those algorithms resort to biased stochastic gradient descent procedures and thus lack theoretical guarantees. In this paper, we formalize and study the VR-IWAE bound, a generalization of the Importance Weighted Auto-Encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several desirable properties and notably leads to the same stochastic gradient descent procedure as the VR bound in the reparameterized case, but this time by relying on unbiased gradient estimators. We then provide two complementary theoretical analyses of the VR-IWAE bound and thus of the standard IWAE bound. Those analyses shed light on the benefits or lack thereof of these bounds. Lastly, we illustrate our theoretical claims over toy and real-data examples.",
    "link": "http://arxiv.org/abs/2210.06226",
    "context": "Title: Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics. (arXiv:2210.06226v2 [stat.ML] UPDATED)\nAbstract: Several algorithms involving the Variational R\\'enyi (VR) bound have been proposed to minimize an alpha-divergence between a target posterior distribution and a variational distribution. Despite promising empirical results, those algorithms resort to biased stochastic gradient descent procedures and thus lack theoretical guarantees. In this paper, we formalize and study the VR-IWAE bound, a generalization of the Importance Weighted Auto-Encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several desirable properties and notably leads to the same stochastic gradient descent procedure as the VR bound in the reparameterized case, but this time by relying on unbiased gradient estimators. We then provide two complementary theoretical analyses of the VR-IWAE bound and thus of the standard IWAE bound. Those analyses shed light on the benefits or lack thereof of these bounds. Lastly, we illustrate our theoretical claims over toy and real-data examples.",
    "path": "papers/22/10/2210.06226.json",
    "total_tokens": 947,
    "translated_title": "Alpha-divergence变分推断与重要性加权自编码器的结合：方法和渐近性",
    "translated_abstract": "针对目标后验分布和变分分布之间的alpha散度，已经提出了几个涉及变分Rényi (VR)下界的算法。尽管有令人满意的实证结果，但这些算法都采用了有偏的随机梯度下降过程，因此缺乏理论保证。本文对VR-IWAE下界进行了正式化和研究，该下界是重要性加权自编码器(IWAE)下界的推广。我们证明了VR-IWAE下界具有几个可取的特性，特别是在重新参数化的情况下与VR下界导致相同的随机梯度下降过程，但这次是依靠无偏梯度估计器。然后，我们提供了对VR-IWAE下界以及标准IWAE下界的两种互补的理论分析。这些分析揭示了这些下界的好处和缺点。最后，我们通过玩具和真实数据示例来说明我们的理论观点。",
    "tldr": "本文提出了VR-IWAE下界，该下界是IWAE下界的推广，采用无偏梯度估计器能够实现与VR下界相同的随机梯度下降过程，对该下界进行了理论分析，揭示了其优势和不足，并通过示例验证了理论观点。",
    "en_tdlr": "This paper introduces the VR-IWAE bound, a generalization of the IWAE bound, which achieves the same stochastic gradient descent procedure as the VR bound by using unbiased gradient estimators. The theoretical analysis sheds light on the advantages and limitations of this bound, which are validated through examples."
}