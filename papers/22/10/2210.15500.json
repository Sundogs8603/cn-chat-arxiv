{
    "title": "COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation. (arXiv:2210.15500v2 [cs.CL] UPDATED)",
    "abstract": "As language models become increasingly integrated into our digital lives, Personalized Text Generation (PTG) has emerged as a pivotal component with a wide range of applications. However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate different levels of linguistic quality with users' protected attributes. The model can inherit the bias and perpetuate inequality in generating text w.r.t. users' protected attributes, leading to unfair treatment when serving users. In this work, we investigate fairness of PTG in the context of personalized explanation generation for recommendations. We first discuss the biases in generated explanations and their fairness implications. To promote fairness, we introduce a general framework to achieve measure-specific counterfactual fairness in explanation generation. Extensive experiments and human evaluations demonstrate the effectiveness of our method.",
    "link": "http://arxiv.org/abs/2210.15500",
    "context": "Title: COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation. (arXiv:2210.15500v2 [cs.CL] UPDATED)\nAbstract: As language models become increasingly integrated into our digital lives, Personalized Text Generation (PTG) has emerged as a pivotal component with a wide range of applications. However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate different levels of linguistic quality with users' protected attributes. The model can inherit the bias and perpetuate inequality in generating text w.r.t. users' protected attributes, leading to unfair treatment when serving users. In this work, we investigate fairness of PTG in the context of personalized explanation generation for recommendations. We first discuss the biases in generated explanations and their fairness implications. To promote fairness, we introduce a general framework to achieve measure-specific counterfactual fairness in explanation generation. Extensive experiments and human evaluations demonstrate the effectiveness of our method.",
    "path": "papers/22/10/2210.15500.json",
    "total_tokens": 843,
    "translated_title": "COFFEE: 可解释性推荐中个性化文本生成的反事实公平性",
    "translated_abstract": "随着语言模型越来越多地融入到我们的数字生活中，个性化文本生成（PTG）已成为一个具有广泛应用的重要组成部分。然而，用于PTG模型训练的用户编写文本中存在的偏见可能会无意中将不同水平的语言质量与用户的受保护属性关联起来。模型可以继承这种偏见，并在生成与用户的受保护属性相关的文本时延续不平等，导致在为用户提供服务时出现不公平的对待。在这项工作中，我们研究了个性化解释生成中PTG的公平性。我们首先讨论了生成的解释中的偏见及其公平性影响。为了促进公平性，我们引入了一个通用框架，以实现解释生成中的度量特定的反事实公平性。大量实验和人工评估表明了我们方法的有效性。",
    "tldr": "本研究探讨了个性化解释生成中的反事实公平性问题，在解释生成中引入了一个通用框架以实现度量特定的反事实公平性，实验证明了方法的有效性。",
    "en_tdlr": "This research investigates counterfactual fairness in personalized explanation generation and introduces a general framework to achieve measure-specific counterfactual fairness in explanation generation, which has been proven effective through extensive experiments and human evaluations."
}