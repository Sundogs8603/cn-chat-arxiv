{
    "title": "MTEB: Massive Text Embedding Benchmark. (arXiv:2210.07316v3 [cs.CL] UPDATED)",
    "abstract": "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and",
    "link": "http://arxiv.org/abs/2210.07316",
    "context": "Title: MTEB: Massive Text Embedding Benchmark. (arXiv:2210.07316v3 [cs.CL] UPDATED)\nAbstract: Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and",
    "path": "papers/22/10/2210.07316.json",
    "total_tokens": 1099,
    "translated_title": "MTEB: 大规模文本嵌入基准测试",
    "translated_abstract": "文本嵌入通常在覆盖其他任务的可能应用时，仅在单个任务的少量数据集上进行评估。目前还不清楚在语义文本相似度（STS）上最先进的嵌入方法是否同样适用于其他任务，比如聚类或重新排序。这使得评估该领域的进展变得困难，因为各种模型不断被提出却没有得到适当的评估。为了解决这个问题，我们引入了大规模文本嵌入基准测试（MTEB）。MTEB涵盖8个嵌入任务，涵盖58个数据集和112个语言。通过在MTEB上对33个模型进行基准测试，我们建立了迄今为止最全面的文本嵌入基准。我们发现，没有任何一种特定的文本嵌入方法在所有任务中都占据优势。这表明该领域尚未收敛于一种通用的文本嵌入方法，并将其扩展足够大以在所有嵌入任务中提供最先进的结果。MTEB附带开源代码和数据，以使社区能够基准测试新的嵌入模型并跟踪该领域的进展。",
    "tldr": "本文提出了一个大规模文本嵌入基准测试(MTEB)，该基准测试涵盖了8个嵌入任务、58个数据集和112种语言，以解决文本嵌入在不同任务中表现差异的问题。通过33个模型的测试，作者发现该领域尚未收敛于一种通用的文本嵌入方法，",
    "en_tdlr": "This paper proposes the Massive Text Embedding Benchmark (MTEB) covering 8 embedding tasks, 58 datasets, and 112 languages to solve the issue that text embedding is commonly evaluated only on a small set of datasets from a single task. Through testing 33 models on MTEB, the authors find that there is no universal text embedding method that dominates across all tasks. The open-source code and data provided with MTEB enable the community to benchmark new embedding models and track progress in the field."
}