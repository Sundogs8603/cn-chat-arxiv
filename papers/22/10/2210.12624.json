{
    "title": "Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Stochastic Approach",
    "abstract": "arXiv:2210.12624v2 Announce Type: replace  Abstract: Machine learning problems with multiple objective functions appear either in learning with multiple criteria where learning has to make a trade-off between multiple performance metrics such as fairness, safety and accuracy; or, in multi-task learning where multiple tasks are optimized jointly, sharing inductive bias between them. This problems are often tackled by the multi-objective optimization framework. However, existing stochastic multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad, etc.) all adopt a biased noisy gradient direction, which leads to degraded empirical performance. To this end, we develop a stochastic Multi-objective gradient Correction (MoCo) method for multi-objective optimization. The unique feature of our method is that it can guarantee convergence without increasing the batch size even in the non-convex setting. Simulations on multi-task supervised and reinforcement learning demonstra",
    "link": "https://arxiv.org/abs/2210.12624",
    "context": "Title: Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Stochastic Approach\nAbstract: arXiv:2210.12624v2 Announce Type: replace  Abstract: Machine learning problems with multiple objective functions appear either in learning with multiple criteria where learning has to make a trade-off between multiple performance metrics such as fairness, safety and accuracy; or, in multi-task learning where multiple tasks are optimized jointly, sharing inductive bias between them. This problems are often tackled by the multi-objective optimization framework. However, existing stochastic multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad, etc.) all adopt a biased noisy gradient direction, which leads to degraded empirical performance. To this end, we develop a stochastic Multi-objective gradient Correction (MoCo) method for multi-objective optimization. The unique feature of our method is that it can guarantee convergence without increasing the batch size even in the non-convex setting. Simulations on multi-task supervised and reinforcement learning demonstra",
    "path": "papers/22/10/2210.12624.json",
    "total_tokens": 846,
    "translated_title": "缓解多目标学习中的梯度偏差：一种可证明收敛的随机方法",
    "translated_abstract": "具有多个目标函数的机器学习问题通常出现在需要在多个性能指标（如公平性，安全性和准确性）之间进行权衡的多目标学习中；或者在多任务学习中，多个任务联合优化，共享它们之间的归纳偏差。然而，现有的随机多目标梯度方法及其变体（例如，MGDA，PCGrad，CAGrad等）都采用带偏差的噪声梯度方向，导致经验性能下降。为此，我们开发了一种用于多目标优化的随机多目标梯度校正（MoCo）方法。我们方法的独特之处在于，即使在非凸设置中也能保证收敛而不增加批量大小。对多任务监督学习和强化学习进行了模拟实验。",
    "tldr": "提出了一种随机多目标梯度校正（MoCo）方法，能够在不增加批量大小的情况下保证收敛，解决了多目标学习中梯度偏差导致性能下降的问题。",
    "en_tdlr": "Introduced a stochastic Multi-objective gradient Correction (MoCo) method that guarantees convergence without increasing batch size, addressing the issue of degraded performance in multi-objective learning due to gradient bias."
}