{
    "title": "Analyzing Acoustic Word Embeddings from Pre-trained Self-supervised Speech Models. (arXiv:2210.16043v2 [cs.CL] UPDATED)",
    "abstract": "Given the strong results of self-supervised models on various tasks, there have been surprisingly few studies exploring self-supervised representations for acoustic word embeddings (AWE), fixed-dimensional vectors representing variable-length spoken word segments. In this work, we study several pre-trained models and pooling methods for constructing AWEs with self-supervised representations. Owing to the contextualized nature of self-supervised representations, we hypothesize that simple pooling methods, such as averaging, might already be useful for constructing AWEs. When evaluating on a standard word discrimination task, we find that HuBERT representations with mean-pooling rival the state of the art on English AWEs. More surprisingly, despite being trained only on English, HuBERT representations evaluated on Xitsonga, Mandarin, and French consistently outperform the multilingual model XLSR-53 (as well as Wav2Vec 2.0 trained on English).",
    "link": "http://arxiv.org/abs/2210.16043",
    "context": "Title: Analyzing Acoustic Word Embeddings from Pre-trained Self-supervised Speech Models. (arXiv:2210.16043v2 [cs.CL] UPDATED)\nAbstract: Given the strong results of self-supervised models on various tasks, there have been surprisingly few studies exploring self-supervised representations for acoustic word embeddings (AWE), fixed-dimensional vectors representing variable-length spoken word segments. In this work, we study several pre-trained models and pooling methods for constructing AWEs with self-supervised representations. Owing to the contextualized nature of self-supervised representations, we hypothesize that simple pooling methods, such as averaging, might already be useful for constructing AWEs. When evaluating on a standard word discrimination task, we find that HuBERT representations with mean-pooling rival the state of the art on English AWEs. More surprisingly, despite being trained only on English, HuBERT representations evaluated on Xitsonga, Mandarin, and French consistently outperform the multilingual model XLSR-53 (as well as Wav2Vec 2.0 trained on English).",
    "path": "papers/22/10/2210.16043.json",
    "total_tokens": 925,
    "translated_title": "从预训练的自监督语音模型分析声学词嵌入",
    "translated_abstract": "自监督模型取得了在各种任务上的强大结果，但对于用于代表可变长的口语单词片段的声学词嵌入（AWE），却很少有研究探索自监督表示法。在这项工作中，我们研究了几种预训练模型和汇聚方法，用于构建具有自监督表示法的AWE。由于自监督表示法具有上下文化的特点，我们假设简单的汇聚方法，如平均法，可能已经有助于构建AWE。在标准的单词区分任务上进行评估时，我们发现具有均值汇聚的HuBERT表示法已经可以与英语AWE的最新表现相媲美。更令人惊讶的是，尽管只在英语上进行训练，但在Xitsonga、普通话和法语上评估的HuBERT表示法始终优于多语言模型XLSR-53（以及在英语上训练的Wav2Vec 2.0）。",
    "tldr": "本篇论文研究了自监督模型用于构建声学词嵌入（AWE）的效果，发现平均汇聚的HuBERT表示法已经可以与英语AWE的最新表现相媲美，而且在其他语言上也表现优异。",
    "en_tdlr": "This paper studies the effectiveness of self-supervised models for constructing acoustic word embeddings (AWE), and finds that mean-pooling HuBERT representations can rival the state of the art on English AWEs and outperform multilingual models, even when evaluated on different languages."
}