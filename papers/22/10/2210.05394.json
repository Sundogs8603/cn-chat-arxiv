{
    "title": "Computationally-efficient initialisation of GPs: The generalised variogram method. (arXiv:2210.05394v3 [cs.LG] UPDATED)",
    "abstract": "We present a computationally-efficient strategy to initialise the hyperparameters of a Gaussian process (GP) avoiding the computation of the likelihood function. Our strategy can be used as a pretraining stage to find initial conditions for maximum-likelihood (ML) training, or as a standalone method to compute hyperparameters values to be plugged in directly into the GP model. Motivated by the fact that training a GP via ML is equivalent (on average) to minimising the KL-divergence between the true and learnt model, we set to explore different metrics/divergences among GPs that are computationally inexpensive and provide hyperparameter values that are close to those found via ML. In practice, we identify the GP hyperparameters by projecting the empirical covariance or (Fourier) power spectrum onto a parametric family, thus proposing and studying various measures of discrepancy operating on the temporal and frequency domains. Our contribution extends the variogram method developed by th",
    "link": "http://arxiv.org/abs/2210.05394",
    "context": "Title: Computationally-efficient initialisation of GPs: The generalised variogram method. (arXiv:2210.05394v3 [cs.LG] UPDATED)\nAbstract: We present a computationally-efficient strategy to initialise the hyperparameters of a Gaussian process (GP) avoiding the computation of the likelihood function. Our strategy can be used as a pretraining stage to find initial conditions for maximum-likelihood (ML) training, or as a standalone method to compute hyperparameters values to be plugged in directly into the GP model. Motivated by the fact that training a GP via ML is equivalent (on average) to minimising the KL-divergence between the true and learnt model, we set to explore different metrics/divergences among GPs that are computationally inexpensive and provide hyperparameter values that are close to those found via ML. In practice, we identify the GP hyperparameters by projecting the empirical covariance or (Fourier) power spectrum onto a parametric family, thus proposing and studying various measures of discrepancy operating on the temporal and frequency domains. Our contribution extends the variogram method developed by th",
    "path": "papers/22/10/2210.05394.json",
    "total_tokens": 897,
    "translated_title": "高斯过程的计算有效初始化：广义变异函数方法",
    "translated_abstract": "我们提出了一种旨在初始化高斯过程（GP）超参数的计算有效策略，避免了计算似然函数。我们的策略可以用作预训练阶段，以查找最大似然（ML）训练的初始条件，或作为独立方法来计算超参数值，以直接插入GP模型中。我们受到这样一个事实的启发，即通过ML训练来训练GP与最小化真实模型和学习模型之间的KL散度（平均而言）是等效的，因此我们开始探索不同的GP度量/散度，它们计算起来廉价，并提供接近通过ML发现的超参数值。在实践中，我们通过将经验协方差或（傅里叶）功率谱投影到参数族上来识别GP超参数，因此提出并研究在时间和频率域上运作的各种差异度量。我们的贡献扩展了由th开发的变异函数方法",
    "tldr": "该论文提出了一种计算有效的策略，避免了计算似然函数，可用作初始化超参数，使模型得到更好的训练，该策略在实践中证明了对于高斯过程是有效的。",
    "en_tdlr": "This paper proposes a computationally-efficient strategy for initializing the hyperparameters of a Gaussian process (GP), which avoids computing the likelihood function and can be used to improve model training. The proposed method has been proven effective in practice for GP."
}