{
    "title": "\"Why did the Model Fail?\": Attributing Model Performance Changes to Distribution Shifts. (arXiv:2210.10769v3 [cs.LG] UPDATED)",
    "abstract": "Machine learning models frequently experience performance drops under distribution shifts. The underlying cause of such shifts may be multiple simultaneous factors such as changes in data quality, differences in specific covariate distributions, or changes in the relationship between label and features. When a model does fail during deployment, attributing performance change to these factors is critical for the model developer to identify the root cause and take mitigating actions. In this work, we introduce the problem of attributing performance differences between environments to distribution shifts in the underlying data generating mechanisms. We formulate the problem as a cooperative game where the players are distributions. We define the value of a set of distributions to be the change in model performance when only this set of distributions has changed between environments, and derive an importance weighting method for computing the value of an arbitrary set of distributions. The",
    "link": "http://arxiv.org/abs/2210.10769",
    "context": "Title: \"Why did the Model Fail?\": Attributing Model Performance Changes to Distribution Shifts. (arXiv:2210.10769v3 [cs.LG] UPDATED)\nAbstract: Machine learning models frequently experience performance drops under distribution shifts. The underlying cause of such shifts may be multiple simultaneous factors such as changes in data quality, differences in specific covariate distributions, or changes in the relationship between label and features. When a model does fail during deployment, attributing performance change to these factors is critical for the model developer to identify the root cause and take mitigating actions. In this work, we introduce the problem of attributing performance differences between environments to distribution shifts in the underlying data generating mechanisms. We formulate the problem as a cooperative game where the players are distributions. We define the value of a set of distributions to be the change in model performance when only this set of distributions has changed between environments, and derive an importance weighting method for computing the value of an arbitrary set of distributions. The",
    "path": "papers/22/10/2210.10769.json",
    "total_tokens": 837,
    "translated_title": "为什么模型会失败？将模型性能变化归因于分布偏移",
    "translated_abstract": "机器学习模型在分布偏移下经常会出现性能下降的情况。这种偏移的根本原因可能是多重的因素，比如数据质量的变化、特定协变量分布的差异或者标签与特征之间的关系变化等。当模型在部署时失败时，将性能变化归因于这些因素对于模型开发人员来说至关重要，以识别根本原因并采取缓解措施。在本文中，我们介绍了将环境之间的性能差异归因于底层数据生成机制的分布偏移问题。我们将该问题构造为一种合作博弈的形式，其中玩家是分布。我们定义一组分布的价值为当只有这组分布在环境之间发生变化时模型性能的变化，并推导出一种重要性权重方法以计算任意一组分布的价值。",
    "tldr": "本文介绍了一种将模型性能变化归因于底层数据生成机制的分布偏移的方法，并通过推导一种重要性权重方法来计算任意一组分布的价值。",
    "en_tdlr": "This paper introduces a method for attributing performance differences between environments to distribution shifts in the underlying data generating mechanisms, and derives an importance weighting method for computing the value of an arbitrary set of distributions."
}