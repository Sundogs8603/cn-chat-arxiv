{
    "title": "DABERT: Dual Attention Enhanced BERT for Semantic Matching. (arXiv:2210.03454v3 [cs.CL] UPDATED)",
    "abstract": "Transformer-based pre-trained language models such as BERT have achieved remarkable results in Semantic Sentence Matching. However, existing models still suffer from insufficient ability to capture subtle differences. Minor noise like word addition, deletion, and modification of sentences may cause flipped predictions. To alleviate this problem, we propose a novel Dual Attention Enhanced BERT (DABERT) to enhance the ability of BERT to capture fine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention module, which measures soft word matches by introducing a new dual channel alignment mechanism to model affinity and difference attention. (2) Adaptive Fusion module, this module uses attention to learn the aggregation of difference and affinity features, and generates a vector describing the matching details of sentence pairs. We conduct extensive experiments on well-studied semantic matching and robustness test datasets, and the experimental results show the effectiv",
    "link": "http://arxiv.org/abs/2210.03454",
    "context": "Title: DABERT: Dual Attention Enhanced BERT for Semantic Matching. (arXiv:2210.03454v3 [cs.CL] UPDATED)\nAbstract: Transformer-based pre-trained language models such as BERT have achieved remarkable results in Semantic Sentence Matching. However, existing models still suffer from insufficient ability to capture subtle differences. Minor noise like word addition, deletion, and modification of sentences may cause flipped predictions. To alleviate this problem, we propose a novel Dual Attention Enhanced BERT (DABERT) to enhance the ability of BERT to capture fine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention module, which measures soft word matches by introducing a new dual channel alignment mechanism to model affinity and difference attention. (2) Adaptive Fusion module, this module uses attention to learn the aggregation of difference and affinity features, and generates a vector describing the matching details of sentence pairs. We conduct extensive experiments on well-studied semantic matching and robustness test datasets, and the experimental results show the effectiv",
    "path": "papers/22/10/2210.03454.json",
    "total_tokens": 811,
    "translated_title": "DABERT：双重注意力增强的BERT语义匹配模型",
    "translated_abstract": "基于Transformer的预训练语言模型（如BERT）在语义句子匹配方面取得了杰出的成果。然而，现有模型仍然在捕捉微小差异的能力上存在不足。如加入、删除或修改句子中的一个单词等噪声可能导致模型预测出错。为了缓解这个问题，我们提出了一种新颖的双重注意力增强的BERT模型（DABERT），以增强BERT在捕捉句子对之间细微差异方面的能力。DABERT由（1）双重注意力模块和（2）自适应融合模块构成。我们在经典的语义匹配和鲁棒性测试数据集上进行了广泛的实验，实验结果表明了DABERT的有效性。",
    "tldr": "DABERT 通过双重注意力机制和自适应融合模块增强了 BERT 在捕捉句子对之间细微差异的能力，并在实验中取得了良好的效果。",
    "en_tdlr": "DABERT enhances BERT's ability to capture fine-grained differences in sentence pairs by introducing a dual attention mechanism and adaptive fusion module. Experimental results on semantic matching and robustness test datasets show its effectiveness."
}