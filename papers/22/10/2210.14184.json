{
    "title": "Learning Ability of Interpolating Deep Convolutional Neural Networks. (arXiv:2210.14184v2 [stat.ML] UPDATED)",
    "abstract": "It is frequently observed that overparameterized neural networks generalize well. Regarding such phenomena, existing theoretical work mainly devotes to linear settings or fully-connected neural networks. This paper studies the learning ability of an important family of deep neural networks, deep convolutional neural networks (DCNNs), under both underparameterized and overparameterized settings. We establish the first learning rates of underparameterized DCNNs without parameter or function variable structure restrictions presented in the literature. We also show that by adding well-defined layers to a non-interpolating DCNN, we can obtain some interpolating DCNNs that maintain the good learning rates of the non-interpolating DCNN. This result is achieved by a novel network deepening scheme designed for DCNNs. Our work provides theoretical verification of how overfitted DCNNs generalize well.",
    "link": "http://arxiv.org/abs/2210.14184",
    "context": "Title: Learning Ability of Interpolating Deep Convolutional Neural Networks. (arXiv:2210.14184v2 [stat.ML] UPDATED)\nAbstract: It is frequently observed that overparameterized neural networks generalize well. Regarding such phenomena, existing theoretical work mainly devotes to linear settings or fully-connected neural networks. This paper studies the learning ability of an important family of deep neural networks, deep convolutional neural networks (DCNNs), under both underparameterized and overparameterized settings. We establish the first learning rates of underparameterized DCNNs without parameter or function variable structure restrictions presented in the literature. We also show that by adding well-defined layers to a non-interpolating DCNN, we can obtain some interpolating DCNNs that maintain the good learning rates of the non-interpolating DCNN. This result is achieved by a novel network deepening scheme designed for DCNNs. Our work provides theoretical verification of how overfitted DCNNs generalize well.",
    "path": "papers/22/10/2210.14184.json",
    "total_tokens": 857,
    "translated_title": "插值深度卷积神经网络的学习能力",
    "translated_abstract": "高参数神经网络往往具有良好的泛化能力。现有的理论工作主要研究了线性设置或全连接神经网络的情况。本文研究了一类重要的深度神经网络，即深度卷积神经网络（DCNNs）在欠参数和过参数设置下的学习能力。我们在文献中首次建立了无参数或函数变量结构限制的欠参数DCNNs的学习速度。我们还表明，通过向非插值DCNN添加良定义的层，可以获得一些保持非插值DCNN良好学习速度的插值DCNN。这一结果是通过为DCNN设计的一种新颖的网络加深方案实现的。我们的工作在理论上验证了过拟合的DCNN如何良好地进行泛化。",
    "tldr": "本文研究了深度卷积神经网络（DCNNs）在欠参数和过参数设置下的学习能力，建立了欠参数DCNNs的学习速度，并通过一种新颖的网络加深方案获得了插值DCNN，从而验证了过拟合的DCNN的泛化性能。"
}