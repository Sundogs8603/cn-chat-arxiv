{
    "title": "Federated Learning Using Variance Reduced Stochastic Gradient for Probabilistically Activated Agents. (arXiv:2210.14362v2 [cs.LG] UPDATED)",
    "abstract": "This paper proposes an algorithm for Federated Learning (FL) with a two-layer structure that achieves both variance reduction and a faster convergence rate to an optimal solution in the setting where each agent has an arbitrary probability of selection in each iteration. In distributed machine learning, when privacy matters, FL is a functional tool. Placing FL in an environment where it has some irregular connections of agents (devices), reaching a trained model in both an economical and quick way can be a demanding job. The first layer of our algorithm corresponds to the model parameter propagation across agents done by the server. In the second layer, each agent does its local update with a stochastic and variance-reduced technique called Stochastic Variance Reduced Gradient (SVRG). We leverage the concept of variance reduction from stochastic optimization when the agents want to do their local update step to reduce the variance caused by stochastic gradient descent (SGD). We provide",
    "link": "http://arxiv.org/abs/2210.14362",
    "context": "Title: Federated Learning Using Variance Reduced Stochastic Gradient for Probabilistically Activated Agents. (arXiv:2210.14362v2 [cs.LG] UPDATED)\nAbstract: This paper proposes an algorithm for Federated Learning (FL) with a two-layer structure that achieves both variance reduction and a faster convergence rate to an optimal solution in the setting where each agent has an arbitrary probability of selection in each iteration. In distributed machine learning, when privacy matters, FL is a functional tool. Placing FL in an environment where it has some irregular connections of agents (devices), reaching a trained model in both an economical and quick way can be a demanding job. The first layer of our algorithm corresponds to the model parameter propagation across agents done by the server. In the second layer, each agent does its local update with a stochastic and variance-reduced technique called Stochastic Variance Reduced Gradient (SVRG). We leverage the concept of variance reduction from stochastic optimization when the agents want to do their local update step to reduce the variance caused by stochastic gradient descent (SGD). We provide",
    "path": "papers/22/10/2210.14362.json",
    "total_tokens": 821,
    "tldr": "本文提出了一种联合学习算法，采用方差缩减的随机梯度下降技术，使得在可能任意选择的代理中迭代收敛到最优解更快，能够应用于保护隐私的分布式机器学习中。"
}