{
    "title": "Take a Fresh Look at Recommender Systems from an Evaluation Standpoint. (arXiv:2210.04149v2 [cs.IR] UPDATED)",
    "abstract": "Recommendation has become a prominent area of research in the field of Information Retrieval (IR). Evaluation is also a traditional research topic in this community. Motivated by a few counter-intuitive observations reported in recent studies, this perspectives paper takes a fresh look at recommender systems from an evaluation standpoint. Rather than examining metrics like recall, hit rate, or NDCG, or perspectives like novelty and diversity, the key focus here is on how these metrics are calculated when evaluating a recommender algorithm. Specifically, the commonly used train/test data splits and their consequences are re-examined. We begin by examining common data splitting methods, such as random split or leave-one-out, and discuss why the popularity baseline is poorly defined under such splits. We then move on to explore the two implications of neglecting a global timeline during evaluation: data leakage and oversimplification of user preference modeling. Afterwards, we present new",
    "link": "http://arxiv.org/abs/2210.04149",
    "context": "Title: Take a Fresh Look at Recommender Systems from an Evaluation Standpoint. (arXiv:2210.04149v2 [cs.IR] UPDATED)\nAbstract: Recommendation has become a prominent area of research in the field of Information Retrieval (IR). Evaluation is also a traditional research topic in this community. Motivated by a few counter-intuitive observations reported in recent studies, this perspectives paper takes a fresh look at recommender systems from an evaluation standpoint. Rather than examining metrics like recall, hit rate, or NDCG, or perspectives like novelty and diversity, the key focus here is on how these metrics are calculated when evaluating a recommender algorithm. Specifically, the commonly used train/test data splits and their consequences are re-examined. We begin by examining common data splitting methods, such as random split or leave-one-out, and discuss why the popularity baseline is poorly defined under such splits. We then move on to explore the two implications of neglecting a global timeline during evaluation: data leakage and oversimplification of user preference modeling. Afterwards, we present new",
    "path": "papers/22/10/2210.04149.json",
    "total_tokens": 897,
    "tldr": "本文从评估角度重新审视推荐系统。重新审视常用的训练/测试数据分割方法及其后果，探讨流行度基准线定义不明确以及忽略全局时间线对评估的两个影响。最后，提出新的评估框架以便更好地评价推荐系统。"
}