{
    "title": "A Unified Encoder-Decoder Framework with Entity Memory. (arXiv:2210.03273v3 [cs.CL] UPDATED)",
    "abstract": "Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks. We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they suffered from a large computational overhead. In this work, we propose an encoder-decoder framework with an entity memory, namely EDMem. The entity knowledge is stored in the memory as latent representations, and the memory is pre-trained on Wikipedia along with encoder-decoder parameters. To precisely generate entity names, we design three decoding methods to constrain entity generation by linking entities in the memory. EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks. Extensive experimental results show that EDMem outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.",
    "link": "http://arxiv.org/abs/2210.03273",
    "context": "Title: A Unified Encoder-Decoder Framework with Entity Memory. (arXiv:2210.03273v3 [cs.CL] UPDATED)\nAbstract: Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks. We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they suffered from a large computational overhead. In this work, we propose an encoder-decoder framework with an entity memory, namely EDMem. The entity knowledge is stored in the memory as latent representations, and the memory is pre-trained on Wikipedia along with encoder-decoder parameters. To precisely generate entity names, we design three decoding methods to constrain entity generation by linking entities in the memory. EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks. Extensive experimental results show that EDMem outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.",
    "path": "papers/22/10/2210.03273.json",
    "total_tokens": 954,
    "translated_title": "一种具有实体记忆的统一编码器-解码器框架",
    "translated_abstract": "实体作为现实世界知识的重要载体，在许多自然语言处理任务中起着关键作用。本文关注如何将实体知识融入到编码器-解码器框架中，以进行信息性文本生成。现有方法尝试通过索引、检索和阅读外部文档作为证据，但是它们的计算开销很大。为此，我们提出了一个名为EDMem的编码器-解码器框架，其中实体知识以潜在表示形式存储在记忆中，并在维基百科上预先训练记忆和编码器-解码器参数。为了精确生成实体名称，我们设计了三种解码方法，以通过记忆中的实体链接限制实体生成。EDMem是一个统一的框架，可用于各种实体密集型问答和生成任务。广泛的实验结果表明，EDMem优于基于记忆的自动编码器模型和非记忆编码器-解码器模型。",
    "tldr": "本论文提出了一种名为EDMem的具有实体记忆的编码器-解码器框架，可以将实体知识融入到信息性文本生成中。实体知识以潜在表示形式存储在记忆中，并利用记忆中的实体链接限制实体生成。实验结果表明，EDMem优于基于记忆的自动编码器模型和非记忆编码器-解码器模型。",
    "en_tdlr": "This paper proposes a unified encoder-decoder framework with an entity memory (EDMem) to incorporate entity knowledge into informative text generation. EDMem stores entity knowledge in latent representations and constrains entity generation by linking entities in memory. Experimental results demonstrate that EDMem outperforms memory-based auto-encoder and non-memory encoder-decoder models."
}