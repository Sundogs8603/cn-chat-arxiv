{
    "title": "Learning New Tasks from a Few Examples with Soft-Label Prototypes",
    "abstract": "arXiv:2210.17437v3 Announce Type: replace-cross  Abstract: Existing approaches to few-shot learning in NLP rely on large language models and fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a simple yet powerful approach to \"extreme\" few-shot learning, wherein models are exposed to as little as 4 examples per class, based on soft-label prototypes that collectively capture the distribution of different classes across the input domain space. Inspired by previous work (Sucholutsky et al., 2021) on univariate or simple multivariate (synthetic) data, we propose a novel approach that is effective on large, high-dimensional and real-world datasets. We learn soft-label prototypes within a neural framework (DeepSLP) and we experimentally demonstrate that it achieves superior performance on 31/48 tested tasks and few-shot settings while closely matching the performance of strong baselines on the rest. We focus on learning previously unseen NLP tasks from v",
    "link": "https://arxiv.org/abs/2210.17437",
    "context": "Title: Learning New Tasks from a Few Examples with Soft-Label Prototypes\nAbstract: arXiv:2210.17437v3 Announce Type: replace-cross  Abstract: Existing approaches to few-shot learning in NLP rely on large language models and fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a simple yet powerful approach to \"extreme\" few-shot learning, wherein models are exposed to as little as 4 examples per class, based on soft-label prototypes that collectively capture the distribution of different classes across the input domain space. Inspired by previous work (Sucholutsky et al., 2021) on univariate or simple multivariate (synthetic) data, we propose a novel approach that is effective on large, high-dimensional and real-world datasets. We learn soft-label prototypes within a neural framework (DeepSLP) and we experimentally demonstrate that it achieves superior performance on 31/48 tested tasks and few-shot settings while closely matching the performance of strong baselines on the rest. We focus on learning previously unseen NLP tasks from v",
    "path": "papers/22/10/2210.17437.json",
    "total_tokens": 870,
    "translated_title": "用软标签原型从少量示例中学习新任务",
    "translated_abstract": "自然语言处理中的少样本学习现有方法依赖于大型语言模型和对其微调，以在分布外数据上进行泛化。在这项工作中，我们提出了一种简单但强大的“极端”少样本学习方法，其中模型只需接触每个类别至少4个示例，这些示例基于软标签原型，这些软标签原型共同捕获了输入域空间中不同类别的分布。受到先前关于一元或简单多元（合成）数据（Sucholutsky等人，2021）的工作的启发，我们提出了一种在大型、高维和现实世界数据集上有效的新方法。我们在神经框架（DeepSLP）中学习软标签原型，并在实验中展示，它在31/48个测试任务和少样本设置上表现优异，同时在其他任务上与强基线模型的性能相匹配。我们专注于从v中学习以前未见过的NLP任务",
    "tldr": "本研究提出了一种新的极端少样本学习方法，利用软标签原型从少量示例中学习新任务，在大型、高维和现实世界数据集上表现出色。",
    "en_tdlr": "This work introduces a novel approach to \"extreme\" few-shot learning using soft-label prototypes to learn new tasks from a small number of examples, demonstrating superior performance on large, high-dimensional, and real-world datasets."
}