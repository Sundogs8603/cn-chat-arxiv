{
    "title": "Can Language Models Be Specific? How?. (arXiv:2210.05159v2 [cs.CL] UPDATED)",
    "abstract": "\"He is a person\", \"Paris is located on the earth\". Both statements are correct but meaningless - due to lack of specificity. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance, given \"Toronto is located in [MASK].\", we want to test whether a more specific answer will be better filled in by PLMs, e.g., Ontario instead of Canada. From our evaluations, we show that existing PLMs have only a slight preference for more specific answers. We identify underlying factors affecting the specificity and design two prompt-based methods to improve the specificity. Results show that the specificity of the models can be improved by the proposed methods without additional training. We hope this work can bring to awareness the notion of specificity of language models and encourage the research",
    "link": "http://arxiv.org/abs/2210.05159",
    "context": "Title: Can Language Models Be Specific? How?. (arXiv:2210.05159v2 [cs.CL] UPDATED)\nAbstract: \"He is a person\", \"Paris is located on the earth\". Both statements are correct but meaningless - due to lack of specificity. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance, given \"Toronto is located in [MASK].\", we want to test whether a more specific answer will be better filled in by PLMs, e.g., Ontario instead of Canada. From our evaluations, we show that existing PLMs have only a slight preference for more specific answers. We identify underlying factors affecting the specificity and design two prompt-based methods to improve the specificity. Results show that the specificity of the models can be improved by the proposed methods without additional training. We hope this work can bring to awareness the notion of specificity of language models and encourage the research",
    "path": "papers/22/10/2210.05159.json",
    "total_tokens": 909,
    "translated_title": "语言模型能够具体化吗？如何实现？",
    "translated_abstract": "“他是一个人”、“巴黎位于地球上”。这些语句都是正确的，但没有具体性——因为缺乏明确的内容。在本文中，我们提出了一种度量预训练语言模型（PLMs）的具体性的方法。为了实现这一点，我们引入了一种新方法来建立具体性测试的基准，通过带有提示的掩码标记预测任务来实现。例如，给定“多伦多位于[MASK]中”，我们想测试PLMs是否能更好地填写更具体的答案，例如安大略省而不是加拿大。从我们的评估中，我们发现现有的PLMs只对更具体的答案略微更有偏好。我们确定了影响具体性的潜在因素，并设计了两种基于提示的方法以改善具体性。结果表明，通过所提出的方法，模型的具体性可以得到改善，而无需进行额外的训练。我们希望这项工作能带来语言模型具体性的认识，并鼓励相关研究。",
    "tldr": "本论文提出了一种度量预训练语言模型具体性的方法，并设计了两种基于提示的方法，以改善模型具体性，结果表明，模型的具体性可以得到改善，而无需进行额外的训练。",
    "en_tdlr": "The paper proposes a method to measure the specificity of pre-trained language models (PLMs) and designs two prompt-based methods to improve their specificity without extra training. Results show potential for improving specificity, bringing awareness to the notion of specificity of language models."
}