{
    "title": "PathProx: A Proximal Gradient Algorithm for Weight Decay Regularized Deep Neural Networks. (arXiv:2210.03069v3 [cs.LG] UPDATED)",
    "abstract": "Weight decay is one of the most widely used forms of regularization in deep learning, and has been shown to improve generalization and robustness. The optimization objective driving weight decay is a sum of losses plus a term proportional to the sum of squared weights. This paper argues that stochastic gradient descent (SGD) may be an inefficient algorithm for this objective. For neural networks with ReLU activations, solutions to the weight decay objective are equivalent to those of a different objective in which the regularization term is instead a sum of products of $\\ell_2$ (not squared) norms of the input and output weights associated with each ReLU neuron. This alternative (and effectively equivalent) regularization suggests a novel proximal gradient algorithm for network training. Theory and experiments support the new training approach, showing that it can converge much faster to the sparse solutions it shares with standard weight decay training.",
    "link": "http://arxiv.org/abs/2210.03069",
    "context": "Title: PathProx: A Proximal Gradient Algorithm for Weight Decay Regularized Deep Neural Networks. (arXiv:2210.03069v3 [cs.LG] UPDATED)\nAbstract: Weight decay is one of the most widely used forms of regularization in deep learning, and has been shown to improve generalization and robustness. The optimization objective driving weight decay is a sum of losses plus a term proportional to the sum of squared weights. This paper argues that stochastic gradient descent (SGD) may be an inefficient algorithm for this objective. For neural networks with ReLU activations, solutions to the weight decay objective are equivalent to those of a different objective in which the regularization term is instead a sum of products of $\\ell_2$ (not squared) norms of the input and output weights associated with each ReLU neuron. This alternative (and effectively equivalent) regularization suggests a novel proximal gradient algorithm for network training. Theory and experiments support the new training approach, showing that it can converge much faster to the sparse solutions it shares with standard weight decay training.",
    "path": "papers/22/10/2210.03069.json",
    "total_tokens": 885,
    "translated_title": "PathProx: 一种用于权值衰减正则化深度神经网络的近端梯度算法",
    "translated_abstract": "权值衰减是深度学习中最广泛使用的正则化方法之一，已被证明可以提高泛化能力和鲁棒性。驱动权值衰减的优化目标是损失之和加上与权值平方和成比例的项。本文认为，随机梯度下降（SGD）可能是这个目标的一种低效算法。对于带有ReLU激活函数的神经网络，权重衰减目标的解与另一个目标的解是等价的，其中正则化项改为与每个ReLU神经元关联的输入和输出权重的$\\ell_2$（不是平方）范数乘积之和。这种替代（并且有效等价）的正则化方法提出了一种用于网络训练的新近端梯度算法。理论和实验证实了这种新的训练方法，显示它可以更快地收敛到标准权值衰减训练所共享的稀疏解。",
    "tldr": "本文提出一种用于权值衰减正则化深度神经网络的近端梯度算法 PathProx，它可以更快地收敛到标准权值衰减训练所共享的稀疏解。",
    "en_tdlr": "This paper proposes a proximal gradient algorithm called PathProx for weight decay regularized deep neural networks, which can converge faster to the sparse solutions shared with standard weight decay training."
}