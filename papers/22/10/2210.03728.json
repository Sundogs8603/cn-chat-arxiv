{
    "title": "Dynamic Latent Separation for Deep Learning",
    "abstract": "A core problem in machine learning is to learn expressive latent variables for model prediction on complex data that involves multiple sub-components in a flexible and interpretable fashion. Here, we develop an approach that improves expressiveness, provides partial interpretation, and is not restricted to specific applications. The key idea is to dynamically distance data samples in the latent space and thus enhance the output diversity. Our dynamic latent separation method, inspired by atomic physics, relies on the jointly learned structures of each data sample, which also reveal the importance of each sub-component for distinguishing data samples. This approach, atom modeling, requires no supervision of the latent space and allows us to learn extra partially interpretable representations besides the original goal of a model. We empirically demonstrate that the algorithm also enhances the performance of small to larger-scale models in various classification and generation problems.",
    "link": "https://arxiv.org/abs/2210.03728",
    "context": "Title: Dynamic Latent Separation for Deep Learning\nAbstract: A core problem in machine learning is to learn expressive latent variables for model prediction on complex data that involves multiple sub-components in a flexible and interpretable fashion. Here, we develop an approach that improves expressiveness, provides partial interpretation, and is not restricted to specific applications. The key idea is to dynamically distance data samples in the latent space and thus enhance the output diversity. Our dynamic latent separation method, inspired by atomic physics, relies on the jointly learned structures of each data sample, which also reveal the importance of each sub-component for distinguishing data samples. This approach, atom modeling, requires no supervision of the latent space and allows us to learn extra partially interpretable representations besides the original goal of a model. We empirically demonstrate that the algorithm also enhances the performance of small to larger-scale models in various classification and generation problems.",
    "path": "papers/22/10/2210.03728.json",
    "total_tokens": 872,
    "translated_title": "深度学习的动态潜变量分离",
    "translated_abstract": "机器学习中的一个核心问题是以灵活和可解释的方式学习用于复杂数据模型预测的表达性潜变量，这些数据包含多个子组件。我们开发了一种方法，改进了表达性，提供了部分解释，并且不限于特定的应用。关键思想是在潜空间中动态地分离数据样本，从而增强输出的多样性。我们的动态潜变量分离方法受到原子物理学的启发，依赖于每个数据样本共同学习的结构，这也揭示出了每个子组件在区分数据样本中的重要性。这种方法，原子建模，不需要对潜空间进行监督，并且允许我们学习额外的部分可解释表示，除了模型的原始目标。实验证明，该算法还提高了各种分类和生成问题中小到大规模模型的性能。",
    "tldr": "本研究提出了动态潜变量分离的方法，可以在复杂数据中学习表达性强的潜变量，提升输出的多样性。该方法受原子物理学启发，通过学习每个数据样本的结构来解释各个子组件的重要性。实验证明该方法在不同分类和生成问题中提升了模型的性能。",
    "en_tdlr": "This study proposes a dynamic latent separation method that learns expressive latent variables in complex data, enhancing output diversity. Inspired by atomic physics, the method interprets the importance of each sub-component by learning the structure of each data sample. Experimental results demonstrate improved performance in various classification and generation problems."
}