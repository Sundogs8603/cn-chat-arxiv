{
    "title": "Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)",
    "abstract": "In many complex sequential decision-making tasks, online planning is crucial for high performance. For efficient online planning, Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off exploration for exploitation. MCTS outperforms comparison methods in many discrete decision-making domains such as Go, Chess, and Shogi. Following, extensions of MCTS to continuous domains have been proposed. However, the inherent high branching factor and the resulting explosion of search tree size are limiting existing methods. To address this problem, we propose Continuous Monte Carlo Graph Search (CMCGS), a novel extension of MCTS to online planning in environments with continuous state and action spaces. CMCGS takes advantage of the insight that, during planning, sharing the same action policy between several states can yield high performance. To implement this idea, at each time step, CMCGS clusters similar states into a limited number of stochastic action bandit nodes, which",
    "link": "http://arxiv.org/abs/2210.01426",
    "context": "Title: Continuous Monte Carlo Graph Search. (arXiv:2210.01426v2 [cs.AI] UPDATED)\nAbstract: In many complex sequential decision-making tasks, online planning is crucial for high performance. For efficient online planning, Monte Carlo Tree Search (MCTS) employs a principled mechanism for trading off exploration for exploitation. MCTS outperforms comparison methods in many discrete decision-making domains such as Go, Chess, and Shogi. Following, extensions of MCTS to continuous domains have been proposed. However, the inherent high branching factor and the resulting explosion of search tree size are limiting existing methods. To address this problem, we propose Continuous Monte Carlo Graph Search (CMCGS), a novel extension of MCTS to online planning in environments with continuous state and action spaces. CMCGS takes advantage of the insight that, during planning, sharing the same action policy between several states can yield high performance. To implement this idea, at each time step, CMCGS clusters similar states into a limited number of stochastic action bandit nodes, which",
    "path": "papers/22/10/2210.01426.json",
    "total_tokens": 982,
    "translated_title": "连续蒙特卡洛图搜索",
    "translated_abstract": "在许多复杂的连续决策任务中，在线规划对于高性能至关重要。为了实现高效的在线规划，蒙特卡洛树搜索（MCTS）采用了一个有原则的机制来权衡探索和利用。MCTS在许多离散决策领域（如围棋、国际象棋和将棋）中胜过了其他方法。而针对连续领域的MCTS扩展也已提出。然而，由于固有的高分支因子和导致搜索树大小爆炸的问题，现有方法受到了限制。为了解决这个问题，我们提出了连续蒙特卡洛图搜索（CMCGS），这是一种新颖的MCTS扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS利用了一个洞察力，在规划过程中，将相似状态之间共享相同的动作策略可以得到高性能。为了实现这个想法，CMCGS在每个时间步骤中将相似状态聚类成有限数量的随机动作赌博节点，这些节点共享相同的动作策略。",
    "tldr": "连续蒙特卡洛图搜索（CMCGS）是一种新颖的蒙特卡洛树搜索（MCTS）的扩展，适用于具有连续状态和动作空间的在线规划环境。CMCGS通过将相似状态聚类，并共享相同的动作策略，实现了高性能的在线规划。",
    "en_tdlr": "Continuous Monte Carlo Graph Search (CMCGS) is a novel extension of Monte Carlo Tree Search (MCTS) that is applicable to online planning in environments with continuous state and action spaces. CMCGS achieves high-performance online planning by clustering similar states and sharing the same action policy."
}