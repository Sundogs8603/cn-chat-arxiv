{
    "title": "Multi-User Reinforcement Learning with Low Rank Rewards. (arXiv:2210.05355v2 [cs.LG] UPDATED)",
    "abstract": "In this work, we consider the problem of collaborative multi-user reinforcement learning. In this setting there are multiple users with the same state-action space and transition probabilities but with different rewards. Under the assumption that the reward matrix of the $N$ users has a low-rank structure -- a standard and practically successful assumption in the offline collaborative filtering setting -- the question is can we design algorithms with significantly lower sample complexity compared to the ones that learn the MDP individually for each user. Our main contribution is an algorithm which explores rewards collaboratively with $N$ user-specific MDPs and can learn rewards efficiently in two key settings: tabular MDPs and linear MDPs. When $N$ is large and the rank is constant, the sample complexity per MDP depends logarithmically over the size of the state-space, which represents an exponential reduction (in the state-space size) when compared to the standard ``non-collaborative",
    "link": "http://arxiv.org/abs/2210.05355",
    "context": "Title: Multi-User Reinforcement Learning with Low Rank Rewards. (arXiv:2210.05355v2 [cs.LG] UPDATED)\nAbstract: In this work, we consider the problem of collaborative multi-user reinforcement learning. In this setting there are multiple users with the same state-action space and transition probabilities but with different rewards. Under the assumption that the reward matrix of the $N$ users has a low-rank structure -- a standard and practically successful assumption in the offline collaborative filtering setting -- the question is can we design algorithms with significantly lower sample complexity compared to the ones that learn the MDP individually for each user. Our main contribution is an algorithm which explores rewards collaboratively with $N$ user-specific MDPs and can learn rewards efficiently in two key settings: tabular MDPs and linear MDPs. When $N$ is large and the rank is constant, the sample complexity per MDP depends logarithmically over the size of the state-space, which represents an exponential reduction (in the state-space size) when compared to the standard ``non-collaborative",
    "path": "papers/22/10/2210.05355.json",
    "total_tokens": 823,
    "translated_title": "低秩奖励下的多用户强化学习",
    "translated_abstract": "本文考虑了协作多用户强化学习问题。在此设置中，有多个用户具有相同的状态-动作空间和转移概率，但具有不同的奖励。在假设N个用户的奖励矩阵具有低秩结构的情况下，我们能否设计具有显着较低样本复杂度的算法，与为每个用户分别学习MDP的算法相比。我们的主要贡献是一种算法，它与N个用户特定的MDP一起探索奖励，并可以在两个关键设置中高效地学习奖励：表格MDP和线性MDP。当N很大且秩是常数时，每个MDP的样本复杂度对状态空间大小取对数，这代表了在状态空间大小上的指数降低（与标准的“非协作”相比）。",
    "tldr": "本文提出了一个新的多用户强化学习算法，可以在用户奖励矩阵具有低秩结构的情况下显著降低样本复杂度。",
    "en_tdlr": "This paper proposes a novel multi-user reinforcement learning algorithm that significantly reduces sample complexity when the user reward matrix has a low-rank structure."
}