{
    "title": "Brand New K-FACs: Speeding up K-FAC with Online Decomposition Updates. (arXiv:2210.08494v2 [cs.LG] UPDATED)",
    "abstract": "K-FAC (arXiv:1503.05671, arXiv:1602.01407) is a tractable implementation of Natural Gradient (NG) for Deep Learning (DL), whose bottleneck is computing the inverses of the so-called ``Kronecker-Factors'' (K-factors). RS-KFAC (arXiv:2206.15397) is a K-FAC improvement which provides a cheap way of estimating the K-factors inverses.  In this paper, we exploit the exponential-average construction paradigm of the K-factors, and use online numerical linear algebra techniques to propose an even cheaper (but less accurate) way of estimating the K-factors inverses. In particular, we propose a K-factor inverse update which scales linearly in layer size. We also propose an inverse application procedure which scales linearly as well (the one of K-FAC scales cubically and the one of RS-KFAC scales quadratically). Overall, our proposed algorithm gives an approximate K-FAC implementation whose preconditioning part scales linearly in layer size (compare to cubic for K-FAC and quadratic for RS-KFAC). I",
    "link": "http://arxiv.org/abs/2210.08494",
    "context": "Title: Brand New K-FACs: Speeding up K-FAC with Online Decomposition Updates. (arXiv:2210.08494v2 [cs.LG] UPDATED)\nAbstract: K-FAC (arXiv:1503.05671, arXiv:1602.01407) is a tractable implementation of Natural Gradient (NG) for Deep Learning (DL), whose bottleneck is computing the inverses of the so-called ``Kronecker-Factors'' (K-factors). RS-KFAC (arXiv:2206.15397) is a K-FAC improvement which provides a cheap way of estimating the K-factors inverses.  In this paper, we exploit the exponential-average construction paradigm of the K-factors, and use online numerical linear algebra techniques to propose an even cheaper (but less accurate) way of estimating the K-factors inverses. In particular, we propose a K-factor inverse update which scales linearly in layer size. We also propose an inverse application procedure which scales linearly as well (the one of K-FAC scales cubically and the one of RS-KFAC scales quadratically). Overall, our proposed algorithm gives an approximate K-FAC implementation whose preconditioning part scales linearly in layer size (compare to cubic for K-FAC and quadratic for RS-KFAC). I",
    "path": "papers/22/10/2210.08494.json",
    "total_tokens": 960,
    "translated_title": "全新的K-FACs：利用在线分解更新加速K-FAC",
    "translated_abstract": "K-FAC是Deep Learning中可行的Natural Gradient的实现，其瓶颈在于计算所谓的“Kronecker-Factors”（K-factors）的逆。RS-KFAC是K-FAC的改进，提供了估计K-factors逆的一种廉价方法。本文利用K-factors的指数平均构造范式，并使用在线数值线性代数技术，提出了一种更为廉价（但精度较低）的估计K-factors逆的方法。特别地，我们提出了一个在层大小上线性缩放的K-factor逆更新方法。我们还提出了一个在逆应用过程中也线性缩放的方法（K-FAC的逆应用过程是三次方缩放的，RS-KFAC的是二次缩放的）。总体上，我们提出的算法提供了一个近似的K-FAC实现，其预调节部分在层大小上线性缩放（与K-FAC的三次方和RS-KFAC的二次方相比）。",
    "tldr": "本文提出了全新的K-FACs算法，利用在线分解更新来加速K-FAC的计算速度。这种算法比传统的K-FAC和RS-KFAC更廉价，虽然精度较低，但在预调节部分的计算复杂度上只有线性缩放。",
    "en_tdlr": "This paper proposes a brand new algorithm, K-FACs, to accelerate the computation of K-FAC using online decomposition updates. This algorithm is cheaper compared to traditional K-FAC and RS-KFAC, and although it has lower accuracy, the computational complexity in the preconditioning part scales linearly."
}