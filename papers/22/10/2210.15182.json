{
    "title": "Text2Model: Text-based Model Induction for Zero-shot Image Classification",
    "abstract": "arXiv:2210.15182v2 Announce Type: replace-cross  Abstract: We address the challenge of building task-agnostic classifiers using only text descriptions, demonstrating a unified approach to image classification, 3D point cloud classification, and action recognition from scenes. Unlike approaches that learn a fixed representation of the output classes, we generate at inference time a model tailored to a query classification task. To generate task-based zero-shot classifiers, we train a hypernetwork that receives class descriptions and outputs a multi-class model. The hypernetwork is designed to be equivariant with respect to the set of descriptions and the classification layer, thus obeying the symmetries of the problem and improving generalization. Our approach generates non-linear classifiers and can handle rich textual descriptions. We evaluate this approach in a series of zero-shot classification tasks, for image, point-cloud, and action recognition, using a range of text descriptions",
    "link": "https://arxiv.org/abs/2210.15182",
    "context": "Title: Text2Model: Text-based Model Induction for Zero-shot Image Classification\nAbstract: arXiv:2210.15182v2 Announce Type: replace-cross  Abstract: We address the challenge of building task-agnostic classifiers using only text descriptions, demonstrating a unified approach to image classification, 3D point cloud classification, and action recognition from scenes. Unlike approaches that learn a fixed representation of the output classes, we generate at inference time a model tailored to a query classification task. To generate task-based zero-shot classifiers, we train a hypernetwork that receives class descriptions and outputs a multi-class model. The hypernetwork is designed to be equivariant with respect to the set of descriptions and the classification layer, thus obeying the symmetries of the problem and improving generalization. Our approach generates non-linear classifiers and can handle rich textual descriptions. We evaluate this approach in a series of zero-shot classification tasks, for image, point-cloud, and action recognition, using a range of text descriptions",
    "path": "papers/22/10/2210.15182.json",
    "total_tokens": 773,
    "translated_title": "Text2Model:基于文本的模型归纳用于零样本图像分类",
    "translated_abstract": "我们解决了仅使用文本描述构建与任务无关的分类器的挑战，展示了一种统一的方法来进行图像分类、3D点云分类以及从场景中识别动作。与学习固定输出类别表示的方法不同，我们在推断时生成针对查询分类任务定制的模型。为了生成基于任务的零样本分类器，我们训练一个超网络，该网络接收类描述并输出一个多类模型。超网络设计为对描述集合和分类层具有等变性，因此符合问题的对称性并提高了泛化性能。我们的方法生成非线性分类器，并且可以处理丰富的文本描述。我们在一系列零样本分类任务中评估了这种方法，涵盖了图像、点云和动作识别，并使用一系列文本描述。",
    "tldr": "该论文提出了一种使用文本描述构建与任务无关的分类器的方法，通过生成针对查询分类任务定制的模型来解决零样本图像分类问题。",
    "en_tdlr": "The paper introduces a method to build task-agnostic classifiers using text descriptions, addressing zero-shot image classification by generating models tailored for query classification tasks."
}