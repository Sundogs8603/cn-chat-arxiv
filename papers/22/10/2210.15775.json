{
    "title": "Evaluating context-invariance in unsupervised speech representations. (arXiv:2210.15775v2 [cs.CL] UPDATED)",
    "abstract": "Unsupervised speech representations have taken off, with benchmarks (SUPERB, ZeroSpeech) demonstrating major progress on semi-supervised speech recognition, speech synthesis, and speech-only language modelling. Inspiration comes from the promise of ``discovering the phonemes'' of a language or a similar low-bitrate encoding. However, one of the critical properties of phoneme transcriptions is context-invariance: the phonetic context of a speech sound can have massive influence on the way it is pronounced, while the text remains stable. This is what allows tokens of the same word to have the same transcriptions -- key to language understanding. Current benchmarks do not measure context-invariance. We develop a new version of the ZeroSpeech ABX benchmark that measures context-invariance, and apply it to recent self-supervised representations. We demonstrate that the context-independence of representations is predictive of the stability of word-level representations. We suggest research c",
    "link": "http://arxiv.org/abs/2210.15775",
    "context": "Title: Evaluating context-invariance in unsupervised speech representations. (arXiv:2210.15775v2 [cs.CL] UPDATED)\nAbstract: Unsupervised speech representations have taken off, with benchmarks (SUPERB, ZeroSpeech) demonstrating major progress on semi-supervised speech recognition, speech synthesis, and speech-only language modelling. Inspiration comes from the promise of ``discovering the phonemes'' of a language or a similar low-bitrate encoding. However, one of the critical properties of phoneme transcriptions is context-invariance: the phonetic context of a speech sound can have massive influence on the way it is pronounced, while the text remains stable. This is what allows tokens of the same word to have the same transcriptions -- key to language understanding. Current benchmarks do not measure context-invariance. We develop a new version of the ZeroSpeech ABX benchmark that measures context-invariance, and apply it to recent self-supervised representations. We demonstrate that the context-independence of representations is predictive of the stability of word-level representations. We suggest research c",
    "path": "papers/22/10/2210.15775.json",
    "total_tokens": 815,
    "translated_title": "评估非监督语音表示中的上下文不变性",
    "translated_abstract": "非监督语音表示已经蓬勃发展，基准测试（如SUPERB，ZeroSpeech）已经在半监督语音识别、语音合成和纯语音模型方面取得了重大进展。其灵感来自于“发现语言的音素”或类似的低比特率编码的承诺。然而，音素转录的一个关键属性是上下文不变性：语音声音的语音上下文可能对其发音方式产生巨大影响，而文本保持稳定。这正是同一单词的标记具有相同转录的关键所在--对语言理解至关重要。目前的基准测试并未衡量上下文不变性。我们开发了ZeroSpeech ABX基准测试的新版本，以衡量上下文不变性，并将其应用于最近的自我监督表示。我们证明了表示的上下文无关性可以预测单词级表示的稳定性。我们建议进一步研究。",
    "tldr": "评估非监督语音表示中的上下文不变性，有效预测单词级表示的稳定性。",
    "en_tdlr": "Evaluating context-invariance in unsupervised speech representations can effectively predict the stability of word-level representations."
}