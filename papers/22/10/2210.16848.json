{
    "title": "Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings. (arXiv:2210.16848v2 [cs.CL] UPDATED)",
    "abstract": "Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability. In this paper, we aim to improve word embeddings by 1) incorporating more contextual information from existing pre-trained models into the Skip-gram framework, which we call Context-to-Vec; 2) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution. Through extrinsic and intrinsic tasks, our methods are well proven to outperform the baselines by a large margin.",
    "link": "http://arxiv.org/abs/2210.16848",
    "context": "Title: Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings. (arXiv:2210.16848v2 [cs.CL] UPDATED)\nAbstract: Although contextualized embeddings generated from large-scale pre-trained models perform well in many tasks, traditional static embeddings (e.g., Skip-gram, Word2Vec) still play an important role in low-resource and lightweight settings due to their low computational cost, ease of deployment, and stability. In this paper, we aim to improve word embeddings by 1) incorporating more contextual information from existing pre-trained models into the Skip-gram framework, which we call Context-to-Vec; 2) proposing a post-processing retrofitting method for static embeddings independent of training by employing priori synonym knowledge and weighted vector distribution. Through extrinsic and intrinsic tasks, our methods are well proven to outperform the baselines by a large margin.",
    "path": "papers/22/10/2210.16848.json",
    "total_tokens": 848,
    "translated_title": "使用上下文向量和图形装配改进词嵌入",
    "translated_abstract": "尽管从大型预训练模型生成的上下文化嵌入在许多任务中表现良好，但由于计算成本低、部署便捷、稳定性高，传统的静态嵌入（例如Skip-gram、Word2Vec）仍在低资源和轻量级环境中发挥着重要作用。本文旨在通过以下方法改进词嵌入：1）将更多从现有预训练模型中获得的上下文信息纳入Skip-gram框架中，我们称之为Context-to-Vec；2）提出一个基于先验同义词知识和加权向量分布的静态嵌入后处理装配方法，独立于训练。通过外部和内部任务，我们的方法被证明能够以大幅超越基准线的性能表现。",
    "tldr": "本文提出了一种改进词嵌入的方法，分别为将更多上下文信息纳入Skip-gram框架和提出一个基于先验同义词知识和加权向量分布的静态嵌入后处理装配方法。这两种方法经由外部和内部任务的检验，能够大幅度超越基准线。",
    "en_tdlr": "This paper proposes a method to improve word embeddings by incorporating more contextual information into the Skip-gram framework (called Context-to-Vec) and proposing a post-processing retrofitting method for static embeddings using prior synonym knowledge and weighted vector distribution. These methods have shown significant improvement over baselines through extrinsic and intrinsic tasks."
}