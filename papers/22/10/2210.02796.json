{
    "title": "Hypernetwork approach to Bayesian MAML. (arXiv:2210.02796v2 [cs.LG] UPDATED)",
    "abstract": "The main goal of Few-Shot learning algorithms is to enable learning from small amounts of data. One of the most popular and elegant Few-Shot learning approaches is Model-Agnostic Meta-Learning (MAML). The main idea behind this method is to learn the shared universal weights of a meta-model, which are then adapted for specific tasks. However, the method suffers from over-fitting and poorly quantifies uncertainty due to limited data size. Bayesian approaches could, in principle, alleviate these shortcomings by learning weight distributions in place of point-wise weights. Unfortunately, previous modifications of MAML are limited due to the simplicity of Gaussian posteriors, MAML-like gradient-based weight updates, or by the same structure enforced for universal and adapted weights.  In this paper, we propose a novel framework for Bayesian MAML called BayesianHMAML, which employs Hypernetworks for weight updates. It learns the universal weights point-wise, but a probabilistic structure is ",
    "link": "http://arxiv.org/abs/2210.02796",
    "context": "Title: Hypernetwork approach to Bayesian MAML. (arXiv:2210.02796v2 [cs.LG] UPDATED)\nAbstract: The main goal of Few-Shot learning algorithms is to enable learning from small amounts of data. One of the most popular and elegant Few-Shot learning approaches is Model-Agnostic Meta-Learning (MAML). The main idea behind this method is to learn the shared universal weights of a meta-model, which are then adapted for specific tasks. However, the method suffers from over-fitting and poorly quantifies uncertainty due to limited data size. Bayesian approaches could, in principle, alleviate these shortcomings by learning weight distributions in place of point-wise weights. Unfortunately, previous modifications of MAML are limited due to the simplicity of Gaussian posteriors, MAML-like gradient-based weight updates, or by the same structure enforced for universal and adapted weights.  In this paper, we propose a novel framework for Bayesian MAML called BayesianHMAML, which employs Hypernetworks for weight updates. It learns the universal weights point-wise, but a probabilistic structure is ",
    "path": "papers/22/10/2210.02796.json",
    "total_tokens": 815,
    "translated_title": "Bayesian MAML的超网络方法",
    "translated_abstract": "少样本学习算法的主要目标是能够从少量数据中进行学习。其中一种最流行且优雅的少样本学习方法是模型无关元学习（MAML）。该方法的主要思想是学习元模型的共享通用权重，然后将其适应于特定任务。然而，该方法存在过拟合问题，因为数据量有限，很难准确量化不确定性。贝叶斯方法可以通过学习权重分布而不是点值权重来缓解这些问题。不幸的是，先前修改的MAML方法由于高斯分布的简单性、基于梯度的类MAML权重更新或强制执行相同结构的通用权重和适应权重，受到一定的限制。在本文中，我们提出了一种新的贝叶斯MAML框架，称为贝叶斯HMAML，它利用超网络进行权重更新。",
    "tldr": "该论文提出了一种名为贝叶斯HMAML的新框架，利用超网络进行权重更新，以解决MAML的过拟合和不确定性问题。"
}