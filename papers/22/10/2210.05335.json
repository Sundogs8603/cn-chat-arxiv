{
    "title": "MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model. (arXiv:2210.05335v2 [cs.CV] UPDATED)",
    "abstract": "Multimodal semantic understanding often has to deal with uncertainty, which means the obtained messages tend to refer to multiple targets. Such uncertainty is problematic for our interpretation, including inter- and intra-modal uncertainty. Little effort has studied the modeling of this uncertainty, particularly in pre-training on unlabeled datasets and fine-tuning in task-specific downstream datasets. In this paper, we project the representations of all modalities as probabilistic distributions via a Probability Distribution Encoder (PDE) by utilizing sequence-level interactions. Compared to the existing deterministic methods, such uncertainty modeling can convey richer multimodal semantic information and more complex relationships. Furthermore, we integrate uncertainty modeling with popular pre-training frameworks and propose suitable pre-training tasks: Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribut",
    "link": "http://arxiv.org/abs/2210.05335",
    "context": "Title: MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model. (arXiv:2210.05335v2 [cs.CV] UPDATED)\nAbstract: Multimodal semantic understanding often has to deal with uncertainty, which means the obtained messages tend to refer to multiple targets. Such uncertainty is problematic for our interpretation, including inter- and intra-modal uncertainty. Little effort has studied the modeling of this uncertainty, particularly in pre-training on unlabeled datasets and fine-tuning in task-specific downstream datasets. In this paper, we project the representations of all modalities as probabilistic distributions via a Probability Distribution Encoder (PDE) by utilizing sequence-level interactions. Compared to the existing deterministic methods, such uncertainty modeling can convey richer multimodal semantic information and more complex relationships. Furthermore, we integrate uncertainty modeling with popular pre-training frameworks and propose suitable pre-training tasks: Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribut",
    "path": "papers/22/10/2210.05335.json",
    "total_tokens": 1049,
    "translated_title": "MAP：多模态不确定性感知的视觉语言预训练模型",
    "translated_abstract": "多模态语义理解经常需要处理不确定性，导致所获得的信息倾向于涉及多个目标。这种不确定性对我们的解释来说是有问题的，包括跨模态和内部模态的不确定性。鲜有研究探讨该不确定性的建模，特别是在未标记的数据集上预训练和在特定任务下游数据集中进行微调。在本文中，我们利用序列级交互通过概率分布编码器（PDE）将所有模态的表示投影为概率分布。与现有的确定性方法相比，这种不确定性建模可以传递更丰富的多模态语义信息和更复杂的关系。此外，我们将不确定性建模与流行的预训练框架结合起来，并提出适合的预训练任务:基于分布的视觉语言对比（D-VLC）、基于分布的掩蔽语言建模（D-MLM）和分布式图像检索（D-IR）。我们评估了我们提出的模型——多模态不确定性感知的视觉语言预训练模型（MAP）在各种下游任务中的表现，包括视觉问答、图像字幕和指称表达理解。实验结果表明，MAP比其确定性对应物表现更好，并在几个基准测试中达到了最先进的性能。",
    "tldr": "本文提出了一种利用概率分布编码器进行多模态不确定性建模的预训练模型MAP，该模型在多项下游任务中均表现优异，超越了现有模型。",
    "en_tdlr": "This paper proposes a pre-training model called Multimodal Uncertainty-Aware Vision-Language Pre-training Model (MAP), which models uncertainty using a Probability Distribution Encoder (PDE) and achieves state-of-the-art performance on multiple downstream tasks, surpassing existing models."
}