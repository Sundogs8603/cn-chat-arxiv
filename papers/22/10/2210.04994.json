{
    "title": "Sampling-based inference for large linear models, with application to linearised Laplace. (arXiv:2210.04994v3 [stat.ML] UPDATED)",
    "abstract": "Large-scale linear models are ubiquitous throughout machine learning, with contemporary application as surrogate models for neural network uncertainty quantification; that is, the linearised Laplace method. Alas, the computational cost associated with Bayesian linear models constrains this method's application to small networks, small output spaces and small datasets. We address this limitation by introducing a scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models, together with a matching method for hyperparameter (regularisation) selection. Furthermore, we use a classic feature normalisation method (the g-prior) to resolve a previously highlighted pathology of the linearised Laplace method. Together, these contributions allow us to perform linearised neural network inference with ResNet-18 on CIFAR100 (11M parameters, 100 outputs x 50k datapoints), with ResNet-50 on Imagenet (50M parameters, 1000 outputs x 1.2M datapoints) and with a U-Net ",
    "link": "http://arxiv.org/abs/2210.04994",
    "context": "Title: Sampling-based inference for large linear models, with application to linearised Laplace. (arXiv:2210.04994v3 [stat.ML] UPDATED)\nAbstract: Large-scale linear models are ubiquitous throughout machine learning, with contemporary application as surrogate models for neural network uncertainty quantification; that is, the linearised Laplace method. Alas, the computational cost associated with Bayesian linear models constrains this method's application to small networks, small output spaces and small datasets. We address this limitation by introducing a scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models, together with a matching method for hyperparameter (regularisation) selection. Furthermore, we use a classic feature normalisation method (the g-prior) to resolve a previously highlighted pathology of the linearised Laplace method. Together, these contributions allow us to perform linearised neural network inference with ResNet-18 on CIFAR100 (11M parameters, 100 outputs x 50k datapoints), with ResNet-50 on Imagenet (50M parameters, 1000 outputs x 1.2M datapoints) and with a U-Net ",
    "path": "papers/22/10/2210.04994.json",
    "total_tokens": 1037,
    "tldr": "本文提出了一种可扩展的基于采样的贝叶斯推断方法，用于解决共轭高斯多输出线性模型的超参数选择问题，并在线性化的神经网络不确定性量化中取得了优秀的表现。"
}