{
    "title": "UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a Single Image. (arXiv:2210.09477v4 [cs.CV] UPDATED)",
    "abstract": "Text-driven image generation methods have shown impressive results recently, allowing casual users to generate high quality images by providing textual descriptions. However, similar capabilities for editing existing images are still out of reach. Text-driven image editing methods usually need edit masks, struggle with edits that require significant visual changes and cannot easily keep specific details of the edited portion. In this paper we make the observation that image-generation models can be converted to image-editing models simply by fine-tuning them on a single image. We also show that initializing the stochastic sampler with a noised version of the base image before the sampling and interpolating relevant details from the base image after sampling further increase the quality of the edit operation. Combining these observations, we propose UniTune, a novel image editing method. UniTune gets as input an arbitrary image and a textual edit description, and carries out the edit wh",
    "link": "http://arxiv.org/abs/2210.09477",
    "context": "Title: UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a Single Image. (arXiv:2210.09477v4 [cs.CV] UPDATED)\nAbstract: Text-driven image generation methods have shown impressive results recently, allowing casual users to generate high quality images by providing textual descriptions. However, similar capabilities for editing existing images are still out of reach. Text-driven image editing methods usually need edit masks, struggle with edits that require significant visual changes and cannot easily keep specific details of the edited portion. In this paper we make the observation that image-generation models can be converted to image-editing models simply by fine-tuning them on a single image. We also show that initializing the stochastic sampler with a noised version of the base image before the sampling and interpolating relevant details from the base image after sampling further increase the quality of the edit operation. Combining these observations, we propose UniTune, a novel image editing method. UniTune gets as input an arbitrary image and a textual edit description, and carries out the edit wh",
    "path": "papers/22/10/2210.09477.json",
    "total_tokens": 906,
    "translated_title": "UniTune: 通过在单个图像上微调扩散模型进行文本驱动的图像编辑",
    "translated_abstract": "最近，基于文本的图像生成方法取得了令人瞩目的成果，使普通用户能够通过提供文本描述生成高质量的图像。然而，对于编辑现有图像来说，类似的能力仍然无法实现。基于文本驱动的图像编辑方法通常需要编辑掩码，对于需要进行显著视觉变化的编辑有困难，并且无法轻松保留编辑部分的特定细节。在本文中，我们观察到图像生成模型可以通过在单个图像上进行微调来转换为图像编辑模型。我们还展示了在采样前使用对基础图像添加噪声的初始随机采样器，并在采样后插值相关细节，进一步提高了编辑操作的质量。结合这些观察结果，我们提出了一种新颖的图像编辑方法UniTune。UniTune接收任意图像和文本编辑描述作为输入，并执行编辑操作。",
    "tldr": "本文提出了一种新的图像编辑方法UniTune，通过在单个图像上微调扩散模型来将图像生成模型转换为图像编辑模型。UniTune能够实现基于文本的图像编辑，避免了使用编辑掩码和丢失编辑部分细节的问题。",
    "en_tdlr": "This paper proposes UniTune, a new image editing method that converts image generation models into image editing models by fine-tuning them on a single image. UniTune enables text-driven image editing, avoiding the need for edit masks and preserving specific details of the edited portion."
}