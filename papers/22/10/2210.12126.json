{
    "title": "One-Shot Neural Fields for 3D Object Understanding. (arXiv:2210.12126v3 [cs.RO] UPDATED)",
    "abstract": "We present a unified and compact scene representation for robotics, where each object in the scene is depicted by a latent code capturing geometry and appearance. This representation can be decoded for various tasks such as novel view rendering, 3D reconstruction (e.g. recovering depth, point clouds, or voxel maps), collision checking, and stable grasp prediction. We build our representation from a single RGB input image at test time by leveraging recent advances in Neural Radiance Fields (NeRF) that learn category-level priors on large multiview datasets, then fine-tune on novel objects from one or few views. We expand the NeRF model for additional grasp outputs and explore ways to leverage this representation for robotics. At test-time, we build the representation from a single RGB input image observing the scene from only one viewpoint. We find that the recovered representation allows rendering from novel views, including of occluded object parts, and also for predicting successful ",
    "link": "http://arxiv.org/abs/2210.12126",
    "context": "Title: One-Shot Neural Fields for 3D Object Understanding. (arXiv:2210.12126v3 [cs.RO] UPDATED)\nAbstract: We present a unified and compact scene representation for robotics, where each object in the scene is depicted by a latent code capturing geometry and appearance. This representation can be decoded for various tasks such as novel view rendering, 3D reconstruction (e.g. recovering depth, point clouds, or voxel maps), collision checking, and stable grasp prediction. We build our representation from a single RGB input image at test time by leveraging recent advances in Neural Radiance Fields (NeRF) that learn category-level priors on large multiview datasets, then fine-tune on novel objects from one or few views. We expand the NeRF model for additional grasp outputs and explore ways to leverage this representation for robotics. At test-time, we build the representation from a single RGB input image observing the scene from only one viewpoint. We find that the recovered representation allows rendering from novel views, including of occluded object parts, and also for predicting successful ",
    "path": "papers/22/10/2210.12126.json",
    "total_tokens": 981,
    "translated_title": "一次性神经场用于3D对象理解",
    "translated_abstract": "我们提出了一种用于机器人学的统一且紧凑的场景表示方法，其中场景中的每个对象由捕捉几何和外观的潜在代码来描述。这种表示方法可以解码用于各种任务，例如新视角渲染，3D重建（例如恢复深度，点云或体素图），碰撞检查和稳定抓取预测。我们利用最新的神经辐射场（NeRF）在测试时从单个RGB输入图像构建我们的表示方法，该方法在大型多视图数据集上学习类别级先验知识，然后在少数或仅一个视图的新对象上进行微调。我们扩展了NeRF模型以获得额外的抓取输出，并探索了利用这种表示方法用于机器人学的方法。在测试时，我们从仅一个视点观察到的单个RGB输入图像构建表示方法。我们发现恢复的表示方法允许从新视角进行渲染，包括遮挡的物体部分，并且可以预测成功的抓取。",
    "tldr": "本文提出了一种一次性神经场方法，用于机器人学中的3D对象理解。这种方法利用单个RGB图像构建统一而紧凑的场景表示，可以用于多个任务，如新视角渲染、3D重建、碰撞检查和稳定抓取预测。研究结果表明，这种方法能够从新视角进行渲染并预测成功的抓取。",
    "en_tdlr": "This paper proposes a one-shot neural fields method for 3D object understanding in robotics. The method utilizes a single RGB image to construct a unified and compact scene representation, which can be used for various tasks such as novel view rendering, 3D reconstruction, collision checking, and stable grasp prediction. The results show that this method enables rendering from novel views and predicting successful grasps."
}