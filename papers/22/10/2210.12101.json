{
    "title": "Neural Network Approximations of PDEs Beyond Linearity: A Representational Perspective. (arXiv:2210.12101v2 [cs.LG] UPDATED)",
    "abstract": "A burgeoning line of research leverages deep neural networks to approximate the solutions to high dimensional PDEs, opening lines of theoretical inquiry focused on explaining how it is that these models appear to evade the curse of dimensionality. However, most prior theoretical analyses have been limited to linear PDEs. In this work, we take a step towards studying the representational power of neural networks for approximating solutions to nonlinear PDEs. We focus on a class of PDEs known as \\emph{nonlinear elliptic variational PDEs}, whose solutions minimize an \\emph{Euler-Lagrange} energy functional $\\mathcal{E}(u) = \\int_\\Omega L(x, u(x), \\nabla u(x)) - f(x) u(x)dx$. We show that if composing a function with Barron norm $b$ with partial derivatives of $L$ produces a function of Barron norm at most $B_L b^p$, the solution to the PDE can be $\\epsilon$-approximated in the $L^2$ sense by a function with Barron norm $O\\left(\\left(dB_L\\right)^{\\max\\{p \\log(1/ \\epsilon), p^{\\log(1/\\epsil",
    "link": "http://arxiv.org/abs/2210.12101",
    "context": "Title: Neural Network Approximations of PDEs Beyond Linearity: A Representational Perspective. (arXiv:2210.12101v2 [cs.LG] UPDATED)\nAbstract: A burgeoning line of research leverages deep neural networks to approximate the solutions to high dimensional PDEs, opening lines of theoretical inquiry focused on explaining how it is that these models appear to evade the curse of dimensionality. However, most prior theoretical analyses have been limited to linear PDEs. In this work, we take a step towards studying the representational power of neural networks for approximating solutions to nonlinear PDEs. We focus on a class of PDEs known as \\emph{nonlinear elliptic variational PDEs}, whose solutions minimize an \\emph{Euler-Lagrange} energy functional $\\mathcal{E}(u) = \\int_\\Omega L(x, u(x), \\nabla u(x)) - f(x) u(x)dx$. We show that if composing a function with Barron norm $b$ with partial derivatives of $L$ produces a function of Barron norm at most $B_L b^p$, the solution to the PDE can be $\\epsilon$-approximated in the $L^2$ sense by a function with Barron norm $O\\left(\\left(dB_L\\right)^{\\max\\{p \\log(1/ \\epsilon), p^{\\log(1/\\epsil",
    "path": "papers/22/10/2210.12101.json",
    "total_tokens": 1183,
    "translated_title": "超越线性的偏微分方程神经网络逼近：一种表征视角",
    "translated_abstract": "近年来，一系列的研究借助深度神经网络逼近高维偏微分方程的解，从而开启了理论探究的大门，使人们开始探讨这些模型是如何避免维度灾难的。但是，大多数理论分析都局限于线性偏微分方程。本文致力于研究神经网络逼近非线性偏微分方程解的表征能力。我们集中研究了一类称为\\emph{非线性椭圆变分偏微分方程}的偏微分方程，其解最小化一个\\emph{欧拉-拉格朗日}能量泛函$\\mathcal{E}(u) = \\int_\\Omega L(x, u(x), \\nabla u(x)) - f(x) u(x)dx$。我们表明，如果用 Barron 范数为 $b$ 的函数与 $L$ 的偏导数组合可以产生 Barron 范数不超过 $B_Lb^p$ 的函数，则可以通过拥有 Barron 范数为 $O\\left(\\left(dB_L\\right)^{\\max\\{p \\log(1/ \\epsilon), p^{\\log(1/\\epsilon)}}\\right)}$ 的函数 $\\epsilon$-逼近 $L^2$ 意义下的 PDE 解。",
    "tldr": "本文研究表征能力，使用神经网络逼近非线性偏微分方程解。研究发现，函数与偏导数组合产生的函数Barron范数不超过$B_Lb^p$，可以通过具有Barron范数 $O\\left(\\left(dB_L\\right)^{\\max\\{p \\log(1/ \\epsilon), p^{\\log(1/\\epsilon)}}\\right)}$ 的函数$\\epsilon$-逼近PDE解。",
    "en_tdlr": "This paper studies the representational power of using neural network to approximate solutions to nonlinear PDEs, and found that if composing a function with Barron norm $b$ with partial derivatives of $L$ produces a function of Barron norm at most $B_L b^p$, then the solution to the PDE can be approximated in the $L^2$ sense by a function with Barron norm $O\\left(\\left(dB_L\\right)^{\\max\\{p \\log(1/ \\epsilon), p^{\\log(1/\\epsilon)}}\\right)}$。"
}