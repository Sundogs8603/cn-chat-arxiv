{
    "title": "Regularized Data Programming with Automated Bayesian Prior Selection. (arXiv:2210.08677v2 [cs.LG] UPDATED)",
    "abstract": "The cost of manual data labeling can be a significant obstacle in supervised learning. Data programming (DP) offers a weakly supervised solution for training dataset creation, wherein the outputs of user-defined programmatic labeling functions (LFs) are reconciled through unsupervised learning. However, DP can fail to outperform an unweighted majority vote in some scenarios, including low-data contexts. This work introduces a Bayesian extension of classical DP that mitigates failures of unsupervised learning by augmenting the DP objective with regularization terms. Regularized learning is achieved through maximum a posteriori estimation with informative priors. Majority vote is proposed as a proxy signal for automated prior parameter selection. Results suggest that regularized DP improves performance relative to maximum likelihood and majority voting, confers greater interpretability, and bolsters performance in low-data regimes.",
    "link": "http://arxiv.org/abs/2210.08677",
    "context": "Title: Regularized Data Programming with Automated Bayesian Prior Selection. (arXiv:2210.08677v2 [cs.LG] UPDATED)\nAbstract: The cost of manual data labeling can be a significant obstacle in supervised learning. Data programming (DP) offers a weakly supervised solution for training dataset creation, wherein the outputs of user-defined programmatic labeling functions (LFs) are reconciled through unsupervised learning. However, DP can fail to outperform an unweighted majority vote in some scenarios, including low-data contexts. This work introduces a Bayesian extension of classical DP that mitigates failures of unsupervised learning by augmenting the DP objective with regularization terms. Regularized learning is achieved through maximum a posteriori estimation with informative priors. Majority vote is proposed as a proxy signal for automated prior parameter selection. Results suggest that regularized DP improves performance relative to maximum likelihood and majority voting, confers greater interpretability, and bolsters performance in low-data regimes.",
    "path": "papers/22/10/2210.08677.json",
    "total_tokens": 841,
    "translated_title": "自动贝叶斯先验选择的正则化数据编程",
    "translated_abstract": "在监督学习中，手动数据标记的成本可能是一个重要的障碍。数据编程（DP）提供了一种弱监督的解决方案，用于训练数据集的创建，其中用户定义的编程标签函数（LFs）的输出通过无监督学习进行调和。然而，DP在某些情况下可能无法胜过加权多数投票，包括低数据情境。本文介绍了一种贝叶斯扩展的经典DP，通过增加正则化项来减轻无监督学习的失败。通过最大后验估计和信息先验实现了正则化学习。提出利用多数投票作为自动先验参数选择的代理信号。实验结果表明，正则化的DP相对于最大似然估计和多数投票可以改善性能，提供更大的可解释性，并在低数据情境中增强性能。",
    "tldr": "本文介绍了一种自动贝叶斯先验选择的正则化数据编程方法，通过加入正则化项来提升数据编程在弱监督学习中的性能，在低数据情境中表现出更好的性能，并提供更大的可解释性。",
    "en_tdlr": "This paper proposes a regularized data programming method with automated Bayesian prior selection, which improves the performance of weakly supervised learning by incorporating regularization terms and achieves better performance in low-data scenarios, while providing greater interpretability."
}