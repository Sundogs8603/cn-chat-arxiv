{
    "title": "Aligning Offline Metrics and Human Judgments of Value for Code Generation Models. (arXiv:2210.16494v2 [cs.SE] UPDATED)",
    "abstract": "Large language models have demonstrated great potential to assist programmers in generating code. For such human-AI pair programming scenarios, we empirically demonstrate that while generated code is most often evaluated in terms of their functional correctness (i.e., whether generations pass available unit tests), correctness does not fully capture (e.g., may underestimate) the productivity gains these models may provide. Through a user study with N = 49 experienced programmers, we show that while correctness captures high-value generations, programmers still rate code that fails unit tests as valuable if it reduces the overall effort needed to complete a coding task. Finally, we propose a hybrid metric that combines functional correctness and syntactic similarity and show that it achieves a 14% stronger correlation with value and can therefore better represent real-world gains when evaluating and comparing models.",
    "link": "http://arxiv.org/abs/2210.16494",
    "context": "Title: Aligning Offline Metrics and Human Judgments of Value for Code Generation Models. (arXiv:2210.16494v2 [cs.SE] UPDATED)\nAbstract: Large language models have demonstrated great potential to assist programmers in generating code. For such human-AI pair programming scenarios, we empirically demonstrate that while generated code is most often evaluated in terms of their functional correctness (i.e., whether generations pass available unit tests), correctness does not fully capture (e.g., may underestimate) the productivity gains these models may provide. Through a user study with N = 49 experienced programmers, we show that while correctness captures high-value generations, programmers still rate code that fails unit tests as valuable if it reduces the overall effort needed to complete a coding task. Finally, we propose a hybrid metric that combines functional correctness and syntactic similarity and show that it achieves a 14% stronger correlation with value and can therefore better represent real-world gains when evaluating and comparing models.",
    "path": "papers/22/10/2210.16494.json",
    "total_tokens": 903,
    "translated_title": "对于代码生成模型，离线度量和人类价值判断的对齐。",
    "translated_abstract": "大型语言模型已经展示了协助程序员生成代码的极大潜力。对于这种人工智能协作编程的情形，我们实证展示了生成的代码最常被评估的是其功能正确性（即生成是否通过了可用的单元测试），但是正确性并不能完全反映这些模型可能提供的生产率提升（例如可能会低估）。通过一项拥有N=49有经验的程序员的用户研究，我们展示了，尽管正确性对于高价值的生成有很好的体现，程序员依旧认为不能通过单元测试的代码如果能减少完成编码任务所需的总体工作量的话，其价值也是很高的。最后，我们提出了一种混合度量，结合了功能正确性和语法相似性，并展示了它能够以更强的相关性（14%）来衡量价值，因此可以更好地代表在评估和比较模型时的真实收益。",
    "tldr": "该论文通过一个用户研究表明，对于生成的代码而言，正确性评估不能完全反映大型语言模型协助程序员生成代码所提供的生产力提升，因此该论文提出了一种混合度量，结合了功能正确性和语法相似性，并且能够更好地代表真实世界的收益。",
    "en_tdlr": "This paper shows through a user study that functional correctness evaluation cannot fully reflect the productivity gains of large language models in assisting programmers to generate code, and proposes a hybrid metric that combines functional correctness and syntactic similarity to better represent the real-world gains when evaluating and comparing models."
}