{
    "title": "MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers. (arXiv:2210.06425v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained Language Models (LMs) have become an integral part of Natural Language Processing (NLP) in recent years, due to their superior performance in downstream applications. In spite of this resounding success, the usability of LMs is constrained by computational and time complexity, along with their increasing size; an issue that has been referred to as `overparameterisation'. Different strategies have been proposed in the literature to alleviate these problems, with the aim to create effective compact models that nearly match the performance of their bloated counterparts with negligible performance losses. One of the most popular techniques in this area of research is model distillation. Another potent but underutilised technique is cross-layer parameter sharing. In this work, we combine these two strategies and present MiniALBERT, a technique for converting the knowledge of fully parameterised LMs (such as BERT) into a compact recursive student. In addition, we investigate the ",
    "link": "http://arxiv.org/abs/2210.06425",
    "context": "Title: MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers. (arXiv:2210.06425v2 [cs.CL] UPDATED)\nAbstract: Pre-trained Language Models (LMs) have become an integral part of Natural Language Processing (NLP) in recent years, due to their superior performance in downstream applications. In spite of this resounding success, the usability of LMs is constrained by computational and time complexity, along with their increasing size; an issue that has been referred to as `overparameterisation'. Different strategies have been proposed in the literature to alleviate these problems, with the aim to create effective compact models that nearly match the performance of their bloated counterparts with negligible performance losses. One of the most popular techniques in this area of research is model distillation. Another potent but underutilised technique is cross-layer parameter sharing. In this work, we combine these two strategies and present MiniALBERT, a technique for converting the knowledge of fully parameterised LMs (such as BERT) into a compact recursive student. In addition, we investigate the ",
    "path": "papers/22/10/2210.06425.json",
    "total_tokens": 1099,
    "translated_title": "迷你ALBERT: 基于参数高效递归变换的模型蒸馏",
    "translated_abstract": "近年来，预训练语言模型 (LM) 由于在下游应用中的卓越表现，成为自然语言处理 (NLP) 中不可或缺的一部分。尽管这一辉煌的成就，LM 的可用性受限于计算和时间复杂度，以及它们日益增长的模型大小；这是被称为“过参数化”的问题。文献中提出了不同的策略来缓解这些问题，旨在创建有效的紧凑模型，将它们的性能与其膨胀的对应物几乎相匹配，而几乎不损失性能。在这个研究领域中最受欢迎的技术之一是模型蒸馏。而另一种强大但不常使用的技术是跨层参数共享。在这项工作中，我们将这两个策略结合起来，提出了 MiniALBERT，一种将完全参数化的 LM 的知识转换为紧凑递归学生的技术，同时我们研究了跨层参数共享和其他技术的效能，进一步提高了 MiniALBERT 的效率和性能。我们在基准 NLP 任务上的实验证明，MiniALBERT 优于多个最先进的紧凑型 LM，并保持更少的参数数量。",
    "tldr": "MiniALBERT 是一种模型蒸馏技术，结合了跨层参数共享等策略，将完全参数化的语言模型知识转换成为紧凑递归学生模型。MiniALBERT 在基准 NLP 任务上的实验表明，它在性能上优于多个最先进的紧凑型语言模型，并且具有更少的参数数量。",
    "en_tdlr": "MiniALBERT is a model distillation technique that combines strategies such as cross-layer parameter sharing to convert the knowledge of fully parameterized LMs into a compact recursive student model. MiniALBERT outperforms several state-of-the-art compact LMs on benchmark NLP tasks while maintaining fewer parameters."
}