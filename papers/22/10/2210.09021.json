{
    "title": "Histopathological Image Classification based on Self-Supervised Vision Transformer and Weak Labels. (arXiv:2210.09021v2 [cs.CV] UPDATED)",
    "abstract": "Whole Slide Image (WSI) analysis is a powerful method to facilitate the diagnosis of cancer in tissue samples. Automating this diagnosis poses various issues, most notably caused by the immense image resolution and limited annotations. WSIs commonly exhibit resolutions of 100Kx100K pixels. Annotating cancerous areas in WSIs on the pixel level is prohibitively labor-intensive and requires a high level of expert knowledge. Multiple instance learning (MIL) alleviates the need for expensive pixel-level annotations. In MIL, learning is performed on slide-level labels, in which a pathologist provides information about whether a slide includes cancerous tissue. Here, we propose Self-ViT-MIL, a novel approach for classifying and localizing cancerous areas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViT- MIL is pre-trained in a self-supervised setting to learn rich feature representation without relying on any labels. The recent Vision Tra",
    "link": "http://arxiv.org/abs/2210.09021",
    "context": "Title: Histopathological Image Classification based on Self-Supervised Vision Transformer and Weak Labels. (arXiv:2210.09021v2 [cs.CV] UPDATED)\nAbstract: Whole Slide Image (WSI) analysis is a powerful method to facilitate the diagnosis of cancer in tissue samples. Automating this diagnosis poses various issues, most notably caused by the immense image resolution and limited annotations. WSIs commonly exhibit resolutions of 100Kx100K pixels. Annotating cancerous areas in WSIs on the pixel level is prohibitively labor-intensive and requires a high level of expert knowledge. Multiple instance learning (MIL) alleviates the need for expensive pixel-level annotations. In MIL, learning is performed on slide-level labels, in which a pathologist provides information about whether a slide includes cancerous tissue. Here, we propose Self-ViT-MIL, a novel approach for classifying and localizing cancerous areas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViT- MIL is pre-trained in a self-supervised setting to learn rich feature representation without relying on any labels. The recent Vision Tra",
    "path": "papers/22/10/2210.09021.json",
    "total_tokens": 942,
    "translated_title": "基于自监督视觉Transformer和弱标签的组织病理图像分类",
    "translated_abstract": "整个切片图像（WSI）分析是促进组织样本癌症诊断的强大方法。自动化诊断存在多种问题，最主要的问题是由于巨大的图像分辨率和有限的注释而引起的。WSI通常展示100Kx100K像素的分辨率。在像素级别上注释WSI中的癌细胞区域是费力且需要高水平的专业知识的。多实例学习（MIL）减轻了昂贵的像素级别注释的需求。在MIL中，学习是基于幻灯片级别的标签展开的，其中病理学家提供关于幻灯片中是否包含癌细胞组织的信息。我们提出了一种新的方法Self-ViT-MIL，它基于幻灯片级别的注释对癌细胞区域进行分类和定位，消除了需要像素级别注释的训练数据。Self-ViT-MIL在无需依靠标签的自监督设置下进行预训练，以学习丰富的特征表示。",
    "tldr": "本研究提出了一种新方法Self-ViT-MIL，基于幻灯片级别的注释对癌细胞区域进行分类和定位，消除了需要像素级别注释的训练数据的需求。",
    "en_tdlr": "This study proposes a novel approach, Self-ViT-MIL, for classifying and localizing cancerous areas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViT-MIL is pre-trained in a self-supervised setting to learn rich feature representation."
}