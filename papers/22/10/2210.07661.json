{
    "title": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling. (arXiv:2210.07661v3 [cs.LG] UPDATED)",
    "abstract": "Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tas",
    "link": "http://arxiv.org/abs/2210.07661",
    "context": "Title: CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling. (arXiv:2210.07661v3 [cs.LG] UPDATED)\nAbstract: Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tas",
    "path": "papers/22/10/2210.07661.json",
    "total_tokens": 876,
    "translated_title": "CAB: 长序列建模的全面注意力基准测试",
    "translated_abstract": "Transformer在语言、图像和语音处理方面取得了显著的成就。最近，人们提出了各种高效的注意力架构，以提高Transformer在建模长序列时的效率，同时大幅保留其表现力。一个广泛使用的用于测试这些高效方法在长序列建模能力上的基准是长距离竞技场（LRA）。然而，LRA只关注标准的双向（或非因果）自注意力，完全忽略了交叉注意力和单向（或因果）注意力，而这对于下游应用同样重要。在本文中，我们提出了一种全面的注意力基准测试（CAB），采用了细粒度的注意力分类体系，包括非因果自注意力、因果自注意力、非因果交叉注意力和因果交叉注意力四种可区分的注意力模式。CAB收集了来自不同研究领域的七个真实世界任务，以评估在四种注意力模式下的高效注意力。",
    "tldr": "本文提出了一种全面的注意力基准测试（CAB），用于评估在建模长序列时的高效注意力方法。CAB包括了细粒度的注意力分类体系，涵盖了非因果自注意力、因果自注意力、非因果交叉注意力和因果交叉注意力四种注意力模式，并采集了七个真实世界任务进行评估。"
}