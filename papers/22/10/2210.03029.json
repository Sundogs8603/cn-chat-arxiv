{
    "title": "Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt. (arXiv:2210.03029v3 [cs.CL] UPDATED)",
    "abstract": "Enhancing the zero-shot performance of instruction-following models requires heavy computation, either by scaling the total number of training datasets or the model size. In this work, we explore how retrieval of soft prompts obtained through prompt tuning can efficiently assist hard prompts in zero-shot task generalization. Specifically, we train soft prompt embeddings for each prompt through prompt tuning, store the samples of the training instances mapped with the prompt embeddings, and retrieve the corresponding prompt embedding of the training instance closest to the query instance during inference. While only adding 0.007% additional parameters, retrieval of soft prompt enhances the performance of T0 on unseen tasks by outperforming it on 10 out of 11 datasets as well as improving the mean accuracy of T0 on BIG-bench benchmark by 2.39% points. Also, we report an interesting finding that retrieving source embeddings trained on similar answer choice formats is more important than t",
    "link": "http://arxiv.org/abs/2210.03029",
    "context": "Title: Efficiently Enhancing Zero-Shot Performance of Instruction Following Model via Retrieval of Soft Prompt. (arXiv:2210.03029v3 [cs.CL] UPDATED)\nAbstract: Enhancing the zero-shot performance of instruction-following models requires heavy computation, either by scaling the total number of training datasets or the model size. In this work, we explore how retrieval of soft prompts obtained through prompt tuning can efficiently assist hard prompts in zero-shot task generalization. Specifically, we train soft prompt embeddings for each prompt through prompt tuning, store the samples of the training instances mapped with the prompt embeddings, and retrieve the corresponding prompt embedding of the training instance closest to the query instance during inference. While only adding 0.007% additional parameters, retrieval of soft prompt enhances the performance of T0 on unseen tasks by outperforming it on 10 out of 11 datasets as well as improving the mean accuracy of T0 on BIG-bench benchmark by 2.39% points. Also, we report an interesting finding that retrieving source embeddings trained on similar answer choice formats is more important than t",
    "path": "papers/22/10/2210.03029.json",
    "total_tokens": 865,
    "translated_title": "通过检索软提示增强指令跟随模型的零样本表现效率",
    "translated_abstract": "提升指令跟随模型的零样本表现效率需要大量计算，要么通过扩展训练数据集的总数，要么增加模型的大小。在这项工作中，我们通过提示微调获取软提示，探索了如何通过检索软提示有效辅助硬提示来进行零样本任务泛化。具体而言，我们通过提示微调为每个提示训练软提示嵌入，存储与提示嵌入映射的训练实例样本，并在推理过程中检索最接近查询实例的训练实例对应的提示嵌入。虽然只增加了0.007%的额外参数，检索软提示提高了T0在未见任务上的性能，在11个数据集中有10个表现优于T0，并且将T0在BIG-bench基准测试中的平均准确率提高了2.39个百分点。此外，我们还报告了一个有意思的发现，即检索在相似答案选择格式上训练的源嵌入比提示嵌入更重要。",
    "tldr": "通过检索软提示有效辅助硬提示，在增加少量参数的情况下提高了指令跟随模型在零样本任务上的表现效率。",
    "en_tdlr": "Retrieval of soft prompts efficiently assists hard prompts in enhancing the zero-shot performance of instruction-following models with only a minimal increase in parameters."
}