{
    "title": "How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders. (arXiv:2210.08344v2 [cs.LG] UPDATED)",
    "abstract": "Masked Autoencoders (MAE) based on a reconstruction task have risen to be a promising paradigm for self-supervised learning (SSL) and achieve state-of-the-art performance across different benchmark datasets. However, despite its impressive empirical success, there is still limited theoretical understanding of it. In this paper, we propose a theoretical understanding of how masking matters for MAE to learn meaningful features. We establish a close connection between MAE and contrastive learning, which shows that MAE implicit aligns the mask-induced positive pairs. Built upon this connection, we develop the first downstream guarantees for MAE methods, and analyze the effect of mask ratio. Besides, as a result of the implicit alignment, we also point out the dimensional collapse issue of MAE, and propose a Uniformity-enhanced MAE (U-MAE) loss that can effectively address this issue and bring significant improvements on real-world datasets, including CIFAR-10, ImageNet-100, and ImageNet-1K",
    "link": "http://arxiv.org/abs/2210.08344",
    "context": "Title: How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders. (arXiv:2210.08344v2 [cs.LG] UPDATED)\nAbstract: Masked Autoencoders (MAE) based on a reconstruction task have risen to be a promising paradigm for self-supervised learning (SSL) and achieve state-of-the-art performance across different benchmark datasets. However, despite its impressive empirical success, there is still limited theoretical understanding of it. In this paper, we propose a theoretical understanding of how masking matters for MAE to learn meaningful features. We establish a close connection between MAE and contrastive learning, which shows that MAE implicit aligns the mask-induced positive pairs. Built upon this connection, we develop the first downstream guarantees for MAE methods, and analyze the effect of mask ratio. Besides, as a result of the implicit alignment, we also point out the dimensional collapse issue of MAE, and propose a Uniformity-enhanced MAE (U-MAE) loss that can effectively address this issue and bring significant improvements on real-world datasets, including CIFAR-10, ImageNet-100, and ImageNet-1K",
    "path": "papers/22/10/2210.08344.json",
    "total_tokens": 992,
    "translated_title": "探究掩码自编码器：Masked Autoencoders 的理论解释",
    "translated_abstract": "基于重构任务的Masked Autoencoders (MAE)已成为自监督学习 (SSL) 的一种有前途的范式，并在不同的基准数据集上实现了最先进的性能。然而，尽管其令人印象深刻的实证成功，但对其仍存在有限的理论理解。本文提出了MAE如何通过掩码方式学习有意义的特征的理论理解。我们建立了MAE与对比学习之间的紧密联系，表明MAE隐式地对齐了由掩码引导的正对映样本。基于这一联系，我们开发了MAE方法的第一个下游保证，并分析了掩码比例的影响。此外，由于隐式对齐的结果，我们还指出了MAE的维度坍塌问题，并提出了一种增强均匀性的MAE (U-MAE) 损失函数，可以有效地解决这个问题，并在包括CIFAR-10、ImageNet-100和ImageNet-1K在内的真实世界数据集上带来显着的改进。",
    "tldr": "本论文探究了Masked Autoencoders (MAE)学习有意义特征的理论方法，建立了MAE与对比学习之间的紧密联系，提出了一种增强均匀性的MAE (U-MAE) 损失函数。这些理论和方法在真实世界数据集上带来了显着的改进。"
}