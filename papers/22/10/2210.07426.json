{
    "title": "Skill-Based Reinforcement Learning with Intrinsic Reward Matching. (arXiv:2210.07426v4 [cs.LG] UPDATED)",
    "abstract": "While unsupervised skill discovery has shown promise in autonomously acquiring behavioral primitives, there is still a large methodological disconnect between task-agnostic skill pretraining and downstream, task-aware finetuning. We present Intrinsic Reward Matching (IRM), which unifies these two phases of learning via the $\\textit{skill discriminator}$, a pretraining model component often discarded during finetuning. Conventional approaches finetune pretrained agents directly at the policy level, often relying on expensive environment rollouts to empirically determine the optimal skill. However, often the most concise yet complete description of a task is the reward function itself, and skill learning methods learn an $\\textit{intrinsic}$ reward function via the discriminator that corresponds to the skill policy. We propose to leverage the skill discriminator to $\\textit{match}$ the intrinsic and downstream task rewards and determine the optimal skill for an unseen task without enviro",
    "link": "http://arxiv.org/abs/2210.07426",
    "context": "Title: Skill-Based Reinforcement Learning with Intrinsic Reward Matching. (arXiv:2210.07426v4 [cs.LG] UPDATED)\nAbstract: While unsupervised skill discovery has shown promise in autonomously acquiring behavioral primitives, there is still a large methodological disconnect between task-agnostic skill pretraining and downstream, task-aware finetuning. We present Intrinsic Reward Matching (IRM), which unifies these two phases of learning via the $\\textit{skill discriminator}$, a pretraining model component often discarded during finetuning. Conventional approaches finetune pretrained agents directly at the policy level, often relying on expensive environment rollouts to empirically determine the optimal skill. However, often the most concise yet complete description of a task is the reward function itself, and skill learning methods learn an $\\textit{intrinsic}$ reward function via the discriminator that corresponds to the skill policy. We propose to leverage the skill discriminator to $\\textit{match}$ the intrinsic and downstream task rewards and determine the optimal skill for an unseen task without enviro",
    "path": "papers/22/10/2210.07426.json",
    "total_tokens": 839,
    "translated_title": "基于技能的强化学习与内在奖励匹配",
    "translated_abstract": "虽然无监督技能探索已经展示了自主获取行为原语的潜力，但是任务无关的技能预训练和下游的任务感知调优之间仍存在很大的方法论差距。我们提出内在奖励匹配(IRF)，通过预训练模型组件 \"技能鉴别器\" 统一这两个学习阶段。传统方法在策略级别直接微调预训练代理，通常依赖于昂贵的环境回放来经验性地确定最优技能。然而，任务最简明但完整的描述通常是奖励函数本身，技能学习方法通过鉴别器学习与技能策略相对应的“内在”奖励函数。我们建议利用技能鉴别器“匹配”内在和下游任务奖励，并确定未见任务的最优技能，从而提高效率。",
    "tldr": "该论文提出了内在奖励匹配(IRF)方法，通过技能鉴别器匹配内在和下游任务奖励来确定未见任务的最优技能，提高了系统效率。",
    "en_tdlr": "This paper proposes the Intrinsic Reward Matching (IRM) method, which uses the skill discriminator to match intrinsic and downstream task rewards and determine the optimal skill for an unseen task to improve system efficiency."
}