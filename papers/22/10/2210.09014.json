{
    "title": "Addressing contingency in algorithmic (mis)information classification: Toward a responsible machine learning agenda. (arXiv:2210.09014v2 [cs.CY] UPDATED)",
    "abstract": "Machine learning (ML) enabled classification models are becoming increasingly popular for tackling the sheer volume and speed of online misinformation and other content that could be identified as harmful. In building these models, data scientists need to take a stance on the legitimacy, authoritativeness and objectivity of the sources of ``truth\" used for model training and testing. This has political, ethical and epistemic implications which are rarely addressed in technical papers. Despite (and due to) their reported high accuracy and performance, ML-driven moderation systems have the potential to shape online public debate and create downstream negative impacts such as undue censorship and the reinforcing of false beliefs. Using collaborative ethnography and theoretical insights from social studies of science and expertise, we offer a critical analysis of the process of building ML models for (mis)information classification: we identify a series of algorithmic contingencies--key mo",
    "link": "http://arxiv.org/abs/2210.09014",
    "context": "Title: Addressing contingency in algorithmic (mis)information classification: Toward a responsible machine learning agenda. (arXiv:2210.09014v2 [cs.CY] UPDATED)\nAbstract: Machine learning (ML) enabled classification models are becoming increasingly popular for tackling the sheer volume and speed of online misinformation and other content that could be identified as harmful. In building these models, data scientists need to take a stance on the legitimacy, authoritativeness and objectivity of the sources of ``truth\" used for model training and testing. This has political, ethical and epistemic implications which are rarely addressed in technical papers. Despite (and due to) their reported high accuracy and performance, ML-driven moderation systems have the potential to shape online public debate and create downstream negative impacts such as undue censorship and the reinforcing of false beliefs. Using collaborative ethnography and theoretical insights from social studies of science and expertise, we offer a critical analysis of the process of building ML models for (mis)information classification: we identify a series of algorithmic contingencies--key mo",
    "path": "papers/22/10/2210.09014.json",
    "total_tokens": 1020,
    "translated_title": "处理算法（误）信息分类中的偶然性：走向负责任的机器学习议程",
    "translated_abstract": "机器学习（ML）启用的分类模型越来越受欢迎，用于解决庞大且速度快的在线虚假信息和其他可能被识别为有害的内容。在构建这些模型时，数据科学家需要对用于模型训练和测试的“真相”来源的合法性、权威性和客观性采取立场。这涉及政治、伦理和认识论方面的问题，其在技术论文中很少得到解决。尽管（也就是由于）其报告的高准确性和性能，由ML驱动的审查系统可能会塑造在线公共辩论，并产生负面影响，如不当审查和强化错误信念。我们采用合作的民族志学和社会科学和专业知识的理论洞见，对建立（误）信息分类的ML模型的过程进行了批判性分析，以识别一系列算法的偶然性——关键的模型决策点，这些决策点可能会在技术实现中引入自己的诠释和评估。",
    "tldr": "本文重点讨论了机器学习（ML）启用的分类模型处理在线虚假信息和其他可能被识别为有害的内容时，对“真相”来源的合法性、权威性和客观性所采取的立场以及ML驱动的审查系统可能在不利影响方面产生的问题，分析了算法的偶然性和可能引起的评估误差。",
    "en_tdlr": "This paper discusses the stance of data scientists on the legitimacy, authoritativeness, and objectivity of the sources of \"truth\" used in machine learning (ML) enabled classification models for identifying online misinformation and harmful content, and the potential negative impacts of ML-driven moderation systems on online public debate. The paper offers a critical analysis of the process of building ML models for (mis)information classification, identifying algorithmic contingencies and possible evaluation errors introduced in technical implementations."
}