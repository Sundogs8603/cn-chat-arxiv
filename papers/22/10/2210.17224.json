{
    "title": "Improving Graph Neural Networks with Learnable Propagation Operators. (arXiv:2210.17224v2 [cs.LG] UPDATED)",
    "abstract": "Graph Neural Networks (GNNs) are limited in their propagation operators. In many cases, these operators often contain non-negative elements only and are shared across channels, limiting the expressiveness of GNNs. Moreover, some GNNs suffer from over-smoothing, limiting their depth. On the other hand, Convolutional Neural Networks (CNNs) can learn diverse propagation filters, and phenomena like over-smoothing are typically not apparent in CNNs. In this paper, we bridge these gaps by incorporating trainable channel-wise weighting factors $\\omega$ to learn and mix multiple smoothing and sharpening propagation operators at each layer. Our generic method is called $\\omega$GNN, and is easy to implement. We study two variants: $\\omega$GCN and $\\omega$GAT. For $\\omega$GCN, we theoretically analyse its behaviour and the impact of $\\omega$ on the obtained node features. Our experiments confirm these findings, demonstrating and explaining how both variants do not over-smooth. Additionally, we ex",
    "link": "http://arxiv.org/abs/2210.17224",
    "context": "Title: Improving Graph Neural Networks with Learnable Propagation Operators. (arXiv:2210.17224v2 [cs.LG] UPDATED)\nAbstract: Graph Neural Networks (GNNs) are limited in their propagation operators. In many cases, these operators often contain non-negative elements only and are shared across channels, limiting the expressiveness of GNNs. Moreover, some GNNs suffer from over-smoothing, limiting their depth. On the other hand, Convolutional Neural Networks (CNNs) can learn diverse propagation filters, and phenomena like over-smoothing are typically not apparent in CNNs. In this paper, we bridge these gaps by incorporating trainable channel-wise weighting factors $\\omega$ to learn and mix multiple smoothing and sharpening propagation operators at each layer. Our generic method is called $\\omega$GNN, and is easy to implement. We study two variants: $\\omega$GCN and $\\omega$GAT. For $\\omega$GCN, we theoretically analyse its behaviour and the impact of $\\omega$ on the obtained node features. Our experiments confirm these findings, demonstrating and explaining how both variants do not over-smooth. Additionally, we ex",
    "path": "papers/22/10/2210.17224.json",
    "total_tokens": 926,
    "translated_title": "通过学习传播算子，改进图神经网络",
    "translated_abstract": "图神经网络（GNNs）的传播算子受到限制。在许多情况下，这些算子通常只含有非负元素，并且在通道之间共享，限制了GNNs的表现力。此外，一些GNNs受到全局平滑的限制，而无法很好地表达复杂的网络结构。另一方面，卷积神经网络（CNNs）可以学习多样的传播滤波器，并且通常不会表现出全局平滑的现象。本文通过将可学习的通道加权因子ω纳入每层中，学习和混合多个平滑和锐化滤波器，来填补这些差距。我们制定了一个通用方法，称为ωGNN，并且易于实现。我们研究了两种变体：ωGCN和ωGAT。对于ωGCN，我们从理论上分析了其行为和ω对所得到的节点特征的影响。我们的实验结果证实了这些发现，并解释了这两种变体如何避免全局平滑现象。",
    "tldr": "本文通过引入可学习的通道加权因子ω，学习和混合多个平滑和锐化滤波器，改进了图神经网络（GNNs），避免了全局平滑现象。",
    "en_tdlr": "This paper introduces a generic method called ωGNN to improve Graph Neural Networks (GNNs) by incorporating trainable channel-wise weighting factors ω to learn and mix multiple smoothing and sharpening filters, which avoids the phenomenon of over-smoothing."
}