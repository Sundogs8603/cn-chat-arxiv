{
    "title": "SGD with Large Step Sizes Learns Sparse Features. (arXiv:2210.05337v2 [cs.LG] UPDATED)",
    "abstract": "We showcase important features of the dynamics of the Stochastic Gradient Descent (SGD) in the training of neural networks. We present empirical observations that commonly used large step sizes (i) lead the iterates to jump from one side of a valley to the other causing loss stabilization, and (ii) this stabilization induces a hidden stochastic dynamics orthogonal to the bouncing directions that biases it implicitly toward sparse predictors. Furthermore, we show empirically that the longer large step sizes keep SGD high in the loss landscape valleys, the better the implicit regularization can operate and find sparse representations. Notably, no explicit regularization is used so that the regularization effect comes solely from the SGD training dynamics influenced by the step size schedule. Therefore, these observations unveil how, through the step size schedules, both gradient and noise drive together the SGD dynamics through the loss landscape of neural networks. We justify these find",
    "link": "http://arxiv.org/abs/2210.05337",
    "context": "Title: SGD with Large Step Sizes Learns Sparse Features. (arXiv:2210.05337v2 [cs.LG] UPDATED)\nAbstract: We showcase important features of the dynamics of the Stochastic Gradient Descent (SGD) in the training of neural networks. We present empirical observations that commonly used large step sizes (i) lead the iterates to jump from one side of a valley to the other causing loss stabilization, and (ii) this stabilization induces a hidden stochastic dynamics orthogonal to the bouncing directions that biases it implicitly toward sparse predictors. Furthermore, we show empirically that the longer large step sizes keep SGD high in the loss landscape valleys, the better the implicit regularization can operate and find sparse representations. Notably, no explicit regularization is used so that the regularization effect comes solely from the SGD training dynamics influenced by the step size schedule. Therefore, these observations unveil how, through the step size schedules, both gradient and noise drive together the SGD dynamics through the loss landscape of neural networks. We justify these find",
    "path": "papers/22/10/2210.05337.json",
    "total_tokens": 1012,
    "translated_title": "SGD使用大步长训练能够学习稀疏特征",
    "translated_abstract": "本文展示了随机梯度下降（SGD）在神经网络训练中动力学的重要特征。我们发现：常用的大步长会导致迭代从山谷的一侧跳到另一侧导致损失稳定，同时这种稳定性会引起一个隐含的、垂直于跳跃方向的随机动态，将其偏向于稀疏预测器。此外，我们实验证明，长时间使用大步长可保持SGD在损失平面中的高度，进而能更好地实现隐式正则化和发现稀疏表示。值得注意的是，这里没有使用任何显式正则化，因此正则化效果完全来自于受步长调度影响的SGD训练动态。因此，我们的发现揭示了通过步长调度，梯度和噪声如何共同驱动SGD动态穿过神经网络的损失平面。我们通过展示幂律步长调度匹配奥恩斯坦-乌伦贝克过程的理论预测并导致最稳健和最稀疏的表示来证明这些发现。",
    "tldr": "本文展示了SGD使用大步长训练能够学习稀疏特征，在训练过程中，通过步长调度，梯度和噪声相互作用，共同驱动SGD动态穿过神经网络的损失平面，从而发现稀疏表示。"
}