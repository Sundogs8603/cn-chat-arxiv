{
    "title": "Towards Global Neural Network Abstractions with Locally-Exact Reconstruction. (arXiv:2210.12054v2 [cs.LG] UPDATED)",
    "abstract": "Neural networks are a powerful class of non-linear functions. However, their black-box nature makes it difficult to explain their behaviour and certify their safety. Abstraction techniques address this challenge by transforming the neural network into a simpler, over-approximated function. Unfortunately, existing abstraction techniques are slack, which limits their applicability to small local regions of the input domain. In this paper, we propose Global Interval Neural Network Abstractions with Center-Exact Reconstruction (GINNACER). Our novel abstraction technique produces sound over-approximation bounds over the whole input domain while guaranteeing exact reconstructions for any given local input. Our experiments show that GINNACER is several orders of magnitude tighter than state-of-the-art global abstraction techniques, while being competitive with local ones.",
    "link": "http://arxiv.org/abs/2210.12054",
    "context": "Title: Towards Global Neural Network Abstractions with Locally-Exact Reconstruction. (arXiv:2210.12054v2 [cs.LG] UPDATED)\nAbstract: Neural networks are a powerful class of non-linear functions. However, their black-box nature makes it difficult to explain their behaviour and certify their safety. Abstraction techniques address this challenge by transforming the neural network into a simpler, over-approximated function. Unfortunately, existing abstraction techniques are slack, which limits their applicability to small local regions of the input domain. In this paper, we propose Global Interval Neural Network Abstractions with Center-Exact Reconstruction (GINNACER). Our novel abstraction technique produces sound over-approximation bounds over the whole input domain while guaranteeing exact reconstructions for any given local input. Our experiments show that GINNACER is several orders of magnitude tighter than state-of-the-art global abstraction techniques, while being competitive with local ones.",
    "path": "papers/22/10/2210.12054.json",
    "total_tokens": 809,
    "translated_title": "到达全局神经网络抽象与本地精确重构",
    "translated_abstract": "神经网络是一类强大的非线性函数。但是，它们的黑匣子性质使得解释它们的行为和验证它们的安全性变得困难。抽象技术通过将神经网络转换为更简单的、过度逼近的函数来解决这个挑战。然而，现有的抽象技术存在不足，限制了它们在输入域的小局部的适用性。本文提出了基于中心精确重构的全域区间神经网络抽象（GINNACER）。我们的新型抽象技术在整个输入域产生准确的过度逼近边界，同时对于任何给定的局部输入保证精确重构。我们的实验表明，GINNACER比最先进的全局抽象技术紧凑了几个数量级，同时与局部技术相竞争。",
    "tldr": "GINNACER提出了一种新的全局区间神经网络抽象技术，可以在整个输入域产生准确的上估计边界，同时对于任何给定的局部输入保证精确重构。",
    "en_tdlr": "GINNACER proposes a novel global interval neural network abstraction technique that produces accurate over-approximation bounds over the whole input domain while guaranteeing exact reconstructions for any given local input."
}