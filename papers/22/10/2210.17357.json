{
    "title": "L-GreCo: Layerwise-Adaptive Gradient Compression for Efficient and Accurate Deep Learning. (arXiv:2210.17357v2 [cs.LG] UPDATED)",
    "abstract": "Data-parallel distributed training of deep neural networks (DNN) has gained very widespread adoption, but can still experience communication bottlenecks. To address this issue, entire families of compression mechanisms have been developed, including quantization, sparsification, and low-rank approximation, some of which are seeing significant practical adoption. Despite this progress, almost all known compression schemes apply compression uniformly across DNN layers, although layers are heterogeneous in terms of parameter count and their impact on model accuracy. In this work, we provide a general framework for adapting the degree of compression across the model's layers dynamically during training, improving the overall compression, while leading to substantial speedups, without sacrificing accuracy. Our framework, called L-GreCo, is based on an adaptive algorithm, which automatically picks the optimal compression parameters for model layers guaranteeing the best compression ratio whi",
    "link": "http://arxiv.org/abs/2210.17357",
    "context": "Title: L-GreCo: Layerwise-Adaptive Gradient Compression for Efficient and Accurate Deep Learning. (arXiv:2210.17357v2 [cs.LG] UPDATED)\nAbstract: Data-parallel distributed training of deep neural networks (DNN) has gained very widespread adoption, but can still experience communication bottlenecks. To address this issue, entire families of compression mechanisms have been developed, including quantization, sparsification, and low-rank approximation, some of which are seeing significant practical adoption. Despite this progress, almost all known compression schemes apply compression uniformly across DNN layers, although layers are heterogeneous in terms of parameter count and their impact on model accuracy. In this work, we provide a general framework for adapting the degree of compression across the model's layers dynamically during training, improving the overall compression, while leading to substantial speedups, without sacrificing accuracy. Our framework, called L-GreCo, is based on an adaptive algorithm, which automatically picks the optimal compression parameters for model layers guaranteeing the best compression ratio whi",
    "path": "papers/22/10/2210.17357.json",
    "total_tokens": 905,
    "translated_title": "L-GreCo：层间自适应梯度压缩可用于高效和精确深度学习",
    "translated_abstract": "深度神经网络（DNN）的数据并行分布式训练得到广泛采用，但仍可能存在通信瓶颈。为了解决这个问题，开发了包括量化、稀疏化和低秩逼近在内的整个压缩机制系列，其中一些正在得到显着的实际采用。尽管取得了进展，但几乎所有已知压缩方案均在DNN层上均匀应用压缩，尽管在参数计数和对模型准确性的影响方面，层是异构的。在这项工作中，我们提供了一个通用框架，在训练过程中动态调整模型层之间的压缩程度，提高了总体压缩率，同时导致了大幅加速，而不会损失精度。我们的框架叫做L-GreCo，基于自适应算法，可以自动选择为模型层选择最佳压缩参数，以保证最佳压缩比。",
    "tldr": "L-GreCo是一个基于自适应算法的通用的框架，可在训练过程中动态调整模型层之间的压缩程度，提高了总体压缩率，大幅加速，不损失精度。",
    "en_tdlr": "L-GreCo is a general framework based on an adaptive algorithm that dynamically adjusts the degree of compression across model layers during training, improving overall compression, leading to substantial speedups without sacrificing accuracy."
}