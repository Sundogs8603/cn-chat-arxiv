{
    "title": "Large Language Models can Implement Policy Iteration. (arXiv:2210.03821v2 [cs.LG] UPDATED)",
    "abstract": "This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the \"few-shot\" quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt conten",
    "link": "http://arxiv.org/abs/2210.03821",
    "context": "Title: Large Language Models can Implement Policy Iteration. (arXiv:2210.03821v2 [cs.LG] UPDATED)\nAbstract: This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the \"few-shot\" quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt conten",
    "path": "papers/22/10/2210.03821.json",
    "total_tokens": 934,
    "translated_title": "大型语言模型可以实现策略迭代",
    "translated_abstract": "本文介绍了一种称为“上下文策略迭代”的算法，它利用基础模型在上下文中执行强化学习任务。虽然将基础模型应用于强化学习已经受到了重视，但大多数方法要么依赖于专家示范的策划（通过手动设计或任务特定的预训练），要么通过梯度方法（微调或适配层训练）来适应感兴趣的任务。这些技术都有一些缺点。收集示范是费时费力的，依赖示范的算法并不能超越示范中的专家。而所有的梯度技术天生都很慢，丧失了一开始使上下文学习具有吸引力的“少样本”质量。本文提出了一种名为ICPI的算法，它能够在没有专家示范或梯度的情况下学习执行强化学习任务。我们提出了一种策略迭代方法，其中包括了提示文本。",
    "tldr": "本论文提出一种名为ICPI的算法，利用基础模型在上下文中执行强化学习任务，无需专家示范或梯度。算法采用策略迭代方法，不仅避免了示范收集的繁琐工作，还解决了梯度方法的运行速度问题。",
    "en_tdlr": "This paper presents an algorithm called ICPI, which uses foundation models to perform Reinforcement Learning (RL) tasks in-context without expert demonstrations or gradients. The algorithm utilizes a policy iteration method and overcomes the limitations of collecting demonstrations and the slow speed of gradient-based methods."
}