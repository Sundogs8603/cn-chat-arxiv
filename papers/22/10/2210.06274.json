{
    "title": "Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning. (arXiv:2210.06274v2 [cs.LG] UPDATED)",
    "abstract": "We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully complete cooperative tasks with arbitrary communication levels at execution time by taking advantage of information-sharing among the agents. Under hybrid execution, the communication level can range from a setting in which no communication is allowed between agents (fully decentralized), to a setting featuring full communication (fully centralized), but the agents do not know beforehand which communication level they will encounter at execution time. To formalize our setting, we define a new class of multi-agent partially observable Markov decision processes (POMDPs) that we name hybrid-POMDPs, which explicitly model a communication process between the agents. We contribute MARO, an approach that makes use of an auto-regressive predictive model, trained in a centralized manner, to estimate missing agents' observations at execution time. We evaluate MARO on sta",
    "link": "http://arxiv.org/abs/2210.06274",
    "context": "Title: Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning. (arXiv:2210.06274v2 [cs.LG] UPDATED)\nAbstract: We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully complete cooperative tasks with arbitrary communication levels at execution time by taking advantage of information-sharing among the agents. Under hybrid execution, the communication level can range from a setting in which no communication is allowed between agents (fully decentralized), to a setting featuring full communication (fully centralized), but the agents do not know beforehand which communication level they will encounter at execution time. To formalize our setting, we define a new class of multi-agent partially observable Markov decision processes (POMDPs) that we name hybrid-POMDPs, which explicitly model a communication process between the agents. We contribute MARO, an approach that makes use of an auto-regressive predictive model, trained in a centralized manner, to estimate missing agents' observations at execution time. We evaluate MARO on sta",
    "path": "papers/22/10/2210.06274.json",
    "total_tokens": 842,
    "translated_title": "多智能体强化学习中的混合执行集中式训练",
    "translated_abstract": "我们引入了混合执行集中式训练在多智能体强化学习中的新范式。在这种范式下，智能体们通过信息共享来完成合作任务，并在执行时间中实现通信级别的任意变化。为了形式化我们的设置，我们定义了一类名为“混合-POMDPs”的多智能体部分可观察马尔可夫决策过程，显式地建模了智能体之间的通信过程。我们提出了MARO方法，它利用自回归预测模型在集中式训练下预测执行时间中缺失的智能体观测值。我们评估了MARO在不同任务中的性能，并发现在各种协作任务中，MARO的表现都优于现有的MARL方法。",
    "tldr": "本文介绍了多智能体强化学习中的新范式，提供了MARO方法，通过集中式训练和自回归预测模型，在执行时间中预测缺失的智能体观测值，优于现有方法。",
    "en_tdlr": "This paper introduces a new paradigm in multi-agent reinforcement learning called hybrid execution, and proposes a centralized training method called MARO, which uses an auto-regressive predictive model to estimate missing agent observations at execution time. Results show that MARO outperforms existing MARL methods in various cooperative tasks."
}