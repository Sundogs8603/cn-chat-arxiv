{
    "title": "SignReLU neural network and its approximation ability. (arXiv:2210.10264v2 [cs.LG] UPDATED)",
    "abstract": "Deep neural networks (DNNs) have garnered significant attention in various fields of science and technology in recent years. Activation functions define how neurons in DNNs process incoming signals for them. They are essential for learning non-linear transformations and for performing diverse computations among successive neuron layers. In the last few years, researchers have investigated the approximation ability of DNNs to explain their power and success. In this paper, we explore the approximation ability of DNNs using a different activation function, called SignReLU. Our theoretical results demonstrate that SignReLU networks outperform rational and ReLU networks in terms of approximation performance. Numerical experiments are conducted comparing SignReLU with the existing activations such as ReLU, Leaky ReLU, and ELU, which illustrate the competitive practical performance of SignReLU.",
    "link": "http://arxiv.org/abs/2210.10264",
    "context": "Title: SignReLU neural network and its approximation ability. (arXiv:2210.10264v2 [cs.LG] UPDATED)\nAbstract: Deep neural networks (DNNs) have garnered significant attention in various fields of science and technology in recent years. Activation functions define how neurons in DNNs process incoming signals for them. They are essential for learning non-linear transformations and for performing diverse computations among successive neuron layers. In the last few years, researchers have investigated the approximation ability of DNNs to explain their power and success. In this paper, we explore the approximation ability of DNNs using a different activation function, called SignReLU. Our theoretical results demonstrate that SignReLU networks outperform rational and ReLU networks in terms of approximation performance. Numerical experiments are conducted comparing SignReLU with the existing activations such as ReLU, Leaky ReLU, and ELU, which illustrate the competitive practical performance of SignReLU.",
    "path": "papers/22/10/2210.10264.json",
    "total_tokens": 779,
    "translated_title": "SignReLU神经网络及其逼近能力",
    "translated_abstract": "近年来，深度神经网络（DNNs）在科学和技术的各个领域引起了重大关注。激活函数定义了DNN中神经元如何处理输入信号。它们对于学习非线性变换和在连续神经元层之间执行多样化的计算至关重要。近年来，研究者们通过研究DNN的逼近能力来解释其强大和成功。本文通过使用一种名为SignReLU的不同激活函数，探索了DNN的逼近能力。我们的理论结果表明，SignReLU网络在逼近性能方面优于有理数和ReLU网络。数值实验比较了SignReLU与现有的激活函数（如ReLU、Leaky ReLU和ELU），结果显示了SignReLU的竞争实际性能。",
    "tldr": "本文研究了一种名为SignReLU的不同激活函数对深度神经网络逼近能力的影响，结果表明SignReLU网络在逼近性能方面优于有理数和ReLU网络。"
}