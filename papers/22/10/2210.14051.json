{
    "title": "Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds. (arXiv:2210.14051v3 [cs.LG] UPDATED)",
    "abstract": "We study the regret guarantee for risk-sensitive reinforcement learning (RSRL) via distributional reinforcement learning (DRL) methods. In particular, we consider finite episodic Markov decision processes whose objective is the entropic risk measure (EntRM) of return. By leveraging a key property of the EntRM, the independence property, we establish the risk-sensitive distributional dynamic programming framework. We then propose two novel DRL algorithms that implement optimism through two different schemes, including a model-free one and a model-based one.  We prove that they both attain $\\tilde{\\mathcal{O}}(\\frac{\\exp(|\\beta| H)-1}{|\\beta|}H\\sqrt{S^2AK})$ regret upper bound, where $S$, $A$, $K$, and $H$ represent the number of states, actions, episodes, and the time horizon, respectively. It matches RSVI2 proposed in \\cite{fei2021exponential}, with novel distributional analysis. To the best of our knowledge, this is the first regret analysis that bridges DRL and RSRL in terms of sampl",
    "link": "http://arxiv.org/abs/2210.14051",
    "context": "Title: Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds. (arXiv:2210.14051v3 [cs.LG] UPDATED)\nAbstract: We study the regret guarantee for risk-sensitive reinforcement learning (RSRL) via distributional reinforcement learning (DRL) methods. In particular, we consider finite episodic Markov decision processes whose objective is the entropic risk measure (EntRM) of return. By leveraging a key property of the EntRM, the independence property, we establish the risk-sensitive distributional dynamic programming framework. We then propose two novel DRL algorithms that implement optimism through two different schemes, including a model-free one and a model-based one.  We prove that they both attain $\\tilde{\\mathcal{O}}(\\frac{\\exp(|\\beta| H)-1}{|\\beta|}H\\sqrt{S^2AK})$ regret upper bound, where $S$, $A$, $K$, and $H$ represent the number of states, actions, episodes, and the time horizon, respectively. It matches RSVI2 proposed in \\cite{fei2021exponential}, with novel distributional analysis. To the best of our knowledge, this is the first regret analysis that bridges DRL and RSRL in terms of sampl",
    "path": "papers/22/10/2210.14051.json",
    "total_tokens": 1035,
    "translated_title": "证明了分布式风险敏感强化学习与风险敏感强化学习之间的可证明遗憾上界",
    "translated_abstract": "本论文研究了使用分布式强化学习方法对风险敏感强化学习（RSRL）的遗憾保证问题。具体而言，我们考虑了目标为回报的熵风险测度（EntRM）的有限情节马尔可夫决策过程。通过利用EntRM的一个关键属性，独立性属性，我们建立了风险敏感分布式动态规划框架。然后，我们提出了两种新颖的分布式强化学习算法，通过两种不同的方案实现了乐观性，包括基于模型和无模型的方法。我们证明了这两种算法都达到了$\\tilde{\\mathcal{O}}(\\frac{\\exp(|\\beta| H)-1}{|\\beta|}H\\sqrt{S^2AK})$的遗憾上界，其中$S$，$A$，$K$和$H$分别表示状态的数量，动作的数量，情节的数量和时间的长度。这与\\cite{fei2021exponential}中提出的RSVI2相一致，并进行了新颖的分布式分析。据我们所知，这是第一个以样本复杂度方向将分布式强化学习和风险敏感强化学习联系起来的遗憾分析。",
    "tldr": "本论文证明了使用分布式强化学习方法可以实现风险敏感强化学习的遗憾保证问题，并提出了两种新颖算法，其遗憾上界与先前方法相匹配。"
}