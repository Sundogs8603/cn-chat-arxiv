{
    "title": "Layer-Neighbor Sampling -- Defusing Neighborhood Explosion in GNNs. (arXiv:2210.13339v2 [cs.LG] UPDATED)",
    "abstract": "Graph Neural Networks (GNNs) have received significant attention recently, but training them at a large scale remains a challenge. Mini-batch training coupled with sampling is used to alleviate this challenge. However, existing approaches either suffer from the neighborhood explosion phenomenon or have poor performance. To address these issues, we propose a new sampling algorithm called LAyer-neighBOR sampling (LABOR). It is designed to be a direct replacement for Neighbor Sampling (NS) with the same fanout hyperparameter while sampling up to 7 times fewer vertices, without sacrificing quality. By design, the variance of the estimator of each vertex matches NS from the point of view of a single vertex. Moreover, under the same vertex sampling budget constraints, LABOR converges faster than existing layer sampling approaches and can use up to 112 times larger batch sizes compared to NS.",
    "link": "http://arxiv.org/abs/2210.13339",
    "context": "Title: Layer-Neighbor Sampling -- Defusing Neighborhood Explosion in GNNs. (arXiv:2210.13339v2 [cs.LG] UPDATED)\nAbstract: Graph Neural Networks (GNNs) have received significant attention recently, but training them at a large scale remains a challenge. Mini-batch training coupled with sampling is used to alleviate this challenge. However, existing approaches either suffer from the neighborhood explosion phenomenon or have poor performance. To address these issues, we propose a new sampling algorithm called LAyer-neighBOR sampling (LABOR). It is designed to be a direct replacement for Neighbor Sampling (NS) with the same fanout hyperparameter while sampling up to 7 times fewer vertices, without sacrificing quality. By design, the variance of the estimator of each vertex matches NS from the point of view of a single vertex. Moreover, under the same vertex sampling budget constraints, LABOR converges faster than existing layer sampling approaches and can use up to 112 times larger batch sizes compared to NS.",
    "path": "papers/22/10/2210.13339.json",
    "total_tokens": 896,
    "translated_title": "Layer-Neighbor Sampling -- GNN中缓解邻居爆炸问题的采样算法",
    "translated_abstract": "近年来，图神经网络（GNN）受到了广泛关注，但在大规模训练方面仍存在挑战。采用小批量训练和采样可用于缓解此问题。然而，现有方法要么受到邻域爆炸现象的影响，要么性能较差。为了解决这些问题，我们提出了一种新的采样算法LAyer-neighBOR sampling（LABOR）。它被设计成Neighbor Sampling（NS）的直接替代品，具有相同的扩展因子超参数，同时采样的顶点数最多少7倍，不会牺牲质量。通过设计，每个顶点估计器的方差与单个顶点上的NS相匹配。此外，在相同的顶点采样预算约束下，LABOR比现有的层采样方法收敛更快，并且可以使用比NS大112倍的批处理大小。",
    "tldr": "本文提出了一种新的采样算法，名为LABOR，旨在替代现有的Neighbor Sampling算法。相比之下，它能够采样更少的顶点但不会牺牲质量，并且在相同的顶点采样预算约束下，收敛更快，可以使用更大的批处理大小。",
    "en_tdlr": "This paper proposes a new sampling algorithm called LABOR that aims to replace the existing Neighbor Sampling algorithm in GNNs. Compared to NS, LABOR can sample fewer vertices without sacrificing quality and converges faster under the same vertex sampling budget constraints. It can also use significantly larger batch sizes."
}