{
    "title": "Self-Guided Diffusion Models. (arXiv:2210.06462v2 [cs.CV] UPDATED)",
    "abstract": "Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability, correctness and unbiasedness. In this paper, we eliminate the need for such annotation by instead leveraging the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels, especially on unbalanced data. When equipped with self-supervised box or m",
    "link": "http://arxiv.org/abs/2210.06462",
    "context": "Title: Self-Guided Diffusion Models. (arXiv:2210.06462v2 [cs.CV] UPDATED)\nAbstract: Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability, correctness and unbiasedness. In this paper, we eliminate the need for such annotation by instead leveraging the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a self-annotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels, especially on unbalanced data. When equipped with self-supervised box or m",
    "path": "papers/22/10/2210.06462.json",
    "total_tokens": 844,
    "translated_title": "自我指导扩散模型",
    "translated_abstract": "扩散模型在图像生成方面取得了显著进展，特别是在使用指导来控制生成过程时。然而，指导需要大量的图像-注释对进行训练，因此依赖于其可用性、正确性和无偏性。本文提出一种可以消除这种注释需求的框架，利用自我监督信号的灵活性设计了自我指导扩散模型。通过利用特征提取函数和自我注释函数，我们的方法在各种图像粒度上提供指导信号：从整体图像到物体框，甚至到分割蒙版。我们在单标签和多标签图像数据集上的实验表明，自标记指导始终优于没有指导的扩散模型，并且在不平衡数据上甚至可以超过基于真实标签的指导。",
    "tldr": "本文提出了一种框架，利用自我监督信号的灵活性设计了自我指导扩散模型。实验表明，自标记指导始终优于没有指导的扩散模型，并且在不平衡数据上甚至可以超过基于真实标签的指导。"
}