{
    "title": "ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation. (arXiv:2210.17375v2 [cs.NE] UPDATED)",
    "abstract": "Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithms (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks: 1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re$^2$), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re$^2$ is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual} linear policy representat",
    "link": "http://arxiv.org/abs/2210.17375",
    "context": "Title: ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation. (arXiv:2210.17375v2 [cs.NE] UPDATED)\nAbstract: Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithms (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks: 1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re$^2$), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re$^2$ is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual} linear policy representat",
    "path": "papers/22/10/2210.17375.json",
    "total_tokens": 914,
    "translated_title": "ERL-Re$^2$: 具有共享状态表示和个体策略表示的高效进化强化学习",
    "translated_abstract": "深度强化学习（Deep RL）和进化算法（EA）是两种具有不同学习原理的策略优化的主要范式，即基于梯度与基于非梯度。一种吸引人的研究方向是通过融合它们的互补优势来整合Deep RL和EA以设计新的方法。然而，现有的关于Deep RL和EA结合的工作存在两个常见的缺点：1）RL代理和EA代理分别学习他们的策略，忽视了有用的共享知识的高效共享；2）参数级别的策略优化不能保证EA侧的行为进化的语义级别。在本文中，我们提出了具有双尺度状态表示和策略表示的进化强化学习（ERL-Re$^2$），这是对前述两个缺点的一种新颖解决方案。ERL-Re$^2$的关键思想是双尺度表示：所有EA和RL策略共享相同的非线性状态表示，同时保持个体线性策略表示。",
    "tldr": "ERL-Re$^2$提出了双尺度状态表示和策略表示的进化强化学习方法，解决了现有工作中忽视共享知识和语义级行为进化的问题。",
    "en_tdlr": "ERL-Re$^2$ proposes a two-scale representation approach for evolutionary reinforcement learning, addressing the issues of neglecting shared knowledge and semantic level behavior evolution in existing works."
}