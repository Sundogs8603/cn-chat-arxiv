{
    "title": "On Stability and Generalization of Bilevel Optimization Problem. (arXiv:2210.01063v3 [cs.LG] UPDATED)",
    "abstract": "(Stochastic) bilevel optimization is a frequently encountered problem in machine learning with a wide range of applications such as meta-learning, hyper-parameter optimization, and reinforcement learning. Most of the existing studies on this problem only focused on analyzing the convergence or improving the convergence rate, while little effort has been devoted to understanding its generalization behaviors. In this paper, we conduct a thorough analysis on the generalization of first-order (gradient-based) methods for the bilevel optimization problem. We first establish a fundamental connection between algorithmic stability and generalization error in different forms and give a high probability generalization bound which improves the previous best one from $\\bigO(\\sqrt{n})$ to $\\bigO(\\log n)$, where $n$ is the sample size. We then provide the first stability bounds for the general case where both inner and outer level parameters are subject to continuous update, while existing work allo",
    "link": "http://arxiv.org/abs/2210.01063",
    "context": "Title: On Stability and Generalization of Bilevel Optimization Problem. (arXiv:2210.01063v3 [cs.LG] UPDATED)\nAbstract: (Stochastic) bilevel optimization is a frequently encountered problem in machine learning with a wide range of applications such as meta-learning, hyper-parameter optimization, and reinforcement learning. Most of the existing studies on this problem only focused on analyzing the convergence or improving the convergence rate, while little effort has been devoted to understanding its generalization behaviors. In this paper, we conduct a thorough analysis on the generalization of first-order (gradient-based) methods for the bilevel optimization problem. We first establish a fundamental connection between algorithmic stability and generalization error in different forms and give a high probability generalization bound which improves the previous best one from $\\bigO(\\sqrt{n})$ to $\\bigO(\\log n)$, where $n$ is the sample size. We then provide the first stability bounds for the general case where both inner and outer level parameters are subject to continuous update, while existing work allo",
    "path": "papers/22/10/2210.01063.json",
    "total_tokens": 927,
    "translated_title": "论二层优化问题的稳定性及其泛化性分析",
    "translated_abstract": "（随机）二层优化问题是机器学习中经常遇到的问题，具有元学习、超参数优化和强化学习等广泛应用。现有研究大多关注于分析该问题的收敛性或提高收敛速度，但很少有研究专注于理解其泛化行为。本文针对二层优化问题的一阶（基于梯度的）方法进行了全面的泛化分析。首先在不同形式上建立了算法稳定性与泛化误差之间的基本联系，并给出了高概率泛化界，将其从$ \\bigO(\\sqrt{n}) $改善为$ \\bigO(\\log n) $，其中$ n $是样本量。其次，对于参数持续更新的内部层与外部层通用情况，我们提出了第一个稳定性界限，而现有的工作仅适用于特殊情况。",
    "tldr": "本文对二层优化问题的一阶（基于梯度的）方法进行了全面泛化分析，建立了算法稳定性与泛化误差之间的基本联系，并提出了高概率泛化界，将其从$ \\bigO(\\sqrt{n}) $改善为$ \\bigO(\\log n) $，同时也提出了第一个泛化界限。"
}