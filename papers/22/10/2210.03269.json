{
    "title": "Multi-agent Deep Covering Skill Discovery. (arXiv:2210.03269v2 [cs.LG] UPDATED)",
    "abstract": "The use of skills (a.k.a., options) can greatly accelerate exploration in reinforcement learning, especially when only sparse reward signals are available. While option discovery methods have been proposed for individual agents, in multi-agent reinforcement learning settings, discovering collaborative options that can coordinate the behavior of multiple agents and encourage them to visit the under-explored regions of their joint state space has not been considered. In this case, we propose Multi-agent Deep Covering Option Discovery, which constructs the multi-agent options through minimizing the expected cover time of the multiple agents' joint state space. Also, we propose a novel framework to adopt the multi-agent options in the MARL process. In practice, a multi-agent task can usually be divided into some sub-tasks, each of which can be completed by a sub-group of the agents. Therefore, our algorithm framework first leverages an attention mechanism to find collaborative agent sub-gr",
    "link": "http://arxiv.org/abs/2210.03269",
    "context": "Title: Multi-agent Deep Covering Skill Discovery. (arXiv:2210.03269v2 [cs.LG] UPDATED)\nAbstract: The use of skills (a.k.a., options) can greatly accelerate exploration in reinforcement learning, especially when only sparse reward signals are available. While option discovery methods have been proposed for individual agents, in multi-agent reinforcement learning settings, discovering collaborative options that can coordinate the behavior of multiple agents and encourage them to visit the under-explored regions of their joint state space has not been considered. In this case, we propose Multi-agent Deep Covering Option Discovery, which constructs the multi-agent options through minimizing the expected cover time of the multiple agents' joint state space. Also, we propose a novel framework to adopt the multi-agent options in the MARL process. In practice, a multi-agent task can usually be divided into some sub-tasks, each of which can be completed by a sub-group of the agents. Therefore, our algorithm framework first leverages an attention mechanism to find collaborative agent sub-gr",
    "path": "papers/22/10/2210.03269.json",
    "total_tokens": 907,
    "translated_title": "多智能体深度覆盖技能发现",
    "translated_abstract": "在强化学习中使用技能（即选项）可以大大加快探索速度，尤其是当只有稀疏的奖励信号可用时。虽然已经提出了针对个体智能体的选项发现方法，在多智能体强化学习环境中，尚未考虑如何发现协作选项，以协调多个智能体的行为并鼓励它们访问联合状态空间中未开发的区域。因此，我们提出了多智能体深度覆盖选项发现，通过最小化多个智能体联合状态空间的预期覆盖时间来构建多智能体选项。此外，我们提出了一种新颖的框架来在多智能体强化学习过程中采用多智能体选项。实际上，多智能体任务通常可以分为一些子任务，每个子任务可以由一个子团体的智能体完成。因此，我们的算法框架首先利用注意力机制来找到协作智能体子团体。",
    "tldr": "这篇论文介绍了一种针对多智能体强化学习中合作选项发现的方法，通过最小化多个智能体联合状态空间的预期覆盖时间来构建多智能体选项，并提出了采用这些选项的新框架。",
    "en_tdlr": "This paper presents a method for discovering collaborative options in multi-agent reinforcement learning, constructing the options by minimizing the expected cover time of the joint state space of multiple agents, and proposing a novel framework to adopt these options."
}