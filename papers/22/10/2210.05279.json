{
    "title": "Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity",
    "abstract": "arXiv:2210.05279v2 Announce Type: replace  Abstract: $\\ell_0$ constrained optimization is prevalent in machine learning, particularly for high-dimensional problems, because it is a fundamental approach to achieve sparse learning. Hard-thresholding gradient descent is a dominant technique to solve this problem. However, first-order gradients of the objective function may be either unavailable or expensive to calculate in a lot of real-world problems, where zeroth-order (ZO) gradients could be a good surrogate. Unfortunately, whether ZO gradients can work with the hard-thresholding operator is still an unsolved problem. To solve this puzzle, in this paper, we focus on the $\\ell_0$ constrained black-box stochastic optimization problems, and propose a new stochastic zeroth-order gradient hard-thresholding (SZOHT) algorithm with a general ZO gradient estimator powered by a novel random support sampling. We provide the convergence analysis of SZOHT under standard assumptions. Importantly, we",
    "link": "https://arxiv.org/abs/2210.05279",
    "context": "Title: Zeroth-Order Hard-Thresholding: Gradient Error vs. Expansivity\nAbstract: arXiv:2210.05279v2 Announce Type: replace  Abstract: $\\ell_0$ constrained optimization is prevalent in machine learning, particularly for high-dimensional problems, because it is a fundamental approach to achieve sparse learning. Hard-thresholding gradient descent is a dominant technique to solve this problem. However, first-order gradients of the objective function may be either unavailable or expensive to calculate in a lot of real-world problems, where zeroth-order (ZO) gradients could be a good surrogate. Unfortunately, whether ZO gradients can work with the hard-thresholding operator is still an unsolved problem. To solve this puzzle, in this paper, we focus on the $\\ell_0$ constrained black-box stochastic optimization problems, and propose a new stochastic zeroth-order gradient hard-thresholding (SZOHT) algorithm with a general ZO gradient estimator powered by a novel random support sampling. We provide the convergence analysis of SZOHT under standard assumptions. Importantly, we",
    "path": "papers/22/10/2210.05279.json",
    "total_tokens": 891,
    "translated_title": "零阶硬阈值：梯度误差与扩展性",
    "translated_abstract": "$\\ell_0$约束优化在机器学习中很常见，尤其在高维问题中，因为它是实现稀疏学习的一种基本方法。硬阈值梯度下降是解决这一问题的主要技术。然而，在许多实际问题中，目标函数的一阶梯度可能无法获取或计算代价高昂，这时零阶（ZO）梯度可以成为一个很好的替代方案。遗憾的是，零阶梯度能否与硬阈值算子配合仍然是一个未解决的问题。为了解决这个谜题，本文关注$\\ell_0$约束的黑盒随机优化问题，并提出了一种新的随机零阶梯度硬阈值（SZOHT）算法，其使用通用的ZO梯度估计器，通过新颖的随机支持采样得以实现。我们在标准假设下提供了SZOHT的收敛性分析。",
    "tldr": "本文提出了一种新的随机零阶梯度硬阈值（SZOHT）算法，通过新颖的随机支持采样，解决了$\\ell_0$约束下梯度计算困难的问题。",
    "en_tdlr": "A new stochastic zeroth-order gradient hard-thresholding (SZOHT) algorithm is proposed in this paper, which tackles the difficulty of gradient computation under $\\ell_0$ constraint with novel random support sampling."
}