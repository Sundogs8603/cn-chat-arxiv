{
    "title": "Symmetric (Optimistic) Natural Policy Gradient for Multi-agent Learning with Parameter Convergence. (arXiv:2210.12812v2 [math.OC] UPDATED)",
    "abstract": "Multi-agent interactions are increasingly important in the context of reinforcement learning, and the theoretical foundations of policy gradient methods have attracted surging research interest. We investigate the global convergence of natural policy gradient (NPG) algorithms in multi-agent learning. We first show that vanilla NPG may not have parameter convergence, i.e., the convergence of the vector that parameterizes the policy, even when the costs are regularized (which enabled strong convergence guarantees in the policy space in the literature). This non-convergence of parameters leads to stability issues in learning, which becomes especially relevant in the function approximation setting, where we can only operate on low-dimensional parameters, instead of the high-dimensional policy. We then propose variants of the NPG algorithm, for several standard multi-agent learning scenarios: two-player zero-sum matrix and Markov games, and multi-player monotone games, with global last-iter",
    "link": "http://arxiv.org/abs/2210.12812",
    "context": "Title: Symmetric (Optimistic) Natural Policy Gradient for Multi-agent Learning with Parameter Convergence. (arXiv:2210.12812v2 [math.OC] UPDATED)\nAbstract: Multi-agent interactions are increasingly important in the context of reinforcement learning, and the theoretical foundations of policy gradient methods have attracted surging research interest. We investigate the global convergence of natural policy gradient (NPG) algorithms in multi-agent learning. We first show that vanilla NPG may not have parameter convergence, i.e., the convergence of the vector that parameterizes the policy, even when the costs are regularized (which enabled strong convergence guarantees in the policy space in the literature). This non-convergence of parameters leads to stability issues in learning, which becomes especially relevant in the function approximation setting, where we can only operate on low-dimensional parameters, instead of the high-dimensional policy. We then propose variants of the NPG algorithm, for several standard multi-agent learning scenarios: two-player zero-sum matrix and Markov games, and multi-player monotone games, with global last-iter",
    "path": "papers/22/10/2210.12812.json",
    "total_tokens": 935,
    "translated_title": "对称（乐观）自然策略梯度的参数收敛性多智能体学习",
    "translated_abstract": "在强化学习中，多智能体互动越来越重要，并且策略梯度方法的理论基础引起了人们的浓厚兴趣。本文研究了多智能体学习中自然策略梯度（NPG）算法的全局收敛性。我们首先展示了香草NPG可能没有参数收敛，即参数化策略的向量的收敛，即使成本被规则化（在文献中使策略空间有强的收敛保证）。这些非收敛的参数导致学习中的稳定性问题，在函数逼近的情况下尤为重要，在这种情况下，我们只能处理低维参数，而不是高维策略。然后我们针对几种标准多智能体学习场景提出了NPG算法的变种：两个玩家的零和矩阵和马尔可夫博弈，以及多个玩家的单调博弈，其具有全局最后迭代收敛性。",
    "tldr": "本文研究了多智能体学习中的机器人学习算法，指出了香草自然策略梯度算法可能因具有不收敛的参数而存在学习不稳定性问题，并提出了新的NPG算法变种以解决此问题。",
    "en_tdlr": "This paper investigates the parameter convergence issue of the natural policy gradient algorithm in multi-agent learning, proposes new variants of the NPG algorithm to tackle the instability issue caused by non-converging parameters, and applies these algorithms to standard multi-agent learning scenarios."
}