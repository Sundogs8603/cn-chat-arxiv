{
    "title": "Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v3 [cs.CV] UPDATED)",
    "abstract": "Visual Story-Telling is the process of forming a multi-sentence story from a set of images. Appropriately including visual variation and contextual information captured inside the input images is one of the most challenging aspects of visual storytelling. Consequently, stories developed from a set of images often lack cohesiveness, relevance, and semantic relationship. In this paper, we propose a novel Vision Transformer Based Model for describing a set of images as a story. The proposed method extracts the distinct features of the input images using a Vision Transformer (ViT). Firstly, input images are divided into 16X16 patches and bundled into a linear projection of flattened patches. The transformation from a single image to multiple image patches captures the visual variety of the input visual patterns. These features are used as input to a Bidirectional-LSTM which is part of the sequence encoder. This captures the past and future image context of all image patches. Then, an atten",
    "link": "http://arxiv.org/abs/2210.02762",
    "context": "Title: Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v3 [cs.CV] UPDATED)\nAbstract: Visual Story-Telling is the process of forming a multi-sentence story from a set of images. Appropriately including visual variation and contextual information captured inside the input images is one of the most challenging aspects of visual storytelling. Consequently, stories developed from a set of images often lack cohesiveness, relevance, and semantic relationship. In this paper, we propose a novel Vision Transformer Based Model for describing a set of images as a story. The proposed method extracts the distinct features of the input images using a Vision Transformer (ViT). Firstly, input images are divided into 16X16 patches and bundled into a linear projection of flattened patches. The transformation from a single image to multiple image patches captures the visual variety of the input visual patterns. These features are used as input to a Bidirectional-LSTM which is part of the sequence encoder. This captures the past and future image context of all image patches. Then, an atten",
    "path": "papers/22/10/2210.02762.json",
    "total_tokens": 989,
    "translated_title": "基于视觉Transformer的模型用于将一组图像描述为一个故事",
    "translated_abstract": "视觉故事讲述是将一组图像形成多句故事的过程。恰当地包含输入图像内捕捉到的视觉变化和上下文信息是视觉故事讲述中最具挑战性的方面之一。因此，由一组图像开发的故事经常缺乏凝聚力、相关性和语义关系。在本文中，我们提出了一种新颖的基于视觉Transformer的模型，用于将一组图像描述为一个故事。所提出的方法使用视觉Transformer（ViT）提取输入图像的独特特征。首先，将输入图像分成16X16的补丁，并捆绑到扁平化补丁的线性投影中。从单个图像到多个图像补丁的转换捕捉到了输入视觉模式的视觉多样性。这些特征作为输入传递给双向LSTM，它是序列编码器的一部分。这样可以捕捉到所有图像补丁的过去和未来图像上下文。然后，通过注意力机制来加权计算图像补丁的特征向量，以产生最终的故事描述向量。",
    "tldr": "本研究提出了一种基于视觉Transformer的模型，用于将一组图像描述为一个故事。该模型通过ViT提取输入图像的特征，并使用双向LSTM捕捉图像补丁的过去和未来上下文信息。通过注意力机制加权计算得到最终的故事描述向量。",
    "en_tdlr": "This paper proposes a Vision Transformer based model to describe a set of images as a story. The model utilizes ViT to extract distinct features from the input images and utilizes Bidirectional-LSTM to capture the past and future context of image patches. The final story description vector is generated through attention mechanism weight calculation."
}