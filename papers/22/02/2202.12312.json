{
    "title": "Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies. (arXiv:2202.12312v2 [cs.CL] UPDATED)",
    "abstract": "When we transfer a pretrained language model to a new language, there are many axes of variation that change at once. To disentangle the impact of different factors like syntactic similarity and vocabulary similarity, we propose a set of controlled transfer studies: we systematically transform the language of the GLUE benchmark, altering one axis of crosslingual variation at a time, and then measure the resulting drops in a pretrained model's downstream performance. We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens. %On the other hand, transferring to a dataset with an unaligned vocabulary is extremely hard to recover from in the low-data regime. Moreover, good-quality tokenizers in the transfer language do not make vocabulary alignment easier. Our experiments provide insights into the factors of cross-lingual transfer that rese",
    "link": "http://arxiv.org/abs/2202.12312",
    "context": "Title: Oolong: Investigating What Makes Transfer Learning Hard with Controlled Studies. (arXiv:2202.12312v2 [cs.CL] UPDATED)\nAbstract: When we transfer a pretrained language model to a new language, there are many axes of variation that change at once. To disentangle the impact of different factors like syntactic similarity and vocabulary similarity, we propose a set of controlled transfer studies: we systematically transform the language of the GLUE benchmark, altering one axis of crosslingual variation at a time, and then measure the resulting drops in a pretrained model's downstream performance. We find that models can largely recover from syntactic-style shifts, but cannot recover from vocabulary misalignment and embedding matrix re-initialization, even with continued pretraining on 15 million tokens. %On the other hand, transferring to a dataset with an unaligned vocabulary is extremely hard to recover from in the low-data regime. Moreover, good-quality tokenizers in the transfer language do not make vocabulary alignment easier. Our experiments provide insights into the factors of cross-lingual transfer that rese",
    "path": "papers/22/02/2202.12312.json",
    "total_tokens": 893,
    "translated_title": "Oolong: 用可控实验证明转移学习的困难性",
    "translated_abstract": "当我们将预训练的语言模型转移到新的语言时，有许多变化因素同时发生。为了解交叉语言变体的不同因素（如句法相似性和词汇相似性）的影响，我们提出了一组可控制的转移研究：我们系统地转换GLUE基准测试的语言，逐个改变一种交叉语言变体的因素，然后测量预训练模型在后续性能中的降低。我们发现，模型可以在句法风格的转变中很大程度上恢复，但无法从词汇不对齐和嵌入矩阵重新初始化中恢复，即使在1500万令牌的继续预训练的情况下。%另一方面，在低数据范围内转移到具有不对齐词汇的数据集中很难恢复。此外，转移语言中的高质量分词器也不会使词汇对齐变得更容易。我们的实验提供了关于交叉语言转移因素的洞察。",
    "tldr": "本论文通过对预训练模型进行可控实验证明转移学习的困难性，发现模型可以从句法风格的转变中恢复，但无法从词汇不对齐和嵌入矩阵重新初始化中恢复，即使进行了大量预训练。"
}