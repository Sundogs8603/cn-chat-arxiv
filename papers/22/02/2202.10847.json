{
    "title": "UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography. (arXiv:2202.10847v3 [eess.IV] UPDATED)",
    "abstract": "Implicit neural representations (INRs) have achieved impressive results for scene reconstruction and computer graphics, where their performance has primarily been assessed on reconstruction accuracy. As INRs make their way into other domains, where model predictions inform high-stakes decision-making, uncertainty quantification of INR inference is becoming critical. To that end, we study a Bayesian reformulation of INRs, UncertaINR, in the context of computed tomography, and evaluate several Bayesian deep learning implementations in terms of accuracy and calibration. We find that they achieve well-calibrated uncertainty, while retaining accuracy competitive with other classical, INR-based, and CNN-based reconstruction techniques. Contrary to common intuition in the Bayesian deep learning literature, we find that INRs obtain the best calibration with computationally efficient Monte Carlo dropout, outperforming Hamiltonian Monte Carlo and deep ensembles. Moreover, in contrast to the best",
    "link": "http://arxiv.org/abs/2202.10847",
    "context": "Title: UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography. (arXiv:2202.10847v3 [eess.IV] UPDATED)\nAbstract: Implicit neural representations (INRs) have achieved impressive results for scene reconstruction and computer graphics, where their performance has primarily been assessed on reconstruction accuracy. As INRs make their way into other domains, where model predictions inform high-stakes decision-making, uncertainty quantification of INR inference is becoming critical. To that end, we study a Bayesian reformulation of INRs, UncertaINR, in the context of computed tomography, and evaluate several Bayesian deep learning implementations in terms of accuracy and calibration. We find that they achieve well-calibrated uncertainty, while retaining accuracy competitive with other classical, INR-based, and CNN-based reconstruction techniques. Contrary to common intuition in the Bayesian deep learning literature, we find that INRs obtain the best calibration with computationally efficient Monte Carlo dropout, outperforming Hamiltonian Monte Carlo and deep ensembles. Moreover, in contrast to the best",
    "path": "papers/22/02/2202.10847.json",
    "total_tokens": 1041,
    "translated_abstract": "隐式神经表示（INR）已经在场景重建和计算机图形方面取得了显著的成果，在这些领域中其性能主要通过重建精度来评估。随着INR进入其他领域，在这些领域中，模型预测影响到高风险的决策制定，INR推理的不确定性量化变得至关重要。为此，我们在计算机断层扫描的背景下研究了INR的贝叶斯重构，即UncertaINR，并评估了几种贝叶斯深度学习实现的准确性和校准性。我们发现，它们实现了良好的校准不确定性，同时保持了与其他经典的INR和基于CNN的重建技术竞争性的准确性。与贝叶斯深度学习文献中的普遍直觉相反，我们发现INR使用计算效率高的蒙特卡罗dropout获得了最佳的校准性能，胜过哈密顿蒙特卡罗和深度集合。此外, 不同于最好的Bayesian基准，在本文中INRs未经校准的模型可以提供与传统和CNN 双重或三重等级的保守性能。",
    "tldr": "本文研究了INR的贝叶斯改进，即UncertaINR，以实现计算机断层扫描方面对INR推理的不确定性量化。使用蒙特卡罗dropout实现了最佳的校准性能，同时保持了与其他竞争性的重建技术相似的准确性。"
}