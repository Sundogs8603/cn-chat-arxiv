{
    "title": "Are Transformers More Robust? Towards Exact Robustness Verification for Transformers. (arXiv:2202.03932v4 [cs.LG] UPDATED)",
    "abstract": "As an emerging type of Neural Networks (NNs), Transformers are used in many domains ranging from Natural Language Processing to Autonomous Driving. In this paper, we study the robustness problem of Transformers, a key characteristic as low robustness may cause safety concerns. Specifically, we focus on Sparsemax-based Transformers and reduce the finding of their maximum robustness to a Mixed Integer Quadratically Constrained Programming (MIQCP) problem. We also design two pre-processing heuristics that can be embedded in the MIQCP encoding and substantially accelerate its solving. We then conduct experiments using the application of Land Departure Warning to compare the robustness of Sparsemax-based Transformers against that of the more conventional Multi-Layer-Perceptron (MLP) NNs. To our surprise, Transformers are not necessarily more robust, leading to profound considerations in selecting appropriate NN architectures for safety-critical domain applications.",
    "link": "http://arxiv.org/abs/2202.03932",
    "context": "Title: Are Transformers More Robust? Towards Exact Robustness Verification for Transformers. (arXiv:2202.03932v4 [cs.LG] UPDATED)\nAbstract: As an emerging type of Neural Networks (NNs), Transformers are used in many domains ranging from Natural Language Processing to Autonomous Driving. In this paper, we study the robustness problem of Transformers, a key characteristic as low robustness may cause safety concerns. Specifically, we focus on Sparsemax-based Transformers and reduce the finding of their maximum robustness to a Mixed Integer Quadratically Constrained Programming (MIQCP) problem. We also design two pre-processing heuristics that can be embedded in the MIQCP encoding and substantially accelerate its solving. We then conduct experiments using the application of Land Departure Warning to compare the robustness of Sparsemax-based Transformers against that of the more conventional Multi-Layer-Perceptron (MLP) NNs. To our surprise, Transformers are not necessarily more robust, leading to profound considerations in selecting appropriate NN architectures for safety-critical domain applications.",
    "path": "papers/22/02/2202.03932.json",
    "total_tokens": 879,
    "translated_title": "Transformer更加稳健吗？面向Transformer的确切稳健性验证",
    "translated_abstract": "作为新兴的神经网络类型，Transformer被应用于众多领域，从自然语言处理到自动驾驶。本论文研究Transformers的稳健性问题，这是一个关键特性，低稳健性可能会引起安全问题。具体而言，我们关注基于Sparsemax的Transformers，将找到它们的最大稳健性降低为一个混合整数二次约束编程（MIQCP）问题。我们还设计了两个可嵌入MIQCP编码并大幅加速其求解的预处理启发式方法。然后，我们使用Land Departure Warning应用程序进行实验，比较基于Sparsemax的Transformers与更传统的多层感知器（MLP）神经网络的稳健性。令我们惊讶的是，Transformer并不一定更加稳健，这引发了在选择适用于安全关键领域应用的NN架构方面的深刻考虑。",
    "tldr": "本文研究了基于Sparsemax的Transformers的稳健性问题，并发现Transformer不一定比传统的多层感知器更加稳健，这对于选择适用于安全关键领域应用的NN架构方面有深刻的考虑。",
    "en_tdlr": "This paper studies the robustness problem of Sparsemax-based Transformers and finds that they are not necessarily more robust than conventional Multi-Layer Perceptron neural networks, which has important considerations for selecting appropriate NN architectures for safety-critical domain applications."
}