{
    "title": "A policy gradient approach for optimization of smooth risk measures. (arXiv:2202.11046v2 [cs.LG] UPDATED)",
    "abstract": "We propose policy gradient algorithms for solving a risk-sensitive reinforcement learning problem in on-policy as well as off-policy settings. We consider episodic Markov decision processes, and model the risk using the broad class of smooth risk measures of the cumulative discounted reward. We propose two template policy gradient algorithms that optimize a smooth risk measure in on-policy and off-policy RL settings, respectively. We derive non-asymptotic bounds that quantify the rate of convergence to our proposed algorithms to a stationary point of the smooth risk measure. As special cases, we establish that our algorithms apply to the optimization of mean-variance and distortion risk measures, respectively.",
    "link": "http://arxiv.org/abs/2202.11046",
    "context": "Title: A policy gradient approach for optimization of smooth risk measures. (arXiv:2202.11046v2 [cs.LG] UPDATED)\nAbstract: We propose policy gradient algorithms for solving a risk-sensitive reinforcement learning problem in on-policy as well as off-policy settings. We consider episodic Markov decision processes, and model the risk using the broad class of smooth risk measures of the cumulative discounted reward. We propose two template policy gradient algorithms that optimize a smooth risk measure in on-policy and off-policy RL settings, respectively. We derive non-asymptotic bounds that quantify the rate of convergence to our proposed algorithms to a stationary point of the smooth risk measure. As special cases, we establish that our algorithms apply to the optimization of mean-variance and distortion risk measures, respectively.",
    "path": "papers/22/02/2202.11046.json",
    "total_tokens": 792,
    "translated_title": "平滑风险度量的策略梯度优化方法",
    "translated_abstract": "我们提出了一种策略梯度算法，用于解决风险敏感的强化学习问题，包括on-policy和off-policy情况。我们考虑时间段马尔可夫决策过程，并利用累积折扣奖励的广义平滑风险度量来建模风险。我们提出了两个模板策略梯度算法，分别在on-policy和off-policy RL情况下优化平滑风险度量。我们推导出非渐进性界，量化了我们提出的算法收敛到平滑风险度量的稳态点的速率。作为特例，我们确定了我们的算法分别应用于均值-方差和畸变风险度量的优化。",
    "tldr": "本文介绍了一种应用于on-policy和off-policy RL情况下的策略梯度算法，用于最小化广义平滑风险度量，能够收敛到平滑风险度量的稳态点，并适用于均值-方差和畸变风险度量的优化。",
    "en_tdlr": "The paper proposes policy gradient algorithms for minimizing smooth risk measures in on-policy and off-policy RL scenarios, with non-asymptotic bounds derived for convergence to a stationary point of the measure. The algorithms are applicable to optimization of mean-variance and distortion risk measures."
}