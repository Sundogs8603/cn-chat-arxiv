{
    "title": "Finite-Sum Coupled Compositional Stochastic Optimization: Theory and Applications. (arXiv:2202.12396v7 [math.OC] UPDATED)",
    "abstract": "This paper studies stochastic optimization for a sum of compositional functions, where the inner-level function of each summand is coupled with the corresponding summation index. We refer to this family of problems as finite-sum coupled compositional optimization (FCCO). It has broad applications in machine learning for optimizing non-convex or convex compositional measures/objectives such as average precision (AP), p-norm push, listwise ranking losses, neighborhood component analysis (NCA), deep survival analysis, deep latent variable models, etc., which deserves finer analysis. Yet, existing algorithms and analyses are restricted in one or other aspects. The contribution of this paper is to provide a comprehensive convergence analysis of a simple stochastic algorithm for both non-convex and convex objectives. Our key result is the improved oracle complexity with the parallel speed-up by using the moving-average based estimator with mini-batching. Our theoretical analysis also exhibit",
    "link": "http://arxiv.org/abs/2202.12396",
    "context": "Title: Finite-Sum Coupled Compositional Stochastic Optimization: Theory and Applications. (arXiv:2202.12396v7 [math.OC] UPDATED)\nAbstract: This paper studies stochastic optimization for a sum of compositional functions, where the inner-level function of each summand is coupled with the corresponding summation index. We refer to this family of problems as finite-sum coupled compositional optimization (FCCO). It has broad applications in machine learning for optimizing non-convex or convex compositional measures/objectives such as average precision (AP), p-norm push, listwise ranking losses, neighborhood component analysis (NCA), deep survival analysis, deep latent variable models, etc., which deserves finer analysis. Yet, existing algorithms and analyses are restricted in one or other aspects. The contribution of this paper is to provide a comprehensive convergence analysis of a simple stochastic algorithm for both non-convex and convex objectives. Our key result is the improved oracle complexity with the parallel speed-up by using the moving-average based estimator with mini-batching. Our theoretical analysis also exhibit",
    "path": "papers/22/02/2202.12396.json",
    "total_tokens": 852,
    "translated_title": "有限和耦合组合随机优化：理论与应用",
    "translated_abstract": "本文研究了组合函数求和的随机优化问题，其中每个求和项的内层函数与相应的求和索引配对。我们将这类问题称为有限和耦合组合优化（FCCO）。这在机器学习中有广泛的应用，用于优化非凸或凸组合测量/目标，例如平均精度（AP），p-范数推进，列表式排名损失，邻域组件分析（NCA），深度生存分析和深度潜在变量模型等。然而，现有的算法和分析在某些方面受到限制。本文的贡献是为非凸和凸目标提供了一个简单随机算法的全面收敛性分析。我们的关键结果是使用基于移动平均的估计器与小批量并行加速改进了Oracle复杂度。我们的理论分析还展示了....",
    "tldr": "本文提出了一种适用于凸和非凸组合函数求和的随机算法，并在理论上证明了该算法在Oracle复杂度和并行加速方面的优秀性能。"
}