{
    "title": "Random Feature Amplification: Feature Learning and Generalization in Neural Networks. (arXiv:2202.07626v4 [cs.LG] UPDATED)",
    "abstract": "In this work, we provide a characterization of the feature-learning process in two-layer ReLU networks trained by gradient descent on the logistic loss following random initialization. We consider data with binary labels that are generated by an XOR-like function of the input features. We permit a constant fraction of the training labels to be corrupted by an adversary. We show that, although linear classifiers are no better than random guessing for the distribution we consider, two-layer ReLU networks trained by gradient descent achieve generalization error close to the label noise rate. We develop a novel proof technique that shows that at initialization, the vast majority of neurons function as random features that are only weakly correlated with useful features, and the gradient descent dynamics 'amplify' these weak, random features to strong, useful features.",
    "link": "http://arxiv.org/abs/2202.07626",
    "context": "Title: Random Feature Amplification: Feature Learning and Generalization in Neural Networks. (arXiv:2202.07626v4 [cs.LG] UPDATED)\nAbstract: In this work, we provide a characterization of the feature-learning process in two-layer ReLU networks trained by gradient descent on the logistic loss following random initialization. We consider data with binary labels that are generated by an XOR-like function of the input features. We permit a constant fraction of the training labels to be corrupted by an adversary. We show that, although linear classifiers are no better than random guessing for the distribution we consider, two-layer ReLU networks trained by gradient descent achieve generalization error close to the label noise rate. We develop a novel proof technique that shows that at initialization, the vast majority of neurons function as random features that are only weakly correlated with useful features, and the gradient descent dynamics 'amplify' these weak, random features to strong, useful features.",
    "path": "papers/22/02/2202.07626.json",
    "total_tokens": 976,
    "translated_title": "随机特征放大：神经网络中的特征学习和泛化",
    "translated_abstract": "在这项工作中，我们提供了对由梯度下降训练的两层ReLU网络在随机初始化后，通过逻辑损失函数对特征学习过程的表征。我们考虑由输入特征的XOR-like函数生成的具有二元标签的数据。我们允许一个固定比例的训练标签被对手损坏。我们证明，尽管线性分类器对于我们考虑的分布而言不比随机猜测更好，但通过梯度下降训练的两层ReLU网络的泛化误差接近于标签噪声率。我们开发了一种新颖的证明技巧，表明在初始化时，绝大多数神经元作为仅与有用特征弱相关的随机特征，而梯度下降动力学将这些弱的随机特征放大为强有用的特征。",
    "tldr": "本文通过分析在随机初始化后通过逻辑损失函数进行梯度下降训练的两层ReLU网络在特征学习过程中的表征，证明了对于具有二元标签的由XOR-like函数生成的数据，尽管线性分类器在该分布上无法更好地工作，但该网络的泛化误差接近于标签噪声率。通过一种新颖的证明技巧，揭示了初始化时大多数神经元作为随机特征，随后通过梯度下降动力学将这些弱的随机特征放大为有用的特征。",
    "en_tdlr": "This paper characterizes the feature-learning process in two-layer ReLU networks trained by gradient descent and demonstrates that despite linear classifiers not performing well on the given distribution, the generalized error of these networks approaches the label noise rate. A novel proof technique reveals that most neurons function as random features at initialization, and gradient descent dynamics amplify these weak random features into useful ones."
}