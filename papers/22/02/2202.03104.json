{
    "title": "SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation. (arXiv:2202.03104v3 [cs.LG] UPDATED)",
    "abstract": "Graph contrastive learning (GCL) has emerged as a dominant technique for graph representation learning which maximizes the mutual information between paired graph augmentations that share the same semantics. Unfortunately, it is difficult to preserve semantics well during augmentations in view of the diverse nature of graph data. Currently, data augmentations in GCL that are designed to preserve semantics broadly fall into three unsatisfactory ways. First, the augmentations can be manually picked per dataset by trial-and-errors. Second, the augmentations can be selected via cumbersome search. Third, the augmentations can be obtained by introducing expensive domain-specific knowledge as guidance. All of these limit the efficiency and more general applicability of existing GCL methods. To circumvent these crucial issues, we propose a \\underline{Sim}ple framework for \\underline{GRA}ph \\underline{C}ontrastive l\\underline{E}arning, \\textbf{SimGRACE} for brevity, which does not require data ",
    "link": "http://arxiv.org/abs/2202.03104",
    "context": "Title: SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation. (arXiv:2202.03104v3 [cs.LG] UPDATED)\nAbstract: Graph contrastive learning (GCL) has emerged as a dominant technique for graph representation learning which maximizes the mutual information between paired graph augmentations that share the same semantics. Unfortunately, it is difficult to preserve semantics well during augmentations in view of the diverse nature of graph data. Currently, data augmentations in GCL that are designed to preserve semantics broadly fall into three unsatisfactory ways. First, the augmentations can be manually picked per dataset by trial-and-errors. Second, the augmentations can be selected via cumbersome search. Third, the augmentations can be obtained by introducing expensive domain-specific knowledge as guidance. All of these limit the efficiency and more general applicability of existing GCL methods. To circumvent these crucial issues, we propose a \\underline{Sim}ple framework for \\underline{GRA}ph \\underline{C}ontrastive l\\underline{E}arning, \\textbf{SimGRACE} for brevity, which does not require data ",
    "path": "papers/22/02/2202.03104.json",
    "total_tokens": 1159,
    "translated_title": "SimGRACE: 一种简单的图形对比学习框架不需要数据增强",
    "translated_abstract": "图形对比学习已经成为图形表示学习的一种主流技术，它最大化了共享相同语义的成对图形增强之间的互信息。不幸的是，鉴于图形数据的多样性，很难在增强过程中很好地保留语义。目前，在广泛保留语义的图形对比学习中，数据增强分为三种不令人满意的方式。第一，可以通过尝试错误来手动选择每个数据集的增强。第二，可以通过繁琐的搜索选择增强。第三，可以通过引入昂贵的领域特定知识作为指导来获得增强。所有这些都限制了现有GCL方法的效率和更广泛的适用性。为了解决这些关键问题，我们提出了一个名为SimGRACE的简单图形对比学习框架，它不需要数据增强就能在几个基准数据集上实现最新的结果。SimGRACE采用基于最近提出的InfoNCE损失的简单对比损失函数，并利用高效的负采样策略。此外，我们引入了一种新颖的节点对比损失和一个简单的图形正则化项来进一步增强SimGRACE的性能。我们的实验结果证明了SimGRACE相对于其他最新的GCL方法的有效性和效率。",
    "tldr": "SimGRACE是一种简单的图形对比学习框架，它不需要数据增强就能在几个基准数据集上实现最新的结果，并采用了一种高效的对比损失函数、负采样策略、以及一些附加项来增强性能。",
    "en_tdlr": "SimGRACE is a simple graph contrastive learning framework that achieves state-of-the-art results on several benchmark datasets without requiring data augmentation. It adopts a simple contrastive loss function based on the InfoNCE loss, utilizes an efficient negative sampling strategy, and introduces a novel node-wise contrastive loss and a simple graph regularization term to further enhance performance."
}