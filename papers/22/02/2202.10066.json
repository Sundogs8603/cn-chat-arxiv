{
    "title": "Multi-task Representation Learning with Stochastic Linear Bandits. (arXiv:2202.10066v2 [stat.ML] UPDATED)",
    "abstract": "We study the problem of transfer-learning in the setting of stochastic linear bandit tasks. We consider that a low dimensional linear representation is shared across the tasks, and study the benefit of learning this representation in the multi-task learning setting. Following recent results to design stochastic bandit policies, we propose an efficient greedy policy based on trace norm regularization. It implicitly learns a low dimensional representation by encouraging the matrix formed by the task regression vectors to be of low rank. Unlike previous work in the literature, our policy does not need to know the rank of the underlying matrix. We derive an upper bound on the multi-task regret of our policy, which is, up to logarithmic factors, of order $\\sqrt{NdT(T+d)r}$, where $T$ is the number of tasks, $r$ the rank, $d$ the number of variables and $N$ the number of rounds per task. We show the benefit of our strategy compared to the baseline $Td\\sqrt{N}$ obtained by solving each task i",
    "link": "http://arxiv.org/abs/2202.10066",
    "context": "Title: Multi-task Representation Learning with Stochastic Linear Bandits. (arXiv:2202.10066v2 [stat.ML] UPDATED)\nAbstract: We study the problem of transfer-learning in the setting of stochastic linear bandit tasks. We consider that a low dimensional linear representation is shared across the tasks, and study the benefit of learning this representation in the multi-task learning setting. Following recent results to design stochastic bandit policies, we propose an efficient greedy policy based on trace norm regularization. It implicitly learns a low dimensional representation by encouraging the matrix formed by the task regression vectors to be of low rank. Unlike previous work in the literature, our policy does not need to know the rank of the underlying matrix. We derive an upper bound on the multi-task regret of our policy, which is, up to logarithmic factors, of order $\\sqrt{NdT(T+d)r}$, where $T$ is the number of tasks, $r$ the rank, $d$ the number of variables and $N$ the number of rounds per task. We show the benefit of our strategy compared to the baseline $Td\\sqrt{N}$ obtained by solving each task i",
    "path": "papers/22/02/2202.10066.json",
    "total_tokens": 960,
    "translated_title": "多任务表示学习与随机线性赌博机",
    "translated_abstract": "我们研究了在随机线性赌博机任务中的迁移学习问题。我们考虑跨任务共享低维线性表示，并研究在多任务学习中学习这种表示的益处。根据最新的随机赌博机策略设计结果，我们提出了一种基于迹范数正则化的高效贪婪策略。它通过鼓励任务回归向量形成的矩阵具有低秩来隐式地学习低维表示。与文献中的先前工作不同，我们的策略不需要知道潜在矩阵的秩。我们导出了我们策略的多任务遗憾的上界，该上界在对数因子上是$O(\\sqrt{NdT(T+d)r})$，其中$T$是任务数，$r$是秩，$d$是变量数，$N$是每个任务的回合数。我们展示了与基线$Td\\sqrt{N}$相比，我们策略的益处。",
    "tldr": "本研究通过跨任务共享低维线性表示，提出了一种基于迹范数正则化的高效贪婪策略，在多任务学习中学习低维表示，无需知道潜在矩阵的秩。实验结果表明，该策略相比基线在多任务遗憾上有明显的优势。"
}