{
    "title": "On learning Whittle index policy for restless bandits with scalable regret. (arXiv:2202.03463v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning is an attractive approach to learn good resource allocation and scheduling policies based on data when the system model is unknown. However, the cumulative regret of most RL algorithms scales as $\\tilde O(\\mathsf{S} \\sqrt{\\mathsf{A} T})$, where $\\mathsf{S}$ is the size of the state space, $\\mathsf{A}$ is the size of the action space, $T$ is the horizon, and the $\\tilde{O}(\\cdot)$ notation hides logarithmic terms. Due to the linear dependence on the size of the state space, these regret bounds are prohibitively large for resource allocation and scheduling problems. In this paper, we present a model-based RL algorithm for such problems which has scalable regret. In particular, we consider a restless bandit model, and propose a Thompson-sampling based learning algorithm which is tuned to the underlying structure of the model. We present two characterizations of the regret of the proposed algorithm with respect to the Whittle index policy. First, we show that for a r",
    "link": "http://arxiv.org/abs/2202.03463",
    "context": "Title: On learning Whittle index policy for restless bandits with scalable regret. (arXiv:2202.03463v2 [cs.LG] UPDATED)\nAbstract: Reinforcement learning is an attractive approach to learn good resource allocation and scheduling policies based on data when the system model is unknown. However, the cumulative regret of most RL algorithms scales as $\\tilde O(\\mathsf{S} \\sqrt{\\mathsf{A} T})$, where $\\mathsf{S}$ is the size of the state space, $\\mathsf{A}$ is the size of the action space, $T$ is the horizon, and the $\\tilde{O}(\\cdot)$ notation hides logarithmic terms. Due to the linear dependence on the size of the state space, these regret bounds are prohibitively large for resource allocation and scheduling problems. In this paper, we present a model-based RL algorithm for such problems which has scalable regret. In particular, we consider a restless bandit model, and propose a Thompson-sampling based learning algorithm which is tuned to the underlying structure of the model. We present two characterizations of the regret of the proposed algorithm with respect to the Whittle index policy. First, we show that for a r",
    "path": "papers/22/02/2202.03463.json",
    "total_tokens": 1010,
    "translated_title": "学习Whittle索引策略以解决高效遗憾的不安定赌博机问题",
    "translated_abstract": "当系统模型未知时，强化学习是学习基于数据的良好资源分配和调度策略的有吸引力的方法。然而，大多数强化学习算法的累计遗憾会随$\\tilde O(\\mathsf{S} \\sqrt{\\mathsf{A} T})$规模增加，其中$\\mathsf{S}$为状态空间大小，$\\mathsf{A}$为动作空间大小，$T$为时间步长，而$\\tilde{O}(\\cdot)$符号表示隐藏的对数项。由于具有与状态空间大小线性正相关的遗憾上界，这些遗憾边界对于资源分配和调度问题是不可接受的大。本文提出了一种在这些问题中具有可扩展性的模型基于强化学习算法。具体而言，我们考虑了不安定赌博机模型，并提出了一种根据模型基础结构调整的基于Thompson采样的学习算法。我们提出了两种关于Whittle索引策略的所提算法的遗憾特性说明。首先，我们表明对于一种Whittle索引策略，所提算法的遗憾满足...（摘要未完整展示）",
    "tldr": "本文提出了一种基于不安定赌博机模型的可扩展模型基于强化学习算法，相比于RL算法的常规遗憾界，该方法具有更小的遗憾界。",
    "en_tdlr": "This paper presents a scalable model-based reinforcement learning algorithm for resource allocation and scheduling problems in situations where system models are unknown. Compared to the conventional regret bounds of RL algorithms, the proposed algorithm has smaller regret bounds by utilizing a Thompson-sampling based learning algorithm and by tuning the algorithm to the underlying structure of the restless bandit model."
}