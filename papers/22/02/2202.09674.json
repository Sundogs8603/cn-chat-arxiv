{
    "title": "Generalized Optimistic Methods for Convex-Concave Saddle Point Problems. (arXiv:2202.09674v2 [math.OC] UPDATED)",
    "abstract": "The optimistic gradient method has seen increasing popularity for solving convex-concave saddle point problems. To analyze its iteration complexity, a recent work [arXiv:1906.01115] proposed an interesting perspective that interprets this method as an approximation to the proximal point method. In this paper, we follow this approach and distill the underlying idea of optimism to propose a generalized optimistic method, which includes the optimistic gradient method as a special case. Our general framework can handle constrained saddle point problems with composite objective functions and can work with arbitrary norms using Bregman distances. Moreover, we develop a backtracking line search scheme to select the step sizes without knowledge of the smoothness coefficients. We instantiate our method with first-, second- and higher-order oracles and give best-known global iteration complexity bounds. For our first-order method, we show that the averaged iterates converge at a rate of $O(1/N)$",
    "link": "http://arxiv.org/abs/2202.09674",
    "context": "Title: Generalized Optimistic Methods for Convex-Concave Saddle Point Problems. (arXiv:2202.09674v2 [math.OC] UPDATED)\nAbstract: The optimistic gradient method has seen increasing popularity for solving convex-concave saddle point problems. To analyze its iteration complexity, a recent work [arXiv:1906.01115] proposed an interesting perspective that interprets this method as an approximation to the proximal point method. In this paper, we follow this approach and distill the underlying idea of optimism to propose a generalized optimistic method, which includes the optimistic gradient method as a special case. Our general framework can handle constrained saddle point problems with composite objective functions and can work with arbitrary norms using Bregman distances. Moreover, we develop a backtracking line search scheme to select the step sizes without knowledge of the smoothness coefficients. We instantiate our method with first-, second- and higher-order oracles and give best-known global iteration complexity bounds. For our first-order method, we show that the averaged iterates converge at a rate of $O(1/N)$",
    "path": "papers/22/02/2202.09674.json",
    "total_tokens": 926,
    "translated_title": "广义乐观方法用于凸凹鞍点问题的研究。",
    "translated_abstract": "乐观梯度方法在解决凸凹鞍点问题方面越来越受欢迎。为了分析其迭代复杂度，最近的一项工作提出了一个有趣的观点，将这个方法解释为对邻近点法的近似。在本文中，我们遵循这一方法，并将乐观的思想精炼为一个广义乐观方法，其中包括乐观梯度方法作为一种特殊情况。我们的通用框架可以处理具有复合目标函数的约束鞍点问题，并且可以使用Bregman距离处理任意范数。此外，我们开发了一个回溯线搜索方案，以选择步长，而不需要了解平滑系数。我们利用一阶、二阶和更高阶数学规则来实现我们的方法，并给出了已知的全局迭代复杂度界限。对于我们的一阶方法，我们证明了平均迭代在$O(1/N)$的速度下收敛。",
    "tldr": "本文通过将乐观梯度方法解释为对邻近点法的近似，提出了一个广义乐观方法，可以处理具有复合目标函数的约束鞍点问题，并且可以使用Bregman距离处理任意范数。此外，我们还开发了一个回溯线搜索方案，以选择步长，而不需要了解平滑系数。我们的方法在使用一阶、二阶和更高阶数学规则时，给出了已知的全局迭代复杂度界限。"
}