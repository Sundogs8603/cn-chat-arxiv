{
    "title": "Knowledge Distillation with Deep Supervision. (arXiv:2202.07846v2 [cs.LG] UPDATED)",
    "abstract": "Knowledge distillation aims to enhance the performance of a lightweight student model by exploiting the knowledge from a pre-trained cumbersome teacher model. However, in the traditional knowledge distillation, teacher predictions are only used to provide the supervisory signal for the last layer of the student model, which may result in those shallow student layers lacking accurate training guidance in the layer-by-layer back propagation and thus hinders effective knowledge transfer. To address this issue, we propose Deeply-Supervised Knowledge Distillation (DSKD), which fully utilizes class predictions and feature maps of the teacher model to supervise the training of shallow student layers. A loss-based weight allocation strategy is developed in DSKD to adaptively balance the learning process of each shallow layer, so as to further improve the student performance. Extensive experiments on CIFAR-100 and TinyImageNet with various teacher-student models show significantly performance, ",
    "link": "http://arxiv.org/abs/2202.07846",
    "context": "Title: Knowledge Distillation with Deep Supervision. (arXiv:2202.07846v2 [cs.LG] UPDATED)\nAbstract: Knowledge distillation aims to enhance the performance of a lightweight student model by exploiting the knowledge from a pre-trained cumbersome teacher model. However, in the traditional knowledge distillation, teacher predictions are only used to provide the supervisory signal for the last layer of the student model, which may result in those shallow student layers lacking accurate training guidance in the layer-by-layer back propagation and thus hinders effective knowledge transfer. To address this issue, we propose Deeply-Supervised Knowledge Distillation (DSKD), which fully utilizes class predictions and feature maps of the teacher model to supervise the training of shallow student layers. A loss-based weight allocation strategy is developed in DSKD to adaptively balance the learning process of each shallow layer, so as to further improve the student performance. Extensive experiments on CIFAR-100 and TinyImageNet with various teacher-student models show significantly performance, ",
    "path": "papers/22/02/2202.07846.json",
    "total_tokens": 981,
    "translated_title": "深度监督下的知识蒸馏",
    "translated_abstract": "知识蒸馏旨在利用预先训练好的巨型教师模型的知识来提升轻量级学生模型的性能。然而，在传统的知识蒸馏中，教师预测仅用于为学生模型的最后一层提供监督信号，这可能导致这些浅层学生模型在逐层反向传播时缺乏准确的训练指导，从而阻碍了有效的知识转移。为了解决这个问题，我们提出了深度监督下的知识蒸馏（DSKD），该方法充分利用教师模型的类预测和特征图来监督浅层学生层的训练。在DSKD中，开发了一种基于损失的权重分配策略，以自适应地平衡每个浅层的学习过程，从而进一步提高学生的性能。在使用各种教师-学生模型对CIFAR-100和TinyImageNet进行的广泛实验中，表明了显着的性能提高。",
    "tldr": "本文提出了一种基于深度监督的知识蒸馏方法，该方法利用教师模型的类预测和特征图来监督训练学生模型的浅层，通过基于损失的权重分配策略自适应平衡各个浅层的学习过程，取得了显著的性能提升。",
    "en_tdlr": "This paper proposes a deep-supervision-based knowledge distillation method, which utilizes the class predictions and feature maps of the teacher model to supervise the training of shallow student layers. The proposed loss-based weight allocation strategy adaptively balances the learning process of each shallow layer, and achieves significant performance improvements according to extensive experiments on CIFAR-100 and TinyImageNet with various teacher-student models."
}