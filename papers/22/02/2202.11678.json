{
    "title": "Bayesian Model Selection, the Marginal Likelihood, and Generalization. (arXiv:2202.11678v3 [cs.LG] UPDATED)",
    "abstract": "How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We also re",
    "link": "http://arxiv.org/abs/2202.11678",
    "context": "Title: Bayesian Model Selection, the Marginal Likelihood, and Generalization. (arXiv:2202.11678v3 [cs.LG] UPDATED)\nAbstract: How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We also re",
    "path": "papers/22/02/2202.11678.json",
    "total_tokens": 916,
    "translated_title": "贝叶斯模型选择、边际似然和泛化",
    "translated_abstract": "如何比较与观测完全一致的假设之间的区别？边际似然（亦称为贝叶斯证据）作为生成由先验得到观测结果的概率，为解决这个问题提供了一个独特的方法，自动编码奥卡姆剃刀原理。尽管已经观察到边际似然可能过拟合并且对先验假设很敏感，但其在超参数学习和离散模型比较方面的局限性尚未得到彻底研究。本文首先重温了边际似然的吸引人的特点，包括学习约束和假设测试。然后，我们强调了使用边际似然作为泛化的代理存在的概念和实际问题。我们展示了边际似然如何与泛化呈负相关，并对神经架构搜索产生影响，并且在超参数学习中可能导致欠拟合和过拟合。",
    "tldr": "本文回顾和探讨了边缘似然在构造约束和假设测试方面的有用性，强调了使用边缘似然作为泛化的代理的问题，并展示了其如何与神经架构搜索相关，可能导致超参数学习中的欠拟合和过拟合。",
    "en_tdlr": "This paper reviews and discusses the usefulness of marginal likelihood in constructing constraints and hypothesis testing, emphasizes the problem of using it as a proxy for generalization, and shows how it is related to neural architecture search and may lead to underfitting and overfitting in hyperparameter learning."
}