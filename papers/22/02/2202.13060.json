{
    "title": "Graph Attention Retrospective. (arXiv:2202.13060v5 [cs.LG] UPDATED)",
    "abstract": "Graph-based learning is a rapidly growing sub-field of machine learning with applications in social networks, citation networks, and bioinformatics. One of the most popular models is graph attention networks. They were introduced to allow a node to aggregate information from features of neighbor nodes in a non-uniform way, in contrast to simple graph convolution which does not distinguish the neighbors of a node. In this paper, we theoretically study the behaviour of graph attention networks. We prove multiple results on the performance of the graph attention mechanism for the problem of node classification for a contextual stochastic block model. Here, the node features are obtained from a mixture of Gaussians and the edges from a stochastic block model. We show that in an \"easy\" regime, where the distance between the means of the Gaussians is large enough, graph attention is able to distinguish inter-class from intra-class edges. Thus it maintains the weights of important edges and s",
    "link": "http://arxiv.org/abs/2202.13060",
    "context": "Title: Graph Attention Retrospective. (arXiv:2202.13060v5 [cs.LG] UPDATED)\nAbstract: Graph-based learning is a rapidly growing sub-field of machine learning with applications in social networks, citation networks, and bioinformatics. One of the most popular models is graph attention networks. They were introduced to allow a node to aggregate information from features of neighbor nodes in a non-uniform way, in contrast to simple graph convolution which does not distinguish the neighbors of a node. In this paper, we theoretically study the behaviour of graph attention networks. We prove multiple results on the performance of the graph attention mechanism for the problem of node classification for a contextual stochastic block model. Here, the node features are obtained from a mixture of Gaussians and the edges from a stochastic block model. We show that in an \"easy\" regime, where the distance between the means of the Gaussians is large enough, graph attention is able to distinguish inter-class from intra-class edges. Thus it maintains the weights of important edges and s",
    "path": "papers/22/02/2202.13060.json",
    "total_tokens": 879,
    "translated_title": "图注意力回顾",
    "translated_abstract": "基于图的学习是机器学习中快速发展的一个子领域，应用于社交网络、引文网络和生物信息学。其中最流行的模型之一是图注意力网络。它们被引入使节点能够以非统一的方式从邻居节点的特征中聚合信息，与简单的图卷积不同，后者不能区分节点的邻居。本文在理论上研究了图注意网络的行为。我们对上下文随机块模型的节点分类问题证明了图注意机制的多个性能结果。在该模型中，节点特征来自于高斯混合模型，边缘来自于随机块模型。我们证明，在“易”区间内，高斯分布均值之间的距离足够大时，图注意力可以区分跨类和内类边缘。因此，它维护了重要边缘的权重。",
    "tldr": "图注意力网络是一种能够从邻居节点的特征中聚合信息的模型，通过对上下文随机块模型的节点分类问题进行研究，证明了在“易”区间内，它能够区分跨类和内类边缘并维护重要边缘的权重。"
}