{
    "title": "Parallel Successive Learning for Dynamic Distributed Model Training over Heterogeneous Wireless Networks. (arXiv:2202.02947v6 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FedL) has emerged as a popular technique for distributing model training over a set of wireless devices, via iterative local updates (at devices) and global aggregations (at the server). In this paper, we develop parallel successive learning (PSL), which expands the FedL architecture along three dimensions: (i) Network, allowing decentralized cooperation among the devices via device-to-device (D2D) communications. (ii) Heterogeneity, interpreted at three levels: (ii-a) Learning: PSL considers heterogeneous number of stochastic gradient descent iterations with different mini-batch sizes at the devices; (ii-b) Data: PSL presumes a dynamic environment with data arrival and departure, where the distributions of local datasets evolve over time, captured via a new metric for model/concept drift. (ii-c) Device: PSL considers devices with different computation and communication capabilities. (iii) Proximity, where devices have different distances to each other and the acces",
    "link": "http://arxiv.org/abs/2202.02947",
    "context": "Title: Parallel Successive Learning for Dynamic Distributed Model Training over Heterogeneous Wireless Networks. (arXiv:2202.02947v6 [cs.LG] UPDATED)\nAbstract: Federated learning (FedL) has emerged as a popular technique for distributing model training over a set of wireless devices, via iterative local updates (at devices) and global aggregations (at the server). In this paper, we develop parallel successive learning (PSL), which expands the FedL architecture along three dimensions: (i) Network, allowing decentralized cooperation among the devices via device-to-device (D2D) communications. (ii) Heterogeneity, interpreted at three levels: (ii-a) Learning: PSL considers heterogeneous number of stochastic gradient descent iterations with different mini-batch sizes at the devices; (ii-b) Data: PSL presumes a dynamic environment with data arrival and departure, where the distributions of local datasets evolve over time, captured via a new metric for model/concept drift. (ii-c) Device: PSL considers devices with different computation and communication capabilities. (iii) Proximity, where devices have different distances to each other and the acces",
    "path": "papers/22/02/2202.02947.json",
    "total_tokens": 899,
    "translated_title": "并行连续学习用于异构无线网络上动态分布式模型训练",
    "translated_abstract": "联邦学习(FedL)已成为一种将模型训练分布在一组无线设备上的流行技术，通过设备上的迭代本地更新和服务器上的全局聚合。本文提出并行连续学习（PSL），将FedL架构沿三个维度进行扩展：（i）网络，通过设备间通信实现去中心化协作；（ii）异构性，在三个层次进行解释：（ii-a）学习：PSL考虑到设备上具有不同的小批量大小的异构数量的随机梯度下降迭代；（ii-b）数据：PSL假设数据到达和离开的动态环境中，本地数据集的分布随时间演变，通过新的模型/概念漂移度量来捕获；（ii-c）设备：PSL考虑到具有不同计算和通信能力的设备；（iii）接近性，设备之间以及访问点之间具有不同的距离。",
    "tldr": "本文提出了并行连续学习（PSL），通过网络、异构性和接近性三个维度的扩展，实现了在无线设备集合上的动态分布式模型训练。",
    "en_tdlr": "This paper proposes parallel successive learning (PSL) to expand Federated learning (FedL) architecture to network, heterogeneity, and proximity dimensions, achieving dynamic distributed model training over a set of wireless devices."
}