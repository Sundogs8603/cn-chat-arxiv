{
    "title": "A Law of Robustness beyond Isoperimetry. (arXiv:2202.11592v2 [cs.LG] UPDATED)",
    "abstract": "We study the robust interpolation problem of arbitrary data distributions supported on a bounded space and propose a two-fold law of robustness. Robust interpolation refers to the problem of interpolating $n$ noisy training data points in $\\mathbb{R}^d$ by a Lipschitz function. Although this problem has been well understood when the samples are drawn from an isoperimetry distribution, much remains unknown concerning its performance under generic or even the worst-case distributions. We prove a Lipschitzness lower bound $\\Omega(\\sqrt{n/p})$ of the interpolating neural network with $p$ parameters on arbitrary data distributions. With this result, we validate the law of robustness conjecture in prior work by Bubeck, Li, and Nagaraj on two-layer neural networks with polynomial weights. We then extend our result to arbitrary interpolating approximators and prove a Lipschitzness lower bound $\\Omega(n^{1/d})$ for robust interpolation. Our results demonstrate a two-fold law of robustness: i) w",
    "link": "http://arxiv.org/abs/2202.11592",
    "context": "Title: A Law of Robustness beyond Isoperimetry. (arXiv:2202.11592v2 [cs.LG] UPDATED)\nAbstract: We study the robust interpolation problem of arbitrary data distributions supported on a bounded space and propose a two-fold law of robustness. Robust interpolation refers to the problem of interpolating $n$ noisy training data points in $\\mathbb{R}^d$ by a Lipschitz function. Although this problem has been well understood when the samples are drawn from an isoperimetry distribution, much remains unknown concerning its performance under generic or even the worst-case distributions. We prove a Lipschitzness lower bound $\\Omega(\\sqrt{n/p})$ of the interpolating neural network with $p$ parameters on arbitrary data distributions. With this result, we validate the law of robustness conjecture in prior work by Bubeck, Li, and Nagaraj on two-layer neural networks with polynomial weights. We then extend our result to arbitrary interpolating approximators and prove a Lipschitzness lower bound $\\Omega(n^{1/d})$ for robust interpolation. Our results demonstrate a two-fold law of robustness: i) w",
    "path": "papers/22/02/2202.11592.json",
    "total_tokens": 947,
    "translated_title": "超出等周性的稳健性律法",
    "translated_abstract": "本文研究在一个有界空间上支持任意数据分布的稳健插值问题，并提出了一个两层次的稳健性律法。稳健插值是指通过Lipschitz函数插值$\\mathbb{R}^d$中的$n$个嘈杂训练数据点。尽管当样本来自等周性分布时，已经很好地理解了这个问题，但在一般或最坏情况分布下，其性能仍然不清楚。我们证明了对于任意数据分布，插值神经网络的Lipschitz下界为$\\Omega(\\sqrt{n/p})$，其中$p$表示参数数量。通过这个结果，我们验证了Bubeck，Li和Nagaraj在二层神经网络中使用多项式权重提出的稳健性律法猜想。然后，我们将结果扩展到任意插值逼近器，并证明了稳健插值的Lipschitz下界为$\\Omega(n^{1/d})$。我们的结果证明了一个两层次的稳健性律法：",
    "tldr": "本文提出了一种两层次的稳健性律法，研究了在任意数据分布下的稳健插值问题，并证明其Lipschitz下界分别为$\\Omega(\\sqrt{n/p})$和$\\Omega(n^{1/d})$。",
    "en_tdlr": "This paper proposes a two-fold law of robustness beyond isoperimetry, studying the robust interpolation problem of arbitrary data distributions. The paper proves Lipschitzness lower bounds of $\\Omega(\\sqrt{n/p})$ and $\\Omega(n^{1/d})$ for interpolating neural networks with polynomial weights and arbitrary interpolating approximators, respectively."
}