{
    "title": "Confident Neural Network Regression with Bootstrapped Deep Ensembles. (arXiv:2202.10903v2 [stat.ML] UPDATED)",
    "abstract": "With the rise of the popularity and usage of neural networks, trustworthy uncertainty estimation is becoming increasingly essential. One of the most prominent uncertainty estimation methods is Deep Ensembles (Lakshminarayanan et al., 2017) . A classical parametric model has uncertainty in the parameters due to the fact that the data on which the model is build is a random sample. A modern neural network has an additional uncertainty component since the optimization of the network is random. Lakshminarayanan et al. (2017) noted that Deep Ensembles do not incorporate the classical uncertainty induced by the effect of finite data. In this paper, we present a computationally cheap extension of Deep Ensembles for the regression setting, called Bootstrapped Deep Ensembles, that explicitly takes this classical effect of finite data into account using a modified version of the parametric bootstrap. We demonstrate through an experimental study that our method significantly improves upon standar",
    "link": "http://arxiv.org/abs/2202.10903",
    "context": "Title: Confident Neural Network Regression with Bootstrapped Deep Ensembles. (arXiv:2202.10903v2 [stat.ML] UPDATED)\nAbstract: With the rise of the popularity and usage of neural networks, trustworthy uncertainty estimation is becoming increasingly essential. One of the most prominent uncertainty estimation methods is Deep Ensembles (Lakshminarayanan et al., 2017) . A classical parametric model has uncertainty in the parameters due to the fact that the data on which the model is build is a random sample. A modern neural network has an additional uncertainty component since the optimization of the network is random. Lakshminarayanan et al. (2017) noted that Deep Ensembles do not incorporate the classical uncertainty induced by the effect of finite data. In this paper, we present a computationally cheap extension of Deep Ensembles for the regression setting, called Bootstrapped Deep Ensembles, that explicitly takes this classical effect of finite data into account using a modified version of the parametric bootstrap. We demonstrate through an experimental study that our method significantly improves upon standar",
    "path": "papers/22/02/2202.10903.json",
    "total_tokens": 844,
    "translated_title": "自信的神经网络回归与引导深度集成",
    "translated_abstract": "随着神经网络的流行和使用增加，可信的不确定性估计变得越来越重要。其中一个最突出的不确定性估计方法是Deep Ensembles（Lakshminarayanan等人，2017）。一个经典的参数模型由于建模数据是随机样本，因此其参数存在不确定性。现代神经网络由于网络优化的随机性也具有额外的不确定性成分。Lakshminarayanan等人（2017）指出，Deep Ensembles未考虑到由有限数据效应引起的经典不确定性。在本文中，我们提出了一种用于回归设置的计算廉价性扩展Deep Ensembles的方法，称为Bootstrapped Deep Ensembles，它使用改进版的参数自助法明确考虑了有限数据的经典效应。通过实验研究，我们证明了我们的方法在标准方法的基础上明显改进。",
    "tldr": "本文提出了一种称为Bootstrapped Deep Ensembles的新方法，通过引入经典的有限数据效应，明确考虑神经网络回归中的不确定性，并通过实验证明了该方法的显著改进。"
}