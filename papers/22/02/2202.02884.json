{
    "title": "Exploring Self-Attention Mechanisms for Speech Separation. (arXiv:2202.02884v2 [eess.AS] UPDATED)",
    "abstract": "Transformers have enabled impressive improvements in deep learning. They often outperform recurrent and convolutional models in many tasks while taking advantage of parallel processing. Recently, we proposed the SepFormer, which obtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix datasets. This paper studies in-depth Transformers for speech separation. In particular, we extend our previous findings on the SepFormer by providing results on more challenging noisy and noisy-reverberant datasets, such as LibriMix, WHAM!, and WHAMR!. Moreover, we extend our model to perform speech enhancement and provide experimental evidence on denoising and dereverberation tasks. Finally, we investigate, for the first time in speech separation, the use of efficient self-attention mechanisms such as Linformers, Lonformers, and ReFormers. We found that they reduce memory requirements significantly. For example, we show that the Reformer-based attention outperforms the popular Con",
    "link": "http://arxiv.org/abs/2202.02884",
    "context": "Title: Exploring Self-Attention Mechanisms for Speech Separation. (arXiv:2202.02884v2 [eess.AS] UPDATED)\nAbstract: Transformers have enabled impressive improvements in deep learning. They often outperform recurrent and convolutional models in many tasks while taking advantage of parallel processing. Recently, we proposed the SepFormer, which obtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix datasets. This paper studies in-depth Transformers for speech separation. In particular, we extend our previous findings on the SepFormer by providing results on more challenging noisy and noisy-reverberant datasets, such as LibriMix, WHAM!, and WHAMR!. Moreover, we extend our model to perform speech enhancement and provide experimental evidence on denoising and dereverberation tasks. Finally, we investigate, for the first time in speech separation, the use of efficient self-attention mechanisms such as Linformers, Lonformers, and ReFormers. We found that they reduce memory requirements significantly. For example, we show that the Reformer-based attention outperforms the popular Con",
    "path": "papers/22/02/2202.02884.json",
    "total_tokens": 1028,
    "translated_title": "探究自注意力机制用于语音分离",
    "translated_abstract": "Transformers在深度学习中取得了令人瞩目的成就。它们往往在许多任务中优于循环和卷积模型，同时利用并行处理的优势。最近，我们提出了SepFormer，在WSJ0-2/3 Mix数据集上获得了语音分离方面的最新成果。本文深入研究了用于语音分离的Transformers。特别地，我们通过在更具挑战性的含噪和含噪回声的数据集（如LibriMix、WHAM！和WHAMR！）上提供结果，扩展了我们之前关于SepFormer的发现。此外，我们将我们的模型扩展到执行语音增强，并提供了消除噪声和混响任务的实验证据。最后，我们首次在语音分离中研究了使用高效的自注意力机制，如Linformers、Lonformers和ReFormers。我们发现，它们显著减少了内存需求。例如，我们展示了基于Reformer的注意力机制优于流行的ConvS2s注意力机制，同时仅使用约四分之一的内存。",
    "tldr": "本论文探究了Transformers用于语音分离的应用，提出了SepFormer模型并在包括含噪和含噪回声的数据集上得到了最新成果。同时，还做了语音增强方面的研究，并首次将Linformers、Lonformers和ReFormers等高效的自注意力机制应用于语音分离，发现它们可以显著减少内存需求。",
    "en_tdlr": "This paper studies the use of Transformers for speech separation, proposing the SepFormer model and achieving the latest results on datasets including noisy and noisy-reverberant ones. The study also extends to speech enhancement, and introduces for the first time the use of efficient self-attention mechanisms such as Linformers, Lonformers and ReFormers, showing that they significantly reduce memory requirements."
}