{
    "title": "Provably Efficient Causal Model-Based Reinforcement Learning for Systematic Generalization. (arXiv:2202.06545v3 [cs.LG] UPDATED)",
    "abstract": "In the sequential decision making setting, an agent aims to achieve systematic generalization over a large, possibly infinite, set of environments. Such environments are modeled as discrete Markov decision processes with both states and actions represented through a feature vector. The underlying structure of the environments allows the transition dynamics to be factored into two components: one that is environment-specific and another that is shared. Consider a set of environments that share the laws of motion as an example. In this setting, the agent can take a finite amount of reward-free interactions from a subset of these environments. The agent then must be able to approximately solve any planning task defined over any environment in the original set, relying on the above interactions only. Can we design a provably efficient algorithm that achieves this ambitious goal of systematic generalization? In this paper, we give a partially positive answer to this question. First, we prov",
    "link": "http://arxiv.org/abs/2202.06545",
    "context": "Title: Provably Efficient Causal Model-Based Reinforcement Learning for Systematic Generalization. (arXiv:2202.06545v3 [cs.LG] UPDATED)\nAbstract: In the sequential decision making setting, an agent aims to achieve systematic generalization over a large, possibly infinite, set of environments. Such environments are modeled as discrete Markov decision processes with both states and actions represented through a feature vector. The underlying structure of the environments allows the transition dynamics to be factored into two components: one that is environment-specific and another that is shared. Consider a set of environments that share the laws of motion as an example. In this setting, the agent can take a finite amount of reward-free interactions from a subset of these environments. The agent then must be able to approximately solve any planning task defined over any environment in the original set, relying on the above interactions only. Can we design a provably efficient algorithm that achieves this ambitious goal of systematic generalization? In this paper, we give a partially positive answer to this question. First, we prov",
    "path": "papers/22/02/2202.06545.json",
    "total_tokens": 1189,
    "translated_title": "系统化泛化的可证明有效因果模型强化学习",
    "translated_abstract": "在顺序决策环境中，智能体旨在实现对大规模、可能无限的环境的系统化泛化。这些环境被建模为离散的马尔可夫决策过程，其中状态和动作都用特征向量表示。环境的底层结构允许将转移动态分解为两个组成部分：一个是特定于环境的，另一个是共享的。例如，考虑一组共享运动法则的环境。在这种情况下，智能体可以从这些环境的子集中进行有限的无回报交互。然后，智能体必须能够仅依靠上述交互，近似地解决设置在任何环境中的任何规划任务。我们是否可以设计一个可证明有效的算法，实现这个雄心勃勃的系统化泛化目标呢？在本文中，我们部分地回答了这个问题。首先，我们可以对这类问题进行优化的样本复杂度交换。其次，我们提出了一种新的因果模型强化学习算法，称为通过因果推理实现系统化泛化（SGCI），该算法利用环境的共享结构来最大化系统化泛化。第三，我们证明了我们的算法在学习运动定律共享的环境系列时实现了优化的样本复杂度交换，并进一步展示了算法在一系列具有挑战性的基准问题上的有效性，这些问题对于最先进的强化学习算法而言具有挑战性。",
    "tldr": "本文提出了一种新的因果模型强化学习算法，利用环境的共享结构来最大化系统化泛化，在运动定律共享的环境系列中实现了优化的样本复杂度交换，并在基准问题上展示了有效性和挑战性。",
    "en_tdlr": "This paper proposes a novel causal model-based reinforcement learning algorithm that leverages the shared structure of environments to maximize systematic generalization, achieving optimal sample complexity trade-off in a family of environments where the laws of motion are shared, and demonstrates effectiveness and challenges on benchmark problems."
}