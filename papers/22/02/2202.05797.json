{
    "title": "Distributionally Robust Data Join. (arXiv:2202.05797v2 [cs.LG] UPDATED)",
    "abstract": "Suppose we are given two datasets: a labeled dataset and unlabeled dataset which also has additional auxiliary features not present in the first dataset. What is the most principled way to use these datasets together to construct a predictor?  The answer should depend upon whether these datasets are generated by the same or different distributions over their mutual feature sets, and how similar the test distribution will be to either of those distributions. In many applications, the two datasets will likely follow different distributions, but both may be close to the test distribution. We introduce the problem of building a predictor which minimizes the maximum loss over all probability distributions over the original features, auxiliary features, and binary labels, whose Wasserstein distance is $r_1$ away from the empirical distribution over the labeled dataset and $r_2$ away from that of the unlabeled dataset. This can be thought of as a generalization of distributionally robust opti",
    "link": "http://arxiv.org/abs/2202.05797",
    "context": "Title: Distributionally Robust Data Join. (arXiv:2202.05797v2 [cs.LG] UPDATED)\nAbstract: Suppose we are given two datasets: a labeled dataset and unlabeled dataset which also has additional auxiliary features not present in the first dataset. What is the most principled way to use these datasets together to construct a predictor?  The answer should depend upon whether these datasets are generated by the same or different distributions over their mutual feature sets, and how similar the test distribution will be to either of those distributions. In many applications, the two datasets will likely follow different distributions, but both may be close to the test distribution. We introduce the problem of building a predictor which minimizes the maximum loss over all probability distributions over the original features, auxiliary features, and binary labels, whose Wasserstein distance is $r_1$ away from the empirical distribution over the labeled dataset and $r_2$ away from that of the unlabeled dataset. This can be thought of as a generalization of distributionally robust opti",
    "path": "papers/22/02/2202.05797.json",
    "total_tokens": 821,
    "translated_title": "分布式稳健数据合并",
    "translated_abstract": "假设我们有两个数据集：一个带标签的数据集和一个带有额外辅助特征的未标记数据集。最合理的使用这些数据集构建预测器的方法应该取决于这些数据集是由相同的分布还是不同的分布生成的，以及测试分布与这些分布中的任一分布有多相似。在许多应用中，两个数据集可能遵循不同的分布，但两者都可能接近测试分布。我们介绍了建立一种预测器的问题，该预测器通过将其Wasserstein距离分别为$r_1$和$r_2$的概率分布最大化地减少所有原始特征、辅助特征和二元标签上的最大损失。这可以看作是分布式稳健优化的推广，用于处理数据合并的情况。",
    "tldr": "本论文提出了一个解决数据合并问题的方法，该方法通过最大化所有概率分布上的最大损失，来构建分布式稳健预测器。"
}