{
    "title": "Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks. (arXiv:2202.00293v4 [stat.ML] UPDATED)",
    "abstract": "Despite the non-convex optimization landscape, over-parametrized shallow networks are able to achieve global convergence under gradient descent. The picture can be radically different for narrow networks, which tend to get stuck in badly-generalizing local minima. Here we investigate the cross-over between these two regimes in the high-dimensional setting, and in particular investigate the connection between the so-called mean-field/hydrodynamic regime and the seminal approach of Saad & Solla. Focusing on the case of Gaussian data, we study the interplay between the learning rate, the time scale, and the number of hidden units in the high-dimensional dynamics of stochastic gradient descent (SGD). Our work builds on a deterministic description of SGD in high-dimensions from statistical physics, which we extend and for which we provide rigorous convergence rates.",
    "link": "http://arxiv.org/abs/2202.00293",
    "context": "Title: Phase diagram of Stochastic Gradient Descent in high-dimensional two-layer neural networks. (arXiv:2202.00293v4 [stat.ML] UPDATED)\nAbstract: Despite the non-convex optimization landscape, over-parametrized shallow networks are able to achieve global convergence under gradient descent. The picture can be radically different for narrow networks, which tend to get stuck in badly-generalizing local minima. Here we investigate the cross-over between these two regimes in the high-dimensional setting, and in particular investigate the connection between the so-called mean-field/hydrodynamic regime and the seminal approach of Saad & Solla. Focusing on the case of Gaussian data, we study the interplay between the learning rate, the time scale, and the number of hidden units in the high-dimensional dynamics of stochastic gradient descent (SGD). Our work builds on a deterministic description of SGD in high-dimensions from statistical physics, which we extend and for which we provide rigorous convergence rates.",
    "path": "papers/22/02/2202.00293.json",
    "total_tokens": 904,
    "translated_title": "高维两层神经网络中随机梯度下降的相图",
    "translated_abstract": "虽然非凸优化景观，在过参数化的浅层网络中，梯度下降能够实现全局收敛。但情况对于窄网络却可能完全不同，它们倾向于被困在具有糟糕泛化的局部最小值。在这里，我们研究这两个范畴之间的交界处，特别是我们调查了所谓的平均场/流体力学范畴与Saad和Solla 的开创性方法之间的联系。我们重点研究了在高维随机梯度下降的动态中，学习率、时间尺度和隐含层数量之间的相互作用，以高斯数据为例。我们的工作基于从统计物理学中对于高维度随机梯度下降的确定性描述，我们加以拓展并提供了严密的收敛速率证明。",
    "tldr": "本文研究了高维两层神经网络中随机梯度下降的相图，探究了窄网络和过参数化浅层网络之间的交界处，并研究了三个方面变量之间的相互作用，工作建立在统计物理的框架下。",
    "en_tdlr": "This paper investigates the phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks, particularly the crossover between the regimes of narrow networks and over-parametrized shallow networks. The interplay between the learning rate, time scale, and number of hidden units is studied for Gaussian data, building on a deterministic description of SGD in high-dimensions from statistical physics, for which rigorous convergence rates are provided."
}