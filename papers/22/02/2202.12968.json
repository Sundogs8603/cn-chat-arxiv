{
    "title": "Does Label Differential Privacy Prevent Label Inference Attacks?. (arXiv:2202.12968v2 [cs.LG] UPDATED)",
    "abstract": "Label differential privacy (label-DP) is a popular framework for training private ML models on datasets with public features and sensitive private labels. Despite its rigorous privacy guarantee, it has been observed that in practice label-DP does not preclude label inference attacks (LIAs): Models trained with label-DP can be evaluated on the public training features to recover, with high accuracy, the very private labels that it was designed to protect. In this work, we argue that this phenomenon is not paradoxical and that label-DP is designed to limit the advantage of an LIA adversary compared to predicting training labels using the Bayes classifier. At label-DP $\\epsilon=0$ this advantage is zero, hence the optimal attack is to predict according to the Bayes classifier and is independent of the training labels. Our bound shows the semantic protection conferred by label-DP and gives guidelines on how to choose $\\varepsilon$ to limit the threat of LIAs below a certain level. Finally,",
    "link": "http://arxiv.org/abs/2202.12968",
    "context": "Title: Does Label Differential Privacy Prevent Label Inference Attacks?. (arXiv:2202.12968v2 [cs.LG] UPDATED)\nAbstract: Label differential privacy (label-DP) is a popular framework for training private ML models on datasets with public features and sensitive private labels. Despite its rigorous privacy guarantee, it has been observed that in practice label-DP does not preclude label inference attacks (LIAs): Models trained with label-DP can be evaluated on the public training features to recover, with high accuracy, the very private labels that it was designed to protect. In this work, we argue that this phenomenon is not paradoxical and that label-DP is designed to limit the advantage of an LIA adversary compared to predicting training labels using the Bayes classifier. At label-DP $\\epsilon=0$ this advantage is zero, hence the optimal attack is to predict according to the Bayes classifier and is independent of the training labels. Our bound shows the semantic protection conferred by label-DP and gives guidelines on how to choose $\\varepsilon$ to limit the threat of LIAs below a certain level. Finally,",
    "path": "papers/22/02/2202.12968.json",
    "total_tokens": 906,
    "translated_title": "标签差分隐私是否能够防止标签推断攻击？",
    "translated_abstract": "标签差分隐私（label-DP）是一种流行的框架，用于在具有公共特征和敏感私有标签的数据集上训练私有机器学习模型。尽管它具有严格的隐私保证，但实践中观察到，label-DP并不能防止标签推断攻击（LIAs）：使用label-DP训练的模型可以在公共训练特征上进行评估，以高精度地恢复其旨在保护的非常私人标签。在本研究中，我们认为这种现象并不矛盾，并且label-DP旨在限制LIA对手的优势，以便与使用贝叶斯分类器预测训练标签进行比较。在label-DP $\\epsilon=0$时，该优势为零，因此最佳攻击是根据贝叶斯分类器进行预测，并且与训练标签无关。我们的界限显示了label-DP提供的语义保护，并提供了如何选择 $\\varepsilon$ 以将LIAs的威胁限制在某个水平以下的指南。最后，",
    "tldr": "标签差分隐私不能完全防止标签推断攻击（LIAs），但可以限制LIAs对手的优势和语义保护。",
    "en_tdlr": "Label differential privacy (label-DP) cannot completely prevent label inference attacks (LIAs), but it can limit the advantage of LIAs attackers and provide semantic protection."
}