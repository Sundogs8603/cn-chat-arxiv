{
    "title": "Meta-Learning for Simple Regret Minimization. (arXiv:2202.12888v2 [cs.LG] UPDATED)",
    "abstract": "We develop a meta-learning framework for simple regret minimization in bandits. In this framework, a learning agent interacts with a sequence of bandit tasks, which are sampled i.i.d.\\ from an unknown prior distribution, and learns its meta-parameters to perform better on future tasks. We propose the first Bayesian and frequentist meta-learning algorithms for this setting. The Bayesian algorithm has access to a prior distribution over the meta-parameters and its meta simple regret over $m$ bandit tasks with horizon $n$ is mere $\\tilde{O}(m / \\sqrt{n})$. On the other hand, the meta simple regret of the frequentist algorithm is $\\tilde{O}(\\sqrt{m} n + m/ \\sqrt{n})$. While its regret is worse, the frequentist algorithm is more general because it does not need a prior distribution over the meta-parameters. It can also be analyzed in more settings. We instantiate our algorithms for several classes of bandit problems. Our algorithms are general and we complement our theory by evaluating them",
    "link": "http://arxiv.org/abs/2202.12888",
    "context": "Title: Meta-Learning for Simple Regret Minimization. (arXiv:2202.12888v2 [cs.LG] UPDATED)\nAbstract: We develop a meta-learning framework for simple regret minimization in bandits. In this framework, a learning agent interacts with a sequence of bandit tasks, which are sampled i.i.d.\\ from an unknown prior distribution, and learns its meta-parameters to perform better on future tasks. We propose the first Bayesian and frequentist meta-learning algorithms for this setting. The Bayesian algorithm has access to a prior distribution over the meta-parameters and its meta simple regret over $m$ bandit tasks with horizon $n$ is mere $\\tilde{O}(m / \\sqrt{n})$. On the other hand, the meta simple regret of the frequentist algorithm is $\\tilde{O}(\\sqrt{m} n + m/ \\sqrt{n})$. While its regret is worse, the frequentist algorithm is more general because it does not need a prior distribution over the meta-parameters. It can also be analyzed in more settings. We instantiate our algorithms for several classes of bandit problems. Our algorithms are general and we complement our theory by evaluating them",
    "path": "papers/22/02/2202.12888.json",
    "total_tokens": 1026,
    "translated_title": "用于简单遗憾最小化的元学习",
    "translated_abstract": "我们提出了一个用于在赌博机中简单遗憾最小化的元学习框架。在这个框架中，学习代理与一系列赌博机任务进行交互，这些任务是从一个未知的先验分布中独立采样的，并学习其元参数以在未来任务中表现更好。我们提出了这个设置的第一个贝叶斯和频率派元学习算法。贝叶斯算法可以访问元参数的先验分布，并且其在$m$个赌博机任务中，时间界为$n$的元简单遗憾仅为$\\tilde{O}(m / \\sqrt{n})$。另一方面，频率派算法的元简单遗憾为$\\tilde{O}(\\sqrt{m} n + m/ \\sqrt{n})$。尽管遗憾更大，但频率派算法更通用，因为它不需要元参数的先验分布，并且可以在更多的设置中进行分析。我们通过将算法应用于几类赌博机问题来验证我们的理论。",
    "tldr": "本论文提出了用于在赌博机中进行简单遗憾最小化的元学习框架，并提出了首个贝叶斯和频率派元学习算法。贝叶斯算法具有先验分布并且具有较小的元简单遗憾，而频率派算法更通用且可以在更多的设置中进行分析。通过将算法应用于不同的赌博机问题，我们验证了理论的有效性。",
    "en_tdlr": "This paper presents a meta-learning framework for simple regret minimization in bandits and proposes the first Bayesian and frequentist meta-learning algorithms for this setting. The Bayesian algorithm with prior distribution achieves a lower meta simple regret, while the frequentist algorithm is more general and can be analyzed in more settings. The effectiveness of the theory is validated through the application of the algorithms to various bandit problems."
}