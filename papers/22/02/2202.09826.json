{
    "title": "Continual Learning Beyond a Single Model. (arXiv:2202.09826v3 [cs.LG] UPDATED)",
    "abstract": "A growing body of research in continual learning focuses on the catastrophic forgetting problem. While many attempts have been made to alleviate this problem, the majority of the methods assume a single model in the continual learning setup. In this work, we question this assumption and show that employing ensemble models can be a simple yet effective method to improve continual performance. However, ensembles' training and inference costs can increase significantly as the number of models grows. Motivated by this limitation, we study different ensemble models to understand their benefits and drawbacks in continual learning scenarios. Finally, to overcome the high compute cost of ensembles, we leverage recent advances in neural network subspace to propose a computationally cheap algorithm with similar runtime to a single model yet enjoying the performance benefits of ensembles.",
    "link": "http://arxiv.org/abs/2202.09826",
    "context": "Title: Continual Learning Beyond a Single Model. (arXiv:2202.09826v3 [cs.LG] UPDATED)\nAbstract: A growing body of research in continual learning focuses on the catastrophic forgetting problem. While many attempts have been made to alleviate this problem, the majority of the methods assume a single model in the continual learning setup. In this work, we question this assumption and show that employing ensemble models can be a simple yet effective method to improve continual performance. However, ensembles' training and inference costs can increase significantly as the number of models grows. Motivated by this limitation, we study different ensemble models to understand their benefits and drawbacks in continual learning scenarios. Finally, to overcome the high compute cost of ensembles, we leverage recent advances in neural network subspace to propose a computationally cheap algorithm with similar runtime to a single model yet enjoying the performance benefits of ensembles.",
    "path": "papers/22/02/2202.09826.json",
    "total_tokens": 883,
    "translated_title": "超越单一模型的持续学习",
    "translated_abstract": "在持续学习中，越来越多的研究集中在灾难性遗忘问题上。虽然有很多方法试图缓解这个问题，但大多数方法都假设在持续学习中只有一个模型。在这项工作中，我们质疑这个假设，并展示了使用集成模型可以是一个简单而有效的方法来提高持续性能。然而，随着模型数量的增加，集成的训练和推断成本可能会显著增加。受到这个限制的启发，我们研究了不同的集成模型，以了解它们在持续学习场景中的优点和缺点。最后，为了克服集成的高计算成本，我们利用神经网络子空间的最新进展，提出了一种计算成本较低的算法，其运行时间与单一模型相当，但享有集成的性能优势。",
    "tldr": "本论文研究了超越单一模型的持续学习。通过使用集成模型，能够改善持续性能，但随着模型数量增加，计算成本也会增加。为了解决这个问题，提出了一种计算成本较低的算法，能够在运行时间上与单一模型相当，并享有集成的性能优势。"
}