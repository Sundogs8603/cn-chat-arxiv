{
    "title": "I-Tuning: Tuning Frozen Language Models with Image for Lightweight Image Captioning. (arXiv:2202.06574v3 [cs.CL] UPDATED)",
    "abstract": "Image Captioning is a traditional vision-and-language task that aims to generate the language description of an image. Recent studies focus on scaling up the model size and the number of training data, which significantly increase the cost of model training. Different to these heavy-cost models, we introduce a lightweight image captioning framework (I-Tuning), which contains a small number of trainable parameters. We design a novel I-Tuning cross-attention module to connect the non-trainable pre-trained language decoder GPT2 and vision encoder CLIP-ViT. Since most parameters are not required to be updated during training, our framework is lightweight and fast. Experimental results conducted on three image captioning benchmarks reveal that our framework achieves comparable or better performance than the large-scale baseline systems. But our models contain up to 10 times fewer trainable parameters and require much fewer data for training compared with state-of-the-art baselines.",
    "link": "http://arxiv.org/abs/2202.06574",
    "total_tokens": 1009,
    "translated_title": "I-Tuning: 利用图像对冻结语言模型进行轻量级图像字幕生成的调整",
    "translated_abstract": "图像字幕生成是一项传统的视觉与语言任务，旨在生成图像的语言描述。最近的研究集中在扩大模型规模和训练数据量，这显著增加了模型训练的成本。与这些高成本模型不同，我们引入了一个轻量级的图像字幕生成框架（I-Tuning），其中包含少量可训练参数。我们设计了一种新颖的I-Tuning交叉注意力模块，将不可训练的预训练语言解码器GPT2和视觉编码器CLIP-ViT连接起来。由于大多数参数在训练期间不需要更新，因此我们的框架轻巧快速。在三个图像字幕生成基准测试上进行的实验结果表明，我们的框架实现了与大规模基线系统相当或更好的性能。但与最先进的基线相比，我们的模型包含多达10倍少的可训练参数，并且需要更少的数据进行训练。",
    "tldr": "本文提出了一种轻量级的图像字幕生成框架（I-Tuning），通过设计新颖的交叉注意力模块将不可训练的预训练语言解码器和视觉编码器连接起来，使得模型包含的可训练参数少，训练速度快，同时在三个图像字幕生成基准测试上实现了与大规模基线系统相当或更好的性能，但需要的可训练参数和训练数据量都少得多。",
    "en_tldr": "This paper proposes a lightweight image captioning framework (I-Tuning) that connects the non-trainable pre-trained language decoder GPT2 and vision encoder CLIP-ViT with a novel I-Tuning cross-attention module. The framework contains fewer trainable parameters and achieves comparable or better performance than large-scale baseline systems on three image captioning benchmarks, while requiring much fewer trainable parameters and training data compared with state-of-the-art baselines."
}