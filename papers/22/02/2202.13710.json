{
    "title": "Best of Many Worlds Guarantees for Online Learning with Knapsacks. (arXiv:2202.13710v2 [cs.LG] UPDATED)",
    "abstract": "We study online learning problems in which a decision maker wants to maximize their expected reward without violating a finite set of $m$ resource constraints. By casting the learning process over a suitably defined space of strategy mixtures, we recover strong duality on a Lagrangian relaxation of the underlying optimization problem, even for general settings with non-convex reward and resource-consumption functions. Then, we provide the first best-of-many-worlds type framework for this setting, with no-regret guarantees under stochastic, adversarial, and non-stationary inputs. Our framework yields the same regret guarantees of prior work in the stochastic case. On the other hand, when budgets grow at least linearly in the time horizon, it allows us to provide a constant competitive ratio in the adversarial case, which improves over the best known upper bound bound of $O(\\log m \\log T)$. Moreover, our framework allows the decision maker to handle non-convex reward and cost functions. ",
    "link": "http://arxiv.org/abs/2202.13710",
    "raw_ret": "{\n    \"translated_title\": \"背包在线学习中的多种最佳保证\",\n    \"translated_abstract\": \"本文研究在线学习问题，其中决策者希望在不违反有限的m个资源约束的情况下最大化他们的预期收益。通过将学习过程投射到适当定义的策略混合空间上，我们恢复了基本优化问题的拉格朗日松弛上的强对偶性，即使在具有非凸奖励和资源消耗函数的一般情况下也是如此。然后，我们为这种情况提供了第一个多种世界类型的最佳框架，在随机，对抗性和非平稳输入下具有无悔保证。我们的框架在随机情况下产生了先前工作的相同后悔保证。另一方面，当预算至少线性增长时，它允许我们在对抗性情况下提供恒定的竞争比率，这改进了最好已知的上界$ O(\\log m \\log T) $。此外，我们的框架允许决策者处理非凸奖励和成本函数。\",\n    \"tldr\": \"本文提出了一种在线学习的框架，其中决策者希望在不违反一组资源约束的情况下最大化其预期收益。该框架适用于多种不同的输入类型，并允许决策者处理非凸奖励和成本函数。\"\n}<|im_sep|>",
    "total_tokens": 922
}