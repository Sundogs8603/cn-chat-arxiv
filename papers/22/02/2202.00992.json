{
    "title": "Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions. (arXiv:2202.00992v2 [math.OC] UPDATED)",
    "abstract": "Performance of optimization on quadratic problems sensitively depends on the low-lying part of the spectrum. For large (effectively infinite-dimensional) problems, this part of the spectrum can often be naturally represented or approximated by power law distributions, resulting in power law convergence rates for iterative solutions of these problems by gradient-based algorithms. In this paper, we propose a new spectral condition providing tighter upper bounds for problems with power law optimization trajectories. We use this condition to build a complete picture of upper and lower bounds for a wide range of optimization algorithms -- Gradient Descent, Steepest Descent, Heavy Ball, and Conjugate Gradients -- with an emphasis on the underlying schedules of learning rate and momentum. In particular, we demonstrate how an optimally accelerated method, its schedule, and convergence upper bound can be obtained in a unified manner for a given shape of the spectrum. Also, we provide first proo",
    "link": "http://arxiv.org/abs/2202.00992",
    "context": "Title: Tight Convergence Rate Bounds for Optimization Under Power Law Spectral Conditions. (arXiv:2202.00992v2 [math.OC] UPDATED)\nAbstract: Performance of optimization on quadratic problems sensitively depends on the low-lying part of the spectrum. For large (effectively infinite-dimensional) problems, this part of the spectrum can often be naturally represented or approximated by power law distributions, resulting in power law convergence rates for iterative solutions of these problems by gradient-based algorithms. In this paper, we propose a new spectral condition providing tighter upper bounds for problems with power law optimization trajectories. We use this condition to build a complete picture of upper and lower bounds for a wide range of optimization algorithms -- Gradient Descent, Steepest Descent, Heavy Ball, and Conjugate Gradients -- with an emphasis on the underlying schedules of learning rate and momentum. In particular, we demonstrate how an optimally accelerated method, its schedule, and convergence upper bound can be obtained in a unified manner for a given shape of the spectrum. Also, we provide first proo",
    "path": "papers/22/02/2202.00992.json",
    "total_tokens": 848,
    "translated_title": "基于幂律谱条件下的优化收敛率紧密上界",
    "translated_abstract": "对于二次问题的优化性能，取决于谱的低能部分。对于大型（有效无限维）问题，这部分谱通常可以通过幂律分布自然表示或近似，导致梯度算法的迭代解表现出幂律收敛率。本文提出了一种新的谱条件，用于提供具有幂律优化轨迹的问题的更紧密上界。我们利用这个条件来建立一张广泛优化算法的上下界完整图像——梯度下降、最陡下降、重球、共轭梯度——并强调了学习率和动量的基本计划。特别的，我们演示了如何统一获得最优加速方法及其计划和收敛上界，对于给定谱形状。此外，我们还提供了对于首个证明。",
    "tldr": "本文提出了一种新的谱条件，用于提供具有幂律优化轨迹的问题的更紧密上界，演示了如何统一获得最优加速方法及其计划和收敛上界。",
    "en_tdlr": "This paper proposes a new spectral condition for tighter upper bounds of problems with power law optimization trajectories. It demonstrates how to unifyly obtain the optimally accelerated method, its schedule, and convergence upper bound for a given spectrum shape."
}