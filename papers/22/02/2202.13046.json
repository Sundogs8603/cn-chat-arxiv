{
    "title": "Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced Local Value Functions",
    "abstract": "arXiv:2202.13046v4 Announce Type: replace-cross  Abstract: Achieving distributed reinforcement learning (RL) for large-scale cooperative multi-agent systems (MASs) is challenging because: (i) each agent has access to only limited information; (ii) issues on convergence or computational complexity emerge due to the curse of dimensionality. In this paper, we propose a general computationally efficient distributed framework for cooperative multi-agent reinforcement learning (MARL) by utilizing the structures of graphs involved in this problem. We introduce three coupling graphs describing three types of inter-agent couplings in MARL, namely, the state graph, the observation graph and the reward graph. By further considering a communication graph, we propose two distributed RL approaches based on local value-functions derived from the coupling graphs. The first approach is able to reduce sample complexity significantly under specific conditions on the aforementioned four graphs. The second",
    "link": "https://arxiv.org/abs/2202.13046",
    "context": "Title: Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced Local Value Functions\nAbstract: arXiv:2202.13046v4 Announce Type: replace-cross  Abstract: Achieving distributed reinforcement learning (RL) for large-scale cooperative multi-agent systems (MASs) is challenging because: (i) each agent has access to only limited information; (ii) issues on convergence or computational complexity emerge due to the curse of dimensionality. In this paper, we propose a general computationally efficient distributed framework for cooperative multi-agent reinforcement learning (MARL) by utilizing the structures of graphs involved in this problem. We introduce three coupling graphs describing three types of inter-agent couplings in MARL, namely, the state graph, the observation graph and the reward graph. By further considering a communication graph, we propose two distributed RL approaches based on local value-functions derived from the coupling graphs. The first approach is able to reduce sample complexity significantly under specific conditions on the aforementioned four graphs. The second",
    "path": "papers/22/02/2202.13046.json",
    "total_tokens": 837,
    "translated_title": "基于图诱导的局部值函数的分布式多智能体强化学习",
    "translated_abstract": "实现大规模协作多智能体系统的分布式强化学习(RL)具有挑战性，因为：(i)每个智能体只能访问有限的信息；(ii)由于维度诅咒，会出现收敛或计算复杂性问题。本文提出了一种通用的计算高效的协作多智能体强化学习(MARL)分布式框架，通过利用该问题中涉及的图结构。我们引入了描述MARL中三种类型智能体耦合的三个耦合图，分别是状态图、观测图和奖励图。通过进一步考虑通信图，我们提出了两种基于耦合图中派生的局部值函数的分布式RL方法。第一种方法在前述四个图上的特定条件下可以显著降低样本复杂性。第二种",
    "tldr": "通过利用图结构，本文提出了一种通用计算高效的分布式框架，基于局部值函数的分布式RL方法在协作多智能体强化学习中取得了显著的样本复杂性降低。",
    "en_tdlr": "This paper proposes a general computationally efficient distributed framework based on utilizing the structures of graphs, and the distributed RL approach using local value functions has significantly reduced sample complexity in cooperative multi-agent reinforcement learning."
}