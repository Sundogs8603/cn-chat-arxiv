{
    "title": "Particle Transformer for Jet Tagging. (arXiv:2202.03772v3 [hep-ph] UPDATED)",
    "abstract": "Jet tagging is a critical yet challenging classification task in particle physics. While deep learning has transformed jet tagging and significantly improved performance, the lack of a large-scale public dataset impedes further enhancement. In this work, we present JetClass, a new comprehensive dataset for jet tagging. The JetClass dataset consists of 100 M jets, about two orders of magnitude larger than existing public datasets. A total of 10 types of jets are simulated, including several types unexplored for tagging so far. Based on the large dataset, we propose a new Transformer-based architecture for jet tagging, called Particle Transformer (ParT). By incorporating pairwise particle interactions in the attention mechanism, ParT achieves higher tagging performance than a plain Transformer and surpasses the previous state-of-the-art, ParticleNet, by a large margin. The pre-trained ParT models, once fine-tuned, also substantially enhance the performance on two widely adopted jet taggi",
    "link": "http://arxiv.org/abs/2202.03772",
    "context": "Title: Particle Transformer for Jet Tagging. (arXiv:2202.03772v3 [hep-ph] UPDATED)\nAbstract: Jet tagging is a critical yet challenging classification task in particle physics. While deep learning has transformed jet tagging and significantly improved performance, the lack of a large-scale public dataset impedes further enhancement. In this work, we present JetClass, a new comprehensive dataset for jet tagging. The JetClass dataset consists of 100 M jets, about two orders of magnitude larger than existing public datasets. A total of 10 types of jets are simulated, including several types unexplored for tagging so far. Based on the large dataset, we propose a new Transformer-based architecture for jet tagging, called Particle Transformer (ParT). By incorporating pairwise particle interactions in the attention mechanism, ParT achieves higher tagging performance than a plain Transformer and surpasses the previous state-of-the-art, ParticleNet, by a large margin. The pre-trained ParT models, once fine-tuned, also substantially enhance the performance on two widely adopted jet taggi",
    "path": "papers/22/02/2202.03772.json",
    "total_tokens": 959,
    "translated_title": "粒子变压器用于喷注标记",
    "translated_abstract": "喷注标记是粒子物理学中一个关键而具有挑战性的分类任务。虽然深度学习已经改变了喷注标记，并显著提高了性能，但缺乏一个大规模的公共数据集阻碍了进一步的提升。在这项工作中，我们提出了JetClass，一个用于喷注标记的新的全面数据集。JetClass数据集包含1亿个喷注，比现有公共数据集大两个数量级。总共模拟了10种类型的喷注，包括至今为止尚未用于标记的几种类型。基于大型数据集，我们提出了一种新的基于变压器的用于喷注标记的架构，称为粒子变压器（ParT）。通过在注意力机制中引入配对粒子交互，ParT实现了比简单变压器更高的标记性能，并大幅超过了以前的最先进技术ParticleNet。预训练的ParT模型在微调后还可以大幅提升广泛应用的两种喷注标记的性能。",
    "tldr": "本研究提出了用于喷注标记的粒子变压器，并使用新的全面数据集JetClass进行训练。粒子变压器通过在注意力机制中引入粒子间的交互关系，实现了比传统变压器更高的标记性能，并在两种常用的喷注标记任务上取得了显著的性能提升。",
    "en_tdlr": "This paper introduces a Particle Transformer for jet tagging and trains it using a comprehensive dataset called JetClass. The Particle Transformer incorporates particle interactions in the attention mechanism, resulting in higher tagging performance compared to traditional Transformers. It outperforms the previous state-of-the-art method, ParticleNet, and substantially enhances performance on two widely adopted jet tagging tasks."
}