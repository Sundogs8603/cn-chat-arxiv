{
    "title": "Fabricated Flips: Poisoning Federated Learning without Data. (arXiv:2202.05877v2 [cs.CR] UPDATED)",
    "abstract": "Attacks on Federated Learning (FL) can severely reduce the quality of the generated models and limit the usefulness of this emerging learning paradigm that enables on-premise decentralized learning. However, existing untargeted attacks are not practical for many scenarios as they assume that i) the attacker knows every update of benign clients, or ii) the attacker has a large dataset to locally train updates imitating benign parties. In this paper, we propose a data-free untargeted attack (DFA) that synthesizes malicious data to craft adversarial models without eavesdropping on the transmission of benign clients at all or requiring a large quantity of task-specific training data. We design two variants of DFA, namely DFA-R and DFA-G, which differ in how they trade off stealthiness and effectiveness. Specifically, DFA-R iteratively optimizes a malicious data layer to minimize the prediction confidence of all outputs of the global model, whereas DFA-G interactively trains a malicious dat",
    "link": "http://arxiv.org/abs/2202.05877",
    "context": "Title: Fabricated Flips: Poisoning Federated Learning without Data. (arXiv:2202.05877v2 [cs.CR] UPDATED)\nAbstract: Attacks on Federated Learning (FL) can severely reduce the quality of the generated models and limit the usefulness of this emerging learning paradigm that enables on-premise decentralized learning. However, existing untargeted attacks are not practical for many scenarios as they assume that i) the attacker knows every update of benign clients, or ii) the attacker has a large dataset to locally train updates imitating benign parties. In this paper, we propose a data-free untargeted attack (DFA) that synthesizes malicious data to craft adversarial models without eavesdropping on the transmission of benign clients at all or requiring a large quantity of task-specific training data. We design two variants of DFA, namely DFA-R and DFA-G, which differ in how they trade off stealthiness and effectiveness. Specifically, DFA-R iteratively optimizes a malicious data layer to minimize the prediction confidence of all outputs of the global model, whereas DFA-G interactively trains a malicious dat",
    "path": "papers/22/02/2202.05877.json",
    "total_tokens": 990,
    "translated_title": "《虚构的翻转：无数据的中毒联邦学习》",
    "translated_abstract": "对联邦学习的攻击可以严重降低生成模型的质量，限制这种新兴学习模式的实用性，该模式实现了本地化的分散式学习。然而，现有的非定向攻击对许多场景来说都不实际，因为它们假设攻击者知道良性客户端的每个更新，或者攻击者拥有大量的本地训练数据来模仿良性参与方的更新。在本文中，我们提出了一种无数据非定向攻击（DFA），它通过合成恶意数据来制造对抗模型，而无需窃听良性客户端的传输，也无需大量的任务特定训练数据。我们设计了两种DFA的变体，即DFA-R和DFA-G，它们在如何权衡隐蔽性和效果方面有所不同。具体来说，DFA-R通过迭代优化一个恶意数据层来最小化全局模型所有输出的预测置信度，而DFA-G则通过交互式训练恶意数据来实现。",
    "tldr": "本文介绍了一种名为无数据非定向攻击（DFA）的新方法，它通过合成恶意数据来制造对抗模型，在不需要窃听良性客户端传输或大量任务特定训练数据的情况下实现攻击。这种方法能够在联邦学习中降低生成模型质量，并限制该学习模式的实用性。",
    "en_tdlr": "This paper introduces a new method called Data-Free Untargeted Attack (DFA), which synthesizes malicious data to create adversarial models without the need for eavesdropping on benign client transmissions or requiring large amounts of task-specific training data. This method reduces the quality of generated models in federated learning and limits the practicality of this learning paradigm."
}