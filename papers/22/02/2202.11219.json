{
    "title": "No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling. (arXiv:2202.11219v2 [cs.LG] UPDATED)",
    "abstract": "For each of $T$ time steps, $m$ experts report probability distributions over $n$ outcomes; we wish to learn to aggregate these forecasts in a way that attains a no-regret guarantee. We focus on the fundamental and practical aggregation method known as logarithmic pooling -- a weighted average of log odds -- which is in a certain sense the optimal choice of pooling method if one is interested in minimizing log loss (as we take to be our loss function). We consider the problem of learning the best set of parameters (i.e. expert weights) in an online adversarial setting. We assume (by necessity) that the adversarial choices of outcomes and forecasts are consistent, in the sense that experts report calibrated forecasts. Imposing this constraint creates a (to our knowledge) novel semi-adversarial setting in which the adversary retains a large amount of flexibility. In this setting, we present an algorithm based on online mirror descent that learns expert weights in a way that attains $O(\\s",
    "link": "http://arxiv.org/abs/2202.11219",
    "context": "Title: No-Regret Learning with Unbounded Losses: The Case of Logarithmic Pooling. (arXiv:2202.11219v2 [cs.LG] UPDATED)\nAbstract: For each of $T$ time steps, $m$ experts report probability distributions over $n$ outcomes; we wish to learn to aggregate these forecasts in a way that attains a no-regret guarantee. We focus on the fundamental and practical aggregation method known as logarithmic pooling -- a weighted average of log odds -- which is in a certain sense the optimal choice of pooling method if one is interested in minimizing log loss (as we take to be our loss function). We consider the problem of learning the best set of parameters (i.e. expert weights) in an online adversarial setting. We assume (by necessity) that the adversarial choices of outcomes and forecasts are consistent, in the sense that experts report calibrated forecasts. Imposing this constraint creates a (to our knowledge) novel semi-adversarial setting in which the adversary retains a large amount of flexibility. In this setting, we present an algorithm based on online mirror descent that learns expert weights in a way that attains $O(\\s",
    "path": "papers/22/02/2202.11219.json",
    "total_tokens": 914,
    "translated_title": "无遗憾学习与无界损失：对数池化的情况",
    "translated_abstract": "在每个时间步长$T$中，$m$个专家报告了关于$n$个结果的概率分布；我们希望学习一种聚合这些预测的方法，以达到无遗憾保证。我们关注一种被称为对数池化的基本和实用的聚合方法——log odds 的加权平均，它在某种意义上是一种最优的池化方法选择，如果我们希望最小化 log 损失（作为我们的损失函数）。我们考虑在线对抗环境中学习最佳参数集（即专家权重）的问题。我们假设（必要条件下），对抗选择的结果和预测是一致的，也就是说专家报告了经过校准的预测。施加这个约束条件创建了一个（据我们所知）新颖的半对抗设置，其中对手保留了大量的灵活性。在这个设置下，我们提出了一种基于在线镜像下降的算法，以一种学习专家权重的方式，实现了$O(\\s",
    "tldr": "本文研究了在线对抗环境中，使用对数池化方法学习专家权重的问题。我们提出了一种基于在线镜像下降算法的方法，以达到无遗憾保证。",
    "en_tdlr": "This paper investigates the problem of learning expert weights using logarithmic pooling method in an online adversarial setting. It presents an algorithm based on online mirror descent that achieves a no-regret guarantee."
}