{
    "title": "An alternative approach to train neural networks using monotone variational inequality",
    "abstract": "arXiv:2202.08876v4 Announce Type: replace-cross  Abstract: We propose an alternative approach to neural network training using the monotone vector field, an idea inspired by the seminal work of Juditsky and Nemirovski [Juditsky & Nemirovsky, 2019] developed originally to solve parameter estimation problems for generalized linear models (GLM) by reducing the original non-convex problem to a convex problem of solving a monotone variational inequality (VI). Our approach leads to computationally efficient procedures that converge fast and offer guarantee in some special cases, such as training a single-layer neural network or fine-tuning the last layer of the pre-trained model. Our approach can be used for more efficient fine-tuning of a pre-trained model while freezing the bottom layers, an essential step for deploying many machine learning models such as large language models (LLM). We demonstrate its applicability in training fully-connected (FC) neural networks, graph neural networks (",
    "link": "https://arxiv.org/abs/2202.08876",
    "context": "Title: An alternative approach to train neural networks using monotone variational inequality\nAbstract: arXiv:2202.08876v4 Announce Type: replace-cross  Abstract: We propose an alternative approach to neural network training using the monotone vector field, an idea inspired by the seminal work of Juditsky and Nemirovski [Juditsky & Nemirovsky, 2019] developed originally to solve parameter estimation problems for generalized linear models (GLM) by reducing the original non-convex problem to a convex problem of solving a monotone variational inequality (VI). Our approach leads to computationally efficient procedures that converge fast and offer guarantee in some special cases, such as training a single-layer neural network or fine-tuning the last layer of the pre-trained model. Our approach can be used for more efficient fine-tuning of a pre-trained model while freezing the bottom layers, an essential step for deploying many machine learning models such as large language models (LLM). We demonstrate its applicability in training fully-connected (FC) neural networks, graph neural networks (",
    "path": "papers/22/02/2202.08876.json",
    "total_tokens": 823,
    "translated_title": "使用单调变分不等式训练神经网络的另一种方法",
    "translated_abstract": "我们提出了一种使用单调矢量场的替代方法来训练神经网络，这个想法受到Juditsky和Nemirovski的开创性工作的启发，最初是为了解决广义线性模型（GLM）的参数估计问题，通过将原始非凸问题简化为解决单调变分不等式（VI）的凸问题。我们的方法导致了计算效率高且在某些特殊情况下收敛快速并提供了保证，例如训练单层神经网络或微调预训练模型的最后一层。我们的方法可以用于更高效地微调预训练模型的同时冻结底层，这是部署许多机器学习模型（如大型语言模型LLM）的重要步骤。我们证明了它在训练全连接（FC）神经网络、图神经网络方面的适用性。",
    "tldr": "通过单调变分不等式，提出了一种高效的神经网络训练方法，可以快速收敛并在特定情况下提供保证。",
    "en_tdlr": "An efficient neural network training method is proposed using monotone variational inequality, which converges fast and offers guarantees in specific cases."
}