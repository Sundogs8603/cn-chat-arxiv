{
    "title": "Enhancing Cross-lingual Prompting with Dual Prompt Augmentation. (arXiv:2202.07255v2 [cs.CL] UPDATED)",
    "abstract": "Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited. Zhao and Sch\\\"utze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning. In this paper, we conduct an empirical exploration on the effect of each component in cross-lingual prompting and derive language-agnostic Universal Prompting, which helps alleviate the discrepancies between source-language training and target-language inference. Based on this, we propose DPA, a dual prompt augmentation framework, aiming at relieving the data scarcity issue in few-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54% with only 16 English training examples per class, significantly better than 34.99% of finetuning. Our code is available at https://github.com/DAMO-NLP-SG/DPA.",
    "link": "http://arxiv.org/abs/2202.07255",
    "context": "Title: Enhancing Cross-lingual Prompting with Dual Prompt Augmentation. (arXiv:2202.07255v2 [cs.CL] UPDATED)\nAbstract: Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited. Zhao and Sch\\\"utze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning. In this paper, we conduct an empirical exploration on the effect of each component in cross-lingual prompting and derive language-agnostic Universal Prompting, which helps alleviate the discrepancies between source-language training and target-language inference. Based on this, we propose DPA, a dual prompt augmentation framework, aiming at relieving the data scarcity issue in few-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54% with only 16 English training examples per class, significantly better than 34.99% of finetuning. Our code is available at https://github.com/DAMO-NLP-SG/DPA.",
    "path": "papers/22/02/2202.07255.json",
    "total_tokens": 751,
    "translated_title": "用双重提示增强跨语言提示",
    "translated_abstract": "开发出一种新的双重提示方法(DPA)，用于在跨语言模型训练中缓解数据稀缺问题。该方法采用了语言无关的通用提示方法，可以在仅有每类16个英语训练样本的情况下，将在XNLI任务上的性能从34.99%提高到46.54%。",
    "tldr": "该论文提出了双重提示增强跨语言提示的方法DPA，利用语言无关的通用提示方法，大大缓解了数据稀缺问题。在XNLI任务上，该方法可以在仅有16个英语训练样本的情况下，将性能从34.99%提高到46.54%。",
    "en_tdlr": "This paper proposes a dual prompt augmentation framework (DPA) for enhancing cross-lingual prompting, which uses a language-agnostic Universal Prompting method to alleviate data scarcity issues. Notably, this method achieves a significant improvement on the XNLI task, increasing performance from 34.99% to 46.54% with only 16 English training examples per class."
}