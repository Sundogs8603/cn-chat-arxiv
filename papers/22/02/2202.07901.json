{
    "title": "Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition. (arXiv:2202.07901v3 [cs.LG] UPDATED)",
    "abstract": "Cross-modal representation learning learns a shared embedding between two or more modalities to improve performance in a given task compared to using only one of the modalities. Cross-modal representation learning from different data types -- such as images and time-series data (e.g., audio or text data) -requires a deep metric learning loss that minimizes the distance between the modality embeddings. In this paper, we propose to use the contrastive or triplet loss, which uses positive and negative identities to create sample pairs with different labels, for cross-modal representation learning between image and time-series modalities (CMR-IS). By adapting the triplet loss for cross-modal representation learning, higher accuracy in the main (time-series classification) task can be achieved by exploiting additional information of the auxiliary (image classification) task. We present a triplet loss with a dynamic margin for single label and sequence-to-sequence classification tasks. We ",
    "link": "http://arxiv.org/abs/2202.07901",
    "context": "Title: Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition. (arXiv:2202.07901v3 [cs.LG] UPDATED)\nAbstract: Cross-modal representation learning learns a shared embedding between two or more modalities to improve performance in a given task compared to using only one of the modalities. Cross-modal representation learning from different data types -- such as images and time-series data (e.g., audio or text data) -requires a deep metric learning loss that minimizes the distance between the modality embeddings. In this paper, we propose to use the contrastive or triplet loss, which uses positive and negative identities to create sample pairs with different labels, for cross-modal representation learning between image and time-series modalities (CMR-IS). By adapting the triplet loss for cross-modal representation learning, higher accuracy in the main (time-series classification) task can be achieved by exploiting additional information of the auxiliary (image classification) task. We present a triplet loss with a dynamic margin for single label and sequence-to-sequence classification tasks. We ",
    "path": "papers/22/02/2202.07901.json",
    "total_tokens": 873,
    "translated_title": "在线手写识别中使用三元组损失函数的辅助跨模态表示学习",
    "translated_abstract": "跨模态表示学习通过学习两种或多种模态之间的共享嵌入，比仅使用其中一种模态能够提高给定任务的性能。不同数据类型的跨模态表示学习，如图像和时间序列数据（例如音频或文本数据），需要最小化模态嵌入之间的距离的深度度量学习损失函数。本文提出使用对比损失或三元组损失，利用正负标签创建具有不同标签的样本对，进行图像和时间序列模态之间的跨模态表示学习（CMR-IS）。通过调整三元组损失进行跨模态表示学习，可以利用辅助（图像分类）任务的附加信息，从而在主要的时间序列分类任务中实现更高的准确性。我们提出了适用于单标签和序列到序列分类任务的动态边界三元组损失。",
    "tldr": "本文提出了一种在在线手写识别中使用三元组损失函数的辅助跨模态表示学习方法，通过最小化模态嵌入之间的距离，利用图像分类任务的附加信息，提高了时间序列分类任务的准确性。"
}