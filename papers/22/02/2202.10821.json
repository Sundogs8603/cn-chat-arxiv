{
    "title": "Increasing Depth of Neural Networks for Life-long Learning. (arXiv:2202.10821v2 [cs.LG] UPDATED)",
    "abstract": "Purpose: We propose a novel method for continual learning based on the increasing depth of neural networks. This work explores whether extending neural network depth may be beneficial in a life-long learning setting.  Methods: We propose a novel approach based on adding new layers on top of existing ones to enable the forward transfer of knowledge and adapting previously learned representations. We employ a method of determining the most similar tasks for selecting the best location in our network to add new nodes with trainable parameters. This approach allows for creating a tree-like model, where each node is a set of neural network parameters dedicated to a specific task. The Progressive Neural Network concept inspires the proposed method. Therefore, it benefits from dynamic changes in network structure. However, Progressive Neural Network allocates a lot of memory for the whole network structure during the learning process. The proposed method alleviates this by adding only part of",
    "link": "http://arxiv.org/abs/2202.10821",
    "context": "Title: Increasing Depth of Neural Networks for Life-long Learning. (arXiv:2202.10821v2 [cs.LG] UPDATED)\nAbstract: Purpose: We propose a novel method for continual learning based on the increasing depth of neural networks. This work explores whether extending neural network depth may be beneficial in a life-long learning setting.  Methods: We propose a novel approach based on adding new layers on top of existing ones to enable the forward transfer of knowledge and adapting previously learned representations. We employ a method of determining the most similar tasks for selecting the best location in our network to add new nodes with trainable parameters. This approach allows for creating a tree-like model, where each node is a set of neural network parameters dedicated to a specific task. The Progressive Neural Network concept inspires the proposed method. Therefore, it benefits from dynamic changes in network structure. However, Progressive Neural Network allocates a lot of memory for the whole network structure during the learning process. The proposed method alleviates this by adding only part of",
    "path": "papers/22/02/2202.10821.json",
    "total_tokens": 884,
    "translated_title": "增加神经网络深度进行终身学习的研究",
    "translated_abstract": "目的：我们提出了一种基于增加神经网络深度的终身学习方法。本研究探讨了在终身学习环境下扩展神经网络深度是否有益处。方法：我们提出了一种新方法，通过在现有网络上方添加新层来实现知识的前向传递，并适应以前学过的表示。我们采用一种确定最相似任务的方法，以选择网络中添加有可训练参数的新节点的最佳位置。该方法允许创建一种树形模型，其中每个节点是专用于特定任务的神经网络参数集合。所提出的方法受到渐进式神经网络概念的启发。因此，它可以从网络结构的动态变化中获益。然而，渐进式神经网络在学习过程中为整个网络结构分配大量内存。所提出的方法通过仅添加部分来缓解此问题。",
    "tldr": "本研究提出了一种终身学习方法，通过增加神经网络深度，以节点的方式部署不同任务的神经网络参数，实现前向传递和适应以前学过的表示，解决了渐进式神经网络在整个网络分配大量内存的问题。",
    "en_tdlr": "This study proposes a life-long learning method by increasing the depth of neural networks, which deploys sets of neural network parameters for different tasks in a node-based manner to enable forward transfer of knowledge and adapt previously learned representations, thus addressing the memory allocation issue in Progressive Neural Network."
}