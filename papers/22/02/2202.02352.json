{
    "title": "Learning Interpretable, High-Performing Policies for Autonomous Driving. (arXiv:2202.02352v3 [cs.LG] UPDATED)",
    "abstract": "Gradient-based approaches in reinforcement learning (RL) have achieved tremendous success in learning policies for autonomous vehicles. While the performance of these approaches warrants real-world adoption, these policies lack interpretability, limiting deployability in the safety-critical and legally-regulated domain of autonomous driving (AD). AD requires interpretable and verifiable control policies that maintain high performance. We propose Interpretable Continuous Control Trees (ICCTs), a tree-based model that can be optimized via modern, gradient-based, RL approaches to produce high-performing, interpretable policies. The key to our approach is a procedure for allowing direct optimization in a sparse decision-tree-like representation. We validate ICCTs against baselines across six domains, showing that ICCTs are capable of learning interpretable policy representations that parity or outperform baselines by up to 33% in AD scenarios while achieving a 300x-600x reduction in the nu",
    "link": "http://arxiv.org/abs/2202.02352",
    "context": "Title: Learning Interpretable, High-Performing Policies for Autonomous Driving. (arXiv:2202.02352v3 [cs.LG] UPDATED)\nAbstract: Gradient-based approaches in reinforcement learning (RL) have achieved tremendous success in learning policies for autonomous vehicles. While the performance of these approaches warrants real-world adoption, these policies lack interpretability, limiting deployability in the safety-critical and legally-regulated domain of autonomous driving (AD). AD requires interpretable and verifiable control policies that maintain high performance. We propose Interpretable Continuous Control Trees (ICCTs), a tree-based model that can be optimized via modern, gradient-based, RL approaches to produce high-performing, interpretable policies. The key to our approach is a procedure for allowing direct optimization in a sparse decision-tree-like representation. We validate ICCTs against baselines across six domains, showing that ICCTs are capable of learning interpretable policy representations that parity or outperform baselines by up to 33% in AD scenarios while achieving a 300x-600x reduction in the nu",
    "path": "papers/22/02/2202.02352.json",
    "total_tokens": 998,
    "translated_title": "学习自动驾驶的可解释、高性能策略",
    "translated_abstract": "强化学习中的基于梯度的方法在学习自动驾驶车辆的策略方面取得了巨大的成功。虽然这些方法的性能值得在现实世界中采用，但是这些策略缺乏可解释性，限制了在安全关键和法律监管领域中的部署能力。自动驾驶要求可解释和可验证的控制策略，以保持高性能。我们提出了可解释的连续控制树（ICCTs），这是一种基于树状模型，可以通过现代的基于梯度的强化学习方法进行优化，从而产生高性能、可解释的策略。我们方法的关键是允许在稀疏的类似决策树的表示中进行直接优化的过程。我们在六个领域对ICCTs进行验证，并显示ICCTs能够学习到比基准模型在自动驾驶场景中更可解释的策略表示，并且在性能方面超过基准模型最高达33%，同时实现了300倍至600倍的减少。",
    "tldr": "本研究提出了一种叫做可解释连续控制树（ICCTs）的树状模型，通过现代的强化学习方法进行优化，能够学习到高性能、可解释的策略。这种方法在自动驾驶领域表现出了比基准模型高33%的性能，并且达到300倍至600倍的减少。",
    "en_tdlr": "This study proposes an interpretable tree-based model called Interpretable Continuous Control Trees (ICCTs), which can be optimized using modern reinforcement learning approaches to learn high-performing and interpretable policies. The method outperforms baselines by up to 33% in autonomous driving scenarios and achieves a reduction of 300x-600x."
}