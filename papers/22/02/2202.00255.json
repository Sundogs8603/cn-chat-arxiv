{
    "title": "DoCoM: Compressed Decentralized Optimization with Near-Optimal Sample Complexity. (arXiv:2202.00255v2 [cs.LG] UPDATED)",
    "abstract": "This paper proposes the Doubly Compressed Momentum-assisted stochastic gradient tracking algorithm $\\texttt{DoCoM}$ for communication-efficient decentralized optimization. The algorithm features two main ingredients to achieve a near-optimal sample complexity while allowing for communication compression. First, the algorithm tracks both the averaged iterate and stochastic gradient using compressed gossiping consensus. Second, a momentum step is incorporated for adaptive variance reduction with the local gradient estimates. We show that $\\texttt{DoCoM}$ finds a near-stationary solution at all participating agents satisfying $\\mathbb{E}[ \\| \\nabla f( \\theta ) \\|^2 ] = \\mathcal{O}( 1 / T^{2/3} )$ in $T$ iterations, where $f(\\theta)$ is a smooth (possibly non-convex) objective function. Notice that the proof is achieved via analytically designing a new potential function that tightly tracks the one-iteration progress of $\\texttt{DoCoM}$. As a corollary, our analysis also established the li",
    "link": "http://arxiv.org/abs/2202.00255",
    "context": "Title: DoCoM: Compressed Decentralized Optimization with Near-Optimal Sample Complexity. (arXiv:2202.00255v2 [cs.LG] UPDATED)\nAbstract: This paper proposes the Doubly Compressed Momentum-assisted stochastic gradient tracking algorithm $\\texttt{DoCoM}$ for communication-efficient decentralized optimization. The algorithm features two main ingredients to achieve a near-optimal sample complexity while allowing for communication compression. First, the algorithm tracks both the averaged iterate and stochastic gradient using compressed gossiping consensus. Second, a momentum step is incorporated for adaptive variance reduction with the local gradient estimates. We show that $\\texttt{DoCoM}$ finds a near-stationary solution at all participating agents satisfying $\\mathbb{E}[ \\| \\nabla f( \\theta ) \\|^2 ] = \\mathcal{O}( 1 / T^{2/3} )$ in $T$ iterations, where $f(\\theta)$ is a smooth (possibly non-convex) objective function. Notice that the proof is achieved via analytically designing a new potential function that tightly tracks the one-iteration progress of $\\texttt{DoCoM}$. As a corollary, our analysis also established the li",
    "path": "papers/22/02/2202.00255.json",
    "total_tokens": 933,
    "translated_title": "DoCoM：具有近似最优样本复杂度的压缩分散式优化",
    "translated_abstract": "本文提出了一种名为DoCoM的算法，用于实现通信高效的分散式优化。算法通过压缩式谣言传播一致性追踪来同时跟踪平均迭代和随机梯度，以实现近似最优样本复杂度。其次，通过引入动量步骤和局部梯度估计，实现自适应方差归约。我们证明，在T次迭代中，DoCoM可以在所有参与代理中找到一个近似稳定的解，满足$\\mathbb{E}[ \\| \\nabla f( \\theta ) \\|^2 ] = \\mathcal{O}( 1 / T^{2/3} )$，其中$f(\\theta)$是一个平滑的（可能非凸）目标函数。需要注意的是，我们通过分析设计了一个新的潜力函数，紧密跟踪了DoCoM的一次迭代进展来证明这一点。",
    "tldr": "提出了一种名为DoCoM的分散式优化算法，通过压缩谣言传播一致性追踪同时跟踪平均迭代和随机梯度，并引入动量步骤和局部梯度估计，实现通信高效和近似最优样本复杂度。",
    "en_tdlr": "A compressed decentralized optimization algorithm, DoCoM, is proposed in this paper, which tracks both the averaged iterate and stochastic gradient using compressed gossiping consensus, and incorporates a momentum step and local gradient estimates for communication efficiency and near-optimal sample complexity."
}