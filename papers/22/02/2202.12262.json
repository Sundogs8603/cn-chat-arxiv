{
    "title": "On the Omnipresence of Spurious Local Minima in Certain Neural Network Training Problems. (arXiv:2202.12262v2 [cs.LG] UPDATED)",
    "abstract": "We study the loss landscape of training problems for deep artificial neural networks with a one-dimensional real output whose activation functions contain an affine segment and whose hidden layers have width at least two. It is shown that such problems possess a continuum of spurious (i.e., not globally optimal) local minima for all target functions that are not affine. In contrast to previous works, our analysis covers all sampling and parameterization regimes, general differentiable loss functions, arbitrary continuous nonpolynomial activation functions, and both the finite- and infinite-dimensional setting. It is further shown that the appearance of the spurious local minima in the considered training problems is a direct consequence of the universal approximation theorem and that the underlying mechanisms also cause, e.g., $L^p$-best approximation problems to be ill-posed in the sense of Hadamard for all networks that do not have a dense image. The latter result also holds without ",
    "link": "http://arxiv.org/abs/2202.12262",
    "context": "Title: On the Omnipresence of Spurious Local Minima in Certain Neural Network Training Problems. (arXiv:2202.12262v2 [cs.LG] UPDATED)\nAbstract: We study the loss landscape of training problems for deep artificial neural networks with a one-dimensional real output whose activation functions contain an affine segment and whose hidden layers have width at least two. It is shown that such problems possess a continuum of spurious (i.e., not globally optimal) local minima for all target functions that are not affine. In contrast to previous works, our analysis covers all sampling and parameterization regimes, general differentiable loss functions, arbitrary continuous nonpolynomial activation functions, and both the finite- and infinite-dimensional setting. It is further shown that the appearance of the spurious local minima in the considered training problems is a direct consequence of the universal approximation theorem and that the underlying mechanisms also cause, e.g., $L^p$-best approximation problems to be ill-posed in the sense of Hadamard for all networks that do not have a dense image. The latter result also holds without ",
    "path": "papers/22/02/2202.12262.json",
    "total_tokens": 973,
    "translated_title": "关于某些神经网络训练问题中虚假局部极小值的普遍存在性研究",
    "translated_abstract": "本文研究了具有一维实数输出且激活函数包含仿射段以及隐藏层至少有两个节点的深层人工神经网络的训练问题损失景观。结果表明，对于所有不是仿射的目标函数，这类问题存在一系列虚假的（即非全局最优的）局部极小值。与以往的研究不同，我们的分析涵盖了所有采样和参数化方案，一般可微损失函数，任意连续非多项式激活函数，以及有限维和无限维设置。进一步表明，所考虑的训练问题中虚假局部极小值的出现是通用逼近定理的直接结果，底层机制也导致例如$L^p$-最佳逼近问题对于没有密集图像的所有网络在Hadamard意义下都是不良 posed。后一结果也适用于无限维情况。",
    "tldr": "研究了具有一维实数输出且激活函数包含仿射段以及隐藏层至少有两个节点的深层人工神经网络的训练问题损失景观，发现这类问题对于所有不是仿射的目标函数，都存在一系列虚假的局部极小值，这是由通用逼近定理直接推导得出的。",
    "en_tdlr": "This research focuses on the loss landscape of training problems for deep artificial neural networks with a one-dimensional real output whose activation functions contain an affine segment and whose hidden layers have width at least two. It is found that such problems possess a continuum of spurious local minima for all target functions that are not affine, which is a direct consequence of the universal approximation theorem."
}