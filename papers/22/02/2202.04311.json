{
    "title": "Identifying Backdoor Attacks in Federated Learning via Anomaly Detection. (arXiv:2202.04311v2 [cs.AI] UPDATED)",
    "abstract": "Federated learning has seen increased adoption in recent years in response to the growing regulatory demand for data privacy. However, the opaque local training process of federated learning also sparks rising concerns about model faithfulness. For instance, studies have revealed that federated learning is vulnerable to backdoor attacks, whereby a compromised participant can stealthily modify the model's behavior in the presence of backdoor triggers. This paper proposes an effective defense against the attack by examining shared model updates. We begin with the observation that the embedding of backdoors influences the participants' local model weights in terms of the magnitude and orientation of their model gradients, which can manifest as distinguishable disparities. We enable a robust identification of backdoors by studying the statistical distribution of the models' subsets of gradients. Concretely, we first segment the model gradients into fragment vectors that represent small por",
    "link": "http://arxiv.org/abs/2202.04311",
    "context": "Title: Identifying Backdoor Attacks in Federated Learning via Anomaly Detection. (arXiv:2202.04311v2 [cs.AI] UPDATED)\nAbstract: Federated learning has seen increased adoption in recent years in response to the growing regulatory demand for data privacy. However, the opaque local training process of federated learning also sparks rising concerns about model faithfulness. For instance, studies have revealed that federated learning is vulnerable to backdoor attacks, whereby a compromised participant can stealthily modify the model's behavior in the presence of backdoor triggers. This paper proposes an effective defense against the attack by examining shared model updates. We begin with the observation that the embedding of backdoors influences the participants' local model weights in terms of the magnitude and orientation of their model gradients, which can manifest as distinguishable disparities. We enable a robust identification of backdoors by studying the statistical distribution of the models' subsets of gradients. Concretely, we first segment the model gradients into fragment vectors that represent small por",
    "path": "papers/22/02/2202.04311.json",
    "total_tokens": 832,
    "translated_title": "通过异常检测在联邦学习中识别后门攻击",
    "translated_abstract": "鉴于数据隐私的监管需求不断增长，联邦学习近年来得到了越来越多的应用。然而，联邦学习的不透明的本地训练过程也引发了对模型忠诚度的担忧。研究表明，联邦学习容易受到后门攻击的影响，即在后门触发器的存在下，被入侵参与者可以悄悄修改模型的行为。本文通过检查共享模型更新来提出一种有效的防御方法。我们首先观察到后门的嵌入会以模型梯度的大小和方向的形式影响参与者的本地模型权重，这可以表现为可区分的差异。通过研究模型梯度子集的统计分布，我们能够对后门进行鲁棒的识别。",
    "tldr": "本文提出了一种通过检查共享模型更新来识别联邦学习中后门攻击的有效防御方法。通过研究模型梯度子集的统计分布，能够鲁棒地识别后门攻击。",
    "en_tdlr": "This paper proposes an effective defense against backdoor attacks in federated learning by examining shared model updates. The statistical distribution of subsets of gradients is studied to robustly identify backdoors."
}