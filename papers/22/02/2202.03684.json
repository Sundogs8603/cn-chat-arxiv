{
    "title": "Efficiently Escaping Saddle Points in Bilevel Optimization. (arXiv:2202.03684v2 [cs.LG] UPDATED)",
    "abstract": "Bilevel optimization is one of the fundamental problems in machine learning and optimization. Recent theoretical developments in bilevel optimization focus on finding the first-order stationary points for nonconvex-strongly-convex cases. In this paper, we analyze algorithms that can escape saddle points in nonconvex-strongly-convex bilevel optimization. Specifically, we show that the perturbed approximate implicit differentiation (AID) with a warm start strategy finds $\\epsilon$-approximate local minimum of bilevel optimization in $\\tilde{O}(\\epsilon^{-2})$ iterations with high probability. Moreover, we propose an inexact NEgative-curvature-Originated-from-Noise Algorithm (iNEON), a pure first-order algorithm that can escape saddle point and find local minimum of stochastic bilevel optimization. As a by-product, we provide the first nonasymptotic analysis of perturbed multi-step gradient descent ascent (GDmax) algorithm that converges to local minimax point for minimax problems.",
    "link": "http://arxiv.org/abs/2202.03684",
    "context": "Title: Efficiently Escaping Saddle Points in Bilevel Optimization. (arXiv:2202.03684v2 [cs.LG] UPDATED)\nAbstract: Bilevel optimization is one of the fundamental problems in machine learning and optimization. Recent theoretical developments in bilevel optimization focus on finding the first-order stationary points for nonconvex-strongly-convex cases. In this paper, we analyze algorithms that can escape saddle points in nonconvex-strongly-convex bilevel optimization. Specifically, we show that the perturbed approximate implicit differentiation (AID) with a warm start strategy finds $\\epsilon$-approximate local minimum of bilevel optimization in $\\tilde{O}(\\epsilon^{-2})$ iterations with high probability. Moreover, we propose an inexact NEgative-curvature-Originated-from-Noise Algorithm (iNEON), a pure first-order algorithm that can escape saddle point and find local minimum of stochastic bilevel optimization. As a by-product, we provide the first nonasymptotic analysis of perturbed multi-step gradient descent ascent (GDmax) algorithm that converges to local minimax point for minimax problems.",
    "path": "papers/22/02/2202.03684.json",
    "total_tokens": 990,
    "translated_title": "基于近似隐式微分的双层优化中的鞍点逃逸方法",
    "translated_abstract": "双层优化是机器学习和优化中的基本问题之一。最近的理论发展集中于寻找非凸强凸情况下的一阶稳定点。本文分析了一些算法，这些算法可以在非凸强凸双层优化中逃离鞍点。具体而言，我们展示了一个带有热启动策略的扰动近似隐式微分（AID）可以在$\\tilde{O}(\\epsilon^{-2})$迭代中高概率地找到$\\epsilon$-近似局部最小值的双层优化。此外，我们提出了一种不精确非曲率源自噪声算法（iNEON），这是一种纯一阶算法，可以逃离鞍点并找到随机双层优化的局部最小值。作为一个副产品，我们提供了扰动多步梯度下降（GDmax）算法的第一个非渐近分析，该算法收敛到最小化最大问题的局部极小值点。",
    "tldr": "本文研究了双层优化中的鞍点逃逸方法，提出了两种算法，一种是用于确定性问题的扰动近似隐式微分算法，一种是用于随机问题的不精确非曲率源自噪声算法，并且提供了扰动多步梯度下降算法的非渐近分析。",
    "en_tdlr": "This paper studies efficient methods to escape saddle points in bilevel optimization and proposes two algorithms, a perturbed approximate implicit differentiation algorithm for deterministic problems and an inexact negative-curvature-originated-from-noise algorithm for stochastic problems. The paper also provides the first non-asymptotic analysis of the perturbed multi-step gradient descent algorithm for minimax problems."
}