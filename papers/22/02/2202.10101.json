{
    "title": "BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models in the Biomedical Domain. (arXiv:2202.10101v2 [cs.CL] UPDATED)",
    "abstract": "Recent developments in transfer learning have boosted the advancements in natural language processing tasks. The performance is, however, dependent on high-quality, manually annotated training data. Especially in the biomedical domain, it has been shown that one training corpus is not enough to learn generic models that are able to efficiently predict on new data. Therefore, state-of-the-art models need the ability of lifelong learning in order to improve performance as soon as new data are available - without the need of re-training the whole model from scratch. We present WEAVER, a simple, yet efficient post-processing method that infuses old knowledge into the new model, thereby reducing catastrophic forgetting. We show that applying WEAVER in a sequential manner results in similar word embedding distributions as doing a combined training on all data at once, while being computationally more efficient. Because there is no need of data sharing, the presented method is also easily app",
    "link": "http://arxiv.org/abs/2202.10101",
    "context": "Title: BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models in the Biomedical Domain. (arXiv:2202.10101v2 [cs.CL] UPDATED)\nAbstract: Recent developments in transfer learning have boosted the advancements in natural language processing tasks. The performance is, however, dependent on high-quality, manually annotated training data. Especially in the biomedical domain, it has been shown that one training corpus is not enough to learn generic models that are able to efficiently predict on new data. Therefore, state-of-the-art models need the ability of lifelong learning in order to improve performance as soon as new data are available - without the need of re-training the whole model from scratch. We present WEAVER, a simple, yet efficient post-processing method that infuses old knowledge into the new model, thereby reducing catastrophic forgetting. We show that applying WEAVER in a sequential manner results in similar word embedding distributions as doing a combined training on all data at once, while being computationally more efficient. Because there is no need of data sharing, the presented method is also easily app",
    "path": "papers/22/02/2202.10101.json",
    "total_tokens": 912,
    "translated_title": "BERT WEAVER：使用加权平均使生物医学领域基于Transformer的模型实现终身学习",
    "translated_abstract": "最近在转移学习方面的发展推动了自然语言处理任务的进展。然而，性能取决于高质量的手动标注训练数据。尤其是在生物医学领域，已经表明一种训练语料库不足以学习能够在新数据上高效预测的通用模型。因此，最先进的模型需要具备终身学习的能力，以便在新数据可用时提高性能 - 而无需从头开始重新训练整个模型。我们提出WEAVER，一种简单但高效的后处理方法，将旧知识融入到新模型中，从而减少灾难性遗忘。我们展示了顺序应用WEAVER会产生类似于一次性使用所有数据进行联合训练的词嵌入分布，同时计算效率更高。由于没有数据共享的必要，因此所介绍的方法在数据隐私是一项关注的情况下也可以轻松应用。",
    "tldr": "研究提出了WEAVER方法，它可以将旧知识融入到新模型中，以有效降低灾难性遗忘，并实现生物医学领域基于Transformer的模型的终身学习。"
}