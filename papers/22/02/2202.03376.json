{
    "title": "Message Passing Neural PDE Solvers. (arXiv:2202.03376v3 [cs.LG] UPDATED)",
    "abstract": "The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a metho",
    "link": "http://arxiv.org/abs/2202.03376",
    "context": "Title: Message Passing Neural PDE Solvers. (arXiv:2202.03376v3 [cs.LG] UPDATED)\nAbstract: The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a metho",
    "path": "papers/22/02/2202.03376.json",
    "total_tokens": 989,
    "translated_title": "消息传递神经偏微分方程求解器",
    "translated_abstract": "偏微分方程的数值求解一直是一个困难的问题，迄今已经有一个世纪的研究历程。近年来，出现了建立神经数值混合求解器的趋势，这使得现代完全端对端学习系统的发展变得更为高效。迄今为止，大多数工作只能在一些特定的属性方面进行泛化，这些属性包括分辨率、拓扑、几何形状、边界条件、域离散正则性和维度等等。在这项工作中，我们建立了一个解决这些属性的求解器，其中所有组件都基于神经消息传递，用反向传播优化的神经函数逼近器取代了计算图中所有启发式设计的组件。我们展示了神经消息传递求解器在表示上包含一些经典的方法，如有限差分、有限体积和 WENO 方案。为了在训练自回归模型时鼓励稳定性，我们提出了一种带有完全状态更新的消息传递方法，可以推广到具有经典解结构的任何 PDE 中。",
    "tldr": "本文提出了一种基于神经消息传递的求解器，用反向传播优化的神经函数逼近器取代计算图中所有启发式设计的组件，并建立了一种带有完全状态更新的消息传递方法，可推广到任何具有经典解结构的偏微分方程中。",
    "en_tdlr": "This paper proposes a solver based on neural message passing, replacing heuristically designed components with backprop-optimized neural function approximators. A method for message passing with full-state updates is presented to encourage stability in training autoregressive models and can be generalized to any PDE with classical solution structure."
}