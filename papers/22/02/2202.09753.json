{
    "title": "Finite-Time Analysis of Natural Actor-Critic for POMDPs. (arXiv:2202.09753v3 [cs.LG] UPDATED)",
    "abstract": "We consider the reinforcement learning problem for partially observed Markov decision processes (POMDPs) with large or even countably infinite state spaces, where the controller has access to only noisy observations of the underlying controlled Markov chain. We consider a natural actor-critic method that employs a finite internal memory for policy parameterization, and a multi-step temporal difference learning algorithm for policy evaluation. We establish, to the best of our knowledge, the first non-asymptotic global convergence of actor-critic methods for partially observed systems under function approximation. In particular, in addition to the function approximation and statistical errors that also arise in MDPs, we explicitly characterize the error due to the use of finite-state controllers. This additional error is stated in terms of the total variation distance between the traditional belief state in POMDPs and the posterior distribution of the hidden state when using a finite-sta",
    "link": "http://arxiv.org/abs/2202.09753",
    "context": "Title: Finite-Time Analysis of Natural Actor-Critic for POMDPs. (arXiv:2202.09753v3 [cs.LG] UPDATED)\nAbstract: We consider the reinforcement learning problem for partially observed Markov decision processes (POMDPs) with large or even countably infinite state spaces, where the controller has access to only noisy observations of the underlying controlled Markov chain. We consider a natural actor-critic method that employs a finite internal memory for policy parameterization, and a multi-step temporal difference learning algorithm for policy evaluation. We establish, to the best of our knowledge, the first non-asymptotic global convergence of actor-critic methods for partially observed systems under function approximation. In particular, in addition to the function approximation and statistical errors that also arise in MDPs, we explicitly characterize the error due to the use of finite-state controllers. This additional error is stated in terms of the total variation distance between the traditional belief state in POMDPs and the posterior distribution of the hidden state when using a finite-sta",
    "path": "papers/22/02/2202.09753.json",
    "total_tokens": 896,
    "translated_title": "部分观察的马尔科夫决策过程（POMDPs）的自然演员-评论家方法的有限时间分析",
    "translated_abstract": "我们考虑了有限或可数无限状态空间的部分观察的马尔科夫决策过程（POMDPs）的强化学习问题，其中控制器只能访问基础控制马尔科夫链的噪声观测。我们考虑了一种自然的演员-评论家方法，该方法采用有限的内部存储器进行策略参数化，并使用多步时序差异学习算法进行策略评估。凭借我们的知识，我们首次确立了部分观察系统下基于函数逼近的演员-评论家方法的非渐近全局收敛性。特别地，除了在MDPs中出现的函数逼近和统计误差之外，我们还明确地表征了由于使用有限状态控制器而产生的错误。这种额外的错误是以传统的POMDPs中的信心状态和使用有限状态时的隐藏状态的后验分布之间的总变差距离来表示的。",
    "tldr": "本文分析了部分观察的马尔科夫决策过程（POMDPs）下自然演员-评论家方法的有限时间特性，并对使用有限状态控制器产生的错误进行了明确的表征。",
    "en_tdlr": "This paper presents a finite-time analysis of the natural actor-critic method for partially observed Markov decision processes (POMDPs), and explicitly characterizes the errors due to the use of finite-state controllers."
}