{
    "title": "MAML and ANIL Provably Learn Representations. (arXiv:2202.03483v2 [cs.LG] UPDATED)",
    "abstract": "Recent empirical evidence has driven conventional wisdom to believe that gradient-based meta-learning (GBML) methods perform well at few-shot learning because they learn an expressive data representation that is shared across tasks. However, the mechanics of GBML have remained largely mysterious from a theoretical perspective. In this paper, we prove that two well-known GBML methods, MAML and ANIL, as well as their first-order approximations, are capable of learning common representation among a set of given tasks. Specifically, in the well-known multi-task linear representation learning setting, they are able to recover the ground-truth representation at an exponentially fast rate. Moreover, our analysis illuminates that the driving force causing MAML and ANIL to recover the underlying representation is that they adapt the final layer of their model, which harnesses the underlying task diversity to improve the representation in all directions of interest. To the best of our knowledge,",
    "link": "http://arxiv.org/abs/2202.03483",
    "context": "Title: MAML and ANIL Provably Learn Representations. (arXiv:2202.03483v2 [cs.LG] UPDATED)\nAbstract: Recent empirical evidence has driven conventional wisdom to believe that gradient-based meta-learning (GBML) methods perform well at few-shot learning because they learn an expressive data representation that is shared across tasks. However, the mechanics of GBML have remained largely mysterious from a theoretical perspective. In this paper, we prove that two well-known GBML methods, MAML and ANIL, as well as their first-order approximations, are capable of learning common representation among a set of given tasks. Specifically, in the well-known multi-task linear representation learning setting, they are able to recover the ground-truth representation at an exponentially fast rate. Moreover, our analysis illuminates that the driving force causing MAML and ANIL to recover the underlying representation is that they adapt the final layer of their model, which harnesses the underlying task diversity to improve the representation in all directions of interest. To the best of our knowledge,",
    "path": "papers/22/02/2202.03483.json",
    "total_tokens": 901,
    "translated_title": "MAML和ANIL被证明能够学习表示法",
    "translated_abstract": "最近的经验证据让人们认为，基于梯度的元学习（GBML）方法在少样本学习上表现良好，因为它们学习了一种可共享的表达数据表示法。然而，从理论角度来看，GBML的机制仍然是个谜。在本文中，我们证明了两种著名的GBML方法，MAML和ANIL，以及它们的一阶近似都能够学习一组给定任务之间的共同表示法。具体而言，在著名的多任务线性表示学习环境中，它们能够以指数快的速度恢复地面实况表示法。此外，我们的分析阐明，驱动MAML和ANIL恢复潜在表示法的动力是它们调整模型的最后一层，利用潜在的任务多样性来改善所有感兴趣的方向的表示法。据我们所知，这是首个解释GBML方法导致共享表示法出现的理论工作。",
    "tldr": "本文证明了MAML和ANIL能够在少样本学习中学习出共同的数据表示法，它们通过适应模型的最后一层来改善表示法，这也是导致共享表示法出现的原因。"
}