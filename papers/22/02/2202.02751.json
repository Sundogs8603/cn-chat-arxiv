{
    "title": "Tubes Among Us: Analog Attack on Automatic Speaker Identification. (arXiv:2202.02751v2 [cs.LG] UPDATED)",
    "abstract": "Recent years have seen a surge in the popularity of acoustics-enabled personal devices powered by machine learning. Yet, machine learning has proven to be vulnerable to adversarial examples. A large number of modern systems protect themselves against such attacks by targeting artificiality, i.e., they deploy mechanisms to detect the lack of human involvement in generating the adversarial examples. However, these defenses implicitly assume that humans are incapable of producing meaningful and targeted adversarial examples. In this paper, we show that this base assumption is wrong. In particular, we demonstrate that for tasks like speaker identification, a human is capable of producing analog adversarial examples directly with little cost and supervision: by simply speaking through a tube, an adversary reliably impersonates other speakers in eyes of ML models for speaker identification. Our findings extend to a range of other acoustic-biometric tasks such as liveness detection, bringing ",
    "link": "http://arxiv.org/abs/2202.02751",
    "context": "Title: Tubes Among Us: Analog Attack on Automatic Speaker Identification. (arXiv:2202.02751v2 [cs.LG] UPDATED)\nAbstract: Recent years have seen a surge in the popularity of acoustics-enabled personal devices powered by machine learning. Yet, machine learning has proven to be vulnerable to adversarial examples. A large number of modern systems protect themselves against such attacks by targeting artificiality, i.e., they deploy mechanisms to detect the lack of human involvement in generating the adversarial examples. However, these defenses implicitly assume that humans are incapable of producing meaningful and targeted adversarial examples. In this paper, we show that this base assumption is wrong. In particular, we demonstrate that for tasks like speaker identification, a human is capable of producing analog adversarial examples directly with little cost and supervision: by simply speaking through a tube, an adversary reliably impersonates other speakers in eyes of ML models for speaker identification. Our findings extend to a range of other acoustic-biometric tasks such as liveness detection, bringing ",
    "path": "papers/22/02/2202.02751.json",
    "total_tokens": 828,
    "tldr": "人类能够通过在一根管子中说话，产生具有攻击性的样本欺骗说话人识别等机器学习系统，这就是本文的贡献。"
}