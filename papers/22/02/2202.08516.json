{
    "title": "SAITS: Self-Attention-based Imputation for Time Series. (arXiv:2202.08516v4 [cs.LG] UPDATED)",
    "abstract": "Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weighted-combination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-",
    "link": "http://arxiv.org/abs/2202.08516",
    "context": "Title: SAITS: Self-Attention-based Imputation for Time Series. (arXiv:2202.08516v4 [cs.LG] UPDATED)\nAbstract: Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weighted-combination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-",
    "path": "papers/22/02/2202.08516.json",
    "total_tokens": 955,
    "translated_title": "SAITS: 基于自注意力机制的时间序列插值方法",
    "translated_abstract": "时间序列中的缺失数据常常成为深入分析的障碍。插值是一种常见的解决方法，其核心问题是如何确定缺失值。本文提出了一种新颖的基于自注意力机制的多元时间序列缺失值插值方法——SAITS。通过联合优化的方式训练，SAITS通过两个对角线掩码自注意力块的加权组合来学习缺失值。对角线掩码自注意力块可以明确地捕捉时间步长之间的时序依赖关系和特征相关性，从而提高了插值准确性和训练速度。同时，加权组合设计使得SAITS能够根据注意力图和缺失信息动态地分配来自两个对角线掩码自注意力块的学习表示的权重。广泛的实验证明，SAITS在时间序列插值方面优于现有的方法。",
    "tldr": "SAITS是一种基于自注意力机制的多元时间序列缺失值插值方法，通过两个对角线掩码自注意力块的加权组合，能够明确地捕捉时间步长之间的时序依赖关系和特征相关性，从而提高了插值准确性和训练速度。",
    "en_tdlr": "SAITS is a novel self-attention-based method for missing value imputation in multivariate time series, which learns missing values from a weighted combination of two diagonally-masked self-attention blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, thereby improving imputation accuracy and training speed. SAITS dynamically assigns weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments demonstrate that SAITS outperforms state-of-the-art methods in time series imputation."
}