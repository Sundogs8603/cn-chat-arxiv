{
    "title": "BagPipe: Accelerating Deep Recommendation Model Training. (arXiv:2202.12429v3 [cs.DC] UPDATED)",
    "abstract": "Deep learning based recommendation models (DLRM) are widely used in several business critical applications. Training such recommendation models efficiently is challenging because they contain billions of embedding-based parameters, leading to significant overheads from embedding access. By profiling existing systems for DLRM training, we observe that around 75\\% of the iteration time is spent on embedding access and model synchronization. Our key insight in this paper is that embedding access has a specific structure which can be used to accelerate training. We observe that embedding accesses are heavily skewed, with around 1\\% of embeddings representing more than 92\\% of total accesses. Further, we observe that during offline training we can lookahead at future batches to determine exactly which embeddings will be needed at what iteration in the future. Based on these insights, we develop Bagpipe, a system for training deep recommendation models that uses caching and prefetching to ov",
    "link": "http://arxiv.org/abs/2202.12429",
    "context": "Title: BagPipe: Accelerating Deep Recommendation Model Training. (arXiv:2202.12429v3 [cs.DC] UPDATED)\nAbstract: Deep learning based recommendation models (DLRM) are widely used in several business critical applications. Training such recommendation models efficiently is challenging because they contain billions of embedding-based parameters, leading to significant overheads from embedding access. By profiling existing systems for DLRM training, we observe that around 75\\% of the iteration time is spent on embedding access and model synchronization. Our key insight in this paper is that embedding access has a specific structure which can be used to accelerate training. We observe that embedding accesses are heavily skewed, with around 1\\% of embeddings representing more than 92\\% of total accesses. Further, we observe that during offline training we can lookahead at future batches to determine exactly which embeddings will be needed at what iteration in the future. Based on these insights, we develop Bagpipe, a system for training deep recommendation models that uses caching and prefetching to ov",
    "path": "papers/22/02/2202.12429.json",
    "total_tokens": 905,
    "translated_title": "BagPipe：加速深度推荐模型训练",
    "translated_abstract": "基于深度学习的推荐模型（DLRM）广泛应用于几个关键的商业应用。高效地训练这种推荐模型具有挑战性，因为它们包含数十亿个基于嵌入的参数，从嵌入访问中导致了显著的开销。通过对现有的DLRM训练系统进行分析，我们观察到约75％的迭代时间用于嵌入访问和模型同步。本文的关键见解是嵌入访问具有特定的结构，可以用于加速训练。我们观察到嵌入访问具有严重的偏斜性，约1％的嵌入表示了超过92％的总访问量。此外，我们观察到在离线训练期间，我们可以预测未来批次来确定将在将来的何时迭代需要哪些嵌入。基于这些见解，我们开发了Bagpipe，一种用于训练深度推荐模型的系统，该系统利用缓存和预取来优化训练。",
    "tldr": "本文提出了BagPipe，一种用于加速深度推荐模型训练的系统。该系统利用嵌入访问的特定结构，通过缓存和预取的方式优化训练，实现了对推荐模型的高效训练。"
}