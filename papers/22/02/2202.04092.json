{
    "title": "Machine Explanations and Human Understanding. (arXiv:2202.04092v3 [cs.AI] UPDATED)",
    "abstract": "Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human understanding, and show how human intuitions play a central role in enabling human understanding. Specifically, we identify three core concepts of interest that cover all existing quantitative measures of understanding in the context of human-AI decision making: task decision boundary, model decision boundary, and model error. Our key result is that without assumptions about task-specific intuitions, explanations may potentially improve human understanding of model decision boundary, bu",
    "link": "http://arxiv.org/abs/2202.04092",
    "context": "Title: Machine Explanations and Human Understanding. (arXiv:2202.04092v3 [cs.AI] UPDATED)\nAbstract: Explanations are hypothesized to improve human understanding of machine learning models and achieve a variety of desirable outcomes, ranging from model debugging to enhancing human decision making. However, empirical studies have found mixed and even negative results. An open question, therefore, is under what conditions explanations can improve human understanding and in what way. Using adapted causal diagrams, we provide a formal characterization of the interplay between machine explanations and human understanding, and show how human intuitions play a central role in enabling human understanding. Specifically, we identify three core concepts of interest that cover all existing quantitative measures of understanding in the context of human-AI decision making: task decision boundary, model decision boundary, and model error. Our key result is that without assumptions about task-specific intuitions, explanations may potentially improve human understanding of model decision boundary, bu",
    "path": "papers/22/02/2202.04092.json",
    "total_tokens": 931,
    "translated_title": "机器解释和人类理解",
    "translated_abstract": "解释被假设可以提高人类对机器学习模型的理解，并实现各种有益的结果，从模型调试到增强人类决策制定。然而，实证研究发现了不一致甚至负面的结果。因此，一个开放的问题是在什么条件下解释可以提高人类的理解，并以何种方式。使用改进的因果图表，我们提供了机器解释和人类理解之间相互作用的正式特征化，并展示了人类直觉在启用人类理解中发挥了核心作用。具体而言，我们确定了三个核心概念，涵盖了所有现有量化理解的措施，即在人类与人工智能决策制定的背景下的任务决策边界、模型决策边界和模型错误。我们的关键结果是，如果没有关于特定任务直觉的假设，解释可能会潜在地提高人类对模型决策边界的理解，但对于任务决策边界和模型错误，则没有充分的证据表明解释可以提高人类理解。",
    "tldr": "研究讨论了机器解释和人类理解之间的相互作用，并确定了三个核心概念。结果显示，在没有关于特定任务直觉的假设下，解释可提高人类对模型决策边界的理解，但对任务决策边界和模型错误则没有充分证据支持。",
    "en_tdlr": "This paper discusses the interaction between machine explanations and human understanding, and identifies three core concepts. Results show that without assumptions about task-specific intuitions, explanations may improve human understanding of model decision boundary, but there is not sufficient evidence to support their improvement on task decision boundary and model error."
}