{
    "title": "On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks. (arXiv:2202.03277v2 [cs.LG] UPDATED)",
    "abstract": "While the literature on security attacks and defense of Machine Learning (ML) systems mostly focuses on unrealistic adversarial examples, recent research has raised concern about the under-explored field of realistic adversarial attacks and their implications on the robustness of real-world systems. Our paper paves the way for a better understanding of adversarial robustness against realistic attacks and makes two major contributions. First, we conduct a study on three real-world use cases (text classification, botnet detection, malware detection)) and five datasets in order to evaluate whether unrealistic adversarial examples can be used to protect models against realistic examples. Our results reveal discrepancies across the use cases, where unrealistic examples can either be as effective as the realistic ones or may offer only limited improvement. Second, to explain these results, we analyze the latent representation of the adversarial examples generated with realistic and unrealist",
    "link": "http://arxiv.org/abs/2202.03277",
    "context": "Title: On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks. (arXiv:2202.03277v2 [cs.LG] UPDATED)\nAbstract: While the literature on security attacks and defense of Machine Learning (ML) systems mostly focuses on unrealistic adversarial examples, recent research has raised concern about the under-explored field of realistic adversarial attacks and their implications on the robustness of real-world systems. Our paper paves the way for a better understanding of adversarial robustness against realistic attacks and makes two major contributions. First, we conduct a study on three real-world use cases (text classification, botnet detection, malware detection)) and five datasets in order to evaluate whether unrealistic adversarial examples can be used to protect models against realistic examples. Our results reveal discrepancies across the use cases, where unrealistic examples can either be as effective as the realistic ones or may offer only limited improvement. Second, to explain these results, we analyze the latent representation of the adversarial examples generated with realistic and unrealist",
    "path": "papers/22/02/2202.03277.json",
    "total_tokens": 941,
    "translated_title": "关于不现实对抗硬化策略对现实对抗攻击的实证有效性的探讨",
    "translated_abstract": "机器学习系统的安全攻击和防御文献大多关注于不现实的对抗样本。然而，最近的研究引起了人们对现实对抗攻击领域和其对现实系统鲁棒性的影响的关注。本文通过评估三个真实用例（文本分类、僵尸网络检测、恶意软件检测）和五个数据集，探究了不现实对抗样本能否用来保护模型免受现实对抗攻击。研究结果显示，在不同的用例中存在差异，不现实的例子可以像现实的例子一样有效，也可能只提供有限的改善。为了解释这些结果，我们分析了使用现实和不现实攻击生成的对抗例子的潜在表示，并显示它们存在显着的差异。这表明，目前的根据不现实攻击设计的防御机制可能无法有效地保护免受现实攻击。",
    "tldr": "本文研究了在三个真实用例和五个数据集上的不现实对抗样本对现实对抗攻击的保护效果，并发现防御不现实攻击的策略未必有效。",
    "en_tdlr": "This paper explores the empirical effectiveness of unrealistic adversarial hardening against realistic adversarial attacks by evaluating the protection capability of unrealistic adversarial examples on three real-world use cases and five datasets. Their findings suggest that the current defense mechanisms designed against unrealistic attacks may not be effective in protecting against realistic ones."
}