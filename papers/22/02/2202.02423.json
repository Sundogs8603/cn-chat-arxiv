{
    "title": "Improved Information Theoretic Generalization Bounds for Distributed and Federated Learning. (arXiv:2202.02423v2 [cs.IT] UPDATED)",
    "abstract": "We consider information-theoretic bounds on expected generalization error for statistical learning problems in a networked setting. In this setting, there are $K$ nodes, each with its own independent dataset, and the models from each node have to be aggregated into a final centralized model. We consider both simple averaging of the models as well as more complicated multi-round algorithms. We give upper bounds on the expected generalization error for a variety of problems, such as those with Bregman divergence or Lipschitz continuous losses, that demonstrate an improved dependence of $1/K$ on the number of nodes. These \"per node\" bounds are in terms of the mutual information between the training dataset and the trained weights at each node, and are therefore useful in describing the generalization properties inherent to having communication or privacy constraints at each node.",
    "link": "http://arxiv.org/abs/2202.02423",
    "context": "Title: Improved Information Theoretic Generalization Bounds for Distributed and Federated Learning. (arXiv:2202.02423v2 [cs.IT] UPDATED)\nAbstract: We consider information-theoretic bounds on expected generalization error for statistical learning problems in a networked setting. In this setting, there are $K$ nodes, each with its own independent dataset, and the models from each node have to be aggregated into a final centralized model. We consider both simple averaging of the models as well as more complicated multi-round algorithms. We give upper bounds on the expected generalization error for a variety of problems, such as those with Bregman divergence or Lipschitz continuous losses, that demonstrate an improved dependence of $1/K$ on the number of nodes. These \"per node\" bounds are in terms of the mutual information between the training dataset and the trained weights at each node, and are therefore useful in describing the generalization properties inherent to having communication or privacy constraints at each node.",
    "path": "papers/22/02/2202.02423.json",
    "total_tokens": 784,
    "translated_title": "改进的分布式和联邦学习中的信息论泛化界限",
    "translated_abstract": "我们考虑了网络环境下统计学习问题的信息论界限，该环境中有K个节点，每个节点都有自己的独立数据集，并且每个节点的模型必须被聚合成一个最终的集中模型。我们考虑了简单的模型平均以及更复杂的多轮算法。我们给出了一系列问题的期望泛化误差的上界，例如具有Bregman散度或Lipschitz连续损失的问题，在节点数量上展示了对1/K的改进依赖。这些“每个节点”的界限是关于训练数据集与每个节点训练权重之间的互信息，因此对于描述在每个节点具有通信或隐私约束的泛化性能是有用的。",
    "tldr": "该论文研究了分布式和联邦学习中的信息论泛化界限，提出了对于具有通信或隐私约束的问题，与节点数量成反比的改进上界。",
    "en_tdlr": "This paper investigates information-theoretic generalization bounds in distributed and federated learning, and presents improved upper bounds that have an inverse dependence on the number of nodes for problems with communication or privacy constraints."
}