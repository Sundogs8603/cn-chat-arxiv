{
    "title": "Near-Optimal Learning of Extensive-Form Games with Imperfect Information. (arXiv:2202.01752v2 [cs.LG] UPDATED)",
    "abstract": "This paper resolves the open question of designing near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback. We present the first line of algorithms that require only $\\widetilde{\\mathcal{O}}((XA+YB)/\\varepsilon^2)$ episodes of play to find an $\\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\\widetilde{\\mathcal{O}}((X^2A+Y^2B)/\\varepsilon^2)$ by a factor of $\\widetilde{\\mathcal{O}}(\\max\\{X, Y\\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \\emph{balanced exploration policies} into their classical counterparts. We also extend our results t",
    "link": "http://arxiv.org/abs/2202.01752",
    "context": "Title: Near-Optimal Learning of Extensive-Form Games with Imperfect Information. (arXiv:2202.01752v2 [cs.LG] UPDATED)\nAbstract: This paper resolves the open question of designing near-optimal algorithms for learning imperfect-information extensive-form games from bandit feedback. We present the first line of algorithms that require only $\\widetilde{\\mathcal{O}}((XA+YB)/\\varepsilon^2)$ episodes of play to find an $\\varepsilon$-approximate Nash equilibrium in two-player zero-sum games, where $X,Y$ are the number of information sets and $A,B$ are the number of actions for the two players. This improves upon the best known sample complexity of $\\widetilde{\\mathcal{O}}((X^2A+Y^2B)/\\varepsilon^2)$ by a factor of $\\widetilde{\\mathcal{O}}(\\max\\{X, Y\\})$, and matches the information-theoretic lower bound up to logarithmic factors. We achieve this sample complexity by two new algorithms: Balanced Online Mirror Descent, and Balanced Counterfactual Regret Minimization. Both algorithms rely on novel approaches of integrating \\emph{balanced exploration policies} into their classical counterparts. We also extend our results t",
    "path": "papers/22/02/2202.01752.json",
    "total_tokens": 945,
    "translated_title": "不完美信息博弈中的近似最优学习",
    "translated_abstract": "本文解决了学习不完美信息广义博弈的近似最优算法设计的开放性问题。我们提出了第一种算法系列，仅需要 $\\widetilde{\\mathcal{O}}((XA+YB)/\\varepsilon^2)$ 局游戏即可在两人零和博弈中找到一个 $\\varepsilon$-近似纳什均衡，其中 $X,Y$ 是信息集的数量，$A,B$ 是两名玩家的行动数。这比已知的样本复杂度 $\\widetilde{\\mathcal{O}}((X^2A+Y^2B)/\\varepsilon^2)$ 有着 $\\widetilde{\\mathcal{O}}(\\max\\{X, Y\\})$ 的巨大改进，并且在对数因子内与信息理论下限一致。我们通过两种新算法实现了这种样本复杂度：平衡在线镜面下降和平衡反事实后悔最小化。这两种算法都依赖于将“平衡探索策略”集成到它们的经典对手中的新方法。此外，我们还将我们的结果扩展到了更广泛的支持不完美信息博弈的二人博弈和多人博弈中。",
    "tldr": "本文提出了一种新的算法系列，可以更快速地在不完美信息广义博弈中找到一个近似最优解。",
    "en_tdlr": "This paper proposes a new series of algorithms that can find an approximate optimal solution more quickly in imperfect-information extensive-form games."
}