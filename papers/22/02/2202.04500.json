{
    "title": "Contextualize Me -- The Case for Context in Reinforcement Learning. (arXiv:2202.04500v2 [cs.LG] UPDATED)",
    "abstract": "While Reinforcement Learning ( RL) has made great strides towards solving increasingly complicated problems, many algorithms are still brittle to even slight environmental changes. Contextual Reinforcement Learning (cRL) provides a framework to model such changes in a principled manner, thereby enabling flexible, precise and interpretable task specification and generation. Our goal is to show how the framework of cRL contributes to improving zero-shot generalization in RL through meaningful benchmarks and structured reasoning about generalization tasks. We confirm the insight that optimal behavior in cRL requires context information, as in other related areas of partial observability. To empirically validate this in the cRL framework, we provide various context-extended versions of common RL environments. They are part of the first benchmark library, CARL, designed for generalization based on cRL extensions of popular benchmarks, which we propose as a testbed to further study general a",
    "link": "http://arxiv.org/abs/2202.04500",
    "context": "Title: Contextualize Me -- The Case for Context in Reinforcement Learning. (arXiv:2202.04500v2 [cs.LG] UPDATED)\nAbstract: While Reinforcement Learning ( RL) has made great strides towards solving increasingly complicated problems, many algorithms are still brittle to even slight environmental changes. Contextual Reinforcement Learning (cRL) provides a framework to model such changes in a principled manner, thereby enabling flexible, precise and interpretable task specification and generation. Our goal is to show how the framework of cRL contributes to improving zero-shot generalization in RL through meaningful benchmarks and structured reasoning about generalization tasks. We confirm the insight that optimal behavior in cRL requires context information, as in other related areas of partial observability. To empirically validate this in the cRL framework, we provide various context-extended versions of common RL environments. They are part of the first benchmark library, CARL, designed for generalization based on cRL extensions of popular benchmarks, which we propose as a testbed to further study general a",
    "path": "papers/22/02/2202.04500.json",
    "total_tokens": 897,
    "translated_title": "情境在强化学习中的重要性--情境强化学习框架的案例分析",
    "translated_abstract": "虽然强化学习（RL）在解决越来越复杂的问题方面取得了重大进展，但许多算法仍然对即使微小的环境变化也非常脆弱。情境强化学习（cRL）提供了一种框架，以原则性的方式建模这种变化，从而实现了灵活、精确和可解释的任务规范和生成。我们的目标是展示cRL框架如何通过有意义的基准和关于泛化任务的结构化推理，为改进RL中的零-shot泛化贡献。我们确认在cRL中最优行为需要上下文信息的洞察力，就像其他相关的部分可观察性领域一样。为了在cRL框架中从实证上验证这一点，我们提供了常见RL环境的多种情境扩展版本。它们是首个基准库CARL的一部分，该库旨在基于cRL扩展的普遍基准进行泛化研究。",
    "tldr": "情境强化学习提供了灵活、精确和可解释的任务规范和生成，可帮助改进RL中的零-shot泛化，需要上下文信息的洞察力。"
}