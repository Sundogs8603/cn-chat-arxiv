{
    "title": "Bayesian Active Learning for Discrete Latent Variable Models. (arXiv:2202.13426v2 [cs.LG] UPDATED)",
    "abstract": "Active learning seeks to reduce the amount of data required to fit the parameters of a model, thus forming an important class of techniques in modern machine learning. However, past work on active learning has largely overlooked latent variable models, which play a vital role in neuroscience, psychology, and a variety of other engineering and scientific disciplines. Here we address this gap by proposing a novel framework for maximum-mutual-information input selection for discrete latent variable regression models. We first apply our method to a class of models known as \"mixtures of linear regressions\" (MLR). While it is well known that active learning confers no advantage for linear-Gaussian regression models, we use Fisher information to show analytically that active learning can nevertheless achieve large gains for mixtures of such models, and we validate this improvement using both simulations and real-world data. We then consider a powerful class of temporally structured latent var",
    "link": "http://arxiv.org/abs/2202.13426",
    "context": "Title: Bayesian Active Learning for Discrete Latent Variable Models. (arXiv:2202.13426v2 [cs.LG] UPDATED)\nAbstract: Active learning seeks to reduce the amount of data required to fit the parameters of a model, thus forming an important class of techniques in modern machine learning. However, past work on active learning has largely overlooked latent variable models, which play a vital role in neuroscience, psychology, and a variety of other engineering and scientific disciplines. Here we address this gap by proposing a novel framework for maximum-mutual-information input selection for discrete latent variable regression models. We first apply our method to a class of models known as \"mixtures of linear regressions\" (MLR). While it is well known that active learning confers no advantage for linear-Gaussian regression models, we use Fisher information to show analytically that active learning can nevertheless achieve large gains for mixtures of such models, and we validate this improvement using both simulations and real-world data. We then consider a powerful class of temporally structured latent var",
    "path": "papers/22/02/2202.13426.json",
    "total_tokens": 1121,
    "translated_title": "离散潜变量模型的贝叶斯主动学习",
    "translated_abstract": "主动学习旨在减少拟合模型参数所需的数据量，因此成为现代机器学习中的重要技术类别。然而，过去的主动学习研究往往忽视了在神经科学、心理学和各种工程和科学学科中发挥关键作用的潜变量模型。本文提出了一种新的框架，用于离散潜变量回归模型的最大相互信息输入选择。我们首先将我们的方法应用于一类称为“线性回归混合物”的模型。虽然已知对于线性高斯回归模型，主动学习并不具有优势，但我们使用Fisher信息进行分析，表明即使对于这种混合模型，主动学习仍然可以取得巨大的收益，并使用模拟和真实数据对此进行了验证。然后，我们考虑了一类强大的时间结构潜变量模型，并展示了如何将我们的框架调整为在选择过程中融入时态依赖性。总的来说，我们的工作展示了主动学习在一个新领域的有效性，并提供了一个通用的框架，可应用于广泛的离散潜变量模型，只需进行较少的修改。",
    "tldr": "本文提出了一个新的框架，用于离散潜变量回归模型的最大相互信息输入选择。通过对线性回归混合物模型的Fisher信息分析，证明在这种情况下主动学习可以取得巨大的收益。同时，我们考虑了一类强大的时间结构潜变量模型，并展示了如何将我们的框架调整为在选择过程中融入时态依赖性。",
    "en_tdlr": "This paper proposes a novel framework for maximum-mutual-information input selection for discrete latent variable regression models. The authors demonstrate that active learning can achieve significant gains for mixtures of linear regressions models using Fisher information analysis. They also adapt their framework to incorporate temporal dependence in the selection process for temporally structured latent variable models."
}