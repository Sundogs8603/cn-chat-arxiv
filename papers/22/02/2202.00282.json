{
    "title": "Stabilizing the LIF Neuron Training. (arXiv:2202.00282v3 [cs.NE] UPDATED)",
    "abstract": "Spiking Neuromorphic Computing uses binary activity to improve Artificial Intelligence energy efficiency. However, the non-smoothness of binary activity requires approximate gradients, known as Surrogate Gradients (SG), to close the performance gap with Deep Learning. Several SG have been proposed in the literature, but it remains unclear how to determine the best SG for a given task and network. Good performance can be achieved with most SG shapes, after a costly search of hyper-parameters. Thus, we aim at experimentally and theoretically define the best SG across different stress tests, to reduce future need of grid search. To understand the gap for this line of work, we show that more complex tasks and networks need more careful choice of SG, even if overall the derivative of the fast sigmoid outperforms other SG across tasks and networks, for a wide range of learning rates. We therefore design a stability based theoretical method to choose initialization and SG shape before trainin",
    "link": "http://arxiv.org/abs/2202.00282",
    "context": "Title: Stabilizing the LIF Neuron Training. (arXiv:2202.00282v3 [cs.NE] UPDATED)\nAbstract: Spiking Neuromorphic Computing uses binary activity to improve Artificial Intelligence energy efficiency. However, the non-smoothness of binary activity requires approximate gradients, known as Surrogate Gradients (SG), to close the performance gap with Deep Learning. Several SG have been proposed in the literature, but it remains unclear how to determine the best SG for a given task and network. Good performance can be achieved with most SG shapes, after a costly search of hyper-parameters. Thus, we aim at experimentally and theoretically define the best SG across different stress tests, to reduce future need of grid search. To understand the gap for this line of work, we show that more complex tasks and networks need more careful choice of SG, even if overall the derivative of the fast sigmoid outperforms other SG across tasks and networks, for a wide range of learning rates. We therefore design a stability based theoretical method to choose initialization and SG shape before trainin",
    "path": "papers/22/02/2202.00282.json",
    "total_tokens": 893,
    "translated_title": "稳定LIF神经元训练",
    "translated_abstract": "脉冲神经形态计算利用二进制活动来提高人工智能的能源效率。然而，二进制活动的非平滑性要求使用近似梯度，也称为替代梯度（SG），以弥合与深度学习的性能差距。文献中已提出了几种SG，但目前尚不清楚如何确定适合特定任务和网络的最佳SG。在昂贵的超参数搜索后，大多数SG形状都可以实现良好的性能。因此，我们旨在在不同的压力测试中实验证明最佳SG，并在实验和理论上减少未来对网格搜索的需求。为了理解该领域的差距，我们展示了更复杂的任务和网络需要更慎重地选择SG，即使整体上，快速Sigmoid函数的导数在各种学习率下表现优于其他SG。因此，在训练之前，我们设计了一种基于稳定性的理论方法来选择初始化和SG形状。",
    "tldr": "该论文研究了稳定LIF神经元训练的方法，通过实验和理论分析，确定了在不同任务和网络中选择最佳替代梯度的稳定性与效果的关系，减少了对超参数搜索的需求。",
    "en_tdlr": "This paper investigates methods for stabilizing the training of LIF neurons and determines the relationship between stability and effectiveness of choosing the best surrogate gradient for different tasks and networks through experimental and theoretical analysis, reducing the need for hyper-parameter search."
}