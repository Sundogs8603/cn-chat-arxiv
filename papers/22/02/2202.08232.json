{
    "title": "Quantum Lazy Training. (arXiv:2202.08232v6 [quant-ph] UPDATED)",
    "abstract": "In the training of over-parameterized model functions via gradient descent, sometimes the parameters do not change significantly and remain close to their initial values. This phenomenon is called lazy training, and motivates consideration of the linear approximation of the model function around the initial parameters. In the lazy regime, this linear approximation imitates the behavior of the parameterized function whose associated kernel, called the tangent kernel, specifies the training performance of the model. Lazy training is known to occur in the case of (classical) neural networks with large widths. In this paper, we show that the training of geometrically local parameterized quantum circuits enters the lazy regime for large numbers of qubits. More precisely, we prove bounds on the rate of changes of the parameters of such a geometrically local parameterized quantum circuit in the training process, and on the precision of the linear approximation of the associated quantum model ",
    "link": "http://arxiv.org/abs/2202.08232",
    "context": "Title: Quantum Lazy Training. (arXiv:2202.08232v6 [quant-ph] UPDATED)\nAbstract: In the training of over-parameterized model functions via gradient descent, sometimes the parameters do not change significantly and remain close to their initial values. This phenomenon is called lazy training, and motivates consideration of the linear approximation of the model function around the initial parameters. In the lazy regime, this linear approximation imitates the behavior of the parameterized function whose associated kernel, called the tangent kernel, specifies the training performance of the model. Lazy training is known to occur in the case of (classical) neural networks with large widths. In this paper, we show that the training of geometrically local parameterized quantum circuits enters the lazy regime for large numbers of qubits. More precisely, we prove bounds on the rate of changes of the parameters of such a geometrically local parameterized quantum circuit in the training process, and on the precision of the linear approximation of the associated quantum model ",
    "path": "papers/22/02/2202.08232.json",
    "total_tokens": 829,
    "translated_title": "量子懒惰训练",
    "translated_abstract": "在梯度下降训练超参数模型函数时，有时参数不会发生显着变化，保持接近其初始值。这种现象称为懒惰训练，并激发了对模型函数在初始参数周围的线性逼近的考虑。在懒惰阶段，线性逼近模拟了参数化函数的行为，其相关内核称为切向内核，指定了模型的训练性能。已知大宽度（经典）神经网络的情况下会出现懒惰训练。我们在本文中展示了在量子比特数量较大时，地理局部化参数化量子电路的训练进入懒惰阶段。更准确地说，我们证明了这种地理局部化参数化量子电路参数变化速率的限制，以及其相关量子模型的线性逼近的精度。",
    "tldr": "本文证明了当量子比特数量较大时，地理局部化参数化量子电路的训练进入懒惰阶段，限制了参数变化速率并保证了相应量子模型的线性逼近的精度。",
    "en_tdlr": "This paper proves that the training of geometrically local parameterized quantum circuits enters the lazy regime for large numbers of qubits, limiting the rate of parameter changes and ensuring the precision of the associated quantum model's linear approximation."
}