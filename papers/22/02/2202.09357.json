{
    "title": "ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!. (arXiv:2202.09357v2 [cs.LG] UPDATED)",
    "abstract": "We introduce ProxSkip -- a surprisingly simple and provably efficient method for minimizing the sum of a smooth ($f$) and an expensive nonsmooth proximable ($\\psi$) function. The canonical approach to solving such problems is via the proximal gradient descent (ProxGD) algorithm, which is based on the evaluation of the gradient of $f$ and the prox operator of $\\psi$ in each iteration. In this work we are specifically interested in the regime in which the evaluation of prox is costly relative to the evaluation of the gradient, which is the case in many applications. ProxSkip allows for the expensive prox operator to be skipped in most iterations: while its iteration complexity is $\\mathcal{O}\\left(\\kappa \\log \\frac{1}{\\varepsilon}\\right)$, where $\\kappa$ is the condition number of $f$, the number of prox evaluations is $\\mathcal{O}\\left(\\sqrt{\\kappa} \\log \\frac{1}{\\varepsilon}\\right)$ only. Our main motivation comes from federated learning, where evaluation of the gradient operator corre",
    "link": "http://arxiv.org/abs/2202.09357",
    "context": "Title: ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!. (arXiv:2202.09357v2 [cs.LG] UPDATED)\nAbstract: We introduce ProxSkip -- a surprisingly simple and provably efficient method for minimizing the sum of a smooth ($f$) and an expensive nonsmooth proximable ($\\psi$) function. The canonical approach to solving such problems is via the proximal gradient descent (ProxGD) algorithm, which is based on the evaluation of the gradient of $f$ and the prox operator of $\\psi$ in each iteration. In this work we are specifically interested in the regime in which the evaluation of prox is costly relative to the evaluation of the gradient, which is the case in many applications. ProxSkip allows for the expensive prox operator to be skipped in most iterations: while its iteration complexity is $\\mathcal{O}\\left(\\kappa \\log \\frac{1}{\\varepsilon}\\right)$, where $\\kappa$ is the condition number of $f$, the number of prox evaluations is $\\mathcal{O}\\left(\\sqrt{\\kappa} \\log \\frac{1}{\\varepsilon}\\right)$ only. Our main motivation comes from federated learning, where evaluation of the gradient operator corre",
    "path": "papers/22/02/2202.09357.json",
    "total_tokens": 1055,
    "translated_title": "ProxSkip: 是的！局部梯度步骤可以证明加速通信！终于！（arXiv:2202.09357v2[cs.LG] 已更新）",
    "tldr": "ProxSkip 是一种能够加速通信的高效算法，可以最小化光滑函数和昂贵不光滑可接近函数的和。这个算法跳过了昂贵的 prox 运算符，具有更少的 prox 评估而有效地解决了联邦学习中的问题。",
    "en_tdlr": "ProxSkip is an efficient algorithm that can accelerate communication and minimize the sum of a smooth function and an expensive nonsmooth proximable function by skipping the evaluation of the expensive prox operator in most iterations. ProxSkip's motivation comes from federated learning, where the evaluation of the gradient operator is subject to communication cost constraints."
}