{
    "title": "VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning. (arXiv:2202.10324v3 [cs.CV] UPDATED)",
    "abstract": "We propose VRL3, a powerful data-driven framework with a simple design for solving challenging visual deep reinforcement learning (DRL) tasks. We analyze a number of major obstacles in taking a data-driven approach, and present a suite of design principles, novel findings, and critical insights about data-driven visual DRL. Our framework has three stages: in stage 1, we leverage non-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations; in stage 2, we use offline RL data (e.g. a limited number of expert demonstrations) to convert the task-agnostic representations into more powerful task-specific representations; in stage 3, we fine-tune the agent with online RL. On a set of challenging hand manipulation tasks with sparse reward and realistic visual inputs, compared to the previous SOTA, VRL3 achieves an average of 780% better sample efficiency. And on the hardest task, VRL3 is 1220% more sample efficient (2440% when using a wider encoder) and solves the task with on",
    "link": "http://arxiv.org/abs/2202.10324",
    "context": "Title: VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning. (arXiv:2202.10324v3 [cs.CV] UPDATED)\nAbstract: We propose VRL3, a powerful data-driven framework with a simple design for solving challenging visual deep reinforcement learning (DRL) tasks. We analyze a number of major obstacles in taking a data-driven approach, and present a suite of design principles, novel findings, and critical insights about data-driven visual DRL. Our framework has three stages: in stage 1, we leverage non-RL datasets (e.g. ImageNet) to learn task-agnostic visual representations; in stage 2, we use offline RL data (e.g. a limited number of expert demonstrations) to convert the task-agnostic representations into more powerful task-specific representations; in stage 3, we fine-tune the agent with online RL. On a set of challenging hand manipulation tasks with sparse reward and realistic visual inputs, compared to the previous SOTA, VRL3 achieves an average of 780% better sample efficiency. And on the hardest task, VRL3 is 1220% more sample efficient (2440% when using a wider encoder) and solves the task with on",
    "path": "papers/22/02/2202.10324.json",
    "total_tokens": 1002,
    "translated_title": "VRL3：一种用于视觉深度强化学习的数据驱动框架",
    "translated_abstract": "本文提出了VRL3，这是一个采用简单设计解决具有挑战性的视觉深度强化学习（DRL）任务的强大数据驱动框架。作者分析了采用数据驱动方法的主要障碍，并提出了一系列的设计原则、新的发现和关于数据驱动视觉DRL的关键见解。该框架包括三个阶段：在第一阶段，作者利用非RL数据集（例如ImageNet）来学习任务无关的视觉表示；在第二阶段，作者利用离线RL数据（例如有限数量的专家演示）将任务无关的表示转化为更强大的任务特定的表示；在第三阶段，作者通过在线RL对智能体进行微调。在一组具有稀疏奖励和逼真视觉输入的挑战性手部操纵任务中，与之前的SOTA相比，VRL3的样本效率平均提高了780%。在最难的任务上，VRL3的样本效率提高了1220%（使用更宽的编码器，提高到2440%），并以超过SOTA的性能解决了该任务。",
    "tldr": "本文提出VRL3，一种数据驱动的框架，可用于解决具有挑战性的视觉深度强化学习任务。该框架包含三个阶段，并能在具有稀疏奖励和逼真视觉输入的手部操纵任务中显著提高样本效率。",
    "en_tdlr": "This paper proposes VRL3, a data-driven framework for solving challenging visual deep reinforcement learning tasks. The framework includes three stages and significantly improves sample efficiency on hand manipulation tasks with sparse reward and realistic visual inputs."
}