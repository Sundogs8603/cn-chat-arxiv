{
    "title": "On solutions of the distributional Bellman equation. (arXiv:2202.00081v3 [stat.ML] UPDATED)",
    "abstract": "In distributional reinforcement learning not only expected returns but the complete return distributions of a policy are taken into account. The return distribution for a fixed policy is given as the solution of an associated distributional Bellman equation. In this note we consider general distributional Bellman equations and study existence and uniqueness of their solutions as well as tail properties of return distributions. We give necessary and sufficient conditions for existence and uniqueness of return distributions and identify cases of regular variation. We link distributional Bellman equations to multivariate affine distributional equations. We show that any solution of a distributional Bellman equation can be obtained as the vector of marginal laws of a solution to a multivariate affine distributional equation. This makes the general theory of such equations applicable to the distributional reinforcement learning setting.",
    "link": "http://arxiv.org/abs/2202.00081",
    "context": "Title: On solutions of the distributional Bellman equation. (arXiv:2202.00081v3 [stat.ML] UPDATED)\nAbstract: In distributional reinforcement learning not only expected returns but the complete return distributions of a policy are taken into account. The return distribution for a fixed policy is given as the solution of an associated distributional Bellman equation. In this note we consider general distributional Bellman equations and study existence and uniqueness of their solutions as well as tail properties of return distributions. We give necessary and sufficient conditions for existence and uniqueness of return distributions and identify cases of regular variation. We link distributional Bellman equations to multivariate affine distributional equations. We show that any solution of a distributional Bellman equation can be obtained as the vector of marginal laws of a solution to a multivariate affine distributional equation. This makes the general theory of such equations applicable to the distributional reinforcement learning setting.",
    "path": "papers/22/02/2202.00081.json",
    "total_tokens": 906,
    "translated_title": "论分布贝尔曼方程的解",
    "translated_abstract": "在分布强化学习中，不仅要考虑预期回报，还要考虑策略的完整回报分布。对于固定的策略，其回报分布是相应分布贝尔曼方程的解。本文考虑一般的分布贝尔曼方程，研究解的存在性和唯一性以及回报分布的尾部性质。我们给出了存在和唯一性回报分布的必要和充分条件，并确定了正则变化的情况。我们将分布贝尔曼方程与多元仿射分布方程联系起来。我们表明，在多元仿射分布方程的解的条件下，任何分布贝尔曼方程的解都可以作为解的边缘律的向量来获得。这使得这种方程的一般理论适用于分布强化学习设置。",
    "tldr": "本文研究了分布贝尔曼方程的一般条件，包括解的存在唯一性和回报分布的尾部性质。将分布贝尔曼方程与多元仿射分布方程联系起来，发现任何分布贝尔曼方程的解都可以作为解的边缘律的向量来获得。这一理论适用于分布强化学习领域。",
    "en_tdlr": "This paper studies the general conditions of distributional Bellman equations, including the existence and uniqueness of solutions and tail properties of return distributions. It links distributional Bellman equations to multivariate affine distributional equations, and finds that any solution of a distributional Bellman equation can be obtained as the vector of marginal laws of a solution to a multivariate affine distributional equation. This theory is applicable to the field of distributional reinforcement learning."
}