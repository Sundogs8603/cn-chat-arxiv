{
    "title": "Differentially Private Stochastic Gradient Descent with Low-Noise. (arXiv:2209.04188v2 [stat.ML] UPDATED)",
    "abstract": "Modern machine learning algorithms aim to extract fine-grained information from data to provide accurate predictions, which often conflicts with the goal of privacy protection. This paper addresses the practical and theoretical importance of developing privacy-preserving machine learning algorithms that ensure good performance while preserving privacy. In this paper, we focus on the privacy and utility (measured by excess risk bounds) performances of differentially private stochastic gradient descent (SGD) algorithms in the setting of stochastic convex optimization. Specifically, we examine the pointwise problem in the low-noise setting for which we derive sharper excess risk bounds for the differentially private SGD algorithm. In the pairwise learning setting, we propose a simple differentially private SGD algorithm based on gradient perturbation. Furthermore, we develop novel utility bounds for the proposed algorithm, proving that it achieves optimal excess risk rates even for non-sm",
    "link": "http://arxiv.org/abs/2209.04188",
    "context": "Title: Differentially Private Stochastic Gradient Descent with Low-Noise. (arXiv:2209.04188v2 [stat.ML] UPDATED)\nAbstract: Modern machine learning algorithms aim to extract fine-grained information from data to provide accurate predictions, which often conflicts with the goal of privacy protection. This paper addresses the practical and theoretical importance of developing privacy-preserving machine learning algorithms that ensure good performance while preserving privacy. In this paper, we focus on the privacy and utility (measured by excess risk bounds) performances of differentially private stochastic gradient descent (SGD) algorithms in the setting of stochastic convex optimization. Specifically, we examine the pointwise problem in the low-noise setting for which we derive sharper excess risk bounds for the differentially private SGD algorithm. In the pairwise learning setting, we propose a simple differentially private SGD algorithm based on gradient perturbation. Furthermore, we develop novel utility bounds for the proposed algorithm, proving that it achieves optimal excess risk rates even for non-sm",
    "path": "papers/22/09/2209.04188.json",
    "total_tokens": 923,
    "translated_title": "具有低噪声的差分隐私随机梯度下降法",
    "translated_abstract": "现代机器学习算法旨在从数据中提取细粒度信息，以提供准确的预测，但这往往与保护隐私的目标相冲突。本文探讨了开发保护隐私的机器学习算法在确保良好性能的同时保护隐私的实际和理论重要性。我们在随机凸优化设置中，关注差分隐私随机梯度下降（SGD）算法在隐私性和效用性（通过过剩风险界限衡量）方面的性能。具体而言，我们研究了低噪声设置下的点问题，并得出了差分隐私SGD算法更精确的过剩风险界限。在成对学习设置中，我们提出了一种基于梯度扰动的简单差分隐私SGD算法。此外，我们还提出了所提算法的新型效用界限，证明它即使在非光滑情况下也能达到最优的过剩风险率。",
    "tldr": "本文研究了具有低噪声的差分隐私随机梯度下降法在点问题和成对学习中的性能，并推导出更精确的过剩风险界限。提出的算法基于梯度扰动，具有优化过剩风险率的最佳效果。",
    "en_tdlr": "This paper investigates the performance of differentially private stochastic gradient descent algorithms with low noise in both pointwise and pairwise learning settings. It derives sharper excess risk bounds and proposes a gradient perturbation-based algorithm that achieves optimal excess risk rates."
}