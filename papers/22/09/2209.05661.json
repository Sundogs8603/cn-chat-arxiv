{
    "title": "Unsupervised representation learning with recognition-parametrised probabilistic models. (arXiv:2209.05661v2 [cs.LG] UPDATED)",
    "abstract": "We introduce a new approach to probabilistic unsupervised learning based on the recognition-parametrised model (RPM): a normalised semi-parametric hypothesis class for joint distributions over observed and latent variables. Under the key assumption that observations are conditionally independent given latents, the RPM combines parametric prior and observation-conditioned latent distributions with non-parametric observation marginals. This approach leads to a flexible learnt recognition model capturing latent dependence between observations, without the need for an explicit, parametric generative model. The RPM admits exact maximum-likelihood learning for discrete latents, even for powerful neural-network-based recognition. We develop effective approximations applicable in the continuous-latent case. Experiments demonstrate the effectiveness of the RPM on high-dimensional data, learning image classification from weak indirect supervision; direct image-level latent Dirichlet allocation; ",
    "link": "http://arxiv.org/abs/2209.05661",
    "context": "Title: Unsupervised representation learning with recognition-parametrised probabilistic models. (arXiv:2209.05661v2 [cs.LG] UPDATED)\nAbstract: We introduce a new approach to probabilistic unsupervised learning based on the recognition-parametrised model (RPM): a normalised semi-parametric hypothesis class for joint distributions over observed and latent variables. Under the key assumption that observations are conditionally independent given latents, the RPM combines parametric prior and observation-conditioned latent distributions with non-parametric observation marginals. This approach leads to a flexible learnt recognition model capturing latent dependence between observations, without the need for an explicit, parametric generative model. The RPM admits exact maximum-likelihood learning for discrete latents, even for powerful neural-network-based recognition. We develop effective approximations applicable in the continuous-latent case. Experiments demonstrate the effectiveness of the RPM on high-dimensional data, learning image classification from weak indirect supervision; direct image-level latent Dirichlet allocation; ",
    "path": "papers/22/09/2209.05661.json",
    "total_tokens": 913,
    "translated_title": "无监督表征学习中的识别参数概率模型",
    "translated_abstract": "我们提出了一种基于识别参数模型（RPM）的概率无监督学习新方法：作为关于观察变量和潜在变量的联合分布的归一化半参数化假设类。在观察值在给定潜在变量的条件下是条件独立的关键假设下，RPM将参数先验和观测条件下的潜在分布与非参数观测边缘相结合。该方法可以得到灵活的学习识别模型，捕捉了观测之间的潜在相关性，而不需要显式的参数生成模型。对于离散潜变量，RPM允许进行精确的最大似然学习，即使是基于强大的神经网络识别。我们开发了适用于连续潜变量情况的有效近似方法。实验展示了RPM在高维数据上的有效性，学习从弱间接监督中的图像分类；直接图像级潜在狄利克雷分配的学习。",
    "tldr": "本文提出了一种基于识别参数模型的概率无监督学习新方法，可以灵活地学习识别模型，捕捉观测之间的潜在相关性，为图像分类和潜在分配问题提供了有效解决方案。",
    "en_tdlr": "This paper proposes a new approach to probabilistic unsupervised learning based on a recognition-parametrised model, which learns a flexible recognition model capturing latent dependence between observations without the need for an explicit generative model, providing effective solutions for image classification and latent allocation problems."
}