{
    "title": "Generalization in Neural Networks: A Broad Survey. (arXiv:2209.01610v2 [cs.LG] UPDATED)",
    "abstract": "This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Results on (1) sample generalization show that, in the case of ImageNet, nearly all the recent improvements reduced training error while overfitting stayed flat; with nearly all the training error eliminated, future progress will require a focus on reducing overfitting. Perspectives from statistics highlight how (2) distribution generalization can be viewed alternately as a change in sample weights or a change in the input-output relationship; thus, techniques that have been successful in domain generalization have the potential to be applied to difficult forms of sample or distribution generalization. Transfer learning approaches to (3) domain generalization are summarized, as are recent advances and the wealth of domain a",
    "link": "http://arxiv.org/abs/2209.01610",
    "context": "Title: Generalization in Neural Networks: A Broad Survey. (arXiv:2209.01610v2 [cs.LG] UPDATED)\nAbstract: This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Results on (1) sample generalization show that, in the case of ImageNet, nearly all the recent improvements reduced training error while overfitting stayed flat; with nearly all the training error eliminated, future progress will require a focus on reducing overfitting. Perspectives from statistics highlight how (2) distribution generalization can be viewed alternately as a change in sample weights or a change in the input-output relationship; thus, techniques that have been successful in domain generalization have the potential to be applied to difficult forms of sample or distribution generalization. Transfer learning approaches to (3) domain generalization are summarized, as are recent advances and the wealth of domain a",
    "path": "papers/22/09/2209.01610.json",
    "total_tokens": 949,
    "translated_title": "神经网络中的泛化：综述与分析",
    "translated_abstract": "本文综述了神经网络模型中不同抽象层次的概念、建模方法和最近的研究成果，包括样本、分布、领域、任务、模态和范围上的泛化。在样本泛化方面的研究结果显示，在ImageNet数据集的情况下，几乎所有的最新改进都减小了训练误差，而过拟合保持不变；随着几乎所有的训练误差被消除，未来的进展将需要集中关注减少过拟合。从统计学的角度来看，(2)分布泛化可以被看作是样本权重或输入输出关系的变化；因此，在领域泛化成功的技术有可能应用于困难的样本或分布泛化。本文总结了转移学习方法(3)领域泛化的应用，以及最近的进展和丰富的应用领域。",
    "tldr": "这篇论文总结了神经网络模型中不同抽象层次上的泛化问题及其方法，其中样本泛化已经取得了进展，但未来需要重点关注减少过拟合；分布泛化与领域泛化有相似之处，领域泛化方法可以应用于困难的样本或分布泛化。",
    "en_tdlr": "This paper surveys the concept and modeling approaches of neural network models in different levels of abstraction including generalization across samples, distributions, domains, tasks, modalities, and scopes. While recent improvements have reduced training error in sample generalization, reducing overfitting will be a future focus. Techniques successful in domain generalization have potential for difficult sample or distribution generalization."
}