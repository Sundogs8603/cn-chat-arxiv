{
    "title": "Optimal Regularized Online Allocation by Adaptive Re-Solving. (arXiv:2209.00399v2 [cs.LG] UPDATED)",
    "abstract": "This paper introduces a dual-based algorithm framework for solving the regularized online resource allocation problems, which have potentially non-concave cumulative rewards, hard resource constraints, and a non-separable regularizer. Under a strategy of adaptively updating the resource constraints, the proposed framework only requests approximate solutions to the empirical dual problems up to a certain accuracy and yet delivers an optimal logarithmic regret under a locally second-order growth condition. Surprisingly, a delicate analysis of the dual objective function enables us to eliminate the notorious log-log factor in regret bound. The flexible framework renders renowned and computationally fast algorithms immediately applicable, e.g., dual stochastic gradient descent. Additionally, an infrequent re-solving scheme is proposed, which significantly reduces computational demands without compromising the optimal regret performance. A worst-case square-root regret lower bound is establ",
    "link": "http://arxiv.org/abs/2209.00399",
    "context": "Title: Optimal Regularized Online Allocation by Adaptive Re-Solving. (arXiv:2209.00399v2 [cs.LG] UPDATED)\nAbstract: This paper introduces a dual-based algorithm framework for solving the regularized online resource allocation problems, which have potentially non-concave cumulative rewards, hard resource constraints, and a non-separable regularizer. Under a strategy of adaptively updating the resource constraints, the proposed framework only requests approximate solutions to the empirical dual problems up to a certain accuracy and yet delivers an optimal logarithmic regret under a locally second-order growth condition. Surprisingly, a delicate analysis of the dual objective function enables us to eliminate the notorious log-log factor in regret bound. The flexible framework renders renowned and computationally fast algorithms immediately applicable, e.g., dual stochastic gradient descent. Additionally, an infrequent re-solving scheme is proposed, which significantly reduces computational demands without compromising the optimal regret performance. A worst-case square-root regret lower bound is establ",
    "path": "papers/22/09/2209.00399.json",
    "total_tokens": 991,
    "translated_title": "自适应重求解的最优正则在线分配算法",
    "translated_abstract": "本文介绍了一种基于对偶的算法框架，用于解决具有潜在非凸累积奖励、硬资源约束和非分离正则化器的正则在线资源分配问题。在自适应更新资源约束的策略下，所提出的框架仅要求对经验对偶问题进行近似解，以达到一定的精度，并在局部二阶增长条件下实现了最优的对数遗憾。令人惊讶的是，对偶目标函数的精细分析使得我们能够消除遗憾界中的臭名昭著的对数对数因子。这种灵活的框架立即适用于著名的计算快速算法，例如对偶随机梯度下降。此外，还提出了一种不经常重新求解的方案，可以显著减少计算需求，且不会影响最优遗憾性能。同时建立了最坏情况下的平方根遗憾下界。",
    "tldr": "本文提出了一种自适应重求解的最优正则在线分配算法，该算法通过对偶方法解决了具有非凸累积奖励、硬资源约束和非分离正则化器的问题，并消除了遗憾界中的对数对数因子。该算法具有灵活性和高效性，并且通过不经常重新求解的方案降低了计算需求，同时保持了最优遗憾性能。"
}