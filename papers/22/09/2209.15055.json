{
    "title": "Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v4 [stat.ML] UPDATED)",
    "abstract": "We show that the representation cost of fully connected neural networks with homogeneous nonlinearities - which describes the implicit bias in function space of networks with $L_2$-regularization or with losses such as the cross-entropy - converges as the depth of the network goes to infinity to a notion of rank over nonlinear functions. We then inquire under which conditions the global minima of the loss recover the `true' rank of the data: we show that for too large depths the global minimum will be approximately rank 1 (underestimating the rank); we then argue that there is a range of depths which grows with the number of datapoints where the true rank is recovered. Finally, we discuss the effect of the rank of a classifier on the topology of the resulting class boundaries and show that autoencoders with optimal nonlinear rank are naturally denoising.",
    "link": "http://arxiv.org/abs/2209.15055",
    "context": "Title: Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v4 [stat.ML] UPDATED)\nAbstract: We show that the representation cost of fully connected neural networks with homogeneous nonlinearities - which describes the implicit bias in function space of networks with $L_2$-regularization or with losses such as the cross-entropy - converges as the depth of the network goes to infinity to a notion of rank over nonlinear functions. We then inquire under which conditions the global minima of the loss recover the `true' rank of the data: we show that for too large depths the global minimum will be approximately rank 1 (underestimating the rank); we then argue that there is a range of depths which grows with the number of datapoints where the true rank is recovered. Finally, we discuss the effect of the rank of a classifier on the topology of the resulting class boundaries and show that autoencoders with optimal nonlinear rank are naturally denoising.",
    "path": "papers/22/09/2209.15055.json",
    "total_tokens": 924,
    "translated_title": "大深度网络的隐式偏见：非线性函数的秩定义。",
    "translated_abstract": "我们展示了具有齐次非线性函数的全连接神经网络的表示成本——描述了具有$L_2$正则化或交叉熵等损失网络在函数空间中的隐式偏见——随着网络深度趋近于无穷大，会收敛到非线性函数的秩的概念。然后，我们探究了全局最小值在哪些条件下可以恢复“真实”数据秩：我们展示出，对于太大的深度，全局最小值会近似为秩1（低估秩）；我们随后论证了有一系列深度，随着数据点数量增加，可以恢复真实秩。最后，我们讨论了分类器秩对结果类边界的拓扑结构的影响，并展示了具有最佳非线性秩的自编码器具有自然去噪的特性。",
    "tldr": "本文研究了全连接神经网络的表示成本和深度之间的关系，发现其会收敛到非线性函数的秩的概念。同时，发现在一定的深度范围内，全局最小值可以恢复真实的数据秩，并探讨了分类器秩对类边界拓扑结构的影响。",
    "en_tdlr": "This paper studies the relationship between the representation cost and depth of fully connected neural networks with homogeneous nonlinearities, and shows that it converges to a notion of rank over nonlinear functions. It also investigates the conditions under which the global minima can recover the true rank, and discusses the effect of the rank of a classifier on the topology of the resulting class boundaries. Finally, this paper shows that autoencoders with optimal nonlinear rank are naturally denoising."
}