{
    "title": "Data Poisoning Attacks Against Multimodal Encoders. (arXiv:2209.15266v2 [cs.CR] UPDATED)",
    "abstract": "Recently, the newly emerged multimodal models, which leverage both visual and linguistic modalities to train powerful encoders, have gained increasing attention. However, learning from a large-scale unlabeled dataset also exposes the model to the risk of potential poisoning attacks, whereby the adversary aims to perturb the model's training data to trigger malicious behaviors in it. In contrast to previous work, only poisoning visual modality, in this work, we take the first step to studying poisoning attacks against multimodal models in both visual and linguistic modalities. Specially, we focus on answering two questions: (1) Is the linguistic modality also vulnerable to poisoning attacks? and (2) Which modality is most vulnerable? To answer the two questions, we propose three types of poisoning attacks against multimodal models. Extensive evaluations on different datasets and model architectures show that all three attacks can achieve significant attack performance while maintaining ",
    "link": "http://arxiv.org/abs/2209.15266",
    "context": "Title: Data Poisoning Attacks Against Multimodal Encoders. (arXiv:2209.15266v2 [cs.CR] UPDATED)\nAbstract: Recently, the newly emerged multimodal models, which leverage both visual and linguistic modalities to train powerful encoders, have gained increasing attention. However, learning from a large-scale unlabeled dataset also exposes the model to the risk of potential poisoning attacks, whereby the adversary aims to perturb the model's training data to trigger malicious behaviors in it. In contrast to previous work, only poisoning visual modality, in this work, we take the first step to studying poisoning attacks against multimodal models in both visual and linguistic modalities. Specially, we focus on answering two questions: (1) Is the linguistic modality also vulnerable to poisoning attacks? and (2) Which modality is most vulnerable? To answer the two questions, we propose three types of poisoning attacks against multimodal models. Extensive evaluations on different datasets and model architectures show that all three attacks can achieve significant attack performance while maintaining ",
    "path": "papers/22/09/2209.15266.json",
    "total_tokens": 863,
    "translated_title": "多模态编码器遭受数据污染攻击",
    "translated_abstract": "近年来，结合视觉和语言模态进行训练的多模态模型已经引起了越来越多的关注。然而，从大规模无标签数据集中学习也会使模型面临潜在的数据污染攻击风险，攻击者旨在扰动模型的训练数据以触发恶意行为。本文针对多模态模型的视觉和语言模态进行了数据污染攻击的探究，特别关注两个问题：（1）语言模态是否也容易受到数据污染攻击？（2）哪种模态最容易受到攻击？为了回答这两个问题，我们提出了三种多模态攻击类型。在不同的数据集和模型架构上进行了全面的评估，结果显示所有三种攻击都能够在维持模型准确性的同时，成功地实现了显著的攻击性能。",
    "tldr": "本文研究了多模态编码器遭受数据污染攻击的情况，提出了三种攻击类型，并发现同时进行视觉和语言模态污染的攻击能够实现较好攻击性能。",
    "en_tdlr": "This paper investigates the vulnerability of multimodal encoders to data poisoning attacks, proposes three types of attacks, and finds that attacking both visual and linguistic modalities can achieve significant attack performance while maintaining model accuracy."
}