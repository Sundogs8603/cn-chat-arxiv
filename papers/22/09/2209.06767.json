{
    "title": "Parameter-Efficient Finetuning for Robust Continual Multilingual Learning. (arXiv:2209.06767v3 [cs.CL] UPDATED)",
    "abstract": "We introduce and study the problem of Continual Multilingual Learning (CML) where a previously trained multilingual model is periodically updated using new data arriving in stages. If the new data is present only in a subset of languages, we find that the resulting model shows improved performance only on the languages included in the latest update (and a few closely related languages) while its performance on all the remaining languages degrade significantly. We address this challenge by proposing LAFT-URIEL, a parameter-efficient finetuning strategy which aims to increase the number of languages on which the model improves after an update, while reducing the magnitude of loss in performance for the remaining languages. LAFT-URIEL uses linguistic knowledge to balance overfitting and knowledge sharing across languages, allowing for an additional 25% of task languages to see an improvement in performance after an update, while also reducing the average magnitude of losses on the remaini",
    "link": "http://arxiv.org/abs/2209.06767",
    "context": "Title: Parameter-Efficient Finetuning for Robust Continual Multilingual Learning. (arXiv:2209.06767v3 [cs.CL] UPDATED)\nAbstract: We introduce and study the problem of Continual Multilingual Learning (CML) where a previously trained multilingual model is periodically updated using new data arriving in stages. If the new data is present only in a subset of languages, we find that the resulting model shows improved performance only on the languages included in the latest update (and a few closely related languages) while its performance on all the remaining languages degrade significantly. We address this challenge by proposing LAFT-URIEL, a parameter-efficient finetuning strategy which aims to increase the number of languages on which the model improves after an update, while reducing the magnitude of loss in performance for the remaining languages. LAFT-URIEL uses linguistic knowledge to balance overfitting and knowledge sharing across languages, allowing for an additional 25% of task languages to see an improvement in performance after an update, while also reducing the average magnitude of losses on the remaini",
    "path": "papers/22/09/2209.06767.json",
    "total_tokens": 901,
    "translated_title": "鲁棒性持续多语言学习的参数高效微调",
    "translated_abstract": "我们介绍并研究了鲁棒性持续多语言学习（CML）的问题，即周期性使用新到达的数据对先前训练的多语言模型进行更新。如果新数据仅存在于语种的子集中，我们发现所得到的模型仅在最新更新中包括的语种（和一些紧密相关的语种）上表现出改进，而其在所有剩余语种上的性能则显著下降。我们通过提出LAFT-URIEL来解决这个挑战，这是一种参数高效的微调策略，旨在增加模型更新后在语种上的性能改进数量，同时减少剩余语种性能下降的程度。LAFT-URIEL利用语言知识在语种之间实现过拟合和知识共享的平衡，使得额外的25%任务语种在更新后看到性能改进，同时减小了剩余语种的平均性能损失。",
    "tldr": "提出了一种参数高效微调策略LAFT-URIEL，用于鲁棒性持续多语言学习，通过利用语言知识平衡过拟合和知识共享，使模型在更新后在更多语种上表现出改进，同时减小了剩余语种的性能损失。",
    "en_tdlr": "Proposed a parameter-efficient finetuning strategy, LAFT-URIEL, for robust continual multilingual learning, which balances overfitting and knowledge sharing through linguistic knowledge, resulting in improved performance on more languages after updates and reduced performance degradation on remaining languages."
}