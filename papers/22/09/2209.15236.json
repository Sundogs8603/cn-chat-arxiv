{
    "title": "Language-Family Adapters for Low-Resource Multilingual Neural Machine Translation. (arXiv:2209.15236v3 [cs.CL] UPDATED)",
    "abstract": "Large multilingual models trained with self-supervision achieve state-of-the-art results in a wide range of natural language processing tasks. Self-supervised pretrained models are often fine-tuned on parallel data from one or multiple language pairs for machine translation. Multilingual fine-tuning improves performance on low-resource languages but requires modifying the entire model and can be prohibitively expensive. Training a new adapter on each language pair or training a single adapter on all language pairs without updating the pretrained model has been proposed as a parameter-efficient alternative. However, the former does not permit any sharing between languages, while the latter shares parameters for all languages and is susceptible to negative interference. In this paper, we propose training language-family adapters on top of mBART-50 to facilitate cross-lingual transfer. Our approach outperforms related baselines, yielding higher translation scores on average when translati",
    "link": "http://arxiv.org/abs/2209.15236",
    "context": "Title: Language-Family Adapters for Low-Resource Multilingual Neural Machine Translation. (arXiv:2209.15236v3 [cs.CL] UPDATED)\nAbstract: Large multilingual models trained with self-supervision achieve state-of-the-art results in a wide range of natural language processing tasks. Self-supervised pretrained models are often fine-tuned on parallel data from one or multiple language pairs for machine translation. Multilingual fine-tuning improves performance on low-resource languages but requires modifying the entire model and can be prohibitively expensive. Training a new adapter on each language pair or training a single adapter on all language pairs without updating the pretrained model has been proposed as a parameter-efficient alternative. However, the former does not permit any sharing between languages, while the latter shares parameters for all languages and is susceptible to negative interference. In this paper, we propose training language-family adapters on top of mBART-50 to facilitate cross-lingual transfer. Our approach outperforms related baselines, yielding higher translation scores on average when translati",
    "path": "papers/22/09/2209.15236.json",
    "total_tokens": 850,
    "translated_title": "低资源多语言神经机器翻译的语言家族适配器",
    "translated_abstract": "自监督预训练的大型多语言模型在广泛的自然语言处理任务中取得了最先进的结果。这些预训练模型往往会在一个或多个语言对的平行数据上进行微调，以用于机器翻译。多语言微调可以提高低资源语言的性能，但需要修改整个模型，可能成本过高。一种参数有效的选择是针对每个语言对训练一个新的适配器，或在不更新预训练模型的情况下针对所有语言对进行单一适配器的训练。然而，前者不允许语言之间共享参数，而后者则共享所有语言的参数，容易受到负面干扰。在本文中，我们提出在mBART-50的基础上训练语言家族适配器，以促进跨语言转移。我们的方法优于相关基线，在翻译过程中平均获得更高的翻译分数。",
    "tldr": "本文提出在mBART-50的基础上训练语言家族适配器，以提高低资源语言的翻译性能，该方法优于其他基线。",
    "en_tdlr": "This paper proposes training language-family adapters on top of mBART-50 to improve the translation performance of low-resource languages, which outperforms other baselines."
}