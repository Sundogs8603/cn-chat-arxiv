{
    "title": "Two-Tailed Averaging: Anytime, Adaptive, Once-in-a-While Optimal Weight Averaging for Better Generalization. (arXiv:2209.12581v3 [stat.ML] UPDATED)",
    "abstract": "Tail Averaging improves on Polyak averaging's non-asymptotic behaviour by excluding a number of leading iterates of stochastic optimization from its calculations. In practice, with a finite number of optimization steps and a learning rate that cannot be annealed to zero, Tail Averaging can get much closer to a local minimum point of the training loss than either the individual iterates or the Polyak average. However, the number of leading iterates to ignore is an important hyperparameter, and starting averaging too early or too late leads to inefficient use of resources or suboptimal solutions. Our work focusses on improving generalization, which makes setting this hyperparameter even more difficult, especially in the presence of other hyperparameters and overfitting. Furthermore, before averaging starts, the loss is only weakly informative of the final performance, which makes early stopping unreliable. To alleviate these problems, we propose an anytime variant of Tail Averaging inten",
    "link": "http://arxiv.org/abs/2209.12581",
    "context": "Title: Two-Tailed Averaging: Anytime, Adaptive, Once-in-a-While Optimal Weight Averaging for Better Generalization. (arXiv:2209.12581v3 [stat.ML] UPDATED)\nAbstract: Tail Averaging improves on Polyak averaging's non-asymptotic behaviour by excluding a number of leading iterates of stochastic optimization from its calculations. In practice, with a finite number of optimization steps and a learning rate that cannot be annealed to zero, Tail Averaging can get much closer to a local minimum point of the training loss than either the individual iterates or the Polyak average. However, the number of leading iterates to ignore is an important hyperparameter, and starting averaging too early or too late leads to inefficient use of resources or suboptimal solutions. Our work focusses on improving generalization, which makes setting this hyperparameter even more difficult, especially in the presence of other hyperparameters and overfitting. Furthermore, before averaging starts, the loss is only weakly informative of the final performance, which makes early stopping unreliable. To alleviate these problems, we propose an anytime variant of Tail Averaging inten",
    "path": "papers/22/09/2209.12581.json",
    "total_tokens": 1177,
    "tldr": "本文提出了一种泛化性能更好的加权平均方法——双尾平均，通过排除一些随机优化的前几个迭代来改善 Polyak 平均法的非渐近行为，解决了设置超参数难的问题，并提出了一种可适时调整的 Tail Averaging。",
    "en_tdlr": "This paper proposes a weighted average method called Two-Tailed Averaging, which improves the non-asymptotic behavior of Polyak averaging by excluding a number of leading iterates of stochastic optimization from its calculations. It solves the problem of setting hyperparameters difficulty and proposes an anytime variant of Tail Averaging."
}