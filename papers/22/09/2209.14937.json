{
    "title": "NAG-GS: Semi-Implicit, Accelerated and Robust Stochastic Optimizer. (arXiv:2209.14937v2 [math.OC] UPDATED)",
    "abstract": "Classical machine learning models such as deep neural networks are usually trained by using Stochastic Gradient Descent-based (SGD) algorithms. The classical SGD can be interpreted as a discretization of the stochastic gradient flow. In this paper we propose a novel, robust and accelerated stochastic optimizer that relies on two key elements: (1) an accelerated Nesterov-like Stochastic Differential Equation (SDE) and (2) its semi-implicit Gauss-Seidel type discretization. The convergence and stability of the obtained method, referred to as NAG-GS, are first studied extensively in the case of the minimization of a quadratic function. This analysis allows us to come up with an optimal learning rate in terms of the convergence rate while ensuring the stability of NAG-GS. This is achieved by the careful analysis of the spectral radius of the iteration matrix and the covariance matrix at stationarity with respect to all hyperparameters of our method. Further, we show that NAG- GS is competi",
    "link": "http://arxiv.org/abs/2209.14937",
    "context": "Title: NAG-GS: Semi-Implicit, Accelerated and Robust Stochastic Optimizer. (arXiv:2209.14937v2 [math.OC] UPDATED)\nAbstract: Classical machine learning models such as deep neural networks are usually trained by using Stochastic Gradient Descent-based (SGD) algorithms. The classical SGD can be interpreted as a discretization of the stochastic gradient flow. In this paper we propose a novel, robust and accelerated stochastic optimizer that relies on two key elements: (1) an accelerated Nesterov-like Stochastic Differential Equation (SDE) and (2) its semi-implicit Gauss-Seidel type discretization. The convergence and stability of the obtained method, referred to as NAG-GS, are first studied extensively in the case of the minimization of a quadratic function. This analysis allows us to come up with an optimal learning rate in terms of the convergence rate while ensuring the stability of NAG-GS. This is achieved by the careful analysis of the spectral radius of the iteration matrix and the covariance matrix at stationarity with respect to all hyperparameters of our method. Further, we show that NAG- GS is competi",
    "path": "papers/22/09/2209.14937.json",
    "total_tokens": 896,
    "translated_title": "NAG-GS: 半隐式、加速和稳健的随机优化器",
    "translated_abstract": "传统的机器学习模型，如深度神经网络，通常通过使用基于随机梯度下降（SGD）算法进行训练。经典的SGD可以解释为随机梯度流的离散化。本文提出了一种新颖、稳健且加速的随机优化器，它依赖于两个关键因素：（1）加速的类Nesterov随机微分方程（SDE）和（2）其半隐式Gauss-Seidel类型离散化。首先，在二次函数最小化的情况下广泛研究了所得方法的收敛性和稳定性。通过仔细分析我们方法的所有超参数相对于迭代矩阵和稳态下的协方差矩阵的谱半径，我们可以得到一个优化的学习率，以保证NAG-GS的收敛速度和稳定性。此外，我们还展示了NAG-GS在竞争性任务上的性能。",
    "tldr": "NAG-GS是一种半隐式、加速和稳健的随机优化器，通过使用加速的类Nesterov随机微分方程（SDE）和半隐式Gauss-Seidel类型离散化，该方法在二次函数最小化的情况下具有收敛性和稳定性，并在竞争性任务上表现出良好的性能。"
}