{
    "title": "Learning ASR pathways: A sparse multilingual ASR model. (arXiv:2209.05735v3 [eess.AS] UPDATED)",
    "abstract": "Neural network pruning compresses automatic speech recognition (ASR) models effectively. However, in multilingual ASR, language-agnostic pruning may lead to severe performance drops on some languages because language-agnostic pruning masks may not fit all languages and discard important language-specific parameters. In this work, we present ASR pathways, a sparse multilingual ASR model that activates language-specific sub-networks (\"pathways\"), such that the parameters for each language are learned explicitly. With the overlapping sub-networks, the shared parameters can also enable knowledge transfer for lower-resource languages via joint multilingual training. We propose a novel algorithm to learn ASR pathways, and evaluate the proposed method on 4 languages with a streaming RNN-T model. Our proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models.",
    "link": "http://arxiv.org/abs/2209.05735",
    "total_tokens": 960,
    "translated_title": "学习ASR路径：一种稀疏的多语言ASR模型",
    "translated_abstract": "神经网络剪枝有效地压缩了自动语音识别（ASR）模型。然而，在多语言ASR中，语言不可知的剪枝可能会导致某些语言的性能严重下降，因为语言不可知的剪枝掩码可能不适合所有语言并且丢弃重要的语言特定参数。在这项工作中，我们提出了ASR路径，一种稀疏的多语言ASR模型，它激活语言特定的子网络（“路径”），以便为每种语言显式地学习参数。通过重叠的子网络，共享参数还可以通过联合多语言训练实现对低资源语言的知识转移。我们提出了一种新算法来学习ASR路径，并使用流式RNN-T模型在4种语言上评估了所提出的方法。我们提出的ASR路径模型优于密集模型和语言不可知的剪枝模型，并且与单语稀疏模型相比，在低资源语言上提供更好的性能。",
    "tldr": "本文提出了一种稀疏的多语言ASR模型，通过激活语言特定的子网络来显式地学习每种语言的参数，同时通过联合多语言训练实现对低资源语言的知识转移，相比于密集模型和语言不可知的剪枝模型，在低资源语言上提供更好的性能。",
    "en_tldr": "This paper proposes a sparse multilingual ASR model, which explicitly learns the parameters for each language by activating language-specific sub-networks, and enables knowledge transfer for lower-resource languages via joint multilingual training. The proposed ASR pathways outperform both dense models and a language-agnostically pruned model, and provide better performance on low-resource languages compared to the monolingual sparse models."
}