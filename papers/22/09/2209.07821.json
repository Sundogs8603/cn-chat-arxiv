{
    "title": "Quantization for decentralized learning under subspace constraints. (arXiv:2209.07821v2 [math.OC] UPDATED)",
    "abstract": "In this paper, we consider decentralized optimization problems where agents have individual cost functions to minimize subject to subspace constraints that require the minimizers across the network to lie in low-dimensional subspaces. This constrained formulation includes consensus or single-task optimization as special cases, and allows for more general task relatedness models such as multitask smoothness and coupled optimization. In order to cope with communication constraints, we propose and study an adaptive decentralized strategy where the agents employ differential randomized quantizers to compress their estimates before communicating with their neighbors. The analysis shows that, under some general conditions on the quantization noise, and for sufficiently small step-sizes $\\mu$, the strategy is stable both in terms of mean-square error and average bit rate: by reducing $\\mu$, it is possible to keep the estimation errors small (on the order of $\\mu$) without increasing indefinit",
    "link": "http://arxiv.org/abs/2209.07821",
    "context": "Title: Quantization for decentralized learning under subspace constraints. (arXiv:2209.07821v2 [math.OC] UPDATED)\nAbstract: In this paper, we consider decentralized optimization problems where agents have individual cost functions to minimize subject to subspace constraints that require the minimizers across the network to lie in low-dimensional subspaces. This constrained formulation includes consensus or single-task optimization as special cases, and allows for more general task relatedness models such as multitask smoothness and coupled optimization. In order to cope with communication constraints, we propose and study an adaptive decentralized strategy where the agents employ differential randomized quantizers to compress their estimates before communicating with their neighbors. The analysis shows that, under some general conditions on the quantization noise, and for sufficiently small step-sizes $\\mu$, the strategy is stable both in terms of mean-square error and average bit rate: by reducing $\\mu$, it is possible to keep the estimation errors small (on the order of $\\mu$) without increasing indefinit",
    "path": "papers/22/09/2209.07821.json",
    "total_tokens": 842,
    "translated_title": "基于子空间约束的分散学习中的量化方法",
    "translated_abstract": "本文考虑了分散优化问题，其中每个代理都有自己的代价函数需要最小化，同时还要满足子空间约束，要求网络中的最小化者位于低维子空间中。这种约束形式包括共识或单任务优化作为特殊情况，并允许更一般的任务相关性模型，如多任务平滑性和耦合优化。为了应对通信约束，我们提出并研究了一种自适应的分散策略，其中代理在与邻居通信之前使用微分随机量化器压缩其估计值。分析表明，在量化噪声的一些一般条件和足够小的步长$\\mu$下，该策略在均方误差和平均比特率方面都是稳定的：通过减小$\\mu$，可以保持估计误差很小（与$\\mu$数量级相当），而不增加无限的比特率。",
    "tldr": "本文提出一种基于子空间约束的分散学习中的量化方法，该方法通过使用微分随机量化器压缩估计值，在保持低误差的同时减少通信开销。",
    "en_tdlr": "This paper proposes a quantization method for decentralized learning under subspace constraints, where agents compress their estimates using differential randomized quantizers to reduce communication overhead while maintaining low estimation errors."
}