{
    "title": "Do Large Language Models know what humans know?. (arXiv:2209.01515v3 [cs.CL] UPDATED)",
    "abstract": "Humans can attribute beliefs to others. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language display sensitivity to the implied knowledge states of characters in written passages. In pre-registered analyses, we present a linguistic version of the False Belief Task to both human participants and a Large Language Model, GPT-3. Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans, nor does it explain the full extent of their behavior -- despite being exposed to more language than a human would in a lifetime. This suggests that while statistical learning from language exposure may in part explain how huma",
    "link": "http://arxiv.org/abs/2209.01515",
    "context": "Title: Do Large Language Models know what humans know?. (arXiv:2209.01515v3 [cs.CL] UPDATED)\nAbstract: Humans can attribute beliefs to others. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language display sensitivity to the implied knowledge states of characters in written passages. In pre-registered analyses, we present a linguistic version of the False Belief Task to both human participants and a Large Language Model, GPT-3. Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans, nor does it explain the full extent of their behavior -- despite being exposed to more language than a human would in a lifetime. This suggests that while statistical learning from language exposure may in part explain how huma",
    "path": "papers/22/09/2209.01515.json",
    "total_tokens": 956,
    "translated_title": "大型语言模型能否像人类一样知道别人的信仰？",
    "translated_abstract": "人类能够了解他人的信仰。然而，尚不清楚这种能力在多大程度上是源于天生的生物禀赋，还是来源于儿童发育过程中的经验积累，尤其是通过接受描述他人心理状态的语言而获得的经验。我们通过评估暴露于大量人类语言的模型是否显示对书面段落中角色暗示的知识状态敏感性来测试语言暴露假说的可行性。在预注册的分析中，我们向人类参与者和大型语言模型GPT-3提供了语言版本的误信任务。两者都敏感于他人的信仰，而语言模型显著超过了偶然行为，但它的表现不如人类，并且没有解释他们行为的全部范围--尽管语言模型接受了比一个人一生中接受的语言更多的语言。这表明，虽然从语言暴露中进行的统计学习可能在一定程度上解释了人类如何做到这一点，但它并不能完全解释人类的行为。",
    "tldr": "本文测试了语言暴露假说，评估语言模型是否能够像人类一样感知书面文本中角色的知识状态。结果显示，语言模型虽然超过了偶然行为，但表现不如人类，暗示着仅靠语言暴露难以完全解释人类这种认知能力。",
    "en_tdlr": "The paper tests the language exposure hypothesis by assessing whether language models display sensitivity to the implied knowledge states of characters in written passages. While the language model significantly exceeds chance behavior, it does not perform as well as humans, suggesting that language exposure alone cannot fully explain human cognitive abilities."
}