{
    "title": "Class-Imbalanced Complementary-Label Learning via Weighted Loss. (arXiv:2209.14189v2 [cs.LG] UPDATED)",
    "abstract": "Complementary-label learning (CLL) is widely used in weakly supervised classification, but it faces a significant challenge in real-world datasets when confronted with class-imbalanced training samples. In such scenarios, the number of samples in one class is considerably lower than in other classes, which consequently leads to a decline in the accuracy of predictions. Unfortunately, existing CLL approaches have not investigate this problem. To alleviate this challenge, we propose a novel problem setting that enables learning from class-imbalanced complementary labels for multi-class classification. To tackle this problem, we propose a novel CLL approach called Weighted Complementary-Label Learning (WCLL). The proposed method models a weighted empirical risk minimization loss by utilizing the class-imbalanced complementary labels, which is also applicable to multi-class imbalanced training samples. Furthermore, we derive an estimation error bound to provide theoretical assurance. To ev",
    "link": "http://arxiv.org/abs/2209.14189",
    "context": "Title: Class-Imbalanced Complementary-Label Learning via Weighted Loss. (arXiv:2209.14189v2 [cs.LG] UPDATED)\nAbstract: Complementary-label learning (CLL) is widely used in weakly supervised classification, but it faces a significant challenge in real-world datasets when confronted with class-imbalanced training samples. In such scenarios, the number of samples in one class is considerably lower than in other classes, which consequently leads to a decline in the accuracy of predictions. Unfortunately, existing CLL approaches have not investigate this problem. To alleviate this challenge, we propose a novel problem setting that enables learning from class-imbalanced complementary labels for multi-class classification. To tackle this problem, we propose a novel CLL approach called Weighted Complementary-Label Learning (WCLL). The proposed method models a weighted empirical risk minimization loss by utilizing the class-imbalanced complementary labels, which is also applicable to multi-class imbalanced training samples. Furthermore, we derive an estimation error bound to provide theoretical assurance. To ev",
    "path": "papers/22/09/2209.14189.json",
    "total_tokens": 1085,
    "translated_title": "基于加权损失的类别不平衡互补标签学习",
    "translated_abstract": "互补标签学习 (CLL) 在弱监督分类中得到广泛应用，但在现实世界数据集中面临类别不平衡训练样本的显著挑战。在这种情况下，一个类别中的样本数量比其他类别要少得多，这导致预测准确率下降。不幸的是，现有的CLL方法没有研究解决这个问题。为了缓解这一挑战，我们提出了一种新的问题设置，它能够对多类分类进行类别不平衡互补标签学习。为了解决这个问题，我们提出了一种名为加权互补标签学习 (WCLL) 的新型CLL方法。该方法通过利用类别不平衡的互补标签来建模加权的经验风险最小化损失，也适用于多类别不平衡的训练样本。此外，我们推导了一个估计误差界来提供理论保证。为了评估所提出的方法，我们在三个具有不同程度类别不平衡的公共数据集上进行实验。实验结果表明，我们提出的WCLL方法相比其他最先进的方法具有更好的性能。",
    "tldr": "该论文提出了一种名为加权互补标签学习的新型CLL方法，能够解决现实世界数据集中的类别不平衡训练样本问题，通过利用类别不平衡的互补标签建模加权的经验风险最小化损失，也适用于多类别不平衡的训练样本，并在实验中展示了其比其他最先进的方法更好的性能。",
    "en_tdlr": "This paper proposes a novel CLL approach called Weighted Complementary-Label Learning (WCLL) to address the significant challenge of class-imbalanced training samples in real-world datasets, by modeling a weighted empirical risk minimization loss using the class-imbalanced complementary labels, which is also applicable to multi-class imbalanced training samples and outperforms other state-of-the-art methods in experiments."
}