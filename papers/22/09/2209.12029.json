{
    "title": "Open-Ended Diverse Solution Discovery with Regulated Behavior Patterns for Cross-Domain Adaptation. (arXiv:2209.12029v2 [cs.LG] UPDATED)",
    "abstract": "While Reinforcement Learning can achieve impressive results for complex tasks, the learned policies are generally prone to fail in downstream tasks with even minor model mismatch or unexpected perturbations. Recent works have demonstrated that a policy population with diverse behavior characteristics can generalize to downstream environments with various discrepancies. However, such policies might result in catastrophic damage during the deployment in practical scenarios like real-world systems due to the unrestricted behaviors of trained policies. Furthermore, training diverse policies without regulation of the behavior can result in inadequate feasible policies for extrapolating to a wide range of test conditions with dynamics shifts. In this work, we aim to train diverse policies under the regularization of the behavior patterns. We motivate our paradigm by observing the inverse dynamics in the environment with partial state information and propose Diversity in Regulation (DiR) trai",
    "link": "http://arxiv.org/abs/2209.12029",
    "context": "Title: Open-Ended Diverse Solution Discovery with Regulated Behavior Patterns for Cross-Domain Adaptation. (arXiv:2209.12029v2 [cs.LG] UPDATED)\nAbstract: While Reinforcement Learning can achieve impressive results for complex tasks, the learned policies are generally prone to fail in downstream tasks with even minor model mismatch or unexpected perturbations. Recent works have demonstrated that a policy population with diverse behavior characteristics can generalize to downstream environments with various discrepancies. However, such policies might result in catastrophic damage during the deployment in practical scenarios like real-world systems due to the unrestricted behaviors of trained policies. Furthermore, training diverse policies without regulation of the behavior can result in inadequate feasible policies for extrapolating to a wide range of test conditions with dynamics shifts. In this work, we aim to train diverse policies under the regularization of the behavior patterns. We motivate our paradigm by observing the inverse dynamics in the environment with partial state information and propose Diversity in Regulation (DiR) trai",
    "path": "papers/22/09/2209.12029.json",
    "total_tokens": 872,
    "translated_abstract": "虽然强化学习可以在复杂任务中取得显著成效，但所学习的策略通常容易在下游任务中出现失败，即使只有轻微的模型不匹配或意外扰动。最近的研究表明，行为特征多样的策略人群可以推广到具有各种不一致性的下游环境。然而，这些策略在实际场景中的部署中可能会造成灾难性破坏，原因是训练策略的行为不受限制。此外，如果不规范行为而训练出多样的策略，可能会导致不足够的可行策略推广到具有不同动态的广泛测试条件中。在本文中，我们旨在在行为规制下训练多样化的策略。我们通过观察带有部分状态信息的环境中的逆动力学，并提出了行为多样性调节（DiR）训练来支持我们的模型。",
    "tldr": "本文提出了一种基于调节行为模式的方法来训练多样化策略，以支持在实际场景中的部署并推广到具有不同动态的广泛测试条件中。"
}