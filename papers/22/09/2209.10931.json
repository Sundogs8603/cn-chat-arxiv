{
    "title": "Robust Collaborative Learning with Linear Gradient Overhead. (arXiv:2209.10931v2 [cs.LG] UPDATED)",
    "abstract": "Collaborative learning algorithms, such as distributed SGD (or D-SGD), are prone to faulty machines that may deviate from their prescribed algorithm because of software or hardware bugs, poisoned data or malicious behaviors. While many solutions have been proposed to enhance the robustness of D-SGD to such machines, previous works either resort to strong assumptions (trusted server, homogeneous data, specific noise model) or impose a gradient computational cost that is several orders of magnitude higher than that of D-SGD. We present MoNNA, a new algorithm that (a) is provably robust under standard assumptions and (b) has a gradient computation overhead that is linear in the fraction of faulty machines, which is conjectured to be tight. Essentially, MoNNA uses Polyak's momentum of local gradients for local updates and nearest-neighbor averaging (NNA) for global mixing, respectively. While MoNNA is rather simple to implement, its analysis has been more challenging and relies on two key ",
    "link": "http://arxiv.org/abs/2209.10931",
    "context": "Title: Robust Collaborative Learning with Linear Gradient Overhead. (arXiv:2209.10931v2 [cs.LG] UPDATED)\nAbstract: Collaborative learning algorithms, such as distributed SGD (or D-SGD), are prone to faulty machines that may deviate from their prescribed algorithm because of software or hardware bugs, poisoned data or malicious behaviors. While many solutions have been proposed to enhance the robustness of D-SGD to such machines, previous works either resort to strong assumptions (trusted server, homogeneous data, specific noise model) or impose a gradient computational cost that is several orders of magnitude higher than that of D-SGD. We present MoNNA, a new algorithm that (a) is provably robust under standard assumptions and (b) has a gradient computation overhead that is linear in the fraction of faulty machines, which is conjectured to be tight. Essentially, MoNNA uses Polyak's momentum of local gradients for local updates and nearest-neighbor averaging (NNA) for global mixing, respectively. While MoNNA is rather simple to implement, its analysis has been more challenging and relies on two key ",
    "path": "papers/22/09/2209.10931.json",
    "total_tokens": 891,
    "translated_title": "具有线性梯度开销的强健协同学习",
    "translated_abstract": "协同学习算法，如分布式SGD（或D-SGD），很容易受到存在软件或硬件缺陷、有毒数据或恶意行为的故障节点的影响。针对这些问题，已经提出了许多解决方案，但以前的工作要么依赖于强假设（信任服务器，同质数据，特定噪声模型），要么施加的梯度计算成本比D-SGD高几个数量级。本文提出了MoNNA算法，具有以下特点：（a）在标准假设下可证明鲁棒性；（b）梯度计算开销与错误节点比例成线性关系，这被认为是最紧的。关键思想是使用Polyak的局部梯度动量进行本地更新，并使用最近邻平均（NNA）进行全局混合。虽然MoNNA算法相对简单易实现，但其分析更具挑战性，依赖于两个关键条件。",
    "tldr": "MoNNA算法使用Polyak的局部梯度动量和最近邻平均方法，可在标准假设下证明鲁棒性，并且梯度计算开销与错误节点的比例成线性关系。",
    "en_tdlr": "The MoNNA algorithm uses Polyak's momentum of local gradients and nearest-neighbor averaging method to ensure the robustness under standard assumptions, and the gradient computation overhead is linearly related to the fraction of faulty machines."
}