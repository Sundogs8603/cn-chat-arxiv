{
    "title": "On the Optimization Landscape of Dynamic Output Feedback: A Case Study for Linear Quadratic Regulator. (arXiv:2209.05042v2 [cs.LG] UPDATED)",
    "abstract": "The convergence of policy gradient algorithms hinges on the optimization landscape of the underlying optimal control problem. Theoretical insights into these algorithms can often be acquired from analyzing those of linear quadratic control. However, most of the existing literature only considers the optimization landscape for static full-state or output feedback policies (controllers). We investigate the more challenging case of dynamic output-feedback policies for linear quadratic regulation (abbreviated as dLQR), which is prevalent in practice but has a rather complicated optimization landscape. We first show how the dLQR cost varies with the coordinate transformation of the dynamic controller and then derive the optimal transformation for a given observable stabilizing controller. One of our core results is the uniqueness of the stationary point of dLQR when it is observable, which provides an optimality certificate for solving dynamic controllers using policy gradient methods. More",
    "link": "http://arxiv.org/abs/2209.05042",
    "context": "Title: On the Optimization Landscape of Dynamic Output Feedback: A Case Study for Linear Quadratic Regulator. (arXiv:2209.05042v2 [cs.LG] UPDATED)\nAbstract: The convergence of policy gradient algorithms hinges on the optimization landscape of the underlying optimal control problem. Theoretical insights into these algorithms can often be acquired from analyzing those of linear quadratic control. However, most of the existing literature only considers the optimization landscape for static full-state or output feedback policies (controllers). We investigate the more challenging case of dynamic output-feedback policies for linear quadratic regulation (abbreviated as dLQR), which is prevalent in practice but has a rather complicated optimization landscape. We first show how the dLQR cost varies with the coordinate transformation of the dynamic controller and then derive the optimal transformation for a given observable stabilizing controller. One of our core results is the uniqueness of the stationary point of dLQR when it is observable, which provides an optimality certificate for solving dynamic controllers using policy gradient methods. More",
    "path": "papers/22/09/2209.05042.json",
    "total_tokens": 906,
    "translated_title": "关于动态输出反馈的优化景观: 基于线性二次调节器的案例研究",
    "translated_abstract": "策略梯度算法的收敛取决于基础最优控制问题的优化景观。通过分析线性二次控制的优化景观，我们可以获得这些算法的理论洞见。然而，现有文献大多只考虑静态全状态或输出反馈策略（控制器）的优化景观。我们研究了线性二次调节器中动态输出反馈策略（简称 dLQR）的更具挑战性的情况，这在实践中普遍存在，但其优化景观相对复杂。我们首先展示了 dLQR 成本如何随动态控制器的坐标变换而变化，然后推导了给定可观控稳定控制器的最优变换。我们的一个核心结果是 dLQR 在可观测时静止点的唯一性，这为使用策略梯度方法解决动态控制器提供了最优性证明。",
    "tldr": "本文研究了线性二次调节器中动态输出反馈策略的优化景观，推导了最优变换并证明了当其可观测时静止点的唯一性，从而为使用策略梯度方法解决动态控制器提供了最优性证明。",
    "en_tdlr": "This paper investigates the optimization landscape of dynamic output-feedback policies for linear quadratic regulation (dLQR) and provides a uniqueness proof of the stationary point when it is observable, which offers an optimality certificate for solving dynamic controllers using policy gradient methods."
}