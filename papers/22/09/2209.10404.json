{
    "title": "GP-net: Flexible Viewpoint Grasp Proposal. (arXiv:2209.10404v2 [cs.RO] UPDATED)",
    "abstract": "We present the Grasp Proposal Network (GP-net), a Convolutional Neural Network model which can generate 6-DOF grasps from flexible viewpoints, e.g. as experienced by mobile manipulators. To train GP-net, we synthetically generate a dataset containing depth-images and ground-truth grasp information. In real-world experiments we use the EGAD! grasping benchmark to evaluate GP-net against two commonly used algorithms, the Volumetric Grasping Network (VGN) and the Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In contrast to the state-of-the-art methods in robotic grasping, GP-net can be used for grasping objects from flexible, unknown viewpoints without the need to define the workspace and achieves a grasp success of 51.8% compared to 51.1% for VGN and 33.6% for GPD. We provide a ROS package along with our code and pre-trained models at https://aucoroboticsmu.github.io/GP-net/.",
    "link": "http://arxiv.org/abs/2209.10404",
    "context": "Title: GP-net: Flexible Viewpoint Grasp Proposal. (arXiv:2209.10404v2 [cs.RO] UPDATED)\nAbstract: We present the Grasp Proposal Network (GP-net), a Convolutional Neural Network model which can generate 6-DOF grasps from flexible viewpoints, e.g. as experienced by mobile manipulators. To train GP-net, we synthetically generate a dataset containing depth-images and ground-truth grasp information. In real-world experiments we use the EGAD! grasping benchmark to evaluate GP-net against two commonly used algorithms, the Volumetric Grasping Network (VGN) and the Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In contrast to the state-of-the-art methods in robotic grasping, GP-net can be used for grasping objects from flexible, unknown viewpoints without the need to define the workspace and achieves a grasp success of 51.8% compared to 51.1% for VGN and 33.6% for GPD. We provide a ROS package along with our code and pre-trained models at https://aucoroboticsmu.github.io/GP-net/.",
    "path": "papers/22/09/2209.10404.json",
    "total_tokens": 993,
    "translated_title": "GP-net: 灵活的视角抓取提案",
    "translated_abstract": "我们提出了一种名为 Grasp Proposal Network (GP-net) 的卷积神经网络模型，可以从灵活的视角，例如移动机械臂所体验的视角，生成六自由度的抓取。我们通过合成生成深度图像和标注抓取信息的数据集来训练 GP-net。在真实世界的实验中，我们使用 EGAD! 抓取基准测试对 GP-net 进行评估，并将其与两种常用算法——Volumetric Grasping Network (VGN) 和 Grasp Pose Detection package (GPD) 进行比较，在 PAL TIAGo 移动机器人上。与机器人抓取技术的最新方法相比，GP-net 可以用于从灵活的未知视角抓取对象，而无需定义工作空间，并且抓取成功率达到 51.8%，相比之下，VGN 为 51.1%，GPD 为 33.6%。我们提供了一个 ROS 包，以及我们的代码和预训练模型，网址为 https://aucoroboticsmu.github.io/GP-net/。",
    "tldr": "GP-net 可以从灵活的视角，例如移动机械臂所体验的视角，生成六自由度的抓取，在真实世界的实验中，它实现了 51.8% 的抓取成功率，相比之下，机器人抓取技术的最新方法成功率更低，需要定义工作空间。"
}