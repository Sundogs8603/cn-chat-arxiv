{
    "title": "Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization. (arXiv:2209.15382v2 [cs.LG] UPDATED)",
    "abstract": "We analyze the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy. We then consider the sample-based case, when the best representation of the Q- value function among linear combinations of a known feature function is known up to an estimation error. In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametri",
    "link": "http://arxiv.org/abs/2209.15382",
    "context": "Title: Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization. (arXiv:2209.15382v2 [cs.LG] UPDATED)\nAbstract: We analyze the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy. We then consider the sample-based case, when the best representation of the Q- value function among linear combinations of a known feature function is known up to an estimation error. In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametri",
    "path": "papers/22/09/2209.15382.json",
    "total_tokens": 924,
    "translated_title": "带有对数线性策略参数化的自然策略梯度的线性收敛",
    "translated_abstract": "本文分析了在无限时间折扣马尔科夫决策过程中，使用对数线性策略参数化的未正则化自然策略梯度算法的收敛速率。在确定性情况下，当 Q-值是已知的并且可以通过已知的特征函数的线性组合来逼近一个偏差误差时，我们显示出一种几何增长步长可以线性收敛到最优策略。然后，我们考虑样本为基础的情况，即在已知的特征函数的线性组合中，Q 值函数的最佳表示已知，但可能存在估计误差。在这种情况下，我们展示了该算法具有与确定性情况下相同的线性保证，直到一个依赖于估计误差、偏差误差和特征协方差矩阵条件数的误差项为止。我们的结果基于策略镜像下降的一般框架，并扩展了先前针对 softmax 表参数化的发现。",
    "tldr": "本文分析了无限时间折扣马尔科夫决策过程中，使用对数线性策略参数化的未正则化自然策略梯度算法的收敛速率，证明了一定条件下该算法具有线性收敛保证。",
    "en_tdlr": "This paper analyzes the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes, and proves that this algorithm has linear convergence guarantee under certain conditions."
}