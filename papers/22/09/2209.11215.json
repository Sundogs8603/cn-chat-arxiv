{
    "title": "Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. (arXiv:2209.11215v3 [cs.LG] UPDATED)",
    "abstract": "We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to",
    "link": "http://arxiv.org/abs/2209.11215",
    "context": "Title: Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. (arXiv:2209.11215v3 [cs.LG] UPDATED)\nAbstract: We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to",
    "path": "papers/22/09/2209.11215.json",
    "total_tokens": 1012,
    "translated_title": "采样与学习分数同样简单：对假设数据最小的扩散模型的理论研究",
    "translated_abstract": "我们为得分式生成模型提供了理论上的收敛保证，例如去噪扩散概率模型（DDPM），它是大规模实际生成模型（如DALL·E 2）的支柱。我们的主要结果是，在假设得分估计准确时，这样的SGM可以有效地从任何实际数据分布中采样。与之前的研究不同的是，我们的结果：（1）适用于$L^2$准确的分数估计（而不是$L^\\infty$准确的）；(2) 不需要排除实质性非对数凹性的限制性函数不等式条件；（3）在所有相关的问题参数中具有多项式的规模；（4）匹配最先进的Langevin扩散的离散化的复杂度保证，前提是评分误差足够小。我们认为这是SGM实证成功的强有力理论证明。我们还检查了基于临界阻尼Langevin扩散（CLD）的SGM，与以往研究结论不同，",
    "tldr": "本文对得分式生成模型进行了理论研究，证明其可以有效地从任何实际数据分布中采样。与以往研究不同的是，本文的结果不需要排除实质性非对数凹性的限制性函数不等式条件，并且具有多项式的规模。这是得分式生成模型实证成功的强有力理论证明。",
    "en_tdlr": "This paper provides theoretical guarantees for score-based generative models, proving that they can efficiently sample from any realistic data distribution assuming accurate score estimates. The results do not require restrictive functional inequality conditions and scale polynomially in relevant problem parameters, providing strong theoretical justification for the empirical success of score-based generative models."
}