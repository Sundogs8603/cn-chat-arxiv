{
    "title": "Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels. (arXiv:2209.12016v2 [cs.AI] UPDATED)",
    "abstract": "Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, as shown in the Unsupervised RL Benchmark (URLB; Laskin et al. 2021), whether current unsupervised strategies can improve generalization capabilities is still unclear, especially in visual control settings. In this work, we study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains 93.59% overall normalized performance, surpassing previous baselines by a staggering margin. The approach is empirically evaluated through a large-s",
    "link": "http://arxiv.org/abs/2209.12016",
    "context": "Title: Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels. (arXiv:2209.12016v2 [cs.AI] UPDATED)\nAbstract: Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, as shown in the Unsupervised RL Benchmark (URLB; Laskin et al. 2021), whether current unsupervised strategies can improve generalization capabilities is still unclear, especially in visual control settings. In this work, we study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains 93.59% overall normalized performance, surpassing previous baselines by a staggering margin. The approach is empirically evaluated through a large-s",
    "path": "papers/22/09/2209.12016.json",
    "total_tokens": 1168,
    "translated_title": "从像素开始掌握无监督强化学习基准测试",
    "translated_abstract": "从视觉感知数据中控制人工智能智能体是一项艰巨的任务。强化学习算法可以成功，但需要智能体与环境之间大量的交互。为了缓解这个问题，无监督强化学习提出了使用自我监督交互和学习的策略，以更快地适应未来的任务。然而，正如无监督强化学习基准测试所显示的那样，当前的无监督策略是否能够提高泛化能力在视觉控制环境中尤其不清楚。在这项工作中，我们研究了无监督强化学习基准测试，并提出了一种新的方法来解决它，使用无监督模型驱动的强化学习来预训练智能体，并结合新提出的混合规划器Dyna-MPC来进行任务感知的微调策略，以适应下游任务。在无监督强化学习基准测试中，我们的方法获得了93.59%的整体标准化性能，超过了以前的基线。我们通过大规模消融研究对该方法进行了经验评估，并表明预训练、微调和Dyna-MPC是成功的关键因素。我们的发现为了解无监督强化学习在视觉控制中的潜力和局限性提供了经验的基础。",
    "tldr": "本研究提出了一种新的无监督模型驱动的强化学习方法，结合混合规划器进行任务感知的微调策略，在无监督强化学习基准测试中获得了93.59%的整体标准化性能，超过之前的基线，是解决无监督强化学习中泛化能力提升的一项重要进展。",
    "en_tdlr": "This study proposes a new unsupervised model-based reinforcement learning method, combined with a hybrid planner for task-aware fine-tuning, which achieves an overall normalized performance of 93.59% in the Unsupervised RL Benchmark, exceeding previous baselines, and represents an important breakthrough for improving generalization capabilities in unsupervised reinforcement learning."
}