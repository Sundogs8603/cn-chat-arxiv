{
    "title": "An efficient encoder-decoder architecture with top-down attention for speech separation. (arXiv:2209.15200v5 [cs.SD] UPDATED)",
    "abstract": "Deep neural networks have shown excellent prospects in speech separation tasks. However, obtaining good results while keeping a low model complexity remains challenging in real-world applications. In this paper, we provide a bio-inspired efficient encoder-decoder architecture by mimicking the brain's top-down attention, called TDANet, with decreased model complexity without sacrificing performance. The top-down attention in TDANet is extracted by the global attention (GA) module and the cascaded local attention (LA) layers. The GA module takes multi-scale acoustic features as input to extract global attention signal, which then modulates features of different scales by direct top-down connections. The LA layers use features of adjacent layers as input to extract the local attention signal, which is used to modulate the lateral input in a top-down manner. On three benchmark datasets, TDANet consistently achieved competitive separation performance to previous state-of-the-art (SOTA) meth",
    "link": "http://arxiv.org/abs/2209.15200",
    "context": "Title: An efficient encoder-decoder architecture with top-down attention for speech separation. (arXiv:2209.15200v5 [cs.SD] UPDATED)\nAbstract: Deep neural networks have shown excellent prospects in speech separation tasks. However, obtaining good results while keeping a low model complexity remains challenging in real-world applications. In this paper, we provide a bio-inspired efficient encoder-decoder architecture by mimicking the brain's top-down attention, called TDANet, with decreased model complexity without sacrificing performance. The top-down attention in TDANet is extracted by the global attention (GA) module and the cascaded local attention (LA) layers. The GA module takes multi-scale acoustic features as input to extract global attention signal, which then modulates features of different scales by direct top-down connections. The LA layers use features of adjacent layers as input to extract the local attention signal, which is used to modulate the lateral input in a top-down manner. On three benchmark datasets, TDANet consistently achieved competitive separation performance to previous state-of-the-art (SOTA) meth",
    "path": "papers/22/09/2209.15200.json",
    "total_tokens": 929,
    "translated_title": "一种具有自上而下注意力的高效编码解码结构用于语音分离。",
    "translated_abstract": "深度神经网络在语音分离任务中已经显示出优秀的前景。然而，在保持低模型复杂度的同时获得良好结果在实际应用中仍然具有挑战性。在本文中，我们通过模拟大脑的自上而下注意力提供了一种生物启发的高效编码解码架构，称为TDANet，具有降低的模型复杂度，而不牺牲性能。TDANet中的自上而下注意力通过全局注意力(GA)模块和级联局部注意力(LA)层提取。GA模块将多尺度音频特征作为输入，提取全局注意力信号，然后通过直接自上而下连接来调制不同尺度的特征。LA层使用相邻层的特征作为输入，以提取局部注意力信号，该信号用于以自上而下的方式调制横向输入。在三个基准数据集上，TDANet始终获得了与之前最先进方法相当的分离性能，同时显着减少了参数数量和计算成本。",
    "tldr": "本文提出了一种新的高效编码解码网络结构，名为TDANet，它通过模拟大脑的自上而下注意力来降低模型复杂度，并在语音分离任务中取得了具有竞争力的结果。"
}