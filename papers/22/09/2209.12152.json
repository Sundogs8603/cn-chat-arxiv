{
    "title": "All are Worth Words: A ViT Backbone for Diffusion Models. (arXiv:2209.12152v4 [cs.CV] UPDATED)",
    "abstract": "Vision transformers (ViT) have shown promise in various vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion models. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion models. U-ViT is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and class-conditional image generation, as well as text-to-image generation tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256x256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of generative models. Our results suggest that, for diffusion-b",
    "link": "http://arxiv.org/abs/2209.12152",
    "context": "Title: All are Worth Words: A ViT Backbone for Diffusion Models. (arXiv:2209.12152v4 [cs.CV] UPDATED)\nAbstract: Vision transformers (ViT) have shown promise in various vision tasks while the U-Net based on a convolutional neural network (CNN) remains dominant in diffusion models. We design a simple and general ViT-based architecture (named U-ViT) for image generation with diffusion models. U-ViT is characterized by treating all inputs including the time, condition and noisy image patches as tokens and employing long skip connections between shallow and deep layers. We evaluate U-ViT in unconditional and class-conditional image generation, as well as text-to-image generation tasks, where U-ViT is comparable if not superior to a CNN-based U-Net of a similar size. In particular, latent diffusion models with U-ViT achieve record-breaking FID scores of 2.29 in class-conditional image generation on ImageNet 256x256, and 5.48 in text-to-image generation on MS-COCO, among methods without accessing large external datasets during the training of generative models. Our results suggest that, for diffusion-b",
    "path": "papers/22/09/2209.12152.json",
    "total_tokens": 1116,
    "translated_title": "全部值得一试：适用于扩散模型的ViT骨干网络",
    "translated_abstract": "在各种视觉任务中，视觉变换器（ViT）已经显示出了良好的性能，而基于卷积神经网络（CNN）的U-Net在扩散模型中仍然占主导地位。我们设计了一个简单通用的ViT骨干架构（称为U-ViT），用于生成扩散模型的图像。U-ViT的特点是将所有输入，包括时间、条件和噪声图像块都视为令牌，并在浅层和深层之间使用长跳跃连接。我们在无条件和类条件图像生成以及文本到图像生成任务中评估了U-ViT，在相似大小的基于“U-Net”的CNN模型中，U-ViT具有可比甚至更好的性能。特别是U-ViT的潜在扩散模型在ImageNet 256x256类条件图像生成中取得了2.29的最佳FID分数，在MS-COCO的文本到图像生成中取得了5.48的最佳FID分数，相比其他生成模型，不需要在训练期间访问大型外部数据集。我们的结果表明，在扩散型图像生成模型中，像U-ViT这样的ViT骨干架构可以实现与传统基于CNN的U-Net模型相当或更好的性能，甚至在某些任务中创造新的FID分数记录。",
    "tldr": "本文提出了一种适用于扩散模型的ViT骨干网络U-ViT，用于图像生成任务，相较于传统基于CNN的U-Net模型，U-ViT具有可比甚至更好的性能，甚至在某些任务中创造了新的FID分数记录。",
    "en_tdlr": "This paper proposes a ViT-based architecture named U-ViT for image generation tasks in diffusion models, which achieves comparable or even better performance than traditional CNN-based U-Net models, even setting new FID score records in certain tasks without needing large external datasets during training."
}