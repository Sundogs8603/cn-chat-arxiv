{
    "title": "Deep importance sampling using tensor trains with application to a priori and a posteriori rare event estimation. (arXiv:2209.01941v2 [stat.ML] UPDATED)",
    "abstract": "We propose a deep importance sampling method that is suitable for estimating rare event probabilities in high-dimensional problems. We approximate the optimal importance distribution in a general importance sampling problem as the pushforward of a reference distribution under a composition of order-preserving transformations, in which each transformation is formed by a squared tensor-train decomposition. The squared tensor-train decomposition provides a scalable ansatz for building order-preserving high-dimensional transformations via density approximations. The use of composition of maps moving along a sequence of bridging densities alleviates the difficulty of directly approximating concentrated density functions. To compute expectations over unnormalized probability distributions, we design a ratio estimator that estimates the normalizing constant using a separate importance distribution, again constructed via a composition of transformations in tensor-train format. This offers bett",
    "link": "http://arxiv.org/abs/2209.01941",
    "context": "Title: Deep importance sampling using tensor trains with application to a priori and a posteriori rare event estimation. (arXiv:2209.01941v2 [stat.ML] UPDATED)\nAbstract: We propose a deep importance sampling method that is suitable for estimating rare event probabilities in high-dimensional problems. We approximate the optimal importance distribution in a general importance sampling problem as the pushforward of a reference distribution under a composition of order-preserving transformations, in which each transformation is formed by a squared tensor-train decomposition. The squared tensor-train decomposition provides a scalable ansatz for building order-preserving high-dimensional transformations via density approximations. The use of composition of maps moving along a sequence of bridging densities alleviates the difficulty of directly approximating concentrated density functions. To compute expectations over unnormalized probability distributions, we design a ratio estimator that estimates the normalizing constant using a separate importance distribution, again constructed via a composition of transformations in tensor-train format. This offers bett",
    "path": "papers/22/09/2209.01941.json",
    "total_tokens": 1055,
    "translated_title": "使用张量列进行深度重要性采样及其在先验和后验极端事件估计中的应用",
    "translated_abstract": "我们提出了一种适用于高维问题中估计稀有事件概率的深度重要性采样方法。我们将一般重要性采样问题中的最优重要性分布近似为一个由平方张量列分解形成的顺序保持变换组合下的参考分布推送。张量列提供了一个可扩展的答案，用于通过密度近似构建保序高维变换。沿着一系列过渡密度的映射组成的地图合成减轻了直接近似浓缩密度函数的困难。为了计算未归一化概率分布上的期望值，我们设计了一个比率估计器，使用单独构建的张量列格式的变换组合构建另一个重要性分布来估计归一化常数。与传统的重要性采样相比，这提供了更好的方差减小。我们展示了我们的方法在几个高维稀有事件估计问题上的有效性，包括使用神经网络模型进行的先验和后验估计。",
    "tldr": "本论文提出了一种使用张量列进行深度重要性采样的方法。采用平方张量列分解和顺序保持变换组合进行参考分布的推送，通过张量列的可扩展答案构建保序高维变换，设计了比率估计器来计算未归一化概率分布上的期望值。该方法在高维稀有事件估计问题中表现出了更好的方差减小效果。",
    "en_tdlr": "This paper proposes a method for deep importance sampling using tensor trains, which approximates the optimal importance distribution in high-dimensional problems by a composition of order-preserving transformations formed by a squared tensor-train decomposition. A ratio estimator is designed to compute expectations over unnormalized probability distributions, which offers better variance reduction compared to traditional importance sampling. The method is effective in rare event estimation problems, including a priori and a posteriori estimations in Bayesian inference with neural network models."
}