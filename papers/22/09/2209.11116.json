{
    "title": "Training neural network ensembles via trajectory sampling. (arXiv:2209.11116v2 [cond-mat.stat-mech] UPDATED)",
    "abstract": "In machine learning, there is renewed interest in neural network ensembles (NNEs), whereby predictions are obtained as an aggregate from a diverse set of smaller models, rather than from a single larger model. Here, we show how to define and train a NNE using techniques from the study of rare trajectories in stochastic systems. We define an NNE in terms of the trajectory of the model parameters under a simple, and discrete in time, diffusive dynamics, and train the NNE by biasing these trajectories towards a small time-integrated loss, as controlled by appropriate counting fields which act as hyperparameters. We demonstrate the viability of this technique on a range of simple supervised learning tasks. We discuss potential advantages of our trajectory sampling approach compared with more conventional gradient based methods.",
    "link": "http://arxiv.org/abs/2209.11116",
    "context": "Title: Training neural network ensembles via trajectory sampling. (arXiv:2209.11116v2 [cond-mat.stat-mech] UPDATED)\nAbstract: In machine learning, there is renewed interest in neural network ensembles (NNEs), whereby predictions are obtained as an aggregate from a diverse set of smaller models, rather than from a single larger model. Here, we show how to define and train a NNE using techniques from the study of rare trajectories in stochastic systems. We define an NNE in terms of the trajectory of the model parameters under a simple, and discrete in time, diffusive dynamics, and train the NNE by biasing these trajectories towards a small time-integrated loss, as controlled by appropriate counting fields which act as hyperparameters. We demonstrate the viability of this technique on a range of simple supervised learning tasks. We discuss potential advantages of our trajectory sampling approach compared with more conventional gradient based methods.",
    "path": "papers/22/09/2209.11116.json",
    "total_tokens": 801,
    "translated_title": "通过轨迹抽样训练神经网络集成",
    "translated_abstract": "在机器学习中，神经网络集成(NNEs)近年来再次引起关注。它通过从多个较小的模型中获得预测结果，而不是从单个更大的模型中获得结果。本文展示了如何使用随机系统中稀有轨迹的技术来定义和训练NNE。我们根据模型参数在简单且时间离散的扩散动力学下的轨迹来定义NNE，并通过偏置这些轨迹来训练NNE以达到小的时间积分误差，这由适当的计数场作为超参数来控制。我们在一系列简单的监督学习任务中展示了这种技术的可行性。我们讨论了我们的轨迹抽样方法与更传统的梯度方法相比的潜在优势。",
    "tldr": "本研究使用随机系统中稀有轨迹的技术定义和训练NNE，通过对轨迹进行偏置来训练NNE，相较于更传统的梯度方法具有潜在优势。",
    "en_tdlr": "This article demonstrates defining and training NNE using techniques from stochastic systems to bias the trajectory of model parameters towards a small time-integrated loss, which has potential advantages compared to traditional gradient-based methods."
}