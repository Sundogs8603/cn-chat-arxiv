{
    "title": "Fluid Batching: Exit-Aware Preemptive Serving of Early-Exit Neural Networks on Edge NPUs. (arXiv:2209.13443v2 [cs.LG] UPDATED)",
    "abstract": "With deep neural networks (DNNs) emerging as the backbone in a multitude of computer vision tasks, their adoption in real-world applications broadens continuously. Given the abundance and omnipresence of smart devices in the consumer landscape, \"smart ecosystems'' are being formed where sensing happens concurrently rather than standalone. This is shifting the on-device inference paradigm towards deploying centralised neural processing units (NPUs) at the edge, where multiple devices (e.g. in smart homes or autonomous vehicles) can stream their data for processing with dynamic rates. While this provides enhanced potential for input batching, naive solutions can lead to subpar performance and quality of experience, especially under spiking loads. At the same time, the deployment of dynamic DNNs, comprising stochastic computation graphs (e.g. early-exit (EE) models), introduces a new dimension of dynamic behaviour in such systems. In this work, we propose a novel early-exit-aware scheduli",
    "link": "http://arxiv.org/abs/2209.13443",
    "context": "Title: Fluid Batching: Exit-Aware Preemptive Serving of Early-Exit Neural Networks on Edge NPUs. (arXiv:2209.13443v2 [cs.LG] UPDATED)\nAbstract: With deep neural networks (DNNs) emerging as the backbone in a multitude of computer vision tasks, their adoption in real-world applications broadens continuously. Given the abundance and omnipresence of smart devices in the consumer landscape, \"smart ecosystems'' are being formed where sensing happens concurrently rather than standalone. This is shifting the on-device inference paradigm towards deploying centralised neural processing units (NPUs) at the edge, where multiple devices (e.g. in smart homes or autonomous vehicles) can stream their data for processing with dynamic rates. While this provides enhanced potential for input batching, naive solutions can lead to subpar performance and quality of experience, especially under spiking loads. At the same time, the deployment of dynamic DNNs, comprising stochastic computation graphs (e.g. early-exit (EE) models), introduces a new dimension of dynamic behaviour in such systems. In this work, we propose a novel early-exit-aware scheduli",
    "path": "papers/22/09/2209.13443.json",
    "total_tokens": 868,
    "translated_title": "流体批处理：边缘NPUs上提前退出神经网络的退出感知抢占式服务",
    "translated_abstract": "随着深度神经网络在多种计算机视觉任务中成为主干，它们在现实世界应用中的采用不断扩展。鉴于智能设备在消费领域的丰富和无处不在，正在形成“智能生态系统”，其中感知是并发进行的而不是独立进行的。这将在边缘部署中心化的神经处理单元（NPUs）的设备推理范式转向，智能家居或自动驾驶等多个设备可以流式传输数据以便使用动态速率进行处理。虽然这提供了输入批处理的增强潜力，但简单的解决方案可能导致次优的性能和体验质量，特别是在高负载下。同时，部署动态DNN，包括随机计算图（例如提前退出（EE）模型），在这类系统中引入了一维动态行为。在这项工作中，我们提出了一种新颖的提前退出感知调度算法。",
    "tldr": "本论文提出了一种针对边缘NPUs上的提前退出神经网络进行退出感知抢占式服务的流体批处理方法。",
    "en_tdlr": "This paper proposes a fluid batching method for exit-aware preemptive serving of early-exit neural networks on edge NPUs."
}