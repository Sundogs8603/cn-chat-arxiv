{
    "title": "Regulating eXplainable Artificial Intelligence (XAI) May Harm Consumers",
    "abstract": "arXiv:2209.03499v3 Announce Type: replace  Abstract: Recent AI algorithms are black box models whose decisions are difficult to interpret. eXplainable AI (XAI) is a class of methods that seek to address lack of AI interpretability and trust by explaining to customers their AI decisions. The common wisdom is that regulating AI by mandating fully transparent XAI leads to greater social welfare. Our paper challenges this notion through a game theoretic model of a policy-maker who maximizes social welfare, firms in a duopoly competition that maximize profits, and heterogenous consumers. The results show that XAI regulation may be redundant. In fact, mandating fully transparent XAI may make firms and consumers worse off. This reveals a tradeoff between maximizing welfare and receiving explainable AI outputs. We extend the existing literature on method and substantive fronts, and we introduce and study the notion of XAI fairness, which may be impossible to guarantee even under mandatory XAI.",
    "link": "https://arxiv.org/abs/2209.03499",
    "context": "Title: Regulating eXplainable Artificial Intelligence (XAI) May Harm Consumers\nAbstract: arXiv:2209.03499v3 Announce Type: replace  Abstract: Recent AI algorithms are black box models whose decisions are difficult to interpret. eXplainable AI (XAI) is a class of methods that seek to address lack of AI interpretability and trust by explaining to customers their AI decisions. The common wisdom is that regulating AI by mandating fully transparent XAI leads to greater social welfare. Our paper challenges this notion through a game theoretic model of a policy-maker who maximizes social welfare, firms in a duopoly competition that maximize profits, and heterogenous consumers. The results show that XAI regulation may be redundant. In fact, mandating fully transparent XAI may make firms and consumers worse off. This reveals a tradeoff between maximizing welfare and receiving explainable AI outputs. We extend the existing literature on method and substantive fronts, and we introduce and study the notion of XAI fairness, which may be impossible to guarantee even under mandatory XAI.",
    "path": "papers/22/09/2209.03499.json",
    "total_tokens": 831,
    "translated_title": "规范可解释人工智能（XAI）可能会损害消费者",
    "translated_abstract": "最近的人工智能算法是黑匣子模型，其决策难以解释。可解释人工智能（XAI）是一类方法，旨在通过向客户解释他们的人工智能决策来解决缺乏人工智能可解释性和信任。本文通过一个博弈论模型挑战了监管人工智能通过规定完全透明XAI会导致更大社会福利的普遍观念。研究结果显示，XAI规制可能是多余的。事实上，强制规定完全透明XAI可能会使公司和消费者变得更糟。这揭示了在最大化福利和获得可解释人工智能输出之间的权衡。我们在方法和实质范畴上拓展了现有文献，并引入并研究了XAI公平的概念，即使在强制XAI的情况下也可能无法保证。",
    "tldr": "XAI regulation may be redundant and mandating fully transparent XAI may make firms and consumers worse off, revealing a tradeoff between maximizing welfare and receiving explainable AI outputs.",
    "en_tdlr": "Regulating XAI may be unnecessary and enforcing fully transparent XAI could have negative consequences for both firms and consumers, highlighting a tradeoff between maximizing societal welfare and obtaining interpretable AI outputs."
}