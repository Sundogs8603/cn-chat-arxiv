{
    "title": "Neighborhood Gradient Clustering: An Efficient Decentralized Learning Method for Non-IID Data Distributions. (arXiv:2209.14390v6 [cs.LG] UPDATED)",
    "abstract": "Decentralized learning over distributed datasets can have significantly different data distributions across the agents. The current state-of-the-art decentralized algorithms mostly assume the data distributions to be Independent and Identically Distributed. This paper focuses on improving decentralized learning over non-IID data. We propose \\textit{Neighborhood Gradient Clustering (NGC)}, a novel decentralized learning algorithm that modifies the local gradients of each agent using self- and cross-gradient information. Cross-gradients for a pair of neighboring agents are the derivatives of the model parameters of an agent with respect to the dataset of the other agent. In particular, the proposed method replaces the local gradients of the model with the weighted mean of the self-gradients, model-variant cross-gradients (derivatives of the neighbors' parameters with respect to the local dataset), and data-variant cross-gradients (derivatives of the local model with respect to its neighb",
    "link": "http://arxiv.org/abs/2209.14390",
    "context": "Title: Neighborhood Gradient Clustering: An Efficient Decentralized Learning Method for Non-IID Data Distributions. (arXiv:2209.14390v6 [cs.LG] UPDATED)\nAbstract: Decentralized learning over distributed datasets can have significantly different data distributions across the agents. The current state-of-the-art decentralized algorithms mostly assume the data distributions to be Independent and Identically Distributed. This paper focuses on improving decentralized learning over non-IID data. We propose \\textit{Neighborhood Gradient Clustering (NGC)}, a novel decentralized learning algorithm that modifies the local gradients of each agent using self- and cross-gradient information. Cross-gradients for a pair of neighboring agents are the derivatives of the model parameters of an agent with respect to the dataset of the other agent. In particular, the proposed method replaces the local gradients of the model with the weighted mean of the self-gradients, model-variant cross-gradients (derivatives of the neighbors' parameters with respect to the local dataset), and data-variant cross-gradients (derivatives of the local model with respect to its neighb",
    "path": "papers/22/09/2209.14390.json",
    "total_tokens": 922,
    "translated_title": "邻域梯度聚类：一种用于非独立同分布数据分布的高效去中心化学习方法",
    "translated_abstract": "分布式数据集上的去中心化学习可能在代理之间具有显着不同的数据分布。当前最先进的去中心化算法大多假设数据分布是独立同分布的。本文旨在改进在非独立同分布数据上的去中心化学习。我们提出了一种新的去中心化学习算法Neighborhood Gradient Clustering (NGC)，该算法修改每个代理的局部梯度，使用自身和交叉梯度信息。交叉梯度是指相邻代理的模型参数的导数与另一个代理数据集关于参数的导数。特别地，该方法将模型的局部梯度替换为自梯度，模型变异交叉梯度（相邻代理关于本地数据集参数的导数）和数据变异交叉梯度（本地模型关于邻居数据集的导数）的加权平均值。",
    "tldr": "本文提出一种名为NGC的去中心化学习算法，用于对非独立同分布数据进行学习。该算法使用邻近代理的自梯度，模型变异交叉梯度和数据变异交叉梯度的加权平均值来修改每个代理的局部梯度。",
    "en_tdlr": "This paper proposes a decentralized learning algorithm named NGC for learning on non-IID data distributions. The algorithm modifies the local gradients of each agent using self- and cross-gradient information, which includes model-variant and data-variant cross-gradients. The modified local gradients are obtained as a weighted mean of these gradients."
}