{
    "title": "Adaptive Meta-learner via Gradient Similarity for Few-shot Text Classification. (arXiv:2209.04702v2 [cs.CL] UPDATED)",
    "abstract": "Few-shot text classification aims to classify the text under the few-shot scenario. Most of the previous methods adopt optimization-based meta learning to obtain task distribution. However, due to the neglect of matching between the few amount of samples and complicated models, as well as the distinction between useful and useless task features, these methods suffer from the overfitting issue. To address this issue, we propose a novel Adaptive Meta-learner via Gradient Similarity (AMGS) method to improve the model generalization ability to a new task. Specifically, the proposed AMGS alleviates the overfitting based on two aspects: (i) acquiring the potential semantic representation of samples and improving model generalization through the self-supervised auxiliary task in the inner loop, (ii) leveraging the adaptive meta-learner via gradient similarity to add constraints on the gradient obtained by base-learner in the outer loop. Moreover, we make a systematic analysis of the influence",
    "link": "http://arxiv.org/abs/2209.04702",
    "context": "Title: Adaptive Meta-learner via Gradient Similarity for Few-shot Text Classification. (arXiv:2209.04702v2 [cs.CL] UPDATED)\nAbstract: Few-shot text classification aims to classify the text under the few-shot scenario. Most of the previous methods adopt optimization-based meta learning to obtain task distribution. However, due to the neglect of matching between the few amount of samples and complicated models, as well as the distinction between useful and useless task features, these methods suffer from the overfitting issue. To address this issue, we propose a novel Adaptive Meta-learner via Gradient Similarity (AMGS) method to improve the model generalization ability to a new task. Specifically, the proposed AMGS alleviates the overfitting based on two aspects: (i) acquiring the potential semantic representation of samples and improving model generalization through the self-supervised auxiliary task in the inner loop, (ii) leveraging the adaptive meta-learner via gradient similarity to add constraints on the gradient obtained by base-learner in the outer loop. Moreover, we make a systematic analysis of the influence",
    "path": "papers/22/09/2209.04702.json",
    "total_tokens": 953,
    "translated_title": "基于梯度相似度的自适应元学习器，用于少样本文本分类",
    "translated_abstract": "少样本文本分类旨在在少样本情况下对文本进行分类。大多数先前的方法采用基于优化的元学习来获得任务分布。然而，由于忽视了少量样本和复杂模型之间的匹配，以及有用和无用任务特征之间的区别，这些方法遭受了过拟合问题。为了解决这个问题，我们提出了一种新颖的基于梯度相似度的自适应元学习器（AMGS）方法，以提高模型对新任务的泛化能力。具体而言，所提出的AMGS从两个方面缓解过拟合问题：（i）通过内循环中的自监督辅助任务获得样本的潜在语义表示并改善模型的泛化能力，（ii）通过基于梯度相似度的自适应元学习器在外循环中对基学习器获得的梯度施加约束。此外，我们对影响因素进行了系统分析。",
    "tldr": "提出了一种基于梯度相似度的自适应元学习器方法（AMGS），用于改善少样本文本分类模型的泛化能力。该方法通过自监督辅助任务获得潜在语义表示，并使用梯度相似度约束基学习器的梯度，从而解决了过拟合问题。",
    "en_tdlr": "An Adaptive Meta-learner via Gradient Similarity (AMGS) method is proposed to improve the generalization ability of few-shot text classification models. By acquiring potential semantic representation through self-supervised auxiliary tasks and leveraging gradient similarity to constrain the base-learner, this method addresses the issue of overfitting."
}