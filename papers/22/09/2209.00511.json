{
    "title": "DRL Enabled Coverage and Capacity Optimization in STAR-RIS Assisted Networks. (arXiv:2209.00511v2 [cs.IT] UPDATED)",
    "abstract": "Simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs) is a promising passive device that contributes to a full-space coverage via transmitting and reflecting the incident signal simultaneously. As a new paradigm in wireless communications, how to analyze the coverage and capacity performance of STAR-RISs becomes essential but challenging. To solve the coverage and capacity optimization (CCO) problem in STAR-RIS assisted networks, a multi-objective proximal policy optimization (MO-PPO) algorithm is proposed to handle long-term benefits than conventional optimization algorithms. To strike a balance between each objective, the MO-PPO algorithm provides a set of optimal solutions to form a Pareto front (PF), where any solution on the PF is regarded as an optimal result. Moreover, in order to improve the performance of the MO-PPO algorithm, two update strategies, i.e., action-value-based update strategy (AVUS) and loss function-based update strategy (LFU",
    "link": "http://arxiv.org/abs/2209.00511",
    "context": "Title: DRL Enabled Coverage and Capacity Optimization in STAR-RIS Assisted Networks. (arXiv:2209.00511v2 [cs.IT] UPDATED)\nAbstract: Simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs) is a promising passive device that contributes to a full-space coverage via transmitting and reflecting the incident signal simultaneously. As a new paradigm in wireless communications, how to analyze the coverage and capacity performance of STAR-RISs becomes essential but challenging. To solve the coverage and capacity optimization (CCO) problem in STAR-RIS assisted networks, a multi-objective proximal policy optimization (MO-PPO) algorithm is proposed to handle long-term benefits than conventional optimization algorithms. To strike a balance between each objective, the MO-PPO algorithm provides a set of optimal solutions to form a Pareto front (PF), where any solution on the PF is regarded as an optimal result. Moreover, in order to improve the performance of the MO-PPO algorithm, two update strategies, i.e., action-value-based update strategy (AVUS) and loss function-based update strategy (LFU",
    "path": "papers/22/09/2209.00511.json",
    "total_tokens": 913,
    "translated_title": "STAR-RIS辅助网络中的DRL优化覆盖和容量",
    "translated_abstract": "同时传输和反射可重构智能表面（STAR-RIS）是一种有 promising passivedevice，通过同时传输和反射入射信号来实现整个空间的覆盖。作为无线通信中的新范式，如何分析STAR-RIS的覆盖和容量性能变得很重要但也很具挑战性。为了解决STAR-RIS辅助网络中的覆盖和容量优化（CCO）问题，提出了一种多目标近端政策优化（MO-PPO）算法，该算法处理的是长期效益，而不是传统的优化算法。为了在每个目标之间取得平衡，MO-PPO算法提供了一组最优解来形成帕累托前沿（PF），其中PF上的任何解都被视为最优结果。此外，为了提高MO-PPO算法的性能，提出了两种更新策略，即基于动作价值的更新策略（AVUS）和基于损失函数的更新策略（LFU）。",
    "tldr": "这项研究提出了一种用于STAR-RIS辅助网络中覆盖和容量优化问题的多目标近端政策优化（MO-PPO）算法，该算法通过提供一组最优解来实现长期效益，从而改进了传统优化算法的性能。"
}