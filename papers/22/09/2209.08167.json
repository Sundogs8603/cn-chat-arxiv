{
    "title": "Quantum Vision Transformers",
    "abstract": "arXiv:2209.08167v2 Announce Type: replace-cross  Abstract: In this work, quantum transformers are designed and analysed in detail by extending the state-of-the-art classical transformer neural network architectures known to be very performant in natural language processing and image analysis. Building upon the previous work, which uses parametrised quantum circuits for data loading and orthogonal neural layers, we introduce three types of quantum transformers for training and inference, including a quantum transformer based on compound matrices, which guarantees a theoretical advantage of the quantum attention mechanism compared to their classical counterpart both in terms of asymptotic run time and the number of model parameters. These quantum architectures can be built using shallow quantum circuits and produce qualitatively different classification models. The three proposed quantum attention layers vary on the spectrum between closely following the classical transformers and exhibi",
    "link": "https://arxiv.org/abs/2209.08167",
    "context": "Title: Quantum Vision Transformers\nAbstract: arXiv:2209.08167v2 Announce Type: replace-cross  Abstract: In this work, quantum transformers are designed and analysed in detail by extending the state-of-the-art classical transformer neural network architectures known to be very performant in natural language processing and image analysis. Building upon the previous work, which uses parametrised quantum circuits for data loading and orthogonal neural layers, we introduce three types of quantum transformers for training and inference, including a quantum transformer based on compound matrices, which guarantees a theoretical advantage of the quantum attention mechanism compared to their classical counterpart both in terms of asymptotic run time and the number of model parameters. These quantum architectures can be built using shallow quantum circuits and produce qualitatively different classification models. The three proposed quantum attention layers vary on the spectrum between closely following the classical transformers and exhibi",
    "path": "papers/22/09/2209.08167.json",
    "total_tokens": 740,
    "translated_title": "量子视觉变压器",
    "translated_abstract": "本文通过扩展已知在自然语言处理和图像分析中非常高效的最新经典变压器神经网络架构，设计并详细分析了量子变压器。在之前使用参数化量子电路进行数据加载和正交神经层的基础上，我们介绍了三种类型的量子变压器用于训练和推理，包括基于复合矩阵的量子变压器，它确保了量子注意机制在渐近运行时间和模型参数数量方面相较于它们的经典对应物存在理论优势。这些量子架构可以使用浅量子电路构建，并产生定性不同的分类模型。",
    "tldr": "本研究设计和分析了量子变压器，引入了三种类型的量子变压器用于训练和推理，在保证量子注意机制具有理论优势的基础上，采用浅量子电路构建，产生不同的分类模型。",
    "en_tdlr": "This work designs and analyzes quantum transformers, introduces three types of quantum transformers for training and inference, ensures a theoretical advantage of the quantum attention mechanism, and produces different classification models using shallow quantum circuits."
}