{
    "title": "PaLI: A Jointly-Scaled Multilingual Language-Image Model. (arXiv:2209.06794v3 [cs.CV] UPDATED)",
    "abstract": "Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text traini",
    "link": "http://arxiv.org/abs/2209.06794",
    "context": "Title: PaLI: A Jointly-Scaled Multilingual Language-Image Model. (arXiv:2209.06794v3 [cs.CV] UPDATED)\nAbstract: Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text traini",
    "path": "papers/22/09/2209.06794.json",
    "total_tokens": 1002,
    "translated_title": "PaLI: 一种联合缩放的多语言语言-图像模型",
    "translated_abstract": "有效的缩放和灵活的任务接口使得大型语言模型在许多任务上表现出色。我们提出了PaLI（Pathways Language and Image model），这是一种模型，可将这种方法扩展到语言和视觉的联合建模。PaLI基于视觉和文本输入生成文本，并通过此接口执行许多视觉、语言和多模态任务，在许多语言中完成。为了训练PaLI，我们利用了大型预训练编码器-解码器语言模型和Vision Transformers（ViT）。这使我们能够利用它们现有的能力并利用训练它们的重大成本。我们发现联合缩放视觉和语言组件很重要。由于现有的语言Transformer比它们的视觉对应物要大得多，因此我们训练了一个大型的40亿参数ViT（ViT-e）来量化即使更大容量的视觉模型的好处。为了训练PaLI，我们创建了一个基于新的图像-文本训练数据集的大型多语言的预训练任务混合。",
    "tldr": "PaLI是一种联合缩放的多语言语言-图像模型，可对图像和文本进行建模和执行许多视觉、语言和多模态任务，利用Transformer和Vision Transformer等先前的能力和成本。联合缩放在此任务中很重要，所以我们使用了一个40亿参数的Vision Transformer，以便利用更大容量的视觉模型的优势。",
    "en_tdlr": "PaLI is a jointly-scaled multilingual language-image model that allows for modeling of and performing various visual, language, and multimodal tasks based on text and visual inputs. It capitalizes on the existing capabilities and cost of large pre-trained encoder-decoder language models and Vision Transformers. Joint scaling is crucial for this task, and a 4-billion parameter Vision Transformer was trained to leverage the benefits of larger-capacity visual models."
}