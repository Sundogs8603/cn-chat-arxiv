{
    "title": "AudioLM: a Language Modeling Approach to Audio Generation. (arXiv:2209.03143v2 [cs.SD] UPDATED)",
    "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen",
    "link": "http://arxiv.org/abs/2209.03143",
    "context": "Title: AudioLM: a Language Modeling Approach to Audio Generation. (arXiv:2209.03143v2 [cs.SD] UPDATED)\nAbstract: We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen",
    "path": "papers/22/09/2209.03143.json",
    "total_tokens": 900,
    "translated_title": "AudioLM：一种音频生成的语言模型方法",
    "translated_abstract": "我们引入了AudioLM，这是一个高质量音频生成的框架，具有长期一致性。AudioLM将输入音频映射为一系列离散的符号，并将音频生成任务转化为语言建模任务。我们展示了现有音频分词工具在重构质量和长期结构之间提供的不同权衡，并提出了一种混合分词方案以实现两个目标。具体而言，我们利用在音频上预训练的带有掩码的语言模型的离散激活来捕捉长期结构，并利用神经音频编解码器产生的离散编码来实现高质量的合成。通过在大型原始音频波形语料库上进行训练，AudioLM学习了在给定短提示的情况下生成自然连续的能力。当在语音上进行训练且没有任何转录或注释时，AudioLM生成的语音连续保持语法和语义的合理性，同时还保持了未见过的说话者身份和韵律。",
    "tldr": "AudioLM是一个通过语言建模方法实现音频生成的框架，它通过将输入音频映射为离散符号序列，结合语言模型和神经音频编解码器，能够生成高质量、符合语法和语义要求的连续音频。"
}