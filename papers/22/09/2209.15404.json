{
    "title": "Entropy-driven Unsupervised Keypoint Representation Learning in Videos. (arXiv:2209.15404v2 [cs.CV] UPDATED)",
    "abstract": "Extracting informative representations from videos is fundamental for effectively learning various downstream tasks. We present a novel approach for unsupervised learning of meaningful representations from videos, leveraging the concept of image spatial entropy (ISE) that quantifies the per-pixel information in an image. We argue that \\textit{local entropy} of pixel neighborhoods and their temporal evolution create valuable intrinsic supervisory signals for learning prominent features. Building on this idea, we abstract visual features into a concise representation of keypoints that act as dynamic information transmitters, and design a deep learning model that learns, purely unsupervised, spatially and temporally consistent representations \\textit{directly} from video frames. Two original information-theoretic losses, computed from local entropy, guide our model to discover consistent keypoint representations; a loss that maximizes the spatial information covered by the keypoints and a",
    "link": "http://arxiv.org/abs/2209.15404",
    "context": "Title: Entropy-driven Unsupervised Keypoint Representation Learning in Videos. (arXiv:2209.15404v2 [cs.CV] UPDATED)\nAbstract: Extracting informative representations from videos is fundamental for effectively learning various downstream tasks. We present a novel approach for unsupervised learning of meaningful representations from videos, leveraging the concept of image spatial entropy (ISE) that quantifies the per-pixel information in an image. We argue that \\textit{local entropy} of pixel neighborhoods and their temporal evolution create valuable intrinsic supervisory signals for learning prominent features. Building on this idea, we abstract visual features into a concise representation of keypoints that act as dynamic information transmitters, and design a deep learning model that learns, purely unsupervised, spatially and temporally consistent representations \\textit{directly} from video frames. Two original information-theoretic losses, computed from local entropy, guide our model to discover consistent keypoint representations; a loss that maximizes the spatial information covered by the keypoints and a",
    "path": "papers/22/09/2209.15404.json",
    "total_tokens": 1031,
    "translated_title": "视频中基于信息熵的无监督关键点表示学习",
    "translated_abstract": "从视频中提取有意义的表示是有效地学习各种下游任务的基础。本文提出了一种新的方法，利用图像空间熵（ISE）的概念从视频中无监督地学习有意义的视觉表示。我们认为像素邻域的局部熵和它们的时间变化可以为学习突出的特征提供有价值的内在监督信号。基于这个思想，我们将视觉特征抽象为关键点的简明表示，作为动态信息传递者，并设计了一个深度学习模型，从视频帧中无监督地直接学习空间和时间上一致的表示。利用局部熵计算的两个原始的信息熵损失指导我们的模型发现一致的关键点表示；一个最大化关键点覆盖的空间信息的损失和一个最大化关键点在连续帧之间时间一致性的损失。我们在几个基准数据集上评估了我们的方法，并展示了它在关键点检测、动作识别和无监督聚类方面的有效性，优于现有的最先进方法。",
    "tldr": "本文提出了一种基于图像空间熵的无监督视频关键点表示学习方法，将视觉特征抽象为关键点的十分简明的表示，并通过局部熵计算的信息熵损失指导模型发现一致的关键点表示。该方法在多种基准数据集上展示了优异的性能，超过了现有最先进的方法。",
    "en_tdlr": "This paper proposes an unsupervised learning method for video keypoint representation with image spatial entropy, which abstracts visual features into a concise representation of keypoints and discovers consistent keypoint representations with information-theoretic losses computed from local entropy. The method shows promising performance on various benchmark datasets and outperforms state-of-the-art methods."
}