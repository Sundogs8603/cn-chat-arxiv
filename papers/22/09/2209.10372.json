{
    "title": "WeLM: A Well-Read Pre-trained Language Model for Chinese. (arXiv:2209.10372v5 [cs.CL] UPDATED)",
    "abstract": "Large Language Models pre-trained with self-supervised learning have demonstrated impressive zero-shot generalization capabilities on a wide spectrum of tasks. In this work, we present WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations. WeLM is trained with 10B parameters by \"reading\" a curated high-quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly outperform existing pre-trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi-lingual and code-switching understanding, outperforming existing multilingual language models pre-trained on 30 languages. Furthermore, We collected human-written prompts for a large set of supervised datasets in Chinese and fine-tuned WeLM",
    "link": "http://arxiv.org/abs/2209.10372",
    "context": "Title: WeLM: A Well-Read Pre-trained Language Model for Chinese. (arXiv:2209.10372v5 [cs.CL] UPDATED)\nAbstract: Large Language Models pre-trained with self-supervised learning have demonstrated impressive zero-shot generalization capabilities on a wide spectrum of tasks. In this work, we present WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations. WeLM is trained with 10B parameters by \"reading\" a curated high-quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly outperform existing pre-trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi-lingual and code-switching understanding, outperforming existing multilingual language models pre-trained on 30 languages. Furthermore, We collected human-written prompts for a large set of supervised datasets in Chinese and fine-tuned WeLM",
    "path": "papers/22/09/2209.10372.json",
    "total_tokens": 956,
    "translated_title": "WeLM: 一种面向中文的读过书的预训练语言模型",
    "translated_abstract": "以自我监督学习为基础的大型语言模型在广泛的任务中显示了出色的零样本泛化能力。在本文中，我们介绍了WeLM：一种能够零或少样本演示无缝执行不同类型任务的面向中文的读过书的预训练语言模型。WeLM通过阅读精心策划的高质量语料库中的信息，以100亿个参数进行训练。我们展示了WeLM在各个领域和语言方面具备广泛的知识。在18项独立的（中文）任务中，WeLM能够显著优于现有规模相似且预训练模型，并且能够匹配规模高达25倍的模型的性能。WeLM还表现出强大的多语言和代码转换理解能力，优于预先在30种语言上进行预训练的现有多语言语言模型。此外，我们为大量中文监督式数据集收集了人工编写的提示，并对WeLM进行了微调。",
    "tldr": "WeLM 是一种面向中文的读过书的预训练语言模型，它通过读取高质量的语料库训练了100亿个参数，并可以在18个中文任务中显著优于现有同规模预训练模型，同时表现出强大的多语言和代码转换理解能力。",
    "en_tdlr": "WeLM is a well-read pre-trained language model for Chinese, which is trained with 10B parameters by \"reading\" a curated high-quality corpus. It can significantly outperform existing pre-trained models with similar sizes in 18 monolingual (Chinese) tasks, and has strong capabilities in multi-lingual and code-switching understanding."
}