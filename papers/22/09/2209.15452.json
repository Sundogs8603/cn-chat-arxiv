{
    "title": "Safe Exploration Method for Reinforcement Learning under Existence of Disturbance. (arXiv:2209.15452v2 [cs.LG] UPDATED)",
    "abstract": "Recent rapid developments in reinforcement learning algorithms have been giving us novel possibilities in many fields. However, due to their exploring property, we have to take the risk into consideration when we apply those algorithms to safety-critical problems especially in real environments. In this study, we deal with a safe exploration problem in reinforcement learning under the existence of disturbance. We define the safety during learning as satisfaction of the constraint conditions explicitly defined in terms of the state and propose a safe exploration method that uses partial prior knowledge of a controlled object and disturbance. The proposed method assures the satisfaction of the explicit state constraints with a pre-specified probability even if the controlled object is exposed to a stochastic disturbance following a normal distribution. As theoretical results, we introduce sufficient conditions to construct conservative inputs not containing an exploring aspect used in th",
    "link": "http://arxiv.org/abs/2209.15452",
    "context": "Title: Safe Exploration Method for Reinforcement Learning under Existence of Disturbance. (arXiv:2209.15452v2 [cs.LG] UPDATED)\nAbstract: Recent rapid developments in reinforcement learning algorithms have been giving us novel possibilities in many fields. However, due to their exploring property, we have to take the risk into consideration when we apply those algorithms to safety-critical problems especially in real environments. In this study, we deal with a safe exploration problem in reinforcement learning under the existence of disturbance. We define the safety during learning as satisfaction of the constraint conditions explicitly defined in terms of the state and propose a safe exploration method that uses partial prior knowledge of a controlled object and disturbance. The proposed method assures the satisfaction of the explicit state constraints with a pre-specified probability even if the controlled object is exposed to a stochastic disturbance following a normal distribution. As theoretical results, we introduce sufficient conditions to construct conservative inputs not containing an exploring aspect used in th",
    "path": "papers/22/09/2209.15452.json",
    "total_tokens": 942,
    "translated_title": "存在扰动情况下强化学习的安全探索方法",
    "translated_abstract": "近年来，强化学习算法的快速发展在许多领域为我们提供了新的可能性。然而，由于其探索特性，当我们将这些算法应用于安全关键问题，特别是在实际环境中时，我们必须考虑风险。本文提出了一种涉及存在扰动时强化学习中的安全探索问题的方法。我们将学习过程中的安全性定义为以状态明确定义的限制条件的满足，并提出了一种安全探索方法，该方法利用所控制对象和扰动的部分先验知识。即使所控制对象暴露于遵循正态分布的随机扰动，该方法也能保证以预先指定的概率满足显式状态约束。在理论上，我们引入了足够的条件来构建不包含传统探索方法中的探索因素的保守输入。在摆动任务的模拟实验中，我们证明了所提出的方法可以在满足显式约束条件的情况下安全地学习。",
    "tldr": "本文提出了一种能够在存在扰动情况下安全进行强化学习探索的方法，该方法利用了所控制对象和扰动的部分先验知识，可以保证以预先指定的概率满足显式状态约束。",
    "en_tdlr": "This paper proposes a safe exploration method for reinforcement learning under the existence of disturbance, which uses partial prior knowledge of a controlled object and disturbance to guarantee the satisfaction of explicit state constraints with a pre-specified probability, ensuring safe learning even when the controlled object is exposed to stochastic disturbance following a normal distribution."
}