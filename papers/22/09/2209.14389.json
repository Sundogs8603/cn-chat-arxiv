{
    "title": "Downstream Datasets Make Surprisingly Good Pretraining Corpora. (arXiv:2209.14389v2 [cs.CL] UPDATED)",
    "abstract": "For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets. Despite the success of this approach, it remains unclear to what extent these gains are attributable to the massive background corpora employed for pretraining versus to the pretraining objectives themselves. This paper introduces a large-scale study of self-pretraining, where the same (downstream) training data is used for both pretraining and finetuning. In experiments addressing both ELECTRA and RoBERTa models and 10 distinct downstream classification datasets, we observe that self-pretraining rivals standard pretraining on the BookWiki corpus (despite using around $10\\times$--$500\\times$ less data), outperforming the latter on $7$ and $5$ datasets, respectively. Surprisingly, these task-specific pretrained models often perform well on other tasks, including the GLUE benchmark. Besides classification tasks, self-pretrain",
    "link": "http://arxiv.org/abs/2209.14389",
    "context": "Title: Downstream Datasets Make Surprisingly Good Pretraining Corpora. (arXiv:2209.14389v2 [cs.CL] UPDATED)\nAbstract: For most natural language processing tasks, the dominant practice is to finetune large pretrained transformer models (e.g., BERT) using smaller downstream datasets. Despite the success of this approach, it remains unclear to what extent these gains are attributable to the massive background corpora employed for pretraining versus to the pretraining objectives themselves. This paper introduces a large-scale study of self-pretraining, where the same (downstream) training data is used for both pretraining and finetuning. In experiments addressing both ELECTRA and RoBERTa models and 10 distinct downstream classification datasets, we observe that self-pretraining rivals standard pretraining on the BookWiki corpus (despite using around $10\\times$--$500\\times$ less data), outperforming the latter on $7$ and $5$ datasets, respectively. Surprisingly, these task-specific pretrained models often perform well on other tasks, including the GLUE benchmark. Besides classification tasks, self-pretrain",
    "path": "papers/22/09/2209.14389.json",
    "total_tokens": 982,
    "translated_title": "下游数据集意外地成为良好的预训练语料库",
    "translated_abstract": "对于大多数自然语言处理任务，主要的做法是使用更小的下游数据集对大型预训练变压器模型（例如BERT）进行微调。尽管这种方法取得了成功，但目前尚不清楚这些收益在多大程度上归因于用于预训练的大规模语料库，而不是预训练目标本身。本文介绍了一项关于自我预训练（self-pretraining）的大规模研究，其中相同的（下游）训练数据用于预训练和微调。在针对ELECTRA和RoBERTa模型以及10个不同的下游分类数据集的实验中，我们观察到自我预训练与使用BookWiki语料库进行标准预训练相媲美（尽管使用的数据量仅为后者的$10$倍到$500$倍不等），并且在$7$个和$5$个数据集上分别优于后者。令人惊讶的是，这些针对特定任务的预训练模型在其他任务上表现良好，包括GLUE基准测试。除了分类任务，自我预训练模型还可以用于生成和抽取任务。",
    "tldr": "本文研究了使用下游数据集进行自我预训练的效果，发现这种方法与使用大型语料库进行预训练的标准方法相媲美，并且在某些任务上更加优秀。同时，这些自我预训练模型还表现出了很好的泛化能力。",
    "en_tdlr": "This paper investigates the effectiveness of self-pretraining using downstream datasets and finds that it is comparable to the standard approach of pretraining on large corpora, and performs even better on some tasks. Moreover, the self-pretrained models exhibit good generalization abilities."
}