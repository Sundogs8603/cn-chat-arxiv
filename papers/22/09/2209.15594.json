{
    "title": "Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability. (arXiv:2209.15594v2 [cs.LG] UPDATED)",
    "abstract": "Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $S(\\theta)$, is bounded by $2/\\eta$, training is \"stable\" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen et al. (2021) observed two important phenomena. The first, dubbed progressive sharpening, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\\eta$. The second, dubbed edge of stability, is that the sharpness hovers at $2/\\eta$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessi",
    "link": "http://arxiv.org/abs/2209.15594",
    "context": "Title: Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability. (arXiv:2209.15594v2 [cs.LG] UPDATED)\nAbstract: Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $S(\\theta)$, is bounded by $2/\\eta$, training is \"stable\" and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen et al. (2021) observed two important phenomena. The first, dubbed progressive sharpening, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\\eta$. The second, dubbed edge of stability, is that the sharpness hovers at $2/\\eta$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessi",
    "path": "papers/22/09/2209.15594.json",
    "total_tokens": 1357,
    "translated_title": "自稳定性：梯度下降在稳定边缘的隐式偏差",
    "translated_abstract": "传统的梯度下降分析表明，当Hessian矩阵的最大特征值，也称为锐度$S(\\theta)$，被$2/\\eta$限制时，训练是“稳定的”，训练损失单调下降。然而，最近的研究发现，当用全批量或大批量梯度下降训练现代神经网络时，这种假设不成立。最近，Cohen等人(2021)观察到了两个重要现象。第一个现象被称为渐进锐化，在训练期间锐度稳步增加，直到达到不稳定性截止值$2/\\eta$。第二个现象被称为稳定边缘，在剩余的训练过程中，锐度停留在$2/\\eta$，而损失则持续下降，尽管不是单调的。我们证明了，在稳定边缘处，梯度下降的动态可以由一个三次泰勒展开式捕获:当迭代在Hessian矩阵的最大特征向量方向上发散时，锐度呈二次增长，比例系数由损失的二阶和三阶导数决定。当锐度精确为$2/\\eta$时，我们表明，迭代的自稳定性可以永久地保持在稳定边缘。此外，我们通过实验证明，这种自稳定现象使梯度下降具有隐式偏向稳定边缘解的偏差，而稳定边缘是损失明显低于稳定区域的区域。",
    "tldr": "传统梯度下降分析不适用于现代神经网络用全批次或大批次梯度下降训练，Cohen等人(2021)发现的梯度下降边缘稳定性现象表明，当锐度达到不稳定性截止值$2/\\eta$时，迭代具有自稳定性，并表现出隐式偏向稳定边缘解的偏差，这种现象通过捕获二阶和三阶导数的比例系数得到了解释。",
    "en_tdlr": "Traditional gradient descent analysis is not applicable to modern neural networks trained with full batch or large batch gradient descent. Cohen et al. (2021) discovered the edge of stability phenomenon, which indicates that the iterates have self-stabilization when the sharpness reaches the instability cutoff $2/\\eta$. This self-stabilization phenomenon endows gradient descent with an implicit bias towards solutions at the edge of stability. This phenomenon is explained by capturing the proportionality coefficient through the second and third derivatives of the loss."
}