{
    "title": "MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning. (arXiv:2209.07902v4 [cs.LG] UPDATED)",
    "abstract": "As a successful approach to self-supervised learning, contrastive learning aims to learn invariant information shared among distortions of the input sample. While contrastive learning has yielded continuous advancements in sampling strategy and architecture design, it still remains two persistent defects: the interference of task-irrelevant information and sample inefficiency, which are related to the recurring existence of trivial constant solutions. From the perspective of dimensional analysis, we find out that the dimensional redundancy and dimensional confounder are the intrinsic issues behind the phenomena, and provide experimental evidence to support our viewpoint. We further propose a simple yet effective approach MetaMask, short for the dimensional Mask learned by Meta-learning, to learn representations against dimensional redundancy and confounder. MetaMask adopts the redundancy-reduction technique to tackle the dimensional redundancy issue and innovatively introduces a dimens",
    "link": "http://arxiv.org/abs/2209.07902",
    "context": "Title: MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning. (arXiv:2209.07902v4 [cs.LG] UPDATED)\nAbstract: As a successful approach to self-supervised learning, contrastive learning aims to learn invariant information shared among distortions of the input sample. While contrastive learning has yielded continuous advancements in sampling strategy and architecture design, it still remains two persistent defects: the interference of task-irrelevant information and sample inefficiency, which are related to the recurring existence of trivial constant solutions. From the perspective of dimensional analysis, we find out that the dimensional redundancy and dimensional confounder are the intrinsic issues behind the phenomena, and provide experimental evidence to support our viewpoint. We further propose a simple yet effective approach MetaMask, short for the dimensional Mask learned by Meta-learning, to learn representations against dimensional redundancy and confounder. MetaMask adopts the redundancy-reduction technique to tackle the dimensional redundancy issue and innovatively introduces a dimens",
    "path": "papers/22/09/2209.07902.json",
    "total_tokens": 776,
    "translated_title": "MetaMask：重新思考自监督学习中的维度干扰问题",
    "translated_abstract": "作为自监督学习的成功方法，对比学习旨在学习在输入样本的扭曲之间共享的不变信息。然而，对比学习仍然存在两个持久的缺陷：任务无关信息的干扰和样本效率低下，这与平凡常数解的反复存在相关。从维度分析的角度，我们发现维度冗余和维度干扰是这些现象背后的固有问题，并提供实验证据支持了我们的观点。我们进一步提出了一个简单而有效的方法MetaMask，即通过元学习学习的维度遮罩，以对抗维度冗余和干扰。MetaMask采用冗余减少技术来解决维度冗余问题，并创新地引入了一种维度干扰解决方法。",
    "tldr": "MetaMask是一种通过元学习学习的维度遮罩，用于对抗自监督学习中的维度冗余和干扰问题。"
}