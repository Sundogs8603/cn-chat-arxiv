{
    "title": "Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation. (arXiv:2209.02821v3 [cs.CL] UPDATED)",
    "abstract": "We propose a two-stage approach for training a single NMT model to translate unseen languages both to and from English. For the first stage, we initialize an encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform multilingual fine-tuning on parallel data in 40 languages to English. We find this model can generalize to zero-shot translations on unseen languages. For the second stage, we leverage this generalization ability to generate synthetic parallel data from monolingual datasets, then train with successive rounds of bidirectional back-translation.  We term our approach EcXTra ({E}nglish-{c}entric Crosslingual ({X}) {Tra}nsfer). Our approach is conceptually simple, only using a standard cross-entropy objective throughout, and also is data-driven, sequentially leveraging auxiliary parallel data and monolingual data. We evaluate our unsupervised NMT results on 7 low-resource languages, and find that each round of back-translation training further refines bidirecti",
    "link": "http://arxiv.org/abs/2209.02821",
    "context": "Title: Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation. (arXiv:2209.02821v3 [cs.CL] UPDATED)\nAbstract: We propose a two-stage approach for training a single NMT model to translate unseen languages both to and from English. For the first stage, we initialize an encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform multilingual fine-tuning on parallel data in 40 languages to English. We find this model can generalize to zero-shot translations on unseen languages. For the second stage, we leverage this generalization ability to generate synthetic parallel data from monolingual datasets, then train with successive rounds of bidirectional back-translation.  We term our approach EcXTra ({E}nglish-{c}entric Crosslingual ({X}) {Tra}nsfer). Our approach is conceptually simple, only using a standard cross-entropy objective throughout, and also is data-driven, sequentially leveraging auxiliary parallel data and monolingual data. We evaluate our unsupervised NMT results on 7 low-resource languages, and find that each round of back-translation training further refines bidirecti",
    "path": "papers/22/09/2209.02821.json",
    "total_tokens": 992,
    "translated_title": "通过多语言微调和回译实现的多语言双向无监督翻译",
    "translated_abstract": "我们提出了一种两阶段方法，用于训练单个NMT模型以将未见过的语言从和到英语进行翻译。对于第一阶段，我们将编码器-解码器模型初始化为预训练的XLM-R和RoBERTa权重，然后对40种语言到英语的并行数据进行多语言微调。我们发现，该模型可以推广到未见过的语言的零-shot翻译。对于第二阶段，我们利用这种推广能力从单语言数据集生成合成并行数据，然后使用连续的双向回译轮次进行训练。我们称这种方法为EcXTra（{E}nglish-{c}entric Crosslingual ({X}) {Tra}nsfer）。我们的方法在概念上很简单，只使用标准的交叉熵目标，并且是数据驱动的，依次利用辅助并行数据和单语言数据。我们评估我们在7种低资源语言上的无监督NMT结果，发现每一轮回译训练都进一步优化了双向无监督翻译质量，相比基线提高了多达10个BLEU分数。",
    "tldr": "该论文提出了一种两阶段的方法，实现单个NMT模型对未见过的语言同英语之间的双向无监督翻译。该方法包括多语言微调和双向回译，成功提高了翻译质量。",
    "en_tdlr": "The paper proposes a two-stage approach for bidirectional unsupervised translation between unseen languages and English using a single NMT model, through multilingual fine-tuning and back-translation, achieving improved translation quality."
}