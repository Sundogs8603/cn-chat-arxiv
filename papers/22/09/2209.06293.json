{
    "title": "Do Androids Laugh at Electric Sheep? Humor \"Understanding\" Benchmarks from The New Yorker Caption Contest. (arXiv:2209.06293v2 [cs.CL] UPDATED)",
    "abstract": "Large neural networks can now generate jokes, but do they really \"understand\" humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of \"understanding\" a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of indirect and playful allusions to human experience and culture. We investigate both multimodal and language-only models: the former are challenged with the cartoon images directly, while the latter are given multifaceted descriptions of the visual scene to simulate human-level visual understanding. We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided gr",
    "link": "http://arxiv.org/abs/2209.06293",
    "context": "Title: Do Androids Laugh at Electric Sheep? Humor \"Understanding\" Benchmarks from The New Yorker Caption Contest. (arXiv:2209.06293v2 [cs.CL] UPDATED)\nAbstract: Large neural networks can now generate jokes, but do they really \"understand\" humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of \"understanding\" a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of indirect and playful allusions to human experience and culture. We investigate both multimodal and language-only models: the former are challenged with the cartoon images directly, while the latter are given multifaceted descriptions of the visual scene to simulate human-level visual understanding. We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided gr",
    "path": "papers/22/09/2209.06293.json",
    "total_tokens": 978,
    "translated_title": "Android们是否会笑电子羊？源自纽约客漫画字幕比赛的幽默“理解”评估。",
    "translated_abstract": "目前大型神经网络能够生成笑话，但它们真正“理解”幽默吗？我们通过三个任务挑战AI模型，这些任务源自于纽约客漫画字幕比赛：将笑话与漫画配对、识别获胜字幕并解释为什么获胜的字幕很有趣。这些任务逐渐包含了“理解”漫画的更复杂方面；关键因素是图像与字幕之间的复杂、常常令人惊讶的关系，以及频繁包含对人类经验和文化的间接和富有玩味的暗示。我们研究了多模态和仅文本模型：前者直接面对漫画图像进行挑战，而后者则给出了多方面的视觉场景描述以模拟人类级别的视觉理解。我们发现两种类型的模型在所有三个任务上都面临困难。例如，我们最好的多模态模型在配对任务上的准确率比人类表现低30个百分点，即便在提供了图像描述的情况下。",
    "tldr": "通过纽约客漫画字幕比赛的任务，我们挑战了AI模型对幽默的“理解”。结果发现，无论是多模态模型还是仅文本模型，在配对笑话和漫画、识别获胜字幕以及解释获胜字幕为什么有趣的任务上都存在困难。",
    "en_tdlr": "We challenge AI models to understand humor through tasks derived from the New Yorker Cartoon Caption Contest. Our findings reveal that both multimodal and language-only models struggle to match jokes to cartoons, identify winning captions, and explain the humor behind them."
}