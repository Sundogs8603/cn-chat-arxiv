{
    "title": "What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v2 [cs.CL] UPDATED)",
    "abstract": "In this paper, we propose a theoretical framework to explain the efficacy of prompt learning in zero/few-shot scenarios. First, we prove that conventional pre-training and fine-tuning paradigm fails in few-shot scenarios due to overfitting the unrepresentative labelled data. We then detail the assumption that prompt learning is more effective because it empowers pre-trained language model that is built upon massive text corpora, as well as domain-related human knowledge to participate more in prediction and thereby reduces the impact of limited label information provided by the small training set. We further hypothesize that language discrepancy can measure the quality of prompting. Comprehensive experiments are performed to verify our assumptions. More remarkably, inspired by the theoretical framework, we propose an annotation-agnostic template selection method based on perplexity, which enables us to ``forecast'' the prompting performance in advance. This approach is especially encou",
    "link": "http://arxiv.org/abs/2209.15206",
    "context": "Title: What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v2 [cs.CL] UPDATED)\nAbstract: In this paper, we propose a theoretical framework to explain the efficacy of prompt learning in zero/few-shot scenarios. First, we prove that conventional pre-training and fine-tuning paradigm fails in few-shot scenarios due to overfitting the unrepresentative labelled data. We then detail the assumption that prompt learning is more effective because it empowers pre-trained language model that is built upon massive text corpora, as well as domain-related human knowledge to participate more in prediction and thereby reduces the impact of limited label information provided by the small training set. We further hypothesize that language discrepancy can measure the quality of prompting. Comprehensive experiments are performed to verify our assumptions. More remarkably, inspired by the theoretical framework, we propose an annotation-agnostic template selection method based on perplexity, which enables us to ``forecast'' the prompting performance in advance. This approach is especially encou",
    "path": "papers/22/09/2209.15206.json",
    "total_tokens": 830,
    "translated_title": "预训练语言模型为什么更适合零样本学习？",
    "translated_abstract": "本文提出了一个理论框架来解释prompt learning在零样本/少样本场景下的有效性。首先，我们证明传统的预训练和微调范式在少样本场景下会因为过拟合不具代表性的标注数据而失败。然后，我们详细阐述了prompt learning更有效的假设，因为它使建立在海量文本语料库和领域相关人类知识的预训练语言模型可以更多地参与预测，从而减少小型训练集提供的有限标签信息的影响。我们进一步假设，语言差异可以衡量提示质量。我们进行了全面的实验证明了我们的假设。更为重要的是，受到理论框架的启发，我们提出了一种基于困惑度的注释无关的模板选择方法，可以事先“预测”提示性能。这种方法特别值得鼓励。",
    "tldr": "本文提出一个理论框架来解释prompt learning在零样本/少样本场景下的有效性，并基于此提出了一个注释无关的模板选择方法。",
    "en_tdlr": "This paper proposes a theoretical framework to explain the effectiveness of prompt learning in zero/few-shot scenarios, and proposes an annotation-agnostic template selection method based on perplexity inspired by the framework."
}