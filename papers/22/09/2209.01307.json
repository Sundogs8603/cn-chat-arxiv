{
    "title": "TransPolymer: a Transformer-based language model for polymer property predictions. (arXiv:2209.01307v4 [cs.LG] UPDATED)",
    "abstract": "Accurate and efficient prediction of polymer properties is of great significance in polymer design. Conventionally, expensive and time-consuming experiments or simulations are required to evaluate polymer functions. Recently, Transformer models, equipped with self-attention mechanisms, have exhibited superior performance in natural language processing. However, such methods have not been investigated in polymer sciences. Herein, we report TransPolymer, a Transformer-based language model for polymer property prediction. Our proposed polymer tokenizer with chemical awareness enables learning representations from polymer sequences. Rigorous experiments on ten polymer property prediction benchmarks demonstrate the superior performance of TransPolymer. Moreover, we show that TransPolymer benefits from pretraining on large unlabeled dataset via Masked Language Modeling. Experimental results further manifest the important role of self-attention in modeling polymer sequences. We highlight this",
    "link": "http://arxiv.org/abs/2209.01307",
    "context": "Title: TransPolymer: a Transformer-based language model for polymer property predictions. (arXiv:2209.01307v4 [cs.LG] UPDATED)\nAbstract: Accurate and efficient prediction of polymer properties is of great significance in polymer design. Conventionally, expensive and time-consuming experiments or simulations are required to evaluate polymer functions. Recently, Transformer models, equipped with self-attention mechanisms, have exhibited superior performance in natural language processing. However, such methods have not been investigated in polymer sciences. Herein, we report TransPolymer, a Transformer-based language model for polymer property prediction. Our proposed polymer tokenizer with chemical awareness enables learning representations from polymer sequences. Rigorous experiments on ten polymer property prediction benchmarks demonstrate the superior performance of TransPolymer. Moreover, we show that TransPolymer benefits from pretraining on large unlabeled dataset via Masked Language Modeling. Experimental results further manifest the important role of self-attention in modeling polymer sequences. We highlight this",
    "path": "papers/22/09/2209.01307.json",
    "total_tokens": 881,
    "translated_title": "基于Transformer的高分子属性预测语言模型TransPolymer",
    "translated_abstract": "在高分子设计中，准确且高效地预测高分子的属性具有重要意义。传统上，需要进行昂贵且耗时的实验或模拟才能评估高分子的功能。最近，在自注意力机制的支持下，Transformer模型在自然语言处理方面表现出了出色的性能。然而，这种方法在高分子科学中尚未得到研究。在此，我们报道了TransPolymer，一种基于Transformer的高分子属性预测语言模型。我们提出了一种具有化学感知能力的高分子分词器，使其能够学习高分子序列的表示。在十个高分子属性预测基准测试中的严格实验表明了TransPolymer的优越性能。此外，我们展示了TransPolymer从大型无标记数据集的预训练中受益。实验结果进一步表明了自注意力在建模高分子序列中的重要作用。我们强调了这一点。",
    "tldr": "本文提出了一种基于Transformer的高分子属性预测语言模型TransPolymer，它利用化学感知能力的高分子分词器学习高分子序列的表示，并通过预训练获得了更好的性能。"
}