{
    "title": "Adversarial Coreset Selection for Efficient Robust Training. (arXiv:2209.05785v2 [cs.LG] UPDATED)",
    "abstract": "Neural networks are vulnerable to adversarial attacks: adding well-crafted, imperceptible perturbations to their input can modify their output. Adversarial training is one of the most effective approaches to training robust models against such attacks. Unfortunately, this method is much slower than vanilla training of neural networks since it needs to construct adversarial examples for the entire training data at every iteration. By leveraging the theory of coreset selection, we show how selecting a small subset of training data provides a principled approach to reducing the time complexity of robust training. To this end, we first provide convergence guarantees for adversarial coreset selection. In particular, we show that the convergence bound is directly related to how well our coresets can approximate the gradient computed over the entire training data. Motivated by our theoretical analysis, we propose using this gradient approximation error as our adversarial coreset selection obj",
    "link": "http://arxiv.org/abs/2209.05785",
    "context": "Title: Adversarial Coreset Selection for Efficient Robust Training. (arXiv:2209.05785v2 [cs.LG] UPDATED)\nAbstract: Neural networks are vulnerable to adversarial attacks: adding well-crafted, imperceptible perturbations to their input can modify their output. Adversarial training is one of the most effective approaches to training robust models against such attacks. Unfortunately, this method is much slower than vanilla training of neural networks since it needs to construct adversarial examples for the entire training data at every iteration. By leveraging the theory of coreset selection, we show how selecting a small subset of training data provides a principled approach to reducing the time complexity of robust training. To this end, we first provide convergence guarantees for adversarial coreset selection. In particular, we show that the convergence bound is directly related to how well our coresets can approximate the gradient computed over the entire training data. Motivated by our theoretical analysis, we propose using this gradient approximation error as our adversarial coreset selection obj",
    "path": "papers/22/09/2209.05785.json",
    "total_tokens": 843,
    "translated_title": "针对高效稳健训练的对抗性核心集选择",
    "translated_abstract": "神经网络对对抗攻击具有脆弱性：向其输入加入微小但精心制作的扰动可以改变其输出。对抗训练是训练抵御此类攻击的最有效方法之一。不幸的是，这种方法比原始神经网络的训练慢得多，因为它需要在每次迭代中为整个训练数据构建对抗性示例。通过利用核心集选择理论，我们展示了选择训练数据的一个小子集是减少稳健训练时间复杂度的一个有原则方法。为此，我们首先提供了对抗性核心集选择的收敛保证。特别地，我们展示了收敛上界与核心集能够近似计算整个训练数据梯度的能力之间的直接关系。受到我们的理论分析的启发，我们提出使用梯度近似误差作为对抗性核心集选择目标。",
    "tldr": "本文提出了一种对抗性核心集选择的方法，通过选择训练数据的小子集来降低稳健训练的时间复杂度，并提供了收敛保证。"
}