{
    "title": "Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making. (arXiv:2209.11812v3 [cs.HC] UPDATED)",
    "abstract": "In this work, we study the effects of feature-based explanations on distributive fairness of AI-assisted decisions, specifically focusing on the task of predicting occupations from short textual bios. We also investigate how any effects are mediated by humans' fairness perceptions and their reliance on AI recommendations. Our findings show that explanations influence fairness perceptions, which, in turn, relate to humans' tendency to adhere to AI recommendations. However, we see that such explanations do not enable humans to discern correct and incorrect AI recommendations. Instead, we show that they may affect reliance irrespective of the correctness of AI recommendations. Depending on which features an explanation highlights, this can foster or hinder distributive fairness: when explanations highlight features that are task-irrelevant and evidently associated with the sensitive attribute, this prompts overrides that counter AI recommendations that align with gender stereotypes. Meanw",
    "link": "http://arxiv.org/abs/2209.11812",
    "context": "Title: Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making. (arXiv:2209.11812v3 [cs.HC] UPDATED)\nAbstract: In this work, we study the effects of feature-based explanations on distributive fairness of AI-assisted decisions, specifically focusing on the task of predicting occupations from short textual bios. We also investigate how any effects are mediated by humans' fairness perceptions and their reliance on AI recommendations. Our findings show that explanations influence fairness perceptions, which, in turn, relate to humans' tendency to adhere to AI recommendations. However, we see that such explanations do not enable humans to discern correct and incorrect AI recommendations. Instead, we show that they may affect reliance irrespective of the correctness of AI recommendations. Depending on which features an explanation highlights, this can foster or hinder distributive fairness: when explanations highlight features that are task-irrelevant and evidently associated with the sensitive attribute, this prompts overrides that counter AI recommendations that align with gender stereotypes. Meanw",
    "path": "papers/22/09/2209.11812.json",
    "total_tokens": 938,
    "translated_title": "解释、公正性和人类-AI决策中的适当依赖（arXiv:2209.11812v3 [cs.HC] UPDATED）",
    "translated_abstract": "在这项工作中，我们研究了基于特征的解释对AI辅助决策的公正性的影响，特别关注从简短的文本简介中预测职业的任务。我们还研究了这些影响如何通过人们的公正性感知和对AI建议的依赖来调节。我们的研究结果表明，解释影响了公正性感知，而公正性感知又与人们遵循AI建议的倾向相关。然而，我们发现这样的解释并不能让人们区分正确和错误的AI建议。相反，我们发现解释可能会影响依赖，而不论AI建议的正确性如何。取决于解释突出的特征，这可以促进或阻碍分配公正性：当解释突出与敏感属性明显相关的与任务无关的特征时，这会促使人们覆盖AI与性别刻板印象一致的建议。",
    "tldr": "本研究探讨了基于特征的解释对AI辅助决策公正性的影响，发现解释可以影响公正性感知和人们对AI建议的依赖。然而，解释并不能帮助人们区分正确和错误的AI建议。这些发现对于促进有效的决策和公正性至关重要。",
    "en_tdlr": "This study investigates the impact of feature-based explanations on fairness perceptions and reliance on AI recommendations in decision-making. The findings suggest that explanations influence fairness perceptions and reliance, but do not necessarily improve the ability to discern correct AI recommendations. These findings are crucial for promoting effective decision-making and fairness."
}