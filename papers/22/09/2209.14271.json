{
    "title": "Generalization in Deep Reinforcement Learning for Robotic Navigation by Reward Shaping. (arXiv:2209.14271v2 [cs.RO] UPDATED)",
    "abstract": "In this paper, we study the application of DRL algorithms in the context of local navigation problems, in which a robot moves towards a goal location in unknown and cluttered workspaces equipped only with limited-range exteroceptive sensors, such as LiDAR. Collision avoidance policies based on DRL present some advantages, but they are quite susceptible to local minima, once their capacity to learn suitable actions is limited to the sensor range. Since most robots perform tasks in unstructured environments, it is of great interest to seek generalized local navigation policies capable of avoiding local minima, especially in untrained scenarios. To do so, we propose a novel reward function that incorporates map information gained in the training stage, increasing the agent's capacity to deliberate about the best course of action. Also, we use the SAC algorithm for training our ANN, which shows to be more effective than others in the state-of-the-art literature. A set of sim-to-sim and sim",
    "link": "http://arxiv.org/abs/2209.14271",
    "context": "Title: Generalization in Deep Reinforcement Learning for Robotic Navigation by Reward Shaping. (arXiv:2209.14271v2 [cs.RO] UPDATED)\nAbstract: In this paper, we study the application of DRL algorithms in the context of local navigation problems, in which a robot moves towards a goal location in unknown and cluttered workspaces equipped only with limited-range exteroceptive sensors, such as LiDAR. Collision avoidance policies based on DRL present some advantages, but they are quite susceptible to local minima, once their capacity to learn suitable actions is limited to the sensor range. Since most robots perform tasks in unstructured environments, it is of great interest to seek generalized local navigation policies capable of avoiding local minima, especially in untrained scenarios. To do so, we propose a novel reward function that incorporates map information gained in the training stage, increasing the agent's capacity to deliberate about the best course of action. Also, we use the SAC algorithm for training our ANN, which shows to be more effective than others in the state-of-the-art literature. A set of sim-to-sim and sim",
    "path": "papers/22/09/2209.14271.json",
    "total_tokens": 997,
    "translated_title": "基于奖励塑造的深度强化学习在机器人导航中的泛化能力研究",
    "translated_abstract": "本文研究了深度强化学习算法在局部导航问题中的应用，其中机器人在未知且杂乱的工作区域中，只配备有受限范围外部感知传感器（如LiDAR），向目标位置移动。基于深度强化学习的碰撞避免策略具有一些优势，但它们对局部极小值非常敏感，因为它们只能在传感器范围内学习适当的动作。由于大多数机器人在非结构化环境中执行任务，寻求能够避免局部极小值的泛化局部导航策略对我们来说非常有意义，特别是在非训练场景中。为了实现这一目标，我们提出了一种新颖的奖励函数，该函数融合了在训练阶段获取的地图信息，增加了Agent的谨慎考虑最佳行动方案的能力。此外，我们使用SAC算法来训练我们的ANN，该算法在最新的文献中显示出比其他算法更有效。",
    "tldr": "本文研究了深度强化学习算法在机器人导航中的应用，提出了一种新的奖励函数，结合训练阶段获取的地图信息，使机器人能够在未知环境中避免局部极小值，并使用SAC算法进行训练，相比其他算法更有效。",
    "en_tdlr": "This paper investigates the application of deep reinforcement learning algorithms in robotic navigation and proposes a novel reward function that incorporates map information to enable the robot to avoid local minima in unknown environments. The SAC algorithm is used for training, which proves to be more effective than other algorithms."
}