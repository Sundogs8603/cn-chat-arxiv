{
    "title": "The Curse of Unrolling: Rate of Differentiating Through Optimization. (arXiv:2209.13271v2 [math.OC] UPDATED)",
    "abstract": "Computing the Jacobian of the solution of an optimization problem is a central problem in machine learning, with applications in hyperparameter optimization, meta-learning, optimization as a layer, and dataset distillation, to name a few. Unrolled differentiation is a popular heuristic that approximates the solution using an iterative solver and differentiates it through the computational path. This work provides a non-asymptotic convergence-rate analysis of this approach on quadratic objectives for gradient descent and the Chebyshev method. We show that to ensure convergence of the Jacobian, we can either 1) choose a large learning rate leading to a fast asymptotic convergence but accept that the algorithm may have an arbitrarily long burn-in phase or 2) choose a smaller learning rate leading to an immediate but slower convergence. We refer to this phenomenon as the curse of unrolling. Finally, we discuss open problems relative to this approach, such as deriving a practical update rul",
    "link": "http://arxiv.org/abs/2209.13271",
    "context": "Title: The Curse of Unrolling: Rate of Differentiating Through Optimization. (arXiv:2209.13271v2 [math.OC] UPDATED)\nAbstract: Computing the Jacobian of the solution of an optimization problem is a central problem in machine learning, with applications in hyperparameter optimization, meta-learning, optimization as a layer, and dataset distillation, to name a few. Unrolled differentiation is a popular heuristic that approximates the solution using an iterative solver and differentiates it through the computational path. This work provides a non-asymptotic convergence-rate analysis of this approach on quadratic objectives for gradient descent and the Chebyshev method. We show that to ensure convergence of the Jacobian, we can either 1) choose a large learning rate leading to a fast asymptotic convergence but accept that the algorithm may have an arbitrarily long burn-in phase or 2) choose a smaller learning rate leading to an immediate but slower convergence. We refer to this phenomenon as the curse of unrolling. Finally, we discuss open problems relative to this approach, such as deriving a practical update rul",
    "path": "papers/22/09/2209.13271.json",
    "total_tokens": 1016,
    "translated_title": "展开的诅咒：系统优化的不同化速率",
    "translated_abstract": "计算优化问题解的雅可比矩阵是机器学习中的一个中心问题，在超参数优化、元学习、优化作为一种层以及数据集蒸馏等方面有着广泛应用。展开求导是一种流行的启发式方法，它使用迭代求解器近似求解解，并通过计算路径进行不同化。本文针对梯度下降和Chebyshev方法中二次目标提供了该方法的非渐进收敛速率分析。我们表明，为了确保雅可比矩阵的收敛，我们可以选择1）选择大的学习率，导致快速的渐进收敛，但可能会接受算法具有任意长的初始阶段，或者2）选择较小的学习率，导致即时但较慢的收敛。我们称之为展开的诅咒。最后，我们讨论了与此方法相关的开放问题，例如导出实用的更新规则。",
    "tldr": "优化问题中的展开求导是机器学习中的一个重要问题，本文对其在梯度下降和 Chebyshev 方法中的二次目标提供了非渐进收敛速率分析，发现我们要确保雅可比矩阵的收敛，就必须面临展开求导的诅咒，即要么选择大的学习率导致快速渐进收敛但算法有较长的初始阶段，要么选择较小的学习率导致即时但较慢的收敛。",
    "en_tdlr": "Unrolled differentiation in optimization problems is crucial in machine learning, and this paper provides non-asymptotic convergence-rate analysis of this approach on quadratic objectives for gradient descent and the Chebyshev method, with the finding that to ensure the convergence of the Jacobian, we must face the curse of unrolling, either choosing a large learning rate with arbitrary long burn-in phase or a smaller one with slower convergence. The paper concludes with open problems related to this approach."
}