{
    "title": "Exploiting Transformer in Sparse Reward Reinforcement Learning for Interpretable Temporal Logic Motion Planning. (arXiv:2209.13220v2 [cs.RO] UPDATED)",
    "abstract": "Automaton based approaches have enabled robots to perform various complex tasks. However, most existing automaton based algorithms highly rely on the manually customized representation of states for the considered task, limiting its applicability in deep reinforcement learning algorithms. To address this issue, by incorporating Transformer into reinforcement learning, we develop a Double-Transformer-guided Temporal Logic framework (T2TL) that exploits the structural feature of Transformer twice, i.e., first encoding the LTL instruction via the Transformer module for efficient understanding of task instructions during the training and then encoding the context variable via the Transformer again for improved task performance. Particularly, the LTL instruction is specified by co-safe LTL. As a semantics-preserving rewriting operation, LTL progression is exploited to decompose the complex task into learnable sub-goals, which not only converts non-Markovian reward decision processes to Mark",
    "link": "http://arxiv.org/abs/2209.13220",
    "context": "Title: Exploiting Transformer in Sparse Reward Reinforcement Learning for Interpretable Temporal Logic Motion Planning. (arXiv:2209.13220v2 [cs.RO] UPDATED)\nAbstract: Automaton based approaches have enabled robots to perform various complex tasks. However, most existing automaton based algorithms highly rely on the manually customized representation of states for the considered task, limiting its applicability in deep reinforcement learning algorithms. To address this issue, by incorporating Transformer into reinforcement learning, we develop a Double-Transformer-guided Temporal Logic framework (T2TL) that exploits the structural feature of Transformer twice, i.e., first encoding the LTL instruction via the Transformer module for efficient understanding of task instructions during the training and then encoding the context variable via the Transformer again for improved task performance. Particularly, the LTL instruction is specified by co-safe LTL. As a semantics-preserving rewriting operation, LTL progression is exploited to decompose the complex task into learnable sub-goals, which not only converts non-Markovian reward decision processes to Mark",
    "path": "papers/22/09/2209.13220.json",
    "total_tokens": 946,
    "translated_title": "在稀疏奖励强化学习中利用Transformer进行可解释的时序逻辑运动规划",
    "translated_abstract": "基于自动机的方法使得机器人能够执行各种复杂任务。然而，大多数现有的基于自动机的算法在考虑的任务中高度依赖于手动定制的状态表示，限制了其在深度强化学习算法中的适用性。为了解决这个问题，我们通过将Transformer引入强化学习，开发了一个双Transformer引导的时序逻辑框架(T2TL)，它两次利用Transformer的结构特性，即首先通过Transformer模块对LTL指令进行编码，以在训练过程中高效理解任务指令，然后再次通过Transformer对上下文变量进行编码，以改进任务的性能。特别地，LTL指令由限守法LTL指定。作为一种语义保持的重写操作，利用LTL进展将复杂任务分解为可学习的子目标，从而将非马尔可夫奖励决策过程转换为马尔可夫奖励决策过程，从而实现了更好的任务性能。",
    "tldr": "该论文提出了一个通过将Transformer应用于稀疏奖励强化学习的方法，开发了一个双Transformer引导的时序逻辑框架(T2TL)，该框架通过两次利用Transformer的结构特性，使得机器人在训练过程中能够高效理解任务指令，并改进任务的性能。",
    "en_tdlr": "This paper proposes a method that applies Transformer in sparse reward reinforcement learning and develops a Double-Transformer-guided Temporal Logic framework (T2TL), which utilizes the structural feature of Transformer to enable efficient understanding of task instructions and improve task performance."
}