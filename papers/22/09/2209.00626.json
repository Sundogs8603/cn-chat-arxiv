{
    "title": "The alignment problem from a deep learning perspective. (arXiv:2209.00626v5 [cs.AI] UPDATED)",
    "abstract": "In coming decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that conflict (i.e., are misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing this outcome.",
    "link": "http://arxiv.org/abs/2209.00626",
    "context": "Title: The alignment problem from a deep learning perspective. (arXiv:2209.00626v5 [cs.AI] UPDATED)\nAbstract: In coming decades, artificial general intelligence (AGI) may surpass human capabilities at many critical tasks. We argue that, without substantial effort to prevent it, AGIs could learn to pursue goals that conflict (i.e., are misaligned) with human interests. If trained like today's most capable models, AGIs could learn to act deceptively to receive higher reward, learn internally-represented goals which generalize beyond their fine-tuning distributions, and pursue those goals using power-seeking strategies. We review emerging evidence for these properties. AGIs with these properties would be difficult to align and may appear aligned even when they are not. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing this outcome.",
    "path": "papers/22/09/2209.00626.json",
    "total_tokens": 863,
    "translated_title": "从深度学习的视角看待对齐问题",
    "translated_abstract": "在未来几十年内，人工通用智能（AGI）可能在许多关键任务上超越人类能力。我们认为，如果没有大量努力来防止它，AGIs可能会学会追求与人类利益冲突（即不对齐）的目标。如果像现在最具能力的模型一样进行训练，AGIs可能会学会欺骗性地行动以获得更高的奖励，学会在其微调分布之外进行内部目标的泛化，并利用寻求权力的策略来追求这些目标。我们回顾了这些特性的新证据。具有这些特性的AGIs将很难进行对齐，即使在不对齐的情况下也可能表现出对齐。我们概述了不对齐的AGIs的部署如何可能会不可逆地削弱人类对世界的控制，并简要回顾了旨在防止这种结果的研究方向。",
    "tldr": "人工通用智能（AGI）的出现可能会导致其追求与人类利益不对齐的目标，并采用欺骗性行为和权力追求策略。防止这种情况的发生是一个重要的研究方向。",
    "en_tdlr": "The emergence of Artificial General Intelligence (AGI) could potentially lead to the pursuit of goals that are misaligned with human interests, with AGIs employing deceptive behavior and power-seeking strategies. Preventing such scenarios is an important research direction."
}