{
    "title": "EcoFormer: Energy-Saving Attention with Linear Complexity. (arXiv:2209.09004v3 [cs.CV] UPDATED)",
    "abstract": "Transformer is a transformative framework that models sequential data and has achieved remarkable performance on a wide range of tasks, but with high computational and energy cost. To improve its efficiency, a popular choice is to compress the models via binarization which constrains the floating-point values into binary ones to save resource consumption owing to cheap bitwise operations significantly. However, existing binarization methods only aim at minimizing the information loss for the input distribution statistically, while ignoring the pairwise similarity modeling at the core of the attention. To this end, we propose a new binarization paradigm customized to high-dimensional softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space. The kernelized hash functions are learned to match the ground-truth similarity relations extracted from the attention map in a self-supervised way. Based on th",
    "link": "http://arxiv.org/abs/2209.09004",
    "context": "Title: EcoFormer: Energy-Saving Attention with Linear Complexity. (arXiv:2209.09004v3 [cs.CV] UPDATED)\nAbstract: Transformer is a transformative framework that models sequential data and has achieved remarkable performance on a wide range of tasks, but with high computational and energy cost. To improve its efficiency, a popular choice is to compress the models via binarization which constrains the floating-point values into binary ones to save resource consumption owing to cheap bitwise operations significantly. However, existing binarization methods only aim at minimizing the information loss for the input distribution statistically, while ignoring the pairwise similarity modeling at the core of the attention. To this end, we propose a new binarization paradigm customized to high-dimensional softmax attention via kernelized hashing, called EcoFormer, to map the original queries and keys into low-dimensional binary codes in Hamming space. The kernelized hash functions are learned to match the ground-truth similarity relations extracted from the attention map in a self-supervised way. Based on th",
    "path": "papers/22/09/2209.09004.json",
    "total_tokens": 1058,
    "translated_title": "EcoFormer：具有线性复杂度的节能注意力机制",
    "translated_abstract": "Transformer是一种用于建模序列数据的革命性框架，在广泛的任务中已经取得了显著的性能，但其高计算量和能源成本限制了其实际应用。为了提高Transformer的效率，压缩模型是一个受欢迎的选择，其中最常用的方法是通过二值化来将浮点值限制为二进制值，以便于进行位运算从而节省计算和能源开销。然而，现有的二值化方法只注重最大化统计上的输入分布信息而忽略了注意力机制中核心的相似度建模问题。为此，我们提出了一个新的二值化方法，通过核哈希技术对高维softmax注意力机制进行定制化处理，将原始查询和键嵌入到哈明空间的低维二进制编码中。核哈希函数是以自监督方式从注意力图中提取基本关系的相似度所学习的。在这些二进制编码的基础上，我们开发了一种具有线性时间复杂度的高效注意力模块，从而显著降低了计算和能量开销，相比于基本模型实现了较高的性能和可比性。我们进行了大量实验来验证其在图像分类、语言模型预训练和机器翻译等多个任务上的性能。",
    "tldr": "EcoFormer是一种通过核哈希技术进行高维softmax注意力二值化的新方法，实现了具有线性时间复杂度的高效注意力模块，从而显著降低了计算和能量开销，同时在多个任务上取得了高性能。"
}