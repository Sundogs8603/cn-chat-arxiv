{
    "title": "TRBoost: A Generic Gradient Boosting Machine based on Trust-region Method. (arXiv:2209.13791v3 [cs.LG] UPDATED)",
    "abstract": "Gradient Boosting Machines (GBMs) have demonstrated remarkable success in solving diverse problems by utilizing Taylor expansions in functional space. However, achieving a balance between performance and generality has posed a challenge for GBMs. In particular, gradient descent-based GBMs employ the first-order Taylor expansion to ensure applicability to all loss functions, while Newton's method-based GBMs use positive Hessian information to achieve superior performance at the expense of generality. To address this issue, this study proposes a new generic Gradient Boosting Machine called Trust-region Boosting (TRBoost). In each iteration, TRBoost uses a constrained quadratic model to approximate the objective and applies the Trust-region algorithm to solve it and obtain a new learner. Unlike Newton's method-based GBMs, TRBoost does not require the Hessian to be positive definite, thereby allowing it to be applied to arbitrary loss functions while still maintaining competitive performan",
    "link": "http://arxiv.org/abs/2209.13791",
    "context": "Title: TRBoost: A Generic Gradient Boosting Machine based on Trust-region Method. (arXiv:2209.13791v3 [cs.LG] UPDATED)\nAbstract: Gradient Boosting Machines (GBMs) have demonstrated remarkable success in solving diverse problems by utilizing Taylor expansions in functional space. However, achieving a balance between performance and generality has posed a challenge for GBMs. In particular, gradient descent-based GBMs employ the first-order Taylor expansion to ensure applicability to all loss functions, while Newton's method-based GBMs use positive Hessian information to achieve superior performance at the expense of generality. To address this issue, this study proposes a new generic Gradient Boosting Machine called Trust-region Boosting (TRBoost). In each iteration, TRBoost uses a constrained quadratic model to approximate the objective and applies the Trust-region algorithm to solve it and obtain a new learner. Unlike Newton's method-based GBMs, TRBoost does not require the Hessian to be positive definite, thereby allowing it to be applied to arbitrary loss functions while still maintaining competitive performan",
    "path": "papers/22/09/2209.13791.json",
    "total_tokens": 896,
    "translated_title": "TRBoost: 基于信赖域方法的通用梯度提升机",
    "translated_abstract": "梯度提升机 (GBMs) 利用函数空间的泰勒展开显著成功地解决了各种问题。然而，性能和通用性之间的平衡对 GBMs 提出了挑战。尤其是，基于梯度下降的 GBMs 使用一阶泰勒展开以确保适用于所有损失函数，而基于牛顿方法的 GBMs 利用正定的黑塞矩阵获得卓越的性能，但以牺牲通用性为代价。为解决这个问题，本研究提出了一种新型的通用梯度提升机，称为 TRBoost。在每次迭代中，TRBoost 使用一个约束二次模型来近似目标并应用信赖域算法来解决它并获得一个新的学习器。与基于牛顿方法的 GBMs 不同，TRBoost 不要求黑塞矩阵是正定的，因此允许它用于任意损失函数，同时仍保持竞争性能。",
    "tldr": "TRBoost 是一种新型通用梯度提升机，使用约束二次模型来近似目标并应用信赖域算法来获得新的学习器，具有适用于任意损失函数的通用性和竞争性能。",
    "en_tdlr": "TRBoost is a novel generic Gradient Boosting Machine that uses a constrained quadratic model to approximate the objective and applies Trust-region algorithm to obtain new learners, possessing both generality and competitive performance for any loss functions."
}