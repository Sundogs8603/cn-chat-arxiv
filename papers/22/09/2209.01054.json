{
    "title": "Taming Multi-Agent Reinforcement Learning with Estimator Variance Reduction. (arXiv:2209.01054v2 [cs.MA] UPDATED)",
    "abstract": "Centralised training with decentralised execution (CT-DE) serves as the foundation of many leading multi-agent reinforcement learning (MARL) algorithms. Despite its popularity, it suffers from a critical drawback due to its reliance on learning from a single sample of the joint-action at a given state. As agents explore and update their policies during training, these single samples may poorly represent the actual joint-policy of the system of agents leading to high variance gradient estimates that hinder learning. To address this problem, we propose an enhancement tool that accommodates any actor-critic MARL method. Our framework, Performance Enhancing Reinforcement Learning Apparatus (PERLA), introduces a sampling technique of the agents' joint-policy into the critics while the agents train. This leads to TD updates that closely approximate the true expected value under the current joint-policy rather than estimates from a single sample of the joint-action at a given state. This prod",
    "link": "http://arxiv.org/abs/2209.01054",
    "context": "Title: Taming Multi-Agent Reinforcement Learning with Estimator Variance Reduction. (arXiv:2209.01054v2 [cs.MA] UPDATED)\nAbstract: Centralised training with decentralised execution (CT-DE) serves as the foundation of many leading multi-agent reinforcement learning (MARL) algorithms. Despite its popularity, it suffers from a critical drawback due to its reliance on learning from a single sample of the joint-action at a given state. As agents explore and update their policies during training, these single samples may poorly represent the actual joint-policy of the system of agents leading to high variance gradient estimates that hinder learning. To address this problem, we propose an enhancement tool that accommodates any actor-critic MARL method. Our framework, Performance Enhancing Reinforcement Learning Apparatus (PERLA), introduces a sampling technique of the agents' joint-policy into the critics while the agents train. This leads to TD updates that closely approximate the true expected value under the current joint-policy rather than estimates from a single sample of the joint-action at a given state. This prod",
    "path": "papers/22/09/2209.01054.json",
    "total_tokens": 1020,
    "translated_title": "使用评估器方差降低控制多智能体强化学习",
    "translated_abstract": "集中式训练和分散式执行（CT-DE）是许多领先的多智能体强化学习算法的基础。尽管它很受欢迎，但由于其依赖于在给定状态下对联合动作的单个样本进行学习，因此存在一个关键缺陷。随着智能体在训练过程中探索和更新策略，这些单个样本可能较差地代表系统代理的实际联合策略，导致梯度估计方差高，阻碍学习。为了解决这个问题，我们提出了一个增强工具，该工具适用于任何演员-评论员多智能体强化学习方法。我们的框架Performance Enhancing Reinforcement Learning Apparatus（PERLA）在代理训练时引入了一种代理的联合策略采样技术到评论家中。这将导致TD更新更接近当前联合策略的真正期望值，而不是在给定状态下对联合动作的单个样本的估计值。这产生了方差减少的梯度估计，从而为CT-DE MARL算法提供更稳定和更快的学习。",
    "tldr": "本文提出了一种名为“PERLA”的增强工具，它通过将智能体的联合策略采样技术引入评论家中，减少了联合动作单一样本造成的梯度估计方差，使得CT-DE MARL算法的学习更加稳定且更快速。",
    "en_tdlr": "This paper proposes an enhancement tool called \"PERLA\" that reduces the variance of gradient estimates caused by single-sample joint-action learning in CT-DE MARL algorithms, leading to more stable and faster learning by introducing a sampling technique of agents' joint-policy into the critics."
}