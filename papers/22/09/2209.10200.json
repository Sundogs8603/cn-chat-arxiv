{
    "title": "Performance Optimization for Variable Bitwidth Federated Learning in Wireless Networks. (arXiv:2209.10200v2 [cs.LG] UPDATED)",
    "abstract": "This paper considers improving wireless communication and computation efficiency in federated learning (FL) via model quantization. In the proposed bitwidth FL scheme, edge devices train and transmit quantized versions of their local FL model parameters to a coordinating server, which aggregates them into a quantized global model and synchronizes the devices. The goal is to jointly determine the bitwidths employed for local FL model quantization and the set of devices participating in FL training at each iteration. We pose this as an optimization problem that aims to minimize the training loss of quantized FL under a per-iteration device sampling budget and delay requirement. However, the formulated problem is difficult to solve without (i) a concrete understanding of how quantization impacts global ML performance and (ii) the ability of the server to construct estimates of this process efficiently. To address the first challenge, we analytically characterize how limited wireless resou",
    "link": "http://arxiv.org/abs/2209.10200",
    "context": "Title: Performance Optimization for Variable Bitwidth Federated Learning in Wireless Networks. (arXiv:2209.10200v2 [cs.LG] UPDATED)\nAbstract: This paper considers improving wireless communication and computation efficiency in federated learning (FL) via model quantization. In the proposed bitwidth FL scheme, edge devices train and transmit quantized versions of their local FL model parameters to a coordinating server, which aggregates them into a quantized global model and synchronizes the devices. The goal is to jointly determine the bitwidths employed for local FL model quantization and the set of devices participating in FL training at each iteration. We pose this as an optimization problem that aims to minimize the training loss of quantized FL under a per-iteration device sampling budget and delay requirement. However, the formulated problem is difficult to solve without (i) a concrete understanding of how quantization impacts global ML performance and (ii) the ability of the server to construct estimates of this process efficiently. To address the first challenge, we analytically characterize how limited wireless resou",
    "path": "papers/22/09/2209.10200.json",
    "total_tokens": 1176,
    "translated_title": "无线网络中基于可变位宽的联邦学习性能优化",
    "translated_abstract": "本文研究了通过模型量化来改善联邦学习（FL）中的无线通信和计算效率。在所提出的位宽FL方案中，边缘设备训练和传输其本地FL模型参数的量化版本到一个协调服务器，将它们聚合成一个量化的全局模型并同步设备。目标是共同确定用于本地FL模型量化的位宽和每次迭代参与FL训练的设备集合。我们将这视为一个优化问题，旨在在每次迭代的设备抽样预算和延迟要求下最小化量化FL的训练损失。然而，所制定的问题难以解决，没有(i)对量化如何影响全局ML性能的具体理解以及(ii)服务器构建这个过程估计的能力。为了解决第一个挑战，我们分析地表征了有限的无线资源如何影响量化FL性能，并推导出了最优的位宽分配策略。为了应对第二个挑战，我们提出了一种新颖的多尺度量化FL聚合算法，使服务器能够轻松地从分布式量化FL模型中重构全局量化FL模型。广泛的仿真验证了我们的方法的有效性，在培训损失和收敛速度方面比现有技术方案提高了高达35％。",
    "tldr": "本文提出了一种在联邦学习中应用模型量化的方案，通过联邦学习的可变位宽优化来提高无线通信和计算效率，在无线资源受限的情况下，采用多尺度量化联邦学习聚合算法，能够有效改善联邦学习的性能，具备更高的收敛速度和更少的训练损失。",
    "en_tdlr": "This paper proposes a novel scheme for federated learning that applies model quantization and optimizes variable bitwidth to improve wireless communication and computation efficiency. A multi-scale quantized FL aggregation algorithm is introduced to construct global quantized FL model from distributed ones. The experimental results show the proposed approaches outperform the state-of-the-art alternatives by up to 35% in terms of training loss and convergence speed."
}