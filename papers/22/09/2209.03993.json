{
    "title": "Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline RL. (arXiv:2209.03993v4 [cs.LG] UPDATED)",
    "abstract": "Recent works have shown that tackling offline reinforcement learning (RL) with a conditional policy produces promising results. The Decision Transformer (DT) combines the conditional policy approach and a transformer architecture, showing competitive performance against several benchmarks. However, DT lacks stitching ability -- one of the critical abilities for offline RL to learn the optimal policy from sub-optimal trajectories. This issue becomes particularly significant when the offline dataset only contains sub-optimal trajectories. On the other hand, the conventional RL approaches based on Dynamic Programming (such as Q-learning) do not have the same limitation; however, they suffer from unstable learning behaviours, especially when they rely on function approximation in an off-policy learning setting. In this paper, we propose the Q-learning Decision Transformer (QDT) to address the shortcomings of DT by leveraging the benefits of Dynamic Programming (Q-learning). It utilises the",
    "link": "http://arxiv.org/abs/2209.03993",
    "context": "Title: Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline RL. (arXiv:2209.03993v4 [cs.LG] UPDATED)\nAbstract: Recent works have shown that tackling offline reinforcement learning (RL) with a conditional policy produces promising results. The Decision Transformer (DT) combines the conditional policy approach and a transformer architecture, showing competitive performance against several benchmarks. However, DT lacks stitching ability -- one of the critical abilities for offline RL to learn the optimal policy from sub-optimal trajectories. This issue becomes particularly significant when the offline dataset only contains sub-optimal trajectories. On the other hand, the conventional RL approaches based on Dynamic Programming (such as Q-learning) do not have the same limitation; however, they suffer from unstable learning behaviours, especially when they rely on function approximation in an off-policy learning setting. In this paper, we propose the Q-learning Decision Transformer (QDT) to address the shortcomings of DT by leveraging the benefits of Dynamic Programming (Q-learning). It utilises the",
    "path": "papers/22/09/2209.03993.json",
    "total_tokens": 1026,
    "translated_title": "Q-学习决策Transformer：利用动态规划进行条件序列建模的离线RL",
    "translated_abstract": "最近的研究表明，采用条件策略来解决离线强化学习（RL）问题具有不错的结果。决策Transformer（DT）结合了条件策略方法和Transformer架构，展示了与多个基准测试相竞争的性能。然而，DT缺少缝合能力——离线RL学习最优政策依赖于来自次优轨迹的数据，这一点变得尤为重要。另一方面，基于动态规划（如Q-learning）的传统RL方法没有相同的限制；然而，在离线学习设置中，尤其是当他们依赖函数逼近时，它们容易受到不稳定的学习行为的影响。本文提出了Q-learning决策Transformer（QDT）来解决DT的缺点，它利用动态规划（Q-learning）的优点创造了一个缝合函数，从而使学习最优策略成为可能。同时，QDT利用DT的transformer架构进行条件策略建模。我们对基准数据集的实验表明，QDT胜过DT和其他最先进的离线RL方法。",
    "tldr": "本文提出了Q-learning决策Transformer（QDT），使用动态规划的优点创造缝合函数以从次优数据中学习最优政策，并利用DT的transformer架构进行条件策略建模。实验证明QDT比DT和其他最先进的离线RL方法表现更优。",
    "en_tdlr": "This paper proposes a Q-learning Decision Transformer (QDT) that leverages the benefits of dynamic programming to create a stitching function for learning an optimal policy from sub-optimal data, while also utilizing the transformer architecture for conditional policy modelling. Experimental results demonstrate that QDT outperforms other state-of-the-art offline RL methods."
}