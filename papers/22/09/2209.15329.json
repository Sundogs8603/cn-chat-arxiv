{
    "title": "SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data. (arXiv:2209.15329v2 [cs.CL] UPDATED)",
    "abstract": "How to boost speech pre-training with textual data is an unsolved problem due to the fact that speech and text are very different modalities with distinct characteristics. In this paper, we propose a cross-modal Speech and Language Model (SpeechLM) to explicitly align speech and text pre-training with a pre-defined unified discrete representation. Specifically, we introduce two alternative discrete tokenizers to bridge the speech and text modalities, including phoneme-unit and hidden-unit tokenizers, which can be trained using a small amount of paired speech-text data. Based on the trained tokenizers, we convert the unlabeled speech and text data into tokens of phoneme units or hidden units. The pre-training objective is designed to unify the speech and the text into the same discrete semantic space with a unified Transformer network. Leveraging only 10K text sentences, our SpeechLM gets a 16\\% relative WER reduction over the best base model performance (from 6.8 to 5.7) on the public ",
    "link": "http://arxiv.org/abs/2209.15329",
    "context": "Title: SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data. (arXiv:2209.15329v2 [cs.CL] UPDATED)\nAbstract: How to boost speech pre-training with textual data is an unsolved problem due to the fact that speech and text are very different modalities with distinct characteristics. In this paper, we propose a cross-modal Speech and Language Model (SpeechLM) to explicitly align speech and text pre-training with a pre-defined unified discrete representation. Specifically, we introduce two alternative discrete tokenizers to bridge the speech and text modalities, including phoneme-unit and hidden-unit tokenizers, which can be trained using a small amount of paired speech-text data. Based on the trained tokenizers, we convert the unlabeled speech and text data into tokens of phoneme units or hidden units. The pre-training objective is designed to unify the speech and the text into the same discrete semantic space with a unified Transformer network. Leveraging only 10K text sentences, our SpeechLM gets a 16\\% relative WER reduction over the best base model performance (from 6.8 to 5.7) on the public ",
    "path": "papers/22/09/2209.15329.json",
    "total_tokens": 982,
    "tldr": "SpeechLM提出了一个跨模态的语音和语言模型，使用两种离散标记器将语音和文本预训练对齐，用统一的Transformer网络将它们统一到相同的离散语义空间中。只使用很少量的配对数据，SpeechLM在公共数据集上相对最佳基线模型性能减少了16%的字错率。"
}