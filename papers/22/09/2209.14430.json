{
    "title": "Minimax Optimal Kernel Operator Learning via Multilevel Training. (arXiv:2209.14430v3 [cs.LG] UPDATED)",
    "abstract": "Learning mappings between infinite-dimensional function spaces has achieved empirical success in many disciplines of machine learning, including generative modeling, functional data analysis, causal inference, and multi-agent reinforcement learning. In this paper, we study the statistical limit of learning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces. We establish the information-theoretic lower bound in terms of the Sobolev Hilbert-Schmidt norm and show that a regularization that learns the spectral components below the bias contour and ignores the ones that are above the variance contour can achieve the optimal learning rate. At the same time, the spectral components between the bias and variance contours give us flexibility in designing computationally feasible machine learning algorithms. Based on this observation, we develop a multilevel kernel operator learning algorithm that is optimal when learning linear operators betwee",
    "link": "http://arxiv.org/abs/2209.14430",
    "context": "Title: Minimax Optimal Kernel Operator Learning via Multilevel Training. (arXiv:2209.14430v3 [cs.LG] UPDATED)\nAbstract: Learning mappings between infinite-dimensional function spaces has achieved empirical success in many disciplines of machine learning, including generative modeling, functional data analysis, causal inference, and multi-agent reinforcement learning. In this paper, we study the statistical limit of learning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces. We establish the information-theoretic lower bound in terms of the Sobolev Hilbert-Schmidt norm and show that a regularization that learns the spectral components below the bias contour and ignores the ones that are above the variance contour can achieve the optimal learning rate. At the same time, the spectral components between the bias and variance contours give us flexibility in designing computationally feasible machine learning algorithms. Based on this observation, we develop a multilevel kernel operator learning algorithm that is optimal when learning linear operators betwee",
    "path": "papers/22/09/2209.14430.json",
    "total_tokens": 932,
    "translated_title": "最小化损失的多层训练方法下的最优核算子学习",
    "translated_abstract": "在无穷维函数空间中学习映射已经在机器学习的许多领域中取得了经验上的成功，包括生成模型、函数数据分析、因果推断和多智能体强化学习。本文研究了学习两个无穷维Sobolev再生核希尔伯特空间之间的Hilbert-Schmidt算子的统计极限。我们建立了使用Sobolev Hilbert-Schmidt范数的信息理论下界，并展示了一个规则化方法，通过学习偏差轮廓以下的谱分量并忽略方差轮廓以上的分量，可以达到最优的学习速率。同时，偏差和方差轮廓之间的谱分量给我们在设计计算上可行的机器学习算法时提供了灵活性。基于这一观察，我们开发了一个多层级的核算子学习算法，当学习线性算子时它是最优的。",
    "tldr": "本文研究了学习两个无穷维Sobolev再生核希尔伯特空间之间的Hilbert-Schmidt算子的统计极限，在多层级训练方法下，通过学习偏差以下的谱分量和忽略方差以上的分量，可以达到最优的学习速率。",
    "en_tdlr": "This paper studies the statistical limit of learning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces. It shows that by learning the spectral components below the bias contour and ignoring the ones above the variance contour, a regularization can achieve the optimal learning rate. The proposed multilevel kernel operator learning algorithm is optimal for learning linear operators."
}