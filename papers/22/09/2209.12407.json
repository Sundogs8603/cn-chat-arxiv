{
    "title": "Entailment Semantics Can Be Extracted from an Ideal Language Model. (arXiv:2209.12407v3 [cs.CL] UPDATED)",
    "abstract": "Language models are often trained on text alone, without additional grounding. There is debate as to how much of natural language semantics can be inferred from such a procedure. We prove that entailment judgments between sentences can be extracted from an ideal language model that has perfectly learned its target distribution, assuming the training sentences are generated by Gricean agents, i.e., agents who follow fundamental principles of communication from the linguistic theory of pragmatics. We also show entailment judgments can be decoded from the predictions of a language model trained on such Gricean data. Our results reveal a pathway for understanding the semantic information encoded in unlabeled linguistic data and a potential framework for extracting semantics from language models.",
    "link": "http://arxiv.org/abs/2209.12407",
    "context": "Title: Entailment Semantics Can Be Extracted from an Ideal Language Model. (arXiv:2209.12407v3 [cs.CL] UPDATED)\nAbstract: Language models are often trained on text alone, without additional grounding. There is debate as to how much of natural language semantics can be inferred from such a procedure. We prove that entailment judgments between sentences can be extracted from an ideal language model that has perfectly learned its target distribution, assuming the training sentences are generated by Gricean agents, i.e., agents who follow fundamental principles of communication from the linguistic theory of pragmatics. We also show entailment judgments can be decoded from the predictions of a language model trained on such Gricean data. Our results reveal a pathway for understanding the semantic information encoded in unlabeled linguistic data and a potential framework for extracting semantics from language models.",
    "path": "papers/22/09/2209.12407.json",
    "total_tokens": 805,
    "translated_title": "从理想语言模型中可以提取蕴含语义",
    "translated_abstract": "语言模型通常仅通过文本训练，没有额外的基础。关于这种过程能够推断出多少自然语言语义存在争议。我们证明，假设训练句子由Gricean代理生成（即遵循语用学中的交际基本原则的代理），可以从完美学习了目标分布的理想语言模型中提取出句子之间的蕴含判断。我们还展示了可以从训练在这种Gricean数据上的语言模型的预测中解码出蕴含判断。我们的结果揭示了理解未标注语言数据中编码的语义信息的途径，以及从语言模型中提取语义的潜在框架。",
    "tldr": "该论文证明，假设训练句子由遵循交际基本原则的代理生成，那么可以从目标分布完美学习的理想语言模型中提取出蕴含判断。这个结果揭示了从未标注语言数据中理解语义和从语言模型中提取语义的潜在方法。"
}