{
    "title": "Towards Optimization and Model Selection for Domain Generalization: A Mixup-guided Solution. (arXiv:2209.00652v2 [cs.LG] UPDATED)",
    "abstract": "The distribution shifts between training and test data typically undermine the performance of models. In recent years, lots of work pays attention to domain generalization (DG) where distribution shifts exist, and target data are unseen. Despite the progress in algorithm design, two foundational factors have long been ignored: 1) the optimization for regularization-based objectives, and 2) the model selection for DG since no knowledge about the target domain can be utilized. In this paper, we propose Mixup guided optimization and selection techniques for DG. For optimization, we utilize an adapted Mixup to generate an out-of-distribution dataset that can guide the preference direction and optimize with Pareto optimization. For model selection, we generate a validation dataset with a closer distance to the target distribution, and thereby it can better represent the target data. We also present some theoretical insights behind our proposals. Comprehensive experiments demonstrate that ou",
    "link": "http://arxiv.org/abs/2209.00652",
    "context": "Title: Towards Optimization and Model Selection for Domain Generalization: A Mixup-guided Solution. (arXiv:2209.00652v2 [cs.LG] UPDATED)\nAbstract: The distribution shifts between training and test data typically undermine the performance of models. In recent years, lots of work pays attention to domain generalization (DG) where distribution shifts exist, and target data are unseen. Despite the progress in algorithm design, two foundational factors have long been ignored: 1) the optimization for regularization-based objectives, and 2) the model selection for DG since no knowledge about the target domain can be utilized. In this paper, we propose Mixup guided optimization and selection techniques for DG. For optimization, we utilize an adapted Mixup to generate an out-of-distribution dataset that can guide the preference direction and optimize with Pareto optimization. For model selection, we generate a validation dataset with a closer distance to the target distribution, and thereby it can better represent the target data. We also present some theoretical insights behind our proposals. Comprehensive experiments demonstrate that ou",
    "path": "papers/22/09/2209.00652.json",
    "total_tokens": 880,
    "translated_title": "近似优化和模型选择在领域泛化中的应用：一种基于Mixup引导的解决方案",
    "translated_abstract": "训练和测试数据之间的分布偏移通常会削弱模型的性能。近年来，许多工作关注于领域泛化 (DG)，其中存在分布偏移且目标数据是未见过的。尽管在算法设计方面取得了进展，但两个基本因素长期以来一直被忽视：1) 面向正则化目标的优化，以及2) 针对DG的模型选择，因为无法利用有关目标领域的知识。在本文中，我们提出了一种基于Mixup引导的优化与选择技术，用于DG。对于优化，我们利用调整后的Mixup生成一个分布与目标领域有指导作用的超出分布的数据集，并使用Pareto优化进行优化。对于模型选择，我们生成一个更接近目标分布的验证数据集，从而可以更好地代表目标数据。我们还提供了一些关于我们提议的理论见解。全面的实验表明我们的方法的有效性。",
    "tldr": "本文提出了一种基于Mixup引导的优化与选择技术，用于领域泛化，通过生成目标领域指导数据和更接近目标分布的验证数据集，能够提高模型性能。",
    "en_tdlr": "This paper proposes a Mixup guided optimization and selection technique for domain generalization. By generating target domain-guided data and a validation dataset closer to the target distribution, it improves model performance."
}