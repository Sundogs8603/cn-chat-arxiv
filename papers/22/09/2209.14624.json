{
    "title": "Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning. (arXiv:2209.14624v2 [cs.LG] UPDATED)",
    "abstract": "Pruning neural networks has become popular in the last decade when it was shown that a large number of weights can be safely removed from modern neural networks without compromising accuracy. Numerous pruning methods have been proposed since then, each claiming to be better than the previous. Many state-of-the-art (SOTA) techniques today rely on complex pruning methodologies utilizing importance scores, getting feedback through back-propagation or having heuristics-based pruning rules amongst others. In this work, we question whether this pattern of introducing complexity is really necessary to achieve better pruning results. We benchmark these SOTA techniques against a naive pruning baseline, namely, Global Magnitude Pruning (Global MP). Global MP ranks weights in order of their magnitudes and prunes the smallest ones. Hence, in its vanilla form, it is one of the simplest pruning techniques. Surprisingly, we find that vanilla Global MP outperforms all the other SOTA techniques and ach",
    "link": "http://arxiv.org/abs/2209.14624",
    "context": "Title: Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning. (arXiv:2209.14624v2 [cs.LG] UPDATED)\nAbstract: Pruning neural networks has become popular in the last decade when it was shown that a large number of weights can be safely removed from modern neural networks without compromising accuracy. Numerous pruning methods have been proposed since then, each claiming to be better than the previous. Many state-of-the-art (SOTA) techniques today rely on complex pruning methodologies utilizing importance scores, getting feedback through back-propagation or having heuristics-based pruning rules amongst others. In this work, we question whether this pattern of introducing complexity is really necessary to achieve better pruning results. We benchmark these SOTA techniques against a naive pruning baseline, namely, Global Magnitude Pruning (Global MP). Global MP ranks weights in order of their magnitudes and prunes the smallest ones. Hence, in its vanilla form, it is one of the simplest pruning techniques. Surprisingly, we find that vanilla Global MP outperforms all the other SOTA techniques and ach",
    "path": "papers/22/09/2209.14624.json",
    "total_tokens": 920,
    "translated_title": "神经网络修剪是否需要复杂性？一个关于全局幅度修剪的案例研究",
    "translated_abstract": "在过去的十年中，修剪神经网络变得越来越流行，因为人们发现在不降低准确性的情况下可以安全地删除现代神经网络中的大量权重。自那时以来，已经提出了许多修剪方法，每一种都声称比前一种更好。今天，许多最先进的技术都依赖于使用重要性分数、通过反向传播获取反馈或基于启发式修剪规则等复杂的修剪方法。在这项工作中，我们质疑这种引入复杂性是否真的有必要来实现更好的修剪结果。我们将这些最先进的技术与一个简单的基准线进行了比较，即全局幅度修剪(Global MP)。全局幅度修剪根据权重的大小对其进行排序并修剪最小的权重。因此，在其原始形式中，它是最简单的修剪技术之一。令人惊讶的是，我们发现原始的全局幅度修剪优于所有其他最先进的技术并取得了最好的结果。",
    "tldr": "本论文研究了神经网络修剪中是否需要复杂性，并通过与全局幅度修剪进行比较发现，原始的全局幅度修剪方法优于其他最先进的修剪技术。",
    "en_tdlr": "This paper investigates whether complexity is necessary for neural network pruning, and surprisingly finds that the original Global Magnitude Pruning technique outperforms other state-of-the-art methods."
}