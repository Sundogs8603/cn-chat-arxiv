{
    "title": "PlaStIL: Plastic and Stable Memory-Free Class-Incremental Learning. (arXiv:2209.06606v2 [cs.CV] UPDATED)",
    "abstract": "Plasticity and stability are needed in class-incremental learning in order to learn from new data while preserving past knowledge. Due to catastrophic forgetting, finding a compromise between these two properties is particularly challenging when no memory buffer is available. Mainstream methods need to store two deep models since they integrate new classes using fine-tuning with knowledge distillation from the previous incremental state. We propose a method which has similar number of parameters but distributes them differently in order to find a better balance between plasticity and stability. Following an approach already deployed by transfer-based incremental methods, we freeze the feature extractor after the initial state. Classes in the oldest incremental states are trained with this frozen extractor to ensure stability. Recent classes are predicted using partially fine-tuned models in order to introduce plasticity. Our proposed plasticity layer can be incorporated to any transfer",
    "link": "http://arxiv.org/abs/2209.06606",
    "context": "Title: PlaStIL: Plastic and Stable Memory-Free Class-Incremental Learning. (arXiv:2209.06606v2 [cs.CV] UPDATED)\nAbstract: Plasticity and stability are needed in class-incremental learning in order to learn from new data while preserving past knowledge. Due to catastrophic forgetting, finding a compromise between these two properties is particularly challenging when no memory buffer is available. Mainstream methods need to store two deep models since they integrate new classes using fine-tuning with knowledge distillation from the previous incremental state. We propose a method which has similar number of parameters but distributes them differently in order to find a better balance between plasticity and stability. Following an approach already deployed by transfer-based incremental methods, we freeze the feature extractor after the initial state. Classes in the oldest incremental states are trained with this frozen extractor to ensure stability. Recent classes are predicted using partially fine-tuned models in order to introduce plasticity. Our proposed plasticity layer can be incorporated to any transfer",
    "path": "papers/22/09/2209.06606.json",
    "total_tokens": 888,
    "translated_title": "PlaStIL：无需内存的稳定可塑性增量学习",
    "translated_abstract": "在增量学习中，需要同时具备可塑性和稳定性，以便从新数据中学习同时保留过去的知识。由于灾难性遗忘，当没有内存缓冲区可用时，在这两个属性之间找到平衡尤为挑战。主流方法需要存储两个深度模型，因为它们使用知识蒸馏从之前的增量状态进行微调并集成新类。我们提出了一种方法，与这些方法的参数数量相似，但以不同的方式分配参数，以便更好地在可塑性和稳定性之间找到平衡。在初始状态之后，我们冻结特征提取器，这是一种已经被转移学习增量方法采用的方法。最早的增量状态中的类使用冻结的提取器进行训练，以确保稳定性。最近的类别使用部分微调模型进行预测，以引入可塑性。我们提出的可塑性层可以添加到任何转移学习方法中。",
    "tldr": "PlaStIL是一种无需内存的增量学习方法，通过在最早的增量状态中使用冻结的特征提取器来保证稳定性，同时使用部分微调模型来引入可塑性，以找到可塑性和稳定性之间的平衡。"
}