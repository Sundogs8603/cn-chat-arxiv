{
    "title": "A simple approach for quantizing neural networks. (arXiv:2209.03487v2 [cs.LG] UPDATED)",
    "abstract": "In this short note, we propose a new method for quantizing the weights of a fully trained neural network. A simple deterministic pre-processing step allows us to quantize network layers via memoryless scalar quantization while preserving the network performance on given training data. On one hand, the computational complexity of this pre-processing slightly exceeds that of state-of-the-art algorithms in the literature. On the other hand, our approach does not require any hyper-parameter tuning and, in contrast to previous methods, allows a plain analysis. We provide rigorous theoretical guarantees in the case of quantizing single network layers and show that the relative error decays with the number of parameters in the network if the training data behaves well, e.g., if it is sampled from suitable random distributions. The developed method also readily allows the quantization of deep networks by consecutive application to single layers.",
    "link": "http://arxiv.org/abs/2209.03487",
    "context": "Title: A simple approach for quantizing neural networks. (arXiv:2209.03487v2 [cs.LG] UPDATED)\nAbstract: In this short note, we propose a new method for quantizing the weights of a fully trained neural network. A simple deterministic pre-processing step allows us to quantize network layers via memoryless scalar quantization while preserving the network performance on given training data. On one hand, the computational complexity of this pre-processing slightly exceeds that of state-of-the-art algorithms in the literature. On the other hand, our approach does not require any hyper-parameter tuning and, in contrast to previous methods, allows a plain analysis. We provide rigorous theoretical guarantees in the case of quantizing single network layers and show that the relative error decays with the number of parameters in the network if the training data behaves well, e.g., if it is sampled from suitable random distributions. The developed method also readily allows the quantization of deep networks by consecutive application to single layers.",
    "path": "papers/22/09/2209.03487.json",
    "total_tokens": 686,
    "translated_title": "一种简单的神经网络量化方法",
    "translated_abstract": "在这篇短文中，我们提出了一种新的方法来量化完全训练过的神经网络的权重。简单的确定性预处理步骤使我们能够通过无记忆标量量化来量化网络层，同时保持在给定训练数据上的网络性能。我们的方法不需要任何超参数调整，并且与以前的方法相比，可以进行简单的分析。",
    "tldr": "本论文提出了一种简单的确定性预处理步骤来量化神经网络层的权重，并在不需要超参数调整的情况下保持网络性能，且相对误差随网络参数数量的增加而降低。",
    "en_tdlr": "This paper proposes a simple deterministic pre-processing step to quantize the weights of fully trained neural networks via memoryless scalar quantization while preserving performance on training data, without needing hyper-parameter tuning, with relative error decreasing as the number of parameters in the network increases."
}