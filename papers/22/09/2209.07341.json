{
    "title": "Does CLIP Know My Face?. (arXiv:2209.07341v3 [cs.LG] UPDATED)",
    "abstract": "With the rise of deep learning in various applications, privacy concerns around the protection of training data has become a critical area of research. Whereas prior studies have focused on privacy risks in single-modal models, we introduce a novel method to assess privacy for multi-modal models, specifically vision-language models like CLIP. The proposed Identity Inference Attack (IDIA) reveals whether an individual was included in the training data by querying the model with images of the same person. Letting the model choose from a wide variety of possible text labels, the model reveals whether it recognizes the person and, therefore, was used for training. Our large-scale experiments on CLIP demonstrate that individuals used for training can be identified with very high accuracy. We confirm that the model has learned to associate names with depicted individuals, implying the existence of sensitive information that can be extracted by adversaries. Our results highlight the need for ",
    "link": "http://arxiv.org/abs/2209.07341",
    "context": "Title: Does CLIP Know My Face?. (arXiv:2209.07341v3 [cs.LG] UPDATED)\nAbstract: With the rise of deep learning in various applications, privacy concerns around the protection of training data has become a critical area of research. Whereas prior studies have focused on privacy risks in single-modal models, we introduce a novel method to assess privacy for multi-modal models, specifically vision-language models like CLIP. The proposed Identity Inference Attack (IDIA) reveals whether an individual was included in the training data by querying the model with images of the same person. Letting the model choose from a wide variety of possible text labels, the model reveals whether it recognizes the person and, therefore, was used for training. Our large-scale experiments on CLIP demonstrate that individuals used for training can be identified with very high accuracy. We confirm that the model has learned to associate names with depicted individuals, implying the existence of sensitive information that can be extracted by adversaries. Our results highlight the need for ",
    "path": "papers/22/09/2209.07341.json",
    "total_tokens": 901,
    "translated_title": "CLIP是否知道我的脸？",
    "translated_abstract": "随着深度学习在各个应用中的普及，保护训练数据的隐私问题已经成为一个关键的研究领域。以前的研究主要关注单模型的隐私风险，我们提出了一种新的方法来评估多模型的隐私，特别是像CLIP这样的视觉语言模型。所提出的身份推断攻击(IDIA)通过用同一人的图片向模型查询，从而揭示该个人是否被包含在训练数据中。让模型从各种可能的文本标签中选择，模型会透露是否识别该人物，从而表明其被用于训练。我们在CLIP上进行的大规模实验表明，使用于训练的个人可以被非常高的准确率识别出来。我们确认该模型已经学会将名称与描绘的个人相关联，这意味着敏感信息存在于其中，可以被对手提取。我们的结果凸显了需要在视觉语言模型中更好地解决隐私问题。",
    "tldr": "本文提出了一种新方法IDIA来评估视觉语言模型的隐私，大规模实验表明使用于训练的个人可以被非常高的准确率识别出来，表明需要更好地解决视觉语言模型中的隐私问题。"
}