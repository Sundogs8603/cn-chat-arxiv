{
    "title": "Proximal Point Imitation Learning. (arXiv:2209.10968v3 [cs.LG] UPDATED)",
    "abstract": "This work develops new algorithms with rigorous efficiency guarantees for infinite horizon imitation learning (IL) with linear function approximation without restrictive coherence assumptions. We begin with the minimax formulation of the problem and then outline how to leverage classical tools from optimization, in particular, the proximal-point method (PPM) and dual smoothing, for online and offline IL, respectively. Thanks to PPM, we avoid nested policy evaluation and cost updates for online IL appearing in the prior literature. In particular, we do away with the conventional alternating updates by the optimization of a single convex and smooth objective over both cost and Q-functions. When solved inexactly, we relate the optimization errors to the suboptimality of the recovered policy. As an added bonus, by re-interpreting PPM as dual smoothing with the expert policy as a center point, we also obtain an offline IL algorithm enjoying theoretical guarantees in terms of required expert",
    "link": "http://arxiv.org/abs/2209.10968",
    "context": "Title: Proximal Point Imitation Learning. (arXiv:2209.10968v3 [cs.LG] UPDATED)\nAbstract: This work develops new algorithms with rigorous efficiency guarantees for infinite horizon imitation learning (IL) with linear function approximation without restrictive coherence assumptions. We begin with the minimax formulation of the problem and then outline how to leverage classical tools from optimization, in particular, the proximal-point method (PPM) and dual smoothing, for online and offline IL, respectively. Thanks to PPM, we avoid nested policy evaluation and cost updates for online IL appearing in the prior literature. In particular, we do away with the conventional alternating updates by the optimization of a single convex and smooth objective over both cost and Q-functions. When solved inexactly, we relate the optimization errors to the suboptimality of the recovered policy. As an added bonus, by re-interpreting PPM as dual smoothing with the expert policy as a center point, we also obtain an offline IL algorithm enjoying theoretical guarantees in terms of required expert",
    "path": "papers/22/09/2209.10968.json",
    "total_tokens": 975,
    "translated_title": "近端点模仿学习",
    "translated_abstract": "本文提出了一种用于具有线性函数逼近但没有限制一致性假设的无限时域模仿学习（IL）的新算法，并具有严格的效率保证。我们从问题的极小化-极大化形式开始，并概述如何利用优化中的经典工具-近端点法（PPM）和对偶平滑来进行在线和离线IL。由于PPM，我们避免了先前文献中出现的在线IL中出现的嵌套策略评估和成本更新。特别地，我们通过优化一个单个的凸平滑目标来同时更新成本和Q函数，摆脱了常规的交替更新。当不精确地求解时，我们将优化误差与恢复策略的次优性相关联。作为额外的奖励，通过将PPM重新解释为以专家策略为中心点的对偶平滑，我们还获得了离线IL算法，享有关于所需专家所具有的理论保证。",
    "tldr": "本文提出了一种适用于无限时域模仿学习的新算法，采用了优化中的经典工具近端点法和对偶平滑，从而得到了比之前更好的效率保证。通过优化一个单个的凸平滑目标来同时更新成本和Q函数，避免了嵌套策略评估和成本更新，具有更好的应用前景。",
    "en_tdlr": "This paper proposes a new algorithm for infinite horizon imitation learning with linear function approximation and without restrictive coherence assumptions. By leveraging classical tools from optimization, in particular, the proximal-point method and dual smoothing, the algorithm achieves better efficiency guarantees compared to previous methods. It also avoids nested policy evaluation and cost updates for online IL and updates cost and Q-functions simultaneously with a single convex and smooth objective. Additionally, the paper presents an offline IL algorithm with theoretical guarantees in terms of required expert."
}