{
    "title": "Unraveling the Connections between Privacy and Certified Robustness in Federated Learning Against Poisoning Attacks. (arXiv:2209.04030v2 [cs.CR] UPDATED)",
    "abstract": "Federated learning (FL) provides an efficient paradigm to jointly train a global model leveraging data from distributed users. As local training data comes from different users who may not be trustworthy, several studies have shown that FL is vulnerable to poisoning attacks. Meanwhile, to protect the privacy of local users, FL is usually trained in a differentially private way (DPFL). Thus, in this paper, we ask: What are the underlying connections between differential privacy and certified robustness in FL against poisoning attacks? Can we leverage the innate privacy property of DPFL to provide certified robustness for FL? Can we further improve the privacy of FL to improve such robustness certification? We first investigate both user-level and instance-level privacy of FL and provide formal privacy analysis to achieve improved instance-level privacy. We then provide two robustness certification criteria: certified prediction and certified attack inefficacy for DPFL on both user and i",
    "link": "http://arxiv.org/abs/2209.04030",
    "context": "Title: Unraveling the Connections between Privacy and Certified Robustness in Federated Learning Against Poisoning Attacks. (arXiv:2209.04030v2 [cs.CR] UPDATED)\nAbstract: Federated learning (FL) provides an efficient paradigm to jointly train a global model leveraging data from distributed users. As local training data comes from different users who may not be trustworthy, several studies have shown that FL is vulnerable to poisoning attacks. Meanwhile, to protect the privacy of local users, FL is usually trained in a differentially private way (DPFL). Thus, in this paper, we ask: What are the underlying connections between differential privacy and certified robustness in FL against poisoning attacks? Can we leverage the innate privacy property of DPFL to provide certified robustness for FL? Can we further improve the privacy of FL to improve such robustness certification? We first investigate both user-level and instance-level privacy of FL and provide formal privacy analysis to achieve improved instance-level privacy. We then provide two robustness certification criteria: certified prediction and certified attack inefficacy for DPFL on both user and i",
    "path": "papers/22/09/2209.04030.json",
    "total_tokens": 896,
    "translated_title": "揭示隐私与认证稳定性之间在联邦学习中对抗毒化攻击中的联系",
    "translated_abstract": "联邦学习（FL）提供了一种有效的范式，可以利用分布式用户的数据共同训练全局模型。由于本地训练数据来自不可信任的不同用户，一些研究表明FL容易受到毒化攻击。与此同时，为了保护本地用户的隐私，FL通常采用差分隐私的方式进行训练（DPFL）。因此，在本文中，我们提出以下问题：差分隐私和联邦学习中的认证稳定性之间存在何种联系？我们能否利用DPFL的隐私属性为FL提供认证稳定性？我们如何进一步改进FL的隐私以提高这种稳定性认证？我们首先研究FL的用户级和实例级隐私，并提供正式的隐私分析以实现提高实例级隐私。然后，我们提供了两个稳定性认证指标：认证预测和认证攻击无效性，用于用户和实例级DPFL的认证。",
    "tldr": "该论文研究了联邦学习中隐私和认证稳定性之间的联系，并探讨了如何利用差分隐私提供认证稳定性以及如何改进隐私以提高稳定性认证。"
}