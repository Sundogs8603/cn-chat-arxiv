{
    "title": "SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation. (arXiv:2209.15323v2 [cs.CV] UPDATED)",
    "abstract": "Recent advances in image captioning have focused on scaling the data and model size, substantially increasing the cost of pre-training and finetuning. As an alternative to large models, we present SmallCap, which generates a caption conditioned on an input image and related captions retrieved from a datastore. Our model is lightweight and fast to train, as the only learned parameters are in newly introduced cross-attention layers between a pre-trained CLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without additional finetuning and can exploit large-scale data in a training-free fashion since the contents of the datastore can be readily replaced. Our experiments show that SmallCap, trained only on COCO, has competitive performance on this benchmark, and also transfers to other domains without retraining, solely through retrieval from target-domain data. Further improvement is achieved through the training-free exploitation of diverse human-labeled and web data, whi",
    "link": "http://arxiv.org/abs/2209.15323",
    "context": "Title: SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation. (arXiv:2209.15323v2 [cs.CV] UPDATED)\nAbstract: Recent advances in image captioning have focused on scaling the data and model size, substantially increasing the cost of pre-training and finetuning. As an alternative to large models, we present SmallCap, which generates a caption conditioned on an input image and related captions retrieved from a datastore. Our model is lightweight and fast to train, as the only learned parameters are in newly introduced cross-attention layers between a pre-trained CLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without additional finetuning and can exploit large-scale data in a training-free fashion since the contents of the datastore can be readily replaced. Our experiments show that SmallCap, trained only on COCO, has competitive performance on this benchmark, and also transfers to other domains without retraining, solely through retrieval from target-domain data. Further improvement is achieved through the training-free exploitation of diverse human-labeled and web data, whi",
    "path": "papers/22/09/2209.15323.json",
    "total_tokens": 973,
    "translated_title": "SmallCap：以检索增强为条件的轻量级图像字幕生成模型",
    "translated_abstract": "近年来，图像字幕生成技术的进展主要集中在扩展数据和模型规模，大大增加了预训练和微调的成本。为了避免大模型的缺点，我们提出了SmallCap模型，通过从数据存储库中检索到的相关字幕为条件生成文字描述。该模型轻量且训练速度快，仅需要在预训练的CLIP编码器和GPT-2解码器之间加入新的交叉注意力层进行学习。SmallCap模型无需额外微调即可完成跨领域迁移，还能够轻松利用数据存储库中的大规模数据。实验表明，在仅使用COCO数据集进行训练的情况下，SmallCap模型在此基准测试中表现竞争力，并且仅通过从目标领域数据中检索就可以进行跨域迁移。通过充分利用人工标注和网络数据，SmallCap模型的性能还可以进一步提高。",
    "tldr": "SmallCap是一种轻量级图像字幕生成模型，通过从数据存储库中检索到相关字幕为条件生成文字描述。模型训练速度快，无需额外微调即可跨领域迁移，还能够充分利用数据存储库中的大规模数据。在实验中表现出竞争力，并可以通过利用人工标注和网络数据来进一步提高性能。",
    "en_tdlr": "SmallCap is a lightweight image captioning model that generates text descriptions conditioned on related captions retrieved from a data store, which is fast to train and can transfer to new domains without additional finetuning. It can also exploit large-scale data in a training-free fashion. Experimental results show competitive performance on benchmark datasets and further improvement can be achieved through the use of diverse human-labeled and web data."
}