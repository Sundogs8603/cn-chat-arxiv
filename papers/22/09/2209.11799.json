{
    "title": "Augmenting Interpretable Models with LLMs during Training. (arXiv:2209.11799v3 [cs.AI] UPDATED)",
    "abstract": "Recent large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains (e.g. medicine) and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Augmented Interpretable Models (Aug-imodels), a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1,000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: (i) Aug-GAM, which augments a generalized additive model with decoupled embeddings from an LLM and (ii) Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented co",
    "link": "http://arxiv.org/abs/2209.11799",
    "context": "Title: Augmenting Interpretable Models with LLMs during Training. (arXiv:2209.11799v3 [cs.AI] UPDATED)\nAbstract: Recent large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains (e.g. medicine) and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Augmented Interpretable Models (Aug-imodels), a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1,000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: (i) Aug-GAM, which augments a generalized additive model with decoupled embeddings from an LLM and (ii) Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented co",
    "path": "papers/22/09/2209.11799.json",
    "total_tokens": 985,
    "translated_title": "使用LLM来增强可解释模型的训练",
    "translated_abstract": "近期，大型语言模型（LLMs）在越来越多的任务中表现出了出色的表现。然而，它们进入高风险领域（例如医学）和计算资源有限的环境中，对解释性和效率的需求日益增加。我们提出了增强可解释模型（Aug-imodels）框架，利用LLMs所学习的知识建立极其高效且可解释的模型。Aug-imodels在拟合过程中使用LLMs，但在推理过程中不使用，从而实现了完全的透明性，并且与LLMs相比，推理速度和内存性能有了大于1000倍的提高。我们探讨了两种Aug-imodels在自然语言处理中的具体实例：（i）Aug-GAM，它使用来自LLM的解耦嵌入来增强广义加性模型；（ii）Aug-Tree，它通过LLM特征扩展来增强决策树。在各种文本分类数据集中，这两种方法都优于其未增强的对照模型。",
    "tldr": "本文提出了 Aug-imodels 框架，利用 LLMs 的知识在拟合过程中构建高效且可解释的模型，在推理过程中不使用 LLMs，具备完全的透明性。研究探讨了两种不同方式的实现，并在多种文本分类数据集中表现出优异的效果。"
}