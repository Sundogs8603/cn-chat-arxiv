{
    "title": "News Summarization and Evaluation in the Era of GPT-3. (arXiv:2209.12356v2 [cs.CL] UPDATED)",
    "abstract": "The recent success of prompting large language models like GPT-3 has led to a paradigm shift in NLP research. In this paper, we study its impact on text summarization, focusing on the classic benchmark domain of news summarization. First, we investigate how GPT-3 compares against fine-tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT-3 summaries, prompted using only a task description, but these also do not suffer from common dataset-specific issues such as poor factuality. Next, we study what this means for evaluation, particularly the role of gold standard test sets. Our experiments show that both reference-based and reference-free automatic metrics cannot reliably evaluate GPT-3 summaries. Finally, we evaluate models on a setting beyond generic summarization, specifically keyword-based summarization, and show how dominant fine-tuning approaches compare to prompting.  To support further research, we release: (a) a corpus o",
    "link": "http://arxiv.org/abs/2209.12356",
    "context": "Title: News Summarization and Evaluation in the Era of GPT-3. (arXiv:2209.12356v2 [cs.CL] UPDATED)\nAbstract: The recent success of prompting large language models like GPT-3 has led to a paradigm shift in NLP research. In this paper, we study its impact on text summarization, focusing on the classic benchmark domain of news summarization. First, we investigate how GPT-3 compares against fine-tuned models trained on large summarization datasets. We show that not only do humans overwhelmingly prefer GPT-3 summaries, prompted using only a task description, but these also do not suffer from common dataset-specific issues such as poor factuality. Next, we study what this means for evaluation, particularly the role of gold standard test sets. Our experiments show that both reference-based and reference-free automatic metrics cannot reliably evaluate GPT-3 summaries. Finally, we evaluate models on a setting beyond generic summarization, specifically keyword-based summarization, and show how dominant fine-tuning approaches compare to prompting.  To support further research, we release: (a) a corpus o",
    "path": "papers/22/09/2209.12356.json",
    "total_tokens": 894,
    "translated_title": "GPT-3时代的新闻摘要与评估",
    "translated_abstract": "最近大型语言模型，如GPT-3在NLP研究中得到了极大的成功，我们着眼于经典基准领域新闻摘要，研究了其对文本摘要的影响。我们展示了GPT-3摘要与基于大型摘要数据集训练的微调模型相比较，不仅人类更喜欢使用仅一个任务描述来促发的GPT-3摘要，而且这些摘要也不会像一些常规数据集具有的信息事实不准确等问题。我们进一步研究了这对于评估的意义，尤其是金标准测试集的作用。我们的实验表明，无论是基于参考文本还是无参考文本的自动评价指标都不能可靠地评估GPT-3摘要。最后，我们评估了模型在通用摘要之外的一种场景，即基于关键词的摘要，并展示了微调方法与促发方法之间的比较。为了支持进一步的研究，我们发布了一个语料库。",
    "tldr": "GPT-3在新闻摘要中具有出色表现，但自动评价指标无法可靠评估其清晰度和准确性，从而提出了一个新的评估挑战。",
    "en_tdlr": "GPT-3 performs well in news summarization, but automatic evaluation metrics cannot reliably assess its clarity and accuracy, posing a new evaluation challenge."
}