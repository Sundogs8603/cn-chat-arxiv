{
    "title": "COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization. (arXiv:2209.14569v2 [cs.CL] UPDATED)",
    "abstract": "Traditional training paradigms for extractive and abstractive summarization systems always only use token-level or sentence-level training objectives. However, the output summary is always evaluated from summary-level which leads to the inconsistency in training and evaluation. In this paper, we propose a Contrastive Learning based re-ranking framework for one-stage summarization called COLO. By modeling a contrastive objective, we show that the summarization model is able to directly generate summaries according to the summary-level score without additional modules and parameters. Extensive experiments demonstrate that COLO boosts the extractive and abstractive results of one-stage systems on CNN/DailyMail benchmark to 44.58 and 46.33 ROUGE-1 score while preserving the parameter efficiency and inference efficiency. Compared with state-of-the-art multi-stage systems, we save more than 100 GPU training hours and obtaining 3~8 speed-up ratio during inference while maintaining comparable ",
    "link": "http://arxiv.org/abs/2209.14569",
    "context": "Title: COLO: A Contrastive Learning based Re-ranking Framework for One-Stage Summarization. (arXiv:2209.14569v2 [cs.CL] UPDATED)\nAbstract: Traditional training paradigms for extractive and abstractive summarization systems always only use token-level or sentence-level training objectives. However, the output summary is always evaluated from summary-level which leads to the inconsistency in training and evaluation. In this paper, we propose a Contrastive Learning based re-ranking framework for one-stage summarization called COLO. By modeling a contrastive objective, we show that the summarization model is able to directly generate summaries according to the summary-level score without additional modules and parameters. Extensive experiments demonstrate that COLO boosts the extractive and abstractive results of one-stage systems on CNN/DailyMail benchmark to 44.58 and 46.33 ROUGE-1 score while preserving the parameter efficiency and inference efficiency. Compared with state-of-the-art multi-stage systems, we save more than 100 GPU training hours and obtaining 3~8 speed-up ratio during inference while maintaining comparable ",
    "path": "papers/22/09/2209.14569.json",
    "total_tokens": 950,
    "translated_title": "COLO：基于对比学习的一阶摘要重排序框架",
    "translated_abstract": "传统的抽取式和生成式摘要系统的训练范式通常只使用令牌级别或句子级别的训练目标，但是，总结输出总是从摘要级别进行评估，这导致训练和评估不一致。在本文中，我们提出了一种基于对比学习的一阶摘要重排序框架COLO。通过建模对比目标，我们展示了摘要模型能够根据摘要级别得分直接生成摘要，而无需额外的模块和参数。广泛的实验表明，COLO提高了CNN/DailyMail基准上一阶系统的抽取和生成结果，同时保持了参数效率和推理效率，ROUGE-1分数分别达到44.58和46.33。与最先进的多阶段系统相比，我们在训练时节省了100多个GPU小时，在推理期间获得了3~8倍的加速比，同时保持了可比的性能。",
    "tldr": "本文提出了一种基于对比学习的一阶摘要重排序框架COLO，广泛的实验表明它能够提高CNN/DailyMail基准上一阶系统的抽取和生成结果，同时保持参数和推理效率，并且相比于多阶段系统，具有更高的训练和推理速度、以及可比的性能。",
    "en_tdlr": "This paper proposes a one-stage summarization re-ranking framework COLO based on contrastive learning, which boosts the extractive and abstractive results of one-stage systems on CNN/DailyMail benchmark while maintaining parameter and inference efficiency. Experimental results show that it outperforms state-of-the-art multi-stage systems in terms of training and inference speed and achieves comparable performance."
}