{
    "title": "Hebbian Deep Learning Without Feedback. (arXiv:2209.11883v2 [cs.NE] UPDATED)",
    "abstract": "Recent approximations to backpropagation (BP) have mitigated many of BP's computational inefficiencies and incompatibilities with biology, but important limitations still remain. Moreover, the approximations significantly decrease accuracy in benchmarks, suggesting that an entirely different approach may be more fruitful. Here, grounded on recent theory for Hebbian learning in soft winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm that trains deep neural networks, without any feedback, target, or error signals. As a result, it achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals -- which were necessary in other approaches. Its increased efficiency and biological compatibility do not trade off accuracy compared to state-of-the-art bio-plausible learning, but rather improve it. With up to five hidden layers and an added linear classifier, acc",
    "link": "http://arxiv.org/abs/2209.11883",
    "context": "Title: Hebbian Deep Learning Without Feedback. (arXiv:2209.11883v2 [cs.NE] UPDATED)\nAbstract: Recent approximations to backpropagation (BP) have mitigated many of BP's computational inefficiencies and incompatibilities with biology, but important limitations still remain. Moreover, the approximations significantly decrease accuracy in benchmarks, suggesting that an entirely different approach may be more fruitful. Here, grounded on recent theory for Hebbian learning in soft winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm that trains deep neural networks, without any feedback, target, or error signals. As a result, it achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals -- which were necessary in other approaches. Its increased efficiency and biological compatibility do not trade off accuracy compared to state-of-the-art bio-plausible learning, but rather improve it. With up to five hidden layers and an added linear classifier, acc",
    "path": "papers/22/09/2209.11883.json",
    "total_tokens": 855,
    "translated_title": "无反馈的Hebbian深度学习",
    "translated_abstract": "近期对反向传播算法的近似方法在减少计算效率和与生物学的不兼容性方面有所改进，但仍存在重要限制。此外，这些近似方法显著降低了基准测试的准确性，这表明完全不同的方法可能更有成效。基于最近关于软赢者全拓扑网络中Hebbian学习的理论，我们提出了多层SoftHebb算法，即训练深度神经网络的一种算法，无需任何反馈、目标或错误信号。因此，与其他方法相比，它通过避免权重传输、非局部可塑性、层更新的时间锁定、迭代平衡以及（自身）监督或其他反馈信号来实现效率提升和生物兼容性，而不是以准确性为代价。添加线性分类器后，它在最多五个隐藏层的情况下达到了与最先进的可生物学习相当的准确性。",
    "tldr": "本文提出了无反馈的Hebbian深度学习算法，通过避免传统方法中的反馈信号，实现了高效性、生物兼容性和准确性的同时提升。",
    "en_tdlr": "This paper proposes a Hebbian deep learning algorithm without feedback, achieving efficiency, biological compatibility, and improved accuracy by avoiding traditional feedback signals."
}