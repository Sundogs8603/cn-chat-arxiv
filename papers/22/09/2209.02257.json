{
    "title": "Faster federated optimization under second-order similarity. (arXiv:2209.02257v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FL) is a subfield of machine learning where multiple clients try to collaboratively learn a model over a network under communication constraints. We consider finite-sum federated optimization under a second-order function similarity condition and strong convexity, and propose two new algorithms: SVRP and Catalyzed SVRP. This second-order similarity condition has grown popular recently, and is satisfied in many applications including distributed statistical learning and differentially private empirical risk minimization. The first algorithm, SVRP, combines approximate stochastic proximal point evaluations, client sampling, and variance reduction. We show that SVRP is communication efficient and achieves superior performance to many existing algorithms when function similarity is high enough. Our second algorithm, Catalyzed SVRP, is a Catalyst-accelerated variant of SVRP that achieves even better performance and uniformly improves upon existing algorithms for federate",
    "link": "http://arxiv.org/abs/2209.02257",
    "context": "Title: Faster federated optimization under second-order similarity. (arXiv:2209.02257v2 [cs.LG] UPDATED)\nAbstract: Federated learning (FL) is a subfield of machine learning where multiple clients try to collaboratively learn a model over a network under communication constraints. We consider finite-sum federated optimization under a second-order function similarity condition and strong convexity, and propose two new algorithms: SVRP and Catalyzed SVRP. This second-order similarity condition has grown popular recently, and is satisfied in many applications including distributed statistical learning and differentially private empirical risk minimization. The first algorithm, SVRP, combines approximate stochastic proximal point evaluations, client sampling, and variance reduction. We show that SVRP is communication efficient and achieves superior performance to many existing algorithms when function similarity is high enough. Our second algorithm, Catalyzed SVRP, is a Catalyst-accelerated variant of SVRP that achieves even better performance and uniformly improves upon existing algorithms for federate",
    "path": "papers/22/09/2209.02257.json",
    "total_tokens": 896,
    "translated_title": "基于二阶相似性的更快联邦优化",
    "translated_abstract": "联邦学习是机器学习的一个分支，在通信约束下，多个客户端尝试在网络上协作学习模型。我们考虑在二阶函数相似性条件和强凸性下的有限和联邦优化，并提出了两种新算法：SVRP 和催化 SVRP。近年来，二阶相似性条件已经变得流行起来，并在许多应用中得到满足，包括分布式统计学习和差分隐私经验风险最小化。第一个算法 SVRP 组合了近似随机近端点评估、客户端抽样和方差缩减。我们证明了 SVRP 具有通信效率，并且在函数相似性足够高的情况下，可以获得优越的性能，优于许多现有算法。我们的第二个算法，Catalyzed SVRP 是 SVRP 的催化剂加速变体，可以实现更好的性能，并统一改进现有联邦学习算法。",
    "tldr": "提出两种新的联邦学习算法，SVRP 和 Catalyzed SVRP，它们都有较高的通信效率和性能表现，并广泛适用于分布式统计学习和差分隐私经验风险最小化等领域。",
    "en_tdlr": "Two new algorithms, SVRP and Catalyzed SVRP, are proposed for finite-sum federated optimization under second-order function similarity and strong convexity, which achieve high communication efficiency and superior performance. They are widely applicable in areas such as distributed statistical learning and differentially private empirical risk minimization."
}