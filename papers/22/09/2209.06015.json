{
    "title": "Black-box Dataset Ownership Verification via Backdoor Watermarking. (arXiv:2209.06015v2 [cs.CR] UPDATED)",
    "abstract": "Deep learning, especially deep neural networks (DNNs), has been widely and successfully adopted in many critical applications for its high effectiveness and efficiency. The rapid development of DNNs has benefited from the existence of some high-quality datasets ($e.g.$, ImageNet), which allow researchers and developers to easily verify the performance of their methods. Currently, almost all existing released datasets require that they can only be adopted for academic or educational purposes rather than commercial purposes without permission. However, there is still no good way to ensure that. In this paper, we formulate the protection of released datasets as verifying whether they are adopted for training a (suspicious) third-party model, where defenders can only query the model while having no information about its parameters and training details. Based on this formulation, we propose to embed external patterns via backdoor watermarking for the ownership verification to protect them. ",
    "link": "http://arxiv.org/abs/2209.06015",
    "context": "Title: Black-box Dataset Ownership Verification via Backdoor Watermarking. (arXiv:2209.06015v2 [cs.CR] UPDATED)\nAbstract: Deep learning, especially deep neural networks (DNNs), has been widely and successfully adopted in many critical applications for its high effectiveness and efficiency. The rapid development of DNNs has benefited from the existence of some high-quality datasets ($e.g.$, ImageNet), which allow researchers and developers to easily verify the performance of their methods. Currently, almost all existing released datasets require that they can only be adopted for academic or educational purposes rather than commercial purposes without permission. However, there is still no good way to ensure that. In this paper, we formulate the protection of released datasets as verifying whether they are adopted for training a (suspicious) third-party model, where defenders can only query the model while having no information about its parameters and training details. Based on this formulation, we propose to embed external patterns via backdoor watermarking for the ownership verification to protect them. ",
    "path": "papers/22/09/2209.06015.json",
    "total_tokens": 795,
    "translated_title": "通过后门水印的黑盒数据集所有权验证",
    "translated_abstract": "深度学习，特别是深度神经网络（DNNs），由于其高效性和高效性，在许多重要应用中被广泛且成功地采用。DNN的快速发展受益于一些高质量数据集（例如ImageNet）的存在，这些数据集允许研究人员和开发者轻松验证其方法的性能。目前，几乎所有现有的已发布数据集都要求它们仅能用于学术或教育目的而非商业目的，但仍然没有很好的方法来确保这一点。在本文中，我们将保护已发布数据集的形式化为验证它们是否被用于训练（可疑的）第三方模型，而防御者只能查询模型，而没有关于其参数和训练细节的信息。",
    "tldr": "本文提出了一种通过后门水印技术验证已发布数据集的所有权的方法，以检测其是否被用于训练（可疑的）第三方模型。",
    "en_tdlr": "This paper proposes a method to verify the ownership of released datasets by backdoor watermarking to detect whether they are used to train (suspicious) third-party models."
}