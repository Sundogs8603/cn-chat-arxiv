{
    "title": "Characterizing Internal Evasion Attacks in Federated Learning. (arXiv:2209.08412v3 [cs.LG] UPDATED)",
    "abstract": "Federated learning allows for clients in a distributed system to jointly train a machine learning model. However, clients' models are vulnerable to attacks during the training and testing phases. In this paper, we address the issue of adversarial clients performing \"internal evasion attacks\": crafting evasion attacks at test time to deceive other clients. For example, adversaries may aim to deceive spam filters and recommendation systems trained with federated learning for monetary gain. The adversarial clients have extensive information about the victim model in a federated learning setting, as weight information is shared amongst clients. We are the first to characterize the transferability of such internal evasion attacks for different learning methods and analyze the trade-off between model accuracy and robustness depending on the degree of similarities in client data. We show that adversarial training defenses in the federated learning setting only display limited improvements aga",
    "link": "http://arxiv.org/abs/2209.08412",
    "context": "Title: Characterizing Internal Evasion Attacks in Federated Learning. (arXiv:2209.08412v3 [cs.LG] UPDATED)\nAbstract: Federated learning allows for clients in a distributed system to jointly train a machine learning model. However, clients' models are vulnerable to attacks during the training and testing phases. In this paper, we address the issue of adversarial clients performing \"internal evasion attacks\": crafting evasion attacks at test time to deceive other clients. For example, adversaries may aim to deceive spam filters and recommendation systems trained with federated learning for monetary gain. The adversarial clients have extensive information about the victim model in a federated learning setting, as weight information is shared amongst clients. We are the first to characterize the transferability of such internal evasion attacks for different learning methods and analyze the trade-off between model accuracy and robustness depending on the degree of similarities in client data. We show that adversarial training defenses in the federated learning setting only display limited improvements aga",
    "path": "papers/22/09/2209.08412.json",
    "total_tokens": 899,
    "translated_title": "在联邦学习中特征化内部规避攻击",
    "translated_abstract": "联邦学习允许分布式系统中的客户共同训练一个机器学习模型。然而，在训练和测试阶段，客户的模型容易受到攻击。本文解决了对抗性客户执行\"内部规避攻击\"的问题：在测试时制造规避攻击以欺骗其他客户。例如，对手可能旨在通过伪造的垃圾邮件过滤器和推荐系统来进行联邦学习训练以获取利益。在联邦学习环境中，对抗性客户对受害模型拥有广泛的信息，因为权重信息在客户之间共享。我们是第一个对不同学习方法下此类内部规避攻击的可迁移性进行特征化，并分析了模型精度和鲁棒性之间的权衡，具体取决于客户数据的相似程度。我们表明，在联邦学习环境中，对抗性训练防御仅显示有限的改进。",
    "tldr": "本文通过特征化不同学习方法下内部规避攻击的可迁移性，并分析了模型精度和鲁棒性之间的权衡，首次研究了在联邦学习中对抗性客户制造规避攻击的问题。",
    "en_tdlr": "This paper characterizes the transferability of internal evasion attacks for different learning methods in federated learning and analyzes the trade-off between model accuracy and robustness. It is the first study to address the issue of adversarial clients performing evasion attacks in federated learning."
}