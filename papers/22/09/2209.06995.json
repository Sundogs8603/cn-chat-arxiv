{
    "title": "Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach. (arXiv:2209.06995v2 [cs.CL] UPDATED)",
    "abstract": "Large Language Models have demonstrated remarkable few-shot performance, but the performance can be sensitive to the selection of few-shot instances. We propose PATRON, a new method that uses prompt-based uncertainty estimation for data selection for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON is available at \\url{https://github.com/yueyu1030/Patron}.",
    "link": "http://arxiv.org/abs/2209.06995",
    "context": "Title: Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach. (arXiv:2209.06995v2 [cs.CL] UPDATED)\nAbstract: Large Language Models have demonstrated remarkable few-shot performance, but the performance can be sensitive to the selection of few-shot instances. We propose PATRON, a new method that uses prompt-based uncertainty estimation for data selection for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON is available at \\url{https://github.com/yueyu1030/Patron}.",
    "path": "papers/22/09/2209.06995.json",
    "total_tokens": 1020,
    "translated_title": "冷启动情况下的数据选择策略：一种基于提示信息传递不确定性估计的few-shot语言模型微调方法",
    "translated_abstract": "大型语言模型展现出了出色的few-shot性能，但性能对于few-shot实例的选择非常敏感。我们提出了一种名为PATRON的新方法，该方法使用基于提示信息的不确定性估计来选择预训练语言模型微调的数据，在冷启动情况下没有初始标记数据可用。在PATRON中，我们设计了（1）基于提示信息的不确定性传播方法来估计数据点的重要性和（2）一种分割-重写（PTR）策略，以在查询注释时促进样本的多样性。在六个文本分类数据集上的实验表明，PATRON的表现比最强的冷启动数据选择基线优越了6.9%。此外，仅使用128标签，PATRON基于普通微调和基于提示信息学习分别达到了91.0%和92.1%的完全监督性能。我们的PATRON实现可在\\url{https://github.com/yueyu1030/Patron}上获得。",
    "tldr": "本文提出了一种名为PATRON的方法，使用基于提示信息的不确定性的数据选择策略来提高预训练语言模型微调的few-shot性能，在六个文本分类数据集上实验证实该方法的性能优于最先进的冷启动数据选择基线，且仅使用128标签的情况下，该方法可以达到91.0%和92.1%的完全监督性能。",
    "en_tdlr": "This paper proposes a method called PATRON that uses a prompt-based uncertainty data selection strategy to improve few-shot performance in pre-trained language model fine-tuning under cold-start scenarios. Experimental results on six text classification datasets show that PATRON outperforms state-of-the-art cold-start data selection baselines, and with only 128 labels, it achieves 91.0% and 92.1% of fully supervised performance based on vanilla fine-tuning and prompt-based learning, respectively."
}