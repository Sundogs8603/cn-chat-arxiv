{
    "title": "A Max-relevance-min-divergence Criterion for Data Discretization with Applications on Naive Bayes. (arXiv:2209.10095v2 [cs.LG] UPDATED)",
    "abstract": "In many classification models, data is discretized to better estimate its distribution. Existing discretization methods often target at maximizing the discriminant power of discretized data, while overlooking the fact that the primary target of data discretization in classification is to improve the generalization performance. As a result, the data tend to be over-split into many small bins since the data without discretization retain the maximal discriminant information. Thus, we propose a Max-Dependency-Min-Divergence (MDmD) criterion that maximizes both the discriminant information and generalization ability of the discretized data. More specifically, the Max-Dependency criterion maximizes the statistical dependency between the discretized data and the classification variable while the Min-Divergence criterion explicitly minimizes the JS-divergence between the training data and the validation data for a given discretization scheme. The proposed MDmD criterion is technically appealin",
    "link": "http://arxiv.org/abs/2209.10095",
    "context": "Title: A Max-relevance-min-divergence Criterion for Data Discretization with Applications on Naive Bayes. (arXiv:2209.10095v2 [cs.LG] UPDATED)\nAbstract: In many classification models, data is discretized to better estimate its distribution. Existing discretization methods often target at maximizing the discriminant power of discretized data, while overlooking the fact that the primary target of data discretization in classification is to improve the generalization performance. As a result, the data tend to be over-split into many small bins since the data without discretization retain the maximal discriminant information. Thus, we propose a Max-Dependency-Min-Divergence (MDmD) criterion that maximizes both the discriminant information and generalization ability of the discretized data. More specifically, the Max-Dependency criterion maximizes the statistical dependency between the discretized data and the classification variable while the Min-Divergence criterion explicitly minimizes the JS-divergence between the training data and the validation data for a given discretization scheme. The proposed MDmD criterion is technically appealin",
    "path": "papers/22/09/2209.10095.json",
    "total_tokens": 869,
    "translated_title": "基于最大相关最小差异标准的数据离散化及其在朴素贝叶斯上的应用",
    "translated_abstract": "在许多分类模型中，为更好地估计其分布，数据会被离散化。现有的离散化方法往往针对最大化离散化后数据的判别能力，而忽视了数据离散化在分类中的主要目标是提高泛化性能。因此，为同时最大化离散化后数据的判别信息和泛化能力，我们提出了Max-Dependency-Min-Divergence（MDmD）准则。具体而言，最大相关性准则最大化了离散化数据与分类变量之间的统计依赖性，而最小分歧准则在给定离散化方案时显式地最小化了训练数据与验证数据之间的JS距离。提出的MDmD准则在技术上非常有吸引力。",
    "tldr": "该论文提出了一种Max-Dependency-Min-Divergence (MDmD)准则，旨在同时最大化离散化数据的判别信息和泛化能力。此准则可应用于离散化数据的分类模型中，如朴素贝叶斯。"
}