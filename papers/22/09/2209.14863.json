{
    "title": "Neural Networks Efficiently Learn Low-Dimensional Representations with SGD. (arXiv:2209.14863v2 [stat.ML] UPDATED)",
    "abstract": "We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input $\\boldsymbol{x}\\in \\mathbb{R}^d$ is Gaussian and the target $y \\in \\mathbb{R}$ follows a multiple-index model, i.e., $y=g(\\langle\\boldsymbol{u_1},\\boldsymbol{x}\\rangle,...,\\langle\\boldsymbol{u_k},\\boldsymbol{x}\\rangle)$ with a noisy link function $g$. We prove that the first-layer weights of the NN converge to the $k$-dimensional principal subspace spanned by the vectors $\\boldsymbol{u_1},...,\\boldsymbol{u_k}$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when $k \\ll d$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of $O(\\sqrt{{kd}/{T}})$ after $T$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, SGD-trained ReLU NNs can learn a single-index target of the form $y=f",
    "link": "http://arxiv.org/abs/2209.14863",
    "context": "Title: Neural Networks Efficiently Learn Low-Dimensional Representations with SGD. (arXiv:2209.14863v2 [stat.ML] UPDATED)\nAbstract: We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input $\\boldsymbol{x}\\in \\mathbb{R}^d$ is Gaussian and the target $y \\in \\mathbb{R}$ follows a multiple-index model, i.e., $y=g(\\langle\\boldsymbol{u_1},\\boldsymbol{x}\\rangle,...,\\langle\\boldsymbol{u_k},\\boldsymbol{x}\\rangle)$ with a noisy link function $g$. We prove that the first-layer weights of the NN converge to the $k$-dimensional principal subspace spanned by the vectors $\\boldsymbol{u_1},...,\\boldsymbol{u_k}$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when $k \\ll d$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of $O(\\sqrt{{kd}/{T}})$ after $T$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, SGD-trained ReLU NNs can learn a single-index target of the form $y=f",
    "path": "papers/22/09/2209.14863.json",
    "total_tokens": 1153,
    "translated_title": "神经网络通过随机梯度下降有效地学习低维表示",
    "translated_abstract": "本文研究了使用随机梯度下降训练任意宽度的两层神经网络(NN)的问题，其中输入$ \\boldsymbol{x} \\in \\mathbb {R}^d $为高斯分布，目标$ y \\in \\mathbb {R}$遵循多指数模型，即 $ y = g(\\langle\\boldsymbol{u_1},\\boldsymbol{x}\\rangle,...,\\langle\\boldsymbol{u_k},\\boldsymbol{x}\\rangle)$ ，其中函数$g$为有噪声的连接函数，我们证明当使用带有权重衰减的在线SGD进行训练时，NN的第一层权重会收敛到真实模型中$ \\boldsymbol{u_1},...,\\boldsymbol{u_k}$的$k$维主子空间，当$k \\ll d$ 时，该现象有几个重要的影响。首先，通过在这个更小的子空间上使用均匀收敛，我们建立了在SGD进行$T$次迭代后的广义误差边界为$ O(\\sqrt{{kd}/{T}})$，这不依赖于NN的宽度。我们进一步证明，SGD训练的ReLU NN可以学习形如$y = f(\\langle\\boldsymbol{u},\\boldsymbol{x}\\rangle)$的单指数目标.",
    "tldr": "本文研究使用随机梯度下降训练任意宽度的两层神经网络的问题，当输入为高斯分布，目标为多指数模型时，NN的第一层权重会收敛到真实模型中$k$维主子空间, 可以通过在子空间上使用均匀收敛建立广义误差边界为$O(\\sqrt{{kd}/{T}})$, SGD训练的ReLU NN可以学习形如$y = f(\\langle\\boldsymbol{u},\\boldsymbol{x}\\rangle)$的单指数目标。"
}