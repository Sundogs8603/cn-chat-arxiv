{
    "title": "Learning Continuous Control Policies for Information-Theoretic Active Perception. (arXiv:2209.12427v2 [cs.RO] UPDATED)",
    "abstract": "This paper proposes a method for learning continuous control policies for active landmark localization and exploration using an information-theoretic cost. We consider a mobile robot detecting landmarks within a limited sensing range, and tackle the problem of learning a control policy that maximizes the mutual information between the landmark states and the sensor observations. We employ a Kalman filter to convert the partially observable problem in the landmark state to Markov decision process (MDP), a differentiable field of view to shape the reward, and an attention-based neural network to represent the control policy. The approach is further unified with active volumetric mapping to promote exploration in addition to landmark localization. The performance is demonstrated in several simulated landmark localization tasks in comparison with benchmark methods.",
    "link": "http://arxiv.org/abs/2209.12427",
    "context": "Title: Learning Continuous Control Policies for Information-Theoretic Active Perception. (arXiv:2209.12427v2 [cs.RO] UPDATED)\nAbstract: This paper proposes a method for learning continuous control policies for active landmark localization and exploration using an information-theoretic cost. We consider a mobile robot detecting landmarks within a limited sensing range, and tackle the problem of learning a control policy that maximizes the mutual information between the landmark states and the sensor observations. We employ a Kalman filter to convert the partially observable problem in the landmark state to Markov decision process (MDP), a differentiable field of view to shape the reward, and an attention-based neural network to represent the control policy. The approach is further unified with active volumetric mapping to promote exploration in addition to landmark localization. The performance is demonstrated in several simulated landmark localization tasks in comparison with benchmark methods.",
    "path": "papers/22/09/2209.12427.json",
    "total_tokens": 898,
    "translated_title": "学习信息理论主动感知的连续控制策略",
    "translated_abstract": "本文提出了一种使用信息理论代价来学习主动地标定位和探索的连续控制策略方法。我们考虑一个移动机器人在有限感知范围内检测地标，并解决学习控制策略的问题，该策略最大化地标状态与传感器观测之间的互信息。我们采用卡尔曼滤波器把地标状态的部分可观察问题转换为马尔科夫决策过程（MDP），使用可微分的视野来塑造奖励，并使用基于注意力机制的神经网络来表示控制策略。该方法进一步与主动体积建图结合起来，以促进地标定位和探索。在与基准方法的比较中，本文展示了在几个模拟的地标定位任务中的性能。",
    "tldr": "本文提出了一种使用信息理论代价来学习主动地标定位和探索的连续控制策略方法，该方法使用卡尔曼滤波器转换部分可观察问题为MDP，使用视野来塑造奖励，使用基于注意力机制的神经网络来表示控制策略，并与主动体积建图结合以促进地标定位和探索。",
    "en_tdlr": "This paper proposes a method for learning continuous control policies for active landmark localization and exploration using an information-theoretic cost, which employs a Kalman filter to convert partially observable problem to MDP, a differentiable field of view to shape the reward, and an attention-based neural network to represent the control policy, and is further unified with active volumetric mapping to promote exploration in addition to landmark localization."
}