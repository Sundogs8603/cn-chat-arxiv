{
    "title": "SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval. (arXiv:2209.05917v2 [cs.IR] UPDATED)",
    "abstract": "Sparse document representations have been widely used to retrieve relevant documents via exact lexical matching. Owing to the pre-computed inverted index, it supports fast ad-hoc search but incurs the vocabulary mismatch problem. Although recent neural ranking models using pre-trained language models can address this problem, they usually require expensive query inference costs, implying the trade-off between effectiveness and efficiency. Tackling the trade-off, we propose a novel uni-encoder ranking model, Sparse retriever using a Dual document Encoder (SpaDE), learning document representation via the dual encoder. Each encoder plays a central role in (i) adjusting the importance of terms to improve lexical matching and (ii) expanding additional terms to support semantic matching. Furthermore, our co-training strategy trains the dual encoder effectively and avoids unnecessary intervention in training each other. Experimental results on several benchmarks show that SpaDE outperforms ex",
    "link": "http://arxiv.org/abs/2209.05917",
    "context": "Title: SpaDE: Improving Sparse Representations using a Dual Document Encoder for First-stage Retrieval. (arXiv:2209.05917v2 [cs.IR] UPDATED)\nAbstract: Sparse document representations have been widely used to retrieve relevant documents via exact lexical matching. Owing to the pre-computed inverted index, it supports fast ad-hoc search but incurs the vocabulary mismatch problem. Although recent neural ranking models using pre-trained language models can address this problem, they usually require expensive query inference costs, implying the trade-off between effectiveness and efficiency. Tackling the trade-off, we propose a novel uni-encoder ranking model, Sparse retriever using a Dual document Encoder (SpaDE), learning document representation via the dual encoder. Each encoder plays a central role in (i) adjusting the importance of terms to improve lexical matching and (ii) expanding additional terms to support semantic matching. Furthermore, our co-training strategy trains the dual encoder effectively and avoids unnecessary intervention in training each other. Experimental results on several benchmarks show that SpaDE outperforms ex",
    "path": "papers/22/09/2209.05917.json",
    "total_tokens": 918,
    "translated_title": "SpaDE: 一种利用双重文档编码器改善稀疏表示的第一阶段检索方法",
    "translated_abstract": "稀疏的文档表示经常被用来通过精确的词汇匹配来检索相关文档。然而，由于预先计算的倒排索引，会引发词汇不匹配的问题。虽然最近使用预训练语言模型的神经排序模型可以解决这个问题，但它们通常需要昂贵的查询推理成本，这意味着效率和效果之间存在权衡。为了解决这个问题，我们提出了一种新的单编码器排名模型，利用双重编码器学习文档表示，称为 Sparse retriever using a Dual document Encoder (SpaDE)。每个编码器在改善词汇匹配和扩展额外术语来支持语义匹配方面发挥着核心作用。此外，我们的协同训练策略可以有效地训练双重编码器，并避免不必要的干预彼此的训练过程。在几个基准测试中的实验结果表明，SpaDE 超越了现有的检索方法。",
    "tldr": "SpaDE 是一种利用双重编码器学习文档表示的第一阶段检索模型，可以同时改善词汇匹配和扩展额外术语来支持语义匹配，且在实验中表现优异。",
    "en_tdlr": "SpaDE is a novel uni-encoder ranking model that utilizes dual document encoders to learn document representations for first-stage retrieval, improving both lexical and semantic matching. Experimental results demonstrate its superior performance compared to existing retrieval methods."
}