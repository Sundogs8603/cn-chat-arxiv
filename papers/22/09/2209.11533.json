{
    "title": "A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models. (arXiv:2209.11533v2 [cs.LG] UPDATED)",
    "abstract": "Variational inference with Gaussian mixture models (GMMs) enables learning of highly tractable yet multi-modal approximations of intractable target distributions with up to a few hundred dimensions. The two currently most effective methods for GMM-based variational inference, VIPS and iBayes-GMM, both employ independent natural gradient updates for the individual components and their weights. We show for the first time, that their derived updates are equivalent, although their practical implementations and theoretical guarantees differ. We identify several design choices that distinguish both approaches, namely with respect to sample selection, natural gradient estimation, stepsize adaptation, and whether trust regions are enforced or the number of components adapted. We argue that for both approaches, the quality of the learned approximations can heavily suffer from the respective design choices: By updating the individual components using samples from the mixture model, iBayes-GMM of",
    "link": "http://arxiv.org/abs/2209.11533",
    "context": "Title: A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models. (arXiv:2209.11533v2 [cs.LG] UPDATED)\nAbstract: Variational inference with Gaussian mixture models (GMMs) enables learning of highly tractable yet multi-modal approximations of intractable target distributions with up to a few hundred dimensions. The two currently most effective methods for GMM-based variational inference, VIPS and iBayes-GMM, both employ independent natural gradient updates for the individual components and their weights. We show for the first time, that their derived updates are equivalent, although their practical implementations and theoretical guarantees differ. We identify several design choices that distinguish both approaches, namely with respect to sample selection, natural gradient estimation, stepsize adaptation, and whether trust regions are enforced or the number of components adapted. We argue that for both approaches, the quality of the learned approximations can heavily suffer from the respective design choices: By updating the individual components using samples from the mixture model, iBayes-GMM of",
    "path": "papers/22/09/2209.11533.json",
    "total_tokens": 1040,
    "translated_title": "使用高斯混合模型的自然梯度变分推断的统一视角",
    "translated_abstract": "使用高斯混合模型（GMM）进行变分推断能够以高度可行但多模态的方式学习难以处理的目标分布，具有最多几百个维度。目前对于基于GMM的变分推断来说，VIPS和iBayes-GMM是最有效的两种方法，它们都使用独立的自然梯度更新来更新各个组件及其权重。我们首次证明了它们派生的更新是等价的，尽管它们的实际实现和理论保证有所不同。我们确定了几个区分这两种方法的设计选择，包括样本选择、自然梯度估计、步长适应以及是否强制实施可信区域或调整组件的数量。我们认为，对于这两种方法，所学近似的质量可能会受到相应设计选择的严重影响：通过使用混合模型中的样本来更新各个组件，iBayes-GMM的学习近似质量可能受到更严重影响。",
    "tldr": "本论文提出了一种统一的视角来理解使用高斯混合模型进行自然梯度变分推断的方法。研究发现，VIPS和iBayes-GMM这两种目前最有效的方法，在更新各个组件和权重时使用的自然梯度更新是等价的，但其实现和理论保证存在差异。研究还发现，这两种方法在样本选择、自然梯度估计、步长适应以及可信区域或组件数量的调整等设计选择上存在区别，对于学习近似的质量有重要影响。",
    "en_tdlr": "This paper presents a unified perspective on natural gradient variational inference with Gaussian mixture models. It shows that the two most effective methods, VIPS and iBayes-GMM, have equivalent updates for the individual components and their weights, but differ in practical implementations and theoretical guarantees. Several design choices are identified that impact the quality of the learned approximations, including sample selection, natural gradient estimation, stepsize adaptation, and trust region enforcement or component number adaptation."
}