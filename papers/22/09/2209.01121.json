{
    "title": "Back-to-Bones: Rediscovering the Role of Backbones in Domain Generalization. (arXiv:2209.01121v2 [cs.CV] UPDATED)",
    "abstract": "Domain Generalization (DG) studies the capability of a deep learning model to generalize to out-of-training distributions. In the last decade, literature has been massively filled with training methodologies that claim to obtain more abstract and robust data representations to tackle domain shifts. Recent research has provided a reproducible benchmark for DG, pointing out the effectiveness of naive empirical risk minimization (ERM) over existing algorithms. Nevertheless, researchers persist in using the same outdated feature extractors, and no attention has been given to the effects of different backbones yet. In this paper, we start back to the backbones proposing a comprehensive analysis of their intrinsic generalization capabilities, which so far have been ignored by the research community. We evaluate a wide variety of feature extractors, from standard residual solutions to transformer-based architectures, finding an evident linear correlation between large-scale single-domain clas",
    "link": "http://arxiv.org/abs/2209.01121",
    "context": "Title: Back-to-Bones: Rediscovering the Role of Backbones in Domain Generalization. (arXiv:2209.01121v2 [cs.CV] UPDATED)\nAbstract: Domain Generalization (DG) studies the capability of a deep learning model to generalize to out-of-training distributions. In the last decade, literature has been massively filled with training methodologies that claim to obtain more abstract and robust data representations to tackle domain shifts. Recent research has provided a reproducible benchmark for DG, pointing out the effectiveness of naive empirical risk minimization (ERM) over existing algorithms. Nevertheless, researchers persist in using the same outdated feature extractors, and no attention has been given to the effects of different backbones yet. In this paper, we start back to the backbones proposing a comprehensive analysis of their intrinsic generalization capabilities, which so far have been ignored by the research community. We evaluate a wide variety of feature extractors, from standard residual solutions to transformer-based architectures, finding an evident linear correlation between large-scale single-domain clas",
    "path": "papers/22/09/2209.01121.json",
    "total_tokens": 908,
    "translated_title": "重归骨干：重新发现骨干在域泛化中的作用",
    "translated_abstract": "域泛化(DG)研究深度学习模型对于超出训练分布的泛化能力。在过去十年中，文献中充斥着声称能够获得更抽象和稳健的数据表示以应对域偏移的训练方法。最近的研究为DG提供了可重复的基准，指出了天真的经验风险最小化(ERM)算法的有效性。然而，研究人员仍然坚持使用相同过时的特征提取器，迄今为止尚未注意到不同骨干的影响。在本文中，我们重新回到骨干，提出了对它们内在泛化能力的全面分析，这在研究社区中被忽视。我们评估了各种特征提取器，从标准的残差解决方案到基于Transformer的架构，发现了一个明显的线性相关性，就是说，大规模单域分类任务的性能可以通过骨干的准确性凸显出来。",
    "tldr": "本文重点研究骨干在深度学习模型中域泛化能力的影响，通过评估各种特征提取器发现其准确性与单域分类任务性能有显著线性相关性。",
    "en_tdlr": "This paper focuses on the role of backbones in domain generalization of deep learning models. It evaluates various feature extractors and discovers a significant linear correlation between their accuracy and performance of single-domain classification tasks."
}