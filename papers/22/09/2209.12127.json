{
    "title": "SpeedLimit: Neural Architecture Search for Quantized Transformer Models. (arXiv:2209.12127v2 [cs.LG] UPDATED)",
    "abstract": "While research in the field of transformer models has primarily focused on enhancing performance metrics such as accuracy and perplexity, practical applications in industry often necessitate a rigorous consideration of inference latency constraints. Addressing this challenge, we introduce SpeedLimit, a novel Neural Architecture Search (NAS) technique that optimizes accuracy whilst adhering to an upper-bound latency constraint. Our method incorporates 8-bit integer quantization in the search process to outperform the current state-of-the-art technique. Our results underline the feasibility and efficacy of seeking an optimal balance between performance and latency, providing new avenues for deploying state-of-the-art transformer models in latency-sensitive environments.",
    "link": "http://arxiv.org/abs/2209.12127",
    "context": "Title: SpeedLimit: Neural Architecture Search for Quantized Transformer Models. (arXiv:2209.12127v2 [cs.LG] UPDATED)\nAbstract: While research in the field of transformer models has primarily focused on enhancing performance metrics such as accuracy and perplexity, practical applications in industry often necessitate a rigorous consideration of inference latency constraints. Addressing this challenge, we introduce SpeedLimit, a novel Neural Architecture Search (NAS) technique that optimizes accuracy whilst adhering to an upper-bound latency constraint. Our method incorporates 8-bit integer quantization in the search process to outperform the current state-of-the-art technique. Our results underline the feasibility and efficacy of seeking an optimal balance between performance and latency, providing new avenues for deploying state-of-the-art transformer models in latency-sensitive environments.",
    "path": "papers/22/09/2209.12127.json",
    "total_tokens": 783,
    "translated_title": "SpeedLimit：量化Transformer模型的神经架构搜索",
    "translated_abstract": "尽管Transformer模型的研究主要集中在提高诸如准确性和复杂度这样的性能指标上，但实际应用通常需要严格考虑推理延迟约束。为了解决这个挑战，我们引入了SpeedLimit，一种新的神经架构搜索技术，它在保持上限延迟约束的前提下优化准确性。我们的方法在搜索过程中结合了8位整数量化，超过了当前最先进的技术。我们的结果强调了在性能和延迟之间寻求最佳平衡的可行性和有效性，为在延迟敏感的环境中部署最先进的Transformer模型提供了新的方法。",
    "tldr": "本文介绍了SpeedLimit——一种新的神经架构搜索技术，通过在量化的Transformer模型中添加上限延迟约束，优化准确性。该方法比当前最先进的技术表现更好，为在延迟敏感的环境中使用Transformer模型提供了新的可能性。",
    "en_tdlr": "This paper introduces SpeedLimit, a novel neural architecture search technique that optimizes accuracy while adhering to an upper-bound latency constraint by incorporating 8-bit integer quantization in the search process for transformer models. The method outperforms current state-of-the-art techniques and provides new possibilities for deploying transformer models in latency-sensitive environments."
}