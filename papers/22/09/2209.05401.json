{
    "title": "MaXM: Towards Multilingual Visual Question Answering. (arXiv:2209.05401v3 [cs.CL] UPDATED)",
    "abstract": "Visual Question Answering (VQA) has been primarily studied through the lens of the English language. Yet, tackling VQA in other languages in the same manner would require a considerable amount of resources. In this paper, we propose scalable solutions to multilingual visual question answering (mVQA), on both data and modeling fronts. We first propose a translation-based framework to mVQA data generation that requires much less human annotation efforts than the conventional approach of directly collection questions and answers. Then, we apply our framework to the multilingual captions in the Crossmodal-3600 dataset and develop an efficient annotation protocol to create MaXM, a test-only VQA benchmark in 7 diverse languages. Finally, we develop a simple, lightweight, and effective approach as well as benchmark state-of-the-art English and multilingual VQA models. We hope that our benchmark encourages further research on mVQA.",
    "link": "http://arxiv.org/abs/2209.05401",
    "context": "Title: MaXM: Towards Multilingual Visual Question Answering. (arXiv:2209.05401v3 [cs.CL] UPDATED)\nAbstract: Visual Question Answering (VQA) has been primarily studied through the lens of the English language. Yet, tackling VQA in other languages in the same manner would require a considerable amount of resources. In this paper, we propose scalable solutions to multilingual visual question answering (mVQA), on both data and modeling fronts. We first propose a translation-based framework to mVQA data generation that requires much less human annotation efforts than the conventional approach of directly collection questions and answers. Then, we apply our framework to the multilingual captions in the Crossmodal-3600 dataset and develop an efficient annotation protocol to create MaXM, a test-only VQA benchmark in 7 diverse languages. Finally, we develop a simple, lightweight, and effective approach as well as benchmark state-of-the-art English and multilingual VQA models. We hope that our benchmark encourages further research on mVQA.",
    "path": "papers/22/09/2209.05401.json",
    "total_tokens": 932,
    "translated_title": "MaXM：走向多语言视觉问答",
    "translated_abstract": "视觉问答(VQA)主要通过英语进行研究。然而，以同样的方式在其他语言中解决VQA问题需要大量的资源。本文提出了可扩展的解决方案，用于多语言视觉问答(mVQA)，包括数据和建模方面。我们首先提出了一种基于翻译的mVQA数据生成框架，比直接收集问题和答案的传统方法需要更少的人工注释工作量。然后，我们将该框架应用于Crossmodal-3600数据集中的多语言字幕，开发了一种高效的注释协议，创建了MaXM，一个包含7种不同语言的仅用于测试的VQA基准。最后，我们开发了一个简单、轻量级、高效的方法，并基于此构建了英语和多语言VQA模型。我们希望我们的基准能够鼓励进一步研究mVQA。",
    "tldr": "本文提出了针对多语言视觉问答的可扩展解决方案，包括数据生成和建模。通过基于翻译的数据生成框架和高效的注释协议，创建了包含7种不同语言的mVQA基准MaXM。同时，开发了简单、轻量级、高效的VQA模型。鼓励进一步研究mVQA。",
    "en_tdlr": "This paper proposes scalable solutions for multilingual visual question answering (mVQA), including data generation and modeling. A translation-based data generation framework and an efficient annotation protocol are utilized to create the MaXM benchmark, which includes 7 different languages. Additionally, a simple, lightweight, and effective VQA model is developed. Encouragement for further research on mVQA is provided."
}