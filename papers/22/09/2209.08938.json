{
    "title": "Accelerating Neural Network Inference with Processing-in-DRAM: From the Edge to the Cloud. (arXiv:2209.08938v2 [cs.AR] UPDATED)",
    "abstract": "Neural networks (NNs) are growing in importance and complexity. A neural network's performance (and energy efficiency) can be bound either by computation or memory resources. The processing-in-memory (PIM) paradigm, where computation is placed near or within memory arrays, is a viable solution to accelerate memory-bound NNs. However, PIM architectures vary in form, where different PIM approaches lead to different trade-offs. Our goal is to analyze, discuss, and contrast DRAM-based PIM architectures for NN performance and energy efficiency. To do so, we analyze three state-of-the-art PIM architectures: (1) UPMEM, which integrates processors and DRAM arrays into a single 2D chip; (2) Mensa, a 3D-stack-based PIM architecture tailored for edge devices; and (3) SIMDRAM, which uses the analog principles of DRAM to execute bit-serial operations. Our analysis reveals that PIM greatly benefits memory-bound NNs: (1) UPMEM provides 23x the performance of a high-end GPU when the GPU requires memor",
    "link": "http://arxiv.org/abs/2209.08938",
    "context": "Title: Accelerating Neural Network Inference with Processing-in-DRAM: From the Edge to the Cloud. (arXiv:2209.08938v2 [cs.AR] UPDATED)\nAbstract: Neural networks (NNs) are growing in importance and complexity. A neural network's performance (and energy efficiency) can be bound either by computation or memory resources. The processing-in-memory (PIM) paradigm, where computation is placed near or within memory arrays, is a viable solution to accelerate memory-bound NNs. However, PIM architectures vary in form, where different PIM approaches lead to different trade-offs. Our goal is to analyze, discuss, and contrast DRAM-based PIM architectures for NN performance and energy efficiency. To do so, we analyze three state-of-the-art PIM architectures: (1) UPMEM, which integrates processors and DRAM arrays into a single 2D chip; (2) Mensa, a 3D-stack-based PIM architecture tailored for edge devices; and (3) SIMDRAM, which uses the analog principles of DRAM to execute bit-serial operations. Our analysis reveals that PIM greatly benefits memory-bound NNs: (1) UPMEM provides 23x the performance of a high-end GPU when the GPU requires memor",
    "path": "papers/22/09/2209.08938.json",
    "total_tokens": 951,
    "translated_title": "基于DRAM的处理器加速神经网络推理：从边缘到云端",
    "translated_abstract": "神经网络（NN）在重要性和复杂性上不断增长。神经网络的性能（及能效）可能会受到计算或内存资源的限制。在内存阵列附近或内部放置计算的内存中处理（PIM）范例是加速内存受限的NN的可行解决方案。然而，PIM架构因形式而异，不同的PIM方法导致不同的权衡。我们的目标是分析、讨论和对比基于DRAM的PIM架构对NN性能和能效的影响。为此，我们分析了三种最先进的PIM架构：（1）UPMEM，将处理器和DRAM阵列集成到单个二维芯片中；（2）Mensa，一种针对边缘设备量身定制的三维堆栈式PIM架构；和（3）SIMDRAM，它使用DRAM的模拟原理执行位串行操作。我们的分析表明，PIM极大地有利于内存受限的NN：（1）当GPU需要内存时，UPMEM提供了高端GPU的23倍性能。",
    "tldr": "该论文讨论了基于DRAM的内存中处理（PIM）方法加速神经网络推理的可行性。通过分析三种最先进的PIM架构，得出PIM极大地有利于内存受限的NN的结论。",
    "en_tdlr": "This paper discusses the feasibility of using processing-in-memory (PIM) with DRAM to accelerate neural network inference. By analyzing three state-of-the-art PIM architectures, the study concludes that PIM greatly benefits memory-bound neural networks."
}