{
    "title": "An Analysis of Collocation on GPUs for Deep Learning Training. (arXiv:2209.06018v3 [cs.LG] UPDATED)",
    "abstract": "Deep learning training is an expensive process that extensively uses GPUs, but not all model training saturates modern powerful GPUs. Multi-Instance GPU (MIG) is a new technology introduced by NVIDIA that can partition a GPU to better-fit workloads that do not require all the memory and compute resources of a full GPU. In this paper, we examine the performance of a MIG-enabled A100 GPU under deep learning workloads containing various sizes and combinations of models. We contrast the benefits of MIG to older workload collocation methods on GPUs: na\\\"ively submitting multiple processes on the same GPU and utilizing Multi-Process Service (MPS). Our results demonstrate that collocating multiple model training runs may yield significant benefits. In certain cases, it can lead up to four times training throughput despite increased epoch time. On the other hand, the aggregate memory footprint and compute needs of the models trained in parallel must fit the available memory and compute resourc",
    "link": "http://arxiv.org/abs/2209.06018",
    "context": "Title: An Analysis of Collocation on GPUs for Deep Learning Training. (arXiv:2209.06018v3 [cs.LG] UPDATED)\nAbstract: Deep learning training is an expensive process that extensively uses GPUs, but not all model training saturates modern powerful GPUs. Multi-Instance GPU (MIG) is a new technology introduced by NVIDIA that can partition a GPU to better-fit workloads that do not require all the memory and compute resources of a full GPU. In this paper, we examine the performance of a MIG-enabled A100 GPU under deep learning workloads containing various sizes and combinations of models. We contrast the benefits of MIG to older workload collocation methods on GPUs: na\\\"ively submitting multiple processes on the same GPU and utilizing Multi-Process Service (MPS). Our results demonstrate that collocating multiple model training runs may yield significant benefits. In certain cases, it can lead up to four times training throughput despite increased epoch time. On the other hand, the aggregate memory footprint and compute needs of the models trained in parallel must fit the available memory and compute resourc",
    "path": "papers/22/09/2209.06018.json",
    "total_tokens": 1075,
    "translated_title": "GPU上深度学习训练的协同分析",
    "translated_abstract": "深度学习训练是一项昂贵的过程，极大地使用GPU，但并不是所有模型训练都能充分利用现代强大的GPU。多实例GPU（MIG）是NVIDIA引入的一项新技术，可以分区GPU以更好地适应不需要完整GPU内存和计算资源的工作负载。本文研究了在包含各种大小和模型组合的深度学习工作负载下，MIG启用的A100 GPU的性能。我们将MIG的优势与旧的GPU工作负载协同方法进行对比：在同一GPU上直接提交多个进程和使用多进程服务（MPS）。我们的结果表明，协同多个模型训练可以产生显着的效益。在某些情况下，即使epoch时间增加，训练吞吐量也可能增加了四倍。另一方面，同时训练的模型的聚合内存占用和计算需求必须符合GPU的可用内存和计算资源。我们还研究了MIG放置策略在分配模型训练资源时对性能的影响。我们的实验表明，具有优化放置策略的MIG一般优于所有其他方法。",
    "tldr": "本文通过研究GPU上深度学习训练中的协同分析，证明了协同多个模型训练可以产生显着的效益。MIG技术通过将GPU分区，更好地适应不需要完整GPU内存和计算资源的工作负载，具有优势，并且具有优化的放置策略的MIG一般优于所有其他方法。",
    "en_tdlr": "This paper analyzes collocation on GPUs for deep learning training and demonstrates that collocating multiple model training runs can yield significant benefits. The Multi-Instance GPU (MIG) technology introduced by NVIDIA, which can partition a GPU to better-fit workloads, has advantages and MIG with an optimized placement policy generally outperforms all other methods."
}