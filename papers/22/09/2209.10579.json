{
    "title": "First-order Policy Optimization for Robust Markov Decision Process. (arXiv:2209.10579v2 [cs.LG] UPDATED)",
    "abstract": "We consider the problem of solving robust Markov decision process (MDP), which involves a set of discounted, finite state, finite action space MDPs with uncertain transition kernels. The goal of planning is to find a robust policy that optimizes the worst-case values against the transition uncertainties, and thus encompasses the standard MDP planning as a special case. For $(\\mathbf{s},\\mathbf{a})$-rectangular uncertainty sets, we establish several structural observations on the robust objective, which facilitates the development of a policy-based first-order method, namely the robust policy mirror descent (RPMD). An $\\mathcal{O}(\\log(1/\\epsilon))$ iteration complexity for finding an $\\epsilon$-optimal policy is established with linearly increasing stepsizes. We further develop a stochastic variant of the robust policy mirror descent method, named SRPMD, when the first-order information is only available through online interactions with the nominal environment. We show that the optimal",
    "link": "http://arxiv.org/abs/2209.10579",
    "context": "Title: First-order Policy Optimization for Robust Markov Decision Process. (arXiv:2209.10579v2 [cs.LG] UPDATED)\nAbstract: We consider the problem of solving robust Markov decision process (MDP), which involves a set of discounted, finite state, finite action space MDPs with uncertain transition kernels. The goal of planning is to find a robust policy that optimizes the worst-case values against the transition uncertainties, and thus encompasses the standard MDP planning as a special case. For $(\\mathbf{s},\\mathbf{a})$-rectangular uncertainty sets, we establish several structural observations on the robust objective, which facilitates the development of a policy-based first-order method, namely the robust policy mirror descent (RPMD). An $\\mathcal{O}(\\log(1/\\epsilon))$ iteration complexity for finding an $\\epsilon$-optimal policy is established with linearly increasing stepsizes. We further develop a stochastic variant of the robust policy mirror descent method, named SRPMD, when the first-order information is only available through online interactions with the nominal environment. We show that the optimal",
    "path": "papers/22/09/2209.10579.json",
    "total_tokens": 1000,
    "translated_title": "保证鲁棒性马尔可夫决策过程的一阶策略优化",
    "translated_abstract": "本文考虑解决鲁棒性马尔可夫决策过程（MDP）问题，包括一组具有不确定转移核的折扣、有限状态、有限动作空间的MDP。规划的目标是找到一个鲁棒策略来优化对转移不确定性的最坏情况值，因此包括标准MDP规划作为一种特殊情况。对于$(\\mathbf{s},\\mathbf{a})$-矩形不确定集，本文建立了鲁棒目标的几个结构性观察，从而便于开发基于策略的一阶方法，即鲁棒策略镜像下降算法（RPMD）。使用线性递增的步长，建立了找到$\\epsilon$-最优策略的$\\mathcal{O}(\\log(1/\\epsilon))$迭代复杂度。当一阶信息仅通过与名义环境的在线交互获得时，我们进一步开发了鲁棒策略镜像下降方法的随机变体，即SRPMD。我们证明了最优策略的取得方式与基于一阶信息的方法是等价的。",
    "tldr": "本论文介绍了一种用于解决鲁棒性马尔科夫决策过程的一阶方法，称为鲁棒策略镜像下降算法（RPMD）。通过使用线性递增步进，算法具有较低的复杂度，并且能够在不确定情况下找到最优策略。",
    "en_tdlr": "This paper introduces a first-order method, called robust policy mirror descent (RPMD), for solving robust Markov decision processes. By using linearly increasing stepsizes, the algorithm has low complexity and can find an optimal policy in uncertain situations."
}