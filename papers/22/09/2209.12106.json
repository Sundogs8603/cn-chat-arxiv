{
    "title": "Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity. (arXiv:2209.12106v2 [cs.CL] UPDATED)",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases. This study investigates whether LLMs reproduce the moral biases associated with political groups in the United States, an instance of a broader capability herein termed moral mimicry. This hypothesis is explored in the GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral Foundations Theory, it is shown that these LLMs are indeed moral mimics. When prompted with a liberal or conservative political identity, the models generate text reflecting corresponding moral biases. This study also explores the relationship between moral mimicry and model size, and similarity between human and LLM moral word use.",
    "link": "http://arxiv.org/abs/2209.12106",
    "context": "Title: Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity. (arXiv:2209.12106v2 [cs.CL] UPDATED)\nAbstract: Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases. This study investigates whether LLMs reproduce the moral biases associated with political groups in the United States, an instance of a broader capability herein termed moral mimicry. This hypothesis is explored in the GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral Foundations Theory, it is shown that these LLMs are indeed moral mimics. When prompted with a liberal or conservative political identity, the models generate text reflecting corresponding moral biases. This study also explores the relationship between moral mimicry and model size, and similarity between human and LLM moral word use.",
    "path": "papers/22/09/2209.12106.json",
    "total_tokens": 765,
    "translated_title": "道德模仿：大型语言模型生成适应政治身份的道德辩护",
    "translated_abstract": "大型语言模型(LLMs)在生成流畅文本方面表现出惊人能力，但也倾向于重复不良社会偏见。本研究调查了LLMs是否会复制与美国政治团体相关的道德偏见，即所述的更广泛的道德模仿能力。该假设在基于Transformer的LLMs家族中的GPT-3 / 3.5和OPT中得到了探讨。使用道德基础理论工具，表明这些LLMs确实是道德模仿者。当以自由主义或保守主义政治身份为提示时，模型会生成反映相应道德偏见的文本。本研究还探讨了道德模仿与模型大小的关系，以及人类和LLM道德用语的相似性。",
    "tldr": "本研究证明大型语言模型表现出道德模仿能力，会根据政治身份生成反映相应道德偏见的文本。",
    "en_tdlr": "This study demonstrates that large language models exhibit moral mimicry, generating text reflecting corresponding moral biases according to political identity prompts."
}