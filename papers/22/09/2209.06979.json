{
    "title": "Efficient Quantized Sparse Matrix Operations on Tensor Cores. (arXiv:2209.06979v4 [cs.DC] UPDATED)",
    "abstract": "The exponentially growing model size drives the continued success of deep learning, but it brings prohibitive computation and memory cost. From the algorithm perspective, model sparsification and quantization have been studied to alleviate the problem. From the architecture perspective, hardware vendors provide Tensor cores for acceleration. However, it is very challenging to gain practical speedups from sparse, low-precision matrix operations on Tensor cores, because of the strict requirements for data layout and lack of support for efficiently manipulating the low-precision integers. We propose Magicube, a high-performance sparse-matrix library for low-precision integers on Tensor cores. Magicube supports SpMM and SDDMM, two major sparse operations in deep learning with mixed precision. Experimental results on an NVIDIA A100 GPU show that Magicube achieves on average 1.44x (up to 2.37x) speedup over the vendor-optimized library for sparse kernels, and 1.43x speedup over the state-of-",
    "link": "http://arxiv.org/abs/2209.06979",
    "context": "Title: Efficient Quantized Sparse Matrix Operations on Tensor Cores. (arXiv:2209.06979v4 [cs.DC] UPDATED)\nAbstract: The exponentially growing model size drives the continued success of deep learning, but it brings prohibitive computation and memory cost. From the algorithm perspective, model sparsification and quantization have been studied to alleviate the problem. From the architecture perspective, hardware vendors provide Tensor cores for acceleration. However, it is very challenging to gain practical speedups from sparse, low-precision matrix operations on Tensor cores, because of the strict requirements for data layout and lack of support for efficiently manipulating the low-precision integers. We propose Magicube, a high-performance sparse-matrix library for low-precision integers on Tensor cores. Magicube supports SpMM and SDDMM, two major sparse operations in deep learning with mixed precision. Experimental results on an NVIDIA A100 GPU show that Magicube achieves on average 1.44x (up to 2.37x) speedup over the vendor-optimized library for sparse kernels, and 1.43x speedup over the state-of-",
    "path": "papers/22/09/2209.06979.json",
    "total_tokens": 1010,
    "translated_title": "张量核上高效的量化稀疏矩阵计算",
    "translated_abstract": "深度学习不断取得成功，但模型大小的指数级增长带来了计算和内存成本的难题。为了缓解这个问题，模型的稀疏化和量化已经被研究。从硬件角度来看，硬件供应商为加速提供了张量核。然而，从低精度矩阵运算的角度来看，在张量核上实现稀疏，低精度矩阵操作非常具有挑战性，因为需要满足不同的数据布局需求和有效地操作低精度整数的不足。我们提出了Magicube，一个基于张量核的低精度整数的高性能稀疏矩阵库。Magicube支持混合精度下的SpMM和SDDMM，这是深度学习中两个主要的稀疏操作。在NVIDIA A100 GPU上的实验结果表明，Magicube相对于供应商优化库，平均获得1.44倍（最高2.37倍）速度提升，并且相对于状态-of-the-art的GPU库，速度提升了1.43倍。",
    "tldr": "本文提出了Magicube，一个基于张量核的低精度整数的高性能稀疏矩阵库，支持深度学习中的稀疏运算。在NVIDIA A100 GPU上实验表明，Magicube相对供应商优化库平均获得了1.44倍的速度提升，并且相对于状态-of-the-art的GPU库，速度提升了1.43倍。"
}