{
    "title": "Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement Learning in Unknown Stochastic Environments. (arXiv:2209.15090v3 [eess.SY] UPDATED)",
    "abstract": "It is quite challenging to ensure the safety of reinforcement learning (RL) agents in an unknown and stochastic environment under hard constraints that require the system state not to reach certain specified unsafe regions. Many popular safe RL methods such as those based on the Constrained Markov Decision Process (CMDP) paradigm formulate safety violations in a cost function and try to constrain the expectation of cumulative cost under a threshold. However, it is often difficult to effectively capture and enforce hard reachability-based safety constraints indirectly with such constraints on safety violation costs. In this work, we leverage the notion of barrier function to explicitly encode the hard safety constraints, and given that the environment is unknown, relax them to our design of \\emph{generative-model-based soft barrier functions}. Based on such soft barriers, we propose a safe RL approach that can jointly learn the environment and optimize the control policy, while effectiv",
    "link": "http://arxiv.org/abs/2209.15090",
    "context": "Title: Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement Learning in Unknown Stochastic Environments. (arXiv:2209.15090v3 [eess.SY] UPDATED)\nAbstract: It is quite challenging to ensure the safety of reinforcement learning (RL) agents in an unknown and stochastic environment under hard constraints that require the system state not to reach certain specified unsafe regions. Many popular safe RL methods such as those based on the Constrained Markov Decision Process (CMDP) paradigm formulate safety violations in a cost function and try to constrain the expectation of cumulative cost under a threshold. However, it is often difficult to effectively capture and enforce hard reachability-based safety constraints indirectly with such constraints on safety violation costs. In this work, we leverage the notion of barrier function to explicitly encode the hard safety constraints, and given that the environment is unknown, relax them to our design of \\emph{generative-model-based soft barrier functions}. Based on such soft barriers, we propose a safe RL approach that can jointly learn the environment and optimize the control policy, while effectiv",
    "path": "papers/22/09/2209.15090.json",
    "total_tokens": 895,
    "translated_title": "在未知随机环境中使用软障碍强制执行硬约束：安全强化学习",
    "translated_abstract": "在要求系统状态不到达某些指定的不安全区域的硬约束下，确保强化学习代理在未知和随机环境中的安全性是相当具有挑战性的。许多流行的安全强化学习方法，如基于约束马尔可夫决策过程（CMDP）范式的方法，将安全违规形式化为成本函数，并试图将累积成本的期望限制在阈值下。然而，通过这样对安全违规成本的限制，间接地捕捉和强制执行硬可达性安全约束通常很难。在这项工作中，我们利用障碍函数的概念来显式地编码硬安全约束，并在环境未知的情况下，将它们放松到我们设计的基于生成模型的软障碍函数中。基于这样的软障碍，我们提出了一种安全强化学习方法，可以同时学习环境和优化控制策略，同时有效地实施硬安全约束。",
    "tldr": "在未知环境下，通过软障碍函数强制实施硬安全约束，提出了一种安全强化学习方法，可以同时学习环境和优化控制策略。",
    "en_tdlr": "This paper proposes a safe reinforcement learning approach by enforcing hard safety constraints with soft barrier functions in unknown environments, enabling joint learning of the environment and control policy."
}