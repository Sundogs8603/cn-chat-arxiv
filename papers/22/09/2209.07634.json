{
    "title": "Stateful Memory-Augmented Transformers for Efficient Dialogue Modeling. (arXiv:2209.07634v2 [cs.CL] UPDATED)",
    "abstract": "Transformer encoder-decoder models have achieved great performance in dialogue generation tasks, however, their inability to process long dialogue history often leads to truncation of the context To address this problem, we propose a novel memory-augmented transformer that is compatible with existing pre-trained encoder-decoder models and enables efficient preservation of the dialogue history information. By incorporating a separate memory module alongside the pre-trained transformer, the model can effectively interchange information between the memory states and the current input context. We evaluate our model on three dialogue datasets and two language modeling datasets. Experimental results show that our method has achieved superior efficiency and performance compared to other pre-trained Transformer baselines.",
    "link": "http://arxiv.org/abs/2209.07634",
    "context": "Title: Stateful Memory-Augmented Transformers for Efficient Dialogue Modeling. (arXiv:2209.07634v2 [cs.CL] UPDATED)\nAbstract: Transformer encoder-decoder models have achieved great performance in dialogue generation tasks, however, their inability to process long dialogue history often leads to truncation of the context To address this problem, we propose a novel memory-augmented transformer that is compatible with existing pre-trained encoder-decoder models and enables efficient preservation of the dialogue history information. By incorporating a separate memory module alongside the pre-trained transformer, the model can effectively interchange information between the memory states and the current input context. We evaluate our model on three dialogue datasets and two language modeling datasets. Experimental results show that our method has achieved superior efficiency and performance compared to other pre-trained Transformer baselines.",
    "path": "papers/22/09/2209.07634.json",
    "total_tokens": 728,
    "translated_title": "用于高效对话建模的状态记忆增强变压器",
    "translated_abstract": "变压器编码器-解码器模型在对话生成任务中取得了很好的性能，然而它们无法处理长对话历史，常常导致上下文被截断。为了解决这个问题，我们提出了一种新颖的记忆增强变压器，与现有的预训练编码器-解码器模型兼容，可以有效地保存对话历史信息。通过将一个单独的记忆模块与预训练变压器相结合，这个模型可以有效地交换记忆状态和当前输入上下文之间的信息。我们在三个对话数据集和两个语言建模数据集上评估了我们的模型。实验结果表明，与其他预训练Transformer基线相比，我们的方法在效率和性能方面都具有优越性。",
    "tldr": "本文提出了一种记忆增强变压器，它可以高效地保存对话历史信息，并且在对话生成任务中表现出卓越的性能。",
    "en_tdlr": "This paper proposes a memory-augmented transformer that efficiently preserves dialogue history information and achieves superior performance in dialogue generation tasks."
}