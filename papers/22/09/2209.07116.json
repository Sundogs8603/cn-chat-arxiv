{
    "title": "On Generalization of Decentralized Learning with Separable Data. (arXiv:2209.07116v4 [cs.LG] UPDATED)",
    "abstract": "Decentralized learning offers privacy and communication efficiency when data are naturally distributed among agents communicating over an underlying graph. Motivated by overparameterized learning settings, in which models are trained to zero training loss, we study algorithmic and generalization properties of decentralized learning with gradient descent on separable data. Specifically, for decentralized gradient descent (DGD) and a variety of loss functions that asymptote to zero at infinity (including exponential and logistic losses), we derive novel finite-time generalization bounds. This complements a long line of recent work that studies the generalization performance and the implicit bias of gradient descent over separable data, but has thus far been limited to centralized learning scenarios. Notably, our generalization bounds approximately match in order their centralized counterparts. Critical behind this, and of independent interest, is establishing novel bounds on the training",
    "link": "http://arxiv.org/abs/2209.07116",
    "context": "Title: On Generalization of Decentralized Learning with Separable Data. (arXiv:2209.07116v4 [cs.LG] UPDATED)\nAbstract: Decentralized learning offers privacy and communication efficiency when data are naturally distributed among agents communicating over an underlying graph. Motivated by overparameterized learning settings, in which models are trained to zero training loss, we study algorithmic and generalization properties of decentralized learning with gradient descent on separable data. Specifically, for decentralized gradient descent (DGD) and a variety of loss functions that asymptote to zero at infinity (including exponential and logistic losses), we derive novel finite-time generalization bounds. This complements a long line of recent work that studies the generalization performance and the implicit bias of gradient descent over separable data, but has thus far been limited to centralized learning scenarios. Notably, our generalization bounds approximately match in order their centralized counterparts. Critical behind this, and of independent interest, is establishing novel bounds on the training",
    "path": "papers/22/09/2209.07116.json",
    "total_tokens": 870,
    "translated_title": "论分离数据的分布式学习的泛化",
    "translated_abstract": "当数据自然地分布在基础图上传递的代理之间时，分布式学习提供了隐私和通信效率。 在过度参数化的学习设置中，模型被训练为零训练损失。 本文研究了梯度下降分布式学习在分离数据上的算法和广义性质。 具体来说，对于分布式梯度下降（DGD）和一系列在无限远处渐近为零的损失函数（包括指数和对数损失），我们推导出了新颖的有限时间广义性能界限。 这补充了最近一系列研究梯度下降在分离数据上的广义性能和隐式偏差，但目前仅限于集中式学习场景的工作。值得注意的是，我们的泛化界限与其集中式对应物的大小近乎相等。 独立于此，本文还建立了关于DGD的新边界。",
    "tldr": "本文研究了梯度下降分布式学习在分离数据上的广义性质，并推导出新颖的有限时间广义性能界限，这对于当前仅限于集中式学习场景下的研究有所补充。"
}