{
    "title": "Cooperation in the Latent Space: The Benefits of Adding Mixture Components in Variational Autoencoders. (arXiv:2209.15514v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we show how the mixture components cooperate when they jointly adapt to maximize the ELBO. We build upon recent advances in the multiple and adaptive importance sampling literature. We then model the mixture components using separate encoder networks and show empirically that the ELBO is monotonically non-decreasing as a function of the number of mixture components. These results hold for a range of different VAE architectures on the MNIST, FashionMNIST, and CIFAR-10 datasets. In this work, we also demonstrate that increasing the number of mixture components improves the latent-representation capabilities of the VAE on both image and single-cell datasets. This cooperative behavior motivates that using Mixture VAEs should be considered a standard approach for obtaining more flexible variational approximations. Finally, Mixture VAEs are here, for the first time, compared and combined with normalizing flows, hierarchical models and/or the VampPrior in an extensive ablation ",
    "link": "http://arxiv.org/abs/2209.15514",
    "context": "Title: Cooperation in the Latent Space: The Benefits of Adding Mixture Components in Variational Autoencoders. (arXiv:2209.15514v2 [cs.LG] UPDATED)\nAbstract: In this paper, we show how the mixture components cooperate when they jointly adapt to maximize the ELBO. We build upon recent advances in the multiple and adaptive importance sampling literature. We then model the mixture components using separate encoder networks and show empirically that the ELBO is monotonically non-decreasing as a function of the number of mixture components. These results hold for a range of different VAE architectures on the MNIST, FashionMNIST, and CIFAR-10 datasets. In this work, we also demonstrate that increasing the number of mixture components improves the latent-representation capabilities of the VAE on both image and single-cell datasets. This cooperative behavior motivates that using Mixture VAEs should be considered a standard approach for obtaining more flexible variational approximations. Finally, Mixture VAEs are here, for the first time, compared and combined with normalizing flows, hierarchical models and/or the VampPrior in an extensive ablation ",
    "path": "papers/22/09/2209.15514.json",
    "total_tokens": 919,
    "translated_title": "在潜在空间中的合作：在变分自编码器中添加混合成分的好处",
    "translated_abstract": "本文展示了混合成分在共同适应最大化ELBO时的合作方式。我们借鉴了最近在多个和自适应重要性采样文献中的进展。我们使用单独的编码器网络对混合成分进行建模，并在实证上证明ELBO随混合成分数量的增加是单调非减的。这些结果适用于MNIST、FashionMNIST和CIFAR-10数据集上的不同VAE架构。本工作还表明增加混合成分的数量能够改善VAE在图像和单细胞数据集上的潜在表示能力。这种合作行为表明，使用混合VAE应被视为获取更灵活的变分近似的标准方法。最后，我们首次在大范围的消融实验中将混合VAE与归一化流、层次模型和/或VampPrior进行了比较和结合。",
    "tldr": "本研究展示了在变分自编码器中添加混合成分的好处，并证明了混合成分的增加能够提高其在图像和单细胞数据集上的潜在表示能力。这表明使用混合VAE是获取更灵活变分逼近的标准方法。",
    "en_tdlr": "This study demonstrates the benefits of adding mixture components in variational autoencoders and shows that increasing the number of mixture components improves their latent-representation capabilities on both image and single-cell datasets. This suggests that using Mixture VAEs is a standard approach for obtaining more flexible variational approximations."
}