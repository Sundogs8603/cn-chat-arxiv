{
    "title": "The Geometry and Calculus of Losses. (arXiv:2209.00238v2 [cs.LG] UPDATED)",
    "abstract": "Statistical decision problems lie at the heart of statistical machine learning. The simplest problems are binary and multiclass classification and class probability estimation. Central to their definition is the choice of loss function, which is the means by which the quality of a solution is evaluated. In this paper we systematically develop the theory of loss functions for such problems from a novel perspective whose basic ingredients are convex sets with a particular structure. The loss function is defined as the subgradient of the support function of the convex set. It is consequently automatically proper (calibrated for probability estimation). This perspective provides three novel opportunities. It enables the development of a fundamental relationship between losses and (anti)-norms that appears to have not been noticed before. Second, it enables the development of a calculus of losses induced by the calculus of convex sets which allows the interpolation between different losses,",
    "link": "http://arxiv.org/abs/2209.00238",
    "context": "Title: The Geometry and Calculus of Losses. (arXiv:2209.00238v2 [cs.LG] UPDATED)\nAbstract: Statistical decision problems lie at the heart of statistical machine learning. The simplest problems are binary and multiclass classification and class probability estimation. Central to their definition is the choice of loss function, which is the means by which the quality of a solution is evaluated. In this paper we systematically develop the theory of loss functions for such problems from a novel perspective whose basic ingredients are convex sets with a particular structure. The loss function is defined as the subgradient of the support function of the convex set. It is consequently automatically proper (calibrated for probability estimation). This perspective provides three novel opportunities. It enables the development of a fundamental relationship between losses and (anti)-norms that appears to have not been noticed before. Second, it enables the development of a calculus of losses induced by the calculus of convex sets which allows the interpolation between different losses,",
    "path": "papers/22/09/2209.00238.json",
    "total_tokens": 845,
    "translated_title": "损失的几何和微积分",
    "translated_abstract": "统计决策问题是统计机器学习的核心。最简单的问题是二元和多类分类以及类概率估计。它们的定义的核心是选择损失函数，这是评估解决方案质量的手段。在本文中，我们从一种新颖的角度系统地发展了这类问题的损失函数理论，其基本要素是具有特定结构的凸集。损失函数被定义为凸集的支撑函数的次梯度。因此，它自动是合适的（用于概率估计）。这种视角提供了三个新颖的机会。它使得损失和(反)范数之间的基本关系的发展成为可能，这似乎以前没有被注意到。其次，它通过凸集的微积分使得损失的微积分的发展成为可能，从而允许在不同的损失之间进行插值。",
    "tldr": "本文从凸集的新角度系统地发展了损失函数理论，引入了一种自动合适的损失函数定义方法，并提供了损失和范数之间关系的新机会，以及凸集微积分下的损失插值方法。"
}