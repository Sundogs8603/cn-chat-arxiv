{
    "title": "Reduction from Complementary-Label Learning to Probability Estimates. (arXiv:2209.09500v2 [cs.LG] UPDATED)",
    "abstract": "Complementary-Label Learning (CLL) is a weakly-supervised learning problem that aims to learn a multi-class classifier from only complementary labels, which indicate a class to which an instance does not belong. Existing approaches mainly adopt the paradigm of reduction to ordinary classification, which applies specific transformations and surrogate losses to connect CLL back to ordinary classification. Those approaches, however, face several limitations, such as the tendency to overfit or be hooked on deep models. In this paper, we sidestep those limitations with a novel perspective--reduction to probability estimates of complementary classes. We prove that accurate probability estimates of complementary labels lead to good classifiers through a simple decoding step. The proof establishes a reduction framework from CLL to probability estimates. The framework offers explanations of several key CLL approaches as its special cases and allows us to design an improved algorithm that is mor",
    "link": "http://arxiv.org/abs/2209.09500",
    "context": "Title: Reduction from Complementary-Label Learning to Probability Estimates. (arXiv:2209.09500v2 [cs.LG] UPDATED)\nAbstract: Complementary-Label Learning (CLL) is a weakly-supervised learning problem that aims to learn a multi-class classifier from only complementary labels, which indicate a class to which an instance does not belong. Existing approaches mainly adopt the paradigm of reduction to ordinary classification, which applies specific transformations and surrogate losses to connect CLL back to ordinary classification. Those approaches, however, face several limitations, such as the tendency to overfit or be hooked on deep models. In this paper, we sidestep those limitations with a novel perspective--reduction to probability estimates of complementary classes. We prove that accurate probability estimates of complementary labels lead to good classifiers through a simple decoding step. The proof establishes a reduction framework from CLL to probability estimates. The framework offers explanations of several key CLL approaches as its special cases and allows us to design an improved algorithm that is mor",
    "path": "papers/22/09/2209.09500.json",
    "total_tokens": 902,
    "translated_title": "从互补标签学习到概率估计的简化",
    "translated_abstract": "互补标签学习是一个弱监督学习问题，旨在仅从互补标签中学习多类分类器，其指示实例不属于的类。现有方法主要采用减少到普通分类的范例，对CLL进行特定转换和替代损失。然而，这些方法面临着几个限制，例如过拟合的倾向或深度模型的偏见。在本文中，我们通过一种新颖的视角-减少到互补类的概率估计，规避了这些限制。我们证明了准确的互补标签概率估计通过简单的解码步骤产生良好的分类器。该证明建立了一个从CLL到概率估计的简化框架。该框架提供了解释几个关键CLL方法的特殊情况的方法，并允许我们设计一种改进的算法，这种算法在某些情况下比现有的CLL方法更具优越性。",
    "tldr": "本论文提出了一种新的对互补标签学习的简化方法，它减少到互补类的概率估计，通过一个简单的解码步骤，准确的互补标签概率估计可以产生良好的分类器。",
    "en_tdlr": "This paper proposes a novel simplification method for Complementary-Label Learning (CLL) by reducing it to probability estimates, which leads to good classifiers through a simple decoding step. Accurate probability estimates of complementary labels are proven to be effective, and the proposed framework offers a better algorithm than existing approaches."
}