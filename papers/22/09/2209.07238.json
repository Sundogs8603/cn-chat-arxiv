{
    "title": "Generalization Properties of NAS under Activation and Skip Connection Search. (arXiv:2209.07238v4 [cs.LG] UPDATED)",
    "abstract": "Neural Architecture Search (NAS) has fostered the automatic discovery of state-of-the-art neural architectures. Despite the progress achieved with NAS, so far there is little attention to theoretical guarantees on NAS. In this work, we study the generalization properties of NAS under a unifying framework enabling (deep) layer skip connection search and activation function search. To this end, we derive the lower (and upper) bounds of the minimum eigenvalue of the Neural Tangent Kernel (NTK) under the (in)finite-width regime using a certain search space including mixed activation functions, fully connected, and residual neural networks. We use the minimum eigenvalue to establish generalization error bounds of NAS in the stochastic gradient descent training. Importantly, we theoretically and experimentally show how the derived results can guide NAS to select the top-performing architectures, even in the case without training, leading to a train-free algorithm based on our theory. Accordi",
    "link": "http://arxiv.org/abs/2209.07238",
    "context": "Title: Generalization Properties of NAS under Activation and Skip Connection Search. (arXiv:2209.07238v4 [cs.LG] UPDATED)\nAbstract: Neural Architecture Search (NAS) has fostered the automatic discovery of state-of-the-art neural architectures. Despite the progress achieved with NAS, so far there is little attention to theoretical guarantees on NAS. In this work, we study the generalization properties of NAS under a unifying framework enabling (deep) layer skip connection search and activation function search. To this end, we derive the lower (and upper) bounds of the minimum eigenvalue of the Neural Tangent Kernel (NTK) under the (in)finite-width regime using a certain search space including mixed activation functions, fully connected, and residual neural networks. We use the minimum eigenvalue to establish generalization error bounds of NAS in the stochastic gradient descent training. Importantly, we theoretically and experimentally show how the derived results can guide NAS to select the top-performing architectures, even in the case without training, leading to a train-free algorithm based on our theory. Accordi",
    "path": "papers/22/09/2209.07238.json",
    "total_tokens": 871,
    "translated_title": "NAS在激活和跳跃连接搜索下的泛化性质",
    "translated_abstract": "神经网络结构搜索（NAS）促进了自动发现最先进的神经网络结构。尽管NAS取得了一些进展，但对于NAS的理论保证关注甚少。本文研究了NAS在统一框架下的泛化性质，包括了（深层）层级跳跃连接搜索和激活函数搜索。为此，我们利用包括混合激活函数、全连接和残差神经网络在内的特定搜索空间，推导出（无）限宽度情况下神经切向核（NTK）的最小特征值的下（上）界。我们使用最小特征值来建立NAS在随机梯度下降训练中的泛化误差界限。重要的是，我们理论上和实验上展示了如何根据我们推导出的结果引导NAS选择性能最好的架构，即使在无需训练的情况下，这是一种基于我们的理论的无需训练的算法。",
    "tldr": "本文研究了NAS在激活和跳跃连接搜索下的泛化性质，并提出了一种无需训练的基于理论的算法来选择性能最好的架构。",
    "en_tdlr": "This paper investigates the generalization properties of neural architecture search (NAS) under activation and skip connection search. It proposes a train-free algorithm based on theory to select the top-performing architectures."
}