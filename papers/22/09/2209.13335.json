{
    "title": "PROD: Progressive Distillation for Dense Retrieval. (arXiv:2209.13335v3 [cs.IR] UPDATED)",
    "abstract": "Knowledge distillation is an effective way to transfer knowledge from a strong teacher to an efficient student model. Ideally, we expect the better the teacher is, the better the student. However, this expectation does not always come true. It is common that a better teacher model results in a bad student via distillation due to the nonnegligible gap between teacher and student. To bridge the gap, we propose PROD, a PROgressive Distillation method, for dense retrieval. PROD consists of a teacher progressive distillation and a data progressive distillation to gradually improve the student. We conduct extensive experiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19, TREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves the state-of-the-art within the distillation methods for dense retrieval. The code and models will be released.",
    "link": "http://arxiv.org/abs/2209.13335",
    "context": "Title: PROD: Progressive Distillation for Dense Retrieval. (arXiv:2209.13335v3 [cs.IR] UPDATED)\nAbstract: Knowledge distillation is an effective way to transfer knowledge from a strong teacher to an efficient student model. Ideally, we expect the better the teacher is, the better the student. However, this expectation does not always come true. It is common that a better teacher model results in a bad student via distillation due to the nonnegligible gap between teacher and student. To bridge the gap, we propose PROD, a PROgressive Distillation method, for dense retrieval. PROD consists of a teacher progressive distillation and a data progressive distillation to gradually improve the student. We conduct extensive experiments on five widely-used benchmarks, MS MARCO Passage, TREC Passage 19, TREC Document 19, MS MARCO Document and Natural Questions, where PROD achieves the state-of-the-art within the distillation methods for dense retrieval. The code and models will be released.",
    "path": "papers/22/09/2209.13335.json",
    "total_tokens": 841,
    "translated_title": "PROD：渐进式蒸馏用于密集检索",
    "translated_abstract": "知识蒸馏是将强教师的知识传递给高效学生模型的有效方法。然而，通常情况下预期的更好的教师会导致经过蒸馏后学生更糟。为了填补这一差距，本文提出了一种用于密集检索的PROgressive Distillation (PROD)方法，包括教师渐进式蒸馏和数据渐进式蒸馏两个阶段，从而逐步提高学生的检索绩效。在五个被广泛使用的基准数据集（MS MARCO Passage、TREC Passage 19、TREC Document 19、MS MARCO Document和自然问题）上进行了大量实验验证，PROD在密集检索的蒸馏方法中表现出最先进的性能。代码和模型将会发布。",
    "tldr": "本文提出了一种渐进式蒸馏方法PROD，用于密集检索，通过逐步改进学生模型来填补教师和学生之间的差距，并在五个基准数据集上取得了最先进的性能。",
    "en_tdlr": "This paper proposes a progressive distillation method, PROD, for dense retrieval, which gradually improves the student model to bridge the gap between teacher and student via a teacher progressive distillation and a data progressive distillation. PROD achieves state-of-the-art performance among distillation methods for dense retrieval on five widely-used benchmarks."
}