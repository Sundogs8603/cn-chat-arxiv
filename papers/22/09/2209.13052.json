{
    "title": "Training Efficient Controllers via Analytic Policy Gradient. (arXiv:2209.13052v3 [cs.RO] UPDATED)",
    "abstract": "Control design for robotic systems is complex and often requires solving an optimization to follow a trajectory accurately. Online optimization approaches like Model Predictive Control (MPC) have been shown to achieve great tracking performance, but require high computing power. Conversely, learning-based offline optimization approaches, such as Reinforcement Learning (RL), allow fast and efficient execution on the robot but hardly match the accuracy of MPC in trajectory tracking tasks. In systems with limited compute, such as aerial vehicles, an accurate controller that is efficient at execution time is imperative. We propose an Analytic Policy Gradient (APG) method to tackle this problem. APG exploits the availability of differentiable simulators by training a controller offline with gradient descent on the tracking error. We address training instabilities that frequently occur with APG through curriculum learning and experiment on a widely used controls benchmark, the CartPole, and ",
    "link": "http://arxiv.org/abs/2209.13052",
    "context": "Title: Training Efficient Controllers via Analytic Policy Gradient. (arXiv:2209.13052v3 [cs.RO] UPDATED)\nAbstract: Control design for robotic systems is complex and often requires solving an optimization to follow a trajectory accurately. Online optimization approaches like Model Predictive Control (MPC) have been shown to achieve great tracking performance, but require high computing power. Conversely, learning-based offline optimization approaches, such as Reinforcement Learning (RL), allow fast and efficient execution on the robot but hardly match the accuracy of MPC in trajectory tracking tasks. In systems with limited compute, such as aerial vehicles, an accurate controller that is efficient at execution time is imperative. We propose an Analytic Policy Gradient (APG) method to tackle this problem. APG exploits the availability of differentiable simulators by training a controller offline with gradient descent on the tracking error. We address training instabilities that frequently occur with APG through curriculum learning and experiment on a widely used controls benchmark, the CartPole, and ",
    "path": "papers/22/09/2209.13052.json",
    "total_tokens": 929,
    "translated_title": "通过解析策略梯度训练高效控制器",
    "translated_abstract": "机器人系统的控制设计很复杂，通常需要解决一个优化问题以准确地跟踪轨迹。虽然在线优化方法如模型预测控制（MPC）已经证明可以实现很好的跟踪性能，但需要高算力。相反，基于学习的离线优化方法，如强化学习（RL），允许机器人快速，高效地执行，但在轨迹跟踪任务中很难与MPC的准确性相匹配。对于算力有限的系统（如空中飞行器），高效的控制器的准确性至关重要。我们提出了一种解析策略梯度（APG）方法来解决这个问题。APG利用可微分仿真器的可用性，通过在跟踪误差上使用梯度下降算法离线训练控制器。我们通过课程学习来解决APG经常出现的训练不稳定性，并在广泛使用的控制基准测试CartPole上进行了实验。",
    "tldr": "本文提出了一种名为“解析策略梯度（APG）”的离线学习控制器方法，在跟踪误差上使用梯度下降算法，通过可微分仿真器离线训练控制器。该方法可在算力有限的系统上实现高效，精确的控制。",
    "en_tdlr": "This paper proposes an Analytic Policy Gradient (APG) method for training a controller offline using gradient descent on tracking error, exploiting the availability of differentiable simulators, which achieves high accuracy and efficiency in computation-limited systems."
}