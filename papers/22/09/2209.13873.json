{
    "title": "InFi: End-to-End Learning to Filter Input for Resource-Efficiency in Mobile-Centric Inference. (arXiv:2209.13873v2 [cs.AI] UPDATED)",
    "abstract": "Mobile-centric AI applications have high requirements for resource-efficiency of model inference. Input filtering is a promising approach to eliminate the redundancy so as to reduce the cost of inference. Previous efforts have tailored effective solutions for many applications, but left two essential questions unanswered: (1) theoretical filterability of an inference workload to guide the application of input filtering techniques, thereby avoiding the trial-and-error cost for resource-constrained mobile applications; (2) robust discriminability of feature embedding to allow input filtering to be widely effective for diverse inference tasks and input content. To answer them, we first formalize the input filtering problem and theoretically compare the hypothesis complexity of inference models and input filters to understand the optimization potential. Then we propose the first end-to-end learnable input filtering framework that covers most state-of-the-art methods and surpasses them in f",
    "link": "http://arxiv.org/abs/2209.13873",
    "context": "Title: InFi: End-to-End Learning to Filter Input for Resource-Efficiency in Mobile-Centric Inference. (arXiv:2209.13873v2 [cs.AI] UPDATED)\nAbstract: Mobile-centric AI applications have high requirements for resource-efficiency of model inference. Input filtering is a promising approach to eliminate the redundancy so as to reduce the cost of inference. Previous efforts have tailored effective solutions for many applications, but left two essential questions unanswered: (1) theoretical filterability of an inference workload to guide the application of input filtering techniques, thereby avoiding the trial-and-error cost for resource-constrained mobile applications; (2) robust discriminability of feature embedding to allow input filtering to be widely effective for diverse inference tasks and input content. To answer them, we first formalize the input filtering problem and theoretically compare the hypothesis complexity of inference models and input filters to understand the optimization potential. Then we propose the first end-to-end learnable input filtering framework that covers most state-of-the-art methods and surpasses them in f",
    "path": "papers/22/09/2209.13873.json",
    "total_tokens": 953,
    "translated_title": "InFi：移动端推理的资源高效性学习过程中的端到端输入过滤",
    "translated_abstract": "移动端AI应用对模型推理的资源高效性有很高的要求。输入过滤是一种有前途的方法，可以消除冗余，从而降低推理成本。以往的研究已经为许多应用程序量身定制了有效的解决方案，但留下了两个基本问题未解答：（1）推理工作量的理论可过滤性，以指导输入过滤技术的应用，从而避免资源受限的移动应用程序的试错成本；（2）特征嵌入的鲁棒性区分度，以使输入过滤对多样化推理任务和输入内容普遍有效。为了回答这些问题，我们首先形式化输入过滤问题，并在理论上比较推理模型和输入过滤器的假设复杂性，以了解优化潜力。然后我们提出了第一个端到端可学习的输入过滤框架，涵盖了大多数最先进的方法，并在f值、推理速度和内存占用方面超越了它们。",
    "tldr": "本研究提出了一个端到端可学习的输入过滤框架，通过对推理模型和输入过滤器的假设复杂性进行理论比较，从而了解优化潜力。该框架减少冗余，降低推理成本，并在f值、推理速度和内存占用方面超越其他方法。"
}