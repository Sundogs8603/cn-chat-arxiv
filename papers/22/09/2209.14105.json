{
    "title": "Exploring the Relationship between Architecture and Adversarially Robust Generalization. (arXiv:2209.14105v2 [cs.LG] UPDATED)",
    "abstract": "Adversarial training has been demonstrated to be one of the most effective remedies for defending adversarial examples, yet it often suffers from the huge robustness generalization gap on unseen testing adversaries, deemed as the adversarially robust generalization problem. Despite the preliminary understandings devoted to adversarially robust generalization, little is known from the architectural perspective. To bridge the gap, this paper for the first time systematically investigated the relationship between adversarially robust generalization and architectural design. Inparticular, we comprehensively evaluated 20 most representative adversarially trained architectures on ImageNette and CIFAR-10 datasets towards multiple `p-norm adversarial attacks. Based on the extensive experiments, we found that, under aligned settings, Vision Transformers (e.g., PVT, CoAtNet) often yield better adversarially robust generalization while CNNs tend to overfit on specific attacks and fail to generali",
    "link": "http://arxiv.org/abs/2209.14105",
    "raw_ret": "{\n    \"translated_title\": \"探究架构与对抗性鲁棒性泛化之间的关系\",\n    \"translated_abstract\": \"对抗训练已被证明是捍卫对抗性样本最有效的方法之一，但它经常面临着在未见过的测试对手方面的巨大鲁棒性广义差距，被视为对抗性鲁棒性泛化问题。尽管对于对抗性鲁棒性泛化的初步理解已有所贡献，但从架构角度了解甚少。为了弥合这一差距，本文首次系统地研究了对抗性鲁棒性泛化与架构设计之间的关系。特别地，我们对ImageNette和CIFAR-10数据集上最具代表性的20种对抗性训练架构进行了全面评估，针对多种p-范数对抗性攻击。基于广泛的实验，我们发现，在对齐设置下，Vision Transformers（例如PVT，CoAtNet）通常能够产生更好的对抗性鲁棒性泛化，而CNN则倾向于在特定攻击上过拟合且无法泛化\",\n    \"tldr\": \"本文首次系统地研究了对抗性鲁棒性泛化与架构设计之间的关系，发现在对齐设置下，Vision Transformers（例如PVT，CoAtNet）通常能够产生更好的对抗性鲁棒性泛化，而CNN则倾向于在特定攻击上过拟合且无法泛化。\"\n}<|im_sep|>",
    "total_tokens": 934,
    "ret": {
        "translated_title": "探究架构与对抗性鲁棒性泛化之间的关系",
        "translated_abstract": "对抗训练已被证明是捍卫对抗性样本最有效的方法之一，但它经常面临着在未见过的测试对手方面的巨大鲁棒性广义差距，被视为对抗性鲁棒性泛化问题。尽管对于对抗性鲁棒性泛化的初步理解已有所贡献，但从架构角度了解甚少。为了弥合这一差距，本文首次系统地研究了对抗性鲁棒性泛化与架构设计之间的关系。特别地，我们对ImageNette和CIFAR-10数据集上最具代表性的20种对抗性训练架构进行了全面评估，针对多种p-范数对抗性攻击。基于广泛的实验，我们发现，在对齐设置下，Vision Transformers（例如PVT，CoAtNet）通常能够产生更好的对抗性鲁棒性泛化，而CNN则倾向于在特定攻击上过拟合且无法泛化",
        "tldr": "本文首次系统地研究了对抗性鲁棒性泛化与架构设计之间的关系，发现在对齐设置下，Vision Transformers（例如PVT，CoAtNet）通常能够产生更好的对抗性鲁棒性泛化，而CNN则倾向于在特定攻击上过拟合且无法泛化。"
    }
}