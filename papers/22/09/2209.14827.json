{
    "title": "On the Convergence of AdaGrad on $\\R^{d}$: Beyond Convexity, Non-Asymptotic Rate and Acceleration. (arXiv:2209.14827v2 [cs.LG] UPDATED)",
    "abstract": "Existing analysis of AdaGrad and other adaptive methods for smooth convex optimization is typically for functions with bounded domain diameter. In unconstrained problems, previous works guarantee an asymptotic convergence rate without an explicit constant factor that holds true for the entire function class. Furthermore, in the stochastic setting, only a modified version of AdaGrad, different from the one commonly used in practice, in which the latest gradient is not used to update the stepsize, has been analyzed. Our paper aims at bridging these gaps and developing a deeper understanding of AdaGrad and its variants in the standard setting of smooth convex functions as well as the more general setting of quasar convex functions. First, we demonstrate new techniques to explicitly bound the convergence rate of the vanilla AdaGrad for unconstrained problems in both deterministic and stochastic settings. Second, we propose a variant of AdaGrad for which we can show the convergence of the l",
    "link": "http://arxiv.org/abs/2209.14827",
    "context": "Title: On the Convergence of AdaGrad on $\\R^{d}$: Beyond Convexity, Non-Asymptotic Rate and Acceleration. (arXiv:2209.14827v2 [cs.LG] UPDATED)\nAbstract: Existing analysis of AdaGrad and other adaptive methods for smooth convex optimization is typically for functions with bounded domain diameter. In unconstrained problems, previous works guarantee an asymptotic convergence rate without an explicit constant factor that holds true for the entire function class. Furthermore, in the stochastic setting, only a modified version of AdaGrad, different from the one commonly used in practice, in which the latest gradient is not used to update the stepsize, has been analyzed. Our paper aims at bridging these gaps and developing a deeper understanding of AdaGrad and its variants in the standard setting of smooth convex functions as well as the more general setting of quasar convex functions. First, we demonstrate new techniques to explicitly bound the convergence rate of the vanilla AdaGrad for unconstrained problems in both deterministic and stochastic settings. Second, we propose a variant of AdaGrad for which we can show the convergence of the l",
    "path": "papers/22/09/2209.14827.json",
    "total_tokens": 951,
    "translated_title": "论AdaGrad在$\\R^{d}$上的收敛性：超越凸性、非渐近速率和加速（arXiv：2209.14827v2 [cs.LG] UPDATED）",
    "translated_abstract": "现有的关于平滑凸优化的AdaGrad和其他自适应方法的分析通常是针对具有有界定义域直径的函数。在无约束问题中，以前的研究保证了渐近收敛速率，但没有明确的恒定因子，这适用于整个函数类。此外，在随机环境中，只分析了一个修改版本的AdaGrad，与通常实践中使用的版本不同，在这个回归中不使用最新的梯度来更新步幅。我们的论文旨在弥合这些差距，并在平滑凸函数的标准情况下以及更一般的quasar凸函数的情况下深入理解AdaGrad及其变种。首先，我们展示了新技术，明确地限定了vanilla AdaGrad在无约束问题中的收敛速率，无论是确定性的还是随机的情况下。其次，我们提出了一种AdaGrad变种，我们可以展示l的收敛",
    "tldr": "本论文主要展示了AdaGrad在平滑凸函数和更一般的quasar凸函数的情况下的收敛性。具体地，我们提出了新的技术，明确限定了vanilla AdaGrad在无约束问题中的收敛速率，并提出了一种AdaGrad变种，可以实现更快的收敛。",
    "en_tdlr": "This paper mainly demonstrates the convergence of AdaGrad in the case of smooth convex functions and more general quasar convex functions. Specifically, we propose new techniques to explicitly bound the convergence rate of vanilla AdaGrad in unconstrained problems and present a variant of AdaGrad that achieves faster convergence."
}