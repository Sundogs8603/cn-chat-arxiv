{
    "title": "Learning from Symmetry: Meta-Reinforcement Learning with Symmetrical Behaviors and Language Instructions. (arXiv:2209.10656v2 [cs.AI] UPDATED)",
    "abstract": "Meta-reinforcement learning (meta-RL) is a promising approach that enables the agent to learn new tasks quickly. However, most meta-RL algorithms show poor generalization in multi-task scenarios due to the insufficient task information provided only by rewards. Language-conditioned meta-RL improves the generalization capability by matching language instructions with the agent's behaviors. While both behaviors and language instructions have symmetry, which can speed up human learning of new knowledge. Thus, combining symmetry and language instructions into meta-RL can help improve the algorithm's generalization and learning efficiency. We propose a dual-MDP meta-reinforcement learning method that enables learning new tasks efficiently with symmetrical behaviors and language instructions. We evaluate our method in multiple challenging manipulation tasks, and experimental results show that our method can greatly improve the generalization and learning efficiency of meta-reinforcement lear",
    "link": "http://arxiv.org/abs/2209.10656",
    "context": "Title: Learning from Symmetry: Meta-Reinforcement Learning with Symmetrical Behaviors and Language Instructions. (arXiv:2209.10656v2 [cs.AI] UPDATED)\nAbstract: Meta-reinforcement learning (meta-RL) is a promising approach that enables the agent to learn new tasks quickly. However, most meta-RL algorithms show poor generalization in multi-task scenarios due to the insufficient task information provided only by rewards. Language-conditioned meta-RL improves the generalization capability by matching language instructions with the agent's behaviors. While both behaviors and language instructions have symmetry, which can speed up human learning of new knowledge. Thus, combining symmetry and language instructions into meta-RL can help improve the algorithm's generalization and learning efficiency. We propose a dual-MDP meta-reinforcement learning method that enables learning new tasks efficiently with symmetrical behaviors and language instructions. We evaluate our method in multiple challenging manipulation tasks, and experimental results show that our method can greatly improve the generalization and learning efficiency of meta-reinforcement lear",
    "path": "papers/22/09/2209.10656.json",
    "total_tokens": 906,
    "translated_title": "从对称中学习：具有对称行为和语言指令的元强化学习",
    "translated_abstract": "元强化学习是一种有望能够使代理快速学习新任务的方法。然而，由于只有奖励提供的任务信息不足，大多数元强化学习算法在多任务场景中表现出很差的泛化能力。以语言为条件的元强化学习通过将语言指令与代理的行为进行匹配，提高了泛化能力。行为和语言指令都具有对称性，这可以加速人类对新知识的学习。因此，将对称性和语言指令结合到元强化学习中可以改善算法的泛化能力和学习效率。我们提出了一种双马尔可夫决策过程元强化学习方法，通过对称行为和语言指令，能够有效地学习新任务。我们在多个具有挑战性的操作任务中评估了我们的方法，实验结果表明我们的方法能够极大地改善元强化学习的泛化能力和学习效率。",
    "tldr": "这篇论文研究了元强化学习中结合对称行为和语言指令的方法，以提高算法的泛化能力和学习效率。实验证明该方法能够极大地改善元强化学习的泛化能力和学习效率。"
}