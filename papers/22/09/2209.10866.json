{
    "title": "A One-shot Framework for Distributed Clustered Learning in Heterogeneous Environments. (arXiv:2209.10866v4 [cs.LG] UPDATED)",
    "abstract": "The paper proposes a family of communication efficient methods for distributed learning in heterogeneous environments in which users obtain data from one of $K$ different distributions. In the proposed setup, the grouping of users (based on the data distributions they sample), as well as the underlying statistical properties of the distributions, are apriori unknown. A family of One-shot Distributed Clustered Learning methods (ODCL-$\\mathcal{C}$) is proposed, parametrized by the set of admissible clustering algorithms $\\mathcal{C}$, with the objective of learning the true model at each user. The admissible clustering methods include $K$-means (KM) and convex clustering (CC), giving rise to various one-shot methods within the proposed family, such as ODCL-KM and ODCL-CC. The proposed one-shot approach, based on local computations at the users and a clustering based aggregation step at the server is shown to provide strong learning guarantees. In particular, for strongly convex problems ",
    "link": "http://arxiv.org/abs/2209.10866",
    "context": "Title: A One-shot Framework for Distributed Clustered Learning in Heterogeneous Environments. (arXiv:2209.10866v4 [cs.LG] UPDATED)\nAbstract: The paper proposes a family of communication efficient methods for distributed learning in heterogeneous environments in which users obtain data from one of $K$ different distributions. In the proposed setup, the grouping of users (based on the data distributions they sample), as well as the underlying statistical properties of the distributions, are apriori unknown. A family of One-shot Distributed Clustered Learning methods (ODCL-$\\mathcal{C}$) is proposed, parametrized by the set of admissible clustering algorithms $\\mathcal{C}$, with the objective of learning the true model at each user. The admissible clustering methods include $K$-means (KM) and convex clustering (CC), giving rise to various one-shot methods within the proposed family, such as ODCL-KM and ODCL-CC. The proposed one-shot approach, based on local computations at the users and a clustering based aggregation step at the server is shown to provide strong learning guarantees. In particular, for strongly convex problems ",
    "path": "papers/22/09/2209.10866.json",
    "total_tokens": 911,
    "translated_title": "异构环境下分布式聚类学习的一次性框架",
    "translated_abstract": "本文提出了一种通信效率高的方法族，用于解决在用户从$K$个不同分布中获取数据的异构环境中进行分布式学习的问题。在所提出的设置中，用户的分组（基于他们采样的数据分布）以及分布的统计属性是先验未知的。提出了一系列基于集合可接受的聚类算法$\\mathcal{C}$参数化的一次性分布式聚类学习方法（ODCL-$\\mathcal{C}$），其目标是在每个用户处学习真实模型。可接受的聚类方法包括$K$均值（KM）和凸聚类（CC），从而产生了所提出的各种一次性方法，如ODCL-KM和ODCL-CC。所提出的一次性方法，基于用户的本地计算和服务器上基于聚类的聚合步骤，被证明能够提供强大的学习保证。特别是，对于强凸问题，",
    "tldr": "本文提出了一种异构环境下分布式聚类学习的通信效率高的一次性方法族，通过局部计算和聚类聚合步骤，在每个用户处学习出真实模型，具有强大的学习保证。",
    "en_tdlr": "This paper proposes a family of communication efficient one-shot methods for distributed learning in heterogeneous environments, using local computations at the users and a clustering based aggregation step at the server to learn the true model at each user. The proposed methods exhibit strong learning guarantees for strongly convex problems."
}