{
    "title": "Learning Decoupled Retrieval Representation for Nearest Neighbour Neural Machine Translation. (arXiv:2209.08738v3 [cs.CL] UPDATED)",
    "abstract": "K-Nearest Neighbor Neural Machine Translation (kNN-MT) successfully incorporates external corpus by retrieving word-level representations at test time. Generally, kNN-MT borrows the off-the-shelf context representation in the translation task, e.g., the output of the last decoder layer, as the query vector of the retrieval task. In this work, we highlight that coupling the representations of these two tasks is sub-optimal for fine-grained retrieval. To alleviate it, we leverage supervised contrastive learning to learn the distinctive retrieval representation derived from the original context representation. We also propose a fast and effective approach to constructing hard negative samples. Experimental results on five domains show that our approach improves the retrieval accuracy and BLEU score compared to vanilla kNN-MT.",
    "link": "http://arxiv.org/abs/2209.08738",
    "context": "Title: Learning Decoupled Retrieval Representation for Nearest Neighbour Neural Machine Translation. (arXiv:2209.08738v3 [cs.CL] UPDATED)\nAbstract: K-Nearest Neighbor Neural Machine Translation (kNN-MT) successfully incorporates external corpus by retrieving word-level representations at test time. Generally, kNN-MT borrows the off-the-shelf context representation in the translation task, e.g., the output of the last decoder layer, as the query vector of the retrieval task. In this work, we highlight that coupling the representations of these two tasks is sub-optimal for fine-grained retrieval. To alleviate it, we leverage supervised contrastive learning to learn the distinctive retrieval representation derived from the original context representation. We also propose a fast and effective approach to constructing hard negative samples. Experimental results on five domains show that our approach improves the retrieval accuracy and BLEU score compared to vanilla kNN-MT.",
    "path": "papers/22/09/2209.08738.json",
    "total_tokens": 799,
    "translated_title": "学习解耦的检索表示用于最近邻神经机器翻译",
    "translated_abstract": "K-最近邻神经机器翻译（kNN-MT）成功地在测试时通过检索单词级别的表示来引入外部语料库。通常，kNN-MT借用翻译任务中现成的上下文表示（例如最后一个解码器层的输出）作为检索任务的查询向量。在本研究中，我们强调将这两个任务的表示耦合对于细粒度的检索是次优的。为了缓解这个问题，我们利用监督对比学习来学习从原始上下文表示派生的独特检索表示。我们还提出了一种快速有效的构建难负样本的方法。在五个领域的实验结果表明，与原始kNN-MT相比，我们的方法提高了检索准确性和BLEU分数。",
    "tldr": "本文提出了一种学习解耦的检索表示用于最近邻神经机器翻译，通过使用监督对比学习和构建难负样本，改进了检索准确性和BLEU分数。"
}