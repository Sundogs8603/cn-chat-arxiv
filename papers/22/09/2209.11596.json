{
    "title": "Quantification before Selection: Active Dynamics Preference for Robust Reinforcement Learning. (arXiv:2209.11596v3 [cs.LG] UPDATED)",
    "abstract": "Training a robust policy is critical for policy deployment in real-world systems or dealing with unknown dynamics mismatch in different dynamic systems. Domain Randomization~(DR) is a simple and elegant approach that trains a conservative policy to counter different dynamic systems without expert knowledge about the target system parameters. However, existing works reveal that the policy trained through DR tends to be over-conservative and performs poorly in target domains. Our key insight is that dynamic systems with different parameters provide different levels of difficulty for the policy, and the difficulty of behaving well in a system is constantly changing due to the evolution of the policy. If we can actively sample the systems with proper difficulty for the policy on the fly, it will stabilize the training process and prevent the policy from becoming over-conservative or over-optimistic. To operationalize this idea, we introduce Active Dynamics Preference~(ADP), which quantifie",
    "link": "http://arxiv.org/abs/2209.11596",
    "context": "Title: Quantification before Selection: Active Dynamics Preference for Robust Reinforcement Learning. (arXiv:2209.11596v3 [cs.LG] UPDATED)\nAbstract: Training a robust policy is critical for policy deployment in real-world systems or dealing with unknown dynamics mismatch in different dynamic systems. Domain Randomization~(DR) is a simple and elegant approach that trains a conservative policy to counter different dynamic systems without expert knowledge about the target system parameters. However, existing works reveal that the policy trained through DR tends to be over-conservative and performs poorly in target domains. Our key insight is that dynamic systems with different parameters provide different levels of difficulty for the policy, and the difficulty of behaving well in a system is constantly changing due to the evolution of the policy. If we can actively sample the systems with proper difficulty for the policy on the fly, it will stabilize the training process and prevent the policy from becoming over-conservative or over-optimistic. To operationalize this idea, we introduce Active Dynamics Preference~(ADP), which quantifie",
    "path": "papers/22/09/2209.11596.json",
    "total_tokens": 1131,
    "translated_title": "量化优于选择：使用主动动态偏好进行稳健强化学习",
    "translated_abstract": "在现实世界中部署策略或处理不同动态系统中的未知动态失配，对于训练鲁棒性策略至关重要。领域随机化(DR)是一种简单而优雅的方法，它训练一个保守的策略来抵消不同的动态系统，而不需要关于目标系统参数的专家知识。然而，现有的研究表明，通过DR训练的策略往往过于保守，在目标域中表现不佳。我们的关键洞察是，具有不同参数的动态系统为策略提供了不同程度的难度，而在系统中表现良好的难度由于策略的演变而不断变化。如果我们能够在运行过程中积极地采样适合策略难度的系统，就可以稳定训练过程，防止策略过于保守或过于乐观。为了落实这个想法，我们引入了主动动态偏好~(ADP)，通过熵量化动态偏好并主动选择策略与之交互的动态系统，在探索和利用之间平衡。我们证明了ADP显著提高了策略在各种目标域中的鲁棒性，在几个基准任务上优于现有的状态-of-the-art方法。",
    "tldr": "本论文提出主动动态偏好(ADP)方法，在稳健强化学习中使用熵量化动态偏好来活跃地平衡策略的探索和利用，有效避免策略的过于保守或过于乐观，提高了策略在各种目标域中的鲁棒性，超过现有的state-of-the-art方法。",
    "en_tdlr": "This paper proposes the Active Dynamics Preference (ADP) approach, which uses entropy to quantify dynamics preference and actively balances the exploration and exploitation of the policy in robust reinforcement learning. It effectively avoids the policy becoming over-conservative or over-optimistic and improves the robustness of the policy in various target domains, outperforming existing state-of-the-art methods on several benchmark tasks."
}