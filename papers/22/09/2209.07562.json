{
    "title": "TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter. (arXiv:2209.07562v3 [cs.CL] UPDATED)",
    "abstract": "Pre-trained language models (PLMs) are fundamental for natural language processing applications. Most existing PLMs are not tailored to the noisy user-generated text on social media, and the pre-training does not factor in the valuable social engagement logs available in a social network. We present TwHIN-BERT, a multilingual language model productionized at Twitter, trained on in-domain data from the popular social network. TwHIN-BERT differs from prior pre-trained language models as it is trained with not only text-based self-supervision, but also with a social objective based on the rich social engagements within a Twitter heterogeneous information network (TwHIN). Our model is trained on 7 billion tweets covering over 100 distinct languages, providing a valuable representation to model short, noisy, user-generated text. We evaluate our model on various multilingual social recommendation and semantic understanding tasks and demonstrate significant metric improvement over established",
    "link": "http://arxiv.org/abs/2209.07562",
    "context": "Title: TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for Multilingual Tweet Representations at Twitter. (arXiv:2209.07562v3 [cs.CL] UPDATED)\nAbstract: Pre-trained language models (PLMs) are fundamental for natural language processing applications. Most existing PLMs are not tailored to the noisy user-generated text on social media, and the pre-training does not factor in the valuable social engagement logs available in a social network. We present TwHIN-BERT, a multilingual language model productionized at Twitter, trained on in-domain data from the popular social network. TwHIN-BERT differs from prior pre-trained language models as it is trained with not only text-based self-supervision, but also with a social objective based on the rich social engagements within a Twitter heterogeneous information network (TwHIN). Our model is trained on 7 billion tweets covering over 100 distinct languages, providing a valuable representation to model short, noisy, user-generated text. We evaluate our model on various multilingual social recommendation and semantic understanding tasks and demonstrate significant metric improvement over established",
    "path": "papers/22/09/2209.07562.json",
    "total_tokens": 985,
    "translated_title": "TwHIN-BERT：一种用于Twitter多语言推特表示的社交增强预训练语言模型",
    "translated_abstract": "预训练语言模型（PLMs）对于自然语言处理应用至关重要。现有的大多数PLMs并不针对社交媒体上的嘈杂用户生成文本进行优化，并且预训练过程中没有考虑社交网络中可用的有价值的社交参与日志。我们介绍了TwHIN-BERT，这是一种在Twitter上生产化的多语言语言模型，训练数据来自流行的社交网络。与先前的预训练语言模型不同，TwHIN-BERT不仅通过基于文本的自监督进行训练，还利用Twitter异构信息网络（TwHIN）中丰富的社交参与对象进行社交目标训练。我们的模型训练数据涵盖了超过100种不同语言的70亿条推文，为建模短、嘈杂、用户生成的文本提供了有价值的表示。我们在各种多语言社交推荐和语义理解任务上评估了我们的模型，并展示了与已建立模型相比的显著指标改进。",
    "tldr": "TwHIN-BERT是一个在Twitter上训练的多语言语言模型，通过结合基于文本的自监督和基于丰富社交参与对象的社交目标训练，能够有效地表示短、嘈杂的用户生成文本。在各种多语言社交推荐和语义理解任务中，TwHIN-BERT展示了显著的指标提升。",
    "en_tdlr": "TwHIN-BERT is a multilingual language model trained on Twitter, which combines text-based self-supervision and social objective training based on rich social engagements. It effectively represents short and noisy user-generated text and demonstrates significant metric improvement in various multilingual social recommendation and semantic understanding tasks."
}