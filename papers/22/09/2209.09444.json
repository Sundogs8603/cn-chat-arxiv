{
    "title": "Vega-MT: The JD Explore Academy Translation System for WMT22. (arXiv:2209.09444v4 [cs.CL] UPDATED)",
    "abstract": "We describe the JD Explore Academy's submission of the WMT 2022 shared general translation task. We participated in all high-resource tracks and one medium-resource track, including Chinese-English, German-English, Czech-English, Russian-English, and Japanese-English. We push the limit of our previous work -- bidirectional training for translation by scaling up two main factors, i.e. language pairs and model sizes, namely the \\textbf{Vega-MT} system. As for language pairs, we scale the \"bidirectional\" up to the \"multidirectional\" settings, covering all participating languages, to exploit the common knowledge across languages, and transfer them to the downstream bilingual tasks. As for model sizes, we scale the Transformer-Big up to the extremely large model that owns nearly 4.7 Billion parameters, to fully enhance the model capacity for our Vega-MT. Also, we adopt the data augmentation strategies, e.g. cycle translation for monolingual data, and bidirectional self-training for bilingua",
    "link": "http://arxiv.org/abs/2209.09444",
    "context": "Title: Vega-MT: The JD Explore Academy Translation System for WMT22. (arXiv:2209.09444v4 [cs.CL] UPDATED)\nAbstract: We describe the JD Explore Academy's submission of the WMT 2022 shared general translation task. We participated in all high-resource tracks and one medium-resource track, including Chinese-English, German-English, Czech-English, Russian-English, and Japanese-English. We push the limit of our previous work -- bidirectional training for translation by scaling up two main factors, i.e. language pairs and model sizes, namely the \\textbf{Vega-MT} system. As for language pairs, we scale the \"bidirectional\" up to the \"multidirectional\" settings, covering all participating languages, to exploit the common knowledge across languages, and transfer them to the downstream bilingual tasks. As for model sizes, we scale the Transformer-Big up to the extremely large model that owns nearly 4.7 Billion parameters, to fully enhance the model capacity for our Vega-MT. Also, we adopt the data augmentation strategies, e.g. cycle translation for monolingual data, and bidirectional self-training for bilingua",
    "path": "papers/22/09/2209.09444.json",
    "total_tokens": 905,
    "translated_title": "Vega-MT: JD Explore Academy的WMT22翻译系统",
    "translated_abstract": "本文描述了JD Explore Academy参加WMT 2022共享翻译任务的情况。我们参加了所有的高资源轨道和一个中资源轨道，包括中英文、德英文、捷英文、俄英文和日英文。我们通过扩展语言对和模型大小这两个主要因素，即Vega-MT系统，来推动我们之前进行的翻译的双向训练的极限。就语言对而言，我们将“双向”扩展到“多向”设置，涵盖了所有参与的语言，以利用跨语言的共同知识，并将其转移到下游的双语任务中。就模型大小而言，我们将Transformer-Big扩展到几乎拥有47亿参数的极大模型，以充分提升我们的Vega-MT的模型容量。同时，我们采取数据增强策略，如单语数据的循环翻译和双语自我训练等方法。",
    "tldr": "Vega-MT是JD Explore Academy为WMT22共享的翻译任务开发的系统。该系统扩大了语言对和模型大小，实现了“双向”到“多向”的拓展并采取了数据增强策略，从而提高翻译质量。",
    "en_tdlr": "Vega-MT is a translation system developed by JD Explore Academy for the WMT22 shared translation task, which expands language pairs and model sizes to achieve bidirectional to multidirectional settings and adopts data augmentation strategies to improve translation quality."
}