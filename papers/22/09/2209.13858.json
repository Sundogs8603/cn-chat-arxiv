{
    "title": "Variance Tolerance Factors For Interpreting ALL Neural Networks. (arXiv:2209.13858v2 [cs.LG] UPDATED)",
    "abstract": "Black box models only provide results for deep learning tasks, and lack informative details about how these results were obtained. Knowing how input variables are related to outputs, in addition to why they are related, can be critical to translating predictions into laboratory experiments, or defending a model prediction under scrutiny. In this paper, we propose a general theory that defines a variance tolerance factor (VTF) inspired by influence function, to interpret features in the context of black box neural networks by ranking the importance of features, and construct a novel architecture consisting of a base model and feature model to explore the feature importance in a Rashomon set that contains all well-performing neural networks. Two feature importance ranking methods in the Rashomon set and a feature selection method based on the VTF are created and explored. A thorough evaluation on synthetic and benchmark datasets is provided, and the method is applied to two real world ex",
    "link": "http://arxiv.org/abs/2209.13858",
    "context": "Title: Variance Tolerance Factors For Interpreting ALL Neural Networks. (arXiv:2209.13858v2 [cs.LG] UPDATED)\nAbstract: Black box models only provide results for deep learning tasks, and lack informative details about how these results were obtained. Knowing how input variables are related to outputs, in addition to why they are related, can be critical to translating predictions into laboratory experiments, or defending a model prediction under scrutiny. In this paper, we propose a general theory that defines a variance tolerance factor (VTF) inspired by influence function, to interpret features in the context of black box neural networks by ranking the importance of features, and construct a novel architecture consisting of a base model and feature model to explore the feature importance in a Rashomon set that contains all well-performing neural networks. Two feature importance ranking methods in the Rashomon set and a feature selection method based on the VTF are created and explored. A thorough evaluation on synthetic and benchmark datasets is provided, and the method is applied to two real world ex",
    "path": "papers/22/09/2209.13858.json",
    "total_tokens": 998,
    "translated_title": "解释所有神经网络的方差容忍因子",
    "translated_abstract": "黑匣子模型只提供深度学习任务的结果，缺乏有关如何获得这些结果的详细信息。知道输入变量与输出的关系，以及为什么它们相关，可以在将预测转化为实验或在受到审查时维护模型预测的关键时刻起到重要作用。在本文中，我们提出了一个一般性理论，通过定义一个受影响函数启发的方差容忍因子（VTF），从排名特征的角度解释黑匣子神经网络中的特征，并构建一个包含基本模型和特征模型的新型架构，以探索包含所有表现良好的神经网络的瑞士军刀集中的特征重要性。创建并探索了两种Rashomon集中的特征重要性排名方法和基于VTF的特征选择方法。我们提供了对合成数据集和基准数据集的彻底评估，并将该方法应用于基因组学和材料科学中的两个真实世界实验。",
    "tldr": "本文提出一种用于解释黑盒神经网络的方差容忍因子（VTF）理论，通过排名特征的方式探索特征的重要性，同时构建一个基本模型和特征模型的新型架构，来探索所有表现良好的神经网络中特征的重要性，并且经过基准测试和应用于实际环境中的实验验证了方法的可靠性。",
    "en_tdlr": "This paper proposed a theory of variance tolerance factor (VTF) for interpreting black box neural networks by ranking the importance of features, and constructed a new architecture consisting of a base model and feature model to explore the feature importance in a Rashomon set that contains all well-performing neural networks. The method has been thoroughly evaluated on synthetic and benchmark datasets, and applied to real world experiments in genomics and materials science, demonstrating its reliability."
}