{
    "title": "APTx: better activation function than MISH, SWISH, and ReLU's variants used in deep learning. (arXiv:2209.06119v4 [cs.LG] UPDATED)",
    "abstract": "Activation Functions introduce non-linearity in the deep neural networks. This nonlinearity helps the neural networks learn faster and efficiently from the dataset. In deep learning, many activation functions are developed and used based on the type of problem statement. ReLU's variants, SWISH, and MISH are goto activation functions. MISH function is considered having similar or even better performance than SWISH, and much better than ReLU. In this paper, we propose an activation function named APTx which behaves similar to MISH, but requires lesser mathematical operations to compute. The lesser computational requirements of APTx does speed up the model training, and thus also reduces the hardware requirement for the deep learning model.",
    "link": "http://arxiv.org/abs/2209.06119",
    "raw_ret": "{\n    \"translated_title\": \"APTx：比MISH、SWISH和ReLU的变种更好的深度学习激活函数（arXiv：2209.06119v4[cs.LG]已更新）\",\n    \"translated_abstract\": \"激活函数在深度神经网络中引入了非线性。这种非线性有助于神经网络从数据集中更快、更有效地学习。在深度学习中，根据问题陈述的类型，开发和使用了许多激活函数。ReLU的变种、SWISH和MISH是常用的激活函数。MISH函数被认为具有与SWISH类似甚至更好的性能，比ReLU要好得多。在本文中，我们提出了一个名为APTx的激活函数，它的行为类似于MISH，但需要更少的数学运算来计算。APTx的更少的计算要求加速了模型的训练，从而也减少了深度学习模型的硬件要求。\",\n    \"tldr\": \"本文提出了一种名为APTx的激活函数，它的行为类似于MISH，但需要更少的数学运算来计算，加速了模型的训练，减少了深度学习模型的硬件要求。\"\n}",
    "total_tokens": 781,
    "ret": {
        "translated_title": "APTx：比MISH、SWISH和ReLU的变种更好的深度学习激活函数（arXiv：2209.06119v4[cs.LG]已更新）",
        "translated_abstract": "激活函数在深度神经网络中引入了非线性。这种非线性有助于神经网络从数据集中更快、更有效地学习。在深度学习中，根据问题陈述的类型，开发和使用了许多激活函数。ReLU的变种、SWISH和MISH是常用的激活函数。MISH函数被认为具有与SWISH类似甚至更好的性能，比ReLU要好得多。在本文中，我们提出了一个名为APTx的激活函数，它的行为类似于MISH，但需要更少的数学运算来计算。APTx的更少的计算要求加速了模型的训练，从而也减少了深度学习模型的硬件要求。",
        "tldr": "本文提出了一种名为APTx的激活函数，它的行为类似于MISH，但需要更少的数学运算来计算，加速了模型的训练，减少了深度学习模型的硬件要求。"
    }
}