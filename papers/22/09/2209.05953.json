{
    "title": "Sample Complexity Bounds for Learning High-dimensional Simplices in Noisy Regimes. (arXiv:2209.05953v2 [stat.ML] UPDATED)",
    "abstract": "In this paper, we find a sample complexity bound for learning a simplex from noisy samples. Assume a dataset of size $n$ is given which includes i.i.d. samples drawn from a uniform distribution over an unknown simplex in $\\mathbb{R}^K$, where samples are assumed to be corrupted by a multi-variate additive Gaussian noise of an arbitrary magnitude. We prove the existence of an algorithm that with high probability outputs a simplex having a $\\ell_2$ distance of at most $\\varepsilon$ from the true simplex (for any $\\varepsilon>0$). Also, we theoretically show that in order to achieve this bound, it is sufficient to have $n\\ge\\left(K^2/\\varepsilon^2\\right)e^{\\Omega\\left(K/\\mathrm{SNR}^2\\right)}$ samples, where $\\mathrm{SNR}$ stands for the signal-to-noise ratio. This result solves an important open problem and shows as long as $\\mathrm{SNR}\\ge\\Omega\\left(K^{1/2}\\right)$, the sample complexity of the noisy regime has the same order to that of the noiseless case. Our proofs are a combination ",
    "link": "http://arxiv.org/abs/2209.05953",
    "context": "Title: Sample Complexity Bounds for Learning High-dimensional Simplices in Noisy Regimes. (arXiv:2209.05953v2 [stat.ML] UPDATED)\nAbstract: In this paper, we find a sample complexity bound for learning a simplex from noisy samples. Assume a dataset of size $n$ is given which includes i.i.d. samples drawn from a uniform distribution over an unknown simplex in $\\mathbb{R}^K$, where samples are assumed to be corrupted by a multi-variate additive Gaussian noise of an arbitrary magnitude. We prove the existence of an algorithm that with high probability outputs a simplex having a $\\ell_2$ distance of at most $\\varepsilon$ from the true simplex (for any $\\varepsilon>0$). Also, we theoretically show that in order to achieve this bound, it is sufficient to have $n\\ge\\left(K^2/\\varepsilon^2\\right)e^{\\Omega\\left(K/\\mathrm{SNR}^2\\right)}$ samples, where $\\mathrm{SNR}$ stands for the signal-to-noise ratio. This result solves an important open problem and shows as long as $\\mathrm{SNR}\\ge\\Omega\\left(K^{1/2}\\right)$, the sample complexity of the noisy regime has the same order to that of the noiseless case. Our proofs are a combination ",
    "path": "papers/22/09/2209.05953.json",
    "total_tokens": 1040,
    "translated_title": "学习高维单纯形在噪声环境下的样本复杂度界限",
    "translated_abstract": "本文研究了从含有噪声的样本中学习单纯形的样本复杂度。假设给定一个大小为$n$的数据集，其中包含从$\\mathbb{R}^K$中的未知单纯形上均匀分布中独立同分布抽样的样本，假设这些样本被一个任意幅度的多元加性高斯噪声所污染。我们证明了存在一种算法，以高概率输出一个与真实单纯形的$\\ell_2$距离最大为$\\varepsilon$的单纯形（对于任意$\\varepsilon>0$）。同时，我们在理论上证明，为了实现这个界限，需要有 $n\\ge\\left(K^2/\\varepsilon^2\\right)e^{\\Omega\\left(K/\\mathrm{SNR}^2\\right)}$ 个样本，其中 $\\mathrm{SNR}$ 代表信噪比。这个结果解决了一个重要的开放性问题，并表明只要 $\\mathrm{SNR}\\ge\\Omega\\left(K^{1/2}\\right)$，在噪声环境下的样本复杂度与无噪声情况具有相同的阶。本文的证明是各种工具的组合。",
    "tldr": "本文解决了一个重要的开放性问题，提出了在噪声环境下从样本中学习单纯形的样本复杂度界限，并证明了只要信噪比较高，样本复杂度与无噪声情况具有相同的阶。",
    "en_tdlr": "This paper solves an important open problem by finding a sample complexity bound for learning a simplex from noisy samples, and theoretically shows that as long as the signal-to-noise ratio is relatively high, the sample complexity of the noisy regime has the same order as that of the noiseless case."
}