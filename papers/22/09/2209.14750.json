{
    "title": "Non-contrastive representation learning for intervals from well logs. (arXiv:2209.14750v2 [cs.AI] UPDATED)",
    "abstract": "The representation learning problem in the oil & gas industry aims to construct a model that provides a representation based on logging data for a well interval. Previous attempts are mainly supervised and focus on similarity task, which estimates closeness between intervals. We desire to build informative representations without using supervised (labelled) data. One of the possible approaches is self-supervised learning (SSL). In contrast to the supervised paradigm, this one requires little or no labels for the data. Nowadays, most SSL approaches are either contrastive or non-contrastive. Contrastive methods make representations of similar (positive) objects closer and distancing different (negative) ones. Due to possible wrong marking of positive and negative pairs, these methods can provide an inferior performance. Non-contrastive methods don't rely on such labelling and are widespread in computer vision. They learn using only pairs of similar objects that are easier to identify in ",
    "link": "http://arxiv.org/abs/2209.14750",
    "context": "Title: Non-contrastive representation learning for intervals from well logs. (arXiv:2209.14750v2 [cs.AI] UPDATED)\nAbstract: The representation learning problem in the oil & gas industry aims to construct a model that provides a representation based on logging data for a well interval. Previous attempts are mainly supervised and focus on similarity task, which estimates closeness between intervals. We desire to build informative representations without using supervised (labelled) data. One of the possible approaches is self-supervised learning (SSL). In contrast to the supervised paradigm, this one requires little or no labels for the data. Nowadays, most SSL approaches are either contrastive or non-contrastive. Contrastive methods make representations of similar (positive) objects closer and distancing different (negative) ones. Due to possible wrong marking of positive and negative pairs, these methods can provide an inferior performance. Non-contrastive methods don't rely on such labelling and are widespread in computer vision. They learn using only pairs of similar objects that are easier to identify in ",
    "path": "papers/22/09/2209.14750.json",
    "total_tokens": 871,
    "translated_title": "针对井测数据的非对比度表示学习",
    "translated_abstract": "石油和天然气行业中的表示学习问题旨在构建一个模型，根据钻井数据为井段提供表示形式。以往的尝试主要是有监督的，并且关注于相似性任务，即估计井段之间的相似程度。我们希望在不使用已标记数据的情况下构建信息量丰富的表示形式。其中一个可能的方法是自我监督学习（SSL）。与有监督范式相反，这个方法对数据需要很少或者没有标签。现今，大多数SSL方法要么是对比的，要么是非对比的。对比方法使相似的（正）对象的表示变得更加接近，并将不同的（负）对象与之距离。由于可能存在错误的正负标注，这些方法可能会提供更差的性能。非对比方法不依赖于此类标注，在计算机视觉领域广泛应用。它们仅使用容易识别的相似对象对进行学习。",
    "tldr": "本文提出了一种新的方法来处理井测数据表示学习问题，采用自我监督学习的方法进行非对比度的表示学习，减少对数据的标注需求，并提高了算法性能。"
}