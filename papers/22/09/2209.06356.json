{
    "title": "A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism. (arXiv:2209.06356v2 [cs.LG] UPDATED)",
    "abstract": "Animals are able to rapidly infer from limited experience when sets of state action pairs have equivalent reward and transition dynamics. On the other hand, modern reinforcement learning systems must painstakingly learn through trial and error that sets of state action pairs are value equivalent -- requiring an often prohibitively large amount of samples from their environment. MDP homomorphisms have been proposed that reduce the observed MDP of an environment to an abstract MDP, which can enable more sample efficient policy learning. Consequently, impressive improvements in sample efficiency have been achieved when a suitable MDP homomorphism can be constructed a priori -- usually by exploiting a practioner's knowledge of environment symmetries. We propose a novel approach to constructing a homomorphism in discrete action spaces, which uses a partial model of environment dynamics to infer which state action pairs lead to the same state -- reducing the size of the state-action space by",
    "link": "http://arxiv.org/abs/2209.06356",
    "context": "Title: A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism. (arXiv:2209.06356v2 [cs.LG] UPDATED)\nAbstract: Animals are able to rapidly infer from limited experience when sets of state action pairs have equivalent reward and transition dynamics. On the other hand, modern reinforcement learning systems must painstakingly learn through trial and error that sets of state action pairs are value equivalent -- requiring an often prohibitively large amount of samples from their environment. MDP homomorphisms have been proposed that reduce the observed MDP of an environment to an abstract MDP, which can enable more sample efficient policy learning. Consequently, impressive improvements in sample efficiency have been achieved when a suitable MDP homomorphism can be constructed a priori -- usually by exploiting a practioner's knowledge of environment symmetries. We propose a novel approach to constructing a homomorphism in discrete action spaces, which uses a partial model of environment dynamics to infer which state action pairs lead to the same state -- reducing the size of the state-action space by",
    "path": "papers/22/09/2209.06356.json",
    "total_tokens": 865,
    "translated_title": "使用学习到的MDP同态映射的简单状态-动作抽象方法",
    "translated_abstract": "动物能够在有限的经验中迅速推断出等价奖励和转移动力学的状态动作对集合。与此相反，现代强化学习系统必须通过反复试错来学习状态动作对集合的值等价性，这通常需要大量样本。已经提出了MDP同态映射的方法，将环境的观察MDP简化为抽象MDP，可以实现更高效的策略学习。因此，当可以事先构建适当的MDP同态映射时，可以取得令人印象深刻的样本效率改进，通常通过利用环境的对称性来实现。我们提出了一种新颖的方法来构建离散动作空间中的同态映射，该方法使用环境动力学的部分模型来推断哪些状态动作对导致相同的状态，从而减小状态-动作空间的大小。",
    "tldr": "本论文提出了一种在离散动作空间中构建同态映射的新方法，通过使用环境动力学的部分模型来推断相同状态的状态动作对，从而减小状态-动作空间的大小。",
    "en_tdlr": "This paper proposes a novel approach for constructing a homomorphism in discrete action spaces, which uses a partial model of environment dynamics to infer state-action pairs that lead to the same state, thereby reducing the size of the state-action space."
}