{
    "title": "The Mori-Zwanzig formulation of deep learning. (arXiv:2209.05544v4 [cs.LG] UPDATED)",
    "abstract": "We develop a new formulation of deep learning based on the Mori-Zwanzig (MZ) formalism of irreversible statistical mechanics. The new formulation is built upon the well-known duality between deep neural networks and discrete dynamical systems, and it allows us to directly propagate quantities of interest (conditional expectations and probability density functions) forward and backward through the network by means of exact linear operator equations. Such new equations can be used as a starting point to develop new effective parameterizations of deep neural networks, and provide a new framework to study deep-learning via operator theoretic methods. The proposed MZ formulation of deep learning naturally introduces a new concept, i.e., the memory of the neural network, which plays a fundamental role in low-dimensional modeling and parameterization. By using the theory of contraction mappings, we develop sufficient conditions for the memory of the neural network to decay with the number of ",
    "link": "http://arxiv.org/abs/2209.05544",
    "context": "Title: The Mori-Zwanzig formulation of deep learning. (arXiv:2209.05544v4 [cs.LG] UPDATED)\nAbstract: We develop a new formulation of deep learning based on the Mori-Zwanzig (MZ) formalism of irreversible statistical mechanics. The new formulation is built upon the well-known duality between deep neural networks and discrete dynamical systems, and it allows us to directly propagate quantities of interest (conditional expectations and probability density functions) forward and backward through the network by means of exact linear operator equations. Such new equations can be used as a starting point to develop new effective parameterizations of deep neural networks, and provide a new framework to study deep-learning via operator theoretic methods. The proposed MZ formulation of deep learning naturally introduces a new concept, i.e., the memory of the neural network, which plays a fundamental role in low-dimensional modeling and parameterization. By using the theory of contraction mappings, we develop sufficient conditions for the memory of the neural network to decay with the number of ",
    "path": "papers/22/09/2209.05544.json",
    "total_tokens": 908,
    "translated_title": "深度学习的莫里-茨旺齐格表述",
    "translated_abstract": "本文基于不可逆统计力学的莫里-茨旺齐格（MZ）形式主义，提出了深度学习的新表述。这种新的表述基于深度神经网络和离散动力系统之间的对偶关系，通过线性算子方程直接向前和向后传播感兴趣的量（条件期望和概率密度函数）。这些新方程可以作为开发深度神经网络新的有效参数化的起点，并提供了一种新的通过算子理论方法研究深度学习的框架。所提出的MZ形式主义自然引入了神经网络记忆的新概念，在低维建模和参数化中起着 fundamental 的作用。通过使用收缩映射理论，我们开发出了记忆衰减随网络层数增加的充分条件。",
    "tldr": "本文提出了基于莫里-茨旺齐格形式主义的深度学习新表述，引入了神经网络记忆的新概念，并通过线性算子方程直接向前和向后传播感兴趣的量。收缩映射理论被用来开发记忆衰减随网络层数增加的充分条件。"
}