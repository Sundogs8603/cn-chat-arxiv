{
    "title": "Minimalistic Unsupervised Learning with the Sparse Manifold Transform. (arXiv:2209.15261v2 [cs.LG] UPDATED)",
    "abstract": "We describe a minimalistic and interpretable method for unsupervised learning, without resorting to data augmentation, hyperparameter tuning, or other engineering designs, that achieves performance close to the SOTA SSL methods. Our approach leverages the sparse manifold transform, which unifies sparse coding, manifold learning, and slow feature analysis. With a one-layer deterministic sparse manifold transform, one can achieve 99.3% KNN top-1 accuracy on MNIST, 81.1% KNN top-1 accuracy on CIFAR-10 and 53.2% on CIFAR-100. With a simple gray-scale augmentation, the model gets 83.2% KNN top-1 accuracy on CIFAR-10 and 57% on CIFAR-100. These results significantly close the gap between simplistic \"white-box\" methods and the SOTA methods. Additionally, we provide visualization to explain how an unsupervised representation transform is formed. The proposed method is closely connected to latent-embedding self-supervised methods and can be treated as the simplest form of VICReg. Though there r",
    "link": "http://arxiv.org/abs/2209.15261",
    "context": "Title: Minimalistic Unsupervised Learning with the Sparse Manifold Transform. (arXiv:2209.15261v2 [cs.LG] UPDATED)\nAbstract: We describe a minimalistic and interpretable method for unsupervised learning, without resorting to data augmentation, hyperparameter tuning, or other engineering designs, that achieves performance close to the SOTA SSL methods. Our approach leverages the sparse manifold transform, which unifies sparse coding, manifold learning, and slow feature analysis. With a one-layer deterministic sparse manifold transform, one can achieve 99.3% KNN top-1 accuracy on MNIST, 81.1% KNN top-1 accuracy on CIFAR-10 and 53.2% on CIFAR-100. With a simple gray-scale augmentation, the model gets 83.2% KNN top-1 accuracy on CIFAR-10 and 57% on CIFAR-100. These results significantly close the gap between simplistic \"white-box\" methods and the SOTA methods. Additionally, we provide visualization to explain how an unsupervised representation transform is formed. The proposed method is closely connected to latent-embedding self-supervised methods and can be treated as the simplest form of VICReg. Though there r",
    "path": "papers/22/09/2209.15261.json",
    "total_tokens": 1003,
    "translated_abstract": "我们描述了一种极简和可解释的无监督学习方法，它不需要数据增强、超参数调整或其他工程设计，但其性能接近于目前最先进的半监督学习方法。我们的方法利用稀疏流形变换，它将稀疏编码、流形学习和慢速特征分析统一起来。利用一层确定性稀疏流形变换，在MNIST上可以达到99.3％的KNN top-1准确率，在CIFAR-10上可以达到81.1％的KNN top-1准确率，CIFAR-100上可以达到53.2％的准确率。使用简单的灰度增强，该模型在CIFAR-10上可以获得83.2％的KNN top-1准确率，在CIFAR-100上可以获得57％的准确率。这些结果显著缩小了简单“白盒”方法和最先进方法之间的差距。此外，我们提供了可视化方法来解释无监督表示转换是如何形成的。该方法与潜在嵌入自监督方法密切相关，并可被视为VICReg的最简形式。",
    "tldr": "该论文描述了一种极简的无监督学习方法，利用稀疏流形变换实现了接近半监督学习方法的性能，同时提供了可视化解释无监督表示转换的方法。"
}