{
    "title": "On the Reuse Bias in Off-Policy Reinforcement Learning. (arXiv:2209.07074v2 [cs.LG] UPDATED)",
    "abstract": "Importance sampling (IS) is a popular technique in off-policy evaluation, which re-weights the return of trajectories in the replay buffer to boost sample efficiency. However, training with IS can be unstable and previous attempts to address this issue mainly focus on analyzing the variance of IS. In this paper, we reveal that the instability is also related to a new notion of Reuse Bias of IS -- the bias in off-policy evaluation caused by the reuse of the replay buffer for evaluation and optimization. We theoretically show that the off-policy evaluation and optimization of the current policy with the data from the replay buffer result in an overestimation of the objective, which may cause an erroneous gradient update and degenerate the performance. We further provide a high-probability upper bound of the Reuse Bias, and show that controlling one term of the upper bound can control the Reuse Bias by introducing the concept of stability for off-policy algorithms. Based on these analyses",
    "link": "http://arxiv.org/abs/2209.07074",
    "context": "Title: On the Reuse Bias in Off-Policy Reinforcement Learning. (arXiv:2209.07074v2 [cs.LG] UPDATED)\nAbstract: Importance sampling (IS) is a popular technique in off-policy evaluation, which re-weights the return of trajectories in the replay buffer to boost sample efficiency. However, training with IS can be unstable and previous attempts to address this issue mainly focus on analyzing the variance of IS. In this paper, we reveal that the instability is also related to a new notion of Reuse Bias of IS -- the bias in off-policy evaluation caused by the reuse of the replay buffer for evaluation and optimization. We theoretically show that the off-policy evaluation and optimization of the current policy with the data from the replay buffer result in an overestimation of the objective, which may cause an erroneous gradient update and degenerate the performance. We further provide a high-probability upper bound of the Reuse Bias, and show that controlling one term of the upper bound can control the Reuse Bias by introducing the concept of stability for off-policy algorithms. Based on these analyses",
    "path": "papers/22/09/2209.07074.json",
    "total_tokens": 1123,
    "translated_title": "关于离线策略强化学习中重复使用偏见的研究",
    "translated_abstract": "重要性采样是离线评估中常用的技术，它通过重新加权回放缓冲区中的轨迹收益来提高样本效率。然而，使用重要性采样训练可能不稳定，并且以前解决这个问题的尝试主要集中在分析重要性采样的方差上。本文揭示了这种不稳定性也与一个新的重复使用偏见有关——由回放缓冲区的评估和优化重复使用造成的离线评估中的偏差。我们在理论上展示了当前策略的离线评估和优化与回放缓冲区的数据导致目标的过高估计，这可能导致错误的梯度更新并退化性能。我们进一步提供一个重复使用偏见的高概率上限，并展示通过引入离线算法的稳定性概念，可以通过控制上限的某一项来控制重复使用偏差。基于这些分析，我们提出了一个简单而有效的方法，称为重复使用感知重要性加权（RAW），来纠正重复使用偏见并提高离线策略强化学习的稳定性。我们还提供了实证证据来证明，RAW可以显着提高离线方法的样本效率和鲁棒性，包括DDPG、SAC和TD3。",
    "tldr": "本文揭示了离线强化学习中一个新的偏见问题：重复使用偏见，提出了一种简单有效的方法——重复使用感知重要性加权（RAW）来解决这个问题，并证明RAW显著提高了离线方法的样本效率和鲁棒性。"
}