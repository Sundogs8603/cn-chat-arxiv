{
    "title": "Sequential Attention for Feature Selection. (arXiv:2209.14881v3 [cs.LG] UPDATED)",
    "abstract": "Feature selection is the problem of selecting a subset of features for a machine learning model that maximizes model quality subject to a budget constraint. For neural networks, prior methods, including those based on $\\ell_1$ regularization, attention, and other techniques, typically select the entire feature subset in one evaluation round, ignoring the residual value of features during selection, i.e., the marginal contribution of a feature given that other features have already been selected. We propose a feature selection algorithm called Sequential Attention that achieves state-of-the-art empirical results for neural networks. This algorithm is based on an efficient one-pass implementation of greedy forward selection and uses attention weights at each step as a proxy for feature importance. We give theoretical insights into our algorithm for linear regression by showing that an adaptation to this setting is equivalent to the classical Orthogonal Matching Pursuit (OMP) algorithm, a",
    "link": "http://arxiv.org/abs/2209.14881",
    "context": "Title: Sequential Attention for Feature Selection. (arXiv:2209.14881v3 [cs.LG] UPDATED)\nAbstract: Feature selection is the problem of selecting a subset of features for a machine learning model that maximizes model quality subject to a budget constraint. For neural networks, prior methods, including those based on $\\ell_1$ regularization, attention, and other techniques, typically select the entire feature subset in one evaluation round, ignoring the residual value of features during selection, i.e., the marginal contribution of a feature given that other features have already been selected. We propose a feature selection algorithm called Sequential Attention that achieves state-of-the-art empirical results for neural networks. This algorithm is based on an efficient one-pass implementation of greedy forward selection and uses attention weights at each step as a proxy for feature importance. We give theoretical insights into our algorithm for linear regression by showing that an adaptation to this setting is equivalent to the classical Orthogonal Matching Pursuit (OMP) algorithm, a",
    "path": "papers/22/09/2209.14881.json",
    "total_tokens": 816,
    "translated_title": "特征选择的序列关注",
    "translated_abstract": "特征选择是为了选择一个子集用于机器学习模型，而这个子集能最大化模型质量，并且要求在预算范围内。对于神经网络，采用的传统方法包括基于$\\ell_1$正则化、注意力和其他技术的方法，通常在一次评估中选择整个特征子集，忽略了在选择期间特征的残留价值，即在选择其他特征后给与特征的边际贡献。我们提出了一种名为序列关注的特征选择算法，它在神经网络中实现了最新的实证结果。该算法基于一遍高效的贪心前向选择实现，并在每个步骤使用注意力权重作为特征重要性的代理。我们通过展示适用于线性回归的理论意义，说明了我们的算法相当于经典的正交匹配追踪（OMP）算法。",
    "tldr": "在神经网络中，我们提出一种名为序列关注的特征选择算法，它在每个步骤使用注意力权重作为特征重要性的代理，实现了最新的实证结果。"
}