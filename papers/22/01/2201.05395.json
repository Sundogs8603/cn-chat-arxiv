{
    "title": "De Rham compatible Deep Neural Network FEM. (arXiv:2201.05395v3 [math.NA] UPDATED)",
    "abstract": "On general regular simplicial partitions $\\mathcal{T}$ of bounded polytopal domains $\\Omega \\subset \\mathbb{R}^d$, $d\\in\\{2,3\\}$, we construct \\emph{exact neural network (NN) emulations} of all lowest order finite element spaces in the discrete de Rham complex. These include the spaces of piecewise constant functions, continuous piecewise linear (CPwL) functions, the classical ``Raviart-Thomas element'', and the ``N\\'{e}d\\'{e}lec edge element''. For all but the CPwL case, our network architectures employ both ReLU (rectified linear unit) and BiSU (binary step unit) activations to capture discontinuities. In the important case of CPwL functions, we prove that it suffices to work with pure ReLU nets. Our construction and DNN architecture generalizes previous results in that no geometric restrictions on the regular simplicial partitions $\\mathcal{T}$ of $\\Omega$ are required for DNN emulation. In addition, for CPwL functions our DNN construction is valid in any dimension $d\\geq 2$. Our ``",
    "link": "http://arxiv.org/abs/2201.05395",
    "context": "Title: De Rham compatible Deep Neural Network FEM. (arXiv:2201.05395v3 [math.NA] UPDATED)\nAbstract: On general regular simplicial partitions $\\mathcal{T}$ of bounded polytopal domains $\\Omega \\subset \\mathbb{R}^d$, $d\\in\\{2,3\\}$, we construct \\emph{exact neural network (NN) emulations} of all lowest order finite element spaces in the discrete de Rham complex. These include the spaces of piecewise constant functions, continuous piecewise linear (CPwL) functions, the classical ``Raviart-Thomas element'', and the ``N\\'{e}d\\'{e}lec edge element''. For all but the CPwL case, our network architectures employ both ReLU (rectified linear unit) and BiSU (binary step unit) activations to capture discontinuities. In the important case of CPwL functions, we prove that it suffices to work with pure ReLU nets. Our construction and DNN architecture generalizes previous results in that no geometric restrictions on the regular simplicial partitions $\\mathcal{T}$ of $\\Omega$ are required for DNN emulation. In addition, for CPwL functions our DNN construction is valid in any dimension $d\\geq 2$. Our ``",
    "path": "papers/22/01/2201.05395.json",
    "total_tokens": 1060,
    "translated_title": "De Rham兼容的深度神经网络有限元方法",
    "translated_abstract": "在有界多面体区域$\\Omega\\subset \\mathbb{R}^d$ $(d\\in\\{2,3\\})$的一般正则单形分割$\\mathcal{T}$上，我们构建了所有离散de Rham复形中最低阶有限元空间的“精确神经网络（NN）仿真”。 这些包括分段常数函数空间，连续分段线性（CPwL）函数空间，经典的“Raviart-Thomas元”和“Nédélec边缘元”。 对于除CPwL元之外的所有元素，我们的网络架构采用ReLU（修正线性单元）和BiSU（二元阶跃单元）激活来捕捉不连续性。 在CPwL函数的重要情况下，我们证明只需要使用纯ReLU网络即可。 我们的构造和DNN架构在没有对$\\Omega$的正则单形分割$\\mathcal{T}$的几何限制的情况下推广了以前的结果。 另外，对于CPwL函数，我们的DNN构造在任何维度$d\\geq 2$中都是有效的。",
    "tldr": "本篇论文提出了在普通正则单形分割上构建精确神经网络有限元的方法，能够适用于分段常数函数空间、连续分段线性函数空间、经典的Raviart-Thomas元和Nédélec边缘元，且能够捕捉不连续性，其中对于CPwL函数的情况，只需使用纯ReLU网络即可。",
    "en_tdlr": "This paper proposes a method to construct exact neural network finite elements on regular simplicial partitions, which can be used in spaces of piecewise constant functions, continuous piecewise linear functions, the classical Raviart-Thomas element, and the Nédélec edge element. The method can capture discontinuities using ReLU and BiSU activations and is valid in any dimension for CPwL functions using pure ReLU nets."
}