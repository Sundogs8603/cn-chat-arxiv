{
    "title": "Communication-Efficient Federated Learning with Accelerated Client Gradient",
    "abstract": "arXiv:2201.03172v2 Announce Type: replace-cross  Abstract: Federated learning often suffers from slow and unstable convergence due to the heterogeneous characteristics of participating client datasets. Such a tendency is aggravated when the client participation ratio is low since the information collected from the clients has large variations. To address this challenge, we propose a simple but effective federated learning framework, which improves the consistency across clients and facilitates the convergence of the server model. This is achieved by making the server broadcast a global model with a lookahead gradient. This strategy enables the proposed approach to convey the projected global update information to participants effectively without additional client memory and extra communication costs. We also regularize local updates by aligning each client with the overshot global model to reduce bias and improve the stability of our algorithm. We provide the theoretical convergence ra",
    "link": "https://arxiv.org/abs/2201.03172",
    "context": "Title: Communication-Efficient Federated Learning with Accelerated Client Gradient\nAbstract: arXiv:2201.03172v2 Announce Type: replace-cross  Abstract: Federated learning often suffers from slow and unstable convergence due to the heterogeneous characteristics of participating client datasets. Such a tendency is aggravated when the client participation ratio is low since the information collected from the clients has large variations. To address this challenge, we propose a simple but effective federated learning framework, which improves the consistency across clients and facilitates the convergence of the server model. This is achieved by making the server broadcast a global model with a lookahead gradient. This strategy enables the proposed approach to convey the projected global update information to participants effectively without additional client memory and extra communication costs. We also regularize local updates by aligning each client with the overshot global model to reduce bias and improve the stability of our algorithm. We provide the theoretical convergence ra",
    "path": "papers/22/01/2201.03172.json",
    "total_tokens": 837,
    "translated_title": "具有加速客户梯度的通信高效联邦学习",
    "translated_abstract": "联邦学习常常因参与客户数据集的异质性特征而导致收敛缓慢不稳定。当客户参与率较低时，此趋势会加剧，因为从客户收集的信息具有较大的变化。为了解决这一挑战，我们提出了一个简单但有效的联邦学习框架，改善了客户间的一致性，促进了服务器模型的收敛。通过使服务器广播具有前瞻梯度的全局模型，从而实现了该策略，使所提出的方法能够有效地向参与者传达投影的全局更新信息，而无需额外的客户内存和通信成本。我们还通过将每个客户端与超调全局模型对齐来规范本地更新，以减少偏差并改善算法的稳定性。我们提供了理论收敛率。",
    "tldr": "通过使用具有前瞻梯度的全局模型，并通过与超调全局模型对齐来规范本地更新，提出了一种简单但有效的联邦学习框架，以改善参与者之间的一致性，促进服务器模型的收敛。",
    "en_tdlr": "Proposing a simple yet effective federated learning framework that enhances consistency among participants and facilitates the convergence of the server model by using a global model with lookahead gradient and aligning local updates with the overshoot global model."
}