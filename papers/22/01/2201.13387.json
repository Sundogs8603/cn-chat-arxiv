{
    "title": "L-SVRG and L-Katyusha with Adaptive Sampling. (arXiv:2201.13387v2 [cs.LG] UPDATED)",
    "abstract": "Stochastic gradient-based optimization methods, such as L-SVRG and its accelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train machine learning models.The theoretical and empirical performance of L-SVRG and L-Katyusha can be improved by sampling observations from a non-uniform distribution (Qian et al., 2021). However,designing a desired sampling distribution requires prior knowledge of smoothness constants, which can be computationally intractable to obtain in practice when the dimension of the model parameter is high. To address this issue, we propose an adaptive sampling strategy for L-SVRG and L-Katyusha that can learn the sampling distribution with little computational overhead, while allowing it to change with iterates, and at the same time does not require any prior knowledge of the problem parameters. We prove convergence guarantees for L-SVRG and L-Katyusha for convex objectives when the sampling distribution changes with iterates. Our results show that",
    "link": "http://arxiv.org/abs/2201.13387",
    "context": "Title: L-SVRG and L-Katyusha with Adaptive Sampling. (arXiv:2201.13387v2 [cs.LG] UPDATED)\nAbstract: Stochastic gradient-based optimization methods, such as L-SVRG and its accelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train machine learning models.The theoretical and empirical performance of L-SVRG and L-Katyusha can be improved by sampling observations from a non-uniform distribution (Qian et al., 2021). However,designing a desired sampling distribution requires prior knowledge of smoothness constants, which can be computationally intractable to obtain in practice when the dimension of the model parameter is high. To address this issue, we propose an adaptive sampling strategy for L-SVRG and L-Katyusha that can learn the sampling distribution with little computational overhead, while allowing it to change with iterates, and at the same time does not require any prior knowledge of the problem parameters. We prove convergence guarantees for L-SVRG and L-Katyusha for convex objectives when the sampling distribution changes with iterates. Our results show that",
    "path": "papers/22/01/2201.13387.json",
    "total_tokens": 799,
    "translated_title": "自适应采样的L-SVRG和L-Katyusha优化方法",
    "translated_abstract": "基于随机梯度的优化方法，如L-SVRG及其加速变种L-Katyusha在训练机器学习模型时得到广泛应用。本文提出了一种自适应采样策略，可以实现在少量计算开销内学习采样分布，同时可以随着迭代而改变，而且不需要先验知识。对于凸目标，我们证明了L-SVRG和L-Katyusha的收敛性保证。",
    "tldr": "本文提出了一种自适应采样策略来提高L-SVRG和L-Katyusha优化方法在训练机器学习模型中的性能表现，可以在少量计算开销内实现采样分布的学习，同时不需要先验知识，并证明了其收敛性保证。",
    "en_tdlr": "This paper proposes an adaptive sampling strategy to improve the performance of L-SVRG and L-Katyusha optimization methods in training machine learning models. The strategy can learn the sampling distribution with little computational overhead, can change with iterates, and does not require prior knowledge. Convergence guarantees are proved for convex objectives."
}