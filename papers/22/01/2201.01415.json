{
    "title": "Problem-dependent attention and effort in neural networks with applications to image resolution and model selection. (arXiv:2201.01415v4 [cs.CV] UPDATED)",
    "abstract": "This paper introduces two new ensemble-based methods to reduce the data and computation costs of image classification. They can be used with any set of classifiers and do not require additional training. In the first approach, data usage is reduced by only analyzing a full-sized image if the model has low confidence in classifying a low-resolution pixelated version. When applied on the best performing classifiers considered here, data usage is reduced by 61.2% on MNIST, 69.6% on KMNIST, 56.3% on FashionMNIST, 84.6% on SVHN, 40.6% on ImageNet, and 27.6% on ImageNet-V2, all with a less than 5% reduction in accuracy. However, for CIFAR-10, the pixelated data are not particularly informative, and the ensemble approach increases data usage while reducing accuracy. In the second approach, compute costs are reduced by only using a complex model if a simpler model has low confidence in its classification. Computation cost is reduced by 82.1% on MNIST, 47.6% on KMNIST, 72.3% on FashionMNIST, 86",
    "link": "http://arxiv.org/abs/2201.01415",
    "context": "Title: Problem-dependent attention and effort in neural networks with applications to image resolution and model selection. (arXiv:2201.01415v4 [cs.CV] UPDATED)\nAbstract: This paper introduces two new ensemble-based methods to reduce the data and computation costs of image classification. They can be used with any set of classifiers and do not require additional training. In the first approach, data usage is reduced by only analyzing a full-sized image if the model has low confidence in classifying a low-resolution pixelated version. When applied on the best performing classifiers considered here, data usage is reduced by 61.2% on MNIST, 69.6% on KMNIST, 56.3% on FashionMNIST, 84.6% on SVHN, 40.6% on ImageNet, and 27.6% on ImageNet-V2, all with a less than 5% reduction in accuracy. However, for CIFAR-10, the pixelated data are not particularly informative, and the ensemble approach increases data usage while reducing accuracy. In the second approach, compute costs are reduced by only using a complex model if a simpler model has low confidence in its classification. Computation cost is reduced by 82.1% on MNIST, 47.6% on KMNIST, 72.3% on FashionMNIST, 86",
    "path": "papers/22/01/2201.01415.json",
    "total_tokens": 1108,
    "translated_title": "问题相关的神经网络关注力和努力在图像分辨率和模型选择中的应用",
    "translated_abstract": "本文介绍了两种新的基于集成的方法，以减少图像分类的数据和计算成本。它们可以与任何分类器集合一起使用，而且不需要额外的训练。在第一种方法中，通过只在模型对低分辨率像素化版本的分类表现出低信心时才分析完整尺寸的图像，从而减少数据使用量。当应用于本文考虑的最佳分类器时，MNIST数据集的数据使用量减少了61.2%，KMNIST减少了69.6%，FashionMNIST减少了56.3%，SVHN减少了84.6%，ImageNet减少了40.6%，ImageNet-V2减少了27.6%，但所有数据集的分类精度均不降低超过5%。然而，对于CIFAR-10，像素化的数据并不特别有信息量，而集成方法会增加数据使用量同时降低精度。在第二种方法中，只有在一个简单模型的分类置信度较低时，才使用一个复杂模型以减少计算成本。在MNIST上的计算成本降低了82.1%，KMNIST降低了47.6%，FashionMNIST降低了72.3%，同时分类精度不降低超过0.5%。",
    "tldr": "本文介绍了两种新的基于集成的方法以减少数据和计算成本。第一种方法通过降低数据使用量来减少成本，第二种方法通过使用较简单的模型来降低计算成本。本文考虑的最佳分类器在所有数据集上的分类精度不降低超过5%。",
    "en_tdlr": "This paper introduces two new ensemble-based methods to reduce the data and computation costs of image classification. The first method reduces data usage by analyzing a full-sized image only when the model has low confidence in classifying a low-resolution pixelated version. The second method reduces computation cost by using a complex model only when a simpler model has low confidence in its classification. The best performing classifiers considered in this paper achieve classification accuracy reduction by less than 5% on all datasets."
}