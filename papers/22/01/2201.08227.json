{
    "title": "Learning Multi-agent Skills for Tabular Reinforcement Learning using Factor Graphs. (arXiv:2201.08227v3 [cs.MA] UPDATED)",
    "abstract": "Covering skill (a.k.a., option) discovery has been developed to improve the exploration of reinforcement learning in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. However, these option discovery methods cannot be directly extended to multi-agent scenarios, since the joint state space grows exponentially with the number of agents in the system. Thus, existing researches on adopting options in multi-agent scenarios still rely on single-agent option discovery and fail to directly discover the joint options that can improve the connectivity of the joint state space of agents. In this paper, we show that it is indeed possible to directly compute multi-agent options with collaborative exploratory behaviors among the agents, while still enjoying the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph -- the Kronecker ",
    "link": "http://arxiv.org/abs/2201.08227",
    "context": "Title: Learning Multi-agent Skills for Tabular Reinforcement Learning using Factor Graphs. (arXiv:2201.08227v3 [cs.MA] UPDATED)\nAbstract: Covering skill (a.k.a., option) discovery has been developed to improve the exploration of reinforcement learning in single-agent scenarios with sparse reward signals, through connecting the most distant states in the embedding space provided by the Fiedler vector of the state transition graph. However, these option discovery methods cannot be directly extended to multi-agent scenarios, since the joint state space grows exponentially with the number of agents in the system. Thus, existing researches on adopting options in multi-agent scenarios still rely on single-agent option discovery and fail to directly discover the joint options that can improve the connectivity of the joint state space of agents. In this paper, we show that it is indeed possible to directly compute multi-agent options with collaborative exploratory behaviors among the agents, while still enjoying the ease of decomposition. Our key idea is to approximate the joint state space as a Kronecker graph -- the Kronecker ",
    "path": "papers/22/01/2201.08227.json",
    "total_tokens": 840,
    "translated_title": "使用因子图学习表格强化学习的多智能体技能",
    "translated_abstract": "技能发现被开发用于改善单智能体情景中稀疏奖励信号的强化学习探索能力，通过连接状态转移图的Fiedler向量提供的嵌入空间中最远的状态。然而，这些技能发现方法无法直接推广到多智能体场景，因为系统中的智能体数量增加，联合状态空间呈指数增长。因此，现有研究在多智能体场景中采用技能仍然依赖于单智能体技能发现，并未直接发现能够改善智能体联合状态空间连通性的联合技能。在本文中，我们展示了使用合作性探索行为在智能体之间直接计算多智能体技能的可行性，同时仍然享受分解的便利性。我们的关键思想是将联合状态空间逼近为Kronecker图——Kronecker",
    "tldr": "本论文提出了一个方法可以直接在多智能体场景中计算多智能体技能，通过智能体之间的合作性探索行为来改善联合状态空间的连通性。",
    "en_tdlr": "This paper proposes a method to directly compute multi-agent skills in multi-agent scenarios, improving the connectivity of the joint state space through collaborative exploratory behaviors among agents."
}