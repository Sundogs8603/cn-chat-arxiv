{
    "title": "Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms. (arXiv:2201.07856v2 [cs.AI] UPDATED)",
    "abstract": "An increasing number of reports raise concerns about the risk that machine learning algorithms could amplify health disparities due to biases embedded in the training data. Seyyed-Kalantari et al. find that models trained on three chest X-ray datasets yield disparities in false-positive rates (FPR) across subgroups on the 'no-finding' label (indicating the absence of disease). The models consistently yield higher FPR on subgroups known to be historically underserved, and the study concludes that the models exhibit and potentially even amplify systematic underdiagnosis. We argue that the experimental setup in the study is insufficient to study algorithmic underdiagnosis. In the absence of specific knowledge (or assumptions) about the extent and nature of the dataset bias, it is difficult to investigate model bias. Importantly, their use of test data exhibiting the same bias as the training data (due to random splitting) severely complicates the interpretation of the reported disparities",
    "link": "http://arxiv.org/abs/2201.07856",
    "context": "Title: Potential sources of dataset bias complicate investigation of underdiagnosis by machine learning algorithms. (arXiv:2201.07856v2 [cs.AI] UPDATED)\nAbstract: An increasing number of reports raise concerns about the risk that machine learning algorithms could amplify health disparities due to biases embedded in the training data. Seyyed-Kalantari et al. find that models trained on three chest X-ray datasets yield disparities in false-positive rates (FPR) across subgroups on the 'no-finding' label (indicating the absence of disease). The models consistently yield higher FPR on subgroups known to be historically underserved, and the study concludes that the models exhibit and potentially even amplify systematic underdiagnosis. We argue that the experimental setup in the study is insufficient to study algorithmic underdiagnosis. In the absence of specific knowledge (or assumptions) about the extent and nature of the dataset bias, it is difficult to investigate model bias. Importantly, their use of test data exhibiting the same bias as the training data (due to random splitting) severely complicates the interpretation of the reported disparities",
    "path": "papers/22/01/2201.07856.json",
    "total_tokens": 1031,
    "translated_title": "数据集偏倚的潜在来源使机器学习算法对未诊断问题的研究复杂化",
    "translated_abstract": "越来越多的报告引起了对机器学习算法可能放大由训练数据中嵌入的偏见导致的健康差异的担忧。 Seyyed-Kalantari等人发现，在三个胸部X射线数据集上训练的模型在“未发现”标签（表示没有疾病）的亚组之间产生了虚报率（FPR）的差异。这些模型在已知历史上未得到足够服务的亚组中一直产生更高的FPR，并且该研究得出结论，这些模型显示并且可能会放大系统的未诊断问题。我们认为该研究中的实验设置不足以研究算法未诊断问题。在缺乏关于数据集偏倚程度和性质的具体知识（或假设）的情况下，很难调查模型偏倚问题。重要的是，他们使用的测试数据展示了与训练数据相同的偏倚（由于随机分割），严重复杂化了所报道的差异的解释。",
    "tldr": "这项研究发现，机器学习算法在胸部X射线数据集上训练时会在未得到足够服务的人群中产生高虚报率，可能放大了系统的未诊断问题。然而，研究的实验设置不足以全面研究算法的未诊断问题，而且使用与训练数据相同偏倚的测试数据进一步加剧了结果的解释难度。",
    "en_tdlr": "This study finds that machine learning algorithms trained on chest X-ray datasets yield higher false-positive rates in underserved populations, potentially amplifying systematic underdiagnosis. However, the experimental setup of the study is insufficient to thoroughly investigate algorithmic underdiagnosis, and the use of test data exhibiting the same bias as the training data further complicates the interpretation of the reported disparities."
}