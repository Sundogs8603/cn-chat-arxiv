{
    "title": "A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v3 [cs.CL] UPDATED)",
    "abstract": "Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that are more natural and better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the lower level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks which may require different types of controlled constraints. In this paper, we present a systematic critical review on the com",
    "link": "http://arxiv.org/abs/2201.05337",
    "context": "Title: A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v3 [cs.CL] UPDATED)\nAbstract: Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that are more natural and better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the lower level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks which may require different types of controlled constraints. In this paper, we present a systematic critical review on the com",
    "path": "papers/22/01/2201.05337.json",
    "total_tokens": 900,
    "translated_title": "基于Transformer预训练语言模型的可控文本生成综述",
    "translated_abstract": "可控文本生成是自然语言生成领域中新兴的方向，被认为对于开发更自然、更符合特定应用场景的先进文本生成技术至关重要。近年来，利用大规模预训练语言模型（PLMs），尤其是广泛使用的基于Transformer的PLMs，已成为自然语言生成新范式，可以生成更多样化、更流畅的文本。然而，由于深度神经网络的可解释性较低，这些方法的可控性需要得到保证。为此，基于Transformer的PLMs的可控文本生成已成为一个快速增长但具有挑战性的新研究热点。最近3-4年出现了各种方法，针对可能需要不同类型控制约束的不同CTG任务。在本文中，我们对当前基于Transformer预训练语言模型的可控文本生成方法进行了系统的综述。",
    "tldr": "这篇论文综述了基于Transformer预训练语言模型的可控文本生成方法。这些方法利用大规模预训练语言模型生成多样化、流畅的文本，但由于深度神经网络的可解释性较低，需要保证其可控性。",
    "en_tdlr": "This paper provides a systematic review of controllable text generation methods based on Transformer pre-trained language models. These methods use large-scale pre-trained language models to generate diverse and fluent text, but their controllability needs to be ensured due to the low interpretability of deep neural networks."
}