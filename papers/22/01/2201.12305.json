{
    "title": "A Post-Quantum Associative Memory. (arXiv:2201.12305v2 [quant-ph] CROSS LISTED)",
    "abstract": "Associative memories are devices storing information that can be fully retrieved given partial disclosure of it. We examine a toy model of associative memory and the ultimate limitations it is subjected to within the framework of general probabilistic theories (GPTs), which represent the most general class of physical theories satisfying some basic operational axioms. We ask ourselves how large the dimension of a GPT should be so that it can accommodate $2^m$ states with the property that any $N$ of them are perfectly distinguishable. Call $d(N,m)$ the minimal such dimension. Invoking an old result by Danzer and Gr\\\"unbaum, we prove that $d(2,m)=m+1$, to be compared with $O(2^m)$ when the GPT is required to be either classical or quantum. This yields an example of a task where GPTs outperform both classical and quantum theory exponentially. More generally, we resolve the case of fixed $N$ and asymptotically large $m$, proving that $d(N,m) \\leq m^{1+o_N(1)}$ (as $m\\to\\infty$) for every ",
    "link": "http://arxiv.org/abs/2201.12305",
    "context": "Title: A Post-Quantum Associative Memory. (arXiv:2201.12305v2 [quant-ph] CROSS LISTED)\nAbstract: Associative memories are devices storing information that can be fully retrieved given partial disclosure of it. We examine a toy model of associative memory and the ultimate limitations it is subjected to within the framework of general probabilistic theories (GPTs), which represent the most general class of physical theories satisfying some basic operational axioms. We ask ourselves how large the dimension of a GPT should be so that it can accommodate $2^m$ states with the property that any $N$ of them are perfectly distinguishable. Call $d(N,m)$ the minimal such dimension. Invoking an old result by Danzer and Gr\\\"unbaum, we prove that $d(2,m)=m+1$, to be compared with $O(2^m)$ when the GPT is required to be either classical or quantum. This yields an example of a task where GPTs outperform both classical and quantum theory exponentially. More generally, we resolve the case of fixed $N$ and asymptotically large $m$, proving that $d(N,m) \\leq m^{1+o_N(1)}$ (as $m\\to\\infty$) for every ",
    "path": "papers/22/01/2201.12305.json",
    "total_tokens": 973,
    "translated_title": "后量子联想记忆",
    "translated_abstract": "联想记忆是一种可以通过部分信息检索完整信息的设备。在广义概率论（GPT）的框架内，我们研究了一种联想记忆的玩具模型，以及它所受到的极限限制。在GPT的规定下，我们探讨了可以容纳$2^m$个状态且其中的任意$N$个状态都是完全可区分的最小GPT维度$d(N,m)$。通过引用Danzer和Grünbaum的老结果，我们证明了当$m$为2时$d(2,m)=m+1$，而在需要GPT分别为经典或量子理论的情况下，该维度分别为$O(2^m)$。这提供了一个例子表明，在某些任务上GPT比经典和量子理论都有指数级的优势。更一般地，我们解决了固定$N$和渐近大的$m$的情况，证明了$d(N,m) \\leq m^{1+o_N(1)}$（当$m\\to\\infty$时）。",
    "tldr": "该论文研究了一种基于广义概率理论(GPT)的联想记忆的模型，证明了GPT可以比经典和量子理论更好地处理具有指数级优势的特定任务。",
    "en_tdlr": "This paper investigates a toy model of associative memory based on general probabilistic theories (GPTs), which can outperform classical and quantum theories exponentially in certain tasks. The authors prove that the minimal GPT dimension to accommodate $2^m$ perfectly distinguishable states is $m+1$, compared to $O(2^m)$ for classical and quantum theories. The study provides a new perspective on the potential of GPTs in information processing."
}