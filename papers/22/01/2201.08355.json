{
    "title": "Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees. (arXiv:2201.08355v4 [cs.RO] UPDATED)",
    "abstract": "Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world. In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior. In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution. To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments. Additionally, inheriting from the HJ ",
    "link": "http://arxiv.org/abs/2201.08355",
    "context": "Title: Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees. (arXiv:2201.08355v4 [cs.RO] UPDATED)\nAbstract: Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world. In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior. In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution. To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments. Additionally, inheriting from the HJ ",
    "path": "papers/22/01/2201.08355.json",
    "total_tokens": 1239,
    "translated_title": "模拟-实验室-真实环境下的安全强化学习及泛化保证",
    "translated_abstract": "安全性是自主系统的关键组成部分，对于基于学习的策略在真实世界中的应用仍然是一个挑战。特别是，使用强化学习学习的策略由于不安全的行为而经常无法推广到新颖的环境中。在本文中，我们提出了模拟-实验室-真实环境下的框架，利用概率保证的安全感知策略分布来弥合现实差距。为了提高安全性，我们采用双重策略设置，一个性能策略使用累积任务奖励进行训练，而备份（安全）策略则通过求解基于哈密顿-雅可比（HJ）可达性分析的安全贝尔曼方程进行训练。在模拟-实验室转移中，我们采用监督控制策略，在探索过程中保护不安全的行为；在实验室-真实转移中，我们利用“可能近似正确（PAC）-Bayes”框架为在未知环境中的策略提供性能和安全性的下限。此外，我们还从HJ可达性分析中得出了我们策略的泛化保证。我们在移动机器人导航任务和四轴飞行任务上对所提出的模拟-实验室-真实环境框架进行了评估，并进行了实验验证，结果表明相比基线方法，该框架提高了安全性和泛化能力。",
    "tldr": "本文提出了模拟-实验室-真实环境下的安全强化学习及泛化保证框架，利用概率保证的安全感知策略分布来弥合现实差距，具有双重策略设置、监督控制策略和“可能近似正确（PAC）-Bayes”框架等特点，能够在移动机器人导航和四轴飞行等任务中实现改进的安全性和泛化能力。",
    "en_tdlr": "This article proposes a Sim-to-Lab-to-Real framework for safe reinforcement learning and generalization guarantees in autonomous systems. The framework utilizes a probabilistically guaranteed safety-aware policy distribution, dual policy setup, supervisory control scheme, and Probably Approximately Correct (PAC)-Bayes framework. It demonstrated improved safety and generalization capabilities compared to baseline methods in mobile robot navigation and quadcopter flying tasks."
}