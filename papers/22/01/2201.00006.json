{
    "title": "Leveraging Queue Length and Attention Mechanisms for Enhanced Traffic Signal Control Optimization. (arXiv:2201.00006v3 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning (RL) techniques for traffic signal control (TSC) have gained increasing popularity in recent years. However, most existing RL-based TSC methods tend to focus primarily on the RL model structure while neglecting the significance of proper traffic state representation. Furthermore, some RL-based methods heavily rely on expert-designed traffic signal phase competition. In this paper, we present a novel approach to TSC that utilizes queue length as an efficient state representation. We propose two new methods: (1) Max Queue-Length (M-QL), an optimization-based traditional method designed based on the property of queue length; and (2) AttentionLight, an RL model that employs the self-attention mechanism to capture the signal phase correlation without requiring human knowledge of phase relationships. Comprehensive experiments on multiple real-world datasets demonstrate the effectiveness of our approach: (1) the M-QL method outperforms the latest RL-based methods; (2) A",
    "link": "http://arxiv.org/abs/2201.00006",
    "context": "Title: Leveraging Queue Length and Attention Mechanisms for Enhanced Traffic Signal Control Optimization. (arXiv:2201.00006v3 [cs.LG] UPDATED)\nAbstract: Reinforcement learning (RL) techniques for traffic signal control (TSC) have gained increasing popularity in recent years. However, most existing RL-based TSC methods tend to focus primarily on the RL model structure while neglecting the significance of proper traffic state representation. Furthermore, some RL-based methods heavily rely on expert-designed traffic signal phase competition. In this paper, we present a novel approach to TSC that utilizes queue length as an efficient state representation. We propose two new methods: (1) Max Queue-Length (M-QL), an optimization-based traditional method designed based on the property of queue length; and (2) AttentionLight, an RL model that employs the self-attention mechanism to capture the signal phase correlation without requiring human knowledge of phase relationships. Comprehensive experiments on multiple real-world datasets demonstrate the effectiveness of our approach: (1) the M-QL method outperforms the latest RL-based methods; (2) A",
    "path": "papers/22/01/2201.00006.json",
    "total_tokens": 986,
    "translated_title": "利用队列长度和注意力机制增强交通信号控制优化",
    "translated_abstract": "近年来，强化学习技术在交通信号控制中获得了越来越多的关注。然而，大多数现有的基于强化学习的交通信号控制方法往往主要关注强化学习模型结构，而忽视了适当的交通状态表示的重要性。此外，一些基于强化学习的方法在很大程度上依赖于专家设计的交通信号相位竞争。在本文中，我们提出了一种利用队列长度作为高效状态表示的TSC新方法。我们提出了两种新方法：(1) 基于队列长度属性设计的优化传统方法Max Queue-Length (M-QL)；(2) AttentionLight，一种利用自注意力机制捕捉信号相位相关性的强化学习模型，而无需人工知识的相位关系。对多个实际数据集进行的综合实验表明了我们方法的有效性：(1) M-QL方法优于最新的基于强化学习的方法；(2) 适用于各种交通场景，且相对于专家设计的方法具有更好的性能表现。",
    "tldr": "这篇论文提出了一种利用队列长度和注意力机制的交通信号控制优化方法。作者提出了Max Queue-Length (M-QL)和AttentionLight两种新方法，实验结果表明M-QL方法优于现有的强化学习方法，并且AttentionLight方法适用于各种交通场景并具有更好的性能表现。"
}