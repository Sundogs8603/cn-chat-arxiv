{
    "title": "Representations learnt by SGD and Adaptive learning rules: Conditions that vary sparsity and selectivity in neural network",
    "abstract": "arXiv:2201.11653v2 Announce Type: replace  Abstract: From the point of view of the human brain, continual learning can perform various tasks without mutual interference. An effective way to reduce mutual interference can be found in sparsity and selectivity of neurons. According to Aljundi et al. and Hadsell et al., imposing sparsity at the representational level is advantageous for continual learning because sparse neuronal activations encourage less overlap between parameters, resulting in less interference. Similarly, highly selective neural networks are likely to induce less interference since particular response in neurons will reduce the chance of overlap with other parameters. Considering that the human brain performs continual learning over the lifespan, finding conditions where sparsity and selectivity naturally arises may provide insight for understanding how the brain functions. This paper investigates various conditions that naturally increase sparsity and selectivity in a ",
    "link": "https://arxiv.org/abs/2201.11653",
    "context": "Title: Representations learnt by SGD and Adaptive learning rules: Conditions that vary sparsity and selectivity in neural network\nAbstract: arXiv:2201.11653v2 Announce Type: replace  Abstract: From the point of view of the human brain, continual learning can perform various tasks without mutual interference. An effective way to reduce mutual interference can be found in sparsity and selectivity of neurons. According to Aljundi et al. and Hadsell et al., imposing sparsity at the representational level is advantageous for continual learning because sparse neuronal activations encourage less overlap between parameters, resulting in less interference. Similarly, highly selective neural networks are likely to induce less interference since particular response in neurons will reduce the chance of overlap with other parameters. Considering that the human brain performs continual learning over the lifespan, finding conditions where sparsity and selectivity naturally arises may provide insight for understanding how the brain functions. This paper investigates various conditions that naturally increase sparsity and selectivity in a ",
    "path": "papers/22/01/2201.11653.json",
    "total_tokens": 795,
    "translated_title": "《SGD和自适应学习规则学到的表示：变化稀疏性和选择性的神经网络条件》",
    "translated_abstract": "从人脑的角度来看，连续学习可以执行各种任务而互不干扰。减少互相干扰的有效方式可以在神经元的稀疏性和选择性中找到。根据Aljundi等人和Hadsell等人的观点，在表示水平施加稀疏性对连续学习是有利的，因为稀疏的神经元激活鼓励参数之间的少重叠，导致更少的干扰。同样，高度选择性的神经网络可能会引起较少的干扰，因为神经元中的特定响应将减少与其他参数的重叠机会。考虑到人脑在一生中执行连续学习，找到自然增加稀疏性和选择性的条件可能为了解大脑功能提供见解。本文调查了自然增加神经网络稀疏性和选择性的各种条件。",
    "tldr": "本文调查了导致神经网络稀疏性和选择性增加的各种条件。",
    "en_tdlr": "This paper investigates various conditions that naturally increase sparsity and selectivity in neural networks."
}