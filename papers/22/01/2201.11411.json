{
    "title": "Restarted Nonconvex Accelerated Gradient Descent: No More Polylogarithmic Factor in the $O(\\epsilon^{-7/4})$ Complexity. (arXiv:2201.11411v4 [math.OC] UPDATED)",
    "abstract": "This paper studies accelerated gradient methods for nonconvex optimization with Lipschitz continuous gradient and Hessian. We propose two simple accelerated gradient methods, restarted accelerated gradient descent (AGD) and restarted heavy ball (HB) method, and establish that our methods achieve an $\\epsilon$-approximate first-order stationary point within $O(\\epsilon^{-7/4})$ number of gradient evaluations by elementary proofs. Theoretically, our complexity does not hide any polylogarithmic factors, and thus it improves over the best known one by the $O(\\log\\frac{1}{\\epsilon})$ factor. Our algorithms are simple in the sense that they only consist of Nesterov's classical AGD or Polyak's HB iterations, as well as a restart mechanism. They do not invoke negative curvature exploitation or minimization of regularized surrogate functions as the subroutines. In contrast with existing analysis, our elementary proofs use less advanced techniques and do not invoke the analysis of strongly conve",
    "link": "http://arxiv.org/abs/2201.11411",
    "context": "Title: Restarted Nonconvex Accelerated Gradient Descent: No More Polylogarithmic Factor in the $O(\\epsilon^{-7/4})$ Complexity. (arXiv:2201.11411v4 [math.OC] UPDATED)\nAbstract: This paper studies accelerated gradient methods for nonconvex optimization with Lipschitz continuous gradient and Hessian. We propose two simple accelerated gradient methods, restarted accelerated gradient descent (AGD) and restarted heavy ball (HB) method, and establish that our methods achieve an $\\epsilon$-approximate first-order stationary point within $O(\\epsilon^{-7/4})$ number of gradient evaluations by elementary proofs. Theoretically, our complexity does not hide any polylogarithmic factors, and thus it improves over the best known one by the $O(\\log\\frac{1}{\\epsilon})$ factor. Our algorithms are simple in the sense that they only consist of Nesterov's classical AGD or Polyak's HB iterations, as well as a restart mechanism. They do not invoke negative curvature exploitation or minimization of regularized surrogate functions as the subroutines. In contrast with existing analysis, our elementary proofs use less advanced techniques and do not invoke the analysis of strongly conve",
    "path": "papers/22/01/2201.11411.json",
    "total_tokens": 1039,
    "translated_title": "重启的非凸加速梯度下降：在$O(\\epsilon^{-7/4})$复杂度中不再需要多项式对数因子",
    "translated_abstract": "本文研究具有Lipschitz连续梯度和Hessian的非凸优化的加速梯度方法。我们提出了两种简单的加速梯度方法，重启式加速梯度下降（AGD）和重启式重球（HB）方法，并通过基本证明证明了我们的方法可以在$O(\\epsilon^{-7/4})$个梯度评估内达到$\\epsilon$近似的一阶稳定点。在理论上，我们的复杂度没有隐藏任何的多项式对数因子，并因此比已知的最佳复杂度提高了$O(\\log \\frac{1}{\\epsilon})$的因子。我们的算法非常简单，因为它们只是由Nesterov的经典AGD或Polyak的HB迭代以及重启机制组成。与现有的分析相比，我们的基本证明使用了不那么先进的技术，并且不涉及强凸性的分析或正则化代理函数的最小化作为子例程。",
    "tldr": "本文提出了重启式加速梯度下降（AGD）和重启式重球（HB）方法来解决非凸优化问题，并通过基本证明证明了这些方法可以在$O(\\epsilon^{-7/4})$个梯度评估内达到$\\epsilon$近似的一阶稳定点。这项工作的复杂度没有多项式对数因子，并优于以前的最佳复杂度。",
    "en_tdlr": "The paper proposes a Restarted Accelerated Gradient Descent (AGD) and a Restarted Heavy Ball (HB) method for nonconvex optimization problems. These methods achieve an $\\epsilon$-approximate first-order stationary point within $O(\\epsilon^{-7/4})$ number of gradient evaluations with elementary proofs, improving over the best-known complexity by a factor of $O(\\log\\frac{1}{\\epsilon})$ without any polylogarithmic factors."
}