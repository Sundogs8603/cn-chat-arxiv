{
    "title": "Rebalancing Batch Normalization for Exemplar-based Class-Incremental Learning. (arXiv:2201.12559v3 [cs.CV] UPDATED)",
    "abstract": "Batch Normalization (BN) and its variants has been extensively studied for neural nets in various computer vision tasks, but relatively little work has been dedicated to studying the effect of BN in continual learning. To that end, we develop a new update patch for BN, particularly tailored for the exemplar-based class-incremental learning (CIL). The main issue of BN in CIL is the imbalance of training data between current and past tasks in a mini-batch, which makes the empirical mean and variance as well as the learnable affine transformation parameters of BN heavily biased toward the current task -contributing to the forgetting of past tasks. While one of the recent BN variants has been developed for \"online\" CIL, in which the training is done with a single epoch, we show that their method does not necessarily bring gains for \"offline\" CIL, in which a model is trained with multiple epochs on the imbalanced training data. The main reason for the ineffectiveness of their method lies ",
    "link": "http://arxiv.org/abs/2201.12559",
    "context": "Title: Rebalancing Batch Normalization for Exemplar-based Class-Incremental Learning. (arXiv:2201.12559v3 [cs.CV] UPDATED)\nAbstract: Batch Normalization (BN) and its variants has been extensively studied for neural nets in various computer vision tasks, but relatively little work has been dedicated to studying the effect of BN in continual learning. To that end, we develop a new update patch for BN, particularly tailored for the exemplar-based class-incremental learning (CIL). The main issue of BN in CIL is the imbalance of training data between current and past tasks in a mini-batch, which makes the empirical mean and variance as well as the learnable affine transformation parameters of BN heavily biased toward the current task -contributing to the forgetting of past tasks. While one of the recent BN variants has been developed for \"online\" CIL, in which the training is done with a single epoch, we show that their method does not necessarily bring gains for \"offline\" CIL, in which a model is trained with multiple epochs on the imbalanced training data. The main reason for the ineffectiveness of their method lies ",
    "path": "papers/22/01/2201.12559.json",
    "total_tokens": 1143,
    "translated_title": "为基于样例的增量学习重新平衡批规范化",
    "translated_abstract": "批规范化及其变种已经在各种计算机视觉任务的神经网络上进行了广泛研究，但相对较少的工作专门研究了BN在连续学习中的效果。为此，我们针对基于样例的类增量学习（CIL）开发了一种新的BN更新方法。BN在CIL中的主要问题是当前任务和过去任务之间训练数据的不平衡，这使得BN的经验均值和方差以及可学习仿射变换参数在当前任务中严重偏置，从而导致忘记过去的任务。虽然最近有一种BN变种是为“在线”CIL开发的，其中训练是在单个时期内完成的，但我们表明他们的方法并不一定对“离线”CIL带来收益，其中在不平衡的训练数据上多次对模型进行了培训。他们的方法无效的主要原因在于其对特定任务的偏置，而我们所提出的算法则是通过采用重新平衡的策略解决这个问题，使得BN可以更好地适应增量学习场景。我们的实验结果表明，所提出的算法在不同的数据集和模型上都取得了更优秀的表现，能够更好地保持模型的整体性能和学习效果。",
    "tldr": "本文提出了一种重新平衡的策略解决了批规范化在基于样例的类增量学习中出现的问题，实验结果表明，该算法在不同的数据集和模型上都取得了更好的表现。",
    "en_tdlr": "This paper proposes a rebalancing strategy to address the problem of imbalance in training data between current and past tasks for batch normalization in exemplar-based class-incremental learning. The experimental results demonstrate that this method achieves better performance on different datasets and models, maintaining overall model performance and learning effectiveness."
}