{
    "title": "Learning Stationary Nash Equilibrium Policies in $n$-Player Stochastic Games with Independent Chains. (arXiv:2201.12224v4 [cs.LG] UPDATED)",
    "abstract": "We consider a subclass of $n$-player stochastic games, in which players have their own internal state/action spaces while they are coupled through their payoff functions. It is assumed that players' internal chains are driven by independent transition probabilities. Moreover, players can receive only realizations of their payoffs, not the actual functions, and cannot observe each other's states/actions. For this class of games, we first show that finding a stationary Nash equilibrium (NE) policy without any assumption on the reward functions is interactable. However, for general reward functions, we develop polynomial-time learning algorithms based on dual averaging and dual mirror descent, which converge in terms of the averaged Nikaido-Isoda distance to the set of $\\epsilon$-NE policies almost surely or in expectation. In particular, under extra assumptions on the reward functions such as social concavity, we derive polynomial upper bounds on the number of iterates to achieve an $\\ep",
    "link": "http://arxiv.org/abs/2201.12224",
    "context": "Title: Learning Stationary Nash Equilibrium Policies in $n$-Player Stochastic Games with Independent Chains. (arXiv:2201.12224v4 [cs.LG] UPDATED)\nAbstract: We consider a subclass of $n$-player stochastic games, in which players have their own internal state/action spaces while they are coupled through their payoff functions. It is assumed that players' internal chains are driven by independent transition probabilities. Moreover, players can receive only realizations of their payoffs, not the actual functions, and cannot observe each other's states/actions. For this class of games, we first show that finding a stationary Nash equilibrium (NE) policy without any assumption on the reward functions is interactable. However, for general reward functions, we develop polynomial-time learning algorithms based on dual averaging and dual mirror descent, which converge in terms of the averaged Nikaido-Isoda distance to the set of $\\epsilon$-NE policies almost surely or in expectation. In particular, under extra assumptions on the reward functions such as social concavity, we derive polynomial upper bounds on the number of iterates to achieve an $\\ep",
    "path": "papers/22/01/2201.12224.json",
    "total_tokens": 955,
    "translated_title": "独立链$n$人随机博弈中学习稳定纳什均衡策略",
    "translated_abstract": "我们研究了一个$n$人随机博弈的子类，在该子类中，玩家具有自己的内部状态/动作空间，但通过其支付功能相互耦合。假设玩家的内部链是由独立的转移概率驱动的。此外，玩家只能收到他们的支付实现，而不是实际的功能，并且不能观察彼此的状态/动作。针对这类游戏，我们首先表明，不假设奖励函数，求解固定纳什均衡（NE）策略是一个不可交互的问题。然而，对于一般的奖励函数，我们基于对偶平均和对偶镜面下降的多项式时间学习算法，这些算法以期望或几乎确定地以平均的Nikaido-Isoda距离收敛到$\\epsilon$-NE策略集合。特别地，我们在额外假设奖励函数为社会凸性的情况下，推导出多项式上界，以实现$\\epsilon$-NE策略所需的迭代次数。",
    "tldr": "本文针对一个$n$人随机博弈的子类，设计了多项式时间的学习算法，可以学习出$\\epsilon$-NE策略，在奖励函数为社会凸性的情况下具有多项式复杂度上限。",
    "en_tdlr": "This paper presents polynomial-time learning algorithms for a subclass of $n$-player stochastic games, which can learn $\\epsilon$-NE policies with polynomial upper bounds on the number of iterates, especially under the assumption of social concavity reward functions."
}