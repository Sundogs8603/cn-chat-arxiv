{
    "title": "Conservative Distributional Reinforcement Learning with Safety Constraints. (arXiv:2201.07286v2 [cs.LG] UPDATED)",
    "abstract": "Safety exploration can be regarded as a constrained Markov decision problem where the expected long-term cost is constrained. Previous off-policy algorithms convert the constrained optimization problem into the corresponding unconstrained dual problem by introducing the Lagrangian relaxation technique. However, the cost function of the above algorithms provides inaccurate estimations and causes the instability of the Lagrange multiplier learning. In this paper, we present a novel off-policy reinforcement learning algorithm called Conservative Distributional Maximum a Posteriori Policy Optimization (CDMPO). At first, to accurately judge whether the current situation satisfies the constraints, CDMPO adapts distributional reinforcement learning method to estimate the Q-function and C-function. Then, CDMPO uses a conservative value function loss to reduce the number of violations of constraints during the exploration process. In addition, we utilize Weighted Average Proportional Integral D",
    "link": "http://arxiv.org/abs/2201.07286",
    "context": "Title: Conservative Distributional Reinforcement Learning with Safety Constraints. (arXiv:2201.07286v2 [cs.LG] UPDATED)\nAbstract: Safety exploration can be regarded as a constrained Markov decision problem where the expected long-term cost is constrained. Previous off-policy algorithms convert the constrained optimization problem into the corresponding unconstrained dual problem by introducing the Lagrangian relaxation technique. However, the cost function of the above algorithms provides inaccurate estimations and causes the instability of the Lagrange multiplier learning. In this paper, we present a novel off-policy reinforcement learning algorithm called Conservative Distributional Maximum a Posteriori Policy Optimization (CDMPO). At first, to accurately judge whether the current situation satisfies the constraints, CDMPO adapts distributional reinforcement learning method to estimate the Q-function and C-function. Then, CDMPO uses a conservative value function loss to reduce the number of violations of constraints during the exploration process. In addition, we utilize Weighted Average Proportional Integral D",
    "path": "papers/22/01/2201.07286.json",
    "total_tokens": 831,
    "translated_title": "具有安全约束的保守分布强化学习",
    "translated_abstract": "安全探索可以被看作是一个受约束的马尔可夫决策问题，在这个问题中，预期的长期成本是受到约束的。先前的离策略算法通过引入拉格朗日松弛技术，将约束优化问题转换为相应的无约束对偶问题。然而，上述算法的成本函数提供了不准确的估计，并导致拉格朗日乘子学习的不稳定性。在本文中，我们提出了一种新颖的离策略强化学习算法，称为保守分布最大后验策略优化(CDMPO)。首先，为了准确判断当前情况是否满足约束条件，CDMPO采用分布强化学习方法来估计Q函数和C函数。然后，CDMPO使用保守的价值函数损失来减少探索过程中违约次数。此外，我们利用加权平均比例积分D进行优化。",
    "tldr": "本文提出了一种名为保守分布最大后验策略优化 (CDMPO) 的离策略强化学习算法，通过使用分布强化学习方法来估计Q函数和C函数，以及使用保守的价值函数损失来减少约束违反次数。"
}