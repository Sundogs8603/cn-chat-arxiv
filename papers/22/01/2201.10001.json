{
    "title": "E-ADDA: Unsupervised Adversarial Domain Adaptation Enhanced by a New Mahalanobis Distance Loss for Smart Computing. (arXiv:2201.10001v5 [cs.LG] UPDATED)",
    "abstract": "In smart computing, the labels of training samples for a specific task are not always abundant. However, the labels of samples in a relevant but different dataset are available. As a result, researchers have relied on unsupervised domain adaptation to leverage the labels in a dataset (the source domain) to perform better classification in a different, unlabeled dataset (target domain). Existing non-generative adversarial solutions for UDA aim at achieving domain confusion through adversarial training. The ideal scenario is that perfect domain confusion is achieved, but this is not guaranteed to be true. To further enforce domain confusion on top of the adversarial training, we propose a novel UDA algorithm, \\textit{E-ADDA}, which uses both a novel variation of the Mahalanobis distance loss and an out-of-distribution detection subroutine. The Mahalanobis distance loss minimizes the distribution-wise distance between the encoded target samples and the distribution of the source domain, t",
    "link": "http://arxiv.org/abs/2201.10001",
    "context": "Title: E-ADDA: Unsupervised Adversarial Domain Adaptation Enhanced by a New Mahalanobis Distance Loss for Smart Computing. (arXiv:2201.10001v5 [cs.LG] UPDATED)\nAbstract: In smart computing, the labels of training samples for a specific task are not always abundant. However, the labels of samples in a relevant but different dataset are available. As a result, researchers have relied on unsupervised domain adaptation to leverage the labels in a dataset (the source domain) to perform better classification in a different, unlabeled dataset (target domain). Existing non-generative adversarial solutions for UDA aim at achieving domain confusion through adversarial training. The ideal scenario is that perfect domain confusion is achieved, but this is not guaranteed to be true. To further enforce domain confusion on top of the adversarial training, we propose a novel UDA algorithm, \\textit{E-ADDA}, which uses both a novel variation of the Mahalanobis distance loss and an out-of-distribution detection subroutine. The Mahalanobis distance loss minimizes the distribution-wise distance between the encoded target samples and the distribution of the source domain, t",
    "path": "papers/22/01/2201.10001.json",
    "total_tokens": 925,
    "translated_abstract": "在智能计算中，对于特定任务的训练样本标签并不总是丰富的。但是，在相关但不同的数据集中得到了样本标签。因此，研究人员依赖于无监督领域适应来利用数据集中的标签（源域）以在不同的未标记数据集（目标域）中执行更好的分类。现有非生成对抗解决方案旨在通过对抗训练实现域混淆。理想情况下，可以实现完美的领域混淆，但不能保证这一点。为了在对抗训练之上进一步强制执行领域混淆，我们提出了一种新的UDA算法，称为“E-ADDA”，它使用马氏距离损失的一种新变体和一个超出分布检测子程序。",
    "tldr": "E-ADDA是一种无监督领域适应算法，通过马氏距离损失和超出分布检测子程序强制执行领域混淆，以实现在不同的未标记数据集上更好的分类性能。"
}