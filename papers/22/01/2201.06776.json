{
    "title": "Pruning-aware Sparse Regularization for Network Pruning. (arXiv:2201.06776v2 [cs.CV] UPDATED)",
    "abstract": "Structural neural network pruning aims to remove the redundant channels in the deep convolutional neural networks (CNNs) by pruning the filters of less importance to the final output accuracy. To reduce the degradation of performance after pruning, many methods utilize the loss with sparse regularization to produce structured sparsity. In this paper, we analyze these sparsity-training-based methods and find that the regularization of unpruned channels is unnecessary. Moreover, it restricts the network's capacity, which leads to under-fitting. To solve this problem, we propose a novel pruning method, named MaskSparsity, with pruning-aware sparse regularization. MaskSparsity imposes the fine-grained sparse regularization on the specific filters selected by a pruning mask, rather than all the filters of the model. Before the fine-grained sparse regularization of MaskSparity, we can use many methods to get the pruning mask, such as running the global sparse regularization. MaskSparsity ach",
    "link": "http://arxiv.org/abs/2201.06776",
    "context": "Title: Pruning-aware Sparse Regularization for Network Pruning. (arXiv:2201.06776v2 [cs.CV] UPDATED)\nAbstract: Structural neural network pruning aims to remove the redundant channels in the deep convolutional neural networks (CNNs) by pruning the filters of less importance to the final output accuracy. To reduce the degradation of performance after pruning, many methods utilize the loss with sparse regularization to produce structured sparsity. In this paper, we analyze these sparsity-training-based methods and find that the regularization of unpruned channels is unnecessary. Moreover, it restricts the network's capacity, which leads to under-fitting. To solve this problem, we propose a novel pruning method, named MaskSparsity, with pruning-aware sparse regularization. MaskSparsity imposes the fine-grained sparse regularization on the specific filters selected by a pruning mask, rather than all the filters of the model. Before the fine-grained sparse regularization of MaskSparity, we can use many methods to get the pruning mask, such as running the global sparse regularization. MaskSparsity ach",
    "path": "papers/22/01/2201.06776.json",
    "total_tokens": 922,
    "translated_title": "针对网络剪枝的修剪感知稀疏正则化",
    "translated_abstract": "结构化神经网络剪枝旨在通过修剪对最终输出精度不重要的滤波器，从而去除深度卷积神经网络（CNNs）中的冗余通道。为了减少剪枝后性能的下降，许多方法利用稀疏正则化的损失来产生结构化稀疏性。本文分析了这些基于稀疏正则化训练的方法，并发现对未修剪通道进行正则化是不必要的，而且限制了网络的容量，导致欠拟合。为了解决这个问题，我们提出了一种新的剪枝方法，命名为MaskSparsity，采用了修剪感知的稀疏正则化。MaskSparsity仅对由修剪掩模选择的特定滤波器施加精细的稀疏正则化，而不是对模型的所有滤波器施加。在MaskSparity的精细稀疏正则化之前，我们可以使用许多方法来获取剪枝掩模，例如运行全局稀疏正则化。",
    "tldr": "本文提出了一种修剪感知的稀疏正则化的网络剪枝方法，使用精细的稀疏正则化仅对由剪枝掩模选择的特定滤波器进行处理，从而减少网络容量限制和欠拟合。"
}