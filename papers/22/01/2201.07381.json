{
    "title": "Unveiling Project-Specific Bias in Neural Code Models",
    "abstract": "arXiv:2201.07381v2 Announce Type: replace  Abstract: Deep learning has introduced significant improvements in many software analysis tasks. Although the Large Language Models (LLMs) based neural code models demonstrate commendable performance when trained and tested within the intra-project independent and identically distributed (IID) setting, they often struggle to generalize effectively to real-world inter-project out-of-distribution (OOD) data. In this work, we show that this phenomenon is caused by the heavy reliance on project-specific shortcuts for prediction instead of ground-truth evidence. We propose a Cond-Idf measurement to interpret this behavior, which quantifies the relatedness of a token with a label and its project-specificness. The strong correlation between model behavior and the proposed measurement indicates that without proper regularization, models tend to leverage spurious statistical cues for prediction. Equipped with these observations, we propose a novel bias",
    "link": "https://arxiv.org/abs/2201.07381",
    "context": "Title: Unveiling Project-Specific Bias in Neural Code Models\nAbstract: arXiv:2201.07381v2 Announce Type: replace  Abstract: Deep learning has introduced significant improvements in many software analysis tasks. Although the Large Language Models (LLMs) based neural code models demonstrate commendable performance when trained and tested within the intra-project independent and identically distributed (IID) setting, they often struggle to generalize effectively to real-world inter-project out-of-distribution (OOD) data. In this work, we show that this phenomenon is caused by the heavy reliance on project-specific shortcuts for prediction instead of ground-truth evidence. We propose a Cond-Idf measurement to interpret this behavior, which quantifies the relatedness of a token with a label and its project-specificness. The strong correlation between model behavior and the proposed measurement indicates that without proper regularization, models tend to leverage spurious statistical cues for prediction. Equipped with these observations, we propose a novel bias",
    "path": "papers/22/01/2201.07381.json",
    "total_tokens": 917,
    "translated_title": "揭示神经代码模型中的项目特定偏见",
    "translated_abstract": "深度学习在许多软件分析任务中取得了显著的改进。尽管基于大型语言模型（LLMs）的神经代码模型在项目内独立和同分布（IID）设置下训练和测试时表现出可观的性能，但它们往往无法有效地推广到现实世界的跨项目分布（OOD）数据。在这项工作中，我们表明这种现象是由于过度依赖项目特定快捷方式进行预测，而不是真实证据导致的。我们提出了一种名为Cond-Idf的度量来解释这种行为，该度量量化了令牌与标签及其项目特定性之间的相关性。模型行为与提出的度量之间的强相关性表明，在没有适当的正则化的情况下，模型往往倾向于利用虚假的统计线索进行预测。借助这些观察结果，我们提出了一种新颖的偏见",
    "tldr": "神经代码模型在训练和测试时表现出可观的性能，但往往无法有效地推广到跨项目分布的数据，原因在于过度依赖项目特定快捷方式进行预测。我们提出了一种量化项目特定性的度量Cond-Idf，并指出模型倾向于利用虚假统计线索进行预测。",
    "en_tdlr": "Neural code models demonstrate commendable performance within intra-project IID setting but struggle to generalize to inter-project OOD data due to heavy reliance on project-specific shortcuts for prediction. We propose a quantification metric, Cond-Idf, to measure project-specificness and highlight the tendency of models to leverage spurious statistical cues for prediction."
}