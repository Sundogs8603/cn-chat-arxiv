{
    "title": "Evaluating a Methodology for Increasing AI Transparency: A Case Study",
    "abstract": "arXiv:2201.13224v2 Announce Type: replace-cross  Abstract: In reaction to growing concerns about the potential harms of artificial intelligence (AI), societies have begun to demand more transparency about how AI models and systems are created and used. To address these concerns, several efforts have proposed documentation templates containing questions to be answered by model developers. These templates provide a useful starting point, but no single template can cover the needs of diverse documentation consumers. It is possible in principle, however, to create a repeatable methodology to generate truly useful documentation. Richards et al. [25] proposed such a methodology for identifying specific documentation needs and creating templates to address those needs. Although this is a promising proposal, it has not been evaluated.   This paper presents the first evaluation of this user-centered methodology in practice, reporting on the experiences of a team in the domain of AI for healthca",
    "link": "https://arxiv.org/abs/2201.13224",
    "context": "Title: Evaluating a Methodology for Increasing AI Transparency: A Case Study\nAbstract: arXiv:2201.13224v2 Announce Type: replace-cross  Abstract: In reaction to growing concerns about the potential harms of artificial intelligence (AI), societies have begun to demand more transparency about how AI models and systems are created and used. To address these concerns, several efforts have proposed documentation templates containing questions to be answered by model developers. These templates provide a useful starting point, but no single template can cover the needs of diverse documentation consumers. It is possible in principle, however, to create a repeatable methodology to generate truly useful documentation. Richards et al. [25] proposed such a methodology for identifying specific documentation needs and creating templates to address those needs. Although this is a promising proposal, it has not been evaluated.   This paper presents the first evaluation of this user-centered methodology in practice, reporting on the experiences of a team in the domain of AI for healthca",
    "path": "papers/22/01/2201.13224.json",
    "total_tokens": 830,
    "translated_title": "评估增加人工智能透明度的方法论：一项案例研究",
    "translated_abstract": "针对人工智能（AI）潜在危害的增长关切，社会已开始要求更多关于AI模型和系统创建和使用方式的透明度。为解决这些问题，一些努力提出了包含模型开发者需要回答的问题的文档模板。这些模板提供了一个有用的起点，但是没有单一模板可以涵盖各种文档使用者的需求。然而，原则上可以创建一种可重复的方法论来生成真正有用的文档。Richards等人[25]提出了这样一种方法，用于识别具体的文档需求，并创建模板来满足这些需求。虽然这是一个有前途的提议，但尚未进行评估。 本文首次在实践中评估了这种以用户为中心的方法论，并报告了一个在医疗人工智能领域团队的经验。",
    "tldr": "该论文评估了一个针对人工智能透明度增加的方法论，首次将该用户中心方法论应用于实践，报告了在医疗人工智能领域团队的经验。",
    "en_tdlr": "This paper evaluates a methodology for increasing AI transparency, presents the first practical application of this user-centered methodology, and reports on the experiences of a team in the field of AI for healthcare."
}