{
    "title": "Compositionality as Lexical Symmetry. (arXiv:2201.12926v2 [cs.CL] UPDATED)",
    "abstract": "In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme -- a procedure for transforming examples into other well formed examples -- that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LEXSYM that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LEXSYM can be deplo",
    "link": "http://arxiv.org/abs/2201.12926",
    "context": "Title: Compositionality as Lexical Symmetry. (arXiv:2201.12926v2 [cs.CL] UPDATED)\nAbstract: In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme -- a procedure for transforming examples into other well formed examples -- that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LEXSYM that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LEXSYM can be deplo",
    "path": "papers/22/01/2201.12926.json",
    "total_tokens": 829,
    "translated_title": "《组合性作为词汇对称性》",
    "translated_abstract": "在语义解析、指令遵循和问题回答等任务中，标准的深度网络在从小数据集中进行组合泛化时会失败。许多现有方法通过强制实施句子解释的组合过程的模型架构来克服这个限制。本文提出了一个领域通用和模型无关的组合性形式，将其作为数据分布的对称性约束而不是模型。我们证明了，无论何时一个任务可以通过一个组合模型来解决，都存在一个相应的数据增强方案——将示例转换为其他合适示例的过程——可以为解决相同任务的任何训练模型赋予组合归纳偏置。我们描述了一个自动发现这些转换并将其应用于普通神经序列模型训练数据的过程，称为LEXSYM。与现有的组合数据增强过程不同，LEXSYM可以被快速部署。",
    "tldr": "本文将组合性定义为对数据分布的对称性约束，而不是模型，通过自动发现数据转换并应用于训练数据，提高模型的组合归纳偏置。"
}