{
    "title": "Stochastic regularized majorization-minimization with weakly convex and multi-convex surrogates. (arXiv:2201.01652v3 [math.OC] UPDATED)",
    "abstract": "Stochastic majorization-minimization (SMM) is a class of stochastic optimization algorithms that proceed by sampling new data points and minimizing a recursive average of surrogate functions of an objective function. The surrogates are required to be strongly convex and convergence rate analysis for the general non-convex setting was not available. In this paper, we propose an extension of SMM where surrogates are allowed to be only weakly convex or block multi-convex, and the averaged surrogates are approximately minimized with proximal regularization or block-minimized within diminishing radii, respectively. For the general nonconvex constrained setting with non-i.i.d. data samples, we show that the first-order optimality gap of the proposed algorithm decays at the rate $O((\\log n)^{1+\\epsilon}/n^{1/2})$ for the empirical loss and $O((\\log n)^{1+\\epsilon}/n^{1/4})$ for the expected loss, where $n$ denotes the number of data samples processed. Under some additional assumption, the lat",
    "link": "http://arxiv.org/abs/2201.01652",
    "context": "Title: Stochastic regularized majorization-minimization with weakly convex and multi-convex surrogates. (arXiv:2201.01652v3 [math.OC] UPDATED)\nAbstract: Stochastic majorization-minimization (SMM) is a class of stochastic optimization algorithms that proceed by sampling new data points and minimizing a recursive average of surrogate functions of an objective function. The surrogates are required to be strongly convex and convergence rate analysis for the general non-convex setting was not available. In this paper, we propose an extension of SMM where surrogates are allowed to be only weakly convex or block multi-convex, and the averaged surrogates are approximately minimized with proximal regularization or block-minimized within diminishing radii, respectively. For the general nonconvex constrained setting with non-i.i.d. data samples, we show that the first-order optimality gap of the proposed algorithm decays at the rate $O((\\log n)^{1+\\epsilon}/n^{1/2})$ for the empirical loss and $O((\\log n)^{1+\\epsilon}/n^{1/4})$ for the expected loss, where $n$ denotes the number of data samples processed. Under some additional assumption, the lat",
    "path": "papers/22/01/2201.01652.json",
    "total_tokens": 993,
    "translated_title": "弱凸和多凸代理函数的随机正则化主导极小化算法。",
    "translated_abstract": "随机主导极小化（SMM）是一类采样新数据点并最小化目标函数的代理函数的递归平均数的优化算法。要求代理函数是强凸的，非凸情况下的收敛率分析并不可行。该论文提出了一个SMM的扩展，该扩展允许代理函数仅为弱凸或块多凸，并且平均代理函数在近端正则化或块最小化的减小半径中近似最小化。实验证明，该算法在凸约束下处理非i.i.d.数据样本的一阶最优性差距以速率$O((\\log n)^{1+\\epsilon}/n^{1/2})$下降，期望损失为$O((\\log n)^{1+\\epsilon}/n^{1/4})$，其中$n$表示处理的数据样本数量。在一些额外的假设下，还提供了目标函数的后期收敛速率。该方法在多个真实数据集上得到展示，并与最先进的方法进行了比较。",
    "tldr": "本文提出了一个SMM的扩展，该扩展支持代理函数的弱凸性和块多凸性，并在解决非凸约束问题上具有显著优越性能。",
    "en_tdlr": "The paper proposes an extension of SMM for weakly convex and multi-convex surrogate functions, achieving superior performance in solving non-convex constrained problems, with a convergence rate of O((log n)^(1+epsilon)/n^(1/2)) for empirical loss and O((log n)^(1+epsilon)/n^(1/4)) for expected loss."
}