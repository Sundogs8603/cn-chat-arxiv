{
    "title": "Zero-Shot Machine Unlearning. (arXiv:2201.05629v3 [cs.LG] UPDATED)",
    "abstract": "Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. In case of machine learning (ML) applications, this necessitates deletion of data not only from storage archives but also from ML models. Due to an increasing need for regulatory compliance required for ML applications, machine unlearning is becoming an emerging research problem. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch after discarding the deleted data. The few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, in many cases, no data related to the training process or training samples may be accessible for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning wit",
    "link": "http://arxiv.org/abs/2201.05629",
    "context": "Title: Zero-Shot Machine Unlearning. (arXiv:2201.05629v3 [cs.LG] UPDATED)\nAbstract: Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. In case of machine learning (ML) applications, this necessitates deletion of data not only from storage archives but also from ML models. Due to an increasing need for regulatory compliance required for ML applications, machine unlearning is becoming an emerging research problem. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch after discarding the deleted data. The few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, in many cases, no data related to the training process or training samples may be accessible for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning wit",
    "path": "papers/22/01/2201.05629.json",
    "total_tokens": 846,
    "translated_title": "零样本机器遗忘",
    "translated_abstract": "现代隐私法规赋予公民被产品、服务和公司遗忘的权利。对于机器学习（ML）应用而言，这需要从存储归档和ML模型中删除数据。由于ML应用需要越来越多的监管合规性，机器遗忘已成为一个不断出现的研究问题。被遗忘请求以删除已经训练好的ML模型中的一定集合或类别的数据的形式提出。实际考虑阻止丢弃删除的数据后从头重新训练模型。现有少数研究使用整个训练数据、训练数据子集或在训练期间存储的一些元数据更新遗忘的模型权重。然而，在许多情况下，训练过程或训练样本相关的数据可能无法访问，我们因此提出问题：是否可能通过零样本学习实现遗忘。",
    "tldr": "零样本机器遗忘是一个新兴的研究问题，允许从已经训练好的ML模型中删除数据。因为这些请求可能会涉及到无法访问的训练数据，因此需要新的解决方法。",
    "en_tdlr": "Zero-shot machine unlearning is an emerging research problem that allows for deletion of data from a trained ML model. As the requests may involve inaccessible training data, new approaches are needed."
}