{
    "title": "Joint Differentiable Optimization and Verification for Certified Reinforcement Learning. (arXiv:2201.12243v2 [cs.LG] UPDATED)",
    "abstract": "In model-based reinforcement learning for safety-critical control systems, it is important to formally certify system properties (e.g., safety, stability) under the learned controller. However, as existing methods typically apply formal verification \\emph{after} the controller has been learned, it is sometimes difficult to obtain any certificate, even after many iterations between learning and verification. To address this challenge, we propose a framework that jointly conducts reinforcement learning and formal verification by formulating and solving a novel bilevel optimization problem, which is differentiable by the gradients from the value function and certificates. Experiments on a variety of examples demonstrate the significant advantages of our framework over the model-based stochastic value gradient (SVG) method and the model-free proximal policy optimization (PPO) method in finding feasible controllers with barrier functions and Lyapunov functions that ensure system safety and ",
    "link": "http://arxiv.org/abs/2201.12243",
    "context": "Title: Joint Differentiable Optimization and Verification for Certified Reinforcement Learning. (arXiv:2201.12243v2 [cs.LG] UPDATED)\nAbstract: In model-based reinforcement learning for safety-critical control systems, it is important to formally certify system properties (e.g., safety, stability) under the learned controller. However, as existing methods typically apply formal verification \\emph{after} the controller has been learned, it is sometimes difficult to obtain any certificate, even after many iterations between learning and verification. To address this challenge, we propose a framework that jointly conducts reinforcement learning and formal verification by formulating and solving a novel bilevel optimization problem, which is differentiable by the gradients from the value function and certificates. Experiments on a variety of examples demonstrate the significant advantages of our framework over the model-based stochastic value gradient (SVG) method and the model-free proximal policy optimization (PPO) method in finding feasible controllers with barrier functions and Lyapunov functions that ensure system safety and ",
    "path": "papers/22/01/2201.12243.json",
    "total_tokens": 976,
    "translated_title": "基于模型的强化学习的可证实性优化与验证",
    "translated_abstract": "在面对安全关键控制系统的模型化强化学习中，根据学习到的控制器正式认证系统属性（例如安全、稳定）是非常重要的。然而，现有方法通常在学习控制器之后才应用形式验证，在经历了多次迭代的学习和验证之后语言考取得到任何证书有时是非常困难的。为了解决这个问题，我们提出了一个框架，通过构建和解决一个新的双层优化问题来同时进行强化学习和正式验证，该问题可以由价值函数和证书的梯度进行微分。在各种示例的实验中，我们的框架在与基于模型的随机值梯度（SVG）方法和基于模型的无模型近端策略优化（PPO）方法相比，在寻找具有保护函数和李雅普诺夫函数的可行控制器方面具有显著的优势，从而保证了系统的安全性。",
    "tldr": "本文提出了一个新的优化问题框架，用于基于模型的强化学习中的正式验证，同时解决了在前期学习和后期验证之间难以获得证书的问题。该框架通过梯度来微分证书和价值函数，并且在实验中表现出大于其他方法的优势，可以找到具有保护函数和李雅普诺夫函数的可行控制器，确保了系统的安全性。"
}