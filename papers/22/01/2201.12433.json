{
    "title": "FedGCN: Convergence and Communication Tradeoffs in Federated Training of Graph Convolutional Networks. (arXiv:2201.12433v6 [cs.LG] UPDATED)",
    "abstract": "Methods for training models on graphs distributed across multiple clients have recently grown in popularity, due to the size of these graphs as well as regulations on keeping data where it is generated. However, a single connected graph cannot be disjointly partitioned onto multiple clients due to the cross-client edges connecting graph nodes. Thus, distributed methods for training a model on a single graph incur either significant communication overhead between clients or a loss of available information to the training. We introduce the Federated Graph Convolutional Network (FedGCN) algorithm, which uses federated learning to train GCN models for semi-supervised node classification with fast convergence and little communication. Compared to prior methods that require communication among clients at each training round, FedGCN clients only communicate with the central server in one pre-training step, greatly reducing communication costs and allowing the use of homomorphic encryption to ",
    "link": "http://arxiv.org/abs/2201.12433",
    "context": "Title: FedGCN: Convergence and Communication Tradeoffs in Federated Training of Graph Convolutional Networks. (arXiv:2201.12433v6 [cs.LG] UPDATED)\nAbstract: Methods for training models on graphs distributed across multiple clients have recently grown in popularity, due to the size of these graphs as well as regulations on keeping data where it is generated. However, a single connected graph cannot be disjointly partitioned onto multiple clients due to the cross-client edges connecting graph nodes. Thus, distributed methods for training a model on a single graph incur either significant communication overhead between clients or a loss of available information to the training. We introduce the Federated Graph Convolutional Network (FedGCN) algorithm, which uses federated learning to train GCN models for semi-supervised node classification with fast convergence and little communication. Compared to prior methods that require communication among clients at each training round, FedGCN clients only communicate with the central server in one pre-training step, greatly reducing communication costs and allowing the use of homomorphic encryption to ",
    "path": "papers/22/01/2201.12433.json",
    "total_tokens": 966,
    "translated_title": "FedGCN：联邦训练中图卷积网络的收敛与通信的权衡",
    "translated_abstract": "近年来，在分布于多个客户端的图上训练模型的方法因其图的规模和数据保留规定的原因而越来越受欢迎。然而，由于连接图节点的跨客户端边缘，单个连接图不能被分别分隔到多个客户端。因此，在单个图上分布式训练模型会导致客户端之间的通信开销巨大或训练中信息的丢失。我们介绍了FedGCN算法，它使用联邦学习来训练用于半监督节点分类的GCN模型，而且能够快速收敛而且通信量较小。与之前需要在每个训练轮次中客户端之间进行通信的方法相比，FedGCN客户端仅在一个预训练步骤中与中央服务器通信，从而极大地减少了通信成本，并允许使用同态加密来保护本地数据的隐私。在四个基准数据集上的实验结果表明，FedGCN相比于最先进的集中式训练方法，能够取得竞争性的表现，同时使用的通信量明显更少。",
    "tldr": "介绍了一个新算法 FedGCN，使用联邦学习训练 GCN 模型进行半监督节点分类，实现收敛快，通信量小，同时还能够保护本地数据隐私。",
    "en_tdlr": "The FedGCN algorithm is introduced to use federated learning to train GCN models for semi-supervised node classification with fast convergence, little communication, and privacy preservation of local data."
}