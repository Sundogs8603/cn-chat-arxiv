{
    "title": "Minimax risk classifiers with 0-1 loss. (arXiv:2201.06487v6 [stat.ML] UPDATED)",
    "abstract": "Supervised classification techniques use training samples to learn a classification rule with small expected 0-1 loss (error probability). Conventional methods enable tractable learning and provide out-of-sample generalization by using surrogate losses instead of the 0-1 loss and considering specific families of rules (hypothesis classes). This paper presents minimax risk classifiers (MRCs) that minize the worst-case 0-1 loss with respect to uncertainty sets of distributions that can include the underlying distribution, with a tunable confidence. We show that MRCs can provide tight performance guarantees at learning and are strongly universally consistent using feature mappings given by characteristic kernels. The paper also proposes efficient optimization techniques for MRC learning and shows that the methods presented can provide accurate classification together with tight performance guarantees in practice.",
    "link": "http://arxiv.org/abs/2201.06487",
    "context": "Title: Minimax risk classifiers with 0-1 loss. (arXiv:2201.06487v6 [stat.ML] UPDATED)\nAbstract: Supervised classification techniques use training samples to learn a classification rule with small expected 0-1 loss (error probability). Conventional methods enable tractable learning and provide out-of-sample generalization by using surrogate losses instead of the 0-1 loss and considering specific families of rules (hypothesis classes). This paper presents minimax risk classifiers (MRCs) that minize the worst-case 0-1 loss with respect to uncertainty sets of distributions that can include the underlying distribution, with a tunable confidence. We show that MRCs can provide tight performance guarantees at learning and are strongly universally consistent using feature mappings given by characteristic kernels. The paper also proposes efficient optimization techniques for MRC learning and shows that the methods presented can provide accurate classification together with tight performance guarantees in practice.",
    "path": "papers/22/01/2201.06487.json",
    "total_tokens": 924,
    "translated_title": "最小化最大化风险分类器与0-1损失",
    "translated_abstract": "监督分类技术使用训练样本来学习一种具有小期望0-1损失（错误概率）的分类规则。传统方法通过使用代理损失而不是0-1损失，并考虑特定的规则族（假设类）来实现可计算的学习和样本外泛化。本文提出了最小化最大化风险分类器（MRCs），它们最小化与可以包含基础分布的分布的不确定性集合相对应的最坏情况下的0-1损失，具有可调的置信度。我们证明了MRCs可以在学习时提供严格的性能保证，并且使用由特征映射给出的特征核是强度普遍一致的。本文还提出了MRC学习的高效优化技术，并展示了所提出的方法在实践中可以提供准确的分类和严格的性能保证。",
    "tldr": "本论文介绍了最小化最大化风险分类器（MRCs），旨在通过最小化与可能包含基础分布的分布的不确定性集合相对应的最坏情况下的0-1损失来提供严格的性能保证。使用特征映射和特征核，MRCs在学习时具有强度普遍一致性，并且提供了高效的优化技术和准确的分类能力。",
    "en_tdlr": "This paper presents minimax risk classifiers (MRCs) that aim to provide tight performance guarantees by minimizing the worst-case 0-1 loss corresponding to uncertainty sets of distributions that can include the underlying distribution. Using feature mappings and characteristic kernels, MRCs have strong universal consistency in learning and offer efficient optimization techniques and accurate classification capabilities."
}