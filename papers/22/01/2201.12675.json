{
    "title": "Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models. (arXiv:2201.12675v2 [cs.LG] UPDATED)",
    "abstract": "A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought.",
    "link": "http://arxiv.org/abs/2201.12675",
    "context": "Title: Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models. (arXiv:2201.12675v2 [cs.LG] UPDATED)\nAbstract: A central tenet of Federated learning (FL), which trains models without centralizing user data, is privacy. However, previous work has shown that the gradient updates used in FL can leak user information. While the most industrial uses of FL are for text applications (e.g. keystroke prediction), nearly all attacks on FL privacy have focused on simple image classifiers. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. This work suggests that FL on text, which has historically been resistant to privacy attacks, is far more vulnerable than previously thought.",
    "path": "papers/22/01/2201.12675.json",
    "total_tokens": 933,
    "translated_title": "破坏者：训练语言模型的联邦学习中的变形金刚隐私泄露",
    "translated_abstract": "联邦学习(Federated learning, FL)的核心原则是在不集中用户数据的情况下训练模型，这种方法强调隐私保护。然而，先前的研究表明，FL中使用的梯度更新可能泄露用户信息。尽管FL在文本应用领域（例如击键预测）中很常见，但对于FL隐私的几乎所有攻击都集中在简单的图像分类器上。我们提出了一种新的攻击方法，通过部署恶意参数向量来揭示私人用户文本，这种攻击可以成功地进行mini-batches训练，适用于多个用户和长序列。与以往针对FL的攻击不同的是，这种攻击利用了Transformer架构和标记嵌入(token embedding)的特性，分别提取标记和位置嵌入以检索高保真度文本。这项工作表明，文本领域的FL在历史上一直能够抵御隐私攻击，但比先前认为的更加脆弱。",
    "tldr": "该论文提出了一种针对联邦学习中文本的隐私攻击方法，通过部署恶意参数向量来揭示私人用户文本，并成功地进行mini-batches训练，适用于多个用户和长序列，提示了文本领域的FL比先前认为的更脆弱。",
    "en_tdlr": "The paper proposes a novel attack on privacy in Federated Learning (FL), specifically targeting text applications, by deploying malicious parameter vectors to extract private user information and revealing high-fidelity text, indicating that FL on text is far more vulnerable than previously thought."
}