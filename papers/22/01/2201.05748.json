{
    "title": "Concise Logarithmic Loss Function for Robust Training of Anomaly Detection Model. (arXiv:2201.05748v2 [cs.LG] UPDATED)",
    "abstract": "Recently, deep learning-based algorithms are widely adopted due to the advantage of being able to establish anomaly detection models without or with minimal domain knowledge of the task. Instead, to train the artificial neural network more stable, it should be better to define the appropriate neural network structure or the loss function. For the training anomaly detection model, the mean squared error (MSE) function is adopted widely. On the other hand, the novel loss function, logarithmic mean squared error (LMSE), is proposed in this paper to train the neural network more stable. This study covers a variety of comparisons from mathematical comparisons, visualization in the differential domain for backpropagation, loss convergence in the training process, and anomaly detection performance. In an overall view, LMSE is superior to the existing MSE function in terms of strongness of loss convergence, anomaly detection performance. The LMSE function is expected to be applicable for train",
    "link": "http://arxiv.org/abs/2201.05748",
    "context": "Title: Concise Logarithmic Loss Function for Robust Training of Anomaly Detection Model. (arXiv:2201.05748v2 [cs.LG] UPDATED)\nAbstract: Recently, deep learning-based algorithms are widely adopted due to the advantage of being able to establish anomaly detection models without or with minimal domain knowledge of the task. Instead, to train the artificial neural network more stable, it should be better to define the appropriate neural network structure or the loss function. For the training anomaly detection model, the mean squared error (MSE) function is adopted widely. On the other hand, the novel loss function, logarithmic mean squared error (LMSE), is proposed in this paper to train the neural network more stable. This study covers a variety of comparisons from mathematical comparisons, visualization in the differential domain for backpropagation, loss convergence in the training process, and anomaly detection performance. In an overall view, LMSE is superior to the existing MSE function in terms of strongness of loss convergence, anomaly detection performance. The LMSE function is expected to be applicable for train",
    "path": "papers/22/01/2201.05748.json",
    "total_tokens": 908,
    "translated_title": "稳健训练异常检测模型的简明对数损失函数",
    "translated_abstract": "近年来，由于能够在没有或最少领域知识情况下建立异常检测模型的优势，基于深度学习的算法被广泛采用。然而，为了更稳定地训练人工神经网络，应该定义适当的神经网络结构或损失函数。对于异常检测模型的训练，均方误差（MSE）函数被广泛采用。另一方面，本文提出了一种新的损失函数，对数均方误差（LMSE），以更稳定地训练神经网络。本研究涵盖了数学比较，反向传播的差分域可视化，训练过程中的损失收敛和异常检测性能等各个方面的比较。在总体上，LMSE在损失收敛强度、异常检测性能方面优于现有的MSE函数。LMSE函数预计可适用于训练各种类型的异常检测模型。",
    "tldr": "本研究提出了一种新的对数均方误差（LMSE）损失函数，相比于现有的均方误差（MSE）函数，在神经网络训练中更稳定、具有更强的收敛性和更好的异常检测性能。",
    "en_tdlr": "A novel logarithmic mean squared error (LMSE) loss function is proposed for stable training of neural networks in anomaly detection, which outperforms the existing MSE function in terms of stronger loss convergence and better anomaly detection performance."
}