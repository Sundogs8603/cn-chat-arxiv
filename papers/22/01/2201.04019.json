{
    "title": "Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v3 [cs.CV] UPDATED)",
    "abstract": "The recently proposed MaskFormer gives a refreshed perspective on the task of semantic segmentation: it shifts from the popular pixel-level classification paradigm to a mask-level classification method. In essence, it generates paired probabilities and masks corresponding to category segments and combines them during inference for the segmentation maps. In our study, we find that per-mask classification decoder on top of a single-scale feature is not effective enough to extract reliable probability or mask. To mine for rich semantic information across the feature pyramid, we propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask approach semantic segmentation with multi-scale features. The proposed transformer decoder performs cross-attention between the learnable queries and each spatial feature from the feature pyramid in parallel and uses cross-scale inter-query attention to exchange complimentary information. We achieve competitive performance on three widely use",
    "link": "http://arxiv.org/abs/2201.04019",
    "context": "Title: Pyramid Fusion Transformer for Semantic Segmentation. (arXiv:2201.04019v3 [cs.CV] UPDATED)\nAbstract: The recently proposed MaskFormer gives a refreshed perspective on the task of semantic segmentation: it shifts from the popular pixel-level classification paradigm to a mask-level classification method. In essence, it generates paired probabilities and masks corresponding to category segments and combines them during inference for the segmentation maps. In our study, we find that per-mask classification decoder on top of a single-scale feature is not effective enough to extract reliable probability or mask. To mine for rich semantic information across the feature pyramid, we propose a transformer-based Pyramid Fusion Transformer (PFT) for per-mask approach semantic segmentation with multi-scale features. The proposed transformer decoder performs cross-attention between the learnable queries and each spatial feature from the feature pyramid in parallel and uses cross-scale inter-query attention to exchange complimentary information. We achieve competitive performance on three widely use",
    "path": "papers/22/01/2201.04019.json",
    "total_tokens": 812,
    "translated_title": "金字塔融合变换器用于语义分割",
    "translated_abstract": "最近提出的MaskFormer为语义分割任务提供了一种新的视角：它从流行的像素级分类范式转移到了一种面向掩码级别的分类方法。本质上，它生成与类别片段对应的成对概率和掩码，并在推断过程中将它们结合起来生成分割图。在我们的研究中，我们发现仅基于单尺度特征的每个掩码分类解码器不足以提取可靠的概率或掩码。为了挖掘跨特征金字塔的丰富语义信息，我们提出了一种基于变换器的用于单尺度语义分割的金字塔融合变换器（PFT）。所提出的变换器解码器在每个空间特征和可学习查询之间并行执行交叉注意，并使用跨尺度间查询注意来交换补充信息。",
    "tldr": "本文提出了一种基于变换器的金字塔融合方法用于语义分割，来挖掘跨特征金字塔的丰富语义信息，取得了有竞争力的表现。",
    "en_tdlr": "This paper proposes a pyramid fusion transformer method for semantic segmentation, which utilizes transformer decoders to perform cross-attention and exchange information across the feature pyramid for extracting rich semantic information, achieving competitive performance."
}