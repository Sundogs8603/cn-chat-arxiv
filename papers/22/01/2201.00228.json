{
    "title": "The Complexity of Dynamic Least-Squares Regression. (arXiv:2201.00228v2 [cs.DS] UPDATED)",
    "abstract": "We settle the complexity of dynamic least-squares regression (LSR), where rows and labels $(\\mathbf{A}^{(t)}, \\mathbf{b}^{(t)})$ can be adaptively inserted and/or deleted, and the goal is to efficiently maintain an $\\epsilon$-approximate solution to $\\min_{\\mathbf{x}^{(t)}} \\| \\mathbf{A}^{(t)} \\mathbf{x}^{(t)} - \\mathbf{b}^{(t)} \\|_2$ for all $t\\in [T]$. We prove sharp separations ($d^{2-o(1)}$ vs. $\\sim d$) between the amortized update time of: (i) Fully vs. Partially dynamic $0.01$-LSR; (ii) High vs. low-accuracy LSR in the partially-dynamic (insertion-only) setting.  Our lower bounds follow from a gap-amplification reduction -- reminiscent of iterative refinement -- rom the exact version of the Online Matrix Vector Conjecture (OMv) [HKNS15], to constant approximate OMv over the reals, where the $i$-th online product $\\mathbf{H}\\mathbf{v}^{(i)}$ only needs to be computed to $0.1$-relative error. All previous fine-grained reductions from OMv to its approximate versions only show hardn",
    "link": "http://arxiv.org/abs/2201.00228",
    "context": "Title: The Complexity of Dynamic Least-Squares Regression. (arXiv:2201.00228v2 [cs.DS] UPDATED)\nAbstract: We settle the complexity of dynamic least-squares regression (LSR), where rows and labels $(\\mathbf{A}^{(t)}, \\mathbf{b}^{(t)})$ can be adaptively inserted and/or deleted, and the goal is to efficiently maintain an $\\epsilon$-approximate solution to $\\min_{\\mathbf{x}^{(t)}} \\| \\mathbf{A}^{(t)} \\mathbf{x}^{(t)} - \\mathbf{b}^{(t)} \\|_2$ for all $t\\in [T]$. We prove sharp separations ($d^{2-o(1)}$ vs. $\\sim d$) between the amortized update time of: (i) Fully vs. Partially dynamic $0.01$-LSR; (ii) High vs. low-accuracy LSR in the partially-dynamic (insertion-only) setting.  Our lower bounds follow from a gap-amplification reduction -- reminiscent of iterative refinement -- rom the exact version of the Online Matrix Vector Conjecture (OMv) [HKNS15], to constant approximate OMv over the reals, where the $i$-th online product $\\mathbf{H}\\mathbf{v}^{(i)}$ only needs to be computed to $0.1$-relative error. All previous fine-grained reductions from OMv to its approximate versions only show hardn",
    "path": "papers/22/01/2201.00228.json",
    "total_tokens": 1096,
    "translated_title": "动态最小二乘回归的复杂度",
    "translated_abstract": "我们研究了动态最小二乘回归（LSR）的复杂度，其中行和标签 $(\\mathbf{A}^{(t)},\\mathbf{b}^{(t)})$ 可以被自适应地插入和/或删除，目标是有效地维护 $\\min_{\\mathbf{x}^{(t)}} \\| \\mathbf{A}^{(t)} \\mathbf{x}^{(t)} - \\mathbf{b}^{(t)} \\|_2$ 的 $\\epsilon$-近似解对于所有 $t\\in [T]$。我们证明了（i）完全动态 vs 部分动态 $0.01$-LSR；（ii）高精度 vs 低精度LSR在部分动态（仅插入）设置中的平摊更新时间的尖锐分离($d^{2-o(1)}$ vs. $\\sim d$)。我们的下界来自一个间隙放大约简（类似于迭代重构）——从准确的在线矩阵向量猜想（OMv）[HKNS15]到实数上的常数近似OMv，其中第 $i$ 个在线积 $\\mathbf{H}\\mathbf{v}^{(i)}$ 只需要计算到 $0.1$-相对误差。所有以前的由OMv到它的近似版本的精细约简都只表现出强困难性。",
    "tldr": "本文研究了动态最小二乘回归的复杂度，证明了在部分动态设置中高精度和低精度LSR的平摊更新时间有尖锐分离，并利用在线矩阵向量猜想进行了间隙放大约简。",
    "en_tdlr": "This paper settles the complexity of dynamic least-squares regression, proving sharp separations between high-accuracy and low-accuracy LSR in the partially-dynamic setting, and providing a gap-amplification reduction using the Online Matrix Vector Conjecture."
}