{
    "title": "Self-Supervised Beat Tracking in Musical Signals with Polyphonic Contrastive Learning. (arXiv:2201.01771v2 [cs.SD] UPDATED)",
    "abstract": "Annotating musical beats is a very long and tedious process. In order to combat this problem, we present a new self-supervised learning pretext task for beat tracking and downbeat estimation. This task makes use of Spleeter, an audio source separation model, to separate a song's drums from the rest of its signal. The first set of signals are used as positives, and by extension negatives, for contrastive learning pre-training. The drum-less signals, on the other hand, are used as anchors. When pre-training a fully-convolutional and recurrent model using this pretext task, an onset function is learned. In some cases, this function is found to be mapped to periodic elements in a song. We find that pre-trained models outperform randomly initialized models when a beat tracking training set is extremely small (less than 10 examples). When this is not the case, pre-training leads to a learning speed-up that causes the model to overfit to the training set. More generally, this work defines new",
    "link": "http://arxiv.org/abs/2201.01771",
    "context": "Title: Self-Supervised Beat Tracking in Musical Signals with Polyphonic Contrastive Learning. (arXiv:2201.01771v2 [cs.SD] UPDATED)\nAbstract: Annotating musical beats is a very long and tedious process. In order to combat this problem, we present a new self-supervised learning pretext task for beat tracking and downbeat estimation. This task makes use of Spleeter, an audio source separation model, to separate a song's drums from the rest of its signal. The first set of signals are used as positives, and by extension negatives, for contrastive learning pre-training. The drum-less signals, on the other hand, are used as anchors. When pre-training a fully-convolutional and recurrent model using this pretext task, an onset function is learned. In some cases, this function is found to be mapped to periodic elements in a song. We find that pre-trained models outperform randomly initialized models when a beat tracking training set is extremely small (less than 10 examples). When this is not the case, pre-training leads to a learning speed-up that causes the model to overfit to the training set. More generally, this work defines new",
    "path": "papers/22/01/2201.01771.json",
    "total_tokens": 1142,
    "translated_title": "音乐信号中的自监督节拍跟踪与多声对比学习",
    "translated_abstract": "标注音乐节拍是一项非常漫长乏味的过程。为了解决这个问题，我们提出了一种新的自监督学习预训练任务，用于节拍跟踪和下拍估计。该任务利用了音频信号分离模型Spleeter，将歌曲的鼓声与其余信号分离开来。第一组信号被用作对比学习预训练的正样本和负样本，而去鼓声的信号则作为锚点。通过使用这个预训练任务来对完全卷积和递归模型进行预训练，可以学习到一个起始函数。在某些情况下，发现这个函数被映射到歌曲中的周期性元素。研究发现，在节拍跟踪训练集非常小的情况下（小于10个示例），预训练模型优于随机初始化模型。当情况不是这样时，预训练会导致学习速度加快，模型过拟合于训练集。总的来说，这项工作定义了新的节拍跟踪任务和预训练方法，为音乐信号处理领域的研究和实践提供了重要的贡献。",
    "tldr": "本论文提出了一种新的自监督学习预训练任务，用于音乐节拍跟踪和下拍估计。通过使用音频信号分离模型，可以实现对节拍的自动标注，并在预训练模型中学习到起始函数。研究发现，预训练模型在节拍跟踪训练集非常小的情况下表现优于随机初始化模型。该工作为音乐信号处理领域的研究和实践提供了重要的贡献。",
    "en_tdlr": "This paper introduces a new self-supervised learning pretext task for beat tracking and downbeat estimation in musical signals. By utilizing an audio source separation model, automatic beat annotation is achieved and an onset function is learned through pre-training. The study finds that pre-trained models outperform randomly initialized models when the beat tracking training set is extremely small. This work makes significant contributions to the field of music signal processing."
}