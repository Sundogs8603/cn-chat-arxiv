{
    "title": "AdaTerm: Adaptive T-Distribution Estimated Robust Moments for Noise-Robust Stochastic Gradient Optimization. (arXiv:2201.06714v3 [cs.LG] UPDATED)",
    "abstract": "With the increasing practicality of deep learning applications, practitioners are inevitably faced with datasets corrupted by noise from various sources such as measurement errors, mislabeling, and estimated surrogate inputs/outputs that can adversely impact the optimization results. It is a common practice to improve the optimization algorithm's robustness to noise, since this algorithm is ultimately in charge of updating the network parameters. Previous studies revealed that the first-order moment used in Adam-like stochastic gradient descent optimizers can be modified based on the Student's t-distribution. While this modification led to noise-resistant updates, the other associated statistics remained unchanged, resulting in inconsistencies in the assumed models. In this paper, we propose AdaTerm, a novel approach that incorporates the Student's t-distribution to derive not only the first-order moment but also all the associated statistics. This provides a unified treatment of the o",
    "link": "http://arxiv.org/abs/2201.06714",
    "context": "Title: AdaTerm: Adaptive T-Distribution Estimated Robust Moments for Noise-Robust Stochastic Gradient Optimization. (arXiv:2201.06714v3 [cs.LG] UPDATED)\nAbstract: With the increasing practicality of deep learning applications, practitioners are inevitably faced with datasets corrupted by noise from various sources such as measurement errors, mislabeling, and estimated surrogate inputs/outputs that can adversely impact the optimization results. It is a common practice to improve the optimization algorithm's robustness to noise, since this algorithm is ultimately in charge of updating the network parameters. Previous studies revealed that the first-order moment used in Adam-like stochastic gradient descent optimizers can be modified based on the Student's t-distribution. While this modification led to noise-resistant updates, the other associated statistics remained unchanged, resulting in inconsistencies in the assumed models. In this paper, we propose AdaTerm, a novel approach that incorporates the Student's t-distribution to derive not only the first-order moment but also all the associated statistics. This provides a unified treatment of the o",
    "path": "papers/22/01/2201.06714.json",
    "total_tokens": 781,
    "translated_title": "AdaTerm: 自适应T分布估计稳健矩用于噪声健壮随机梯度优化",
    "translated_abstract": "随着深度学习应用的增加，从各种来源如测量误差、错误标记和估计代理输入/输出中受损的数据集不可避免地影响了优化结果，提高优化算法对噪声的稳健性已成为常见做法。之前的研究发现，Adam-like随机梯度下降优化器中使用的一阶矩可以基于学生t分布进行修改。然而，这种修改只影响了一阶矩，其他关联的统计量保持不变，导致了所假设模型的一致性问题。本文提出了AdaTerm，一种新颖方法，将学生t分布用于推导一阶矩和所有关联的统计量，从而提供了对优化算法的统一处理。",
    "tldr": "AdaTerm是一种自适应T分布估计稳健矩的方法，提供了对优化算法的统一处理。",
    "en_tdlr": "AdaTerm is a method that incorporates adaptive T-distribution estimated robust moments to provide a unified treatment of optimization algorithms."
}