{
    "title": "Federated Optimization of Smooth Loss Functions. (arXiv:2201.01954v2 [cs.LG] UPDATED)",
    "abstract": "In this work, we study empirical risk minimization (ERM) within a federated learning framework, where a central server minimizes an ERM objective function using training data that is stored across $m$ clients. In this setting, the Federated Averaging (FedAve) algorithm is the staple for determining $\\epsilon$-approximate solutions to the ERM problem. Similar to standard optimization algorithms, the convergence analysis of FedAve only relies on smoothness of the loss function in the optimization parameter. However, loss functions are often very smooth in the training data too. To exploit this additional smoothness, we propose the Federated Low Rank Gradient Descent (FedLRGD) algorithm. Since smoothness in data induces an approximate low rank structure on the loss function, our method first performs a few rounds of communication between the server and clients to learn weights that the server can use to approximate clients' gradients. Then, our method solves the ERM problem at the server ",
    "link": "http://arxiv.org/abs/2201.01954",
    "context": "Title: Federated Optimization of Smooth Loss Functions. (arXiv:2201.01954v2 [cs.LG] UPDATED)\nAbstract: In this work, we study empirical risk minimization (ERM) within a federated learning framework, where a central server minimizes an ERM objective function using training data that is stored across $m$ clients. In this setting, the Federated Averaging (FedAve) algorithm is the staple for determining $\\epsilon$-approximate solutions to the ERM problem. Similar to standard optimization algorithms, the convergence analysis of FedAve only relies on smoothness of the loss function in the optimization parameter. However, loss functions are often very smooth in the training data too. To exploit this additional smoothness, we propose the Federated Low Rank Gradient Descent (FedLRGD) algorithm. Since smoothness in data induces an approximate low rank structure on the loss function, our method first performs a few rounds of communication between the server and clients to learn weights that the server can use to approximate clients' gradients. Then, our method solves the ERM problem at the server ",
    "path": "papers/22/01/2201.01954.json",
    "total_tokens": 903,
    "translated_title": "平均分布优化平滑损失函数",
    "translated_abstract": "在这项工作中，我们研究了在联邦学习框架下的经验风险最小化（ERM），其中一个中央服务器使用存储在$m$个客户端上的训练数据来最小化ERM目标函数。在这种情况下，联邦平均（FedAve）算法是确定ERM问题的$\\epsilon$近似解的主要方法。类似于标准优化算法，FedAve的收敛分析仅依赖于优化参数中损失函数的平滑性。然而，损失函数在训练数据中通常也非常平滑。为了利用这种额外的平滑性，我们提出了联邦低秩梯度下降（FedLRGD）算法。由于数据的平滑性在损失函数上引入了近似的低秩结构，我们的方法首先在服务器和客户端之间进行了几轮通信，以学习服务器可以用来近似客户端梯度的权重。然后，我们的方法在服务器上解决ERM问题。",
    "tldr": "本文研究了在联邦学习框架下的平均分布优化平滑损失函数的问题，提出了联邦低秩梯度下降（FedLRGD）算法来利用数据的平滑性，从而实现更好的优化结果。",
    "en_tdlr": "This paper investigates the optimization of smooth loss functions in a federated learning framework and proposes the Federated Low Rank Gradient Descent (FedLRGD) algorithm to leverage the smoothness of the data for improved optimization results."
}