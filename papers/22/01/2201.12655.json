{
    "title": "Error Scaling Laws for Kernel Classification under Source and Capacity Conditions. (arXiv:2201.12655v3 [stat.ML] UPDATED)",
    "abstract": "We consider the problem of kernel classification. While worst-case bounds on the decay rate of the prediction error with the number of samples are known for some classifiers, they often fail to accurately describe the learning curves of real data sets. In this work, we consider the important class of data sets satisfying the standard source and capacity conditions, comprising a number of real data sets as we show numerically. Under the Gaussian design, we derive the decay rates for the misclassification (prediction) error as a function of the source and capacity coefficients. We do so for two standard kernel classification settings, namely margin-maximizing Support Vector Machines (SVM) and ridge classification, and contrast the two methods. We find that our rates tightly describe the learning curves for this class of data sets, and are also observed on real data. Our results can also be seen as an explicit prediction of the exponents of a scaling law for kernel classification that is ",
    "link": "http://arxiv.org/abs/2201.12655",
    "context": "Title: Error Scaling Laws for Kernel Classification under Source and Capacity Conditions. (arXiv:2201.12655v3 [stat.ML] UPDATED)\nAbstract: We consider the problem of kernel classification. While worst-case bounds on the decay rate of the prediction error with the number of samples are known for some classifiers, they often fail to accurately describe the learning curves of real data sets. In this work, we consider the important class of data sets satisfying the standard source and capacity conditions, comprising a number of real data sets as we show numerically. Under the Gaussian design, we derive the decay rates for the misclassification (prediction) error as a function of the source and capacity coefficients. We do so for two standard kernel classification settings, namely margin-maximizing Support Vector Machines (SVM) and ridge classification, and contrast the two methods. We find that our rates tightly describe the learning curves for this class of data sets, and are also observed on real data. Our results can also be seen as an explicit prediction of the exponents of a scaling law for kernel classification that is ",
    "path": "papers/22/01/2201.12655.json",
    "total_tokens": 922,
    "translated_title": "核分类问题下的错误缩放定律：源条件和容量条件下的研究",
    "translated_abstract": "我们考虑核分类问题。尽管某些分类器的最坏情况下样本数量与预测错误的衰减率的边界已知，但它们经常不能准确描述真实数据集的学习曲线。在这项工作中，我们考虑满足标准源条件和容量条件的重要数据集类别，其中包括一些实际数据集，我们通过数值分析证明了这一点。在高斯设计下，我们导出了错误分类（预测）误差的衰减率作为源和容量系数的函数。我们针对两种标准的核分类设置（即最大化间隔支持向量机和岭分类）进行了这样的推导，并对比了两种方法。我们发现我们的衰减率紧密地描述了这类数据集的学习曲线，并且也在实际数据上观察到。我们的结果也可以看作是对核分类的缩放定律指数的显式预测。",
    "tldr": "本文研究了核分类问题中的错误缩放定律，针对满足源条件和容量条件的数据集类别，在高斯设计下导出了误差衰减率与源和容量系数的关系，并对比了最大化间隔支持向量机和岭分类两种方法。",
    "en_tdlr": "This paper investigates the error scaling laws in kernel classification, deriving the decay rates of prediction error as a function of source and capacity coefficients for data sets satisfying source and capacity conditions under Gaussian design. The study compares margin-maximizing Support Vector Machines (SVM) and ridge classification and finds that the derived rates accurately describe the learning curves for this class of data sets."
}