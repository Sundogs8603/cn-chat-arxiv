{
    "title": "Limitation of Characterizing Implicit Regularization by Data-independent Functions. (arXiv:2201.12198v2 [cs.LG] UPDATED)",
    "abstract": "In recent years, understanding the implicit regularization of neural networks (NNs) has become a central task in deep learning theory. However, implicit regularization is itself not completely defined and well understood. In this work, we attempt to mathematically define and study implicit regularization. Importantly, we explore the limitations of a common approach to characterizing implicit regularization using data-independent functions. We propose two dynamical mechanisms, i.e., Two-point and One-point Overlapping mechanisms, based on which we provide two recipes for producing classes of one-hidden-neuron NNs that provably cannot be fully characterized by a type of or all data-independent functions. Following the previous works, our results further emphasize the profound data dependency of implicit regularization in general, inspiring us to study in detail the data dependency of NN implicit regularization in the future.",
    "link": "http://arxiv.org/abs/2201.12198",
    "context": "Title: Limitation of Characterizing Implicit Regularization by Data-independent Functions. (arXiv:2201.12198v2 [cs.LG] UPDATED)\nAbstract: In recent years, understanding the implicit regularization of neural networks (NNs) has become a central task in deep learning theory. However, implicit regularization is itself not completely defined and well understood. In this work, we attempt to mathematically define and study implicit regularization. Importantly, we explore the limitations of a common approach to characterizing implicit regularization using data-independent functions. We propose two dynamical mechanisms, i.e., Two-point and One-point Overlapping mechanisms, based on which we provide two recipes for producing classes of one-hidden-neuron NNs that provably cannot be fully characterized by a type of or all data-independent functions. Following the previous works, our results further emphasize the profound data dependency of implicit regularization in general, inspiring us to study in detail the data dependency of NN implicit regularization in the future.",
    "path": "papers/22/01/2201.12198.json",
    "total_tokens": 850,
    "translated_title": "利用数据独立函数的限制进行隐式正则化的特征化",
    "translated_abstract": "近年来，理解神经网络（NNs）的隐式正则化已成为深度学习理论中的一项核心任务。然而，隐式正则化本身并没有完全定义和深入理解。在这项工作中，我们试图对隐式正则化进行数学定义和研究。重要的是，我们探索了使用数据独立函数进行隐式正则化特征化的常见方法的局限性。我们提出了两个动力学机制，即双点和单点重叠机制，并基于这些机制提供了两种能够证明不能完全由一种或所有数据独立函数特征化的单隐藏神经元NNs类的生成方法。与先前的工作类似，我们的结果进一步强调了隐式正则化的数据依赖性，激励我们未来详细研究NN隐式正则化的数据依赖性。",
    "tldr": "本研究旨在数学定义和研究隐式正则化，发现使用数据独立函数进行特征化的方法存在局限性，并提出了两个动力学机制以及相应的生成方法，进一步强调了隐式正则化的数据依赖性。"
}