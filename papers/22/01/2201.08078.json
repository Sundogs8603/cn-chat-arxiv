{
    "title": "Addressing Maximization Bias in Reinforcement Learning with Two-Sample Testing. (arXiv:2201.08078v3 [cs.LG] UPDATED)",
    "abstract": "Value-based reinforcement-learning algorithms have shown strong results in games, robotics, and other real-world applications. Overestimation bias is a known threat to those algorithms and can lead to dramatic performance decreases or even complete algorithmic failure. We frame the bias problem statistically and consider it an instance of estimating the maximum expected value (MEV) of a set of random variables. We propose the $T$-Estimator (TE) based on two-sample testing for the mean, that flexibly interpolates between over- and underestimation by adjusting the significance level of the underlying hypothesis tests. A generalization, termed $K$-Estimator (KE), obeys the same bias and variance bounds as the TE while relying on a nearly arbitrary kernel function. We introduce modifications of $Q$-Learning and the Bootstrapped Deep $Q$-Network (BDQN) using the TE and the KE, and prove convergence in the tabular setting. Furthermore, we propose an adaptive variant of the TE-based BDQN that",
    "link": "http://arxiv.org/abs/2201.08078",
    "context": "Title: Addressing Maximization Bias in Reinforcement Learning with Two-Sample Testing. (arXiv:2201.08078v3 [cs.LG] UPDATED)\nAbstract: Value-based reinforcement-learning algorithms have shown strong results in games, robotics, and other real-world applications. Overestimation bias is a known threat to those algorithms and can lead to dramatic performance decreases or even complete algorithmic failure. We frame the bias problem statistically and consider it an instance of estimating the maximum expected value (MEV) of a set of random variables. We propose the $T$-Estimator (TE) based on two-sample testing for the mean, that flexibly interpolates between over- and underestimation by adjusting the significance level of the underlying hypothesis tests. A generalization, termed $K$-Estimator (KE), obeys the same bias and variance bounds as the TE while relying on a nearly arbitrary kernel function. We introduce modifications of $Q$-Learning and the Bootstrapped Deep $Q$-Network (BDQN) using the TE and the KE, and prove convergence in the tabular setting. Furthermore, we propose an adaptive variant of the TE-based BDQN that",
    "path": "papers/22/01/2201.08078.json",
    "total_tokens": 968,
    "translated_title": "用两样本检验解决强化学习中的最大化偏差问题",
    "translated_abstract": "基于值的强化学习算法在游戏、机器人学和其他现实世界应用中取得了强大的结果。过度估计偏差是这些算法面临的已知威胁，可能导致性能急剧下降甚至完全失败。我们将偏差问题从统计学角度进行框架化，将其视为估计一组随机变量的最大期望值（MEV）的实例。我们提出了基于两样本检验的$T$-估计器（TE），通过调整底层假设检验的显著性水平，灵活地插值过度估计和欠估计之间的关系。一种命名为$K$-估计器（KE）的推广遵守与TE相同的偏差和方差界限，同时依赖于几乎任意的核函数。我们介绍了使用TE和KE的$Q$学习和引导化深度Q网络（BDQN）的修改，并在表格设置中证明其收敛性。此外，我们提出了一种自适应变体的基于TE的BDQN。",
    "tldr": "本文提出了一种解决强化学习中最大化偏差问题的方法，使用了两样本检验的估计器，能够灵活地插值过度估计和欠估计之间的关系，并在$Q$学习和引导化深度Q网络中得到了验证。",
    "en_tdlr": "This paper proposes a method to address maximization bias in reinforcement learning by using a two-sample testing estimator, which flexibly interpolates between overestimation and underestimation, and has been validated in Q-learning and bootstrapped deep Q-networks."
}