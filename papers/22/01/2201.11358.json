{
    "title": "Fairness Implications of Encoding Protected Categorical Attributes. (arXiv:2201.11358v2 [cs.LG] UPDATED)",
    "abstract": "Past research has demonstrated that the explicit use of protected attributes in machine learning can improve both performance and fairness. Many machine learning algorithms, however, cannot directly process categorical attributes, such as country of birth or ethnicity. Because protected attributes frequently are categorical, they must be encoded as features that can be input to a chosen machine learning algorithm, e.g.\\ support vector machines, gradient boosting decision trees or linear models. Thereby, encoding methods influence how and what the machine learning algorithm will learn, affecting model performance and fairness. This work compares the accuracy and fairness implications of the two most well-known encoding methods: \\emph{one-hot encoding} and \\emph{target encoding}. We distinguish between two types of induced bias that may arise from these encoding methods and may lead to unfair models. The first type, \\textit{irreducible bias}, is due to direct group category discriminatio",
    "link": "http://arxiv.org/abs/2201.11358",
    "context": "Title: Fairness Implications of Encoding Protected Categorical Attributes. (arXiv:2201.11358v2 [cs.LG] UPDATED)\nAbstract: Past research has demonstrated that the explicit use of protected attributes in machine learning can improve both performance and fairness. Many machine learning algorithms, however, cannot directly process categorical attributes, such as country of birth or ethnicity. Because protected attributes frequently are categorical, they must be encoded as features that can be input to a chosen machine learning algorithm, e.g.\\ support vector machines, gradient boosting decision trees or linear models. Thereby, encoding methods influence how and what the machine learning algorithm will learn, affecting model performance and fairness. This work compares the accuracy and fairness implications of the two most well-known encoding methods: \\emph{one-hot encoding} and \\emph{target encoding}. We distinguish between two types of induced bias that may arise from these encoding methods and may lead to unfair models. The first type, \\textit{irreducible bias}, is due to direct group category discriminatio",
    "path": "papers/22/01/2201.11358.json",
    "total_tokens": 835,
    "translated_title": "编码保护分类属性的公平性影响。",
    "translated_abstract": "过去的研究表明，在机器学习中明确使用保护属性可以同时提高性能和公平性。但是，许多机器学习算法无法直接处理分类属性，例如出生国家或种族。由于保护属性经常是分类的，因此必须将其编码为可以输入所选择的机器学习算法的特征，例如支持向量机、梯度提升决策树或线性模型。编码方法影响机器学习算法将学习如何和什么，影响模型的性能和公平性。该研究比较了两种最著名的编码方法——“one-hot编码”和“target编码”的准确性和公平性影响。我们区分了这些编码方法可能产生的两种诱导偏差类型，这可能导致不公平的模型。第一种类型是无法消除的偏差，由于直接组别类别歧视而导致。",
    "tldr": "该研究比较了两种常用的编码方法-“one-hot编码”和“target编码”，并探讨了其对机器学习模型性能和公平性的影响。",
    "en_tdlr": "This study compares the accuracy and fairness implications of the two most well-known encoding methods, one-hot encoding and target encoding, and discusses their impact on the performance and fairness of machine learning models."
}