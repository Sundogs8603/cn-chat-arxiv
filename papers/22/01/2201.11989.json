{
    "title": "Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)",
    "abstract": "Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu",
    "link": "http://arxiv.org/abs/2201.11989",
    "context": "Title: Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)\nAbstract: Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu",
    "path": "papers/22/01/2201.11989.json",
    "total_tokens": 1030,
    "translated_title": "两时间尺度更新规则训练生成式对抗网络中的关键批次大小的存在和估计",
    "translated_abstract": "先前的研究表明，在理论和实践中，使用不同的学习率，如不同的恒定率或不同的衰减率等，使用两时间尺度更新规则（TTUR）有助于训练生成式对抗网络（GAN）。此外，批次大小对于使用TTUR训练GANs也很重要，两者都影响了训练所需的步骤数量。本文基于恒定学习率研究了批次大小与使用TTUR训练GANs所需步骤数量之间的关系。我们理论上表明，对于具有恒定学习率的TTUR，为了找到鉴别器和生成器损失函数的稳定点，所需步骤数随着批次大小的增加而减少，并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。然后，我们使用Fr'echet Inception Distance（FID）作为训练的性能测量，并提供了...",
    "tldr": "本文研究了使用两时间尺度更新规则（TTUR）训练生成式对抗网络（GAN）时批次大小与训练所需步骤数量之间的关系，理论上证明了为了找到稳定点，随着批次大小的增加所需步骤数量会减少并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。",
    "en_tdlr": "This paper studies the relationship between batch size and the number of steps needed for training GANs with two time-scale update rule (TTUR). It theoretically proves that, for a TTUR with constant learning rates, the number of steps needed to find stationary points decreases as the batch size increases and there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. The Fréchet inception distance (FID) is used as the performance measure for training."
}