{
    "title": "Transformer Module Networks for Systematic Generalization in Visual Question Answering. (arXiv:2201.11316v2 [cs.CV] UPDATED)",
    "abstract": "Transformers achieve great performance on Visual Question Answering (VQA). However, their systematic generalization capabilities, i.e., handling novel combinations of known concepts, is unclear. We reveal that Neural Module Networks (NMNs), i.e., question-specific compositions of modules that tackle a sub-task, achieve better or similar systematic generalization performance than the conventional Transformers, even though NMNs' modules are CNN-based. In order to address this shortcoming of Transformers with respect to NMNs, in this paper we investigate whether and how modularity can bring benefits to Transformers. Namely, we introduce Transformer Module Network (TMN), a novel NMN based on compositions of Transformer modules. TMNs achieve state-of-the-art systematic generalization performance in three VQA datasets, improving more than 30% over standard Transformers for novel compositions of sub-tasks. We show that not only the module composition but also the module specialization for eac",
    "link": "http://arxiv.org/abs/2201.11316",
    "context": "Title: Transformer Module Networks for Systematic Generalization in Visual Question Answering. (arXiv:2201.11316v2 [cs.CV] UPDATED)\nAbstract: Transformers achieve great performance on Visual Question Answering (VQA). However, their systematic generalization capabilities, i.e., handling novel combinations of known concepts, is unclear. We reveal that Neural Module Networks (NMNs), i.e., question-specific compositions of modules that tackle a sub-task, achieve better or similar systematic generalization performance than the conventional Transformers, even though NMNs' modules are CNN-based. In order to address this shortcoming of Transformers with respect to NMNs, in this paper we investigate whether and how modularity can bring benefits to Transformers. Namely, we introduce Transformer Module Network (TMN), a novel NMN based on compositions of Transformer modules. TMNs achieve state-of-the-art systematic generalization performance in three VQA datasets, improving more than 30% over standard Transformers for novel compositions of sub-tasks. We show that not only the module composition but also the module specialization for eac",
    "path": "papers/22/01/2201.11316.json",
    "total_tokens": 848,
    "translated_title": "Transformer模块网络用于视觉问答中的系统泛化",
    "translated_abstract": "Transformer在视觉问答中取得了非常好的性能，但它们的系统化泛化能力，即处理已知概念的新组合，尚不清楚。我们发现神经模块网络（NMN）可以取得更好或与传统的Transformer相似的系统化泛化性能，即使NMN的模块是基于卷积神经网络的。为了解决Transformer在NMNs方面的不足，本文研究了模块化如何为Transformer带来益处。我们引入了一种新型的NMN，即Transformer模块网络（TMN）是由Transformer模块组成的组合。在三个VQA数据集中，TMN取得了最先进的系统泛化性能，针对子任务的新组合比标准Transformer提高了30%以上。我们展示了不仅模块组合而且每个组件的专业化都是重要的。",
    "tldr": "本文引入了Transformer模块网络（TMN），它是由Transformer模块组成的新型NMN，能够取得在VQA任务中最先进的系统性能，针对子任务的新组合比标准Transformer提高了30%以上。",
    "en_tdlr": "The paper introduces Transformer Module Network (TMN), a novel NMN composed of Transformer modules, which achieves state-of-the-art systematic performance on the VQA task, improving over standard Transformers by more than 30% for novel compositions of subtasks."
}