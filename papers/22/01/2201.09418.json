{
    "title": "Approximation bounds for norm constrained neural networks with applications to regression and GANs. (arXiv:2201.09418v3 [cs.LG] UPDATED)",
    "abstract": "This paper studies the approximation capacity of ReLU neural networks with norm constraint on the weights. We prove upper and lower bounds on the approximation error of these networks for smooth function classes. The lower bound is derived through the Rademacher complexity of neural networks, which may be of independent interest. We apply these approximation bounds to analyze the convergences of regression using norm constrained neural networks and distribution estimation by GANs. In particular, we obtain convergence rates for over-parameterized neural networks. It is also shown that GANs can achieve optimal rate of learning probability distributions, when the discriminator is a properly chosen norm constrained neural network.",
    "link": "http://arxiv.org/abs/2201.09418",
    "context": "Title: Approximation bounds for norm constrained neural networks with applications to regression and GANs. (arXiv:2201.09418v3 [cs.LG] UPDATED)\nAbstract: This paper studies the approximation capacity of ReLU neural networks with norm constraint on the weights. We prove upper and lower bounds on the approximation error of these networks for smooth function classes. The lower bound is derived through the Rademacher complexity of neural networks, which may be of independent interest. We apply these approximation bounds to analyze the convergences of regression using norm constrained neural networks and distribution estimation by GANs. In particular, we obtain convergence rates for over-parameterized neural networks. It is also shown that GANs can achieve optimal rate of learning probability distributions, when the discriminator is a properly chosen norm constrained neural network.",
    "path": "papers/22/01/2201.09418.json",
    "total_tokens": 836,
    "translated_title": "具范数约束的神经网络的逼近误差界与应用研究",
    "translated_abstract": "本文研究带权重范数约束的ReLU神经网络的逼近能力，对于平滑的函数类，我们证明了这些网络的逼近误差上下界。通过神经网络的Rademacher复杂度导出下界证明，这可能具有独立的研究价值。我们应用这些逼近误差界限来分析使用具范数约束的神经网络进行回归和GAN分布估计的收敛性。特别的，我们得到了过参数神经网络的收敛速率。同时，我们还证明了当判别器选择合适的具范数约束的神经网络时，GAN可以实现学习概率分布的最优速率。",
    "tldr": "本文研究了具范数约束的ReLU神经网络的逼近能力，并证明了对于平滑函数类，这些网络的逼近误差有上下界。此外，应用结果分析了回归和GAN分布估计问题的收敛性，最终证明了当GAN的判别器选择合适的具范数约束的神经网络时，可以实现学习概率分布的最优速率。",
    "en_tdlr": "This paper studies the approximation capacity of ReLU neural networks with norm constraint on the weights, and proves upper and lower bounds on the approximation error for smooth function classes. The results are applied to analyze the convergence of regression and GAN distribution estimation, and show that GANs with the discriminator as a properly chosen norm constrained neural network can achieve optimal rate of learning probability distributions."
}