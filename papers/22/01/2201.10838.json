{
    "title": "Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant. (arXiv:2201.10838v3 [cs.CR] UPDATED)",
    "abstract": "Logistic regression training over encrypted data has been an attractive idea to security concerns for years. In this paper, we propose a faster gradient variant called $\\texttt{quadratic gradient}$ to implement logistic regression training in a homomorphic encryption domain, the core of which can be seen as an extension of the simplified fixed Hessian.  We enhance Nesterov's accelerated gradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with this gradient variant and evaluate the enhanced algorithms on several datasets. Experimental results show that the enhanced methods have a state-of-the-art performance in convergence speed compared to the naive first-order gradient methods. We then adopt the enhanced NAG method to implement homomorphic logistic regression training and obtain a comparable result by only $3$ iterations.",
    "link": "http://arxiv.org/abs/2201.10838",
    "context": "Title: Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant. (arXiv:2201.10838v3 [cs.CR] UPDATED)\nAbstract: Logistic regression training over encrypted data has been an attractive idea to security concerns for years. In this paper, we propose a faster gradient variant called $\\texttt{quadratic gradient}$ to implement logistic regression training in a homomorphic encryption domain, the core of which can be seen as an extension of the simplified fixed Hessian.  We enhance Nesterov's accelerated gradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with this gradient variant and evaluate the enhanced algorithms on several datasets. Experimental results show that the enhanced methods have a state-of-the-art performance in convergence speed compared to the naive first-order gradient methods. We then adopt the enhanced NAG method to implement homomorphic logistic regression training and obtain a comparable result by only $3$ iterations.",
    "path": "papers/22/01/2201.10838.json",
    "total_tokens": 861,
    "translated_title": "隐私保护的逻辑回归训练及其更快的梯度变种",
    "translated_abstract": "多年来，加密数据上的逻辑回归训练一直是一个有吸引力的安全解决方案。本文提出了一种更快的梯度变种——二次梯度，用于在同态加密领域实现逻辑回归训练，其核心可以看作是简化的固定Hessian的扩展。我们分别向Nesterov的加速梯度方法（NAG）和自适应梯度算法（Adagrad）增强了该梯度变种，并在多个数据集上评估了增强算法。实验结果表明，增强方法在收敛速度上比朴素的一阶梯度方法具有最先进的性能。然后，我们采用增强的NAG方法来实现同态逻辑回归训练，并在仅3次迭代中获得了可比较的结果。",
    "tldr": "本文提出了一种更快的梯度变种——二次梯度，用于在同态加密领域实现隐私保护的逻辑回归训练，并成功提升了收敛速度，实现了同态逻辑回归训练仅需较少的迭代次数。",
    "en_tdlr": "This paper proposes a faster gradient variant, called quadratic gradient, for privacy-preserving logistic regression training in homomorphic encryption domain, which successfully improves the convergence speed and achieves comparable results with only a few iterations of homomorphic logistic regression training using the enhanced Nesterov's accelerated gradient method."
}