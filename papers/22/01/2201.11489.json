{
    "title": "The Implicit Bias of Benign Overfitting. (arXiv:2201.11489v5 [cs.LG] UPDATED)",
    "abstract": "The phenomenon of benign overfitting, where a predictor perfectly fits noisy training data while attaining near-optimal expected loss, has received much attention in recent years, but still remains not fully understood beyond well-specified linear regression setups. In this paper, we provide several new results on when one can or cannot expect benign overfitting to occur, for both regression and classification tasks. We consider a prototypical and rather generic data model for benign overfitting of linear predictors, where an arbitrary input distribution of some fixed dimension $k$ is concatenated with a high-dimensional distribution. For linear regression which is not necessarily well-specified, we show that the minimum-norm interpolating predictor (that standard training methods converge to) is biased towards an inconsistent solution in general, hence benign overfitting will generally not occur. Moreover, we show how this can be extended beyond standard linear regression, by an argum",
    "link": "http://arxiv.org/abs/2201.11489",
    "context": "Title: The Implicit Bias of Benign Overfitting. (arXiv:2201.11489v5 [cs.LG] UPDATED)\nAbstract: The phenomenon of benign overfitting, where a predictor perfectly fits noisy training data while attaining near-optimal expected loss, has received much attention in recent years, but still remains not fully understood beyond well-specified linear regression setups. In this paper, we provide several new results on when one can or cannot expect benign overfitting to occur, for both regression and classification tasks. We consider a prototypical and rather generic data model for benign overfitting of linear predictors, where an arbitrary input distribution of some fixed dimension $k$ is concatenated with a high-dimensional distribution. For linear regression which is not necessarily well-specified, we show that the minimum-norm interpolating predictor (that standard training methods converge to) is biased towards an inconsistent solution in general, hence benign overfitting will generally not occur. Moreover, we show how this can be extended beyond standard linear regression, by an argum",
    "path": "papers/22/01/2201.11489.json",
    "total_tokens": 946,
    "translated_title": "善意过拟合的隐性偏差",
    "translated_abstract": "过拟合现象中的善意过拟合，指的是分类器完美地拟合了带有噪声的训练数据，同时达到接近最优的期望损失。近年来，这一现象受到了广泛关注，但除了线性回归设置外，仍然没有得到充分理解。在本文中，我们针对回归和分类任务，提供了关于何时可以或不能期望善意过拟合发生的若干新结果。我们考虑了一个典型且相当通用的线性分类器善意过拟合数据模型，其中将某个固定维度 $k$ 的任意输入分布与高维分布连接在一起。对于非必须经过良好规定的线性回归，我们证明最小范数插值预测器（标准训练方法所收敛到的）在一般情况下是偏向于不一致的解的，因此通常不会发生善意过拟合。此外，我们展示了如何通过一种方法将其扩展到标准线性回归以外。",
    "tldr": "本文针对善意过拟合现象，提供了非线性回归的最小范数插值预测器在一般情况下偏向于不一致解的证明，从而说明善意过拟合不会发生，同时展示了如何将其扩展到标准线性回归以外。",
    "en_tdlr": "This paper provides new results on benign overfitting for regression and classification tasks, showing that the minimum-norm interpolating predictor for non-well-specified linear regression is biased towards inconsistent solutions, indicating that benign overfitting does not generally occur. It also demonstrates how this can be extended beyond standard linear regression."
}