{
    "title": "Matryoshka Representation Learning",
    "abstract": "Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context rigid, fixed capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The f",
    "link": "https://arxiv.org/abs/2205.13147",
    "context": "Title: Matryoshka Representation Learning\nAbstract: Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context rigid, fixed capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The f",
    "path": "papers/22/05/2205.13147.json",
    "total_tokens": 786,
    "translated_title": "Matryoshka表示学习",
    "translated_abstract": "学习得到的表示是现代机器学习系统中的关键组件，用于服务各种下游任务。在训练这种表示时，往往无法确定每个下游任务的计算和统计约束。在这种情况下，刚性的固定容量表示可能过度或不足地适应当前任务。这使我们产生了一个问题：我们能否设计一种灵活的表示，以适应具有不同计算资源的多个下游任务？我们的主要贡献是Matryoshka表示学习（MRL），它在不同的粒度上编码信息，并允许单个嵌入适应下游任务的计算约束。MRL对现有的表示学习流程进行最小修改，并在推理和部署过程中不增加任何额外成本。MRL学习的粗粒度到细粒度的表示至少与独立训练的低维表示一样准确和丰富。",
    "tldr": "Matryoshka表示学习（MRL）是一种灵活的表示学习方法，能够在不同计算资源下适应多个下游任务，同时保持准确性和丰富性。",
    "en_tdlr": "Matryoshka Representation Learning (MRL) is a flexible representation learning approach that can adapt to multiple downstream tasks with different computational resources while maintaining accuracy and richness."
}