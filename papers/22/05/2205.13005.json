{
    "title": "QGNN: Value Function Factorisation with Graph Neural Networks. (arXiv:2205.13005v2 [cs.LG] UPDATED)",
    "abstract": "In multi-agent reinforcement learning, the use of a global objective is a powerful tool for incentivising cooperation. Unfortunately, it is not sample-efficient to train individual agents with a global reward, because it does not necessarily correlate with an agent's individual actions. This problem can be solved by factorising the global value function into local value functions. Early work in this domain performed factorisation by conditioning local value functions purely on local information. Recently, it has been shown that providing both local information and an encoding of the global state can promote cooperative behaviour. In this paper we propose QGNN, the first value factorisation method to use a graph neural network (GNN) based model. The multi-layer message passing architecture of QGNN provides more representational complexity than models in prior work, allowing it to produce a more effective factorisation. QGNN also introduces a permutation invariant mixer which is able to ",
    "link": "http://arxiv.org/abs/2205.13005",
    "context": "Title: QGNN: Value Function Factorisation with Graph Neural Networks. (arXiv:2205.13005v2 [cs.LG] UPDATED)\nAbstract: In multi-agent reinforcement learning, the use of a global objective is a powerful tool for incentivising cooperation. Unfortunately, it is not sample-efficient to train individual agents with a global reward, because it does not necessarily correlate with an agent's individual actions. This problem can be solved by factorising the global value function into local value functions. Early work in this domain performed factorisation by conditioning local value functions purely on local information. Recently, it has been shown that providing both local information and an encoding of the global state can promote cooperative behaviour. In this paper we propose QGNN, the first value factorisation method to use a graph neural network (GNN) based model. The multi-layer message passing architecture of QGNN provides more representational complexity than models in prior work, allowing it to produce a more effective factorisation. QGNN also introduces a permutation invariant mixer which is able to ",
    "path": "papers/22/05/2205.13005.json",
    "total_tokens": 1132,
    "translated_title": "QGNN: 基于图神经网络的价值函数分解方法",
    "translated_abstract": "在多智能体强化学习中，使用全局目标是促进合作的一种强有力的工具。然而，使用全局奖励来训练个体智能体并不具备高效的样本数据利用率，因为它不一定与单个智能体的行动相关。为了解决这个问题，可以将全局价值函数分解为局部价值函数。这个领域的早期工作是通过将局部价值函数纯粹地置于本地信息条件下来进行分解。最近的研究表明，在提供局部信息和全局状态编码的情况下也能促进合作行为。本文提出了QGNN，这是首个使用基于图神经网络（GNN）模型的价值分解方法。QGNN的多层消息传递架构提供了比先前工作更多的表示复杂度，从而使其产生更有效的分解。QGNN还引入了一种置换不变的混合器，能够处理智能体数量和顺序变化，使其更具伸缩性和灵活性。我们在一系列多智能体场景中评估了QGNN，包括一个网格世界导航任务和一个星际争霸微观管理任务，并表明在合作和非合作任务中，它均优于先前的方法。",
    "tldr": "本文提出了一种基于图神经网络的QGNN价值函数分解方法，可以解决多智能体样本数据利用率低的问题。相比以往的方法，QGNN的多层消息传递架构提供了更高的表示复杂度，引入的置换不变的混合器也使其更具伸缩性和灵活性，在合作和非合作任务中均表现出优于先前的方法的性能。",
    "en_tdlr": "This paper proposes QGNN, the first value factorisation method to use a graph neural network based model, which can solve the problem of low sample efficiency in multi-agent reinforcement learning. Compared to previous methods, QGNN's multi-layer message passing architecture provides higher representational complexity, and the introduced permutation invariant mixer also makes it more scalable and flexible, performing better than previous methods in both cooperative and non-cooperative tasks."
}