{
    "title": "Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation. (arXiv:2205.12593v2 [cs.CL] UPDATED)",
    "abstract": "Recent research has revealed that deep neural networks often take dataset biases as a shortcut to make decisions rather than understand tasks, leading to failures in real-world applications. In this study, we focus on the spurious correlation between word features and labels that models learn from the biased data distribution of training data. In particular, we define the word highly co-occurring with a specific label as biased word, and the example containing biased word as biased example. Our analysis shows that biased examples are easier for models to learn, while at the time of prediction, biased words make a significantly higher contribution to the models' predictions, and models tend to assign predicted labels over-relying on the spurious correlation between words and labels. To mitigate models' over-reliance on the shortcut (i.e. spurious correlation), we propose a training strategy Less-Learn-Shortcut (LLS): our strategy quantifies the biased degree of the biased examples and d",
    "link": "http://arxiv.org/abs/2205.12593",
    "context": "Title: Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation. (arXiv:2205.12593v2 [cs.CL] UPDATED)\nAbstract: Recent research has revealed that deep neural networks often take dataset biases as a shortcut to make decisions rather than understand tasks, leading to failures in real-world applications. In this study, we focus on the spurious correlation between word features and labels that models learn from the biased data distribution of training data. In particular, we define the word highly co-occurring with a specific label as biased word, and the example containing biased word as biased example. Our analysis shows that biased examples are easier for models to learn, while at the time of prediction, biased words make a significantly higher contribution to the models' predictions, and models tend to assign predicted labels over-relying on the spurious correlation between words and labels. To mitigate models' over-reliance on the shortcut (i.e. spurious correlation), we propose a training strategy Less-Learn-Shortcut (LLS): our strategy quantifies the biased degree of the biased examples and d",
    "path": "papers/22/05/2205.12593.json",
    "total_tokens": 974,
    "translated_title": "《Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation》 （arXiv:2205.12593v2 [cs.CL] UPDATED）",
    "translated_abstract": "最近的研究表明，深度神经网络通常将数据集的偏置作为捷径来做出决策，而不是理解任务，导致在现实世界的应用中失败。本研究聚焦于模型从训练数据的偏置分布中学到的单词特征与标签间的错误相关性。特别地，我们将高度共现在某个特定标签下的单词定义为有偏单词，将包含有偏单词的样本定义为有偏样本。我们的分析表明，有偏样本对于模型来说更容易学习，在预测时，有偏单词在模型的预测中作出了显著的贡献，模型往往过度依赖单词与标签之间的错误相关性来进行标签预测。为了缓解模型过度依赖这种捷径（即错误相关性），我们提出了一种训练策略Less-Learn-Shortcut（LLS）：我们的策略量化了有偏样本的有偏程度，同时加强了无偏样本的训练。",
    "tldr": "本研究提出了一种训练策略Less-Learn-Shortcut (LLS)，以减少模型过度依赖单词与标签之间的错误相关性。同时，通过量化有偏样本的有偏程度，加强无偏样本的训练。"
}