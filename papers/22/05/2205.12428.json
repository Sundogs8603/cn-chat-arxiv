{
    "title": "Do we need Label Regularization to Fine-tune Pre-trained Language Models?. (arXiv:2205.12428v2 [cs.LG] UPDATED)",
    "abstract": "Knowledge Distillation (KD) is a prominent neural model compression technique that heavily relies on teacher network predictions to guide the training of a student model. Considering the ever-growing size of pre-trained language models (PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is evident that in KD, deploying the teacher network during training adds to the memory and computational requirements of training. In the computer vision literature, the necessity of the teacher network is put under scrutiny by showing that KD is a label regularization technique that can be replaced with lighter teacher-free variants such as the label-smoothing technique. However, to the best of our knowledge, this issue is not investigated in NLP. Therefore, this work concerns studying different label regularization techniques and whether we actually need them to improve the fine-tuning of smaller PLM networks on downstream tasks. In this regard, we did a comprehensive set of exp",
    "link": "http://arxiv.org/abs/2205.12428",
    "context": "Title: Do we need Label Regularization to Fine-tune Pre-trained Language Models?. (arXiv:2205.12428v2 [cs.LG] UPDATED)\nAbstract: Knowledge Distillation (KD) is a prominent neural model compression technique that heavily relies on teacher network predictions to guide the training of a student model. Considering the ever-growing size of pre-trained language models (PLMs), KD is often adopted in many NLP tasks involving PLMs. However, it is evident that in KD, deploying the teacher network during training adds to the memory and computational requirements of training. In the computer vision literature, the necessity of the teacher network is put under scrutiny by showing that KD is a label regularization technique that can be replaced with lighter teacher-free variants such as the label-smoothing technique. However, to the best of our knowledge, this issue is not investigated in NLP. Therefore, this work concerns studying different label regularization techniques and whether we actually need them to improve the fine-tuning of smaller PLM networks on downstream tasks. In this regard, we did a comprehensive set of exp",
    "path": "papers/22/05/2205.12428.json",
    "total_tokens": 1039,
    "translated_title": "我们需要标签规范化来微调预训练语言模型吗？",
    "translated_abstract": "知识蒸馏是一种重要的神经模型压缩技术，它严重依赖于教师网络的预测来指导学生模型的训练。然而，由于预训练语言模型的不断增大，知识蒸馏经常被采用在许多涉及预训练语言模型的自然语言处理任务中。本文通过对各种预训练语言模型进行全面的实验，展示了标签规范化通常会提高微调模型的性能，即使使用较小的预训练语言模型。我们还发现，该方法的好处取决于教师网络的大小，并提出了一种新的无教师网络的知识蒸馏方法，称为“联合微调”，它使用学生模型的集合来代替教师网络，取得了与知识蒸馏相当的结果。",
    "tldr": "本文研究了不同的标签规范化技术，发现标签规范化通常会提高微调模型的性能，尤其是当使用较小的预训练语言模型时。提出了一种新的无教师网络的知识蒸馏方法，并发现其效果与知识蒸馏相当，但取决于教师网络的大小和下游任务。",
    "en_tdlr": "This paper investigates the need for label regularization in fine-tuning pre-trained language models and proposes a new teacher-free variant of knowledge distillation called Co-Tuning. The experiments show that label regularization generally improves the performance of fine-tuned models, even with smaller PLMs, and that the benefit of KD depends on the size of the teacher network."
}