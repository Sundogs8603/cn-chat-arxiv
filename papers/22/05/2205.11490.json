{
    "title": "Local Byte Fusion for Neural Machine Translation. (arXiv:2205.11490v3 [cs.CL] UPDATED)",
    "abstract": "Subword tokenization schemes are the dominant technique used in current NLP models. However, such schemes can be rigid and tokenizers built on one corpus do not adapt well to other parallel corpora. It has also been observed that in multilingual corpora, subword tokenization schemes over-segment low-resource languages leading to a drop in translation performance. A simple alternative to subword tokenizers is byte-based methods i.e. tokenization into byte sequences using encoding schemes such as UTF-8. Byte tokens often represent inputs at a sub-character granularity i.e. one character can be represented by a sequence of multiple byte tokens. This results in byte sequences that are significantly longer than character sequences. Enforcing aggregation of local information in the lower layers can guide the model to build higher-level semantic information. We propose a Local Byte Fusion (LOBEF) method for byte-based machine translation -- utilizing byte $n$-gram and word boundaries -- to ag",
    "link": "http://arxiv.org/abs/2205.11490",
    "context": "Title: Local Byte Fusion for Neural Machine Translation. (arXiv:2205.11490v3 [cs.CL] UPDATED)\nAbstract: Subword tokenization schemes are the dominant technique used in current NLP models. However, such schemes can be rigid and tokenizers built on one corpus do not adapt well to other parallel corpora. It has also been observed that in multilingual corpora, subword tokenization schemes over-segment low-resource languages leading to a drop in translation performance. A simple alternative to subword tokenizers is byte-based methods i.e. tokenization into byte sequences using encoding schemes such as UTF-8. Byte tokens often represent inputs at a sub-character granularity i.e. one character can be represented by a sequence of multiple byte tokens. This results in byte sequences that are significantly longer than character sequences. Enforcing aggregation of local information in the lower layers can guide the model to build higher-level semantic information. We propose a Local Byte Fusion (LOBEF) method for byte-based machine translation -- utilizing byte $n$-gram and word boundaries -- to ag",
    "path": "papers/22/05/2205.11490.json",
    "total_tokens": 929,
    "translated_title": "本地字节融合用于神经机器翻译",
    "translated_abstract": "当前NLP模型中，子词标记化方案是主要的技术。然而，这种方案可能过于死板，并且在一个语料库上构建的标记器对其他平行语料库的适应性不佳。观察发现，在多语种语料库中，子词标记化方案会对低资源语言进行过度切分，从而导致翻译性能下降。子词标记化的一个简单替代方法是基于字节的方法，即使用编码方案（如UTF-8）将输入进行字节序列标记化。字节标记通常在子字符粒度上表示输入，即一个字符可以由多个字节标记序列表示。这导致字节序列比字符序列长得多。在较低层中强制执行局部信息的聚合可以指导模型构建更高层次的语义信息。我们提出了一种利用字节n-gram和单词边界的本地字节融合（LOBEF）方法用于基于字节的机器翻译。",
    "tldr": "本文提出了一种基于字节的本地字节融合方法，用于神经机器翻译。该方法可以解决当前NLP模型中子词标记化方案的刚性和对其他语料库适应性差的问题，同时避免了在多语种语料库中过度切分低资源语言的影响。"
}