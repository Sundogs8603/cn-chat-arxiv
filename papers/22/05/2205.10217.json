{
    "title": "Memorization and Optimization in Deep Neural Networks with Minimum Over-parameterization. (arXiv:2205.10217v3 [stat.ML] UPDATED)",
    "abstract": "The Neural Tangent Kernel (NTK) has emerged as a powerful tool to provide memorization, optimization and generalization guarantees in deep neural networks. A line of work has studied the NTK spectrum for two-layer and deep networks with at least a layer with $\\Omega(N)$ neurons, $N$ being the number of training samples. Furthermore, there is increasing evidence suggesting that deep networks with sub-linear layer widths are powerful memorizers and optimizers, as long as the number of parameters exceeds the number of samples. Thus, a natural open question is whether the NTK is well conditioned in such a challenging sub-linear setup. In this paper, we answer this question in the affirmative. Our key technical contribution is a lower bound on the smallest NTK eigenvalue for deep networks with the minimum possible over-parameterization: the number of parameters is roughly $\\Omega(N)$ and, hence, the number of neurons is as little as $\\Omega(\\sqrt{N})$. To showcase the applicability of our N",
    "link": "http://arxiv.org/abs/2205.10217",
    "context": "Title: Memorization and Optimization in Deep Neural Networks with Minimum Over-parameterization. (arXiv:2205.10217v3 [stat.ML] UPDATED)\nAbstract: The Neural Tangent Kernel (NTK) has emerged as a powerful tool to provide memorization, optimization and generalization guarantees in deep neural networks. A line of work has studied the NTK spectrum for two-layer and deep networks with at least a layer with $\\Omega(N)$ neurons, $N$ being the number of training samples. Furthermore, there is increasing evidence suggesting that deep networks with sub-linear layer widths are powerful memorizers and optimizers, as long as the number of parameters exceeds the number of samples. Thus, a natural open question is whether the NTK is well conditioned in such a challenging sub-linear setup. In this paper, we answer this question in the affirmative. Our key technical contribution is a lower bound on the smallest NTK eigenvalue for deep networks with the minimum possible over-parameterization: the number of parameters is roughly $\\Omega(N)$ and, hence, the number of neurons is as little as $\\Omega(\\sqrt{N})$. To showcase the applicability of our N",
    "path": "papers/22/05/2205.10217.json",
    "total_tokens": 934,
    "translated_title": "带有最小超参数化的深度神经网络中的记忆化与优化",
    "translated_abstract": "神经切向核（NTK）已成为提供深度神经网络记忆化、优化和泛化保证的强大工具。部分学者已研究了至少一层具有$\\Omega(N)$个神经元的两层和深层网络的NTK谱，其中$N$是训练样本数量。此外，越来越多的证据表明，具有次线性层宽的深层神经网络是强大的记忆器和优化器，只要参数数量超过样本数量即可。因此，一个自然的开放性问题是在这种具有挑战性的次线性设置下，NTK是否存在良好的条件。在本文中，我们肯定地回答了这个问题。我们的主要技术贡献是提供了一个深度神经网络中最小的NTK特征值的下界，即参数数量大约为$\\Omega(N)$，因此神经元数量至少为$\\Omega(\\sqrt{N})$。为展示我们算法的适用性，我们在多项任务上进行了实证分析。",
    "tldr": "本文提供了一个最小超参数化的深度神经网络中最小的NTK特征值的下界，具有次线性层宽的深层神经网络是强大的记忆器和优化器，只要参数数量超过样本数量。",
    "en_tdlr": "This paper provides a lower bound on the smallest NTK eigenvalue for a deep neural network with minimal over-parameterization, and shows that deep networks with sub-linear layer widths are powerful memorizers and optimizers as long as the number of parameters exceeds the number of samples."
}