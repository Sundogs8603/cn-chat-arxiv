{
    "title": "Few-Shot Unlearning by Model Inversion. (arXiv:2205.15567v2 [cs.LG] UPDATED)",
    "abstract": "We consider a practical scenario of machine unlearning to erase a target dataset, which causes unexpected behavior from the trained model. The target dataset is often assumed to be fully identifiable in a standard unlearning scenario. Such a flawless identification, however, is almost impossible if the training dataset is inaccessible at the time of unlearning. Unlike previous approaches requiring a complete set of targets, we consider few-shot unlearning scenario when only a few samples of target data are available. To this end, we formulate the few-shot unlearning problem specifying intentions behind the unlearning request (e.g., purely unlearning, mislabel correction, privacy protection), and we devise a straightforward framework that (i) retrieves a proxy of the training data via model inversion fully exploiting information available in the context of unlearning; (ii) adjusts the proxy according to the unlearning intention; and (iii) updates the model with the adjusted proxy. We de",
    "link": "http://arxiv.org/abs/2205.15567",
    "context": "Title: Few-Shot Unlearning by Model Inversion. (arXiv:2205.15567v2 [cs.LG] UPDATED)\nAbstract: We consider a practical scenario of machine unlearning to erase a target dataset, which causes unexpected behavior from the trained model. The target dataset is often assumed to be fully identifiable in a standard unlearning scenario. Such a flawless identification, however, is almost impossible if the training dataset is inaccessible at the time of unlearning. Unlike previous approaches requiring a complete set of targets, we consider few-shot unlearning scenario when only a few samples of target data are available. To this end, we formulate the few-shot unlearning problem specifying intentions behind the unlearning request (e.g., purely unlearning, mislabel correction, privacy protection), and we devise a straightforward framework that (i) retrieves a proxy of the training data via model inversion fully exploiting information available in the context of unlearning; (ii) adjusts the proxy according to the unlearning intention; and (iii) updates the model with the adjusted proxy. We de",
    "path": "papers/22/05/2205.15567.json",
    "total_tokens": 775,
    "translated_title": "模型反演实现少样本去除学习",
    "translated_abstract": "论文讨论了一个实际的机器去除学习情境，即去除一个目标数据集，以消除已训练模型的意外行为。然而，在标准去除学习情境下，目标数据集通常被认为是可以完全确定的。但是如果训练数据集在去除学习时不可访问，则几乎不可能实现完美识别。与之前的方法不同，该论文考虑少样本去除学习情境，即仅有少量目标数据样本可用。为此，作者提出了一个直接的框架来解决问题：（i）通过模型反演获取训练数据的代理；（ii）根据去除学习的意图调整代理；（iii）用调整后的代理更新模型。",
    "tldr": "该论文提出了一个少样本去除学习的框架，通过模型反演获取训练数据代理，并根据去除学习意图进行调整。",
    "en_tdlr": "This paper proposes a framework for few-shot unlearning by retrieving a proxy of the training data via model inversion and adjusting it according to the unlearning intention."
}