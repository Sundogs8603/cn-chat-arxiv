{
    "title": "Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints. (arXiv:2205.15953v3 [cs.LG] UPDATED)",
    "abstract": "Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage. Determining \\textit{when to act} is crucial for achieving successful outcomes and yet, the challenge of efficiently \\textit{learning} to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we introduce a reinforcement learning (RL) framework named \\textbf{L}earnable \\textbf{I}mpulse \\textbf{C}ontrol \\textbf{R}einforcement \\textbf{A}lgorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as \\textit{impulse control} which learns to maximise objectives when actions",
    "link": "http://arxiv.org/abs/2205.15953",
    "context": "Title: Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints. (arXiv:2205.15953v3 [cs.LG] UPDATED)\nAbstract: Many real-world settings involve costs for performing actions; transaction costs in financial systems and fuel costs being common examples. In these settings, performing actions at each time step quickly accumulates costs leading to vastly suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and ultimately, damage. Determining \\textit{when to act} is crucial for achieving successful outcomes and yet, the challenge of efficiently \\textit{learning} to behave optimally when actions incur minimally bounded costs remains unresolved. In this paper, we introduce a reinforcement learning (RL) framework named \\textbf{L}earnable \\textbf{I}mpulse \\textbf{C}ontrol \\textbf{R}einforcement \\textbf{A}lgorithm (LICRA), for learning to optimally select both when to act and which actions to take when actions incur costs. At the core of LICRA is a nested structure that combines RL and a form of policy known as \\textit{impulse control} which learns to maximise objectives when actions",
    "path": "papers/22/05/2205.15953.json",
    "total_tokens": 831,
    "translated_title": "时机至关重要：学习在代价高昂的行动和预算限制下进行选择性行动",
    "translated_abstract": "许多实际应用场景中，执行行动都会产生成本；金融系统中的交易成本和燃油成本是常见的例子。在这些情况下，每个时间步骤执行行动迅速积累成本，导致极其不理想的结果。此外，反复行动会产生磨损和最终损坏。确定“何时行动”对于实现成功的结果至关重要，然而，在行动产生最小限制成本的情况下，高效地“学习”行为最优策略的挑战仍未得到解决。本文介绍了一种强化学习（RL）框架，名为Learnable Impulse Control Reinforcement Algorithm（LICRA），用于在行动产生成本的情况下学习选择何时行动和采取哪些行动以实现最优选择。",
    "tldr": "LICRA是学习在代价高昂的行动和预算限制下进行选择性行动的强化学习框架。"
}