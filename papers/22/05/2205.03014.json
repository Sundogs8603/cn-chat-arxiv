{
    "title": "Differentially Private Generalized Linear Models Revisited",
    "abstract": "arXiv:2205.03014v2 Announce Type: replace  Abstract: We study the problem of $(\\epsilon,\\delta)$-differentially private learning of linear predictors with convex losses. We provide results for two subclasses of loss functions. The first case is when the loss is smooth and non-negative but not necessarily Lipschitz (such as the squared loss). For this case, we establish an upper bound on the excess population risk of $\\tilde{O}\\left(\\frac{\\Vert w^*\\Vert}{\\sqrt{n}} + \\min\\left\\{\\frac{\\Vert w^* \\Vert^2}{(n\\epsilon)^{2/3}},\\frac{\\sqrt{d}\\Vert w^*\\Vert^2}{n\\epsilon}\\right\\}\\right)$, where $n$ is the number of samples, $d$ is the dimension of the problem, and $w^*$ is the minimizer of the population risk. Apart from the dependence on $\\Vert w^\\ast\\Vert$, our bound is essentially tight in all parameters. In particular, we show a lower bound of $\\tilde{\\Omega}\\left(\\frac{1}{\\sqrt{n}} + {\\min\\left\\{\\frac{\\Vert w^*\\Vert^{4/3}}{(n\\epsilon)^{2/3}}, \\frac{\\sqrt{d}\\Vert w^*\\Vert}{n\\epsilon}\\right\\}}",
    "link": "https://arxiv.org/abs/2205.03014",
    "context": "Title: Differentially Private Generalized Linear Models Revisited\nAbstract: arXiv:2205.03014v2 Announce Type: replace  Abstract: We study the problem of $(\\epsilon,\\delta)$-differentially private learning of linear predictors with convex losses. We provide results for two subclasses of loss functions. The first case is when the loss is smooth and non-negative but not necessarily Lipschitz (such as the squared loss). For this case, we establish an upper bound on the excess population risk of $\\tilde{O}\\left(\\frac{\\Vert w^*\\Vert}{\\sqrt{n}} + \\min\\left\\{\\frac{\\Vert w^* \\Vert^2}{(n\\epsilon)^{2/3}},\\frac{\\sqrt{d}\\Vert w^*\\Vert^2}{n\\epsilon}\\right\\}\\right)$, where $n$ is the number of samples, $d$ is the dimension of the problem, and $w^*$ is the minimizer of the population risk. Apart from the dependence on $\\Vert w^\\ast\\Vert$, our bound is essentially tight in all parameters. In particular, we show a lower bound of $\\tilde{\\Omega}\\left(\\frac{1}{\\sqrt{n}} + {\\min\\left\\{\\frac{\\Vert w^*\\Vert^{4/3}}{(n\\epsilon)^{2/3}}, \\frac{\\sqrt{d}\\Vert w^*\\Vert}{n\\epsilon}\\right\\}}",
    "path": "papers/22/05/2205.03014.json",
    "total_tokens": 1019,
    "translated_title": "重新审视差分隐私广义线性模型",
    "translated_abstract": "我们研究了具有凸损失的线性预测器的$(\\epsilon,\\delta)$-差分隐私学习问题。我们针对两个损失函数子类提供了结果。第一种情况是当损失是光滑且非负但不一定利普希兹时（如平方损失）。对于这种情况，我们建立了关于过量总体风险的上界，为$\\tilde{O}\\left(\\frac{\\Vert w^*\\Vert}{\\sqrt{n}} + \\min\\left\\{\\frac{\\Vert w^* \\Vert^2}{(n\\epsilon)^{2/3}},\\frac{\\sqrt{d}\\Vert w^*\\Vert^2}{n\\epsilon}\\right\\}\\right)$，其中$n$是样本数，$d$是问题的维度，$w^*$是总体风险的最小化者。除了对$\\Vert w^\\ast\\Vert$的依赖之外，我们的界限在所有参数上基本上是紧密的。特别地，我们展示了一个$\\tilde{\\Omega}\\left(\\frac{1}{\\sqrt{n}} + {\\min\\left\\{\\frac{\\Vert w^*\\Vert^{4/3}}{(n\\epsilon)^{2/3}}, \\frac{\\sqrt{d}\\Vert w^*\\Vert}{n\\epsilon}\\right\\}}$的下界。",
    "tldr": "研究了具有凸损失的线性预测器的$(\\epsilon,\\delta)$-差分隐私学习问题，为两个损失函数子类提供了结果，并在所有参数上展示了基本紧密的上界和下界。",
    "en_tdlr": "Investigated $(\\epsilon,\\delta)$-differentially private learning of linear predictors with convex losses, provided results for two subclasses of loss functions, and showed essentially tight upper and lower bounds in all parameters."
}