{
    "title": "On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation. (arXiv:2205.16001v4 [cs.CL] UPDATED)",
    "abstract": "A good automatic evaluation metric for language generation ideally correlates highly with human judgements of text quality. Yet, there is a dearth of such metrics, which inhibits the rapid and efficient progress of language generators. One exception is the recently proposed Mauve. In theory, Mauve measures an information-theoretic divergence between two probability distributions over strings: one representing the language generator under evaluation; the other representing the true natural language distribution. Mauve's authors argue that its success comes from the qualitative properties of their proposed divergence. Yet in practice, as this divergence is uncomputable, Mauve approximates it by measuring the divergence between multinomial distributions over clusters instead, where cluster assignments are attained by grouping strings based on a pre-trained language model's embeddings. As we show, however, this is not a tight approximation -- in either theory or practice. This begs the que",
    "link": "http://arxiv.org/abs/2205.16001",
    "context": "Title: On the Usefulness of Embeddings, Clusters and Strings for Text Generator Evaluation. (arXiv:2205.16001v4 [cs.CL] UPDATED)\nAbstract: A good automatic evaluation metric for language generation ideally correlates highly with human judgements of text quality. Yet, there is a dearth of such metrics, which inhibits the rapid and efficient progress of language generators. One exception is the recently proposed Mauve. In theory, Mauve measures an information-theoretic divergence between two probability distributions over strings: one representing the language generator under evaluation; the other representing the true natural language distribution. Mauve's authors argue that its success comes from the qualitative properties of their proposed divergence. Yet in practice, as this divergence is uncomputable, Mauve approximates it by measuring the divergence between multinomial distributions over clusters instead, where cluster assignments are attained by grouping strings based on a pre-trained language model's embeddings. As we show, however, this is not a tight approximation -- in either theory or practice. This begs the que",
    "path": "papers/22/05/2205.16001.json",
    "total_tokens": 892,
    "translated_title": "关于嵌入、聚类和字符串在文本生成器评估中的实用性",
    "translated_abstract": "一种好的自动评估语言生成度量标准应该与人类对文本质量的判断高度相关。然而，这样的度量标准很少，这阻碍了语言生成器的快速和高效发展。一个例外是最近提出的Mauve度量标准。理论上，Mauve度量的是两个概率分布之间的信息论差异：一个表示被评估的语言生成器，另一个表示真正的自然语言分布。Mauve的作者认为其成功来自于所提出差异的定性特性。然而在实践中，由于这个差异不可计算，Mauve通过衡量聚类上多项式分布之间的差异来近似表示，其中聚类分配是通过基于预训练语言模型的嵌入进行分组字符串获得的。然而，正如我们所展示的，这并不是一个严格的近似——无论是在理论还是实践中。",
    "tldr": "这篇论文讨论了语言生成器评估中的自动度量标准问题，以及目前存在的Mauve度量标准的局限性。当前的方法通过近似计算来衡量概率分布之间的差异，但在实践中并不是一个严格的近似。",
    "en_tdlr": "This paper discusses the issue of automatic evaluation metrics for language generation and the limitations of the current Mauve metric. The current approach approximates the divergence between probability distributions through clustering, but it is not a tight approximation in practice."
}