{
    "title": "Meta Self-Refinement for Robust Learning with Weak Supervision. (arXiv:2205.07290v2 [cs.CL] UPDATED)",
    "abstract": "Training deep neural networks (DNNs) under weak supervision has attracted increasing research attention as it can significantly reduce the annotation cost. However, labels from weak supervision can be noisy, and the high capacity of DNNs enables them to easily overfit the label noise, resulting in poor generalization. Recent methods leverage self-training to build noise-resistant models, in which a teacher trained under weak supervision is used to provide highly confident labels for teaching the students. Nevertheless, the teacher derived from such frameworks may have fitted a substantial amount of noise and therefore produce incorrect pseudo-labels with high confidence, leading to severe error propagation. In this work, we propose Meta Self-Refinement (MSR), a noise-resistant learning framework, to effectively combat label noise from weak supervision. Instead of relying on a fixed teacher trained with noisy labels, we encourage the teacher to refine its pseudo-labels. At each training",
    "link": "http://arxiv.org/abs/2205.07290",
    "context": "Title: Meta Self-Refinement for Robust Learning with Weak Supervision. (arXiv:2205.07290v2 [cs.CL] UPDATED)\nAbstract: Training deep neural networks (DNNs) under weak supervision has attracted increasing research attention as it can significantly reduce the annotation cost. However, labels from weak supervision can be noisy, and the high capacity of DNNs enables them to easily overfit the label noise, resulting in poor generalization. Recent methods leverage self-training to build noise-resistant models, in which a teacher trained under weak supervision is used to provide highly confident labels for teaching the students. Nevertheless, the teacher derived from such frameworks may have fitted a substantial amount of noise and therefore produce incorrect pseudo-labels with high confidence, leading to severe error propagation. In this work, we propose Meta Self-Refinement (MSR), a noise-resistant learning framework, to effectively combat label noise from weak supervision. Instead of relying on a fixed teacher trained with noisy labels, we encourage the teacher to refine its pseudo-labels. At each training",
    "path": "papers/22/05/2205.07290.json",
    "total_tokens": 1152,
    "translated_title": "元自我完善：使用弱监督实现鲁棒学习",
    "translated_abstract": "在弱监督下训练深度神经网络（DNN）已经引起越来越多的研究关注，因为它可以显著降低注释成本。然而，弱监督的标签可能存在噪声，而DNN的高容量使其很容易过度适应标签噪声，导致泛化性能差。最近的方法利用自我训练来构建抗噪声模型，其中使用在弱监督下训练的教师模型来为学生模型提供高度自信的标签。然而，从这种框架中推导出的教师模型可能已经拟合了大量噪声，并因此产生了高置信度的错误伪标签，导致严重的错误传播。在这项工作中，我们提出了一种名为元自我完善（MSR）的抗噪声学习框架，以有效地抵抗来自弱监督的标签噪声。我们鼓励教师模型改善其伪标签，而不是依赖于使用噪声标签训练的固定教师模型。在每次训练迭代中，MSR同时更新学生和教师模型，其中教师模型通过聚合不同轮次学生模型的输出来更新。我们在几个基准数据集上进行了大量实验，使用不同类型的弱监督，我们的提出的MSR方法始终提高了分类性能，相对于现有最先进方法有明显的提高。",
    "tldr": "提出了一个名为元自我完善（MSR）的抗噪声学习框架，采用教师模型来提供高度自信的标签，并通过聚合不同轮次学生模型的输出来更新教师模型，较现有最先进方法有明显提高。"
}