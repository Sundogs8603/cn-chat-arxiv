{
    "title": "On the efficiency of Stochastic Quasi-Newton Methods for Deep Learning. (arXiv:2205.09121v2 [cs.LG] UPDATED)",
    "abstract": "While first-order methods are popular for solving optimization problems that arise in large-scale deep learning problems, they come with some acute deficiencies. To diminish such shortcomings, there has been recent interest in applying second-order methods such as quasi-Newton based methods which construct Hessians approximations using only gradient information. The main focus of our work is to study the behaviour of stochastic quasi-Newton algorithms for training deep neural networks. We have analyzed the performance of two well-known quasi-Newton updates, the limited memory Broyden-Fletcher-Goldfarb-Shanno (BFGS) and the Symmetric Rank One (SR1). This study fills a gap concerning the real performance of both updates and analyzes whether more efficient training is obtained when using the more robust BFGS update or the cheaper SR1 formula which allows for indefinite Hessian approximations and thus can potentially help to better navigate the pathological saddle points present in the non",
    "link": "http://arxiv.org/abs/2205.09121",
    "context": "Title: On the efficiency of Stochastic Quasi-Newton Methods for Deep Learning. (arXiv:2205.09121v2 [cs.LG] UPDATED)\nAbstract: While first-order methods are popular for solving optimization problems that arise in large-scale deep learning problems, they come with some acute deficiencies. To diminish such shortcomings, there has been recent interest in applying second-order methods such as quasi-Newton based methods which construct Hessians approximations using only gradient information. The main focus of our work is to study the behaviour of stochastic quasi-Newton algorithms for training deep neural networks. We have analyzed the performance of two well-known quasi-Newton updates, the limited memory Broyden-Fletcher-Goldfarb-Shanno (BFGS) and the Symmetric Rank One (SR1). This study fills a gap concerning the real performance of both updates and analyzes whether more efficient training is obtained when using the more robust BFGS update or the cheaper SR1 formula which allows for indefinite Hessian approximations and thus can potentially help to better navigate the pathological saddle points present in the non",
    "path": "papers/22/05/2205.09121.json",
    "total_tokens": 958,
    "translated_title": "关于随机拟牛顿方法在深度学习中的效率问题",
    "translated_abstract": "尽管在大规模深度学习问题中，一阶方法非常流行来解决优化问题，但它们存在一些明显的缺点。为了减少这些缺点，最近一直有兴趣应用基于拟牛顿的二阶方法，这些方法使用梯度信息构建Hessian矩阵的近似。我们的研究主要关注随机拟牛顿算法在训练深度神经网络中的行为。我们分析了两种著名的拟牛顿更新算法，即有限内存Broyden-Fletcher-Goldfarb-Shanno（BFGS）和对称秩一（SR1）算法。这项研究填补了关于这两种方法真实性能的空白，并分析了在使用更强大的BFGS算法还是更廉价的SR1算法进行训练时是否得到更高效的结果，SR1算法允许不定Hessian矩阵近似，从而有助于更好地避开非凸优化问题中出现的病态鞍点。",
    "tldr": "本文研究了随机拟牛顿算法在深度学习中的效率，分析了有限内存BFGS和对称秩一SR1两种更新算法的性能表现，并比较了两者的优劣，探讨了SR1算法在处理非凸优化问题中病态鞍点时的潜力。",
    "en_tdlr": "This paper investigates the efficiency of stochastic quasi-Newton algorithms for deep learning, specifically analyzing the performance of limited memory BFGS and symmetric rank one SR1 updates. The study aims to compare their effectiveness and explore the potential of SR1 in handling pathological saddle points in non-convex optimization problems."
}