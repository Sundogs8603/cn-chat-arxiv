{
    "title": "On the Inconsistency of Kernel Ridgeless Regression in Fixed Dimensions. (arXiv:2205.13525v3 [cs.LG] UPDATED)",
    "abstract": "``Benign overfitting'', the ability of certain algorithms to interpolate noisy training data and yet perform well out-of-sample, has been a topic of considerable recent interest. We show, using a fixed design setup, that an important class of predictors, kernel machines with translation-invariant kernels, does not exhibit benign overfitting in fixed dimensions. In particular, the estimated predictor does not converge to the ground truth with increasing sample size, for any non-zero regression function and any (even adaptive) bandwidth selection. To prove these results, we give exact expressions for the generalization error, and its decomposition in terms of an approximation error and an estimation error that elicits a trade-off based on the selection of the kernel bandwidth. Our results apply to commonly used translation-invariant kernels such as Gaussian, Laplace, and Cauchy.",
    "link": "http://arxiv.org/abs/2205.13525",
    "context": "Title: On the Inconsistency of Kernel Ridgeless Regression in Fixed Dimensions. (arXiv:2205.13525v3 [cs.LG] UPDATED)\nAbstract: ``Benign overfitting'', the ability of certain algorithms to interpolate noisy training data and yet perform well out-of-sample, has been a topic of considerable recent interest. We show, using a fixed design setup, that an important class of predictors, kernel machines with translation-invariant kernels, does not exhibit benign overfitting in fixed dimensions. In particular, the estimated predictor does not converge to the ground truth with increasing sample size, for any non-zero regression function and any (even adaptive) bandwidth selection. To prove these results, we give exact expressions for the generalization error, and its decomposition in terms of an approximation error and an estimation error that elicits a trade-off based on the selection of the kernel bandwidth. Our results apply to commonly used translation-invariant kernels such as Gaussian, Laplace, and Cauchy.",
    "path": "papers/22/05/2205.13525.json",
    "total_tokens": 825,
    "translated_title": "关于固定维度下核岭回归的不一致性",
    "translated_abstract": "“良性过拟合”是指某些算法能够插值噪声训练数据并在样本外表现良好，已经引起了最近的相当大的关注。我们使用固定设计设置表明，具有平移不变核的重要预测类别，例如高斯核，拉普拉斯核和柯西核在固定维度下并不表现出良性过拟合。特别地，任何非零回归函数和任何（甚至是自适应的）带宽选择都不会使得估计的预测器随着样本量的增加收敛于基本真相。为了证明这些结果，我们给出了泛化误差及其在对kernel带宽的选择上产生权衡之下的近似误差和估计误差的确切表达式。",
    "tldr": "在固定维度下，平移不变核的重要预测类别，如高斯核、拉普拉斯核和柯西核，在任何非零回归函数和任何带宽选择下都不具备“良性过拟合”特性。",
    "en_tdlr": "In fixed dimensions, an important class of predictors, kernel machines with translation-invariant kernels such as Gaussian, Laplace, and Cauchy, does not exhibit benign overfitting for any non-zero regression function and any bandwidth selection."
}