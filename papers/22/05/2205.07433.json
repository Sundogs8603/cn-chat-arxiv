{
    "title": "Binarizing by Classification: Is soft function really necessary?. (arXiv:2205.07433v3 [cs.CV] UPDATED)",
    "abstract": "Binary neural networks leverage $\\mathrm{Sign}$ function to binarize weights and activations, which require gradient estimators to overcome its non-differentiability and will inevitably bring gradient errors during backpropagation. Although many hand-designed soft functions have been proposed as gradient estimators to better approximate gradients, their mechanism is not clear and there are still huge performance gaps between binary models and their full-precision counterparts. To address these issues and reduce gradient error, we propose to tackle network binarization as a binary classification problem and use a multi-layer perceptron (MLP) as the classifier in the forward pass and gradient estimator in the backward pass. Benefiting from the MLP's theoretical capability to fit any continuous function, it can be adaptively learned to binarize networks and backpropagate gradients without any prior knowledge of soft functions. From this perspective, we further empirically justify that eve",
    "link": "http://arxiv.org/abs/2205.07433",
    "context": "Title: Binarizing by Classification: Is soft function really necessary?. (arXiv:2205.07433v3 [cs.CV] UPDATED)\nAbstract: Binary neural networks leverage $\\mathrm{Sign}$ function to binarize weights and activations, which require gradient estimators to overcome its non-differentiability and will inevitably bring gradient errors during backpropagation. Although many hand-designed soft functions have been proposed as gradient estimators to better approximate gradients, their mechanism is not clear and there are still huge performance gaps between binary models and their full-precision counterparts. To address these issues and reduce gradient error, we propose to tackle network binarization as a binary classification problem and use a multi-layer perceptron (MLP) as the classifier in the forward pass and gradient estimator in the backward pass. Benefiting from the MLP's theoretical capability to fit any continuous function, it can be adaptively learned to binarize networks and backpropagate gradients without any prior knowledge of soft functions. From this perspective, we further empirically justify that eve",
    "path": "papers/22/05/2205.07433.json",
    "total_tokens": 917,
    "translated_title": "基于分类的二值化：软函数真的必要吗？",
    "translated_abstract": "二值神经网络利用\"Sign\"函数对权重和激活进行二值化，这需要梯度估计器来克服其不可微性，并且在反向传播过程中不可避免地带来梯度误差。虽然许多手动设计的软函数被提出作为梯度估计器以更好地逼近梯度，但它们的机制尚不清楚，并且二值模型与完全精度模型之间仍存在巨大的性能差距。为了解决这些问题并减少梯度误差，我们提出将网络二值化视为一个二值分类问题，并在前向传递中使用多层感知器（MLP）作为分类器以及在反向传递中作为梯度估计器。由于MLP具有适应任何连续函数的理论能力，它可以自适应地学习二值化网络并在没有任何先验知识的情况下反向传播梯度。从这个角度来看，我们进一步通过实验证明了即使在没有任何软函数的情况下，MLP也具有进行网络二值化和反向传播梯度的能力。",
    "tldr": "本文提出将网络二值化视为一个二值分类问题，使用多层感知器作为分类器和梯度估计器，以解决二值神经网络的梯度估计问题，从而实现更好的性能。",
    "en_tdlr": "This paper proposes to tackle network binarization as a binary classification problem and use a multi-layer perceptron as the classifier and gradient estimator to overcome the gradient estimation issues in binary neural networks, achieving improved performance."
}