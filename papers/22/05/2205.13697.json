{
    "title": "FedFormer: Contextual Federation with Attention in Reinforcement Learning. (arXiv:2205.13697v3 [cs.LG] CROSS LISTED)",
    "abstract": "A core issue in multi-agent federated reinforcement learning is defining how to aggregate insights from multiple agents. This is commonly done by taking the average of each participating agent's model weights into one common model (FedAvg). We instead propose FedFormer, a novel federation strategy that utilizes Transformer Attention to contextually aggregate embeddings from models originating from different learner agents. In so doing, we attentively weigh the contributions of other agents with respect to the current agent's environment and learned relationships, thus providing a more effective and efficient federation. We evaluate our methods on the Meta-World environment and find that our approach yields significant improvements over FedAvg and non-federated Soft Actor-Critic single-agent methods. Our results compared to Soft Actor-Critic show that FedFormer achieves higher episodic return while still abiding by the privacy constraints of federated learning. Finally, we also demonstr",
    "link": "http://arxiv.org/abs/2205.13697",
    "context": "Title: FedFormer: Contextual Federation with Attention in Reinforcement Learning. (arXiv:2205.13697v3 [cs.LG] CROSS LISTED)\nAbstract: A core issue in multi-agent federated reinforcement learning is defining how to aggregate insights from multiple agents. This is commonly done by taking the average of each participating agent's model weights into one common model (FedAvg). We instead propose FedFormer, a novel federation strategy that utilizes Transformer Attention to contextually aggregate embeddings from models originating from different learner agents. In so doing, we attentively weigh the contributions of other agents with respect to the current agent's environment and learned relationships, thus providing a more effective and efficient federation. We evaluate our methods on the Meta-World environment and find that our approach yields significant improvements over FedAvg and non-federated Soft Actor-Critic single-agent methods. Our results compared to Soft Actor-Critic show that FedFormer achieves higher episodic return while still abiding by the privacy constraints of federated learning. Finally, we also demonstr",
    "path": "papers/22/05/2205.13697.json",
    "total_tokens": 854,
    "translated_title": "FedFormer：强化学习中的上下文联邦学习与注意力",
    "translated_abstract": "多智能体联邦强化学习中一个核心问题是如何汇总多个智能体的见解。通常采用将每个参与智能体的模型权重取平均得到一个共同模型（FedAvg）。我们提出了FedFormer，一种新的联邦策略，利用Transformer Attention来上下文地汇总来自不同学习智能体的嵌入。通过这样做，我们根据当前智能体的环境和学得关系有选择地衡量其他智能体的贡献，从而提供更有效和高效的联邦学习。我们在Meta-World环境中对我们的方法进行了评估，并发现我们的方法在FedAvg和非联邦Soft Actor-Critic单智能体方法上都取得了显著的改进。与Soft Actor-Critic相比，我们的结果表明FedFormer在仍遵守联邦学习的隐私约束的同时获得了更高的分集回报。最后，我们还演示了该方法的实现。",
    "tldr": "FedFormer是一种新的强化学习联邦策略，利用Transformer Attention上下文地汇总来自不同学习智能体的嵌入，具有更高的回报和更高的效率。",
    "en_tdlr": "FedFormer is a novel reinforcement learning federation strategy that utilizes Transformer Attention to contextually aggregate embeddings from models originating from different learner agents, achieving higher returns and greater efficiency."
}