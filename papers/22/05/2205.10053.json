{
    "title": "What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders. (arXiv:2205.10053v2 [cs.LG] UPDATED)",
    "abstract": "The last years have witnessed the emergence of a promising self-supervised learning strategy, referred to as masked autoencoding. However, there is a lack of theoretical understanding of how masking matters on graph autoencoders (GAEs). In this work, we present masked graph autoencoder (MaskGAE), a self-supervised learning framework for graph-structured data. Different from standard GAEs, MaskGAE adopts masked graph modeling (MGM) as a principled pretext task - masking a portion of edges and attempting to reconstruct the missing part with partially visible, unmasked graph structure. To understand whether MGM can help GAEs learn better representations, we provide both theoretical and empirical evidence to comprehensively justify the benefits of this pretext task. Theoretically, we establish close connections between GAEs and contrastive learning, showing that MGM significantly improves the self-supervised learning scheme of GAEs. Empirically, we conduct extensive experiments on a variet",
    "link": "http://arxiv.org/abs/2205.10053",
    "context": "Title: What's Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders. (arXiv:2205.10053v2 [cs.LG] UPDATED)\nAbstract: The last years have witnessed the emergence of a promising self-supervised learning strategy, referred to as masked autoencoding. However, there is a lack of theoretical understanding of how masking matters on graph autoencoders (GAEs). In this work, we present masked graph autoencoder (MaskGAE), a self-supervised learning framework for graph-structured data. Different from standard GAEs, MaskGAE adopts masked graph modeling (MGM) as a principled pretext task - masking a portion of edges and attempting to reconstruct the missing part with partially visible, unmasked graph structure. To understand whether MGM can help GAEs learn better representations, we provide both theoretical and empirical evidence to comprehensively justify the benefits of this pretext task. Theoretically, we establish close connections between GAEs and contrastive learning, showing that MGM significantly improves the self-supervised learning scheme of GAEs. Empirically, we conduct extensive experiments on a variet",
    "path": "papers/22/05/2205.10053.json",
    "total_tokens": 1179,
    "translated_title": "探究图自编码器中 Masked Graph Modeling 的作用",
    "translated_abstract": "近年来，自监督学习的一种颇具潜力的策略被称为 Masked Autoencoding。然而，对于在图自编码器中如何实现 Masking，理论上仍然存在缺失。本文提出了 Masked Graph Autoencoder （MaskGAE），它是用于图结构数据的自监督学习框架。相较于其它普通的 GAEs，MaskGAE 采用基于蒙版的图建模（Masked Graph Modeling，MGM）作为一个原先的任务。在这个任务中，“掩蔽”一个部分边缘，以部分可视、非掩蔽的图结构，试图重构缺失的部分。我们提供了理论及实证证据，全面证明了此预测任务优势的作用，以探究 MGM 对 GAEs 的输出表示的改善作用。在理论上，我们建立了 GAEs 与对比学习的紧密关系，表明 MGM 明显改善了 GAEs 的自监督学习方案。在经验方面，我们在各种基准数据集上进行了广泛实验，并展示了 MaskGAE 在各种评估指标下始终优于最先进的 GAEs。本研究为探究 Masked Graph Modeling 在图自编码器中的重要性提供了启示，并为图表示学习提供了一种可扩展和有效的自监督学习方法。",
    "tldr": "本文探究了 Masked Graph Modeling 在图自编码器中的有效应用，并提出了基于蒙版的图建模（MGM）作为自监督学习的一个原先的任务，证明了它的优势作用。在获得了理论及实证证据的支持后，我们提出了 MaskGAE，一种可扩展有效的自监督学习方法。",
    "en_tdlr": "This article explores the effective application of Masked Graph Modeling in the graph autoencoder (GAE) and proposes Masked Graph Autoencoder (MaskGAE) as a self-supervised learning framework for graph-structured data. The article provides theoretical and empirical evidence to justify the benefits of MGM as a pretext task for GAEs, showing that it significantly improves the self-supervised learning scheme of GAEs. The study sheds light on the significance of masked graph modeling for GAEs and provides a scalable and effective self-supervised learning approach for graph representation learning."
}