{
    "title": "Deletion and Insertion Tests in Regression Models. (arXiv:2205.12423v3 [cs.LG] UPDATED)",
    "abstract": "A basic task in explainable AI (XAI) is to identify the most important features behind a prediction made by a black box function $f$. The insertion and deletion tests of Petsiuk et al. (2018) can be used to judge the quality of algorithms that rank pixels from most to least important for a classification. Motivated by regression problems we establish a formula for their area under the curve (AUC) criteria in terms of certain main effects and interactions in an anchored decomposition of $f$. We find an expression for the expected value of the AUC under a random ordering of inputs to $f$ and propose an alternative area above a straight line for the regression setting. We use this criterion to compare feature importances computed by integrated gradients (IG) to those computed by Kernel SHAP (KS) as well as LIME, DeepLIFT, vanilla gradient and input$\\times$gradient methods. KS has the best overall performance in two datasets we consider but it is very expensive to compute. We find that IG ",
    "link": "http://arxiv.org/abs/2205.12423",
    "context": "Title: Deletion and Insertion Tests in Regression Models. (arXiv:2205.12423v3 [cs.LG] UPDATED)\nAbstract: A basic task in explainable AI (XAI) is to identify the most important features behind a prediction made by a black box function $f$. The insertion and deletion tests of Petsiuk et al. (2018) can be used to judge the quality of algorithms that rank pixels from most to least important for a classification. Motivated by regression problems we establish a formula for their area under the curve (AUC) criteria in terms of certain main effects and interactions in an anchored decomposition of $f$. We find an expression for the expected value of the AUC under a random ordering of inputs to $f$ and propose an alternative area above a straight line for the regression setting. We use this criterion to compare feature importances computed by integrated gradients (IG) to those computed by Kernel SHAP (KS) as well as LIME, DeepLIFT, vanilla gradient and input$\\times$gradient methods. KS has the best overall performance in two datasets we consider but it is very expensive to compute. We find that IG ",
    "path": "papers/22/05/2205.12423.json",
    "total_tokens": 964,
    "translated_title": "回归模型中的删除和插入测试",
    "translated_abstract": "解释性人工智能（XAI）中的一个基本任务是确定黑盒函数$f$预测背后最重要的特征。Petsiuk等人（2018）的插入和删除测试可以用来评判对于分类中像素从重要到不重要进行排序的算法的质量。在回归问题上，我们建立了一个公式，以$f$的主效应和交互作用来衡量其曲线下面积（AUC）的标准。我们找到了在输入随机顺序下AUC的期望值的表达式，并提出了一个适用于回归设置的直线上方面积的替代指标。我们使用这个指标将集成梯度（IG）计算的特征重要性与Kernel SHAP（KS）、LIME、DeepLIFT、vanilla gradient和input$\\times$gradient方法计算的特征重要性进行比较。在我们考虑的两个数据集中，KS的整体表现最好，但计算代价很高。我们发现IG在一些数据集上和KS表现相近，但计算更快速。",
    "tldr": "本研究提出了一种在回归模型中评估删除和插入测试的方法，用于确定解释性人工智能中最重要的特征。我们通过比较不同算法计算的特征重要性，发现Kernel SHAP在综合性能方面表现最佳。我们还提出了一种计算更快速的替代指标，适用于回归设置。",
    "en_tdlr": "This study proposes a method to evaluate deletion and insertion tests in regression models for determining the most important features in explainable AI. By comparing feature importances computed by different algorithms, Kernel SHAP performs the best overall. An alternative metric that is computationally efficient for regression settings is also proposed."
}