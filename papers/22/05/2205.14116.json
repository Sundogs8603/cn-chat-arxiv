{
    "title": "Don't Explain Noise: Robust Counterfactuals for Randomized Ensembles",
    "abstract": "arXiv:2205.14116v3 Announce Type: replace  Abstract: Counterfactual explanations describe how to modify a feature vector in order to flip the outcome of a trained classifier. Obtaining robust counterfactual explanations is essential to provide valid algorithmic recourse and meaningful explanations. We study the robustness of explanations of randomized ensembles, which are always subject to algorithmic uncertainty even when the training data is fixed. We formalize the generation of robust counterfactual explanations as a probabilistic problem and show the link between the robustness of ensemble models and the robustness of base learners. We develop a practical method with good empirical performance and support it with theoretical guarantees for ensembles of convex base learners. Our results show that existing methods give surprisingly low robustness: the validity of naive counterfactuals is below $50\\%$ on most data sets and can fall to $20\\%$ on problems with many features. In contrast",
    "link": "https://arxiv.org/abs/2205.14116",
    "context": "Title: Don't Explain Noise: Robust Counterfactuals for Randomized Ensembles\nAbstract: arXiv:2205.14116v3 Announce Type: replace  Abstract: Counterfactual explanations describe how to modify a feature vector in order to flip the outcome of a trained classifier. Obtaining robust counterfactual explanations is essential to provide valid algorithmic recourse and meaningful explanations. We study the robustness of explanations of randomized ensembles, which are always subject to algorithmic uncertainty even when the training data is fixed. We formalize the generation of robust counterfactual explanations as a probabilistic problem and show the link between the robustness of ensemble models and the robustness of base learners. We develop a practical method with good empirical performance and support it with theoretical guarantees for ensembles of convex base learners. Our results show that existing methods give surprisingly low robustness: the validity of naive counterfactuals is below $50\\%$ on most data sets and can fall to $20\\%$ on problems with many features. In contrast",
    "path": "papers/22/05/2205.14116.json",
    "total_tokens": 913,
    "translated_title": "不解释噪音：面向随机集成的稳健反事实表达",
    "translated_abstract": "反事实解释描述了如何修改特征向量以改变经过训练的分类器的结果。获得稳健的反事实解释对于提供有效的算法补救和有意义的解释至关重要。我们研究了随机集成的解释的稳健性，即使在训练数据固定的情况下，它们始终受到算法不确定性的影响。我们将稳健反事实解释的生成形式化为一个概率问题，并展示了集成模型的稳健性与基学习器的稳健性之间的联系。我们开发了一种在凸基学习器的集成上具有良好实证表现的实用方法，并用理论保证支持它。我们的结果表明，现有方法的稳健性令人惊讶地低：大多数数据集上天真反事实的有效性低于50％，在具有许多特征的问题上可能会降至20％。",
    "tldr": "本研究研究了随机集成的解释的稳健性，提出了一种生成稳健反事实解释的概率方法，展示了集成模型稳健性与基学习器稳健性之间的联系，并为凸基学习器的集成提供了实用方法和理论保证。",
    "en_tdlr": "This study examines the robustness of explanations for randomized ensembles, presents a probabilistic method for generating robust counterfactual explanations, demonstrates the connection between ensemble model robustness and base learner robustness, and provides a practical method and theoretical guarantees for ensembles of convex base learners."
}