{
    "title": "Unsupervised Discovery and Composition of Object Light Fields. (arXiv:2205.03923v2 [cs.CV] UPDATED)",
    "abstract": "Neural scene representations, both continuous and discrete, have recently emerged as a powerful new paradigm for 3D scene understanding. Recent efforts have tackled unsupervised discovery of object-centric neural scene representations. However, the high cost of ray-marching, exacerbated by the fact that each object representation has to be ray-marched separately, leads to insufficiently sampled radiance fields and thus, noisy renderings, poor framerates, and high memory and time complexity during training and rendering. Here, we propose to represent objects in an object-centric, compositional scene representation as light fields. We propose a novel light field compositor module that enables reconstructing the global light field from a set of object-centric light fields. Dubbed Compositional Object Light Fields (COLF), our method enables unsupervised learning of object-centric neural scene representations, state-of-the-art reconstruction and novel view synthesis performance on standard ",
    "link": "http://arxiv.org/abs/2205.03923",
    "context": "Title: Unsupervised Discovery and Composition of Object Light Fields. (arXiv:2205.03923v2 [cs.CV] UPDATED)\nAbstract: Neural scene representations, both continuous and discrete, have recently emerged as a powerful new paradigm for 3D scene understanding. Recent efforts have tackled unsupervised discovery of object-centric neural scene representations. However, the high cost of ray-marching, exacerbated by the fact that each object representation has to be ray-marched separately, leads to insufficiently sampled radiance fields and thus, noisy renderings, poor framerates, and high memory and time complexity during training and rendering. Here, we propose to represent objects in an object-centric, compositional scene representation as light fields. We propose a novel light field compositor module that enables reconstructing the global light field from a set of object-centric light fields. Dubbed Compositional Object Light Fields (COLF), our method enables unsupervised learning of object-centric neural scene representations, state-of-the-art reconstruction and novel view synthesis performance on standard ",
    "path": "papers/22/05/2205.03923.json",
    "total_tokens": 850,
    "translated_title": "无监督发现和合成物体光场",
    "translated_abstract": "近期，神经场景表示法，包括连续和离散表示，已经成为三维场景理解的强大新范式。最近的研究致力于无监督发现以物体为中心的神经场景表示法。然而，射线行进的高成本，加上每个物体表示法都必须单独进行射线行进的事实，导致采样不足的亮度场，从而产生噪点渲染、低帧率、高内存和时间复杂度的训练和渲染。在这里，我们提出将物体以物体为中心的光场表示法来表示。我们提出了一种新颖的光场复合模块，可以从一组以物体为中心的光场重建全局光场。我们的方法被称为组合性物体光场（COLF），可以实现无监督学习以物体为中心的神经场景表示法，在标准数据集上实现最先进的重建和新视角合成性能。",
    "tldr": "本文提出了一种无监督发现和合成物体光场的方法，通过将物体表示为以物体为中心的光场来提高渲染质量和操作效率。",
    "en_tdlr": "This paper proposes a method for unsupervised discovery and composition of object light fields, enhancing rendering quality and operational efficiency by representing objects as object-centric light fields."
}