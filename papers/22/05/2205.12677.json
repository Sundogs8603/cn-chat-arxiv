{
    "title": "Language Anisotropic Cross-Lingual Model Editing. (arXiv:2205.12677v2 [cs.CL] UPDATED)",
    "abstract": "Multilingual pre-trained language models can learn task-specific abilities or memorize facts across multiple languages but inevitably make undesired predictions with specific inputs. Under similar observation, model editing aims to post-hoc calibrate a model targeted to specific inputs with keeping the model's raw behavior. However, existing work only studies the monolingual scenario, which lacks the cross-lingual transferability to perform editing simultaneously across languages. In this work, we focus on cross-lingual model editing. Firstly, we define the cross-lingual model editing task and corresponding metrics, where an edit in one language propagates to the others. Next, we propose a framework to naturally adapt monolingual model editing approaches to the cross-lingual scenario using parallel corpus. Further, we propose language anisotropic editing to improve cross-lingual editing by amplifying different subsets of parameters for each language. On the newly defined cross-lingual ",
    "link": "http://arxiv.org/abs/2205.12677",
    "context": "Title: Language Anisotropic Cross-Lingual Model Editing. (arXiv:2205.12677v2 [cs.CL] UPDATED)\nAbstract: Multilingual pre-trained language models can learn task-specific abilities or memorize facts across multiple languages but inevitably make undesired predictions with specific inputs. Under similar observation, model editing aims to post-hoc calibrate a model targeted to specific inputs with keeping the model's raw behavior. However, existing work only studies the monolingual scenario, which lacks the cross-lingual transferability to perform editing simultaneously across languages. In this work, we focus on cross-lingual model editing. Firstly, we define the cross-lingual model editing task and corresponding metrics, where an edit in one language propagates to the others. Next, we propose a framework to naturally adapt monolingual model editing approaches to the cross-lingual scenario using parallel corpus. Further, we propose language anisotropic editing to improve cross-lingual editing by amplifying different subsets of parameters for each language. On the newly defined cross-lingual ",
    "path": "papers/22/05/2205.12677.json",
    "total_tokens": 1007,
    "translated_title": "《语言的各向异性跨语言模型编辑》",
    "translated_abstract": "多语言预训练语言模型可以在多种语言中学习特定任务的能力或记住事实，但不可避免地会在使用特定输入时做出不良预测。与此类似，模型编辑旨在事后校准针对特定输入的模型，并保持模型的原始行为。然而，现有工作仅研究了单语境结构，缺乏跨语境传递性以同时执行跨语言编辑。本文重点研究跨语言模型编辑。首先，我们定义了跨语言模型编辑任务和相应的度量标准，其中一个语言的编辑会传播到其他语言。接下来，我们提出了一个框架，通过平行语料库自然地适应单语境模型编辑方法到跨语境情况。此外，我们提出了语言各向异性编辑方法，以改善跨语境编辑，通过增强每种语言的不同参数子集。在新定义的跨语境模型编辑任务中，我们的提出的框架在单语言和多语言评估中都取得了显着改进。我们的方法可以校准针对特定输入的预测，同时保持模型的原始行为，使其在实际应用中更加可靠。",
    "tldr": "本文提出了语言各向异性跨语言模型编辑的框架，并通过在跨语言任务上的测试证明了该方法的有效性，可以针对特定输入进行校准而不影响模型的原始行为。",
    "en_tdlr": "This paper proposes a language anisotropic cross-lingual model editing framework, which shows its effectiveness in cross-lingual tasks. It can calibrate predictions for specific inputs without affecting the model's raw behavior."
}