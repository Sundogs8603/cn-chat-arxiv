{
    "title": "Gradient Descent, Stochastic Optimization, and Other Tales. (arXiv:2205.00832v2 [cs.LG] UPDATED)",
    "abstract": "The goal of this paper is to debunk and dispel the magic behind black-box optimizers and stochastic optimizers. It aims to build a solid foundation on how and why the techniques work. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind the strategies. This tutorial doesn't shy away from addressing both the formal and informal aspects of gradient descent and stochastic optimization methods. By doing so, it hopes to provide readers with a deeper understanding of these techniques as well as the when, the how and the why of applying these algorithms.  Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize machine learning tasks. Its stochastic version receives attention in recent years, and this is particularly true for optimizing deep neural networks. In deep neural networks, the gradient followed by a single sample or a batch of samples is employed to save computational r",
    "link": "http://arxiv.org/abs/2205.00832",
    "context": "Title: Gradient Descent, Stochastic Optimization, and Other Tales. (arXiv:2205.00832v2 [cs.LG] UPDATED)\nAbstract: The goal of this paper is to debunk and dispel the magic behind black-box optimizers and stochastic optimizers. It aims to build a solid foundation on how and why the techniques work. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind the strategies. This tutorial doesn't shy away from addressing both the formal and informal aspects of gradient descent and stochastic optimization methods. By doing so, it hopes to provide readers with a deeper understanding of these techniques as well as the when, the how and the why of applying these algorithms.  Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize machine learning tasks. Its stochastic version receives attention in recent years, and this is particularly true for optimizing deep neural networks. In deep neural networks, the gradient followed by a single sample or a batch of samples is employed to save computational r",
    "path": "papers/22/05/2205.00832.json",
    "total_tokens": 818,
    "translated_title": "梯度下降，随机优化和其他故事",
    "translated_abstract": "本文旨在揭示黑盒优化器和随机优化器背后的神奇，并建立起这些技术的工作原理和原因的坚实基础。通过推导简单直觉背后的数学策略，本文将这些知识凝结成文字。本教程毫不回避梯度下降和随机优化方法的形式和非形式方面。通过这样做，它希望向读者提供更深入的理解，以及何时、如何和为什么应用这些算法。梯度下降是执行优化任务最流行的算法之一，也是优化机器学习任务最常见的方法。最近几年，其随机版本备受关注，特别适用于优化深度神经网络。在深度神经网络中，使用单个样本或一批样本的梯度来节省计算资源。",
    "tldr": "本文旨在揭示黑盒优化器和随机优化器背后的神奇，并建立起这些技术的工作原理和原因的坚实基础。"
}