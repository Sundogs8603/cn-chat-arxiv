{
    "title": "Multi-resolution partial differential equations preserved learning framework for spatiotemporal dynamics. (arXiv:2205.03990v3 [cs.LG] UPDATED)",
    "abstract": "Traditional data-driven deep learning models often struggle with high training costs, error accumulation, and poor generalizability in complex physical processes. Physics-informed deep learning (PiDL) addresses these challenges by incorporating physical principles into the model. Most PiDL approaches regularize training by embedding governing equations into the loss function, yet this depends heavily on extensive hyperparameter tuning to weigh each loss term. To this end, we propose to leverage physics prior knowledge by ``baking'' the discretized governing equations into the neural network architecture via the connection between the partial differential equations (PDE) operators and network structures, resulting in a PDE-preserved neural network (PPNN). This method, embedding discretized PDEs through convolutional residual networks in a multi-resolution setting, largely improves the generalizability and long-term prediction accuracy, outperforming conventional black-box models. The ef",
    "link": "http://arxiv.org/abs/2205.03990",
    "context": "Title: Multi-resolution partial differential equations preserved learning framework for spatiotemporal dynamics. (arXiv:2205.03990v3 [cs.LG] UPDATED)\nAbstract: Traditional data-driven deep learning models often struggle with high training costs, error accumulation, and poor generalizability in complex physical processes. Physics-informed deep learning (PiDL) addresses these challenges by incorporating physical principles into the model. Most PiDL approaches regularize training by embedding governing equations into the loss function, yet this depends heavily on extensive hyperparameter tuning to weigh each loss term. To this end, we propose to leverage physics prior knowledge by ``baking'' the discretized governing equations into the neural network architecture via the connection between the partial differential equations (PDE) operators and network structures, resulting in a PDE-preserved neural network (PPNN). This method, embedding discretized PDEs through convolutional residual networks in a multi-resolution setting, largely improves the generalizability and long-term prediction accuracy, outperforming conventional black-box models. The ef",
    "path": "papers/22/05/2205.03990.json",
    "total_tokens": 917,
    "translated_title": "多分辨率偏微分方程保留学习框架用于时空动态",
    "translated_abstract": "传统的数据驱动深度学习模型在复杂物理过程中往往面临高训练成本、误差积累和泛化能力差的问题。物理启发式深度学习（PiDL）通过将物理原理融入模型来解决这些挑战。大多数PiDL方法通过将控制方程嵌入损失函数来进行训练的规范化，但这在很大程度上依赖于大量的超参数调整来权衡每个损失项。为此，我们提出利用物理先验知识，通过偏微分方程（PDE）算子和网络结构之间的连接，将离散化的控制方程“烘烤”到神经网络架构中，从而形成一个保留PDE的神经网络（PPNN）。这种方法通过在多分辨率环境中将离散化的PDE嵌入卷积残差网络中，大大提高了泛化能力和长期预测的准确性，优于传统的黑盒模型。",
    "tldr": "本论文提出了一种利用物理先验知识的多分辨率偏微分方程保留学习框架，通过将离散化的控制方程嵌入神经网络架构中，提高了泛化能力和长期预测的准确性。",
    "en_tdlr": "This paper proposes a multi-resolution partial differential equations preserved learning framework that leverages physics prior knowledge to improve the generalizability and long-term prediction accuracy by embedding the discretized governing equations into the neural network architecture"
}