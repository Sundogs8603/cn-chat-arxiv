{
    "title": "Robust Anytime Learning of Markov Decision Processes. (arXiv:2205.15827v4 [cs.AI] UPDATED)",
    "abstract": "Markov decision processes (MDPs) are formal models commonly used in sequential decision-making. MDPs capture the stochasticity that may arise, for instance, from imprecise actuators via probabilities in the transition function. However, in data-driven applications, deriving precise probabilities from (limited) data introduces statistical errors that may lead to unexpected or undesirable outcomes. Uncertain MDPs (uMDPs) do not require precise probabilities but instead use so-called uncertainty sets in the transitions, accounting for such limited data. Tools from the formal verification community efficiently compute robust policies that provably adhere to formal specifications, like safety constraints, under the worst-case instance in the uncertainty set. We continuously learn the transition probabilities of an MDP in a robust anytime-learning approach that combines a dedicated Bayesian inference scheme with the computation of robust policies. In particular, our method (1) approximates p",
    "link": "http://arxiv.org/abs/2205.15827",
    "context": "Title: Robust Anytime Learning of Markov Decision Processes. (arXiv:2205.15827v4 [cs.AI] UPDATED)\nAbstract: Markov decision processes (MDPs) are formal models commonly used in sequential decision-making. MDPs capture the stochasticity that may arise, for instance, from imprecise actuators via probabilities in the transition function. However, in data-driven applications, deriving precise probabilities from (limited) data introduces statistical errors that may lead to unexpected or undesirable outcomes. Uncertain MDPs (uMDPs) do not require precise probabilities but instead use so-called uncertainty sets in the transitions, accounting for such limited data. Tools from the formal verification community efficiently compute robust policies that provably adhere to formal specifications, like safety constraints, under the worst-case instance in the uncertainty set. We continuously learn the transition probabilities of an MDP in a robust anytime-learning approach that combines a dedicated Bayesian inference scheme with the computation of robust policies. In particular, our method (1) approximates p",
    "path": "papers/22/05/2205.15827.json",
    "total_tokens": 906,
    "translated_title": "强健的马尔可夫决策过程即时学习",
    "translated_abstract": "马尔可夫决策过程（MDP）是经常用于顺序决策的形式模型。MDP通过转移函数中的概率捕获可能出现的来自不精确执行器的随机性。然而，在数据驱动的应用中，从（有限的）数据中推导出准确的概率会引入统计误差，可能导致意外或不良结果。不确定MDP（uMDP）不需要准确的概率，而是使用所谓的不确定性集，在转换中考虑这些有限的数据。我们通过组合专门的贝叶斯推理方案和计算强健策略的方法，在一个强健的即时学习方法中不断学习MDP的转移概率。特别地，我们的方法（1）近似p",
    "tldr": "文章介绍了一种将贝叶斯推理方案和计算强健策略相结合的、不断学习马尔可夫决策过程转移概率的方法，阐述了不确定MDP（uMDP）的概念，针对应用中有限数据导致的统计误差问题提出了基于不确定性集的解决方法，并介绍了计算强健策略以遵循形式规范的工具。",
    "en_tdlr": "The article proposes a method that combines Bayesian inference and computation of robust policies to continuously learn Markov decision processes' transition probabilities. The concept of uncertain MDPs (uMDPs) is introduced to address statistical errors that arise from limited data in data-driven applications. Tools for computing robust policies to adhere to formal specifications are also discussed."
}