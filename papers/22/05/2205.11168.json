{
    "title": "Logarithmic regret bounds for continuous-time average-reward Markov decision processes. (arXiv:2205.11168v3 [cs.LG] UPDATED)",
    "abstract": "We consider reinforcement learning for continuous-time Markov decision processes (MDPs) in the infinite-horizon, average-reward setting. In contrast to discrete-time MDPs, a continuous-time process moves to a state and stays there for a random holding time after an action is taken. With unknown transition probabilities and rates of exponential holding times, we derive instance-dependent regret lower bounds that are logarithmic in the time horizon. Moreover, we design a learning algorithm and establish a finite-time regret bound that achieves the logarithmic growth rate. Our analysis builds upon upper confidence reinforcement learning, a delicate estimation of the mean holding times, and stochastic comparison of point processes.",
    "link": "http://arxiv.org/abs/2205.11168",
    "context": "Title: Logarithmic regret bounds for continuous-time average-reward Markov decision processes. (arXiv:2205.11168v3 [cs.LG] UPDATED)\nAbstract: We consider reinforcement learning for continuous-time Markov decision processes (MDPs) in the infinite-horizon, average-reward setting. In contrast to discrete-time MDPs, a continuous-time process moves to a state and stays there for a random holding time after an action is taken. With unknown transition probabilities and rates of exponential holding times, we derive instance-dependent regret lower bounds that are logarithmic in the time horizon. Moreover, we design a learning algorithm and establish a finite-time regret bound that achieves the logarithmic growth rate. Our analysis builds upon upper confidence reinforcement learning, a delicate estimation of the mean holding times, and stochastic comparison of point processes.",
    "path": "papers/22/05/2205.11168.json",
    "total_tokens": 804,
    "translated_title": "连续时间平均奖励马尔可夫决策过程的对数遗憾界限",
    "translated_abstract": "我们考虑了在无限时间跨度、平均奖励设定下的连续时间马尔可夫决策过程（MDPs）的强化学习。与离散时间MDPs不同，连续时间过程在采取行动后会移动到一个状态并在此停留一个随机持续时间。在未知的转移概率和指数持续时间变化率下，我们得到了一个与时间跨度对数相关的实例相关遗憾下界。此外，我们设计了一个学习算法，并建立了一个有限时间的遗憾界限，能够实现对数增长速率。我们的分析建立在上限置信增强学习、均值持续时间的精细估计以及点过程的随机比较之上。",
    "tldr": "这项研究考虑了连续时间马尔可夫决策过程（MDPs）的平均奖励设置下的强化学习问题，并找到了实例相关的对数遗憾下界，并设计出了一个能够实现对数增长速率的学习算法。",
    "en_tdlr": "This research considers reinforcement learning for continuous-time Markov decision processes (MDPs) in the average-reward setting, and finds an instance-dependent logarithmic regret lower bound. It also designs a learning algorithm that achieves logarithmic growth rate."
}