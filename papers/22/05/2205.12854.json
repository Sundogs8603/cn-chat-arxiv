{
    "title": "Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors. (arXiv:2205.12854v2 [cs.CL] UPDATED)",
    "abstract": "The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems' outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant rec",
    "link": "http://arxiv.org/abs/2205.12854",
    "context": "Title: Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors. (arXiv:2205.12854v2 [cs.CL] UPDATED)\nAbstract: The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems' outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant rec",
    "path": "papers/22/05/2205.12854.json",
    "total_tokens": 910,
    "translated_title": "理解总结中的事实错误：错误，摘要生成器，数据集和错误检测器",
    "translated_abstract": "抽象摘要生成模型制造事实错误的倾向已经得到了广泛的研究，包括设计用于检测事实错误的度量标准和注释当前系统输出中的错误。然而，总结系统、度量标准和注释基准的不断发展使得事实评价成为一个移动的目标，并且在度量标准之间进行清晰的比较变得越来越困难。在本文中，我们聚合了来自九个现有数据集的事实错误注释，并根据底层的摘要生成模型进行分类。我们比较了最先进的事实度量标准的性能，包括最近的ChatGPT-based度量标准，在这个分层基准上展示了它们在不同类型的摘要生成模型上的性能差异。关键是，我们的分析显示，最近在事实检测空间的很大改进是针对旧的(前Transformer) 模型的总结，而不是更相关的模型。",
    "tldr": "本文聚合了来自九个现有数据集的事实错误注释，针对底层的摘要生成模型进行分类，并比较了最先进的事实度量标准的性能。结果表明，度量标准的性能因不同的摘要生成模型而有显著差异。",
    "en_tdlr": "This paper aggregates factuality error annotations from nine existing datasets, stratifies them by the underlying summarization model, and compares the performance of state-of-the-art factuality metrics. Results show significant performance differences across different types of summarization models."
}