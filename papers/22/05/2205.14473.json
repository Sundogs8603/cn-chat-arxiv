{
    "title": "Efficient-Adam: Communication-Efficient Distributed Adam. (arXiv:2205.14473v2 [cs.LG] UPDATED)",
    "abstract": "Distributed adaptive stochastic gradient methods have been widely used for large-scale nonconvex optimization, such as training deep learning models. However, their communication complexity on finding $\\varepsilon$-stationary points has rarely been analyzed in the nonconvex setting. In this work, we present a novel communication-efficient distributed Adam in the parameter-server model for stochastic nonconvex optimization, dubbed {\\em Efficient-Adam}. Specifically, we incorporate a two-way quantization scheme into Efficient-Adam to reduce the communication cost between the workers and server. Simultaneously, we adopt a two-way error feedback strategy to reduce the biases caused by the two-way quantization on both the server and workers, respectively. In addition, we establish the iteration complexity for the proposed Efficient-Adam with a class of quantization operators, and further characterize its communication complexity between the server and workers when an $\\varepsilon$-stationar",
    "link": "http://arxiv.org/abs/2205.14473",
    "context": "Title: Efficient-Adam: Communication-Efficient Distributed Adam. (arXiv:2205.14473v2 [cs.LG] UPDATED)\nAbstract: Distributed adaptive stochastic gradient methods have been widely used for large-scale nonconvex optimization, such as training deep learning models. However, their communication complexity on finding $\\varepsilon$-stationary points has rarely been analyzed in the nonconvex setting. In this work, we present a novel communication-efficient distributed Adam in the parameter-server model for stochastic nonconvex optimization, dubbed {\\em Efficient-Adam}. Specifically, we incorporate a two-way quantization scheme into Efficient-Adam to reduce the communication cost between the workers and server. Simultaneously, we adopt a two-way error feedback strategy to reduce the biases caused by the two-way quantization on both the server and workers, respectively. In addition, we establish the iteration complexity for the proposed Efficient-Adam with a class of quantization operators, and further characterize its communication complexity between the server and workers when an $\\varepsilon$-stationar",
    "path": "papers/22/05/2205.14473.json",
    "total_tokens": 873,
    "translated_title": "高效Adam：高效通信的分布式Adam",
    "translated_abstract": "分布式自适应随机梯度方法广泛应用于大规模非凸优化，如深度学习模型训练。然而，在非凸环境中很少有关于找到ε-稳定点的通信复杂性分析。在这项工作中，我们提出了一种新颖的高效通信的分布式Adam，用于随机非凸优化的参数服务器模型，称为\"高效-Adam\"。具体来说，我们将双向量化方案纳入高效-Adam，以减少工作者和服务器之间的通信成本。同时，我们采用双向误差反馈策略，分别减小了双向量化对服务器和工作者的偏差影响。此外，我们还建立了一类量化算子的高效-Adam的迭代复杂度，并进一步刻画了服务器和工作者之间的通信复杂度，当ε-稳定点满足时。",
    "tldr": "这项工作提出了一种名为高效-Adam的通信效率更高的分布式优化方法，在非凸环境下通过双向量化和双向误差反馈策略来降低通信成本，并对其迭代复杂度和通信复杂度进行了分析。",
    "en_tdlr": "This work presents Efficient-Adam, a communication-efficient distributed optimization method, which incorporates two-way quantization and two-way error feedback strategies to reduce communication cost. The proposed method is analyzed in terms of iteration complexity and communication complexity in the nonconvex setting."
}