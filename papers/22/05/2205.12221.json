{
    "title": "ClaimDiff: Comparing and Contrasting Claims on Contentious Issues. (arXiv:2205.12221v2 [cs.CL] UPDATED)",
    "abstract": "With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, one's argument is not necessarily more true than the other, requiring comparison rather than verification. In this study, we propose ClaimDiff, a novel dataset that primarily focuses on comparing the nuance between claim pairs. In ClaimDiff, we provide 2,941 annotated claim pairs from 268 news articles. We observe that while humans are capable of detecting the nuances between claims, strong baselines struggle to detect them, showing over a 19% absolute gap with the humans. We hope this initial study could help readers to gain an unbiased grasp of contentious issues through machine-aided",
    "link": "http://arxiv.org/abs/2205.12221",
    "context": "Title: ClaimDiff: Comparing and Contrasting Claims on Contentious Issues. (arXiv:2205.12221v2 [cs.CL] UPDATED)\nAbstract: With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, one's argument is not necessarily more true than the other, requiring comparison rather than verification. In this study, we propose ClaimDiff, a novel dataset that primarily focuses on comparing the nuance between claim pairs. In ClaimDiff, we provide 2,941 annotated claim pairs from 268 news articles. We observe that while humans are capable of detecting the nuances between claims, strong baselines struggle to detect them, showing over a 19% absolute gap with the humans. We hope this initial study could help readers to gain an unbiased grasp of contentious issues through machine-aided",
    "path": "papers/22/05/2205.12221.json",
    "total_tokens": 926,
    "translated_title": "ClaimDiff：比较和对比有争议问题的声明",
    "translated_abstract": "随着发现虚假信息的重要性日益增加，许多研究都专注于检验事实声明来检索证据。然而，传统事实验证任务并不适用于捕捉事实上一致但可能仍会对读者产生偏见的微妙差异，尤其是在有争议的政治或经济问题上。我们的基本假设是，在受信任的信息源中，一个人的论点并不一定比另一个人更正确，需要进行比较而不是简单的验证。在这项研究中，我们提出了ClaimDiff，这是一个主要关注声明对之间细微差别的新数据集。在ClaimDiff中，我们提供了来自268篇新闻文章的2,941个标注声明对。我们观察到，虽然人类能够检测到声明之间的细微差别，但强大的基准测试无法探测到它们，与人类存在超过19%的绝对差距。我们希望这项初步研究能够通过机器辅助帮助读者获得有争议问题的无偏视角。",
    "tldr": "本研究提出ClaimDiff数据集，主要关注声明之间的微妙差别，并观察到强有力的基准测试无法探测这些差别，与人类存在超过19%的绝对差距。",
    "en_tdlr": "This study proposes the ClaimDiff dataset, which focuses on comparing the nuances between claims on contentious issues. The authors observe that while humans can detect these nuances, strong baselines struggle to do so, with over a 19% absolute gap compared to human performance."
}