{
    "title": "RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states. (arXiv:2205.07229v2 [cs.LG] UPDATED)",
    "abstract": "Multi-agent deep reinforcement learning makes optimal decisions dependent on system states observed by agents, but any uncertainty on the observations may mislead agents to take wrong actions. The Mean-Field Actor-Critic reinforcement learning (MFAC) is well-known in the multi-agent field since it can effectively handle a scalability problem. However, it is sensitive to state perturbations that can significantly degrade the team rewards. This work proposes a Robust Mean-field Actor-Critic reinforcement learning (RoMFAC) that has two innovations: 1) a new objective function of training actors, composed of a \\emph{policy gradient function} that is related to the expected cumulative discount reward on sampled clean states and an \\emph{action loss function} that represents the difference between actions taken on clean and adversarial states; and 2) a repetitive regularization of the action loss, ensuring the trained actors to obtain excellent performance. Furthermore, this work proposes a ",
    "link": "http://arxiv.org/abs/2205.07229",
    "context": "Title: RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states. (arXiv:2205.07229v2 [cs.LG] UPDATED)\nAbstract: Multi-agent deep reinforcement learning makes optimal decisions dependent on system states observed by agents, but any uncertainty on the observations may mislead agents to take wrong actions. The Mean-Field Actor-Critic reinforcement learning (MFAC) is well-known in the multi-agent field since it can effectively handle a scalability problem. However, it is sensitive to state perturbations that can significantly degrade the team rewards. This work proposes a Robust Mean-field Actor-Critic reinforcement learning (RoMFAC) that has two innovations: 1) a new objective function of training actors, composed of a \\emph{policy gradient function} that is related to the expected cumulative discount reward on sampled clean states and an \\emph{action loss function} that represents the difference between actions taken on clean and adversarial states; and 2) a repetitive regularization of the action loss, ensuring the trained actors to obtain excellent performance. Furthermore, this work proposes a ",
    "path": "papers/22/05/2205.07229.json",
    "total_tokens": 770,
    "translated_title": "RoMFAC: 一种对于状态异常干扰具有鲁棒性的均场演员-评论家强化学习算法",
    "translated_abstract": "多智能体强化学习需要基于观察到的系统状态做出最优决策，但是观察中的不确定性可能会导致智能体做出错误的行动。本文提出一种新的算法RoMFAC，它通过一个策略梯度函数和一个代表状态干扰影响的行动损失函数训练演员。同时，RoMFAC还引入一个重复的正则化行动损失函数的方法，以确保训练演员具有出色的性能表现。",
    "tldr": "本文提出了RoMFAC算法，通过新的训练目标和重复的正则化损失函数，使其对于异常状态干扰具有鲁棒性并获得出色性能表现。",
    "en_tdlr": "This paper proposes a robust mean-field actor-critic reinforcement learning algorithm, RoMFAC, which introduces a new training objective and repetitive regularization of action loss to handle adversarial perturbations on states and obtain excellent performance."
}