{
    "title": "Complex behavior from intrinsic motivation to occupy action-state path space",
    "abstract": "arXiv:2205.10316v2 Announce Type: replace  Abstract: Most theories of behavior posit that agents tend to maximize some form of reward or utility. However, animals very often move with curiosity and seem to be motivated in a reward-free manner. Here we abandon the idea of reward maximization, and propose that the goal of behavior is maximizing occupancy of future paths of actions and states. According to this maximum occupancy principle, rewards are the means to occupy path space, not the goal per se; goal-directedness simply emerges as rational ways of searching for resources so that movement, understood amply, never ends. We find that action-state path entropy is the only measure consistent with additivity and other intuitive properties of expected future action-state path occupancy. We provide analytical expressions that relate the optimal policy and state-value function, and prove convergence of our value iteration algorithm. Using discrete and continuous state tasks, including a hi",
    "link": "https://arxiv.org/abs/2205.10316",
    "context": "Title: Complex behavior from intrinsic motivation to occupy action-state path space\nAbstract: arXiv:2205.10316v2 Announce Type: replace  Abstract: Most theories of behavior posit that agents tend to maximize some form of reward or utility. However, animals very often move with curiosity and seem to be motivated in a reward-free manner. Here we abandon the idea of reward maximization, and propose that the goal of behavior is maximizing occupancy of future paths of actions and states. According to this maximum occupancy principle, rewards are the means to occupy path space, not the goal per se; goal-directedness simply emerges as rational ways of searching for resources so that movement, understood amply, never ends. We find that action-state path entropy is the only measure consistent with additivity and other intuitive properties of expected future action-state path occupancy. We provide analytical expressions that relate the optimal policy and state-value function, and prove convergence of our value iteration algorithm. Using discrete and continuous state tasks, including a hi",
    "path": "papers/22/05/2205.10316.json",
    "total_tokens": 900,
    "translated_title": "从内在动机到占据行动-状态路径空间的复杂行为",
    "translated_abstract": "大多数行为理论认为，代理人倾向于最大化某种形式的奖励或效用。然而，动物经常出于好奇心移动，并且似乎在没有奖励的情况下受到激励。在这里，我们放弃了奖励最大化的概念，提出行为的目标是最大化未来行动和状态路径的占用。根据最大占用原则，奖励是占用路径空间的手段，而不是目标本身；目标导向性简单地作为搜索资源的理性方式而出现，以使运动从广义上理解永不结束。我们发现，行动-状态路径熵是唯一与可加性和其他直观的预期未来行动-状态路径占用性质一致的度量。我们提供了关于最优策略和状态值函数的解析表达式，并证明了我们的值迭代算法的收敛性。使用离散和连续状态任务，包括一个高",
    "tldr": "行为的目标是最大化未来行动和状态路径的占用，根据最大占用原则，奖励是占用路径空间的手段，而不是目标本身，并提供了与最优策略和状态值函数相关的解析表达式，证明了值迭代算法的收敛性",
    "en_tdlr": "The goal of behavior is to maximize the occupancy of future paths of actions and states, with rewards being the means to occupy path space rather than the goal itself. The paper provides analytical expressions related to the optimal policy and state-value function, and proves convergence of the value iteration algorithm."
}