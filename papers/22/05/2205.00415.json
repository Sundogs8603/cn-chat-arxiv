{
    "title": "Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions",
    "abstract": "arXiv:2205.00415v3 Announce Type: replace  Abstract: In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator's instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize bey",
    "link": "https://arxiv.org/abs/2205.00415",
    "context": "Title: Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions\nAbstract: arXiv:2205.00415v3 Announce Type: replace  Abstract: In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator's instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize bey",
    "path": "papers/22/05/2205.00415.json",
    "total_tokens": 841,
    "translated_title": "不要责怪注释人员：偏见已经开始于注释指令",
    "translated_abstract": "近年来，自然语言理解（NLU）的进展主要是由基准驱动的。这些基准通常通过众包收集，注释人员根据数据集创建者制定的注释指令编写示例。在这项工作中，我们假设注释人员会注意到众包指令中的模式，使他们写出许多相似的示例，随后这些示例在收集的数据中被过度呈现。我们研究了这种偏见形式，称之为指令偏见，在最近的14个NLU基准中展示了指令示例通常表现出具体模式，这些模式被工人群体传播到收集的数据中。这扩展了先前的工作（Geva等，2019年），提出了一个新的关注点，即我们是否在模拟数据集创建者的指令，而不是任务本身。通过一系列实验，我们展示了指令偏见确实可能导致对模型性能的过高估计，并且模型难以泛化。",
    "tldr": "基准数据集中存在的指令偏见可能导致对模型性能的高估，模型泛化能力受到影响。",
    "en_tdlr": "Instruction bias in benchmark datasets may lead to overestimation of model performance and hinder model generalization."
}