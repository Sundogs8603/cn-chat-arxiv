{
    "title": "Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v5 [cs.CL] UPDATED)",
    "abstract": "Multimodal Knowledge Graphs (MKGs), which organize visual-text factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware f",
    "link": "http://arxiv.org/abs/2205.02357",
    "context": "Title: Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v5 [cs.CL] UPDATED)\nAbstract: Multimodal Knowledge Graphs (MKGs), which organize visual-text factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware f",
    "path": "papers/22/05/2205.02357.json",
    "total_tokens": 829,
    "translated_title": "混合Transformer与多级融合用于多模态知识图谱补全",
    "translated_abstract": "最近，多模态知识图谱（MKG）在信息检索、问答和推荐系统等任务中取得了成功，MKG组织了视觉-文本事实知识。然而，由于大多数MKG都不完整，因此提出了广泛的知识图谱补全研究，重点关注多模态实体、关系提取和链接预测。本文针对这些问题提出了一种混合Transformer与多级融合的方法。具体来说，我们利用一种混合Transformer架构和统一的输入-输出来完成多样的多模态知识图谱补全任务。此外，我们提出了多级融合，通过粗粒度前缀引导交互和细粒度相关感知将视觉和文本表示集成起来。",
    "tldr": "本文提出了一种混合Transformer与多级融合的方法，用于解决多模态知识图谱补全的问题。该方法通过统一的输入-输出架构适用于多样的任务，同时利用多级融合将视觉和文本表示集成起来。"
}