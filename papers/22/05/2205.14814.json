{
    "title": "Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding. (arXiv:2205.14814v2 [cs.LG] UPDATED)",
    "abstract": "Contrastive learning, especially self-supervised contrastive learning (SSCL), has achieved great success in extracting powerful features from unlabeled data. In this work, we contribute to the theoretical understanding of SSCL and uncover its connection to the classic data visualization method, stochastic neighbor embedding (SNE), whose goal is to preserve pairwise distances. From the perspective of preserving neighboring information, SSCL can be viewed as a special case of SNE with the input space pairwise similarities specified by data augmentation. The established correspondence facilitates deeper theoretical understanding of learned features of SSCL, as well as methodological guidelines for practical improvement. Specifically, through the lens of SNE, we provide novel analysis on domain-agnostic augmentations, implicit bias and robustness of learned features. To illustrate the practical advantage, we demonstrate that the modifications from SNE to $t$-SNE can also be adopted in the ",
    "link": "http://arxiv.org/abs/2205.14814",
    "context": "Title: Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding. (arXiv:2205.14814v2 [cs.LG] UPDATED)\nAbstract: Contrastive learning, especially self-supervised contrastive learning (SSCL), has achieved great success in extracting powerful features from unlabeled data. In this work, we contribute to the theoretical understanding of SSCL and uncover its connection to the classic data visualization method, stochastic neighbor embedding (SNE), whose goal is to preserve pairwise distances. From the perspective of preserving neighboring information, SSCL can be viewed as a special case of SNE with the input space pairwise similarities specified by data augmentation. The established correspondence facilitates deeper theoretical understanding of learned features of SSCL, as well as methodological guidelines for practical improvement. Specifically, through the lens of SNE, we provide novel analysis on domain-agnostic augmentations, implicit bias and robustness of learned features. To illustrate the practical advantage, we demonstrate that the modifications from SNE to $t$-SNE can also be adopted in the ",
    "path": "papers/22/05/2205.14814.json",
    "total_tokens": 1123,
    "translated_title": "您的对比学习其实是在做随机邻居嵌入",
    "translated_abstract": "自监督对比学习（SSCL）在从非标记数据中提取有效特征方面取得了巨大成功。本文揭示了SSCL和随机邻居嵌入（SNE）之间的联系，即从保留邻近信息的角度来看，SSCL可以被视为SNE的一种特殊情况，其输入空间的成对相似性由数据增强指定。通过SNE的视角，对领域无关增广、隐式偏差和学到特征的鲁棒性等问题进行了新的分析，并提出了实用指南。实验表明，从SNE到t-SNE的修改对SSCL具有积极作用。",
    "tldr": "本文揭示了自监督对比学习（SSCL）与随机邻居嵌入（SNE）的联系，SSCL实际上是SNE的一种特殊情况，通过SNE的视角，本文提供了关于领域无关增广、隐式偏差和学到特征的鲁棒性等问题的新分析和实用指南，该方法可以优化SSCL的特征提取。"
}