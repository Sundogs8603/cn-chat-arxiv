{
    "title": "Trainable Weight Averaging: A General Approach for Subspace Training. (arXiv:2205.13104v2 [cs.LG] UPDATED)",
    "abstract": "Training deep neural networks (DNNs) in low-dimensional subspaces is a promising direction for achieving efficient training and better generalization performance. Previous works extract the subspaces by using random projection or performing dimensionality reduction method on the training trajectory, but these methods can be inefficient or unstable in terms of dimensionality and numerical operations. In this paper, we connect subspace training to weight averaging and propose Trainable Weight Averaging (TWA), a general approach for subspace training that generalizes the previous efforts. TWA is efficient in terms of dimensionality and also easy to use, making it a promising new method for subspace training. We further design an efficient scheme for subspace training to cope with large-scale problems, which allows parallel training across multiple nodes and evenly distributing the memory and computation burden to each node. We apply TWA to efficient neural network training and improving f",
    "link": "http://arxiv.org/abs/2205.13104",
    "context": "Title: Trainable Weight Averaging: A General Approach for Subspace Training. (arXiv:2205.13104v2 [cs.LG] UPDATED)\nAbstract: Training deep neural networks (DNNs) in low-dimensional subspaces is a promising direction for achieving efficient training and better generalization performance. Previous works extract the subspaces by using random projection or performing dimensionality reduction method on the training trajectory, but these methods can be inefficient or unstable in terms of dimensionality and numerical operations. In this paper, we connect subspace training to weight averaging and propose Trainable Weight Averaging (TWA), a general approach for subspace training that generalizes the previous efforts. TWA is efficient in terms of dimensionality and also easy to use, making it a promising new method for subspace training. We further design an efficient scheme for subspace training to cope with large-scale problems, which allows parallel training across multiple nodes and evenly distributing the memory and computation burden to each node. We apply TWA to efficient neural network training and improving f",
    "path": "papers/22/05/2205.13104.json",
    "total_tokens": 918,
    "translated_title": "可训练的权重平均值：子空间训练的一般方法",
    "translated_abstract": "在低维子空间中训练深度神经网络(DNNs)是实现高效训练和更好的泛化性能的一个有前景的方向。以往的工作通过使用随机投影或在训练轨迹上执行降维方法来提取子空间，但这些方法在维度和数值运算方面可能效率低下或不稳定。在本文中，我们将子空间训练与权重平均值联系起来，并提出了可训练权重平均值(TWA)，这是一种泛化以前努力的子空间训练的一般方法。TWA在维度方面具有高效性，并且易于使用，使其成为一种有前景的子空间训练新方法。我们进一步设计了一个有效的方案来应对大规模问题的子空间训练，它允许多个节点上的并行训练，并将内存和计算负担均匀分配给每个节点。我们将TWA应用于高效的神经网络训练和改进",
    "tldr": "可训练的权重平均值是一种通用的子空间训练方法，通过连接子空间训练和权重平均值，提供高效的训练和易于使用的方法。这种方法可以用于改进神经网络训练效果和降低计算负担。",
    "en_tdlr": "Trainable Weight Averaging is a general approach for subspace training that connects weight averaging and subspace training, providing efficient training and ease of use. This method can be used to improve neural network training and reduce computational burden."
}