{
    "title": "Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey. (arXiv:2205.08099v2 [cs.LG] UPDATED)",
    "abstract": "State-of-the-art deep learning models have a parameter count that reaches into the billions. Training, storing and transferring such models is energy and time consuming, thus costly. A big part of these costs is caused by training the network. Model compression lowers storage and transfer costs, and can further make training more efficient by decreasing the number of computations in the forward and/or backward pass. Thus, compressing networks also at training time while maintaining a high performance is an important research topic. This work is a survey on methods which reduce the number of trained weights in deep learning models throughout the training. Most of the introduced methods set network parameters to zero which is called pruning. The presented pruning approaches are categorized into pruning at initialization, lottery tickets and dynamic sparse training. Moreover, we discuss methods that freeze parts of a network at its random initialization. By freezing weights, the number of",
    "link": "http://arxiv.org/abs/2205.08099",
    "context": "Title: Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey. (arXiv:2205.08099v2 [cs.LG] UPDATED)\nAbstract: State-of-the-art deep learning models have a parameter count that reaches into the billions. Training, storing and transferring such models is energy and time consuming, thus costly. A big part of these costs is caused by training the network. Model compression lowers storage and transfer costs, and can further make training more efficient by decreasing the number of computations in the forward and/or backward pass. Thus, compressing networks also at training time while maintaining a high performance is an important research topic. This work is a survey on methods which reduce the number of trained weights in deep learning models throughout the training. Most of the introduced methods set network parameters to zero which is called pruning. The presented pruning approaches are categorized into pruning at initialization, lottery tickets and dynamic sparse training. Moreover, we discuss methods that freeze parts of a network at its random initialization. By freezing weights, the number of",
    "path": "papers/22/05/2205.08099.json",
    "total_tokens": 1127,
    "translated_title": "剪枝和冻结深度神经网络的一部分以进行降维优化的训练方法: 一项调查研究",
    "translated_abstract": "最先进的深度学习模型的参数计数达到了千亿级别。训练、存储和传输这样的模型是耗费能量和时间的，因此代价很高。其中很大一部分成本是由网络的训练引起的。模型压缩可以降低存储和传输成本，并通过减少前向或后向传递中的计算数量，进一步使训练更有效。因此，在训练时压缩网络并保持高性能是一个重要的研究课题。本文是关于如何在深度学习模型的训练过程中减少已训练权重数量的方法的调查研究。介绍的大多数方法将网络参数设置为零，这被称为剪枝。所介绍的剪枝方法被归类为初始化时的剪枝、奖励彩票和动态稀疏训练。此外，我们还讨论了在随机初始化时冻结网络一部分的方法。通过冻结权重，也可以减少已训练的权重数量。在本次调研中，我们提供了深度神经网络降维优化训练中剪枝和冻结网络参数的当前最先进技术的概述。",
    "tldr": "本文是一项关于在深度学习模型训练过程中通过剪枝和冻结网络参数减少已训练权重数量的调查研究。剪枝方法可以分为初始化时的剪枝、奖励彩票和动态稀疏训练，而冻结权重同样能够减少已训练的权重数量，这些技术可以在降低存储和传输成本的同时提高训练效率。",
    "en_tdlr": "This survey covers methods for reducing the number of trained weights in deep learning models through pruning and freezing network parameters during training. The introduced pruning methods are categorized into pruning at initialization, lottery tickets, and dynamic sparse training. Additionally, methods for freezing parts of a network at its random initialization are discussed. These techniques can improve training efficiency while reducing storage and transfer costs."
}