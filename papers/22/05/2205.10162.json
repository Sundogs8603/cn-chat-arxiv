{
    "title": "FedAdapter: Efficient Federated Learning for Modern NLP. (arXiv:2205.10162v2 [cs.LG] UPDATED)",
    "abstract": "Transformer-based pre-trained models have revolutionized NLP for superior performance and generality. Fine-tuning pre-trained models for downstream tasks often requires private data, for which federated learning is the de-facto approach (i.e., FedNLP). However, our measurements show that FedNLP is prohibitively slow due to the large model sizes and the resultant high network/computation cost. Towards practical FedNLP, we identify as the key building blocks adapters, small bottleneck modules inserted at a variety of model layers. A key challenge is to properly configure the depth and width of adapters, to which the training speed and efficiency is highly sensitive. No silver-bullet configuration exists: the optimal choice varies across downstream NLP tasks, desired model accuracy, and mobile resources. To automate adapter configuration, we propose FedAdapter, a framework that enhances the existing FedNLP with two novel designs. First, FedAdapter progressively upgrades the adapter config",
    "link": "http://arxiv.org/abs/2205.10162",
    "context": "Title: FedAdapter: Efficient Federated Learning for Modern NLP. (arXiv:2205.10162v2 [cs.LG] UPDATED)\nAbstract: Transformer-based pre-trained models have revolutionized NLP for superior performance and generality. Fine-tuning pre-trained models for downstream tasks often requires private data, for which federated learning is the de-facto approach (i.e., FedNLP). However, our measurements show that FedNLP is prohibitively slow due to the large model sizes and the resultant high network/computation cost. Towards practical FedNLP, we identify as the key building blocks adapters, small bottleneck modules inserted at a variety of model layers. A key challenge is to properly configure the depth and width of adapters, to which the training speed and efficiency is highly sensitive. No silver-bullet configuration exists: the optimal choice varies across downstream NLP tasks, desired model accuracy, and mobile resources. To automate adapter configuration, we propose FedAdapter, a framework that enhances the existing FedNLP with two novel designs. First, FedAdapter progressively upgrades the adapter config",
    "path": "papers/22/05/2205.10162.json",
    "total_tokens": 851,
    "translated_title": "FedAdapter: 面向现代 NLP 的高效联邦学习",
    "translated_abstract": "基于 Transformer 预训练模型的出现，为 NLP 带来了卓越的性能和通用性。但是，微调预训练模型对下游任务的需要私有数据，而联邦学习是解决这一问题的黄金方法（即FedNLP）。然而，我们的测量表明，由于大型模型的存在以及相应的高网络/计算成本，FedNLP无法进行。为了实现实用的FedNLP，我们确定了适配器作为关键构建块，这是一种插入各种模型层的小瓶颈模块。关键挑战是正确配置适配器的深度和宽度，这对训练速度和效率非常敏感。并不存在适用于所有情况的最佳配置：最佳选择因下游NLP任务、所需模型精度和移动资源而异。为了自动化适配器配置，我们提出了 FedAdapter，这是一个增强现有 FedNLP 的框架，具有两个新颖的设计。",
    "tldr": "FedAdapter是一个框架，可以自动化适配器配置以提高联邦学习的效率，有助于面向现代NLP的高效训练。",
    "en_tdlr": "FedAdapter is a framework that automates adapter configuration to improve the efficiency of federated learning, which helps efficient training for modern NLP."
}