{
    "title": "Learning from time-dependent streaming data with online stochastic algorithms. (arXiv:2205.12549v2 [cs.LG] UPDATED)",
    "abstract": "This paper addresses stochastic optimization in a streaming setting with time-dependent and biased gradient estimates. We analyze several first-order methods, including Stochastic Gradient Descent (SGD), mini-batch SGD, and time-varying mini-batch SGD, along with their Polyak-Ruppert averages. Our non-asymptotic analysis establishes novel heuristics that link dependence, biases, and convexity levels, enabling accelerated convergence. Specifically, our findings demonstrate that (i) time-varying mini-batch SGD methods have the capability to break long- and short-range dependence structures, (ii) biased SGD methods can achieve comparable performance to their unbiased counterparts, and (iii) incorporating Polyak-Ruppert averaging can accelerate the convergence of the stochastic optimization algorithms. To validate our theoretical findings, we conduct a series of experiments using both simulated and real-life time-dependent data.",
    "link": "http://arxiv.org/abs/2205.12549",
    "context": "Title: Learning from time-dependent streaming data with online stochastic algorithms. (arXiv:2205.12549v2 [cs.LG] UPDATED)\nAbstract: This paper addresses stochastic optimization in a streaming setting with time-dependent and biased gradient estimates. We analyze several first-order methods, including Stochastic Gradient Descent (SGD), mini-batch SGD, and time-varying mini-batch SGD, along with their Polyak-Ruppert averages. Our non-asymptotic analysis establishes novel heuristics that link dependence, biases, and convexity levels, enabling accelerated convergence. Specifically, our findings demonstrate that (i) time-varying mini-batch SGD methods have the capability to break long- and short-range dependence structures, (ii) biased SGD methods can achieve comparable performance to their unbiased counterparts, and (iii) incorporating Polyak-Ruppert averaging can accelerate the convergence of the stochastic optimization algorithms. To validate our theoretical findings, we conduct a series of experiments using both simulated and real-life time-dependent data.",
    "path": "papers/22/05/2205.12549.json",
    "total_tokens": 949,
    "translated_title": "学习处理时间相关的流式数据的在线随机算法",
    "translated_abstract": "本文探讨了在时间相关且有偏倚梯度估计下的流式优化问题。我们分析了一些一阶方法，包括随机梯度下降（SGD）、小批量SGD和时间变化的小批量SGD，以及它们的Polyak-Ruppert平均值。我们的非渐进分析建立了新颖的启发式算法，将依赖性、偏倚和凸性水平联系起来，实现了加速收敛。具体来说，我们的研究结果表明：（i）时间变化的小批量SGD方法能够打破长期和短期的依赖结构；（ii）有偏倚的SGD方法可以达到与无偏倚方法相当的性能；（iii）使用Polyak-Ruppert平均化方法可以加速随机优化算法的收敛。为了验证我们的理论发现，我们在模拟和现实的时间相关数据上进行了一系列实验。",
    "tldr": "本文研究了处理时间相关的流式数据的在线随机算法，并通过非渐进分析建立了新颖的启发式算法，加速收敛。实验证明时间变化的小批量SGD方法可以打破依赖结构，有偏倚的SGD方法具有与无偏倚方法相当的性能，并且使用Polyak-Ruppert平均化方法能够加快随机优化算法的收敛。"
}