{
    "title": "Fine-grained Image Captioning with CLIP Reward. (arXiv:2205.13115v2 [cs.CL] UPDATED)",
    "abstract": "Modern image captioning models are usually trained with text similarity objectives. However, since reference captions in public datasets often describe the most salient common objects, models trained with text similarity objectives tend to ignore specific and detailed aspects of an image that distinguish it from others. Toward more descriptive and distinctive caption generation, we propose using CLIP, a multimodal encoder trained on huge image-text pairs from web, to calculate multimodal similarity and use it as a reward function. We also propose a simple finetuning strategy of the CLIP text encoder to improve grammar that does not require extra text annotation. This completely eliminates the need for reference captions during the reward computation. To comprehensively evaluate descriptive captions, we introduce FineCapEval, a new dataset for caption evaluation with fine-grained criteria: overall, background, object, relations. In our experiments on text-to-image retrieval and FineCapE",
    "link": "http://arxiv.org/abs/2205.13115",
    "context": "Title: Fine-grained Image Captioning with CLIP Reward. (arXiv:2205.13115v2 [cs.CL] UPDATED)\nAbstract: Modern image captioning models are usually trained with text similarity objectives. However, since reference captions in public datasets often describe the most salient common objects, models trained with text similarity objectives tend to ignore specific and detailed aspects of an image that distinguish it from others. Toward more descriptive and distinctive caption generation, we propose using CLIP, a multimodal encoder trained on huge image-text pairs from web, to calculate multimodal similarity and use it as a reward function. We also propose a simple finetuning strategy of the CLIP text encoder to improve grammar that does not require extra text annotation. This completely eliminates the need for reference captions during the reward computation. To comprehensively evaluate descriptive captions, we introduce FineCapEval, a new dataset for caption evaluation with fine-grained criteria: overall, background, object, relations. In our experiments on text-to-image retrieval and FineCapE",
    "path": "papers/22/05/2205.13115.json",
    "total_tokens": 929,
    "translated_title": "带有CLIP奖励的细粒度图像描述",
    "translated_abstract": "现代图像描述模型通常使用文本相似性目标进行训练。然而，由于公共数据集中的参考描述通常描述最显著的共同对象，使用文本相似性目标进行训练的模型往往忽略了区分图像与其他图像的特定和详细方面。为了更详细和独特地生成标题，我们建议使用CLIP作为奖励函数，计算从网络中巨大的图像-文本对中训练的多模式编码器的多模式相似性。我们还提出了一个简单的fine-tuning策略，改进了CLIP文本编码器的语法，不需要额外的文本注释。这完全消除了在奖励计算期间参考标题的需要。为了全面评估描述性标题，我们引入了FineCapEval，这是一个具有细粒度标准（整体、背景、对象、关系）的新标注数据集。在我们的文本-图像检索和FineCapEval实验中，我们展示了我们的方法在客观指标和人类评估方面都优于最先进的模型。",
    "tldr": "本研究提出使用CLIP作为奖励函数, 生成更细致、独特的图像标题。通过FineCapEval测试，该方法在客观指标和人类评估方面均优于最先进的模型。",
    "en_tdlr": "This study proposes using CLIP as a reward function to generate more detailed and unique image captions. Through the FineCapEval test, the method outperforms state-of-the-art models in both objective metrics and human evaluations."
}