{
    "title": "Visual Spatial Reasoning. (arXiv:2205.00363v3 [cs.CL] UPDATED)",
    "abstract": "Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (such as: under, in front of, and facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: the human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs' by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.",
    "link": "http://arxiv.org/abs/2205.00363",
    "context": "Title: Visual Spatial Reasoning. (arXiv:2205.00363v3 [cs.CL] UPDATED)\nAbstract: Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (such as: under, in front of, and facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: the human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs' by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.",
    "path": "papers/22/05/2205.00363.json",
    "total_tokens": 911,
    "translated_title": "视觉空间推理",
    "translated_abstract": "空间关系是人类认知的基本组成部分。然而，它们以各种方式用自然语言表达，先前的研究表明，目前的视觉和语言模型（VLMs）难以捕捉关系信息。在本文中，我们提出了一种名为Visual Spatial Reasoning（VSR）的数据集，其中包含超过10k个自然文本-图像配对，包括66种英语的空间关系（如：在下面，在前面和面对）。虽然使用了看似简单的注释格式，我们展示了数据集包括具有挑战性的语言现象，例如不同的参考框架。我们展示了人类和模型表现之间的巨大差距：人类准确率高达95%以上，而最先进的模型仅能达到70%左右。我们注意到VLM按关系表现的能力与训练示例数量几乎没有相关性，并且测试的模型通常无法识别涉及对象方向的关系。",
    "tldr": "本文介绍了一种名为Visual Spatial Reasoning（VSR）的数据集，其中包含超过10k个自然文本-图像配对，用于推理包括66种空间关系，研究发现目前的视觉和语言模型（VLMs）难以捕捉关系信息和较少关注物体的方向关系。",
    "en_tdlr": "This paper presents a Visual Spatial Reasoning (VSR) dataset with over 10k natural text-image pairs and 66 types of spatial relations, showing that current vision-and-language models struggle to capture relational information, especially regarding object orientations. Furthermore, there is a significant performance discrepancy between human and model abilities."
}