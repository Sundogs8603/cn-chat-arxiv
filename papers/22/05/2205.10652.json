{
    "title": "Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?. (arXiv:2205.10652v3 [cs.AI] UPDATED)",
    "abstract": "Knowledge graphs (KGs) facilitate a wide variety of applications. Despite great efforts in creation and maintenance, even the largest KGs are far from complete. Hence, KG completion (KGC) has become one of the most crucial tasks for KG research. Recently, considerable literature in this space has centered around the use of Message Passing (Graph) Neural Networks (MPNNs), to learn powerful embeddings. The success of these methods is naturally attributed to the use of MPNNs over simpler multi-layer perceptron (MLP) models, given their additional message passing (MP) component. In this work, we find that surprisingly, simple MLP models are able to achieve comparable performance to MPNNs, suggesting that MP may not be as crucial as previously believed. With further exploration, we show careful scoring function and loss function design has a much stronger influence on KGC model performance. This suggests a conflation of scoring function design, loss function design, and MP in prior work, wi",
    "link": "http://arxiv.org/abs/2205.10652",
    "context": "Title: Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?. (arXiv:2205.10652v3 [cs.AI] UPDATED)\nAbstract: Knowledge graphs (KGs) facilitate a wide variety of applications. Despite great efforts in creation and maintenance, even the largest KGs are far from complete. Hence, KG completion (KGC) has become one of the most crucial tasks for KG research. Recently, considerable literature in this space has centered around the use of Message Passing (Graph) Neural Networks (MPNNs), to learn powerful embeddings. The success of these methods is naturally attributed to the use of MPNNs over simpler multi-layer perceptron (MLP) models, given their additional message passing (MP) component. In this work, we find that surprisingly, simple MLP models are able to achieve comparable performance to MPNNs, suggesting that MP may not be as crucial as previously believed. With further exploration, we show careful scoring function and loss function design has a much stronger influence on KGC model performance. This suggests a conflation of scoring function design, loss function design, and MP in prior work, wi",
    "path": "papers/22/05/2205.10652.json",
    "total_tokens": 988,
    "translated_title": "消息传递神经网络对于知识图谱补全真的有帮助吗？",
    "translated_abstract": "知识图谱（KG）在各种应用中发挥了重要作用。尽管在创建和维护方面做出了巨大努力，但即使是最大的KG也远未完备。因此，知识图谱补全（KGC）已成为KG研究中最关键的任务之一。最近，这一领域的大量文献都集中在使用消息传递（图）神经网络（MPNNs）来学习强大的嵌入表示。这些方法的成功自然归因于相比于简单的多层感知器（MLP）模型，使用了额外的消息传递（MP）组件的MPNNs。在这项工作中，我们发现令人惊讶的是，简单的MLP模型能够达到与MPNNs相当的性能，这表明MP可能并不像之前认为的那样关键。通过进一步探索，我们展示了仔细的评分函数和损失函数设计对于KGC模型性能有更强的影响。这表明以前的工作中对评分函数设计、损失函数设计和MP的混淆。",
    "tldr": "这项研究发现简单的多层感知器（MLP）模型在知识图谱补全任务上能够与消息传递神经网络（MPNNs）相媲美，暗示消息传递可能不像之前认为的那样关键。评分函数和损失函数设计对于模型性能有更大影响。",
    "en_tdlr": "This study found that simple Multi-Layer Perceptron (MLP) models can achieve comparable performance to Message Passing Neural Networks (MPNNs) in knowledge graph completion tasks, suggesting that message passing may not be as crucial as previously believed. The design of scoring functions and loss functions has a greater influence on model performance."
}