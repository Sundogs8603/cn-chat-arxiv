{
    "title": "Learning in Mean Field Games: A Survey",
    "abstract": "arXiv:2205.12944v3 Announce Type: replace-cross  Abstract: Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced by Lasry and Lions, and Huang, Caines and Malham\\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems at scale. The combination of RL and MFGs is promising to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn equilibria and social optima in MFGs. We first identify the most common settings (static, stationary, and evolutive) of M",
    "link": "https://arxiv.org/abs/2205.12944",
    "context": "Title: Learning in Mean Field Games: A Survey\nAbstract: arXiv:2205.12944v3 Announce Type: replace-cross  Abstract: Non-cooperative and cooperative games with a very large number of players have many applications but remain generally intractable when the number of players increases. Introduced by Lasry and Lions, and Huang, Caines and Malham\\'e, Mean Field Games (MFGs) rely on a mean-field approximation to allow the number of players to grow to infinity. Traditional methods for solving these games generally rely on solving partial or stochastic differential equations with a full knowledge of the model. Recently, Reinforcement Learning (RL) has appeared promising to solve complex problems at scale. The combination of RL and MFGs is promising to solve games at a very large scale both in terms of population size and environment complexity. In this survey, we review the quickly growing recent literature on RL methods to learn equilibria and social optima in MFGs. We first identify the most common settings (static, stationary, and evolutive) of M",
    "path": "papers/22/05/2205.12944.json",
    "total_tokens": 859,
    "translated_title": "在均场博弈中的学习：一项调查",
    "translated_abstract": "非合作和合作游戏在拥有大量玩家时有许多应用，但随着玩家数量的增加，通常变得难以解决。均场博弈(Mean Field Games, MFGs)由Lasry和Lions以及Huang，Caines和Malham\\'e引入，依靠均场近似允许玩家数量增长到无穷大。传统解决这些游戏的方法通常依赖于解决带有对模型的完全了解的偏微分方程或随机微分方程。最近，强化学习(Reinforcement Learning, RL)出现在解决规模复杂问题上表现出了很大的潜力。RL和MFGs的结合有望解决在人口规模和环境复杂性方面非常庞大的游戏。在这项调查中，我们回顾了最近迅速增长的关于RL方法在MFGs中学习均衡和社交最优的文献。我们首先确定了M中最常见的设置(静态、稳态和进化的)。",
    "tldr": "强化学习和均场博弈的结合有望在很大规模上解决游戏的均衡和社会最优问题。",
    "en_tdlr": "The combination of reinforcement learning and mean field games shows great potential in solving equilibria and social optima problems at a very large scale."
}