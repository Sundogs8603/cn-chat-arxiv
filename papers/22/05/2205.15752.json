{
    "title": "Hierarchies of Reward Machines. (arXiv:2205.15752v2 [cs.LG] UPDATED)",
    "abstract": "Reward machines (RMs) are a recent formalism for representing the reward function of a reinforcement learning task through a finite-state machine whose edges encode subgoals of the task using high-level events. The structure of RMs enables the decomposition of a task into simpler and independently solvable subtasks that help tackle long-horizon and/or sparse reward tasks. We propose a formalism for further abstracting the subtask structure by endowing an RM with the ability to call other RMs, thus composing a hierarchy of RMs (HRM). We exploit HRMs by treating each call to an RM as an independently solvable subtask using the options framework, and describe a curriculum-based method to learn HRMs from traces observed by the agent. Our experiments reveal that exploiting a handcrafted HRM leads to faster convergence than with a flat HRM, and that learning an HRM is feasible in cases where its equivalent flat representation is not.",
    "link": "http://arxiv.org/abs/2205.15752",
    "context": "Title: Hierarchies of Reward Machines. (arXiv:2205.15752v2 [cs.LG] UPDATED)\nAbstract: Reward machines (RMs) are a recent formalism for representing the reward function of a reinforcement learning task through a finite-state machine whose edges encode subgoals of the task using high-level events. The structure of RMs enables the decomposition of a task into simpler and independently solvable subtasks that help tackle long-horizon and/or sparse reward tasks. We propose a formalism for further abstracting the subtask structure by endowing an RM with the ability to call other RMs, thus composing a hierarchy of RMs (HRM). We exploit HRMs by treating each call to an RM as an independently solvable subtask using the options framework, and describe a curriculum-based method to learn HRMs from traces observed by the agent. Our experiments reveal that exploiting a handcrafted HRM leads to faster convergence than with a flat HRM, and that learning an HRM is feasible in cases where its equivalent flat representation is not.",
    "path": "papers/22/05/2205.15752.json",
    "total_tokens": 884,
    "translated_title": "奖励状态机的层次化结构",
    "translated_abstract": "奖励状态机（RM）是一种新的形式化工具，用于通过一个有限状态机来表示强化学习任务的奖励函数，其边缘使用高级事件编码任务的子目标。 RM的结构使得将一个任务分解成简单和独立可解的子任务成为可能，这有助于处理长期规划和/或奖励稀疏的任务。我们提出了一种形式化工具，通过赋予RM调用其他RM的能力，从而组合一个RM的层次结构（HRM）来进一步抽象子任务结构。我们利用HRM通过将对RM的每个调用视为单独可解的子任务来使用选项框架，并描述了一种基于课程的方法来从代理观察到的轨迹中学习HRM。我们的实验表明，利用手工制作的HRM比扁平的HRM收敛更快，并且在等价的扁平表示不可行的情况下，学习HRM是可行的。",
    "tldr": "本文提出了一种奖励状态机（RM）的层次化结构（HRM），利用它可以将任务进一步抽象为多个子任务，每个子任务都可以独立解决；使用 HRM 可以帮助加快收敛速度且在学习中是可行的。",
    "en_tdlr": "This paper proposes a hierarchy of reward machines (HRM) for further abstraction of subtask structure and independent solvability, which can help accelerate convergence and is feasible in learning."
}