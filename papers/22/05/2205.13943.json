{
    "title": "Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v4 [cs.CV] UPDATED)",
    "abstract": "Masked image modeling, an emerging self-supervised pre-training method, has shown impressive success across numerous downstream vision tasks with Vision transformers. Its underlying idea is simple: a portion of the input image is masked out and then reconstructed via a pre-text task. However, the working principle behind MIM is not well explained, and previous studies insist that MIM primarily works for the Transformer family but is incompatible with CNNs. In this work, we observe that MIM essentially teaches the model to learn better middle-order interactions among patches for more generalized feature extraction. We then propose an Architecture-Agnostic Masked Image Modeling framework (A$^2$MIM), which is compatible with both Transformers and CNNs in a unified way. Extensive experiments on popular benchmarks show that A$^2$MIM learns better representations without explicit design and endows the backbone model with the stronger capability to transfer to various downstream tasks.",
    "link": "http://arxiv.org/abs/2205.13943",
    "context": "Title: Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v4 [cs.CV] UPDATED)\nAbstract: Masked image modeling, an emerging self-supervised pre-training method, has shown impressive success across numerous downstream vision tasks with Vision transformers. Its underlying idea is simple: a portion of the input image is masked out and then reconstructed via a pre-text task. However, the working principle behind MIM is not well explained, and previous studies insist that MIM primarily works for the Transformer family but is incompatible with CNNs. In this work, we observe that MIM essentially teaches the model to learn better middle-order interactions among patches for more generalized feature extraction. We then propose an Architecture-Agnostic Masked Image Modeling framework (A$^2$MIM), which is compatible with both Transformers and CNNs in a unified way. Extensive experiments on popular benchmarks show that A$^2$MIM learns better representations without explicit design and endows the backbone model with the stronger capability to transfer to various downstream tasks.",
    "path": "papers/22/05/2205.13943.json",
    "total_tokens": 940,
    "translated_title": "无架构偏见的遮蔽图像建模——从ViT回到CNN",
    "translated_abstract": "遮蔽图像建模（MIM）是一种新兴的自监督预训练方法，在许多下游视觉任务中，如Vision transformers，已经显示出惊人的成功。其基本思想很简单：把输入图像的一部分遮挡住，然后通过预热任务进行重构。然而，MIM背后的工作原理并没有得到很好的解释，以前的研究坚持认为MIM主要适用于Transformer家族，但与CNN不兼容。 在这项工作中，我们观察到，MIM实质上是教模型学习更好的中级交互关系，以进行更广义的特征提取。然后，我们提出了一种无架构偏见的遮蔽图像建模框架（A$^2$MIM），它以统一的方式兼容Transformers和CNNs。对流行基准的大量实验表明，A$^2$MIM学习更好的表示形式，无需显式设计，并赋予骨干模型更强的能力来转移到各种下游任务。",
    "tldr": "本文提出一种无架构偏见的遮蔽图像建模框架（A$^2$MIM），既适用于Transformers也适用于CNNs，并通过学习中级交互关系来提高特征提取的一般性能。",
    "en_tdlr": "This paper proposes an architecture-agnostic masked image modeling framework (A$^2$MIM), compatible with both Transformers and CNNs, and enhances generalized feature extraction by learning better middle-order interactions, which endows the backbone model with a stronger capability to transfer to various downstream tasks."
}