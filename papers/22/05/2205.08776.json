{
    "title": "AdaMCT: Adaptive Mixture of CNN-Transformer for Sequential Recommendation. (arXiv:2205.08776v3 [cs.IR] UPDATED)",
    "abstract": "Sequential recommendation (SR) aims to model users dynamic preferences from a series of interactions. A pivotal challenge in user modeling for SR lies in the inherent variability of user preferences. An effective SR model is expected to capture both the long-term and short-term preferences exhibited by users, wherein the former can offer a comprehensive understanding of stable interests that impact the latter. To more effectively capture such information, we incorporate locality inductive bias into the Transformer by amalgamating its global attention mechanism with a local convolutional filter, and adaptively ascertain the mixing importance on a personalized basis through layer-aware adaptive mixture units, termed as AdaMCT. Moreover, as users may repeatedly browse potential purchases, it is expected to consider multiple relevant items concurrently in long-/short-term preferences modeling. Given that softmax-based attention may promote unimodal activation, we propose the Squeeze-Excita",
    "link": "http://arxiv.org/abs/2205.08776",
    "context": "Title: AdaMCT: Adaptive Mixture of CNN-Transformer for Sequential Recommendation. (arXiv:2205.08776v3 [cs.IR] UPDATED)\nAbstract: Sequential recommendation (SR) aims to model users dynamic preferences from a series of interactions. A pivotal challenge in user modeling for SR lies in the inherent variability of user preferences. An effective SR model is expected to capture both the long-term and short-term preferences exhibited by users, wherein the former can offer a comprehensive understanding of stable interests that impact the latter. To more effectively capture such information, we incorporate locality inductive bias into the Transformer by amalgamating its global attention mechanism with a local convolutional filter, and adaptively ascertain the mixing importance on a personalized basis through layer-aware adaptive mixture units, termed as AdaMCT. Moreover, as users may repeatedly browse potential purchases, it is expected to consider multiple relevant items concurrently in long-/short-term preferences modeling. Given that softmax-based attention may promote unimodal activation, we propose the Squeeze-Excita",
    "path": "papers/22/05/2205.08776.json",
    "total_tokens": 971,
    "translated_title": "AdaMCT：适应性CNN-Transformer混合模型用于顺序推荐",
    "translated_abstract": "顺序推荐旨在从一系列交互中建模用户的动态偏好。顺序推荐中的一个关键挑战在于用户偏好的固有变化性。一个有效的顺序推荐模型应该能够捕捉到用户展示的长期和短期偏好，其中前者可以提供对影响后者的稳定兴趣的全面理解。为了更有效地捕捉这样的信息，我们将局部感知性偏差引入Transformer中，通过将其全局注意机制与局部卷积滤波器结合起来，并通过层感知的自适应混合单元AdaMCT以个性化基础确定混合重要性。此外，由于用户可能会反复浏览潜在的购买选项，在长期和短期偏好建模中同时考虑多个相关项目是可预期的。鉴于基于softmax的注意力可能会促进单峰激活，我们提出了Squeeze-Excita方法。",
    "tldr": "这项研究提出了一种名为AdaMCT的适应性混合CNN-Transformer模型，用于顺序推荐。该模型结合了Transformer的全局注意机制和局部卷积滤波器，以更好地捕捉用户的长期和短期偏好，并通过个性化的方法确定混合重要性。另外，研究还提出了Squeeze-Excita方法，以同时考虑多个相关项目的购买选项。",
    "en_tdlr": "This study proposes an adaptive mixture of CNN-Transformer model called AdaMCT for sequential recommendation. The model combines the global attention mechanism of Transformer with local convolutional filters to capture both long-term and short-term user preferences, and determines the mixing importance on a personalized basis. Additionally, the study introduces the Squeeze-Excita method to consider multiple relevant items concurrently in the recommendation process."
}