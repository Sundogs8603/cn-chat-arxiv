{
    "title": "Memory-efficient Reinforcement Learning with Value-based Knowledge Consolidation. (arXiv:2205.10868v5 [cs.LG] UPDATED)",
    "abstract": "Artificial neural networks are promising for general function approximation but challenging to train on non-independent or non-identically distributed data due to catastrophic forgetting. The experience replay buffer, a standard component in deep reinforcement learning, is often used to reduce forgetting and improve sample efficiency by storing experiences in a large buffer and using them for training later. However, a large replay buffer results in a heavy memory burden, especially for onboard and edge devices with limited memory capacities. We propose memory-efficient reinforcement learning algorithms based on the deep Q-network algorithm to alleviate this problem. Our algorithms reduce forgetting and maintain high sample efficiency by consolidating knowledge from the target Q-network to the current Q-network. Compared to baseline methods, our algorithms achieve comparable or better performance in both feature-based and image-based tasks while easing the burden of large experience re",
    "link": "http://arxiv.org/abs/2205.10868",
    "context": "Title: Memory-efficient Reinforcement Learning with Value-based Knowledge Consolidation. (arXiv:2205.10868v5 [cs.LG] UPDATED)\nAbstract: Artificial neural networks are promising for general function approximation but challenging to train on non-independent or non-identically distributed data due to catastrophic forgetting. The experience replay buffer, a standard component in deep reinforcement learning, is often used to reduce forgetting and improve sample efficiency by storing experiences in a large buffer and using them for training later. However, a large replay buffer results in a heavy memory burden, especially for onboard and edge devices with limited memory capacities. We propose memory-efficient reinforcement learning algorithms based on the deep Q-network algorithm to alleviate this problem. Our algorithms reduce forgetting and maintain high sample efficiency by consolidating knowledge from the target Q-network to the current Q-network. Compared to baseline methods, our algorithms achieve comparable or better performance in both feature-based and image-based tasks while easing the burden of large experience re",
    "path": "papers/22/05/2205.10868.json",
    "total_tokens": 971,
    "translated_title": "基于价值知识整合的高效内存强化学习",
    "translated_abstract": "人工神经网络在通用函数逼近上非常有前途，但面临灾难性遗忘的训练难题，特别是在非独立或非同分布的数据上训练困难。经验重放缓存器是深度强化学习中的标准组件，通常通过将经验存储在大缓存器中并延迟使用进行训练，以减少遗忘并提高样本效率。然而，大型重放缓存器会导致重负载内存，特别是对于内存容量有限的边缘设备和嵌入式设备。我们提出了基于深度 Q 网络算法的内存高效强化学习算法，以缓解这个问题。我们的算法通过从目标 Q 网络向当前 Q 网络整合知识，减少遗忘并保持高样本效率。与基准方法相比，我们的算法在基于特征和基于图像的任务中实现了可比较或更好的性能，同时减轻了大经验重放缓存器的负担。",
    "tldr": "本论文提出了一种基于价值知识整合的高效内存强化学习算法，通过从目标 Q 网络向当前 Q 网络整合知识，减少遗忘并保持高样本效率。相较于传统方法，本算法在特征和图像任务中表现相当或更好，同时减轻了大经验重放缓存器带来的内存负担。",
    "en_tdlr": "This paper proposes a memory-efficient reinforcement learning algorithm based on value-based knowledge consolidation. By consolidating knowledge from the target Q-network to the current Q-network, our algorithm reduces forgetting and maintains high sample efficiency. Compared to baseline methods, it achieves comparable or better performance in both feature-based and image-based tasks while easing the burden of large experience replay buffers."
}