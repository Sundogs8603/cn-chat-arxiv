{
    "title": "Transformer for Partial Differential Equations' Operator Learning. (arXiv:2205.13671v3 [cs.LG] UPDATED)",
    "abstract": "Data-driven learning of partial differential equations' solution operators has recently emerged as a promising paradigm for approximating the underlying solutions. The solution operators are usually parameterized by deep learning models that are built upon problem-specific inductive biases. An example is a convolutional or a graph neural network that exploits the local grid structure where functions' values are sampled. The attention mechanism, on the other hand, provides a flexible way to implicitly exploit the patterns within inputs, and furthermore, relationship between arbitrary query locations and inputs. In this work, we present an attention-based framework for data-driven operator learning, which we term Operator Transformer (OFormer). Our framework is built upon self-attention, cross-attention, and a set of point-wise multilayer perceptrons (MLPs), and thus it makes few assumptions on the sampling pattern of the input function or query locations. We show that the proposed frame",
    "link": "http://arxiv.org/abs/2205.13671",
    "context": "Title: Transformer for Partial Differential Equations' Operator Learning. (arXiv:2205.13671v3 [cs.LG] UPDATED)\nAbstract: Data-driven learning of partial differential equations' solution operators has recently emerged as a promising paradigm for approximating the underlying solutions. The solution operators are usually parameterized by deep learning models that are built upon problem-specific inductive biases. An example is a convolutional or a graph neural network that exploits the local grid structure where functions' values are sampled. The attention mechanism, on the other hand, provides a flexible way to implicitly exploit the patterns within inputs, and furthermore, relationship between arbitrary query locations and inputs. In this work, we present an attention-based framework for data-driven operator learning, which we term Operator Transformer (OFormer). Our framework is built upon self-attention, cross-attention, and a set of point-wise multilayer perceptrons (MLPs), and thus it makes few assumptions on the sampling pattern of the input function or query locations. We show that the proposed frame",
    "path": "papers/22/05/2205.13671.json",
    "total_tokens": 922,
    "translated_title": "用Transformer进行偏微分方程算子学习",
    "translated_abstract": "数据驱动的偏微分方程解算子学习近年来已成为一种有前途的范式，用于近似基础解。解算子通常由基于问题特定归纳偏见的深度学习模型参数化。例如，卷积或图神经网络利用了函数值被采样的本地网格结构。另一方面，注意力机制提供了一种灵活的方式来隐式利用输入中的模式，以及任意查询位置和输入之间的关系。在本研究中，我们提出了一种基于注意力的数据驱动算子学习框架，称为Operator Transformer (OFormer)。我们的框架建立在自注意、交叉注意和一组逐点多层感知机(MLP)之上，因此在输入函数或查询位置的采样模式上做出了很少的假设。我们展示了所提出的框架能够广泛适用于不同的偏微分方程，并且具有与先前基于卷积和图神经网络的方法相当的准确性，并且在某些情况下具有更好的表现。",
    "tldr": "本文提出一种基于注意力机制的数据驱动算子学习框架OFormer，它可以广泛适用于不同的偏微分方程，并且具有与传统方法相当的准确性，甚至在某些情况下具有更好的表现。",
    "en_tdlr": "This paper proposes an attention-based framework for data-driven operator learning, named Operator Transformer (OFormer), which can be widely applied to different partial differential equations and has comparable accuracy to traditional methods, and even better performance in some cases."
}