{
    "title": "Arithmetic-Based Pretraining -- Improving Numeracy of Pretrained Language Models. (arXiv:2205.06733v2 [cs.CL] UPDATED)",
    "abstract": "State-of-the-art pretrained language models tend to perform below their capabilities when applied out-of-the-box on tasks that require understanding and working with numbers. Recent work suggests two main reasons for this: (1) popular tokenisation algorithms have limited expressiveness for numbers, and (2) common pretraining objectives do not target numeracy. Approaches that address these shortcomings usually require architectural changes or pretraining from scratch. In this paper, we propose a new extended pretraining approach called Arithmetic-Based Pretraining that jointly addresses both in one extended pretraining step without requiring architectural changes or pretraining from scratch. Arithmetic-Based Pretraining combines contrastive learning to improve the number representation, and a novel extended pretraining objective called Inferable Number Prediction Task to improve numeracy. Our experiments show the effectiveness of Arithmetic-Based Pretraining in three different tasks tha",
    "link": "http://arxiv.org/abs/2205.06733",
    "context": "Title: Arithmetic-Based Pretraining -- Improving Numeracy of Pretrained Language Models. (arXiv:2205.06733v2 [cs.CL] UPDATED)\nAbstract: State-of-the-art pretrained language models tend to perform below their capabilities when applied out-of-the-box on tasks that require understanding and working with numbers. Recent work suggests two main reasons for this: (1) popular tokenisation algorithms have limited expressiveness for numbers, and (2) common pretraining objectives do not target numeracy. Approaches that address these shortcomings usually require architectural changes or pretraining from scratch. In this paper, we propose a new extended pretraining approach called Arithmetic-Based Pretraining that jointly addresses both in one extended pretraining step without requiring architectural changes or pretraining from scratch. Arithmetic-Based Pretraining combines contrastive learning to improve the number representation, and a novel extended pretraining objective called Inferable Number Prediction Task to improve numeracy. Our experiments show the effectiveness of Arithmetic-Based Pretraining in three different tasks tha",
    "path": "papers/22/05/2205.06733.json",
    "total_tokens": 909,
    "translated_title": "基于算术的预训练——提高预训练语言模型的数字能力",
    "translated_abstract": "目前最先进的预训练语言模型在需要理解和使用数字的任务上表现不佳。最近的研究表明，这主要有两个原因：(1)常用的分词算法对数字的表达能力有限，(2)常见的预训练目标不针对数值能力。解决这些缺点的方法通常需要架构变化或重新预训练。本文提出了一种新的扩展预训练方法，称为基于算术的预训练，它可以在一个扩展的预训练步骤中同时解决这两个缺点，而不需要架构变化或重新预训练。基于算术的预训练将对比学习与新的可推断数预测任务的预训练目标相结合，以改进数字能力。实验结果表明，基于算术的预训练在三个不同的任务中都非常有效。",
    "tldr": "本论文提出了一种新的扩展预训练方法——基于算术的预训练，它可以同时解决数字表达能力和数值能力不足的问题，而不需要进行架构变化或重新预训练。实验结果表明，基于算术的预训练在三个不同的任务中都非常有效。",
    "en_tdlr": "This paper proposes a new extended pretraining approach called Arithmetic-Based Pretraining, which addresses the issues of limited expressiveness for numbers and lack of numeracy in pretrained language models. The approach combines contrastive learning and a novel pretraining objective called Inferable Number Prediction Task. Experimental results show the effectiveness of the approach in three different tasks."
}