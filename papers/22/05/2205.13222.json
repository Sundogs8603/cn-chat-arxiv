{
    "title": "Combating Client Dropout in Federated Learning via Friend Model Substitution. (arXiv:2205.13222v3 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FL) is a new distributed machine learning framework known for its benefits on data privacy and communication efficiency. Since full client participation in many cases is infeasible due to constrained resources, partial participation FL algorithms have been investigated that proactively select/sample a subset of clients, aiming to achieve learning performance close to the full participation case. This paper studies a passive partial client participation scenario that is much less well understood, where partial participation is a result of external events, namely client dropout, rather than a decision of the FL algorithm. We cast FL with client dropout as a special case of a larger class of FL problems where clients can submit substitute (possibly inaccurate) local model updates. Based on our convergence analysis, we develop a new algorithm FL-FDMS that discovers friends of clients (i.e., clients whose data distributions are similar) on-the-fly and uses friends' local",
    "link": "http://arxiv.org/abs/2205.13222",
    "context": "Title: Combating Client Dropout in Federated Learning via Friend Model Substitution. (arXiv:2205.13222v3 [cs.LG] UPDATED)\nAbstract: Federated learning (FL) is a new distributed machine learning framework known for its benefits on data privacy and communication efficiency. Since full client participation in many cases is infeasible due to constrained resources, partial participation FL algorithms have been investigated that proactively select/sample a subset of clients, aiming to achieve learning performance close to the full participation case. This paper studies a passive partial client participation scenario that is much less well understood, where partial participation is a result of external events, namely client dropout, rather than a decision of the FL algorithm. We cast FL with client dropout as a special case of a larger class of FL problems where clients can submit substitute (possibly inaccurate) local model updates. Based on our convergence analysis, we develop a new algorithm FL-FDMS that discovers friends of clients (i.e., clients whose data distributions are similar) on-the-fly and uses friends' local",
    "path": "papers/22/05/2205.13222.json",
    "total_tokens": 1109,
    "translated_title": "通过友好模型替换解决联合学习中的客户端退役问题",
    "translated_abstract": "联合学习是一种新的分布式机器学习框架，以其数据隐私和通信效率的优势而闻名。由于许多情况下完全客户端参与不可行，研究了主动选择/采样一部分客户端的部分参与FL算法，旨在实现接近全参与情况的学习性能。本文研究了一种被动部分客户端参与情况，这种情况理解不足得多，部分参与是由于外部事件（即客户端退出）而不是FL算法的决定而导致的。我们将具有客户端退役的FL视为FL问题中的一类特殊情况，其中客户端可以提交替代（可能不准确）的本地模型更新。根据我们的收敛分析，我们开发了一种新算法FL-FDMS，可以即时发现客户端的朋友（即数据分布相似的客户端），并使用朋友的本地模型更新来替代放弃的客户端的缺失更新。我们在假设下证明FL-FDMS在理论上收敛到最优全局模型，该假设足以涵盖许多现有的FL算法。实验结果表明，与几个基线相比，当客户端退出率适中到高时，FL-FDMS实现了更高的学习准确性和更低的通信成本。",
    "tldr": "本研究提出了一种新算法FL-FDMS，可以在客户端退役的情况下，即时找到数据分布相似的客户端，并使用这些客户端的本地模型更新来替代缺失的客户端的更新，实现更高的学习准确性和更低的通信成本。",
    "en_tdlr": "This paper proposes a new algorithm, FL-FDMS, that can discover friends of clients on-the-fly and use their local model updates to substitute for the missing updates of dropped-out clients, achieving higher learning accuracy and lower communication cost in the case of client dropout."
}