{
    "title": "Stochastic Gradient Methods with Compressed Communication for Decentralized Saddle Point Problems. (arXiv:2205.14452v2 [cs.LG] UPDATED)",
    "abstract": "We develop two compression based stochastic gradient algorithms to solve a class of non-smooth strongly convex-strongly concave saddle-point problems in a decentralized setting (without a central server). Our first algorithm is a Restart-based Decentralized Proximal Stochastic Gradient method with Compression (C-RDPSG) for general stochastic settings. We provide rigorous theoretical guarantees of C-RDPSG with gradient computation complexity and communication complexity of order $\\mathcal{O}( (1+\\delta)^4 \\frac{1}{L^2}{\\kappa_f^2}\\kappa_g^2 \\frac{1}{\\epsilon} )$, to achieve an $\\epsilon$-accurate saddle-point solution, where $\\delta$ denotes the compression factor, $\\kappa_f$ and $\\kappa_g$ denote respectively the condition numbers of objective function and communication graph, and $L$ denotes the smoothness parameter of the smooth part of the objective function. Next, we present a Decentralized Proximal Stochastic Variance Reduced Gradient algorithm with Compression (C-DPSVRG) for fini",
    "link": "http://arxiv.org/abs/2205.14452",
    "context": "Title: Stochastic Gradient Methods with Compressed Communication for Decentralized Saddle Point Problems. (arXiv:2205.14452v2 [cs.LG] UPDATED)\nAbstract: We develop two compression based stochastic gradient algorithms to solve a class of non-smooth strongly convex-strongly concave saddle-point problems in a decentralized setting (without a central server). Our first algorithm is a Restart-based Decentralized Proximal Stochastic Gradient method with Compression (C-RDPSG) for general stochastic settings. We provide rigorous theoretical guarantees of C-RDPSG with gradient computation complexity and communication complexity of order $\\mathcal{O}( (1+\\delta)^4 \\frac{1}{L^2}{\\kappa_f^2}\\kappa_g^2 \\frac{1}{\\epsilon} )$, to achieve an $\\epsilon$-accurate saddle-point solution, where $\\delta$ denotes the compression factor, $\\kappa_f$ and $\\kappa_g$ denote respectively the condition numbers of objective function and communication graph, and $L$ denotes the smoothness parameter of the smooth part of the objective function. Next, we present a Decentralized Proximal Stochastic Variance Reduced Gradient algorithm with Compression (C-DPSVRG) for fini",
    "path": "papers/22/05/2205.14452.json",
    "total_tokens": 933,
    "translated_title": "压缩通信的随机梯度方法用于分散鞍点问题解决",
    "translated_abstract": "本文提出了两种基于压缩的随机梯度算法来解决一类非光滑强凸-强凹鞍点问题的分散式（无中央服务器）求解。首先，我们提出了一种面向一般随机情形的基于重启的压缩分散式近端随机梯度方法（C-RDPSG）。我们提供了严格的理论保证，其中C-RDPSG的梯度计算复杂度和通信复杂度分别为$\\mathcal{O}( (1+\\delta)^4 \\frac{1}{L^2}{\\kappa_f^2}\\kappa_g^2 \\frac{1}{\\epsilon} )$，以实现$\\epsilon$精度的鞍点解。接下来，我们提出了一种基于压缩的分散式近端随机方差缩减梯度算法(C-DPSVRG)。",
    "tldr": "这篇论文提出了两种基于压缩的随机梯度算法，用于解决非光滑强凸-强凹鞍点问题的分散求解，并提供了理论保证，包括梯度计算和通信复杂度的限制。"
}