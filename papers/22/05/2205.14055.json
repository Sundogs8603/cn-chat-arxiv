{
    "title": "A Blessing of Dimensionality in Membership Inference through Regularization. (arXiv:2205.14055v2 [cs.LG] UPDATED)",
    "abstract": "Is overparameterization a privacy liability? In this work, we study the effect that the number of parameters has on a classifier's vulnerability to membership inference attacks. We first demonstrate how the number of parameters of a model can induce a privacy--utility trade-off: increasing the number of parameters generally improves generalization performance at the expense of lower privacy. However, remarkably, we then show that if coupled with proper regularization, increasing the number of parameters of a model can actually simultaneously increase both its privacy and performance, thereby eliminating the privacy--utility trade-off. Theoretically, we demonstrate this curious phenomenon for logistic regression with ridge regularization in a bi-level feature ensemble setting. Pursuant to our theoretical exploration, we develop a novel leave-one-out analysis tool to precisely characterize the vulnerability of a linear classifier to the optimal membership inference attack. We empirically",
    "link": "http://arxiv.org/abs/2205.14055",
    "context": "Title: A Blessing of Dimensionality in Membership Inference through Regularization. (arXiv:2205.14055v2 [cs.LG] UPDATED)\nAbstract: Is overparameterization a privacy liability? In this work, we study the effect that the number of parameters has on a classifier's vulnerability to membership inference attacks. We first demonstrate how the number of parameters of a model can induce a privacy--utility trade-off: increasing the number of parameters generally improves generalization performance at the expense of lower privacy. However, remarkably, we then show that if coupled with proper regularization, increasing the number of parameters of a model can actually simultaneously increase both its privacy and performance, thereby eliminating the privacy--utility trade-off. Theoretically, we demonstrate this curious phenomenon for logistic regression with ridge regularization in a bi-level feature ensemble setting. Pursuant to our theoretical exploration, we develop a novel leave-one-out analysis tool to precisely characterize the vulnerability of a linear classifier to the optimal membership inference attack. We empirically",
    "path": "papers/22/05/2205.14055.json",
    "total_tokens": 902,
    "translated_title": "正则化对成员推理中维度的祝福",
    "translated_abstract": "这项研究探讨了过度参数化对分类器在成员推理攻击中的易受攻击性的影响。我们首先展示了模型参数数量如何引发隐私和效用的权衡问题：增加参数数量通常会提高泛化性能，但会降低隐私保障。然而，令人惊讶的是，我们随后证明，如果与适当的正则化相结合，增加模型参数数量实际上可以同时增加其隐私和性能，从而消除隐私与效用之间的权衡问题。在理论上，我们通过对双重特征集合设置的岭回归逻辑回归进行的实验来证明了这一神奇现象。在我们的理论探索之后，我们开发了一种新的leave-one-out分析工具，以精确刻画线性分类器对最佳成员推理攻击的易受攻击性。最后我们还在实验中进行了验证。",
    "tldr": "研究探讨了过度参数化对成员推理攻击易受攻击性的影响，发现适当的正则化可以在增加模型参数数量的同时提高隐私和性能，消除了隐私与效用之间的权衡问题。",
    "en_tdlr": "This work studies the effect of overparameterization on a classifier's vulnerability to membership inference attacks and shows that proper regularization can increase both privacy and performance, eliminating the trade-off between them. Logistic regression with ridge regularization is experimentally shown to have the curious phenomenon of increasing both privacy and performance with increased parameter number. A novel leave-one-out analysis tool is also developed to characterize a linear classifier's vulnerability to optimal membership inference attacks."
}