{
    "title": "Analysis of functional neural codes of deep learning models. (arXiv:2205.10952v2 [cs.LG] UPDATED)",
    "abstract": "Deep neural networks (DNNs), the agents of deep learning (DL), require a massive number of parallel/sequential operations. This makes it difficult to comprehend DNNs' operations and impedes proper diagnosis. Without better knowledge of their internal process, deploying DNNs in high-stakes domains can lead to catastrophic failures. Therefore, to build more reliable DNNs/DL to be deployed in high-stakes real-world problems, it is imperative that we gain insights into DNNs' internal operations underlying their decision-making. Here, we use the self-organizing map (SOM) to analyze DL models' internal codes associated with DNNs' decision-making. Our analyses suggest that shallow layers close to the input layer compress features into condensed space and that deep layers close to the output layer expand feature space. We also found evidence indicating that compressed features may underlie DNNs' vulnerabilities to adversarial perturbations.",
    "link": "http://arxiv.org/abs/2205.10952",
    "context": "Title: Analysis of functional neural codes of deep learning models. (arXiv:2205.10952v2 [cs.LG] UPDATED)\nAbstract: Deep neural networks (DNNs), the agents of deep learning (DL), require a massive number of parallel/sequential operations. This makes it difficult to comprehend DNNs' operations and impedes proper diagnosis. Without better knowledge of their internal process, deploying DNNs in high-stakes domains can lead to catastrophic failures. Therefore, to build more reliable DNNs/DL to be deployed in high-stakes real-world problems, it is imperative that we gain insights into DNNs' internal operations underlying their decision-making. Here, we use the self-organizing map (SOM) to analyze DL models' internal codes associated with DNNs' decision-making. Our analyses suggest that shallow layers close to the input layer compress features into condensed space and that deep layers close to the output layer expand feature space. We also found evidence indicating that compressed features may underlie DNNs' vulnerabilities to adversarial perturbations.",
    "path": "papers/22/05/2205.10952.json",
    "total_tokens": 942,
    "translated_title": "深度学习模型的功能性神经编码分析",
    "translated_abstract": "深度神经网络(DNNs)作为深度学习(DL)的代理，需要大量的并行/顺序操作。这使得理解DNNs的操作变得困难，阻碍了适当的诊断。在没有对其内部过程有更好的了解之前，在高风险领域部署DNNs可能导致灾难性故障。因此，为了构建更可靠的DNNs/DL来解决高风险现实世界问题，我们必须深入了解DNNs决策背后的内部操作。在这里，我们使用自组织映射(SOM)分析与DNNs决策相关的DL模型的内部编码。我们的分析表明，靠近输入层的浅层将特征压缩到紧凑空间中，而靠近输出层的深层将特征空间扩展。我们还发现有证据表明，压缩特征可能导致DNNs对敌对扰动的脆弱性。",
    "tldr": "本研究使用自组织映射(SOM)分析了深度学习模型中与决策相关的内部编码，发现浅层将特征压缩到紧凑空间中，而深层将特征空间扩展，并指出压缩特征可能导致对敌对扰动的脆弱性。"
}