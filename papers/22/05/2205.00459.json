{
    "title": "Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation. (arXiv:2205.00459v2 [cs.NE] UPDATED)",
    "abstract": "Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when represe",
    "link": "http://arxiv.org/abs/2205.00459",
    "context": "Title: Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation. (arXiv:2205.00459v2 [cs.NE] UPDATED)\nAbstract: Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when represe",
    "path": "papers/22/05/2205.00459.json",
    "total_tokens": 999,
    "translated_title": "基于脉冲表示的微分训练高性能低延迟脉冲神经网络",
    "translated_abstract": "脉冲神经网络（SNN）是一种非常有前途的能源高效的人工智能模型，尤其当它在神经形态硬件上实现时。然而，由于其不可微性，高效地训练SNN是一项挑战。目前大多数方法要么存在高延迟（即长的仿真时间步长）要么无法达到与人工神经网络（ANN）相当的高性能。本文提出了基于脉冲表示的微分（DSR）方法，可以实现与ANN相竞争的高性能且低延迟的训练。首先，我们使用（加权）发射率编码将脉冲列编码为脉冲表示。在脉冲表示的基础上，我们系统地推导出具有常见神经模型的脉冲动力学可以表示为一些次微分映射。从这个角度出发，我们提出的DSR方法通过这些映射的梯度训练SNN并避免了SNN训练中常见的不可微性问题。然后我们对噪声下表示误差进行了分析，并指出DSR方法对这些误差表现出强有力的容忍度。",
    "tldr": "本文提出了基于脉冲表示的微分（DSR）方法，可以实现与人工神经网络（ANN）相竞争的高性能、低延迟训练的脉冲神经网络（SNN）。",
    "en_tdlr": "This paper proposes the Differentiation on Spike Representation (DSR) method, which achieves high-performance and low-latency training for Spiking Neural Networks (SNNs) competitive to Artificial Neural Networks (ANNs) by deriving sub-differentiable mappings from spiking dynamics with common neural models through gradients of the mapping."
}