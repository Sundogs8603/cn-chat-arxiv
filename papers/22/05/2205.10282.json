{
    "title": "Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks. (arXiv:2205.10282v2 [cs.CL] UPDATED)",
    "abstract": "Representation learning on networks aims to derive a meaningful vector representation for each node, thereby facilitating downstream tasks such as link prediction, node classification, and node clustering. In heterogeneous text-rich networks, this task is more challenging due to (1) presence or absence of text: Some nodes are associated with rich textual information, while others are not; (2) diversity of types: Nodes and edges of multiple types form a heterogeneous network structure. As pretrained language models (PLMs) have demonstrated their effectiveness in obtaining widely generalizable text representations, a substantial amount of effort has been made to incorporate PLMs into representation learning on text-rich networks. However, few of them can jointly consider heterogeneous structure (network) information as well as rich textual semantic information of each node effectively. In this paper, we propose Heterformer, a Heterogeneous Network-Empowered Transformer that performs cont",
    "link": "http://arxiv.org/abs/2205.10282",
    "context": "Title: Heterformer: Transformer-based Deep Node Representation Learning on Heterogeneous Text-Rich Networks. (arXiv:2205.10282v2 [cs.CL] UPDATED)\nAbstract: Representation learning on networks aims to derive a meaningful vector representation for each node, thereby facilitating downstream tasks such as link prediction, node classification, and node clustering. In heterogeneous text-rich networks, this task is more challenging due to (1) presence or absence of text: Some nodes are associated with rich textual information, while others are not; (2) diversity of types: Nodes and edges of multiple types form a heterogeneous network structure. As pretrained language models (PLMs) have demonstrated their effectiveness in obtaining widely generalizable text representations, a substantial amount of effort has been made to incorporate PLMs into representation learning on text-rich networks. However, few of them can jointly consider heterogeneous structure (network) information as well as rich textual semantic information of each node effectively. In this paper, we propose Heterformer, a Heterogeneous Network-Empowered Transformer that performs cont",
    "path": "papers/22/05/2205.10282.json",
    "total_tokens": 809,
    "translated_title": "Heterformer：基于Transformer的异构文本网络深度节点表示学习",
    "translated_abstract": "网络表示学习旨在为每个节点推导出有意义的向量表示，从而促进诸如链接预测、节点分类和节点聚类等下游任务。在异构文本网络中，由于文本的存在与缺失以及多种类型的节点和边缘形成的异构网络结构，这项任务更加具有挑战性。由于预训练语言模型（PLMs）已经证明了在获得广泛通用性文本表示方面的有效性，在文本丰富的网络表示学习中，人们已经付出了大量的努力来将PLMs包含到其中。然而，很少有研究可以有效地共同考虑异构结构（网络）信息和每个节点的丰富文本语义信息。本文提出了Heterformer，一种异构网络强化的Transformer，可同时处理网络结构信息和每个节点的丰富文本语义信息。",
    "tldr": "本论文提出了一种名为Heterformer的基于Transformer的深度节点表示学习方法，它可以同时处理异构结构和丰富的文本语义信息。",
    "en_tdlr": "This paper proposes a Transformer-based deep node representation learning method called Heterformer, which can effectively handle both heterogeneous network structure and rich textual semantic information."
}