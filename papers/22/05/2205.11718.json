{
    "title": "Semi-Parametric Inducing Point Networks and Neural Processes. (arXiv:2205.11718v2 [cs.LG] UPDATED)",
    "abstract": "We introduce semi-parametric inducing point networks (SPIN), a general-purpose architecture that can query the training set at inference time in a compute-efficient manner. Semi-parametric architectures are typically more compact than parametric models, but their computational complexity is often quadratic. In contrast, SPIN attains linear complexity via a cross-attention mechanism between datapoints inspired by inducing point methods. Querying large training sets can be particularly useful in meta-learning, as it unlocks additional training signal, but often exceeds the scaling limits of existing models. We use SPIN as the basis of the Inducing Point Neural Process, a probabilistic model which supports large contexts in meta-learning and achieves high accuracy where existing models fail. In our experiments, SPIN reduces memory requirements, improves accuracy across a range of meta-learning tasks, and improves state-of-the-art performance on an important practical problem, genotype imp",
    "link": "http://arxiv.org/abs/2205.11718",
    "context": "Title: Semi-Parametric Inducing Point Networks and Neural Processes. (arXiv:2205.11718v2 [cs.LG] UPDATED)\nAbstract: We introduce semi-parametric inducing point networks (SPIN), a general-purpose architecture that can query the training set at inference time in a compute-efficient manner. Semi-parametric architectures are typically more compact than parametric models, but their computational complexity is often quadratic. In contrast, SPIN attains linear complexity via a cross-attention mechanism between datapoints inspired by inducing point methods. Querying large training sets can be particularly useful in meta-learning, as it unlocks additional training signal, but often exceeds the scaling limits of existing models. We use SPIN as the basis of the Inducing Point Neural Process, a probabilistic model which supports large contexts in meta-learning and achieves high accuracy where existing models fail. In our experiments, SPIN reduces memory requirements, improves accuracy across a range of meta-learning tasks, and improves state-of-the-art performance on an important practical problem, genotype imp",
    "path": "papers/22/05/2205.11718.json",
    "total_tokens": 884,
    "translated_title": "半参数诱导点网络和神经过程",
    "translated_abstract": "本文提出了半参数诱导点网络（SPIN），这是一种通用的体系结构，可以在推理时间以计算有效的方式查询训练集。半参数体系结构通常比参数模型更紧凑，但它们的计算复杂度通常是二次的。相反，SPIN通过数据点之间受诱导点方法启发的交叉注意机制实现了线性复杂度。查询大型训练集在元学习中尤其有用，因为它解锁了额外的训练信号，但常常超出现有模型的缩放限制。我们使用SPIN作为诱导点神经过程的基础，这是一种概率模型，支持在元学习中使用大型上下文，并在现有模型失败的情况下实现了高精度。在我们的实验中，SPIN减少了内存需求，在一系列元学习任务中提高了准确性，并在重要的实际问题，基因型 im...（根据原文长度截断）",
    "tldr": "提出一种半参数体系结构调用训练集的网络SPIN，并以之为基础构建出适用于大型上下文的诱导点神经过程，成功应用于元学习领域并提高了准确性表现。"
}