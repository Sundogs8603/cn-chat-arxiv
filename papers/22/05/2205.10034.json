{
    "title": "SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System. (arXiv:2205.10034v2 [cs.DC] UPDATED)",
    "abstract": "With the increasing diversity of ML infrastructures nowadays, distributed training over heterogeneous computing systems is desired to facilitate the production of big models. Mixture-of-Experts (MoE) models have been proposed to lower the cost of training subject to the overall size of models/data through gating and parallelism in a divide-and-conquer fashion. While DeepSpeed has made efforts in carrying out large-scale MoE training over heterogeneous infrastructures, the efficiency of training and inference could be further improved from several system aspects, including load balancing, communication/computation efficiency, and memory footprint limits. In this work, we present SE-MoE that proposes Elastic MoE training with 2D prefetch and Fusion communication over Hierarchical storage, so as to enjoy efficient parallelisms in various types. For scalable inference in a single node, especially when the model size is larger than GPU memory, SE-MoE forms the CPU-GPU memory jointly into a ",
    "link": "http://arxiv.org/abs/2205.10034",
    "context": "Title: SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System. (arXiv:2205.10034v2 [cs.DC] UPDATED)\nAbstract: With the increasing diversity of ML infrastructures nowadays, distributed training over heterogeneous computing systems is desired to facilitate the production of big models. Mixture-of-Experts (MoE) models have been proposed to lower the cost of training subject to the overall size of models/data through gating and parallelism in a divide-and-conquer fashion. While DeepSpeed has made efforts in carrying out large-scale MoE training over heterogeneous infrastructures, the efficiency of training and inference could be further improved from several system aspects, including load balancing, communication/computation efficiency, and memory footprint limits. In this work, we present SE-MoE that proposes Elastic MoE training with 2D prefetch and Fusion communication over Hierarchical storage, so as to enjoy efficient parallelisms in various types. For scalable inference in a single node, especially when the model size is larger than GPU memory, SE-MoE forms the CPU-GPU memory jointly into a ",
    "path": "papers/22/05/2205.10034.json",
    "total_tokens": 910,
    "translated_title": "SE-MoE: 一种可扩展高效的分布式训练和推理混合专家系统",
    "translated_abstract": "随着机器学习架构的多样性增加，将模型分布式训练在异构计算系统上以方便生成大模型成为了一种趋势。混合专家模型利用分治策略通过门控和并行处理方式来降低训练成本并控制总体模型/数据大小。虽然 DeepSpeed 在进行大规模 MoE 训练上进行了尝试，但训练和推理的效率仍有提升的空间，包括负载平衡、通信/计算效率和内存限制等方面。本工作提出了 SE-MoE，使用弹性 MoE 训练、基于层次存储的 2D 预取和融合通信等方法，以便在不同的类型中获得高效的并行处理。针对单节点的可扩展推理，特别是当模型大小大于 GPU 内存时，SE-MoE 将 CPU-GPU 存储联合形成一个更大的存储空间。",
    "tldr": "SE-MoE提出了一种弹性的训练方式和不同优化措施，以提高混合专家模型的分布式训练和推理效率，解决了负载平衡、通信/计算效率和内存限制等问题。",
    "en_tdlr": "SE-MoE proposes an elastic training approach and various optimization measures to improve the efficiency of distributed training and inference for mixture-of-experts models, addressing the issues of load balancing, communication/computation efficiency, and memory constraints."
}