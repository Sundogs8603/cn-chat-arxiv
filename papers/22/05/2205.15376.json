{
    "title": "Reinforcement Learning with a Terminator. (arXiv:2205.15376v2 [cs.LG] UPDATED)",
    "abstract": "We present the problem of reinforcement learning with exogenous termination. We define the Termination Markov Decision Process (TerMDP), an extension of the MDP framework, in which episodes may be interrupted by an external non-Markovian observer. This formulation accounts for numerous real-world situations, such as a human interrupting an autonomous driving agent for reasons of discomfort. We learn the parameters of the TerMDP and leverage the structure of the estimation problem to provide state-wise confidence bounds. We use these to construct a provably-efficient algorithm, which accounts for termination, and bound its regret. Motivated by our theoretical analysis, we design and implement a scalable approach, which combines optimism (w.r.t. termination) and a dynamic discount factor, incorporating the termination probability. We deploy our method on high-dimensional driving and MinAtar benchmarks. Additionally, we test our approach on human data in a driving setting. Our results dem",
    "link": "http://arxiv.org/abs/2205.15376",
    "context": "Title: Reinforcement Learning with a Terminator. (arXiv:2205.15376v2 [cs.LG] UPDATED)\nAbstract: We present the problem of reinforcement learning with exogenous termination. We define the Termination Markov Decision Process (TerMDP), an extension of the MDP framework, in which episodes may be interrupted by an external non-Markovian observer. This formulation accounts for numerous real-world situations, such as a human interrupting an autonomous driving agent for reasons of discomfort. We learn the parameters of the TerMDP and leverage the structure of the estimation problem to provide state-wise confidence bounds. We use these to construct a provably-efficient algorithm, which accounts for termination, and bound its regret. Motivated by our theoretical analysis, we design and implement a scalable approach, which combines optimism (w.r.t. termination) and a dynamic discount factor, incorporating the termination probability. We deploy our method on high-dimensional driving and MinAtar benchmarks. Additionally, we test our approach on human data in a driving setting. Our results dem",
    "path": "papers/22/05/2205.15376.json",
    "total_tokens": 946,
    "translated_title": "强化学习中的终止问题",
    "translated_abstract": "我们提出了强化学习中的外部终止问题。我们定义了终止马尔可夫决策过程（TerMDP），它是马尔可夫决策过程（MDP）框架的扩展，在这个框架中，episode可能会被外部的非马尔可夫观察者中断。这个定义考虑了许多现实世界中的情况，比如人类出于不适因素中断自主驾驶的代理程序。我们学习了TerMDP的参数，并利用估计问题的结构提供了状态置信界限。我们使用这些界限构建了一个能够证明有效性的算法，该算法考虑了终止情况，并限制了遗憾值。在理论分析的基础上，我们设计并实施了一种可扩展的方法，将乐观性（相对于终止）与动态折扣因子相结合，同时考虑到终止概率。我们将我们的方法应用于高维驾驶和MinAtar基准测试中。此外，我们还在驾驶环境中对人类数据进行了测试。我们的结果表明...",
    "tldr": "这是一个关于强化学习中外部终止问题的论文，通过定义终止马尔可夫决策过程（TerMDP）并学习其参数，提出了一种能够考虑终止情况并限制遗憾值的算法，并在驾驶和基准测试中验证了其有效性。"
}