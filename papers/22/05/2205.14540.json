{
    "title": "SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners. (arXiv:2205.14540v3 [cs.CV] UPDATED)",
    "abstract": "Recently, self-supervised Masked Autoencoders (MAE) have attracted unprecedented attention for their impressive representation learning ability. However, the pretext task, Masked Image Modeling (MIM), reconstructs the missing local patches, lacking the global understanding of the image. This paper extends MAE to a fully supervised setting by adding a supervised classification branch, thereby enabling MAE to learn global features from golden labels effectively. The proposed Supervised MAE (SupMAE) only exploits a visible subset of image patches for classification, unlike the standard supervised pre-training where all image patches are used. Through experiments, we demonstrate that SupMAE is not only more training efficient but it also learns more robust and transferable features. Specifically, SupMAE achieves comparable performance with MAE using only 30% of compute when evaluated on ImageNet with the ViT-B/16 model. SupMAE's robustness on ImageNet variants and transfer learning perform",
    "link": "http://arxiv.org/abs/2205.14540",
    "context": "Title: SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners. (arXiv:2205.14540v3 [cs.CV] UPDATED)\nAbstract: Recently, self-supervised Masked Autoencoders (MAE) have attracted unprecedented attention for their impressive representation learning ability. However, the pretext task, Masked Image Modeling (MIM), reconstructs the missing local patches, lacking the global understanding of the image. This paper extends MAE to a fully supervised setting by adding a supervised classification branch, thereby enabling MAE to learn global features from golden labels effectively. The proposed Supervised MAE (SupMAE) only exploits a visible subset of image patches for classification, unlike the standard supervised pre-training where all image patches are used. Through experiments, we demonstrate that SupMAE is not only more training efficient but it also learns more robust and transferable features. Specifically, SupMAE achieves comparable performance with MAE using only 30% of compute when evaluated on ImageNet with the ViT-B/16 model. SupMAE's robustness on ImageNet variants and transfer learning perform",
    "path": "papers/22/05/2205.14540.json",
    "total_tokens": 957,
    "translated_title": "SupMAE: 监督式掩膜自编码器是高效的视觉学习器",
    "translated_abstract": "最近，自监督的掩膜自编码器（MAE）因其出色的表示学习能力而受到了前所未有的关注。但是，预处理任务掩膜图像建模（MIM）只能重构缺失的局部图像块，缺乏对图像的整体理解。本文通过添加一个监督分类分支，将MAE扩展到完全监督的设置中，从而使MAE能够有效地从黄金标签中学习全局特征。所提出的监督式MAE（SupMAE）仅利用图像中可见的图像块子集进行分类，而不是使用所有图像块的标准监督预训练。通过实验证明，SupMAE在训练效率上更高，并且学习到更强大和可迁移的特征。具体而言，SupMAE在使用ViT-B / 16模型在ImageNet上评估时，只使用30％的计算资源即可达到与MAE相当的性能。SupMAE在ImageNet变体上的健壮性和迁移学习性能也非常好。",
    "tldr": "SupMAE是一种监督式掩膜自编码器，通过添加监督分类分支扩展了MAE，并从黄金标签中有效学习全局特征。通过实验证明，SupMAE在训练效率、特征的鲁棒性和迁移学习性能等方面都表现出了优势。"
}