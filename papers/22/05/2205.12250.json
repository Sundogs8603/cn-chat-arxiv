{
    "title": "Efficient anti-symmetrization of a neural network layer by taming the sign problem. (arXiv:2205.12250v2 [cs.LG] UPDATED)",
    "abstract": "Explicit antisymmetrization of a neural network is a potential candidate for a universal function approximator for generic antisymmetric functions, which are ubiquitous in quantum physics. However, this procedure is a priori factorially costly to implement, making it impractical for large numbers of particles. The strategy also suffers from a sign problem. Namely, due to near-exact cancellation of positive and negative contributions, the magnitude of the antisymmetrized function may be significantly smaller than before anti-symmetrization. We show that the anti-symmetric projection of a two-layer neural network can be evaluated efficiently, opening the door to using a generic antisymmetric layer as a building block in anti-symmetric neural network Ansatzes. This approximation is effective when the sign problem is controlled, and we show that this property depends crucially the choice of activation function under standard Xavier/He initialization methods. As a consequence, using a smoot",
    "link": "http://arxiv.org/abs/2205.12250",
    "context": "Title: Efficient anti-symmetrization of a neural network layer by taming the sign problem. (arXiv:2205.12250v2 [cs.LG] UPDATED)\nAbstract: Explicit antisymmetrization of a neural network is a potential candidate for a universal function approximator for generic antisymmetric functions, which are ubiquitous in quantum physics. However, this procedure is a priori factorially costly to implement, making it impractical for large numbers of particles. The strategy also suffers from a sign problem. Namely, due to near-exact cancellation of positive and negative contributions, the magnitude of the antisymmetrized function may be significantly smaller than before anti-symmetrization. We show that the anti-symmetric projection of a two-layer neural network can be evaluated efficiently, opening the door to using a generic antisymmetric layer as a building block in anti-symmetric neural network Ansatzes. This approximation is effective when the sign problem is controlled, and we show that this property depends crucially the choice of activation function under standard Xavier/He initialization methods. As a consequence, using a smoot",
    "path": "papers/22/05/2205.12250.json",
    "total_tokens": 870,
    "translated_title": "通过驯服符号问题，对神经网络层进行高效的反对称化",
    "translated_abstract": "神经网络的显式反对称化是一种用于通用反对称函数的通用函数逼近方法，而这种函数在量子物理中无处不在。然而，这个过程的实施成本极高，对于大量粒子来说是不可行的。该策略还存在符号问题。即由于正负贡献的近乎完全抵消，反对称化函数的幅值可能明显小于反对称化之前的幅值。我们展示了如何高效地评估两层神经网络的反对称投影，从而为在反对称神经网络中使用通用的反对称层作为构建模块打开了大门。这种近似在符号问题受控时是有效的，我们展示了这种性质在使用标准Xavier/He初始化方法时，关键取决于激活函数的选择。",
    "tldr": "通过驯服符号问题，我们展示了如何高效地实现神经网络层的反对称化，为在反对称神经网络中使用通用的反对称层作为构建模块打开了大门，并且该近似方法在符号问题受控时非常有效。",
    "en_tdlr": "We demonstrate an efficient approach to anti-symmetrizing neural network layers by taming the sign problem, enabling the use of generic anti-symmetric layers as building blocks in anti-symmetric neural networks, and showing its effectiveness when the sign problem is controlled."
}