{
    "title": "Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression. (arXiv:2205.14846v2 [cs.LG] UPDATED)",
    "abstract": "As modern machine learning models continue to advance the computational frontier, it has become increasingly important to develop precise estimates for expected performance improvements under different model and data scaling regimes. Currently, theoretical understanding of the learning curves that characterize how the prediction error depends on the number of samples is restricted to either large-sample asymptotics ($m\\to\\infty$) or, for certain simple data distributions, to the high-dimensional asymptotics in which the number of samples scales linearly with the dimension ($m\\propto d$). There is a wide gulf between these two regimes, including all higher-order scaling relations $m\\propto d^r$, which are the subject of the present paper. We focus on the problem of kernel ridge regression for dot-product kernels and present precise formulas for the test error, bias, and variance, for data drawn uniformly from the sphere in the $r$th-order asymptotic scaling regime $m\\to\\infty$ with $m/d",
    "link": "http://arxiv.org/abs/2205.14846",
    "context": "Title: Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression. (arXiv:2205.14846v2 [cs.LG] UPDATED)\nAbstract: As modern machine learning models continue to advance the computational frontier, it has become increasingly important to develop precise estimates for expected performance improvements under different model and data scaling regimes. Currently, theoretical understanding of the learning curves that characterize how the prediction error depends on the number of samples is restricted to either large-sample asymptotics ($m\\to\\infty$) or, for certain simple data distributions, to the high-dimensional asymptotics in which the number of samples scales linearly with the dimension ($m\\propto d$). There is a wide gulf between these two regimes, including all higher-order scaling relations $m\\propto d^r$, which are the subject of the present paper. We focus on the problem of kernel ridge regression for dot-product kernels and present precise formulas for the test error, bias, and variance, for data drawn uniformly from the sphere in the $r$th-order asymptotic scaling regime $m\\to\\infty$ with $m/d",
    "path": "papers/22/05/2205.14846.json",
    "total_tokens": 899,
    "translated_title": "点积核回归的精确学习曲线和高阶标度极限",
    "translated_abstract": "随着现代机器学习模型不断推进计算前沿，开发对不同模型和数据缩放方案下预期性能提高的精确估计变得越来越重要。目前，关于描述预测误差如何随着样本数量而变化的学习曲线的理论理解受限于大样本渐近性 ($m\\to\\infty$) 或对于某些简单数据分布的高维渐近性，其中样本数量与维数成线性比例 ($m\\propto d$)。这两个范畴之间存在很大差距，包括所有高阶标度关系 $m\\propto d^r$，这是本文的研究对象。我们专注于点积核岭回归的问题，并提供了在 $m/d\\rightarrow2r$ 的 $r$ 阶渐近标度下（其中 $m\\to\\infty$），对于从球面上均匀抽取的数据，测试误差、偏差和方差的精确公式。",
    "tldr": "本文细致研究了点积核岭回归问题，针对 $m\\propto d^r$ 高阶标度关系提出了精确的测试误差、偏差和方差公式。",
    "en_tdlr": "This paper studies the problem of kernel ridge regression for dot-product kernels and provides precise formulas for test error, bias, and variance under the high-order scaling relation $m\\propto d^r$."
}