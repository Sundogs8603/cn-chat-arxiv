{
    "title": "Energy-efficient Deployment of Deep Learning Applications on Cortex-M based Microcontrollers using Deep Compression. (arXiv:2205.10369v2 [cs.LG] UPDATED)",
    "abstract": "Large Deep Neural Networks (DNNs) are the backbone of today's artificial intelligence due to their ability to make accurate predictions when being trained on huge datasets. With advancing technologies, such as the Internet of Things, interpreting large quantities of data generated by sensors is becoming an increasingly important task. However, in many applications not only the predictive performance but also the energy consumption of deep learning models is of major interest. This paper investigates the efficient deployment of deep learning models on resource-constrained microcontroller architectures via network compression. We present a methodology for the systematic exploration of different DNN pruning, quantization, and deployment strategies, targeting different ARM Cortex-M based low-power systems. The exploration allows to analyze trade-offs between key metrics such as accuracy, memory consumption, execution time, and power consumption. We discuss experimental results on three dif",
    "link": "http://arxiv.org/abs/2205.10369",
    "context": "Title: Energy-efficient Deployment of Deep Learning Applications on Cortex-M based Microcontrollers using Deep Compression. (arXiv:2205.10369v2 [cs.LG] UPDATED)\nAbstract: Large Deep Neural Networks (DNNs) are the backbone of today's artificial intelligence due to their ability to make accurate predictions when being trained on huge datasets. With advancing technologies, such as the Internet of Things, interpreting large quantities of data generated by sensors is becoming an increasingly important task. However, in many applications not only the predictive performance but also the energy consumption of deep learning models is of major interest. This paper investigates the efficient deployment of deep learning models on resource-constrained microcontroller architectures via network compression. We present a methodology for the systematic exploration of different DNN pruning, quantization, and deployment strategies, targeting different ARM Cortex-M based low-power systems. The exploration allows to analyze trade-offs between key metrics such as accuracy, memory consumption, execution time, and power consumption. We discuss experimental results on three dif",
    "path": "papers/22/05/2205.10369.json",
    "total_tokens": 881,
    "translated_title": "基于Cortex-M微控制器的能效高的深度学习应用部署使用深度压缩技术",
    "translated_abstract": "大规模深度神经网络(DNN)是当今人工智能的基石，因为它们在被训练大量数据集时能够做出准确的预测。随着物联网等技术的发展，解释传感器产生的大量数据正在成为一个越来越重要的任务。然而，在许多应用中，不仅预测性能，还有深度学习模型的能耗也是一个主要关注点。本文通过网络压缩研究了在资源受限的微控制器架构上高效部署深度学习模型的方法。我们提出了一种系统性的方法来探索不同的DNN修剪、量化和部署策略，针对不同的基于ARM Cortex-M的低功耗系统。探索过程可以分析准确性、内存消耗、执行时间和功耗等关键指标之间的权衡。我们讨论了在三种不同微控制器上的实验结果。",
    "tldr": "本研究通过深度压缩技术实现了在资源受限的微控制器上高效部署深度学习模型，分析了准确性、内存消耗、执行时间和功耗之间的权衡关系。",
    "en_tdlr": "This study achieves efficient deployment of deep learning models on resource-constrained microcontrollers using deep compression technique, analyzing the trade-offs between accuracy, memory consumption, execution time, and power consumption."
}