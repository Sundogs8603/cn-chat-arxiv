{
    "title": "Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting. (arXiv:2205.14568v3 [stat.ML] UPDATED)",
    "abstract": "Uncertainty quantification is crucial for assessing the predictive ability of AI algorithms. Much research has been devoted to describing the predictive distribution (PD) $F(y|\\mathbf{x})$ of a target variable $y \\in \\mathbb{R}$ given complex input features $\\mathbf{x} \\in \\mathcal{X}$. However, off-the-shelf PDs (from, e.g., normalizing flows and Bayesian neural networks) often lack conditional calibration with the probability of occurrence of an event given input $\\mathbf{x}$ being significantly different from the predicted probability. Current calibration methods do not fully assess and enforce conditionally calibrated PDs. Here we propose \\texttt{Cal-PIT}, a method that addresses both PD diagnostics and recalibration by learning a single probability-probability map from calibration data. The key idea is to regress probability integral transform scores against $\\mathbf{x}$. The estimated regression provides interpretable diagnostics of conditional coverage across the feature space. ",
    "link": "http://arxiv.org/abs/2205.14568",
    "context": "Title: Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting. (arXiv:2205.14568v3 [stat.ML] UPDATED)\nAbstract: Uncertainty quantification is crucial for assessing the predictive ability of AI algorithms. Much research has been devoted to describing the predictive distribution (PD) $F(y|\\mathbf{x})$ of a target variable $y \\in \\mathbb{R}$ given complex input features $\\mathbf{x} \\in \\mathcal{X}$. However, off-the-shelf PDs (from, e.g., normalizing flows and Bayesian neural networks) often lack conditional calibration with the probability of occurrence of an event given input $\\mathbf{x}$ being significantly different from the predicted probability. Current calibration methods do not fully assess and enforce conditionally calibrated PDs. Here we propose \\texttt{Cal-PIT}, a method that addresses both PD diagnostics and recalibration by learning a single probability-probability map from calibration data. The key idea is to regress probability integral transform scores against $\\mathbf{x}$. The estimated regression provides interpretable diagnostics of conditional coverage across the feature space. ",
    "path": "papers/22/05/2205.14568.json",
    "total_tokens": 953,
    "translated_title": "通过概率-概率映射实现有条件校准的预测分布：在银河红移估计和概率预测中的应用",
    "translated_abstract": "不确定性量化对于评估AI算法的预测能力至关重要。过去的研究致力于描述目标变量$y \\in \\mathbb{R}$在给定复杂输入特征$\\mathbf{x} \\in \\mathcal{X}$的条件下的预测分布$F(y|\\mathbf{x})$。然而，现有的预测分布（例如，归一化流和贝叶斯神经网络）往往缺乏条件校准，即给定输入$\\mathbf{x}$的事件发生的概率与预测概率显著不同。当前的校准方法不能完全评估和实施有条件校准的预测分布。在这里，我们提出了一种名为Cal-PIT的方法，它通过从校准数据中学习一个概率-概率映射来同时解决预测分布的诊断和校准问题。关键思想是对概率积分变换分数进行$\\mathbf{x}$的回归。估计的回归提供了对特征空间中条件覆盖的可解释诊断。",
    "tldr": "本研究提出了一种名为Cal-PIT的方法，通过学习一个概率-概率映射，解决了预测分布的诊断和校准问题，来实现有条件校准。",
    "en_tdlr": "This study proposes a method called Cal-PIT, which addresses both diagnostic and recalibration issues of predictive distributions by learning a probability-probability map, enabling conditional calibration."
}