{
    "title": "An Adaptive Incremental Gradient Method With Support for Non-Euclidean Norms. (arXiv:2205.02273v2 [math.OC] UPDATED)",
    "abstract": "Stochastic variance reduced methods have shown strong performance in solving finite-sum problems. However, these methods usually require the users to manually tune the step-size, which is time-consuming or even infeasible for some large-scale optimization tasks. To overcome the problem, we propose and analyze several novel adaptive variants of the popular SAGA algorithm. Eventually, we design a variant of Barzilai-Borwein step-size which is tailored for the incremental gradient method to ensure memory efficiency and fast convergence. We establish its convergence guarantees under general settings that allow non-Euclidean norms in the definition of smoothness and the composite objectives, which cover a broad range of applications in machine learning. We improve the analysis of SAGA to support non-Euclidean norms, which fills the void of existing work. Numerical experiments on standard datasets demonstrate a competitive performance of the proposed algorithm compared with existing variance",
    "link": "http://arxiv.org/abs/2205.02273",
    "context": "Title: An Adaptive Incremental Gradient Method With Support for Non-Euclidean Norms. (arXiv:2205.02273v2 [math.OC] UPDATED)\nAbstract: Stochastic variance reduced methods have shown strong performance in solving finite-sum problems. However, these methods usually require the users to manually tune the step-size, which is time-consuming or even infeasible for some large-scale optimization tasks. To overcome the problem, we propose and analyze several novel adaptive variants of the popular SAGA algorithm. Eventually, we design a variant of Barzilai-Borwein step-size which is tailored for the incremental gradient method to ensure memory efficiency and fast convergence. We establish its convergence guarantees under general settings that allow non-Euclidean norms in the definition of smoothness and the composite objectives, which cover a broad range of applications in machine learning. We improve the analysis of SAGA to support non-Euclidean norms, which fills the void of existing work. Numerical experiments on standard datasets demonstrate a competitive performance of the proposed algorithm compared with existing variance",
    "path": "papers/22/05/2205.02273.json",
    "total_tokens": 972,
    "translated_title": "支持非欧几里德范数的自适应增量梯度法",
    "translated_abstract": "随机方差降低方法在解决有限和问题时表现出很强的性能。然而，这些方法通常需要用户手动调整步长，这对于一些大规模优化任务来说是耗时甚至不可行的。为了解决这个问题，我们提出并分析了几种新型的自适应变种SAGA算法。最终，我们设计了一种针对增量梯度法的Barzilai-Borwein步长变体，以确保内存效率和快速收敛。我们在允许在平滑度和复合目标的定义中使用非欧几里德范数的一般设置下建立其收敛性保证，这涵盖了机器学习中广泛应用的广泛范围应用。我们改进了SAGA的分析，以支持非欧几里德范数，填补了现有工作的空白。标准数据集上的数值实验表明，与现有方差方法相比，所提出的算法具有竞争力的性能。",
    "tldr": "我们提出了一个自适应增量梯度法来解决大规模优化任务中手动调整步长的问题，并设计了针对增量梯度法的Barzilai-Borwein步长变体。我们通过支持非欧几里德范数填补了现有SAGA算法分析的空白，并在广泛的机器学习应用中得到了验证。",
    "en_tdlr": "We propose an adaptive incremental gradient method to address the issue of manually tuning step-size in large-scale optimization tasks and design a variant of Barzilai-Borwein step-size tailored for the incremental gradient method. We fill the gap in the existing SAGA algorithm analysis by supporting non-Euclidean norms and validate it in a wide range of machine learning applications."
}