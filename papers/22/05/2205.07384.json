{
    "title": "Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel. (arXiv:2205.07384v7 [cs.LG] UPDATED)",
    "abstract": "It is challenging to guide neural network (NN) learning with prior knowledge. In contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a Gaussian process (GP). Many deep learning applications could be enhanced by modeling such known properties. For example, convolutional neural networks (CNNs) are frequently used in remote sensing, which is subject to strong seasonal effects. We propose to blend the strengths of deep learning and the clear modeling capabilities of GPs by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). We implement this idea by combining a deep network and an efficient mapping based on the Nystrom approximation, which we call Implicit Composite Kernel (ICK). We then adopt a sample-then-optimize approach to approximate the full GP posterior distribution. We demons",
    "link": "http://arxiv.org/abs/2205.07384",
    "context": "Title: Incorporating Prior Knowledge into Neural Networks through an Implicit Composite Kernel. (arXiv:2205.07384v7 [cs.LG] UPDATED)\nAbstract: It is challenging to guide neural network (NN) learning with prior knowledge. In contrast, many known properties, such as spatial smoothness or seasonality, are straightforward to model by choosing an appropriate kernel in a Gaussian process (GP). Many deep learning applications could be enhanced by modeling such known properties. For example, convolutional neural networks (CNNs) are frequently used in remote sensing, which is subject to strong seasonal effects. We propose to blend the strengths of deep learning and the clear modeling capabilities of GPs by using a composite kernel that combines a kernel implicitly defined by a neural network with a second kernel function chosen to model known properties (e.g., seasonality). We implement this idea by combining a deep network and an efficient mapping based on the Nystrom approximation, which we call Implicit Composite Kernel (ICK). We then adopt a sample-then-optimize approach to approximate the full GP posterior distribution. We demons",
    "path": "papers/22/05/2205.07384.json",
    "total_tokens": 922,
    "translated_title": "通过隐式复合核将先验知识融入神经网络",
    "translated_abstract": "引导神经网络（NN）学习以先验知识是具有挑战性的。相比之下，许多已知特性，如空间平滑性或季节性，在高斯过程（GP）中通过选择适当的核函数来建模是直接的。许多深度学习应用可以通过建模这些已知特性来改进。例如，卷积神经网络（CNNs）广泛用于遥感，这受到强烈的季节效应影响。我们提出通过使用由神经网络隐式定义的核函数与选择用于建模已知特性的第二个核函数（例如季节性）相结合的复合核来结合深度学习和GP的建模能力。我们通过将深度网络和基于Nystrom近似的高效映射相结合来实现这一想法，将其称为隐式复合核（ICK）。然后，我们采用样本优化的方法来近似完整的GP后验分布。我们证明了ICK的有效性，并在遥感和时间序列的任务上进行了实验。",
    "tldr": "本论文提出了一种通过深度学习和高斯过程的复合核来将先验知识融入神经网络的方法。通过隐式定义的神经网络核函数和选择的第二个核函数，可以模拟已知特性，并提高深度学习应用的性能。",
    "en_tdlr": "This paper proposes a method of incorporating prior knowledge into neural networks through a composite kernel that combines deep learning and Gaussian processes. By using an implicitly defined neural network kernel and a chosen second kernel function, known properties can be modeled, enhancing the performance of deep learning applications."
}