{
    "title": "Engineering flexible machine learning systems by traversing functionally-invariant paths. (arXiv:2205.00334v4 [cs.LG] UPDATED)",
    "abstract": "Transformers have emerged as the state of the art neural network architecture for natural language processing and computer vision. In the foundation model paradigm, large transformer models (BERT, GPT3/4, Bloom, ViT) are pre-trained on self-supervised tasks such as word or image masking, and then, adapted through fine-tuning for downstream user applications including instruction following and Question Answering. While many approaches have been developed for model fine-tuning including low-rank weight update strategies (eg. LoRA), underlying mathematical principles that enable network adaptation without knowledge loss remain poorly understood. Here, we introduce a differential geometry framework, functionally invariant paths (FIP), that provides flexible and continuous adaptation of neural networks for a range of machine learning goals and network sparsification objectives. We conceptualize the weight space of a neural network as a curved Riemannian manifold equipped with a metric tenso",
    "link": "http://arxiv.org/abs/2205.00334",
    "context": "Title: Engineering flexible machine learning systems by traversing functionally-invariant paths. (arXiv:2205.00334v4 [cs.LG] UPDATED)\nAbstract: Transformers have emerged as the state of the art neural network architecture for natural language processing and computer vision. In the foundation model paradigm, large transformer models (BERT, GPT3/4, Bloom, ViT) are pre-trained on self-supervised tasks such as word or image masking, and then, adapted through fine-tuning for downstream user applications including instruction following and Question Answering. While many approaches have been developed for model fine-tuning including low-rank weight update strategies (eg. LoRA), underlying mathematical principles that enable network adaptation without knowledge loss remain poorly understood. Here, we introduce a differential geometry framework, functionally invariant paths (FIP), that provides flexible and continuous adaptation of neural networks for a range of machine learning goals and network sparsification objectives. We conceptualize the weight space of a neural network as a curved Riemannian manifold equipped with a metric tenso",
    "path": "papers/22/05/2205.00334.json",
    "total_tokens": 866,
    "translated_title": "通过遍历功能不变路径，构建灵活的机器学习系统",
    "translated_abstract": "变压器已成为自然语言处理和计算机视觉中最先进的神经网络架构。在基础模型范例中，大型变压器模型（BERT、GPT3/4、Bloom、ViT）通过自监督任务（如词或图像屏蔽）进行预训练，然后通过微调适应于下游用户应用，包括指令跟随和问答。虽然有许多模型微调方法（如低秩权重更新策略，如LoRA），但仍然对实现网络适应性而不损失知识的数学原理知之甚少。在这里，我们引入了一个差分几何框架，功能不变路径（FIP），为一系列机器学习目标和网络稀疏化目标提供灵活和连续的神经网络适应。我们将神经网络的权重空间构想为一个曲率的黎曼流形，并配备了一个度规张量。",
    "tldr": "该论文介绍了一个名为功能不变路径（FIP）的差分几何框架，用于实现神经网络的灵活、连续适应，以应对各种机器学习目标和网络稀疏化目标。"
}