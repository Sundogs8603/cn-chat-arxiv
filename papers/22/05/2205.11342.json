{
    "title": "The Diminishing Returns of Masked Language Models to Science. (arXiv:2205.11342v2 [cs.CL] UPDATED)",
    "abstract": "Transformer-based masked language models such as BERT, trained on general corpora, have shown impressive performance on downstream tasks. It has also been demonstrated that the downstream task performance of such models can be improved by pretraining larger models for longer on more data. In this work, we empirically evaluate the extent to which these results extend to tasks in science. We use 14 domain-specific transformer-based models (including ScholarBERT, a new 770M-parameter science-focused masked language model pretrained on up to 225B tokens) to evaluate the impact of training data, model size, pretraining and finetuning time on 12 downstream scientific tasks. Interestingly, we find that increasing model sizes, training data, or compute time does not always lead to significant improvements (i.e., >1% F1), if at all, in scientific information extraction tasks and offered possible explanations for the surprising performance differences.",
    "link": "http://arxiv.org/abs/2205.11342",
    "context": "Title: The Diminishing Returns of Masked Language Models to Science. (arXiv:2205.11342v2 [cs.CL] UPDATED)\nAbstract: Transformer-based masked language models such as BERT, trained on general corpora, have shown impressive performance on downstream tasks. It has also been demonstrated that the downstream task performance of such models can be improved by pretraining larger models for longer on more data. In this work, we empirically evaluate the extent to which these results extend to tasks in science. We use 14 domain-specific transformer-based models (including ScholarBERT, a new 770M-parameter science-focused masked language model pretrained on up to 225B tokens) to evaluate the impact of training data, model size, pretraining and finetuning time on 12 downstream scientific tasks. Interestingly, we find that increasing model sizes, training data, or compute time does not always lead to significant improvements (i.e., >1% F1), if at all, in scientific information extraction tasks and offered possible explanations for the surprising performance differences.",
    "path": "papers/22/05/2205.11342.json",
    "total_tokens": 905,
    "translated_title": "掩码语言模型在科学中的减弱收益",
    "translated_abstract": "基于Transformer的掩码语言模型，如BERT，在一般语料库上训练后在下游任务中表现出色。还已经证明这些模型的下游任务性能可以通过在更多数据上更长时间地预训练更大的模型来提高。在这项工作中，我们经验性地评估了这些结果在科学任务中的适用程度。我们使用了14个特定领域的基于Transformer的模型（包括ScholarBERT，一个新的 7.7 亿参数的科学聚焦掩码语言模型，预先训练了 高达 225B 个令牌），以评估训练数据、模型大小、预训练和微调时间对12个下游科学任务的影响。有趣的是，我们发现，在科学信息提取任务中，增加模型大小、训练数据或计算时间并不总是会导致显着的提高（即 >1% F1），如果有的话，可能会有出乎意料的性能差异，并提供了可能的解释。",
    "tldr": "本文从科学角度出发，评估了14个领域特定的基于Transformer的模型在12个下游科学任务中的表现，发现增加模型大小、训练数据或计算时间并不总是会导致显着提高，可能会有出乎意料的性能差异。",
    "en_tdlr": "This paper evaluates the performance of 14 domain-specific transformer-based models on 12 downstream scientific tasks, and finds that increasing model size, training data, or compute time does not always lead to significant improvements, and may lead to unexpected performance differences."
}