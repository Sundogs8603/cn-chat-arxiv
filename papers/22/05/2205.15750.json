{
    "title": "Variable importance without impossible data. (arXiv:2205.15750v3 [cs.LG] UPDATED)",
    "abstract": "The most popular methods for measuring importance of the variables in a black box prediction algorithm make use of synthetic inputs that combine predictor variables from multiple subjects. These inputs can be unlikely, physically impossible, or even logically impossible. As a result, the predictions for such cases can be based on data very unlike any the black box was trained on. We think that users cannot trust an explanation of the decision of a prediction algorithm when the explanation uses such values. Instead we advocate a method called Cohort Shapley that is grounded in economic game theory and unlike most other game theoretic methods, it uses only actually observed data to quantify variable importance. Cohort Shapley works by narrowing the cohort of subjects judged to be similar to a target subject on one or more features. We illustrate it on an algorithmic fairness problem where it is essential to attribute importance to protected variables that the model was not trained on.",
    "link": "http://arxiv.org/abs/2205.15750",
    "context": "Title: Variable importance without impossible data. (arXiv:2205.15750v3 [cs.LG] UPDATED)\nAbstract: The most popular methods for measuring importance of the variables in a black box prediction algorithm make use of synthetic inputs that combine predictor variables from multiple subjects. These inputs can be unlikely, physically impossible, or even logically impossible. As a result, the predictions for such cases can be based on data very unlike any the black box was trained on. We think that users cannot trust an explanation of the decision of a prediction algorithm when the explanation uses such values. Instead we advocate a method called Cohort Shapley that is grounded in economic game theory and unlike most other game theoretic methods, it uses only actually observed data to quantify variable importance. Cohort Shapley works by narrowing the cohort of subjects judged to be similar to a target subject on one or more features. We illustrate it on an algorithmic fairness problem where it is essential to attribute importance to protected variables that the model was not trained on.",
    "path": "papers/22/05/2205.15750.json",
    "total_tokens": 902,
    "translated_title": "无需不可能数据的变量重要性分析方法",
    "translated_abstract": "目前，评估黑箱预测模型中变量重要性的最流行方法是使用人工合成的输入数据，这些数据结合了多个参与者的预测变量，这些输入数据可能是不可能的、物理上不可能的，甚至是逻辑上不可能的，由此得出的预测结果可能与黑箱训练数据有很大不同。因此，当解释决策时使用这些值时，用户不能信任预测算法的解释。我们提倡一种名为Cohort Shapley的方法，它基于经济博弈理论，与大多数其他博弈论方法不同，仅使用实际观测到的数据来量化变量重要性。Cohort Shapley通过缩小与目标对象在一个或多个特征上相似的对象组来实现。我们将其应用于一个算法公平性问题，其中必须将重要性归因于模型未经训练的受保护变量。",
    "tldr": "评估黑箱预测模型中变量重要性的流行方法不可信，因为使用了不可能的数据。本文提出一种名为Cohort Shapley的方法，它基于经济博弈理论，仅使用实际观测到的数据来量化变量重要性，可以解决算法公平性问题。",
    "en_tdlr": "The popular methods for evaluating variable importance in black box prediction models using synthetic inputs are unreliable as they use impossible data. This study proposes Cohort Shapley, a method based on economic game theory and using only observed data to quantify variable importance, to address algorithmic fairness problems in which protected variables that were not trained on need to be attributed importance."
}