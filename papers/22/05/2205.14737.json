{
    "title": "Stochastic Zeroth Order Gradient and Hessian Estimators: Variance Reduction and Refined Bias Bounds. (arXiv:2205.14737v3 [cs.LG] UPDATED)",
    "abstract": "We study stochastic zeroth order gradient and Hessian estimators for real-valued functions in $\\mathbb{R}^n$. We show that, via taking finite difference along random orthogonal directions, the variance of the stochastic finite difference estimators can be significantly reduced. In particular, we design estimators for smooth functions such that, if one uses $ \\Theta \\left( k \\right) $ random directions sampled from the Stiefel's manifold $ \\text{St} (n,k) $ and finite-difference granularity $\\delta$, the variance of the gradient estimator is bounded by $ \\mathcal{O} \\left( \\left( \\frac{n}{k} - 1 \\right) + \\left( \\frac{n^2}{k} - n \\right) \\delta^2 + \\frac{ n^2 \\delta^4 }{ k } \\right) $, and the variance of the Hessian estimator is bounded by $\\mathcal{O} \\left( \\left( \\frac{n^2}{k^2} - 1 \\right) + \\left( \\frac{n^4}{k^2} - n^2 \\right) \\delta^2 + \\frac{n^4 \\delta^4 }{k^2} \\right) $. When $k = n$, the variances become negligibly small. In addition, we provide improved bias bounds for the es",
    "link": "http://arxiv.org/abs/2205.14737",
    "context": "Title: Stochastic Zeroth Order Gradient and Hessian Estimators: Variance Reduction and Refined Bias Bounds. (arXiv:2205.14737v3 [cs.LG] UPDATED)\nAbstract: We study stochastic zeroth order gradient and Hessian estimators for real-valued functions in $\\mathbb{R}^n$. We show that, via taking finite difference along random orthogonal directions, the variance of the stochastic finite difference estimators can be significantly reduced. In particular, we design estimators for smooth functions such that, if one uses $ \\Theta \\left( k \\right) $ random directions sampled from the Stiefel's manifold $ \\text{St} (n,k) $ and finite-difference granularity $\\delta$, the variance of the gradient estimator is bounded by $ \\mathcal{O} \\left( \\left( \\frac{n}{k} - 1 \\right) + \\left( \\frac{n^2}{k} - n \\right) \\delta^2 + \\frac{ n^2 \\delta^4 }{ k } \\right) $, and the variance of the Hessian estimator is bounded by $\\mathcal{O} \\left( \\left( \\frac{n^2}{k^2} - 1 \\right) + \\left( \\frac{n^4}{k^2} - n^2 \\right) \\delta^2 + \\frac{n^4 \\delta^4 }{k^2} \\right) $. When $k = n$, the variances become negligibly small. In addition, we provide improved bias bounds for the es",
    "path": "papers/22/05/2205.14737.json",
    "total_tokens": 1092,
    "translated_title": "随机零阶梯度和Hessian估计：方差约简和精细偏差边界",
    "translated_abstract": "我们研究了实值函数在$\\mathbb{R}^n$中的随机零阶梯度和Hessian估计。我们展示了，通过沿着随机正交方向进行有限差分，可以大大减少随机有限差分估计器的方差。特别地，我们为光滑函数设计了估计器，如果从Stiefel流形$\\text{St}(n,k)$中采样$ \\Theta \\left( k \\right) $随机方向，并且采用有限差分粒度$\\delta$，则梯度估计器的方差受到 $ \\mathcal{O} \\left( \\left( \\frac{n}{k} - 1 \\right) + \\left( \\frac{n^2}{k} - n \\right) \\delta^2 + \\frac{ n^2 \\delta^4 }{ k } \\right) $的约束，Hessian估计器的方差受到 $\\mathcal{O} \\left( \\left( \\frac{n^2}{k^2} - 1 \\right) + \\left( \\frac{n^4}{k^2} - n^2 \\right) \\delta^2 + \\frac{n^4 \\delta^4 }{k^2} \\right) $的限制。当$k=n$时，方差变得非常小。此外，我们提供了改进的偏差边界。",
    "tldr": "研究了随机零阶梯度和Hessian估计，通过随机正交方向进行有限差分来减少方差。除了提供方差的约束，还提供改进的偏差边界。"
}