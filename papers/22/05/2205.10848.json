{
    "title": "Robust Quantity-Aware Aggregation for Federated Learning. (arXiv:2205.10848v2 [cs.CR] UPDATED)",
    "abstract": "Federated learning (FL) enables multiple clients to collaboratively train models without sharing their local data, and becomes an important privacy-preserving machine learning framework. However, classical FL faces serious security and robustness problem, e.g., malicious clients can poison model updates and at the same time claim large quantities to amplify the impact of their model updates in the model aggregation. Existing defense methods for FL, while all handling malicious model updates, either treat all quantities benign or simply ignore/truncate the quantities of all clients. The former is vulnerable to quantity-enhanced attack, while the latter leads to sub-optimal performance since the local data on different clients is usually in significantly different sizes. In this paper, we propose a robust quantity-aware aggregation algorithm for federated learning, called FedRA, to perform the aggregation with awareness of local data quantities while being able to defend against quantity",
    "link": "http://arxiv.org/abs/2205.10848",
    "context": "Title: Robust Quantity-Aware Aggregation for Federated Learning. (arXiv:2205.10848v2 [cs.CR] UPDATED)\nAbstract: Federated learning (FL) enables multiple clients to collaboratively train models without sharing their local data, and becomes an important privacy-preserving machine learning framework. However, classical FL faces serious security and robustness problem, e.g., malicious clients can poison model updates and at the same time claim large quantities to amplify the impact of their model updates in the model aggregation. Existing defense methods for FL, while all handling malicious model updates, either treat all quantities benign or simply ignore/truncate the quantities of all clients. The former is vulnerable to quantity-enhanced attack, while the latter leads to sub-optimal performance since the local data on different clients is usually in significantly different sizes. In this paper, we propose a robust quantity-aware aggregation algorithm for federated learning, called FedRA, to perform the aggregation with awareness of local data quantities while being able to defend against quantity",
    "path": "papers/22/05/2205.10848.json",
    "total_tokens": 855,
    "translated_title": "强健的数量感知聚合方法用于联邦学习",
    "translated_abstract": "联邦学习（FL）是一种能够使多个客户端协同训练模型而不共享本地数据的隐私保护机器学习框架。然而，传统的FL面临严重的安全性和鲁棒性问题，例如，恶意的客户端可以污染模型更新，并同时虚报大量以放大其在模型聚合中的影响。现有的FL防御方法，虽然都能处理恶意的模型更新，但要么将所有数量视为良性，要么简单地忽略/截断所有客户端的数量。前者容易受到数量增强攻击的影响，而后者会导致子优的性能，因为不同客户端上的本地数据通常具有显着不同的大小。在本文中，我们提出了一种 robust quantity-aware aggregation algorithm for federated learning (FedRA)，通过对本地数据数量的感知来执行聚合，并能够抵御数量增强攻击。",
    "tldr": "本文提出了一种针对联邦学习的强健的数量感知聚合算法，名为FedRA，能够在聚合模型时考虑本地数据的数量，并能够抵御数量增强攻击。",
    "en_tdlr": "This paper proposes a robust quantity-aware aggregation algorithm, FedRA, for federated learning, which takes into account the quantity of local data when aggregating models and can defend against quantity-enhanced attacks."
}