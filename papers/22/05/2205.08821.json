{
    "title": "Lessons Learned: Defending Against Property Inference Attacks. (arXiv:2205.08821v4 [cs.CR] UPDATED)",
    "abstract": "This work investigates and evaluates multiple defense strategies against property inference attacks (PIAs), a privacy attack against machine learning models. Given a trained machine learning model, PIAs aim to extract statistical properties of its underlying training data, e.g., reveal the ratio of men and women in a medical training data set. While for other privacy attacks like membership inference, a lot of research on defense mechanisms has been published, this is the first work focusing on defending against PIAs. With the primary goal of developing a generic mitigation strategy against white-box PIAs, we propose the novel approach property unlearning. Extensive experiments with property unlearning show that while it is very effective when defending target models against specific adversaries, property unlearning is not able to generalize, i.e., protect against a whole class of PIAs. To investigate the reasons behind this limitation, we present the results of experiments with the ex",
    "link": "http://arxiv.org/abs/2205.08821",
    "context": "Title: Lessons Learned: Defending Against Property Inference Attacks. (arXiv:2205.08821v4 [cs.CR] UPDATED)\nAbstract: This work investigates and evaluates multiple defense strategies against property inference attacks (PIAs), a privacy attack against machine learning models. Given a trained machine learning model, PIAs aim to extract statistical properties of its underlying training data, e.g., reveal the ratio of men and women in a medical training data set. While for other privacy attacks like membership inference, a lot of research on defense mechanisms has been published, this is the first work focusing on defending against PIAs. With the primary goal of developing a generic mitigation strategy against white-box PIAs, we propose the novel approach property unlearning. Extensive experiments with property unlearning show that while it is very effective when defending target models against specific adversaries, property unlearning is not able to generalize, i.e., protect against a whole class of PIAs. To investigate the reasons behind this limitation, we present the results of experiments with the ex",
    "path": "papers/22/05/2205.08821.json",
    "total_tokens": 904,
    "translated_title": "《经验教训：抵御属性推断攻击》",
    "translated_abstract": "本研究探讨和评估多种防御策略来对抗属性推断攻击（PIA），这是一种针对机器学习模型的隐私攻击。在给定一个训练好的机器学习模型的情况下，PIA旨在提取其底层训练数据的统计属性，例如揭示医疗数据集中男性和女性的比例。虽然针对其他隐私攻击，例如成员推断，已经有很多关于防御机制的研究发表，但这是第一个专注于防御PIA的工作。我们的主要目标是开发一种通用的抵御白盒PIA的策略，我们提出了一种新颖的方法-属性遗忘。通过大量的属性遗忘实验，我们发现虽然属性遗忘对于针对特定对手的目标模型的防御非常有效，但无法概括，即无法保护整个PIA类别。为了探究这种限制的原因，我们展示了在实验中的结果。",
    "tldr": "本研究提出了一种新颖的方法——属性遗忘来对抗属性推断攻击，但发现该方法虽然对于特定对手的目标模型防御非常有效，但无法对抗整个PIA类别。",
    "en_tdlr": "This paper proposes a novel approach, property unlearning, to defend against property inference attacks (PIAs), but finds that while effective against specific adversaries, it cannot generalize to protect against the entire class of PIAs."
}