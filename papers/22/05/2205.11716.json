{
    "title": "Randomly Initialized One-Layer Neural Networks Make Data Linearly Separable. (arXiv:2205.11716v2 [cs.LG] UPDATED)",
    "abstract": "Recently, neural networks have demonstrated remarkable capabilities in mapping two arbitrary sets to two linearly separable sets. The prospect of achieving this with randomly initialized neural networks is particularly appealing due to the computational efficiency compared to fully trained networks. This paper contributes by establishing that, given sufficient width, a randomly initialized one-layer neural network can, with high probability, transform two sets into two linearly separable sets without any training. Moreover, we furnish precise bounds on the necessary width of the neural network for this phenomenon to occur. Our initial bound exhibits exponential dependence on the input dimension while maintaining polynomial dependence on all other parameters. In contrast, our second bound is independent of input dimension, effectively surmounting the curse of dimensionality. The main tools used in our proof heavily relies on a fusion of geometric principles and concentration of random m",
    "link": "http://arxiv.org/abs/2205.11716",
    "context": "Title: Randomly Initialized One-Layer Neural Networks Make Data Linearly Separable. (arXiv:2205.11716v2 [cs.LG] UPDATED)\nAbstract: Recently, neural networks have demonstrated remarkable capabilities in mapping two arbitrary sets to two linearly separable sets. The prospect of achieving this with randomly initialized neural networks is particularly appealing due to the computational efficiency compared to fully trained networks. This paper contributes by establishing that, given sufficient width, a randomly initialized one-layer neural network can, with high probability, transform two sets into two linearly separable sets without any training. Moreover, we furnish precise bounds on the necessary width of the neural network for this phenomenon to occur. Our initial bound exhibits exponential dependence on the input dimension while maintaining polynomial dependence on all other parameters. In contrast, our second bound is independent of input dimension, effectively surmounting the curse of dimensionality. The main tools used in our proof heavily relies on a fusion of geometric principles and concentration of random m",
    "path": "papers/22/05/2205.11716.json",
    "total_tokens": 807,
    "translated_title": "随机初始化的单层神经网络能够使数据线性可分",
    "translated_abstract": "最近，神经网络在将两个任意集合映射为两个线性可分集合方面展示出了显著的能力。相比完全训练的网络，随机初始化的神经网络具有计算效率上的吸引力。本文的贡献在于建立了在足够宽度的情况下，随机初始化的单层神经网络有很高的概率能够将两个集合转化为线性可分的集合，而无需任何训练。此外，我们给出了神经网络必要宽度的精确界限。我们的初始界限在输入维度上呈指数依赖关系，同时在其他参数上呈多项式依赖关系。相反，我们的第二个界限与输入维度无关，有效地克服了维度灾难。我们证明中使用的主要工具在很大程度上依赖于几何原理和随机集中性。",
    "tldr": "随机初始化的单层神经网络可以将两个集合转化为线性可分的集合，而无需训练，具有计算效率高的优点。",
    "en_tdlr": "Randomly initialized one-layer neural networks can transform two sets into linearly separable sets without training, providing computational efficiency."
}