{
    "title": "MixFlows: principled variational inference via mixed flows. (arXiv:2205.07475v5 [stat.ML] UPDATED)",
    "abstract": "This work presents mixed variational flows (MixFlows), a new variational family that consists of a mixture of repeated applications of a map to an initial reference distribution. First, we provide efficient algorithms for i.i.d. sampling, density evaluation, and unbiased ELBO estimation. We then show that MixFlows have MCMC-like convergence guarantees when the flow map is ergodic and measure-preserving, and provide bounds on the accumulation of error for practical implementations where the flow map is approximated. Finally, we develop an implementation of MixFlows based on uncorrected discretized Hamiltonian dynamics combined with deterministic momentum refreshment. Simulated and real data experiments show that MixFlows can provide more reliable posterior approximations than several black-box normalizing flows, as well as samples of comparable quality to those obtained from state-of-the-art MCMC methods.",
    "link": "http://arxiv.org/abs/2205.07475",
    "context": "Title: MixFlows: principled variational inference via mixed flows. (arXiv:2205.07475v5 [stat.ML] UPDATED)\nAbstract: This work presents mixed variational flows (MixFlows), a new variational family that consists of a mixture of repeated applications of a map to an initial reference distribution. First, we provide efficient algorithms for i.i.d. sampling, density evaluation, and unbiased ELBO estimation. We then show that MixFlows have MCMC-like convergence guarantees when the flow map is ergodic and measure-preserving, and provide bounds on the accumulation of error for practical implementations where the flow map is approximated. Finally, we develop an implementation of MixFlows based on uncorrected discretized Hamiltonian dynamics combined with deterministic momentum refreshment. Simulated and real data experiments show that MixFlows can provide more reliable posterior approximations than several black-box normalizing flows, as well as samples of comparable quality to those obtained from state-of-the-art MCMC methods.",
    "path": "papers/22/05/2205.07475.json",
    "total_tokens": 914,
    "translated_title": "混合流：基于混合流的原则变分推理",
    "translated_abstract": "本文提出了混合变分流（MixFlows），这是一种新的变分家族，由对初始参考分布的映射重复应用的混合组成。首先，我们提供了有效的算法，用于i.i.d.采样、密度评估和无偏ELBO估计。然后，我们证明了当流映射是遍历和保度量的时，MixFlows具有类似于MCMC的收敛保证，并为流映射近似实现提供了误差积累的界限。最后，我们基于未纠正的离散哈密顿动力学和确定性动量恢复开发了 MixFlows 的实现。模拟和实际数据实验表明，MixFlows可以提供比几种黑盒归一化流更可靠的后验逼近，以及与最先进的MCMC方法所获得的样本质量相当的样本。",
    "tldr": "本文提出了混合变分流（MixFlows），是由对初始参考分布的映射重复应用的混合组成的一种新的变分家族。MixFlows具有类似于MCMC的收敛保证，并可以提供比几种黑盒归一化流更可靠的后验逼近，与最先进的MCMC方法所获得的样本质量相当。",
    "en_tdlr": "This work proposes a new variational family called MixFlows, which consists of a mixture of repeated applications of a map to an initial reference distribution. MixFlows have MCMC-like convergence guarantees and are shown to provide more reliable posterior approximations than several black-box normalizing flows, with samples comparable to state-of-the-art MCMC methods."
}