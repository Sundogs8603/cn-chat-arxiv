{
    "title": "Provably Safe Reinforcement Learning: A Theoretical and Experimental Comparison. (arXiv:2205.06750v2 [cs.LG] UPDATED)",
    "abstract": "Ensuring safety of reinforcement learning (RL) algorithms is crucial to unlock their potential for many real-world tasks. However, vanilla RL does not guarantee safety. In recent years, several methods have been proposed to provide safety guarantees for RL by design. Yet, there is no comprehensive comparison of these provably safe RL methods. We therefore introduce a categorization of existing provably safe RL methods, present the theoretical foundations for both continuous and discrete action spaces, and benchmark the methods' performance empirically. The methods are categorized based on how the action is adapted by the safety method: action replacement, action projection, and action masking. Our experiments on an inverted pendulum and quadrotor stabilization task show that all provably safe methods are indeed always safe. Furthermore, their trained performance is comparable to unsafe baselines. The benchmarking suggests that different provably safe RL approaches should be selected de",
    "link": "http://arxiv.org/abs/2205.06750",
    "context": "Title: Provably Safe Reinforcement Learning: A Theoretical and Experimental Comparison. (arXiv:2205.06750v2 [cs.LG] UPDATED)\nAbstract: Ensuring safety of reinforcement learning (RL) algorithms is crucial to unlock their potential for many real-world tasks. However, vanilla RL does not guarantee safety. In recent years, several methods have been proposed to provide safety guarantees for RL by design. Yet, there is no comprehensive comparison of these provably safe RL methods. We therefore introduce a categorization of existing provably safe RL methods, present the theoretical foundations for both continuous and discrete action spaces, and benchmark the methods' performance empirically. The methods are categorized based on how the action is adapted by the safety method: action replacement, action projection, and action masking. Our experiments on an inverted pendulum and quadrotor stabilization task show that all provably safe methods are indeed always safe. Furthermore, their trained performance is comparable to unsafe baselines. The benchmarking suggests that different provably safe RL approaches should be selected de",
    "path": "papers/22/05/2205.06750.json",
    "total_tokens": 889,
    "translated_title": "可证明安全的强化学习：理论和实验比较",
    "translated_abstract": "确保强化学习（RL）算法的安全性对于开发其在许多实际任务中的潜力至关重要。然而，普通的RL并不保证安全。近年来，已经提出了几种方法来通过设计来提供RL的安全保证。然而，这些可证明安全的RL方法还没有进行全面比较。因此，我们介绍了现有可证明安全的RL方法的分类，介绍了连续和离散动作空间的理论基础，并在实验中对这些方法的性能进行了评估。这些方法根据安全方法如何适应动作进行分类：动作替换、动作投影和动作掩蔽。我们在倒立摆和四旋翼稳定任务上的实验表明，所有可证明安全的方法确实是安全的。此外，它们的训练表现可与不安全的基线相媲美。基准测试表明，应选择不同的可证明安全的RL方法。",
    "tldr": "该论文介绍了现有可证明安全的RL方法的分类，并在倒立摆和四旋翼稳定任务上进行了实验，证明这些方法都是安全的且表现与不安全的方法相媲美。",
    "en_tdlr": "This paper introduces a categorization of existing provably safe RL methods, presents the theoretical foundations for both continuous and discrete action spaces, and benchmarks the methods' performance empirically on inverted pendulum and quadrotor stabilization tasks, showing that all methods are safe and perform comparably to unsafe baselines."
}