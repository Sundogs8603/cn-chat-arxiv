{
    "title": "Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification. (arXiv:2205.13094v4 [cs.LG] UPDATED)",
    "abstract": "While a broad range of techniques have been proposed to tackle distribution shift, the simple baseline of training on an $\\textit{undersampled}$ balanced dataset often achieves close to state-of-the-art-accuracy across several popular benchmarks. This is rather surprising, since undersampling algorithms discard excess majority group data. To understand this phenomenon, we ask if learning is fundamentally constrained by a lack of minority group samples. We prove that this is indeed the case in the setting of nonparametric binary classification. Our results show that in the worst case, an algorithm cannot outperform undersampling unless there is a high degree of overlap between the train and test distributions (which is unlikely to be the case in real-world datasets), or if the algorithm leverages additional structure about the distribution shift. In particular, in the case of label shift we show that there is always an undersampling algorithm that is minimax optimal. In the case of grou",
    "link": "http://arxiv.org/abs/2205.13094",
    "context": "Title: Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification. (arXiv:2205.13094v4 [cs.LG] UPDATED)\nAbstract: While a broad range of techniques have been proposed to tackle distribution shift, the simple baseline of training on an $\\textit{undersampled}$ balanced dataset often achieves close to state-of-the-art-accuracy across several popular benchmarks. This is rather surprising, since undersampling algorithms discard excess majority group data. To understand this phenomenon, we ask if learning is fundamentally constrained by a lack of minority group samples. We prove that this is indeed the case in the setting of nonparametric binary classification. Our results show that in the worst case, an algorithm cannot outperform undersampling unless there is a high degree of overlap between the train and test distributions (which is unlikely to be the case in real-world datasets), or if the algorithm leverages additional structure about the distribution shift. In particular, in the case of label shift we show that there is always an undersampling algorithm that is minimax optimal. In the case of grou",
    "path": "papers/22/05/2205.13094.json",
    "total_tokens": 960,
    "translated_title": "非参数分类中的欠采样是一种极小化极差风险的鲁棒性干预方法",
    "translated_abstract": "尽管已经提出了广泛的技术来解决分布偏移问题，但在几个流行的基准测试中，基于欠采样的平衡数据集的训练通常能够实现接近最先进准确性。我们证明了在非参数二元分类设置下，学习的基本限制是由于缺乏少数群体样本而产生的。我们的结果表明，除非训练和测试分布之间存在高度重叠（这在真实数据集中不太可能），否则算法无法超越欠采样，除非算法利用有关分布偏移的其他结构。特别地，在标签转移的情况下，我们证明了总是存在一种最小化极差风险的欠采样算法。在组转换的情况下，我们介绍了一类最小极差风险的欠采样算法。我们在合成和真实数据集上进行了验证实验以验证我们的理论结果。",
    "tldr": "该论文证明在非参数二元分类中，缺乏少数派样本是学习的根本限制，并探讨了欠采样算法的最小化极差风险的鲁棒性表现，特别是在标签转移的情况下可以最优化。",
    "en_tdlr": "This paper proves that the lack of minority samples is a fundamental limitation in nonparametric binary classification and explores the robustness of undersampling algorithms in minimizing worst-case risk. Specifically, it shows that undersampling is minimax optimal in label shift and introduces a family of minimax optimal undersampling algorithms in group shift."
}