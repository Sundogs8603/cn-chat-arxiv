{
    "title": "Posterior and Computational Uncertainty in Gaussian Processes. (arXiv:2205.15449v4 [cs.LG] UPDATED)",
    "abstract": "Gaussian processes scale prohibitively with the size of the dataset. In response, many approximation methods have been developed, which inevitably introduce approximation error. This additional source of uncertainty, due to limited computation, is entirely ignored when using the approximate posterior. Therefore in practice, GP models are often as much about the approximation method as they are about the data. Here, we develop a new class of methods that provides consistent estimation of the combined uncertainty arising from both the finite number of data observed and the finite amount of computation expended. The most common GP approximations map to an instance in this class, such as methods based on the Cholesky factorization, conjugate gradients, and inducing points. For any method in this class, we prove (i) convergence of its posterior mean in the associated RKHS, (ii) decomposability of its combined posterior covariance into mathematical and computational covariances, and (iii) th",
    "link": "http://arxiv.org/abs/2205.15449",
    "context": "Title: Posterior and Computational Uncertainty in Gaussian Processes. (arXiv:2205.15449v4 [cs.LG] UPDATED)\nAbstract: Gaussian processes scale prohibitively with the size of the dataset. In response, many approximation methods have been developed, which inevitably introduce approximation error. This additional source of uncertainty, due to limited computation, is entirely ignored when using the approximate posterior. Therefore in practice, GP models are often as much about the approximation method as they are about the data. Here, we develop a new class of methods that provides consistent estimation of the combined uncertainty arising from both the finite number of data observed and the finite amount of computation expended. The most common GP approximations map to an instance in this class, such as methods based on the Cholesky factorization, conjugate gradients, and inducing points. For any method in this class, we prove (i) convergence of its posterior mean in the associated RKHS, (ii) decomposability of its combined posterior covariance into mathematical and computational covariances, and (iii) th",
    "path": "papers/22/05/2205.15449.json",
    "total_tokens": 819,
    "tldr": "本文提出了一种新的方法，能够一致地估计由于观察到的有限数据和计算量的有限量引起的组合不确定性。"
}