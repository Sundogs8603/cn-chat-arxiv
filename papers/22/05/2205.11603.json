{
    "title": "Representation Projection Invariance Mitigates Representation Collapse. (arXiv:2205.11603v2 [cs.CL] UPDATED)",
    "abstract": "Fine-tuning contextualized representations learned by pre-trained language models remains a prevalent practice in NLP. However, fine-tuning can lead to representation degradation (also known as representation collapse), which may result in instability, sub-optimal performance, and weak generalization.  In this paper, we propose Representation Projection Invariance (REPINA), a novel regularization method to maintain the information content of representation and reduce representation collapse during fine-tuning by discouraging undesirable changes in the representations. We study the empirical behavior of the proposed regularization in comparison to 5 comparable baselines across 13 language understanding tasks (GLUE benchmark and six additional datasets). When evaluating in-domain performance, REPINA consistently outperforms other baselines on most tasks (10 out of 13). We also demonstrate its effectiveness in few-shot settings and robustness to label perturbation. As a by-product, we ext",
    "link": "http://arxiv.org/abs/2205.11603",
    "context": "Title: Representation Projection Invariance Mitigates Representation Collapse. (arXiv:2205.11603v2 [cs.CL] UPDATED)\nAbstract: Fine-tuning contextualized representations learned by pre-trained language models remains a prevalent practice in NLP. However, fine-tuning can lead to representation degradation (also known as representation collapse), which may result in instability, sub-optimal performance, and weak generalization.  In this paper, we propose Representation Projection Invariance (REPINA), a novel regularization method to maintain the information content of representation and reduce representation collapse during fine-tuning by discouraging undesirable changes in the representations. We study the empirical behavior of the proposed regularization in comparison to 5 comparable baselines across 13 language understanding tasks (GLUE benchmark and six additional datasets). When evaluating in-domain performance, REPINA consistently outperforms other baselines on most tasks (10 out of 13). We also demonstrate its effectiveness in few-shot settings and robustness to label perturbation. As a by-product, we ext",
    "path": "papers/22/05/2205.11603.json",
    "total_tokens": 909,
    "translated_title": "表示投影不变性缓解表示崩溃问题",
    "translated_abstract": "对预训练语言模型学习的上下文化表示进行微调在自然语言处理领域中仍然是一种流行的做法。然而，微调可能会导致表示降级（也被称为表示崩溃），这可能会导致不稳定性、次优性能和弱泛化。在本文中，我们提出了“表示投影不变性”（REPINA），这是一种新颖的正则化方法，通过抑制表示中的不良变化来维护表示的信息内容并减少表示崩溃问题。我们研究了所提出的正则化与5个可比较基线在13个语言理解任务（GLUE基准测试和其他六个数据集）中的实证行为。在评估内域性能时，REPINA 在大多数任务（13项中的10项）上始终优于其他基线。我们还证明了它在少样本设置中的有效性和对标签扰动的鲁棒性。作为副产品，我们扩展了已有的先前工作的范围，这些工作通过包括预测任务在内的自监督学习来降低表示崩溃率。",
    "tldr": "本文提出了一种新的正则化方法 REPINA，旨在减少表示崩溃问题，结果在 13 个语言理解任务上表现出良好的效果。",
    "en_tdlr": "This paper proposes a novel regularization method called REPINA to mitigate representation collapse during fine-tuning of contextualized representations learned by pre-trained language models. The proposed method consistently outperforms baselines on most of the evaluated tasks and exhibits effectiveness in few-shot and perturbation settings."
}