{
    "title": "CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models. (arXiv:2205.12213v2 [cs.CL] UPDATED)",
    "abstract": "Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes through inference APIs. Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners. In this work, we present a lightweight method for adapting large LMs to new domains and tasks, assuming no access to their weights or intermediate activations. Our approach fine-tunes a small white-box LM and combines it with the large black-box LM at the probability level through a small network, learned on a small validation set. We validate our approach by adapting a large LM (OPT-30B) to several domains and a downstream task (machine translation), observing improved performance in all cases, of up to 9\\%, while using a domain expert 23x ",
    "link": "http://arxiv.org/abs/2205.12213",
    "context": "Title: CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models. (arXiv:2205.12213v2 [cs.CL] UPDATED)\nAbstract: Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes through inference APIs. Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners. In this work, we present a lightweight method for adapting large LMs to new domains and tasks, assuming no access to their weights or intermediate activations. Our approach fine-tunes a small white-box LM and combines it with the large black-box LM at the probability level through a small network, learned on a small validation set. We validate our approach by adapting a large LM (OPT-30B) to several domains and a downstream task (machine translation), observing improved performance in all cases, of up to 9\\%, while using a domain expert 23x ",
    "path": "papers/22/05/2205.12213.json",
    "total_tokens": 940,
    "translated_title": "CombLM: 通过小型微调模型调整黑盒语言模型",
    "translated_abstract": "传统上，将语言模型适应新任务和域的方法通常假设对模型有白盒访问，并通过修改其参数进行操作。但这与该领域的最高质量模型仅通过推理API作为黑盒可用的最近趋势不兼容。即使可用模型权重，对大型语言模型进行微调的计算成本也可能对大多数研究人员来说是禁止的。在本研究中，我们提出了一种轻量级的方法，用于调整大型语言模型以适应新领域和任务，假设没有访问它们的权重或中间激活的权限。我们的方法通过在小验证集上学习的小型网络，在概率级别上微调小型白盒LM，并将其与大型黑盒LM结合起来。我们通过将大型LM（OPT-30B）适应多个领域和下游任务（机器翻译），在所有情况下观察到性能的提高，最高可达9\\%，同时使用领域专家23倍。",
    "tldr": "本论文提出了一种 CombLM 方法，通过小型微调模型调整大型黑盒语言模型以适应新领域和任务，且不需要访问它们的权重或中间激活。实验证明在多个领域和下游任务中，性能得到提高。",
    "en_tdlr": "This paper proposes a CombLM method that adapts large black-box language models to new tasks and domains through small fine-tuned models, without requiring access to their weights or intermediate activations. Experimental results show performance improvement across multiple domains and downstream tasks."
}