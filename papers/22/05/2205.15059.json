{
    "title": "Hilbert Curve Projection Distance for Distribution Comparison",
    "abstract": "Distribution comparison plays a central role in many machine learning tasks like data classification and generative modeling. In this study, we propose a novel metric, called Hilbert curve projection (HCP) distance, to measure the distance between two probability distributions with low complexity. In particular, we first project two high-dimensional probability distributions using Hilbert curve to obtain a coupling between them, and then calculate the transport distance between these two distributions in the original space, according to the coupling. We show that HCP distance is a proper metric and is well-defined for probability measures with bounded supports. Furthermore, we demonstrate that the modified empirical HCP distance with the $L_p$ cost in the $d$-dimensional space converges to its population counterpart at a rate of no more than $O(n^{-1/2\\max\\{d,p\\}})$. To suppress the curse-of-dimensionality, we also develop two variants of the HCP distance using (learnable) subspace pro",
    "link": "https://arxiv.org/abs/2205.15059",
    "context": "Title: Hilbert Curve Projection Distance for Distribution Comparison\nAbstract: Distribution comparison plays a central role in many machine learning tasks like data classification and generative modeling. In this study, we propose a novel metric, called Hilbert curve projection (HCP) distance, to measure the distance between two probability distributions with low complexity. In particular, we first project two high-dimensional probability distributions using Hilbert curve to obtain a coupling between them, and then calculate the transport distance between these two distributions in the original space, according to the coupling. We show that HCP distance is a proper metric and is well-defined for probability measures with bounded supports. Furthermore, we demonstrate that the modified empirical HCP distance with the $L_p$ cost in the $d$-dimensional space converges to its population counterpart at a rate of no more than $O(n^{-1/2\\max\\{d,p\\}})$. To suppress the curse-of-dimensionality, we also develop two variants of the HCP distance using (learnable) subspace pro",
    "path": "papers/22/05/2205.15059.json",
    "total_tokens": 934,
    "translated_title": "Hilbert曲线投影距离用于分布比较",
    "translated_abstract": "分布比较在数据分类和生成建模等许多机器学习任务中起着核心作用。本研究提出了一种新的度量方法，称为Hilbert曲线投影(HCP)距离，用于测量两个概率分布之间的距离，并具有低复杂度。具体而言，我们首先使用Hilbert曲线将两个高维概率分布投影到一起，得到它们之间的耦合，然后根据耦合在原始空间中计算这两个分布之间的运输距离。我们证明了HCP距离是一个适当的度量，并且对于有界支撑的概率测度是良定义的。此外，我们还证明了在$d$维空间中具有$L_p$成本的改进经验HCP距离以不超过$O(n^{-1/2\\max\\{d,p\\}})$的速率收敛到其总体对应项。为了抑制维度灾难，我们还开发了两个HCP距离的变体，使用（可学习的）子空间投影。",
    "tldr": "本研究提出了一种新的度量方法，称为Hilbert曲线投影(HCP)距离，用于测量两个概率分布之间的距离，并具有低复杂度。通过Hilbert曲线投影和运输距离计算，该方法适用于有界支撑的概率测度，并在高维空间中具有较好的收敛性能。为了解决维度灾难，还开发了两个HCP距离的变体，使用子空间投影。"
}