{
    "title": "Quantum Self-Attention Neural Networks for Text Classification. (arXiv:2205.05625v2 [quant-ph] UPDATED)",
    "abstract": "An emerging direction of quantum computing is to establish meaningful quantum applications in various fields of artificial intelligence, including natural language processing (NLP). Although some efforts based on syntactic analysis have opened the door to research in Quantum NLP (QNLP), limitations such as heavy syntactic preprocessing and syntax-dependent network architecture make them impracticable on larger and real-world data sets. In this paper, we propose a new simple network architecture, called the quantum self-attention neural network (QSANN), which can compensate for these limitations. Specifically, we introduce the self-attention mechanism into quantum neural networks and then utilize a Gaussian projected quantum self-attention serving as a sensible quantum version of self-attention. As a result, QSANN is effective and scalable on larger data sets and has the desirable property of being implementable on near-term quantum devices. In particular, our QSANN outperforms the best",
    "link": "http://arxiv.org/abs/2205.05625",
    "context": "Title: Quantum Self-Attention Neural Networks for Text Classification. (arXiv:2205.05625v2 [quant-ph] UPDATED)\nAbstract: An emerging direction of quantum computing is to establish meaningful quantum applications in various fields of artificial intelligence, including natural language processing (NLP). Although some efforts based on syntactic analysis have opened the door to research in Quantum NLP (QNLP), limitations such as heavy syntactic preprocessing and syntax-dependent network architecture make them impracticable on larger and real-world data sets. In this paper, we propose a new simple network architecture, called the quantum self-attention neural network (QSANN), which can compensate for these limitations. Specifically, we introduce the self-attention mechanism into quantum neural networks and then utilize a Gaussian projected quantum self-attention serving as a sensible quantum version of self-attention. As a result, QSANN is effective and scalable on larger data sets and has the desirable property of being implementable on near-term quantum devices. In particular, our QSANN outperforms the best",
    "path": "papers/22/05/2205.05625.json",
    "total_tokens": 931,
    "translated_title": "量子自注意力神经网络用于文本分类",
    "translated_abstract": "量子计算的一个新兴方向是在包括自然语言处理在内的人工智能各个领域建立有意义的量子应用。尽管基于句法分析的一些工作为量子自然语言处理研究打开了大门，但是诸如繁重的句法预处理和句法相关的网络结构等限制使得它们在更大规模和实际数据集上不可行。在本文中，我们提出了一种新的简单网络架构，称为量子自注意力神经网络（QSANN），可以弥补这些限制。具体而言，我们将自注意机制引入到量子神经网络中，然后利用高斯投影的量子自注意力作为自注意力的量子版本。结果表明，QSANN在更大规模的数据集上具有有效和可扩展的性能，并且具有在近期量子设备上可实现的理想性质。特别地，我们的QSANN优于最佳的现有模型。",
    "tldr": "本论文提出了一种名为量子自注意力神经网络（QSANN）的简单网络架构，通过将自注意机制引入到量子神经网络中，并利用高斯投影的量子自注意力，弥补了现有量子自然语言处理方法的一些限制。QSANN在更大规模的数据集上具有有效和可扩展的性能，并且可以在近期量子设备上实现。"
}