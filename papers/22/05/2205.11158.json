{
    "title": "IDEAL: Query-Efficient Data-Free Learning from Black-box Models. (arXiv:2205.11158v2 [cs.AI] UPDATED)",
    "abstract": "Knowledge Distillation (KD) is a typical method for training a lightweight student model with the help of a well-trained teacher model. However, most KD methods require access to either the teacher's training data or model parameters, which is unrealistic. To tackle this problem, recent works study KD under data-free and black-box settings. Nevertheless, these works require a large number of queries to the teacher model, which incurs significant monetary and computational costs. To address these problems, we propose a novel method called \\emph{query-effIcient Data-free lEarning from blAck-box modeLs} (IDEAL), which aims to query-efficiently learn from black-box model APIs to train a good student without any real data. In detail, IDEAL trains the student model in two stages: data generation and model distillation. Note that IDEAL does not require any query in the data generation stage and queries the teacher only once for each sample in the distillation stage. Extensive experiments on v",
    "link": "http://arxiv.org/abs/2205.11158",
    "context": "Title: IDEAL: Query-Efficient Data-Free Learning from Black-box Models. (arXiv:2205.11158v2 [cs.AI] UPDATED)\nAbstract: Knowledge Distillation (KD) is a typical method for training a lightweight student model with the help of a well-trained teacher model. However, most KD methods require access to either the teacher's training data or model parameters, which is unrealistic. To tackle this problem, recent works study KD under data-free and black-box settings. Nevertheless, these works require a large number of queries to the teacher model, which incurs significant monetary and computational costs. To address these problems, we propose a novel method called \\emph{query-effIcient Data-free lEarning from blAck-box modeLs} (IDEAL), which aims to query-efficiently learn from black-box model APIs to train a good student without any real data. In detail, IDEAL trains the student model in two stages: data generation and model distillation. Note that IDEAL does not require any query in the data generation stage and queries the teacher only once for each sample in the distillation stage. Extensive experiments on v",
    "path": "papers/22/05/2205.11158.json",
    "total_tokens": 949,
    "translated_title": "IDEAL: 无需数据学习黑盒模型的高效查询方法",
    "translated_abstract": "知识蒸馏（KD）是一种使用经过良好训练的教师模型帮助训练轻量级学生模型的典型方法。然而，大多数KD方法要求访问教师的训练数据或模型参数，这是不切实际的。为了解决这个问题，最近的研究在无数据和黑盒设置下研究了KD。然而，这些方法需要大量的查询教师模型，这会造成显著的金钱和计算成本。为了解决这些问题，我们提出了一种新方法，名为IDEAL（query-effIcient Data-free lEarning from blAck-box modeLs），旨在通过查询黑盒模型API高效地从中学习，并且无需任何真实数据来训练一个好的学生模型。具体而言，IDEAL分为两个阶段进行学生模型的训练：数据生成和模型蒸馏。请注意，IDEAL不需要在数据生成阶段进行任何查询，并且在蒸馏阶段对每个样本只查询教师模型一次。",
    "tldr": "IDEAL是一种无需真实数据的高效查询方法，用于从黑盒模型API中学习并训练一个优秀的学生模型。它通过在两个阶段进行训练，即数据生成和模型蒸馏，只需要对教师模型进行少量的查询。",
    "en_tdlr": "IDEAL is an efficient query method that does not require real data to learn from black-box model APIs and train a good student model. It achieves this by training in two stages, data generation and model distillation, with minimal queries to the teacher model."
}