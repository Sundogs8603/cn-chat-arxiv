{
    "title": "Aligning Logits Generatively for Principled Black-Box Knowledge Distillation",
    "abstract": "arXiv:2205.10490v2 Announce Type: replace-cross  Abstract: Black-Box Knowledge Distillation (B2KD) is a formulated problem for cloud-to-edge model compression with invisible data and models hosted on the server. B2KD faces challenges such as limited Internet exchange and edge-cloud disparity of data distributions. In this paper, we formalize a two-step workflow consisting of deprivatization and distillation, and theoretically provide a new optimization direction from logits to cell boundary different from direct logits alignment. With its guidance, we propose a new method Mapping-Emulation KD (MEKD) that distills a black-box cumbersome model into a lightweight one. Our method does not differentiate between treating soft or hard responses, and consists of: 1) deprivatization: emulating the inverse mapping of the teacher function with a generator, and 2) distillation: aligning low-dimensional logits of the teacher and student models by reducing the distance of high-dimensional image poin",
    "link": "https://arxiv.org/abs/2205.10490",
    "context": "Title: Aligning Logits Generatively for Principled Black-Box Knowledge Distillation\nAbstract: arXiv:2205.10490v2 Announce Type: replace-cross  Abstract: Black-Box Knowledge Distillation (B2KD) is a formulated problem for cloud-to-edge model compression with invisible data and models hosted on the server. B2KD faces challenges such as limited Internet exchange and edge-cloud disparity of data distributions. In this paper, we formalize a two-step workflow consisting of deprivatization and distillation, and theoretically provide a new optimization direction from logits to cell boundary different from direct logits alignment. With its guidance, we propose a new method Mapping-Emulation KD (MEKD) that distills a black-box cumbersome model into a lightweight one. Our method does not differentiate between treating soft or hard responses, and consists of: 1) deprivatization: emulating the inverse mapping of the teacher function with a generator, and 2) distillation: aligning low-dimensional logits of the teacher and student models by reducing the distance of high-dimensional image poin",
    "path": "papers/22/05/2205.10490.json",
    "total_tokens": 892,
    "translated_title": "将生成的对数进行准则对齐的黑盒知识蒸馏",
    "translated_abstract": "黑盒知识蒸馏（B2KD）是一个处理云端到边缘模型压缩的问题，其中数据和模型托管在服务器上且无法看见。B2KD面临的挑战包括互联网交换受限和数据分布在边缘和云端之间的不一致。本文提出了一个包括去隐去和蒸馏两步工作流程，并在理论上提供了一个从对数到单元边界的新优化方向，不同于直接对数对齐。在其指导下，我们提出了一种新方法Mapping-Emulation KD（MEKD），将一个黑盒繁琐模型蒸馏成一个轻量级模型。我们的方法不区分软或硬响应处理，并包括：1）去隐去：通过生成器模拟教师函数的逆映射，和2）蒸馏：通过减小高维图像点之间的距离来对齐教师模型和学生模型的低维度对数。",
    "tldr": "本文提出了一个新的黑盒知识蒸馏方法MEKD，通过将教师和学生模型的低维度对数对齐，实现将一个繁琐模型压缩成轻量级模型。",
    "en_tdlr": "This paper introduces a new method MEKD for black-box knowledge distillation, which aligns the low-dimensional logits of teacher and student models to compress a cumbersome model into a lightweight one."
}