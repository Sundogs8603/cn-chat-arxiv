{
    "title": "Secure & Private Federated Neuroimaging. (arXiv:2205.05249v2 [cs.LG] UPDATED)",
    "abstract": "The amount of biomedical data continues to grow rapidly. However, collecting data from multiple sites for joint analysis remains challenging due to security, privacy, and regulatory concerns. To overcome this challenge, we use Federated Learning, which enables distributed training of neural network models over multiple data sources without sharing data. Each site trains the neural network over its private data for some time, then shares the neural network parameters (i.e., weights, gradients) with a Federation Controller, which in turn aggregates the local models, sends the resulting community model back to each site, and the process repeats. Our Federated Learning architecture, MetisFL, provides strong security and privacy. First, sample data never leaves a site. Second, neural network parameters are encrypted before transmission and the global neural model is computed under fully-homomorphic encryption. Finally, we use information-theoretic methods to limit information leakage from t",
    "link": "http://arxiv.org/abs/2205.05249",
    "context": "Title: Secure & Private Federated Neuroimaging. (arXiv:2205.05249v2 [cs.LG] UPDATED)\nAbstract: The amount of biomedical data continues to grow rapidly. However, collecting data from multiple sites for joint analysis remains challenging due to security, privacy, and regulatory concerns. To overcome this challenge, we use Federated Learning, which enables distributed training of neural network models over multiple data sources without sharing data. Each site trains the neural network over its private data for some time, then shares the neural network parameters (i.e., weights, gradients) with a Federation Controller, which in turn aggregates the local models, sends the resulting community model back to each site, and the process repeats. Our Federated Learning architecture, MetisFL, provides strong security and privacy. First, sample data never leaves a site. Second, neural network parameters are encrypted before transmission and the global neural model is computed under fully-homomorphic encryption. Finally, we use information-theoretic methods to limit information leakage from t",
    "path": "papers/22/05/2205.05249.json",
    "total_tokens": 854,
    "translated_title": "安全和私密的联合神经影像学",
    "translated_abstract": "生物医学数据的数量不断增长。然而，由于安全、隐私和监管问题，从多个站点收集数据进行联合分析仍然具有挑战性。为了克服这个挑战，我们使用联邦学习，它能够在多个数据源上进行神经网络模型的分布式训练，而不共享数据。每个站点在其私有数据上训练神经网络一段时间，然后将神经网络参数（即权重、梯度）与联邦控制器共享，联邦控制器再聚合本地模型，将结果模型发送回每个站点，这个过程不断重复。我们的联邦学习架构MetisFL提供了强大的安全性和隐私保护。首先，样本数据永远不会离开站点。其次，在传输之前对神经网络参数进行加密，并且全同态加密下计算全局神经模型。最后，我们使用信息理论方法来限制信息泄露。",
    "tldr": "本论文介绍了一种安全和私密的联合神经影像学方法，通过联邦学习实现了在保护数据隐私的情况下对分布式数据进行神经网络模型的训练和分析。",
    "en_tdlr": "This paper presents a secure and private federated neuroimaging approach, which utilizes federated learning to train and analyze neural network models on distributed data while protecting data privacy."
}