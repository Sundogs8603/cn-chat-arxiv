{
    "title": "PSO-Convolutional Neural Networks with Heterogeneous Learning Rate. (arXiv:2205.10456v3 [cs.CV] UPDATED)",
    "abstract": "Convolutional Neural Networks (ConvNets or CNNs) have been candidly deployed in the scope of computer vision and related fields. Nevertheless, the dynamics of training of these neural networks lie still elusive: it is hard and computationally expensive to train them. A myriad of architectures and training strategies have been proposed to overcome this challenge and address several problems in image processing such as speech, image and action recognition as well as object detection. In this article, we propose a novel Particle Swarm Optimization (PSO) based training for ConvNets. In such framework, the vector of weights of each ConvNet is typically cast as the position of a particle in phase space whereby PSO collaborative dynamics intertwines with Stochastic Gradient Descent (SGD) in order to boost training performance and generalization. Our approach goes as follows: i) [regular phase] each ConvNet is trained independently via SGD; ii) [collaborative phase] ConvNets share among themse",
    "link": "http://arxiv.org/abs/2205.10456",
    "context": "Title: PSO-Convolutional Neural Networks with Heterogeneous Learning Rate. (arXiv:2205.10456v3 [cs.CV] UPDATED)\nAbstract: Convolutional Neural Networks (ConvNets or CNNs) have been candidly deployed in the scope of computer vision and related fields. Nevertheless, the dynamics of training of these neural networks lie still elusive: it is hard and computationally expensive to train them. A myriad of architectures and training strategies have been proposed to overcome this challenge and address several problems in image processing such as speech, image and action recognition as well as object detection. In this article, we propose a novel Particle Swarm Optimization (PSO) based training for ConvNets. In such framework, the vector of weights of each ConvNet is typically cast as the position of a particle in phase space whereby PSO collaborative dynamics intertwines with Stochastic Gradient Descent (SGD) in order to boost training performance and generalization. Our approach goes as follows: i) [regular phase] each ConvNet is trained independently via SGD; ii) [collaborative phase] ConvNets share among themse",
    "path": "papers/22/05/2205.10456.json",
    "total_tokens": 917,
    "translated_title": "具有异构学习率的PSO卷积神经网络",
    "translated_abstract": "卷积神经网络（ConvNets或CNN）在计算机视觉和相关领域中已被广泛应用。然而，这些神经网络训练的动态仍然难以捉摸：训练它们是困难且计算开销大的。为了解决这个挑战并解决图像处理中的几个问题，如语音、图像和动作识别以及物体检测，已经提出了许多架构和训练策略。在本文中，我们提出了一种基于粒子群优化（PSO）的ConvNets训练方法。在这种框架中，每个ConvNet的权重向量通常被视为相位空间中粒子的位置，其中PSO协同动力学与随机梯度下降（SGD）相结合，以提高训练性能和泛化能力。我们的方法如下：i) [常规阶段]每个ConvNet通过SGD独立训练；ii) [协同阶段]ConvNets之间共享学习。",
    "tldr": "本论文提出了一种基于粒子群优化的PSO卷积神经网络训练方法，通过在训练的不同阶段实现卷积神经网络之间的协同学习，提高了训练性能和泛化能力。",
    "en_tdlr": "This paper proposes a PSO-based training method for ConvNets, which achieves collaborative learning between ConvNets at different stages of training, improving training performance and generalization."
}