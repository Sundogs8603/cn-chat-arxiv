{
    "title": "Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing. (arXiv:2205.06978v3 [cs.LG] UPDATED)",
    "abstract": "Reinforcement Learning (RL) has opened up new opportunities to enhance existing smart systems that generally include a complex decision-making process. However, modern RL algorithms, e.g., Deep Q-Networks (DQN), are based on deep neural networks, resulting in high computational costs. In this paper, we propose QHD, an off-policy value-based Hyperdimensional Reinforcement Learning, that mimics brain properties toward robust and real-time learning. QHD relies on a lightweight brain-inspired model to learn an optimal policy in an unknown environment. On both desktop and power-limited embedded platforms, QHD achieves significantly better overall efficiency than DQN while providing higher or comparable rewards. QHD is also suitable for highly-efficient reinforcement learning with great potential for online and real-time learning. Our solution supports a small experience replay batch size that provides 12.3 times speedup compared to DQN while ensuring minimal quality loss. Our evaluation sho",
    "link": "http://arxiv.org/abs/2205.06978",
    "context": "Title: Efficient Off-Policy Reinforcement Learning via Brain-Inspired Computing. (arXiv:2205.06978v3 [cs.LG] UPDATED)\nAbstract: Reinforcement Learning (RL) has opened up new opportunities to enhance existing smart systems that generally include a complex decision-making process. However, modern RL algorithms, e.g., Deep Q-Networks (DQN), are based on deep neural networks, resulting in high computational costs. In this paper, we propose QHD, an off-policy value-based Hyperdimensional Reinforcement Learning, that mimics brain properties toward robust and real-time learning. QHD relies on a lightweight brain-inspired model to learn an optimal policy in an unknown environment. On both desktop and power-limited embedded platforms, QHD achieves significantly better overall efficiency than DQN while providing higher or comparable rewards. QHD is also suitable for highly-efficient reinforcement learning with great potential for online and real-time learning. Our solution supports a small experience replay batch size that provides 12.3 times speedup compared to DQN while ensuring minimal quality loss. Our evaluation sho",
    "path": "papers/22/05/2205.06978.json",
    "total_tokens": 973,
    "translated_title": "基于脑启发计算的高效离线强化学习",
    "translated_abstract": "强化学习（RL）已经开创了增强现有智能系统的新机会，这些系统通常包括复杂的决策过程。然而，现代RL算法，如Deep Q-Networks（DQN），基于深度神经网络，导致计算成本高昂。本文提出了QHD，一种基于超维强化学习的离线策略，模仿大脑的属性，实现稳健和实时学习。QHD依靠轻量级的脑启发模型，在未知环境中学习最佳策略。在桌面和功率限制的嵌入式平台上，QHD的整体效率显著优于DQN，同时提供更高或可比较的回报。QHD也适用于高度有效的强化学习，具有极大的在线和实时学习潜力。我们的解决方案支持小的经验重放批量，与DQN相比提供12.3倍的加速，并确保最小的质量损失。我们的评估显示，QHD与先进的RL算法相比，具有更少的计算成本和更高的效率。",
    "tldr": "本文提出了一种QHD离线策略，它基于超维强化学习与脑启发计算，实现稳健和实时学习。QHD相比于DQN具有更高效率且适用于高效的强化学习，具有在线和实时学习潜力。"
}