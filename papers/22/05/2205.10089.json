{
    "title": "Kernel Normalized Convolutional Networks",
    "abstract": "arXiv:2205.10089v4 Announce Type: replace  Abstract: Existing convolutional neural network architectures frequently rely upon batch normalization (BatchNorm) to effectively train the model. BatchNorm, however, performs poorly with small batch sizes, and is inapplicable to differential privacy. To address these limitations, we propose the kernel normalization (KernelNorm) and kernel normalized convolutional layers, and incorporate them into kernel normalized convolutional networks (KNConvNets) as the main building blocks. We implement KNConvNets corresponding to the state-of-the-art ResNets while forgoing the BatchNorm layers. Through extensive experiments, we illustrate that KNConvNets achieve higher or competitive performance compared to the BatchNorm counterparts in image classification and semantic segmentation. They also significantly outperform their batch-independent competitors including those based on layer and group normalization in non-private and differentially private train",
    "link": "https://arxiv.org/abs/2205.10089",
    "context": "Title: Kernel Normalized Convolutional Networks\nAbstract: arXiv:2205.10089v4 Announce Type: replace  Abstract: Existing convolutional neural network architectures frequently rely upon batch normalization (BatchNorm) to effectively train the model. BatchNorm, however, performs poorly with small batch sizes, and is inapplicable to differential privacy. To address these limitations, we propose the kernel normalization (KernelNorm) and kernel normalized convolutional layers, and incorporate them into kernel normalized convolutional networks (KNConvNets) as the main building blocks. We implement KNConvNets corresponding to the state-of-the-art ResNets while forgoing the BatchNorm layers. Through extensive experiments, we illustrate that KNConvNets achieve higher or competitive performance compared to the BatchNorm counterparts in image classification and semantic segmentation. They also significantly outperform their batch-independent competitors including those based on layer and group normalization in non-private and differentially private train",
    "path": "papers/22/05/2205.10089.json",
    "total_tokens": 868,
    "translated_title": "核规范化卷积网络",
    "translated_abstract": "现有的卷积神经网络体系结构通常依赖于批量规范化（BatchNorm）来有效训练模型。然而，BatchNorm 在小批量大小时性能较差，并且不适用于差分隐私。为了解决这些限制，我们提出了核规范化（KernelNorm）和核规范化卷积层，并将它们作为主要构建模块融入核规范化卷积网络（KNConvNets）中。我们实现了相应于最先进的ResNets的KNConvNets，同时放弃了BatchNorm层。通过大量实验，我们展示了KNConvNets在图像分类和语义分割中相比于BatchNorm对应的模型能实现更高或相媲美的性能。在非私密和差分私密训练中，它们还显著优于基于层和组规范化的批量独立竞争对手。",
    "tldr": "提出了核规范化卷积网络（KNConvNets），通过替代BatchNorm层，在图像分类和语义分割中实现比BatchNorm更高或相媲美的性能，同时也在非私密和差分私密训练中显著优于基于层和组规范化的竞争对手。",
    "en_tdlr": "Introduced kernel normalized convolutional networks (KNConvNets) which achieve higher or competitive performance compared to BatchNorm counterparts in image classification and semantic segmentation by replacing BatchNorm layers, also significantly outperforming batch-independent competitors based on layer and group normalization in non-private and differentially private training."
}