{
    "title": "Self-Supervised Time Series Representation Learning via Cross Reconstruction Transformer. (arXiv:2205.09928v2 [cs.LG] UPDATED)",
    "abstract": "Unsupervised/self-supervised representation learning in time series is critical since labeled samples are usually scarce in real-world scenarios. Existing approaches mainly leverage the contrastive learning framework, which automatically learns to understand the similar and dissimilar data pairs. Nevertheless, they are restricted to the prior knowledge of constructing pairs, cumbersome sampling policy, and unstable performances when encountering sampling bias. Also, few works have focused on effectively modeling across temporal-spectral relations to extend the capacity of representations. In this paper, we aim at learning representations for time series from a new perspective and propose Cross Reconstruction Transformer (CRT) to solve the aforementioned problems in a unified way. CRT achieves time series representation learning through a cross-domain dropping-reconstruction task. Specifically, we transform time series into the frequency domain and randomly drop certain parts in both ti",
    "link": "http://arxiv.org/abs/2205.09928",
    "context": "Title: Self-Supervised Time Series Representation Learning via Cross Reconstruction Transformer. (arXiv:2205.09928v2 [cs.LG] UPDATED)\nAbstract: Unsupervised/self-supervised representation learning in time series is critical since labeled samples are usually scarce in real-world scenarios. Existing approaches mainly leverage the contrastive learning framework, which automatically learns to understand the similar and dissimilar data pairs. Nevertheless, they are restricted to the prior knowledge of constructing pairs, cumbersome sampling policy, and unstable performances when encountering sampling bias. Also, few works have focused on effectively modeling across temporal-spectral relations to extend the capacity of representations. In this paper, we aim at learning representations for time series from a new perspective and propose Cross Reconstruction Transformer (CRT) to solve the aforementioned problems in a unified way. CRT achieves time series representation learning through a cross-domain dropping-reconstruction task. Specifically, we transform time series into the frequency domain and randomly drop certain parts in both ti",
    "path": "papers/22/05/2205.09928.json",
    "total_tokens": 913,
    "translated_title": "通过交叉重构变压器进行自监督时间序列表示学习",
    "translated_abstract": "无监督/自监督的时间序列表示学习对于真实场景中稀缺标记样本至关重要。现有方法主要利用对比学习框架，自动学习理解相似和不相似的数据对。然而，它们受限于构建对的先验知识、繁琐的采样策略以及在遇到采样偏差时性能不稳定。此外，很少有工作专注于有效地建模跨时谱关系以扩展表示的容量。本文旨在从新的角度学习时间序列的表示，并提出交叉重构变压器(CRT)以统一地解决上述问题。CRT通过跨域丢弃-重构任务实现时间序列表示学习。具体而言，我们将时间序列转换为频域，并随机丢弃时间和频率域中的某些部分，然后通过重构任务来学习表示。",
    "tldr": "本文提出了一种称为交叉重构变压器(CRT)的方法，通过跨域丢弃-重构任务实现了时间序列的表示学习。CRT解决了构建数据对的先验知识、采样策略繁琐和采样偏差导致性能不稳定等问题，同时有效地建模了跨时谱关系以扩展表示的容量。",
    "en_tdlr": "This paper introduces a method called Cross Reconstruction Transformer (CRT) for time series representation learning, which is achieved through a cross-domain dropping-reconstruction task. CRT addresses issues related to constructing data pairs, cumbersome sampling policies, unstable performances caused by sampling bias, as well as effectively modeling cross-temporal-spectral relations to expand the capacity of representations."
}