{
    "title": "Deep neural networks with dependent weights: Gaussian Process mixture limit, heavy tails, sparsity and compressibility. (arXiv:2205.08187v2 [stat.ML] UPDATED)",
    "abstract": "This article studies the infinite-width limit of deep feedforward neural networks whose weights are dependent, and modelled via a mixture of Gaussian distributions. Each hidden node of the network is assigned a nonnegative random variable that controls the variance of the outgoing weights of that node. We make minimal assumptions on these per-node random variables: they are iid and their sum, in each layer, converges to some finite random variable in the infinite-width limit. Under this model, we show that each layer of the infinite-width neural network can be characterised by two simple quantities: a non-negative scalar parameter and a L\\'evy measure on the positive reals. If the scalar parameters are strictly positive and the L\\'evy measures are trivial at all hidden layers, then one recovers the classical Gaussian process (GP) limit, obtained with iid Gaussian weights. More interestingly, if the L\\'evy measure of at least one layer is non-trivial, we obtain a mixture of Gaussian pro",
    "link": "http://arxiv.org/abs/2205.08187",
    "context": "Title: Deep neural networks with dependent weights: Gaussian Process mixture limit, heavy tails, sparsity and compressibility. (arXiv:2205.08187v2 [stat.ML] UPDATED)\nAbstract: This article studies the infinite-width limit of deep feedforward neural networks whose weights are dependent, and modelled via a mixture of Gaussian distributions. Each hidden node of the network is assigned a nonnegative random variable that controls the variance of the outgoing weights of that node. We make minimal assumptions on these per-node random variables: they are iid and their sum, in each layer, converges to some finite random variable in the infinite-width limit. Under this model, we show that each layer of the infinite-width neural network can be characterised by two simple quantities: a non-negative scalar parameter and a L\\'evy measure on the positive reals. If the scalar parameters are strictly positive and the L\\'evy measures are trivial at all hidden layers, then one recovers the classical Gaussian process (GP) limit, obtained with iid Gaussian weights. More interestingly, if the L\\'evy measure of at least one layer is non-trivial, we obtain a mixture of Gaussian pro",
    "path": "papers/22/05/2205.08187.json",
    "total_tokens": 990,
    "translated_title": "具有相关权重的深度神经网络：高斯过程混合极限、重尾、稀疏性和可压缩性",
    "translated_abstract": "本文研究了具有相关权重并通过高斯分布混合建模的无限宽度前馈深度神经网络的极限。网络的每个隐藏节点被分配一个非负随机变量，该随机变量控制该节点的输出权重的方差。我们对这些节点随机变量做了最小的假设：它们是独立同分布的，并且在无限宽度极限下，每一层的随机变量和收敛到一些有限的随机变量。在这个模型下，我们证明了无限宽度神经网络的每一层可以通过两个简单的量来刻画：一个非负标量参数和一个正实数上的Lévy测度。如果标量参数严格为正且所有隐藏层的Lévy测度都是平凡的，那么就得到了经典的高斯过程(GP)极限，即通过独立同分布的高斯权重获得。更有趣的是，如果至少一层的Lévy测度是非平凡的，我们得到了高斯过程的混合模型。",
    "tldr": "本文研究了具有相关权重的深度神经网络的极限行为，发现无限宽度神经网络的每一层可以通过两个简单的量来刻画，当其中至少一层的量是非平凡的时候，得到了高斯过程的混合模型。",
    "en_tdlr": "This article investigates the limit behavior of deep neural networks with dependent weights and shows that each layer of the infinite-width network can be characterized by two simple quantities. When at least one of these quantities is non-trivial, a mixture model of Gaussian processes is obtained."
}