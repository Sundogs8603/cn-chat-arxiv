{
    "title": "Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space. (arXiv:2205.08129v2 [cs.RO] UPDATED)",
    "abstract": "General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long-horizon tasks that require multiple distinct types of interactions to solve. Our approach is based on two key ideas. First, we decompose the goal-reaching problem hierarchically, with a high-level planner that sets intermediate subgoals using conditional subgoal generators in the latent space for a low-level model-free policy. Second, we propose a hybrid approach which first pre-trains both the conditional subgoal generator and the policy on previously collected",
    "link": "http://arxiv.org/abs/2205.08129",
    "context": "Title: Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space. (arXiv:2205.08129v2 [cs.RO] UPDATED)\nAbstract: General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long-horizon tasks that require multiple distinct types of interactions to solve. Our approach is based on two key ideas. First, we decompose the goal-reaching problem hierarchically, with a high-level planner that sets intermediate subgoals using conditional subgoal generators in the latent space for a low-level model-free policy. Second, we propose a hybrid approach which first pre-trains both the conditional subgoal generator and the policy on previously collected",
    "path": "papers/22/05/2205.08129.json",
    "total_tokens": 1089,
    "translated_title": "计划到实践：在潜空间中组合目标的高效在线微调",
    "translated_abstract": "通用型机器人需要多样化的行为来完成现实世界中无结构环境下的挑战性任务。为了解决这个问题，目标导向强化学习旨在获得可以在命令下达时到达可配置目标的策略，完成广泛的任务。然而，这类目标导向策略非常难以从头开始训练。本文提出了Planning to Practice（PTP），一种实现训练目标导向策略的方法，用于解决需要多个不同类型交互才能解决的长时程任务。我们的方法基于两个重要的思想。首先，我们分层地分解了到达目标的问题，使用条件子目标生成器在低级无模型策略中的潜空间中设置中间子目标的高级计划器。其次，我们提出了一种混合方法，即首先在以前收集的数据上预训练条件子目标生成器和策略，然后在线微调以适应新的目标，以提高收敛速度。实验表明，PTP可以同时提高数据效率和样本效率，使得机器人可以很高效地采集长期目标导向的经验。",
    "tldr": "本文提出了一种名为PTP的方法，利用高层规划器和潜空间中的条件生成器来分解目标成子目标，并在以前的数据上预训练条件子目标生成器和策略，然后在线微调以适应新的目标，从而训练目标导向的策略，有效提高了机器人长期目标导向的经验采集效率。",
    "en_tdlr": "This paper proposes a method called PTP, which decomposes goals into subgoals using a high-level planner and conditional generators in latent space, pre-trains them on previous data and then fine-tunes them online to improve the efficiency of training goal-conditioned policies for long-horizon tasks that require multiple types of interactions to solve."
}