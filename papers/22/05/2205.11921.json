{
    "title": "Compression-aware Training of Neural Networks using Frank-Wolfe",
    "abstract": "arXiv:2205.11921v2 Announce Type: replace Abstract: Many existing Neural Network pruning approaches rely on either retraining or inducing a strong bias in order to converge to a sparse solution throughout training. A third paradigm, 'compression-aware' training, aims to obtain state-of-the-art dense models that are robust to a wide range of compression ratios using a single dense training run while also avoiding retraining. We propose a framework centered around a versatile family of norm constraints and the Stochastic Frank-Wolfe (SFW) algorithm that encourage convergence to well-performing solutions while inducing robustness towards convolutional filter pruning and low-rank matrix decomposition. Our method is able to outperform existing compression-aware approaches and, in the case of low-rank matrix decomposition, it also requires significantly less computational resources than approaches based on nuclear-norm regularization. Our findings indicate that dynamically adjusting the lear",
    "link": "https://arxiv.org/abs/2205.11921",
    "context": "Title: Compression-aware Training of Neural Networks using Frank-Wolfe\nAbstract: arXiv:2205.11921v2 Announce Type: replace Abstract: Many existing Neural Network pruning approaches rely on either retraining or inducing a strong bias in order to converge to a sparse solution throughout training. A third paradigm, 'compression-aware' training, aims to obtain state-of-the-art dense models that are robust to a wide range of compression ratios using a single dense training run while also avoiding retraining. We propose a framework centered around a versatile family of norm constraints and the Stochastic Frank-Wolfe (SFW) algorithm that encourage convergence to well-performing solutions while inducing robustness towards convolutional filter pruning and low-rank matrix decomposition. Our method is able to outperform existing compression-aware approaches and, in the case of low-rank matrix decomposition, it also requires significantly less computational resources than approaches based on nuclear-norm regularization. Our findings indicate that dynamically adjusting the lear",
    "path": "papers/22/05/2205.11921.json",
    "total_tokens": 1010,
    "translated_title": "使用Frank-Wolfe算法进行神经网络的压缩感知训练",
    "translated_abstract": "许多现有的神经网络剪枝方法要么依赖重新训练，要么在训练过程中引入强大的偏差，以便收敛到一个稀疏解。而第三种范式，“压缩感知”训练旨在通过单个密集训练运行获得最先进的密集模型，这些模型对多种压缩比都具有鲁棒性，同时避免重新训练。我们提出了一个以多功能的范数约束和随机Frank-Wolfe(SFW)算法为中心的框架，促使收敛到表现良好的解，同时在卷积滤波器剪枝和低秩矩阵分解方面具有鲁棒性。我们的方法能够优于现有的压缩感知方法，并且在低秩矩阵分解的情况下，相对于基于核范数正则化的方法，它还需要显著更少的计算资源。我们的研究结果表明，动态调整学习率直到压缩过程比较好是非常重要的。",
    "tldr": "本论文提出了一种使用Frank-Wolfe算法进行神经网络压缩感知训练的框架，通过使用多功能的范数约束和SFW算法，实现了在单次密集训练中获得最先进的密集模型，并具有对压缩比鲁棒性和处理卷积滤波器剪枝和低秩矩阵分解的能力。这种方法优于现有的压缩感知方法，并且在低秩矩阵分解的情况下需要更少的计算资源。"
}