{
    "title": "Towards A Proactive ML Approach for Detecting Backdoor Poison Samples. (arXiv:2205.13616v3 [cs.LG] UPDATED)",
    "abstract": "Adversaries can embed backdoors in deep learning models by introducing backdoor poison samples into training datasets. In this work, we investigate how to detect such poison samples to mitigate the threat of backdoor attacks. First, we uncover a post-hoc workflow underlying most prior work, where defenders passively allow the attack to proceed and then leverage the characteristics of the post-attacked model to uncover poison samples. We reveal that this workflow does not fully exploit defenders' capabilities, and defense pipelines built on it are prone to failure or performance degradation in many scenarios. Second, we suggest a paradigm shift by promoting a proactive mindset in which defenders engage proactively with the entire model training and poison detection pipeline, directly enforcing and magnifying distinctive characteristics of the post-attacked model to facilitate poison detection. Based on this, we formulate a unified framework and provide practical insights on designing de",
    "link": "http://arxiv.org/abs/2205.13616",
    "context": "Title: Towards A Proactive ML Approach for Detecting Backdoor Poison Samples. (arXiv:2205.13616v3 [cs.LG] UPDATED)\nAbstract: Adversaries can embed backdoors in deep learning models by introducing backdoor poison samples into training datasets. In this work, we investigate how to detect such poison samples to mitigate the threat of backdoor attacks. First, we uncover a post-hoc workflow underlying most prior work, where defenders passively allow the attack to proceed and then leverage the characteristics of the post-attacked model to uncover poison samples. We reveal that this workflow does not fully exploit defenders' capabilities, and defense pipelines built on it are prone to failure or performance degradation in many scenarios. Second, we suggest a paradigm shift by promoting a proactive mindset in which defenders engage proactively with the entire model training and poison detection pipeline, directly enforcing and magnifying distinctive characteristics of the post-attacked model to facilitate poison detection. Based on this, we formulate a unified framework and provide practical insights on designing de",
    "path": "papers/22/05/2205.13616.json",
    "total_tokens": 947,
    "translated_title": "面向主动式机器学习方法的后门毒样本检测研究",
    "translated_abstract": "攻击者可以通过在训练数据集中引入后门毒样本来嵌入深度学习模型中的后门。本研究探讨如何检测这些毒样本，以缓解后门攻击的威胁。首先，我们揭示了大多数先前工作中潜在的后处理工作流程，即防御者被动允许攻击进行，并利用后受攻击模型的特征来揭示毒样本。我们发现该工作流程未充分利用防御者的能力，在许多场景下，建立在该工作流程之上的防御管道容易失败或性能下降。其次，我们提出一种范式转变，通过推广主动思维，使得防御者可以主动参与整个模型训练和毒样本检测管道，直接强制实施并放大后受攻击模型的特殊特征以促进毒样本检测。基于此，我们制定了一个统一的框架，并提供了设计防御策略的实用见解。",
    "tldr": "本研究提出了一种主动式机器学习方法，以检测深度学习模型中的后门毒样本，强调防御者的主动介入，直接强制实施并放大后受攻击模型的特殊特征，缓解后门攻击的威胁。",
    "en_tdlr": "This study proposes a proactive approach to detect backdoor poison samples in deep learning models, emphasizing defenders' active intervention, directly enforcing and magnifying distinctive characteristics of the post-attacked model to mitigate the threat of backdoor attacks."
}