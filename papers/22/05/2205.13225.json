{
    "title": "DevFormer: A Symmetric Transformer for Context-Aware Device Placement. (arXiv:2205.13225v3 [cs.LG] UPDATED)",
    "abstract": "In this paper, we present DevFormer, a novel transformer-based architecture for addressing the complex and computationally demanding problem of hardware design optimization. Despite the demonstrated efficacy of transformers in domains including natural language processing and computer vision, their use in hardware design has been limited by the scarcity of offline data. Our approach addresses this limitation by introducing strong inductive biases such as relative positional embeddings and action-permutation symmetricity that effectively capture the hardware context and enable efficient design optimization with limited offline data. We apply DevFoemer to the problem of decoupling capacitor placement and show that it outperforms state-of-the-art methods in both simulated and real hardware, leading to improved performances while reducing the number of components by more than $30\\%$. Finally, we show that our approach achieves promising results in other offline contextual learning-based co",
    "link": "http://arxiv.org/abs/2205.13225",
    "context": "Title: DevFormer: A Symmetric Transformer for Context-Aware Device Placement. (arXiv:2205.13225v3 [cs.LG] UPDATED)\nAbstract: In this paper, we present DevFormer, a novel transformer-based architecture for addressing the complex and computationally demanding problem of hardware design optimization. Despite the demonstrated efficacy of transformers in domains including natural language processing and computer vision, their use in hardware design has been limited by the scarcity of offline data. Our approach addresses this limitation by introducing strong inductive biases such as relative positional embeddings and action-permutation symmetricity that effectively capture the hardware context and enable efficient design optimization with limited offline data. We apply DevFoemer to the problem of decoupling capacitor placement and show that it outperforms state-of-the-art methods in both simulated and real hardware, leading to improved performances while reducing the number of components by more than $30\\%$. Finally, we show that our approach achieves promising results in other offline contextual learning-based co",
    "path": "papers/22/05/2205.13225.json",
    "total_tokens": 859,
    "translated_title": "DevFormer: 一种用于上下文感知硬件布局的对称Transformer",
    "translated_abstract": "本文提出了 DevFormer，一种新颖的基于Transformer的架构，用于解决硬件设计优化等问题。我们通过引入相对位置嵌入和动作置换对称性等强归纳偏置来有效捕捉硬件上下文并使用有限的离线数据实现有效的设计优化。我们将DevFoemer应用于分离电容器放置问题，并展示它在模拟和实际硬件中优于现有方法，可在减少超过30％的组件数量的同时提高性能。最后，我们展示了我们的方法在其他基于离线上下文学习的协作任务中取得了有希望的结果。",
    "tldr": "本文提出了DevFormer，一种用于硬件设计优化的对称Transformer，引入相对位置嵌入和动作置换对称性等强归纳偏置以有效捕捉硬件上下文，能有效地用有限的离线数据实现设计优化，并且在分离电容器放置问题中表现出众，可提高性能并减少组件数量。",
    "en_tdlr": "This paper proposes DevFormer, a symmetric Transformer for hardware design optimization, which effectively captures the hardware context by introducing strong inductive biases such as relative positional embeddings and action-permutation symmetry, and achieves efficient design optimization with limited offline data. DevFormer outperforms state-of-the-art methods in decoupling capacitor placement problem, leading to improved performances while reducing the number of components by more than 30%. Promising results are also achieved in other offline contextual learning-based collaborative tasks."
}