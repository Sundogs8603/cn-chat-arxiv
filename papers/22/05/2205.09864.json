{
    "title": "Automated Scoring for Reading Comprehension via In-context BERT Tuning. (arXiv:2205.09864v2 [cs.LG] UPDATED)",
    "abstract": "Automated scoring of open-ended student responses has the potential to significantly reduce human grader effort. Recent advances in automated scoring often leverage textual representations based on pre-trained language models such as BERT and GPT as input to scoring models. Most existing approaches train a separate model for each item/question, which is suitable for scenarios such as essay scoring where items can be quite different from one another. However, these approaches have two limitations: 1) they fail to leverage item linkage for scenarios such as reading comprehension where multiple items may share a reading passage; 2) they are not scalable since storing one model per item becomes difficult when models have a large number of parameters. In this paper, we report our (grand prize-winning) solution to the National Assessment of Education Progress (NAEP) automated scoring challenge for reading comprehension. Our approach, in-context BERT fine-tuning, produces a single shared scor",
    "link": "http://arxiv.org/abs/2205.09864",
    "context": "Title: Automated Scoring for Reading Comprehension via In-context BERT Tuning. (arXiv:2205.09864v2 [cs.LG] UPDATED)\nAbstract: Automated scoring of open-ended student responses has the potential to significantly reduce human grader effort. Recent advances in automated scoring often leverage textual representations based on pre-trained language models such as BERT and GPT as input to scoring models. Most existing approaches train a separate model for each item/question, which is suitable for scenarios such as essay scoring where items can be quite different from one another. However, these approaches have two limitations: 1) they fail to leverage item linkage for scenarios such as reading comprehension where multiple items may share a reading passage; 2) they are not scalable since storing one model per item becomes difficult when models have a large number of parameters. In this paper, we report our (grand prize-winning) solution to the National Assessment of Education Progress (NAEP) automated scoring challenge for reading comprehension. Our approach, in-context BERT fine-tuning, produces a single shared scor",
    "path": "papers/22/05/2205.09864.json",
    "total_tokens": 894,
    "translated_title": "基于上下文BERT调整的阅读理解自动评分模型",
    "translated_abstract": "自动评分模型有着极大的潜力可以降低人工评分的成本。最近的一些自动评分方法利用了基于预训练语言模型（例如BERT和GPT）的文本表示作为评分模型的输入。然而，大多数方法为每个问题/题目训练一个单独的模型，这适用于试题种类千差万别的作文评分等场景。但是这种方法存在两个问题：1）在阅读理解等与多个问题/题目相关的场景下，无法利用题目之间的关联性；2）当模型参数数量庞大时，存储每个题目独立模型变得很困难。本文介绍了我们在国家教育进步评估（NAEP）阅读理解自动评分挑战赛中获得的（一等奖）解决方案。我们的方法——基于上下文BERT微调，产生一个共用的评分器。",
    "tldr": "本文介绍了一种基于预训练语言模型的文本表示和上下文BERT微调的阅读理解自动评分模型，解决了单个问题/题目模型无法利用题目间关联性以及存储模型困难的问题。",
    "en_tdlr": "This paper presents an automated scoring model for reading comprehension based on pre-trained language models and in-context BERT fine-tuning. It overcomes the limitations of single-model-per-item approach in leveraging item linkage and storage scalability. The approach won the grand prize in the National Assessment of Education Progress automated scoring challenge."
}