{
    "title": "Frustratingly Easy Regularization on Representation Can Boost Deep Reinforcement Learning. (arXiv:2205.14557v2 [cs.LG] UPDATED)",
    "abstract": "Deep reinforcement learning (DRL) gives the promise that an agent learns good policy from high-dimensional information, whereas representation learning removes irrelevant and redundant information and retains pertinent information. In this work, we demonstrate that the learned representation of the $Q$-network and its target $Q$-network should, in theory, satisfy a favorable distinguishable representation property. Specifically, there exists an upper bound on the representation similarity of the value functions of two adjacent time steps in a typical DRL setting. However, through illustrative experiments, we show that the learned DRL agent may violate this property and lead to a sub-optimal policy. Therefore, we propose a simple yet effective regularizer called Policy Evaluation with Easy Regularization on Representation (PEER), which aims to maintain the distinguishable representation property via explicit regularization on internal representations. And we provide the convergence rate",
    "link": "http://arxiv.org/abs/2205.14557",
    "context": "Title: Frustratingly Easy Regularization on Representation Can Boost Deep Reinforcement Learning. (arXiv:2205.14557v2 [cs.LG] UPDATED)\nAbstract: Deep reinforcement learning (DRL) gives the promise that an agent learns good policy from high-dimensional information, whereas representation learning removes irrelevant and redundant information and retains pertinent information. In this work, we demonstrate that the learned representation of the $Q$-network and its target $Q$-network should, in theory, satisfy a favorable distinguishable representation property. Specifically, there exists an upper bound on the representation similarity of the value functions of two adjacent time steps in a typical DRL setting. However, through illustrative experiments, we show that the learned DRL agent may violate this property and lead to a sub-optimal policy. Therefore, we propose a simple yet effective regularizer called Policy Evaluation with Easy Regularization on Representation (PEER), which aims to maintain the distinguishable representation property via explicit regularization on internal representations. And we provide the convergence rate",
    "path": "papers/22/05/2205.14557.json",
    "total_tokens": 846,
    "translated_title": "令人沮丧的简单正则化可以提升深度强化学习的效果",
    "translated_abstract": "深度强化学习(DRL)承诺代理能够从高维信息中学习到良好的策略，而表示学习则能够消除不相关和冗余的信息并保留相关的信息。本文证明了Q网络及其目标Q网络的学习表示在理论上应该满足一个有利的可区分表示属性。具体来说，在典型的DRL设置中两个相邻时间步长的价值函数的表示相似度存在一个上界。但是，通过说明性实验，我们发现学习到的DRL代理可能违反这个属性，并导致次优策略。因此，我们提出了一种名为\"表示简单正则化的策略评估\"(PEER)的简单而有效的正则化器，旨在通过对内部表示进行显式正则化来维持可区分表示属性。同时，我们提供了收敛速度分析。",
    "tldr": "本文证明了DRL的学习表示应该满足一个有利的可区分表示属性，提出了一种正则化器PEER，旨在通过对内部表示进行显式正则化来维持可区分表示属性。",
    "en_tdlr": "This paper demonstrates that the learned representation of the Q-network and its target Q-network should satisfy a favorable distinguishable representation property, and proposes a simple yet effective regularizer called PEER to maintain this property via explicit regularization on internal representations."
}