{
    "title": "Robust Losses for Learning Value Functions. (arXiv:2205.08464v2 [cs.LG] UPDATED)",
    "abstract": "Most value function learning algorithms in reinforcement learning are based on the mean squared (projected) Bellman error. However, squared errors are known to be sensitive to outliers, both skewing the solution of the objective and resulting in high-magnitude and high-variance gradients. To control these high-magnitude updates, typical strategies in RL involve clipping gradients, clipping rewards, rescaling rewards, or clipping errors. While these strategies appear to be related to robust losses -- like the Huber loss -- they are built on semi-gradient update rules which do not minimize a known loss. In this work, we build on recent insights reformulating squared Bellman errors as a saddlepoint optimization problem and propose a saddlepoint reformulation for a Huber Bellman error and Absolute Bellman error. We start from a formalization of robust losses, then derive sound gradient-based approaches to minimize these losses in both the online off-policy prediction and control settings. ",
    "link": "http://arxiv.org/abs/2205.08464",
    "context": "Title: Robust Losses for Learning Value Functions. (arXiv:2205.08464v2 [cs.LG] UPDATED)\nAbstract: Most value function learning algorithms in reinforcement learning are based on the mean squared (projected) Bellman error. However, squared errors are known to be sensitive to outliers, both skewing the solution of the objective and resulting in high-magnitude and high-variance gradients. To control these high-magnitude updates, typical strategies in RL involve clipping gradients, clipping rewards, rescaling rewards, or clipping errors. While these strategies appear to be related to robust losses -- like the Huber loss -- they are built on semi-gradient update rules which do not minimize a known loss. In this work, we build on recent insights reformulating squared Bellman errors as a saddlepoint optimization problem and propose a saddlepoint reformulation for a Huber Bellman error and Absolute Bellman error. We start from a formalization of robust losses, then derive sound gradient-based approaches to minimize these losses in both the online off-policy prediction and control settings. ",
    "path": "papers/22/05/2205.08464.json",
    "total_tokens": 923,
    "translated_title": "学习值函数的鲁棒损失函数",
    "translated_abstract": "强化学习中大多数值函数学习算法基于均方（投影）贝尔曼误差。然而，已知二次误差对离群值很敏感，不仅会偏移目标的解决方案，还会导致具有高振幅和高方差的梯度。为了控制这些高振幅更新，RL中的典型策略包括剪切梯度、剪切奖励、重新缩放奖励或剪切误差。虽然这些策略似乎与Huber loss等鲁棒损失有关，但它们建立在半梯度更新规则上，不会最小化已知损失。在本文中，我们基于最近对将平方贝尔曼误差重构为鞍点优化问题的见解，提出了Huber贝尔曼误差和绝对贝尔曼误差的鞍点重构。我们从鲁棒损失的形式化开始，然后推导出稳健的基于梯度的方法，以在在线脱机预测和控制设置中最小化这些损失。",
    "tldr": "本文提出了基于鲁棒损失的学习值函数的方法，使用了鞍点优化问题的见解，并在在线脱机预测和控制设置中，推导了稳健的基于梯度的方法。"
}