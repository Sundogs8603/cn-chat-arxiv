{
    "title": "On the Calibration of Probabilistic Classifier Sets. (arXiv:2205.10082v2 [stat.ML] UPDATED)",
    "abstract": "Multi-class classification methods that produce sets of probabilistic classifiers, such as ensemble learning methods, are able to model aleatoric and epistemic uncertainty. Aleatoric uncertainty is then typically quantified via the Bayes error, and epistemic uncertainty via the size of the set. In this paper, we extend the notion of calibration, which is commonly used to evaluate the validity of the aleatoric uncertainty representation of a single probabilistic classifier, to assess the validity of an epistemic uncertainty representation obtained by sets of probabilistic classifiers. Broadly speaking, we call a set of probabilistic classifiers calibrated if one can find a calibrated convex combination of these classifiers. To evaluate this notion of calibration, we propose a novel nonparametric calibration test that generalizes an existing test for single probabilistic classifiers to the case of sets of probabilistic classifiers. Making use of this test, we empirically show that ensemb",
    "link": "http://arxiv.org/abs/2205.10082",
    "context": "Title: On the Calibration of Probabilistic Classifier Sets. (arXiv:2205.10082v2 [stat.ML] UPDATED)\nAbstract: Multi-class classification methods that produce sets of probabilistic classifiers, such as ensemble learning methods, are able to model aleatoric and epistemic uncertainty. Aleatoric uncertainty is then typically quantified via the Bayes error, and epistemic uncertainty via the size of the set. In this paper, we extend the notion of calibration, which is commonly used to evaluate the validity of the aleatoric uncertainty representation of a single probabilistic classifier, to assess the validity of an epistemic uncertainty representation obtained by sets of probabilistic classifiers. Broadly speaking, we call a set of probabilistic classifiers calibrated if one can find a calibrated convex combination of these classifiers. To evaluate this notion of calibration, we propose a novel nonparametric calibration test that generalizes an existing test for single probabilistic classifiers to the case of sets of probabilistic classifiers. Making use of this test, we empirically show that ensemb",
    "path": "papers/22/05/2205.10082.json",
    "total_tokens": 838,
    "tldr": "本文提出了用于测量概率分类器集合校准性的新方法，该方法通过计算凸组合，评估了认识不确定度表示的有效性，并且实验证明了集成分类器比单个分类器更准确。"
}