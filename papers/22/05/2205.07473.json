{
    "title": "Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with Dual-Phase Optimization",
    "abstract": "arXiv:2205.07473v3 Announce Type: replace-cross  Abstract: Spiking neural networks (SNNs) operating with asynchronous discrete events show higher energy efficiency with sparse computation. A popular approach for implementing deep SNNs is ANN-SNN conversion combining both efficient training of ANNs and efficient inference of SNNs. However, the accuracy loss is usually non-negligible, especially under a few time steps, which restricts the applications of SNN on latency-sensitive edge devices greatly. In this paper, we first identify that such performance degradation stems from the misrepresentation of the negative or overflow residual membrane potential in SNNs. Inspired by this, we decompose the conversion error into three parts: quantization error, clipping error, and residual membrane potential representation error. With such insights, we propose a two-stage conversion algorithm to minimize those errors respectively. Besides, We show each stage achieves significant performance gains i",
    "link": "https://arxiv.org/abs/2205.07473",
    "context": "Title: Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with Dual-Phase Optimization\nAbstract: arXiv:2205.07473v3 Announce Type: replace-cross  Abstract: Spiking neural networks (SNNs) operating with asynchronous discrete events show higher energy efficiency with sparse computation. A popular approach for implementing deep SNNs is ANN-SNN conversion combining both efficient training of ANNs and efficient inference of SNNs. However, the accuracy loss is usually non-negligible, especially under a few time steps, which restricts the applications of SNN on latency-sensitive edge devices greatly. In this paper, we first identify that such performance degradation stems from the misrepresentation of the negative or overflow residual membrane potential in SNNs. Inspired by this, we decompose the conversion error into three parts: quantization error, clipping error, and residual membrane potential representation error. With such insights, we propose a two-stage conversion algorithm to minimize those errors respectively. Besides, We show each stage achieves significant performance gains i",
    "path": "papers/22/05/2205.07473.json",
    "total_tokens": 888,
    "translated_title": "实现超低延迟下损失less ANN-SNN转换的双相优化",
    "translated_abstract": "神经脉冲网络 (SNNs) 以异步离散事件操作，显示出更高的能耗效率和稀疏计算。实现深度 SNNs 的一种常见方法是通过结合 ANN-SNN 转换来同时实现 ANN 的高效训练和 SNN 的高效推断。然而，精度损失通常是不可忽视的，尤其是在少量时间步下，这大大限制了 SNN 在延迟敏感的边缘设备上的应用。本文首先确定这种性能下降源自于 SNNs 中负面或溢出残余膜电位的错误表示。受此启发，我们将转换误差分解为三部分：量化误差、剪切误差和残余膜电位表示误差。基于这些洞察，我们提出了一个两阶段转换算法，分别最小化这些误差。此外，我们展示每个阶段都取得了显著的性能增益。",
    "tldr": "本研究提出了一种双阶段转换算法，能够最小化量化误差、剪切误差和残余膜电位表示误差，实现了损失less的ANN-SNN转换，提高了在超低延迟条件下的性能。",
    "en_tdlr": "This study presents a two-stage conversion algorithm that minimizes quantization error, clipping error, and residual membrane potential representation error, achieving lossless ANN-SNN conversion and improving performance under ultra-low latency."
}