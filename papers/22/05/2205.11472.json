{
    "title": "Diversity Over Size: On the Effect of Sample and Topic Sizes for Argument Mining Datasets. (arXiv:2205.11472v2 [cs.CL] UPDATED)",
    "abstract": "The task of Argument Mining, that is extracting argumentative sentences for a specific topic from large document sources, is an inherently difficult task for machine learning models and humans alike, as large Argument Mining datasets are rare and recognition of argumentative sentences requires expert knowledge. The task becomes even more difficult if it also involves stance detection of retrieved arguments. Given the cost and complexity of creating suitably large Argument Mining datasets, we ask whether it is necessary for acceptable performance to have datasets growing in size. Our findings show that, when using carefully composed training samples and a model pretrained on related tasks, we can reach 95% of the maximum performance while reducing the training sample size by at least 85%. This gain is consistent across three Argument Mining tasks on three different datasets. We also publish a new dataset for future benchmarking.",
    "link": "http://arxiv.org/abs/2205.11472",
    "context": "Title: Diversity Over Size: On the Effect of Sample and Topic Sizes for Argument Mining Datasets. (arXiv:2205.11472v2 [cs.CL] UPDATED)\nAbstract: The task of Argument Mining, that is extracting argumentative sentences for a specific topic from large document sources, is an inherently difficult task for machine learning models and humans alike, as large Argument Mining datasets are rare and recognition of argumentative sentences requires expert knowledge. The task becomes even more difficult if it also involves stance detection of retrieved arguments. Given the cost and complexity of creating suitably large Argument Mining datasets, we ask whether it is necessary for acceptable performance to have datasets growing in size. Our findings show that, when using carefully composed training samples and a model pretrained on related tasks, we can reach 95% of the maximum performance while reducing the training sample size by at least 85%. This gain is consistent across three Argument Mining tasks on three different datasets. We also publish a new dataset for future benchmarking.",
    "path": "papers/22/05/2205.11472.json",
    "total_tokens": 903,
    "translated_title": "多样性超过规模：关于论证挖掘数据集的样本和主题大小的影响",
    "translated_abstract": "论证挖掘的任务是从大规模文档来源中提取特定主题的论证句子，这对于机器学习模型和人类来说都是一项困难的任务，因为大规模的论证挖掘数据集很少，而识别论证句子需要专业知识。如果还涉及到检测检索到的论证的立场，这个任务就更加困难了。鉴于创建合适规模的论证挖掘数据集的成本和复杂性，我们想知道是否有必要为了达到可接受的性能而增加数据集的大小。我们的研究结果表明，当使用精心组织的训练样本和在相关任务上预训练的模型时，我们可以将训练样本的大小减少至少85％，同时达到最大性能的95％。这种收益在三个不同数据集上的三个论证挖掘任务中是一致的。我们还发布了一个新的数据集供未来进行基准测试。",
    "tldr": "本研究发现，在论证挖掘任务中，使用精心组织的训练样本和预训练模型可以在减小训练样本大小至少85％的情况下，达到最大性能的95％。同时提供了一个新的数据集供未来基准测试。",
    "en_tdlr": "This study found that in the task of argument mining, using carefully composed training samples and pretrained models can achieve 95% of maximum performance while reducing the training sample size by at least 85%. A new dataset for future benchmarking is also provided."
}