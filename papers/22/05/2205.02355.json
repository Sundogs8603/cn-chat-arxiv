{
    "title": "Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In",
    "link": "http://arxiv.org/abs/2205.02355",
    "context": "Title: Relation Extraction as Open-book Examination: Retrieval-enhanced Prompt Tuning. (arXiv:2205.02355v2 [cs.CL] UPDATED)\nAbstract: Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In",
    "path": "papers/22/05/2205.02355.json",
    "total_tokens": 807,
    "translated_title": "关系抽取作为开书考试：检索增强的提示调优",
    "translated_abstract": "预训练语言模型通过展示出卓越的少样本学习能力，在关系抽取方面做出了重要贡献。然而，关系抽取的提示调优方法可能仍然无法推广到那些罕见或困难的模式中。我们将关系抽取视为一种开放式考试，并提出了一种新的检索增强的提示调优的半参数学习范式。我们构建了一个开放式存储库，用于检索基于提示的实例表示和相应的关系标签作为记忆的键值对。在推断过程中，模型可以通过线性插值基于PLM的基本输出与存储库上的非参数最近邻分布来推断关系。",
    "tldr": "提出了一种新的半参数学习范式，即检索增强的提示调优，用于关系抽取。通过构建开放式存储库，并使用线性插值的方式，模型能够在推断过程中根据存储库中的记忆信息推断关系。",
    "en_tdlr": "A novel semi-parametric paradigm, retrieval-enhanced prompt tuning, is proposed for relation extraction. By constructing an open-book datastore and using linear interpolation, the model can infer relations based on the memorized information in the datastore during inference."
}