{
    "title": "PromptDA: Label-guided Data Augmentation for Prompt-based Few-shot Learners. (arXiv:2205.09229v3 [cs.CL] UPDATED)",
    "abstract": "Recent advances in large pre-trained language models (PLMs) lead to impressive gains in natural language understanding (NLU) tasks with task-specific fine-tuning. However, directly fine-tuning PLMs heavily relies on sufficient labeled training instances, which are usually hard to obtain. Prompt-based tuning on PLMs has shown to be powerful for various downstream few-shot tasks. Existing works studying prompt-based tuning for few-shot NLU tasks mainly focus on deriving proper label words with a verbalizer or generating prompt templates to elicit semantics from PLMs. In addition, conventional data augmentation strategies such as synonym substitution, though widely adopted in low-resource scenarios, only bring marginal improvements for prompt-based few-shot learning. Thus, an important research question arises: how to design effective data augmentation methods for prompt-based few-shot tuning? To this end, considering the label semantics are essential in prompt-based tuning, we propose a ",
    "link": "http://arxiv.org/abs/2205.09229",
    "context": "Title: PromptDA: Label-guided Data Augmentation for Prompt-based Few-shot Learners. (arXiv:2205.09229v3 [cs.CL] UPDATED)\nAbstract: Recent advances in large pre-trained language models (PLMs) lead to impressive gains in natural language understanding (NLU) tasks with task-specific fine-tuning. However, directly fine-tuning PLMs heavily relies on sufficient labeled training instances, which are usually hard to obtain. Prompt-based tuning on PLMs has shown to be powerful for various downstream few-shot tasks. Existing works studying prompt-based tuning for few-shot NLU tasks mainly focus on deriving proper label words with a verbalizer or generating prompt templates to elicit semantics from PLMs. In addition, conventional data augmentation strategies such as synonym substitution, though widely adopted in low-resource scenarios, only bring marginal improvements for prompt-based few-shot learning. Thus, an important research question arises: how to design effective data augmentation methods for prompt-based few-shot tuning? To this end, considering the label semantics are essential in prompt-based tuning, we propose a ",
    "path": "papers/22/05/2205.09229.json",
    "total_tokens": 945,
    "tldr": "本论文提出了一种以标签为导向的数据增强方法，用于基于Prompt的小样本学习。与现有的同义词替换方法相比，该方法在提高少样本学习性能方面表现更好。"
}