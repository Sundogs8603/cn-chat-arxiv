{
    "title": "BITE: Textual Backdoor Attacks with Iterative Trigger Injection. (arXiv:2205.12700v3 [cs.CL] UPDATED)",
    "abstract": "Backdoor attacks have become an emerging threat to NLP systems. By providing poisoned training data, the adversary can embed a \"backdoor\" into the victim model, which allows input instances satisfying certain textual patterns (e.g., containing a keyword) to be predicted as a target label of the adversary's choice. In this paper, we demonstrate that it is possible to design a backdoor attack that is both stealthy (i.e., hard to notice) and effective (i.e., has a high attack success rate). We propose BITE, a backdoor attack that poisons the training data to establish strong correlations between the target label and a set of \"trigger words\". These trigger words are iteratively identified and injected into the target-label instances through natural word-level perturbations. The poisoned training data instruct the victim model to predict the target label on inputs containing trigger words, forming the backdoor. Experiments on four text classification datasets show that our proposed attack i",
    "link": "http://arxiv.org/abs/2205.12700",
    "context": "Title: BITE: Textual Backdoor Attacks with Iterative Trigger Injection. (arXiv:2205.12700v3 [cs.CL] UPDATED)\nAbstract: Backdoor attacks have become an emerging threat to NLP systems. By providing poisoned training data, the adversary can embed a \"backdoor\" into the victim model, which allows input instances satisfying certain textual patterns (e.g., containing a keyword) to be predicted as a target label of the adversary's choice. In this paper, we demonstrate that it is possible to design a backdoor attack that is both stealthy (i.e., hard to notice) and effective (i.e., has a high attack success rate). We propose BITE, a backdoor attack that poisons the training data to establish strong correlations between the target label and a set of \"trigger words\". These trigger words are iteratively identified and injected into the target-label instances through natural word-level perturbations. The poisoned training data instruct the victim model to predict the target label on inputs containing trigger words, forming the backdoor. Experiments on four text classification datasets show that our proposed attack i",
    "path": "papers/22/05/2205.12700.json",
    "total_tokens": 891,
    "translated_title": "BITE: 使用迭代触发词注入的文本后门攻击",
    "translated_abstract": "后门攻击已成为自然语言处理系统的新兴威胁。黑客可以通过提供毒化的训练数据将“后门”嵌入受害者模型，这样，满足一定文本模式（例如包含关键字）的输入实例可被预测为黑客控制的目标标签。本文证明可能设计出既难以察觉又具攻击成功率高的后门攻击，提出了使用诱饵数据注入“触发词”的BITE攻击。这些触发词通过自然的单词级扰动不断识别和注入到目标标签实例中。毒化的训练数据指导受害者模型在包含触发词的输入中预测目标标签，形成后门。在四个文本分类数据集上的实验表明，我们的攻击方法可以成功地攻击受害者模型，同时具有难以察觉的特点。",
    "tldr": "提出了一种名为BITE的文本后门攻击方法，通过向训练数据中注入触发词，在迭代的单词级扰动中将这些词注入到输入实例中，成功地攻击了受害者模型。",
    "en_tdlr": "This paper proposes BITE, a backdoor attack method for NLP systems, which poisons the training data by injecting trigger words iteratively into input instances. The proposed attack is both effective and stealthy, as demonstrated by experiments on four text classification datasets."
}