{
    "title": "Multi-scale Attention Flow for Probabilistic Time Series Forecasting. (arXiv:2205.07493v3 [cs.LG] UPDATED)",
    "abstract": "The probability prediction of multivariate time series is a notoriously challenging but practical task. On the one hand, the challenge is how to effectively capture the cross-series correlations between interacting time series, to achieve accurate distribution modeling. On the other hand, we should consider how to capture the contextual information within time series more accurately to model multivariate temporal dynamics of time series. In this work, we proposed a novel non-autoregressive deep learning model, called Multi-scale Attention Normalizing Flow(MANF), where we integrate multi-scale attention and relative position information and the multivariate data distribution is represented by the conditioned normalizing flow. Additionally, compared with autoregressive modeling methods, our model avoids the influence of cumulative error and does not increase the time complexity. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multiva",
    "link": "http://arxiv.org/abs/2205.07493",
    "context": "Title: Multi-scale Attention Flow for Probabilistic Time Series Forecasting. (arXiv:2205.07493v3 [cs.LG] UPDATED)\nAbstract: The probability prediction of multivariate time series is a notoriously challenging but practical task. On the one hand, the challenge is how to effectively capture the cross-series correlations between interacting time series, to achieve accurate distribution modeling. On the other hand, we should consider how to capture the contextual information within time series more accurately to model multivariate temporal dynamics of time series. In this work, we proposed a novel non-autoregressive deep learning model, called Multi-scale Attention Normalizing Flow(MANF), where we integrate multi-scale attention and relative position information and the multivariate data distribution is represented by the conditioned normalizing flow. Additionally, compared with autoregressive modeling methods, our model avoids the influence of cumulative error and does not increase the time complexity. Extensive experiments demonstrate that our model achieves state-of-the-art performance on many popular multiva",
    "path": "papers/22/05/2205.07493.json",
    "total_tokens": 886,
    "translated_title": "多尺度注意力流用于概率时间序列预测",
    "translated_abstract": "多变量时间序列的概率预测是一项具有挑战性但实用的任务。一方面，挑战在于如何有效地捕捉交互时间序列之间的跨序列相关性，以实现准确的分布建模。另一方面，我们应考虑如何更准确地捕捉时间序列内的上下文信息，以建模多变量时间序列的时间动态。在本工作中，我们提出了一种新颖的非自回归深度学习模型，称为多尺度注意力归一化流（MANF），其中我们整合了多尺度注意力和相对位置信息，而多变量数据分布由有条件的归一化流表示。此外，与自回归建模方法相比，我们的模型避免了累积误差的影响，且不增加时间复杂度。大量实验证明，我们的模型在许多流行的多变量时间序列上实现了最先进的性能。",
    "tldr": "本论文提出了一种名为Multi-scale Attention Normalizing Flow(MANF)的非自回归深度学习模型，通过整合多尺度注意力和相对位置信息，实现多变量时间序列的精确分布建模，并且避免了累积误差的影响和增加时间复杂度的问题。",
    "en_tdlr": "This paper proposes a non-autoregressive deep learning model called Multi-scale Attention Normalizing Flow (MANF) to accurately model the distribution of multivariate time series by integrating multi-scale attention and relative position information, while avoiding the issues of cumulative error and increased time complexity."
}