{
    "title": "Global Convergence of Over-parameterized Deep Equilibrium Models. (arXiv:2205.13814v2 [cs.LG] UPDATED)",
    "abstract": "A deep equilibrium model (DEQ) is implicitly defined through an equilibrium point of an infinite-depth weight-tied model with an input-injection. Instead of infinite computations, it solves an equilibrium point directly with root-finding and computes gradients with implicit differentiation. The training dynamics of over-parameterized DEQs are investigated in this study. By supposing a condition on the initial equilibrium point, we show that the unique equilibrium point always exists during the training process, and the gradient descent is proved to converge to a globally optimal solution at a linear convergence rate for the quadratic loss function. In order to show that the required initial condition is satisfied via mild over-parameterization, we perform a fine-grained analysis on random DEQs. We propose a novel probabilistic framework to overcome the technical difficulty in the non-asymptotic analysis of infinite-depth weight-tied models.",
    "link": "http://arxiv.org/abs/2205.13814",
    "context": "Title: Global Convergence of Over-parameterized Deep Equilibrium Models. (arXiv:2205.13814v2 [cs.LG] UPDATED)\nAbstract: A deep equilibrium model (DEQ) is implicitly defined through an equilibrium point of an infinite-depth weight-tied model with an input-injection. Instead of infinite computations, it solves an equilibrium point directly with root-finding and computes gradients with implicit differentiation. The training dynamics of over-parameterized DEQs are investigated in this study. By supposing a condition on the initial equilibrium point, we show that the unique equilibrium point always exists during the training process, and the gradient descent is proved to converge to a globally optimal solution at a linear convergence rate for the quadratic loss function. In order to show that the required initial condition is satisfied via mild over-parameterization, we perform a fine-grained analysis on random DEQs. We propose a novel probabilistic framework to overcome the technical difficulty in the non-asymptotic analysis of infinite-depth weight-tied models.",
    "path": "papers/22/05/2205.13814.json",
    "total_tokens": 881,
    "translated_title": "深度均衡模型全局收敛性研究",
    "translated_abstract": "深度均衡模型（DEQ）通过无限深度的加权-绑定模型中的平衡点与输入注入来隐式定义。它通过根查找直接求解平衡点，并通过隐式微分计算梯度，避免了无限运算。本研究探讨了过参数化DEQ的训练动态。通过对初始平衡点施加一定条件，我们表明唯一平衡点在训练过程中始终存在，并且针对二次损失函数，梯度下降的收敛速率证明为线性收敛到全局最优解。为了展示所需起始条件通过轻微过参数化得到满足，我们在随机DEQ上进行了细粒度的分析。我们提出了一种新的概率框架，以克服非渐近分析无限深度加权绑定模型的技术困难。",
    "tldr": "本研究探讨了深度均衡模型的训练动态，提出唯一平衡点始终存在且梯度下降的收敛速率为线性收敛到全局最优解，可通过轻微过参数化得到满足。",
    "en_tdlr": "This study investigates the training dynamics of deep equilibrium models (DEQs) and proposes that the unique equilibrium point always exists during the training process and the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. This condition can be satisfied via mild over-parameterization."
}