{
    "title": "Optimally tackling covariate shift in RKHS-based nonparametric regression. (arXiv:2205.02986v2 [math.ST] UPDATED)",
    "abstract": "We study the covariate shift problem in the context of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We focus on two natural families of covariate shift problems defined using the likelihood ratios between the source and target distributions. When the likelihood ratios are uniformly bounded, we prove that the kernel ridge regression (KRR) estimator with a carefully chosen regularization parameter is minimax rate-optimal (up to a log factor) for a large family of RKHSs with regular kernel eigenvalues. Interestingly, KRR does not require full knowledge of likelihood ratios apart from an upper bound on them. In striking contrast to the standard statistical setting without covariate shift, we also demonstrate that a naive estimator, which minimizes the empirical risk over the function class, is strictly sub-optimal under covariate shift as compared to KRR. We then address the larger class of covariate shift problems where the likelihood ratio is possibly unbounde",
    "link": "http://arxiv.org/abs/2205.02986",
    "context": "Title: Optimally tackling covariate shift in RKHS-based nonparametric regression. (arXiv:2205.02986v2 [math.ST] UPDATED)\nAbstract: We study the covariate shift problem in the context of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We focus on two natural families of covariate shift problems defined using the likelihood ratios between the source and target distributions. When the likelihood ratios are uniformly bounded, we prove that the kernel ridge regression (KRR) estimator with a carefully chosen regularization parameter is minimax rate-optimal (up to a log factor) for a large family of RKHSs with regular kernel eigenvalues. Interestingly, KRR does not require full knowledge of likelihood ratios apart from an upper bound on them. In striking contrast to the standard statistical setting without covariate shift, we also demonstrate that a naive estimator, which minimizes the empirical risk over the function class, is strictly sub-optimal under covariate shift as compared to KRR. We then address the larger class of covariate shift problems where the likelihood ratio is possibly unbounde",
    "path": "papers/22/05/2205.02986.json",
    "total_tokens": 987,
    "translated_title": "在RKHS的非参数回归中最优解决协变量转移问题",
    "translated_abstract": "在再生核希尔伯特空间（RKHS）上研究了非参数回归中的协变量转移问题。我们关注两个使用源和目标分布之间的似然比定义的自然协变量转移问题族。当似然比被均匀有界时，我们证明带有精心选择的正则化参数的核岭回归(KRR)估计量是极小化率最优的（最多差一个对数因子），对于一大类具有正则核特征值的RKHS而言。有趣的是，除了似然比上界之外，KRR不需要对似然比有完全的知识。与没有协变量转移的标准统计设置形成鲜明对比的是，我们还证明了一个简单估计器，即在函数类中最小化经验风险，与KRR相比，在协变量转移下是严格次优的。然后，我们解决了更大的协变量转移问题类，其中似然比可能是无界的。",
    "tldr": "本文研究了在RKHS的非参数回归中的协变量转移问题，针对两个不同的似然比族，证明了使用KRR估计量具有极小化率最优的特点，尤其是在似然比被均匀有界时。与此同时，本文也证明了，在协变量转移下一个naive的估计器相比于KRR是严格次优的。",
    "en_tdlr": "This paper addresses the covariate shift problem in nonparametric regression over a reproducing kernel Hilbert space (RKHS). The authors prove that the kernel ridge regression (KRR) estimator is minimax rate-optimal for a large family of RKHSs with regular kernel eigenvalues, even when the likelihood ratios are uniformly bounded. Additionally, they show that a naive estimator is strictly sub-optimal under covariate shift as compared to KRR."
}