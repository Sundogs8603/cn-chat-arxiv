{
    "title": "Variational inference via Wasserstein gradient flows. (arXiv:2205.15902v3 [stat.ML] UPDATED)",
    "abstract": "Along with Markov chain Monte Carlo (MCMC) methods, variational inference (VI) has emerged as a central computational approach to large-scale Bayesian inference. Rather than sampling from the true posterior $\\pi$, VI aims at producing a simple but effective approximation $\\hat \\pi$ to $\\pi$ for which summary statistics are easy to compute. However, unlike the well-studied MCMC methodology, algorithmic guarantees for VI are still relatively less well-understood. In this work, we propose principled methods for VI, in which $\\hat \\pi$ is taken to be a Gaussian or a mixture of Gaussians, which rest upon the theory of gradient flows on the Bures--Wasserstein space of Gaussian measures. Akin to MCMC, it comes with strong theoretical guarantees when $\\pi$ is log-concave.",
    "link": "http://arxiv.org/abs/2205.15902",
    "context": "Title: Variational inference via Wasserstein gradient flows. (arXiv:2205.15902v3 [stat.ML] UPDATED)\nAbstract: Along with Markov chain Monte Carlo (MCMC) methods, variational inference (VI) has emerged as a central computational approach to large-scale Bayesian inference. Rather than sampling from the true posterior $\\pi$, VI aims at producing a simple but effective approximation $\\hat \\pi$ to $\\pi$ for which summary statistics are easy to compute. However, unlike the well-studied MCMC methodology, algorithmic guarantees for VI are still relatively less well-understood. In this work, we propose principled methods for VI, in which $\\hat \\pi$ is taken to be a Gaussian or a mixture of Gaussians, which rest upon the theory of gradient flows on the Bures--Wasserstein space of Gaussian measures. Akin to MCMC, it comes with strong theoretical guarantees when $\\pi$ is log-concave.",
    "path": "papers/22/05/2205.15902.json",
    "total_tokens": 795,
    "translated_title": "基于Wasserstein梯度流的变分推断",
    "translated_abstract": "随着马尔可夫链蒙特卡洛 (MCMC) 方法一起，变分推断（VI）已经成为大规模贝叶斯推断的中心计算方法。VI 不是从真实后验 $\\pi$ 中进行采样，而是旨在生成一个简单而有效的近似 $\\hat \\pi$，使得摘要统计量易于计算。然而，与广为研究的 MCMC 方法不同，VI 的算法保证仍然相对较少被理解。在本工作中，我们提出了基于高斯或高斯混合分布的变分推断方法，这些方法基于高斯测度的Bures-Wasserstein 空间上的梯度流理论。当 $\\pi$ 是对数凹的时候，与MCMC类似，该方法具有强大的理论保证。",
    "tldr": "本文提出了一种基于Wasserstein梯度流的变分推断方法，该方法使用高斯或高斯混合分布并在处理对数凹 $\\pi$ 时具有强大的理论保证。",
    "en_tdlr": "This paper proposes a variational inference method based on Wasserstein gradient flows, which uses Gaussian or Gaussian mixture distributions and has strong theoretical guarantees when dealing with log-concave $\\pi$."
}