{
    "title": "Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. (arXiv:2205.09702v5 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks (GNNs) are among the most powerful tools in deep learning. They routinely solve complex problems on unstructured networks, such as node classification, graph classification, or link prediction, with high accuracy. However, both inference and training of GNNs are complex, and they uniquely combine the features of irregular graph processing with dense and regular computations. This complexity makes it very challenging to execute GNNs efficiently on modern massively parallel architectures. To alleviate this, we first design a taxonomy of parallelism in GNNs, considering data and model parallelism, and different forms of pipelining. Then, we use this taxonomy to investigate the amount of parallelism in numerous GNN models, GNN-driven machine learning tasks, software frameworks, or hardware accelerators. We use the work-depth model, and we also assess communication volume and synchronization. We specifically focus on the sparsity/density of the associated tensors, in o",
    "link": "http://arxiv.org/abs/2205.09702",
    "context": "Title: Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. (arXiv:2205.09702v5 [cs.LG] UPDATED)\nAbstract: Graph neural networks (GNNs) are among the most powerful tools in deep learning. They routinely solve complex problems on unstructured networks, such as node classification, graph classification, or link prediction, with high accuracy. However, both inference and training of GNNs are complex, and they uniquely combine the features of irregular graph processing with dense and regular computations. This complexity makes it very challenging to execute GNNs efficiently on modern massively parallel architectures. To alleviate this, we first design a taxonomy of parallelism in GNNs, considering data and model parallelism, and different forms of pipelining. Then, we use this taxonomy to investigate the amount of parallelism in numerous GNN models, GNN-driven machine learning tasks, software frameworks, or hardware accelerators. We use the work-depth model, and we also assess communication volume and synchronization. We specifically focus on the sparsity/density of the associated tensors, in o",
    "path": "papers/22/05/2205.09702.json",
    "total_tokens": 920,
    "translated_title": "并行和分布式图神经网络：深入并发性分析",
    "translated_abstract": "图神经网络（GNNs）是深度学习中最强大的工具之一。它们常常在无结构网络上解决复杂问题，如节点分类、图分类或链接预测，具有高准确性。然而，GNNs的推理和训练都非常复杂，它们独特地将不规则图处理的特性与密集和规则计算相结合。这种复杂性使得在现代大规模并行架构上高效执行GNNs非常具有挑战性。为了缓解这一问题，我们首先设计了GNNs中的并行性分类方法，考虑了数据并行性和模型并行性，以及不同形式的流水线。然后，我们使用这个分类方法来研究众多GNN模型、GNN驱动的机器学习任务、软件框架或硬件加速器中的并行性。我们使用工作深度模型，并评估通信量和同步。我们特别关注相关张量的稀疏性/密度。",
    "tldr": "这项研究深入分析了并行和分布式图神经网络，在设计了并行性分类方法后，研究了各种GNN模型、任务、软件框架和硬件加速器中的并行性，并重点关注相关张量的稀疏性/密度。",
    "en_tdlr": "This research provides an in-depth analysis of parallel and distributed graph neural networks, presenting a taxonomy of parallelism and investigating the amount of parallelism in various GNN models, tasks, software frameworks, and hardware accelerators, with a specific focus on the sparsity/density of associated tensors."
}