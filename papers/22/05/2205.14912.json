{
    "title": "E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation. (arXiv:2205.14912v3 [cs.CL] UPDATED)",
    "abstract": "Sequence-to-sequence (seq2seq) learning is a popular fashion for large-scale pretraining language models. However, the prior seq2seq pretraining models generally focus on reconstructive objectives on the decoder side and neglect the effect of encoder-side supervision, which we argue may lead to sub-optimal performance. To verify our hypothesis, we first empirically study the functionalities of the encoder and decoder in seq2seq pretrained language models, and find that the encoder takes an important but under-exploitation role than the decoder regarding the downstream performance and neuron activation. Therefore, we propose an encoding-enhanced seq2seq pretraining strategy, namely E2S2, which improves the seq2seq models via integrating more efficient self-supervised information into the encoders. Specifically, E2S2 adopts two self-supervised objectives on the encoder side from two aspects: 1) locally denoising the corrupted sentence (denoising objective); and 2) globally learning bette",
    "link": "http://arxiv.org/abs/2205.14912",
    "context": "Title: E2S2: Encoding-Enhanced Sequence-to-Sequence Pretraining for Language Understanding and Generation. (arXiv:2205.14912v3 [cs.CL] UPDATED)\nAbstract: Sequence-to-sequence (seq2seq) learning is a popular fashion for large-scale pretraining language models. However, the prior seq2seq pretraining models generally focus on reconstructive objectives on the decoder side and neglect the effect of encoder-side supervision, which we argue may lead to sub-optimal performance. To verify our hypothesis, we first empirically study the functionalities of the encoder and decoder in seq2seq pretrained language models, and find that the encoder takes an important but under-exploitation role than the decoder regarding the downstream performance and neuron activation. Therefore, we propose an encoding-enhanced seq2seq pretraining strategy, namely E2S2, which improves the seq2seq models via integrating more efficient self-supervised information into the encoders. Specifically, E2S2 adopts two self-supervised objectives on the encoder side from two aspects: 1) locally denoising the corrupted sentence (denoising objective); and 2) globally learning bette",
    "path": "papers/22/05/2205.14912.json",
    "total_tokens": 918,
    "translated_title": "E2S2: 增强编码的序列到序列预训练模型用于语言理解和生成",
    "translated_abstract": "序列到序列(seq2seq)学习是大规模预训练语言模型中流行的方法。然而，之前的seq2seq预训练模型通常只关注解码器方面的重构目标，忽视了编码器方面的监督作用，我们认为这可能导致性能不佳。为验证我们的假设，我们首先实证研究了序列到序列预训练语言模型中编码器和解码器的功能，并发现编码器在下游性能和神经元激活方面扮演着重要但被低估的角色。因此，我们提出了一种增强编码的序列到序列预训练策略，称为E2S2，通过将更有效的自我监督信息整合到编码器中来改进seq2seq模型。具体而言，E2S2在编码器端采用了两个自我监督目标：1) 本地去噪受损句子（去噪目标）；2) 全局学习更好的句子表示（全局学习目标）。",
    "tldr": "E2S2提出了一种增强编码的序列到序列预训练策略，通过在编码器端引入更有效的自我监督信息，改进了语言模型的下游性能。"
}