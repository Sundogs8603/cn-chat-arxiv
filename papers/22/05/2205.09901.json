{
    "title": "Cardinality-Minimal Explanations for Monotonic Neural Networks. (arXiv:2205.09901v3 [cs.LG] UPDATED)",
    "abstract": "In recent years, there has been increasing interest in explanation methods for neural model predictions that offer precise formal guarantees. These include abductive (respectively, contrastive) methods, which aim to compute minimal subsets of input features that are sufficient for a given prediction to hold (respectively, to change a given prediction). The corresponding decision problems are, however, known to be intractable. In this paper, we investigate whether tractability can be regained by focusing on neural models implementing a monotonic function. Although the relevant decision problems remain intractable, we can show that they become solvable in polynomial time by means of greedy algorithms if we additionally assume that the activation functions are continuous everywhere and differentiable almost everywhere. Our experiments suggest favourable performance of our algorithms.",
    "link": "http://arxiv.org/abs/2205.09901",
    "context": "Title: Cardinality-Minimal Explanations for Monotonic Neural Networks. (arXiv:2205.09901v3 [cs.LG] UPDATED)\nAbstract: In recent years, there has been increasing interest in explanation methods for neural model predictions that offer precise formal guarantees. These include abductive (respectively, contrastive) methods, which aim to compute minimal subsets of input features that are sufficient for a given prediction to hold (respectively, to change a given prediction). The corresponding decision problems are, however, known to be intractable. In this paper, we investigate whether tractability can be regained by focusing on neural models implementing a monotonic function. Although the relevant decision problems remain intractable, we can show that they become solvable in polynomial time by means of greedy algorithms if we additionally assume that the activation functions are continuous everywhere and differentiable almost everywhere. Our experiments suggest favourable performance of our algorithms.",
    "path": "papers/22/05/2205.09901.json",
    "total_tokens": 771,
    "translated_title": "基于单调神经网络的最小基数解释方法",
    "translated_abstract": "近年来，对于神经网络模型预测的解释方法越来越受到关注，其中最为准确且形式化的方法包括寻找对于预测结果至关重要的最小输入特征集合，以及改变预测结果的最小输入特征集合。但这些相应的决策问题在计算上是难以处理的。本文研究了单调函数的神经模型在此方面的适用性，发现如果假设激活函数在各处均连续且几乎无处不可导，则可以通过贪心算法在多项式时间内解决相关的难题。我们通过实验验证了该算法的性能。",
    "tldr": "本文研究基于单调神经网络的解释方法，假设激活函数连续可导，则可以通过贪心算法在多项式时间内寻找到最小输入特征集合，解决难以处理的决策问题。"
}