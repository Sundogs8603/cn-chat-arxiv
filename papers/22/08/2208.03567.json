{
    "title": "Proof-of-Learning is Currently More Broken Than You Think. (arXiv:2208.03567v2 [cs.LG] UPDATED)",
    "abstract": "Proof-of-Learning (PoL) proposes that a model owner logs training checkpoints to establish a proof of having expended the computation necessary for training. The authors of PoL forego cryptographic approaches and trade rigorous security guarantees for scalability to deep learning. They empirically argued the benefit of this approach by showing how spoofing--computing a proof for a stolen model--is as expensive as obtaining the proof honestly by training the model. However, recent work has provided a counter-example and thus has invalidated this observation.  In this work we demonstrate, first, that while it is true that current PoL verification is not robust to adversaries, recent work has largely underestimated this lack of robustness. This is because existing spoofing strategies are either unreproducible or target weakened instantiations of PoL--meaning they are easily thwarted by changing hyperparameters of the verification. Instead, we introduce the first spoofing strategies that c",
    "link": "http://arxiv.org/abs/2208.03567",
    "context": "Title: Proof-of-Learning is Currently More Broken Than You Think. (arXiv:2208.03567v2 [cs.LG] UPDATED)\nAbstract: Proof-of-Learning (PoL) proposes that a model owner logs training checkpoints to establish a proof of having expended the computation necessary for training. The authors of PoL forego cryptographic approaches and trade rigorous security guarantees for scalability to deep learning. They empirically argued the benefit of this approach by showing how spoofing--computing a proof for a stolen model--is as expensive as obtaining the proof honestly by training the model. However, recent work has provided a counter-example and thus has invalidated this observation.  In this work we demonstrate, first, that while it is true that current PoL verification is not robust to adversaries, recent work has largely underestimated this lack of robustness. This is because existing spoofing strategies are either unreproducible or target weakened instantiations of PoL--meaning they are easily thwarted by changing hyperparameters of the verification. Instead, we introduce the first spoofing strategies that c",
    "path": "papers/22/08/2208.03567.json",
    "total_tokens": 969,
    "translated_title": "学习证明机制目前存在许多问题",
    "translated_abstract": "学习证明（PoL）提出，模型所有者记录训练检查点，以建立为训练耗费的计算提供证明。 PoL的作者放弃了加密方法，以换取深度学习的可扩展性，从而换取了严格的安全保证。他们通过展示盗用模型的计算证明--计算偷来的模型的证明，和真正地训练模型所需要的证明一样昂贵来实证证明了这种方法的优点。但是，最近的研究提供了一个反例，从而使这个观察失效。在这项工作中，我们首先证明，尽管当前PoL验证对于对手来说不稳健是真实的，但是最近的工作大大低估了这种缺乏稳健性。这是因为现有的欺骗策略要么不可重现，要么针对PoL的削弱形式--这意味着它们很容易被更改验证的超参数来挫败。相反，我们引入了第一批欺骗策略，它们可以打破适用于PoL的最新防御方法，但代价很低。",
    "tldr": "学习证明机制PoL存在不少问题，由于现有的欺骗策略很容易被打败或无法重现，因此对对手的安全保障不稳健。新的欺骗策略引入可以打破PoL的最新防御方法，但成本较低。",
    "en_tdlr": "Proof-of-Learning (PoL) mechanism has issues with robustness against adversaries due to existing spoofing strategies being easily defeated or unreproducible. New spoofing strategies introduced in this work can break the latest defenses of PoL at a lower cost."
}