{
    "title": "Labeling Chaos to Learning Harmony: Federated Learning with Noisy Labels. (arXiv:2208.09378v3 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) is a distributed machine learning paradigm that enables learning models from decentralized private datasets, where the labeling effort is entrusted to the clients. While most existing FL approaches assume high-quality labels are readily available on users' devices; in reality, label noise can naturally occur in FL and is closely related to clients' characteristics. Due to scarcity of available data and significant label noise variations among clients in FL, existing state-of-the-art centralized approaches exhibit unsatisfactory performance, while prior FL studies rely on excessive on-device computational schemes or additional clean data available on server. Here, we propose FedLN, a framework to deal with label noise across different FL training stages; namely, FL initialization, on-device model training, and server model aggregation, able to accommodate the diverse computational capabilities of devices in a FL system. Specifically, FedLN computes per-client noi",
    "link": "http://arxiv.org/abs/2208.09378",
    "context": "Title: Labeling Chaos to Learning Harmony: Federated Learning with Noisy Labels. (arXiv:2208.09378v3 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) is a distributed machine learning paradigm that enables learning models from decentralized private datasets, where the labeling effort is entrusted to the clients. While most existing FL approaches assume high-quality labels are readily available on users' devices; in reality, label noise can naturally occur in FL and is closely related to clients' characteristics. Due to scarcity of available data and significant label noise variations among clients in FL, existing state-of-the-art centralized approaches exhibit unsatisfactory performance, while prior FL studies rely on excessive on-device computational schemes or additional clean data available on server. Here, we propose FedLN, a framework to deal with label noise across different FL training stages; namely, FL initialization, on-device model training, and server model aggregation, able to accommodate the diverse computational capabilities of devices in a FL system. Specifically, FedLN computes per-client noi",
    "path": "papers/22/08/2208.09378.json",
    "total_tokens": 958,
    "translated_title": "从混乱标签中学习和谐：带有噪声标签的联邦学习",
    "translated_abstract": "联邦学习（FL）是一种分布式机器学习范例，它能够从分散的私有数据集中学习模型，标签工作委托给客户端。虽然大多数现有的FL方法假设高质量标签已经在用户设备上准备好了，但实际上，在FL中自然可能出现标签噪声，这与客户端的特性密切相关。由于在FL中可用数据的稀缺性和客户端之间显着的标签噪声变化，现有的最先进的集中式方法表现不尽如人意，而先前的FL研究则依赖于额外的干净数据或过度的设备端计算方案。在这里，我们提出了FedLN，一个框架，用于在不同的FL训练阶段处理标签噪声；即FL初始化、设备端模型训练和服务器模型聚合，能够适应FL系统中各种设备的计算能力。具体而言，FedLN计算每个客户端的噪声",
    "tldr": "该论文提出一个名为FedLN的框架，用于在联邦学习（FL）中处理标签噪声，这是FL中一个普遍存在且影响性能的问题。FedLN能够适应不同客户端的计算能力，涵盖FL初始化、设备端模型训练和服务器模型聚合三个阶段。",
    "en_tdlr": "This paper proposes a framework called FedLN for dealing with label noise in Federated Learning (FL), which is a common and performance-impacting problem in FL. FedLN is able to accommodate the diverse computational capabilities of clients and covers three stages of FL: initialization, on-device model training, and server model aggregation."
}