{
    "title": "On the Privacy Effect of Data Enhancement via the Lens of Memorization",
    "abstract": "arXiv:2208.08270v3 Announce Type: replace  Abstract: Machine learning poses severe privacy concerns as it has been shown that the learned models can reveal sensitive information about their training data. Many works have investigated the effect of widely adopted data augmentation and adversarial training techniques, termed data enhancement in the paper, on the privacy leakage of machine learning models. Such privacy effects are often measured by membership inference attacks (MIAs), which aim to identify whether a particular example belongs to the training set or not. We propose to investigate privacy from a new perspective called memorization. Through the lens of memorization, we find that previously deployed MIAs produce misleading results as they are less likely to identify samples with higher privacy risks as members compared to samples with low privacy risks. To solve this problem, we deploy a recent attack that can capture individual samples' memorization degrees for evaluation. T",
    "link": "https://arxiv.org/abs/2208.08270",
    "context": "Title: On the Privacy Effect of Data Enhancement via the Lens of Memorization\nAbstract: arXiv:2208.08270v3 Announce Type: replace  Abstract: Machine learning poses severe privacy concerns as it has been shown that the learned models can reveal sensitive information about their training data. Many works have investigated the effect of widely adopted data augmentation and adversarial training techniques, termed data enhancement in the paper, on the privacy leakage of machine learning models. Such privacy effects are often measured by membership inference attacks (MIAs), which aim to identify whether a particular example belongs to the training set or not. We propose to investigate privacy from a new perspective called memorization. Through the lens of memorization, we find that previously deployed MIAs produce misleading results as they are less likely to identify samples with higher privacy risks as members compared to samples with low privacy risks. To solve this problem, we deploy a recent attack that can capture individual samples' memorization degrees for evaluation. T",
    "path": "papers/22/08/2208.08270.json",
    "total_tokens": 883,
    "translated_title": "关于数据增强对隐私影响的研究",
    "translated_abstract": "机器学习带来了严重的隐私问题，因为已经显示学习的模型可能会揭示有关其训练数据的敏感信息。 许多研究探讨了广泛采用的数据增强和对抗训练技术（在论文中称为数据增强）对机器学习模型隐私泄露的影响。 这种隐私效应通常通过成员推理攻击（MIAs）来衡量，其目的是确定特定示例是否属于训练集。 我们提出从称为记忆的新视角来研究隐私。 通过记忆的视角，我们发现先前部署的MIAs产生误导性结果，因为它们不太可能识别高隐私风险样本是否作为成员，相比之下，识别低隐私风险样本更容易。 为解决这一问题，我们部署了一种最近的攻击，可以捕获个体样本的记忆程度进行评估。",
    "tldr": "该论文通过记忆的视角研究了数据增强对机器学习模型隐私泄露的影响，发现先前的成员推理攻击产生误导性结果，提出了一种新的攻击方法来评估个体样本的记忆程度。",
    "en_tdlr": "The paper investigates the privacy effects of data enhancement on machine learning models from a perspective called memorization, revealing misleading results from previous membership inference attacks and proposing a new method to evaluate the memorization degree of individual samples."
}