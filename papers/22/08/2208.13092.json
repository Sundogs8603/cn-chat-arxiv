{
    "title": "Lottery Aware Sparsity Hunting: Enabling Federated Learning on Resource-Limited Edge. (arXiv:2208.13092v3 [cs.LG] UPDATED)",
    "abstract": "Edge devices can benefit remarkably from federated learning due to their distributed nature; however, their limited resource and computing power poses limitations in deployment. A possible solution to this problem is to utilize off-the-shelf sparse learning algorithms at the clients to meet their resource budget. However, such naive deployment in the clients causes significant accuracy degradation, especially for highly resource-constrained clients. In particular, our investigations reveal that the lack of consensus in the sparsity masks among the clients may potentially slow down the convergence of the global model and cause a substantial accuracy drop. With these observations, we present \\textit{federated lottery aware sparsity hunting} (FLASH), a unified sparse learning framework for training a sparse sub-model that maintains the performance under ultra-low parameter density while yielding proportional communication benefits. Moreover, given that different clients may have different",
    "link": "http://arxiv.org/abs/2208.13092",
    "context": "Title: Lottery Aware Sparsity Hunting: Enabling Federated Learning on Resource-Limited Edge. (arXiv:2208.13092v3 [cs.LG] UPDATED)\nAbstract: Edge devices can benefit remarkably from federated learning due to their distributed nature; however, their limited resource and computing power poses limitations in deployment. A possible solution to this problem is to utilize off-the-shelf sparse learning algorithms at the clients to meet their resource budget. However, such naive deployment in the clients causes significant accuracy degradation, especially for highly resource-constrained clients. In particular, our investigations reveal that the lack of consensus in the sparsity masks among the clients may potentially slow down the convergence of the global model and cause a substantial accuracy drop. With these observations, we present \\textit{federated lottery aware sparsity hunting} (FLASH), a unified sparse learning framework for training a sparse sub-model that maintains the performance under ultra-low parameter density while yielding proportional communication benefits. Moreover, given that different clients may have different",
    "path": "papers/22/08/2208.13092.json",
    "total_tokens": 904,
    "translated_title": "抽奖感知的稀疏度搜索: 在资源有限的边缘环境中实现联邦学习",
    "translated_abstract": "由于边缘设备的分布特性，联邦学习可以极大地受益于它们，然而，由于资源和计算能力的限制，部署存在局限性。解决这个问题的一个可能的方法是在客户端利用现成的稀疏学习算法来满足它们的资源预算。然而，这样朴素地在客户端部署会导致显著的准确度下降，特别是对于资源受限的客户端。我们的研究发现，在客户端之间的稀疏度掩码缺乏共识可能会减慢全局模型的收敛速度并导致显著的准确度下降。基于这些观察，我们提出了一种统一的稀疏学习框架——联邦抽奖感知稀疏度搜索(FLASH)，用于训练一个在超低参数密度下保持性能的稀疏子模型，同时产生相应的通信优势。",
    "tldr": "本论文提出了一种名为FLASH的联邦学习框架，该框架通过抽奖感知的稀疏度搜索，在资源有限的边缘环境中训练稀疏子模型，从而保持性能并获得相应的通信优势。",
    "en_tdlr": "This paper proposes FLASH, a federated learning framework that trains sparse sub-models using lottery aware sparsity hunting, enabling performance maintenance and communication benefits in resource-limited edge environments."
}