{
    "title": "Out of the BLEU: how should we assess quality of the Code Generation models?. (arXiv:2208.03133v2 [cs.SE] UPDATED)",
    "abstract": "In recent years, researchers have created and introduced a significant number of various code generation models. As human evaluation of every new model version is unfeasible, the community adopted automatic evaluation metrics such as BLEU to approximate the results of human judgement. These metrics originate from the machine translation domain and it is unclear whether they are applicable for the code generation tasks and how well they agree with the human evaluation on this task. There are also other metrics, CodeBLEU and RUBY, developed to estimate the similarity of code, that take into account the properties of source code. However, for these metrics there are hardly any studies on their agreement with the human evaluation. Despite all that, minimal differences in the metric scores have been used in recent papers to claim superiority of some code generation models over the others.  In this paper, we present a study on the applicability of six metrics -BLEU, ROUGE-L, METEOR, ChrF, ",
    "link": "http://arxiv.org/abs/2208.03133",
    "context": "Title: Out of the BLEU: how should we assess quality of the Code Generation models?. (arXiv:2208.03133v2 [cs.SE] UPDATED)\nAbstract: In recent years, researchers have created and introduced a significant number of various code generation models. As human evaluation of every new model version is unfeasible, the community adopted automatic evaluation metrics such as BLEU to approximate the results of human judgement. These metrics originate from the machine translation domain and it is unclear whether they are applicable for the code generation tasks and how well they agree with the human evaluation on this task. There are also other metrics, CodeBLEU and RUBY, developed to estimate the similarity of code, that take into account the properties of source code. However, for these metrics there are hardly any studies on their agreement with the human evaluation. Despite all that, minimal differences in the metric scores have been used in recent papers to claim superiority of some code generation models over the others.  In this paper, we present a study on the applicability of six metrics -BLEU, ROUGE-L, METEOR, ChrF, ",
    "path": "papers/22/08/2208.03133.json",
    "total_tokens": 1015,
    "tldr": "评估代码生成模型，BLEU等指标无法完全代替人工评估。本文提出新的评估方法并在基准数据集上进行了实验测试。",
    "en_tdlr": "BLEU and other metrics cannot fully replace human evaluation in assessing code generation models. This paper proposes a new evaluation method and conducts experiments on benchmark datasets."
}