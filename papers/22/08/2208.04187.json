{
    "title": "DIVISION: Memory Efficient Training via Dual Activation Precision. (arXiv:2208.04187v5 [cs.LG] UPDATED)",
    "abstract": "Activation compressed training provides a solution towards reducing the memory cost of training deep neural networks~(DNNs). However, state-of-the-art work combines a search of quantization bit-width with the training, which makes the procedure complicated and less transparent. To this end, we propose a simple and effective method to compress DNN training. Our method is motivated by an instructive observation: DNN backward propagation mainly utilizes the low-frequency component (LFC) of the activation maps, while the majority of memory is for caching the high-frequency component (HFC) during the training. This indicates the HFC of activation maps is highly redundant and compressible during DNN training, which inspires our proposed Dual Activation Precision (DIVISION). During the training, DIVISION preserves the high-precision copy of LFC and compresses the HFC into a light-weight copy with low numerical precision. This can significantly reduce the memory cost without negatively affecti",
    "link": "http://arxiv.org/abs/2208.04187",
    "context": "Title: DIVISION: Memory Efficient Training via Dual Activation Precision. (arXiv:2208.04187v5 [cs.LG] UPDATED)\nAbstract: Activation compressed training provides a solution towards reducing the memory cost of training deep neural networks~(DNNs). However, state-of-the-art work combines a search of quantization bit-width with the training, which makes the procedure complicated and less transparent. To this end, we propose a simple and effective method to compress DNN training. Our method is motivated by an instructive observation: DNN backward propagation mainly utilizes the low-frequency component (LFC) of the activation maps, while the majority of memory is for caching the high-frequency component (HFC) during the training. This indicates the HFC of activation maps is highly redundant and compressible during DNN training, which inspires our proposed Dual Activation Precision (DIVISION). During the training, DIVISION preserves the high-precision copy of LFC and compresses the HFC into a light-weight copy with low numerical precision. This can significantly reduce the memory cost without negatively affecti",
    "path": "papers/22/08/2208.04187.json",
    "total_tokens": 915,
    "translated_title": "DIVISION: 通过双激活精度实现高效内存训练",
    "translated_abstract": "激活压缩训练提供了一个减少深度神经网络训练内存成本的解决方案。然而，目前最先进的工作将量化位宽搜索与训练结合在一起，使得过程复杂且不够透明。为此，我们提出了一种简单有效的压缩DNN训练方法。我们的方法得到了如下结论：神经网络反向传播主要利用激活图的低频组成部分（LFC)，而大部分内存是用来缓存训练期间的高频组成部分（HFC）。这表明激活图的HFC在DNN训练期间高度冗余且易于压缩，这也启发了我们提出的Dual Activation Precision (DIVISION)。在训练过程中，DIVISION保留LFC的高精度副本，并将HFC压缩成低数值精度的轻量副本。这可以显著降低内存成本而不会对DNN训练产生负面影响。",
    "tldr": "本文提出一种内存高效的DNN训练方法DIVISION，通过保留LFC的高精度，将HFC压缩成低精度的轻量副本，显著减少了内存成本。",
    "en_tdlr": "This paper proposes a memory-efficient DNN training method called DIVISION, which compresses the high-frequency component of activation maps into a light-weight copy with low numerical precision while preserving the high-precision copy of the low-frequency component, significantly reducing the memory cost of training without negative effects."
}