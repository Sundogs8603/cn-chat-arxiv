{
    "title": "Antecedent Predictions Are More Important Than You Think: An Effective Method for Tree-Based Code Generation. (arXiv:2208.09998v3 [cs.SE] UPDATED)",
    "abstract": "Code generation focuses on the automatic conversion of natural language (NL) utterances into code snippets. The sequence-to-tree (Seq2Tree) approaches are proposed for code generation, with the guarantee of the grammatical correctness of the generated code, which generate the subsequent Abstract Syntax Tree (AST) node relying on antecedent predictions of AST nodes. Existing Seq2Tree methods tend to treat both antecedent predictions and subsequent predictions equally. However, under the AST constraints, it is difficult for Seq2Tree models to produce the correct subsequent prediction based on incorrect antecedent predictions. Thus, antecedent predictions ought to receive more attention than subsequent predictions. To this end, in this paper, we propose an effective method, named Antecedent Prioritized (AP) Loss, that helps the model attach importance to antecedent predictions by exploiting the position information of the generated AST nodes. We design an AST-to-Vector (AST2Vec) method, t",
    "link": "http://arxiv.org/abs/2208.09998",
    "context": "Title: Antecedent Predictions Are More Important Than You Think: An Effective Method for Tree-Based Code Generation. (arXiv:2208.09998v3 [cs.SE] UPDATED)\nAbstract: Code generation focuses on the automatic conversion of natural language (NL) utterances into code snippets. The sequence-to-tree (Seq2Tree) approaches are proposed for code generation, with the guarantee of the grammatical correctness of the generated code, which generate the subsequent Abstract Syntax Tree (AST) node relying on antecedent predictions of AST nodes. Existing Seq2Tree methods tend to treat both antecedent predictions and subsequent predictions equally. However, under the AST constraints, it is difficult for Seq2Tree models to produce the correct subsequent prediction based on incorrect antecedent predictions. Thus, antecedent predictions ought to receive more attention than subsequent predictions. To this end, in this paper, we propose an effective method, named Antecedent Prioritized (AP) Loss, that helps the model attach importance to antecedent predictions by exploiting the position information of the generated AST nodes. We design an AST-to-Vector (AST2Vec) method, t",
    "path": "papers/22/08/2208.09998.json",
    "total_tokens": 848,
    "translated_title": "先行预测比你想象的更重要：一种有效的基于树的代码生成方法",
    "translated_abstract": "代码生成专注于将自然语言表达转化为代码片段。提出了序列到树（Seq2Tree）方法用于代码生成，保证生成的代码语法正确，并且依赖于AST节点的先行预测来生成后续的抽象语法树节点。现有的Seq2Tree方法倾向于平等对待先行预测和后续预测。然而，在抽象语法树的约束下，Seq2Tree模型很难基于错误的先行预测产生正确的后续预测。因此，先行预测应该比后续预测获得更多关注。为此，本文提出了一种名为先行优先（AP）损失的有效方法，通过利用生成的抽象语法树节点的位置信息，帮助模型重视先行预测。我们设计了一种抽象语法树到向量（AST2Vec）方法，",
    "tldr": "先行预测比后续预测更重要。本文提出了一种名为AP损失的方法，通过利用生成的抽象语法树节点的位置信息，帮助模型重视先行预测。",
    "en_tdlr": "Antecedent predictions are more important than subsequent predictions. This paper proposes an effective method called AP loss that utilizes the position information of generated abstract syntax tree nodes to prioritize antecedent predictions."
}