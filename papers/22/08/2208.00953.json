{
    "title": "Visual Interpretable and Explainable Deep Learning Models for Brain Tumor MRI and COVID-19 Chest X-ray Images. (arXiv:2208.00953v2 [cs.LG] UPDATED)",
    "abstract": "Deep learning shows promise for medical image analysis but lacks interpretability, hindering adoption in healthcare. Attribution techniques that explain model reasoning may increase trust in deep learning among clinical stakeholders. This paper aimed to evaluate attribution methods for illuminating how deep neural networks analyze medical images. Using adaptive path-based gradient integration, we attributed predictions from brain tumor MRI and COVID-19 chest X-ray datasets made by recent deep convolutional neural network models. The technique highlighted possible biomarkers, exposed model biases, and offered insights into the links between input and prediction. Our analysis demonstrates the method's ability to elucidate model reasoning on these datasets. The resulting attributions show promise for improving deep learning transparency for domain experts by revealing the rationale behind predictions. This study advances model interpretability to increase trust in deep learning among heal",
    "link": "http://arxiv.org/abs/2208.00953",
    "context": "Title: Visual Interpretable and Explainable Deep Learning Models for Brain Tumor MRI and COVID-19 Chest X-ray Images. (arXiv:2208.00953v2 [cs.LG] UPDATED)\nAbstract: Deep learning shows promise for medical image analysis but lacks interpretability, hindering adoption in healthcare. Attribution techniques that explain model reasoning may increase trust in deep learning among clinical stakeholders. This paper aimed to evaluate attribution methods for illuminating how deep neural networks analyze medical images. Using adaptive path-based gradient integration, we attributed predictions from brain tumor MRI and COVID-19 chest X-ray datasets made by recent deep convolutional neural network models. The technique highlighted possible biomarkers, exposed model biases, and offered insights into the links between input and prediction. Our analysis demonstrates the method's ability to elucidate model reasoning on these datasets. The resulting attributions show promise for improving deep learning transparency for domain experts by revealing the rationale behind predictions. This study advances model interpretability to increase trust in deep learning among heal",
    "path": "papers/22/08/2208.00953.json",
    "total_tokens": 942,
    "translated_title": "对脑部肿瘤MRI和COVID-19胸部X-ray图像的视觉可解释和可解释深度学习模型的研究",
    "translated_abstract": "深度学习在医学图像分析方面显示出潜力，但缺乏可解释性，这阻碍了其在医疗保健领域的应用。通过解释模型推理的归因技术可能增加临床利益相关者对深度学习的信任。本文旨在评估用于揭示深度神经网络分析医学图像过程中的归因方法。使用自适应基于路径的梯度积分，我们对最近的深度卷积神经网络模型对脑部肿瘤MRI和COVID-19胸部X-ray数据集所做的预测进行了归因。该技术突出了可能的生物标志物，暴露了模型的偏差，并提供了输入与预测之间关联的见解。我们的分析证明了该方法在这些数据集上阐明模型推理能力。由此产生的归因显示了提高深度学习透明度的潜力，为领域专家揭示了预测背后的理由。本研究推进了模型可解释性的发展，以增加医学保健领域对深度学习的信任。",
    "tldr": "本研究通过评估归因方法，揭示深度学习模型分析医学图像的推理过程，提高了医学保健领域对深度学习的信任和可解释性。",
    "en_tdlr": "This study enhances trust and interpretability of deep learning in healthcare by evaluating attribution methods that reveal the reasoning process of deep neural networks in analyzing medical images."
}