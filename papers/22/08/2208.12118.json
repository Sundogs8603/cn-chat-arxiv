{
    "title": "A Globally Convergent Gradient-based Bilevel Hyperparameter Optimization Method. (arXiv:2208.12118v2 [cs.LG] UPDATED)",
    "abstract": "Hyperparameter optimization in machine learning is often achieved using naive techniques that only lead to an approximate set of hyperparameters. Although techniques such as Bayesian optimization perform an intelligent search on a given domain of hyperparameters, it does not guarantee an optimal solution. A major drawback of most of these approaches is an exponential increase of their search domain with number of hyperparameters, increasing the computational cost and making the approaches slow. The hyperparameter optimization problem is inherently a bilevel optimization task, and some studies have attempted bilevel solution methodologies for solving this problem. However, these studies assume a unique set of model weights that minimize the training loss, which is generally violated by deep learning architectures. This paper discusses a gradient-based bilevel method addressing these drawbacks for solving the hyperparameter optimization problem. The proposed method can handle continuous ",
    "link": "http://arxiv.org/abs/2208.12118",
    "context": "Title: A Globally Convergent Gradient-based Bilevel Hyperparameter Optimization Method. (arXiv:2208.12118v2 [cs.LG] UPDATED)\nAbstract: Hyperparameter optimization in machine learning is often achieved using naive techniques that only lead to an approximate set of hyperparameters. Although techniques such as Bayesian optimization perform an intelligent search on a given domain of hyperparameters, it does not guarantee an optimal solution. A major drawback of most of these approaches is an exponential increase of their search domain with number of hyperparameters, increasing the computational cost and making the approaches slow. The hyperparameter optimization problem is inherently a bilevel optimization task, and some studies have attempted bilevel solution methodologies for solving this problem. However, these studies assume a unique set of model weights that minimize the training loss, which is generally violated by deep learning architectures. This paper discusses a gradient-based bilevel method addressing these drawbacks for solving the hyperparameter optimization problem. The proposed method can handle continuous ",
    "path": "papers/22/08/2208.12118.json",
    "total_tokens": 886,
    "tldr": "该论文介绍了一种全局收敛的基于梯度的双层超参数优化方法，可以处理连续的超参数值，解决了传统方法中指数增长的搜索域和深度学习架构中唯一权重集的假设问题。",
    "en_tdlr": "This paper presents a globally convergent gradient-based bilevel hyperparameter optimization method, which can handle continuous hyperparameter values and address the exponential growth of search domain and the assumption of a unique weights set in deep learning architectures in traditional methods."
}