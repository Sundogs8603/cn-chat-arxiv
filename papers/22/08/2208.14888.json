{
    "title": "Feature Alignment by Uncertainty and Self-Training for Source-Free Unsupervised Domain Adaptation. (arXiv:2208.14888v2 [cs.CV] UPDATED)",
    "abstract": "Most unsupervised domain adaptation (UDA) methods assume that labeled source images are available during model adaptation. However, this assumption is often infeasible owing to confidentiality issues or memory constraints on mobile devices. Some recently developed approaches do not require source images during adaptation, but they show limited performance on perturbed images. To address these problems, we propose a novel source-free UDA method that uses only a pre-trained source model and unlabeled target images. Our method captures the aleatoric uncertainty by incorporating data augmentation and trains the feature generator with two consistency objectives. The feature generator is encouraged to learn consistent visual features away from the decision boundaries of the head classifier. Thus, the adapted model becomes more robust to image perturbations. Inspired by self-supervised learning, our method promotes inter-space alignment between the prediction space and the feature space while",
    "link": "http://arxiv.org/abs/2208.14888",
    "context": "Title: Feature Alignment by Uncertainty and Self-Training for Source-Free Unsupervised Domain Adaptation. (arXiv:2208.14888v2 [cs.CV] UPDATED)\nAbstract: Most unsupervised domain adaptation (UDA) methods assume that labeled source images are available during model adaptation. However, this assumption is often infeasible owing to confidentiality issues or memory constraints on mobile devices. Some recently developed approaches do not require source images during adaptation, but they show limited performance on perturbed images. To address these problems, we propose a novel source-free UDA method that uses only a pre-trained source model and unlabeled target images. Our method captures the aleatoric uncertainty by incorporating data augmentation and trains the feature generator with two consistency objectives. The feature generator is encouraged to learn consistent visual features away from the decision boundaries of the head classifier. Thus, the adapted model becomes more robust to image perturbations. Inspired by self-supervised learning, our method promotes inter-space alignment between the prediction space and the feature space while",
    "path": "papers/22/08/2208.14888.json",
    "total_tokens": 1138,
    "translated_title": "基于不确定性和自训练的要素对齐进行源自由无监督域自适应",
    "translated_abstract": "大多数无监督域自适应方法假定在模型适应期间有可用的标记源图像。然而，由于机密性问题或移动设备的内存限制，这种假设通常是不可行的。为了解决这些问题，我们提出了一种新颖的源自由无监督域自适应方法，仅使用预训练的源模型和无标签目标图像。我们的方法通过加入数据增强来捕获因素不确定性，并使用两个一致性目标来训练特征生成器。该特征生成器被鼓励在头分类器的决策边界之外学习一致的视觉特征。因此，适应模型对图像扰动的鲁棒性更强。受自监督学习的启发，我们的方法在促进预测空间和特征空间之间的内空间对齐的同时，保持数据分布的一致性，从而增强了适应能力。我们的方法采用了两个源主导和目标主导的不同自训练阶段，以增加源自适应模型的泛化能力。我们的方法在多个基准数据集上进行了实验，证明了其优越性。",
    "tldr": "提出了一种新的源自由无监督域适应(UDA)方法，该方法仅使用预训练的源模型和无标签目标图像。方法对模型的特征生成器进行训练，通过捕获因素不确定性、一致性约束和两个不同的自训练阶段，增强了模型的适应能力，并在多个基准数据集上实现了优越性能。",
    "en_tdlr": "A novel source-free unsupervised domain adaptation method is proposed, which trains only with a pre-trained source model and unlabeled target images. The method trains the feature generator of the model by capturing aleatoric uncertainty, enforcing consistency, and using two different self-training stages to enhance the adaptation ability of the model. Experiment results on multiple benchmark datasets show its superiority."
}