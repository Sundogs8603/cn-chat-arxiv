{
    "title": "Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training. (arXiv:2208.12511v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we investigate on improving the adversarial robustness obtained in adversarial training (AT) via reducing the difficulty of optimization. To better study this problem, we build a novel Bregman divergence perspective for AT, in which AT can be viewed as the sliding process of the training data points on the negative entropy curve. Based on this perspective, we analyze the learning objectives of two typical AT methods, i.e., PGD-AT and TRADES, and we find that the optimization process of TRADES is easier than PGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function of entropy in TRADES, and we find that models with high entropy can be better robustness learners. Inspired by the above findings, we propose two methods, i.e., FAIT and MER, which can both not only reduce the difficulty of optimization under the 10-step PGD adversaries, but also provide better robustness. Our work suggests that reducing the difficulty of optimization under the 10-step PGD a",
    "link": "http://arxiv.org/abs/2208.12511",
    "context": "Title: Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training. (arXiv:2208.12511v2 [cs.LG] UPDATED)\nAbstract: In this paper, we investigate on improving the adversarial robustness obtained in adversarial training (AT) via reducing the difficulty of optimization. To better study this problem, we build a novel Bregman divergence perspective for AT, in which AT can be viewed as the sliding process of the training data points on the negative entropy curve. Based on this perspective, we analyze the learning objectives of two typical AT methods, i.e., PGD-AT and TRADES, and we find that the optimization process of TRADES is easier than PGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function of entropy in TRADES, and we find that models with high entropy can be better robustness learners. Inspired by the above findings, we propose two methods, i.e., FAIT and MER, which can both not only reduce the difficulty of optimization under the 10-step PGD adversaries, but also provide better robustness. Our work suggests that reducing the difficulty of optimization under the 10-step PGD a",
    "path": "papers/22/08/2208.12511.json",
    "total_tokens": 1032,
    "translated_title": "降低难度和提高鲁棒性：基于 Bregman 散度视角的对抗训练",
    "translated_abstract": "本文研究了通过降低优化难度来改善对抗训练（AT）中获得的对抗鲁棒性。为了更好地研究这个问题，我们建立了一个新颖的基于 Bregman 散度的对抗训练视角，在这个视角下，AT可以看作是训练数据点在负熵曲线上的滑动过程。基于这个视角，我们分析了两种典型的AT方法，即PGD-AT和TRADES的学习目标，并发现TRADES的优化过程比PGD-AT更容易，因为TRADES可以分离PGD-AT。此外，我们讨论了TRADES中熵的作用，并发现具有高熵的模型能够更好地学习鲁棒性。受到以上发现的启发，我们提出了两种方法，即FAIT和MER，它们不仅可以在10步PGD对手下降低优化难度，还能提供更好的鲁棒性。我们的工作表明，在10步PGD对手下降低优化难度是可行的，并能提供更好的鲁棒性。",
    "tldr": "本文研究了通过降低优化难度来提高对抗训练中的鲁棒性。基于新颖的Bregman散度视角，分析了两种典型的对抗训练方法的学习目标，并找到了优化过程更容易的方法。提出的两种方法能够降低优化难度，并提供更好的鲁棒性。",
    "en_tdlr": "This paper investigates improving the adversarial robustness in adversarial training by reducing the optimization difficulty. Based on a novel Bregman divergence perspective, the learning objectives of two typical adversarial training methods are analyzed, and methods with easier optimization process and better robustness are proposed."
}