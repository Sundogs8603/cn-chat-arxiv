{
    "title": "Easy Differentially Private Linear Regression. (arXiv:2208.07353v2 [cs.LG] UPDATED)",
    "abstract": "Linear regression is a fundamental tool for statistical analysis. This has motivated the development of linear regression methods that also satisfy differential privacy and thus guarantee that the learned model reveals little about any one data point used to construct it. However, existing differentially private solutions assume that the end user can easily specify good data bounds and hyperparameters. Both present significant practical obstacles. In this paper, we study an algorithm which uses the exponential mechanism to select a model with high Tukey depth from a collection of non-private regression models. Given $n$ samples of $d$-dimensional data used to train $m$ models, we construct an efficient analogue using an approximate Tukey depth that runs in time $O(d^2n + dm\\log(m))$. We find that this algorithm obtains strong empirical performance in the data-rich setting with no data bounds or hyperparameter selection required.",
    "link": "http://arxiv.org/abs/2208.07353",
    "context": "Title: Easy Differentially Private Linear Regression. (arXiv:2208.07353v2 [cs.LG] UPDATED)\nAbstract: Linear regression is a fundamental tool for statistical analysis. This has motivated the development of linear regression methods that also satisfy differential privacy and thus guarantee that the learned model reveals little about any one data point used to construct it. However, existing differentially private solutions assume that the end user can easily specify good data bounds and hyperparameters. Both present significant practical obstacles. In this paper, we study an algorithm which uses the exponential mechanism to select a model with high Tukey depth from a collection of non-private regression models. Given $n$ samples of $d$-dimensional data used to train $m$ models, we construct an efficient analogue using an approximate Tukey depth that runs in time $O(d^2n + dm\\log(m))$. We find that this algorithm obtains strong empirical performance in the data-rich setting with no data bounds or hyperparameter selection required.",
    "path": "papers/22/08/2208.07353.json",
    "total_tokens": 888,
    "translated_title": "简单的差分隐私线性回归",
    "translated_abstract": "线性回归是统计分析的基础工具。这促使开发出满足差分隐私的线性回归方法，从而保证学习的模型泄露的个人数据信息很少。然而，现有的差分隐私解决方案假定最终用户可以轻松指定好的数据边界和超参数，但两者都存在实际障碍。在本文中，我们研究了一种算法，该算法使用指数机制从一组非私有回归模型中选择具有高 Tukey 深度的模型。给定用于训练 $m$ 个模型的 $n$ 个 $d$ 维数据样本，我们使用近似的 Tukey 深度构造了一个高效的类比，它的运行时间为 $O(d^2n + dm\\log(m))$。我们发现，该算法在无数据边界或超参数选择要求的数据丰富环境中获得了强大的实证表现。",
    "tldr": "本文研究了一种简单的差分隐私线性回归算法，它使用指数机制从一组非私有回归模型中选择具有高Tukey深度的模型，并在无需数据边界和超参数选择的数据丰富环境中获得了强大的实证表现。",
    "en_tdlr": "This paper proposes a simple differentially private linear regression algorithm, which uses the exponential mechanism to select a model with high Tukey depth from a collection of non-private regression models. It obtains strong empirical performance in the data-rich setting with no data bounds or hyperparameter selection required."
}