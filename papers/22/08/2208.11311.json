{
    "title": "Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments. (arXiv:2208.11311v3 [cs.LG] UPDATED)",
    "abstract": "In federated learning, all networked clients contribute to the model training cooperatively. However, with model sizes increasing, even sharing the trained partial models often leads to severe communication bottlenecks in underlying networks, especially when communicated iteratively. In this paper, we introduce a federated learning framework FedD3 requiring only one-shot communication by integrating dataset distillation instances. Instead of sharing model updates in other federated learning approaches, FedD3 allows the connected clients to distill the local datasets independently, and then aggregates those decentralized distilled datasets (e.g. a few unrecognizable images) from networks for model training. Our experimental results show that FedD3 significantly outperforms other federated learning frameworks in terms of needed communication volumes, while it provides the additional benefit to be able to balance the trade-off between accuracy and communication cost, depending on usage sc",
    "link": "http://arxiv.org/abs/2208.11311",
    "context": "Title: Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments. (arXiv:2208.11311v3 [cs.LG] UPDATED)\nAbstract: In federated learning, all networked clients contribute to the model training cooperatively. However, with model sizes increasing, even sharing the trained partial models often leads to severe communication bottlenecks in underlying networks, especially when communicated iteratively. In this paper, we introduce a federated learning framework FedD3 requiring only one-shot communication by integrating dataset distillation instances. Instead of sharing model updates in other federated learning approaches, FedD3 allows the connected clients to distill the local datasets independently, and then aggregates those decentralized distilled datasets (e.g. a few unrecognizable images) from networks for model training. Our experimental results show that FedD3 significantly outperforms other federated learning frameworks in terms of needed communication volumes, while it provides the additional benefit to be able to balance the trade-off between accuracy and communication cost, depending on usage sc",
    "path": "papers/22/08/2208.11311.json",
    "total_tokens": 928,
    "translated_title": "基于分散式数据集提炼的边缘资源受限环境下联邦学习",
    "translated_abstract": "在联邦学习中，所有的联网客户端协作地进行模型训练。然而，随着模型大小的增加，即使在迭代通信中共享已训练的部分模型，也往往会导致底层网络中的严重通信瓶颈。本文介绍了一种联邦学习框架FedD3，它只需要一次通信，并集成了数据集提炼实例。FedD3不同于其他联邦学习方法中的共享模型更新，它允许连接的客户端独立地提炼本地数据集，然后从网络中聚合那些分散的提炼数据集（例如，一些无法识别的图像）并用于模型训练。我们的实验结果表明，与其他联邦学习方法相比，FedD3在需要通信的数据量方面表现显著更好，同时它能够在使用场景中平衡准确性和通信成本。",
    "tldr": "本论文介绍了一种名为FedD3的联邦学习框架，通过集成数据集提炼实例仅需要一次通信，与其他联邦学习方法相比，在需要通信的数据量方面表现显著更好，同时通过平衡准确性和通信成本来适应使用场景。",
    "en_tdlr": "This paper proposes a federated learning framework named FedD3, which requires only one-shot communication by integrating dataset distillation instances. Compared with others, FedD3 outperforms in terms of needed communication volumes and can adapt to usage scenarios by balancing the trade-off between accuracy and communication cost."
}