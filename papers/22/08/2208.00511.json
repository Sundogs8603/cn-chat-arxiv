{
    "title": "Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval. (arXiv:2208.00511v2 [cs.IR] UPDATED)",
    "abstract": "Pre-trained language models have been successful in many knowledge-intensive NLP tasks. However, recent work has shown that models such as BERT are not ``structurally ready'' to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR). This ``lack of readiness'' results from the gap between language model pre-training and DPR fine-tuning. Previous solutions call for computationally expensive techniques such as hard negative mining, cross-encoder distillation, and further pre-training to learn a robust DPR model. In this work, we instead propose to fully exploit knowledge in a pre-trained language model for DPR by aggregating the contextualized token embeddings into a dense vector, which we call agg*. By concatenating vectors from the [CLS] token and agg*, our Aggretriever model substantially improves the effectiveness of dense retrieval models on both in-domain and zero-shot evaluations without introducing substantial training overhead. Code is available at h",
    "link": "http://arxiv.org/abs/2208.00511",
    "context": "Title: Aggretriever: A Simple Approach to Aggregate Textual Representations for Robust Dense Passage Retrieval. (arXiv:2208.00511v2 [cs.IR] UPDATED)\nAbstract: Pre-trained language models have been successful in many knowledge-intensive NLP tasks. However, recent work has shown that models such as BERT are not ``structurally ready'' to aggregate textual information into a [CLS] vector for dense passage retrieval (DPR). This ``lack of readiness'' results from the gap between language model pre-training and DPR fine-tuning. Previous solutions call for computationally expensive techniques such as hard negative mining, cross-encoder distillation, and further pre-training to learn a robust DPR model. In this work, we instead propose to fully exploit knowledge in a pre-trained language model for DPR by aggregating the contextualized token embeddings into a dense vector, which we call agg*. By concatenating vectors from the [CLS] token and agg*, our Aggretriever model substantially improves the effectiveness of dense retrieval models on both in-domain and zero-shot evaluations without introducing substantial training overhead. Code is available at h",
    "path": "papers/22/08/2208.00511.json",
    "total_tokens": 1058,
    "translated_title": "Aggretriever：一种简单的聚合文本表示方法，用于强大的密集式段落检索",
    "translated_abstract": "预训练语言模型在很多知识密集型NLP任务中取得了成功。然而，最近的研究表明，如BERT这样的模型在将文本信息聚合成[CLS]向量以进行密集式段落检索（DPR）时并不是“结构上准备好的”。这种“准备不足”是由语言模型预训练和DPR微调之间的差距造成的。以前的解决方案要求使用计算量昂贵的技术，如硬负采样、交叉编码器蒸馏和更进一步的预训练来学习强大的DPR模型。在这项工作中，我们建议通过聚合上下文化的token嵌入到一个密集向量中，充分利用预训练语言模型在DPR中的知识，我们将其称为agg*。通过将来自[CLS] token和agg*的向量进行串联，我们的Aggretriever模型在不引入实质性的训练开销的情况下，显著提高了密集式检索模型在域内和零-shot评估中的有效性。可在h上获取代码。",
    "tldr": "这项工作提出了一种简单的方法，将预训练语言模型中的知识充分应用于密集式段落检索，称为Aggretriever，通过将上下文化的token嵌入聚合到密集向量中，相对于以前需要采用计算量昂贵的技术进行训练的DPR模型，Aggretriever不需引入实质性的训练开销，能显著提高在域内和零-shot评估中有效性。"
}