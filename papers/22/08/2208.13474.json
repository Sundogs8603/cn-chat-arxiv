{
    "title": "Prompt Tuning with Soft Context Sharing for Vision-Language Models. (arXiv:2208.13474v1 [cs.CV] CROSS LISTED)",
    "abstract": "Vision-language models have recently shown great potential on many computer vision tasks. Meanwhile, prior work demonstrates prompt tuning designed for vision-language models could acquire superior performance on few-shot image recognition compared to linear probe, a strong baseline. In real-world applications, many few-shot tasks are correlated, particularly in a specialized area. However, such information is ignored by previous work. Inspired by the fact that modeling task relationships by multi-task learning can usually boost performance, we propose a novel method SoftCPT (Soft Context Sharing for Prompt Tuning) to fine-tune pre-trained vision-language models on multiple target few-shot tasks, simultaneously. Specifically, we design a task-shared meta network to generate prompt vector for each task using pre-defined task name together with a learnable meta prompt as input. As such, the prompt vectors of all tasks will be shared in a soft manner. The parameters of this shared meta ne",
    "link": "http://arxiv.org/abs/2208.13474",
    "raw_ret": "{\n    \"translated_title\": \"视觉-语言模型的软上下文共享提示微调\",\n    \"translated_abstract\": \"视觉-语言模型最近在许多计算机视觉任务上展现出巨大的潜力。同时，以前的工作证明，为视觉-语言模型设计的提示微调相对于线性探测，具有更好的少样本图像识别性能，这是一个强大的基准。在现实世界的应用中，许多少样本任务是相关的，特别是在一个专业领域。然而，以前的工作忽略了这样的信息。受到多任务学习通过建模任务关系通常可以提高性能的事实的启发，我们提出了一种新的方法SoftCPT（软上下文共享提示微调）来同时在多个目标少样本任务上对预训练的视觉-语言模型进行微调。具体地，我们设计了一个任务共享的元网络，使用预定义的任务名称和可学习的元提示作为输入，为每个任务生成提示向量。因此，所有任务的提示向量将以软方式共享。该共享元网络的参数将会被fine-tune以适应多个目标任务\",\n    \"tldr\": \"该论文提出了一种新的方法SoftCPT，通过在多个目标少样本任务上对预训练的视觉-语言模型进行微调，并设计了一个任务共享的元网络来生成每个任务的提示向量，以软方式共享所有任务的提示向量。\"\n}<|im_sep|>",
    "total_tokens": 872,
    "ret": {
        "translated_title": "视觉-语言模型的软上下文共享提示微调",
        "translated_abstract": "视觉-语言模型最近在许多计算机视觉任务上展现出巨大的潜力。同时，以前的工作证明，为视觉-语言模型设计的提示微调相对于线性探测，具有更好的少样本图像识别性能，这是一个强大的基准。在现实世界的应用中，许多少样本任务是相关的，特别是在一个专业领域。然而，以前的工作忽略了这样的信息。受到多任务学习通过建模任务关系通常可以提高性能的事实的启发，我们提出了一种新的方法SoftCPT（软上下文共享提示微调）来同时在多个目标少样本任务上对预训练的视觉-语言模型进行微调。具体地，我们设计了一个任务共享的元网络，使用预定义的任务名称和可学习的元提示作为输入，为每个任务生成提示向量。因此，所有任务的提示向量将以软方式共享。该共享元网络的参数将会被fine-tune以适应多个目标任务",
        "tldr": "该论文提出了一种新的方法SoftCPT，通过在多个目标少样本任务上对预训练的视觉-语言模型进行微调，并设计了一个任务共享的元网络来生成每个任务的提示向量，以软方式共享所有任务的提示向量。"
    }
}