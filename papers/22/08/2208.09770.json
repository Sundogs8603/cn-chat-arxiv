{
    "title": "Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization. (arXiv:2208.09770v2 [cs.CL] UPDATED)",
    "abstract": "This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state of the art encoder-decoder model using three techniques. First, we use a two-phase pre-training process to improve model's performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, and then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ creates new state of the art on 9 out of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM-540B on XSum, and the finetuned 200x l",
    "link": "http://arxiv.org/abs/2208.09770",
    "context": "Title: Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization. (arXiv:2208.09770v2 [cs.CL] UPDATED)\nAbstract: This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state of the art encoder-decoder model using three techniques. First, we use a two-phase pre-training process to improve model's performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, and then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ creates new state of the art on 9 out of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM-540B on XSum, and the finetuned 200x l",
    "path": "papers/22/08/2208.09770.json",
    "total_tokens": 1007,
    "translated_title": "Z-Code++：一种针对抽象文本摘要优化的预训练语言模型",
    "translated_abstract": "本文介绍了Z-Code++，一种新的针对抽象文本摘要优化的预训练语言模型。该模型扩展了最先进的编码器-解码器模型，运用了三种技术。首先，我们使用了两阶段的预训练过程，以提高模型在低资源摘要任务上的性能。该模型首先使用文本语料库进行语言理解的预训练，然后在摘要语料库上进行连续的预训练以提高其基于文本生成的能力。其次，我们用解耦的注意力层取代编码器中的自注意力层，其中每个单词分别使用两个向量来表示其内容和位置。第三，我们使用编码器中的融合编码方法，以一种分层的方式对长序列进行编码。Z-Code++在5种语言的13个文本摘要任务中有9个取得了最新的最优效果。我们的模型在参数效率方面表现出色，在XSum数据集上的性能超过了比其大600倍的PaLM-540B，以及比其大200倍的FeBERT。",
    "tldr": "本文介绍了一种新的预训练语言模型Z-Code++，它使用两种预训练阶段和三种技术进行优化，其中包括解耦的注意力层和融合编码方法。该模型在抽象文本摘要任务上优于其他模型，是一种高效的参数化模型。",
    "en_tdlr": "This paper introduces a new pre-trained language model, Z-Code++, optimized for abstractive text summarization. The model uses a two-phase pre-training process and three techniques, including disentangled attention layers and fusion-in-encoder. Z-Code++ outperforms other models on abstractive summarization tasks and is an efficient parameterized model."
}