{
    "title": "Three New Validators and a Large-Scale Benchmark Ranking for Unsupervised Domain Adaptation. (arXiv:2208.07360v4 [cs.CV] UPDATED)",
    "abstract": "Changes to hyperparameters can have a dramatic effect on model accuracy. Thus, the tuning of hyperparameters plays an important role in optimizing machine-learning models. An integral part of the hyperparameter-tuning process is the evaluation of model checkpoints, which is done through the use of \"validators\". In a supervised setting, these validators evaluate checkpoints by computing accuracy on a validation set that has labels. In contrast, in an unsupervised setting, the validation set has no such labels. Without any labels, it is impossible to compute accuracy, so validators must estimate accuracy instead. But what is the best approach to estimating accuracy? In this paper, we consider this question in the context of unsupervised domain adaptation (UDA). Specifically, we propose three new validators, and we compare and rank them against five other existing validators, on a large dataset of 1,000,000 checkpoints. Extensive experimental results show that two of our proposed validato",
    "link": "http://arxiv.org/abs/2208.07360",
    "context": "Title: Three New Validators and a Large-Scale Benchmark Ranking for Unsupervised Domain Adaptation. (arXiv:2208.07360v4 [cs.CV] UPDATED)\nAbstract: Changes to hyperparameters can have a dramatic effect on model accuracy. Thus, the tuning of hyperparameters plays an important role in optimizing machine-learning models. An integral part of the hyperparameter-tuning process is the evaluation of model checkpoints, which is done through the use of \"validators\". In a supervised setting, these validators evaluate checkpoints by computing accuracy on a validation set that has labels. In contrast, in an unsupervised setting, the validation set has no such labels. Without any labels, it is impossible to compute accuracy, so validators must estimate accuracy instead. But what is the best approach to estimating accuracy? In this paper, we consider this question in the context of unsupervised domain adaptation (UDA). Specifically, we propose three new validators, and we compare and rank them against five other existing validators, on a large dataset of 1,000,000 checkpoints. Extensive experimental results show that two of our proposed validato",
    "path": "papers/22/08/2208.07360.json",
    "total_tokens": 987,
    "translated_title": "三个新的验证器及用于无监督领域自适应的大规模基准排名",
    "translated_abstract": "超参数的改变可以对模型准确度产生巨大影响。因此，超参数调整在优化机器学习模型方面发挥着重要作用。超参数调整过程的一个重要组成部分是通过使用“验证器”评估模型检查点。在有标签的监督设置中，这些验证器通过计算在验证集上的准确率来评估检查点。相反，在无监督设置中，验证集没有这样的标签。没有任何标签，因此无法计算准确性，因此验证器必须估计准确性。但是估计准确性的最佳方法是什么？在本文中，我们考虑这个问题在无监督领域适应（UDA）的上下文中。具体而言，我们提出了三种新的验证器，并将其与其他五个现有的验证器在包含100万个检查点的大型数据集上进行比较和排名。广泛的实验结果表明，我们提供的两个验证器优于现有的验证器，并且估计准确性的最佳方法因UDA任务类型而异。",
    "tldr": "本文对无监督领域自适应中估计准确性的三种新验证器进行了研究，并在100万个检查点的大数据集上与其他五种验证器进行了比较和排名。实验结果表明，我们提出的两个验证器优于现有的验证器，并且最佳的估计准确性方法因任务类型而异。",
    "en_tdlr": "This paper introduces three new validators and compares them against five existing validators for estimating accuracy in unsupervised domain adaptation. Two of the proposed validators outperform existing ones, and the best approach to estimating accuracy varies depending on the type of UDA task."
}