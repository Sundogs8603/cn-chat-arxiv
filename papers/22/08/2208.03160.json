{
    "title": "Almost-Orthogonal Layers for Efficient General-Purpose Lipschitz Networks. (arXiv:2208.03160v2 [cs.LG] UPDATED)",
    "abstract": "It is a highly desirable property for deep networks to be robust against small input changes. One popular way to achieve this property is by designing networks with a small Lipschitz constant. In this work, we propose a new technique for constructing such Lipschitz networks that has a number of desirable properties: it can be applied to any linear network layer (fully-connected or convolutional), it provides formal guarantees on the Lipschitz constant, it is easy to implement and efficient to run, and it can be combined with any training objective and optimization method. In fact, our technique is the first one in the literature that achieves all of these properties simultaneously. Our main contribution is a rescaling-based weight matrix parametrization that guarantees each network layer to have a Lipschitz constant of at most 1 and results in the learned weight matrices to be close to orthogonal. Hence we call such layers almost-orthogonal Lipschitz (AOL). Experiments and ablation stu",
    "link": "http://arxiv.org/abs/2208.03160",
    "context": "Title: Almost-Orthogonal Layers for Efficient General-Purpose Lipschitz Networks. (arXiv:2208.03160v2 [cs.LG] UPDATED)\nAbstract: It is a highly desirable property for deep networks to be robust against small input changes. One popular way to achieve this property is by designing networks with a small Lipschitz constant. In this work, we propose a new technique for constructing such Lipschitz networks that has a number of desirable properties: it can be applied to any linear network layer (fully-connected or convolutional), it provides formal guarantees on the Lipschitz constant, it is easy to implement and efficient to run, and it can be combined with any training objective and optimization method. In fact, our technique is the first one in the literature that achieves all of these properties simultaneously. Our main contribution is a rescaling-based weight matrix parametrization that guarantees each network layer to have a Lipschitz constant of at most 1 and results in the learned weight matrices to be close to orthogonal. Hence we call such layers almost-orthogonal Lipschitz (AOL). Experiments and ablation stu",
    "path": "papers/22/08/2208.03160.json",
    "total_tokens": 1015,
    "translated_title": "高效通用的Lipschitz网络的近正交层",
    "translated_abstract": "对于深度网络来说，能够对小的输入变化具有鲁棒性是非常理想的属性。实现这个属性的一种常见方法是设计具有较小的Lipschitz常数的网络。在本文中，我们提出了一种构建这样的Lipschitz网络的新技术，它具有多个优点：可以应用于任何线性网络层（全连接或卷积），对Lipschitz常数提供形式化的保证，易于实现和高效运行，并且可以与任何训练目标和优化方法相结合。事实上，我们的技术是文献中第一个同时实现所有这些属性的技术。我们的主要贡献是基于重新缩放的权重矩阵参数化，保证每个网络层的Lipschitz常数最大为1，并且导致学习的权重矩阵接近正交。因此，我们将这样的层称为几乎正交Lipschitz（AOL）。实验证明和消融研究表明...",
    "tldr": "本文提出了一种新技术，可以构建具有小Lipschitz常数的高效通用Lipschitz网络。该技术具有形式化保证、易于实现和高效运行的优点，并且可以与任何训练目标和优化方法相结合。主要贡献是通过重新缩放的权重矩阵参数化，确保每个网络层的Lipschitz常数最大为1，并且导致学习的权重矩阵接近正交。",
    "en_tdlr": "This paper proposes a new technique to construct efficient general-purpose Lipschitz networks with a small Lipschitz constant. The technique provides formal guarantees, easy implementation, and efficiency, and can be combined with any training objective and optimization method. The main contribution is a rescaling-based weight matrix parametrization that ensures each network layer to have a maximum Lipschitz constant of 1 and results in the learned weight matrices to be close to orthogonal."
}