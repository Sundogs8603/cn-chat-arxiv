{
    "title": "Federated Learning of Neural ODE Models with Different Iteration Counts. (arXiv:2208.09478v4 [cs.LG] UPDATED)",
    "abstract": "Federated learning is a distributed machine learning approach in which clients train models locally with their own data and upload them to a server so that their trained results are shared between them without uploading raw data to the server. There are some challenges in federated learning, such as communication size reduction and client heterogeneity. The former can mitigate the communication overheads, and the latter can allow the clients to choose proper models depending on their available compute resources. To address these challenges, in this paper, we utilize Neural ODE based models for federated learning. The proposed flexible federated learning approach can reduce the communication size while aggregating models with different iteration counts or depths. Our contribution is that we experimentally demonstrate that the proposed federated learning can aggregate models with different iteration counts or depths. It is compared with a different federated learning approach in terms of",
    "link": "http://arxiv.org/abs/2208.09478",
    "context": "Title: Federated Learning of Neural ODE Models with Different Iteration Counts. (arXiv:2208.09478v4 [cs.LG] UPDATED)\nAbstract: Federated learning is a distributed machine learning approach in which clients train models locally with their own data and upload them to a server so that their trained results are shared between them without uploading raw data to the server. There are some challenges in federated learning, such as communication size reduction and client heterogeneity. The former can mitigate the communication overheads, and the latter can allow the clients to choose proper models depending on their available compute resources. To address these challenges, in this paper, we utilize Neural ODE based models for federated learning. The proposed flexible federated learning approach can reduce the communication size while aggregating models with different iteration counts or depths. Our contribution is that we experimentally demonstrate that the proposed federated learning can aggregate models with different iteration counts or depths. It is compared with a different federated learning approach in terms of",
    "path": "papers/22/08/2208.09478.json",
    "total_tokens": 878,
    "translated_title": "具有不同迭代次数的神经常微分方程模型的联邦学习",
    "translated_abstract": "联邦学习是一种分布式机器学习方法，其中客户端使用自己的数据在本地训练模型，并将其上传到服务器，以便在它们之间共享训练结果，而无需将原始数据上传到服务器。联邦学习面临一些挑战，如通信量的减少和客户端的异质性。前者可以减轻通信开销，后者可以让客户端根据其可用计算资源选择适当的模型。为了解决这些挑战，本文利用基于神经常微分方程的模型进行联邦学习。所提出的灵活联邦学习方法可以在聚合具有不同迭代次数或深度的模型时减少通信量。我们的贡献是通过实验证明了所提出的联邦学习方法可以聚合具有不同迭代次数或深度的模型。它与另一种联邦学习方法在通信开销、模型质量等方面进行了比较。",
    "tldr": "本文提出了一种利用神经常微分方程模型进行联邦学习的方法，能够在聚合具有不同迭代次数或深度的模型时减少通信量，并通过实验证明了该方法的有效性。",
    "en_tdlr": "This paper proposes a method for federated learning using Neural ODE models, which can reduce communication size when aggregating models with different iteration counts or depths, and its effectiveness is experimentally demonstrated."
}