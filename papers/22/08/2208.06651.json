{
    "title": "Revisiting Adversarial Attacks on Graph Neural Networks for Graph Classification. (arXiv:2208.06651v2 [cs.SI] UPDATED)",
    "abstract": "Graph neural networks (GNNs) have achieved tremendous success in the task of graph classification and its diverse downstream real-world applications. Despite the huge success in learning graph representations, current GNN models have demonstrated their vulnerability to potentially existent adversarial examples on graph-structured data. Existing approaches are either limited to structure attacks or restricted to local information, urging for the design of a more general attack framework on graph classification, which faces significant challenges due to the complexity of generating local-node-level adversarial examples using the global-graph-level information. To address this \"global-to-local\" attack challenge, we present a novel and general framework to generate adversarial examples via manipulating graph structure and node features. Specifically, we make use of Graph Class Activation Mapping and its variant to produce node-level importance corresponding to the graph classification task",
    "link": "http://arxiv.org/abs/2208.06651",
    "context": "Title: Revisiting Adversarial Attacks on Graph Neural Networks for Graph Classification. (arXiv:2208.06651v2 [cs.SI] UPDATED)\nAbstract: Graph neural networks (GNNs) have achieved tremendous success in the task of graph classification and its diverse downstream real-world applications. Despite the huge success in learning graph representations, current GNN models have demonstrated their vulnerability to potentially existent adversarial examples on graph-structured data. Existing approaches are either limited to structure attacks or restricted to local information, urging for the design of a more general attack framework on graph classification, which faces significant challenges due to the complexity of generating local-node-level adversarial examples using the global-graph-level information. To address this \"global-to-local\" attack challenge, we present a novel and general framework to generate adversarial examples via manipulating graph structure and node features. Specifically, we make use of Graph Class Activation Mapping and its variant to produce node-level importance corresponding to the graph classification task",
    "path": "papers/22/08/2208.06651.json",
    "total_tokens": 862,
    "translated_title": "重新审视对图分类的图神经网络的对抗性攻击",
    "translated_abstract": "图神经网络（GNN）在图分类任务及其各种实际应用中取得了巨大成功。尽管在学习图表示方面取得了巨大成功，当前的GNN模型显示出对可能存在的图结构数据中的对抗性示例的脆弱性。现有方法要么局限于结构攻击，要么局限于局部信息，迫切需要设计一种更通用的图分类攻击框架，面临着使用全局图级信息生成局部节点级对抗性示例的复杂性挑战。为了解决这一“从全局到局部”的攻击挑战，我们提出了一种通过操作图结构和节点特征生成对抗性示例的新型通用框架。具体来说，我们利用图类别激活映射及其变种来生成与图分类任务对应的节点级重要性。",
    "tldr": "该论文提出了一种针对图分类任务的图神经网络的对抗性攻击方法。通过操作图结构和节点特征，生成对抗性示例，解决了从全局到局部的攻击挑战。这项工作为提高图神经网络的安全性和鲁棒性提供了重要的指导。",
    "en_tdlr": "This paper presents an adversarial attack method for graph neural networks in graph classification tasks. By manipulating graph structure and node features, adversarial examples are generated to address the challenge of attacking from global to local. This work provides important guidance for enhancing the security and robustness of graph neural networks."
}