{
    "title": "Deep Generative Modeling on Limited Data with Regularization by Nontransferable Pre-trained Models. (arXiv:2208.14133v3 [cs.LG] UPDATED)",
    "abstract": "Deep generative models (DGMs) are data-eager because learning a complex model on limited data suffers from a large variance and easily overfits. Inspired by the classical perspective of the bias-variance tradeoff, we propose regularized deep generative model (Reg-DGM), which leverages a nontransferable pre-trained model to reduce the variance of generative modeling with limited data. Formally, Reg-DGM optimizes a weighted sum of a certain divergence and the expectation of an energy function, where the divergence is between the data and the model distributions, and the energy function is defined by the pre-trained model w.r.t. the model distribution. We analyze a simple yet representative Gaussian-fitting case to demonstrate how the weighting hyperparameter trades off the bias and the variance. Theoretically, we characterize the existence and the uniqueness of the global minimum of Reg-DGM in a non-parametric setting and prove its convergence with neural networks trained by gradient-bas",
    "link": "http://arxiv.org/abs/2208.14133",
    "context": "Title: Deep Generative Modeling on Limited Data with Regularization by Nontransferable Pre-trained Models. (arXiv:2208.14133v3 [cs.LG] UPDATED)\nAbstract: Deep generative models (DGMs) are data-eager because learning a complex model on limited data suffers from a large variance and easily overfits. Inspired by the classical perspective of the bias-variance tradeoff, we propose regularized deep generative model (Reg-DGM), which leverages a nontransferable pre-trained model to reduce the variance of generative modeling with limited data. Formally, Reg-DGM optimizes a weighted sum of a certain divergence and the expectation of an energy function, where the divergence is between the data and the model distributions, and the energy function is defined by the pre-trained model w.r.t. the model distribution. We analyze a simple yet representative Gaussian-fitting case to demonstrate how the weighting hyperparameter trades off the bias and the variance. Theoretically, we characterize the existence and the uniqueness of the global minimum of Reg-DGM in a non-parametric setting and prove its convergence with neural networks trained by gradient-bas",
    "path": "papers/22/08/2208.14133.json",
    "total_tokens": 874,
    "translated_title": "有限数据下的深度生成模型与非可转移预训练模型正则化",
    "translated_abstract": "在有限数据上学习复杂模型很容易出现大方差和过拟合问题，因此深度生成模型需要大量数据才能实现较好的性能。针对这个问题，本文提出了一种正则化深度生成模型（Reg-DGM），通过利用非可转移预训练模型来降低有限数据的生成建模方差。Reg-DGM 优化一定的差异和能量函数期望的加权和（能量函数是通过预训练模型定义的）。我们通过高斯拟合来分析加权超参数如何权衡偏差和方差。理论上，我们在非参数设置下表征了 Reg-DGM 的全局最小值的存在性和唯一性，并证明了其与梯度基的神经网络的收敛性。",
    "tldr": "本文提出了一种正则化深度生成模型（Reg-DGM），通过利用非可转移预训练模型来降低有限数据的生成建模方差，在极限情况下确保全局最小点的存在和唯一性。",
    "en_tdlr": "This paper proposes a regularized deep generative model (Reg-DGM) that leverages a nontransferable pre-trained model to reduce the variance of generative modeling with limited data. The paper characterizes the existence and uniqueness of the global minimum of Reg-DGM in a non-parametric setting and proves its convergence with neural networks trained by gradient-based methods."
}