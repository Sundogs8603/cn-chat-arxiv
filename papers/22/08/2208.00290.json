{
    "title": "A Gradient Smoothed Functional Algorithm with Truncated Cauchy Random Perturbations for Stochastic Optimization. (arXiv:2208.00290v3 [math.OC] UPDATED)",
    "abstract": "In this paper, we present a stochastic gradient algorithm for minimizing a smooth objective function that is an expectation over noisy cost samples, and only the latter are observed for any given parameter. Our algorithm employs a gradient estimation scheme with random perturbations, which are formed using the truncated Cauchy distribution from the delta sphere. We analyze the bias and variance of the proposed gradient estimator. Our algorithm is found to be particularly useful in the case when the objective function is non-convex, and the parameter dimension is high. From an asymptotic convergence analysis, we establish that our algorithm converges almost surely to the set of stationary points of the objective function and obtains the asymptotic convergence rate. We also show that our algorithm avoids unstable equilibria, implying convergence to local minima. Further, we perform a non-asymptotic convergence analysis of our algorithm. In particular, we establish here a non-asymptotic b",
    "link": "http://arxiv.org/abs/2208.00290",
    "context": "Title: A Gradient Smoothed Functional Algorithm with Truncated Cauchy Random Perturbations for Stochastic Optimization. (arXiv:2208.00290v3 [math.OC] UPDATED)\nAbstract: In this paper, we present a stochastic gradient algorithm for minimizing a smooth objective function that is an expectation over noisy cost samples, and only the latter are observed for any given parameter. Our algorithm employs a gradient estimation scheme with random perturbations, which are formed using the truncated Cauchy distribution from the delta sphere. We analyze the bias and variance of the proposed gradient estimator. Our algorithm is found to be particularly useful in the case when the objective function is non-convex, and the parameter dimension is high. From an asymptotic convergence analysis, we establish that our algorithm converges almost surely to the set of stationary points of the objective function and obtains the asymptotic convergence rate. We also show that our algorithm avoids unstable equilibria, implying convergence to local minima. Further, we perform a non-asymptotic convergence analysis of our algorithm. In particular, we establish here a non-asymptotic b",
    "path": "papers/22/08/2208.00290.json",
    "total_tokens": 939,
    "translated_title": "一种具有截断柯西随机扰动的渐进平滑函数算法用于随机优化",
    "translated_abstract": "本文提出了一种随机梯度算法，用于最小化一个光滑的目标函数，该函数是噪声成本样本的期望，而只有后者对任何给定的参数进行观测。我们的算法采用带有随机扰动的梯度估计方案，这些扰动使用从delta球中得到的截断柯西分布形成。我们分析了所提出的梯度估计器的偏差和方差。我们发现，当目标函数是非凸的，而参数维数很高时，我们的算法非常有用。从渐近收敛分析中，我们建立了我们的算法几乎确定地收敛于目标函数的稳定点集合，并获得了收敛的渐近速率。我们还表明，我们的算法避免了不稳定的平衡点，意味着收敛到局部最小值。此外，我们对我们的算法进行了非渐近收敛性分析。特别地，我们在这里建立了一个非渐近保证收敛率的收敛性结果，该结果是实数值的和给出了精确的界限。",
    "tldr": "本文提出了一种具有截断柯西随机扰动的随机梯度算法用于非凸目标函数的优化，算法具有稳定性与快速收敛性。",
    "en_tdlr": "This paper presents a stochastic gradient algorithm for non-convex optimization with truncated Cauchy random perturbations, which achieves stability and fast convergence, and is analyzed both asymptotically and non-asymptotically."
}