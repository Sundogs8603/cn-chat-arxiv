{
    "title": "Oracle-free Reinforcement Learning in Mean-Field Games along a Single Sample Path. (arXiv:2208.11639v3 [cs.LG] UPDATED)",
    "abstract": "We consider online reinforcement learning in Mean-Field Games (MFGs). Unlike traditional approaches, we alleviate the need for a mean-field oracle by developing an algorithm that approximates the Mean-Field Equilibrium (MFE) using the single sample path of the generic agent. We call this {\\it Sandbox Learning}, as it can be used as a warm-start for any agent learning in a multi-agent non-cooperative setting. We adopt a two time-scale approach in which an online fixed-point recursion for the mean-field operates on a slower time-scale, in tandem with a control policy update on a faster time-scale for the generic agent. Given that the underlying Markov Decision Process (MDP) of the agent is communicating, we provide finite sample convergence guarantees in terms of convergence of the mean-field and control policy to the mean-field equilibrium. The sample complexity of the Sandbox learning algorithm is $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ where $\\epsilon$ is the MFE approximation error. Thi",
    "link": "http://arxiv.org/abs/2208.11639",
    "context": "Title: Oracle-free Reinforcement Learning in Mean-Field Games along a Single Sample Path. (arXiv:2208.11639v3 [cs.LG] UPDATED)\nAbstract: We consider online reinforcement learning in Mean-Field Games (MFGs). Unlike traditional approaches, we alleviate the need for a mean-field oracle by developing an algorithm that approximates the Mean-Field Equilibrium (MFE) using the single sample path of the generic agent. We call this {\\it Sandbox Learning}, as it can be used as a warm-start for any agent learning in a multi-agent non-cooperative setting. We adopt a two time-scale approach in which an online fixed-point recursion for the mean-field operates on a slower time-scale, in tandem with a control policy update on a faster time-scale for the generic agent. Given that the underlying Markov Decision Process (MDP) of the agent is communicating, we provide finite sample convergence guarantees in terms of convergence of the mean-field and control policy to the mean-field equilibrium. The sample complexity of the Sandbox learning algorithm is $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$ where $\\epsilon$ is the MFE approximation error. Thi",
    "path": "papers/22/08/2208.11639.json",
    "total_tokens": 957,
    "translated_title": "基于单一样本路径的均场博弈无需oracle的强化学习",
    "translated_abstract": "本文考虑均场博弈中的在线强化学习。与传统方法不同的是，我们通过开发一种算法来利用单个智能体的样本路径近似均场平衡(MFE)，从而消除了均场oracle的需求。我们将其称为“沙箱学习”，因为任何在多智能体非合作设置中学习的代理都可以使用它作为预热开始。我们采用两个时间尺度的方法，在较慢的时间尺度上进行均场的在线固定点递归，与普通代理的更快时间尺度上的控制策略更新一起操作。鉴于代理的基本马尔可夫决策过程(MDP)是连通的，我们提供了保证均场和控制策略收敛于均场平衡的有限样本收敛保证。沙箱学习算法的样本复杂度是$\\tilde{\\mathcal{O}}(\\epsilon^{-4})$，其中$\\epsilon$是MFE近似误差。",
    "tldr": "本文中提出了一种基于单一智能体样本路径的“沙箱学习”算法，可以在多智能体非合作的环境下作为预热开始。算法不需要均场oracle，样本复杂度为$\\tilde{\\mathcal{O}}(\\epsilon^{-4})$。",
    "en_tdlr": "This paper proposes a \"Sandbox Learning\" algorithm based on a single agent sample path, which can be used as a warm start in a multi-agent non-cooperative environment. The algorithm does not require a mean-field oracle and has a sample complexity of $\\tilde{\\mathcal{O}}(\\epsilon^{-4})$."
}