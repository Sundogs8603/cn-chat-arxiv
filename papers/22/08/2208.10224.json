{
    "title": "Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks. (arXiv:2208.10224v4 [cs.CR] UPDATED)",
    "abstract": "A powerful category of (invisible) data poisoning attacks modify a subset of training examples by small adversarial perturbations to change the prediction of certain test-time data. Existing defense mechanisms are not desirable to deploy in practice, as they often either drastically harm the generalization performance, or are attack-specific, and prohibitively slow to apply. Here, we propose a simple but highly effective approach that unlike existing methods breaks various types of invisible poisoning attacks with the slightest drop in the generalization performance. We make the key observation that attacks introduce local sharp regions of high training loss, which when minimized, results in learning the adversarial perturbations and makes the attack successful. To break poisoning attacks, our key idea is to alleviate the sharp loss regions introduced by poisons. To do so, our approach comprises two components: an optimized friendly noise that is generated to maximally perturb examples",
    "link": "http://arxiv.org/abs/2208.10224",
    "context": "Title: Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks. (arXiv:2208.10224v4 [cs.CR] UPDATED)\nAbstract: A powerful category of (invisible) data poisoning attacks modify a subset of training examples by small adversarial perturbations to change the prediction of certain test-time data. Existing defense mechanisms are not desirable to deploy in practice, as they often either drastically harm the generalization performance, or are attack-specific, and prohibitively slow to apply. Here, we propose a simple but highly effective approach that unlike existing methods breaks various types of invisible poisoning attacks with the slightest drop in the generalization performance. We make the key observation that attacks introduce local sharp regions of high training loss, which when minimized, results in learning the adversarial perturbations and makes the attack successful. To break poisoning attacks, our key idea is to alleviate the sharp loss regions introduced by poisons. To do so, our approach comprises two components: an optimized friendly noise that is generated to maximally perturb examples",
    "path": "papers/22/08/2208.10224.json",
    "total_tokens": 958,
    "translated_title": "对抗性噪声的友好噪声：针对数据污染攻击的强大防御策略",
    "translated_abstract": "一类强大的（不可见的）数据污染攻击通过微小的对抗扰动修改训练样本的子集，以改变特定测试数据的预测结果。现有的防御机制不适合实际部署，因为它们往往要么严重影响泛化性能，要么是特定攻击的，且应用起来速度过慢。在这里，我们提出了一种简单但非常有效的方法，与现有方法不同，它能在泛化性能稍微下降的情况下防御各种类型的不可见污染攻击。我们观察到攻击会引入局部尖锐的高训练损失区域，当这些区域被最小化时，攻击就会成功。为了防止污染攻击，我们的关键思想是减轻毒素引入的尖锐损失区域。为此，我们的方法包括两个组成部分：优化的友好噪声，用于最大程度地扰动样本。",
    "tldr": "本研究提出了一种简单但非常有效的方法，能够在不影响泛化性能的情况下防御各种类型的不可见污染攻击。我们通过减轻污染引入的尖锐损失区域，生成优化的友好噪声，实现对抗性噪声的防御。",
    "en_tdlr": "This research proposes a simple but highly effective approach to defend against various types of invisible data poisoning attacks without sacrificing generalization performance. By mitigating sharp loss regions introduced by adversarial perturbations, an optimized friendly noise is generated for defense."
}