{
    "title": "Massively Multilingual Lexical Specialization of Multilingual Transformers. (arXiv:2208.01018v3 [cs.CL] UPDATED)",
    "abstract": "While pretrained language models (PLMs) primarily serve as general-purpose text encoders that can be fine-tuned for a wide variety of downstream tasks, recent work has shown that they can also be rewired to produce high-quality word representations (i.e., static word embeddings) and yield good performance in type-level lexical tasks. While existing work primarily focused on the lexical specialization of monolingual PLMs with immense quantities of monolingual constraints, in this work we expose massively multilingual transformers (MMTs, e.g., mBERT or XLM-R) to multilingual lexical knowledge at scale, leveraging BabelNet as the readily available rich source of multilingual and cross-lingual type-level lexical knowledge. Concretely, we use BabelNet's multilingual synsets to create synonym pairs (or synonym-gloss pairs) across 50 languages and then subject the MMTs (mBERT and XLM-R) to a lexical specialization procedure guided by a contrastive objective. We show that such massively multil",
    "link": "http://arxiv.org/abs/2208.01018",
    "context": "Title: Massively Multilingual Lexical Specialization of Multilingual Transformers. (arXiv:2208.01018v3 [cs.CL] UPDATED)\nAbstract: While pretrained language models (PLMs) primarily serve as general-purpose text encoders that can be fine-tuned for a wide variety of downstream tasks, recent work has shown that they can also be rewired to produce high-quality word representations (i.e., static word embeddings) and yield good performance in type-level lexical tasks. While existing work primarily focused on the lexical specialization of monolingual PLMs with immense quantities of monolingual constraints, in this work we expose massively multilingual transformers (MMTs, e.g., mBERT or XLM-R) to multilingual lexical knowledge at scale, leveraging BabelNet as the readily available rich source of multilingual and cross-lingual type-level lexical knowledge. Concretely, we use BabelNet's multilingual synsets to create synonym pairs (or synonym-gloss pairs) across 50 languages and then subject the MMTs (mBERT and XLM-R) to a lexical specialization procedure guided by a contrastive objective. We show that such massively multil",
    "path": "papers/22/08/2208.01018.json",
    "total_tokens": 1008,
    "translated_title": "大规模多语言变压器的词汇专门化",
    "translated_abstract": "预训练语言模型（PLMs）主要用作通用文本编码器，可以微调用于各种下游任务中，但最近的研究表明它们也可以被重构为生成高质量的词表示，从而在类型级别的词汇任务中表现良好。本文的研究重点是将大规模多语言变压器（MMTs，例如mBERT或XLM-R）暴露于规模化的多语言词汇知识中，利用BabelNet作为可用的多语言和跨语言类型级别词汇知识的丰富来源。具体而言，我们使用BabelNet的多语言同义词集创建跨50种语言的同义词对（或同义词-词汇对），然后通过对比目标引导MMTs（mBERT和XLM-R）进行词汇专门化处理。我们展示了这种大规模多语言词汇专门化方法极大地提高了跨语言词嵌入、多语言语义检索和多语言文档分类的质量，优于使用单语和双语约束的最先进方法。",
    "tldr": "使用BabelNet的多语言同义词集，让大规模多语言变压器进行词汇专门化处理，能够大大提高跨语言词嵌入、多语言语义检索和多语言文档分类的表现。",
    "en_tdlr": "By exposing massively multilingual transformers to multilingual lexical knowledge at scale and subjecting them to a lexical specialization procedure guided by a contrastive objective, this work greatly improves the quality of cross-lingual word embeddings, multilingual semantic retrieval, and multilingual document classification, outperforming state-of-the-art methods that use monolingual and bilingual constraints."
}