{
    "title": "ILLUME: Rationalizing Vision-Language Models through Human Interactions. (arXiv:2208.08241v4 [cs.LG] UPDATED)",
    "abstract": "Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine-generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intent. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised finetuning while using significantly fewer training data and only requiring minimal feedback.",
    "link": "http://arxiv.org/abs/2208.08241",
    "context": "Title: ILLUME: Rationalizing Vision-Language Models through Human Interactions. (arXiv:2208.08241v4 [cs.LG] UPDATED)\nAbstract: Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine-generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intent. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised finetuning while using significantly fewer training data and only requiring minimal feedback.",
    "path": "papers/22/08/2208.08241.json",
    "total_tokens": 896,
    "translated_title": "ILLUME：通过人机交互来合理化视觉-语言模型",
    "translated_abstract": "基于预训练语言模型的引导已被证明是构建视觉-语言模型（VLM）的有效方法，可用于图像字幕或视觉问题回答等任务。然而，这些模型的输出很少与用户对特定答案的理性相一致。为了改善这种对齐并加强常识原因，我们提出了一种基于人机生成数据的调整范例。我们的ILLUME执行以下循环：给定一个图像-问题-答案提示，VLM样本多个候选原理，人类评论家通过偏好选择提供反馈，用于微调。这个循环增加了训练数据，并逐渐雕刻出与人类意图相一致的VLM的理性能力。我们的详尽实验表明，ILLUME在使用 significantly 更少的训练数据仅需要 minimal 反馈的同时，与标准监督微调具有竞争力。",
    "tldr": "本文提出了一种新的调整范例，名为ILLUME，通过人机交互来合理化视觉-语言模型，从而使模型的输出更符合人的思维方式。在使用相对较少的训练数据和最少的人类反馈下，ILLUME表现出与标准监督微调相当的竞争力。",
    "en_tdlr": "This paper proposes a new tuning paradigm called ILLUME that uses human interactions to rationalize vision-language models and align their outputs with human intent. Results show that ILLUME is competitive with standard supervised finetuning while using significantly fewer training data and only requiring minimal feedback."
}