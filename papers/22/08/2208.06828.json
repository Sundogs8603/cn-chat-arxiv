{
    "title": "Multinomial Logistic Regression Algorithms via Quadratic Gradient. (arXiv:2208.06828v2 [cs.LG] UPDATED)",
    "abstract": "Multinomial logistic regression, also known by other names such as multiclass logistic regression and softmax regression, is a fundamental classification method that generalizes binary logistic regression to multiclass problems. A recently work proposed a faster gradient called $\\texttt{quadratic gradient}$ that can accelerate the binary logistic regression training, and presented an enhanced Nesterov's accelerated gradient (NAG) method for binary logistic regression.  In this paper, we extend this work to multiclass logistic regression and propose an enhanced Adaptive Gradient Algorithm (Adagrad) that can accelerate the original Adagrad method. We test the enhanced NAG method and the enhanced Adagrad method on some multiclass-problem datasets. Experimental results show that both enhanced methods converge faster than their original ones respectively.",
    "link": "http://arxiv.org/abs/2208.06828",
    "context": "Title: Multinomial Logistic Regression Algorithms via Quadratic Gradient. (arXiv:2208.06828v2 [cs.LG] UPDATED)\nAbstract: Multinomial logistic regression, also known by other names such as multiclass logistic regression and softmax regression, is a fundamental classification method that generalizes binary logistic regression to multiclass problems. A recently work proposed a faster gradient called $\\texttt{quadratic gradient}$ that can accelerate the binary logistic regression training, and presented an enhanced Nesterov's accelerated gradient (NAG) method for binary logistic regression.  In this paper, we extend this work to multiclass logistic regression and propose an enhanced Adaptive Gradient Algorithm (Adagrad) that can accelerate the original Adagrad method. We test the enhanced NAG method and the enhanced Adagrad method on some multiclass-problem datasets. Experimental results show that both enhanced methods converge faster than their original ones respectively.",
    "path": "papers/22/08/2208.06828.json",
    "total_tokens": 783,
    "translated_title": "通过二次梯度的多项式逻辑回归算法",
    "translated_abstract": "多项式逻辑回归是一种基础的分类方法，它将二元逻辑回归推广到多类问题。最近，一项工作提出了称为“二次梯度”的更快梯度，可以加速二元逻辑回归训练，并提出了一种增强的Nesterov加速梯度（NAG）方法用于二元逻辑回归。本文将这项工作扩展到多类逻辑回归，并提出了一种增强的自适应梯度算法(Adagrad)，可以加速原始Adagrad方法。我们在一些多类问题数据集上测试了增强的NAG方法和增强的Adagrad方法。实验结果显示，这两种增强方法分别比它们的原始方法更快地收敛。",
    "tldr": "本文扩展了二次梯度的方法到多类逻辑回归，提出了一个增强的自适应梯度算法，实验结果表明，这两种增强方法能够分别比它们的原始方法更快地收敛。",
    "en_tdlr": "This paper extends the quadratic gradient method to multinomial logistic regression, proposes an enhanced Adagrad algorithm, and experimental results show that both enhanced methods converge faster than their original ones respectively."
}