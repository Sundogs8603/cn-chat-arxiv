{
    "title": "Efficient Adaptive Activation Rounding for Post-Training Quantization. (arXiv:2208.11945v3 [cs.LG] UPDATED)",
    "abstract": "Post-training quantization attracts increasing attention due to its convenience in deploying quantized neural networks. Although rounding-to-nearest remains the prevailing method for DNN quantization, prior research has demonstrated its suboptimal nature when applied to weight quantization. They propose optimizing weight rounding schemes by leveraging output error rather than the traditional weight quantization error. Our study reveals that similar rounding challenges also extend to activation quantization. Despite the easy generalization, the challenges lie in the dynamic nature of activation. Adaptive rounding is expected for varying activations and the method is subjected to runtime overhead. To tackle this, we propose the AQuant quantization framework with a novel perspective to reduce output error by adjusting rounding schemes of activations. Instead of using the constant rounding border 0.5 of the rounding-to-nearest operation, we make the border become a function w.r.t. the acti",
    "link": "http://arxiv.org/abs/2208.11945",
    "context": "Title: Efficient Adaptive Activation Rounding for Post-Training Quantization. (arXiv:2208.11945v3 [cs.LG] UPDATED)\nAbstract: Post-training quantization attracts increasing attention due to its convenience in deploying quantized neural networks. Although rounding-to-nearest remains the prevailing method for DNN quantization, prior research has demonstrated its suboptimal nature when applied to weight quantization. They propose optimizing weight rounding schemes by leveraging output error rather than the traditional weight quantization error. Our study reveals that similar rounding challenges also extend to activation quantization. Despite the easy generalization, the challenges lie in the dynamic nature of activation. Adaptive rounding is expected for varying activations and the method is subjected to runtime overhead. To tackle this, we propose the AQuant quantization framework with a novel perspective to reduce output error by adjusting rounding schemes of activations. Instead of using the constant rounding border 0.5 of the rounding-to-nearest operation, we make the border become a function w.r.t. the acti",
    "path": "papers/22/08/2208.11945.json",
    "total_tokens": 879,
    "translated_title": "高效自适应激活舍入用于训练后量化",
    "translated_abstract": "由于在部署量化神经网络方面的便利性，训练后量化受到越来越多的关注。尽管最近的研究表明最近舍入仍然是DNN量化的主流方法，但它在权重量化中表现出了次优的性质。他们提出通过利用输出误差而不是传统的权重量化误差来优化权重舍入方案。我们的研究发现相似的舍入挑战也适用于激活量化。尽管很容易推广，但挑战在于激活的动态性。需要针对不同的激活进行自适应舍入，并且该方法会导致运行时开销。为了解决这个问题，我们提出了一种新的AQuant量化框架，通过调整激活的舍入方案来降低输出误差。与使用常数舍入边界0.5的最近舍入操作不同，我们使边界成为激活的函数。",
    "tldr": "本论文提出了一种高效自适应激活舍入的训练后量化方法，通过调整激活的舍入方案来降低输出误差，并解决了动态激活和运行时开销的挑战。",
    "en_tdlr": "This paper proposes an efficient adaptive activation rounding method for post-training quantization, which reduces output error by adjusting the rounding schemes of activations, addressing the challenges of dynamic activation and runtime overhead."
}