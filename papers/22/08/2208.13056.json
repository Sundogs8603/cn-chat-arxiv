{
    "title": "Lossy Image Compression with Quantized Hierarchical VAEs. (arXiv:2208.13056v2 [eess.IV] UPDATED)",
    "abstract": "Recent research has shown a strong theoretical connection between variational autoencoders (VAEs) and the rate-distortion theory. Motivated by this, we consider the problem of lossy image compression from the perspective of generative modeling. Starting with ResNet VAEs, which are originally designed for data (image) distribution modeling, we redesign their latent variable model using a quantization-aware posterior and prior, enabling easy quantization and entropy coding at test time. Along with improved neural network architecture, we present a powerful and efficient model that outperforms previous methods on natural image lossy compression. Our model compresses images in a coarse-to-fine fashion and supports parallel encoding and decoding, leading to fast execution on GPUs. Code is available at https://github.com/duanzhiihao/lossy-vae.",
    "link": "http://arxiv.org/abs/2208.13056",
    "context": "Title: Lossy Image Compression with Quantized Hierarchical VAEs. (arXiv:2208.13056v2 [eess.IV] UPDATED)\nAbstract: Recent research has shown a strong theoretical connection between variational autoencoders (VAEs) and the rate-distortion theory. Motivated by this, we consider the problem of lossy image compression from the perspective of generative modeling. Starting with ResNet VAEs, which are originally designed for data (image) distribution modeling, we redesign their latent variable model using a quantization-aware posterior and prior, enabling easy quantization and entropy coding at test time. Along with improved neural network architecture, we present a powerful and efficient model that outperforms previous methods on natural image lossy compression. Our model compresses images in a coarse-to-fine fashion and supports parallel encoding and decoding, leading to fast execution on GPUs. Code is available at https://github.com/duanzhiihao/lossy-vae.",
    "path": "papers/22/08/2208.13056.json",
    "total_tokens": 842,
    "translated_title": "量化分层VAE的有损图像压缩",
    "translated_abstract": "最近的研究表明了变分自编码器（VAE）与速率失真理论之间的强大理论联系。鉴于此，我们从生成建模的角度考虑了有损图像压缩问题。通过使用量化感知后验和先验重新设计ResNet VAE的潜在变量模型，使其能够在测试时轻松量化和熵编码。我们采用改进的神经网络架构，呈现了一个强大而高效的模型，其在自然图像有损压缩上优于以往方法。我们的模型以粗糙到精细的方式压缩图像，支持并行编解码，从而在GPU上快速执行。代码可在https://github.com/duanzhiihao/lossy-vae获得。",
    "tldr": "该论文提出了一种量化分层VAE模型，通过重新设计潜在变量模型，让模型在测试时能够容易地进行量化和熵编码，该模型通过粗糙到精细的方式压缩图像，支持并行编解码，在自然图像有损压缩上表现优异。",
    "en_tdlr": "The paper proposes a quantized hierarchical VAE model which enables easy quantization and entropy coding at test time through redesigning the latent variable model, compresses images in a coarse-to-fine fashion, and supports parallel encoding and decoding, outperforming previous methods on natural image lossy compression."
}