{
    "title": "Why Deep Learning's Performance Data Are Misleading. (arXiv:2208.11228v3 [cs.LG] UPDATED)",
    "abstract": "This is a theoretical paper, as a companion paper of the keynote talk at the same conference AIEE 2023. In contrast to conscious learning, many projects in AI have employed so-called \"deep learning\" many of which seemed to give impressive performance. This paper explains that such performance data are deceptively inflated due to two misconducts: \"data deletion\" and \"test on training set\". This paper clarifies \"data deletion\" and \"test on training set\" in deep learning and why they are misconducts. A simple classification method is defined, called Nearest Neighbor With Threshold (NNWT). A theorem is established that the NNWT method reaches a zero error on any validation set and any test set using the two misconducts, as long as the test set is in the possession of the author and both the amount of storage space and the time of training are finite but unbounded like with many deep learning methods. However, many deep learning methods, like the NNWT method, are all not generalizable since",
    "link": "http://arxiv.org/abs/2208.11228",
    "context": "Title: Why Deep Learning's Performance Data Are Misleading. (arXiv:2208.11228v3 [cs.LG] UPDATED)\nAbstract: This is a theoretical paper, as a companion paper of the keynote talk at the same conference AIEE 2023. In contrast to conscious learning, many projects in AI have employed so-called \"deep learning\" many of which seemed to give impressive performance. This paper explains that such performance data are deceptively inflated due to two misconducts: \"data deletion\" and \"test on training set\". This paper clarifies \"data deletion\" and \"test on training set\" in deep learning and why they are misconducts. A simple classification method is defined, called Nearest Neighbor With Threshold (NNWT). A theorem is established that the NNWT method reaches a zero error on any validation set and any test set using the two misconducts, as long as the test set is in the possession of the author and both the amount of storage space and the time of training are finite but unbounded like with many deep learning methods. However, many deep learning methods, like the NNWT method, are all not generalizable since",
    "path": "papers/22/08/2208.11228.json",
    "total_tokens": 886,
    "translated_title": "深度学习性能数据为何误导人",
    "translated_abstract": "这篇论文是AIEE 2023年同名会议的主题演讲的附带论文，属于一篇理论性文章。文章解释了AI项目中所谓的“深度学习”给出了印象深刻的性能表现，但这些表现数据实际上被错误地夸大了。文章阐明了“数据删除”和“在训练集上测试”的误导性，并定义了一种简单的分类方法“最近邻加阈值”（NNWT）。文章建立了一个定理，即在具有有限但无限的存储空间和训练时间（与许多深度学习方法类似）的情况下，NNWT方法使用这两种不当行为可以在任何验证集和任何测试集上达到零误差，但是许多深度学习方法，包括NNWT方法在内，并不具有可泛化性。",
    "tldr": "本文解释了“深度学习”中的两种误导性行为“数据删除”和“在训练集上测试”，并建立了一个定理，即NNWT方法可以使用这些不当行为在任何验证集和测试集上达到零误差，但是这些方法并不具有可泛化性。",
    "en_tdlr": "The paper explains two misleading behaviors, \"data deletion\" and \"test on training set\", in deep learning, and establishes a theorem that the NNWT method can achieve zero error on any validation and test set using these behaviors, which, however, are not generalizable."
}