{
    "title": "On the Evaluation of User Privacy in Deep Neural Networks using Timing Side Channel",
    "abstract": "arXiv:2208.01113v3 Announce Type: replace-cross  Abstract: Recent Deep Learning (DL) advancements in solving complex real-world tasks have led to its widespread adoption in practical applications. However, this opportunity comes with significant underlying risks, as many of these models rely on privacy-sensitive data for training in a variety of applications, making them an overly-exposed threat surface for privacy violations. Furthermore, the widespread use of cloud-based Machine-Learning-as-a-Service (MLaaS) for its robust infrastructure support has broadened the threat surface to include a variety of remote side-channel attacks. In this paper, we first identify and report a novel data-dependent timing side-channel leakage (termed Class Leakage) in DL implementations originating from non-constant time branching operation in a widely used DL framework PyTorch. We further demonstrate a practical inference-time attack where an adversary with user privilege and hard-label black-box acces",
    "link": "https://arxiv.org/abs/2208.01113",
    "context": "Title: On the Evaluation of User Privacy in Deep Neural Networks using Timing Side Channel\nAbstract: arXiv:2208.01113v3 Announce Type: replace-cross  Abstract: Recent Deep Learning (DL) advancements in solving complex real-world tasks have led to its widespread adoption in practical applications. However, this opportunity comes with significant underlying risks, as many of these models rely on privacy-sensitive data for training in a variety of applications, making them an overly-exposed threat surface for privacy violations. Furthermore, the widespread use of cloud-based Machine-Learning-as-a-Service (MLaaS) for its robust infrastructure support has broadened the threat surface to include a variety of remote side-channel attacks. In this paper, we first identify and report a novel data-dependent timing side-channel leakage (termed Class Leakage) in DL implementations originating from non-constant time branching operation in a widely used DL framework PyTorch. We further demonstrate a practical inference-time attack where an adversary with user privilege and hard-label black-box acces",
    "path": "papers/22/08/2208.01113.json",
    "total_tokens": 864,
    "translated_title": "对使用时间侧信道在深度神经网络中评估用户隐私的研究",
    "translated_abstract": "最近深度学习（DL）在解决复杂现实世界任务方面取得的进展导致其在实际应用中得到广泛采用。然而，这一机会伴随着重要的潜在风险，因为许多模型依赖于用于训练的涉及隐私的数据，在各种应用中使其成为过度暴露的隐私侵犯威胁面。此外，广泛使用基于云的机器学习即服务（MLaaS）以获取其强大的基础设施支持，将威胁面扩展到包括各种远程侧信道攻击。在本文中，我们首先识别并报告了一个新颖的数据相关的DL实现中的时序侧信道泄漏（称为类泄漏），源自于广泛使用的DL框架PyTorch中的非常量时间分支操作。我们进一步展示了一种实用的推理时攻击，其中拥有用户特权和硬标签黑盒访问权限的对手",
    "tldr": "在本研究中，我们发现并报告了DL实现中的一种新型数据依赖性时序侧信道泄漏，并展示了一个实用的推理时攻击。",
    "en_tdlr": "In this study, we identified and reported a novel data-dependent timing side-channel leakage in DL implementations and demonstrated a practical inference-time attack."
}