{
    "title": "What Can Be Learnt With Wide Convolutional Neural Networks?. (arXiv:2208.01003v5 [stat.ML] UPDATED)",
    "abstract": "Understanding how convolutional neural networks (CNNs) can efficiently learn high-dimensional functions remains a fundamental challenge. A popular belief is that these models harness the local and hierarchical structure of natural data such as images. Yet, we lack a quantitative understanding of how such structure affects performance, e.g., the rate of decay of the generalisation error with the number of training samples. In this paper, we study infinitely-wide deep CNNs in the kernel regime. First, we show that the spectrum of the corresponding kernel inherits the hierarchical structure of the network, and we characterise its asymptotics. Then, we use this result together with generalisation bounds to prove that deep CNNs adapt to the spatial scale of the target function. In particular, we find that if the target function depends on low-dimensional subsets of adjacent input variables, then the decay of the error is controlled by the effective dimensionality of these subsets. Conversel",
    "link": "http://arxiv.org/abs/2208.01003",
    "context": "Title: What Can Be Learnt With Wide Convolutional Neural Networks?. (arXiv:2208.01003v5 [stat.ML] UPDATED)\nAbstract: Understanding how convolutional neural networks (CNNs) can efficiently learn high-dimensional functions remains a fundamental challenge. A popular belief is that these models harness the local and hierarchical structure of natural data such as images. Yet, we lack a quantitative understanding of how such structure affects performance, e.g., the rate of decay of the generalisation error with the number of training samples. In this paper, we study infinitely-wide deep CNNs in the kernel regime. First, we show that the spectrum of the corresponding kernel inherits the hierarchical structure of the network, and we characterise its asymptotics. Then, we use this result together with generalisation bounds to prove that deep CNNs adapt to the spatial scale of the target function. In particular, we find that if the target function depends on low-dimensional subsets of adjacent input variables, then the decay of the error is controlled by the effective dimensionality of these subsets. Conversel",
    "path": "papers/22/08/2208.01003.json",
    "total_tokens": 1030,
    "translated_title": "宽卷积神经网络能够学到什么？",
    "translated_abstract": "理解卷积神经网络（CNN）如何高效地学习高维函数仍然是一个基本挑战。人们普遍认为，这些模型利用了自然数据（如图像）的局部和分层结构。然而，我们缺乏如此结构如何影响性能的量化理解，如泛化误差随训练样本数量的衰减速率。本文研究了在内核环境下的无限宽卷积神经网络，并展示了相应核的谱沿袭了网络的分层结构，并表征了其渐进性。然后，我们将这个结果与泛化误差界限结合起来，证明了深层CNN能够适应目标函数的空间尺度。特别是，我们发现，如果目标函数依赖于相邻输入变量的低维子集，则误差的衰减受到这些子集的有效维数的控制。相反，如果函数依赖于高维结构，则有效维数受网络宽度的影响。我们的结果表明，在过度参数化的情况下，即使数据没有局部结构，深层CNN也可以学习，只要全局结构可以被利用。",
    "tldr": "本文研究在内核环境下的无限宽卷积神经网络，证明了深层CNN能够适应目标函数的空间尺度，即使数据没有局部结构，深层CNN也可以学习，只要全局结构可以被利用。",
    "en_tdlr": "This paper studies infinitely-wide deep CNNs in the kernel regime and proves that deep CNNs can adapt to the spatial scale of the target function, even when data does not have local structure and that learning can be achieved as long as the global structure can be exploited."
}