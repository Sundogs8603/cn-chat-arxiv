{
    "title": "Optimal scheduling of entropy regulariser for continuous-time linear-quadratic reinforcement learning. (arXiv:2208.04466v3 [cs.LG] UPDATED)",
    "abstract": "This work uses the entropy-regularised relaxed stochastic control perspective as a principled framework for designing reinforcement learning (RL) algorithms. Herein agent interacts with the environment by generating noisy controls distributed according to the optimal relaxed policy. The noisy policies on the one hand, explore the space and hence facilitate learning but, on the other hand, introduce bias by assigning a positive probability to non-optimal actions. This exploration-exploitation trade-off is determined by the strength of entropy regularisation. We study algorithms resulting from two entropy regularisation formulations: the exploratory control approach, where entropy is added to the cost objective, and the proximal policy update approach, where entropy penalises policy divergence between consecutive episodes. We focus on the finite horizon continuous-time linear-quadratic (LQ) RL problem, where a linear dynamics with unknown drift coefficients is controlled subject to quadr",
    "link": "http://arxiv.org/abs/2208.04466",
    "context": "Title: Optimal scheduling of entropy regulariser for continuous-time linear-quadratic reinforcement learning. (arXiv:2208.04466v3 [cs.LG] UPDATED)\nAbstract: This work uses the entropy-regularised relaxed stochastic control perspective as a principled framework for designing reinforcement learning (RL) algorithms. Herein agent interacts with the environment by generating noisy controls distributed according to the optimal relaxed policy. The noisy policies on the one hand, explore the space and hence facilitate learning but, on the other hand, introduce bias by assigning a positive probability to non-optimal actions. This exploration-exploitation trade-off is determined by the strength of entropy regularisation. We study algorithms resulting from two entropy regularisation formulations: the exploratory control approach, where entropy is added to the cost objective, and the proximal policy update approach, where entropy penalises policy divergence between consecutive episodes. We focus on the finite horizon continuous-time linear-quadratic (LQ) RL problem, where a linear dynamics with unknown drift coefficients is controlled subject to quadr",
    "path": "papers/22/08/2208.04466.json",
    "total_tokens": 910,
    "translated_title": "连续时间线性二次强化学习中熵正则化的最优调度",
    "translated_abstract": "本研究使用熵正则化的松弛随机控制视角作为设计强化学习算法的基础框架。在这里，Agent通过生成符合最优松弛策略的噪声控制与环境进行交互。噪声策略一方面可以探索空间并促进学习，但另一方面也会引入偏差，将正概率分配给非最优动作。这种探索与利用的权衡由熵正则化的强度来确定。我们研究了两种熵正则化形式得到的算法：探索性控制方法，在成本目标中添加熵；近端策略更新方法，在连续的Episode之间对策略差异进行熵惩罚。我们重点研究了有限时间连续时间线性二次强化学习问题，其中具有未知漂移系数的线性动力学受到四次方约束的控制。",
    "tldr": "本研究利用熵正则化的松弛随机控制视角设计了连续时间线性二次强化学习算法，并通过探索性控制方法和近端策略更新方法实现了探索和利用的权衡，以解决有限时间线性二次强化学习问题。",
    "en_tdlr": "This work presents an optimal scheduling approach for entropy regularisation in continuous-time linear-quadratic reinforcement learning, using the exploratory control and proximal policy update methods to handle the exploration-exploitation trade-off. The proposed algorithms effectively address the finite horizon problem with linear dynamics and unknown drift coefficients."
}