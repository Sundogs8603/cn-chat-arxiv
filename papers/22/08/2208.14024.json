{
    "title": "Positive Difference Distribution for Image Outlier Detection using Normalizing Flows and Contrastive Data. (arXiv:2208.14024v2 [cs.LG] UPDATED)",
    "abstract": "Detecting test data deviating from training data is a central problem for safe and robust machine learning. Likelihoods learned by a generative model, e.g., a normalizing flow via standard log-likelihood training, perform poorly as an outlier score. We propose to use an unlabelled auxiliary dataset and a probabilistic outlier score for outlier detection. We use a self-supervised feature extractor trained on the auxiliary dataset and train a normalizing flow on the extracted features by maximizing the likelihood on in-distribution data and minimizing the likelihood on the contrastive dataset. We show that this is equivalent to learning the normalized positive difference between the in-distribution and the contrastive feature density. We conduct experiments on benchmark datasets and compare to the likelihood, the likelihood ratio and state-of-the-art anomaly detection methods.",
    "link": "http://arxiv.org/abs/2208.14024",
    "context": "Title: Positive Difference Distribution for Image Outlier Detection using Normalizing Flows and Contrastive Data. (arXiv:2208.14024v2 [cs.LG] UPDATED)\nAbstract: Detecting test data deviating from training data is a central problem for safe and robust machine learning. Likelihoods learned by a generative model, e.g., a normalizing flow via standard log-likelihood training, perform poorly as an outlier score. We propose to use an unlabelled auxiliary dataset and a probabilistic outlier score for outlier detection. We use a self-supervised feature extractor trained on the auxiliary dataset and train a normalizing flow on the extracted features by maximizing the likelihood on in-distribution data and minimizing the likelihood on the contrastive dataset. We show that this is equivalent to learning the normalized positive difference between the in-distribution and the contrastive feature density. We conduct experiments on benchmark datasets and compare to the likelihood, the likelihood ratio and state-of-the-art anomaly detection methods.",
    "path": "papers/22/08/2208.14024.json",
    "total_tokens": 867,
    "translated_title": "基于归一化流和对比数据的图像异常检测中的正差分布",
    "translated_abstract": "检测与训练数据有偏差的测试数据是安全和稳健机器学习的一个中心问题。通过标准对数似然训练，例如归一化流学习的可能性作为异常得分效果不好。本文提出使用未标记的辅助数据集和概率异常得分进行异常检测。我们使用自监督特征提取器对辅助数据集进行训练，并通过最大化在分布数据上的可能性和最小化在对比数据上的可能性训练提取特征的归一化流。我们证明这相当于学习分布数据和对比数据之间的正常化正差分布。我们在基准数据集上进行实验，并将其与似然、似然比和最先进的异常检测方法进行比较。",
    "tldr": "本研究提出了一种使用未标记的辅助数据集和概率异常得分进行异常检测的方法，可以有效地检测与训练数据有偏差的测试数据。该方法使用自监督特征提取器训练，通过学习正差分布来提高检测效果并在基准数据集上进行验证。",
    "en_tdlr": "This paper proposes a method of outlier detection using an unlabelled auxiliary dataset and a probabilistic outlier score. The approach uses a self-supervised feature extractor to train a normalizing flow and learns the positive difference distribution between in-distribution and contrastive feature density to enhance detection. The experiments on benchmark datasets demonstrate the effectiveness of the proposed method."
}