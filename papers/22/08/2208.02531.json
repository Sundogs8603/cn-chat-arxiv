{
    "title": "InitialGAN: A Language GAN with Completely Random Initialization. (arXiv:2208.02531v3 [cs.CL] UPDATED)",
    "abstract": "Text generative models trained via Maximum Likelihood Estimation (MLE) suffer from the notorious exposure bias problem, and Generative Adversarial Networks (GANs) are shown to have potential to tackle this problem. Existing language GANs adopt estimators like REINFORCE or continuous relaxations to model word probabilities. The inherent limitations of such estimators lead current models to rely on pre-training techniques (MLE pre-training or pre-trained embeddings). Representation modeling methods which are free from those limitations, however, are seldomly explored because of their poor performance in previous attempts. Our analyses reveal that invalid sampling methods and unhealthy gradients are the main contributors to such unsatisfactory performance. In this work, we present two techniques to tackle these problems: dropout sampling and fully normalized LSTM. Based on these two techniques, we propose InitialGAN whose parameters are randomly initialized in full. Besides, we introduce ",
    "link": "http://arxiv.org/abs/2208.02531",
    "context": "Title: InitialGAN: A Language GAN with Completely Random Initialization. (arXiv:2208.02531v3 [cs.CL] UPDATED)\nAbstract: Text generative models trained via Maximum Likelihood Estimation (MLE) suffer from the notorious exposure bias problem, and Generative Adversarial Networks (GANs) are shown to have potential to tackle this problem. Existing language GANs adopt estimators like REINFORCE or continuous relaxations to model word probabilities. The inherent limitations of such estimators lead current models to rely on pre-training techniques (MLE pre-training or pre-trained embeddings). Representation modeling methods which are free from those limitations, however, are seldomly explored because of their poor performance in previous attempts. Our analyses reveal that invalid sampling methods and unhealthy gradients are the main contributors to such unsatisfactory performance. In this work, we present two techniques to tackle these problems: dropout sampling and fully normalized LSTM. Based on these two techniques, we propose InitialGAN whose parameters are randomly initialized in full. Besides, we introduce ",
    "path": "papers/22/08/2208.02531.json",
    "total_tokens": 895,
    "translated_title": "InitialGAN：一种完全随机初始化的语言生成对抗网络",
    "translated_abstract": "通过最大似然估计（MLE）训练的文本生成模型存在“曝光偏差”问题，而生成对抗网络（GAN）被证明可以解决这一问题。现有的语言GAN采用REINFORCE或连续弛豫等估计器来建模词概率。然而，这些估计器的固有局限性导致当前模型依赖预训练技术（MLE预训练或预训练嵌入）。然而，由于以前的尝试中这些表示建模方法的性能较差，因此很少被研究。我们的分析表明，无效的采样方法和不健康的梯度是导致这种不令人满意的性能的主要因素。在这项工作中，我们提出了两种解决这些问题的技巧：dropout采样和完全归一化的LSTM。基于这两种技术，我们提出了完全随机初始化参数的InitialGAN。此外，我们还引入了",
    "tldr": "本论文提出了一种名为InitialGAN的语言生成对抗网络，通过采用dropout采样和完全归一化的LSTM等技术，解决了当前语言GAN模型依赖预训练技术的限制，实现了完全随机初始化参数的模型。"
}