{
    "title": "Visual correspondence-based explanations improve AI robustness and human-AI team accuracy. (arXiv:2208.00780v5 [cs.CV] UPDATED)",
    "abstract": "Explaining artificial intelligence (AI) predictions is increasingly important and even imperative in many high-stakes applications where humans are the ultimate decision-makers. In this work, we propose two novel architectures of self-interpretable image classifiers that first explain, and then predict (as opposed to post-hoc explanations) by harnessing the visual correspondences between a query image and exemplars. Our models consistently improve (by 1 to 4 points) on out-of-distribution (OOD) datasets while performing marginally worse (by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB, our correspondence-based explanations are found to be more useful to users than kNN explanations. Our explanations help users more accurately reject AI's wrong decisions than all other tested methods. Interestingly, for the first time, we show that it is possible to achieve complementary human-AI tea",
    "link": "http://arxiv.org/abs/2208.00780",
    "context": "Title: Visual correspondence-based explanations improve AI robustness and human-AI team accuracy. (arXiv:2208.00780v5 [cs.CV] UPDATED)\nAbstract: Explaining artificial intelligence (AI) predictions is increasingly important and even imperative in many high-stakes applications where humans are the ultimate decision-makers. In this work, we propose two novel architectures of self-interpretable image classifiers that first explain, and then predict (as opposed to post-hoc explanations) by harnessing the visual correspondences between a query image and exemplars. Our models consistently improve (by 1 to 4 points) on out-of-distribution (OOD) datasets while performing marginally worse (by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB, our correspondence-based explanations are found to be more useful to users than kNN explanations. Our explanations help users more accurately reject AI's wrong decisions than all other tested methods. Interestingly, for the first time, we show that it is possible to achieve complementary human-AI tea",
    "path": "papers/22/08/2208.00780.json",
    "total_tokens": 1016,
    "translated_title": "基于视觉对应的解释提高了AI的鲁棒性和人机团队准确性",
    "translated_abstract": "在许多高风险应用中，解释人工智能（AI）的预测变得越来越重要，甚至是必不可少的，因为人类是最终的决策者。在这项工作中，我们提出了两种新颖的自解释图像分类器架构，它们首先通过利用查询图像和示例之间的视觉对应关系进行解释，然后进行预测（与事后解释相对）。我们的模型在超出分布（OOD）数据集上一致改进（1到4个点），而在分布测试上表现略次于ResNet-50和一个k最近邻分类器（kNN）（下降1到2个点）。通过对ImageNet和CUB进行的大规模人类研究，我们发现基于对应关系的解释比kNN解释对用户更有用。我们的解释帮助用户更准确地拒绝AI的错误决策，胜过所有其他测试方法。有趣的是，我们首次展示了实现互补人机团队准确性的可能性。",
    "tldr": "该论文提出了基于视觉对应的解释方法，用于改善AI的鲁棒性和人机团队的准确性。在大规模的人类研究中，该方法被发现比kNN解释更有用，帮助用户更准确地拒绝AI的错误决策。同时，该方法在超出分布数据集上改进了性能，并实现了互补人机团队准确性的可能性。"
}