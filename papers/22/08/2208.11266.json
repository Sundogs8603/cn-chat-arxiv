{
    "title": "SCALE: Online Self-Supervised Lifelong Learning without Prior Knowledge. (arXiv:2208.11266v5 [cs.LG] UPDATED)",
    "abstract": "Unsupervised lifelong learning refers to the ability to learn over time while memorizing previous patterns without supervision. Although great progress has been made in this direction, existing work often assumes strong prior knowledge about the incoming data (e.g., knowing the class boundaries), which can be impossible to obtain in complex and unpredictable environments. In this paper, motivated by real-world scenarios, we propose a more practical problem setting called online self-supervised lifelong learning without prior knowledge. The proposed setting is challenging due to the non-iid and single-pass data, the absence of external supervision, and no prior knowledge. To address the challenges, we propose Self-Supervised ContrAstive Lifelong LEarning without Prior Knowledge (SCALE) which can extract and memorize representations on the fly purely from the data continuum. SCALE is designed around three major components: a pseudo-supervised contrastive loss, a self-supervised forgettin",
    "link": "http://arxiv.org/abs/2208.11266",
    "context": "Title: SCALE: Online Self-Supervised Lifelong Learning without Prior Knowledge. (arXiv:2208.11266v5 [cs.LG] UPDATED)\nAbstract: Unsupervised lifelong learning refers to the ability to learn over time while memorizing previous patterns without supervision. Although great progress has been made in this direction, existing work often assumes strong prior knowledge about the incoming data (e.g., knowing the class boundaries), which can be impossible to obtain in complex and unpredictable environments. In this paper, motivated by real-world scenarios, we propose a more practical problem setting called online self-supervised lifelong learning without prior knowledge. The proposed setting is challenging due to the non-iid and single-pass data, the absence of external supervision, and no prior knowledge. To address the challenges, we propose Self-Supervised ContrAstive Lifelong LEarning without Prior Knowledge (SCALE) which can extract and memorize representations on the fly purely from the data continuum. SCALE is designed around three major components: a pseudo-supervised contrastive loss, a self-supervised forgettin",
    "path": "papers/22/08/2208.11266.json",
    "total_tokens": 904,
    "translated_title": "SCALE：无先验知识的在线自监督终身学习",
    "translated_abstract": "无监督的终身学习是指能够在没有监督的情况下在时间上学习新的模式并记忆以前的模式。尽管在这个方向上取得了很大的进展，但现有的工作通常假设有关输入数据的强先验知识（例如，已知类别边界），而这在复杂和不可预测的环境中可能是不可能获得的。为了解决这个问题，本文提出了在线自监督终身学习无先验知识的更实践的问题设置。为了应对挑战，我们提出了一种名为SCALE的自监督对比终身学习方法，它可以纯粹地从数据连续体中提取和记忆表示。",
    "tldr": "本文提出了一个更加实用的在线自监督终身学习的问题设置，该设置具有挑战性，因为它涉及到数据的非独立同分布和单次遍历、缺乏外部监督和先验知识等。为了解决这些问题，我们提出了SCALE方法，它可以纯粹地从数据连续体中提取和记忆表示。",
    "en_tdlr": "This paper proposes a more practical problem setting for unsupervised lifelong learning called online self-supervised lifelong learning without prior knowledge. To address the challenges of non-iid and single-pass data, absence of external supervision, and no prior knowledge, the authors propose a method called SCALE that can extract and memorize representations on the fly purely from the data continuum using a pseudo-supervised contrastive loss and a self-supervised forgetting mechanism."
}