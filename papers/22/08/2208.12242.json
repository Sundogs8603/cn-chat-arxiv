{
    "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. (arXiv:2208.12242v2 [cs.CV] UPDATED)",
    "abstract": "Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \"personalization\" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions th",
    "link": "http://arxiv.org/abs/2208.12242",
    "context": "Title: DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. (arXiv:2208.12242v2 [cs.CV] UPDATED)\nAbstract: Large text-to-image models achieved a remarkable leap in the evolution of AI, enabling high-quality and diverse synthesis of images from a given text prompt. However, these models lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts. In this work, we present a new approach for \"personalization\" of text-to-image diffusion models. Given as input just a few images of a subject, we fine-tune a pretrained text-to-image model such that it learns to bind a unique identifier with that specific subject. Once the subject is embedded in the output domain of the model, the unique identifier can be used to synthesize novel photorealistic images of the subject contextualized in different scenes. By leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss, our technique enables synthesizing the subject in diverse scenes, poses, views and lighting conditions th",
    "path": "papers/22/08/2208.12242.json",
    "total_tokens": 976,
    "translated_title": "DreamBooth：针对主题驱动的生成进行文本到图像扩散模型微调",
    "translated_abstract": "大型文本到图像模型的发展让AI的演变达到了一个显著的飞跃，实现了从给定文本提示中高质量和多样化的图像合成。然而，这些模型缺乏模仿给定参考集中主体出现的外观和在不同上下文中合成它们的新版本的能力。在本项工作中，我们提出了一种用于“个性化”文本到图像扩散模型的新方法。只需输入一些该主题的图像，我们就对预训练的文本到图像模型进行微调，从而使其学会将唯一标识符与该特定主题绑定。一旦该主题被嵌入模型的输出域中，该唯一标识符就可以用于在不同场景中合成主题的新颖逼真图像。通过利用嵌入在模型中的语义先验和新的自治类特定先验保存损失，我们的技术使得在不同场景、姿态、视角和光照条件下合成主题成为可能。",
    "tldr": "DreamBooth是一种针对主题驱动的文本到图像扩散模型个性化方法，通过微调预训练的文本到图像模型，使用样本图像来实现生成逼真图像的独特标识符绑定，使其可以在不同场景中合成该主题的新版逼真图像。",
    "en_tdlr": "DreamBooth is a personalized method for subject-driven text-to-image diffusion models, which fine-tunes a pretrained model using sample images to bind a unique identifier with a specific subject, enabling the synthesis of novel photorealistic images of the subject in different scenes. This is achieved by leveraging the semantic prior embedded in the model with a new autogenous class-specific prior preservation loss."
}