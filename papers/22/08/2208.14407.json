{
    "title": "An Analysis of Abstracted Model-Based Reinforcement Learning. (arXiv:2208.14407v2 [cs.LG] UPDATED)",
    "abstract": "Many methods for Model-based Reinforcement learning (MBRL) in Markov decision processes (MDPs) provide guarantees for both the accuracy of the model they can deliver and the learning efficiency. At the same time, state abstraction techniques allow for a reduction of the size of an MDP while maintaining a bounded loss with respect to the original problem. Therefore, it may come as a surprise that no such guarantees are available when combining both techniques, i.e., where MBRL merely observes abstract states. Our theoretical analysis shows that abstraction can introduce a dependence between samples collected online (e.g., in the real world). That means that, without taking this dependence into account, results for MBRL do not directly extend to this setting. Our result shows that we can use concentration inequalities for martingales to overcome this problem. This result makes it possible to extend the guarantees of existing MBRL algorithms to the setting with abstraction. We illustrate ",
    "link": "http://arxiv.org/abs/2208.14407",
    "context": "Title: An Analysis of Abstracted Model-Based Reinforcement Learning. (arXiv:2208.14407v2 [cs.LG] UPDATED)\nAbstract: Many methods for Model-based Reinforcement learning (MBRL) in Markov decision processes (MDPs) provide guarantees for both the accuracy of the model they can deliver and the learning efficiency. At the same time, state abstraction techniques allow for a reduction of the size of an MDP while maintaining a bounded loss with respect to the original problem. Therefore, it may come as a surprise that no such guarantees are available when combining both techniques, i.e., where MBRL merely observes abstract states. Our theoretical analysis shows that abstraction can introduce a dependence between samples collected online (e.g., in the real world). That means that, without taking this dependence into account, results for MBRL do not directly extend to this setting. Our result shows that we can use concentration inequalities for martingales to overcome this problem. This result makes it possible to extend the guarantees of existing MBRL algorithms to the setting with abstraction. We illustrate ",
    "path": "papers/22/08/2208.14407.json",
    "total_tokens": 856,
    "translated_title": "抽象模型驱动的强化学习的分析",
    "translated_abstract": "在马尔可夫决策过程中，许多模型驱动的强化学习（MBRL）方法都能够提供对模型准确性和学习效率的保证。同时，状态抽象技术可以在保持与原问题有界损失的同时减少MDP的大小。然而，当将这两种技术结合起来时，即MBRL仅仅观察抽象状态时，却没有相应的保证可用。我们的理论分析表明，抽象可以引入在线采集样本之间的相关性（例如在真实世界中采集的样本）。这意味着，如果不考虑这种相关性，MBRL的结果不能直接推广到这个设置中。我们的结果表明，我们可以使用鞅不等式来克服这个问题。这个结果使得将现有MBRL算法的保证扩展到带有抽象的设置成为可能。",
    "tldr": "本论文分析了抽象模型驱动的强化学习中的问题，揭示抽象状态会引入样本相关性，而使用鞅不等式可以解决这个问题，从而将现有MBRL算法的保证扩展到带有抽象的设置。",
    "en_tdlr": "This paper analyzes the issues in abstracted model-based reinforcement learning, revealing that abstract states introduce sample dependence, and proposes the use of martingale concentration inequalities to extend the guarantees of existing MBRL algorithms to the setting with abstraction."
}