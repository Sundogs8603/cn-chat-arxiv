{
    "title": "UniCausal: Unified Benchmark and Repository for Causal Text Mining. (arXiv:2208.09163v2 [cs.CL] UPDATED)",
    "abstract": "Current causal text mining datasets vary in objectives, data coverage, and annotation schemes. These inconsistent efforts prevent modeling capabilities and fair comparisons of model performance. Furthermore, few datasets include cause-effect span annotations, which are needed for end-to-end causal relation extraction. To address these issues, we propose UniCausal, a unified benchmark for causal text mining across three tasks: (I) Causal Sequence Classification, (II) Cause-Effect Span Detection and (III) Causal Pair Classification. We consolidated and aligned annotations of six high quality, mainly human-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165 examples for each task respectively. Since the definition of causality can be subjective, our framework was designed to allow researchers to work on some or all datasets and tasks. To create an initial benchmark, we fine-tuned BERT pre-trained language models to each task, achieving 70.10% Binary F1, 52.42% Macro F1, ",
    "link": "http://arxiv.org/abs/2208.09163",
    "context": "Title: UniCausal: Unified Benchmark and Repository for Causal Text Mining. (arXiv:2208.09163v2 [cs.CL] UPDATED)\nAbstract: Current causal text mining datasets vary in objectives, data coverage, and annotation schemes. These inconsistent efforts prevent modeling capabilities and fair comparisons of model performance. Furthermore, few datasets include cause-effect span annotations, which are needed for end-to-end causal relation extraction. To address these issues, we propose UniCausal, a unified benchmark for causal text mining across three tasks: (I) Causal Sequence Classification, (II) Cause-Effect Span Detection and (III) Causal Pair Classification. We consolidated and aligned annotations of six high quality, mainly human-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165 examples for each task respectively. Since the definition of causality can be subjective, our framework was designed to allow researchers to work on some or all datasets and tasks. To create an initial benchmark, we fine-tuned BERT pre-trained language models to each task, achieving 70.10% Binary F1, 52.42% Macro F1, ",
    "path": "papers/22/08/2208.09163.json",
    "total_tokens": 1087,
    "translated_title": "UniCausal：因果关系文本挖掘的统一基准与仓库",
    "translated_abstract": "当前的因果关系文本挖掘数据集在目标、数据覆盖和注释方案等方面存在差异。这些不一致的努力妨碍了建模能力和模型性能的公平比较。此外，很少有数据集包括因果关系跨度注释，这是进行端到端因果关系提取所必需的。为了解决这些问题，我们提出了UniCausal，这是一个跨三个任务的统一因果关系文本挖掘基准：（I）因果序列分类，（II）因果跨度检测和（III）因果对分类。我们整合和对齐了六个高质量的，主要是人工注释的语料库的注释，分别为每个任务提供了总数为58,720、12,144和69,165个示例。由于因果关系的定义可能是主观的，我们的框架旨在允许研究人员在某些或所有数据集和任务上工作。为了创建一个初始基准，我们对BERT预训练语言模型进行了微调，分别针对每个任务实现了70.10％的二进制F1、52.42％的宏F1和67.18％的宏F1。UniCausal可以作为评估现有模型能力的基准，并鼓励开发因果关系文本挖掘的新方法和框架。",
    "tldr": "UniCausal是一个跨三个任务的因果关系文本挖掘统一基准，整合了六个高质量的语料库的注释。UniCausal可用于评估现有模型能力，并鼓励开发新的因果关系文本挖掘方法和框架。",
    "en_tdlr": "UniCausal is a unified benchmark for causal text mining across three tasks, which consolidates annotations of six high quality corpora to evaluate existing models and encourage the development of new methods and frameworks for causal text mining."
}