{
    "title": "StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation",
    "abstract": "arXiv:2208.10922v2 Announce Type: replace-cross  Abstract: We propose StyleTalker, a novel audio-driven talking head generation model that can synthesize a video of a talking person from a single reference image with accurately audio-synced lip shapes, realistic head poses, and eye blinks. Specifically, by leveraging a pretrained image generator and an image encoder, we estimate the latent codes of the talking head video that faithfully reflects the given audio. This is made possible with several newly devised components: 1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A conditional sequential variational autoencoder that learns the latent motion space disentangled from the lip movements, such that we can independently manipulate the motions and lip movements while preserving the identity. 3) An auto-regressive prior augmented with normalizing flow to learn a complex audio-to-motion multi-modal latent space. Equipped with these components, StyleTalker can g",
    "link": "https://arxiv.org/abs/2208.10922",
    "context": "Title: StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation\nAbstract: arXiv:2208.10922v2 Announce Type: replace-cross  Abstract: We propose StyleTalker, a novel audio-driven talking head generation model that can synthesize a video of a talking person from a single reference image with accurately audio-synced lip shapes, realistic head poses, and eye blinks. Specifically, by leveraging a pretrained image generator and an image encoder, we estimate the latent codes of the talking head video that faithfully reflects the given audio. This is made possible with several newly devised components: 1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A conditional sequential variational autoencoder that learns the latent motion space disentangled from the lip movements, such that we can independently manipulate the motions and lip movements while preserving the identity. 3) An auto-regressive prior augmented with normalizing flow to learn a complex audio-to-motion multi-modal latent space. Equipped with these components, StyleTalker can g",
    "path": "papers/22/08/2208.10922.json",
    "total_tokens": 857,
    "translated_title": "StyleTalker: 一次样式驱动的音频驱动的说话头视频生成",
    "translated_abstract": "我们提出了StyleTalker，一种新颖的音频驱动的说话头生成模型，可以从单个参考图像合成一个说话人的视频，其中包含准确音频同步的唇形、逼真的头部姿态和眨眼动作。具体地，通过利用预训练的图像生成器和图像编码器，我们估算了言语头部视频的潜在代码，忠实地反映了给定音频。这得益于几个新设计的组件：1）用于准确唇部同步的对比度唇同步鉴别器，2）学习与唇部运动分离的潜在运动空间的有条件序列变分自动编码器，这样我们可以独立地操纵运动和嘴唇运动，同时保持身份。3）配备了正规化流的自回归先验，学习了复杂的音频到运动多模潜在空间。借助这些组件，StyleTalker可以生成...",
    "tldr": "提出了StyleTalker，一种能够从单个参考图像合成具有准确音频同步的说话人视频的模型，并且具有几个新设计的组件来实现这一目标。"
}