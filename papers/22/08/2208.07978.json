{
    "title": "Enhancing Heterogeneous Federated Learning with Knowledge Extraction and Multi-Model Fusion. (arXiv:2208.07978v2 [cs.DC] UPDATED)",
    "abstract": "Concerned with user data privacy, this paper presents a new federated learning (FL) method that trains machine learning models on edge devices without accessing sensitive data. Traditional FL methods, although privacy-protective, fail to manage model heterogeneity and incur high communication costs due to their reliance on aggregation methods. To address this limitation, we propose a resource-aware FL method that aggregates local knowledge from edge models and distills it into robust global knowledge through knowledge distillation. This method allows efficient multi-model knowledge fusion and the deployment of resource-aware models while preserving model heterogeneity. Our method improves communication cost and performance in heterogeneous data and models compared to existing FL algorithms. Notably, it reduces the communication cost of ResNet-32 by up to 50\\% and VGG-11 by up to 10$\\times$ while delivering superior performance.",
    "link": "http://arxiv.org/abs/2208.07978",
    "context": "Title: Enhancing Heterogeneous Federated Learning with Knowledge Extraction and Multi-Model Fusion. (arXiv:2208.07978v2 [cs.DC] UPDATED)\nAbstract: Concerned with user data privacy, this paper presents a new federated learning (FL) method that trains machine learning models on edge devices without accessing sensitive data. Traditional FL methods, although privacy-protective, fail to manage model heterogeneity and incur high communication costs due to their reliance on aggregation methods. To address this limitation, we propose a resource-aware FL method that aggregates local knowledge from edge models and distills it into robust global knowledge through knowledge distillation. This method allows efficient multi-model knowledge fusion and the deployment of resource-aware models while preserving model heterogeneity. Our method improves communication cost and performance in heterogeneous data and models compared to existing FL algorithms. Notably, it reduces the communication cost of ResNet-32 by up to 50\\% and VGG-11 by up to 10$\\times$ while delivering superior performance.",
    "path": "papers/22/08/2208.07978.json",
    "total_tokens": 935,
    "translated_title": "提升异构联邦学习的知识提取和多模型融合",
    "translated_abstract": "本文提出了一种新的联邦学习方法，旨在解决用户数据隐私问题，该方法可以在边缘设备上训练机器学习模型而不访问敏感数据。传统的联邦学习方法虽然具有隐私保护性，但由于依赖于聚合方法，无法管理模型异质性，并造成高通信成本。为了解决这个问题，我们提出了一种资源感知的联邦学习方法，该方法通过知识蒸馏，将来自边缘模型的本地知识整合为鲁棒的全局知识。这种方法可以实现高效的多模型知识融合，并在保持模型异质性的同时部署资源感知的模型。与现有的联邦学习算法相比，我们的方法在异构数据和模型中改善了通信成本和性能。值得注意的是，我们的方法将ResNet-32的通信成本降低了最多50\\％，将VGG-11的通信成本降低了最多10倍，同时提供了更优越的性能。",
    "tldr": "本文提出了一种资源感知的联邦学习方法，通过知识蒸馏将边缘模型的本地知识整合为鲁棒的全局知识，实现高效的多模型知识融合，并在保持模型异质性的同时降低通信成本和提高性能。",
    "en_tdlr": "This paper proposes a resource-aware federated learning method that utilizes knowledge distillation to aggregate local knowledge from edge models into robust global knowledge, enabling efficient multi-model knowledge fusion while reducing communication costs and improving performance while maintaining model heterogeneity."
}