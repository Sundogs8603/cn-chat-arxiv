{
    "title": "gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window. (arXiv:2208.11718v2 [cs.CV] UPDATED)",
    "abstract": "Following the success in language domain, the self-attention mechanism (transformer) is adopted in the vision domain and achieving great success recently. Additionally, as another stream, multi-layer perceptron (MLP) is also explored in the vision domain. These architectures, other than traditional CNNs, have been attracting attention recently, and many methods have been proposed. As one that combines parameter efficiency and performance with locality and hierarchy in image recognition, we propose gSwin, which merges the two streams; Swin Transformer and (multi-head) gMLP. We showed that our gSwin can achieve better accuracy on three vision tasks, image classification, object detection and semantic segmentation, than Swin Transformer, with smaller model size.",
    "link": "http://arxiv.org/abs/2208.11718",
    "context": "Title: gSwin: Gated MLP Vision Model with Hierarchical Structure of Shifted Window. (arXiv:2208.11718v2 [cs.CV] UPDATED)\nAbstract: Following the success in language domain, the self-attention mechanism (transformer) is adopted in the vision domain and achieving great success recently. Additionally, as another stream, multi-layer perceptron (MLP) is also explored in the vision domain. These architectures, other than traditional CNNs, have been attracting attention recently, and many methods have been proposed. As one that combines parameter efficiency and performance with locality and hierarchy in image recognition, we propose gSwin, which merges the two streams; Swin Transformer and (multi-head) gMLP. We showed that our gSwin can achieve better accuracy on three vision tasks, image classification, object detection and semantic segmentation, than Swin Transformer, with smaller model size.",
    "path": "papers/22/08/2208.11718.json",
    "total_tokens": 846,
    "translated_title": "gSwin: 具有移动窗口的分层结构的门控多层感知器视觉模型",
    "translated_abstract": "在语言领域取得成功后，自注意机制（transformer）得到了在视觉领域的应用，并取得了巨大的成功。此外，作为另一种流派，多层感知器（MLP）也在视觉领域进行了探索。除了传统的卷积神经网络，这些架构近来一直引起关注，并提出了许多方法。作为一种能够在图像识别中兼顾参数效率、性能、局部性和层级的方法，我们提出了gSwin，将两个流派合并起来，即Swin Transformer和（多头）gMLP。我们展示了我们的gSwin在图像分类、目标检测和语义分割等三个视觉任务上能够达到比Swin Transformer更高的准确度，并且模型尺寸更小。",
    "tldr": "提出了一种名为gSwin的视觉模型，结合了Swin Transformer和（多头）gMLP两个流派，能够同时兼顾参数效率、性能、局部性和层级，在图像分类、目标检测和语义分割等任务中取得更高的准确度，并且模型尺寸更小。"
}