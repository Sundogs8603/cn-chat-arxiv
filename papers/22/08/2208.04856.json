{
    "title": "Fully probabilistic deep models for forward and inverse problems in parametric PDEs. (arXiv:2208.04856v2 [stat.ML] UPDATED)",
    "abstract": "We introduce a physics-driven deep latent variable model (PDDLVM) to learn simultaneously parameter-to-solution (forward) and solution-to-parameter (inverse) maps of parametric partial differential equations (PDEs). Our formulation leverages conventional PDE discretization techniques, deep neural networks, probabilistic modelling, and variational inference to assemble a fully probabilistic coherent framework. In the posited probabilistic model, both the forward and inverse maps are approximated as Gaussian distributions with a mean and covariance parameterized by deep neural networks. The PDE residual is assumed to be an observed random vector of value zero, hence we model it as a random vector with a zero mean and a user-prescribed covariance. The model is trained by maximizing the probability, that is the evidence or marginal likelihood, of observing a residual of zero by maximizing the evidence lower bound (ELBO). Consequently, the proposed methodology does not require any independe",
    "link": "http://arxiv.org/abs/2208.04856",
    "context": "Title: Fully probabilistic deep models for forward and inverse problems in parametric PDEs. (arXiv:2208.04856v2 [stat.ML] UPDATED)\nAbstract: We introduce a physics-driven deep latent variable model (PDDLVM) to learn simultaneously parameter-to-solution (forward) and solution-to-parameter (inverse) maps of parametric partial differential equations (PDEs). Our formulation leverages conventional PDE discretization techniques, deep neural networks, probabilistic modelling, and variational inference to assemble a fully probabilistic coherent framework. In the posited probabilistic model, both the forward and inverse maps are approximated as Gaussian distributions with a mean and covariance parameterized by deep neural networks. The PDE residual is assumed to be an observed random vector of value zero, hence we model it as a random vector with a zero mean and a user-prescribed covariance. The model is trained by maximizing the probability, that is the evidence or marginal likelihood, of observing a residual of zero by maximizing the evidence lower bound (ELBO). Consequently, the proposed methodology does not require any independe",
    "path": "papers/22/08/2208.04856.json",
    "total_tokens": 936,
    "translated_title": "参数化偏微分方程中前向和反向问题的全概率深度模型",
    "translated_abstract": "我们引入了一种物理驱动的深度潜变量模型（PDDLVM），用于同时学习参数化偏微分方程（PDEs）的参数到解（前向）和解到参数（反向）映射。我们的方法结合了传统的PDE离散化技术、深度神经网络、概率建模和变分推断，形成了一个完全概率一致的框架。在所假设的概率模型中，前向和反向映射均被近似为由深度神经网络参数化的高斯分布。我们假设PDE残差是一个观测到的随机向量，值为零，因此我们将其建模为一个均值为零、用户指定协方差的随机向量。该模型通过最大化观察到零残差的概率（即证据或边际似然）来进行训练，最大化证据下界（ELBO）。因此，该方法不需要任何独立的训练数据。",
    "tldr": "本论文提出了一种基于深度学习和概率建模的全概率深度模型，用于学习参数化偏微分方程中的前向和反向映射。模型通过最大化观察到的零残差的概率来进行训练，不需要独立的训练数据。",
    "en_tdlr": "This paper proposes a fully probabilistic deep model based on deep learning and probabilistic modeling to learn forward and inverse mappings in parametric partial differential equations. The model is trained by maximizing the probability of observing zero residuals and does not require independent training data."
}