{
    "title": "Convergence Rates of Training Deep Neural Networks via Alternating Minimization Methods. (arXiv:2208.14318v2 [cs.LG] UPDATED)",
    "abstract": "Training deep neural networks (DNNs) is an important and challenging optimization problem in machine learning due to its non-convexity and non-separable structure. The alternating minimization (AM) approaches split the composition structure of DNNs and have drawn great interest in the deep learning and optimization communities. In this paper, we propose a unified framework for analyzing the convergence rate of AM-type network training methods. Our analysis is based on the non-monotone $j$-step sufficient decrease conditions and the Kurdyka-Lojasiewicz (KL) property, which relaxes the requirement of designing descent algorithms. We show the detailed local convergence rate if the KL exponent $\\theta$ varies in $[0,1)$. Moreover, the local R-linear convergence is discussed under a stronger $j$-step sufficient decrease condition.",
    "link": "http://arxiv.org/abs/2208.14318",
    "context": "Title: Convergence Rates of Training Deep Neural Networks via Alternating Minimization Methods. (arXiv:2208.14318v2 [cs.LG] UPDATED)\nAbstract: Training deep neural networks (DNNs) is an important and challenging optimization problem in machine learning due to its non-convexity and non-separable structure. The alternating minimization (AM) approaches split the composition structure of DNNs and have drawn great interest in the deep learning and optimization communities. In this paper, we propose a unified framework for analyzing the convergence rate of AM-type network training methods. Our analysis is based on the non-monotone $j$-step sufficient decrease conditions and the Kurdyka-Lojasiewicz (KL) property, which relaxes the requirement of designing descent algorithms. We show the detailed local convergence rate if the KL exponent $\\theta$ varies in $[0,1)$. Moreover, the local R-linear convergence is discussed under a stronger $j$-step sufficient decrease condition.",
    "path": "papers/22/08/2208.14318.json",
    "total_tokens": 915,
    "translated_title": "通过交替最小化方法训练深度神经网络的收敛速率",
    "translated_abstract": "由于其非凸性和非可分性，训练深度神经网络（DNN）是机器学习中的一个重要且具有挑战性的优化问题。交替最小化（AM）方法在DNN的复合结构方面进行了拆分，并在深度学习和优化社区引起了极大的关注。在本文中，我们提出了一个统一的框架来分析AM类型的网络训练方法的收敛速率。我们的分析基于非单调的$j$-步充分减少条件和Kurdyka-Lojasiewicz（KL）性质，这放宽了设计下降算法的要求。我们在KL指数$ \\theta $在$ [0,1) $变化时展示了详细的局部收敛速率。此外，在更强的$j$-步充分减少条件下讨论了局部R-线性收敛。",
    "tldr": "本文提出了一个统一的框架用于分析AM类型的网络训练方法的收敛速率。研究基于非单调的$j$-步充分减少条件和Kurdyka-Lojasiewicz（KL）性质，并在KL指数$ \\theta $在$ [0,1) $变化时展示了详细的局部收敛速率和局部R-线性收敛。",
    "en_tdlr": "This paper proposes a unified framework for analyzing the convergence rate of AM-type network training methods. The research is based on non-monotone $j$-step sufficient decrease conditions and the Kurdyka-Lojasiewicz (KL) property, and shows the detailed local convergence rate and local R-linear convergence as KL exponent $\\theta$ varies in $[0,1)$."
}