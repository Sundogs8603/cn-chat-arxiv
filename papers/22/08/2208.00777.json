{
    "title": "D3Former: Debiased Dual Distilled Transformer for Incremental Learning. (arXiv:2208.00777v3 [cs.CV] UPDATED)",
    "abstract": "In class incremental learning (CIL) setting, groups of classes are introduced to a model in each learning phase. The goal is to learn a unified model performant on all the classes observed so far. Given the recent popularity of Vision Transformers (ViTs) in conventional classification settings, an interesting question is to study their continual learning behaviour. In this work, we develop a Debiased Dual Distilled Transformer for CIL dubbed $\\textrm{D}^3\\textrm{Former}$. The proposed model leverages a hybrid nested ViT design to ensure data efficiency and scalability to small as well as large datasets. In contrast to a recent ViT based CIL approach, our $\\textrm{D}^3\\textrm{Former}$ does not dynamically expand its architecture when new tasks are learned and remains suitable for a large number of incremental tasks. The improved CIL behaviour of $\\textrm{D}^3\\textrm{Former}$ owes to two fundamental changes to the ViT design. First, we treat the incremental learning as a long-tail classi",
    "link": "http://arxiv.org/abs/2208.00777",
    "context": "Title: D3Former: Debiased Dual Distilled Transformer for Incremental Learning. (arXiv:2208.00777v3 [cs.CV] UPDATED)\nAbstract: In class incremental learning (CIL) setting, groups of classes are introduced to a model in each learning phase. The goal is to learn a unified model performant on all the classes observed so far. Given the recent popularity of Vision Transformers (ViTs) in conventional classification settings, an interesting question is to study their continual learning behaviour. In this work, we develop a Debiased Dual Distilled Transformer for CIL dubbed $\\textrm{D}^3\\textrm{Former}$. The proposed model leverages a hybrid nested ViT design to ensure data efficiency and scalability to small as well as large datasets. In contrast to a recent ViT based CIL approach, our $\\textrm{D}^3\\textrm{Former}$ does not dynamically expand its architecture when new tasks are learned and remains suitable for a large number of incremental tasks. The improved CIL behaviour of $\\textrm{D}^3\\textrm{Former}$ owes to two fundamental changes to the ViT design. First, we treat the incremental learning as a long-tail classi",
    "path": "papers/22/08/2208.00777.json",
    "total_tokens": 1043,
    "translated_title": "D3Former: 无偏差双重蒸馏变形器用于增量学习",
    "translated_abstract": "在类别增量学习（CIL）设置中，每次学习阶段向模型引入一组类别，目标是学习一个统一的模型，在所有观测到的类别上表现良好。考虑到Vision Transformers（ViTs）在传统分类设置中的普及，在本文中，我们开发了一种用于CIL的无偏差双重蒸馏Transformer，称为D3Former。所提出的模型利用混合嵌套ViT设计，以确保对小型和大型数据集的数据效率和可伸缩性。与最近的基于ViT的CIL方法相比，我们的D3Former在学习新的任务时不会动态扩展其架构，并且适用于大量增量任务。D3Former行为的改进归功于ViT设计的两个基本变化。首先，我们将增量学习视为长尾分类问题，引入了偏差校正模块，缓解了旧类别和新类别之间的不平衡。其次，我们提出了一种新的蒸馏目标，利用师生ViT之间的知识传输，确保更好地推广到新任务。",
    "tldr": "本文提出了D3Former，一种用于类别增量学习的无偏差双重蒸馏Transformer。D3Former利用混合嵌套ViT设计，不会动态扩展其架构，并通过偏差校正模块和新的蒸馏目标来改进其CIL行为。",
    "en_tdlr": "This paper proposes D3Former, an unbiased dual distilled Transformer for class incremental learning that leverages a hybrid nested ViT design and improves the CIL behavior through a bias correction module and a new distillation objective."
}