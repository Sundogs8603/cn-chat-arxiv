{
    "title": "Cell-Free Latent Go-Explore. (arXiv:2208.14928v3 [cs.LG] UPDATED)",
    "abstract": "In this paper, we introduce Latent Go-Explore (LGE), a simple and general approach based on the Go-Explore paradigm for exploration in reinforcement learning (RL). Go-Explore was initially introduced with a strong domain knowledge constraint for partitioning the state space into cells. However, in most real-world scenarios, drawing domain knowledge from raw observations is complex and tedious. If the cell partitioning is not informative enough, Go-Explore can completely fail to explore the environment. We argue that the Go-Explore approach can be generalized to any environment without domain knowledge and without cells by exploiting a learned latent representation. Thus, we show that LGE can be flexibly combined with any strategy for learning a latent representation. Our results indicate that LGE, although simpler than Go-Explore, is more robust and outperforms state-of-the-art algorithms in terms of pure exploration on multiple hard-exploration environments including Montezuma's Reven",
    "link": "http://arxiv.org/abs/2208.14928",
    "context": "Title: Cell-Free Latent Go-Explore. (arXiv:2208.14928v3 [cs.LG] UPDATED)\nAbstract: In this paper, we introduce Latent Go-Explore (LGE), a simple and general approach based on the Go-Explore paradigm for exploration in reinforcement learning (RL). Go-Explore was initially introduced with a strong domain knowledge constraint for partitioning the state space into cells. However, in most real-world scenarios, drawing domain knowledge from raw observations is complex and tedious. If the cell partitioning is not informative enough, Go-Explore can completely fail to explore the environment. We argue that the Go-Explore approach can be generalized to any environment without domain knowledge and without cells by exploiting a learned latent representation. Thus, we show that LGE can be flexibly combined with any strategy for learning a latent representation. Our results indicate that LGE, although simpler than Go-Explore, is more robust and outperforms state-of-the-art algorithms in terms of pure exploration on multiple hard-exploration environments including Montezuma's Reven",
    "path": "papers/22/08/2208.14928.json",
    "total_tokens": 777,
    "translated_title": "无细胞Latent Go-Explore",
    "translated_abstract": "本文提出了一种名为Latent Go-Explore (LGE)的简单通用方法，基于Go-Explore范式探索强化学习。我们认为，Go-Explore方法可以通过利用学习到的潜在表示在没有领域知识和单元的情况下泛化到任何环境中。我们展示了LGE可以灵活地与任何学习潜在表示的策略相结合。实验结果表明，LGE比Go-Explore更加鲁棒，在多个难以探索的环境中（包括Montezuma的复仇）表现出优异的探索性能，超越了现有算法。",
    "tldr": "本文提出了无细胞Latent Go-Explore方法用于强化学习探索，通过学习潜在表示泛化到任何环境，实验结果表明其表现优异。",
    "en_tdlr": "The paper introduces a simple and general approach called Latent Go-Explore for exploration in reinforcement learning, which can be generalized to any environment without domain knowledge and without cells by exploiting a learned latent representation. The results indicate that it outperforms state-of-the-art algorithms in terms of pure exploration on multiple hard-exploration environments."
}