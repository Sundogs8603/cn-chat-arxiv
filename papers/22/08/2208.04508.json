{
    "title": "Training Overparametrized Neural Networks in Sublinear Time",
    "abstract": "The success of deep learning comes at a tremendous computational and energy cost, and the scalability of training massively overparametrized neural networks is becoming a real barrier to the progress of artificial intelligence (AI). Despite the popularity and low cost-per-iteration of traditional backpropagation via gradient decent, stochastic gradient descent (SGD) has prohibitive convergence rate in non-convex settings, both in theory and practice.   To mitigate this cost, recent works have proposed to employ alternative (Newton-type) training methods with much faster convergence rate, albeit with higher cost-per-iteration. For a typical neural network with $m=\\mathrm{poly}(n)$ parameters and input batch of $n$ datapoints in $\\mathbb{R}^d$, the previous work of [Brand, Peng, Song, and Weinstein, ITCS'2021] requires $\\sim mnd + n^3$ time per iteration. In this paper, we present a novel training method that requires only $m^{1-\\alpha} n d + n^3$ amortized time in the same overparametri",
    "link": "https://arxiv.org/abs/2208.04508",
    "context": "Title: Training Overparametrized Neural Networks in Sublinear Time\nAbstract: The success of deep learning comes at a tremendous computational and energy cost, and the scalability of training massively overparametrized neural networks is becoming a real barrier to the progress of artificial intelligence (AI). Despite the popularity and low cost-per-iteration of traditional backpropagation via gradient decent, stochastic gradient descent (SGD) has prohibitive convergence rate in non-convex settings, both in theory and practice.   To mitigate this cost, recent works have proposed to employ alternative (Newton-type) training methods with much faster convergence rate, albeit with higher cost-per-iteration. For a typical neural network with $m=\\mathrm{poly}(n)$ parameters and input batch of $n$ datapoints in $\\mathbb{R}^d$, the previous work of [Brand, Peng, Song, and Weinstein, ITCS'2021] requires $\\sim mnd + n^3$ time per iteration. In this paper, we present a novel training method that requires only $m^{1-\\alpha} n d + n^3$ amortized time in the same overparametri",
    "path": "papers/22/08/2208.04508.json",
    "total_tokens": 892,
    "translated_title": "在次线性时间内训练过参数化的神经网络",
    "translated_abstract": "深度学习的成功付出了巨大的计算和能源成本，训练过参数化的神经网络的可扩展性正在成为人工智能进展的真正障碍。尽管传统的反向传播通过梯度下降的成本每次迭代很低，但随机梯度下降在非凸设置中具有禁止的收敛速度，无论是在理论上还是实践中。为了缓解这种成本，最近的研究提出了采用具有更快收敛速度但每次迭代成本更高的替代（牛顿类型）训练方法。对于具有$m=\\mathrm{poly}(n)$个参数和输入批次$n$个数据点在$\\mathbb{R}^d$中的典型神经网络，[Brand, Peng, Song, and Weinstein, ITCS'2021]的先前工作每次迭代需要$\\sim mnd+n^3$的时间。在本文中，我们提出了一种新的训练方法，只需要$m^{1-\\alpha}nd+n^3$的摊销时间，与先前的方法相比具有更高的效率。",
    "tldr": "这项研究提出了一种新的训练方法，可以在次线性时间内训练过参数化的神经网络，提高了训练的效率。",
    "en_tdlr": "This research presents a novel training method that enables training overparametrized neural networks in sublinear time, improving the efficiency of training."
}