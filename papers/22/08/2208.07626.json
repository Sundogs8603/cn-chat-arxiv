{
    "title": "Algorithmic Assistance with Recommendation-Dependent Preferences. (arXiv:2208.07626v3 [cs.LG] UPDATED)",
    "abstract": "When an algorithm provides risk assessments, we typically think of them as helpful inputs to human decisions, such as when risk scores are presented to judges or doctors. However, a decision-maker may not only react to the information provided by the algorithm. The decision-maker may also view the algorithmic recommendation as a default action, making it costly for them to deviate, such as when a judge is reluctant to overrule a high-risk assessment for a defendant or a doctor fears the consequences of deviating from recommended procedures. To address such unintended consequences of algorithmic assistance, we propose a principal-agent model of joint human-machine decision-making. Within this model, we consider the effect and design of algorithmic recommendations when they affect choices not just by shifting beliefs, but also by altering preferences. We motivate this assumption from institutional factors, such as a desire to avoid audits, as well as from well-established models in behav",
    "link": "http://arxiv.org/abs/2208.07626",
    "context": "Title: Algorithmic Assistance with Recommendation-Dependent Preferences. (arXiv:2208.07626v3 [cs.LG] UPDATED)\nAbstract: When an algorithm provides risk assessments, we typically think of them as helpful inputs to human decisions, such as when risk scores are presented to judges or doctors. However, a decision-maker may not only react to the information provided by the algorithm. The decision-maker may also view the algorithmic recommendation as a default action, making it costly for them to deviate, such as when a judge is reluctant to overrule a high-risk assessment for a defendant or a doctor fears the consequences of deviating from recommended procedures. To address such unintended consequences of algorithmic assistance, we propose a principal-agent model of joint human-machine decision-making. Within this model, we consider the effect and design of algorithmic recommendations when they affect choices not just by shifting beliefs, but also by altering preferences. We motivate this assumption from institutional factors, such as a desire to avoid audits, as well as from well-established models in behav",
    "path": "papers/22/08/2208.07626.json",
    "total_tokens": 873,
    "translated_title": "算法辅助下的推荐相关偏好",
    "translated_abstract": "当算法提供风险评估时，我们通常将其视为对人类决策的有益输入，例如将风险评分呈现给法官或医生。然而，决策者可能不仅仅只针对算法提供的信息做出反应。决策者还可能将算法推荐视为默认操作，使其难以偏离，例如法官在对被告进行高风险评估的时候不愿意推翻，或医生担心偏离推荐的程序会带来后果。为了解决算法辅助的这种意外后果，我们提出了一个联合人机决策的委托代理模型。在该模型中，我们考虑了算法推荐对选择的影响和设计，这种影响不仅仅是通过改变信念，还通过改变偏好。我们从制度因素和行为经济学中的已有模型等方面进行了这个假设的动机论证。",
    "tldr": "本研究提出了一个联合人机决策的委托代理模型，探讨了算法推荐对选择的影响和设计，特别关注算法对偏好的改变，以解决算法辅助可能带来的意外后果。",
    "en_tdlr": "This study proposes a principal-agent model of joint human-machine decision-making to address the unintended consequences of algorithmic assistance, focusing on the effect and design of algorithmic recommendations on preferences and choices."
}