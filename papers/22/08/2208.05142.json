{
    "title": "Plug-and-Play Model-Agnostic Counterfactual Policy Synthesis for Deep Reinforcement Learning based Recommendation. (arXiv:2208.05142v3 [cs.IR] UPDATED)",
    "abstract": "Recent advances in recommender systems have proved the potential of Reinforcement Learning (RL) to handle the dynamic evolution processes between users and recommender systems. However, learning to train an optimal RL agent is generally impractical with commonly sparse user feedback data in the context of recommender systems. To circumvent the lack of interaction of current RL-based recommender systems, we propose to learn a general Model-Agnostic Counterfactual Synthesis (MACS) Policy for counterfactual user interaction data augmentation. The counterfactual synthesis policy aims to synthesise counterfactual states while preserving significant information in the original state relevant to the user's interests, building upon two different training approaches we designed: learning with expert demonstrations and joint training. As a result, the synthesis of each counterfactual data is based on the current recommendation agent's interaction with the environment to adapt to users' dynamic i",
    "link": "http://arxiv.org/abs/2208.05142",
    "context": "Title: Plug-and-Play Model-Agnostic Counterfactual Policy Synthesis for Deep Reinforcement Learning based Recommendation. (arXiv:2208.05142v3 [cs.IR] UPDATED)\nAbstract: Recent advances in recommender systems have proved the potential of Reinforcement Learning (RL) to handle the dynamic evolution processes between users and recommender systems. However, learning to train an optimal RL agent is generally impractical with commonly sparse user feedback data in the context of recommender systems. To circumvent the lack of interaction of current RL-based recommender systems, we propose to learn a general Model-Agnostic Counterfactual Synthesis (MACS) Policy for counterfactual user interaction data augmentation. The counterfactual synthesis policy aims to synthesise counterfactual states while preserving significant information in the original state relevant to the user's interests, building upon two different training approaches we designed: learning with expert demonstrations and joint training. As a result, the synthesis of each counterfactual data is based on the current recommendation agent's interaction with the environment to adapt to users' dynamic i",
    "path": "papers/22/08/2208.05142.json",
    "total_tokens": 922,
    "translated_title": "Plug-and-Play模型无关的反事实策略综合用于基于深度强化学习的推荐系统",
    "translated_abstract": "最近推荐系统的进展证明了强化学习（RL）处理用户与推荐系统之间动态演化过程的潜力。然而，学习训练最佳RL代理通常在推荐系统的常见稀疏用户反馈数据中是不切实际的。为了避免当前基于RL的推荐系统缺乏交互的问题，我们提出了学习通用的模型无关的反事实综合（MACS）策略，用于合成反事实用户交互数据增强。反事实综合策略旨在合成反事实状态，同时保留原始状态中与用户兴趣相关的重要信息，建立在我们设计的两种不同训练方法之上：专家演示学习和联合训练。因此，每个反事实数据的综合都基于当前推荐代理与环境的交互来适应用户的动态。",
    "tldr": "该论文提出了一种基于深度强化学习的推荐系统解决方案，通过学习模型无关的反事实综合策略来处理用户反馈数据的稀疏性。该策略能够合成具有用户兴趣相关信息的反事实数据，从而提升推荐系统的性能。",
    "en_tdlr": "This paper proposes a solution for reinforcement learning-based recommender systems by learning a model-agnostic counterfactual synthesis policy to handle sparse user feedback data. The policy is able to synthesize counterfactual data with relevant information to user interests, improving the performance of recommender systems."
}