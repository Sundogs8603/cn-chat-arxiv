{
    "title": "Interactive Code Generation via Test-Driven User-Intent Formalization. (arXiv:2208.05950v2 [cs.SE] UPDATED)",
    "abstract": "Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, when interacting with LLMs, users have no guarantees that the code suggestions produced correctly satisfy the intent they provided. In fact, it is hard to define a notion of correctness since natural language can be ambiguous and lacks a formal semantics.  In this paper, we propose the workflow of {\\it interactive test-driven code generation}, which leverages lightweight user feedback to (a) formalize the user intent using generated tests that can be useful for debugging, and (b) produce an improved set of code suggestions by pruning and ranking candidate code suggestions. We describe a language-agnostic abstract algorithm and a concrete implementation TiCoder. We perform an automated evaluation of TiCoder on the \\emph{MBPP} and \\emph{HumanEval} code generation benchmarks. Our results are promising with using ",
    "link": "http://arxiv.org/abs/2208.05950",
    "context": "Title: Interactive Code Generation via Test-Driven User-Intent Formalization. (arXiv:2208.05950v2 [cs.SE] UPDATED)\nAbstract: Large language models (LLMs) have shown great potential in automating significant aspects of coding by producing natural code from informal natural language (NL) intent. However, when interacting with LLMs, users have no guarantees that the code suggestions produced correctly satisfy the intent they provided. In fact, it is hard to define a notion of correctness since natural language can be ambiguous and lacks a formal semantics.  In this paper, we propose the workflow of {\\it interactive test-driven code generation}, which leverages lightweight user feedback to (a) formalize the user intent using generated tests that can be useful for debugging, and (b) produce an improved set of code suggestions by pruning and ranking candidate code suggestions. We describe a language-agnostic abstract algorithm and a concrete implementation TiCoder. We perform an automated evaluation of TiCoder on the \\emph{MBPP} and \\emph{HumanEval} code generation benchmarks. Our results are promising with using ",
    "path": "papers/22/08/2208.05950.json",
    "total_tokens": 857,
    "translated_title": "通过测试驱动的用户意图规范化进行交互式代码生成",
    "translated_abstract": "大型语言模型（LLMs）展示了通过从非正式的自然语言（NL）意图生成自然代码来自动化编码的巨大潜力。然而，与LLMs进行交互时，用户不能保证生成的代码建议正确地满足其提供的意图。事实上，很难定义正确性的概念，因为自然语言可能存在歧义，并且缺乏形式化语义。在本文中，我们提出了“交互式测试驱动代码生成”的工作流程，它利用轻量级用户反馈来（a）使用生成的测试来形式化用户意图，这对于调试非常有用，以及（b）通过修剪和排名候选代码建议来生成改进的代码建议集。我们描述了一种与语言无关的抽象算法和一个具体的实现TiCoder。我们在\\emph {MBPP}和\\emph {HumanEval}代码生成基准上对TiCoder进行了自动评估。我们的结果令人鼓舞。",
    "tldr": "本文提出了交互式测试驱动代码生成的工作流程，该方法通过生成的测试形式化用户意图，并通过修剪和排名代码建议来提供改进的代码建议集。",
    "en_tdlr": "This paper proposes the workflow of interactive test-driven code generation, which formalizes user intent using generated tests, and provides an improved set of code suggestions by pruning and ranking candidate code suggestions."
}