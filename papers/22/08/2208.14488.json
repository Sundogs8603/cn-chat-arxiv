{
    "title": "Constraining Representations Yields Models That Know What They Don't Know. (arXiv:2208.14488v3 [cs.LG] UPDATED)",
    "abstract": "A well-known failure mode of neural networks is that they may confidently return erroneous predictions. Such unsafe behaviour is particularly frequent when the use case slightly differs from the training context, and/or in the presence of an adversary. This work presents a novel direction to address these issues in a broad, general manner: imposing class-aware constraints on a model's internal activation patterns. Specifically, we assign to each class a unique, fixed, randomly-generated binary vector - hereafter called class code and train the model so that its cross-depths activation patterns predict the appropriate class code according to the input sample's class. The resulting predictors are dubbed Total Activation Classifiers (TAC), and TACs may either be trained from scratch, or used with negligible cost as a thin add-on on top of a frozen, pre-trained neural network. The distance between a TAC's activation pattern and the closest valid code acts as an additional confidence scor",
    "link": "http://arxiv.org/abs/2208.14488",
    "context": "Title: Constraining Representations Yields Models That Know What They Don't Know. (arXiv:2208.14488v3 [cs.LG] UPDATED)\nAbstract: A well-known failure mode of neural networks is that they may confidently return erroneous predictions. Such unsafe behaviour is particularly frequent when the use case slightly differs from the training context, and/or in the presence of an adversary. This work presents a novel direction to address these issues in a broad, general manner: imposing class-aware constraints on a model's internal activation patterns. Specifically, we assign to each class a unique, fixed, randomly-generated binary vector - hereafter called class code and train the model so that its cross-depths activation patterns predict the appropriate class code according to the input sample's class. The resulting predictors are dubbed Total Activation Classifiers (TAC), and TACs may either be trained from scratch, or used with negligible cost as a thin add-on on top of a frozen, pre-trained neural network. The distance between a TAC's activation pattern and the closest valid code acts as an additional confidence scor",
    "path": "papers/22/08/2208.14488.json",
    "total_tokens": 899,
    "translated_title": "限制网络表示，让模型知道自己的不足",
    "translated_abstract": "神经网络的一个已知失败模式是它们可能自信地返回错误的预测。这种不安全的行为在使用案例略有不同的情况下特别频繁，或者在面对敌手时。本文介绍了一种新的方法来以一种广泛且一般的方式解决这些问题：对模型内部的激活模式施加类感知约束。具体而言，我们为每个类分配一个独特的、固定的、随机生成的二进制向量（后文称为类编码），并训练模型，使其通过交叉深度的激活模式根据输入样本的类别预测相应的类编码。结果预测器被称为总激活分类器（TAC），TAC可以从头开始训练，也可以在冻结的预训练神经网络顶部用极小的代价作为薄层附加使用。TAC的激活模式与最接近的有效编码之间的距离作为额外的置信度评分。",
    "tldr": "通过对模型内部激活模式施加类感知约束的方法，本文提出总激活分类器（TAC）可以让模型更加安全、可靠，并具有广泛的应用前景。",
    "en_tdlr": "By imposing class-aware constraints on a model's internal activation patterns, this paper proposes Total Activation Classifiers (TACs) as a novel direction to address the issue of unsafe behavior of neural networks, which can improve the safety and reliability of models and has broad application prospects."
}