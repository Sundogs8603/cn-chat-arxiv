{
    "title": "FP8 Quantization: The Power of the Exponent",
    "abstract": "arXiv:2208.09225v2 Announce Type: replace  Abstract: When quantizing neural networks for efficient inference, low-bit integers are the go-to format for efficiency. However, low-bit floating point numbers have an extra degree of freedom, assigning some bits to work on an exponential scale instead. This paper in-depth investigates this benefit of the floating point format for neural network inference. We detail the choices that can be made for the FP8 format, including the important choice of the number of bits for the mantissa and exponent, and show analytically in which settings these choices give better performance. Then we show how these findings translate to real networks, provide an efficient implementation for FP8 simulation, and a new algorithm that enables the learning of both the scale parameters and the number of exponent bits in the FP8 format. Our chief conclusion is that when doing post-training quantization for a wide range of networks, the FP8 format is better than INT8 i",
    "link": "https://arxiv.org/abs/2208.09225",
    "context": "Title: FP8 Quantization: The Power of the Exponent\nAbstract: arXiv:2208.09225v2 Announce Type: replace  Abstract: When quantizing neural networks for efficient inference, low-bit integers are the go-to format for efficiency. However, low-bit floating point numbers have an extra degree of freedom, assigning some bits to work on an exponential scale instead. This paper in-depth investigates this benefit of the floating point format for neural network inference. We detail the choices that can be made for the FP8 format, including the important choice of the number of bits for the mantissa and exponent, and show analytically in which settings these choices give better performance. Then we show how these findings translate to real networks, provide an efficient implementation for FP8 simulation, and a new algorithm that enables the learning of both the scale parameters and the number of exponent bits in the FP8 format. Our chief conclusion is that when doing post-training quantization for a wide range of networks, the FP8 format is better than INT8 i",
    "path": "papers/22/08/2208.09225.json",
    "total_tokens": 830,
    "translated_title": "FP8量化：指数的力量",
    "translated_abstract": "当将神经网络量化以进行高效推理时，低比特整数是效率的首选格式。然而，低比特浮点数具有额外的自由度，可以将一些比特分配到指数刻度上。本文深入研究了浮点格式对神经网络推理的这种好处。我们详细介绍了FP8格式的选择，包括尾数和指数位数的重要选择，并分析了在哪些设置中这些选择会带来更好的性能。然后我们展示了这些发现如何转化为真实网络，提供了FP8模拟的高效实现，以及一种新算法，可以学习FP8格式中的比例参数和指数位数。我们的主要结论是，在对各种网络进行后训练量化时，FP8格式优于INT8格式。",
    "tldr": "本文深入研究了FP8格式对神经网络推理的好处，提出了在不同设置中选择尾数和指数位数的方法，并证明了FP8格式在实际网络中的表现更好，尤其对于后训练量化来说优于INT8格式。",
    "en_tdlr": "This paper thoroughly investigates the benefits of the FP8 format for neural network inference, presents methods for choosing the mantissa and exponent bits in different settings, and demonstrates superior performance of the FP8 format in real networks, particularly outperforming the INT8 format for post-training quantization."
}