{
    "title": "DPA-1: Pretraining of Attention-based Deep Potential Model for Molecular Simulation. (arXiv:2208.08236v4 [physics.chem-ph] UPDATED)",
    "abstract": "Machine learning assisted modeling of the inter-atomic potential energy surface (PES) is revolutionizing the field of molecular simulation. With the accumulation of high-quality electronic structure data, a model that can be pretrained on all available data and finetuned on downstream tasks with a small additional effort would bring the field to a new stage. Here we propose DPA-1, a Deep Potential model with a novel attention mechanism, which is highly effective for representing the conformation and chemical spaces of atomic systems and learning the PES. We tested DPA-1 on a number of systems and observed superior performance compared with existing benchmarks. When pretrained on large-scale datasets containing 56 elements, DPA-1 can be successfully applied to various downstream tasks with a great improvement of sample efficiency. Surprisingly, for different elements, the learned type embedding parameters form a $spiral$ in the latent space and have a natural correspondence with their p",
    "link": "http://arxiv.org/abs/2208.08236",
    "context": "Title: DPA-1: Pretraining of Attention-based Deep Potential Model for Molecular Simulation. (arXiv:2208.08236v4 [physics.chem-ph] UPDATED)\nAbstract: Machine learning assisted modeling of the inter-atomic potential energy surface (PES) is revolutionizing the field of molecular simulation. With the accumulation of high-quality electronic structure data, a model that can be pretrained on all available data and finetuned on downstream tasks with a small additional effort would bring the field to a new stage. Here we propose DPA-1, a Deep Potential model with a novel attention mechanism, which is highly effective for representing the conformation and chemical spaces of atomic systems and learning the PES. We tested DPA-1 on a number of systems and observed superior performance compared with existing benchmarks. When pretrained on large-scale datasets containing 56 elements, DPA-1 can be successfully applied to various downstream tasks with a great improvement of sample efficiency. Surprisingly, for different elements, the learned type embedding parameters form a $spiral$ in the latent space and have a natural correspondence with their p",
    "path": "papers/22/08/2208.08236.json",
    "total_tokens": 921,
    "translated_title": "DPA-1: 运用注意力机制的深度势能模型在分子模拟中的预训练",
    "translated_abstract": "机器学习辅助建模的原子间势能能量面（PES）正彻底改变分子模拟领域。随着高质量电子结构数据的积累，一个能够预先训练所有可用数据并在下游任务中通过少量额外工作进行微调的模型将使该领域进入一个新阶段。本文提出了DPA-1，一种具有新颖注意力机制的深度势能模型，对原子系统的构象和化学空间具有高效表示能力，并且能够学习到PES。我们在多个系统上测试了DPA-1，并观察到与现有基准相比表现出更好的性能。当在包含56个元素的大规模数据集上进行预训练时，DPA-1可以在各种下游任务中取得极大的样本效率改进。令人惊讶的是，对于不同的元素，学到的类型嵌入参数在潜在空间中形成了一个\"螺旋\"形状，并且与它们的p",
    "tldr": "DPA-1是一种具有新颖注意力机制的深度势能模型，能够高效表示原子系统的构象和化学空间，并且在分子模拟中能够通过预训练和微调取得卓越的性能。",
    "en_tdlr": "DPA-1 is a deep potential model with a novel attention mechanism that effectively represents the conformation and chemical spaces of atomic systems and achieves superior performance in molecular simulation through pretraining and fine-tuning."
}