{
    "title": "Integrating Knowledge Graph embedding and pretrained Language Models in Hypercomplex Spaces. (arXiv:2208.02743v3 [cs.CL] UPDATED)",
    "abstract": "Knowledge Graphs, such as Wikidata, comprise structural and textual knowledge in order to represent knowledge. For each of the two modalities dedicated approaches for graph embedding and language models learn patterns that allow for predicting novel structural knowledge. Few approaches have integrated learning and inference with both modalities and these existing ones could only partially exploit the interaction of structural and textual knowledge. In our approach, we build on existing strong representations of single modalities and we use hypercomplex algebra to represent both, (i), single-modality embedding as well as, (ii), the interaction between different modalities and their complementary means of knowledge representation. More specifically, we suggest Dihedron and Quaternion representations of 4D hypercomplex numbers to integrate four modalities namely structural knowledge graph embedding, word-level representations (e.g.\\ Word2vec, Fasttext), sentence-level representations (Sen",
    "link": "http://arxiv.org/abs/2208.02743",
    "context": "Title: Integrating Knowledge Graph embedding and pretrained Language Models in Hypercomplex Spaces. (arXiv:2208.02743v3 [cs.CL] UPDATED)\nAbstract: Knowledge Graphs, such as Wikidata, comprise structural and textual knowledge in order to represent knowledge. For each of the two modalities dedicated approaches for graph embedding and language models learn patterns that allow for predicting novel structural knowledge. Few approaches have integrated learning and inference with both modalities and these existing ones could only partially exploit the interaction of structural and textual knowledge. In our approach, we build on existing strong representations of single modalities and we use hypercomplex algebra to represent both, (i), single-modality embedding as well as, (ii), the interaction between different modalities and their complementary means of knowledge representation. More specifically, we suggest Dihedron and Quaternion representations of 4D hypercomplex numbers to integrate four modalities namely structural knowledge graph embedding, word-level representations (e.g.\\ Word2vec, Fasttext), sentence-level representations (Sen",
    "path": "papers/22/08/2208.02743.json",
    "total_tokens": 867,
    "translated_title": "在超复数空间中整合知识图谱嵌入和预训练语言模型",
    "translated_abstract": "知识图谱如Wikidata在表示知识时包含了结构性和文本性知识。针对这两种模态，专门的图嵌入和语言模型方法学习了能够预测新的结构性知识的模式。目前只有少数方法将学习和推理与两种模态整合起来，而且现有方法只能部分地利用结构性和文本性知识的相互作用。我们的方法利用现有强大的单模态表示，并使用超复数代数表示单模态嵌入以及不同模态之间以及它们作为知识表示的互补手段的交互。具体而言，我们建议使用四维超复数的二面体和四元数表示来整合四种模态，即结构性知识图谱嵌入、词级表示（例如Word2vec、Fasttext）、句级表示（Sen",
    "tldr": "本文提出在超复数空间中整合知识图谱嵌入和预训练语言模型的方法，通过利用超复数代数来表示单模态嵌入以及不同模态之间的交互，并且能够更好地利用结构性和文本性知识的相互作用。",
    "en_tdlr": "This paper proposes a method of integrating knowledge graph embedding and pretrained language models in hypercomplex spaces, utilizing hypercomplex algebra to represent single-modality embedding and the interaction between different modalities, allowing for better utilization of the interaction between structural and textual knowledge."
}