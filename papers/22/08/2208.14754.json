{
    "title": "LexMAE: Lexicon-Bottlenecked Pretraining for Large-Scale Retrieval. (arXiv:2208.14754v2 [cs.IR] UPDATED)",
    "abstract": "In large-scale retrieval, the lexicon-weighting paradigm, learning weighted sparse representations in vocabulary space, has shown promising results with high quality and low latency. Despite it deeply exploiting the lexicon-representing capability of pre-trained language models, a crucial gap remains between language modeling and lexicon-weighting retrieval -- the former preferring certain or low-entropy words whereas the latter favoring pivot or high-entropy words -- becoming the main barrier to lexicon-weighting performance for large-scale retrieval. To bridge this gap, we propose a brand-new pre-training framework, lexicon-bottlenecked masked autoencoder (LexMAE), to learn importance-aware lexicon representations. Essentially, we present a lexicon-bottlenecked module between a normal language modeling encoder and a weakened decoder, where a continuous bag-of-words bottleneck is constructed to learn a lexicon-importance distribution in an unsupervised fashion. The pre-trained LexMAE ",
    "link": "http://arxiv.org/abs/2208.14754",
    "context": "Title: LexMAE: Lexicon-Bottlenecked Pretraining for Large-Scale Retrieval. (arXiv:2208.14754v2 [cs.IR] UPDATED)\nAbstract: In large-scale retrieval, the lexicon-weighting paradigm, learning weighted sparse representations in vocabulary space, has shown promising results with high quality and low latency. Despite it deeply exploiting the lexicon-representing capability of pre-trained language models, a crucial gap remains between language modeling and lexicon-weighting retrieval -- the former preferring certain or low-entropy words whereas the latter favoring pivot or high-entropy words -- becoming the main barrier to lexicon-weighting performance for large-scale retrieval. To bridge this gap, we propose a brand-new pre-training framework, lexicon-bottlenecked masked autoencoder (LexMAE), to learn importance-aware lexicon representations. Essentially, we present a lexicon-bottlenecked module between a normal language modeling encoder and a weakened decoder, where a continuous bag-of-words bottleneck is constructed to learn a lexicon-importance distribution in an unsupervised fashion. The pre-trained LexMAE ",
    "path": "papers/22/08/2208.14754.json",
    "total_tokens": 1149,
    "translated_title": "LexMAE：用于大规模检索的词汇限制预训练",
    "translated_abstract": "在大规模检索中，利用词汇权重范例，学习在词汇空间中的加权稀疏表示，显示出了高质量和低延迟的有前途的结果。为了填补语言建模和词汇加权检索之间的巨大差距，我们提出了一种全新的预训练框架，LexMAE，用于学习重要性感知的词汇表示。这一框架通过加入一个词汇瓶颈模块，将普通的语言模型编码器和弱化的解码器之间加入了一个连续词袋瓶颈，以实现无监督学习词汇重要性分布。预训练后的LexMAE已经表现出了在大规模信息检索中的卓越性能。",
    "tldr": "LexMAE是一个新的预训练框架，用于学习重要性感知的词汇表示，在大规模信息检索中表现出了卓越的性能。",
    "en_tdlr": "LexMAE is a new pre-training framework for learning importance-aware lexicon representations and has shown excellent performance in large-scale information retrieval."
}