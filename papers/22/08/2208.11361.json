{
    "title": "Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning. (arXiv:2208.11361v2 [cs.LG] UPDATED)",
    "abstract": "Under sparse extrinsic reward settings, reinforcement learning has remained challenging, despite surging interests in this field. Previous attempts suggest that intrinsic reward can alleviate the issue caused by sparsity. In this article, we present a novel intrinsic reward that is inspired by human learning, as humans evaluate curiosity by comparing current observations with historical knowledge. Our method involves training a self-supervised prediction model, saving snapshots of the model parameters, and using nuclear norm to evaluate the temporal inconsistency between the predictions of different snapshots as intrinsic rewards. We also propose a variational weighting mechanism to assign weight to different snapshots in an adaptive manner. Our experimental results on various benchmark environments demonstrate the efficacy of our method, which outperforms other intrinsic reward-based methods without additional training costs and with higher noise tolerance. This work has been submitte",
    "link": "http://arxiv.org/abs/2208.11361",
    "context": "Title: Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning. (arXiv:2208.11361v2 [cs.LG] UPDATED)\nAbstract: Under sparse extrinsic reward settings, reinforcement learning has remained challenging, despite surging interests in this field. Previous attempts suggest that intrinsic reward can alleviate the issue caused by sparsity. In this article, we present a novel intrinsic reward that is inspired by human learning, as humans evaluate curiosity by comparing current observations with historical knowledge. Our method involves training a self-supervised prediction model, saving snapshots of the model parameters, and using nuclear norm to evaluate the temporal inconsistency between the predictions of different snapshots as intrinsic rewards. We also propose a variational weighting mechanism to assign weight to different snapshots in an adaptive manner. Our experimental results on various benchmark environments demonstrate the efficacy of our method, which outperforms other intrinsic reward-based methods without additional training costs and with higher noise tolerance. This work has been submitte",
    "path": "papers/22/08/2208.11361.json",
    "total_tokens": 937,
    "translated_title": "基于时间不一致的自监督探索在强化学习中的应用",
    "translated_abstract": "在稀疏外在奖励的情况下，强化学习仍然具有挑战性，尽管对这个领域的兴趣不断增加。先前的尝试表明，内在奖励可以缓解稀疏性带来的问题。在本文中，我们提出了一种受人类学习启发的新型内在奖励，人类通过将当前观察与历史知识进行比较来评估好奇心。我们的方法涉及训练一个自监督预测模型，保存模型参数的快照，并使用核范数来评估不同快照之间预测的时间不一致性作为内在奖励。我们还提出了一种变分加权机制，以自适应的方式为不同的快照分配权重。我们在各种基准环境上的实验结果表明了我们方法的有效性，该方法在没有额外训练成本且具有更高噪声容忍度的情况下优于其他基于内在奖励的方法。",
    "tldr": "本文在强化学习中提出了一种新的内在奖励方法，通过比较当前观察与历史知识的差异来评估好奇心，并利用时间不一致性作为内在奖励。实验证明该方法在稀疏外在奖励的情况下具有更高的性能和噪声容忍度。"
}