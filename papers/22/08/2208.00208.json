{
    "title": "DRSOM: A Dimension Reduced Second-Order Method. (arXiv:2208.00208v3 [math.OC] UPDATED)",
    "abstract": "In this paper, we propose a Dimension-Reduced Second-Order Method (DRSOM) for convex and nonconvex (unconstrained) optimization. Under a trust-region-like framework, our method preserves the convergence of the second-order method while using only curvature information in a few directions. Consequently, the computational overhead of our method remains comparable to the first-order such as the gradient descent method. Theoretically, we show that the method has a local quadratic convergence and a global convergence rate of $O(\\epsilon^{-3/2})$ to satisfy the first-order and second-order conditions if the subspace satisfies a commonly adopted approximated Hessian assumption. We further show that this assumption can be removed if we perform a corrector step using a Krylov-like method periodically at the end stage of the algorithm. The applicability and performance of DRSOM are exhibited by various computational experiments, including $L_2 - L_p$ minimization, CUTEst problems, and sensor net",
    "link": "http://arxiv.org/abs/2208.00208",
    "context": "Title: DRSOM: A Dimension Reduced Second-Order Method. (arXiv:2208.00208v3 [math.OC] UPDATED)\nAbstract: In this paper, we propose a Dimension-Reduced Second-Order Method (DRSOM) for convex and nonconvex (unconstrained) optimization. Under a trust-region-like framework, our method preserves the convergence of the second-order method while using only curvature information in a few directions. Consequently, the computational overhead of our method remains comparable to the first-order such as the gradient descent method. Theoretically, we show that the method has a local quadratic convergence and a global convergence rate of $O(\\epsilon^{-3/2})$ to satisfy the first-order and second-order conditions if the subspace satisfies a commonly adopted approximated Hessian assumption. We further show that this assumption can be removed if we perform a corrector step using a Krylov-like method periodically at the end stage of the algorithm. The applicability and performance of DRSOM are exhibited by various computational experiments, including $L_2 - L_p$ minimization, CUTEst problems, and sensor net",
    "path": "papers/22/08/2208.00208.json",
    "total_tokens": 922,
    "translated_title": "DRSOM: 一种降维的二阶优化方法",
    "translated_abstract": "本文提出了一种用于凸和非凸（无约束）优化的降维二阶方法（DRSOM）。在类似信赖域的框架下，我们的方法在只使用少数方向的曲率信息的同时保持了二阶方法的收敛性。因此，我们的方法的计算开销仍然与一阶方法（如梯度下降法）相当。在理论上，我们证明了该方法具有局部二次收敛性和全局收敛速度为 $O(\\epsilon^{-3/2})$，以满足一阶和二阶条件，如果子空间满足常用的近似黑塞矩阵的假设。我们进一步展示了，如果在算法的最后阶段定期使用类似Krylov方法的纠正步骤，那么可以消除该假设。通过各种计算实验展示了DRSOM的适用性和性能，包括$L_2 - L_p$最小化、CUTEst问题和传感器网络。",
    "tldr": "本文提出了一种降维的二阶优化方法（DRSOM），通过在少数方向上利用曲率信息来保持二阶方法的收敛性，并将计算开销控制在与一阶方法相当的程度。理论上证明了该方法在满足近似黑塞矩阵假设的条件下具有局部二次收敛和全局收敛速度为 $O(\\epsilon^{-3/2})$。通过各种计算实验展示了该方法的适用性和性能。"
}