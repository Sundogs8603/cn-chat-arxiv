{
    "title": "Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation. (arXiv:2208.10734v2 [cs.CL] UPDATED)",
    "abstract": "Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates. Given two snapshots $C_1$ and $C_2$ of a corpus taken respectively at two distinct timestamps $T_1$ and $T_2$, we first propose an unsupervised method to select (a) \\emph{pivot} terms related to both $C_1$ and $C_2$, and (b) \\emph{anchor} terms that are associated with a specific pivot term in each individual snapshot. We then generate prompts by filling manually compiled templates using the extracted pivot and anchor terms. Moreover, we propose an automatic method to learn time-sensitive templates from $C_1$ and $C_2$, without requiring any human supervision. Next, we use the generated prompts to adapt a pretrained MLM to $T_2$ by fine-tuning using those prompts. Multiple experiments show that our proposed method reduces the perplexity of test sentences in $C_",
    "link": "http://arxiv.org/abs/2208.10734",
    "context": "Title: Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation. (arXiv:2208.10734v2 [cs.CL] UPDATED)\nAbstract: Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates. Given two snapshots $C_1$ and $C_2$ of a corpus taken respectively at two distinct timestamps $T_1$ and $T_2$, we first propose an unsupervised method to select (a) \\emph{pivot} terms related to both $C_1$ and $C_2$, and (b) \\emph{anchor} terms that are associated with a specific pivot term in each individual snapshot. We then generate prompts by filling manually compiled templates using the extracted pivot and anchor terms. Moreover, we propose an automatic method to learn time-sensitive templates from $C_1$ and $C_2$, without requiring any human supervision. Next, we use the generated prompts to adapt a pretrained MLM to $T_2$ by fine-tuning using those prompts. Multiple experiments show that our proposed method reduces the perplexity of test sentences in $C_",
    "path": "papers/22/08/2208.10734.json",
    "total_tokens": 1061,
    "translated_title": "基于模板化时间适应的动态上下文词嵌入学习方法",
    "translated_abstract": "动态上下文词嵌入（DCWE）表示单词的语义随时间的变化。本文提出了一种使用感知时间的模板学习预训练掩蔽语言模型（MLM）得到DCWE的方法。本文首先提出一种无监督方法，通过选择与$C_1$和$C_2$相关的“枢轴”（pivot）术语和在各个快照中与特定枢轴术语相关的“锚”（anchor）术语来生成提示。然后，本文提出了一种自动学习时间敏感的模板的方法，从$C_1$和$C_2$中进行学习，无需任何人工监督。接下来，本文使用这些生成的提示通过微调来适应预训练的MLM到$T_2$。多个实验证明，我们的方法在超过强基线9.2点的同时降低了$C_1$测试句子的困惑度，优于现有的动态上下文词嵌入方法。此外，我们还证明了我们的方法对情感分析和命名实体识别等下游任务是有用的。",
    "tldr": "本文提出一种基于时间适应的动态上下文词嵌入学习方法，使用感知时间的模板生成提示，通过微调预训练的MLM得到DCWE，实验结果表明该方法在困惑度和下游任务上优于现有方法。",
    "en_tdlr": "This paper proposes a method for learning dynamic contextualised word embeddings (DCWEs) by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates. The generated prompts are used to fine-tune the MLM to obtain DCWEs. The proposed method outperforms existing methods in perplexity and downstream tasks such as sentiment analysis and named entity recognition."
}