{
    "title": "GHN-Q: Parameter Prediction for Unseen Quantized Convolutional Architectures via Graph Hypernetworks. (arXiv:2208.12489v2 [cs.LG] UPDATED)",
    "abstract": "Deep convolutional neural network (CNN) training via iterative optimization has had incredible success in finding optimal parameters. However, modern CNN architectures often contain millions of parameters. Thus, any given model for a single architecture resides in a massive parameter space. Models with similar loss could have drastically different characteristics such as adversarial robustness, generalizability, and quantization robustness. For deep learning on the edge, quantization robustness is often crucial. Finding a model that is quantization-robust can sometimes require significant efforts. Recent works using Graph Hypernetworks (GHN) have shown remarkable performance predicting high-performant parameters of varying CNN architectures. Inspired by these successes, we wonder if the graph representations of GHN-2 can be leveraged to predict quantization-robust parameters as well, which we call GHN-Q. We conduct the first-ever study exploring the use of graph hypernetworks for predi",
    "link": "http://arxiv.org/abs/2208.12489",
    "context": "Title: GHN-Q: Parameter Prediction for Unseen Quantized Convolutional Architectures via Graph Hypernetworks. (arXiv:2208.12489v2 [cs.LG] UPDATED)\nAbstract: Deep convolutional neural network (CNN) training via iterative optimization has had incredible success in finding optimal parameters. However, modern CNN architectures often contain millions of parameters. Thus, any given model for a single architecture resides in a massive parameter space. Models with similar loss could have drastically different characteristics such as adversarial robustness, generalizability, and quantization robustness. For deep learning on the edge, quantization robustness is often crucial. Finding a model that is quantization-robust can sometimes require significant efforts. Recent works using Graph Hypernetworks (GHN) have shown remarkable performance predicting high-performant parameters of varying CNN architectures. Inspired by these successes, we wonder if the graph representations of GHN-2 can be leveraged to predict quantization-robust parameters as well, which we call GHN-Q. We conduct the first-ever study exploring the use of graph hypernetworks for predi",
    "path": "papers/22/08/2208.12489.json",
    "total_tokens": 912,
    "translated_title": "GHN-Q：通过图形超网络预测未见量化卷积架构的参数",
    "translated_abstract": "深度卷积神经网络（CNN）通过迭代优化训练已经取得了令人难以置信的成功，找到了最佳参数。然而，现代CNN架构通常包含数百万个参数。因此，对于单个架构的任何给定模型都存在一个庞大的参数空间。具有相似损失的模型可能具有截然不同的特性，如对抗鲁棒性、泛化能力和量化鲁棒性。对于边缘上的深度学习，量化鲁棒性通常至关重要。找到一个量化鲁棒的模型有时可能需要很大的努力。最近使用图形超网络（GHN）的研究表明，它在预测不同CNN架构的高性能参数方面表现出了显著的性能。受到这些成功的启发，我们想知道GHN-2的图形表示是否也可以用于预测量化鲁棒的参数，我们将其称为GHN-Q。我们进行了有史以来第一次探索使用图形超网络来预测量化鲁棒参数的研究。",
    "tldr": "本论文提出了一种名为GHN-Q的方法，通过图形超网络来预测未见量化卷积架构的参数，以提高量化鲁棒性。",
    "en_tdlr": "This paper proposes a method called GHN-Q, which uses graph hypernetworks to predict parameters for unseen quantized convolutional architectures, aiming to improve quantization robustness."
}