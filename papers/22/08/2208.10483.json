{
    "title": "Prioritizing Samples in Reinforcement Learning with Reducible Loss. (arXiv:2208.10483v3 [cs.LG] UPDATED)",
    "abstract": "Most reinforcement learning algorithms take advantage of an experience replay buffer to repeatedly train on samples the agent has observed in the past. Not all samples carry the same amount of significance and simply assigning equal importance to each of the samples is a na\\\"ive strategy. In this paper, we propose a method to prioritize samples based on how much we can learn from a sample. We define the learn-ability of a sample as the steady decrease of the training loss associated with this sample over time. We develop an algorithm to prioritize samples with high learn-ability, while assigning lower priority to those that are hard-to-learn, typically caused by noise or stochasticity. We empirically show that our method is more robust than random sampling and also better than just prioritizing with respect to the training loss, i.e. the temporal difference loss, which is used in prioritized experience replay.",
    "link": "http://arxiv.org/abs/2208.10483",
    "context": "Title: Prioritizing Samples in Reinforcement Learning with Reducible Loss. (arXiv:2208.10483v3 [cs.LG] UPDATED)\nAbstract: Most reinforcement learning algorithms take advantage of an experience replay buffer to repeatedly train on samples the agent has observed in the past. Not all samples carry the same amount of significance and simply assigning equal importance to each of the samples is a na\\\"ive strategy. In this paper, we propose a method to prioritize samples based on how much we can learn from a sample. We define the learn-ability of a sample as the steady decrease of the training loss associated with this sample over time. We develop an algorithm to prioritize samples with high learn-ability, while assigning lower priority to those that are hard-to-learn, typically caused by noise or stochasticity. We empirically show that our method is more robust than random sampling and also better than just prioritizing with respect to the training loss, i.e. the temporal difference loss, which is used in prioritized experience replay.",
    "path": "papers/22/08/2208.10483.json",
    "total_tokens": 897,
    "translated_title": "在可约损失中为强化学习优先选择样本",
    "translated_abstract": "大多数强化学习算法利用经验回放缓冲区反复训练代理已观察到的样本。并非所有样本具有相同的重要性，简单地赋予每个样本相等的重要性是一种天真的策略。在本文中，我们提出一种基于我们可以从样本中学到多少的方法来优先选择样本。我们将样本的可学习性定义为与样本相关的训练损失随时间持续下降的程度。我们开发了一种算法来优先选择具有较高可学习性的样本，同时将较难学习的样本（通常由噪声或随机性引起）赋予较低的优先级。我们通过实验证明，我们的方法比随机抽样更加稳健，也优于仅根据训练损失（即时间差分损失）进行优先选择，这在优先经验回放中使用。",
    "tldr": "本文提出了一种在强化学习中基于可学习性的方法来优先选择样本，通过稳定降低样本的训练损失来定义样本的可学习性。实验证明，该方法相比于随机抽样和仅根据训练损失进行优先选择的方法更加稳健。",
    "en_tdlr": "This paper proposes a method to prioritize samples in reinforcement learning based on their learn-ability, defined as the steady decrease of the training loss associated with the sample over time. The method outperforms random sampling and prioritizing based on training loss alone, showing increased robustness."
}