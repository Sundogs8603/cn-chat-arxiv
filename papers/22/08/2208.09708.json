{
    "title": "DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization. (arXiv:2208.09708v2 [cs.CV] UPDATED)",
    "abstract": "Efficiently deploying deep neural networks on low-resource edge devices is challenging due to their ever-increasing resource requirements. To address this issue, researchers have proposed multiplication-free neural networks, such as Power-of-Two quantization, or also known as Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift networks are not as accurate as their full-precision counterparts, typically suffering from limited weight range encoding schemes and quantization loss. In this paper, we propose the DenseShift network, which significantly improves the accuracy of Shift networks, achieving competitive performance to full-precision networks for vision and speech applications. In addition, we introduce a method to deploy an efficient DenseShift network using non-quantized floating-point activations, while obtaining 1.6X speed-up over existing methods. To achieve this, we demonstrate that zero-weight values in low-bit Shift netw",
    "link": "http://arxiv.org/abs/2208.09708",
    "context": "Title: DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization. (arXiv:2208.09708v2 [cs.CV] UPDATED)\nAbstract: Efficiently deploying deep neural networks on low-resource edge devices is challenging due to their ever-increasing resource requirements. To address this issue, researchers have proposed multiplication-free neural networks, such as Power-of-Two quantization, or also known as Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift networks are not as accurate as their full-precision counterparts, typically suffering from limited weight range encoding schemes and quantization loss. In this paper, we propose the DenseShift network, which significantly improves the accuracy of Shift networks, achieving competitive performance to full-precision networks for vision and speech applications. In addition, we introduce a method to deploy an efficient DenseShift network using non-quantized floating-point activations, while obtaining 1.6X speed-up over existing methods. To achieve this, we demonstrate that zero-weight values in low-bit Shift netw",
    "path": "papers/22/08/2208.09708.json",
    "total_tokens": 805,
    "translated_title": "DenseShift: 实现准确和高效的低位幂乘法的量化",
    "translated_abstract": "在低资源边缘设备上高效部署深度神经网络是具有挑战性的，因为其不断增加的资源需求。为了解决这个问题，研究人员提出了无乘法神经网络，如幂乘法的量化，也被称为Shift网络，旨在减少内存使用和简化计算。然而，现有的低位Shift网络不如全精度网络准确，通常受到有限权重范围编码方案和量化损失的影响。在本文中，我们提出了DenseShift网络，显著提高了Shift网络的准确性，为视觉和语音应用实现了与全精度网络相媲美的性能。此外，我们引入了一种使用非量化浮点激活的高效DenseShift网络部署方法，同时获得了现有方法的1.6倍加速。为了实现这一点，我们证明了低位Shift网络中零权重值的作用。",
    "tldr": "DenseShift网络是一种准确和高效的低位幂乘法量化方法，通过改进Shift网络的精度和引入非量化浮点激活来提高性能。"
}