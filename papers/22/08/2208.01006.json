{
    "title": "Multi-Document Summarization with Centroid-Based Pretraining. (arXiv:2208.01006v2 [cs.CL] UPDATED)",
    "abstract": "In Multi-Document Summarization (MDS), the input can be modeled as a set of documents, and the output is its summary. In this paper, we focus on pretraining objectives for MDS. Specifically, we introduce a novel pretraining objective, which involves selecting the ROUGE-based centroid of each document cluster as a proxy for its summary. Our objective thus does not require human written summaries and can be utilized for pretraining on a dataset consisting solely of document sets. Through zero-shot, few-shot, and fully supervised experiments on multiple MDS datasets, we show that our model Centrum is better or comparable to a state-of-the-art model. We make the pretrained and fine-tuned models freely available to the research community https://github.com/ratishsp/centrum.",
    "link": "http://arxiv.org/abs/2208.01006",
    "context": "Title: Multi-Document Summarization with Centroid-Based Pretraining. (arXiv:2208.01006v2 [cs.CL] UPDATED)\nAbstract: In Multi-Document Summarization (MDS), the input can be modeled as a set of documents, and the output is its summary. In this paper, we focus on pretraining objectives for MDS. Specifically, we introduce a novel pretraining objective, which involves selecting the ROUGE-based centroid of each document cluster as a proxy for its summary. Our objective thus does not require human written summaries and can be utilized for pretraining on a dataset consisting solely of document sets. Through zero-shot, few-shot, and fully supervised experiments on multiple MDS datasets, we show that our model Centrum is better or comparable to a state-of-the-art model. We make the pretrained and fine-tuned models freely available to the research community https://github.com/ratishsp/centrum.",
    "path": "papers/22/08/2208.01006.json",
    "total_tokens": 801,
    "translated_title": "基于质心预训练的多文档摘要",
    "translated_abstract": "在多文档摘要(MDS)中，输入可以被建模为一组文档，输出是它们的摘要。本文侧重于MDS的预训练目标。具体而言，我们引入了一种新的预训练目标，它涉及选择每个文档聚类的基于ROUGE的质心作为其摘要的代理。我们的目标不需要人工编写的摘要，可以用于仅由文档集组成的数据集的预训练。通过在多个MDS数据集上进行零样本、少样本和完全监督实验，我们展示了我们的模型Centrum比现有的先进模型更好或可比。我们将预训练和微调的模型免费提供给研究社区https://github.com/ratishsp/centrum。",
    "tldr": "本文提出一种基于ROUGE的质心聚类预训练方法，可用于多文档摘要中，不需要人工编写的摘要，模型Centrum比现有先进模型更好。",
    "en_tdlr": "This paper proposes a centroid-based pretraining method using ROUGE, which can be utilized in Multi-Document Summarization without requiring human written summaries. The model Centrum shows better performance than the state-of-the-art models in zero-shot, few-shot, and fully supervised experiments on multiple MDS datasets."
}