{
    "title": "Concept Gradient: Concept-based Interpretation Without Linear Assumption",
    "abstract": "Concept-based interpretations of black-box models are often more intuitive for humans to understand. The most widely adopted approach for concept-based interpretation is Concept Activation Vector (CAV). CAV relies on learning a linear relation between some latent representation of a given model and concepts. The linear separability is usually implicitly assumed but does not hold true in general. In this work, we started from the original intent of concept-based interpretation and proposed Concept Gradient (CG), extending concept-based interpretation beyond linear concept functions. We showed that for a general (potentially non-linear) concept, we can mathematically evaluate how a small change of concept affecting the model's prediction, which leads to an extension of gradient-based interpretation to the concept space. We demonstrated empirically that CG outperforms CAV in both toy examples and real world datasets.",
    "link": "https://arxiv.org/abs/2208.14966",
    "context": "Title: Concept Gradient: Concept-based Interpretation Without Linear Assumption\nAbstract: Concept-based interpretations of black-box models are often more intuitive for humans to understand. The most widely adopted approach for concept-based interpretation is Concept Activation Vector (CAV). CAV relies on learning a linear relation between some latent representation of a given model and concepts. The linear separability is usually implicitly assumed but does not hold true in general. In this work, we started from the original intent of concept-based interpretation and proposed Concept Gradient (CG), extending concept-based interpretation beyond linear concept functions. We showed that for a general (potentially non-linear) concept, we can mathematically evaluate how a small change of concept affecting the model's prediction, which leads to an extension of gradient-based interpretation to the concept space. We demonstrated empirically that CG outperforms CAV in both toy examples and real world datasets.",
    "path": "papers/22/08/2208.14966.json",
    "total_tokens": 867,
    "translated_title": "概念梯度：基于概念的解释不依赖于线性假设",
    "translated_abstract": "基于概念的解释对于人类更易理解。目前最常用的基于概念的解释方法是概念激活向量（CAV）。CAV依赖于学习给定模型的某种潜在表示与概念之间的线性关系。通常隐含地假设线性可分，但并不总是成立。在这项工作中，我们从基于概念解释的原始目的出发，提出了概念梯度（CG），将基于概念的解释扩展到了非线性概念函数。我们证明了对于一般（可能非线性）的概念，我们可以数学上评估概念的微小变化如何影响模型的预测，从而将梯度解释扩展到了概念空间。我们在玩具示例和真实世界数据集上进行了实证，证明了概念梯度在性能上优于概念激活向量（CAV）。",
    "tldr": "本文提出了一种概念梯度（CG）的方法，将基于概念的解释扩展到了非线性概念函数，并证明在玩具示例和真实世界数据集上，概念梯度（CG）优于概念激活向量（CAV）。"
}