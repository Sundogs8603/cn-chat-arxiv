{
    "title": "Why do networks have inhibitory/negative connections?. (arXiv:2208.03211v8 [cs.LG] UPDATED)",
    "abstract": "Why do brains have inhibitory connections? Why do deep networks have negative weights? We propose an answer from the perspective of representation capacity. We believe representing functions is the primary role of both (i) the brain in natural intelligence, and (ii) deep networks in artificial intelligence. Our answer to why there are inhibitory/negative weights is: to learn more functions. We prove that, in the absence of negative weights, neural networks with non-decreasing activation functions are not universal approximators. While this may be an intuitive result to some, to the best of our knowledge, there is no formal theory, in either machine learning or neuroscience, that demonstrates why negative weights are crucial in the context of representation capacity. Further, we provide insights on the geometric properties of the representation space that non-negative deep networks cannot represent. We expect these insights will yield a deeper understanding of more sophisticated inducti",
    "link": "http://arxiv.org/abs/2208.03211",
    "context": "Title: Why do networks have inhibitory/negative connections?. (arXiv:2208.03211v8 [cs.LG] UPDATED)\nAbstract: Why do brains have inhibitory connections? Why do deep networks have negative weights? We propose an answer from the perspective of representation capacity. We believe representing functions is the primary role of both (i) the brain in natural intelligence, and (ii) deep networks in artificial intelligence. Our answer to why there are inhibitory/negative weights is: to learn more functions. We prove that, in the absence of negative weights, neural networks with non-decreasing activation functions are not universal approximators. While this may be an intuitive result to some, to the best of our knowledge, there is no formal theory, in either machine learning or neuroscience, that demonstrates why negative weights are crucial in the context of representation capacity. Further, we provide insights on the geometric properties of the representation space that non-negative deep networks cannot represent. We expect these insights will yield a deeper understanding of more sophisticated inducti",
    "path": "papers/22/08/2208.03211.json",
    "total_tokens": 900,
    "translated_title": "为什么网络具有抑制性/负向连接？",
    "translated_abstract": "大脑为什么具有抑制性连接？深度网络为什么具有负权重？我们从表征能力的角度提出了一个答案。我们认为，在自然智能中，大脑的主要作用是表征功能，在人工智能中，深度网络的主要作用也是如此。我们的答案是为什么存在抑制性/负向权重：为了学习更多的功能。我们证明了，在没有负权重的情况下，具有非递增激活函数的神经网络无法成为普适近似器。尽管这可能对一些人来说是一种直观的结果，但据我们所知，无论是在机器学习还是神经科学领域，都没有提供正式理论来证明为什么在表征能力的背景下，负权重至关重要。此外，我们还提供了非负深度网络无法表示的表征空间的几何特性的见解。我们希望这些见解能够带来对更复杂的归纳过程的更深入了解。",
    "tldr": "神经网络具有抑制性/负向连接是为了学习更多的功能，负权重在表征能力中起着至关重要的作用，并且非负深度网络无法表示某些表征空间的几何特性。"
}