{
    "title": "Combating Mode Collapse in GANs via Manifold Entropy Estimation. (arXiv:2208.12055v6 [cs.CV] UPDATED)",
    "abstract": "Generative Adversarial Networks (GANs) have shown compelling results in various tasks and applications in recent years. However, mode collapse remains a critical problem in GANs. In this paper, we propose a novel training pipeline to address the mode collapse issue of GANs. Different from existing methods, we propose to generalize the discriminator as feature embedding and maximize the entropy of distributions in the embedding space learned by the discriminator. Specifically, two regularization terms, i.e., Deep Local Linear Embedding (DLLE) and Deep Isometric feature Mapping (DIsoMap), are designed to encourage the discriminator to learn the structural information embedded in the data, such that the embedding space learned by the discriminator can be well-formed. Based on the well-learned embedding space supported by the discriminator, a non-parametric entropy estimator is designed to efficiently maximize the entropy of embedding vectors, playing as an approximation of maximizing the ",
    "link": "http://arxiv.org/abs/2208.12055",
    "context": "Title: Combating Mode Collapse in GANs via Manifold Entropy Estimation. (arXiv:2208.12055v6 [cs.CV] UPDATED)\nAbstract: Generative Adversarial Networks (GANs) have shown compelling results in various tasks and applications in recent years. However, mode collapse remains a critical problem in GANs. In this paper, we propose a novel training pipeline to address the mode collapse issue of GANs. Different from existing methods, we propose to generalize the discriminator as feature embedding and maximize the entropy of distributions in the embedding space learned by the discriminator. Specifically, two regularization terms, i.e., Deep Local Linear Embedding (DLLE) and Deep Isometric feature Mapping (DIsoMap), are designed to encourage the discriminator to learn the structural information embedded in the data, such that the embedding space learned by the discriminator can be well-formed. Based on the well-learned embedding space supported by the discriminator, a non-parametric entropy estimator is designed to efficiently maximize the entropy of embedding vectors, playing as an approximation of maximizing the ",
    "path": "papers/22/08/2208.12055.json",
    "total_tokens": 910,
    "translated_title": "通过流形熵估计解决GAN中的模式崩溃问题",
    "translated_abstract": "近年来，生成对抗网络（GAN）在各种任务和应用中展示了令人信服的结果。然而，在GAN中，模式崩溃仍然是一个关键问题。本文提出了一种新的训练流程来解决GAN的模式崩溃问题。与现有方法不同，我们将判别器作为特征嵌入进行泛化，最大化判别器在嵌入空间中学习到的分布熵。具体而言，设计了两个正则化项，即深度局部线性嵌入（DLLE）和深度等距特征映射（DIsoMap），以鼓励判别器学习嵌入在数据中的结构信息，使得判别器学习到的嵌入空间可以被很好地形成。基于由判别器支持的学习良好的嵌入空间，设计了一个非参数熵估计器，以高效地最大化嵌入向量的熵，作为最大化数据分布熵的近似。",
    "tldr": "本文提出了一种新的GAN训练流程，通过将判别器作为特征嵌入进行泛化并设计两个正则化项，从而最大化学习到的分布熵以解决GAN中的模式崩溃问题。",
    "en_tdlr": "This paper proposes a novel GAN training pipeline that generalizes the discriminator as feature embedding and designs two regularization terms to encourage the discriminator to learn the structure information embedded in the data, thereby maximizing the learned distribution entropy to solve the mode collapse problem in GANs."
}