{
    "title": "PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation",
    "abstract": "arXiv:2208.10160v2 Announce Type: replace  Abstract: Prompt Transfer (PoT) is a recently-proposed approach to improve prompt-tuning, by initializing the target prompt with the existing prompt trained on similar source tasks. However, such a vanilla PoT approach usually achieves sub-optimal performance, as (i) the PoT is sensitive to the similarity of source-target pair and (ii) directly fine-tuning the prompt initialized with source prompt on target task might lead to forgetting of the useful general knowledge learned from source task. To tackle these issues, we propose a new metric to accurately predict the prompt transferability (regarding (i)), and a novel PoT approach (namely PANDA) that leverages the knowledge distillation technique to alleviate the knowledge forgetting effectively (regarding (ii)). Extensive and systematic experiments on 189 combinations of 21 source and 9 target datasets across 5 scales of PLMs demonstrate that: 1) our proposed metric works well to predict the p",
    "link": "https://arxiv.org/abs/2208.10160",
    "context": "Title: PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation\nAbstract: arXiv:2208.10160v2 Announce Type: replace  Abstract: Prompt Transfer (PoT) is a recently-proposed approach to improve prompt-tuning, by initializing the target prompt with the existing prompt trained on similar source tasks. However, such a vanilla PoT approach usually achieves sub-optimal performance, as (i) the PoT is sensitive to the similarity of source-target pair and (ii) directly fine-tuning the prompt initialized with source prompt on target task might lead to forgetting of the useful general knowledge learned from source task. To tackle these issues, we propose a new metric to accurately predict the prompt transferability (regarding (i)), and a novel PoT approach (namely PANDA) that leverages the knowledge distillation technique to alleviate the knowledge forgetting effectively (regarding (ii)). Extensive and systematic experiments on 189 combinations of 21 source and 9 target datasets across 5 scales of PLMs demonstrate that: 1) our proposed metric works well to predict the p",
    "path": "papers/22/08/2208.10160.json",
    "total_tokens": 828,
    "translated_title": "PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model Adaptation",
    "translated_abstract": "提示转移（PoT）是一种最近提出的方法，用于改进提示微调，通过将目标提示初始化为在类似源任务上训练的现有提示。然而，这种普通的PoT方法通常会达到次优性能，因为（i）PoT对源-目标对的相似性敏感，并且（ii）在目标任务上直接微调使用源提示初始化的提示可能导致遗忘从源任务学到的有用的通用知识。为了解决这些问题，我们提出了一种新的度量标准来准确预测提示的可转移性（关于（i）），以及一种利用知识蒸馏技术有效缓解知识遗忘的新颖PoT方法（即PANDA）（关于（ii））。对于21个源和9个目标数据集的189种组合，在5个规模的PLM上进行了广泛而系统的实验，结果表明：1）我们提出的度量标准能够很好地预测p",
    "tldr": "PANDA是一种新颖的PoT方法，通过引入知识蒸馏技术有效缓解知识遗忘问题。",
    "en_tdlr": "PANDA is a novel PoT approach that effectively mitigates knowledge forgetting by introducing knowledge distillation technique."
}