{
    "title": "Adaptive Domain Generalization via Online Disagreement Minimization. (arXiv:2208.01996v2 [cs.CV] UPDATED)",
    "abstract": "Deep neural networks suffer from significant performance deterioration when there exists distribution shift between deployment and training. Domain Generalization (DG) aims to safely transfer a model to unseen target domains by only relying on a set of source domains. Although various DG approaches have been proposed, a recent study named DomainBed, reveals that most of them do not beat the simple Empirical Risk Minimization (ERM). To this end, we propose a general framework that is orthogonal to existing DG algorithms and could improve their performance consistently. Unlike previous DG works that stake on a static source model to be hopefully a universal one, our proposed AdaODM adaptively modifies the source model at test time for different target domains. Specifically, we create multiple domain-specific classifiers upon a shared domain-generic feature extractor. The feature extractor and classifiers are trained in an adversarial way, where the feature extractor embeds the input samp",
    "link": "http://arxiv.org/abs/2208.01996",
    "context": "Title: Adaptive Domain Generalization via Online Disagreement Minimization. (arXiv:2208.01996v2 [cs.CV] UPDATED)\nAbstract: Deep neural networks suffer from significant performance deterioration when there exists distribution shift between deployment and training. Domain Generalization (DG) aims to safely transfer a model to unseen target domains by only relying on a set of source domains. Although various DG approaches have been proposed, a recent study named DomainBed, reveals that most of them do not beat the simple Empirical Risk Minimization (ERM). To this end, we propose a general framework that is orthogonal to existing DG algorithms and could improve their performance consistently. Unlike previous DG works that stake on a static source model to be hopefully a universal one, our proposed AdaODM adaptively modifies the source model at test time for different target domains. Specifically, we create multiple domain-specific classifiers upon a shared domain-generic feature extractor. The feature extractor and classifiers are trained in an adversarial way, where the feature extractor embeds the input samp",
    "path": "papers/22/08/2208.01996.json",
    "total_tokens": 866,
    "translated_title": "自适应领域泛化通过在线争议最小化",
    "translated_abstract": "当部署和训练之间存在分布偏移时，深度神经网络的性能会显著下降。领域泛化（DG）旨在通过仅依赖一组源领域将模型安全地转移到未知的目标领域。尽管已经提出了各种DG方法，但最近的一项研究称为DomainBed揭示了大部分方法都没有超过简单的经验风险最小化（ERM）。为此，我们提出了一个通用框架，与现有的DG算法正交，并可以持续改进它们的性能。与先前的DG工作不同，该工作基于一个静态的源模型希望它是通用的，我们提出的AdaODM在测试时间自适应地修改源模型，以适应不同的目标领域。具体而言，我们在共享的领域通用特征提取器上创建多个领域特定分类器。特征提取器和分类器通过对抗训练的方式进行训练，其中特征提取器嵌入输入样本。",
    "tldr": "本文提出了一个自适应领域泛化的通用框架，通过在线争议最小化对源模型进行适应性修改，从而改善了领域泛化算法的性能。"
}