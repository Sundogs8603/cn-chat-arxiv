{
    "title": "What Does the Gradient Tell When Attacking the Graph Structure. (arXiv:2208.12815v2 [cs.LG] UPDATED)",
    "abstract": "Recent research has revealed that Graph Neural Networks (GNNs) are susceptible to adversarial attacks targeting the graph structure. A malicious attacker can manipulate a limited number of edges, given the training labels, to impair the victim model's performance. Previous empirical studies indicate that gradient-based attackers tend to add edges rather than remove them. In this paper, we present a theoretical demonstration revealing that attackers tend to increase inter-class edges due to the message passing mechanism of GNNs, which explains some previous empirical observations. By connecting dissimilar nodes, attackers can more effectively corrupt node features, making such attacks more advantageous. However, we demonstrate that the inherent smoothness of GNN's message passing tends to blur node dissimilarity in the feature space, leading to the loss of crucial information during the forward process. To address this issue, we propose a novel surrogate model with multi-level propagati",
    "link": "http://arxiv.org/abs/2208.12815",
    "context": "Title: What Does the Gradient Tell When Attacking the Graph Structure. (arXiv:2208.12815v2 [cs.LG] UPDATED)\nAbstract: Recent research has revealed that Graph Neural Networks (GNNs) are susceptible to adversarial attacks targeting the graph structure. A malicious attacker can manipulate a limited number of edges, given the training labels, to impair the victim model's performance. Previous empirical studies indicate that gradient-based attackers tend to add edges rather than remove them. In this paper, we present a theoretical demonstration revealing that attackers tend to increase inter-class edges due to the message passing mechanism of GNNs, which explains some previous empirical observations. By connecting dissimilar nodes, attackers can more effectively corrupt node features, making such attacks more advantageous. However, we demonstrate that the inherent smoothness of GNN's message passing tends to blur node dissimilarity in the feature space, leading to the loss of crucial information during the forward process. To address this issue, we propose a novel surrogate model with multi-level propagati",
    "path": "papers/22/08/2208.12815.json",
    "total_tokens": 986,
    "translated_title": "当攻击图形结构时，梯度告诉我们什么",
    "translated_abstract": "最近的研究表明，图神经网络 (GNNs) 容易受到针对图形结构的对抗攻击。一个恶意攻击者可以在有限的边缘范围内，通过给出的训练标签，来破坏受害模型的性能。之前的经验性研究表明，基于梯度的攻击者更倾向于添加边缘而不是删除。本文提出了一个理论证明，揭示了攻击者倾向于增加类间边缘，这是由于 GNN 的消息传递机制，这也解释了之前的一些经验观察。通过连接不同类的节点，攻击者可以更有效地破坏节点特征，从而使此类攻击更具优势。但是，我们证明了 GNN 消息传递的固有平滑性会将特征空间中的节点差异模糊化，导致在前向过程中丢失关键信息。为了解决这个问题，我们提出了一个具有多级传播的新型代理模型。",
    "tldr": "本文研究了图神经网络中针对图形结构的对抗攻击，发现攻击者更倾向于增加类间边缘，通过连接不同类的节点来更有效地破坏节点特征。然而，GNN 的固有平滑性会导致在前向过程中丢失关键信息。为此，我们提出了一个具有多级传播的新型代理模型来解决这个问题。",
    "en_tdlr": "This paper investigates adversarial attacks targeting the graph structure in Graph Neural Networks (GNNs) and finds that attackers tend to increase inter-class edges to more effectively corrupt node features. However, GNN's inherent smoothness leads to the loss of crucial information during the forward process. To address this issue, the paper proposes a novel surrogate model with multi-level propagation."
}