{
    "title": "Learning to Learn to Predict Performance Regressions in Production at Meta. (arXiv:2208.04351v2 [cs.SE] UPDATED)",
    "abstract": "Catching and attributing code change-induced performance regressions in production is hard; predicting them beforehand, even harder. A primer on automatically learning to predict performance regressions in software, this article gives an account of the experiences we gained when researching and deploying an ML-based regression prediction pipeline at Meta. In this paper, we report on a comparative study with four ML models of increasing complexity, from (1) code-opaque, over (2) Bag of Words, (3) off-the-shelve Transformer-based, to (4) a bespoke Transformer-based model, coined SuperPerforator. Our investigation shows the inherent difficulty of the performance prediction problem, which is characterized by a large imbalance of benign onto regressing changes. Our results also call into question the general applicability of Transformer-based architectures for performance prediction: an off-the-shelve CodeBERT-based approach had surprisingly poor performance; our highly customized SuperPerf",
    "link": "http://arxiv.org/abs/2208.04351",
    "context": "Title: Learning to Learn to Predict Performance Regressions in Production at Meta. (arXiv:2208.04351v2 [cs.SE] UPDATED)\nAbstract: Catching and attributing code change-induced performance regressions in production is hard; predicting them beforehand, even harder. A primer on automatically learning to predict performance regressions in software, this article gives an account of the experiences we gained when researching and deploying an ML-based regression prediction pipeline at Meta. In this paper, we report on a comparative study with four ML models of increasing complexity, from (1) code-opaque, over (2) Bag of Words, (3) off-the-shelve Transformer-based, to (4) a bespoke Transformer-based model, coined SuperPerforator. Our investigation shows the inherent difficulty of the performance prediction problem, which is characterized by a large imbalance of benign onto regressing changes. Our results also call into question the general applicability of Transformer-based architectures for performance prediction: an off-the-shelve CodeBERT-based approach had surprisingly poor performance; our highly customized SuperPerf",
    "path": "papers/22/08/2208.04351.json",
    "total_tokens": 1025,
    "translated_title": "学习学习在Meta的生产中预测性能回归",
    "translated_abstract": "在生产环境中捕捉和归因于代码变更引起的性能回归是困难的；预测它们在前期更加困难。本文是关于自动学习预测软件性能回归的入门介绍，我们在Meta研究和部署了一个基于机器学习的回归预测流水线后获得了一些经验。本文报告了一个比较研究结果，包括四个逐渐增加复杂度的机器学习模型：(1) 模糊代码，(2) 词袋模型，(3) 预先训练好的Transformer，和(4) 自定义Transformer模型，名为SuperPerforator。我们的研究显示了性能预测问题的固有难度，这一问题的特点是良性变更对恶性回归变更数量的巨大不平衡。我们的结果还质疑了Transformer架构在性能预测上的普适性：一个预训练的CodeBERT方法表现出惊人的 poor 表现；我们高度自定义的SuperPerforator模型--采用高级技术，如无效代码移除、合成数据增强和随机权重平均--表现最佳。最后，我们总结了所学经验，并勾勒出未来研究的有趣方向。",
    "tldr": "本文介绍了在Meta研究和部署的基于机器学习的回归预测流程，研究结果显示性能预测问题的固有难度，SuperPerforator模型表现最佳。",
    "en_tdlr": "This article introduces a machine learning-based regression prediction pipeline researched and deployed at Meta, which has revealed the inherent difficulty of performance prediction and the superiority of the highly customized SuperPerforator model over Transformer-based methods."
}