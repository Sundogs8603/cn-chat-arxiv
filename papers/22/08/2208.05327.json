{
    "title": "Fast Offline Policy Optimization for Large Scale Recommendation. (arXiv:2208.05327v4 [cs.IR] UPDATED)",
    "abstract": "Personalised interactive systems such as recommender systems require selecting relevant items from massive catalogs dependent on context. Reward-driven offline optimisation of these systems can be achieved by a relaxation of the discrete problem resulting in policy learning or REINFORCE style learning algorithms. Unfortunately, this relaxation step requires computing a sum over the entire catalogue making the complexity of the evaluation of the gradient (and hence each stochastic gradient descent iterations) linear in the catalogue size. This calculation is untenable in many real world examples such as large catalogue recommender systems, severely limiting the usefulness of this method in practice. In this paper, we derive an approximation of these policy learning algorithms that scale logarithmically with the catalogue size. Our contribution is based upon combining three novel ideas: a new Monte Carlo estimate of the gradient of a policy, the self normalised importance sampling estima",
    "link": "http://arxiv.org/abs/2208.05327",
    "context": "Title: Fast Offline Policy Optimization for Large Scale Recommendation. (arXiv:2208.05327v4 [cs.IR] UPDATED)\nAbstract: Personalised interactive systems such as recommender systems require selecting relevant items from massive catalogs dependent on context. Reward-driven offline optimisation of these systems can be achieved by a relaxation of the discrete problem resulting in policy learning or REINFORCE style learning algorithms. Unfortunately, this relaxation step requires computing a sum over the entire catalogue making the complexity of the evaluation of the gradient (and hence each stochastic gradient descent iterations) linear in the catalogue size. This calculation is untenable in many real world examples such as large catalogue recommender systems, severely limiting the usefulness of this method in practice. In this paper, we derive an approximation of these policy learning algorithms that scale logarithmically with the catalogue size. Our contribution is based upon combining three novel ideas: a new Monte Carlo estimate of the gradient of a policy, the self normalised importance sampling estima",
    "path": "papers/22/08/2208.05327.json",
    "total_tokens": 868,
    "translated_abstract": "个性化交互系统（如推荐系统）需要根据上下文从海量目录中选择相关项目。通过对离线优化这些系统进行奖励驱动，可以实现策略学习或REINFORCE风格的学习算法。不幸的是，这个弱化的问题需要计算整个目录的总和，使得梯度的评估复杂度（因此每个随机梯度下降迭代）与目录大小成线性关系。这个计算在许多现实世界的例子（如大型目录推荐系统）中是不可行的，严重限制了该方法在实践中的使用价值。在本文中，我们推导出这些策略学习算法的一个近似值，其随着目录大小的对数尺度而增加。我们的贡献基于三个新颖的想法的结合：策略梯度的新蒙特卡罗估计、自标准化重要性抽样估计和中位值作为熵 Regularization。",
    "tldr": "该论文介绍了一种基于三个新颖的想法的算法，通过近似策略学习算法，使得大规模目录推荐系统的离线优化复杂度从线性关系变为对数尺度。",
    "en_tdlr": "This paper presents an algorithm based on three novel ideas that approximates the policy learning algorithms and reduces the complexity of offline optimization for large-scale catalog recommendation systems from linear to logarithmic scale."
}