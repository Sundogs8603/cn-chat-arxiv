{
    "title": "Robust Methods for High-Dimensional Linear Learning. (arXiv:2208.05447v2 [stat.ML] UPDATED)",
    "abstract": "We propose statistically robust and computationally efficient linear learning methods in the high-dimensional batch setting, where the number of features $d$ may exceed the sample size $n$. We employ, in a generic learning setting, two algorithms depending on whether the considered loss function is gradient-Lipschitz or not. Then, we instantiate our framework on several applications including vanilla sparse, group-sparse and low-rank matrix recovery. This leads, for each application, to efficient and robust learning algorithms, that reach near-optimal estimation rates under heavy-tailed distributions and the presence of outliers. For vanilla $s$-sparsity, we are able to reach the $s\\log (d)/n$ rate under heavy-tails and $\\eta$-corruption, at a computational cost comparable to that of non-robust analogs. We provide an efficient implementation of our algorithms in an open-source $\\mathtt{Python}$ library called $\\mathtt{linlearn}$, by means of which we carry out numerical experiments whi",
    "link": "http://arxiv.org/abs/2208.05447",
    "context": "Title: Robust Methods for High-Dimensional Linear Learning. (arXiv:2208.05447v2 [stat.ML] UPDATED)\nAbstract: We propose statistically robust and computationally efficient linear learning methods in the high-dimensional batch setting, where the number of features $d$ may exceed the sample size $n$. We employ, in a generic learning setting, two algorithms depending on whether the considered loss function is gradient-Lipschitz or not. Then, we instantiate our framework on several applications including vanilla sparse, group-sparse and low-rank matrix recovery. This leads, for each application, to efficient and robust learning algorithms, that reach near-optimal estimation rates under heavy-tailed distributions and the presence of outliers. For vanilla $s$-sparsity, we are able to reach the $s\\log (d)/n$ rate under heavy-tails and $\\eta$-corruption, at a computational cost comparable to that of non-robust analogs. We provide an efficient implementation of our algorithms in an open-source $\\mathtt{Python}$ library called $\\mathtt{linlearn}$, by means of which we carry out numerical experiments whi",
    "path": "papers/22/08/2208.05447.json",
    "total_tokens": 950,
    "translated_title": "模型训练中鲁棒性高的高维线性学习方法",
    "translated_abstract": "我们提出了一种高维批处理中具有统计鲁棒性和计算有效性的线性学习方法，其中特征数d可能超过样本数n。我们在通用学习设置中采用了两种算法，取决于所考虑的损失函数是否是梯度Lipschitz的。然后，我们将我们的框架实例化到几个应用程序上，包括香草稀疏，组稀疏和低秩矩阵恢复。这导致了每个应用程序的高效和鲁棒的学习算法，在重尾分布和异常值的情况下，达到接近最优的估计速率。对于香草$s$-稀疏，我们能够在重尾和$\\eta$-污染下达到$s\\log(d)/n$的速率，计算成本与非鲁棒模拟相当。我们提供了一个开源的$\\mathtt{Python}$库$\\mathtt{linlearn}$来实现我们的算法，通过这个库进行数值实验，证明了我们方法的有效性和可扩展性。",
    "tldr": "该论文提出了一种在高维批量训练中具有高度鲁棒性和计算效率的线性学习方法，在多个应用程序中均能达到接近最优的估计速率，并提供了一个开源的Python库进行实现。",
    "en_tdlr": "This paper proposes a highly robust and computationally efficient linear learning method for high-dimensional batch training, which can achieve near-optimal estimation rates in several applications, and provides an open-source Python library for implementation and numerical experiments."
}