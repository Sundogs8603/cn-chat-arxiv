{
    "title": "See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation. (arXiv:2208.00759v4 [cs.RO] UPDATED)",
    "abstract": "We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person-view images. In order to overcome the need for positioning, we train the sensors to encode and communicate relevant viewpoint information to the mobile robot, whose objective it is to use this information to navigate as efficiently as possible to the target. We overcome the challenge of enabling all the sensors (even those that cannot directly see the target) to predict the direction along the shortest path to the target by implementing a neighborhood-based feature aggregation module using a Graph Neural Network (GNN) architecture. In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Our results show that by using communication between the sensors and the robot, we achieve up to",
    "link": "http://arxiv.org/abs/2208.00759",
    "context": "Title: See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation. (arXiv:2208.00759v4 [cs.RO] UPDATED)\nAbstract: We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person-view images. In order to overcome the need for positioning, we train the sensors to encode and communicate relevant viewpoint information to the mobile robot, whose objective it is to use this information to navigate as efficiently as possible to the target. We overcome the challenge of enabling all the sensors (even those that cannot directly see the target) to predict the direction along the shortest path to the target by implementing a neighborhood-based feature aggregation module using a Graph Neural Network (GNN) architecture. In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Our results show that by using communication between the sensors and the robot, we achieve up to",
    "path": "papers/22/08/2208.00759.json",
    "total_tokens": 808,
    "translated_title": "看见机器人看不到的东西：学习协作感知进行视觉导航",
    "translated_abstract": "本文讨论了如何在一个未知环境中利用视觉传感器引导机器人到达目的地的问题。在缺乏全局定位信息的条件下，我们通过训练传感器编码和传递相关的视角信息给机器人，让机器人能够在使用第一视角图像的情况下尽可能高效地导航到目标。通过实现一个基于邻域的特征聚合模块，我们使用图神经网络（GNN）架构克服了让所有传感器甚至无法直接看到目标的传感器预测通向目标最短路方向的挑战。",
    "tldr": "使用图神经网络架构的邻域特征聚合模块实现了所有传感器间的通讯，解决了机器人视觉导航中缺乏全局定位信息的问题并实现了高效导航。"
}