{
    "title": "Masked Vision and Language Modeling for Multi-modal Representation Learning. (arXiv:2208.02131v2 [cs.CV] UPDATED)",
    "abstract": "In this paper, we study how to use masked signal modeling in vision and language (V+L) representation learning. Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, we propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. This is motivated by the nature of image-text paired data that both of the image and the text convey almost the same information but in different formats. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. Our experiments on various V+L tasks show that the proposed method, along with common V+L alignment losses, achieves state-of-the-art performance in the regime of millions of pre-training data. Also, we outperforms the other competitors by a significant margin in limited data scenarios.",
    "link": "http://arxiv.org/abs/2208.02131",
    "context": "Title: Masked Vision and Language Modeling for Multi-modal Representation Learning. (arXiv:2208.02131v2 [cs.CV] UPDATED)\nAbstract: In this paper, we study how to use masked signal modeling in vision and language (V+L) representation learning. Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, we propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. This is motivated by the nature of image-text paired data that both of the image and the text convey almost the same information but in different formats. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. Our experiments on various V+L tasks show that the proposed method, along with common V+L alignment losses, achieves state-of-the-art performance in the regime of millions of pre-training data. Also, we outperforms the other competitors by a significant margin in limited data scenarios.",
    "path": "papers/22/08/2208.02131.json",
    "total_tokens": 855,
    "translated_title": "多模态表示学习中的遮蔽视觉和语言建模",
    "translated_abstract": "本文研究了如何在视觉和语言（V + L）表示学习中使用遮蔽信号建模。我们提出了建立联合遮蔽视觉和语言建模，其中一个模态的遮蔽信号在另一个模态的帮助下进行重建。这是由图像文本配对数据的性质所驱动的，因为图像和文本都传达几乎相同的信息但以不同的格式呈现。一个模态的遮蔽信号重建以另一模态为条件也可以隐式地学习语言标记和图像补丁之间的跨模态对齐。我们在各种V + L任务上进行的实验表明，该方法连同常见的V + L对齐损失，在百万级别的预训练数据范围内取得了最先进的性能。此外，在有限的数据场景中，我们超过了其他竞争对手的表现。",
    "tldr": "本文提出了联合遮蔽视觉和语言建模，在跨模态对齐方面取得成果，并在百万级别的预训练数据范围内取得了最先进的性能。",
    "en_tdlr": "The paper proposes joint masked vision and language modeling for cross-modal alignment, achieving state-of-the-art performance in the regime of millions of pre-training data and outperforming competitors in limited data scenarios."
}