{
    "title": "Enhancement Encoding: A Novel Imbalanced Classification Approach via Encoding the Training Labels. (arXiv:2208.11056v2 [cs.LG] UPDATED)",
    "abstract": "Class imbalance, which is also called long-tailed distribution, is a common problem in classification tasks based on machine learning. If it happens, the minority data will be overwhelmed by the majority, which presents quite a challenge for data science. To address the class imbalance problem, researchers have proposed lots of methods: some people make the data set balanced (SMOTE), some others refine the loss function (Focal Loss), and even someone has noticed the value of labels influences class-imbalanced learning (Yang and Xu. Rethinking the value of labels for improving class-imbalanced learning. In NeurIPS 2020), but no one changes the way to encode the labels of data yet. Nowadays, the most prevailing technique to encode labels is the one-hot encoding due to its nice performance in the general situation. However, it is not a good choice for imbalanced data, because the classifier will treat majority and minority samples equally. In this paper, we innovatively propose the enhanc",
    "link": "http://arxiv.org/abs/2208.11056",
    "context": "Title: Enhancement Encoding: A Novel Imbalanced Classification Approach via Encoding the Training Labels. (arXiv:2208.11056v2 [cs.LG] UPDATED)\nAbstract: Class imbalance, which is also called long-tailed distribution, is a common problem in classification tasks based on machine learning. If it happens, the minority data will be overwhelmed by the majority, which presents quite a challenge for data science. To address the class imbalance problem, researchers have proposed lots of methods: some people make the data set balanced (SMOTE), some others refine the loss function (Focal Loss), and even someone has noticed the value of labels influences class-imbalanced learning (Yang and Xu. Rethinking the value of labels for improving class-imbalanced learning. In NeurIPS 2020), but no one changes the way to encode the labels of data yet. Nowadays, the most prevailing technique to encode labels is the one-hot encoding due to its nice performance in the general situation. However, it is not a good choice for imbalanced data, because the classifier will treat majority and minority samples equally. In this paper, we innovatively propose the enhanc",
    "path": "papers/22/08/2208.11056.json",
    "total_tokens": 1133,
    "translated_title": "增强编码：通过对训练标签进行编码的新型不平衡分类方法。",
    "translated_abstract": "类别不平衡，也称为长尾分布，在基于机器学习的分类任务中是常见问题。如果出现类别不平衡，那么少数类数据将被多数类淹没，这对数据科学提出了巨大挑战。为了解决类别不平衡问题，研究人员提出了许多方法：一些人使数据集平衡（SMOTE），一些人改进损失函数（Focal Loss），甚至有人注意到标签似乎影响类别不平衡学习（Yang和Xu。重塑标记的价值，以提高类别不平衡学习。在NeurIPS 2020中），但还没有人改变数据标签的编码方式。目前，编码标签的最流行技术是一位编码，因为它在一般情况下具有良好的性能。但对于不平衡的数据，它并不是一个好选择，因为分类器将平等对待多数和少数样本。本文创新性地提出了增强编码方法来解决类别不平衡问题。具体地，我们利用标签的值为多数和少数样本生成不同的编码向量，这可以在模型训练期间有效平衡不同样本的贡献。在几个基准数据集上的实验证明，所提出的方法在准确性、F1得分和曲线下面积（AUC）方面优于最先进的方法。",
    "tldr": "本文提出了一种增强编码方法来解决类别不平衡问题，该方法可以根据数据集的不同特点对不同的样本进行编码生成不同的编码向量，实验结果表明该方法在几个基准数据集上优于最先进的方法。",
    "en_tdlr": "This paper proposes an enhanced encoding approach to address the class imbalance problem in classification tasks by generating different encoded vectors for minority and majority samples using label values. Experiments show that this method outperforms state-of-the-art approaches on several benchmark datasets in terms of accuracy, F1 score, and AUC."
}