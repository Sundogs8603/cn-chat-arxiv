{
    "title": "FedOBD: Opportunistic Block Dropout for Efficiently Training Large-scale Neural Networks through Federated Learning. (arXiv:2208.05174v4 [cs.LG] UPDATED)",
    "abstract": "Large-scale neural networks possess considerable expressive power. They are well-suited for complex learning tasks in industrial applications. However, large-scale models pose significant challenges for training under the current Federated Learning (FL) paradigm. Existing approaches for efficient FL training often leverage model parameter dropout. However, manipulating individual model parameters is not only inefficient in meaningfully reducing the communication overhead when training large-scale FL models, but may also be detrimental to the scaling efforts and model performance as shown by recent research. To address these issues, we propose the Federated Opportunistic Block Dropout (FedOBD) approach. The key novelty is that it decomposes large-scale models into semantic blocks so that FL participants can opportunistically upload quantized blocks, which are deemed to be significant towards training the model, to the FL server for aggregation. Extensive experiments evaluating FedOBD ag",
    "link": "http://arxiv.org/abs/2208.05174",
    "context": "Title: FedOBD: Opportunistic Block Dropout for Efficiently Training Large-scale Neural Networks through Federated Learning. (arXiv:2208.05174v4 [cs.LG] UPDATED)\nAbstract: Large-scale neural networks possess considerable expressive power. They are well-suited for complex learning tasks in industrial applications. However, large-scale models pose significant challenges for training under the current Federated Learning (FL) paradigm. Existing approaches for efficient FL training often leverage model parameter dropout. However, manipulating individual model parameters is not only inefficient in meaningfully reducing the communication overhead when training large-scale FL models, but may also be detrimental to the scaling efforts and model performance as shown by recent research. To address these issues, we propose the Federated Opportunistic Block Dropout (FedOBD) approach. The key novelty is that it decomposes large-scale models into semantic blocks so that FL participants can opportunistically upload quantized blocks, which are deemed to be significant towards training the model, to the FL server for aggregation. Extensive experiments evaluating FedOBD ag",
    "path": "papers/22/08/2208.05174.json",
    "total_tokens": 913,
    "translated_title": "FedOBD：机会主义块丢弃在联邦学习中高效训练大型神经网络",
    "translated_abstract": "大型神经网络具有相当的表达能力，非常适合行业应用中的复杂学习任务。然而，大型模型在当前的联邦学习范式下训练存在显著的挑战。现有的高效联邦学习训练方法通常利用模型参数dropout，但单独操作模型参数不仅在大规模联邦学习模型训练中无法有效减少通信开销，而且还可能对缩放和模型性能产生不利影响。为了解决这些问题，我们提出了联邦机会主义块丢弃（FedOBD）方法。其关键创新点在于将大型模型分解为语义块，以便联邦学习参与者可以机会主义地上传量化的块，这些块被认为对于训练模型非常重要，以进行聚合。通过大量的实验证明了FedOBD的有效性。",
    "tldr": "FedOBD是一个新的联邦学习方法，它通过将大型模型分解为语义块，允许参与者机会主义地上传量化的块来进行聚合，以此解决了在大规模联邦学习模型训练过程中操作模型参数所产生的通信开销和性能问题。",
    "en_tdlr": "FedOBD is a new federated learning method that decomposes large-scale models into semantic blocks, allowing participants to opportunistically upload quantized blocks for aggregation, thereby addressing the communication overhead and performance issues associated with manipulating model parameters during training of large-scale FL models."
}