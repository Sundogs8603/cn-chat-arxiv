{
    "title": "Federated Learning of Large Models at the Edge via Principal Sub-Model Training. (arXiv:2208.13141v3 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) is emerging as a popular, promising decentralized learning framework that enables collaborative training among clients, with no need to share private data between them or to a centralized server. However, considering many edge clients do not have sufficient computing, memory, or communication capabilities, federated learning of large models still faces significant bottlenecks. To keep such weak but crucial clients in the loop, prior works either consider a heterogeneous-client setting where clients train models with different sizes; or offload training to the server. However, the heterogeneous-client setting requires some clients to train full model, which is not aligned with the resource-constrained setting; while the latter ones break privacy promises in FL when sharing intermediate representations or labels with the server. To overcome these limitations, in this work, we formulate a realistic, but much less explored, cross-device FL setting in which no client",
    "link": "http://arxiv.org/abs/2208.13141",
    "context": "Title: Federated Learning of Large Models at the Edge via Principal Sub-Model Training. (arXiv:2208.13141v3 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) is emerging as a popular, promising decentralized learning framework that enables collaborative training among clients, with no need to share private data between them or to a centralized server. However, considering many edge clients do not have sufficient computing, memory, or communication capabilities, federated learning of large models still faces significant bottlenecks. To keep such weak but crucial clients in the loop, prior works either consider a heterogeneous-client setting where clients train models with different sizes; or offload training to the server. However, the heterogeneous-client setting requires some clients to train full model, which is not aligned with the resource-constrained setting; while the latter ones break privacy promises in FL when sharing intermediate representations or labels with the server. To overcome these limitations, in this work, we formulate a realistic, but much less explored, cross-device FL setting in which no client",
    "path": "papers/22/08/2208.13141.json",
    "total_tokens": 909,
    "translated_title": "在边缘进行主体子模型训练的大型模型联邦学习",
    "translated_abstract": "联邦学习（FL）是一种新兴的分散式学习框架，可以实现客户端之间的协作训练，无需共享私有数据，也无需传输至中心服务器。然而，考虑到许多边缘客户端缺乏足够的计算、内存或通信能力，联邦学习大规模模型仍面临重大瓶颈。为了保持弱但重要的客户端参与，之前的工作要么考虑了异构客户端设置，其中客户端使用不同大小的模型进行训练；要么将训练任务转移到服务端。然而，异构客户端设置要求某些客户端训练完整模型，这与资源受限的设置不一致；而后者在与服务器共享中间表示或标签时违反了FL的隐私承诺。为了克服这些限制，在这项工作中，我们在现实中提出了一个更少探索的跨设备FL设置，其中没有任何客户端需要训练完整模型。",
    "tldr": "本文提出了一种解决资源受限边缘设备下大型模型联邦学习的方法，通过主体子模型训练，在不违反隐私承诺的前提下，使弱但重要的客户端能够参与到协作训练中。",
    "en_tdlr": "This paper proposes a method for federated learning of large models on resource-constrained edge devices, using principal sub-model training to enable the participation of weak but important clients in collaborative training without violating privacy promises."
}