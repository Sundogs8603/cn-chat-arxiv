{
    "title": "Self-supervised Contrastive Representation Learning for Semi-supervised Time-Series Classification. (arXiv:2208.06616v3 [cs.LG] UPDATED)",
    "abstract": "Learning time-series representations when only unlabeled data or few labeled samples are available can be a challenging task. Recently, contrastive self-supervised learning has shown great improvement in extracting useful representations from unlabeled data via contrasting different augmented views of data. In this work, we propose a novel Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC) that learns representations from unlabeled data with contrastive learning. Specifically, we propose time-series-specific weak and strong augmentations and use their views to learn robust temporal relations in the proposed temporal contrasting module, besides learning discriminative representations by our proposed contextual contrasting module. Additionally, we conduct a systematic study of time-series data augmentation selection, which is a key part of contrastive learning. We also extend TS-TCC to the semi-supervised learning settings and propose a Class-A",
    "link": "http://arxiv.org/abs/2208.06616",
    "context": "Title: Self-supervised Contrastive Representation Learning for Semi-supervised Time-Series Classification. (arXiv:2208.06616v3 [cs.LG] UPDATED)\nAbstract: Learning time-series representations when only unlabeled data or few labeled samples are available can be a challenging task. Recently, contrastive self-supervised learning has shown great improvement in extracting useful representations from unlabeled data via contrasting different augmented views of data. In this work, we propose a novel Time-Series representation learning framework via Temporal and Contextual Contrasting (TS-TCC) that learns representations from unlabeled data with contrastive learning. Specifically, we propose time-series-specific weak and strong augmentations and use their views to learn robust temporal relations in the proposed temporal contrasting module, besides learning discriminative representations by our proposed contextual contrasting module. Additionally, we conduct a systematic study of time-series data augmentation selection, which is a key part of contrastive learning. We also extend TS-TCC to the semi-supervised learning settings and propose a Class-A",
    "path": "papers/22/08/2208.06616.json",
    "total_tokens": 884,
    "translated_title": "自监督对比表示学习用于半监督时间序列分类",
    "translated_abstract": "当只有未标记数据或少量标记样本可用时，学习时间序列表示是一项具有挑战性的任务。最近，对比自监督学习通过对数据的不同增强视图进行对比，在从未标记数据中提取有用表示方面取得了巨大的改进。在这项工作中，我们提出了一种新的时间序列表示学习框架，即基于时间和上下文对比的时间序列对比（TS-TCC），通过对比学习从未标记数据中提取表示。具体而言，我们提出了适用于时间序列的弱增强和强增强，并使用它们的视图在提出的时间对比模块中学习强大的时间关系，此外还通过我们提出的上下文对比模块学习有区分力的表示。此外，我们对时间序列数据增强选择进行了系统研究，这是对比学习的关键部分。我们还将TS-TCC推广到半监督学习设置，并提出了一种Class-A。",
    "tldr": "这项工作提出了一种新的自监督对比表示学习框架，通过对比学习从未标记的时间序列数据中提取有用的表示。同时，研究了时间序列数据增强选择，并将该框架扩展到半监督学习设置。",
    "en_tdlr": "This paper proposes a novel self-supervised contrastive representation learning framework for extracting useful representations from unlabeled time-series data. It also explores time-series data augmentation selection and extends the framework to semi-supervised learning settings."
}