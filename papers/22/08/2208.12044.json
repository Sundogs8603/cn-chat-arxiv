{
    "title": "Fed-FSNet: Mitigating Non-I.I.D. Federated Learning via Fuzzy Synthesizing Network. (arXiv:2208.12044v2 [cs.CR] UPDATED)",
    "abstract": "Federated learning (FL) has emerged as a promising privacy-preserving distributed machine learning framework recently. It aims at collaboratively learning a shared global model by performing distributed training locally on edge devices and aggregating local models into a global one without centralized raw data sharing in the cloud server. However, due to the large local data heterogeneities (Non-I.I.D. data) across edge devices, the FL may easily obtain a global model that can produce more shifted gradients on local datasets, thereby degrading the model performance or even suffering from the non-convergence during training. In this paper, we propose a novel FL training framework, dubbed Fed-FSNet, using a properly designed Fuzzy Synthesizing Network (FSNet) to mitigate the Non-I.I.D. FL at-the-source. Concretely, we maintain an edge-agnostic hidden model in the cloud server to estimate a less-accurate while direction-aware inversion of the global model. The hidden model can then fuzzil",
    "link": "http://arxiv.org/abs/2208.12044",
    "context": "Title: Fed-FSNet: Mitigating Non-I.I.D. Federated Learning via Fuzzy Synthesizing Network. (arXiv:2208.12044v2 [cs.CR] UPDATED)\nAbstract: Federated learning (FL) has emerged as a promising privacy-preserving distributed machine learning framework recently. It aims at collaboratively learning a shared global model by performing distributed training locally on edge devices and aggregating local models into a global one without centralized raw data sharing in the cloud server. However, due to the large local data heterogeneities (Non-I.I.D. data) across edge devices, the FL may easily obtain a global model that can produce more shifted gradients on local datasets, thereby degrading the model performance or even suffering from the non-convergence during training. In this paper, we propose a novel FL training framework, dubbed Fed-FSNet, using a properly designed Fuzzy Synthesizing Network (FSNet) to mitigate the Non-I.I.D. FL at-the-source. Concretely, we maintain an edge-agnostic hidden model in the cloud server to estimate a less-accurate while direction-aware inversion of the global model. The hidden model can then fuzzil",
    "path": "papers/22/08/2208.12044.json",
    "total_tokens": 898,
    "translated_title": "Fed-FSNet: 通过模糊合成网络缓解非独立同分布联邦学习问题",
    "translated_abstract": "联邦学习是一种最近出现的有前途的隐私保护分布式机器学习框架，它旨在通过在边缘设备上进行分布式训练，将本地模型聚合成全局模型，而无需在云服务器上共享中心化原始数据。但是，由于边缘设备之间存在着大量本地数据异质性（非独立同分布数据），因此联邦学习很容易获得一个能够在本地数据集上产生更多偏移梯度的全局模型，从而降低模型性能甚至在训练过程中无法收敛。在本文中，我们提出了一种新的FL训练框架，称为Fed-FSNet，它使用一个适当设计的模糊合成网络（FSNet）来缓解非独立同分布FL问题。",
    "tldr": "本文提出一种名为Fed-FSNet的新型联邦学习训练框架，通过使用模糊合成网络（FSNet）在源头处缓解非独立同分布问题，从而提高全局模型的性能表现。",
    "en_tdlr": "This paper proposes a novel federated learning training framework called Fed-FSNet, which uses a properly designed Fuzzy Synthesizing Network (FSNet) to mitigate non-I.I.D. problems at the source, thereby improving the performance of the global model."
}