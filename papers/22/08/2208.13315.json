{
    "title": "ANAct: Adaptive Normalization for Activation Functions",
    "abstract": "In this paper, we investigate the negative effect of activation functions on forward and backward propagation and how to counteract this effect. First, We examine how activation functions affect the forward and backward propagation of neural networks and derive a general form for gradient variance that extends the previous work in this area. We try to use mini-batch statistics to dynamically update the normalization factor to ensure the normalization property throughout the training process, rather than only accounting for the state of the neural network after weight initialization. Second, we propose ANAct, a method that normalizes activation functions to maintain consistent gradient variance across layers and demonstrate its effectiveness through experiments. We observe that the convergence rate is roughly related to the normalization property. We compare ANAct with several common activation functions on CNNs and residual networks and show that ANAct consistently improves their perfo",
    "link": "https://arxiv.org/abs/2208.13315",
    "context": "Title: ANAct: Adaptive Normalization for Activation Functions\nAbstract: In this paper, we investigate the negative effect of activation functions on forward and backward propagation and how to counteract this effect. First, We examine how activation functions affect the forward and backward propagation of neural networks and derive a general form for gradient variance that extends the previous work in this area. We try to use mini-batch statistics to dynamically update the normalization factor to ensure the normalization property throughout the training process, rather than only accounting for the state of the neural network after weight initialization. Second, we propose ANAct, a method that normalizes activation functions to maintain consistent gradient variance across layers and demonstrate its effectiveness through experiments. We observe that the convergence rate is roughly related to the normalization property. We compare ANAct with several common activation functions on CNNs and residual networks and show that ANAct consistently improves their perfo",
    "path": "papers/22/08/2208.13315.json",
    "total_tokens": 861,
    "translated_title": "ANAct: 自适应归一化的激活函数",
    "translated_abstract": "本文研究了激活函数对前向和反向传播的负面影响，以及如何抵消这种影响。首先，我们考察了激活函数对神经网络前向和反向传播的影响，并推导出了梯度差异的一般形式，扩展了此领域的先前工作。我们尝试使用小批量统计来动态更新归一化因子，以确保在训练过程中始终保持归一化属性，而不仅仅考虑权重初始化后的神经网络状态。其次，我们提出了ANAct方法，该方法对激活函数进行归一化，以在各层之间保持一致的梯度差异，并通过实验证明了其有效性。我们观察到收敛速度与归一化属性大致相关。我们将ANAct与几种常见的激活函数在卷积神经网络和残差网络上进行比较，并显示ANAct可以持续改善它们的性能。",
    "tldr": "本文研究了激活函数对神经网络前向和反向传播的负面影响，并提出了一种自适应归一化的激活函数方法ANAct来保持一致的梯度差异，通过实验证明了其有效性。"
}