{
    "title": "Reduced Implication-bias Logic Loss for Neuro-Symbolic Learning. (arXiv:2208.06838v2 [cs.AI] UPDATED)",
    "abstract": "Integrating logical reasoning and machine learning by approximating logical inference with differentiable operators is a widely used technique in Neuro-Symbolic systems.  However, some differentiable operators could bring a significant bias during backpropagation and degrade the performance of Neuro-Symbolic learning.  In this paper, we reveal that this bias, named \\textit{Implication Bias} is common in loss functions derived from fuzzy logic operators.  Furthermore, we propose a simple yet effective method to transform the biased loss functions into \\textit{Reduced Implication-bias Logic Loss (RILL)} to address the above problem.  Empirical study shows that RILL can achieve significant improvements compared with the biased logic loss functions, especially when the knowledge base is incomplete, and keeps more robust than the compared methods when labelled data is insufficient.",
    "link": "http://arxiv.org/abs/2208.06838",
    "context": "Title: Reduced Implication-bias Logic Loss for Neuro-Symbolic Learning. (arXiv:2208.06838v2 [cs.AI] UPDATED)\nAbstract: Integrating logical reasoning and machine learning by approximating logical inference with differentiable operators is a widely used technique in Neuro-Symbolic systems.  However, some differentiable operators could bring a significant bias during backpropagation and degrade the performance of Neuro-Symbolic learning.  In this paper, we reveal that this bias, named \\textit{Implication Bias} is common in loss functions derived from fuzzy logic operators.  Furthermore, we propose a simple yet effective method to transform the biased loss functions into \\textit{Reduced Implication-bias Logic Loss (RILL)} to address the above problem.  Empirical study shows that RILL can achieve significant improvements compared with the biased logic loss functions, especially when the knowledge base is incomplete, and keeps more robust than the compared methods when labelled data is insufficient.",
    "path": "papers/22/08/2208.06838.json",
    "total_tokens": 947,
    "translated_title": "减少蕴含偏差逻辑损失用于神经符号学习",
    "translated_abstract": "通过使用可微分算子来近似逻辑推理，将逻辑推理与机器学习相结合是神经符号系统中广泛使用的技术。然而，某些可微分算子在反向传播过程中可能带来显著的偏差，并降低神经符号学习的性能。本文揭示了这种偏差，称之为“蕴含偏差”，常见于从模糊逻辑算子中派生的损失函数。此外，我们提出了一种简单而有效的方法，将有偏差的损失函数转化为“减少蕴含偏差逻辑损失（RILL）”，以解决上述问题。实证研究表明，与有偏差的逻辑损失函数相比，RILL在知识库不完整时可以取得显著改进，并在标记数据不足时保持更为稳健。",
    "tldr": "本文提出了一种减少蕴含偏差逻辑损失（RILL）的方法，用于解决在神经符号学习中由于从模糊逻辑算子中派生的损失函数带来的偏差问题。实证研究表明，RILL相比有偏差的逻辑损失函数在知识库不完整和标记数据不足时具有显著的改进和更强的稳健性。",
    "en_tdlr": "This paper proposes a method called Reduced Implication-bias Logic Loss (RILL) to address the bias issue caused by loss functions derived from fuzzy logic operators in neuro-symbolic learning. Empirical study shows that RILL achieves significant improvements and better robustness compared to biased logic loss functions, especially when the knowledge base is incomplete and labeled data is insufficient."
}