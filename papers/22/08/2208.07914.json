{
    "title": "PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm. (arXiv:2208.07914v3 [cs.LG] UPDATED)",
    "abstract": "Multi-objective reinforcement learning (MORL) approaches have emerged to tackle many real-world problems with multiple conflicting objectives by maximizing a joint objective function weighted by a preference vector. These approaches find fixed customized policies corresponding to preference vectors specified during training. However, the design constraints and objectives typically change dynamically in real-life scenarios. Furthermore, storing a policy for each potential preference is not scalable. Hence, obtaining a set of Pareto front solutions for the entire preference space in a given domain with a single training is critical. To this end, we propose a novel MORL algorithm that trains a single universal network to cover the entire preference space scalable to continuous robotic tasks. The proposed approach, Preference-Driven MORL (PD-MORL), utilizes the preferences as guidance to update the network parameters. It also employs a novel parallelization approach to increase sample effi",
    "link": "http://arxiv.org/abs/2208.07914",
    "context": "Title: PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm. (arXiv:2208.07914v3 [cs.LG] UPDATED)\nAbstract: Multi-objective reinforcement learning (MORL) approaches have emerged to tackle many real-world problems with multiple conflicting objectives by maximizing a joint objective function weighted by a preference vector. These approaches find fixed customized policies corresponding to preference vectors specified during training. However, the design constraints and objectives typically change dynamically in real-life scenarios. Furthermore, storing a policy for each potential preference is not scalable. Hence, obtaining a set of Pareto front solutions for the entire preference space in a given domain with a single training is critical. To this end, we propose a novel MORL algorithm that trains a single universal network to cover the entire preference space scalable to continuous robotic tasks. The proposed approach, Preference-Driven MORL (PD-MORL), utilizes the preferences as guidance to update the network parameters. It also employs a novel parallelization approach to increase sample effi",
    "path": "papers/22/08/2208.07914.json",
    "total_tokens": 1000,
    "translated_title": "基于偏好的多目标强化学习算法：PD-MORL",
    "translated_abstract": "多目标强化学习（MORL）方法通过最大化由偏好向量加权的联合目标函数，针对多个冲突目标的实际问题进行了处理。然而，在现实情况下，设计约束和目标通常会动态变化。此外，存储每个潜在偏好的策略是不可扩展的。因此，在给定域中使用单次训练获取整个偏好空间的帕累托前沿解集是至关重要的。为此，我们提出了一种新的MORL算法，训练一个单一的通用网络以覆盖整个偏好空间，并可扩展到连续的机器人任务。所提出的方法“基于偏好的MORL（PD-MORL）”利用偏好以更新网络参数，还采用了一种新颖的并行化方法以提高样本效率并适应当前情况下的各种偏好空间的目标。我们在多目标机器人控制任务上评估PD-MORL，并表明它在帕累托前沿覆盖和样本效率方面优于现有的MORL方法。",
    "tldr": "PD-MORL是一种基于偏好的多目标强化学习算法，其采用偏好作为指导，适应于各种偏好空间的目标，可扩展到连续的机器人任务，并在多目标机器人控制任务上优于现有的MORL方法。",
    "en_tdlr": "PD-MORL is a preference-driven multi-objective reinforcement learning algorithm that uses preferences as guidance, adapts to various preference spaces, scalable to continuous robotic tasks, and outperforms existing MORL approaches in multi-objective robotic control tasks in terms of Pareto front coverage and sample efficiency."
}