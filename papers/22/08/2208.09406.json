{
    "title": "Dance Style Transfer with Cross-modal Transformer. (arXiv:2208.09406v3 [cs.LG] UPDATED)",
    "abstract": "We present CycleDance, a dance style transfer system to transform an existing motion clip in one dance style to a motion clip in another dance style while attempting to preserve motion context of the dance. Our method extends an existing CycleGAN architecture for modeling audio sequences and integrates multimodal transformer encoders to account for music context. We adopt sequence length-based curriculum learning to stabilize training. Our approach captures rich and long-term intra-relations between motion frames, which is a common challenge in motion transfer and synthesis work. We further introduce new metrics for gauging transfer strength and content preservation in the context of dance movements. We perform an extensive ablation study as well as a human study including 30 participants with 5 or more years of dance experience. The results demonstrate that CycleDance generates realistic movements with the target style, significantly outperforming the baseline CycleGAN on naturalness,",
    "link": "http://arxiv.org/abs/2208.09406",
    "context": "Title: Dance Style Transfer with Cross-modal Transformer. (arXiv:2208.09406v3 [cs.LG] UPDATED)\nAbstract: We present CycleDance, a dance style transfer system to transform an existing motion clip in one dance style to a motion clip in another dance style while attempting to preserve motion context of the dance. Our method extends an existing CycleGAN architecture for modeling audio sequences and integrates multimodal transformer encoders to account for music context. We adopt sequence length-based curriculum learning to stabilize training. Our approach captures rich and long-term intra-relations between motion frames, which is a common challenge in motion transfer and synthesis work. We further introduce new metrics for gauging transfer strength and content preservation in the context of dance movements. We perform an extensive ablation study as well as a human study including 30 participants with 5 or more years of dance experience. The results demonstrate that CycleDance generates realistic movements with the target style, significantly outperforming the baseline CycleGAN on naturalness,",
    "path": "papers/22/08/2208.09406.json",
    "total_tokens": 936,
    "translated_title": "跨模态变换器进行舞蹈风格转换",
    "translated_abstract": "我们提出了一种名为CycleDance的舞蹈风格转换系统，可将一种舞蹈风格中的动作转换为另一种舞蹈风格中的动作，并尝试保留舞蹈的动态上下文。我们的方法扩展了现有的CycleGAN架构，用于建模音频序列，并集成了多模态变换器编码器来考虑音乐上下文。我们采用基于序列长度的课程学习来稳定训练。我们的方法可以捕捉动态帧之间丰富而长期的内在关系，这是动态转移和合成工作中的常见挑战。在舞蹈运动背景下，我们进一步介绍了评估转移强度和内容保留的新指标。我们进行了广泛的削减研究和人类研究，包括30名具有5年或更多舞蹈经验的参与者。结果表明，CycleDance可以生成具有目标风格的逼真动作，其自然程度显著优于基线CycleGAN。",
    "tldr": "提出了一种名为CycleDance的舞蹈风格转换系统，通过跨模态变换器编码器，并采用基于序列长度的课程学习和新指标对舞蹈运动进行转移和合成 ，能够实现逼真的舞蹈风格转换。"
}