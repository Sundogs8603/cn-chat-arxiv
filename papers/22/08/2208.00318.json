{
    "title": "Smoothing Entailment Graphs with Language Models. (arXiv:2208.00318v2 [cs.CL] UPDATED)",
    "abstract": "The diversity and Zipfian frequency distribution of natural language predicates in corpora leads to sparsity in Entailment Graphs (EGs) built by Open Relation Extraction (ORE). EGs are computationally efficient and explainable models of natural language inference, but as symbolic models, they fail if a novel premise or hypothesis vertex is missing at test-time. We present theory and methodology for overcoming such sparsity in symbolic models. First, we introduce a theory of optimal smoothing of EGs by constructing transitive chains. We then demonstrate an efficient, open-domain, and unsupervised smoothing method using an off-the-shelf Language Model to find approximations of missing premise predicates. This improves recall by 25.1 and 16.3 percentage points on two difficult directional entailment datasets, while raising average precision and maintaining model explainability. Further, in a QA task we show that EG smoothing is most useful for answering questions with lesser supporting te",
    "link": "http://arxiv.org/abs/2208.00318",
    "context": "Title: Smoothing Entailment Graphs with Language Models. (arXiv:2208.00318v2 [cs.CL] UPDATED)\nAbstract: The diversity and Zipfian frequency distribution of natural language predicates in corpora leads to sparsity in Entailment Graphs (EGs) built by Open Relation Extraction (ORE). EGs are computationally efficient and explainable models of natural language inference, but as symbolic models, they fail if a novel premise or hypothesis vertex is missing at test-time. We present theory and methodology for overcoming such sparsity in symbolic models. First, we introduce a theory of optimal smoothing of EGs by constructing transitive chains. We then demonstrate an efficient, open-domain, and unsupervised smoothing method using an off-the-shelf Language Model to find approximations of missing premise predicates. This improves recall by 25.1 and 16.3 percentage points on two difficult directional entailment datasets, while raising average precision and maintaining model explainability. Further, in a QA task we show that EG smoothing is most useful for answering questions with lesser supporting te",
    "path": "papers/22/08/2208.00318.json",
    "total_tokens": 922,
    "translated_title": "使用语言模型平滑蕴含图",
    "translated_abstract": "自然语言谓词在语料库中的多样性和Zipf分布导致通过开放关系抽取构建的蕴含图（EGs）的稀疏性。EGs是计算高效和可解释的自然语言推理模型，但作为符号模型，如果测试时缺少新的前提或假设顶点，它们会失败。我们提出了一种克服这种符号模型稀疏性的理论和方法。首先，我们引入了一种通过构建传递链条来构建EGs的最优平滑理论。然后，我们使用一个现成的语言模型来找到丢失的前提谓词的近似，展示了一种高效、开放域和无监督的平滑方法。在两个困难的定向蕴含数据集上，这提高了25.1和16.3个百分点的召回率，并提高了平均精度并保持模型的可解释性。此外，在一个QA任务中，我们展示了EG平滑在回答支持较少的问题时的最实用性。",
    "tldr": "本文提出了一种使用语言模型平滑蕴含图的方法，通过构建传递链条和使用现成的语言模型找到丢失的前提谓词的近似，可以提高自然语言推理模型的召回率和平均精度，并保持模型的可解释性。",
    "en_tdlr": "This paper proposes a method for smoothing entailment graphs using language models. By constructing transitive chains and finding approximations of missing premise predicates using an off-the-shelf language model, it improves the recall and average precision of natural language inference models while maintaining model explainability."
}