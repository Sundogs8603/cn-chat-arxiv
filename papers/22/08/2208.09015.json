{
    "title": "Treeformer: Dense Gradient Trees for Efficient Attention Computation. (arXiv:2208.09015v2 [cs.CL] UPDATED)",
    "abstract": "Standard inference and training with transformer based architectures scale quadratically with input sequence length. This is prohibitively large for a variety of applications especially in web-page translation, query-answering etc. Consequently, several approaches have been developed recently to speedup attention computation by enforcing different attention structures such as sparsity, low-rank, approximating attention using kernels. In this work, we view attention computation as that of nearest neighbor retrieval, and use decision tree based hierarchical navigation to reduce the retrieval cost per query token from linear in sequence length to nearly logarithmic. Based on such hierarchical navigation, we design Treeformer which can use one of two efficient attention layers -- TF-Attention and TC-Attention. TF-Attention computes the attention in a fine-grained style, while TC-Attention is a coarse attention layer which also ensures that the gradients are \"dense\". To optimize such challe",
    "link": "http://arxiv.org/abs/2208.09015",
    "context": "Title: Treeformer: Dense Gradient Trees for Efficient Attention Computation. (arXiv:2208.09015v2 [cs.CL] UPDATED)\nAbstract: Standard inference and training with transformer based architectures scale quadratically with input sequence length. This is prohibitively large for a variety of applications especially in web-page translation, query-answering etc. Consequently, several approaches have been developed recently to speedup attention computation by enforcing different attention structures such as sparsity, low-rank, approximating attention using kernels. In this work, we view attention computation as that of nearest neighbor retrieval, and use decision tree based hierarchical navigation to reduce the retrieval cost per query token from linear in sequence length to nearly logarithmic. Based on such hierarchical navigation, we design Treeformer which can use one of two efficient attention layers -- TF-Attention and TC-Attention. TF-Attention computes the attention in a fine-grained style, while TC-Attention is a coarse attention layer which also ensures that the gradients are \"dense\". To optimize such challe",
    "path": "papers/22/08/2208.09015.json",
    "total_tokens": 915,
    "translated_title": "Treeformer: 稠密梯度树实现高效注意力计算",
    "translated_abstract": "基于Transformer架构的推理和训练的时间复杂度和输入序列长度成二次关系，这对于一些应用如网页翻译和查询-回答等速度要求较高的应用是不可行的。因此，为了加速注意力计算，近期提出了多种方法，如强制使用不同的注意力结构，如稀疏、低秩、使用核函数逼近。本文提出一种将注意力计算看作最近邻检索的方法，利用基于决策树的分层导航，将每个查询标记的检索成本从线性降为近似对数级别。基于这种分层导航，我们设计出Treeformer，它可以使用两种有效的关注层-- TF-Attention和TC-Attention。TF-Attention以细粒度方式计算其中的关注，而TC-Attention是一种更粗粒度的关注层，也确保梯度是“密集”的。",
    "tldr": "本文提出了Treeformer，一种基于决策树的分层导航方法，用于高效地计算注意力。与传统的注意力计算方法相比，Treeformer 可以将检索成本从线性降为近似对数级别，并提供两种有效的关注层。算法的目标是处理输入序列长度过长的应用，提高计算效率。"
}