{
    "title": "Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning. (arXiv:2208.06193v3 [cs.LG] UPDATED)",
    "abstract": "Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Q-learning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness",
    "link": "http://arxiv.org/abs/2208.06193",
    "context": "Title: Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning. (arXiv:2208.06193v3 [cs.LG] UPDATED)\nAbstract: Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Q-learning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness",
    "path": "papers/22/08/2208.06193.json",
    "total_tokens": 959,
    "translated_title": "离线强化学习中的扩散策略作为表达性策略类的研究",
    "translated_abstract": "离线强化学习是通过利用先前收集的静态数据集来学习最优策略的重要强化学习范式。标准的强化学习方法在这种情况下通常表现不佳，原因是在分布不匹配的行为上存在函数逼近误差。尽管已经提出了各种正则化方法来缓解这个问题，但它们往往受限于具有有限表达能力的策略类，可能导致高度次优的解决方案。本文提出了以扩散模型作为策略表示的方法，这是一种近期出现的高度表达能力的深度生成模型类。我们引入了扩散Q-learning（Diffusion-QL），利用条件扩散模型来表示策略。在我们的方法中，我们学习一个动作值函数，并将最大化动作值的项加入到条件扩散模型的训练损失中，从而得到一个寻求接近行为策略的最优动作的损失函数。我们展示了扩散策略的表达能力。",
    "tldr": "本文提出了一种将策略表示为扩散模型的方法，用于离线强化学习。我们引入了Diffusion Q-learning（Diffusion-QL），利用条件扩散模型表示策略，并通过最大化动作值来寻求接近行为策略的最优动作。",
    "en_tdlr": "This paper proposes a method of representing the policy as a diffusion model for offline reinforcement learning. They introduce Diffusion Q-learning (Diffusion-QL), which utilizes a conditional diffusion model to represent the policy and seeks optimal actions by maximizing action-values that are close to the behavior policy."
}