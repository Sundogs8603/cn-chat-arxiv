{
    "title": "Implicit Two-Tower Policies. (arXiv:2208.01191v2 [cs.LG] UPDATED)",
    "abstract": "We present a new class of structured reinforcement learning policy-architectures, Implicit Two-Tower (ITT) policies, where the actions are chosen based on the attention scores of their learnable latent representations with those of the input states. By explicitly disentangling action from state processing in the policy stack, we achieve two main goals: substantial computational gains and better performance. Our architectures are compatible with both: discrete and continuous action spaces. By conducting tests on 15 environments from OpenAI Gym and DeepMind Control Suite, we show that ITT-architectures are particularly suited for blackbox/evolutionary optimization and the corresponding policy training algorithms outperform their vanilla unstructured implicit counterparts as well as commonly used explicit policies. We complement our analysis by showing how techniques such as hashing and lazy tower updates, critically relying on the two-tower structure of ITTs, can be applied to obtain add",
    "link": "http://arxiv.org/abs/2208.01191",
    "context": "Title: Implicit Two-Tower Policies. (arXiv:2208.01191v2 [cs.LG] UPDATED)\nAbstract: We present a new class of structured reinforcement learning policy-architectures, Implicit Two-Tower (ITT) policies, where the actions are chosen based on the attention scores of their learnable latent representations with those of the input states. By explicitly disentangling action from state processing in the policy stack, we achieve two main goals: substantial computational gains and better performance. Our architectures are compatible with both: discrete and continuous action spaces. By conducting tests on 15 environments from OpenAI Gym and DeepMind Control Suite, we show that ITT-architectures are particularly suited for blackbox/evolutionary optimization and the corresponding policy training algorithms outperform their vanilla unstructured implicit counterparts as well as commonly used explicit policies. We complement our analysis by showing how techniques such as hashing and lazy tower updates, critically relying on the two-tower structure of ITTs, can be applied to obtain add",
    "path": "papers/22/08/2208.01191.json",
    "total_tokens": 872,
    "translated_title": "隐式双塔策略",
    "translated_abstract": "我们提出了一种新的结构化强化学习策略体系，即隐式双塔（ITT）策略，其中动作基于其可学习的潜在表示与输入状态的注意力分数进行选择。通过在策略堆栈中显式区分动作和状态处理，我们实现了两个主要目标：显著的计算效益和更好的性能。我们的架构适用于离散和连续动作空间。通过在OpenAI Gym和DeepMind Control Suite的15个环境上进行测试，我们展示了ITT架构特别适用于黑盒/进化优化，相应的策略训练算法优于其草率的隐式对应物以及常用的显式策略。我们通过展示如何应用哈希和惰性塔更新等技术，关键依赖于ITT的双塔结构，来补充我们的分析。",
    "tldr": "隐式双塔策略（ITT）是一种新的结构化强化学习策略体系，通过在策略堆栈中显式区分动作和状态处理，实现了显著的计算效益和更好的性能，在黑盒/进化优化方面表现出色。",
    "en_tdlr": "Implicit Two-Tower (ITT) policies are a new class of structured reinforcement learning policy-architectures that achieve substantial computational gains and better performance by explicitly disentangling action from state processing in the policy stack. They show excellent performance in blackbox/evolutionary optimization."
}