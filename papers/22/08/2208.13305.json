{
    "title": "Neural Network Approximation of Continuous Functions in High Dimensions with Applications to Inverse Problems. (arXiv:2208.13305v3 [stat.ML] UPDATED)",
    "abstract": "The remarkable successes of neural networks in a huge variety of inverse problems have fueled their adoption in disciplines ranging from medical imaging to seismic analysis over the past decade. However, the high dimensionality of such inverse problems has simultaneously left current theory, which predicts that networks should scale exponentially in the dimension of the problem, unable to explain why the seemingly small networks used in these settings work as well as they do in practice. To reduce this gap between theory and practice, we provide a general method for bounding the complexity required for a neural network to approximate a H\\\"older (or uniformly) continuous function defined on a high-dimensional set with a low-complexity structure. The approach is based on the observation that the existence of a Johnson-Lindenstrauss embedding $A\\in\\mathbb{R}^{d\\times D}$ of a given high-dimensional set $S\\subset\\mathbb{R}^D$ into a low dimensional cube $[-M,M]^d$ implies that for any H\\\"o",
    "link": "http://arxiv.org/abs/2208.13305",
    "context": "Title: Neural Network Approximation of Continuous Functions in High Dimensions with Applications to Inverse Problems. (arXiv:2208.13305v3 [stat.ML] UPDATED)\nAbstract: The remarkable successes of neural networks in a huge variety of inverse problems have fueled their adoption in disciplines ranging from medical imaging to seismic analysis over the past decade. However, the high dimensionality of such inverse problems has simultaneously left current theory, which predicts that networks should scale exponentially in the dimension of the problem, unable to explain why the seemingly small networks used in these settings work as well as they do in practice. To reduce this gap between theory and practice, we provide a general method for bounding the complexity required for a neural network to approximate a H\\\"older (or uniformly) continuous function defined on a high-dimensional set with a low-complexity structure. The approach is based on the observation that the existence of a Johnson-Lindenstrauss embedding $A\\in\\mathbb{R}^{d\\times D}$ of a given high-dimensional set $S\\subset\\mathbb{R}^D$ into a low dimensional cube $[-M,M]^d$ implies that for any H\\\"o",
    "path": "papers/22/08/2208.13305.json",
    "total_tokens": 971,
    "translated_title": "在高维度中利用神经网络逼近连续函数，并应用于逆问题",
    "translated_abstract": "在过去的十年中，神经网络在各个领域中的逆问题中取得了显著的成功，这推动了它们在医学影像到地震分析等领域的采用。然而，这些逆问题的高维度使得现有理论无法解释在实践中为什么使用看似较小的网络也能得到良好的效果。为了缩小理论与实践之间的差距，我们提供了一种通用的方法来界定神经网络逼近高维集合上的H\\\"older（或均匀）连续函数所需的复杂度。这种方法基于一个观察：给定一个高维集合$S\\subset\\mathbb{R}^D$，如果存在一个Johnson-Lindenstrauss嵌入$A\\in\\mathbb{R}^{d\\times D}$，其中$d$较小，将$S$嵌入到一个低维立方体$[-M,M]^d$中，那么对于任何H\\\"o",
    "tldr": "该论文提出了一种通用的方法，用于解决神经网络在高维空间中逼近连续函数的问题，并对理论与实践之间的差距进行了缩小。该方法基于Johnson-Lindenstrauss嵌入的观察，通过将高维集合嵌入到低维空间中，使得较小的神经网络可以有效逼近高维连续函数。",
    "en_tdlr": "This paper proposes a general method to address the issue of neural network approximation of continuous functions in high dimensional spaces, and reduces the gap between theory and practice. The method is based on the observation of Johnson-Lindenstrauss embeddings, which allow for effective approximation of high dimensional continuous functions using smaller networks."
}