{
    "title": "Global Context Vision Transformers. (arXiv:2206.09959v5 [cs.CV] UPDATED)",
    "abstract": "We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and 85.7% Top-1 accuracy, respectively, at 224 image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based Conv",
    "link": "http://arxiv.org/abs/2206.09959",
    "context": "Title: Global Context Vision Transformers. (arXiv:2206.09959v5 [cs.CV] UPDATED)\nAbstract: We propose global context vision transformer (GC ViT), a novel architecture that enhances parameter and compute utilization for computer vision. Our method leverages global context self-attention modules, joint with standard local self-attention, to effectively and efficiently model both long and short-range spatial interactions, without the need for expensive operations such as computing attention masks or shifting local windows. In addition, we address the lack of the inductive bias in ViTs, and propose to leverage a modified fused inverted residual blocks in our architecture. Our proposed GC ViT achieves state-of-the-art results across image classification, object detection and semantic segmentation tasks. On ImageNet-1K dataset for classification, the variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and 85.7% Top-1 accuracy, respectively, at 224 image resolution and without any pre-training, hence surpassing comparably-sized prior art such as CNN-based Conv",
    "path": "papers/22/06/2206.09959.json",
    "total_tokens": 998,
    "translated_title": "全局上下文视觉Transformer",
    "translated_abstract": "我们提出了一种新颖的架构——全局上下文视觉Transformer (GC ViT), 可以增强计算机视觉中的参数和计算利用。我们的方法利用全局上下文自注意力模块和标准的局部自注意力，对长距离和短距离空间相互作用进行有效而高效的建模，无需进行像计算注意力掩码或移动本地窗口这样的昂贵操作。并且，我们解决了ViTs中缺乏归纳偏差的问题，并在我们的架构中使用一种修改后的融合反向残差块。我们提出的GC ViT在图像分类、目标检测和语义分割任务中均取得了最先进的成果。在ImageNet-1K数据集上进行分类，GC ViT的51M、90M和201M参数变体在224像素分辨率下都能够达到84.3%、85.0%和85.7%的Top-1精度，而且无需任何预训练，因此超越了CNN-based Conv等先前的艺术品。",
    "tldr": "提出了全局上下文视觉Transformer (GC ViT) 架构，利用全局上下文自注意力模块和标准的局部自注意力对长距离和短距离空间相互作用进行有效而高效的建模，同时解决了ViTs中缺乏归纳偏差的问题，在图像分类、目标检测和语义分割任务中表现出最先进的结果。",
    "en_tdlr": "Proposed a novel architecture, Global Context Vision Transformer (GC ViT), that models long and short-range spatial interactions efficiently using global context self-attention modules and standard local self-attention without the need for expensive operations. Addressed the lack of inductive bias in ViTs by leveraging a modified fused inverted residual blocks. Achieved state-of-the-art results in image classification, object detection and semantic segmentation tasks surpassing prior art."
}