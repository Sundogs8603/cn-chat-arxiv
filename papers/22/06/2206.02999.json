{
    "title": "DiMS: Distilling Multiple Steps of Iterative Non-Autoregressive Transformers for Machine Translation. (arXiv:2206.02999v2 [cs.CL] UPDATED)",
    "abstract": "The computational benefits of iterative non-autoregressive transformers decrease as the number of decoding steps increases. As a remedy, we introduce Distill Multiple Steps (DiMS), a simple yet effective distillation technique to decrease the number of required steps to reach a certain translation quality. The distilled model enjoys the computational benefits of early iterations while preserving the enhancements from several iterative steps. DiMS relies on two models namely student and teacher. The student is optimized to predict the output of the teacher after multiple decoding steps while the teacher follows the student via a slow-moving average. The moving average keeps the teacher's knowledge updated and enhances the quality of the labels provided by the teacher. During inference, the student is used for translation and no additional computation is added. We verify the effectiveness of DiMS on various models obtaining 7.8 and 12.9 BLEU points improvements in single-step translation",
    "link": "http://arxiv.org/abs/2206.02999",
    "context": "Title: DiMS: Distilling Multiple Steps of Iterative Non-Autoregressive Transformers for Machine Translation. (arXiv:2206.02999v2 [cs.CL] UPDATED)\nAbstract: The computational benefits of iterative non-autoregressive transformers decrease as the number of decoding steps increases. As a remedy, we introduce Distill Multiple Steps (DiMS), a simple yet effective distillation technique to decrease the number of required steps to reach a certain translation quality. The distilled model enjoys the computational benefits of early iterations while preserving the enhancements from several iterative steps. DiMS relies on two models namely student and teacher. The student is optimized to predict the output of the teacher after multiple decoding steps while the teacher follows the student via a slow-moving average. The moving average keeps the teacher's knowledge updated and enhances the quality of the labels provided by the teacher. During inference, the student is used for translation and no additional computation is added. We verify the effectiveness of DiMS on various models obtaining 7.8 and 12.9 BLEU points improvements in single-step translation",
    "path": "papers/22/06/2206.02999.json",
    "total_tokens": 884,
    "translated_title": "DiMS：迭代非自回归Transformer的压缩多步骤机制在机器翻译中的应用",
    "translated_abstract": "迭代非自回归Transformer的计算优势在解码步骤增加时会减弱。我们引入了Distill Multiple Steps（DiMS），这是一种简单而有效的压缩技术，可减少达到特定翻译质量所需的步骤数。压缩后的模型既享受了早期迭代的计算优势，同时又保留了多个迭代步骤的增强效果。DiMS需要两个模型，即学生模型和教师模型。学生模型通过优化以预测多次解码后的教师模型输出，而教师模型则通过缓慢移动平均跟随学生模型，这使得教师模型的知识得到更新，并提高了提供的标签的质量。在推理过程中，学生模型用于翻译，并且没有额外的计算负担。我们在各种模型上验证了DiMS的有效性，单步翻译中获得了7.8和12.9 BLEU分的改进。",
    "tldr": "该论文提出了一种新的技术DiMS，可以通过压缩多步骤机制来优化解码过程，提高机器翻译的效率和质量。"
}