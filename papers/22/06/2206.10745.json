{
    "title": "Derivative-Informed Neural Operator: An Efficient Framework for High-Dimensional Parametric Derivative Learning. (arXiv:2206.10745v4 [math.NA] UPDATED)",
    "abstract": "We propose derivative-informed neural operators (DINOs), a general family of neural networks to approximate operators as infinite-dimensional mappings from input function spaces to output function spaces or quantities of interest. After discretizations both inputs and outputs are high-dimensional. We aim to approximate not only the operators with improved accuracy but also their derivatives (Jacobians) with respect to the input function-valued parameter to empower derivative-based algorithms in many applications, e.g., Bayesian inverse problems, optimization under parameter uncertainty, and optimal experimental design. The major difficulties include the computational cost of generating derivative training data and the high dimensionality of the problem leading to large training cost. To address these challenges, we exploit the intrinsic low-dimensionality of the derivatives and develop algorithms for compressing derivative information and efficiently imposing it in neural operator trai",
    "link": "http://arxiv.org/abs/2206.10745",
    "context": "Title: Derivative-Informed Neural Operator: An Efficient Framework for High-Dimensional Parametric Derivative Learning. (arXiv:2206.10745v4 [math.NA] UPDATED)\nAbstract: We propose derivative-informed neural operators (DINOs), a general family of neural networks to approximate operators as infinite-dimensional mappings from input function spaces to output function spaces or quantities of interest. After discretizations both inputs and outputs are high-dimensional. We aim to approximate not only the operators with improved accuracy but also their derivatives (Jacobians) with respect to the input function-valued parameter to empower derivative-based algorithms in many applications, e.g., Bayesian inverse problems, optimization under parameter uncertainty, and optimal experimental design. The major difficulties include the computational cost of generating derivative training data and the high dimensionality of the problem leading to large training cost. To address these challenges, we exploit the intrinsic low-dimensionality of the derivatives and develop algorithms for compressing derivative information and efficiently imposing it in neural operator trai",
    "path": "papers/22/06/2206.10745.json",
    "total_tokens": 907,
    "translated_title": "导数信息驱动的神经算子：一个高维参数导数学习的高效框架",
    "translated_abstract": "我们提出了导数信息驱动的神经算子（DINOs），这是一个通用的神经网络家族，用于近似作为从输入函数空间到输出函数空间或感兴趣的值的无限维映射的算子。在离散化之后，输入和输出都是高维的。我们旨在不仅以更高的准确性近似算子，而且近似它们对输入函数值参数的导数（雅可比矩阵），以增强基于导数的算法在许多应用中的能力，例如贝叶斯逆问题，参数不确定性下的优化和最优实验设计。主要的困难包括生成导数训练数据的计算成本和问题的高维度导致大的训练成本。为了解决这些挑战，我们利用导数的固有低维性质，开发了用于压缩导数信息并将其高效地应用于神经算子训练的算法。",
    "tldr": "导数信息驱动的神经算子（DINOs）是一个高效的框架，用于近似高维参数导数学习。通过压缩导数信息和高效应用于神经算子训练，DINOs可以提高算子和导数的准确性，应用于众多领域。",
    "en_tdlr": "The Derivative-Informed Neural Operator (DINOs) is an efficient framework for approximating high-dimensional parametric derivatives. By compressing derivative information and applying it efficiently in neural operator training, DINOs improve the accuracy of both operators and their derivatives for various applications."
}