{
    "title": "Masked World Models for Visual Control. (arXiv:2206.14244v3 [cs.RO] UPDATED)",
    "abstract": "Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling a",
    "link": "http://arxiv.org/abs/2206.14244",
    "context": "Title: Masked World Models for Visual Control. (arXiv:2206.14244v3 [cs.RO] UPDATED)\nAbstract: Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling a",
    "path": "papers/22/06/2206.14244.json",
    "total_tokens": 898,
    "translated_title": "可掩蔽世界模型用于视觉控制",
    "translated_abstract": "视觉模型驱动的强化学习（RL）有潜力使机器人能够从视觉观测中实现样本有效的学习。但目前的方法通常训练一个端到端的单一模型来学习视觉表示和动力学，使得准确建模机器人与小物体之间的相互作用变得困难。本研究介绍了一种视觉模型驱动的RL框架，将视觉表示学习和动力学学习分离。具体来说，我们使用卷积层和视觉变换器（ViT）来训练自编码器，以在给定掩蔽卷积特征的情况下重构像素，并学习一个操作自编码器表示的潜在动力学模型。此外，为了编码任务相关信息，我们引入了一个辅助奖励预测目标来训练自编码器。我们使用从环境交互中收集的在线样本不断更新自编码器和动力学模型。我们证明了我们的分离策略可以有效地学习机器人控制策略。",
    "tldr": "本研究提出了一种视觉模型驱动的RL框架，将视觉表示学习和动力学学习分离，使用自编码器和潜在动力学模型来准确建模机器人控制策略。",
    "en_tdlr": "This study proposes a visual model-driven RL framework that decouples visual representation learning and dynamics learning, effectively models robot control strategies using an autoencoder and a latent dynamics model."
}