{
    "title": "Beyond Uniform Lipschitz Condition in Differentially Private Optimization. (arXiv:2206.10713v2 [cs.LG] UPDATED)",
    "abstract": "Most prior results on differentially private stochastic gradient descent (DP-SGD) are derived under the simplistic assumption of uniform Lipschitzness, i.e., the per-sample gradients are uniformly bounded. We generalize uniform Lipschitzness by assuming that the per-sample gradients have sample-dependent upper bounds, i.e., per-sample Lipschitz constants, which themselves may be unbounded. We provide principled guidance on choosing the clip norm in DP-SGD for convex over-parameterized settings satisfying our general version of Lipschitzness when the per-sample Lipschitz constants are bounded; specifically, we recommend tuning the clip norm only till values up to the minimum per-sample Lipschitz constant. This finds application in the private training of a softmax layer on top of a deep network pre-trained on public data. We verify the efficacy of our recommendation via experiments on 8 datasets. Furthermore, we provide new convergence results for DP-SGD on convex and nonconvex function",
    "link": "http://arxiv.org/abs/2206.10713",
    "context": "Title: Beyond Uniform Lipschitz Condition in Differentially Private Optimization. (arXiv:2206.10713v2 [cs.LG] UPDATED)\nAbstract: Most prior results on differentially private stochastic gradient descent (DP-SGD) are derived under the simplistic assumption of uniform Lipschitzness, i.e., the per-sample gradients are uniformly bounded. We generalize uniform Lipschitzness by assuming that the per-sample gradients have sample-dependent upper bounds, i.e., per-sample Lipschitz constants, which themselves may be unbounded. We provide principled guidance on choosing the clip norm in DP-SGD for convex over-parameterized settings satisfying our general version of Lipschitzness when the per-sample Lipschitz constants are bounded; specifically, we recommend tuning the clip norm only till values up to the minimum per-sample Lipschitz constant. This finds application in the private training of a softmax layer on top of a deep network pre-trained on public data. We verify the efficacy of our recommendation via experiments on 8 datasets. Furthermore, we provide new convergence results for DP-SGD on convex and nonconvex function",
    "path": "papers/22/06/2206.10713.json",
    "total_tokens": 898,
    "translated_title": "差分隐私优化中超越统一李普希茨条件",
    "translated_abstract": "大多数关于差分隐私随机梯度下降（DP-SGD）的先前结果都是在统一李普希茨性的简化假设下导出的，即每个样本的梯度都是均匀有界的。我们通过假定每个样本的梯度具有样本相关的上界，即每个样本的李普希茨常数，从而推广了统一李普希茨性。这些本身可能是无界的。当每个样本的李普希茨常数是有界的时，我们为DP-SGD在凸超参数化设置中选择剪辑范数提供了原则性指导；具体而言，我们建议仅调整剪辑范数，直到最小每个样本李普希茨常数的值。这在深度网络上预先训练公共数据的 softmax 层的私人训练中有应用。我们通过对8个数据集的实验验证了我们的建议的功效。此外，我们还为DP-SGD在凸和非凸函数上提供了新的收敛结果。",
    "tldr": "本文提出了一种新的差分隐私优化算法来处理其它算法无法处理的非均匀李普希茨情形，并且在具体应用中提供了相应的参数调整方案。",
    "en_tdlr": "This paper proposes a new differentially private optimization algorithm to handle non-uniform Lipschitz cases that other algorithms cannot handle, and provides corresponding parameter adjustment schemes for specific applications."
}