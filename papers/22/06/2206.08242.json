{
    "title": "Catastrophic overfitting can be induced with discriminative non-robust features. (arXiv:2206.08242v2 [cs.LG] UPDATED)",
    "abstract": "Adversarial training (AT) is the de facto method for building robust neural networks, but it can be computationally expensive. To mitigate this, fast single-step attacks can be used, but this may lead to catastrophic overfitting (CO). This phenomenon appears when networks gain non-trivial robustness during the first stages of AT, but then reach a breaking point where they become vulnerable in just a few iterations. The mechanisms that lead to this failure mode are still poorly understood. In this work, we study the onset of CO in single-step AT methods through controlled modifications of typical datasets of natural images. In particular, we show that CO can be induced at much smaller $\\epsilon$ values than it was observed before just by injecting images with seemingly innocuous features. These features aid non-robust classification but are not enough to achieve robustness on their own. Through extensive experiments we analyze this novel phenomenon and discover that the presence of thes",
    "link": "http://arxiv.org/abs/2206.08242",
    "context": "Title: Catastrophic overfitting can be induced with discriminative non-robust features. (arXiv:2206.08242v2 [cs.LG] UPDATED)\nAbstract: Adversarial training (AT) is the de facto method for building robust neural networks, but it can be computationally expensive. To mitigate this, fast single-step attacks can be used, but this may lead to catastrophic overfitting (CO). This phenomenon appears when networks gain non-trivial robustness during the first stages of AT, but then reach a breaking point where they become vulnerable in just a few iterations. The mechanisms that lead to this failure mode are still poorly understood. In this work, we study the onset of CO in single-step AT methods through controlled modifications of typical datasets of natural images. In particular, we show that CO can be induced at much smaller $\\epsilon$ values than it was observed before just by injecting images with seemingly innocuous features. These features aid non-robust classification but are not enough to achieve robustness on their own. Through extensive experiments we analyze this novel phenomenon and discover that the presence of thes",
    "path": "papers/22/06/2206.08242.json",
    "total_tokens": 865,
    "translated_title": "非鲁棒性特征会导致灾难性的过度拟合",
    "translated_abstract": "对抗训练是构建鲁棒神经网络的事实方法，但计算成本较高。为了减轻这一问题，可以使用快速单步攻击，但这可能导致灾难性的过度拟合。本文通过对典型的自然图像数据集进行控制性修改，研究了单步对抗训练方法中灾难性过度拟合的出现。特别地，我们表明，通过注入看似无害的特征，可以在比之前观察到的较小的epsilon值下引发灾难性过度拟合。这些特征有助于非鲁棒性分类，但不能单独实现鲁棒性。通过大量实验，我们分析了这一新现象，并发现了这种失败模式的机制还不够清楚。",
    "tldr": "本研究通过控制性修改典型的自然图像数据集，研究了对抗训练中灾难性过度拟合的出现。通过注入看似无害的特征，可以在较小的epsilon值下引发灾难性过度拟合。",
    "en_tdlr": "This paper investigates the occurrence of catastrophic overfitting in adversarial training by controlling modifications on typical datasets of natural images. It is discovered that catastrophic overfitting can be induced at smaller epsilon values by injecting seemingly innocuous features."
}