{
    "title": "A Communication-efficient Algorithm with Linear Convergence for Federated Minimax Learning. (arXiv:2206.01132v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we study a large-scale multi-agent minimax optimization problem, which models many interesting applications in statistical learning and game theory, including Generative Adversarial Networks (GANs). The overall objective is a sum of agents' private local objective functions. We first analyze an important special case, empirical minimax problem, where the overall objective approximates a true population minimax risk by statistical samples. We provide generalization bounds for learning with this objective through Rademacher complexity analysis. Then, we focus on the federated setting, where agents can perform local computation and communicate with a central server. Most existing federated minimax algorithms either require communication per iteration or lack performance guarantees with the exception of Local Stochastic Gradient Descent Ascent (SGDA), a multiple-local-update descent ascent algorithm which guarantees convergence under a diminishing stepsize. By analyzing Loca",
    "link": "http://arxiv.org/abs/2206.01132",
    "context": "Title: A Communication-efficient Algorithm with Linear Convergence for Federated Minimax Learning. (arXiv:2206.01132v2 [cs.LG] UPDATED)\nAbstract: In this paper, we study a large-scale multi-agent minimax optimization problem, which models many interesting applications in statistical learning and game theory, including Generative Adversarial Networks (GANs). The overall objective is a sum of agents' private local objective functions. We first analyze an important special case, empirical minimax problem, where the overall objective approximates a true population minimax risk by statistical samples. We provide generalization bounds for learning with this objective through Rademacher complexity analysis. Then, we focus on the federated setting, where agents can perform local computation and communicate with a central server. Most existing federated minimax algorithms either require communication per iteration or lack performance guarantees with the exception of Local Stochastic Gradient Descent Ascent (SGDA), a multiple-local-update descent ascent algorithm which guarantees convergence under a diminishing stepsize. By analyzing Loca",
    "path": "papers/22/06/2206.01132.json",
    "total_tokens": 925,
    "translated_title": "一种具有线性收敛的高效通信算法，用于联邦极小值学习",
    "translated_abstract": "本文研究了大规模多智能体极小化最优化问题，该问题模拟了统计学习和博弈论中的许多有趣应用，包括生成式对抗网络(GAN)。总体目标是代理的私有本地目标函数之和。我们首先分析了重要的特殊情况，即经验最小极小值问题，在其中，整体目标通过统计样本近似一个真实的极小值风险。我们通过Rademacher复杂度分析提供了学习此目标的泛化界限。然后，我们关注联邦设置，其中代理可以执行本地计算并与中央服务器通信。大多数现有的联邦极小值算法要么需要每次迭代进行通信，要么缺乏性能保证，除了本地随机梯度下降上升(SGDA)，这是一种多个本地更新下降上升算法，在缩小步长的情况下保证收敛。",
    "tldr": "本文提出了一种高效的通信算法，用于联邦极小值学习，具有线性收敛特性。研究了重要的特殊情况，并提供了学习该目标的泛化界限。针对联邦设置，提出了Local Stochastic Gradient Descent Ascent (SGDA)算法，可保证收敛性。",
    "en_tdlr": "This paper proposes an efficient communication algorithm for federated minimax learning with linear convergence. It studies an important special case, provides generalization bounds, and introduces Local Stochastic Gradient Descent Ascent (SGDA) algorithm for the federated setting with guaranteed convergence."
}