{
    "title": "A Search-Based Testing Approach for Deep Reinforcement Learning Agents. (arXiv:2206.07813v3 [cs.SE] UPDATED)",
    "abstract": "Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One way to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Their main goal is to test the robustness of DRL agents rather than testing the compliance of agents' policies with respect to requirements. Due to the huge state spac",
    "link": "http://arxiv.org/abs/2206.07813",
    "context": "Title: A Search-Based Testing Approach for Deep Reinforcement Learning Agents. (arXiv:2206.07813v3 [cs.SE] UPDATED)\nAbstract: Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One way to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Their main goal is to test the robustness of DRL agents rather than testing the compliance of agents' policies with respect to requirements. Due to the huge state spac",
    "path": "papers/22/06/2206.07813.json",
    "total_tokens": 964,
    "translated_title": "一种基于搜索的测试方法，用于深度强化学习代理",
    "translated_abstract": "近十年来，深度强化学习算法越来越多地被应用于解决自动驾驶和机器人等各种决策问题。然而，由于深度强化学习代理在生命安全环境中经常表现出错误行为，导致潜在的重大错误，因此它们面临着巨大的挑战。为了评估DRL代理的安全性，一种方法是对它们进行测试，以检测可能导致关键故障的故障。这就提出了一个问题，即我们如何有效地测试DRL策略，以确保它们的正确性和遵守安全要求。本文提出了一种基于搜索的测试方法，通过引导代理生成满足安全要求的状态序列变化，以探索环境的状态空间。我们的方法在三种不同的DRL基准测试中进行了评估，并在保持相似或更好的测试效果的同时，实现了比基线方法更高的状态空间覆盖率。",
    "tldr": "本文提出一种基于搜索的测试方法，探索状态空间以检测DRL代理的安全性，并在三个基准测试中取得了比基线方法更高的状态空间覆盖率。",
    "en_tdlr": "This paper proposes a search-based testing approach to explore the state space of environments for detecting safety issues in deep reinforcement learning (DRL) agents. The approach achieves higher state space coverage compared to baseline methods and is evaluated on three DRL benchmarks."
}