{
    "title": "On the Maximum Hessian Eigenvalue and Generalization. (arXiv:2206.10654v3 [cs.LG] UPDATED)",
    "abstract": "The mechanisms by which certain training interventions, such as increasing learning rates and applying batch normalization, improve the generalization of deep networks remains a mystery. Prior works have speculated that \"flatter\" solutions generalize better than \"sharper\" solutions to unseen data, motivating several metrics for measuring flatness (particularly $\\lambda_{max}$, the largest eigenvalue of the Hessian of the loss); and algorithms, such as Sharpness-Aware Minimization (SAM) [1], that directly optimize for flatness. Other works question the link between $\\lambda_{max}$ and generalization. In this paper, we present findings that call $\\lambda_{max}$'s influence on generalization further into question. We show that: (1) while larger learning rates reduce $\\lambda_{max}$ for all batch sizes, generalization benefits sometimes vanish at larger batch sizes; (2) by scaling batch size and learning rate simultaneously, we can change $\\lambda_{max}$ without affecting generalization; (",
    "link": "http://arxiv.org/abs/2206.10654",
    "context": "Title: On the Maximum Hessian Eigenvalue and Generalization. (arXiv:2206.10654v3 [cs.LG] UPDATED)\nAbstract: The mechanisms by which certain training interventions, such as increasing learning rates and applying batch normalization, improve the generalization of deep networks remains a mystery. Prior works have speculated that \"flatter\" solutions generalize better than \"sharper\" solutions to unseen data, motivating several metrics for measuring flatness (particularly $\\lambda_{max}$, the largest eigenvalue of the Hessian of the loss); and algorithms, such as Sharpness-Aware Minimization (SAM) [1], that directly optimize for flatness. Other works question the link between $\\lambda_{max}$ and generalization. In this paper, we present findings that call $\\lambda_{max}$'s influence on generalization further into question. We show that: (1) while larger learning rates reduce $\\lambda_{max}$ for all batch sizes, generalization benefits sometimes vanish at larger batch sizes; (2) by scaling batch size and learning rate simultaneously, we can change $\\lambda_{max}$ without affecting generalization; (",
    "path": "papers/22/06/2206.10654.json",
    "total_tokens": 805,
    "tldr": "本文发现了一些调查结果，质疑了 $\\lambda_{max}$ 在深度网络泛化中的影响。",
    "en_tdlr": "This paper presents findings that call into question the influence of $\\lambda_{max}$ on the generalization of deep networks. The study shows that larger learning rates reduce $\\lambda_{max}$ for all batch sizes, but the generalization benefits sometimes vanish at larger batch sizes, and scaling batch size and learning rate simultaneously can change $\\lambda_{max}$ without affecting generalization."
}