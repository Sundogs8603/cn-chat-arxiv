{
    "title": "Improving generalization by mimicking the human visual diet. (arXiv:2206.07802v2 [cs.CV] UPDATED)",
    "abstract": "We present a new perspective on bridging the generalization gap between biological and computer vision -- mimicking the human visual diet. While computer vision models rely on internet-scraped datasets, humans learn from limited 3D scenes under diverse real-world transformations with objects in natural context. Our results demonstrate that incorporating variations and contextual cues ubiquitous in the human visual training data (visual diet) significantly improves generalization to real-world transformations such as lighting, viewpoint, and material changes. This improvement also extends to generalizing from synthetic to real-world data -- all models trained with a human-like visual diet outperform specialized architectures by large margins when tested on natural image data. These experiments are enabled by our two key contributions: a novel dataset capturing scene context and diverse real-world transformations to mimic the human visual diet, and a transformer model tailored to leverag",
    "link": "http://arxiv.org/abs/2206.07802",
    "context": "Title: Improving generalization by mimicking the human visual diet. (arXiv:2206.07802v2 [cs.CV] UPDATED)\nAbstract: We present a new perspective on bridging the generalization gap between biological and computer vision -- mimicking the human visual diet. While computer vision models rely on internet-scraped datasets, humans learn from limited 3D scenes under diverse real-world transformations with objects in natural context. Our results demonstrate that incorporating variations and contextual cues ubiquitous in the human visual training data (visual diet) significantly improves generalization to real-world transformations such as lighting, viewpoint, and material changes. This improvement also extends to generalizing from synthetic to real-world data -- all models trained with a human-like visual diet outperform specialized architectures by large margins when tested on natural image data. These experiments are enabled by our two key contributions: a novel dataset capturing scene context and diverse real-world transformations to mimic the human visual diet, and a transformer model tailored to leverag",
    "path": "papers/22/06/2206.07802.json",
    "total_tokens": 1021,
    "translated_title": "通过模仿人类视觉经验改善泛化能力",
    "translated_abstract": "我们提出了一个新的视角，用于弥合生物和计算机视觉之间的泛化差距 - 模仿人类的视觉经验。尽管计算机视觉模型依赖于互联网网站上获取的数据集，但人类是通过在自然环境中的有限3D场景中学习，并且接触到各种真实世界的变化，其中包括自然环境中的对象。我们的结果表明，将人类视觉训练数据（视觉经验）中普遍存在的变化和上下文线索纳入模型中，显著改善了对真实世界变换（如光照、视角和材料变化）的泛化能力。这种改进还扩展到从合成数据到真实世界数据的泛化 - 所有经过人类视觉经验训练的模型在自然图像数据上的测试中，均表现优于专门的架构。这些实验得益于我们的两个关键贡献：一个新颖的数据集，捕捉了场景上下文和各种真实世界的变换，以模拟人类的视觉经验，以及一个针对该数据集定制的转换模型，用于利用上下文和变换信息。",
    "tldr": "本文提出了一个新的视角，通过模仿人类视觉经验的方法来缩小生物和计算机视觉之间的泛化差距。结果显示，将人类的视觉经验中的变化和上下文信息纳入模型中，显著改善了模型对真实世界变换的泛化能力，并在从合成到真实数据的过程中取得了较大的优势。",
    "en_tdlr": "This paper presents a new perspective on bridging the generalization gap between biological and computer vision by mimicking the human visual diet. The results show that incorporating variations and contextual cues from human visual experience significantly improves the model's generalization to real-world transformations and outperforms specialized architectures in generalizing from synthetic to real-world data."
}