{
    "title": "Stein Variational Goal Generation for adaptive Exploration in Multi-Goal Reinforcement Learning. (arXiv:2206.06719v2 [cs.LG] UPDATED)",
    "abstract": "In multi-goal Reinforcement Learning, an agent can share experience between related training tasks, resulting in better generalization for new tasks at test time. However, when the goal space has discontinuities and the reward is sparse, a majority of goals are difficult to reach. In this context, a curriculum over goals helps agents learn by adapting training tasks to their current capabilities. In this work we propose Stein Variational Goal Generation (SVGG), which samples goals of intermediate difficulty for the agent, by leveraging a learned predictive model of its goal reaching capabilities. The distribution of goals is modeled with particles that are attracted in areas of appropriate difficulty using Stein Variational Gradient Descent. We show that SVGG outperforms state-of-the-art multi-goal Reinforcement Learning methods in terms of success coverage in hard exploration problems, and demonstrate that it is endowed with a useful recovery property when the environment changes.",
    "link": "http://arxiv.org/abs/2206.06719",
    "context": "Title: Stein Variational Goal Generation for adaptive Exploration in Multi-Goal Reinforcement Learning. (arXiv:2206.06719v2 [cs.LG] UPDATED)\nAbstract: In multi-goal Reinforcement Learning, an agent can share experience between related training tasks, resulting in better generalization for new tasks at test time. However, when the goal space has discontinuities and the reward is sparse, a majority of goals are difficult to reach. In this context, a curriculum over goals helps agents learn by adapting training tasks to their current capabilities. In this work we propose Stein Variational Goal Generation (SVGG), which samples goals of intermediate difficulty for the agent, by leveraging a learned predictive model of its goal reaching capabilities. The distribution of goals is modeled with particles that are attracted in areas of appropriate difficulty using Stein Variational Gradient Descent. We show that SVGG outperforms state-of-the-art multi-goal Reinforcement Learning methods in terms of success coverage in hard exploration problems, and demonstrate that it is endowed with a useful recovery property when the environment changes.",
    "path": "papers/22/06/2206.06719.json",
    "total_tokens": 919,
    "translated_title": "自适应探索的Stein变分目标生成在多目标强化学习中的应用",
    "translated_abstract": "在多目标强化学习中，智能体可以在相关的训练任务之间共享经验，从而在测试时实现更好的泛化。但是，在目标空间具有不连续性且奖励稀疏的情况下，大多数目标难以达成。在这种情况下，目标的课程有助于智能体通过适应其当前能力来学习。本文提出了一种名为Stein Variational Goal Generation (SVGG)的方法，通过利用其目标到达能力的学习预测模型，为智能体采样中等难度的目标。目标分布采用粒子来建模，并利用Stein变分梯度下降将粒子吸引到适当难度的区域。我们展示SVGG在难度较高的探索问题中胜过最先进的多目标强化学习方法，并证明它在环境发生变化时具有有用的恢复特性。",
    "tldr": "本文提出了一种自适应探索的Stein变分目标生成方法，通过粒子建模并引入适当难度区域，提高了多目标强化学习中目标达成的成功覆盖率，同时在环境变化时表现出有用的恢复特性。",
    "en_tdlr": "This paper proposes a Stein Variational Goal Generation method for adaptive exploration in multi-goal Reinforcement Learning, which samples intermediate difficulty goals for agents by predicting their goal reaching capabilities and using particles to model the distribution of goals. The method outperforms state-of-the-art approaches in success coverage and demonstrates useful recovery properties when the environment changes."
}