{
    "title": "The Neural Covariance SDE: Shaped Infinite Depth-and-Width Networks at Initialization. (arXiv:2206.02768v3 [stat.ML] UPDATED)",
    "abstract": "The logit outputs of a feedforward neural network at initialization are conditionally Gaussian, given a random covariance matrix defined by the penultimate layer. In this work, we study the distribution of this random matrix. Recent work has shown that shaping the activation function as network depth grows large is necessary for this covariance matrix to be non-degenerate. However, the current infinite-width-style understanding of this shaping method is unsatisfactory for large depth: infinite-width analyses ignore the microscopic fluctuations from layer to layer, but these fluctuations accumulate over many layers.  To overcome this shortcoming, we study the random covariance matrix in the shaped infinite-depth-and-width limit. We identify the precise scaling of the activation function necessary to arrive at a non-trivial limit, and show that the random covariance matrix is governed by a stochastic differential equation (SDE) that we call the Neural Covariance SDE. Using simulations, w",
    "link": "http://arxiv.org/abs/2206.02768",
    "context": "Title: The Neural Covariance SDE: Shaped Infinite Depth-and-Width Networks at Initialization. (arXiv:2206.02768v3 [stat.ML] UPDATED)\nAbstract: The logit outputs of a feedforward neural network at initialization are conditionally Gaussian, given a random covariance matrix defined by the penultimate layer. In this work, we study the distribution of this random matrix. Recent work has shown that shaping the activation function as network depth grows large is necessary for this covariance matrix to be non-degenerate. However, the current infinite-width-style understanding of this shaping method is unsatisfactory for large depth: infinite-width analyses ignore the microscopic fluctuations from layer to layer, but these fluctuations accumulate over many layers.  To overcome this shortcoming, we study the random covariance matrix in the shaped infinite-depth-and-width limit. We identify the precise scaling of the activation function necessary to arrive at a non-trivial limit, and show that the random covariance matrix is governed by a stochastic differential equation (SDE) that we call the Neural Covariance SDE. Using simulations, w",
    "path": "papers/22/06/2206.02768.json",
    "total_tokens": 1003,
    "translated_title": "神经协方差SDE：初始化时具有无限深度和宽度的网络的形状。",
    "translated_abstract": "在初始化时，前馈神经网络的logit输出在给定由次表层定义的随机协方差矩阵的条件下是条件高斯分布的。本文研究了这种随机矩阵的分布。最近的研究表明，当网络深度增加时，对激活函数进行形状塑造是必要的，以使得这个协方差矩阵是非退化的。然而，当前无限宽度样式的这种理解在大深度时存在不足：无限宽度分析忽略了从层到层的微观波动，但这些波动在许多层上积累。为了克服这个问题，我们研究了由形状塑造的无限深度和宽度极限中的随机协方差矩阵。我们确定了到达非平凡极限所需的激活函数的精确缩放，并表明随机协方差矩阵受到我们称之为神经协方差SDE的随机微分方程的控制。使用模拟，我们证明了我们对神经协方差SDE的理解是准确的。",
    "tldr": "本文研究了前馈神经网络初始化时的随机协方差矩阵分布，发现对激活函数进行形状塑造可以使协方差矩阵是非退化的，而随机协方差矩阵受到神经协方差SDE的随机微分方程的控制。",
    "en_tdlr": "This paper studies the distribution of the random covariance matrix of a feedforward neural network at initialization and finds that shaping the activation function can make the covariance matrix non-degenerate, with the random covariance matrix being governed by a stochastic differential equation called the Neural Covariance SDE."
}