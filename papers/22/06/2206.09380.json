{
    "title": "Supervision Adaptation Balancing In-distribution Generalization and Out-of-distribution Detection. (arXiv:2206.09380v2 [cs.LG] UPDATED)",
    "abstract": "The discrepancy between in-distribution (ID) and out-of-distribution (OOD) samples can lead to \\textit{distributional vulnerability} in deep neural networks, which can subsequently lead to high-confidence predictions for OOD samples. This is mainly due to the absence of OOD samples during training, which fails to constrain the network properly. To tackle this issue, several state-of-the-art methods include adding extra OOD samples to training and assign them with manually-defined labels. However, this practice can introduce unreliable labeling, negatively affecting ID classification. The distributional vulnerability presents a critical challenge for non-IID deep learning, which aims for OOD-tolerant ID classification by balancing ID generalization and OOD detection. In this paper, we introduce a novel \\textit{supervision adaptation} approach to generate adaptive supervision information for OOD samples, making them more compatible with ID samples. Firstly, we measure the dependency betw",
    "link": "http://arxiv.org/abs/2206.09380",
    "context": "Title: Supervision Adaptation Balancing In-distribution Generalization and Out-of-distribution Detection. (arXiv:2206.09380v2 [cs.LG] UPDATED)\nAbstract: The discrepancy between in-distribution (ID) and out-of-distribution (OOD) samples can lead to \\textit{distributional vulnerability} in deep neural networks, which can subsequently lead to high-confidence predictions for OOD samples. This is mainly due to the absence of OOD samples during training, which fails to constrain the network properly. To tackle this issue, several state-of-the-art methods include adding extra OOD samples to training and assign them with manually-defined labels. However, this practice can introduce unreliable labeling, negatively affecting ID classification. The distributional vulnerability presents a critical challenge for non-IID deep learning, which aims for OOD-tolerant ID classification by balancing ID generalization and OOD detection. In this paper, we introduce a novel \\textit{supervision adaptation} approach to generate adaptive supervision information for OOD samples, making them more compatible with ID samples. Firstly, we measure the dependency betw",
    "path": "papers/22/06/2206.09380.json",
    "total_tokens": 851,
    "translated_title": "监督适应性平衡内部分布泛化与外部分布检测",
    "translated_abstract": "深度神经网络中内部分布（ID）和外部分布（OOD）样本之间的差异可能导致网络的分布脆弱性，进而导致对OOD样本的高置信度预测。这主要是由于训练过程中缺少OOD样本，无法充分约束网络。为了解决这个问题，一些最先进的方法包括在训练中添加额外的OOD样本，并为其分配手动定义的标签。然而，这种做法可能导致不可靠的标注，对ID分类产生负面影响。分布脆弱性对于非IID深度学习提出了一个关键挑战，即通过平衡ID泛化和OOD检测来实现对OOD容忍的ID分类。本文提出了一种新颖的“监督适应性”方法，用于为OOD样本生成自适应的监督信息，使其更兼容ID样本。",
    "tldr": "该论文提出了一种监督适应性方法，通过生成自适应的监督信息来解决深度神经网络中内部分布泛化与外部分布检测之间的差异问题。",
    "en_tdlr": "This paper introduces a novel supervision adaptation approach to tackle the discrepancy between in-distribution generalization and out-of-distribution detection in deep neural networks."
}