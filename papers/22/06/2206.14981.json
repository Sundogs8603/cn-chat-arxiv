{
    "title": "Randomized Coordinate Subgradient Method for Nonsmooth Optimization. (arXiv:2206.14981v2 [math.OC] UPDATED)",
    "abstract": "In this work, we propose the {Randomized Coordinate Subgradient method} (RCS) for solving nonsmooth convex and nonsmooth nonconvex (nonsmooth weakly convex) optimization problems. RCS randomly selects one block coordinate to update at each iteration, making it more practical than updating all coordinates. We consider the linearly bounded subgradients assumption for the objective function, which is more general than the traditional Lipschitz continuity assumption, to account for practical scenarios. We then conduct thorough convergence analysis for RCS in both convex and nonconvex cases based on this generalized Lipschitz-type assumption. Specifically, we establish the $\\widetilde{\\mathcal{O}}(1/\\sqrt{k})$ convergence rate in expectation and the $\\tilde o(1/\\sqrt{k})$ almost sure asymptotic convergence rate in terms of suboptimality gap when $f$ is nonsmooth convex. If $f$ further satisfies the global quadratic growth condition, the improved $\\mathcal{O}(1/k)$ rate is shown in terms of ",
    "link": "http://arxiv.org/abs/2206.14981",
    "context": "Title: Randomized Coordinate Subgradient Method for Nonsmooth Optimization. (arXiv:2206.14981v2 [math.OC] UPDATED)\nAbstract: In this work, we propose the {Randomized Coordinate Subgradient method} (RCS) for solving nonsmooth convex and nonsmooth nonconvex (nonsmooth weakly convex) optimization problems. RCS randomly selects one block coordinate to update at each iteration, making it more practical than updating all coordinates. We consider the linearly bounded subgradients assumption for the objective function, which is more general than the traditional Lipschitz continuity assumption, to account for practical scenarios. We then conduct thorough convergence analysis for RCS in both convex and nonconvex cases based on this generalized Lipschitz-type assumption. Specifically, we establish the $\\widetilde{\\mathcal{O}}(1/\\sqrt{k})$ convergence rate in expectation and the $\\tilde o(1/\\sqrt{k})$ almost sure asymptotic convergence rate in terms of suboptimality gap when $f$ is nonsmooth convex. If $f$ further satisfies the global quadratic growth condition, the improved $\\mathcal{O}(1/k)$ rate is shown in terms of ",
    "path": "papers/22/06/2206.14981.json",
    "total_tokens": 1058,
    "translated_title": "非光滑优化的随机坐标半梯度法",
    "translated_abstract": "本文提出了适用于解决非光滑凸性和非光滑非凸性（非光滑弱凸性）优化问题的“随机坐标半梯度法”（RCS）。RCS在每次迭代中随机选择一个坐标块进行更新，比更新所有坐标更实用。我们考虑了目标函数的线性有界次梯度假设，该假设比传统的Lipschitz连续性假设更为通用，以适应实际情况。此外，我们基于这种广义的Lipschitz类型假设对RCS在凸和非凸情况下进行了全面的收敛性分析。具体来说，当$f$为非光滑凸函数时，我们在期望上建立了$\\widetilde{\\mathcal{O}}(1/\\sqrt{k})$收敛速率，以及在次优间隙方面的$\\tilde o(1/\\sqrt{k})$几乎肯定渐近收敛速率。如果$f$进一步满足全局二次增长条件，则显示了改进的$\\mathcal{O}(1/k)$速率。",
    "tldr": "本文提出了适用于非光滑优化问题的随机坐标半梯度法，该方法在每次迭代只更新一个坐标块，考虑了目标函数的线性有界次梯度假设，并在凸和非凸情况下建立了全面的收敛性分析，收敛速率为$\\widetilde{\\mathcal{O}}(1/\\sqrt{k})$和$\\mathcal{O}(1/k)$。"
}