{
    "title": "On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective. (arXiv:2206.06854v2 [cs.AI] UPDATED)",
    "abstract": "Input gradients have a pivotal role in a variety of applications, including adversarial attack algorithms for evaluating model robustness, explainable AI techniques for generating Saliency Maps, and counterfactual explanations. However, Saliency Maps generated by traditional neural networks are often noisy and provide limited insights. In this paper, we demonstrate that, on the contrary, the Saliency Maps of 1-Lipschitz neural networks, learnt with the dual loss of an optimal transportation problem, exhibit desirable XAI properties: They are highly concentrated on the essential parts of the image with low noise, significantly outperforming state-of-the-art explanation approaches across various models and metrics. We also prove that these maps align unprecedentedly well with human explanations on ImageNet. To explain the particularly beneficial properties of the Saliency Map for such models, we prove this gradient encodes both the direction of the transportation plan and the direction t",
    "link": "http://arxiv.org/abs/2206.06854",
    "context": "Title: On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective. (arXiv:2206.06854v2 [cs.AI] UPDATED)\nAbstract: Input gradients have a pivotal role in a variety of applications, including adversarial attack algorithms for evaluating model robustness, explainable AI techniques for generating Saliency Maps, and counterfactual explanations. However, Saliency Maps generated by traditional neural networks are often noisy and provide limited insights. In this paper, we demonstrate that, on the contrary, the Saliency Maps of 1-Lipschitz neural networks, learnt with the dual loss of an optimal transportation problem, exhibit desirable XAI properties: They are highly concentrated on the essential parts of the image with low noise, significantly outperforming state-of-the-art explanation approaches across various models and metrics. We also prove that these maps align unprecedentedly well with human explanations on ImageNet. To explain the particularly beneficial properties of the Saliency Map for such models, we prove this gradient encodes both the direction of the transportation plan and the direction t",
    "path": "papers/22/06/2206.06854.json",
    "total_tokens": 976,
    "translated_abstract": "输入梯度在各种应用中都具有关键作用，包括评估模型鲁棒性的对抗性攻击算法、生成显著性地图（Saliency Maps）的可解释AI技术和反事实解释。然而，传统神经网络生成的显著性地图常常带有噪音并提供有限的信息。本文中，我们证明了利用最优传输问题的对偶损失学习的1-Lipschitz神经网络的Saliency Maps表现出理想的可解释性质：它们高度集中于图像的关键部分，噪音很小，在各种模型和度量方面显著优于现有的解释方法。我们还证明了这些显著性地图与ImageNet上的人类解释具有空前的契合度。为了解释这些模型显著性地图这种尤其有利的特性，我们证明了这种梯度既编码了传输计划的方向，也编码了方向t。",
    "tldr": "本文证明了利用最优传输问题的对偶损失学习的1-Lipschitz神经网络具有出色的可解释性，其生成的显著性地图高度集中、噪音小，并且与人类解释契合度空前。同时，这些显著性地图编码了传输计划和方向t的方向信息。",
    "en_tdlr": "This paper demonstrates that 1-Lipschitz neural networks, learned with the dual loss of an optimal transportation problem, exhibit high explainability through producing Salency Maps that are highly concentrated with low noise, outperforming state-of-the-art explanation methods. The generated Salency Maps encode both the direction of the transportation plan and the direction t, and align well with human explanations on ImageNet."
}