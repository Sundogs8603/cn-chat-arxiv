{
    "title": "Joint Generator-Ranker Learning for Natural Language Generation. (arXiv:2206.13974v3 [cs.CL] UPDATED)",
    "abstract": "Generate-then-rank is a widely used mechanism for text generation, where a generator produces multiple text candidates and a ranker chooses the best one among the text candidates. However, existing methods usually train the generator and the ranker individually, neglecting the mutual feedback that could further enhance the generation quality. To tackle this limitation, we propose JGR, a novel joint training algorithm that integrates the generator and the ranker in a single framework. JGR optimizes the generator with a hybrid objective that combines data likelihood and ranker reward, and trains the ranker with a contrastive loss that compares the generator outputs. By iteratively updating the generator and the ranker, JGR can effectively harmonize their learning and enhance their quality jointly. We evaluate JGR on various text generation tasks and demonstrate that it surpasses existing methods on four public datasets across three common generation scenarios. Our code and models are pub",
    "link": "http://arxiv.org/abs/2206.13974",
    "context": "Title: Joint Generator-Ranker Learning for Natural Language Generation. (arXiv:2206.13974v3 [cs.CL] UPDATED)\nAbstract: Generate-then-rank is a widely used mechanism for text generation, where a generator produces multiple text candidates and a ranker chooses the best one among the text candidates. However, existing methods usually train the generator and the ranker individually, neglecting the mutual feedback that could further enhance the generation quality. To tackle this limitation, we propose JGR, a novel joint training algorithm that integrates the generator and the ranker in a single framework. JGR optimizes the generator with a hybrid objective that combines data likelihood and ranker reward, and trains the ranker with a contrastive loss that compares the generator outputs. By iteratively updating the generator and the ranker, JGR can effectively harmonize their learning and enhance their quality jointly. We evaluate JGR on various text generation tasks and demonstrate that it surpasses existing methods on four public datasets across three common generation scenarios. Our code and models are pub",
    "path": "papers/22/06/2206.13974.json",
    "total_tokens": 978,
    "translated_title": "自然语言生成的生成器-评分器联合学习",
    "translated_abstract": "生成-排序（generate-then-rank）是文本生成中广泛使用的机制，其中生成器会产生多个文本候选项，评分器会在候选项中选择最佳的一个。然而，现有的方法通常单独对生成器和评分器进行训练，忽略了可以进一步提高生成质量的相互反馈。为了解决这个限制，我们提出了一种新的联合训练算法 JGR，它将生成器和评分器集成在一个单一的框架中。JGR通过混合目标来优化生成器，该混合目标结合了数据似然和评分器奖励，并使用对比损失训练评分器，对生成器输出进行比较。通过迭代更新生成器和评分器，JGR可以有效地协调它们的学习，并共同提高它们的质量。我们在各种文本生成任务上评估了JGR，并证明它在三种常见生成场景下的四个公共数据集上优于现有方法。我们的代码和模型已在 https://github.com/thudm/JGR 上发布。",
    "tldr": "提出了一种新的自然语言生成算法JGR，该算法将生成器和评分器集成在一个单一的框架中进行联合训练，通过混合目标优化生成器和使用对比损失训练评分器。在各种文本生成任务中，JGR在三种常见生成场景下的四个公共数据集上均优于现有方法。",
    "en_tdlr": "JGR is a novel algorithm for natural language generation that integrates the generator and the ranker in a single framework for joint training, optimizes the generator with a hybrid objective that combines data likelihood and ranker reward, and trains the ranker with a contrastive loss that compares the generator outputs. JGR outperforms existing methods on four public datasets across three common generation scenarios."
}