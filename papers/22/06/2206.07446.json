{
    "title": "Boosting DNN Cold Inference on Edge Devices. (arXiv:2206.07446v2 [cs.LG] UPDATED)",
    "abstract": "DNNs are ubiquitous on edge devices nowadays. With its increasing importance and use cases, it's not likely to pack all DNNs into device memory and expect that each inference has been warmed up. Therefore, cold inference, the process to read, initialize, and execute a DNN model, is becoming commonplace and its performance is urgently demanded to be optimized. To this end, we present NNV12, the first on-device inference engine that optimizes for cold inference NNV12 is built atop 3 novel optimization knobs: selecting a proper kernel (implementation) for each DNN operator, bypassing the weights transformation process by caching the post-transformed weights on disk, and pipelined execution of many kernels on asymmetric processors. To tackle with the huge search space, NNV12 employs a heuristic-based scheme to obtain a near-optimal kernel scheduling plan. We fully implement a prototype of NNV12 and evaluate its performance across extensive experiments. It shows that NNV12 achieves up to 15",
    "link": "http://arxiv.org/abs/2206.07446",
    "context": "Title: Boosting DNN Cold Inference on Edge Devices. (arXiv:2206.07446v2 [cs.LG] UPDATED)\nAbstract: DNNs are ubiquitous on edge devices nowadays. With its increasing importance and use cases, it's not likely to pack all DNNs into device memory and expect that each inference has been warmed up. Therefore, cold inference, the process to read, initialize, and execute a DNN model, is becoming commonplace and its performance is urgently demanded to be optimized. To this end, we present NNV12, the first on-device inference engine that optimizes for cold inference NNV12 is built atop 3 novel optimization knobs: selecting a proper kernel (implementation) for each DNN operator, bypassing the weights transformation process by caching the post-transformed weights on disk, and pipelined execution of many kernels on asymmetric processors. To tackle with the huge search space, NNV12 employs a heuristic-based scheme to obtain a near-optimal kernel scheduling plan. We fully implement a prototype of NNV12 and evaluate its performance across extensive experiments. It shows that NNV12 achieves up to 15",
    "path": "papers/22/06/2206.07446.json",
    "total_tokens": 939,
    "translated_title": "在边缘设备上提升DNN冷启动推理能力",
    "translated_abstract": "如今，DNN在边缘设备上十分常见。随着其重要性和使用案例的增加，不太可能将所有的DNN打包到设备内存中，并期望每个推理都已经预热。因此，冷启动推理，即读取、初始化和执行DNN模型的过程，变得普遍起来，并迫切需要优化其性能。为此，我们提出了NNV12，这是第一个针对冷启动推理进行优化的在设备上推理引擎。NNV12建立在三个新颖的优化策略上：为每个DNN算子选择适合的内核（实现方式），通过将后转换的权重缓存在磁盘上绕过权重转换过程，并在异构处理器上进行多个内核的流水线执行。为了应对巨大的搜索空间，NNV12采用基于启发式的方案获取近似最优的内核调度计划。我们完全实现了NNV12的原型，并通过广泛的实验评估了其性能。结果显示，NNV12的性能提升最高达到15倍。",
    "tldr": "NNV12是第一个在边缘设备上优化冷启动推理能力的推理引擎，通过选择合适的内核、缓存权重转换后的结果以及在异构处理器上进行流水线执行来提升性能，实验结果显示性能提升最高达到15倍。",
    "en_tdlr": "NNV12 is the first inference engine that optimizes cold inference on edge devices, achieving performance improvements of up to 15x through selecting proper kernels, caching transformed weights, and pipelining execution on asymmetric processors."
}