{
    "title": "Dynamic mean field programming. (arXiv:2206.05200v2 [stat.ML] UPDATED)",
    "abstract": "A dynamic mean field theory is developed for finite state and action Bayesian reinforcement learning in the large state space limit. In an analogy with statistical physics, the Bellman equation is studied as a disordered dynamical system; the Markov decision process transition probabilities are interpreted as couplings and the value functions as deterministic spins that evolve dynamically. Thus, the mean-rewards and transition probabilities are considered to be quenched random variables. The theory reveals that, under certain assumptions, the state-action values are statistically independent across state-action pairs in the asymptotic state space limit, and provides the form of the distribution exactly. The results hold in the finite and discounted infinite horizon settings, for both value iteration and policy evaluation. The state-action value statistics can be computed from a set of mean field equations, which we call dynamic mean field programming (DMFP). For policy evaluation the e",
    "link": "http://arxiv.org/abs/2206.05200",
    "context": "Title: Dynamic mean field programming. (arXiv:2206.05200v2 [stat.ML] UPDATED)\nAbstract: A dynamic mean field theory is developed for finite state and action Bayesian reinforcement learning in the large state space limit. In an analogy with statistical physics, the Bellman equation is studied as a disordered dynamical system; the Markov decision process transition probabilities are interpreted as couplings and the value functions as deterministic spins that evolve dynamically. Thus, the mean-rewards and transition probabilities are considered to be quenched random variables. The theory reveals that, under certain assumptions, the state-action values are statistically independent across state-action pairs in the asymptotic state space limit, and provides the form of the distribution exactly. The results hold in the finite and discounted infinite horizon settings, for both value iteration and policy evaluation. The state-action value statistics can be computed from a set of mean field equations, which we call dynamic mean field programming (DMFP). For policy evaluation the e",
    "path": "papers/22/06/2206.05200.json",
    "total_tokens": 889,
    "translated_title": "动态均场规划",
    "translated_abstract": "在大状态空间限制下，发展了一种动态均场理论，用于有限状态和行为的贝叶斯强化学习。类比于统计物理学，对贝尔曼方程进行了研究，将马尔科夫决策过程的转移概率解释为耦合，将值函数解释为动态演化的确定性自旋。因此，平均回报和转移概率被认为是淬灭随机变量。该理论揭示了在某些假设下，在渐近状态空间极限下，状态行为值在状态行为对之间具有统计独立性，并提供了确切的分布形式。这些结果适用于有限和无限折现时间视野，在价值迭代和策略评估中均成立。状态行为值的统计信息可以从一组均场方程中计算，我们称之为动态均场规划（DMFP）。对于策略评估，可以使用期望值迭代算法。",
    "tldr": "本文发展了一种动态均场规划方法，用于有限状态和行为的贝叶斯强化学习。通过模拟统计物理中的概念，研究了贝尔曼方程作为一种无序动力学系统，并通过均场方程计算状态行为值的统计信息。"
}