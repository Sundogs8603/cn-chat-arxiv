{
    "title": "Latent Diffusion Energy-Based Model for Interpretable Text Modeling. (arXiv:2206.05895v4 [cs.LG] UPDATED)",
    "abstract": "Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in generative modeling. Fueled by its flexibility in the formulation and strong modeling power of the latent space, recent works built upon it have made interesting attempts aiming at the interpretability of text modeling. However, latent space EBMs also inherit some flaws from EBMs in data space; the degenerate MCMC sampling quality in practice can lead to poor generation quality and instability in training, especially on data with complex latent structures. Inspired by the recent efforts that leverage diffusion recovery likelihood learning as a cure for the sampling issue, we introduce a novel symbiosis between the diffusion models and latent space EBMs in a variational learning framework, coined as the latent diffusion energy-based model. We develop a geometric clustering-based regularization jointly with the information bottleneck to further improve the quality of the learned la",
    "link": "http://arxiv.org/abs/2206.05895",
    "context": "Title: Latent Diffusion Energy-Based Model for Interpretable Text Modeling. (arXiv:2206.05895v4 [cs.LG] UPDATED)\nAbstract: Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in generative modeling. Fueled by its flexibility in the formulation and strong modeling power of the latent space, recent works built upon it have made interesting attempts aiming at the interpretability of text modeling. However, latent space EBMs also inherit some flaws from EBMs in data space; the degenerate MCMC sampling quality in practice can lead to poor generation quality and instability in training, especially on data with complex latent structures. Inspired by the recent efforts that leverage diffusion recovery likelihood learning as a cure for the sampling issue, we introduce a novel symbiosis between the diffusion models and latent space EBMs in a variational learning framework, coined as the latent diffusion energy-based model. We develop a geometric clustering-based regularization jointly with the information bottleneck to further improve the quality of the learned la",
    "path": "papers/22/06/2206.05895.json",
    "total_tokens": 892,
    "translated_title": "深度扩散能量模型用于可解释文本建模",
    "translated_abstract": "潜在空间能量模型（EBMs）在生成建模中引起了越来越多的关注。最近的研究工作在其基础上进行了有趣的尝试，旨在实现文本建模的可解释性。然而，潜在空间EBMs也继承了数据空间EBMs的一些缺陷；实践中退化的MCMC采样质量可能导致生成质量差和训练不稳定，尤其是在具有复杂潜在结构的数据上。受到最近利用扩散恢复似然学习作为解决采样问题的努力的启发，我们在变分学习框架中引入了扩散模型和潜在空间EBMs之间的新型共生关系，称为潜在扩散能量模型。我们开发了基于几何聚类的正则化方法，以及信息瓶颈来进一步提高学习潜空间质量。",
    "tldr": "该论文介绍了一种新颖的深度扩散能量模型，通过在变分学习框架中引入扩散模型和潜在空间EBMs之间的共生关系，解决了潜在空间EBMs在采样质量和训练稳定性方面的问题。",
    "en_tdlr": "This paper introduces a novel latent diffusion energy-based model that addresses the sampling quality and training stability issues of latent space EBMs by incorporating diffusion models in a variational learning framework."
}