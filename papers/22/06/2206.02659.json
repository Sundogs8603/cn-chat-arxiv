{
    "title": "Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees. (arXiv:2206.02659v5 [cs.LG] UPDATED)",
    "abstract": "We consider fine-tuning a pretrained deep neural network on a target task. We study the generalization properties of fine-tuning to understand the problem of overfitting, which has often been observed (e.g., when the target dataset is small or when the training labels are noisy). Existing generalization measures for deep networks depend on notions such as distance from the initialization (i.e., the pretrained network) of the fine-tuned model and noise stability properties of deep networks. This paper identifies a Hessian-based distance measure through PAC-Bayesian analysis, which is shown to correlate well with observed generalization gaps of fine-tuned models. Theoretically, we prove Hessian distance-based generalization bounds for fine-tuned models. We also describe an extended study of fine-tuning against label noise, where overfitting is against a critical problem; We present an algorithm and a generalization error guarantee for this algorithm under a class conditional independent ",
    "link": "http://arxiv.org/abs/2206.02659",
    "context": "Title: Robust Fine-Tuning of Deep Neural Networks with Hessian-based Generalization Guarantees. (arXiv:2206.02659v5 [cs.LG] UPDATED)\nAbstract: We consider fine-tuning a pretrained deep neural network on a target task. We study the generalization properties of fine-tuning to understand the problem of overfitting, which has often been observed (e.g., when the target dataset is small or when the training labels are noisy). Existing generalization measures for deep networks depend on notions such as distance from the initialization (i.e., the pretrained network) of the fine-tuned model and noise stability properties of deep networks. This paper identifies a Hessian-based distance measure through PAC-Bayesian analysis, which is shown to correlate well with observed generalization gaps of fine-tuned models. Theoretically, we prove Hessian distance-based generalization bounds for fine-tuned models. We also describe an extended study of fine-tuning against label noise, where overfitting is against a critical problem; We present an algorithm and a generalization error guarantee for this algorithm under a class conditional independent ",
    "path": "papers/22/06/2206.02659.json",
    "total_tokens": 970,
    "translated_title": "具有基于Hessian的泛化保证的深度神经网络鲁棒微调",
    "translated_abstract": "本文考虑在目标任务上对预训练的深度神经网络进行微调。我们研究微调的泛化特性，以理解过拟合问题，这在目标数据集较小或训练标签噪声时经常观察到。现有的深度网络泛化度量依赖于与微调模型的初始化（即预训练网络）距离和深度网络的噪声稳定性等概念。本文通过PAC-Bayesian分析确定了一种基于Hessian的距离度量，它与微调模型的观察到的泛化差距相关性很强。从理论上我们证明了基于Hessian距离的微调模型的泛化界。我们还对微调对抗标签噪声进行了扩展研究，过拟合是一个关键问题；我们提出了一种算法，并在类条件独立假设下给出了该算法的泛化误差保证。",
    "tldr": "本文通过Hessian-based分析，提出一种基于距离的泛化度量方法，用于理解深度神经网络微调的泛化特性。通过PAC-Bayesian分析，给出了基于Hessian距离的微调模型泛化界。此外，还对微调面对标签噪声的问题进行了研究，并提出了一种相关算法和泛化误差保证。"
}