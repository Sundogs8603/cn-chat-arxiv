{
    "title": "When Does Re-initialization Work?. (arXiv:2206.10011v2 [cs.LG] UPDATED)",
    "abstract": "Re-initializing a neural network during training has been observed to improve generalization in recent works. Yet it is neither widely adopted in deep learning practice nor is it often used in state-of-the-art training protocols. This raises the question of when re-initialization works, and whether it should be used together with regularization techniques such as data augmentation, weight decay and learning rate schedules. In this work, we conduct an extensive empirical comparison of standard training with a selection of re-initialization methods to answer this question, training over 15,000 models on a variety of image classification benchmarks. We first establish that such methods are consistently beneficial for generalization in the absence of any other regularization. However, when deployed alongside other carefully tuned regularization techniques, re-initialization methods offer little to no added benefit for generalization, although optimal generalization performance becomes less",
    "link": "http://arxiv.org/abs/2206.10011",
    "context": "Title: When Does Re-initialization Work?. (arXiv:2206.10011v2 [cs.LG] UPDATED)\nAbstract: Re-initializing a neural network during training has been observed to improve generalization in recent works. Yet it is neither widely adopted in deep learning practice nor is it often used in state-of-the-art training protocols. This raises the question of when re-initialization works, and whether it should be used together with regularization techniques such as data augmentation, weight decay and learning rate schedules. In this work, we conduct an extensive empirical comparison of standard training with a selection of re-initialization methods to answer this question, training over 15,000 models on a variety of image classification benchmarks. We first establish that such methods are consistently beneficial for generalization in the absence of any other regularization. However, when deployed alongside other carefully tuned regularization techniques, re-initialization methods offer little to no added benefit for generalization, although optimal generalization performance becomes less",
    "path": "papers/22/06/2206.10011.json",
    "total_tokens": 909,
    "translated_title": "何时重新初始化神经网络是有效的？",
    "translated_abstract": "在最近的研究中观察到，重新初始化神经网络在训练过程中有助于提高泛化性能。然而，在深度学习实践中，这种方法并不被广泛采用，也不常用于最先进的训练协议中。因此，本文提出了一个问题，即重新初始化什么时候有效，是否应该与数据增强、权重衰减和学习率调整等正则化技术一起使用。本研究对标准训练与一些重新初始化方法进行了广泛的实证比较，训练了超过15,000个模型，并在各种图像分类基准上进行了测试，以回答这个问题。我们首先确定，当没有任何其他正则化存在时，这些方法可以在泛化方面持续改善。然而，当这些方法与其他经过精心调整的正则化技术一起使用时，重新初始化方法对泛化性能几乎没有额外的帮助，尽管在这种情况下，最佳泛化性能变得更加稳定。",
    "tldr": "研究表明，当没有其他正则化技术时，重新初始化神经网络有助于提高泛化性能。但是，当它与其他正则化技术一起使用时，对泛化性能的额外帮助很小，尽管最佳泛化性能变得更加稳定。",
    "en_tdlr": "Re-initialization of neural networks has been shown to improve generalization performance when used without other regularization techniques. However, when combined with other carefully tuned regularization techniques, re-initialization methods offer little to no added benefit for generalization, although optimal generalization performance becomes more stable."
}