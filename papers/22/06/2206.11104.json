{
    "title": "OpenXAI: Towards a Transparent Evaluation of Model Explanations",
    "abstract": "arXiv:2206.11104v4 Announce Type: replace-cross  Abstract: While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, and (ii) open-source implementations of eleven quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, in turn providing comparisons of several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI ",
    "link": "https://arxiv.org/abs/2206.11104",
    "context": "Title: OpenXAI: Towards a Transparent Evaluation of Model Explanations\nAbstract: arXiv:2206.11104v4 Announce Type: replace-cross  Abstract: While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, and (ii) open-source implementations of eleven quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, in turn providing comparisons of several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI ",
    "path": "papers/22/06/2206.11104.json",
    "total_tokens": 836,
    "translated_title": "OpenXAI: 迈向透明评估模型解释",
    "translated_abstract": "虽然最近文献中提出了几种后续解释方法，但对这些方法进行系统性基准测试的工作非常少。在这里，我们介绍了OpenXAI，一个全面且可扩展的开源框架，用于评估和基准测试后续解释方法。OpenXAI包括以下关键组件：（i）灵活的合成数据生成器和各种真实世界数据集、预训练模型和最先进特征归属方法的集合，以及（ii）用于评估解释方法忠实度、稳定性（鲁棒性）和公平性的十一种量化度量标准的开源实现，从而提供了对多种度量标准、模型和数据集上几种解释方法的比较。OpenXAI易于扩展，用户可以轻松评估自定义解释方法并将其纳入我们的排行榜中。",
    "tldr": "OpenXAI 是一个开源框架，旨在评估和基准测试后续解释方法，提供了灵活的数据生成器、多种数据集和评估指标，用户可轻松扩展和比较不同解释方法。",
    "en_tdlr": "OpenXAI is an open-source framework designed for evaluating and benchmarking post hoc explanation methods, providing a flexible data generator, diverse datasets, and evaluation metrics for users to easily extend and compare different explanation methods."
}