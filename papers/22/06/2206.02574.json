{
    "title": "On the duality between contrastive and non-contrastive self-supervised learning. (arXiv:2206.02574v3 [cs.LG] UPDATED)",
    "abstract": "Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also ch",
    "link": "http://arxiv.org/abs/2206.02574",
    "context": "Title: On the duality between contrastive and non-contrastive self-supervised learning. (arXiv:2206.02574v3 [cs.LG] UPDATED)\nAbstract: Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg's with careful hyperparameter tuning, improving significantly over known baselines. We also ch",
    "path": "papers/22/06/2206.02574.json",
    "total_tokens": 907,
    "tldr": "本文研究对比自监督学习和非对比自监督学习之间的对偶关系，展示了这两种方法在一定条件下的等价性，并探究了设计选择对实际应用下性能的影响。文章还发现了SimCLR表现不佳的原因并提出了改进方法，达到了VICReg的性能。",
    "en_tdlr": "This paper explores the duality between contrastive and non-contrastive self-supervised learning methods, and shows the equivalence between them under certain assumptions. The paper also investigates the impact of design choices on practical performance, and proposes a method for improving the poor performance of SimCLR."
}