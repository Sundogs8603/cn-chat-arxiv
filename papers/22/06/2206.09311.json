{
    "title": "Primal Estimated Subgradient Solver for SVM for Imbalanced Classification. (arXiv:2206.09311v4 [cs.LG] UPDATED)",
    "abstract": "We aim to demonstrate in experiments that our cost sensitive PEGASOS SVM achieves good performance on imbalanced data sets with a Majority to Minority Ratio ranging from 8.6:1 to 130:1 and to ascertain whether the including intercept (bias), regularization and parameters affects performance on our selection of datasets. Although many resort to SMOTE methods, we aim for a less computationally intensive method. We evaluate the performance by examining the learning curves. These curves diagnose whether we overfit or underfit or we choose over representative or under representative training/test data. We will also see the background of the hyperparameters versus the test and train error in validation curves. We benchmark our PEGASOS Cost-Sensitive SVM's results of Ding's LINEAR SVM DECIDL method. He obtained an ROC-AUC of .5 in one dataset. Our work will extend the work of Ding by incorporating kernels into SVM. We will use Python rather than MATLAB as python has dictionaries for storing m",
    "link": "http://arxiv.org/abs/2206.09311",
    "context": "Title: Primal Estimated Subgradient Solver for SVM for Imbalanced Classification. (arXiv:2206.09311v4 [cs.LG] UPDATED)\nAbstract: We aim to demonstrate in experiments that our cost sensitive PEGASOS SVM achieves good performance on imbalanced data sets with a Majority to Minority Ratio ranging from 8.6:1 to 130:1 and to ascertain whether the including intercept (bias), regularization and parameters affects performance on our selection of datasets. Although many resort to SMOTE methods, we aim for a less computationally intensive method. We evaluate the performance by examining the learning curves. These curves diagnose whether we overfit or underfit or we choose over representative or under representative training/test data. We will also see the background of the hyperparameters versus the test and train error in validation curves. We benchmark our PEGASOS Cost-Sensitive SVM's results of Ding's LINEAR SVM DECIDL method. He obtained an ROC-AUC of .5 in one dataset. Our work will extend the work of Ding by incorporating kernels into SVM. We will use Python rather than MATLAB as python has dictionaries for storing m",
    "path": "papers/22/06/2206.09311.json",
    "total_tokens": 943,
    "translated_title": "基于原始估计亚梯度求解器的不平衡分类SVM",
    "translated_abstract": "本研究旨在通过实验证明我们的成本敏感PEGASOS SVM在主多次要比从8.6：1到130：1的不平衡数据集上具有良好的性能，并确定包括截距（偏见）、正则化和参数是否会影响我们选择的数据集上的性能。虽然许多人采用SMOTE方法，但我们旨在采用一种计算量较小的方法。通过检查学习曲线来评估性能，这些曲线可以诊断我们是过度拟合还是欠拟合，或者我们选择了过度代表性或欠代表性的训练/测试数据。我们还将在验证曲线中查看超参数的背景与测试和训练误差之间的关系。我们将基准化我们的PEGASOS成本敏感SVM与Ding的LINEAR SVM DECIDL方法的结果。他在一个数据集中获得了0.5的ROC-AUC。我们的工作将通过将核函数纳入SVM来扩展Ding的工作。我们将使用Python而不是MATLAB，因为Python具有更有效地存储我的数据集的字典。",
    "tldr": "本研究旨在实现对不平衡数据集的分类，并评估成本敏感的PEGASOS SVM的性能，同时将核函数纳入SVM中扩展Ding的工作。",
    "en_tdlr": "This study aims to achieve classification on imbalanced datasets and evaluate the performance of cost-sensitive PEGASOS SVM, while extending the work of Ding by incorporating kernels into SVM. The learning curves were examined to diagnose overfitting, underfitting and representativeness of data, and Python was used for storing data more efficiently."
}