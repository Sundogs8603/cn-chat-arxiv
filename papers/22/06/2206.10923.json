{
    "title": "FairGrad: Fairness Aware Gradient Descent. (arXiv:2206.10923v2 [cs.LG] UPDATED)",
    "abstract": "We address the problem of group fairness in classification, where the objective is to learn models that do not unjustly discriminate against subgroups of the population. Most existing approaches are limited to simple binary tasks or involve difficult to implement training mechanisms which reduces their practical applicability. In this paper, we propose FairGrad, a method to enforce fairness based on a re-weighting scheme that iteratively learns group specific weights based on whether they are advantaged or not. FairGrad is easy to implement, accommodates various standard fairness definitions, and comes with minimal overhead. Furthermore, we show that it is competitive with standard baselines over various datasets including ones used in natural language processing and computer vision.  FairGrad is available as a PyPI package at https://pypi.org/project/fairgrad",
    "link": "http://arxiv.org/abs/2206.10923",
    "context": "Title: FairGrad: Fairness Aware Gradient Descent. (arXiv:2206.10923v2 [cs.LG] UPDATED)\nAbstract: We address the problem of group fairness in classification, where the objective is to learn models that do not unjustly discriminate against subgroups of the population. Most existing approaches are limited to simple binary tasks or involve difficult to implement training mechanisms which reduces their practical applicability. In this paper, we propose FairGrad, a method to enforce fairness based on a re-weighting scheme that iteratively learns group specific weights based on whether they are advantaged or not. FairGrad is easy to implement, accommodates various standard fairness definitions, and comes with minimal overhead. Furthermore, we show that it is competitive with standard baselines over various datasets including ones used in natural language processing and computer vision.  FairGrad is available as a PyPI package at https://pypi.org/project/fairgrad",
    "path": "papers/22/06/2206.10923.json",
    "total_tokens": 836,
    "translated_title": "公平感知梯度下降：FairGrad",
    "translated_abstract": "本文解决了分类中的群体公平性问题，目标是学习不会不公平歧视人群子集的模型。之前的方法大多限于简单的二分类任务，或者涉及难以实现的训练机制，降低了它们的实际可应用性。本文提出了一种名为FairGrad的方法，它基于重新加权方案来强化公平性，通过迭代学习群体特定的权重，这些权重取决于是否具有优势。FairGrad易于实现，适用于各种标准的公平性定义，并且开销最小。此外，我们还展示了FairGrad在包括自然语言处理和计算机视觉在内的各种数据集上与标准基线方法具有竞争力。FairGrad可以在https://pypi.org/project/fairgrad上作为一个PyPI包获得。",
    "tldr": "FairGrad是一种公平感知梯度下降方法，通过重新加权方案迭代学习群体特定权重来实现群体公平性。它易于实现，适用于各种标准的公平性定义，并且在各种数据集上与标准基线方法具有竞争力。",
    "en_tdlr": "FairGrad is a fairness-aware gradient descent method that enforces group fairness by iteratively learning group-specific weights through a re-weighting scheme. It is easy to implement, accommodates various standard fairness definitions, and is competitive with standard baselines over various datasets."
}