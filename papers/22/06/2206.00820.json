{
    "title": "NIPQ: Noise proxy-based Integrated Pseudo-Quantization. (arXiv:2206.00820v2 [cs.LG] UPDATED)",
    "abstract": "Straight-through estimator (STE), which enables the gradient flow over the non-differentiable function via approximation, has been favored in studies related to quantization-aware training (QAT). However, STE incurs unstable convergence during QAT, resulting in notable quality degradation in low precision. Recently, pseudoquantization training has been proposed as an alternative approach to updating the learnable parameters using the pseudo-quantization noise instead of STE. In this study, we propose a novel noise proxy-based integrated pseudoquantization (NIPQ) that enables unified support of pseudoquantization for both activation and weight by integrating the idea of truncation on the pseudo-quantization framework. NIPQ updates all of the quantization parameters (e.g., bit-width and truncation boundary) as well as the network parameters via gradient descent without STE instability. According to our extensive experiments, NIPQ outperforms existing quantization algorithms in various vi",
    "link": "http://arxiv.org/abs/2206.00820",
    "context": "Title: NIPQ: Noise proxy-based Integrated Pseudo-Quantization. (arXiv:2206.00820v2 [cs.LG] UPDATED)\nAbstract: Straight-through estimator (STE), which enables the gradient flow over the non-differentiable function via approximation, has been favored in studies related to quantization-aware training (QAT). However, STE incurs unstable convergence during QAT, resulting in notable quality degradation in low precision. Recently, pseudoquantization training has been proposed as an alternative approach to updating the learnable parameters using the pseudo-quantization noise instead of STE. In this study, we propose a novel noise proxy-based integrated pseudoquantization (NIPQ) that enables unified support of pseudoquantization for both activation and weight by integrating the idea of truncation on the pseudo-quantization framework. NIPQ updates all of the quantization parameters (e.g., bit-width and truncation boundary) as well as the network parameters via gradient descent without STE instability. According to our extensive experiments, NIPQ outperforms existing quantization algorithms in various vi",
    "path": "papers/22/06/2206.00820.json",
    "total_tokens": 940,
    "translated_title": "NIPQ: 基于噪声代理的集成伪量化",
    "translated_abstract": "直通估计器（STE）在量化感知训练（QAT）相关研究中备受青睐，它通过近似使梯度流过非可微函数。然而，在QAT中，STE导致不稳定的收敛，降低了低精度的质量。最近，伪量化训练被提出作为使用伪量化噪声而不是STE更新可学习参数的替代方法。在这项研究中，我们提出了一种新颖的基于噪声代理的集成伪量化（NIPQ）方法，通过将截断理念融入伪量化框架，为激活和权重两方面提供了统一的伪量化支持。NIPQ通过梯度下降更新所有的量化参数（如位宽和截断边界）以及网络参数，避免了STE不稳定性。根据我们的广泛实验，NIPQ在各种视图下表现优于现有的量化算法。",
    "tldr": "NIPQ提出了一种基于噪声代理的集成伪量化方法，通过融合截断理念，能够统一支持激活和权重的伪量化。NIPQ避免了直通估计器(STE)在量化感知训练中的不稳定性，并在实验中展现了优于现有量化算法的性能。",
    "en_tdlr": "NIPQ proposes a noise proxy-based integrated pseudoquantization method that unifies support for pseudoquantization of both activation and weight, avoiding the instability of straight-through estimator (STE) in quantization-aware training (QAT) and outperforming existing quantization algorithms in experiments."
}