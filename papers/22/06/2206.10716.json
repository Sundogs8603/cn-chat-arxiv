{
    "title": "Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach",
    "abstract": "arXiv:2206.10716v2 Announce Type: replace-cross  Abstract: In meta reinforcement learning (meta RL), an agent learns from a set of training tasks how to quickly solve a new task, drawn from the same task distribution. The optimal meta RL policy, a.k.a. the Bayes-optimal behavior, is well defined, and guarantees optimal reward in expectation, taken with respect to the task distribution. The question we explore in this work is how many training tasks are required to guarantee approximately optimal behavior with high probability. Recent work provided the first such PAC analysis for a model-free setting, where a history-dependent policy was learned from the training tasks. In this work, we propose a different approach: directly learn the task distribution, using density estimation techniques, and then train a policy on the learned task distribution. We show that our approach leads to bounds that depend on the dimension of the task distribution. In particular, in settings where the task dis",
    "link": "https://arxiv.org/abs/2206.10716",
    "context": "Title: Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach\nAbstract: arXiv:2206.10716v2 Announce Type: replace-cross  Abstract: In meta reinforcement learning (meta RL), an agent learns from a set of training tasks how to quickly solve a new task, drawn from the same task distribution. The optimal meta RL policy, a.k.a. the Bayes-optimal behavior, is well defined, and guarantees optimal reward in expectation, taken with respect to the task distribution. The question we explore in this work is how many training tasks are required to guarantee approximately optimal behavior with high probability. Recent work provided the first such PAC analysis for a model-free setting, where a history-dependent policy was learned from the training tasks. In this work, we propose a different approach: directly learn the task distribution, using density estimation techniques, and then train a policy on the learned task distribution. We show that our approach leads to bounds that depend on the dimension of the task distribution. In particular, in settings where the task dis",
    "path": "papers/22/06/2206.10716.json",
    "total_tokens": 841,
    "translated_title": "具有有限训练任务的元强化学习--一种密度估计方法",
    "translated_abstract": "在元强化学习(meta RL)中，一个智能体通过一组训练任务学习如何快速解决一个新任务，新任务来自相同的任务分布。优化的元RL策略，即贝叶斯最优行为，是明确定义的，并保证期望下相对于任务分布的最优奖励。我们在这项工作中探讨的问题是需要多少个训练任务才能保证以高概率近似地获得最优行为。最近的工作为模型无关设置提供了第一个这样的PAC分析，其中从训练任务中学习了一种历史相关的策略。在这项工作中，我们提出了一种不同的方法：直接学习任务分布，使用密度估计技术，然后在学习的任务分布上训练策略。我们展示了我们的方法导致依赖于任务分布维度的界限。特别是，在任务分布维度较大的情况下。",
    "tldr": "通过密度估计技术直接学习任务分布，从而对元强化学习中所需训练任务数提出了新的界限分析方法",
    "en_tdlr": "Proposing a new analysis approach for the required number of training tasks in meta reinforcement learning by directly learning the task distribution using density estimation techniques."
}