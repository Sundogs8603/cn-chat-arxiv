{
    "title": "Beyond the Edge of Stability via Two-step Gradient Updates. (arXiv:2206.04172v3 [cs.LG] UPDATED)",
    "abstract": "Gradient Descent (GD) is a powerful workhorse of modern machine learning thanks to its scalability and efficiency in high-dimensional spaces. Its ability to find local minimisers is only guaranteed for losses with Lipschitz gradients, where it can be seen as a `bona-fide' discretisation of an underlying gradient flow. Yet, many ML setups involving overparametrised models do not fall into this problem class, which has motivated research beyond the so-called ``Edge of Stability'' (EoS), where the step-size crosses the admissibility threshold inversely proportional to the Lipschitz constant above. Perhaps surprisingly, GD has been empirically observed to still converge regardless of local instability and oscillatory behavior.  The incipient theoretical analysis of this phenomena has mainly focused in the overparametrised regime, where the effect of choosing a large learning rate may be associated to a `Sharpness-Minimisation' implicit regularisation within the manifold of minimisers, unde",
    "link": "http://arxiv.org/abs/2206.04172",
    "context": "Title: Beyond the Edge of Stability via Two-step Gradient Updates. (arXiv:2206.04172v3 [cs.LG] UPDATED)\nAbstract: Gradient Descent (GD) is a powerful workhorse of modern machine learning thanks to its scalability and efficiency in high-dimensional spaces. Its ability to find local minimisers is only guaranteed for losses with Lipschitz gradients, where it can be seen as a `bona-fide' discretisation of an underlying gradient flow. Yet, many ML setups involving overparametrised models do not fall into this problem class, which has motivated research beyond the so-called ``Edge of Stability'' (EoS), where the step-size crosses the admissibility threshold inversely proportional to the Lipschitz constant above. Perhaps surprisingly, GD has been empirically observed to still converge regardless of local instability and oscillatory behavior.  The incipient theoretical analysis of this phenomena has mainly focused in the overparametrised regime, where the effect of choosing a large learning rate may be associated to a `Sharpness-Minimisation' implicit regularisation within the manifold of minimisers, unde",
    "path": "papers/22/06/2206.04172.json",
    "total_tokens": 917,
    "translated_title": "通过两步梯度更新超越稳定边界",
    "translated_abstract": "梯度下降（GD）是现代机器学习中的强大工具，因其在高维空间中的可扩展性和效率而闻名。对于具有Lipschitz梯度的损失函数，GD只能找到局部极小值点，可以看作是潜在梯度流的“真实”离散化方法。然而，许多涉及过参数化模型的机器学习设置并不属于这个问题类别，这促使研究超越所谓的“稳定边界”（Edge of Stability，EoS），其中步长越过与Lipschitz常数成反比的可允许阈值。令人惊讶的是，经验证明尽管存在局部不稳定性和振荡行为，GD仍然收敛。对这一现象的初步理论分析主要集中在过参数化的范围内，在此范围内选择较大的学习率可能与在最小化器流形内隐含的“尖度最小化”正则化相关，请详见论文了解更多细节。",
    "tldr": "本文研究了梯度下降对于非Lipschitz梯度的损失函数的收敛性，发现在过参数化模型中，尽管存在局部不稳定性和振荡行为，梯度下降仍然能够收敛。"
}