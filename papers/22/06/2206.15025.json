{
    "title": "On the Convergence of Distributed Stochastic Bilevel Optimization Algorithms over a Network. (arXiv:2206.15025v2 [cs.LG] UPDATED)",
    "abstract": "Bilevel optimization has been applied to a wide variety of machine learning models, and numerous stochastic bilevel optimization algorithms have been developed in recent years. However, most existing algorithms restrict their focus on the single-machine setting so that they are incapable of handling the distributed data. To address this issue, under the setting where all participants compose a network and perform peer-to-peer communication in this network, we developed two novel decentralized stochastic bilevel optimization algorithms based on the gradient tracking communication mechanism and two different gradient estimators. Additionally, we established their convergence rates for nonconvex-strongly-convex problems with novel theoretical analysis strategies. To our knowledge, this is the first work achieving these theoretical results. Finally, we applied our algorithms to practical machine learning models, and the experimental results confirmed the efficacy of our algorithms.",
    "link": "http://arxiv.org/abs/2206.15025",
    "context": "Title: On the Convergence of Distributed Stochastic Bilevel Optimization Algorithms over a Network. (arXiv:2206.15025v2 [cs.LG] UPDATED)\nAbstract: Bilevel optimization has been applied to a wide variety of machine learning models, and numerous stochastic bilevel optimization algorithms have been developed in recent years. However, most existing algorithms restrict their focus on the single-machine setting so that they are incapable of handling the distributed data. To address this issue, under the setting where all participants compose a network and perform peer-to-peer communication in this network, we developed two novel decentralized stochastic bilevel optimization algorithms based on the gradient tracking communication mechanism and two different gradient estimators. Additionally, we established their convergence rates for nonconvex-strongly-convex problems with novel theoretical analysis strategies. To our knowledge, this is the first work achieving these theoretical results. Finally, we applied our algorithms to practical machine learning models, and the experimental results confirmed the efficacy of our algorithms.",
    "path": "papers/22/06/2206.15025.json",
    "total_tokens": 845,
    "translated_title": "关于网络中分布式随机双层优化算法的收敛性研究",
    "translated_abstract": "双层优化算法已被应用于各种机器学习模型中，并且近年来已开发出许多随机双层优化算法。然而，大多数现有算法都将重点放在单机设置上，因此无法处理分布式数据。为了解决这个问题，在所有参与者组成网络并在该网络中进行点对点通信的情况下，我们开发了两种基于渐变跟踪通信机制和两种不同梯度估计器的新型分散式随机双层优化算法。此外，我们运用新型理论分析策略证明了这些算法在非凸强凸问题方面的收敛速度。据我们所知，这是第一个实现这些理论结果的研究。最后，我们将算法应用于实际机器学习模型中，实验结果证实了我们算法的有效性。",
    "tldr": "本论文提出了两种新型分散式随机双层优化算法，在网络中解决了分布式数据的问题，并初步证明了其在非凸强凸问题方面的收敛速度。实验结果表明，算法是有效的。",
    "en_tdlr": "This paper proposes two novel decentralized stochastic bilevel optimization algorithms to handle distributed data over a network, and preliminarily proves their convergence rates for nonconvex-strongly-convex problems. Experimental results confirm their efficacy."
}