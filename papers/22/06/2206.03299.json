{
    "title": "Generalization Error Bounds for Deep Neural Networks Trained by SGD. (arXiv:2206.03299v2 [cs.LG] UPDATED)",
    "abstract": "Generalization error bounds for deep neural networks trained by stochastic gradient descent (SGD) are derived by combining a dynamical control of an appropriate parameter norm and the Rademacher complexity estimate based on parameter norms. The bounds explicitly depend on the loss along the training trajectory, and work for a wide range of network architectures including multilayer perceptron (MLP) and convolutional neural networks (CNN). Compared with other algorithm-depending generalization estimates such as uniform stability-based bounds, our bounds do not require $L$-smoothness of the nonconvex loss function, and apply directly to SGD instead of Stochastic Langevin gradient descent (SGLD). Numerical results show that our bounds are non-vacuous and robust with the change of optimizer and network hyperparameters.",
    "link": "http://arxiv.org/abs/2206.03299",
    "context": "Title: Generalization Error Bounds for Deep Neural Networks Trained by SGD. (arXiv:2206.03299v2 [cs.LG] UPDATED)\nAbstract: Generalization error bounds for deep neural networks trained by stochastic gradient descent (SGD) are derived by combining a dynamical control of an appropriate parameter norm and the Rademacher complexity estimate based on parameter norms. The bounds explicitly depend on the loss along the training trajectory, and work for a wide range of network architectures including multilayer perceptron (MLP) and convolutional neural networks (CNN). Compared with other algorithm-depending generalization estimates such as uniform stability-based bounds, our bounds do not require $L$-smoothness of the nonconvex loss function, and apply directly to SGD instead of Stochastic Langevin gradient descent (SGLD). Numerical results show that our bounds are non-vacuous and robust with the change of optimizer and network hyperparameters.",
    "path": "papers/22/06/2206.03299.json",
    "total_tokens": 882,
    "translated_title": "由 SGD 训练的深度神经网络的泛化误差界",
    "translated_abstract": "本文通过将适当参数规范的动态控制和基于参数规范的 Rademacher 复杂度估计相结合，导出了由随机梯度下降（SGD）训练的深度神经网络的泛化误差界。这些界明确取决于沿训练轨迹的损失，并适用于包括多层感知机（MLP）和卷积神经网络（CNN）在内的广泛网络架构。与其他算法依赖的泛化估计（如基于全局稳定性的界）相比，我们的界不需要非凸损失函数的 $L$-平滑性，并且直接适用于 SGD，而不是随机 Langevin 梯度下降（SGLD）。数值结果表明，我们的界是非虚假和强健的，能够适应优化器和网络超参数的变化。",
    "tldr": "通过对于适当参数规范的动态控制结合基于参数规范的 Rademacher 复杂度估计导出了深度神经网络的泛化误差界，适用于包括 MLP 和 CNN 在内的广泛网络架构，结果表明这个方法能够适应优化器和网络超参数的变化。",
    "en_tdlr": "This paper derives generalization error bounds for deep neural networks trained by SGD, by combining dynamical control of an appropriate parameter norm and the Rademacher complexity estimate based on parameter norms. The bounds work for a wide range of network architectures including MLP and CNN, and are non-vacuous and robust with changes in optimizer and network hyperparameters."
}