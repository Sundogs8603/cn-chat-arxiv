{
    "title": "Memorization-Dilation: Modeling Neural Collapse Under Label Noise. (arXiv:2206.05530v3 [cs.LG] UPDATED)",
    "abstract": "The notion of neural collapse refers to several emergent phenomena that have been empirically observed across various canonical classification problems. During the terminal phase of training a deep neural network, the feature embedding of all examples of the same class tend to collapse to a single representation, and the features of different classes tend to separate as much as possible. Neural collapse is often studied through a simplified model, called the unconstrained feature representation, in which the model is assumed to have \"infinite expressivity\" and can map each data point to any arbitrary representation. In this work, we propose a more realistic variant of the unconstrained feature representation that takes the limited expressivity of the network into account. Empirical evidence suggests that the memorization of noisy data points leads to a degradation (dilation) of the neural collapse. Using a model of the memorization-dilation (M-D) phenomenon, we show one mechanism by wh",
    "link": "http://arxiv.org/abs/2206.05530",
    "context": "Title: Memorization-Dilation: Modeling Neural Collapse Under Label Noise. (arXiv:2206.05530v3 [cs.LG] UPDATED)\nAbstract: The notion of neural collapse refers to several emergent phenomena that have been empirically observed across various canonical classification problems. During the terminal phase of training a deep neural network, the feature embedding of all examples of the same class tend to collapse to a single representation, and the features of different classes tend to separate as much as possible. Neural collapse is often studied through a simplified model, called the unconstrained feature representation, in which the model is assumed to have \"infinite expressivity\" and can map each data point to any arbitrary representation. In this work, we propose a more realistic variant of the unconstrained feature representation that takes the limited expressivity of the network into account. Empirical evidence suggests that the memorization of noisy data points leads to a degradation (dilation) of the neural collapse. Using a model of the memorization-dilation (M-D) phenomenon, we show one mechanism by wh",
    "path": "papers/22/06/2206.05530.json",
    "total_tokens": 988,
    "translated_title": "记忆-膨胀：建模标签噪声下神经崩溃",
    "translated_abstract": "神经崩溃是指在各种典型分类问题中经验观察到的多种紧急现象。在训练深度神经网络的终止阶段，同一类别的所有示例的特征嵌入 tend to collapse 到单个表示，并且不同类别的特征 tend to separate。本文提出了一个更现实的无限制特征表示变体，它考虑了网络的有限表达性。实证证据表明记忆噪声数据点会导致神经崩溃的恶化（膨胀）。使用记忆膨胀（M-D）现象模型，我们展示了标签噪声如何导致神经崩溃的机制。具体而言，我们提出了一种结合记忆和 dropout 的新模型 MD-Dropout，作为一种正则化器来防止膨胀神经崩溃。我们的实验表明，MD-Dropout 提高了标签噪声的鲁棒性，并在多个数据集上优于竞争方法。",
    "tldr": "本文提出了一个更现实的无限制特征表示变体，它考虑了网络的有限表达性。结合记忆和 dropout 的新模型 MD-Dropout 有效防止了膨胀神经崩溃，并提高了鲁棒性。",
    "en_tdlr": "The paper proposes a new variant of the unconstrained feature representation that considers the network's limited expressivity to model neural collapse under label noise. The MD-Dropout model, which combines memorization and dropout as a regularizer, is proposed to prevent dilated neural collapse and improve robustness to label noise."
}