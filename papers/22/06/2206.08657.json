{
    "title": "BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v5 [cs.CV] UPDATED)",
    "abstract": "Vision-Language (VL) models with the Two-Tower architecture have dominated visual-language representation learning in recent years. Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder. Both approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the cross-modal encoder. This enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BridgeTower achieves state-of-the-art performance o",
    "link": "http://arxiv.org/abs/2206.08657",
    "context": "Title: BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning. (arXiv:2206.08657v5 [cs.CV] UPDATED)\nAbstract: Vision-Language (VL) models with the Two-Tower architecture have dominated visual-language representation learning in recent years. Current VL models either use lightweight uni-modal encoders and learn to extract, align and fuse both modalities simultaneously in a deep cross-modal encoder, or feed the last-layer uni-modal representations from the deep pre-trained uni-modal encoders into the top cross-modal encoder. Both approaches potentially restrict vision-language representation learning and limit model performance. In this paper, we propose BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni-modal encoders and each layer of the cross-modal encoder. This enables effective bottom-up cross-modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni-modal encoders in the cross-modal encoder. Pre-trained with only 4M images, BridgeTower achieves state-of-the-art performance o",
    "path": "papers/22/06/2206.08657.json",
    "total_tokens": 893,
    "translated_title": "BridgeTower：在视觉语言表示学习中建立编码器之间的桥梁",
    "translated_abstract": "最近几年，拥有双塔架构的视觉语言模型巩固了视觉语言表示学习的地位。当前的视觉语言模型要么使用轻量级的单模态编码器，学习从深度跨模编码器中提取、对齐和融合两种模态的表示，要么将深度预训练的单模态编码器的最后一层单模态表示馈送到顶部跨模编码器中。这两种方法都可能限制视觉语言表示学习并限制模型性能。本文提出了BridgeTower，它引入了多个桥接层，建立了单模态编码器的顶层与跨模编码器的每一层之间的连接。这使得来自预训练单模态编码器的不同语义级别的视觉和文本表示在跨模编码器中实现了有效的自下而上跨模态对齐和融合。通过仅使用400万张图像进行预训练，BridgeTower实现了最先进的性能。",
    "tldr": "BridgeTower提出了多个桥接层建立了单模态编码器的顶层与跨模编码器的连接，实现了有效的自下而上跨模态对齐和融合，显著提升了模型性能。",
    "en_tdlr": "BridgeTower introduces multiple bridge layers to establish a connection between the top layers of uni-modal encoders and each layer of the cross-modal encoder, enabling effective bottom-up cross-modal alignment and fusion, and significantly improving model performance."
}