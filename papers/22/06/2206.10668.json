{
    "title": "BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing. (arXiv:2206.10668v2 [cs.CL] UPDATED)",
    "abstract": "Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free grammars for seven semantic parsing datasets and two syntactic parsing datasets with varied output representations, as well as a constrained decoding interface to generate only valid outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports evaluation of language models using prompt-based learning as well as fine-tuning. We benchmark eight language models, including two GPT-3 variants available only through an API. Our experiments show that encoder-decoder pretrained language models can achieve similar performance or surpass state-of-the-a",
    "link": "http://arxiv.org/abs/2206.10668",
    "context": "Title: BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing. (arXiv:2206.10668v2 [cs.CL] UPDATED)\nAbstract: Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free grammars for seven semantic parsing datasets and two syntactic parsing datasets with varied output representations, as well as a constrained decoding interface to generate only valid outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports evaluation of language models using prompt-based learning as well as fine-tuning. We benchmark eight language models, including two GPT-3 variants available only through an API. Our experiments show that encoder-decoder pretrained language models can achieve similar performance or surpass state-of-the-a",
    "path": "papers/22/06/2206.10668.json",
    "total_tokens": 971,
    "translated_title": "BenchCLAMP：用于评估语言模型在句法和语义解析上的基准测试",
    "translated_abstract": "近期研究表明，当输出被限制为有效的语义表示时，通过提示或精调的语言模型在语义解析上表现良好。我们引入了BenchCLAMP，这是一个用于评估约束语言模型解析的基准测试，包括七个语义解析数据集的上下文无关文法和两个具有不同输出表示的句法解析数据集，以及一个受限解码界面，仅生成这些文法包含的有效输出。我们为每个数据集提供了低、中、高资源划分，可以准确比较不同数据策略下各种语言模型的性能。我们的基准测试支持使用基于提示的学习和精调评估语言模型。我们对包括两个仅通过API可用的GPT-3变体在内的八个语言模型进行了基准测试。我们的实验证明，编码器-解码器预训练语言模型可以达到类似或超过现有最先进技术的性能水平。",
    "tldr": "BenchCLAMP是一个用于评估语言模型在句法和语义解析上的基准测试。它包括七个语义解析数据集和两个句法解析数据集，提供了不同数据策略下的资源划分，并支持基于提示的学习和精调评估。实验证明，编码器-解码器预训练语言模型可以达到类似或超过现有最先进技术的性能水平。",
    "en_tdlr": "BenchCLAMP is a benchmark for evaluating language models on syntactic and semantic parsing. It includes seven semantic parsing datasets and two syntactic parsing datasets, provides resource splits under different data strategies, and supports prompt-based learning and fine-tuning evaluation. Experiments show that encoder-decoder pretrained language models can achieve similar or surpass state-of-the-art performance."
}