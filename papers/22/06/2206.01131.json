{
    "title": "Predictive Multiplicity in Probabilistic Classification. (arXiv:2206.01131v3 [cs.LG] UPDATED)",
    "abstract": "Machine learning models are often used to inform real world risk assessment tasks: predicting consumer default risk, predicting whether a person suffers from a serious illness, or predicting a person's risk to appear in court. Given multiple models that perform almost equally well for a prediction task, to what extent do predictions vary across these models? If predictions are relatively consistent for similar models, then the standard approach of choosing the model that optimizes a penalized loss suffices. But what if predictions vary significantly for similar models? In machine learning, this is referred to as predictive multiplicity i.e. the prevalence of conflicting predictions assigned by near-optimal competing models. In this paper, we present a framework for measuring predictive multiplicity in probabilistic classification (predicting the probability of a positive outcome). We introduce measures that capture the variation in risk estimates over the set of competing models, and d",
    "link": "http://arxiv.org/abs/2206.01131",
    "context": "Title: Predictive Multiplicity in Probabilistic Classification. (arXiv:2206.01131v3 [cs.LG] UPDATED)\nAbstract: Machine learning models are often used to inform real world risk assessment tasks: predicting consumer default risk, predicting whether a person suffers from a serious illness, or predicting a person's risk to appear in court. Given multiple models that perform almost equally well for a prediction task, to what extent do predictions vary across these models? If predictions are relatively consistent for similar models, then the standard approach of choosing the model that optimizes a penalized loss suffices. But what if predictions vary significantly for similar models? In machine learning, this is referred to as predictive multiplicity i.e. the prevalence of conflicting predictions assigned by near-optimal competing models. In this paper, we present a framework for measuring predictive multiplicity in probabilistic classification (predicting the probability of a positive outcome). We introduce measures that capture the variation in risk estimates over the set of competing models, and d",
    "path": "papers/22/06/2206.01131.json",
    "total_tokens": 806,
    "translated_title": "概率分类中的预测多样性",
    "translated_abstract": "机器学习模型经常用于风险评估任务，比如预测消费者违约风险、预测一个人是否患有严重疾病或预测一个人在法庭上出现的风险。但如果有多个模型在一个预测任务中表现几乎同样好，那么这些模型的预测结果会有多大的变化？如果相似模型的预测结果相对一致，则可以选择优化惩罚损失的模型。但如果相似模型的预测结果有显著差异，这在机器学习中被称为预测多样性，即通过优化判定标准而获得的相似分类器的决策差异。本文提出了一种概率分类的预测多样性度量框架，引入了一些衡量在一组相似模型上风险评估变化的量",
    "tldr": "本文提出了一种概率分类的预测多样性度量框架，用于衡量在一组相似模型上风险评估的变化。",
    "en_tdlr": "This paper presents a framework for measuring predictive multiplicity in probabilistic classification and introduces measures that capture the variation in risk estimates over the set of competing models."
}