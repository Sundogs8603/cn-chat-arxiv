{
    "title": "Edge Inference with Fully Differentiable Quantized Mixed Precision Neural Networks. (arXiv:2206.07741v2 [cs.LG] UPDATED)",
    "abstract": "The large computing and memory cost of deep neural networks (DNNs) often precludes their use in resource-constrained devices. Quantizing the parameters and operations to lower bit-precision offers substantial memory and energy savings for neural network inference, facilitating the use of DNNs on edge computing platforms. Recent efforts at quantizing DNNs have employed a range of techniques encompassing progressive quantization, step-size adaptation, and gradient scaling. This paper proposes a new quantization approach for mixed precision convolutional neural networks (CNNs) targeting edge-computing. Our method establishes a new pareto frontier in model accuracy and memory footprint demonstrating a range of quantized models, delivering best-in-class accuracy below 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions are: (i) hardware-aware heterogeneous differentiable quantization with tensor-sliced learned precision, (ii) targeted gradient modification for wgts. an",
    "link": "http://arxiv.org/abs/2206.07741",
    "context": "Title: Edge Inference with Fully Differentiable Quantized Mixed Precision Neural Networks. (arXiv:2206.07741v2 [cs.LG] UPDATED)\nAbstract: The large computing and memory cost of deep neural networks (DNNs) often precludes their use in resource-constrained devices. Quantizing the parameters and operations to lower bit-precision offers substantial memory and energy savings for neural network inference, facilitating the use of DNNs on edge computing platforms. Recent efforts at quantizing DNNs have employed a range of techniques encompassing progressive quantization, step-size adaptation, and gradient scaling. This paper proposes a new quantization approach for mixed precision convolutional neural networks (CNNs) targeting edge-computing. Our method establishes a new pareto frontier in model accuracy and memory footprint demonstrating a range of quantized models, delivering best-in-class accuracy below 4.3 MB of weights (wgts.) and activations (acts.). Our main contributions are: (i) hardware-aware heterogeneous differentiable quantization with tensor-sliced learned precision, (ii) targeted gradient modification for wgts. an",
    "path": "papers/22/06/2206.07741.json",
    "total_tokens": 926,
    "translated_title": "具有完全可微分量化的混合精度神经网络的边缘推理",
    "translated_abstract": "深度神经网络(DNNs)的大量计算和存储成本常常限制了其在资源受限设备上的使用。将参数和操作量化为低位精度可以大幅节省神经网络推理的内存和能量，便于在边缘计算平台上使用DNNs。最近的量化DNNs的努力采用了一系列的技术，包括渐进式量化、步长适应和梯度缩放。本文提出了一种针对边缘计算的混合精度卷积神经网络(CNNs)的新的量化方法。我们的方法在模型准确性和内存占用方面建立了一个新的帕累托前沿，展示了一系列量化模型，在不到4.3 MB的权重和激活下实现了最佳的准确性。我们的主要贡献包括：(i) 基于硬件感知的异构可微分量化，使用张量分块学习的精度，(ii) 针对权重的有针对性的梯度修改",
    "tldr": "本论文提出了一种新的边缘推理混合精度神经网络的量化方法，通过硬件感知的异构可微分量化和目标梯度修改，实现了在较小的内存占用下达到最优的模型准确性",
    "en_tdlr": "This paper proposes a new quantization approach for mixed precision convolutional neural networks for edge inference, achieving optimal model accuracy with reduced memory footprint through hardware-aware heterogeneous differentiable quantization and targeted gradient modification."
}