{
    "title": "Communication-Efficient Federated Learning With Data and Client Heterogeneity. (arXiv:2206.10032v3 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) enables large-scale distributed training of machine learning models, while still allowing individual nodes to maintain data locally.  However, executing FL at scale comes with inherent practical challenges:  1) heterogeneity of the local node data distributions,  2) heterogeneity of node computational speeds (asynchrony),  but also 3) constraints in the amount of communication between the clients and the server.  In this work, we present the first variant of the classic federated averaging (FedAvg) algorithm  which, at the same time, supports data heterogeneity, partial client asynchrony, and communication compression.  Our algorithm comes with a rigorous analysis showing that, in spite of these system relaxations,  it can provide similar convergence to FedAvg in interesting parameter regimes.  Experimental results in the rigorous LEAF benchmark on setups of up to $300$ nodes show that our algorithm ensures fast convergence for standard federated tasks, improvin",
    "link": "http://arxiv.org/abs/2206.10032",
    "context": "Title: Communication-Efficient Federated Learning With Data and Client Heterogeneity. (arXiv:2206.10032v3 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) enables large-scale distributed training of machine learning models, while still allowing individual nodes to maintain data locally.  However, executing FL at scale comes with inherent practical challenges:  1) heterogeneity of the local node data distributions,  2) heterogeneity of node computational speeds (asynchrony),  but also 3) constraints in the amount of communication between the clients and the server.  In this work, we present the first variant of the classic federated averaging (FedAvg) algorithm  which, at the same time, supports data heterogeneity, partial client asynchrony, and communication compression.  Our algorithm comes with a rigorous analysis showing that, in spite of these system relaxations,  it can provide similar convergence to FedAvg in interesting parameter regimes.  Experimental results in the rigorous LEAF benchmark on setups of up to $300$ nodes show that our algorithm ensures fast convergence for standard federated tasks, improvin",
    "path": "papers/22/06/2206.10032.json",
    "total_tokens": 920,
    "translated_title": "具有数据和客户端异质性的通信高效的联邦学习",
    "translated_abstract": "联邦学习（FL）允许机器学习模型进行大规模分布式训练，同时仍允许各个节点保持本地数据。然而，进行大规模FL时存在固有的实用挑战：1）局部节点数据分布的异质性，2）节点计算速度（异步性）的异质性，以及3）客户端和服务器之间通信的限制。在本文中，我们提出了第一种经典联邦平均算法（FedAvg）的变体，同时支持数据异质性、部分客户端异步性和通信压缩。我们的算法提供了严密的分析，表明尽管存在这些系统放宽，它仍然可以在有趣的参数范围内提供类似于FedAvg的收敛性。在多达300个节点的严格LEAF基准测试方案中的实验结果显示，我们的算法确保了标准联合任务的快速收敛，提高了",
    "tldr": "本文提出了适用于具有数据和客户端异质性的通信高效的联邦学习算法，并通过实验结果验证了其在标准联合任务的快速收敛性，以及在有趣的参数范围内可以提供类似于经典联邦平均算法的收敛性。",
    "en_tdlr": "This paper proposes a communication-efficient Federated Learning algorithm that supports data and client heterogeneity, and validates its fast convergence in standard federated tasks through experimental results, while also providing similar convergence to the classic Federated Averaging algorithm within certain parameter regimes."
}