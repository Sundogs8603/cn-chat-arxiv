{
    "title": "Decentralized Training of Foundation Models in Heterogeneous Environments. (arXiv:2206.01288v4 [cs.DC] UPDATED)",
    "abstract": "Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous ne",
    "link": "http://arxiv.org/abs/2206.01288",
    "context": "Title: Decentralized Training of Foundation Models in Heterogeneous Environments. (arXiv:2206.01288v4 [cs.DC] UPDATED)\nAbstract: Training foundation models, such as GPT-3 and PaLM, can be extremely expensive, often involving tens of thousands of GPUs running continuously for months. These models are typically trained in specialized clusters featuring fast, homogeneous interconnects and using carefully designed software systems that support both data parallelism and model/pipeline parallelism. Such dedicated clusters can be costly and difficult to obtain. Can we instead leverage the much greater amount of decentralized, heterogeneous, and lower-bandwidth interconnected compute? Previous works examining the heterogeneous, decentralized setting focus on relatively small models that can be trained in a purely data parallel manner. State-of-the-art schemes for model parallel foundation model training, such as Megatron, only consider the homogeneous data center setting. In this paper, we present the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous ne",
    "path": "papers/22/06/2206.01288.json",
    "total_tokens": 859,
    "translated_title": "分布式环境下基础模型的去中心化训练",
    "translated_abstract": "训练基础模型（如GPT-3和PaLM）可能非常昂贵，往往需要数万个GPU连续运行数月。这些模型通常在专用集群中进行训练，这些集群拥有快速的同构互联和使用精心设计的软件系统，支持数据并行和模型/管道并行。这样的专用集群成本高昂且难以获得。我们能否利用分布式、异构和带宽较低的计算资源呢？以往研究异构、分布式场景主要关注小型模型，可以以纯数据并行方式进行训练。用于模型并行基础模型训练的最先进方案（如Megatron）仅涉及同构数据中心环境。在本文中，我们首次提出了在分布式异构环境中使用模型并行进行大型基础模型的训练的研究。",
    "tldr": "本文研究了在分布式异构环境下进行模型并行训练大型基础模型的技术，提出了一种可以利用更多分布式、异构和低带宽互联计算资源的训练方案。",
    "en_tdlr": "This paper presents the first study of training large foundation models with model parallelism in a decentralized regime over a heterogeneous network, providing a way to leverage more distributed, heterogeneous, and lower-bandwidth interconnected compute resources for training."
}