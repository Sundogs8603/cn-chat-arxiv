{
    "title": "MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging. (arXiv:2206.01408v2 [cs.CV] UPDATED)",
    "abstract": "In medical image analysis, transfer learning is a powerful method for deep neural networks (DNNs) to generalize well on limited medical data. Prior efforts have focused on developing pre-training algorithms on domains such as lung ultrasound, chest X-ray, and liver CT to bridge domain gaps. However, we find that model fine-tuning also plays a crucial role in adapting medical knowledge to target tasks. The common fine-tuning method is manually picking transferable layers (e.g., the last few layers) to update, which is labor-expensive. In this work, we propose a meta-learning-based LR tuner, named MetaLR, to make different layers automatically co-adapt to downstream tasks based on their transferabilities across domains. MetaLR learns appropriate LRs for different layers in an online manner, preventing highly transferable layers from forgetting their medical representation abilities and driving less transferable layers to adapt actively to new domains. Extensive experiments on various med",
    "link": "http://arxiv.org/abs/2206.01408",
    "context": "Title: MetaLR: Meta-tuning of Learning Rates for Transfer Learning in Medical Imaging. (arXiv:2206.01408v2 [cs.CV] UPDATED)\nAbstract: In medical image analysis, transfer learning is a powerful method for deep neural networks (DNNs) to generalize well on limited medical data. Prior efforts have focused on developing pre-training algorithms on domains such as lung ultrasound, chest X-ray, and liver CT to bridge domain gaps. However, we find that model fine-tuning also plays a crucial role in adapting medical knowledge to target tasks. The common fine-tuning method is manually picking transferable layers (e.g., the last few layers) to update, which is labor-expensive. In this work, we propose a meta-learning-based LR tuner, named MetaLR, to make different layers automatically co-adapt to downstream tasks based on their transferabilities across domains. MetaLR learns appropriate LRs for different layers in an online manner, preventing highly transferable layers from forgetting their medical representation abilities and driving less transferable layers to adapt actively to new domains. Extensive experiments on various med",
    "path": "papers/22/06/2206.01408.json",
    "total_tokens": 1024,
    "translated_title": "MetaLR: 医学图像迁移学习中学习速率的元调整方法",
    "translated_abstract": "在医学图像分析中，迁移学习是深度神经网络（DNN）能够对有限的医学数据进行泛化的强大方法。先前的研究集中于开发针对肺部超声，胸部X射线和肝脏CT等领域的预训练算法，以弥合领域差距。然而，我们发现模型微调在适应目标任务中也扮演着至关重要的角色。常见的微调方法是手动选择可转移层（例如最后几层）进行更新，这是非常耗时的。在这项工作中，我们提出了一种基于元学习的学习速率调整器，名为MetaLR，使不同层次的层次在跨域任务中根据它们在领域之间的可转移性自动协同适应下游任务。MetaLR在线学习不同层次的适当学习率，防止高度可转移层忘记其医学表示能力，并促使其他可转移层主动适应新领域。在各种医学图像数据集上的大量实验表明，我们提出的MetaLR可以显着改善几种领先的微调方法的性能。",
    "tldr": "提出了一种基于元学习的学习率调整器MetaLR，可以使不同层次的层次根据它们的可转移性自动协同适应下游任务，在多个医学图像数据集上实验表明MetaLR在微调方面的效果优于多种领先的微调方法。"
}