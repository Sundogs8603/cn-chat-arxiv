{
    "title": "Understanding convolution on graphs via energies. (arXiv:2206.10991v4 [cs.LG] UPDATED)",
    "abstract": "Graph Neural Networks (GNNs) typically operate by message-passing, where the state of a node is updated based on the information received from its neighbours. Most message-passing models act as graph convolutions, where features are mixed by a shared, linear transformation before being propagated over the edges. On node-classification tasks, graph convolutions have been shown to suffer from two limitations: poor performance on heterophilic graphs, and over-smoothing. It is common belief that both phenomena occur because such models behave as low-pass filters, meaning that the Dirichlet energy of the features decreases along the layers incurring a smoothing effect that ultimately makes features no longer distinguishable. In this work, we rigorously prove that simple graph-convolutional models can actually enhance high frequencies and even lead to an asymptotic behaviour we refer to as over-sharpening, opposite to over-smoothing. We do so by showing that linear graph convolutions with sy",
    "link": "http://arxiv.org/abs/2206.10991",
    "context": "Title: Understanding convolution on graphs via energies. (arXiv:2206.10991v4 [cs.LG] UPDATED)\nAbstract: Graph Neural Networks (GNNs) typically operate by message-passing, where the state of a node is updated based on the information received from its neighbours. Most message-passing models act as graph convolutions, where features are mixed by a shared, linear transformation before being propagated over the edges. On node-classification tasks, graph convolutions have been shown to suffer from two limitations: poor performance on heterophilic graphs, and over-smoothing. It is common belief that both phenomena occur because such models behave as low-pass filters, meaning that the Dirichlet energy of the features decreases along the layers incurring a smoothing effect that ultimately makes features no longer distinguishable. In this work, we rigorously prove that simple graph-convolutional models can actually enhance high frequencies and even lead to an asymptotic behaviour we refer to as over-sharpening, opposite to over-smoothing. We do so by showing that linear graph convolutions with sy",
    "path": "papers/22/06/2206.10991.json",
    "total_tokens": 1029,
    "translated_title": "通过图上的能量理解卷积",
    "translated_abstract": "图神经网络（GNN）通常通过消息传递操作，其中节点的状态是基于其邻居收到的信息进行更新的。大多数消息传递模型都是作为图卷积进行操作的，其中特征在被传播到边缘之前通过共享的线性变换混合。在节点分类任务中，图卷积已经表现出两个限制：在heterophilic图上表现欠佳，并且过度平滑。常见的看法是，这两种现象的发生是因为这种模型表现为低通滤波器，意味着在图层间特征的Dirichlet能量会减少，导致平滑效应，最终特征不再可区分。在这项工作中，我们严谨地证明了简单的图卷积模型实际上可以增强高频率甚至引导一种我们所称的过度锐化的渐近行为，与过度平滑相反。我们通过表明对称滤波器的线性图卷积可以被解释为在图形上的能量最小化问题来做到这一点。具体而言，能量函数惩罚高能信号，有效地抑制低频，同时促进相关的高频。我们的结果表明，精心设计的图卷积模型可以在同质和异质任务上提供更好的性能。",
    "tldr": "本论文结合能量的概念，证明了带对称滤波器的线性图卷积可以增强高频率，使图神经网络在同质和异质任务中表现更好。",
    "en_tdlr": "This paper combines the concept of energies and proves that linear graph convolutions with symmetric filters can enhance high frequencies, resulting in better performance on both homophilic and heterophilic tasks for graph neural networks."
}