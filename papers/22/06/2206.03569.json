{
    "title": "Overcoming the Long Horizon Barrier for Sample-Efficient Reinforcement Learning with Latent Low-Rank Structure. (arXiv:2206.03569v4 [cs.LG] UPDATED)",
    "abstract": "The practicality of reinforcement learning algorithms has been limited due to poor scaling with respect to the problem size, as the sample complexity of learning an $\\epsilon$-optimal policy is $\\tilde{\\Omega}\\left(|S||A|H^3 / \\epsilon^2\\right)$ over worst case instances of an MDP with state space $S$, action space $A$, and horizon $H$. We consider a class of MDPs for which the associated optimal $Q^*$ function is low rank, where the latent features are unknown. While one would hope to achieve linear sample complexity in $|S|$ and $|A|$ due to the low rank structure, we show that without imposing further assumptions beyond low rank of $Q^*$, if one is constrained to estimate the $Q$ function using only observations from a subset of entries, there is a worst case instance in which one must incur a sample complexity exponential in the horizon $H$ to learn a near optimal policy. We subsequently show that under stronger low rank structural assumptions, given access to a generative model, L",
    "link": "http://arxiv.org/abs/2206.03569",
    "context": "Title: Overcoming the Long Horizon Barrier for Sample-Efficient Reinforcement Learning with Latent Low-Rank Structure. (arXiv:2206.03569v4 [cs.LG] UPDATED)\nAbstract: The practicality of reinforcement learning algorithms has been limited due to poor scaling with respect to the problem size, as the sample complexity of learning an $\\epsilon$-optimal policy is $\\tilde{\\Omega}\\left(|S||A|H^3 / \\epsilon^2\\right)$ over worst case instances of an MDP with state space $S$, action space $A$, and horizon $H$. We consider a class of MDPs for which the associated optimal $Q^*$ function is low rank, where the latent features are unknown. While one would hope to achieve linear sample complexity in $|S|$ and $|A|$ due to the low rank structure, we show that without imposing further assumptions beyond low rank of $Q^*$, if one is constrained to estimate the $Q$ function using only observations from a subset of entries, there is a worst case instance in which one must incur a sample complexity exponential in the horizon $H$ to learn a near optimal policy. We subsequently show that under stronger low rank structural assumptions, given access to a generative model, L",
    "path": "papers/22/06/2206.03569.json",
    "total_tokens": 1087,
    "translated_title": "具有潜在低秩结构的样本有效强化学习克服长时差障碍。",
    "translated_abstract": "由于与问题规模相关的缩放性能差，强化学习算法的实用性受到限制，学习ε-最优策略的样本复杂度为 $\\tilde{\\Omega}\\left(|S||A|H^3 / \\epsilon^2\\right)$，其中$S$是状态空间，$A$是动作空间，$H$是时间横跨的多个状态。 我们考虑一类具有潜在低秩结构的MDP，其中相关的最优 $Q^*$ 函数是低秩的，潜在的特征是未知的。虽然由于低秩结构，我们希望能够在 $|S|$ 和 $|A|$ 上实现线性样本复杂度，但如果除了 $Q^*$ 的低秩性之外没有施加进一步的假设，则在仅使用来自条目子集的观察来估计 $Q$ 函数的情况下，存在最坏的实例，在这种情况下，要学习接近最优策略，必须承担随着时间视野 $H$ 指数增长的样本复杂度。我们随后展示，在更强的低秩结构假设下，假设有一个生成模型，利用我们提出的算法，我们在小于多项式增长的样本复杂度内，即 $\\tilde O\\left(|S|^{3/2}/\\epsilon\\right)$，有效地学习了一个近似的最优策略。",
    "tldr": "本文提出一类具有潜在低秩结构的MDP问题，其中相关的最优$Q^*$函数是低秩的，利用算法可以在小于多项式增长的样本复杂度内，有效地学习了一个近似的最优策略。",
    "en_tdlr": "This paper proposes a class of MDP problems with latent low-rank structure, in which the associated optimal $Q^*$ function is low rank, and presents an algorithm that can effectively learn an approximate optimal policy with the sample complexity of less than polynomial growth."
}