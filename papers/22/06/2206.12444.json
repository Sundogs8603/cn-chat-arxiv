{
    "title": "Gated Domain Units for Multi-source Domain Generalization. (arXiv:2206.12444v2 [cs.LG] UPDATED)",
    "abstract": "The phenomenon of distribution shift (DS) occurs when a dataset at test time differs from the dataset at training time, which can significantly impair the performance of a machine learning model in practical settings due to a lack of knowledge about the data's distribution at test time. To address this problem, we postulate that real-world distributions are composed of latent Invariant Elementary Distributions (I.E.D) across different domains. This assumption implies an invariant structure in the solution space that enables knowledge transfer to unseen domains. To exploit this property for domain generalization, we introduce a modular neural network layer consisting of Gated Domain Units (GDUs) that learn a representation for each latent elementary distribution. During inference, a weighted ensemble of learning machines can be created by comparing new observations with the representations of each elementary distribution. Our flexible framework also accommodates scenarios where explicit",
    "link": "http://arxiv.org/abs/2206.12444",
    "context": "Title: Gated Domain Units for Multi-source Domain Generalization. (arXiv:2206.12444v2 [cs.LG] UPDATED)\nAbstract: The phenomenon of distribution shift (DS) occurs when a dataset at test time differs from the dataset at training time, which can significantly impair the performance of a machine learning model in practical settings due to a lack of knowledge about the data's distribution at test time. To address this problem, we postulate that real-world distributions are composed of latent Invariant Elementary Distributions (I.E.D) across different domains. This assumption implies an invariant structure in the solution space that enables knowledge transfer to unseen domains. To exploit this property for domain generalization, we introduce a modular neural network layer consisting of Gated Domain Units (GDUs) that learn a representation for each latent elementary distribution. During inference, a weighted ensemble of learning machines can be created by comparing new observations with the representations of each elementary distribution. Our flexible framework also accommodates scenarios where explicit",
    "path": "papers/22/06/2206.12444.json",
    "total_tokens": 936,
    "translated_title": "多源域一般化的门控领域单元",
    "translated_abstract": "分布转移现象（DS）发生于测试时间数据集与训练时间数据集不同时的情况，这可能会在实际应用中明显影响机器学习模型性能，因为它缺乏有关测试时数据分布的知识。为解决这个问题，我们假设现实世界的分布由不同域中的潜在不变的基元分布（I.E.D）组成。这个假设意味着解空间中存在一个不变的结构，可以实现对看不见的域的知识转移。为了利用这个性质来进行域一般化，我们引入了一个由门控域单元（GDUs）组成的模块化神经网络层，它学习每个潜在基元分布的表示。在推断过程中，可以通过将新观测值与每个基元分布的表示进行比较，创建一个加权的学习机集合。我们的灵活框架还可以适应明确的情况。",
    "tldr": "该研究提出了一个朴素的假设，即现实世界的分布由不同域中的潜在不变的基元分布（I.E.D）组成，其引入了一个由门控域单元（GDUs）组成的模块化神经网络层。这种方法可以更好地实现对看不见的域的知识转移，从而提高模型的性能。",
    "en_tdlr": "This study proposes a naive assumption that real-world distributions are composed of latent Invariant Elementary Distributions (I.E.D) across different domains. It introduces a modular neural network layer consisting of Gated Domain Units (GDUs) that learn a representation for each latent elementary distribution, which can better achieve knowledge transfer to unseen domains and improve the performance of the model."
}