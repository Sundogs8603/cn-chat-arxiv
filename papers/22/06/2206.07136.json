{
    "title": "Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v3 [cs.LG] UPDATED)",
    "abstract": "Per-example gradient clipping is a key algorithmic step that enables practical differential private (DP) training for deep learning models. The choice of clipping threshold R, however, is vital for achieving high accuracy under DP. We propose an easy-to-use replacement, called automatic clipping, that eliminates the need to tune R for any DP optimizers, including DP-SGD, DP-Adam, DP-LAMB and many others. The automatic variants are as private and computationally efficient as existing DP optimizers, but require no DP-specific hyperparameters and thus make DP training as amenable as the standard non-private training. We give a rigorous convergence analysis of automatic DP-SGD in the non-convex setting, showing that it can enjoy an asymptotic convergence rate that matches the standard SGD, under a symmetric gradient noise assumption of the per-sample gradients (commonly used in the non-DP literature). We demonstrate on various language and vision tasks that automatic clipping outperforms o",
    "link": "http://arxiv.org/abs/2206.07136",
    "context": "Title: Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger. (arXiv:2206.07136v3 [cs.LG] UPDATED)\nAbstract: Per-example gradient clipping is a key algorithmic step that enables practical differential private (DP) training for deep learning models. The choice of clipping threshold R, however, is vital for achieving high accuracy under DP. We propose an easy-to-use replacement, called automatic clipping, that eliminates the need to tune R for any DP optimizers, including DP-SGD, DP-Adam, DP-LAMB and many others. The automatic variants are as private and computationally efficient as existing DP optimizers, but require no DP-specific hyperparameters and thus make DP training as amenable as the standard non-private training. We give a rigorous convergence analysis of automatic DP-SGD in the non-convex setting, showing that it can enjoy an asymptotic convergence rate that matches the standard SGD, under a symmetric gradient noise assumption of the per-sample gradients (commonly used in the non-DP literature). We demonstrate on various language and vision tasks that automatic clipping outperforms o",
    "path": "papers/22/06/2206.07136.json",
    "total_tokens": 929,
    "translated_title": "自动剪辑：更简单和更强大的差分隐私深度学习",
    "translated_abstract": "单样本梯度剪辑是实现深度学习模型的差分隐私(DP)训练的关键算法步骤。然而，剪辑阈值R的选择对于在DP下实现高准确性至关重要。我们提出了一种易于使用的替代方案，称为自动剪辑，它消除了为任何DP优化器（包括DP-SGD、DP-Adam、DP-LAMB等）调整R的需要。自动变量与现有的DP优化器一样具有隐私性和计算效率，但不需要DP特定的超参数，因此使得DP训练像标准的非隐私训练一样可行。我们在非凸情况下对自动DP-SGD进行了严格的收敛性分析，表明在样本梯度的对称性噪声假设下（在非DP文献中常用），它能够享受与标准SGD相同的渐近收敛速率。我们在各种语言和视觉任务中展示了自动剪辑的优势。",
    "tldr": "本论文提出了一种自动剪辑的替代方案，它消除了为差分隐私优化器调整剪辑阈值的需要，并在非凸情况下进行了收敛性分析。在多个任务中证明了自动剪辑的优势。",
    "en_tdlr": "This paper introduces an alternative approach called automatic clipping that eliminates the need to tune the clipping threshold for differentially private optimizers. It also provides a convergence analysis in the non-convex setting and demonstrates the advantages of automatic clipping in various tasks."
}