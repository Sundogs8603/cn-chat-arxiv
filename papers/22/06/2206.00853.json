{
    "title": "Masked Bayesian Neural Networks : Computation and Optimality. (arXiv:2206.00853v2 [stat.ML] UPDATED)",
    "abstract": "As data size and computing power increase, the architectures of deep neural networks (DNNs) have been getting more complex and huge, and thus there is a growing need to simplify such complex and huge DNNs. In this paper, we propose a novel sparse Bayesian neural network (BNN) which searches a good DNN with an appropriate complexity. We employ the masking variables at each node which can turn off some nodes according to the posterior distribution to yield a nodewise sparse DNN. We devise a prior distribution such that the posterior distribution has theoretical optimalities (i.e. minimax optimality and adaptiveness), and develop an efficient MCMC algorithm. By analyzing several benchmark datasets, we illustrate that the proposed BNN performs well compared to other existing methods in the sense that it discovers well condensed DNN architectures with similar prediction accuracy and uncertainty quantification compared to large DNNs.",
    "link": "http://arxiv.org/abs/2206.00853",
    "context": "Title: Masked Bayesian Neural Networks : Computation and Optimality. (arXiv:2206.00853v2 [stat.ML] UPDATED)\nAbstract: As data size and computing power increase, the architectures of deep neural networks (DNNs) have been getting more complex and huge, and thus there is a growing need to simplify such complex and huge DNNs. In this paper, we propose a novel sparse Bayesian neural network (BNN) which searches a good DNN with an appropriate complexity. We employ the masking variables at each node which can turn off some nodes according to the posterior distribution to yield a nodewise sparse DNN. We devise a prior distribution such that the posterior distribution has theoretical optimalities (i.e. minimax optimality and adaptiveness), and develop an efficient MCMC algorithm. By analyzing several benchmark datasets, we illustrate that the proposed BNN performs well compared to other existing methods in the sense that it discovers well condensed DNN architectures with similar prediction accuracy and uncertainty quantification compared to large DNNs.",
    "path": "papers/22/06/2206.00853.json",
    "total_tokens": 1001,
    "translated_title": "掩码贝叶斯神经网络: 计算与优越性",
    "translated_abstract": "随着数据量和计算能力的增长，深度神经网络（DNN）的架构变得越来越复杂和庞大，因此需要简化这种复杂和庞大的DNN。在本文中，我们提出了一种新颖的稀疏贝叶斯神经网络（BNN），该网络可以找到一个适当复杂度的 DNN。我们在每个节点上使用掩码变量，根据后验分布关闭一些节点，以产生稀疏 DNN。我们设计了一个先验分布，使得后验分布具有理论上的最优性（即极小极大优越性和自适应性），并开发了一种高效的MCMC算法。通过分析几个基准数据集，我们证明所提出的BNN表现良好，与大型DNN相比，它发现了精简的DNN结构，具有相似的预测准确性和不确定性量化。",
    "tldr": "本文提出了一种新颖的稀疏贝叶斯神经网络（BNN），它可以使用掩码变量在节点级别上关闭一些节点，以产生稀疏的DNN结构。我们还设计了一个先验分布，使得后验分布具有理论上的最优性，并开发了一种高效的MCMC算法。该方法在几个基准数据集上表现良好，能够发现精简的DNN结构，具有与大型DNN相似的预测准确性和不确定性量化能力。",
    "en_tdlr": "This paper introduces a novel sparse Bayesian neural network (BNN) that utilizes masking variables at the node to create a sparser DNN structure. The proposed method also has an optimally designed prior distribution which allows for a theoretically optimal posterior distribution and an efficient MCMC algorithm. Experimental results on benchmark datasets illustrate that the proposed method outperforms other existing methods in discovering condensed DNN architectures with similar prediction accuracy and uncertainty quantification compared to large DNNs."
}