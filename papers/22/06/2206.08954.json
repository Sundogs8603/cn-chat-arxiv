{
    "title": "Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning. (arXiv:2206.08954v2 [cs.CV] UPDATED)",
    "abstract": "Self-supervised learning (SSL) has recently achieved tremendous empirical advancements in learning image representation. However, our understanding of the principle behind learning such a representation is still limited. This work shows that joint-embedding SSL approaches primarily learn a representation of image patches, which reflects their co-occurrence. Such a connection to co-occurrence modeling can be established formally, and it supplements the prevailing invariance perspective. We empirically show that learning a representation for fixed-scale patches and aggregating local patch representations as the image representation achieves similar or even better results than the baseline methods. We denote this process as BagSSL. Even with 32x32 patch representation, BagSSL achieves 62% top-1 linear probing accuracy on ImageNet. On the other hand, with a multi-scale pretrained model, we show that the whole image embedding is approximately the average of local patch embeddings. While the",
    "link": "http://arxiv.org/abs/2206.08954",
    "context": "Title: Bag of Image Patch Embedding Behind the Success of Self-Supervised Learning. (arXiv:2206.08954v2 [cs.CV] UPDATED)\nAbstract: Self-supervised learning (SSL) has recently achieved tremendous empirical advancements in learning image representation. However, our understanding of the principle behind learning such a representation is still limited. This work shows that joint-embedding SSL approaches primarily learn a representation of image patches, which reflects their co-occurrence. Such a connection to co-occurrence modeling can be established formally, and it supplements the prevailing invariance perspective. We empirically show that learning a representation for fixed-scale patches and aggregating local patch representations as the image representation achieves similar or even better results than the baseline methods. We denote this process as BagSSL. Even with 32x32 patch representation, BagSSL achieves 62% top-1 linear probing accuracy on ImageNet. On the other hand, with a multi-scale pretrained model, we show that the whole image embedding is approximately the average of local patch embeddings. While the",
    "path": "papers/22/06/2206.08954.json",
    "total_tokens": 923,
    "translated_title": "基于图像块嵌入的自监督学习背后的成功之道",
    "translated_abstract": "自监督学习已经在学习图像表示方面取得了巨大的经验进展。然而，我们对学习这种表示背后的原理的理解仍然有限。这项工作表明，联合嵌入自监督学习方法主要学习图像块的表示，该表示反映了它们的共同出现。这种与共现建模的联系可以被正式地建立，并且它补充了主流的不变性视角。我们从经验上表明，学习固定尺度图像块的表示，并将局部图像块表示聚合为图像表示，可以实现与基线方法相似甚至更好的结果。我们将此过程称为BagSSL。即使使用32x32的块表示，在ImageNet上，BagSSL也能够达到62%的top-1线性探测准确率。另一方面，通过多尺度预训练模型，我们展示了整个图像嵌入大致上是局部图像块嵌入结果的平均值。",
    "tldr": "这项工作发现，自监督学习方法主要学习图像块的表示，反映出它们的共同出现。学习固定尺度图像块的表示，并将局部表示聚合为图像表示，称为BagSSL，能够实现与基线方法相似甚至更好的结果。"
}