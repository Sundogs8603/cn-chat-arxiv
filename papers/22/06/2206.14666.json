{
    "title": "Conditionally Elicitable Dynamic Risk Measures for Deep Reinforcement Learning. (arXiv:2206.14666v3 [cs.LG] UPDATED)",
    "abstract": "We propose a novel framework to solve risk-sensitive reinforcement learning (RL) problems where the agent optimises time-consistent dynamic spectral risk measures. Based on the notion of conditional elicitability, our methodology constructs (strictly consistent) scoring functions that are used as penalizers in the estimation procedure. Our contribution is threefold: we (i) devise an efficient approach to estimate a class of dynamic spectral risk measures with deep neural networks, (ii) prove that these dynamic spectral risk measures may be approximated to any arbitrary accuracy using deep neural networks, and (iii) develop a risk-sensitive actor-critic algorithm that uses full episodes and does not require any additional nested transitions. We compare our conceptually improved reinforcement learning algorithm with the nested simulation approach and illustrate its performance in two settings: statistical arbitrage and portfolio allocation on both simulated and real data.",
    "link": "http://arxiv.org/abs/2206.14666",
    "context": "Title: Conditionally Elicitable Dynamic Risk Measures for Deep Reinforcement Learning. (arXiv:2206.14666v3 [cs.LG] UPDATED)\nAbstract: We propose a novel framework to solve risk-sensitive reinforcement learning (RL) problems where the agent optimises time-consistent dynamic spectral risk measures. Based on the notion of conditional elicitability, our methodology constructs (strictly consistent) scoring functions that are used as penalizers in the estimation procedure. Our contribution is threefold: we (i) devise an efficient approach to estimate a class of dynamic spectral risk measures with deep neural networks, (ii) prove that these dynamic spectral risk measures may be approximated to any arbitrary accuracy using deep neural networks, and (iii) develop a risk-sensitive actor-critic algorithm that uses full episodes and does not require any additional nested transitions. We compare our conceptually improved reinforcement learning algorithm with the nested simulation approach and illustrate its performance in two settings: statistical arbitrage and portfolio allocation on both simulated and real data.",
    "path": "papers/22/06/2206.14666.json",
    "total_tokens": 944,
    "translated_title": "条件可引出的深度强化学习动态风险度量",
    "translated_abstract": "我们提出了一种解决风险敏感型强化学习问题的新框架，其中代理通过优化时间一致的动态谱风险度量来实现。基于条件可引出性的概念，我们的方法构建了（严格一致的）评分函数，用作估计过程中的处罚项。我们的贡献有三个方面：（i）设计了一种利用深度神经网络估计一类动态谱风险度量的有效方法，（ii）证明了这些动态谱风险度量可以使用深度神经网络进行任意精度的逼近，以及（iii）开发了一种风险敏感的演员-评论家算法，该算法使用完整的剧集，不需要任何额外的嵌套转换。我们将概念上改进的强化学习算法与嵌套模拟方法进行了比较，并在两个设置中说明了其在统计套利和组合分配方面的性能，包括模拟和真实数据。",
    "tldr": "本文提出一种可解决风险敏感型强化学习问题的新框架，使用动态谱风险度量进行优化，设计了一种可以逼近这些度量的深度神经网络算法，并开发了一种不需要额外嵌套转换的风险敏感演员-评论家算法。",
    "en_tdlr": "This paper proposes a novel framework for solving risk-sensitive reinforcement learning problems, using dynamic spectral risk measures for optimization, and develops a risk-sensitive actor-critic algorithm that does not require additional nested transitions. A deep neural network algorithm is designed to approximate these measures and outperforms the nested simulation approach in both simulated and real data settings."
}