{
    "title": "B2T Connection: Serving Stability and Performance in Deep Transformers. (arXiv:2206.00330v2 [cs.LG] UPDATED)",
    "abstract": "From the perspective of the layer normalization (LN) positions, the architectures of Transformers can be categorized into two types: Post-LN and Pre-LN. Recent Transformers tend to be Pre-LN because, in Post-LN with deep Transformers (e.g., those with ten or more layers), the training is often unstable, resulting in useless models. However, Post-LN has consistently achieved better performance than Pre-LN in relatively shallow Transformers (e.g., those with six or fewer layers). This study first investigates the reason for these discrepant observations empirically and theoretically and made the following discoveries: 1, the LN in Post-LN is the main source of the vanishing gradient problem that leads to unstable training, whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger gradient norms in higher layers during the back-propagation, which may lead to effective training. Exploiting the new findings, we propose a method that can provide both high stability and effective tr",
    "link": "http://arxiv.org/abs/2206.00330",
    "context": "Title: B2T Connection: Serving Stability and Performance in Deep Transformers. (arXiv:2206.00330v2 [cs.LG] UPDATED)\nAbstract: From the perspective of the layer normalization (LN) positions, the architectures of Transformers can be categorized into two types: Post-LN and Pre-LN. Recent Transformers tend to be Pre-LN because, in Post-LN with deep Transformers (e.g., those with ten or more layers), the training is often unstable, resulting in useless models. However, Post-LN has consistently achieved better performance than Pre-LN in relatively shallow Transformers (e.g., those with six or fewer layers). This study first investigates the reason for these discrepant observations empirically and theoretically and made the following discoveries: 1, the LN in Post-LN is the main source of the vanishing gradient problem that leads to unstable training, whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger gradient norms in higher layers during the back-propagation, which may lead to effective training. Exploiting the new findings, we propose a method that can provide both high stability and effective tr",
    "path": "papers/22/06/2206.00330.json",
    "total_tokens": 1090,
    "translated_title": "B2T 连接：服务于深度 Transformer 的稳定性和性能",
    "translated_abstract": "从层归一化（LN）的角度来看，Transformer 的架构可以分为两种类型：Post-LN 和 Pre-LN。最近的 Transformers 倾向于采用 Pre-LN，因为在 Post-LN 中，特别是在深度 Transformers 中（例如有十个或更多层的模型），训练经常不稳定，导致得到无用的模型。然而，与 Pre-LN 相比，在相对较浅的 Transformers（例如有六个或更少的层）中，Post-LN 一直取得了更好的性能。本研究首先从经验和理论上研究了这些不一致的观察结果，并发现了以下发现：1）Post-LN 中的 LN 是导致不稳定训练的梯度消失问题的主要原因，而 Pre-LN 可以避免这种问题；2）Post-LN 往往会在反向传播的高层保留更大的梯度范数，这可能导致有效的训练。利用这些新发现，我们提出了一种可以同时提供高稳定性和有效的 Transformer 训练的方法，称为 B2T Connection，它连接了 Pre-LN 和 Post-LN 层的输出。我们的实验结果表明，B2T Connection 可以显著提高深度 Transformers（有十个或更多层）的性能，实现了在多个基准数据集上的最先进结果。",
    "tldr": "本研究提出了一种称为 B2T 连接的方法，连接了 Pre-LN 和 Post-LN 层的输出，为深度 Transformer 提供了高稳定性和有效的训练，实验结果表明，在多个基准数据集上取得了最先进结果。",
    "en_tdlr": "This paper proposes a method called B2T Connection, which connects the outputs of the Pre-LN and Post-LN layers to provide high stability and effective training for deep Transformers. The experimental results show that B2T Connection achieves state-of-the-art results on several benchmark datasets."
}