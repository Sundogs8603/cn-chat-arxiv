{
    "title": "How Biased are Your Features?: Computing Fairness Influence Functions with Global Sensitivity Analysis. (arXiv:2206.00667v3 [cs.LG] UPDATED)",
    "abstract": "Fairness in machine learning has attained significant focus due to the widespread application in high-stake decision-making tasks. Unregulated machine learning classifiers can exhibit bias towards certain demographic groups in data, thus the quantification and mitigation of classifier bias is a central concern in fairness in machine learning. In this paper, we aim to quantify the influence of different features in a dataset on the bias of a classifier. To do this, we introduce the Fairness Influence Function (FIF). This function breaks down bias into its components among individual features and the intersection of multiple features. The key idea is to represent existing group fairness metrics as the difference of the scaled conditional variances in the classifier's prediction and apply a decomposition of variance according to global sensitivity analysis. To estimate FIFs, we instantiate an algorithm FairXplainer that applies variance decomposition of classifier's prediction following l",
    "link": "http://arxiv.org/abs/2206.00667",
    "context": "Title: How Biased are Your Features?: Computing Fairness Influence Functions with Global Sensitivity Analysis. (arXiv:2206.00667v3 [cs.LG] UPDATED)\nAbstract: Fairness in machine learning has attained significant focus due to the widespread application in high-stake decision-making tasks. Unregulated machine learning classifiers can exhibit bias towards certain demographic groups in data, thus the quantification and mitigation of classifier bias is a central concern in fairness in machine learning. In this paper, we aim to quantify the influence of different features in a dataset on the bias of a classifier. To do this, we introduce the Fairness Influence Function (FIF). This function breaks down bias into its components among individual features and the intersection of multiple features. The key idea is to represent existing group fairness metrics as the difference of the scaled conditional variances in the classifier's prediction and apply a decomposition of variance according to global sensitivity analysis. To estimate FIFs, we instantiate an algorithm FairXplainer that applies variance decomposition of classifier's prediction following l",
    "path": "papers/22/06/2206.00667.json",
    "total_tokens": 855,
    "translated_title": "特征有多偏见？：通过全局敏感性分析计算公正影响函数",
    "translated_abstract": "在机器学习中的公平性问题因其在高风险决策任务中的广泛应用而受到广泛关注。未受监管的机器学习分类器可能对数据中的某些人口群体表现出偏见，因此量化和减轻分类器偏见是公平性问题的核心关注点。本文旨在量化数据集中不同特征对分类器偏见的影响。为了做到这一点，我们引入了公正影响函数（FIF）。该函数将偏见分解为其在个体特征和多个特征的交集中的组成部分。关键思想是将现有的群体公平性度量表示为分类器预测的条件方差的差异，并根据全局敏感性分析的分解进行方差估计。为了估计FIFs，我们提出了一个名为FairXplainer的算法，该算法应用分类器预测的方差分解。",
    "tldr": "本论文介绍了公正影响函数（FIF），通过全局敏感性分析的方法量化了不同特征对分类器偏见的影响，从而解决了公平性问题中的核心关注点。",
    "en_tdlr": "This paper introduces the Fairness Influence Function (FIF), which quantifies the influence of different features on classifier bias using global sensitivity analysis, addressing the central concern in fairness regarding bias in machine learning."
}