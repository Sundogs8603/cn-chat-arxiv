{
    "title": "Federated Learning with Uncertainty via Distilled Predictive Distributions. (arXiv:2206.07562v2 [cs.LG] UPDATED)",
    "abstract": "Most existing federated learning methods are unable to estimate model/predictive uncertainty since the client models are trained using the standard loss function minimization approach which ignores such uncertainties. In many situations, however, especially in limited data settings, it is beneficial to take into account the uncertainty in the model parameters at each client as it leads to more accurate predictions and also because reliable estimates of uncertainty can be used for tasks, such as out-of-distribution (OOD) detection, and sequential decision-making tasks, such as active learning. We present a framework for federated learning with uncertainty where, in each round, each client infers the posterior distribution over its parameters as well as the posterior predictive distribution (PPD), distills the PPD into a single deep neural network, and sends this network to the server. Unlike some of the recent Bayesian approaches to federated learning, our approach does not require send",
    "link": "http://arxiv.org/abs/2206.07562",
    "context": "Title: Federated Learning with Uncertainty via Distilled Predictive Distributions. (arXiv:2206.07562v2 [cs.LG] UPDATED)\nAbstract: Most existing federated learning methods are unable to estimate model/predictive uncertainty since the client models are trained using the standard loss function minimization approach which ignores such uncertainties. In many situations, however, especially in limited data settings, it is beneficial to take into account the uncertainty in the model parameters at each client as it leads to more accurate predictions and also because reliable estimates of uncertainty can be used for tasks, such as out-of-distribution (OOD) detection, and sequential decision-making tasks, such as active learning. We present a framework for federated learning with uncertainty where, in each round, each client infers the posterior distribution over its parameters as well as the posterior predictive distribution (PPD), distills the PPD into a single deep neural network, and sends this network to the server. Unlike some of the recent Bayesian approaches to federated learning, our approach does not require send",
    "path": "papers/22/06/2206.07562.json",
    "total_tokens": 953,
    "translated_title": "通过蒸馏预测分布的不确定性进行联邦学习",
    "translated_abstract": "大多数现有的联邦学习方法无法估计模型/预测的不确定性，因为客户端模型使用标准损失函数最小化方法进行训练，忽略了这种不确定性。然而，在许多情况下，特别是在有限数据环境中，考虑每个客户端模型参数的不确定性是有益的，因为它可以导致更准确的预测，并且可靠的不确定性估计可以用于诸如分布外（OOD）检测和序贯决策任务（如主动学习）等任务。我们提出了一个带有不确定性的联邦学习框架，在每一轮中，每个客户端推断其参数的后验分布和后验预测分布（PPD），将PPD蒸馏成一个单一的深度神经网络，并将该网络发送到服务器。与最近一些贝叶斯方法不同，我们的方法不要求发送所有原始数据至服务器，保护了客户隐私。",
    "tldr": "本论文提出了一种联邦学习的不确定性框架，每个客户端在每轮中推断其参数的后验分布和后验预测分布，并将其蒸馏为单一的深度神经网络发送给服务器。这种方法可以解决现有联邦学习方法无法估计模型不确定性的问题，并在有限数据环境下取得更准确的预测。"
}