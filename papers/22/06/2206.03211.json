{
    "title": "Variational Meta Reinforcement Learning for Social Robotics. (arXiv:2206.03211v4 [cs.RO] UPDATED)",
    "abstract": "With the increasing presence of robots in our every-day environments, improving their social skills is of utmost importance. Nonetheless, social robotics still faces many challenges. One bottleneck is that robotic behaviors need to be often adapted as social norms depend strongly on the environment. For example, a robot should navigate more carefully around patients in a hospital compared to workers in an office. In this work, we investigate meta-reinforcement learning (meta-RL) as a potential solution. Here, robot behaviors are learned via reinforcement learning where a reward function needs to be chosen so that the robot learns an appropriate behavior for a given environment. We propose to use a variational meta-RL procedure that quickly adapts the robots' behavior to new reward functions. As a result, given a new environment different reward functions can be quickly evaluated and an appropriate one selected. The procedure learns a vectorized representation for reward functions and a",
    "link": "http://arxiv.org/abs/2206.03211",
    "context": "Title: Variational Meta Reinforcement Learning for Social Robotics. (arXiv:2206.03211v4 [cs.RO] UPDATED)\nAbstract: With the increasing presence of robots in our every-day environments, improving their social skills is of utmost importance. Nonetheless, social robotics still faces many challenges. One bottleneck is that robotic behaviors need to be often adapted as social norms depend strongly on the environment. For example, a robot should navigate more carefully around patients in a hospital compared to workers in an office. In this work, we investigate meta-reinforcement learning (meta-RL) as a potential solution. Here, robot behaviors are learned via reinforcement learning where a reward function needs to be chosen so that the robot learns an appropriate behavior for a given environment. We propose to use a variational meta-RL procedure that quickly adapts the robots' behavior to new reward functions. As a result, given a new environment different reward functions can be quickly evaluated and an appropriate one selected. The procedure learns a vectorized representation for reward functions and a",
    "path": "papers/22/06/2206.03211.json",
    "total_tokens": 902,
    "translated_title": "变分元元强化学习在社交机器人中的应用",
    "translated_abstract": "随着机器人在我们日常环境中的普及，提高它们的社交技能变得非常重要。然而，社交机器人仍面临许多挑战。其中一个瓶颈是，由于社交规范在很大程度上取决于环境，机器人行为需要经常适应。例如，在医院中，机器人应该比在办公室中更加小心地导航周围的病人。在这项工作中，我们研究了元强化学习（meta-RL）作为一个潜在的解决方案。在这里，机器人的行为是通过强化学习来学习的，需要选择一个奖励函数，使机器人能够针对给定环境学习出合适的行为。我们提出使用变分元强化学习程序，快速地将机器人的行为适应到新的奖励函数上。因此，在给定一个新的环境时，可以快速评估不同的奖励函数并选择适合的一个。该程序学习了奖励函数的向量化表示和一些其他的技巧。",
    "tldr": "本研究探讨了变分元元强化学习在社交机器人中的应用，通过选择合适的奖励函数，可快速适应不同环境，提高机器人的社交技能。"
}