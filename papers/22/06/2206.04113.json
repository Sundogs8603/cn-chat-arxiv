{
    "title": "Push--Pull with Device Sampling. (arXiv:2206.04113v2 [math.OC] UPDATED)",
    "abstract": "We consider decentralized optimization problems in which a number of agents collaborate to minimize the average of their local functions by exchanging over an underlying communication graph. Specifically, we place ourselves in an asynchronous model where only a random portion of nodes perform computation at each iteration, while the information exchange can be conducted between all the nodes and in an asymmetric fashion. For this setting, we propose an algorithm that combines gradient tracking with a network-level variance reduction (in contrast to variance reduction within each node). This enables each node to track the average of the gradients of the objective functions. Our theoretical analysis shows that the algorithm converges linearly, when the local objective functions are strongly convex, under mild connectivity conditions on the expected mixing matrices. In particular, our result does not require the mixing matrices to be doubly stochastic. In the experiments, we investigate a",
    "link": "http://arxiv.org/abs/2206.04113",
    "context": "Title: Push--Pull with Device Sampling. (arXiv:2206.04113v2 [math.OC] UPDATED)\nAbstract: We consider decentralized optimization problems in which a number of agents collaborate to minimize the average of their local functions by exchanging over an underlying communication graph. Specifically, we place ourselves in an asynchronous model where only a random portion of nodes perform computation at each iteration, while the information exchange can be conducted between all the nodes and in an asymmetric fashion. For this setting, we propose an algorithm that combines gradient tracking with a network-level variance reduction (in contrast to variance reduction within each node). This enables each node to track the average of the gradients of the objective functions. Our theoretical analysis shows that the algorithm converges linearly, when the local objective functions are strongly convex, under mild connectivity conditions on the expected mixing matrices. In particular, our result does not require the mixing matrices to be doubly stochastic. In the experiments, we investigate a",
    "path": "papers/22/06/2206.04113.json",
    "total_tokens": 891,
    "translated_title": "设备采样下的推拉式分布式优化",
    "translated_abstract": "我们考虑分布式优化问题，其中一些代理以交换并通过基础通信图减少其本地函数的平均值。具体而言，我们将自己放在异步模型中，其中每次迭代只有随机部分节点执行计算，而信息交换可以在所有节点之间以非对称的方式进行。针对这种情况，我们提出了一种算法，将梯度跟踪与网络级别的方差减少相结合（与每个节点内的方差减少相对比）。这使每个节点能够跟踪目标函数的梯度平均值。我们的理论分析表明，当局部目标函数强凸时，该算法线性收敛，在预期混合矩阵的温和连接条件下。特别是，我们的结果不要求混合矩阵具有双重随机性。在实验中，我们研究了一个...",
    "tldr": "我们提出了一种算法，采用梯度跟踪和网络级方差减少相结合的方法来解决异步分布式优化问题。该算法能够实现每个节点跟踪目标函数梯度的平均值，并在局部目标函数强凸的情况下，通过减少预期混合矩阵的温和连接条件实现线性收敛。",
    "en_tdlr": "We propose an algorithm that combines gradient tracking with network-level variance reduction to solve asynchronous distributed optimization problems under mild connectivity conditions on expected mixing matrices. The algorithm can track the average of the gradients of the objective functions for each node and achieves linear convergence when the local objective functions are strongly convex."
}