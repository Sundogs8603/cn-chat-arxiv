{
    "title": "On the Importance and Applicability of Pre-Training for Federated Learning. (arXiv:2206.11488v3 [cs.LG] UPDATED)",
    "abstract": "Pre-training is prevalent in nowadays deep learning to improve the learned model's performance. However, in the literature on federated learning (FL), neural networks are mostly initialized with random weights. These attract our interest in conducting a systematic study to explore pre-training for FL. Across multiple visual recognition benchmarks, we found that pre-training can not only improve FL, but also close its accuracy gap to the counterpart centralized learning, especially in the challenging cases of non-IID clients' data. To make our findings applicable to situations where pre-trained models are not directly available, we explore pre-training with synthetic data or even with clients' data in a decentralized manner, and found that they can already improve FL notably. Interestingly, many of the techniques we explore are complementary to each other to further boost the performance, and we view this as a critical result toward scaling up deep FL for real-world applications. We con",
    "link": "http://arxiv.org/abs/2206.11488",
    "context": "Title: On the Importance and Applicability of Pre-Training for Federated Learning. (arXiv:2206.11488v3 [cs.LG] UPDATED)\nAbstract: Pre-training is prevalent in nowadays deep learning to improve the learned model's performance. However, in the literature on federated learning (FL), neural networks are mostly initialized with random weights. These attract our interest in conducting a systematic study to explore pre-training for FL. Across multiple visual recognition benchmarks, we found that pre-training can not only improve FL, but also close its accuracy gap to the counterpart centralized learning, especially in the challenging cases of non-IID clients' data. To make our findings applicable to situations where pre-trained models are not directly available, we explore pre-training with synthetic data or even with clients' data in a decentralized manner, and found that they can already improve FL notably. Interestingly, many of the techniques we explore are complementary to each other to further boost the performance, and we view this as a critical result toward scaling up deep FL for real-world applications. We con",
    "path": "papers/22/06/2206.11488.json",
    "total_tokens": 978,
    "translated_title": "关于预训练在联邦学习中的重要性和适用性的研究",
    "translated_abstract": "预训练在现代深度学习中被广泛用于提高模型的性能。然而，在联邦学习（FL）的文献中，神经网络大多数是使用随机权重初始化的。这引起了我们对进行系统研究探索FL预训练的兴趣。在多个视觉识别基准测试中，我们发现预训练不仅可以提高FL的性能，而且可以缩小它与中心化学习之间的准确度差距，特别是在非独立同分布客户数据的挑战性情况下。为了使我们的发现适用于没有直接获得预训练模型的情况，我们探索了使用合成数据或甚至使用客户端数据进行分散式预训练，并发现它们已经显著改善了FL的性能。有趣的是，我们探索的许多技术互补性很强，可以进一步提高性能，我们将这视为在实际应用中扩展深度FL的关键结果。",
    "tldr": "研究发现，在联邦学习中使用预训练可以改善性能，尤其是在非独立同分布客户数据的情况下。此外，使用合成数据或客户端数据进行分散式预训练也可以显著改善性能，并且不同的技术可以相互补充以进一步提高性能。",
    "en_tdlr": "This study found that using pre-training can improve the performance of federated learning, especially in non-IID client data scenarios. Pre-training with synthetic data or even with client data in a decentralized manner can also boost the performance significantly. The techniques explored are complementary to further enhance the performance towards scaling up deep federated learning for real-world applications."
}