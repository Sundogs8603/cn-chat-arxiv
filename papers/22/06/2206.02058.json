{
    "title": "When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction. (arXiv:2206.02058v3 [stat.ML] UPDATED)",
    "abstract": "Machine learning models are often personalized with categorical attributes that are protected, sensitive, self-reported, or costly to acquire. In this work, we show models that are personalized with group attributes can reduce performance at a group level. We propose formal conditions to ensure the \"fair use\" of group attributes in prediction tasks by training one additional model -- i.e., collective preference guarantees to ensure that each group who provides personal data will receive a tailored gain in performance in return. We present sufficient conditions to ensure fair use in empirical risk minimization and characterize failure modes that lead to fair use violations due to standard practices in model development and deployment. We present a comprehensive empirical study of fair use in clinical prediction tasks. Our results demonstrate the prevalence of fair use violations in practice and illustrate simple interventions to mitigate their harm.",
    "link": "http://arxiv.org/abs/2206.02058",
    "context": "Title: When Personalization Harms: Reconsidering the Use of Group Attributes in Prediction. (arXiv:2206.02058v3 [stat.ML] UPDATED)\nAbstract: Machine learning models are often personalized with categorical attributes that are protected, sensitive, self-reported, or costly to acquire. In this work, we show models that are personalized with group attributes can reduce performance at a group level. We propose formal conditions to ensure the \"fair use\" of group attributes in prediction tasks by training one additional model -- i.e., collective preference guarantees to ensure that each group who provides personal data will receive a tailored gain in performance in return. We present sufficient conditions to ensure fair use in empirical risk minimization and characterize failure modes that lead to fair use violations due to standard practices in model development and deployment. We present a comprehensive empirical study of fair use in clinical prediction tasks. Our results demonstrate the prevalence of fair use violations in practice and illustrate simple interventions to mitigate their harm.",
    "path": "papers/22/06/2206.02058.json",
    "total_tokens": 944,
    "translated_title": "当个性化造成伤害时：重新考虑在预测中使用群体属性",
    "translated_abstract": "机器学习模型通常会使用受保护、敏感、自报告或者昂贵的分类属性进行个性化。本研究指出，使用群体属性进行个性化会降低群体水平的性能。我们提出了一种形式化条件，以确保在预测任务中“公平使用”群体属性，方法是通过训练一个额外的模型，即保证每个提供个人数据的群体会获得相对应的性能提升。我们提出了足够的条件，以确保在经验风险最小化中的公平使用，并描述了导致公平使用违规的故障模式，这是由于模型开发和部署中的标准做法所导致的。我们对临床预测任务进行了全面的实证研究。我们的结果表明，在实践中普遍存在公平使用违规，并说明了减轻其伤害的简单干预手段。",
    "tldr": "本研究发现，使用群体属性个性化机器学习模型可能降低群体水平的性能。为了确保在预测任务中公平使用群体属性，我们提出了形式化条件，并提供了相应的解决方法。我们的实证研究表明，在临床预测任务中普遍存在公平使用违规的情况，但我们也找到了简单干预手段来减轻其伤害。",
    "en_tdlr": "This work investigates the use of group attributes in personalizing machine learning models and finds that it may harm the performance at the group level. The study proposes conditions for fair use of group attributes in prediction tasks and presents interventions to mitigate the harm caused by fair use violations. The empirical study conducted in clinical prediction tasks reveals the prevalent violations and suggests simple remedies."
}