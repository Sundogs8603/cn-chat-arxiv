{
    "title": "I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences. (arXiv:2206.08451v2 [cs.LG] UPDATED)",
    "abstract": "Machine Learning-as-a-Service (MLaaS) has become a widespread paradigm, making even the most complex machine learning models available for clients via e.g. a pay-per-query principle. This allows users to avoid time-consuming processes of data collection, hyperparameter tuning, and model training. However, by giving their customers access to the (predictions of their) models, MLaaS providers endanger their intellectual property, such as sensitive training data, optimised hyperparameters, or learned model parameters. Adversaries can create a copy of the model with (almost) identical behavior using the the prediction labels only. While many variants of this attack have been described, only scattered defence strategies have been proposed, addressing isolated threats. This raises the necessity for a thorough systematisation of the field of model stealing, to arrive at a comprehensive understanding why these attacks are successful, and how they could be holistically defended against. We addr",
    "link": "http://arxiv.org/abs/2206.08451",
    "context": "Title: I Know What You Trained Last Summer: A Survey on Stealing Machine Learning Models and Defences. (arXiv:2206.08451v2 [cs.LG] UPDATED)\nAbstract: Machine Learning-as-a-Service (MLaaS) has become a widespread paradigm, making even the most complex machine learning models available for clients via e.g. a pay-per-query principle. This allows users to avoid time-consuming processes of data collection, hyperparameter tuning, and model training. However, by giving their customers access to the (predictions of their) models, MLaaS providers endanger their intellectual property, such as sensitive training data, optimised hyperparameters, or learned model parameters. Adversaries can create a copy of the model with (almost) identical behavior using the the prediction labels only. While many variants of this attack have been described, only scattered defence strategies have been proposed, addressing isolated threats. This raises the necessity for a thorough systematisation of the field of model stealing, to arrive at a comprehensive understanding why these attacks are successful, and how they could be holistically defended against. We addr",
    "path": "papers/22/06/2206.08451.json",
    "total_tokens": 1018,
    "translated_title": "我知道你去年训练了什么：关于窃取机器学习模型和防御的调查",
    "translated_abstract": "机器学习即服务（MLaaS）已经成为一种广泛的范 paradigm，通过按需付费的原则，甚至可以为客户提供最复杂的机器学习模型的预测。这使用户可以避免耗时的数据收集、超参数调整和模型训练过程。然而，通过让客户访问（其预测的）模型，MLaaS 提供商危及其知识产权，如敏感的训练数据、优化的超参数或学习的模型参数。攻击者可以使用仅预测标签创建具有（几乎）相同行为的模型副本。虽然描述了许多这种攻击的变体，但只提出了分散的防御策略，涉及孤立的威胁。这提出了对模型窃取领域进行彻底系统化的必要性，以全面了解为什么这些攻击成功以及如何全面地进行防御。我们通过提供一项综合性调查来解决这一问题，涵盖模型窃取攻击和相应的对策。",
    "tldr": "随着机器学习即服务（MLaaS）的普及，用户可以使用最复杂的机器学习模型的预测，但也危及了MLaaS提供商的知识产权，攻击者可以创建一个具备相同行为的模型副本，本文针对模型窃取的攻击和相应的对策进行了综合调查。",
    "en_tdlr": "With the widespread use of Machine Learning-as-a-Service (MLaaS), users can have access to predictions of the most complex machine learning models, but this also endangers the intellectual property of MLaaS providers, attackers can create a copy of the model with the same behavior, this paper presents a comprehensive survey on model stealing attacks and the corresponding countermeasures."
}