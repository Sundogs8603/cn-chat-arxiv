{
    "title": "Multiple Testing Framework for Out-of-Distribution Detection. (arXiv:2206.09522v4 [stat.ML] UPDATED)",
    "abstract": "We study the problem of Out-of-Distribution (OOD) detection, that is, detecting whether a learning algorithm's output can be trusted at inference time. While a number of tests for OOD detection have been proposed in prior work, a formal framework for studying this problem is lacking. We propose a definition for the notion of OOD that includes both the input distribution and the learning algorithm, which provides insights for the construction of powerful tests for OOD detection. We propose a multiple hypothesis testing inspired procedure to systematically combine any number of different statistics from the learning algorithm using conformal p-values. We further provide strong guarantees on the probability of incorrectly classifying an in-distribution sample as OOD. In our experiments, we find that threshold-based tests proposed in prior work perform well in specific settings, but not uniformly well across different types of OOD instances. In contrast, our proposed method that combines m",
    "link": "http://arxiv.org/abs/2206.09522",
    "context": "Title: Multiple Testing Framework for Out-of-Distribution Detection. (arXiv:2206.09522v4 [stat.ML] UPDATED)\nAbstract: We study the problem of Out-of-Distribution (OOD) detection, that is, detecting whether a learning algorithm's output can be trusted at inference time. While a number of tests for OOD detection have been proposed in prior work, a formal framework for studying this problem is lacking. We propose a definition for the notion of OOD that includes both the input distribution and the learning algorithm, which provides insights for the construction of powerful tests for OOD detection. We propose a multiple hypothesis testing inspired procedure to systematically combine any number of different statistics from the learning algorithm using conformal p-values. We further provide strong guarantees on the probability of incorrectly classifying an in-distribution sample as OOD. In our experiments, we find that threshold-based tests proposed in prior work perform well in specific settings, but not uniformly well across different types of OOD instances. In contrast, our proposed method that combines m",
    "path": "papers/22/06/2206.09522.json",
    "total_tokens": 824,
    "translated_title": "多重检验框架用于离群分布检测",
    "translated_abstract": "我们研究离群分布（OOD）检测的问题，即在推理时检测学习算法的输出是否可信。尽管之前的工作中提出了一些OOD检测的测试方法，但缺乏一个形式化的框架来研究这个问题。我们提出了一个OOD概念的定义，包括输入分布和学习算法，这为构建强大的OOD检测测试提供了启示。我们提出了一种多重假设检验启发的过程，使用符合性p值系统地结合学习算法中的任意数量的不同统计量。我们进一步对将入群样本错误分类为OOD的概率提供了强有力的保证。在实验中，我们发现之前工作中提出的基于阈值的测试在特定场景下表现良好，但在不同类型的OOD实例中的表现并不一致。相比之下，我们提出的方法结合了m个不同统计量。",
    "tldr": "本研究提出了一个多重检验框架用于离群分布检测的问题，包括了定义OOD概念和提供强有力保证的方法，与之前的基于阈值的测试相比，在不同类型的OOD实例中表现更一致。"
}