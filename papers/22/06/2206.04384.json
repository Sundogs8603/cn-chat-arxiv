{
    "title": "Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning. (arXiv:2206.04384v3 [cs.LG] UPDATED)",
    "abstract": "Reinforcement Learning (RL) methods are typically applied directly in environments to learn policies. In some complex environments with continuous state-action spaces, sparse rewards, and/or long temporal horizons, learning a good policy in the original environments can be difficult. Focusing on the offline RL setting, we aim to build a simple and discrete world model that abstracts the original environment. RL methods are applied to our world model instead of the environment data for simplified policy learning. Our world model, dubbed Value Memory Graph (VMG), is designed as a directed-graph-based Markov decision process (MDP) of which vertices and directed edges represent graph states and graph actions, separately. As state-action spaces of VMG are finite and relatively small compared to the original environment, we can directly apply the value iteration algorithm on VMG to estimate graph state values and figure out the best graph actions. VMG is trained from and built on the offline",
    "link": "http://arxiv.org/abs/2206.04384",
    "context": "Title: Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning. (arXiv:2206.04384v3 [cs.LG] UPDATED)\nAbstract: Reinforcement Learning (RL) methods are typically applied directly in environments to learn policies. In some complex environments with continuous state-action spaces, sparse rewards, and/or long temporal horizons, learning a good policy in the original environments can be difficult. Focusing on the offline RL setting, we aim to build a simple and discrete world model that abstracts the original environment. RL methods are applied to our world model instead of the environment data for simplified policy learning. Our world model, dubbed Value Memory Graph (VMG), is designed as a directed-graph-based Markov decision process (MDP) of which vertices and directed edges represent graph states and graph actions, separately. As state-action spaces of VMG are finite and relatively small compared to the original environment, we can directly apply the value iteration algorithm on VMG to estimate graph state values and figure out the best graph actions. VMG is trained from and built on the offline",
    "path": "papers/22/06/2206.04384.json",
    "total_tokens": 910,
    "translated_title": "基于图结构的世界模型：离线强化学习中的价值记忆图",
    "translated_abstract": "强化学习（RL）方法通常直接应用于环境来学习策略。在一些具有连续状态-动作空间、稀疏奖励和/或长时间间隔的复杂环境中，在原始环境中学习一个好的策略是困难的。在离线RL设置中，我们旨在构建一个简单且离散的世界模型，以抽象出原始环境。RL方法应用于我们的世界模型而非环境数据进行简化的策略学习。我们的世界模型称为价值记忆图（VMG），它被设计为基于有向图的马尔可夫决策过程（MDP），其中顶点和有向边分别代表图状态和图动作。由于VMG的状态空间和动作空间相对于原始环境而言是有限且相对较小的，因此我们可以直接在VMG上应用价值迭代算法来估算图状态值并确定最佳的图动作。VMG是从离线数据中训练和构建的。",
    "tldr": "本论文提出了一种离线强化学习的方法，通过构建简单离散的世界模型（Value Memory Graph，VMG）来抽象原始复杂环境，从而简化策略学习。",
    "en_tdlr": "This paper proposes an offline reinforcement learning approach that builds a simple and discrete world model (Value Memory Graph, VMG) to abstract the original complex environment for simplified policy learning."
}