{
    "title": "Black-box Generalization of Machine Teaching. (arXiv:2206.15205v2 [cs.LG] UPDATED)",
    "abstract": "Hypothesis-pruning maximizes the hypothesis updates for active learning to find those desired unlabeled data. An inherent assumption is that this learning manner can derive those updates into the optimal hypothesis. However, its convergence may not be guaranteed well if those incremental updates are negative and disordered. In this paper, we introduce a black-box teaching hypothesis $h^\\mathcal{T}$ employing a tighter slack term $\\left(1+\\mathcal{F}^{\\mathcal{T}}(\\widehat{h}_t)\\right)\\Delta_t$ to replace the typical $2\\Delta_t$ for pruning. Theoretically, we prove that, under the guidance of this teaching hypothesis, the learner can converge into a tighter generalization error and label complexity bound than those non-educated learners who do not receive any guidance from a teacher:1) the generalization error upper bound can be reduced from $R(h^*)+4\\Delta_{T-1}$ to approximately $R(h^{\\mathcal{T}})+2\\Delta_{T-1}$, and 2) the label complexity upper bound can be decreased from $4 \\theta",
    "link": "http://arxiv.org/abs/2206.15205",
    "context": "Title: Black-box Generalization of Machine Teaching. (arXiv:2206.15205v2 [cs.LG] UPDATED)\nAbstract: Hypothesis-pruning maximizes the hypothesis updates for active learning to find those desired unlabeled data. An inherent assumption is that this learning manner can derive those updates into the optimal hypothesis. However, its convergence may not be guaranteed well if those incremental updates are negative and disordered. In this paper, we introduce a black-box teaching hypothesis $h^\\mathcal{T}$ employing a tighter slack term $\\left(1+\\mathcal{F}^{\\mathcal{T}}(\\widehat{h}_t)\\right)\\Delta_t$ to replace the typical $2\\Delta_t$ for pruning. Theoretically, we prove that, under the guidance of this teaching hypothesis, the learner can converge into a tighter generalization error and label complexity bound than those non-educated learners who do not receive any guidance from a teacher:1) the generalization error upper bound can be reduced from $R(h^*)+4\\Delta_{T-1}$ to approximately $R(h^{\\mathcal{T}})+2\\Delta_{T-1}$, and 2) the label complexity upper bound can be decreased from $4 \\theta",
    "path": "papers/22/06/2206.15205.json",
    "total_tokens": 1021,
    "translated_title": "机器教学的黑盒泛化性能",
    "translated_abstract": "假设修剪最大化用于主动学习的假设更新，以找到期望的未标记数据。其固有的假设是，这种学习方式可以将这些更新导出为最优假设。然而，如果这些递增的更新是负的和无序的，它的收敛性可能无法很好地保证。在本文中，我们引入了一个采用更紧密的松弛项$\\left(1+\\mathcal{F}^{\\mathcal{T}}(\\widehat{h}_t)\\right)\\Delta_t$来替代修剪的典型$2\\Delta_t$的黑盒教学假设$h^\\mathcal{T}$。从理论上讲，我们证明，在这个教学假设的指导下，学习者可以收敛到比那些没有从教师那里得到任何指导的非教育学习者更紧密的泛化误差和标记复杂度界限：1）泛化误差的上界可以从$R(h^*)+4\\Delta_{T-1}$减少到大约$R(h^{\\mathcal{T}})+2\\Delta_{T-1}$，2）标记复杂度的上界可以从$4 \\theta",
    "tldr": "本文提出了一种黑盒教学假设$h^\\mathcal{T}$，通过引入更紧密的松弛项，可以改进主动学习中的假设修剪方法。理论上证明，在这个教学假设的指导下，学习者可以达到比无指导的学习者更好的泛化误差和标记复杂度界限。",
    "en_tdlr": "This paper proposes a black-box teaching hypothesis $h^\\mathcal{T}$ that improves hypothesis pruning in active learning by introducing a tighter slack term. Theoretically, it is proven that, under the guidance of this teaching hypothesis, learners can achieve better generalization error and label complexity bounds compared to non-taught learners."
}