{
    "title": "RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network. (arXiv:2206.14098v2 [cs.LG] UPDATED)",
    "abstract": "This work introduces RevSilo, the first reversible bidirectional multi-scale feature fusion module. Like other reversible methods, RevSilo eliminates the need to store hidden activations by recomputing them. However, existing reversible methods do not apply to multi-scale feature fusion and are, therefore, not applicable to a large class of networks. Bidirectional multi-scale feature fusion promotes local and global coherence and has become a de facto design principle for networks targeting spatially sensitive tasks, e.g., HRNet (Sun et al., 2019a) and EfficientDet (Tan et al., 2020). These networks achieve state-of-the-art results across various computer vision tasks when paired with high-resolution inputs. However, training them requires substantial accelerator memory for saving large, multi-resolution activations. These memory requirements inherently cap the size of neural networks, limiting improvements that come from scale. Operating across resolution scales, RevSilo alleviates th",
    "link": "http://arxiv.org/abs/2206.14098",
    "context": "Title: RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network. (arXiv:2206.14098v2 [cs.LG] UPDATED)\nAbstract: This work introduces RevSilo, the first reversible bidirectional multi-scale feature fusion module. Like other reversible methods, RevSilo eliminates the need to store hidden activations by recomputing them. However, existing reversible methods do not apply to multi-scale feature fusion and are, therefore, not applicable to a large class of networks. Bidirectional multi-scale feature fusion promotes local and global coherence and has become a de facto design principle for networks targeting spatially sensitive tasks, e.g., HRNet (Sun et al., 2019a) and EfficientDet (Tan et al., 2020). These networks achieve state-of-the-art results across various computer vision tasks when paired with high-resolution inputs. However, training them requires substantial accelerator memory for saving large, multi-resolution activations. These memory requirements inherently cap the size of neural networks, limiting improvements that come from scale. Operating across resolution scales, RevSilo alleviates th",
    "path": "papers/22/06/2206.14098.json",
    "total_tokens": 837,
    "translated_title": "RevBiFPN：完全可逆的双向特征金字塔网络",
    "translated_abstract": "本文提出了RevSilo，这是第一个可逆的双向多尺度特征融合模块。与其他可逆方法一样，RevSilo通过重新计算来消除存储隐藏激活所需的内存；然而，现有方法不适用于多尺度特征融合，因此不能应用于大部分网络。双向多尺度特征融合促进了局部和全局的一致性，并已成为针对空间敏感任务的网络的设计原则。这些网络在使用高分辨率输入时，在各种计算机视觉任务中实现了最先进的结果。然而，训练这些网络需要保存大型的多分辨率激活所需的大量加速器内存。这些内存需求本质上限制了神经网络的规模，限制了由规模带来的改进。跨分辨率尺度运作的RevSilo缓解了这些问题。",
    "tldr": "本文提出了RevSilo，一个完全可逆的双向多尺度特征融合模块，它缓解了神经网络规模受限的问题。",
    "en_tdlr": "This paper introduces RevSilo, a fully reversible bidirectional multi-scale feature fusion module which alleviates the issue of limited neural network scale by operating across resolution scales."
}