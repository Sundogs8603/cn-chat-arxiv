{
    "title": "Learning Soft Constraints From Constrained Expert Demonstrations. (arXiv:2206.01311v2 [cs.LG] UPDATED)",
    "abstract": "Inverse reinforcement learning (IRL) methods assume that the expert data is generated by an agent optimizing some reward function. However, in many settings, the agent may optimize a reward function subject to some constraints, where the constraints induce behaviors that may be otherwise difficult to express with just a reward function. We consider the setting where the reward function is given, and the constraints are unknown, and propose a method that is able to recover these constraints satisfactorily from the expert data. While previous work has focused on recovering hard constraints, our method can recover cumulative soft constraints that the agent satisfies on average per episode. In IRL fashion, our method solves this problem by adjusting the constraint function iteratively through a constrained optimization procedure, until the agent behavior matches the expert behavior. We demonstrate our approach on synthetic environments, robotics environments and real world highway driving ",
    "link": "http://arxiv.org/abs/2206.01311",
    "context": "Title: Learning Soft Constraints From Constrained Expert Demonstrations. (arXiv:2206.01311v2 [cs.LG] UPDATED)\nAbstract: Inverse reinforcement learning (IRL) methods assume that the expert data is generated by an agent optimizing some reward function. However, in many settings, the agent may optimize a reward function subject to some constraints, where the constraints induce behaviors that may be otherwise difficult to express with just a reward function. We consider the setting where the reward function is given, and the constraints are unknown, and propose a method that is able to recover these constraints satisfactorily from the expert data. While previous work has focused on recovering hard constraints, our method can recover cumulative soft constraints that the agent satisfies on average per episode. In IRL fashion, our method solves this problem by adjusting the constraint function iteratively through a constrained optimization procedure, until the agent behavior matches the expert behavior. We demonstrate our approach on synthetic environments, robotics environments and real world highway driving ",
    "path": "papers/22/06/2206.01311.json",
    "total_tokens": 821,
    "translated_title": "从约束专家演示中学习软约束",
    "translated_abstract": "反向强化学习方法假设专家数据是由优化某些奖励函数的代理生成的。然而，在许多情况下，代理可能会优化受某些限制的奖励函数，其中这些限制引导代理行为的表达可能更为困难。我们考虑奖励函数已知，但约束未知的情况，并提出了一种能够令代理数据满足约束的方法。以IRL的方式，我们的方法通过迭代调整约束函数，直到代理行为与专家行为匹配来解决这个问题。我们的方法能够恢复每个周期内代理平均满足的累积软约束，而先前的工作集中于恢复硬约束。我们在合成环境、机器人环境和实际的高速公路驾驶环境中展示了我们的方法。",
    "tldr": "本文提出了一种反向强化学习的方法，能够从专家数据中学习软约束，恢复每个周期内代理平均满足的累积约束。方法通过调整约束函数实现。",
    "en_tdlr": "This paper proposes a method for inverse reinforcement learning that can learn soft constraints from expert data and recover the average cumulative constraints satisfied by the agent per episode, by iteratively adjusting the constraint function until the agent behavior matches the expert behavior."
}