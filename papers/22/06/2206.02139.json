{
    "title": "Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks. (arXiv:2206.02139v3 [cs.LG] UPDATED)",
    "abstract": "The convergence of GD and SGD when training mildly parameterized neural networks starting from random initialization is studied. For a broad range of models and loss functions, including the most commonly used square loss and cross entropy loss, we prove an ``early stage convergence'' result. We show that the loss is decreased by a significant amount in the early stage of the training, and this decrease is fast. Furthurmore, for exponential type loss functions, and under some assumptions on the training data, we show global convergence of GD. Instead of relying on extreme over-parameterization, our study is based on a microscopic analysis of the activation patterns for the neurons, which helps us derive more powerful lower bounds for the gradient. The results on activation patterns, which we call ``neuron partition'', help build intuitions for understanding the behavior of neural networks' training dynamics, and may be of independent interest.",
    "link": "http://arxiv.org/abs/2206.02139",
    "context": "Title: Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks. (arXiv:2206.02139v3 [cs.LG] UPDATED)\nAbstract: The convergence of GD and SGD when training mildly parameterized neural networks starting from random initialization is studied. For a broad range of models and loss functions, including the most commonly used square loss and cross entropy loss, we prove an ``early stage convergence'' result. We show that the loss is decreased by a significant amount in the early stage of the training, and this decrease is fast. Furthurmore, for exponential type loss functions, and under some assumptions on the training data, we show global convergence of GD. Instead of relying on extreme over-parameterization, our study is based on a microscopic analysis of the activation patterns for the neurons, which helps us derive more powerful lower bounds for the gradient. The results on activation patterns, which we call ``neuron partition'', help build intuitions for understanding the behavior of neural networks' training dynamics, and may be of independent interest.",
    "path": "papers/22/06/2206.02139.json",
    "total_tokens": 1044,
    "translated_title": "对轻度参数化神经网络训练的早期收敛和全局收敛的研究",
    "translated_abstract": "本文研究了在随机初始化情况下训练轻度参数化神经网络时，GD和SGD的收敛性。对于包括最常用的平方损失和交叉熵损失在内的广泛模型和损失函数，我们证明了“早期收敛”结果。我们表明在训练的早期阶段损失函数会有较大程度的下降，这种下降是很快的。此外，对于指数型损失函数以及对于训练数据的一些假设，我们证明了GD的全局收敛。与依靠极端过度参数化不同的是，我们的研究是基于对神经元激活模式的微观分析，这有助于我们推导出更强大的梯度下界。我们称这种激活模式的结果为“神经元划分”，这有助于理解神经网络训练动态的行为，并可能具有独立的兴趣。",
    "tldr": "本文证明了对于大部分的轻度参数化神经网络和损失函数，包括平方损失和交叉熵损失，在早期阶段会有很快的下降。对于指数型损失函数，在一些对于数据的假设下，本文证明了GD会全局收敛。本文的研究基于对神经元激活模式的微观分析，得出了“神经元划分”的结果，有助于理解神经网络训练动态的行为，并可能具有独立的兴趣。",
    "en_tdlr": "This paper proves the early stage convergence result for a broad range of mildly parameterized neural networks and loss functions, including square loss and cross entropy loss. It also shows global convergence of GD for exponential type loss functions under some assumptions on the training data. The study is based on a microscopic analysis of the activation patterns for the neurons, leading to the result of \"neuron partition\", which can help understand the behavior of neural network training dynamics and may be of independent interest."
}