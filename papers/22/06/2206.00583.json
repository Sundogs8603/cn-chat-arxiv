{
    "title": "Calibrate and Debias Layer-wise Sampling for Graph Convolutional Networks. (arXiv:2206.00583v2 [cs.LG] UPDATED)",
    "abstract": "Multiple sampling-based methods have been developed for approximating and accelerating node embedding aggregation in graph convolutional networks (GCNs) training. Among them, a layer-wise approach recursively performs importance sampling to select neighbors jointly for existing nodes in each layer. This paper revisits the approach from a matrix approximation perspective, and identifies two issues in the existing layer-wise sampling methods: suboptimal sampling probabilities and estimation biases induced by sampling without replacement. To address these issues, we accordingly propose two remedies: a new principle for constructing sampling probabilities and an efficient debiasing algorithm. The improvements are demonstrated by extensive analyses of estimation variance and experiments on common benchmarks. Code and algorithm implementations are publicly available at https://github.com/ychen-stat-ml/GCN-layer-wise-sampling .",
    "link": "http://arxiv.org/abs/2206.00583",
    "context": "Title: Calibrate and Debias Layer-wise Sampling for Graph Convolutional Networks. (arXiv:2206.00583v2 [cs.LG] UPDATED)\nAbstract: Multiple sampling-based methods have been developed for approximating and accelerating node embedding aggregation in graph convolutional networks (GCNs) training. Among them, a layer-wise approach recursively performs importance sampling to select neighbors jointly for existing nodes in each layer. This paper revisits the approach from a matrix approximation perspective, and identifies two issues in the existing layer-wise sampling methods: suboptimal sampling probabilities and estimation biases induced by sampling without replacement. To address these issues, we accordingly propose two remedies: a new principle for constructing sampling probabilities and an efficient debiasing algorithm. The improvements are demonstrated by extensive analyses of estimation variance and experiments on common benchmarks. Code and algorithm implementations are publicly available at https://github.com/ychen-stat-ml/GCN-layer-wise-sampling .",
    "path": "papers/22/06/2206.00583.json",
    "total_tokens": 840,
    "translated_title": "校准和去偏图卷积网络的逐层采样",
    "translated_abstract": "已经开发了多种基于采样的方法来近似和加速图卷积网络（GCN）训练中的节点嵌入聚合。其中，逐层方法递归地执行重要性采样，共同选择每层中现有节点的邻居。本文从矩阵近似的角度重新审视了该方法，并确定了现有逐层采样方法中的两个问题：次优采样概率和无重复采样引起的估计偏差。为了解决这些问题，我们相应地提出了两种解决方案：构建采样概率的新原则和一种高效的去偏算法。通过广泛的估计方差分析和常见基准实验，证明了这些改进。代码和算法实现可在 https://github.com/ychen-stat-ml/GCN-layer-wise-sampling 上公开获取。",
    "tldr": "本文根据矩阵近似的角度重新审视了GCN节点嵌入聚合的逐层采样方法，并提出了解决次优采样概率和估计偏差问题的两种新方案。实验证明这些改进是有效的。",
    "en_tdlr": "This paper revisits the layer-wise approach of node embedding aggregation in graph convolutional networks (GCNs) from a matrix approximation perspective, and proposes two new remedies to address the issues of suboptimal sampling probabilities and estimation biases induced by sampling without replacement. Experimental results demonstrate the effectiveness of the improvements."
}