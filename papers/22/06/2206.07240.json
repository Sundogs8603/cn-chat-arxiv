{
    "title": "Test-Time Adaptation for Visual Document Understanding. (arXiv:2206.07240v2 [cs.CV] UPDATED)",
    "abstract": "For visual document understanding (VDU), self-supervised pretraining has been shown to successfully generate transferable representations, yet, effective adaptation of such representations to distribution shifts at test-time remains to be an unexplored area. We propose DocTTA, a novel test-time adaptation method for documents, that does source-free domain adaptation using unlabeled target document data. DocTTA leverages cross-modality self-supervised learning via masked visual language modeling, as well as pseudo labeling to adapt models learned on a \\textit{source} domain to an unlabeled \\textit{target} domain at test time. We introduce new benchmarks using existing public datasets for various VDU tasks, including entity recognition, key-value extraction, and document visual question answering. DocTTA shows significant improvements on these compared to the source model performance, up to 1.89\\% in (F1 score), 3.43\\% (F1 score), and 17.68\\% (ANLS score), respectively. Our benchmark dat",
    "link": "http://arxiv.org/abs/2206.07240",
    "context": "Title: Test-Time Adaptation for Visual Document Understanding. (arXiv:2206.07240v2 [cs.CV] UPDATED)\nAbstract: For visual document understanding (VDU), self-supervised pretraining has been shown to successfully generate transferable representations, yet, effective adaptation of such representations to distribution shifts at test-time remains to be an unexplored area. We propose DocTTA, a novel test-time adaptation method for documents, that does source-free domain adaptation using unlabeled target document data. DocTTA leverages cross-modality self-supervised learning via masked visual language modeling, as well as pseudo labeling to adapt models learned on a \\textit{source} domain to an unlabeled \\textit{target} domain at test time. We introduce new benchmarks using existing public datasets for various VDU tasks, including entity recognition, key-value extraction, and document visual question answering. DocTTA shows significant improvements on these compared to the source model performance, up to 1.89\\% in (F1 score), 3.43\\% (F1 score), and 17.68\\% (ANLS score), respectively. Our benchmark dat",
    "path": "papers/22/06/2206.07240.json",
    "total_tokens": 998,
    "translated_title": "测试时间自适应视觉文档理解",
    "translated_abstract": "对于视觉文档理解（VDU），自监督预训练已被证明能够成功生成具有可传递性的表示，然而，在测试时间对这种表示进行有效的适应仍然是一个未被探索的领域。我们提出了DocTTA，一种用于文档的新型测试时间适应方法，该方法使用无标签的目标文档数据进行无源域适应。DocTTA利用跨模态自监督学习和伪标签方法，将在“源”域上学到的模型适应到未标记的“目标”域。我们使用现有的公共数据集为各种VDU任务引入了新的基准，包括实体识别、键值提取和文档视觉问答。相比于源模型性能，DocTTA在这些任务上表现出了显著的改进，分别提高了1.89%（F1分数）、3.43%（F1分数）和17.68%（ANLS分数）。",
    "tldr": "该论文提出了一种测试时间自适应方法，用于将自监督预训练得到的表示适应到测试时的分布转移。通过利用跨模态自监督学习和伪标签方法，该方法在文档理解任务中实现了显著的改进，并在实体识别、键值提取和文档视觉问答上分别提高了1.89%、3.43%和17.68%。",
    "en_tdlr": "This paper proposes a test-time adaptation method for visual document understanding that adapts self-supervised pretrained representations to distribution shifts. By leveraging cross-modality self-supervised learning and pseudo labeling, the method achieves significant improvements in document understanding tasks, with up to 1.89% improvement in F1 score for entity recognition, 3.43% improvement in F1 score for key-value extraction, and 17.68% improvement in ANLS score for document visual question answering."
}