{
    "title": "A Conditional Gradient-based Method for Simple Bilevel Optimization with Convex Lower-level Problem. (arXiv:2206.08868v3 [math.OC] UPDATED)",
    "abstract": "In this paper, we study a class of bilevel optimization problems, also known as simple bilevel optimization, where we minimize a smooth objective function over the optimal solution set of another convex constrained optimization problem. Several iterative methods have been developed for tackling this class of problems. Alas, their convergence guarantees are either asymptotic for the upper-level objective, or the convergence rates are slow and sub-optimal. To address this issue, in this paper, we introduce a novel bilevel optimization method that locally approximates the solution set of the lower-level problem via a cutting plane, and then runs a conditional gradient update to decrease the upper-level objective. When the upper-level objective is convex, we show that our method requires ${\\mathcal{O}}(\\max\\{1/\\epsilon_f,1/\\epsilon_g\\})$ iterations to find a solution that is $\\epsilon_f$-optimal for the upper-level objective and $\\epsilon_g$-optimal for the lower-level objective. Moreover,",
    "link": "http://arxiv.org/abs/2206.08868",
    "context": "Title: A Conditional Gradient-based Method for Simple Bilevel Optimization with Convex Lower-level Problem. (arXiv:2206.08868v3 [math.OC] UPDATED)\nAbstract: In this paper, we study a class of bilevel optimization problems, also known as simple bilevel optimization, where we minimize a smooth objective function over the optimal solution set of another convex constrained optimization problem. Several iterative methods have been developed for tackling this class of problems. Alas, their convergence guarantees are either asymptotic for the upper-level objective, or the convergence rates are slow and sub-optimal. To address this issue, in this paper, we introduce a novel bilevel optimization method that locally approximates the solution set of the lower-level problem via a cutting plane, and then runs a conditional gradient update to decrease the upper-level objective. When the upper-level objective is convex, we show that our method requires ${\\mathcal{O}}(\\max\\{1/\\epsilon_f,1/\\epsilon_g\\})$ iterations to find a solution that is $\\epsilon_f$-optimal for the upper-level objective and $\\epsilon_g$-optimal for the lower-level objective. Moreover,",
    "path": "papers/22/06/2206.08868.json",
    "total_tokens": 881,
    "translated_title": "带约束下凸下层问题的简单双层优化条件梯度方法",
    "translated_abstract": "本文研究一类双层优化问题——简单双层优化，其中我们在另一个凸约束优化问题的最优解集上最小化平滑的目标函数。已经发展出了几种迭代方法来处理这类问题，但它们的收敛性保证要么是上层目标的渐近性，要么是收敛速率缓慢且亚优。为了解决这个问题，本文提出了一种新的双层优化方法，该方法通过切割平面局部逼近下层问题的解集，然后运行条件梯度更新来减少上层目标函数。当上层目标函数为凸函数时，我们证明了我们的方法需要${\\mathcal{O}}(\\max\\{1/\\epsilon_f,1/\\epsilon_g\\})$次迭代才能找到一个对于上层 和下层目标函数同时$\\epsilon_f$和$\\epsilon_g$最优的解。",
    "tldr": "本文提出了一种新的双层优化方法，该方法通过局部逼近下层问题的解集，然后运行条件梯度更新来减少上层目标函数，并且收敛性保证较好。",
    "en_tdlr": "This paper proposes a novel bilevel optimization method that locally approximates the solution set of the lower-level problem via a cutting plane, and then runs a conditional gradient update to decrease the upper-level objective, with good convergence guarantees."
}