{
    "title": "ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings. (arXiv:2206.12403v2 [cs.CV] UPDATED)",
    "abstract": "We present a scalable approach for learning open-world object-goal navigation (ObjectNav) -- the task of asking a virtual robot (agent) to find any instance of an object in an unexplored environment (e.g., \"find a sink\"). Our approach is entirely zero-shot -- i.e., it does not require ObjectNav rewards or demonstrations of any kind. Instead, we train on the image-goal navigation (ImageNav) task, in which agents find the location where a picture (i.e., goal image) was captured. Specifically, we encode goal images into a multimodal, semantic embedding space to enable training semantic-goal navigation (SemanticNav) agents at scale in unannotated 3D environments (e.g., HM3D). After training, SemanticNav agents can be instructed to find objects described in free-form natural language (e.g., \"sink\", \"bathroom sink\", etc.) by projecting language goals into the same multimodal, semantic embedding space. As a result, our approach enables open-world ObjectNav. We extensively evaluate our agents ",
    "link": "http://arxiv.org/abs/2206.12403",
    "context": "Title: ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings. (arXiv:2206.12403v2 [cs.CV] UPDATED)\nAbstract: We present a scalable approach for learning open-world object-goal navigation (ObjectNav) -- the task of asking a virtual robot (agent) to find any instance of an object in an unexplored environment (e.g., \"find a sink\"). Our approach is entirely zero-shot -- i.e., it does not require ObjectNav rewards or demonstrations of any kind. Instead, we train on the image-goal navigation (ImageNav) task, in which agents find the location where a picture (i.e., goal image) was captured. Specifically, we encode goal images into a multimodal, semantic embedding space to enable training semantic-goal navigation (SemanticNav) agents at scale in unannotated 3D environments (e.g., HM3D). After training, SemanticNav agents can be instructed to find objects described in free-form natural language (e.g., \"sink\", \"bathroom sink\", etc.) by projecting language goals into the same multimodal, semantic embedding space. As a result, our approach enables open-world ObjectNav. We extensively evaluate our agents ",
    "path": "papers/22/06/2206.12403.json",
    "total_tokens": 993,
    "translated_title": "ZSON: 使用多模式目标嵌入进行零样本目标导航",
    "translated_abstract": "我们提出了一种可扩展的方法来学习开放世界的物体目标导航(ObjectNav) -- 即让虚拟机器人(智能体)在未探索的环境中找到任何一个物体实例(例如，“找到一个水槽”的任务)。我们的方法完全是零样本的 -- 即不需要ObjectNav的奖励或任何形式的示范。相反，我们在图像目标导航(ImageNav)任务上进行训练，即使代理找到一个图片(即目标图片)被拍摄的位置。具体地说，我们将目标图片编码到一个多模式的语义嵌入空间中，以实现在未标注的3D环境(例如HM3D)中训练语义目标导航(SemanticNav)代理的规模扩展。训练后，可以指示SemanticNav代理根据自由形式的自然语言(例如，“水槽”，“浴室水槽”等)来查找物体，通过将语言目标映射到相同的多模式语义嵌入空间。因此，我们的方法实现了开放世界的ObjectNav。我们对代理进行了广泛的评估。",
    "tldr": "本文提出了一种使用多模式目标嵌入进行零样本目标导航的方法，通过在未标注的3D环境中训练语义目标导航代理，将目标图片编码成多模式的语义嵌入，实现了在开放世界中找到物体的能力。",
    "en_tdlr": "This paper presents a method for zero-shot object-goal navigation using multimodal goal embeddings. The approach trains semantic-goal navigation agents by encoding goal images into a multimodal semantic embedding space, enabling them to find objects in open-world environments without rewards or demonstrations."
}