{
    "title": "Provably Efficient Offline Reinforcement Learning with Trajectory-Wise Reward. (arXiv:2206.06426v2 [cs.LG] UPDATED)",
    "abstract": "The remarkable success of reinforcement learning (RL) heavily relies on observing the reward of every visited state-action pair. In many real world applications, however, an agent can observe only a score that represents the quality of the whole trajectory, which is referred to as the {\\em trajectory-wise reward}. In such a situation, it is difficult for standard RL methods to well utilize trajectory-wise reward, and large bias and variance errors can be incurred in policy evaluation. In this work, we propose a novel offline RL algorithm, called Pessimistic vAlue iteRaTion with rEward Decomposition (PARTED), which decomposes the trajectory return into per-step proxy rewards via least-squares-based reward redistribution, and then performs pessimistic value iteration based on the learned proxy reward. To ensure the value functions constructed by PARTED are always pessimistic with respect to the optimal ones, we design a new penalty term to offset the uncertainty of the proxy reward. For ",
    "link": "http://arxiv.org/abs/2206.06426",
    "context": "Title: Provably Efficient Offline Reinforcement Learning with Trajectory-Wise Reward. (arXiv:2206.06426v2 [cs.LG] UPDATED)\nAbstract: The remarkable success of reinforcement learning (RL) heavily relies on observing the reward of every visited state-action pair. In many real world applications, however, an agent can observe only a score that represents the quality of the whole trajectory, which is referred to as the {\\em trajectory-wise reward}. In such a situation, it is difficult for standard RL methods to well utilize trajectory-wise reward, and large bias and variance errors can be incurred in policy evaluation. In this work, we propose a novel offline RL algorithm, called Pessimistic vAlue iteRaTion with rEward Decomposition (PARTED), which decomposes the trajectory return into per-step proxy rewards via least-squares-based reward redistribution, and then performs pessimistic value iteration based on the learned proxy reward. To ensure the value functions constructed by PARTED are always pessimistic with respect to the optimal ones, we design a new penalty term to offset the uncertainty of the proxy reward. For ",
    "path": "papers/22/06/2206.06426.json",
    "total_tokens": 936,
    "translated_title": "带轨迹奖励的离线强化学习的可证效率性",
    "translated_abstract": "强化学习（RL）的显著成功严重依赖于观测每个访问的状态-动作对的奖励。然而在许多真实世界应用中，代理只能观察表示整个轨迹质量的得分，称为\"轨迹奖励\"。在这种情况下，标准的RL方法很难很好地利用轨迹奖励，并且可能会在策略评估中产生大的偏差和方差误差。在本文中，我们提出一种新的离线RL算法PARTED，它通过基于最小二乘的奖励重新分配将轨迹回报分解为每步代理奖励，然后基于学习的代理奖励执行悲观值迭代。为了确保PARTED构建的值函数始终对最优值函数悲观，我们设计了一个新的惩罚项来抵消代理奖励的不确定性。",
    "tldr": "本文提出了一种离线RL算法PARTED，它通过基于最小二乘的奖励重新分配将轨迹回报分解为每步代理奖励，然后基于学习的代理奖励执行悲观值迭代，用于解决轨迹奖励难以很好地利用的问题。"
}