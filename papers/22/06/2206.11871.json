{
    "title": "Offline RL for Natural Language Generation with Implicit Language Q Learning. (arXiv:2206.11871v2 [cs.CL] UPDATED)",
    "abstract": "Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability. Our method employs a combination of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing user-specified utility functions. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language gen",
    "link": "http://arxiv.org/abs/2206.11871",
    "context": "Title: Offline RL for Natural Language Generation with Implicit Language Q Learning. (arXiv:2206.11871v2 [cs.CL] UPDATED)\nAbstract: Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability. Our method employs a combination of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing user-specified utility functions. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language gen",
    "path": "papers/22/06/2206.11871.json",
    "total_tokens": 931,
    "translated_title": "基于隐式语言Q学习的自然语言生成离线强化学习方法",
    "translated_abstract": "巨型语言模型从文本语料库中提炼出广泛的知识。然而，当处理用户指定的任务时，它们可能会存在不一致性。这个问题可以通过在精心策划的数据集上进行监督学习或强化学习来解决。本文提出了一种新颖的离线强化学习方法，即隐式语言Q学习（ILQL），专门设计用于语言模型，它结合了RL算法的灵活效用最大化框架与监督学习利用先前收集的数据的能力，以及其简单性和稳定性。我们的方法在学习价值函数时采用价值保守性和隐式数据集支持约束的组合，然后将其用于引导语言模型生成，最大化用户指定的效用函数。除了经验性地验证了ILQL，我们还在自然语言生成中展示了离线RL能够有用的场景的详细经验分析。",
    "tldr": "本研究提出了一种新颖的自然语言生成离线强化学习方法ILQL，通过在学习价值函数时采用价值保守性和隐式数据集支持约束的组合，引导语言模型生成最大化用户指定的效用函数的语言输出，从而解决巨型语言模型完成用户指定任务时存在的不一致性问题。",
    "en_tdlr": "This study proposes a novel offline reinforcement learning method, ILQL, for natural language generation, which guides language model generations to maximize user-specified utility functions by combining value conservatism and an implicit dataset support constraint in learning value functions, addressing the inconsistency issue of large language models in completing user-specified tasks."
}