{
    "title": "A memory-efficient neural ODE framework based on high-level adjoint differentiation. (arXiv:2206.01298v3 [cs.LG] UPDATED)",
    "abstract": "Neural ordinary differential equations (neural ODEs) have emerged as a novel network architecture that bridges dynamical systems and deep learning. However, the gradient obtained with the continuous adjoint method in the vanilla neural ODE is not reverse-accurate. Other approaches suffer either from an excessive memory requirement due to deep computational graphs or from limited choices for the time integration scheme, hampering their application to large-scale complex dynamical systems. To achieve accurate gradients without compromising memory efficiency and flexibility, we present a new neural ODE framework, PNODE, based on high-level discrete adjoint algorithmic differentiation. By leveraging discrete adjoint time integrators and advanced checkpointing strategies tailored for these integrators, PNODE can provide a balance between memory and computational costs, while computing the gradients consistently and accurately. We provide an open-source implementation based on PyTorch and PE",
    "link": "http://arxiv.org/abs/2206.01298",
    "context": "Title: A memory-efficient neural ODE framework based on high-level adjoint differentiation. (arXiv:2206.01298v3 [cs.LG] UPDATED)\nAbstract: Neural ordinary differential equations (neural ODEs) have emerged as a novel network architecture that bridges dynamical systems and deep learning. However, the gradient obtained with the continuous adjoint method in the vanilla neural ODE is not reverse-accurate. Other approaches suffer either from an excessive memory requirement due to deep computational graphs or from limited choices for the time integration scheme, hampering their application to large-scale complex dynamical systems. To achieve accurate gradients without compromising memory efficiency and flexibility, we present a new neural ODE framework, PNODE, based on high-level discrete adjoint algorithmic differentiation. By leveraging discrete adjoint time integrators and advanced checkpointing strategies tailored for these integrators, PNODE can provide a balance between memory and computational costs, while computing the gradients consistently and accurately. We provide an open-source implementation based on PyTorch and PE",
    "path": "papers/22/06/2206.01298.json",
    "total_tokens": 907,
    "translated_title": "基于高阶伴随微分的内存高效神经ODE框架",
    "translated_abstract": "神经常微分方程(神经ODE)作为一种将动态系统和深度学习相结合的新型网络架构已经出现。然而，用于原始神经ODE中的连续伴随方法求解的梯度并不具有反向精度。其他方法由于深度计算图的需求过高或时间积分方案的选择受限，限制了其应用于大规模复杂动态系统。为了在不影响内存效率和灵活性的情况下实现精确梯度计算，我们提出了一种新的神经ODE框架PNODE，基于高级离散伴随算法微分。通过利用离散伴随时间积分器和为这些积分器量身定制的高级检查点策略，PNODE可在计算和内存成本之间提供平衡，同时计算梯度时保持一致和准确。我们提供了一个基于PyTorch和PE的开源实现。",
    "tldr": "本论文提出了一个基于高阶伴随微分的神经ODE框架PNODE，通过离散伴随时间积分器和先进的检查点策略实现内存高效和梯度计算的准确性，并提供了一个基于PyTorch和PE的开源实现。"
}