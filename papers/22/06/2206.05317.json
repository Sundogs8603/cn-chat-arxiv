{
    "title": "Intrinsic dimensionality and generalization properties of the $\\mathcal{R}$-norm inductive bias. (arXiv:2206.05317v2 [cs.LG] UPDATED)",
    "abstract": "We study the structural and statistical properties of $\\mathcal{R}$-norm minimizing interpolants of datasets labeled by specific target functions. The $\\mathcal{R}$-norm is the basis of an inductive bias for two-layer neural networks, recently introduced to capture the functional effect of controlling the size of network weights, independently of the network width. We find that these interpolants are intrinsically multivariate functions, even when there are ridge functions that fit the data, and also that the $\\mathcal{R}$-norm inductive bias is not sufficient for achieving statistically optimal generalization for certain learning problems. Altogether, these results shed new light on an inductive bias that is connected to practical neural network training.",
    "link": "http://arxiv.org/abs/2206.05317",
    "context": "Title: Intrinsic dimensionality and generalization properties of the $\\mathcal{R}$-norm inductive bias. (arXiv:2206.05317v2 [cs.LG] UPDATED)\nAbstract: We study the structural and statistical properties of $\\mathcal{R}$-norm minimizing interpolants of datasets labeled by specific target functions. The $\\mathcal{R}$-norm is the basis of an inductive bias for two-layer neural networks, recently introduced to capture the functional effect of controlling the size of network weights, independently of the network width. We find that these interpolants are intrinsically multivariate functions, even when there are ridge functions that fit the data, and also that the $\\mathcal{R}$-norm inductive bias is not sufficient for achieving statistically optimal generalization for certain learning problems. Altogether, these results shed new light on an inductive bias that is connected to practical neural network training.",
    "path": "papers/22/06/2206.05317.json",
    "total_tokens": 868,
    "translated_title": "$\\mathcal{R}$-范归纳偏差的内在维度和泛化性质研究",
    "translated_abstract": "我们研究了$\\mathcal{R}$-范最小化插值函数的结构和统计特性，它们对特定目标函数标记的数据集进行标记。$\\mathcal{R}$-范是一个归纳偏差，用于两层神经网络，最近引入了它来捕捉控制网络权重大小的功能效应，不受网络宽度的影响。我们发现，即使存在拟合数据的岭函数，这些插值函数也是内在的多变量函数；同时，$\\mathcal{R}$-范归纳偏差对于某些学习问题的统计最优泛化不足够。总的来说，这些结果揭示了一个与实际神经网络训练相关的归纳偏差的新见解。",
    "tldr": "本研究研究了$\\mathcal{R}$-范归纳偏差的结构和统计特性，发现即使存在岭函数拟合数据，插值函数也是内在的多变量函数；并且，$\\mathcal{R}$-范归纳偏差对于某些学习问题的统计最优泛化不足够。",
    "en_tdlr": "This study examines the structural and statistical properties of $\\mathcal{R}$-norm minimizing interpolants of datasets labeled by specific target functions. The findings suggest that these interpolants are intrinsically multivariate functions and that the $\\mathcal{R}$-norm inductive bias is insufficient for achieving statistically optimal generalization for certain learning problems."
}