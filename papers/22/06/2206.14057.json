{
    "title": "Safe Exploration Incurs Nearly No Additional Sample Complexity for Reward-free RL. (arXiv:2206.14057v3 [cs.LG] UPDATED)",
    "abstract": "Reward-free reinforcement learning (RF-RL), a recently introduced RL paradigm, relies on random action-taking to explore the unknown environment without any reward feedback information. While the primary goal of the exploration phase in RF-RL is to reduce the uncertainty in the estimated model with minimum number of trajectories, in practice, the agent often needs to abide by certain safety constraint at the same time. It remains unclear how such safe exploration requirement would affect the corresponding sample complexity in order to achieve the desired optimality of the obtained policy in planning. In this work, we make a first attempt to answer this question. In particular, we consider the scenario where a safe baseline policy is known beforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET) framework. We then particularize the SWEET framework to the tabular and the low-rank MDP settings, and develop algorithms coined Tabular-SWEET and Low-rank-SWEET, respectively. Bot",
    "link": "http://arxiv.org/abs/2206.14057",
    "context": "Title: Safe Exploration Incurs Nearly No Additional Sample Complexity for Reward-free RL. (arXiv:2206.14057v3 [cs.LG] UPDATED)\nAbstract: Reward-free reinforcement learning (RF-RL), a recently introduced RL paradigm, relies on random action-taking to explore the unknown environment without any reward feedback information. While the primary goal of the exploration phase in RF-RL is to reduce the uncertainty in the estimated model with minimum number of trajectories, in practice, the agent often needs to abide by certain safety constraint at the same time. It remains unclear how such safe exploration requirement would affect the corresponding sample complexity in order to achieve the desired optimality of the obtained policy in planning. In this work, we make a first attempt to answer this question. In particular, we consider the scenario where a safe baseline policy is known beforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET) framework. We then particularize the SWEET framework to the tabular and the low-rank MDP settings, and develop algorithms coined Tabular-SWEET and Low-rank-SWEET, respectively. Bot",
    "path": "papers/22/06/2206.14057.json",
    "total_tokens": 1144,
    "translated_title": "安全探索在没有奖励反馈的强化学习任务中几乎不会增加样本的复杂度",
    "translated_abstract": "无奖励反馈的强化学习（RF-RL）是最近引入的一种强化学习范式，依靠随机采取行动来探索未知的环境，没有任何奖励反馈信息。虽然RF-RL中探索阶段的主要目标是在最少轨迹数量的情况下减少对估计模型的不确定性，但在实践中，智能体经常需要同时遵守某些安全约束。目前尚不明确这种安全探索要求会如何影响相应的样本复杂度，以便在规划中实现所得到策略的所需最优性。在本文中，我们首次尝试回答这个问题。具体而言，我们考虑已知一个安全基线策略的情况，提出了一个统一的安全无奖励探索（SWEET）框架。我们然后特化SWEET框架到表格和低秩MDP设置中，并分别开发了被称为Tabular-SWEET和Low-rank-SWEET的算法。这两种算法能够在RF-RL的探索阶段中并入安全约束，并在某些假设下保证可证明的安全性，同时实现所需的性能。我们表明，在我们提出的算法中，安全探索引发的附加样本复杂度几乎为零，这表明安全约束和最优性目标可以同时实现而不会太大地降低样本效率。",
    "tldr": "本文提出 Safe reWard-frEe ExploraTion (SWEET)框架，在RF-RL任务中可将安全约束和探索效率同时实现，使得安全探索几乎不会增加额外的样本复杂度。",
    "en_tdlr": "This paper proposes the Safe reWard-frEe ExploraTion (SWEET) framework, which can achieve both safety constraints and exploration efficiency in the RF-RL task, and shows that safe exploration almost does not increase the additional sample complexity."
}