{
    "title": "ForestPrune: Compact Depth-Controlled Tree Ensembles. (arXiv:2206.00128v3 [stat.ML] UPDATED)",
    "abstract": "Tree ensembles are powerful models that achieve excellent predictive performances, but can grow to unwieldy sizes. These ensembles are often post-processed (pruned) to reduce memory footprint and improve interpretability. We present ForestPrune, a novel optimization framework to post-process tree ensembles by pruning depth layers from individual trees. Since the number of nodes in a decision tree increases exponentially with tree depth, pruning deep trees drastically compactifies ensembles. We develop a specialized optimization algorithm to efficiently obtain high-quality solutions to problems under ForestPrune. Our algorithm typically reaches good solutions in seconds for medium-size datasets and ensembles, with 10000s of rows and 100s of trees, resulting in significant speedups over existing approaches. Our experiments demonstrate that ForestPrune produces parsimonious models that outperform models extracted by existing post-processing algorithms.",
    "link": "http://arxiv.org/abs/2206.00128",
    "context": "Title: ForestPrune: Compact Depth-Controlled Tree Ensembles. (arXiv:2206.00128v3 [stat.ML] UPDATED)\nAbstract: Tree ensembles are powerful models that achieve excellent predictive performances, but can grow to unwieldy sizes. These ensembles are often post-processed (pruned) to reduce memory footprint and improve interpretability. We present ForestPrune, a novel optimization framework to post-process tree ensembles by pruning depth layers from individual trees. Since the number of nodes in a decision tree increases exponentially with tree depth, pruning deep trees drastically compactifies ensembles. We develop a specialized optimization algorithm to efficiently obtain high-quality solutions to problems under ForestPrune. Our algorithm typically reaches good solutions in seconds for medium-size datasets and ensembles, with 10000s of rows and 100s of trees, resulting in significant speedups over existing approaches. Our experiments demonstrate that ForestPrune produces parsimonious models that outperform models extracted by existing post-processing algorithms.",
    "path": "papers/22/06/2206.00128.json",
    "total_tokens": 879,
    "translated_title": "ForestPrune: 紧凑的深度可控树集成",
    "translated_abstract": "树集成是一种强大的模型，能够实现出色的预测性能，但可能会变得庞大而难以控制。这些集成通常会进行后处理(修剪)，以减少内存占用并提高可解释性。我们提出了一种新颖的优化框架ForestPrune，通过从单个树中修剪深度图层，对树集成进行后处理。由于决策树中节点数量随着树的深度呈指数增长，深树的修剪可显著压缩集成。我们开发了一种专门的优化算法，能够在ForestPrune下高效地获得高质量解决方案。我们的算法通常可以在中等规模的数据集和集成中在几秒钟内获得良好的解决方案，具有数万行和数百棵树，结果比现有的方法快得多。我们的实验表明，ForestPrune产生的简洁模型优于现有后处理算法提取的模型。",
    "tldr": "ForestPrune是一种可以通过修剪深度图层来优化树集成的新颖算法框架，它能够在中等规模的数据集和集成中显著压缩模型，从而实现更快的预测速度。",
    "en_tdlr": "ForestPrune is a novel algorithm framework that optimizes tree ensembles by pruning depth layers, producing smaller and faster models with improved interpretability. This framework is demonstrated to outperform existing post-processing algorithms."
}