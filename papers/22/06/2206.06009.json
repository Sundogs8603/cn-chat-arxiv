{
    "title": "Relative Policy-Transition Optimization for Fast Policy Transfer. (arXiv:2206.06009v3 [cs.LG] UPDATED)",
    "abstract": "We consider the problem of policy transfer between two Markov Decision Processes (MDPs). We introduce a lemma based on existing theoretical results in reinforcement learning to measure the relativity gap between two arbitrary MDPs, that is the difference between any two cumulative expected returns defined on different policies and environment dynamics. Based on this lemma, we propose two new algorithms referred to as Relative Policy Optimization (RPO) and Relative Transition Optimization (RTO), which offer fast policy transfer and dynamics modelling, respectively. RPO transfers the policy evaluated in one environment to maximize the return in another, while RTO updates the parameterized dynamics model to reduce the gap between the dynamics of the two environments. Integrating the two algorithms results in the complete Relative Policy-Transition Optimization (RPTO) algorithm, in which the policy interacts with the two environments simultaneously, such that data collections from two envi",
    "link": "http://arxiv.org/abs/2206.06009",
    "context": "Title: Relative Policy-Transition Optimization for Fast Policy Transfer. (arXiv:2206.06009v3 [cs.LG] UPDATED)\nAbstract: We consider the problem of policy transfer between two Markov Decision Processes (MDPs). We introduce a lemma based on existing theoretical results in reinforcement learning to measure the relativity gap between two arbitrary MDPs, that is the difference between any two cumulative expected returns defined on different policies and environment dynamics. Based on this lemma, we propose two new algorithms referred to as Relative Policy Optimization (RPO) and Relative Transition Optimization (RTO), which offer fast policy transfer and dynamics modelling, respectively. RPO transfers the policy evaluated in one environment to maximize the return in another, while RTO updates the parameterized dynamics model to reduce the gap between the dynamics of the two environments. Integrating the two algorithms results in the complete Relative Policy-Transition Optimization (RPTO) algorithm, in which the policy interacts with the two environments simultaneously, such that data collections from two envi",
    "path": "papers/22/06/2206.06009.json",
    "total_tokens": 940,
    "translated_title": "快速策略转移的相对策略过渡优化",
    "translated_abstract": "本文考虑了在两个马尔可夫决策过程之间的策略转移问题。我们引入了一个基于强化学习中现有理论结果的引理，用于衡量任意两个MDP之间的相对差距，即不同策略和环境动态下累积预期回报的差异。基于该引理，我们提出了两个新算法，分别为相对策略优化（RPO）和相对转移优化（RTO），分别提供了快速策略转移和动态建模。RPO通过将在一个环境中评估的策略转移以最大化在另一个环境中的回报，而RTO则通过更新参数化的动态模型来减小两个环境动态之间的差距。将这两个算法集成在一起可以得到完整的相对策略过渡优化（RPTO）算法，其中策略同时与两个环境进行交互，从而从两个环境中收集数据。",
    "tldr": "本论文介绍了相对策略过渡优化的方法，通过引入一个基于强化学习的引理衡量两个MDP之间的相对差距，并提出了相对策略优化和相对转移优化两个算法来实现快速策略转移和动态建模。同时，将这两个算法集成在一起形成完整的相对策略过渡优化算法。",
    "en_tdlr": "This paper introduces the method of relative policy-transition optimization, which measures the relative gap between two MDPs and proposes two algorithms, relative policy optimization and relative transition optimization, for fast policy transfer and dynamics modeling. The integration of these two algorithms forms the complete relative policy-transition optimization algorithm."
}