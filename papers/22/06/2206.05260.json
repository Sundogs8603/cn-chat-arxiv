{
    "title": "Balanced Product of Calibrated Experts for Long-Tailed Recognition. (arXiv:2206.05260v3 [cs.CV] UPDATED)",
    "abstract": "Many real-world recognition problems are characterized by long-tailed label distributions. These distributions make representation learning highly challenging due to limited generalization over the tail classes. If the test distribution differs from the training distribution, e.g. uniform versus long-tailed, the problem of the distribution shift needs to be addressed. A recent line of work proposes learning multiple diverse experts to tackle this issue. Ensemble diversity is encouraged by various techniques, e.g. by specializing different experts in the head and the tail classes. In this work, we take an analytical approach and extend the notion of logit adjustment to ensembles to form a Balanced Product of Experts (BalPoE). BalPoE combines a family of experts with different test-time target distributions, generalizing several previous approaches. We show how to properly define these distributions and combine the experts in order to achieve unbiased predictions, by proving that the ens",
    "link": "http://arxiv.org/abs/2206.05260",
    "context": "Title: Balanced Product of Calibrated Experts for Long-Tailed Recognition. (arXiv:2206.05260v3 [cs.CV] UPDATED)\nAbstract: Many real-world recognition problems are characterized by long-tailed label distributions. These distributions make representation learning highly challenging due to limited generalization over the tail classes. If the test distribution differs from the training distribution, e.g. uniform versus long-tailed, the problem of the distribution shift needs to be addressed. A recent line of work proposes learning multiple diverse experts to tackle this issue. Ensemble diversity is encouraged by various techniques, e.g. by specializing different experts in the head and the tail classes. In this work, we take an analytical approach and extend the notion of logit adjustment to ensembles to form a Balanced Product of Experts (BalPoE). BalPoE combines a family of experts with different test-time target distributions, generalizing several previous approaches. We show how to properly define these distributions and combine the experts in order to achieve unbiased predictions, by proving that the ens",
    "path": "papers/22/06/2206.05260.json",
    "total_tokens": 868,
    "translated_title": "为长尾识别设计的平衡校准专家乘积",
    "translated_abstract": "许多实际的识别问题都被长尾标签分布所表征。由于对尾部类别的泛化能力有限，这些分布使表示学习变得极具挑战性。如果测试分布与训练分布不同，例如均匀分布与长尾分布，就需要解决分布偏移的问题。最近一系列的工作提出了学习多个不同专家的方法来解决这个问题。通过在头部和尾部类别中专门化不同的专家，来鼓励集合多样性。在本研究中，我们采用分析方法，将逻辑调整的概念扩展到集合中，形成平衡专家乘积(BalPoE)。BalPoE组合了一组具有不同测试目标分布的专家，推广了几种先前的方法。我们展示了如何正确地定义这些分布并组合专家以实现无偏的预测，通过证明该集合可以准确地预测所有目标分布。",
    "tldr": "本文提出了一个平衡校准的专家乘积方法(BalPoE)来解决长尾识别问题，通过组合具有不同测试目标分布的专家以实现无偏的预测。",
    "en_tdlr": "This paper proposes a Balanced Product of Experts (BalPoE) method for addressing the long-tailed recognition problem by combining a family of experts with different test-time target distributions, and achieving unbiased predictions."
}