{
    "title": "Cross-lingual AMR Aligner: Paying Attention to Cross-Attention. (arXiv:2206.07587v2 [cs.CL] UPDATED)",
    "abstract": "This paper introduces a novel aligner for Abstract Meaning Representation (AMR) graphs that can scale cross-lingually, and is thus capable of aligning units and spans in sentences of different languages. Our approach leverages modern Transformer-based parsers, which inherently encode alignment information in their cross-attention weights, allowing us to extract this information during parsing. This eliminates the need for English-specific rules or the Expectation Maximization (EM) algorithm that have been used in previous approaches. In addition, we propose a guided supervised method using alignment to further enhance the performance of our aligner. We achieve state-of-the-art results in the benchmarks for AMR alignment and demonstrate our aligner's ability to obtain them across multiple languages. Our code will be available at \\href{https://www.github.com/Babelscape/AMR-alignment}{github.com/Babelscape/AMR-alignment}.",
    "link": "http://arxiv.org/abs/2206.07587",
    "context": "Title: Cross-lingual AMR Aligner: Paying Attention to Cross-Attention. (arXiv:2206.07587v2 [cs.CL] UPDATED)\nAbstract: This paper introduces a novel aligner for Abstract Meaning Representation (AMR) graphs that can scale cross-lingually, and is thus capable of aligning units and spans in sentences of different languages. Our approach leverages modern Transformer-based parsers, which inherently encode alignment information in their cross-attention weights, allowing us to extract this information during parsing. This eliminates the need for English-specific rules or the Expectation Maximization (EM) algorithm that have been used in previous approaches. In addition, we propose a guided supervised method using alignment to further enhance the performance of our aligner. We achieve state-of-the-art results in the benchmarks for AMR alignment and demonstrate our aligner's ability to obtain them across multiple languages. Our code will be available at \\href{https://www.github.com/Babelscape/AMR-alignment}{github.com/Babelscape/AMR-alignment}.",
    "path": "papers/22/06/2206.07587.json",
    "total_tokens": 886,
    "translated_title": "跨语言AMR Aligner: 重点关注交叉注意力",
    "translated_abstract": "本文介绍了一种新颖的AMR图对齐器，可以跨越多种语言以扩展其规模，因此能够对不同语言的句子中的单元和跨度进行对齐。我们的方法利用基于Transformer的现代解析器，在其交叉注意力权重中固有地编码对齐信息，从而允许我们在解析过程中提取此信息。这消除了以前方法中使用的英语特定规则或期望最大化(EM)算法的需要。此外，我们提出了一种使用对齐的引导监督方法来进一步增强我们的对齐器性能。我们在AMR对齐基准测试中获得了最先进的结果，并展示了我们的对齐器跨多种语言获得这些结果的能力。我们的代码将在 \\href{https://www.github.com/Babelscape/AMR-alignment}{github.com/Babelscape/AMR-alignment} 上公开提供。",
    "tldr": "本文提出了一种跨语言AMR图对齐器，采用现代Transformer解析器编码对齐信息，避免使用英语特定规则或EM算法，同时提出一种引导监督方法并在多语言语料库上实现了最先进的结果。",
    "en_tdlr": "This paper proposes a cross-lingual AMR aligner that leverages modern Transformer-based parsers to encode alignment information, eliminating the need for English-specific rules or the EM algorithm. It also introduces a guided supervised method and achieves state-of-the-art results on multi-lingual corpora."
}