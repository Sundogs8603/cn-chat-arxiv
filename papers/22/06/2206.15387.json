{
    "title": "Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning. (arXiv:2206.15387v3 [cs.LG] UPDATED)",
    "abstract": "An oft-cited challenge of federated learning is the presence of heterogeneity. \\emph{Data heterogeneity} refers to the fact that data from different clients may follow very different distributions. \\emph{System heterogeneity} refers to client devices having different system capabilities. A considerable number of federated optimization methods address this challenge. In the literature, empirical evaluations usually start federated training from random initialization. However, in many practical applications of federated learning, the server has access to proxy data for the training task that can be used to pre-train a model before starting federated training. Using four standard federated learning benchmark datasets, we empirically study the impact of starting from a pre-trained model in federated learning. Unsurprisingly, starting from a pre-trained model reduces the training time required to reach a target error rate and enables the training of more accurate models (up to 40\\%) than is",
    "link": "http://arxiv.org/abs/2206.15387",
    "context": "Title: Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning. (arXiv:2206.15387v3 [cs.LG] UPDATED)\nAbstract: An oft-cited challenge of federated learning is the presence of heterogeneity. \\emph{Data heterogeneity} refers to the fact that data from different clients may follow very different distributions. \\emph{System heterogeneity} refers to client devices having different system capabilities. A considerable number of federated optimization methods address this challenge. In the literature, empirical evaluations usually start federated training from random initialization. However, in many practical applications of federated learning, the server has access to proxy data for the training task that can be used to pre-train a model before starting federated training. Using four standard federated learning benchmark datasets, we empirically study the impact of starting from a pre-trained model in federated learning. Unsurprisingly, starting from a pre-trained model reduces the training time required to reach a target error rate and enables the training of more accurate models (up to 40\\%) than is",
    "path": "papers/22/06/2206.15387.json",
    "total_tokens": 864,
    "translated_title": "从哪里开始？关于联邦学习中预训练和初始化的影响。",
    "translated_abstract": "联邦学习中的一个常见挑战是异构性问题。数据异构性是指来自不同客户端的数据可能遵循非常不同的分布。系统异构性是指客户端设备具有不同的系统能力。许多联邦优化方法都解决了这个挑战。虽然文献中的实证评估通常从随机初始化开始联邦训练，但在许多实际的联邦学习应用中，服务器可以访问用于训练任务的代理数据，可以用这些数据在开始联邦训练之前预训练模型。使用四个标准联邦学习基准数据集，我们经验性地研究了在联邦学习中从经过预训练的模型开始的影响。不出所料，从经过预训练的模型开始可以减少达到目标误差率所需的训练时间，并使模型训练更准确（高达40％）。",
    "tldr": "该论文研究了在联邦学习中从预训练模型开始的影响，证明这种方法可以减少训练时间并提高模型的准确性。",
    "en_tdlr": "This paper studies the impact of starting from a pre-trained model in federated learning, demonstrating that this method can reduce training time and improve model accuracy."
}