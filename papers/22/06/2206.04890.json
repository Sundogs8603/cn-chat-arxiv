{
    "title": "Adversarial Counterfactual Environment Model Learning. (arXiv:2206.04890v2 [cs.LG] UPDATED)",
    "abstract": "A good model for action-effect prediction, named environment model, is important to achieve sample-efficient decision-making policy learning in many domains like robot control, recommender systems, and patients' treatment selection. We can take unlimited trials with such a model to identify the appropriate actions so that the costs of queries in the real world can be saved. It requires the model to handle unseen data correctly, also called counterfactual data. However, standard data fitting techniques do not automatically achieve such generalization ability and commonly result in unreliable models. In this work, we introduce counterfactual-query risk minimization (CQRM) in model learning for generalizing to a counterfactual dataset queried by a specific target policy. Since the target policies can be various and unknown in policy learning, we propose an adversarial CQRM objective in which the model learns on counterfactual data queried by adversarial policies, and finally derive a trac",
    "link": "http://arxiv.org/abs/2206.04890",
    "context": "Title: Adversarial Counterfactual Environment Model Learning. (arXiv:2206.04890v2 [cs.LG] UPDATED)\nAbstract: A good model for action-effect prediction, named environment model, is important to achieve sample-efficient decision-making policy learning in many domains like robot control, recommender systems, and patients' treatment selection. We can take unlimited trials with such a model to identify the appropriate actions so that the costs of queries in the real world can be saved. It requires the model to handle unseen data correctly, also called counterfactual data. However, standard data fitting techniques do not automatically achieve such generalization ability and commonly result in unreliable models. In this work, we introduce counterfactual-query risk minimization (CQRM) in model learning for generalizing to a counterfactual dataset queried by a specific target policy. Since the target policies can be various and unknown in policy learning, we propose an adversarial CQRM objective in which the model learns on counterfactual data queried by adversarial policies, and finally derive a trac",
    "path": "papers/22/06/2206.04890.json",
    "total_tokens": 935,
    "translated_title": "对抗性反事实环境模型学习",
    "translated_abstract": "在机器人控制、推荐系统和患者治疗选择等领域，一个好的行为-效果预测模型，即环境模型，对于实现高效的决策策略学习至关重要。我们可以利用这样的模型进行无限次试验，以确定适当的行动，从而节省在真实世界中的查询成本。这要求模型能够正确处理未知数据，即反事实数据。然而，标准的数据拟合技术不能自动实现这种泛化能力，通常会导致不可靠的模型。在这项工作中，我们引入了针对特定目标策略查询的反事实查询风险最小化（CQRM）方法，在模型学习中实现对反事实数据的泛化。由于策略学习中的目标策略可以是各种各样的且未知的，我们提出了一种对抗性CQRM目标，模型通过对抗性策略查询反事实数据进行学习，最终得到一个稳健且可靠的模型。",
    "tldr": "本研究引入了对抗性反事实环境模型学习方法，通过模型学习对反事实数据进行泛化处理，从而实现高效的决策策略学习。这种方法利用对抗性策略查询反事实数据，最终得到一个稳健且可靠的模型。",
    "en_tdlr": "This paper introduces adversarial counterfactual environment model learning, which achieves sample-efficient decision-making policy learning by generalizing to counterfactual data. The proposed method uses adversarial policy queries on counterfactual data to obtain a robust and reliable model."
}