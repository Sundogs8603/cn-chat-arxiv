{
    "title": "Stochastic Gradient Methods with Preconditioned Updates. (arXiv:2206.00285v2 [math.OC] UPDATED)",
    "abstract": "This work considers the non-convex finite sum minimization problem. There are several algorithms for such problems, but existing methods often work poorly when the problem is badly scaled and/or ill-conditioned, and a primary goal of this work is to introduce methods that alleviate this issue. Thus, here we include a preconditioner based on Hutchinson's approach to approximating the diagonal of the Hessian, and couple it with several gradient-based methods to give new scaled algorithms: Scaled SARAH and Scaled L-SVRG. Theoretical complexity guarantees under smoothness assumptions are presented. We prove linear convergence when both smoothness and the PL condition are assumed. Our adaptively scaled methods use approximate partial second-order curvature information and, therefore, can better mitigate the impact of badly scaled problems. This improved practical performance is demonstrated in the numerical experiments also presented in this work.",
    "link": "http://arxiv.org/abs/2206.00285",
    "context": "Title: Stochastic Gradient Methods with Preconditioned Updates. (arXiv:2206.00285v2 [math.OC] UPDATED)\nAbstract: This work considers the non-convex finite sum minimization problem. There are several algorithms for such problems, but existing methods often work poorly when the problem is badly scaled and/or ill-conditioned, and a primary goal of this work is to introduce methods that alleviate this issue. Thus, here we include a preconditioner based on Hutchinson's approach to approximating the diagonal of the Hessian, and couple it with several gradient-based methods to give new scaled algorithms: Scaled SARAH and Scaled L-SVRG. Theoretical complexity guarantees under smoothness assumptions are presented. We prove linear convergence when both smoothness and the PL condition are assumed. Our adaptively scaled methods use approximate partial second-order curvature information and, therefore, can better mitigate the impact of badly scaled problems. This improved practical performance is demonstrated in the numerical experiments also presented in this work.",
    "path": "papers/22/06/2206.00285.json",
    "total_tokens": 876,
    "translated_title": "具有预条件更新的随机梯度方法",
    "translated_abstract": "本文考虑了非凸有限求和最小化问题。目前已有一些算法用于解决这类问题，但当问题的尺度和/或条件不良时，现有方法通常工作效果较差。因此，本研究的主要目标是引入一种能够解决这个问题的方法。因此，本文采用基于Hutchinson方法近似Hessian对角元素的预条件器，并将其与几种基于梯度的方法相结合，得到了新的尺度算法：Scaled SARAH和Scaled L-SVRG。在光滑性假设下给出了理论复杂度保证。当同时假设光滑性和PL条件时，我们证明了线性收敛性。我们的自适应尺度方法使用了近似的二阶曲率信息，因此可以更好地减轻尺度不好的问题的影响。这种改进的实际性能在本文中的数值实验中也得到了验证。",
    "tldr": "本文介绍了一种具有预条件更新的随机梯度方法，包括了基于Hutchinson方法的预条件器和几种梯度方法，能够有效解决尺度不好和条件不良问题，并在光滑性和PL条件下证明了线性收敛性。"
}