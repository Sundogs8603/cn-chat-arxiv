{
    "title": "Blended Latent Diffusion. (arXiv:2206.02779v2 [cs.CV] UPDATED)",
    "abstract": "The tremendous progress in neural image generation, coupled with the emergence of seemingly omnipotent vision-language models has finally enabled text-based interfaces for creating and editing images. Handling generic images requires a diverse underlying generative model, hence the latest works utilize diffusion models, which were shown to surpass GANs in terms of diversity. One major drawback of diffusion models, however, is their relatively slow inference time. In this paper, we present an accelerated solution to the task of local text-driven editing of generic images, where the desired edits are confined to a user-provided mask. Our solution leverages a recent text-to-image Latent Diffusion Model (LDM), which speeds up diffusion by operating in a lower-dimensional latent space. We first convert the LDM into a local image editor by incorporating Blended Diffusion into it. Next we propose an optimization-based solution for the inherent inability of this LDM to accurately reconstruct i",
    "link": "http://arxiv.org/abs/2206.02779",
    "context": "Title: Blended Latent Diffusion. (arXiv:2206.02779v2 [cs.CV] UPDATED)\nAbstract: The tremendous progress in neural image generation, coupled with the emergence of seemingly omnipotent vision-language models has finally enabled text-based interfaces for creating and editing images. Handling generic images requires a diverse underlying generative model, hence the latest works utilize diffusion models, which were shown to surpass GANs in terms of diversity. One major drawback of diffusion models, however, is their relatively slow inference time. In this paper, we present an accelerated solution to the task of local text-driven editing of generic images, where the desired edits are confined to a user-provided mask. Our solution leverages a recent text-to-image Latent Diffusion Model (LDM), which speeds up diffusion by operating in a lower-dimensional latent space. We first convert the LDM into a local image editor by incorporating Blended Diffusion into it. Next we propose an optimization-based solution for the inherent inability of this LDM to accurately reconstruct i",
    "path": "papers/22/06/2206.02779.json",
    "total_tokens": 899,
    "translated_title": "混合潜在扩散",
    "translated_abstract": "随着神经图像生成的巨大进步以及似乎万能的视觉语言模型的出现，终于实现了基于文本创建和编辑图像的接口。处理通用 图像需要各种各样的生成模型，因此最新的作品采用扩散模型，据显示，在多样性方面超过了 GAN。然而，扩散模型的一个主要缺点是它们的推断时间相对较慢。本文提出了一种加速通用图像本地文本驱动编辑任务的解决方案，其中所需的编辑限制在用户提供的掩模内。我们的解决方案利用了最近的文本到图像潜在扩散模型（LDM），通过在低维潜在空间中操作来加速扩散。我们首先将 LDM 转换为本地图像编辑器，通过将混合扩散组合进去。接下来，我们针对 LDM 固有的无法准确重构图像的问题提出了一种基于优化的解决方案。",
    "tldr": "本文提出了一种加速通用图像本地文本驱动编辑任务的解决方案，其中使用混合扩散对最新的文本到图像潜在扩散模型进行加速，利用用户提供的掩模进行所需编辑。",
    "en_tdlr": "This paper presents an accelerated solution for local text-driven editing of generic images, utilizing blended diffusion to speed up the latest text-to-image latent diffusion model, while confining desired edits to a user-provided mask. An optimization-based solution is proposed for the inherent inability of the model to accurately reconstruct images."
}