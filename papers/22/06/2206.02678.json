{
    "title": "Provably Efficient Risk-Sensitive Reinforcement Learning: Iterated CVaR and Worst Path. (arXiv:2206.02678v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we study a novel episodic risk-sensitive Reinforcement Learning (RL) problem, named Iterated CVaR RL, which aims to maximize the tail of the reward-to-go at each step, and focuses on tightly controlling the risk of getting into catastrophic situations at each stage. This formulation is applicable to real-world tasks that demand strong risk avoidance throughout the decision process, such as autonomous driving, clinical treatment planning and robotics. We investigate two performance metrics under Iterated CVaR RL, i.e., Regret Minimization and Best Policy Identification. For both metrics, we design efficient algorithms ICVaR-RM and ICVaR-BPI, respectively, and provide nearly matching upper and lower bounds with respect to the number of episodes $K$. We also investigate an interesting limiting case of Iterated CVaR RL, called Worst Path RL, where the objective becomes to maximize the minimum possible cumulative reward. For Worst Path RL, we propose an efficient algorithm wi",
    "link": "http://arxiv.org/abs/2206.02678",
    "context": "Title: Provably Efficient Risk-Sensitive Reinforcement Learning: Iterated CVaR and Worst Path. (arXiv:2206.02678v2 [cs.LG] UPDATED)\nAbstract: In this paper, we study a novel episodic risk-sensitive Reinforcement Learning (RL) problem, named Iterated CVaR RL, which aims to maximize the tail of the reward-to-go at each step, and focuses on tightly controlling the risk of getting into catastrophic situations at each stage. This formulation is applicable to real-world tasks that demand strong risk avoidance throughout the decision process, such as autonomous driving, clinical treatment planning and robotics. We investigate two performance metrics under Iterated CVaR RL, i.e., Regret Minimization and Best Policy Identification. For both metrics, we design efficient algorithms ICVaR-RM and ICVaR-BPI, respectively, and provide nearly matching upper and lower bounds with respect to the number of episodes $K$. We also investigate an interesting limiting case of Iterated CVaR RL, called Worst Path RL, where the objective becomes to maximize the minimum possible cumulative reward. For Worst Path RL, we propose an efficient algorithm wi",
    "path": "papers/22/06/2206.02678.json",
    "total_tokens": 1024,
    "translated_title": "证明有效的风险敏感强化学习：迭代CVaR和最坏路径",
    "translated_abstract": "本文研究了一种名为迭代CVaR强化学习的新型风险敏感强化学习问题，旨在最大化每步回报的尾部，并专注于紧密控制每个阶段进入灾难性情况的风险。这种制定适用于需要在决策过程中强烈避免风险的现实任务，如自动驾驶、临床治疗规划和机器人。我们研究了迭代CVaR强化学习中的两个性能指标，即遗憾最小化和最佳策略识别。对于这两个指标，我们分别设计了有效的算法ICVaR-RM和ICVaR-BPI，并提供了与剧集数$K$相匹配的上限和下限。我们还研究了迭代CVaR强化学习的一个有趣的极限情况，称为最坏路径强化学习，其中目标是最大化可能的最小累积回报。对于最坏路径强化学习，我们提出了一种具有严格性能保证的高效算法。我们的理论分析和数值实验展示了我们所提出的算法在解决风险敏感强化学习问题方面的有效性和效率。",
    "tldr": "本文提出了迭代CVaR和最坏路径的风险敏感强化学习问题，并设计了有效算法解决该问题，适用于实际应用中需要避免风险的任务。",
    "en_tdlr": "This paper proposes the Iterated CVaR and Worst Path risk-sensitive Reinforcement Learning problem and designs efficient algorithms to solve it, which is applicable to tasks requiring risk avoidance in practical applications."
}