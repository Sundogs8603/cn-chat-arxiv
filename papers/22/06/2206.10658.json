{
    "title": "Questions Are All You Need to Train a Dense Passage Retriever. (arXiv:2206.10658v4 [cs.CL] UPDATED)",
    "abstract": "We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g. questions and potential answer documents). It uses a new document-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence documents, and (2) the documents are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both document and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtain",
    "link": "http://arxiv.org/abs/2206.10658",
    "context": "Title: Questions Are All You Need to Train a Dense Passage Retriever. (arXiv:2206.10658v4 [cs.CL] UPDATED)\nAbstract: We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g. questions and potential answer documents). It uses a new document-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence documents, and (2) the documents are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both document and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtain",
    "path": "papers/22/06/2206.10658.json",
    "total_tokens": 930,
    "translated_title": "答案有解：无需标记数据来训练密集检索模型",
    "translated_abstract": "我们引入了ART，一种新的语料库级自编码方法，用于训练密集检索模型，不需要任何标记的训练数据。在开放域任务中，如开放式问答（Open QA）中，密集检索是一个中心挑战，其中最先进的方法通常需要大量的监督数据集，具有自定义的硬负面挖掘和正面示例去噪声。与此相反，ART只需要访问未配对的输入和输出（例如，问题和潜在答案文件）。它使用一种新的文档检索自编码方案，其中（1）输入问题用于检索一组证据文档，（2）然后使用文档计算重构原始问题的概率。基于问题重构的检索训练可以有效地进行无监督学习，包括文档和问题编码器，可以稍后将其合并到完整的Open QA系统中，而不需要进一步进行微调。广泛的实验表明，ART获得了良好的性能。",
    "tldr": "ART是一种能够不使用标记数据进行训练的密集检索模型，并且只需要访问未配对的输入和输出。它使用一个新的文档检索自编码方案，通过问题重构进行检索训练，可以有效地进行无监督学习，并且可以将其合并到完整的Open QA系统中。",
    "en_tdlr": "ART is a dense retrieval model that can be trained without the use of labeled training data, and only requires access to unpaired inputs and outputs. It uses a new document-retrieval autoencoding scheme, which enables effective unsupervised learning through question reconstruction-based retrieval training, and can be incorporated into complete Open QA systems."
}