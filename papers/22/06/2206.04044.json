{
    "title": "Model-Based Reinforcement Learning for Offline Zero-Sum Markov Games",
    "abstract": "arXiv:2206.04044v2 Announce Type: replace  Abstract: This paper makes progress towards learning Nash equilibria in two-player zero-sum Markov games from offline data. Specifically, consider a $\\gamma$-discounted infinite-horizon Markov game with $S$ states, where the max-player has $A$ actions and the min-player has $B$ actions. We propose a pessimistic model-based algorithm with Bernstein-style lower confidence bounds -- called VI-LCB-Game -- that provably finds an $\\varepsilon$-approximate Nash equilibrium with a sample complexity no larger than $\\frac{C_{\\mathsf{clipped}}^{\\star}S(A+B)}{(1-\\gamma)^{3}\\varepsilon^{2}}$ (up to some log factor). Here, $C_{\\mathsf{clipped}}^{\\star}$ is some unilateral clipped concentrability coefficient that reflects the coverage and distribution shift of the available data (vis-\\`a-vis the target data), and the target accuracy $\\varepsilon$ can be any value within $\\big(0,\\frac{1}{1-\\gamma}\\big]$. Our sample complexity bound strengthens prior art by a ",
    "link": "https://arxiv.org/abs/2206.04044",
    "context": "Title: Model-Based Reinforcement Learning for Offline Zero-Sum Markov Games\nAbstract: arXiv:2206.04044v2 Announce Type: replace  Abstract: This paper makes progress towards learning Nash equilibria in two-player zero-sum Markov games from offline data. Specifically, consider a $\\gamma$-discounted infinite-horizon Markov game with $S$ states, where the max-player has $A$ actions and the min-player has $B$ actions. We propose a pessimistic model-based algorithm with Bernstein-style lower confidence bounds -- called VI-LCB-Game -- that provably finds an $\\varepsilon$-approximate Nash equilibrium with a sample complexity no larger than $\\frac{C_{\\mathsf{clipped}}^{\\star}S(A+B)}{(1-\\gamma)^{3}\\varepsilon^{2}}$ (up to some log factor). Here, $C_{\\mathsf{clipped}}^{\\star}$ is some unilateral clipped concentrability coefficient that reflects the coverage and distribution shift of the available data (vis-\\`a-vis the target data), and the target accuracy $\\varepsilon$ can be any value within $\\big(0,\\frac{1}{1-\\gamma}\\big]$. Our sample complexity bound strengthens prior art by a ",
    "path": "papers/22/06/2206.04044.json",
    "total_tokens": 925,
    "translated_title": "基于模型的离线零和马尔可夫博弈强化学习",
    "translated_abstract": "本文在从离线数据中学习两人零和马尔可夫博弈的纳什均衡方面取得进展。我们提出了一种基于模型的悲观算法，即具有Bernstein风格的下限置信界的VI-LCB-Game算法，可以证明以样本复杂度不大于$\\frac{C_{\\mathsf{clipped}}^{\\star}S(A+B)}{(1-\\gamma)^{3}\\varepsilon^{2}}$（带有一些对数因子）找到一个$\\varepsilon$-近似的纳什均衡。在这里，$C_{\\mathsf{clipped}}^{\\star}$ 是反映可用数据（关于目标数据）的覆盖率和分布转变的某种单侧剪切的集中度系数，目标精度$\\varepsilon$ 可以是$\\big(0,\\frac{1}{1-\\gamma}\\big]$范围内的任何值。我们的样本复杂度界限加强了先前的研究。",
    "tldr": "本文提出了一种基于模型的悲观算法 VI-LCB-Game，在离线数据中找到了两人零和马尔可夫博弈的纳什均衡，加强了先前研究。",
    "en_tdlr": "The paper presents a model-based pessimistic algorithm VI-LCB-Game that finds Nash equilibria for two-player zero-sum Markov games from offline data, strengthening prior research."
}