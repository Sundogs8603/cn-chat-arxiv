{
    "title": "Discretization Invariant Networks for Learning Maps between Neural Fields. (arXiv:2206.01178v4 [cs.LG] UPDATED)",
    "abstract": "With the emergence of powerful representations of continuous data in the form of neural fields, there is a need for discretization invariant learning: an approach for learning maps between functions on continuous domains without being sensitive to how the function is sampled. We present a new framework for understanding and designing discretization invariant neural networks (DI-Nets), which generalizes many discrete networks such as convolutional neural networks as well as continuous networks such as neural operators. Our analysis establishes upper bounds on the deviation in model outputs under different finite discretizations, and highlights the central role of point set discrepancy in characterizing such bounds. This insight leads to the design of a family of neural networks driven by numerical integration via quasi-Monte Carlo sampling with discretizations of low discrepancy. We prove by construction that DI-Nets universally approximate a large class of maps between integrable funct",
    "link": "http://arxiv.org/abs/2206.01178",
    "context": "Title: Discretization Invariant Networks for Learning Maps between Neural Fields. (arXiv:2206.01178v4 [cs.LG] UPDATED)\nAbstract: With the emergence of powerful representations of continuous data in the form of neural fields, there is a need for discretization invariant learning: an approach for learning maps between functions on continuous domains without being sensitive to how the function is sampled. We present a new framework for understanding and designing discretization invariant neural networks (DI-Nets), which generalizes many discrete networks such as convolutional neural networks as well as continuous networks such as neural operators. Our analysis establishes upper bounds on the deviation in model outputs under different finite discretizations, and highlights the central role of point set discrepancy in characterizing such bounds. This insight leads to the design of a family of neural networks driven by numerical integration via quasi-Monte Carlo sampling with discretizations of low discrepancy. We prove by construction that DI-Nets universally approximate a large class of maps between integrable funct",
    "path": "papers/22/06/2206.01178.json",
    "total_tokens": 961,
    "translated_title": "离散不变网络用于学习神经场之间的映射",
    "translated_abstract": "随着神经场形式的连续数据强大表示的出现，需要一种离散不变的学习方法：一种在连续域上学习函数之间映射的方法，不受函数采样方式的影响。我们提出了一个新的框架用于理解和设计离散不变的神经网络（DI-Nets），它广泛推广了许多离散网络（如卷积神经网络）和连续网络（如神经算子）。我们的分析确定了在不同有限离散化下模型输出的上界，并强调了点集差异在表征这种界限方面的核心作用。这一洞察力促使我们设计了一类由数值积分驱动的神经网络，通过低差异度的离散化进行准蒙特卡洛抽样。我们通过构造证明了DI-Nets可以普遍逼近一类介于可积函数之间的映射。",
    "tldr": "该论文提出了一种用于学习神经场之间映射的离散不变网络（DI-Nets）框架，并通过分析得出模型输出在不同离散化下的上界，并强调了点集差异在界限表征方面的重要性。通过数值积分进行低差异度离散化的网络设计，证明DI-Nets可以普遍逼近一类映射。",
    "en_tdlr": "This paper presents a framework for learning maps between neural fields called Discretization Invariant Networks (DI-Nets), which is designed to be insensitive to how the function is sampled. The analysis establishes upper bounds on the model outputs under different discretizations and emphasizes the significance of point set discrepancy in characterizing these bounds. The proposed DI-Nets leverage numerical integration with low discrepancy discretizations and are proven to universally approximate a class of maps between integrable functions."
}