{
    "title": "Generalized Supervised Contrastive Learning. (arXiv:2206.00384v2 [cs.CV] UPDATED)",
    "abstract": "With the recent promising results of contrastive learning in the self-supervised learning paradigm, supervised contrastive learning has successfully extended these contrastive approaches to supervised contexts, outperforming cross-entropy on various datasets. However, supervised contrastive learning inherently employs label information in a binary form--either positive or negative--using a one-hot target vector. This structure struggles to adapt to methods that exploit label information as a probability distribution, such as CutMix and knowledge distillation. In this paper, we introduce a generalized supervised contrastive loss, which measures cross-entropy between label similarity and latent similarity. This concept enhances the capabilities of supervised contrastive loss by fully utilizing the label distribution and enabling the adaptation of various existing techniques for training modern neural networks. Leveraging this generalized supervised contrastive loss, we construct a tailor",
    "link": "http://arxiv.org/abs/2206.00384",
    "context": "Title: Generalized Supervised Contrastive Learning. (arXiv:2206.00384v2 [cs.CV] UPDATED)\nAbstract: With the recent promising results of contrastive learning in the self-supervised learning paradigm, supervised contrastive learning has successfully extended these contrastive approaches to supervised contexts, outperforming cross-entropy on various datasets. However, supervised contrastive learning inherently employs label information in a binary form--either positive or negative--using a one-hot target vector. This structure struggles to adapt to methods that exploit label information as a probability distribution, such as CutMix and knowledge distillation. In this paper, we introduce a generalized supervised contrastive loss, which measures cross-entropy between label similarity and latent similarity. This concept enhances the capabilities of supervised contrastive loss by fully utilizing the label distribution and enabling the adaptation of various existing techniques for training modern neural networks. Leveraging this generalized supervised contrastive loss, we construct a tailor",
    "path": "papers/22/06/2206.00384.json",
    "total_tokens": 1094,
    "translated_title": "广义的有监督对比学习",
    "translated_abstract": "随着对自监督学习中对比学习的最新进展，有监督对比学习已成功将这些对比方法扩展到有监督的上下文中，在各种数据集上优于交叉熵。然而，有监督对比学习固有地使用以一位热目标向量的形式表示的标签信息，这种结构无法适应利用标签信息作为概率分布的方法，如CutMix和知识蒸馏。在本文中，我们介绍了一种广义的有监督对比损失，该损失度量了标签相似性和潜在相似性之间的交叉熵。这个概念通过充分利用标签分布，并使各种现有技术适应于训练现代神经网络而增强了有监督对比损失的能力。利用这种广义的有监督对比损失，我们为图像分类构建了一个定制的损失函数，并在几个基准数据集上进行了实验，包括CIFAR-10，CIFAR-100和ImageNet。提出的方法显示出比现有的有监督对比学习方法更好的效果，并在CIFAR-10，CIFAR-100和ImageNet上实现了最先进的结果。",
    "tldr": "本文提出了一种广义的有监督对比损失，通过充分利用标签分布来增强有监督对比损失的能力，从而适应各种现有技术，包括CutMix和知识蒸馏。在几个基准数据集上进行了实验，结果显示该方法优于现有的有监督对比学习方法，并在CIFAR-10，CIFAR-100和ImageNet上实现了最先进的结果。"
}