{
    "title": "Interpretable Mixture of Experts. (arXiv:2206.02107v2 [cs.LG] UPDATED)",
    "abstract": "The need for reliable model explanations is prominent for many machine learning applications, particularly for tabular and time-series data as their use cases often involve high-stakes decision making. Towards this goal, we introduce a novel interpretable modeling framework, Interpretable Mixture of Experts (IME), that yields high accuracy, comparable to `black-box' Deep Neural Networks (DNNs) in many cases, along with useful interpretability capabilities. IME consists of an assignment module and a mixture of experts, with each sample being assigned to a single expert for prediction. We introduce multiple options for IME based on the assignment and experts being interpretable. When the experts are chosen to be interpretable such as linear models, IME yields an inherently-interpretable architecture where the explanations produced by IME are the exact descriptions of how the prediction is computed. In addition to constituting a standalone inherently-interpretable architecture, IME has th",
    "link": "http://arxiv.org/abs/2206.02107",
    "context": "Title: Interpretable Mixture of Experts. (arXiv:2206.02107v2 [cs.LG] UPDATED)\nAbstract: The need for reliable model explanations is prominent for many machine learning applications, particularly for tabular and time-series data as their use cases often involve high-stakes decision making. Towards this goal, we introduce a novel interpretable modeling framework, Interpretable Mixture of Experts (IME), that yields high accuracy, comparable to `black-box' Deep Neural Networks (DNNs) in many cases, along with useful interpretability capabilities. IME consists of an assignment module and a mixture of experts, with each sample being assigned to a single expert for prediction. We introduce multiple options for IME based on the assignment and experts being interpretable. When the experts are chosen to be interpretable such as linear models, IME yields an inherently-interpretable architecture where the explanations produced by IME are the exact descriptions of how the prediction is computed. In addition to constituting a standalone inherently-interpretable architecture, IME has th",
    "path": "papers/22/06/2206.02107.json",
    "total_tokens": 1050,
    "translated_title": "可解释的专家混合模型",
    "translated_abstract": "在许多机器学习应用中，特别是在表格和时间序列数据中，需要可靠的模型解释，因为它们的使用案例经常涉及高风险的决策制定。为了实现这一目标，我们引入了一种新的可解释建模框架——可解释的专家混合模型（IME），它在很多情况下可以产生与“黑盒”深度神经网络（DNN）相媲美的高精度，并具有有用的可解释性能力。IME包括一个分配模块和一个专家混合模型，每个样本被分配给一个单一的专家进行预测。我们为IME引入多个选项，基于可解释的分配和专家的选择。当专家被选择为可解释的线性模型时，IME产生了一种内在可解释的体系结构，IME产生的解释是预测计算的精确描述。除了构成独立的内在可解释的体系结构外，IME还具有提供预训练的黑盒模型预测后事实解释的能力，从而增加了DNNs的可解释性。我们的实验表明，IME在表格和时间序列预测任务上实现了有竞争力的性能，并提供了有用的专家和分配解释。",
    "tldr": "介绍了一种新的可解释建模框架——可解释的专家混合模型（IME），能够产生与“黑盒”深度神经网络相媲美的高精度，并在提供有用解释的同时，还具有解释黑盒模型预测的能力。",
    "en_tdlr": "Introducing a novel and interpretable modeling framework, Interpretable Mixture of Experts (IME), that achieves high accuracy comparable to black-box Deep Neural Networks (DNNs) and offers useful explanations for predictions made by both IME and pre-trained black-box models for tabular and time-series data."
}