{
    "title": "Dual Representation Learning for Out-of-Distribution Detection. (arXiv:2206.09387v2 [cs.LG] UPDATED)",
    "abstract": "To classify in-distribution samples, deep neural networks explore strongly label-related information and discard weakly label-related information according to the information bottleneck. Out-of-distribution samples drawn from distributions differing from that of in-distribution samples could be assigned with unexpected high-confidence predictions because they could obtain minimum strongly label-related information. To distinguish in- and out-of-distribution samples, Dual Representation Learning (DRL) makes out-of-distribution samples harder to have high-confidence predictions by exploring both strongly and weakly label-related information from in-distribution samples. For a pretrained network exploring strongly label-related information to learn label-discriminative representations, DRL trains its auxiliary network exploring the remaining weakly label-related information to learn distribution-discriminative representations. Specifically, for a label-discriminative representation, DRL c",
    "link": "http://arxiv.org/abs/2206.09387",
    "context": "Title: Dual Representation Learning for Out-of-Distribution Detection. (arXiv:2206.09387v2 [cs.LG] UPDATED)\nAbstract: To classify in-distribution samples, deep neural networks explore strongly label-related information and discard weakly label-related information according to the information bottleneck. Out-of-distribution samples drawn from distributions differing from that of in-distribution samples could be assigned with unexpected high-confidence predictions because they could obtain minimum strongly label-related information. To distinguish in- and out-of-distribution samples, Dual Representation Learning (DRL) makes out-of-distribution samples harder to have high-confidence predictions by exploring both strongly and weakly label-related information from in-distribution samples. For a pretrained network exploring strongly label-related information to learn label-discriminative representations, DRL trains its auxiliary network exploring the remaining weakly label-related information to learn distribution-discriminative representations. Specifically, for a label-discriminative representation, DRL c",
    "path": "papers/22/06/2206.09387.json",
    "total_tokens": 803,
    "translated_title": "对于非分布检测的双表示学习",
    "translated_abstract": "为了将内部分布样本分类，深度神经网络探索与标签相关的信息，并根据信息瓶颈原则舍弃与标签弱相关的信息。与内部分布样本不同的非分布样本可能被赋予意外的高置信度预测，因为它们可能获得最小的与标签强相关的信息。为了区分内部和非分布样本，双表示学习 （DRL）通过从内部分布样本中同时探索与标签强相关和与标签弱相关的信息，使非分布样本更难获得高置信度预测。对于一个探索与标签强相关信息的预训练网络来学习标签鉴别性表示，DRL训练其探索剩余与标签弱相关信息的辅助网络来学习分布鉴别性表示。",
    "tldr": "本论文提出了双表示学习（DRL）方法，通过同时探索与标签强相关和与标签弱相关的信息，来区分内部和非分布样本，进而提高非分布检测的准确性。",
    "en_tdlr": "This paper proposes a Dual Representation Learning (DRL) method to enhance out-of-distribution detection by exploring both strongly and weakly label-related information, improving the accuracy of distinguishing in-distribution and out-of-distribution samples."
}