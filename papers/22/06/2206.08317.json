{
    "title": "Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition. (arXiv:2206.08317v3 [cs.SD] UPDATED)",
    "abstract": "Transformers have recently dominated the ASR field. Although able to yield good performance, they involve an autoregressive (AR) decoder to generate tokens one by one, which is computationally inefficient. To speed up inference, non-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to enable parallel generation. However, due to an independence assumption within the output tokens, performance of single-step NAR is inferior to that of AR models, especially with a large-scale corpus. There are two challenges to improving single-step NAR: Firstly to accurately predict the number of output tokens and extract hidden variables; secondly, to enhance modeling of interdependence between output tokens. To tackle both challenges, we propose a fast and accurate parallel transformer, termed Paraformer. This utilizes a continuous integrate-and-fire based predictor to predict the number of tokens and generate hidden variables. A glancing language model (GLM) sampler then generates sem",
    "link": "http://arxiv.org/abs/2206.08317",
    "context": "Title: Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition. (arXiv:2206.08317v3 [cs.SD] UPDATED)\nAbstract: Transformers have recently dominated the ASR field. Although able to yield good performance, they involve an autoregressive (AR) decoder to generate tokens one by one, which is computationally inefficient. To speed up inference, non-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to enable parallel generation. However, due to an independence assumption within the output tokens, performance of single-step NAR is inferior to that of AR models, especially with a large-scale corpus. There are two challenges to improving single-step NAR: Firstly to accurately predict the number of output tokens and extract hidden variables; secondly, to enhance modeling of interdependence between output tokens. To tackle both challenges, we propose a fast and accurate parallel transformer, termed Paraformer. This utilizes a continuous integrate-and-fire based predictor to predict the number of tokens and generate hidden variables. A glancing language model (GLM) sampler then generates sem",
    "path": "papers/22/06/2206.08317.json",
    "total_tokens": 1060,
    "translated_title": "Paraformer：用于非自回归端到端语音识别的快速准确并行Transformer",
    "translated_abstract": "近年来，Transformer已经成为ASR领域的主要方法。虽然能够产生良好的性能，但是它们涉及使用自回归（AR）解码器一个一个地生成令牌，这在计算上是低效的。为了加速推理，设计了非自回归（NAR）方法，例如单步NAR，以实现并行生成。但是，由于输出令牌中存在独立性假设，单步NAR的性能不如AR模型，特别是在大规模语料库中。提高单步NAR的两个挑战是：第一是准确预测输出令牌的数量和提取隐藏变量；第二是增强输出令牌之间的相互依赖建模。为了解决这两个挑战，我们提出了一种快速准确的并行Transformer模型，称为Paraformer。这利用基于连续积分-火器基础的预测器来预测令牌数并生成隐藏变量。然后，一个扫视式语言模型（GLM）采样器会根据隐藏变量生成语义合理的子序列。在广泛使用的LibriSpeech数据集上的实验表明，Paraformer在NAR模型的识别精度和推理速度方面均实现了最先进的性能。",
    "tldr": "Paraformer是用于非自回归端到端语音识别的快速准确并行Transformer，通过使用连续积分-火器预测器和扫视式语言模型采样器解决了单步NAR的挑战，并在LibriSpeech数据集上取得了最先进的性能。",
    "en_tdlr": "Paraformer is a fast and accurate parallel Transformer for non-autoregressive end-to-end speech recognition, overcoming the challenges of single-step NAR with a continuous integrate-and-fire predictor and a glancing language model sampler, achieving state-of-the-art performance on the LibriSpeech dataset."
}