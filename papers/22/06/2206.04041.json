{
    "title": "Neural Collapse: A Review on Modelling Principles and Generalization. (arXiv:2206.04041v2 [cs.LG] UPDATED)",
    "abstract": "Deep classifier neural networks enter the terminal phase of training (TPT) when training error reaches zero and tend to exhibit intriguing Neural Collapse (NC) properties. Neural collapse essentially represents a state at which the within-class variability of final hidden layer outputs is infinitesimally small and their class means form a simplex equiangular tight frame. This simplifies the last layer behaviour to that of a nearest-class center decision rule. Despite the simplicity of this state, the dynamics and implications of reaching it are yet to be fully understood. In this work, we review the principles which aid in modelling neural collapse, followed by the implications of this state on generalization and transfer learning capabilities of neural networks. Finally, we conclude by discussing potential avenues and directions for future research.",
    "link": "http://arxiv.org/abs/2206.04041",
    "context": "Title: Neural Collapse: A Review on Modelling Principles and Generalization. (arXiv:2206.04041v2 [cs.LG] UPDATED)\nAbstract: Deep classifier neural networks enter the terminal phase of training (TPT) when training error reaches zero and tend to exhibit intriguing Neural Collapse (NC) properties. Neural collapse essentially represents a state at which the within-class variability of final hidden layer outputs is infinitesimally small and their class means form a simplex equiangular tight frame. This simplifies the last layer behaviour to that of a nearest-class center decision rule. Despite the simplicity of this state, the dynamics and implications of reaching it are yet to be fully understood. In this work, we review the principles which aid in modelling neural collapse, followed by the implications of this state on generalization and transfer learning capabilities of neural networks. Finally, we conclude by discussing potential avenues and directions for future research.",
    "path": "papers/22/06/2206.04041.json",
    "total_tokens": 867,
    "translated_title": "神经崩溃：建模原则和泛化的综述",
    "translated_abstract": "当训练误差降至零并且最终隐藏层输出的类内变异性趋近于零时，深度分类神经网络进入训练终端阶段，往往呈现出引人入胜的神经崩溃特性。神经崩溃实际上代表了一个状态，其中最终隐藏层输出的类均值形成一个简单的等角紧框架。尽管这种状态很简单，但达到它的动态和影响还有待完全理解。在这项工作中，我们回顾了有助于建模神经崩溃的原则，接着研究了这种状态对神经网络的泛化和迁移学习能力的影响。最后，我们讨论了未来研究的潜在途径和方向。",
    "tldr": "神经崩溃是当深度分类神经网络训练误差降至零时出现的一种状态，它简化了最后一层的行为，具有最近类中心决策规则。本文综述了神经崩溃的建模原则，以及它对神经网络的泛化和迁移学习能力的影响。",
    "en_tdlr": "Neural collapse is a state that occurs when the training error of deep classification neural networks reaches zero and simplifies the behavior of the last layer to that of a nearest-class center decision rule. This article reviews the principles of modeling neural collapse and its implications for the generalization and transfer learning capabilities of neural networks."
}