{
    "title": "Proximal Splitting Adversarial Attacks for Semantic Segmentation. (arXiv:2206.07179v2 [cs.LG] UPDATED)",
    "abstract": "Classification has been the focal point of research on adversarial attacks, but only a few works investigate methods suited to denser prediction tasks, such as semantic segmentation. The methods proposed in these works do not accurately solve the adversarial segmentation problem and, therefore, overestimate the size of the perturbations required to fool models. Here, we propose a white-box attack for these models based on a proximal splitting to produce adversarial perturbations with much smaller $\\ell_\\infty$ norms. Our attack can handle large numbers of constraints within a nonconvex minimization framework via an Augmented Lagrangian approach, coupled with adaptive constraint scaling and masking strategies. We demonstrate that our attack significantly outperforms previously proposed ones, as well as classification attacks that we adapted for segmentation, providing a first comprehensive benchmark for this dense task.",
    "link": "http://arxiv.org/abs/2206.07179",
    "context": "Title: Proximal Splitting Adversarial Attacks for Semantic Segmentation. (arXiv:2206.07179v2 [cs.LG] UPDATED)\nAbstract: Classification has been the focal point of research on adversarial attacks, but only a few works investigate methods suited to denser prediction tasks, such as semantic segmentation. The methods proposed in these works do not accurately solve the adversarial segmentation problem and, therefore, overestimate the size of the perturbations required to fool models. Here, we propose a white-box attack for these models based on a proximal splitting to produce adversarial perturbations with much smaller $\\ell_\\infty$ norms. Our attack can handle large numbers of constraints within a nonconvex minimization framework via an Augmented Lagrangian approach, coupled with adaptive constraint scaling and masking strategies. We demonstrate that our attack significantly outperforms previously proposed ones, as well as classification attacks that we adapted for segmentation, providing a first comprehensive benchmark for this dense task.",
    "path": "papers/22/06/2206.07179.json",
    "total_tokens": 826,
    "translated_title": "近端分裂对抗攻击用于语义分割",
    "translated_abstract": "对抗攻击的研究都集中在分类上，但只有少数工作研究了适用于更密集预测任务（如语义分割）的方法。这些工作中提出的方法不能准确地解决对抗性分割问题，因此高估了欺骗模型所需的污染大小。在本文中，作者提出了基于近端分裂的白盒攻击方法，以产生具有更小$\\ell_\\infty$范数的对抗性扰动。作者的攻击可以通过扩展拉格朗日方法处理大量的约束，同时采用自适应约束缩放和屏蔽策略。作者证明了他们的攻击显著优于以前提出的方法以及分类攻击（作者为了分割而改进），为这项密集任务提供了第一个全面的基准测试。",
    "tldr": "本文提出了适用于语义分割的近端分裂对抗攻击方法，采用扩展拉格朗日方法处理大量约束且产生更小的对抗性扰动。",
    "en_tdlr": "This paper proposes a proximal splitting adversarial attack method for semantic segmentation, which can handle large number of constraints via augmented Lagrangian approach and produce smaller perturbations. The proposed attack outperforms previous methods and provides a comprehensive benchmark for this task."
}