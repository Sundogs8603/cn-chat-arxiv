{
    "title": "From Attribution Maps to Human-Understandable Explanations through Concept Relevance Propagation. (arXiv:2206.03208v2 [cs.LG] UPDATED)",
    "abstract": "The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's reasoning to the user. In this work we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the \"where\" and \"what\" questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model's representation and reasoning through concept atlases, ",
    "link": "http://arxiv.org/abs/2206.03208",
    "context": "Title: From Attribution Maps to Human-Understandable Explanations through Concept Relevance Propagation. (arXiv:2206.03208v2 [cs.LG] UPDATED)\nAbstract: The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's reasoning to the user. In this work we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the \"where\" and \"what\" questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model's representation and reasoning through concept atlases, ",
    "path": "papers/22/06/2206.03208.json",
    "total_tokens": 909,
    "translated_title": "从归因图到可理解的人类解释：通过概念相关传播",
    "translated_abstract": "可解释的人工智能（XAI）领域旨在使当今强大但不透明的深度学习模型变得透明。而局部的XAI方法通过归因图解释个别预测，从而确定重要特征出现的位置（但不提供有关它们代表什么的信息），而全局解释技术则可视化模型通常学习编码的概念。因此，这两种方法只提供了部分洞察力，并将解释模型的负担留给用户。在这项工作中，我们介绍了概念相关传播（CRP）方法，它结合了局部和全局的观点，从而能够回答个别预测的“何地”和“何物”的问题。我们展示了我们方法在各种设置中的能力，展示了CRP如何提供更具人类解释性的解释，并通过概念图谱深入了解模型的表示和推理能力。",
    "tldr": "本论文介绍了一种概念相关传播（CRP）方法，将局部和全局观点结合起来，为个别预测提供了“何地”和“何物”两个问题的解答。该方法提供了更人类可解释的解释，并通过概念图谱深入了解模型的表示和推理能力。"
}