{
    "title": "Resource-Efficient Separation Transformer. (arXiv:2206.09507v2 [eess.AS] UPDATED)",
    "abstract": "Transformers have recently achieved state-of-the-art performance in speech separation. These models, however, are computationally demanding and require a lot of learnable parameters. This paper explores Transformer-based speech separation with a reduced computational cost. Our main contribution is the development of the Resource-Efficient Separation Transformer (RE-SepFormer), a self-attention-based architecture that reduces the computational burden in two ways. First, it uses non-overlapping blocks in the latent space. Second, it operates on compact latent summaries calculated from each chunk. The RE-SepFormer reaches a competitive performance on the popular WSJ0-2Mix and WHAM! datasets in both causal and non-causal settings. Remarkably, it scales significantly better than the previous Transformer-based architectures in terms of memory and inference time, making it more suitable for processing long mixtures.",
    "link": "http://arxiv.org/abs/2206.09507",
    "context": "Title: Resource-Efficient Separation Transformer. (arXiv:2206.09507v2 [eess.AS] UPDATED)\nAbstract: Transformers have recently achieved state-of-the-art performance in speech separation. These models, however, are computationally demanding and require a lot of learnable parameters. This paper explores Transformer-based speech separation with a reduced computational cost. Our main contribution is the development of the Resource-Efficient Separation Transformer (RE-SepFormer), a self-attention-based architecture that reduces the computational burden in two ways. First, it uses non-overlapping blocks in the latent space. Second, it operates on compact latent summaries calculated from each chunk. The RE-SepFormer reaches a competitive performance on the popular WSJ0-2Mix and WHAM! datasets in both causal and non-causal settings. Remarkably, it scales significantly better than the previous Transformer-based architectures in terms of memory and inference time, making it more suitable for processing long mixtures.",
    "path": "papers/22/06/2206.09507.json",
    "total_tokens": 876,
    "translated_title": "资源高效的分离Transformer",
    "translated_abstract": "最近Transformer在语音分离中取得了最先进的性能。然而，这些模型计算成本高且需要大量的可学习参数。本文探索了一种计算成本较低的基于Transformer的语音分离方法。我们的主要贡献是开发了一种资源高效的分离Transformer（RE-SepFormer），它是一种基于自注意力的架构，通过两种方式减轻计算负担。首先，它在潜空间中使用非重叠的块。其次，它对每个块计算的紧凑潜变量摘要进行操作。RE-SepFormer在流行的WSJ0-2Mix和WHAM！数据集上的因果和非因果设置下均取得了竞争性能。值得注意的是，它在内存和推断时间方面比先前基于Transformer的架构有显著的扩展性，更适用于处理长的混合音频。",
    "tldr": "本文开发出了一种资源高效的分离Transformer（RE-SepFormer），通过非重叠的潜空间块和紧凑的潜变量摘要操作，降低了计算负担。在语音分离任务中取得了竞争性能，并具有较好的内存和推断时间扩展性。",
    "en_tdlr": "This paper presents a Resource-Efficient Separation Transformer (RE-SepFormer) for speech separation with reduced computational cost. It achieves competitive performance in speech separation by using non-overlapping blocks in the latent space and operating on compact latent summaries. It also has better scalability in terms of memory and inference time compared to previous Transformer-based architectures."
}