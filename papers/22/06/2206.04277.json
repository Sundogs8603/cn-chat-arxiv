{
    "title": "On Hypothesis Transfer Learning of Functional Linear Models",
    "abstract": "arXiv:2206.04277v4 Announce Type: replace-cross  Abstract: We study the transfer learning (TL) for the functional linear regression (FLR) under the Reproducing Kernel Hilbert Space (RKHS) framework, observing the TL techniques in existing high-dimensional linear regression is not compatible with the truncation-based FLR methods as functional data are intrinsically infinite-dimensional and generated by smooth underlying processes. We measure the similarity across tasks using RKHS distance, allowing the type of information being transferred tied to the properties of the imposed RKHS. Building on the hypothesis offset transfer learning paradigm, two algorithms are proposed: one conducts the transfer when positive sources are known, while the other leverages aggregation techniques to achieve robust transfer without prior information about the sources. We establish lower bounds for this learning problem and show the proposed algorithms enjoy a matching asymptotic upper bound. These analyses",
    "link": "https://arxiv.org/abs/2206.04277",
    "context": "Title: On Hypothesis Transfer Learning of Functional Linear Models\nAbstract: arXiv:2206.04277v4 Announce Type: replace-cross  Abstract: We study the transfer learning (TL) for the functional linear regression (FLR) under the Reproducing Kernel Hilbert Space (RKHS) framework, observing the TL techniques in existing high-dimensional linear regression is not compatible with the truncation-based FLR methods as functional data are intrinsically infinite-dimensional and generated by smooth underlying processes. We measure the similarity across tasks using RKHS distance, allowing the type of information being transferred tied to the properties of the imposed RKHS. Building on the hypothesis offset transfer learning paradigm, two algorithms are proposed: one conducts the transfer when positive sources are known, while the other leverages aggregation techniques to achieve robust transfer without prior information about the sources. We establish lower bounds for this learning problem and show the proposed algorithms enjoy a matching asymptotic upper bound. These analyses",
    "path": "papers/22/06/2206.04277.json",
    "total_tokens": 861,
    "translated_title": "关于函数线性模型假设迁移学习的研究",
    "translated_abstract": "我们研究了在再生核希尔伯特空间（RKHS）框架下的函数线性回归（FLR）的迁移学习（TL），观察到现有高维线性回归中的TL技术与基于截断的FLR方法不兼容，因为函数数据在本质上是无限维的，并由平滑的基础过程生成。我们使用RKHS距离来衡量任务之间的相似性，允许传输的信息类型与所施加的RKHS的属性相关联。基于假设偏移迁移学习范式，提出了两种算法：一种在已知正源时进行传输，另一种利用聚合技术实现无需先验信息的稳健传输。我们为这个学习问题建立了下界，并展示了所提出的算法享有匹配的渐近上界。",
    "tldr": "该研究在函数线性回归下探讨了迁移学习，提出了使用RKHS距离衡量任务相似性，并提出了两种算法来处理迁移，一种需要已知正源，另一种利用聚合技术实现无源信息的稳健传输。同时建立了学习问题的下界，并证明了算法的上界。",
    "en_tdlr": "The study examines transfer learning for functional linear regression under the RKHS framework, introducing two algorithms that leverage RKHS distance and aggregation techniques to handle transfer without prior information, with established lower bounds and matching asymptotic upper bound."
}