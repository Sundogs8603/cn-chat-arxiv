{
    "title": "StructCoder: Structure-Aware Transformer for Code Generation. (arXiv:2206.05239v2 [cs.LG] UPDATED)",
    "abstract": "There has been a recent surge of interest in automating software engineering tasks using deep learning. This paper addresses the problem of code generation where the goal is to generate target code given source code in a different language or a natural language description. Most of the state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model where both the encoder and decoder are explicitly trained to recognize the syntax and data flow in the source and target codes, respectively. We not only make the encoder structure-aware by leveraging the source code's syntax tree and data flow graph, but we also support the decoder in preserving the syntax and data flow of the target code by introducing two novel auxiliary tasks: AST (Abstract Syntax",
    "link": "http://arxiv.org/abs/2206.05239",
    "context": "Title: StructCoder: Structure-Aware Transformer for Code Generation. (arXiv:2206.05239v2 [cs.LG] UPDATED)\nAbstract: There has been a recent surge of interest in automating software engineering tasks using deep learning. This paper addresses the problem of code generation where the goal is to generate target code given source code in a different language or a natural language description. Most of the state-of-the-art deep learning models for code generation use training strategies primarily designed for natural language. However, understanding and generating code requires a more rigorous comprehension of the code syntax and semantics. With this motivation, we develop an encoder-decoder Transformer model where both the encoder and decoder are explicitly trained to recognize the syntax and data flow in the source and target codes, respectively. We not only make the encoder structure-aware by leveraging the source code's syntax tree and data flow graph, but we also support the decoder in preserving the syntax and data flow of the target code by introducing two novel auxiliary tasks: AST (Abstract Syntax",
    "path": "papers/22/06/2206.05239.json",
    "total_tokens": 812,
    "translated_title": "StructCoder: 面向代码生成的结构感知Transformer模型",
    "translated_abstract": "近年来，使用深度学习来自动化软件工程任务的兴趣日益增长。本文解决了代码生成问题，目标在于在给定不同语言或自然语言描述的源代码的情况下生成目标代码。针对代码语法和语义的严格理解和生成需要一种更为严谨的训练策略。出于这个动机，我们开发了一个编码器-解码器Transformer模型，在此模型中，编码器和解码器都明确地受过训练，以识别源代码和目标代码的语法和数据流。我们不仅通过利用源代码的语法树和数据流图使编码器结构感知，还通过引入两个新的辅助任务——AST（抽象语法）和DFG（数据流图）帮助解码器保留目标代码的语法和数据流。",
    "tldr": "本文提出了一个结构感知的Transformer模型，通过引入AST和DFG辅助任务，旨在解决现有代码生成模型在面对代码语法和语义时的训练不足问题。",
    "en_tdlr": "This paper presents a structure-aware Transformer model, which aims to address the inadequate training of existing code generation models when faced with code syntax and semantics, by introducing auxiliary tasks of Abstract Syntax Tree (AST) and Data Flow Graph (DFG)."
}