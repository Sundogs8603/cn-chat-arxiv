{
    "title": "OmniMAE: Single Model Masked Pretraining on Images and Videos. (arXiv:2206.08356v2 [cs.CV] UPDATED)",
    "abstract": "Transformer-based architectures have become competitive across a variety of visual domains, most notably images and videos. While prior work studies these modalities in isolation, having a common architecture suggests that one can train a single unified model for multiple visual modalities. Prior attempts at unified modeling typically use architectures tailored for vision tasks, or obtain worse performance compared to single modality models. In this work, we show that masked autoencoding can be used to train a simple Vision Transformer on images and videos, without requiring any labeled data. This single model learns visual representations that are comparable to or better than single-modality representations on both image and video benchmarks, while using a much simpler architecture. Furthermore, this model can be learned by dropping 90% of the image and 95% of the video patches, enabling extremely fast training of huge model architectures. In particular, we show that our single ViT-Hu",
    "link": "http://arxiv.org/abs/2206.08356",
    "context": "Title: OmniMAE: Single Model Masked Pretraining on Images and Videos. (arXiv:2206.08356v2 [cs.CV] UPDATED)\nAbstract: Transformer-based architectures have become competitive across a variety of visual domains, most notably images and videos. While prior work studies these modalities in isolation, having a common architecture suggests that one can train a single unified model for multiple visual modalities. Prior attempts at unified modeling typically use architectures tailored for vision tasks, or obtain worse performance compared to single modality models. In this work, we show that masked autoencoding can be used to train a simple Vision Transformer on images and videos, without requiring any labeled data. This single model learns visual representations that are comparable to or better than single-modality representations on both image and video benchmarks, while using a much simpler architecture. Furthermore, this model can be learned by dropping 90% of the image and 95% of the video patches, enabling extremely fast training of huge model architectures. In particular, we show that our single ViT-Hu",
    "path": "papers/22/06/2206.08356.json",
    "total_tokens": 930,
    "translated_title": "OmniMAE: 图片和视频上的单一模型遮蔽预训练",
    "translated_abstract": "基于Transformer的体系结构在各种视觉领域中已变得竞争力十足，其中最著名的是图像和视频。之前的工作通常是研究这些模态之间的隔离，但是具有相同的架构意味着可以为多个视觉模态训练一个单一的统一模型。之前的统一建模尝试通常使用专门为视觉任务量身定制的架构，或与单模态模型相比表现更差。在这项工作中，我们展示了遮蔽自编码可以用于在图像和视频上训练一个简单的Vision Transformer模型，而无需任何标记数据。这个单一的模型学习的视觉表示与单模态表示在图像和视频基准测试上相当或更好，同时使用更简单的架构。此外，通过删除90％的图像和95％的视频补丁，可以学习该模型，从而实现极快的大型模型架构训练。特别地，我们展示了我们的单一ViT-Hu",
    "tldr": "该论文提出了一种基于遮蔽自编码的方法，可以在图像和视频上训练一个简单的单一Vision Transformer模型，而不需要标记数据，该模型的视觉表示可与单模态表示在基准测试上相当或更好，并且使用更简单的架构。",
    "en_tdlr": "The paper proposes a method based on masked autoencoding, which can train a simple single Vision Transformer model on images and videos without requiring labeled data. The visual representation learned by this model is comparable to or better than single-modality representations on benchmark tests, while using a simpler architecture."
}