{
    "title": "Mildly Conservative Q-Learning for Offline Reinforcement Learning",
    "abstract": "arXiv:2206.04745v3 Announce Type: replace-cross  Abstract: Offline reinforcement learning (RL) defines the task of learning from a static logged dataset without continually interacting with the environment. The distribution shift between the learned policy and the behavior policy makes it necessary for the value function to stay conservative such that out-of-distribution (OOD) actions will not be severely overestimated. However, existing approaches, penalizing the unseen actions or regularizing with the behavior policy, are too pessimistic, which suppresses the generalization of the value function and hinders the performance improvement. This paper explores mild but enough conservatism for offline learning while not harming generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD actions are actively trained by assigning them proper pseudo Q values. We theoretically show that MCQ induces a policy that behaves at least as well as the behavior policy and no erroneous ov",
    "link": "https://arxiv.org/abs/2206.04745",
    "context": "Title: Mildly Conservative Q-Learning for Offline Reinforcement Learning\nAbstract: arXiv:2206.04745v3 Announce Type: replace-cross  Abstract: Offline reinforcement learning (RL) defines the task of learning from a static logged dataset without continually interacting with the environment. The distribution shift between the learned policy and the behavior policy makes it necessary for the value function to stay conservative such that out-of-distribution (OOD) actions will not be severely overestimated. However, existing approaches, penalizing the unseen actions or regularizing with the behavior policy, are too pessimistic, which suppresses the generalization of the value function and hinders the performance improvement. This paper explores mild but enough conservatism for offline learning while not harming generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD actions are actively trained by assigning them proper pseudo Q values. We theoretically show that MCQ induces a policy that behaves at least as well as the behavior policy and no erroneous ov",
    "path": "papers/22/06/2206.04745.json",
    "total_tokens": 908,
    "translated_title": "离线强化学习中的温和保守Q学习",
    "translated_abstract": "离线强化学习定义了从静态记录的数据集中学习而无需持续与环境进行交互的任务。学习策略与行为策略之间的分布转移使得价值函数保持保守成为必要，以确保超出分布（OOD）的动作不会被严重高估。然而，现有的方法，如对未见动作进行惩罚或与行为策略进行正则化，都过于悲观，抑制了价值函数的泛化能力，并阻碍了性能的提高。本文探讨了离线学习中的温和但足够保守，同时不损害泛化能力。我们提出了温和保守Q学习（MCQ），通过为OOD动作分配适当的伪Q值来积极训练它们。我们理论上证明了MCQ会产生一个至少与行为策略一样好的策略，并且不会发生错误的过估计。",
    "tldr": "本文提出了一种离线强化学习中的温和保守Q学习（MCQ）方法，通过为OOD动作分配适当的伪Q值来训练，从而在不损害泛化能力的情况下实现价值函数的保守性，避免过度高估超出分布的动作。",
    "en_tdlr": "This paper introduces Mildly Conservative Q-learning (MCQ) for offline reinforcement learning, where out-of-distribution (OOD) actions are actively trained by assigning them proper pseudo Q values, achieving conservatism in the value function without harming generalization and avoiding severe overestimation of OOD actions."
}