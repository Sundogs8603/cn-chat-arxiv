{
    "title": "Analyzing Explainer Robustness via Lipschitzness of Prediction Functions. (arXiv:2206.12481v2 [cs.LG] UPDATED)",
    "abstract": "Machine learning methods have significantly improved in their predictive capabilities, but at the same time they are becoming more complex and less transparent. As a result, explainers are often relied on to provide interpretability to these black-box prediction models. As crucial diagnostics tools, it is important that these explainers themselves are robust. In this paper we focus on one particular aspect of robustness, namely that an explainer should give similar explanations for similar data inputs. We formalize this notion by introducing and defining explainer astuteness, analogous to astuteness of prediction functions. Our formalism allows us to connect explainer robustness to the predictor's probabilistic Lipschitzness, which captures the probability of local smoothness of a function. We provide lower bound guarantees on the astuteness of a variety of explainers (e.g., SHAP, RISE, CXPlain) given the Lipschitzness of the prediction function. These theoretical results imply that lo",
    "link": "http://arxiv.org/abs/2206.12481",
    "context": "Title: Analyzing Explainer Robustness via Lipschitzness of Prediction Functions. (arXiv:2206.12481v2 [cs.LG] UPDATED)\nAbstract: Machine learning methods have significantly improved in their predictive capabilities, but at the same time they are becoming more complex and less transparent. As a result, explainers are often relied on to provide interpretability to these black-box prediction models. As crucial diagnostics tools, it is important that these explainers themselves are robust. In this paper we focus on one particular aspect of robustness, namely that an explainer should give similar explanations for similar data inputs. We formalize this notion by introducing and defining explainer astuteness, analogous to astuteness of prediction functions. Our formalism allows us to connect explainer robustness to the predictor's probabilistic Lipschitzness, which captures the probability of local smoothness of a function. We provide lower bound guarantees on the astuteness of a variety of explainers (e.g., SHAP, RISE, CXPlain) given the Lipschitzness of the prediction function. These theoretical results imply that lo",
    "path": "papers/22/06/2206.12481.json",
    "total_tokens": 968,
    "translated_title": "分析通过预测函数的Lipschitz率来评估解释器的稳健性",
    "translated_abstract": "机器学习方法在预测能力方面得到了显著提高，但与此同时，它们变得越来越复杂和不透明。因此，解释器总是被依赖于为这些黑盒预测模型提供可解释性。作为关键的诊断工具，重要的是这些解释器本身是稳健的。本文重点关注稳健性的一个特定方面，即解释器在相似的数据输入上应该给出类似的解释。我们通过引入和定义解释器敏锐性的概念来形式化这个观念，类似于预测函数的敏锐性。我们的形式化方法使我们能够将解释器的稳健性与预测器的概率Lipschitz率相联系，该率捕捉了函数局部平滑性的概率。我们根据预测函数的Lipschitz率提供对各种解释器（如SHAP，RISE，CXPlain）的敏锐性的下界保证。这些理论结果暗示了对于具有局部光滑性预测函数的解释器敏锐性。",
    "tldr": "本文研究了通过评估预测函数的Lipschitz率来分析解释器的稳健性。通过引入解释器敏锐性的概念并与预测器的概率Lipschitz率相联系，我们提供了解释器敏锐性的下界保证。",
    "en_tdlr": "This paper analyzes the robustness of explainers by evaluating the Lipschitzness of prediction functions. By introducing the notion of explainer astuteness and connecting it to the probabilistic Lipschitzness of the predictor, lower bound guarantees on the astuteness of explainers are provided."
}