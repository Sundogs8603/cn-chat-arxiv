{
    "title": "Social Bias Meets Data Bias: The Impacts of Labeling and Measurement Errors on Fairness Criteria. (arXiv:2206.00137v3 [cs.LG] UPDATED)",
    "abstract": "Although many fairness criteria have been proposed to ensure that machine learning algorithms do not exhibit or amplify our existing social biases, these algorithms are trained on datasets that can themselves be statistically biased. In this paper, we investigate the robustness of a number of existing (demographic) fairness criteria when the algorithm is trained on biased data. We consider two forms of dataset bias: errors by prior decision makers in the labeling process, and errors in measurement of the features of disadvantaged individuals. We analytically show that some constraints (such as Demographic Parity) can remain robust when facing certain statistical biases, while others (such as Equalized Odds) are significantly violated if trained on biased data. We also analyze the sensitivity of these criteria and the decision maker's utility to biases. We provide numerical experiments based on three real-world datasets (the FICO, Adult, and German credit score datasets) supporting our ",
    "link": "http://arxiv.org/abs/2206.00137",
    "context": "Title: Social Bias Meets Data Bias: The Impacts of Labeling and Measurement Errors on Fairness Criteria. (arXiv:2206.00137v3 [cs.LG] UPDATED)\nAbstract: Although many fairness criteria have been proposed to ensure that machine learning algorithms do not exhibit or amplify our existing social biases, these algorithms are trained on datasets that can themselves be statistically biased. In this paper, we investigate the robustness of a number of existing (demographic) fairness criteria when the algorithm is trained on biased data. We consider two forms of dataset bias: errors by prior decision makers in the labeling process, and errors in measurement of the features of disadvantaged individuals. We analytically show that some constraints (such as Demographic Parity) can remain robust when facing certain statistical biases, while others (such as Equalized Odds) are significantly violated if trained on biased data. We also analyze the sensitivity of these criteria and the decision maker's utility to biases. We provide numerical experiments based on three real-world datasets (the FICO, Adult, and German credit score datasets) supporting our ",
    "path": "papers/22/06/2206.00137.json",
    "total_tokens": 991,
    "translated_title": "社会偏见遇到数据偏见: 标注和测量误差对公平标准的影响",
    "translated_abstract": "尽管已经提出了许多公平标准来确保机器学习算法不会表现出或放大我们现有的社会偏见，但这些算法是在本身可能存在统计偏差的数据集上训练的。在本文中，我们研究了在算法训练在偏见数据集上时一些现有(人口)公平标准的鲁棒性。我们考虑了两种形式的数据集偏差：标记过程中的先前决策制定者的错误和对劣势个体特征的测量误差。我们在理论上证明了一些约束(例如人口均等性)在面对某些统计偏差时可以保持稳健，而另一些(例如平等机会)则会在训练偏见数据集时被显著违反。我们还分析了这些标准和决策制定者效用对偏见的敏感性。我们基于三个真实数据集(FICO、成人和德国信用评分数据集)提供了数值实验，支持我们的结论。",
    "tldr": "本文研究了机器学习算法在训练数据集存在偏见时现有公平标准的鲁棒性，探究了标记和测量误差对其影响。研究发现，一些约束可以在面对某些统计偏差时保持稳健，而另一些则会在训练偏见数据集时被显著违反。",
    "en_tdlr": "This paper investigates the robustness of existing fairness criteria when machine learning algorithms are trained on biased datasets due to errors in labeling or measurement. The study shows that some constraints can remain robust while others are significantly violated when facing certain statistical biases. Numerical experiments on real-world datasets support the findings."
}