{
    "title": "Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance. (arXiv:2206.03787v3 [cs.LG] UPDATED)",
    "abstract": "Many Deep Reinforcement Learning (D-RL) algorithms rely on simple forms of exploration such as the additive action noise often used in continuous control domains. Typically, the scaling factor of this action noise is chosen as a hyper-parameter and is kept constant during training. In this paper, we focus on action noise in off-policy deep reinforcement learning for continuous control. We analyze how the learned policy is impacted by the noise type, noise scale, and impact scaling factor reduction schedule. We consider the two most prominent types of action noise, Gaussian and Ornstein-Uhlenbeck noise, and perform a vast experimental campaign by systematically varying the noise type and scale parameter, and by measuring variables of interest like the expected return of the policy and the state-space coverage during exploration. For the latter, we propose a novel state-space coverage measure $\\operatorname{X}_{\\mathcal{U}\\text{rel}}$ that is more robust to estimation artifacts caused by",
    "link": "http://arxiv.org/abs/2206.03787",
    "context": "Title: Action Noise in Off-Policy Deep Reinforcement Learning: Impact on Exploration and Performance. (arXiv:2206.03787v3 [cs.LG] UPDATED)\nAbstract: Many Deep Reinforcement Learning (D-RL) algorithms rely on simple forms of exploration such as the additive action noise often used in continuous control domains. Typically, the scaling factor of this action noise is chosen as a hyper-parameter and is kept constant during training. In this paper, we focus on action noise in off-policy deep reinforcement learning for continuous control. We analyze how the learned policy is impacted by the noise type, noise scale, and impact scaling factor reduction schedule. We consider the two most prominent types of action noise, Gaussian and Ornstein-Uhlenbeck noise, and perform a vast experimental campaign by systematically varying the noise type and scale parameter, and by measuring variables of interest like the expected return of the policy and the state-space coverage during exploration. For the latter, we propose a novel state-space coverage measure $\\operatorname{X}_{\\mathcal{U}\\text{rel}}$ that is more robust to estimation artifacts caused by",
    "path": "papers/22/06/2206.03787.json",
    "total_tokens": 913,
    "translated_title": "强化学习中动作噪声对探索和性能的影响",
    "translated_abstract": "许多深度强化学习算法依赖于简单的探索形式，如连续控制领域常用的加性动作噪声。通常，在培训期间保持动作噪声的比例不变。本文侧重于连续控制的离线深度强化学习中的动作噪声。我们分析了噪声类型、噪声比例和减少规模因子的影响。我们考虑了两种最常见的动作噪声类型，高斯噪声和 Ornstein-Uhlenbeck 噪声，并通过系统地改变噪声类型和比例参数进行了大量实验研究，并通过测量策略的预期回报和探索期间的状态空间覆盖等有趣的变量来评估结果。对于后者，我们提出了一种新的状态空间覆盖度量X_𝒰rel，该方法对估计引起的估计误差具有更强的鲁棒性。",
    "tldr": "本文研究了动作噪声类型、噪声比例和减少规模因子对深度强化学习离线学习中策略性能和探索效果的影响，提出了一种更具鲁棒性的状态空间覆盖度量。",
    "en_tdlr": "This paper studies the impact of action noise type, noise scale, and scaling factor reduction schedule on the performance and exploration in off-policy deep reinforcement learning for continuous control, and proposes a more robust state-space coverage measure."
}