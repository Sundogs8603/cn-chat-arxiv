{
    "title": "Tutel: Adaptive Mixture-of-Experts at Scale. (arXiv:2206.03382v2 [cs.DC] UPDATED)",
    "abstract": "Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep learning models to trillion-plus parameters with fixed computational cost. The algorithmic performance of MoE relies on its token routing mechanism that forwards each input token to the right sub-models or experts. While token routing dynamically determines the amount of expert workload at runtime, existing systems suffer inefficient computation due to their static execution, namely static parallelism and pipelining, which does not adapt to the dynamic workload. We present Flex, a highly scalable stack design and implementation for MoE with dynamically adaptive parallelism and pipelining. Flex designs an identical layout for distributing MoE model parameters and input data, which can be leveraged by all possible parallelism or pipelining methods without any mathematical inequivalence or tensor migration overhead. This enables adaptive parallelism/pipelining optimization at zero cost during runtime. Based on t",
    "link": "http://arxiv.org/abs/2206.03382",
    "context": "Title: Tutel: Adaptive Mixture-of-Experts at Scale. (arXiv:2206.03382v2 [cs.DC] UPDATED)\nAbstract: Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep learning models to trillion-plus parameters with fixed computational cost. The algorithmic performance of MoE relies on its token routing mechanism that forwards each input token to the right sub-models or experts. While token routing dynamically determines the amount of expert workload at runtime, existing systems suffer inefficient computation due to their static execution, namely static parallelism and pipelining, which does not adapt to the dynamic workload. We present Flex, a highly scalable stack design and implementation for MoE with dynamically adaptive parallelism and pipelining. Flex designs an identical layout for distributing MoE model parameters and input data, which can be leveraged by all possible parallelism or pipelining methods without any mathematical inequivalence or tensor migration overhead. This enables adaptive parallelism/pipelining optimization at zero cost during runtime. Based on t",
    "path": "papers/22/06/2206.03382.json",
    "total_tokens": 875,
    "translated_abstract": "单纯门控的混合专家模型已经被广泛应用于扩展深度学习模型至万亿级参数与固定计算成本。MoE的算法性能取决于其令牌路由机制，该机制将每个输入令牌转发到正确的子模型或专家。虽然令牌路由在运行时动态确定专家的工作量，但现有系统由于其静态执行方式 - 即静态并行处理和流水线处理 - 而导致计算效率低下，无法适应动态工作量。本文提出了Flex，一种高度可扩展的MoE堆栈设计和实现，具有动态自适应的并行处理和流水线处理。Flex设计了一个相同的布局来分配MoE模型参数和输入数据，可以利用所有可能的并行处理或流水线处理方法，而无需进行任何代数不等式或张量迁移开销。这使得在运行时可以零成本进行自适应并行处理/流水线处理优化。",
    "tldr": "本文提出了一种高度可扩展的MoE堆栈设计和实现Flex，其中采用了动态自适应的并行处理和流水线处理，以提高计算效率。",
    "en_tdlr": "This paper presents Flex, a highly scalable stack design and implementation for the sparsely-gated mixture-of-experts (MoE) model with dynamically adaptive parallelism and pipelining, which aims to improve computational efficiency for deep learning models at scale."
}