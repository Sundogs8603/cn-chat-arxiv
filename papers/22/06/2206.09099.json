{
    "title": "The Consistency of Adversarial Training for Binary Classification. (arXiv:2206.09099v2 [cs.LG] UPDATED)",
    "abstract": "Robustness to adversarial perturbations is of paramount concern in modern machine learning. One of the state-of-the-art methods for training robust classifiers is adversarial training, which involves minimizing a supremum-based surrogate risk. The statistical consistency of surrogate risks is well understood in the context of standard machine learning, but not in the adversarial setting. In this paper, we characterize which supremum-based surrogates are consistent for distributions absolutely continuous with respect to Lebesgue measure in binary classification. Furthermore, we obtain quantitative bounds relating adversarial surrogate risks to the adversarial classification risk. Lastly, we discuss implications for the $\\cH$-consistency of adversarial training.",
    "link": "http://arxiv.org/abs/2206.09099",
    "context": "Title: The Consistency of Adversarial Training for Binary Classification. (arXiv:2206.09099v2 [cs.LG] UPDATED)\nAbstract: Robustness to adversarial perturbations is of paramount concern in modern machine learning. One of the state-of-the-art methods for training robust classifiers is adversarial training, which involves minimizing a supremum-based surrogate risk. The statistical consistency of surrogate risks is well understood in the context of standard machine learning, but not in the adversarial setting. In this paper, we characterize which supremum-based surrogates are consistent for distributions absolutely continuous with respect to Lebesgue measure in binary classification. Furthermore, we obtain quantitative bounds relating adversarial surrogate risks to the adversarial classification risk. Lastly, we discuss implications for the $\\cH$-consistency of adversarial training.",
    "path": "papers/22/06/2206.09099.json",
    "total_tokens": 845,
    "translated_title": "对于二元分类问题，对抗性训练的一致性研究",
    "translated_abstract": "在现代机器学习中，对抗扰动的鲁棒性是至关重要的。训练强健分类器的最先进方法之一是对抗性训练，其中涉及最小化基于上确界的代理风险。代理风险的统计一致性在标准机器学习的背景下已经被很好地理解，但在对抗性背景下仍有待进一步研究。在本文中，我们刻画了绝对连续于勒贝格度量的分布中哪些基于上确界的代理风险是一致的，同时我们获得了有关对抗性代理风险与对抗性分类风险的定量界限。最后，我们讨论了对抗性训练的 $\\cH$- 一致性的影响。",
    "tldr": "研究了对于绝对连续于勒贝格度量的分布，哪些基于上确界的代理风险是一致的；定量计算了对抗性代理风险与对抗性分类风险的关系；探讨了对抗性训练的 $\\cH$- 一致性的影响。",
    "en_tdlr": "This paper characterizes which supremum-based surrogates are consistent for distributions absolutely continuous with respect to Lebesgue measure in binary classification, obtains quantitative bounds relating adversarial surrogate risks to the adversarial classification risk, and discusses implications for the $\\cH$-consistency of adversarial training."
}