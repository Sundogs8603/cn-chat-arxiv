{
    "title": "Superiority of GNN over NN in generalizing bandlimited functions. (arXiv:2206.05904v6 [cs.LG] UPDATED)",
    "abstract": "Graph Neural Network (GNN) with its ability to integrate graph information has been widely used for data analyses. However, the expressive power of GNN has only been studied for graph-level tasks but not for node-level tasks, such as node classification, where one tries to interpolate missing nodal labels from the observed ones. In this paper, we study the expressive power of GNN for the said classification task, which is in essence a function interpolation problem. Explicitly, we derive the number of weights and layers needed for a GNN to interpolate a band-limited function in $\\mathbb{R}^d$. Our result shows that, the number of weights needed to $\\epsilon$-approximate a bandlimited function using the GNN architecture is much fewer than the best known one using a fully connected neural network (NN) - in particular, one only needs $O((\\log \\epsilon^{-1})^{d})$ weights using a GNN trained by $O((\\log \\epsilon^{-1})^{d})$ samples to $\\epsilon$-approximate a discretized bandlimited signal",
    "link": "http://arxiv.org/abs/2206.05904",
    "context": "Title: Superiority of GNN over NN in generalizing bandlimited functions. (arXiv:2206.05904v6 [cs.LG] UPDATED)\nAbstract: Graph Neural Network (GNN) with its ability to integrate graph information has been widely used for data analyses. However, the expressive power of GNN has only been studied for graph-level tasks but not for node-level tasks, such as node classification, where one tries to interpolate missing nodal labels from the observed ones. In this paper, we study the expressive power of GNN for the said classification task, which is in essence a function interpolation problem. Explicitly, we derive the number of weights and layers needed for a GNN to interpolate a band-limited function in $\\mathbb{R}^d$. Our result shows that, the number of weights needed to $\\epsilon$-approximate a bandlimited function using the GNN architecture is much fewer than the best known one using a fully connected neural network (NN) - in particular, one only needs $O((\\log \\epsilon^{-1})^{d})$ weights using a GNN trained by $O((\\log \\epsilon^{-1})^{d})$ samples to $\\epsilon$-approximate a discretized bandlimited signal",
    "path": "papers/22/06/2206.05904.json",
    "total_tokens": 925,
    "translated_title": "GNN在推广带限函数方面的优越性比NN更加明显",
    "translated_abstract": "图神经网络（GNN）以其整合图形信息的能力被广泛用于数据分析。然而，GNN的表达能力仅针对图级任务进行了研究，而不是针对节点级任务，例如节点分类，其中试图从观察到的节点标签中插值出缺失的标签信息。本文研究了GNN在所述分类任务中的表达能力，它实质上是一个函数插值问题。具体而言，我们导出了GNN插值$\\mathbb{R}^d$中带限函数所需的权重和层数。我们的结果显示，使用GNN架构以$\\epsilon$-近似离散带限信号仅需要$O((\\log \\epsilon^{-1})^{d})$个权重，这比使用完全连接的神经网络（NN）得到的最佳结果的所需权重少得多 - 特别地，使用使用$O((\\log \\epsilon^{-1})^{d})$个样本来训练GNN以$\\epsilon$-逼近带限函数。",
    "tldr": "本文研究了GNN在节点分类中插值带限函数的表达能力，结果表明，使用GNN结构以相同的精度插值带限函数所需的权重比使用完全连接的神经网络（NN）少得多。",
    "en_tdlr": "This paper studies the expressive power of GNN for interpolating band-limited functions in the node classification task and shows that a GNN architecture requires significantly fewer weights than a fully connected neural network to achieve the same level of precision."
}