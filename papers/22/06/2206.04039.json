{
    "title": "Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v2 [cs.CY] UPDATED)",
    "abstract": "In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diverse tasks performed and uses of the data produced render it difficult to determine when crowdworkers are best thought of as workers (versus human subjects). These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements. We investigate the appropriate designation of ML crowdsourcing studies, focusing our inquiry on natural language processing to expose unique challenges for research oversight. Crucially, under the U.S. Common Rule, these judgments hinge on determinations of aboutness, concerning both whom (or what) ",
    "link": "http://arxiv.org/abs/2206.04039",
    "context": "Title: Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v2 [cs.CY] UPDATED)\nAbstract: In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diverse tasks performed and uses of the data produced render it difficult to determine when crowdworkers are best thought of as workers (versus human subjects). These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements. We investigate the appropriate designation of ML crowdsourcing studies, focusing our inquiry on natural language processing to expose unique challenges for research oversight. Crucially, under the U.S. Common Rule, these judgments hinge on determinations of aboutness, concerning both whom (or what) ",
    "path": "papers/22/06/2206.04039.json",
    "total_tokens": 912,
    "translated_title": "解决机器学习众包工作者的人类受试者身份问题",
    "translated_abstract": "近年来，机器学习(Machine Learning, ML)在构建数据集和解决需要人类交互或判断的研究问题方面，已经严重依赖于众包工作者。由于执行的任务多样化和数据用途的多样性，很难确定何时将众包工作者视为工人(而非人类受试者)。这些困难加剧了政策的冲突，一些机构和研究人员将所有ML众包工作者视为人类受试者，而其他人则认为它们很少构成人类受试者。值得注意的是，包括众包工作的鲜有ML论文提到IRB的监督，引发了违反道德和法规要求的可能性。我们研究了ML众包研究的适当划定，并关注自然语言处理领域暴露出的独特研究监督挑战。至关重要的是，在美国公共规则下，这些判断取决于关于问题的确定，涉及谁(或什么)的问题。",
    "tldr": "机器学习在研究中使用的众包工作者问题引起了对其受试者身份的争议与监管合规性，本文针对该问题进行研究，重点关注了自然语言处理领域中的研究监管挑战。",
    "en_tdlr": "This paper investigates the disputed human subjects status of machine learning crowdworkers, with a focus on natural language processing, to address issues of regulatory compliance and ethical oversight in the use of crowdwork for research purposes."
}