{
    "title": "Algorithmic Foundations of Empirical X-risk Minimization. (arXiv:2206.00439v6 [cs.LG] UPDATED)",
    "abstract": "This manuscript introduces a new optimization framework for machine learning and AI, named {\\bf empirical X-risk minimization (EXM)}. X-risk is a term introduced to represent a family of compositional measures or objectives, in which each data point is compared with a large number of items explicitly or implicitly for defining a risk function. It includes surrogate objectives of many widely used measures and non-decomposable losses, e.g., AUROC, AUPRC, partial AUROC, NDCG, MAP, precision/recall at top $K$ positions, precision at a certain recall level, listwise losses, p-norm push, top push, global contrastive losses, etc. While these non-decomposable objectives and their optimization algorithms have been studied in the literature of machine learning, computer vision, information retrieval, and etc, optimizing these objectives has encountered some unique challenges for deep learning. In this paper, we present recent rigorous efforts for EXM with a focus on its algorithmic foundations a",
    "link": "http://arxiv.org/abs/2206.00439",
    "context": "Title: Algorithmic Foundations of Empirical X-risk Minimization. (arXiv:2206.00439v6 [cs.LG] UPDATED)\nAbstract: This manuscript introduces a new optimization framework for machine learning and AI, named {\\bf empirical X-risk minimization (EXM)}. X-risk is a term introduced to represent a family of compositional measures or objectives, in which each data point is compared with a large number of items explicitly or implicitly for defining a risk function. It includes surrogate objectives of many widely used measures and non-decomposable losses, e.g., AUROC, AUPRC, partial AUROC, NDCG, MAP, precision/recall at top $K$ positions, precision at a certain recall level, listwise losses, p-norm push, top push, global contrastive losses, etc. While these non-decomposable objectives and their optimization algorithms have been studied in the literature of machine learning, computer vision, information retrieval, and etc, optimizing these objectives has encountered some unique challenges for deep learning. In this paper, we present recent rigorous efforts for EXM with a focus on its algorithmic foundations a",
    "path": "papers/22/06/2206.00439.json",
    "total_tokens": 952,
    "translated_title": "算法基础的经验性X风险最小化",
    "translated_abstract": "本文介绍了一种名为\"经验性X风险最小化（EXM）\"的机器学习和人工智能优化框架。X风险是一个用于表示一类组合度量或目标的术语，在其中，将每个数据点与大量的明确或隐含的项目进行比较来定义风险函数。它包括许多广泛使用的代理目标和不可分解的损失函数，例如AUROC、AUPRC、部分AUROC、NDCG、MAP、在前K个位置的精确度/召回率、在特定召回率水平上的精确度、列表损失、p范数推导、顶部推导、全局对比损失等。虽然这些不可分解的目标及其优化算法在机器学习、计算机视觉、信息检索等领域的文献中已经得到了研究，但在深度学习中优化这些目标面临着一些独特的挑战。在本文中，我们重点介绍了EXM的算法基础，并提供了最近的严格工作。",
    "tldr": "本文介绍了一种名为\"经验性X风险最小化（EXM）\"的机器学习和人工智能优化框架，该框架解决了深度学习中优化不可分解目标的困难，并提供了算法基础的详细讨论。",
    "en_tdlr": "This paper introduces a new optimization framework for machine learning and AI called \"empirical X-risk minimization (EXM)\", which addresses the challenges of optimizing non-decomposable objectives in deep learning, providing a detailed discussion on its algorithmic foundations."
}