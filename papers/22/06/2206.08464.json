{
    "title": "PRANC: Pseudo RAndom Networks for Compacting deep models. (arXiv:2206.08464v2 [cs.LG] UPDATED)",
    "abstract": "We demonstrate that a deep model can be reparametrized as a linear combination of several randomly initialized and frozen deep models in the weight space. During training, we seek local minima that reside within the subspace spanned by these random models (i.e., `basis' networks). Our framework, PRANC, enables significant compaction of a deep model. The model can be reconstructed using a single scalar `seed,' employed to generate the pseudo-random `basis' networks, together with the learned linear mixture coefficients.  In practical applications, PRANC addresses the challenge of efficiently storing and communicating deep models, a common bottleneck in several scenarios, including multi-agent learning, continual learners, federated systems, and edge devices, among others. In this study, we employ PRANC to condense image classification models and compress images by compacting their associated implicit neural networks. PRANC outperforms baselines with a large margin on image classificatio",
    "link": "http://arxiv.org/abs/2206.08464",
    "context": "Title: PRANC: Pseudo RAndom Networks for Compacting deep models. (arXiv:2206.08464v2 [cs.LG] UPDATED)\nAbstract: We demonstrate that a deep model can be reparametrized as a linear combination of several randomly initialized and frozen deep models in the weight space. During training, we seek local minima that reside within the subspace spanned by these random models (i.e., `basis' networks). Our framework, PRANC, enables significant compaction of a deep model. The model can be reconstructed using a single scalar `seed,' employed to generate the pseudo-random `basis' networks, together with the learned linear mixture coefficients.  In practical applications, PRANC addresses the challenge of efficiently storing and communicating deep models, a common bottleneck in several scenarios, including multi-agent learning, continual learners, federated systems, and edge devices, among others. In this study, we employ PRANC to condense image classification models and compress images by compacting their associated implicit neural networks. PRANC outperforms baselines with a large margin on image classificatio",
    "path": "papers/22/06/2206.08464.json",
    "total_tokens": 971,
    "translated_title": "PRANC：用于压缩深度模型的伪随机网络",
    "translated_abstract": "我们展示了深度模型可以被重新参数化为在权重空间中几个随机初始化并冻结的深度模型的线性组合。在训练过程中，我们寻找存在于这些随机模型（即“基础”网络）所张成的子空间中的局部最小值。我们的框架PRANC能够显著压缩深度模型。使用一个单一的标量“种子”来生成伪随机的“基础”网络，再结合学习到的线性混合系数，可以重构模型。在实际应用中，PRANC解决了高效存储和传输深度模型的挑战，这在包括多智能体学习、持续学习、联邦系统和边缘设备等多种情况下都是一个常见的瓶颈。在本研究中，我们使用PRANC来压缩图像分类模型并通过压缩其相关的隐式神经网络来压缩图像。PRANC在图像分类任务上表现出比基线方法更好的性能。",
    "tldr": "PRANC是一种用于压缩深度模型的框架，通过将深度模型重新参数化为多个随机初始化的基础网络的线性组合来实现。 PRANC可以显著减小深度模型的大小，并解决了存储和传输深度模型的挑战。在图像分类任务中，PRANC表现出比基线方法更好的性能。",
    "en_tdlr": "PRANC is a framework for compressing deep models by reparametrizing them as a linear combination of randomly initialized \"basis\" networks. It significantly reduces the size of the deep model and addresses the challenges of storing and transmitting deep models. PRANC outperforms baseline methods in image classification tasks."
}