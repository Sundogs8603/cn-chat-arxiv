{
    "title": "Sum-of-Squares Relaxations for Information Theory and Variational Inference. (arXiv:2206.13285v3 [cs.IT] UPDATED)",
    "abstract": "We consider extensions of the Shannon relative entropy, referred to as $f$-divergences.Three classical related computational problems are typically associated with these divergences: (a) estimation from moments, (b) computing normalizing integrals, and (c) variational inference in probabilistic models. These problems are related to one another through convex duality, and for all them, there are many applications throughout data science, and we aim for computationally tractable approximation algorithms that preserve properties of the original problem such as potential convexity or monotonicity. In order to achieve this, we derive a sequence of convex relaxations for computing these divergences from non-centered covariance matrices associated with a given feature vector: starting from the typically non-tractable optimal lower-bound, we consider an additional relaxation based on ``sums-of-squares'', which is is now computable in polynomial time as a semidefinite program. We also provide c",
    "link": "http://arxiv.org/abs/2206.13285",
    "context": "Title: Sum-of-Squares Relaxations for Information Theory and Variational Inference. (arXiv:2206.13285v3 [cs.IT] UPDATED)\nAbstract: We consider extensions of the Shannon relative entropy, referred to as $f$-divergences.Three classical related computational problems are typically associated with these divergences: (a) estimation from moments, (b) computing normalizing integrals, and (c) variational inference in probabilistic models. These problems are related to one another through convex duality, and for all them, there are many applications throughout data science, and we aim for computationally tractable approximation algorithms that preserve properties of the original problem such as potential convexity or monotonicity. In order to achieve this, we derive a sequence of convex relaxations for computing these divergences from non-centered covariance matrices associated with a given feature vector: starting from the typically non-tractable optimal lower-bound, we consider an additional relaxation based on ``sums-of-squares'', which is is now computable in polynomial time as a semidefinite program. We also provide c",
    "path": "papers/22/06/2206.13285.json",
    "total_tokens": 938,
    "translated_title": "信息论和变分推断中的基于平方和松弛的方法",
    "translated_abstract": "我们考虑了香农相对熵的扩展，称为$f$-divergences。这些divergences通常与三个经典的相关计算问题相关联：（a）从矩估计，（b）计算归一化积分，以及（c）概率模型的变分推断。这些问题通过凸对偶性相互关联，对于所有这些问题，都有许多数据科学中的应用，并且我们旨在提出能够保持原始问题特性（如潜在凸性或单调性）的计算上可行的近似算法。为了实现这一目标，我们从与给定特征向量相关的非局部协方差矩阵计算这些divergences的一系列凸松弛开始：从通常不易处理的最优下界开始，我们考虑了一个额外的基于“平方和”的松弛，它现在作为半定规划可以在多项式时间内计算。",
    "tldr": "本论文研究了基于平方和松弛方法在信息论和变分推断中的应用。通过使用这种方法，我们提出了计算$f$-divergences的凸松弛算法，其中涉及到从非局部协方差矩阵计算这些divergences的问题。这些结果对于数据科学中的多个应用具有重要意义。",
    "en_tdlr": "This paper explores the applications of sum-of-squares relaxations in information theory and variational inference. By using this approach, the authors propose computationally tractable convex relaxation algorithms for computing f-divergences, which involve the problem of computing these divergences from non-centered covariance matrices. These results are significant for various applications in data science."
}