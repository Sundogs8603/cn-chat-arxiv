{
    "title": "A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games. (arXiv:2206.05825v4 [cs.LG] UPDATED)",
    "abstract": "This work studies an algorithm, which we call magnetic mirror descent, that is inspired by mirror descent and the non-Euclidean proximal gradient algorithm. Our contribution is demonstrating the virtues of magnetic mirror descent as both an equilibrium solver and as an approach to reinforcement learning in two-player zero-sum games. These virtues include: 1) Being the first quantal response equilibria solver to achieve linear convergence for extensive-form games with first order feedback; 2) Being the first standard reinforcement learning algorithm to achieve empirically competitive results with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning algorithm.",
    "link": "http://arxiv.org/abs/2206.05825",
    "context": "Title: A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games. (arXiv:2206.05825v4 [cs.LG] UPDATED)\nAbstract: This work studies an algorithm, which we call magnetic mirror descent, that is inspired by mirror descent and the non-Euclidean proximal gradient algorithm. Our contribution is demonstrating the virtues of magnetic mirror descent as both an equilibrium solver and as an approach to reinforcement learning in two-player zero-sum games. These virtues include: 1) Being the first quantal response equilibria solver to achieve linear convergence for extensive-form games with first order feedback; 2) Being the first standard reinforcement learning algorithm to achieve empirically competitive results with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning algorithm.",
    "path": "papers/22/06/2206.05825.json",
    "total_tokens": 914,
    "translated_title": "强化学习、量化相应均衡和二人零和博弈的统一方法",
    "translated_abstract": "本文研究了一种算法，我们称之为磁镜面下降，它受到镜面下降和非欧几里德近端梯度算法的启发。我们的贡献在于展示了磁镜面下降作为均衡求解器以及在两人零和博弈中作为强化学习方法的优点。这些优点包括：1) 成为首个对于具有一阶反馈的扩展形式游戏实现线性收敛的量化相应均衡求解器；2) 成为首个在表格式设置中与CFR实现实验性竞争结果的标准强化学习算法；3) 实现了在3x3黑暗六角和幻象井字游戏中成为自我游戏深度强化学习算法的有利性能。",
    "tldr": "本文展示了磁镜面下降算法作为均衡求解器和强化学习方法的优点，包括实现了线性收敛的相应均衡求解器和在表格式设置中实现了与CFR相竞争的标准强化学习算法，以及在“黑暗六角”和“幻象井字”中的自我玩耍深度强化学习算法的良好表现。",
    "en_tdlr": "This work presents the virtues of the magnetic mirror descent algorithm as both an equilibrium solver and a reinforcement learning method, including achieving linear convergence for quantal response equilibria and empirically competitive results with CFR in tabular settings, as well as favorable performance in self-play deep reinforcement learning algorithm in games such as \"Dark Hex\" and \"Phantom Tic-Tac-Toe.\""
}