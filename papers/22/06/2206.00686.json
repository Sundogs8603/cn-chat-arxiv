{
    "title": "Federated Learning in Non-IID Settings Aided by Differentially Private Synthetic Data. (arXiv:2206.00686v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning (FL) is a privacy-promoting framework that enables potentially large number of clients to collaboratively train machine learning models. In a FL system, a server coordinates the collaboration by collecting and aggregating clients' model updates while the clients' data remains local and private. A major challenge in federated learning arises when the local data is heterogeneous -- the setting in which performance of the learned global model may deteriorate significantly compared to the scenario where the data is identically distributed across the clients. In this paper we propose FedDPMS (Federated Differentially Private Means Sharing), an FL algorithm in which clients deploy variational auto-encoders to augment local datasets with data synthesized using differentially private means of latent data representations communicated by a trusted server. Such augmentation ameliorates effects of data heterogeneity across the clients without compromising privacy. Our experiment",
    "link": "http://arxiv.org/abs/2206.00686",
    "context": "Title: Federated Learning in Non-IID Settings Aided by Differentially Private Synthetic Data. (arXiv:2206.00686v2 [cs.LG] UPDATED)\nAbstract: Federated learning (FL) is a privacy-promoting framework that enables potentially large number of clients to collaboratively train machine learning models. In a FL system, a server coordinates the collaboration by collecting and aggregating clients' model updates while the clients' data remains local and private. A major challenge in federated learning arises when the local data is heterogeneous -- the setting in which performance of the learned global model may deteriorate significantly compared to the scenario where the data is identically distributed across the clients. In this paper we propose FedDPMS (Federated Differentially Private Means Sharing), an FL algorithm in which clients deploy variational auto-encoders to augment local datasets with data synthesized using differentially private means of latent data representations communicated by a trusted server. Such augmentation ameliorates effects of data heterogeneity across the clients without compromising privacy. Our experiment",
    "path": "papers/22/06/2206.00686.json",
    "total_tokens": 852,
    "translated_title": "受不同ially Private Synthetic Data支持的非独立同分布设置中的联邦学习。",
    "translated_abstract": "联邦学习(Federated learning, FL)是一种能够让潜在的大量客户端共同训练机器学习模型的隐私保护框架。在FL系统中，服务器通过收集和聚合客户端的模型更新来协调合作，而客户端的数据仍然保持本地和私密。联邦学习中的一个主要挑战是当本地数据异构时，学习到的全局模型的性能可能会明显下降。在本文中，我们提出了FedDPMS(联邦差分私有均值共享)，这是一种FL算法，其中客户端部署变分自动编码器(Variational Auto-encoder, VAE)来利用由信任服务器通信的潜变量数据的差分私有均值来生成合成本地数据集。这种增强可减轻跨客户端的数据异构效应，同时又不会损害隐私。",
    "tldr": "该论文提出了一种名为FedDPMS的算法，该算法利用变分自动编码器来合成本地数据集。这种增强方法可减轻数据异构效应，同时保护隐私。",
    "en_tdlr": "This paper proposes an algorithm called FedDPMS, which uses variational auto-encoders to synthesize local data sets, alleviates the effects of data heterogeneity, and safeguards privacy in FL systems."
}