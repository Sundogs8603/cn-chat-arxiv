{
    "title": "Merak: An Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation Models. (arXiv:2206.04959v3 [cs.LG] UPDATED)",
    "abstract": "Foundation models are becoming the dominant deep learning technologies. Pretraining a foundation model is always time-consumed due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the training process is extremely memory-intensive and communication-intensive. These features make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism and tensor model parallelism, to achieve high training efficiency.  To achieve this goal, some custom software frameworks such as Megatron-LM and DeepSpeed are developed. However, current 3D parallelism frameworks still meet two issues: i) they are not transparent to model developers, which need to manually modify the model to parallelize training. ii) their utilization of computation, GPU memory and network bandwidth are not sufficient. We propose Merak, an automated 3D parallelism deep learning training framework with high resource utilization. Merak automa",
    "link": "http://arxiv.org/abs/2206.04959",
    "context": "Title: Merak: An Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation Models. (arXiv:2206.04959v3 [cs.LG] UPDATED)\nAbstract: Foundation models are becoming the dominant deep learning technologies. Pretraining a foundation model is always time-consumed due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the training process is extremely memory-intensive and communication-intensive. These features make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism and tensor model parallelism, to achieve high training efficiency.  To achieve this goal, some custom software frameworks such as Megatron-LM and DeepSpeed are developed. However, current 3D parallelism frameworks still meet two issues: i) they are not transparent to model developers, which need to manually modify the model to parallelize training. ii) their utilization of computation, GPU memory and network bandwidth are not sufficient. We propose Merak, an automated 3D parallelism deep learning training framework with high resource utilization. Merak automa",
    "path": "papers/22/06/2206.04959.json",
    "total_tokens": 916,
    "translated_title": "Merak：高效的分布式DNN训练框架，具备自动化的三维并行技术，适用于庞大的基础模型",
    "translated_abstract": "基础模型正在成为深度学习技术的主流。由于模型参数和训练数据集规模庞大，预训练基础模型始终需要耗费时间。除了计算密集型，训练过程还极其依赖内存和通信，这就需要应用三维并行技术，即集成数据并行、管道模型并行和张量模型并行，以实现高效训练。为此，研发了一些自定义软件框架，如Megatron-LM和DeepSpeed。然而，当前的三位并行技术框架仍存在两个问题：i）对于需要手动修改模型以并行训练的模型开发人员来说，框架并不透明。ii）它们的计算、GPU内存和网络带宽利用不足。我们提出了Merak，一个高资源利用率的自动化三维并行深度学习训练框架。",
    "tldr": "Merak是一个自动化的三维并行深度学习训练框架，它解决了当前其他框架中需要手动修改模型才可并行的问题，并具有较高的计算、GPU内存和网络带宽利用率。",
    "en_tdlr": "Merak is an automated 3D parallelism deep learning training framework with high resource utilization, which solves the issues in current frameworks that require manual modification of models for parallelism and have insufficient utilization of computation, GPU memory, and network bandwidth."
}