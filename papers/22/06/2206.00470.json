{
    "title": "Good Intentions: Adaptive Parameter Management via Intent Signaling. (arXiv:2206.00470v4 [cs.LG] UPDATED)",
    "abstract": "Parameter management is essential for distributed training of large machine learning (ML) tasks. Some ML tasks are hard to distribute because common approaches to parameter management can be highly inefficient. Advanced parameter management approaches -- such as selective replication or dynamic parameter allocation -- can improve efficiency, but to do so, they typically need to be integrated manually into each task's implementation and they require expensive upfront experimentation to tune correctly. In this work, we explore whether these two problems can be avoided. We first propose a novel intent signaling mechanism that integrates naturally into existing ML stacks and provides the parameter manager with crucial information about parameter accesses. We then describe AdaPM, a fully adaptive, zero-tuning parameter manager based on this mechanism. In contrast to prior systems, this approach separates providing information (simple, done by the task) from exploiting it effectively (hard, ",
    "link": "http://arxiv.org/abs/2206.00470",
    "context": "Title: Good Intentions: Adaptive Parameter Management via Intent Signaling. (arXiv:2206.00470v4 [cs.LG] UPDATED)\nAbstract: Parameter management is essential for distributed training of large machine learning (ML) tasks. Some ML tasks are hard to distribute because common approaches to parameter management can be highly inefficient. Advanced parameter management approaches -- such as selective replication or dynamic parameter allocation -- can improve efficiency, but to do so, they typically need to be integrated manually into each task's implementation and they require expensive upfront experimentation to tune correctly. In this work, we explore whether these two problems can be avoided. We first propose a novel intent signaling mechanism that integrates naturally into existing ML stacks and provides the parameter manager with crucial information about parameter accesses. We then describe AdaPM, a fully adaptive, zero-tuning parameter manager based on this mechanism. In contrast to prior systems, this approach separates providing information (simple, done by the task) from exploiting it effectively (hard, ",
    "path": "papers/22/06/2206.00470.json",
    "total_tokens": 820,
    "translated_title": "善意的举措：通过意图信号进行自适应参数管理",
    "translated_abstract": "参数管理对于大规模机器学习任务的分布式训练至关重要。由于常见的参数管理方法效率低下，一些机器学习任务难以分布式处理。先进的参数管理方法，例如选择性复制或动态参数分配，可以提高效率，但通常需要手动集成到每个任务的实现中，并且需要昂贵的前期实验来正确调整。在本文中，我们探讨了是否可以避免这两个问题。我们首先提出了一种新颖的意图信号机制，它自然地集成到现有的机器学习堆栈中，并为参数管理器提供关键的参数访问信息。然后，我们描述了AdaPM，一种基于这种机制的全自适应、零调试的参数管理器。与之前的系统不同，该方法将提供信息（任务简单完成）与有效利用信息（困难部分）分离开来。",
    "tldr": "本研究提出了一种善意的举措，通过意图信号机制实现自适应参数管理。该方法可以避免手动集成和昂贵的调试过程，提高分布式训练效率。"
}