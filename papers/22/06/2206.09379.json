{
    "title": "0/1 Deep Neural Networks via Block Coordinate Descent. (arXiv:2206.09379v2 [cs.LG] UPDATED)",
    "abstract": "The step function is one of the simplest and most natural activation functions for deep neural networks (DNNs). As it counts 1 for positive variables and 0 for others, its intrinsic characteristics (e.g., discontinuity and no viable information of subgradients) impede its development for several decades. Even if there is an impressive body of work on designing DNNs with continuous activation functions that can be deemed as surrogates of the step function, it is still in the possession of some advantageous properties, such as complete robustness to outliers and being capable of attaining the best learning-theoretic guarantee of predictive accuracy. Hence, in this paper, we aim to train DNNs with the step function used as an activation function (dubbed as 0/1 DNNs). We first reformulate 0/1 DNNs as an unconstrained optimization problem and then solve it by a block coordinate descend (BCD) method. Moreover, we acquire closed-form solutions for sub-problems of BCD as well as its convergenc",
    "link": "http://arxiv.org/abs/2206.09379",
    "context": "Title: 0/1 Deep Neural Networks via Block Coordinate Descent. (arXiv:2206.09379v2 [cs.LG] UPDATED)\nAbstract: The step function is one of the simplest and most natural activation functions for deep neural networks (DNNs). As it counts 1 for positive variables and 0 for others, its intrinsic characteristics (e.g., discontinuity and no viable information of subgradients) impede its development for several decades. Even if there is an impressive body of work on designing DNNs with continuous activation functions that can be deemed as surrogates of the step function, it is still in the possession of some advantageous properties, such as complete robustness to outliers and being capable of attaining the best learning-theoretic guarantee of predictive accuracy. Hence, in this paper, we aim to train DNNs with the step function used as an activation function (dubbed as 0/1 DNNs). We first reformulate 0/1 DNNs as an unconstrained optimization problem and then solve it by a block coordinate descend (BCD) method. Moreover, we acquire closed-form solutions for sub-problems of BCD as well as its convergenc",
    "path": "papers/22/06/2206.09379.json",
    "total_tokens": 922,
    "translated_title": "0/1深度神经网络的块坐标下降算法",
    "translated_abstract": "步函数是深度神经网络（DNNs）中最简单且最自然的激活函数之一。由于它对于正变量计数为1，对于其他变量计数为0，其固有特性（如不连续性和无有效的次梯度信息）阻碍了其几十年的发展。尽管有许多用连续激活函数设计的DNNs，可以视为步函数的替代品，但步函数仍具有一些优势特性，如对异常值的完全鲁棒性和具备最佳学习理论担保的预测准确性。因此，本文旨在训练使用步函数作为激活函数的DNNs（称为0/1 DNNs）。我们首先将0/1 DNNs重新表述为无约束优化问题，然后通过块坐标下降（BCD）方法对其进行求解。此外，我们还获得了BCD子问题的闭式解以及其收敛性。",
    "tldr": "本文介绍了一种用于训练0/1 DNNs的块坐标下降算法，该算法能够有效解决步函数作为激活函数所带来的困难，并获得了具有鲁棒性和最佳预测准确性的DNNs模型。"
}