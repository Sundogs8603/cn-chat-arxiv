{
    "title": "Efficiently Leveraging Multi-level User Intent for Session-based Recommendation via Atten-Mixer Network. (arXiv:2206.12781v3 [cs.IR] UPDATED)",
    "abstract": "Session-based recommendation (SBR) aims to predict the user's next action based on short and dynamic sessions. Recently, there has been an increasing interest in utilizing various elaborately designed graph neural networks (GNNs) to capture the pair-wise relationships among items, seemingly suggesting the design of more complicated models is the panacea for improving the empirical performance. However, these models achieve relatively marginal improvements with exponential growth in model complexity. In this paper, we dissect the classical GNN-based SBR models and empirically find that some sophisticated GNN propagations are redundant, given the readout module plays a significant role in GNN-based models. Based on this observation, we intuitively propose to remove the GNN propagation part, while the readout module will take on more responsibility in the model reasoning process. To this end, we propose the Multi-Level Attention Mixture Network (Atten-Mixer), which leverages both concept-",
    "link": "http://arxiv.org/abs/2206.12781",
    "context": "Title: Efficiently Leveraging Multi-level User Intent for Session-based Recommendation via Atten-Mixer Network. (arXiv:2206.12781v3 [cs.IR] UPDATED)\nAbstract: Session-based recommendation (SBR) aims to predict the user's next action based on short and dynamic sessions. Recently, there has been an increasing interest in utilizing various elaborately designed graph neural networks (GNNs) to capture the pair-wise relationships among items, seemingly suggesting the design of more complicated models is the panacea for improving the empirical performance. However, these models achieve relatively marginal improvements with exponential growth in model complexity. In this paper, we dissect the classical GNN-based SBR models and empirically find that some sophisticated GNN propagations are redundant, given the readout module plays a significant role in GNN-based models. Based on this observation, we intuitively propose to remove the GNN propagation part, while the readout module will take on more responsibility in the model reasoning process. To this end, we propose the Multi-Level Attention Mixture Network (Atten-Mixer), which leverages both concept-",
    "path": "papers/22/06/2206.12781.json",
    "total_tokens": 966,
    "translated_title": "通过Atten-Mixer网络高效利用多级用户意图进行基于会话的推荐",
    "translated_abstract": "基于会话的推荐旨在根据短暂且动态的会话预测用户的下一个动作。最近，在利用各种精心设计的图神经网络(GNN)捕捉物品之间的成对关系方面引起了越来越多的兴趣，似乎表明设计更复杂的模型是提高实证性能的万灵药。然而，这些模型在模型复杂度呈指数增长的同时，仅取得了相对较小的改进。在本文中，我们剖析了经典的基于GNN的SBR模型，并在经验上发现，一些复杂的GNN传播在给定读出模块在GNN模型中起到重要作用的情况下是多余的。基于这一观察，我们直观地提出了移除GNN传播部分的想法，而读出模块将在模型推理过程中承担更多责任。为此，我们提出了Multi-Level Attention Mixture Network (Atten-Mixer)，它同时利用概念-",
    "tldr": "本文针对基于会话的推荐任务，通过剖析经典的基于图神经网络的推荐模型，发现一些复杂的图神经网络传播部分是多余的。基于此观察，我们提出了Multi-Level Attention Mixture Network (Atten-Mixer)，它通过移除多余的传播部分，实现了对读出模块的更高效利用。",
    "en_tdlr": "This paper focuses on session-based recommendation and dissects classical graph neural network-based recommendation models, finding that some complex graph neural network propagation parts are redundant. Based on this observation, we propose the Multi-Level Attention Mixture Network (Atten-Mixer), which efficiently utilizes the readout module by removing unnecessary propagation parts."
}