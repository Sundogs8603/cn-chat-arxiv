{
    "title": "No Reason for No Supervision: Improved Generalization in Supervised Models. (arXiv:2206.15369v2 [cs.CV] UPDATED)",
    "abstract": "We consider the problem of training a deep neural network on a given classification task, e.g., ImageNet-1K (IN1K), so that it excels at both the training task as well as at other (future) transfer tasks. These two seemingly contradictory properties impose a trade-off between improving the model's generalization and maintaining its performance on the original task. Models trained with self-supervised learning tend to generalize better than their supervised counterparts for transfer learning; yet, they still lag behind supervised models on IN1K. In this paper, we propose a supervised learning setup that leverages the best of both worlds. We extensively analyze supervised training using multi-scale crops for data augmentation and an expendable projector head, and reveal that the design of the projector allows us to control the trade-off between performance on the training task and transferability. We further replace the last layer of class weights with class prototypes computed on the fl",
    "link": "http://arxiv.org/abs/2206.15369",
    "raw_ret": "{\n    \"translated_title\": \"无需无监督：监督模型的改进推广性能\",\n    \"translated_abstract\": \"本文考虑在给定的分类任务（例如 ImageNet-1K）上训练深度神经网络，以便它在训练任务和其他（未来的）传输任务上表现出色。本文提出了一种监督学习设置，利用了自监督学习和监督学习之间的优势，并分析了多尺度裁剪和可拓展投影头进行数据增强的监督训练。我们发现，投影的设计允许我们控制性能和传输能力之间的平衡。我们还使用分类原型替换了最后一层的类别权重，这些原型是基于所有训练图像计算的。\",\n    \"tldr\": \"本文提出了一种监督学习设置，利用自监督学习和监督学习之间的优势，设计了多尺度裁剪和可拓展投影头进行数据增强的监督训练，并使用分类原型替换了最后一层的类别权重。这种设置可以使模型在保持在原任务上的性能的同时，可以在传输学习中表现出更好的泛化能力。\"",
    "total_tokens": 823
}