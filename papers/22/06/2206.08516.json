{
    "title": "MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare. (arXiv:2206.08516v3 [cs.LG] UPDATED)",
    "abstract": "Federated learning has attracted increasing attention to building models without accessing the raw user data, especially in healthcare. In real applications, different federations can seldom work together due to possible reasons such as data heterogeneity and distrust/inexistence of the central server. In this paper, we propose a novel framework called MetaFed to facilitate trustworthy FL between different federations. MetaFed obtains a personalized model for each federation without a central server via the proposed Cyclic Knowledge Distillation. Specifically, MetaFed treats each federation as a meta distribution and aggregates knowledge of each federation in a cyclic manner. The training is split into two parts: common knowledge accumulation and personalization. Comprehensive experiments on three benchmarks demonstrate that MetaFed without a server achieves better accuracy compared to state-of-the-art methods (e.g., 10%+ accuracy improvement compared to the baseline for PAMAP2) with f",
    "link": "http://arxiv.org/abs/2206.08516",
    "context": "Title: MetaFed: Federated Learning among Federations with Cyclic Knowledge Distillation for Personalized Healthcare. (arXiv:2206.08516v3 [cs.LG] UPDATED)\nAbstract: Federated learning has attracted increasing attention to building models without accessing the raw user data, especially in healthcare. In real applications, different federations can seldom work together due to possible reasons such as data heterogeneity and distrust/inexistence of the central server. In this paper, we propose a novel framework called MetaFed to facilitate trustworthy FL between different federations. MetaFed obtains a personalized model for each federation without a central server via the proposed Cyclic Knowledge Distillation. Specifically, MetaFed treats each federation as a meta distribution and aggregates knowledge of each federation in a cyclic manner. The training is split into two parts: common knowledge accumulation and personalization. Comprehensive experiments on three benchmarks demonstrate that MetaFed without a server achieves better accuracy compared to state-of-the-art methods (e.g., 10%+ accuracy improvement compared to the baseline for PAMAP2) with f",
    "path": "papers/22/06/2206.08516.json",
    "total_tokens": 945,
    "translated_title": "MetaFed: 个性化医疗中基于循环知识蒸馏的跨联邦联邦学习",
    "translated_abstract": "联邦学习在建立模型时无需访问原始用户数据，尤其在医疗领域引起了越来越多的关注。然而在实际应用中，不同的联邦很少因为数据异构性和中央服务器的不信任/不存在等原因而共同工作。本文提出了一种名为MetaFed的新型框架，以促进不同联邦之间可信的联邦学习。通过所提出的循环知识蒸馏，MetaFed在没有中央服务器的情况下为每个联邦获取个性化模型。具体而言，MetaFed将每个联邦视为一个元分布，并以循环的方式聚合每个联邦的知识。训练分为两个部分：共同知识积累和个性化。对三个基准进行的综合实验表明，与基线方法相比（例如，与基础方法相比，对于PAMAP2，精度提高了10％+），没有服务器的MetaFed能够实现更高的准确性。",
    "tldr": "本文提出了一个名为MetaFed的新型框架，旨在在不同的联邦之间实现可信的联邦学习。通过循环知识蒸馏，MetaFed能够获取每个联邦的个性化模型，无需中央服务器，并且实验证明MetaFed在精度上超过了现有方法。"
}