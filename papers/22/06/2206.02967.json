{
    "title": "Masked Unsupervised Self-training for Label-free Image Classification. (arXiv:2206.02967v2 [cs.CV] UPDATED)",
    "abstract": "State-of-the-art computer vision models are mostly trained with supervised learning using human-labeled images, which limits their scalability due to the expensive annotation cost. While self-supervised representation learning has achieved impressive progress, it still requires a second stage of finetuning on labeled data. On the other hand, models pre-trained with large-scale text-image supervision (e.g., CLIP) have enabled zero-shot transfer to downstream image classification tasks. However, the zero-shot performance of CLIP-like models are often insufficient for real-world adoption. In this paper, we aim to leverage the abundant unlabeled data from a target domain to improve the performance of a pre-trained zero-shot classifier, by unsupervised finetuning of the pre-trained model. We propose Masked Unsupervised Self-Training (MUST), a new unsupervised adaptation method which leverages two different and complementary sources of training signals: pseudo-labels and raw images. MUST joi",
    "link": "http://arxiv.org/abs/2206.02967",
    "raw_ret": "{\n    \"translated_title\": \"基于遮蔽的无标签图像分类的自监督无监督自训练\",\n    \"translated_abstract\": \"现代计算机视觉模型主要是通过使用人工标记的图像进行监督学习进行训练的，这限制了它们的可扩展性，因为注释费用昂贵。尽管自监督表示学习取得了令人印象深刻的进展，但它仍需要在标记数据上进行第二阶段的微调。另一方面，使用大规模文本-图像监督（例如CLIP）预训练的模型已经实现了零-shot转移至下游图像分类任务的能力。然而，类似CLIP的模型的零-shot表现通常不足以用于实际应用。本文旨在利用目标域中丰富的无标签数据，通过预训练的零-shot分类器的自监督微调来改进其性能。我们提出了Masked Unsupervised Self-Training (MUST)——一种新的无监督自适应方法，它利用了两种不同且互补的训练信号来源：伪标签和原始图像。\",\n    \"tldr\": \"本文提出了一种新的无监督自适应方法——Masked Unsupervised Self-Training (MUST)，利用了两种不同且互补的训练信号来源：伪标签和原始图像。旨在利用目标域中丰富的无标签数据，通过预训练的零-shot分类器的自监督微调来改进其性能。\"\n}",
    "total_tokens": 920,
    "ret": {
        "translated_title": "基于遮蔽的无标签图像分类的自监督无监督自训练",
        "translated_abstract": "现代计算机视觉模型主要是通过使用人工标记的图像进行监督学习进行训练的，这限制了它们的可扩展性，因为注释费用昂贵。尽管自监督表示学习取得了令人印象深刻的进展，但它仍需要在标记数据上进行第二阶段的微调。另一方面，使用大规模文本-图像监督（例如CLIP）预训练的模型已经实现了零-shot转移至下游图像分类任务的能力。然而，类似CLIP的模型的零-shot表现通常不足以用于实际应用。本文旨在利用目标域中丰富的无标签数据，通过预训练的零-shot分类器的自监督微调来改进其性能。我们提出了Masked Unsupervised Self-Training (MUST)——一种新的无监督自适应方法，它利用了两种不同且互补的训练信号来源：伪标签和原始图像。",
        "tldr": "本文提出了一种新的无监督自适应方法——Masked Unsupervised Self-Training (MUST)，利用了两种不同且互补的训练信号来源：伪标签和原始图像。旨在利用目标域中丰富的无标签数据，通过预训练的零-shot分类器的自监督微调来改进其性能。"
    }
}