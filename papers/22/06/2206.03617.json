{
    "title": "Subject Granular Differential Privacy in Federated Learning. (arXiv:2206.03617v2 [cs.LG] UPDATED)",
    "abstract": "This paper considers subject level privacy in the FL setting, where a subject is an individual whose private information is embodied by several data items either confined within a single federation user or distributed across multiple federation users. We propose two new algorithms that enforce subject level DP at each federation user locally. Our first algorithm, called LocalGroupDP, is a straightforward application of group differential privacy in the popular DP-SGD algorithm. Our second algorithm is based on a novel idea of hierarchical gradient averaging (HiGradAvgDP) for subjects participating in a training mini-batch. We also show that user level Local Differential Privacy (LDP) naturally guarantees subject level DP. We observe the problem of horizontal composition of subject level privacy loss in FL - subject level privacy loss incurred at individual users composes across the federation. We formally prove the subject level DP guarantee for our algorithms, and also show their effe",
    "link": "http://arxiv.org/abs/2206.03617",
    "context": "Title: Subject Granular Differential Privacy in Federated Learning. (arXiv:2206.03617v2 [cs.LG] UPDATED)\nAbstract: This paper considers subject level privacy in the FL setting, where a subject is an individual whose private information is embodied by several data items either confined within a single federation user or distributed across multiple federation users. We propose two new algorithms that enforce subject level DP at each federation user locally. Our first algorithm, called LocalGroupDP, is a straightforward application of group differential privacy in the popular DP-SGD algorithm. Our second algorithm is based on a novel idea of hierarchical gradient averaging (HiGradAvgDP) for subjects participating in a training mini-batch. We also show that user level Local Differential Privacy (LDP) naturally guarantees subject level DP. We observe the problem of horizontal composition of subject level privacy loss in FL - subject level privacy loss incurred at individual users composes across the federation. We formally prove the subject level DP guarantee for our algorithms, and also show their effe",
    "path": "papers/22/06/2206.03617.json",
    "total_tokens": 961,
    "translated_title": "基于主体的差分隐私在联邦学习中的应用",
    "translated_abstract": "本文考虑在联邦学习设置中实现主体级别的隐私保护。主体是一个个体，其私有信息由单个联邦用户内部或分布在多个联邦用户之间的多个数据项所体现。我们提出了两个新算法，可以在每个联邦用户本地实现主体级别的差分隐私保护。第一个算法称为LocalGroupDP，是在流行的DP-SGD算法中应用组差分隐私的直接应用。我们的第二个算法是基于一种新颖的想法——针对参加训练迷你批次的主体使用分层渐进平均梯度（HiGradAvgDP）。我们还证明了用户级别的局部差分隐私可以自然地保证主体级别的差分隐私。我们观察到，FL中主体级隐私损失的横向组合问题——各个用户产生的主体级隐私损失在联邦中进行组合。我们正式证明了我们算法对主体级差分隐私的保证，并通过真实世界数据集上的实验证明了其有效性。",
    "tldr": "本文提出了两个新算法，可以在每个联邦用户本地实现主体级别的差分隐私保护。用户级别的局部差分隐私可以自然地保证主体级别的差分隐私，并通过真实世界数据集上的实验验证了算法的有效性。",
    "en_tdlr": "This paper proposes two new algorithms for subject level differential privacy protection in federated learning, with one based on hierarchical gradient averaging and the other applying group differential privacy in the popular DP-SGD algorithm. The effectiveness of the algorithms is demonstrated through experiments on real-world datasets, and it is shown that user level local differential privacy can naturally guarantee subject level differential privacy."
}