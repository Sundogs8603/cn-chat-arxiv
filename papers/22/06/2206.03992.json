{
    "title": "Neural Diffusion Processes. (arXiv:2206.03992v2 [stat.ML] UPDATED)",
    "abstract": "Neural network approaches for meta-learning distributions over functions have desirable properties such as increased flexibility and a reduced complexity of inference. Building on the successes of denoising diffusion models for generative modelling, we propose Neural Diffusion Processes (NDPs), a novel approach that learns to sample from a rich distribution over functions through its finite marginals. By introducing a custom attention block we are able to incorporate properties of stochastic processes, such as exchangeability, directly into the NDP's architecture. We empirically show that NDPs can capture functional distributions close to the true Bayesian posterior, demonstrating that they can successfully emulate the behaviour of Gaussian processes and surpass the performance of neural processes. NDPs enable a variety of downstream tasks, including regression, implicit hyperparameter marginalisation, non-Gaussian posterior prediction and global optimisation.",
    "link": "http://arxiv.org/abs/2206.03992",
    "context": "Title: Neural Diffusion Processes. (arXiv:2206.03992v2 [stat.ML] UPDATED)\nAbstract: Neural network approaches for meta-learning distributions over functions have desirable properties such as increased flexibility and a reduced complexity of inference. Building on the successes of denoising diffusion models for generative modelling, we propose Neural Diffusion Processes (NDPs), a novel approach that learns to sample from a rich distribution over functions through its finite marginals. By introducing a custom attention block we are able to incorporate properties of stochastic processes, such as exchangeability, directly into the NDP's architecture. We empirically show that NDPs can capture functional distributions close to the true Bayesian posterior, demonstrating that they can successfully emulate the behaviour of Gaussian processes and surpass the performance of neural processes. NDPs enable a variety of downstream tasks, including regression, implicit hyperparameter marginalisation, non-Gaussian posterior prediction and global optimisation.",
    "path": "papers/22/06/2206.03992.json",
    "total_tokens": 896,
    "translated_title": "神经扩散过程",
    "translated_abstract": "与函数元学习分布相关的神经网络方法具有灵活性增强和推断复杂性降低的优点。在去噪扩散模型用于生成建模方面取得成功的基础上，我们提出了一种新方法——神经扩散过程（NDPs），通过有限边缘学习从丰富的函数分布中进行采样。通过引入自定义注意力块，我们能够将随机过程的属性（如可交换性）直接纳入 NDP 的架构中。我们从实证角度证明了 NDPs 可以捕获接近真实贝叶斯后验的函数分布，表明它们能够成功模拟高斯过程的行为并超越神经过程的表现。NDPs 可以进行多种下游任务，包括回归、隐式超参数边缘化、非高斯后验预测和全局优化。",
    "tldr": "提出了一种新方法——神经扩散过程（NDPs），通过有限边缘学习从丰富的函数分布中进行采样。NDPs 可以捕获接近真实贝叶斯后验的函数分布，具有超越神经过程的表现，实现了多种下游任务，比如回归、隐式超参数边缘化、非高斯后验预测和全局优化。",
    "en_tdlr": "The paper proposes Neural Diffusion Processes (NDPs), a novel method that learns to sample from a rich distribution over functions through its finite marginals by incorporating properties of stochastic processes and surpassing the performance of neural processes. NDPs enable various downstream tasks, such as regression, implicit hyperparameter marginalization, non-Gaussian posterior prediction, and global optimization."
}