{
    "title": "Compressed-VFL: Communication-Efficient Learning with Vertically Partitioned Data. (arXiv:2206.08330v2 [cs.LG] UPDATED)",
    "abstract": "We propose Compressed Vertical Federated Learning (C-VFL) for communication-efficient training on vertically partitioned data. In C-VFL, a server and multiple parties collaboratively train a model on their respective features utilizing several local iterations and sharing compressed intermediate results periodically. Our work provides the first theoretical analysis of the effect message compression has on distributed training over vertically partitioned data. We prove convergence of non-convex objectives at a rate of $O(\\frac{1}{\\sqrt{T}})$ when the compression error is bounded over the course of training. We provide specific requirements for convergence with common compression techniques, such as quantization and top-$k$ sparsification. Finally, we experimentally show compression can reduce communication by over $90\\%$ without a significant decrease in accuracy over VFL without compression.",
    "link": "http://arxiv.org/abs/2206.08330",
    "context": "Title: Compressed-VFL: Communication-Efficient Learning with Vertically Partitioned Data. (arXiv:2206.08330v2 [cs.LG] UPDATED)\nAbstract: We propose Compressed Vertical Federated Learning (C-VFL) for communication-efficient training on vertically partitioned data. In C-VFL, a server and multiple parties collaboratively train a model on their respective features utilizing several local iterations and sharing compressed intermediate results periodically. Our work provides the first theoretical analysis of the effect message compression has on distributed training over vertically partitioned data. We prove convergence of non-convex objectives at a rate of $O(\\frac{1}{\\sqrt{T}})$ when the compression error is bounded over the course of training. We provide specific requirements for convergence with common compression techniques, such as quantization and top-$k$ sparsification. Finally, we experimentally show compression can reduce communication by over $90\\%$ without a significant decrease in accuracy over VFL without compression.",
    "path": "papers/22/06/2206.08330.json",
    "total_tokens": 863,
    "translated_title": "Compressed-VFL: 用于具有纵向分割数据的通信高效学习",
    "translated_abstract": "我们提出了一种用于纵向分割数据的通信高效训练的压缩式纵向联邦学习（C-VFL）。在C-VFL中，服务器和多个参与方利用几个本地迭代协同训练各自特征上的模型，并周期性地共享压缩后的中间结果。我们的工作首次提供了压缩对于纵向分割数据分布式训练的影响的理论分析。我们证明了当训练过程中的压缩误差被限制时，非凸目标的收敛速率为 $O(\\frac{1}{\\sqrt{T}})$。我们提供了使用常用压缩技术（例如量化和前$k$大稀疏化）进行收敛所需的具体要求。最后，我们在实验中展示了，在不明显降低VFL准确性的情况下，压缩可以将通信量减少超过 $90\\%$。",
    "tldr": "提出了一种用于纵向分割数据的通信高效学习方法，通过周期性共享压缩的中间结果，实现了超过90%的通信量减少，且不影响模型准确性。",
    "en_tdlr": "This paper proposes Compressed Vertical Federated Learning (C-VFL) for communication-efficient training on vertically partitioned data. By sharing periodically compressed intermediate results, C-VFL achieves more than 90% reduction in communication and does not affect the model accuracy."
}