{
    "title": "Flexible Differentiable Optimization via Model Transformations. (arXiv:2206.06135v2 [cs.LG] UPDATED)",
    "abstract": "We introduce DiffOpt.jl, a Julia library to differentiate through the solution of optimization problems with respect to arbitrary parameters present in the objective and/or constraints. The library builds upon MathOptInterface, thus leveraging the rich ecosystem of solvers and composing well with modeling languages like JuMP. DiffOpt offers both forward and reverse differentiation modes, enabling multiple use cases from hyperparameter optimization to backpropagation and sensitivity analysis, bridging constrained optimization with end-to-end differentiable programming. DiffOpt is built on two known rules for differentiating quadratic programming and conic programming standard forms. However, thanks ability to differentiate through model transformation, the user is not limited to these forms and can differentiate with respect to the parameters of any model that can be reformulated into these standard forms. This notably includes programs mixing affine conic constraints and convex quadrat",
    "link": "http://arxiv.org/abs/2206.06135",
    "context": "Title: Flexible Differentiable Optimization via Model Transformations. (arXiv:2206.06135v2 [cs.LG] UPDATED)\nAbstract: We introduce DiffOpt.jl, a Julia library to differentiate through the solution of optimization problems with respect to arbitrary parameters present in the objective and/or constraints. The library builds upon MathOptInterface, thus leveraging the rich ecosystem of solvers and composing well with modeling languages like JuMP. DiffOpt offers both forward and reverse differentiation modes, enabling multiple use cases from hyperparameter optimization to backpropagation and sensitivity analysis, bridging constrained optimization with end-to-end differentiable programming. DiffOpt is built on two known rules for differentiating quadratic programming and conic programming standard forms. However, thanks ability to differentiate through model transformation, the user is not limited to these forms and can differentiate with respect to the parameters of any model that can be reformulated into these standard forms. This notably includes programs mixing affine conic constraints and convex quadrat",
    "path": "papers/22/06/2206.06135.json",
    "total_tokens": 1004,
    "translated_title": "通过模型变换实现灵活可导优化",
    "translated_abstract": "本文介绍了DiffOpt.jl，这是一个Julia库，它可以对包含目标值和/或约束条件的任意参数进行优化求解，并进行微分。这个库基于MathOptInterface构建，因此可以利用丰富的求解器生态系统，并与像JuMP这样的建模语言组合得很好。DiffOpt提供前向和反向微分模式，使得可以应用于超参数优化、反向传播和敏感性分析等多种用途，通过将约束优化与端到端可导性编程相结合。DiffOpt是基于两个已知的规则来求解的，分别是凸锥规划和二次规划标准形式的微分。然而，由于可以通过模型转换进行微分，因此用户不仅限于这些形式，还可以针对可以转换为这些标准形式的任何模型的参数进行微分。这特别包括混合仿射锥约束和凸四次约束的程序，这些程序无法直接建模为标准锥形程序。因此，DiffOpt使得可以灵活高效地求解一系列模型的优化问题，为基于优化的机器学习提供了新的机会。",
    "tldr": "本文介绍了DiffOpt.jl，一个基于MathOptInterface构建的Julia库，它可以对任何模型的参数进行微分求解，不仅限于凸锥规划和二次规划标准形式。使用该库, 可以实现灵活高效地求解一系列模型的优化问题，为基于优化的机器学习提供了新的机会。",
    "en_tdlr": "This paper introduces DiffOpt.jl, a Julia library based on MathOptInterface that can differentiate through the solution of optimization problems with respect to arbitrary parameters, not limited to standard quadratic programming and conic programming forms. This library enables flexible and efficient optimization of a wide range of models, providing new opportunities for optimization-based machine learning."
}