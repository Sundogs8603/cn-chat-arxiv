{
    "title": "BRExIt: On Opponent Modelling in Expert Iteration. (arXiv:2206.00113v2 [cs.AI] UPDATED)",
    "abstract": "Finding a best response policy is a central objective in game theory and multi-agent learning, with modern population-based training approaches employing reinforcement learning algorithms as best-response oracles to improve play against candidate opponents (typically previously learnt policies). We propose Best Response Expert Iteration (BRExIt), which accelerates learning in games by incorporating opponent models into the state-of-the-art learning algorithm Expert Iteration (ExIt). BRExIt aims to (1) improve feature shaping in the apprentice, with a policy head predicting opponent policies as an auxiliary task, and (2) bias opponent moves in planning towards the given or learnt opponent model, to generate apprentice targets that better approximate a best response. In an empirical ablation on BRExIt's algorithmic variants against a set of fixed test agents, we provide statistical evidence that BRExIt learns better performing policies than ExIt.",
    "link": "http://arxiv.org/abs/2206.00113",
    "context": "Title: BRExIt: On Opponent Modelling in Expert Iteration. (arXiv:2206.00113v2 [cs.AI] UPDATED)\nAbstract: Finding a best response policy is a central objective in game theory and multi-agent learning, with modern population-based training approaches employing reinforcement learning algorithms as best-response oracles to improve play against candidate opponents (typically previously learnt policies). We propose Best Response Expert Iteration (BRExIt), which accelerates learning in games by incorporating opponent models into the state-of-the-art learning algorithm Expert Iteration (ExIt). BRExIt aims to (1) improve feature shaping in the apprentice, with a policy head predicting opponent policies as an auxiliary task, and (2) bias opponent moves in planning towards the given or learnt opponent model, to generate apprentice targets that better approximate a best response. In an empirical ablation on BRExIt's algorithmic variants against a set of fixed test agents, we provide statistical evidence that BRExIt learns better performing policies than ExIt.",
    "path": "papers/22/06/2206.00113.json",
    "total_tokens": 888,
    "translated_title": "BRExIt：论对手建模在专家迭代中的应用",
    "translated_abstract": "在博弈论和多智能体学习中，寻找最佳反应策略是一个核心目标。现代基于群体的训练方法采用强化学习算法作为最佳反应预测器，以改善玩家对候选对手（通常是先前学习的策略）的游戏表现。我们提出了最佳反应专家迭代（BRExIt）算法，它将对手模型融入到最先进的学习算法Expert Iteration（ExIt）中以加速游戏学习。BRExIt旨在（1）改进学徒中的特征塑造，通过策略头预测对手策略作为辅助任务；（2）将对手移动的规划偏向于给定或已学习的对手模型，以生成更好地逼近最佳反应的学徒目标。我们对BRExIt的算法变体进行了实证消融，针对一组固定测试代理进行比较，发现BRExIt比ExIt学习到更好的策略。",
    "tldr": "BRExIt算法使用对手模型加速游戏学习，提高学徒的特征塑造和规划偏向于对手模型。实验证明BRExIt比ExIt学习到更好的策略。",
    "en_tdlr": "BRExIt algorithm accelerates game learning using opponent modeling, improving feature shaping and planning bias towards the opponent model for the apprentice. Empirical evidence shows that BRExIt learns better policies than ExIt."
}