{
    "title": "Convergence for score-based generative modeling with polynomial complexity. (arXiv:2206.06227v2 [cs.LG] UPDATED)",
    "abstract": "Score-based generative modeling (SGM) is a highly successful approach for learning a probability distribution from data and generating further samples. We prove the first polynomial convergence guarantees for the core mechanic behind SGM: drawing samples from a probability density $p$ given a score estimate (an estimate of $\\nabla \\ln p$) that is accurate in $L^2(p)$. Compared to previous works, we do not incur error that grows exponentially in time or that suffers from a curse of dimensionality. Our guarantee works for any smooth distribution and depends polynomially on its log-Sobolev constant. Using our guarantee, we give a theoretical analysis of score-based generative modeling, which transforms white-noise input into samples from a learned data distribution given score estimates at different noise scales. Our analysis gives theoretical grounding to the observation that an annealed procedure is required in practice to generate good samples, as our proof depends essentially on using",
    "link": "http://arxiv.org/abs/2206.06227",
    "context": "Title: Convergence for score-based generative modeling with polynomial complexity. (arXiv:2206.06227v2 [cs.LG] UPDATED)\nAbstract: Score-based generative modeling (SGM) is a highly successful approach for learning a probability distribution from data and generating further samples. We prove the first polynomial convergence guarantees for the core mechanic behind SGM: drawing samples from a probability density $p$ given a score estimate (an estimate of $\\nabla \\ln p$) that is accurate in $L^2(p)$. Compared to previous works, we do not incur error that grows exponentially in time or that suffers from a curse of dimensionality. Our guarantee works for any smooth distribution and depends polynomially on its log-Sobolev constant. Using our guarantee, we give a theoretical analysis of score-based generative modeling, which transforms white-noise input into samples from a learned data distribution given score estimates at different noise scales. Our analysis gives theoretical grounding to the observation that an annealed procedure is required in practice to generate good samples, as our proof depends essentially on using",
    "path": "papers/22/06/2206.06227.json",
    "total_tokens": 908,
    "translated_title": "得分模型的多项式复杂度的重要性证明",
    "translated_abstract": "得分模型（SGM）是一种学习概率分布并生成更多样本的高效方法。本文证明了SGM背后的核心机制即在$L^2(p)$准确估计$\\nabla \\ln p$后从概率密度$p$中抽样的多项式收敛保证。与之前的工作相比，我们不会产生随时间指数增长或遭受维数灾难的误差。我们的保证适用于任何平滑分布，且与其对数Sobolev常数多项式相关。使用我们的保证，我们对基于得分的生成模型进行了理论分析，将白噪声输入转换为从不同噪声尺度给定得分估计的学习数据分布的样本。我们的分析为使用退火程序生成好的样本提供了理论基础，因为我们的证明基本上是依赖于使用。",
    "tldr": "本文证明了对于得分模型而言，从一个概率分布中抽样的核心机制，在$L^2(p)$准确估计$\\nabla \\ln p$后可以多项式收敛。同时提供了对基于得分的生成模型的理论分析，为使用退火程序生成样本提供了理论基础。"
}