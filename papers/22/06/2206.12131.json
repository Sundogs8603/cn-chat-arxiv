{
    "title": "MVP: Multi-task Supervised Pre-training for Natural Language Generation. (arXiv:2206.12131v3 [cs.CL] UPDATED)",
    "abstract": "Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e. \"supervised pre-training\") showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from $77$ datasets over $11$ diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model's capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tunin",
    "link": "http://arxiv.org/abs/2206.12131",
    "context": "Title: MVP: Multi-task Supervised Pre-training for Natural Language Generation. (arXiv:2206.12131v3 [cs.CL] UPDATED)\nAbstract: Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e. \"supervised pre-training\") showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from $77$ datasets over $11$ diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model's capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tunin",
    "path": "papers/22/06/2206.12131.json",
    "total_tokens": 959,
    "translated_title": "MVP: 自然语言生成中的多任务监督预训练",
    "translated_abstract": "预训练语言模型（PLMs）在自然语言生成（NLG）任务中取得了显着的成功。迄今为止，大部分面向NLG的PLMs都是使用大规模通用语料库非监督预训练的。与此同时，越来越多的使用标记数据预训练（即“监督预训练”）的模型展示出与非监督预训练模型相比更优异的表现。受监督预训练成功的启发，我们提出了用于自然语言生成的多任务监督预训练（MVP）。我们从11个不同的NLG任务以及77个数据集中收集了一个大规模的自然语言生成语料库（MVPCorpus），然后将这些示例统一格式化为一般的文本-文本格式，以监督方式预训练文本生成模型MVP。对于每个任务，我们进一步预训练特定的软提示，以刺激模型执行特定任务的能力。我们的MVP模型可以看作是使用最近的指导微调技术的一种实践。",
    "tldr": "本文提出了用于自然语言生成的多任务监督预训练（MVP）方法，其收集大规模自然语言生成语料库，并使用监督方式对文本生成模型进行训练。MVP模型结合特定的软提示，可以在各种任务中展现出优异的表现。",
    "en_tdlr": "This paper proposes a Multi-task Supervised Pre-training (MVP) method for natural language generation (NLG). MVP collects a large-scale NLG corpus and pre-trains a text generation model in a supervised manner with specific soft prompts for each task, achieving superior performance on various tasks."
}