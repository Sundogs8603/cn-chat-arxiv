{
    "title": "A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel. (arXiv:2206.12543v3 [stat.ML] UPDATED)",
    "abstract": "Empirical neural tangent kernels (eNTKs) can provide a good understanding of a given network's representation: they are often far less expensive to compute and applicable more broadly than infinite width NTKs. For networks with O output units (e.g. an O-class classifier), however, the eNTK on N inputs is of size $NO \\times NO$, taking $O((NO)^2)$ memory and up to $O((NO)^3)$ computation. Most existing applications have therefore used one of a handful of approximations yielding $N \\times N$ kernel matrices, saving orders of magnitude of computation, but with limited to no justification. We prove that one such approximation, which we call \"sum of logits\", converges to the true eNTK at initialization for any network with a wide final \"readout\" layer. Our experiments demonstrate the quality of this approximation for various uses across a range of settings.",
    "link": "http://arxiv.org/abs/2206.12543",
    "context": "Title: A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel. (arXiv:2206.12543v3 [stat.ML] UPDATED)\nAbstract: Empirical neural tangent kernels (eNTKs) can provide a good understanding of a given network's representation: they are often far less expensive to compute and applicable more broadly than infinite width NTKs. For networks with O output units (e.g. an O-class classifier), however, the eNTK on N inputs is of size $NO \\times NO$, taking $O((NO)^2)$ memory and up to $O((NO)^3)$ computation. Most existing applications have therefore used one of a handful of approximations yielding $N \\times N$ kernel matrices, saving orders of magnitude of computation, but with limited to no justification. We prove that one such approximation, which we call \"sum of logits\", converges to the true eNTK at initialization for any network with a wide final \"readout\" layer. Our experiments demonstrate the quality of this approximation for various uses across a range of settings.",
    "path": "papers/22/06/2206.12543.json",
    "total_tokens": 890,
    "translated_title": "一种对经验神经切向核的快速且有根据的近似方法。",
    "translated_abstract": "经验神经切向核（eNTK）可以很好地理解给定网络的表示：它们通常比无限宽NTK计算便宜得多，适用范围更广。然而，对于具有O个输出单元（例如O类分类器）的网络，N个输入的eNTK的大小为$NO\\times NO$，需要$O((NO)^2)$的内存和高达$O((NO)^3)$的计算量。因此，大多数现有的应用程序使用少数几个近似值之一，可以产生$N\\times N$内核矩阵，从而节省数量级的计算，但没有或极少有理论依据。我们证明了其中一种近似方法，我们称之为“逻辑和”，对于任何具有宽的最终“读出”层的网络，在初始化时收敛于真实的eNTK。我们的实验展示了这个近似方法在各种不同设置中的各种用途的质量。",
    "tldr": "该论文提出了一种名为“逻辑和”的经验神经切向核近似方法，可以在计算量上显著降低，同时经过证明在宽的最终“读出”层的网络中初始化后收敛于真实的eNTK。",
    "en_tdlr": "The paper proposes an approximation method for the empirical neural tangent kernel called \"sum of logits\", which significantly reduces computation while converging to the true eNTK at initialization for networks with a wide final \"readout\" layer."
}