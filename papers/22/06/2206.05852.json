{
    "title": "ChordMixer: A Scalable Neural Attention Model for Sequences with Different Lengths. (arXiv:2206.05852v2 [cs.LG] UPDATED)",
    "abstract": "Sequential data naturally have different lengths in many domains, with some very long sequences. As an important modeling tool, neural attention should capture long-range interaction in such sequences. However, most existing neural attention models admit only short sequences, or they have to employ chunking or padding to enforce a constant input length. Here we propose a simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths. Each ChordMixer block consists of a position-wise rotation layer without learnable parameters and an element-wise MLP layer. Repeatedly applying such blocks forms an effective network backbone that mixes the input signals towards the learning targets. We have tested ChordMixer on the synthetic adding problem, long document classification, and DNA sequence-based taxonomy classification. The experiment results show that our method substantially outperforms other neural attention models.",
    "link": "http://arxiv.org/abs/2206.05852",
    "context": "Title: ChordMixer: A Scalable Neural Attention Model for Sequences with Different Lengths. (arXiv:2206.05852v2 [cs.LG] UPDATED)\nAbstract: Sequential data naturally have different lengths in many domains, with some very long sequences. As an important modeling tool, neural attention should capture long-range interaction in such sequences. However, most existing neural attention models admit only short sequences, or they have to employ chunking or padding to enforce a constant input length. Here we propose a simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths. Each ChordMixer block consists of a position-wise rotation layer without learnable parameters and an element-wise MLP layer. Repeatedly applying such blocks forms an effective network backbone that mixes the input signals towards the learning targets. We have tested ChordMixer on the synthetic adding problem, long document classification, and DNA sequence-based taxonomy classification. The experiment results show that our method substantially outperforms other neural attention models.",
    "path": "papers/22/06/2206.05852.json",
    "total_tokens": 849,
    "translated_title": "ChordMixer：一种可扩展的神经注意力模型，适用于长度不同的序列",
    "translated_abstract": "在许多领域中，顺序数据自然具有不同的长度，有些序列非常长。作为重要的建模工具，神经注意力应该捕捉这种序列中的远距离交互。然而，大多数现有的神经注意力模型只能处理短序列，或者它们不得不使用分块或填充来强制输入长度保持恒定。我们在这里提出了一个简单的神经网络构建块，称为ChordMixer，它可以模拟具有可变长度的长序列的注意力。每个ChordMixer块由一个没有可学习参数的位置旋转层和一个逐元素MLP层组成。重复应用这些块形成了一个有效的网络骨干，将输入信号混合到学习目标中。我们在合成加问题、长文档分类和基于DNA序列的分类问题上测试了ChordMixer。实验结果表明，我们的方法明显优于其他神经注意力模型。",
    "tldr": "ChordMixer 提出了一个简单的神经网络建模组，能够处理具有可变长度的长序列的注意力，并在实验中表现出明显的优势。",
    "en_tdlr": "ChordMixer proposes a simple neural network building block that can model attention for long sequences with variable lengths and shows significant advantages over other neural attention models in experiments."
}