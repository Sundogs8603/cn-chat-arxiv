{
    "title": "Toward a realistic model of speech processing in the brain with self-supervised learning. (arXiv:2206.01685v2 [q-bio.NC] UPDATED)",
    "abstract": "Several deep neural networks have recently been shown to generate activations similar to those of the brain in response to the same input. These algorithms, however, remain largely implausible: they require (1) extraordinarily large amounts of data, (2) unobtainable supervised labels, (3) textual rather than raw sensory input, and / or (4) implausibly large memory (e.g. thousands of contextual words). These elements highlight the need to identify algorithms that, under these limitations, would suffice to account for both behavioral and brain responses. Focusing on the issue of speech processing, we here hypothesize that self-supervised algorithms trained on the raw waveform constitute a promising candidate. Specifically, we compare a recent self-supervised architecture, Wav2Vec 2.0, to the brain activity of 412 English, French, and Mandarin individuals recorded with functional Magnetic Resonance Imaging (fMRI), while they listened to ~1h of audio books. Our results are four-fold. First",
    "link": "http://arxiv.org/abs/2206.01685",
    "context": "Title: Toward a realistic model of speech processing in the brain with self-supervised learning. (arXiv:2206.01685v2 [q-bio.NC] UPDATED)\nAbstract: Several deep neural networks have recently been shown to generate activations similar to those of the brain in response to the same input. These algorithms, however, remain largely implausible: they require (1) extraordinarily large amounts of data, (2) unobtainable supervised labels, (3) textual rather than raw sensory input, and / or (4) implausibly large memory (e.g. thousands of contextual words). These elements highlight the need to identify algorithms that, under these limitations, would suffice to account for both behavioral and brain responses. Focusing on the issue of speech processing, we here hypothesize that self-supervised algorithms trained on the raw waveform constitute a promising candidate. Specifically, we compare a recent self-supervised architecture, Wav2Vec 2.0, to the brain activity of 412 English, French, and Mandarin individuals recorded with functional Magnetic Resonance Imaging (fMRI), while they listened to ~1h of audio books. Our results are four-fold. First",
    "path": "papers/22/06/2206.01685.json",
    "total_tokens": 1279,
    "translated_title": "利用自监督学习研究大脑中的语音处理机制",
    "translated_abstract": "最近研究发现一些深度神经网络对于输入的刺激反应能够与人脑神经元的活动十分相似。但这些算法存在数据量巨大、监督标签难以获取、只能接受文本输入以及需要高昂的存储资源等问题。这些限制表明需要寻找在这些限制下能够解释行为和大脑反应的算法。本研究专注于语音处理问题，假设基于原始波形的自监督算法成为一个有前景的候选方案。作者通过比较最近的自监督架构Wav2Vec 2.0和412名英语、法语和汉语听取约1小时音频书籍时的功能磁共振成像（fMRI）数据，展示了四个主要成果：首先，作者发现Wav2Vec 2.0和大脑神经元会将语音音频信息编码到类似时间变化的层级中。其次，作者证明这种对于音频层级的编码不是由于影响常规声音的表面因素导致。第三，作者还表明，与文本表示法或传统语音特征相比，通过无监督学习获得的特征可以更加准确地解释大脑活动的变化。最后，研究还展示了在最少的语言背景和理解知识资源下，可以通过自监督学习解释有意义的大脑活动。",
    "tldr": "本研究针对语音处理问题，使用自监督学习的方法得到的特征与大脑神经元对于语音刺激的反应能够形成类似的层级，且解释了大脑活动的变化。该算法能够最少依赖先验语言和理解知识资源，并且需要的数据量远小于其它模型。",
    "en_tdlr": "This study focuses on speech processing, and proposes that self-supervised algorithms trained on the raw waveform constitute a promising candidate for explaining both behavioral and brain responses under the limitations of data quantity, supervised labels, raw sensory input, and memory constraints. The results show that the features obtained by self-supervised learning can explain the brain activity more accurately, even with minimal linguistic and conceptual knowledge, and require much less data than other models. Moreover, the study reveals a similar hierarchy of temporal modulation in the encoding of speech acoustics between Wav2Vec 2.0 and the brain."
}