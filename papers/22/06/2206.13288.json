{
    "title": "Discovering Salient Neurons in Deep NLP Models. (arXiv:2206.13288v2 [cs.CL] UPDATED)",
    "abstract": "While a lot of work has been done in understanding representations learned within deep NLP models and what knowledge they capture, little attention has been paid towards individual neurons. We present a technique called as Linguistic Correlation Analysis to extract salient neurons in the model, with respect to any extrinsic property - with the goal of understanding how such a knowledge is preserved within neurons. We carry out a fine-grained analysis to answer the following questions: (i) can we identify subsets of neurons in the network that capture specific linguistic properties? (ii) how localized or distributed neurons are across the network? iii) how redundantly is the information preserved? iv) how fine-tuning pre-trained models towards downstream NLP tasks, impacts the learned linguistic knowledge? iv) how do architectures vary in learning different linguistic properties? Our data-driven, quantitative analysis illuminates interesting findings: (i) we found small subsets of neuro",
    "link": "http://arxiv.org/abs/2206.13288",
    "context": "Title: Discovering Salient Neurons in Deep NLP Models. (arXiv:2206.13288v2 [cs.CL] UPDATED)\nAbstract: While a lot of work has been done in understanding representations learned within deep NLP models and what knowledge they capture, little attention has been paid towards individual neurons. We present a technique called as Linguistic Correlation Analysis to extract salient neurons in the model, with respect to any extrinsic property - with the goal of understanding how such a knowledge is preserved within neurons. We carry out a fine-grained analysis to answer the following questions: (i) can we identify subsets of neurons in the network that capture specific linguistic properties? (ii) how localized or distributed neurons are across the network? iii) how redundantly is the information preserved? iv) how fine-tuning pre-trained models towards downstream NLP tasks, impacts the learned linguistic knowledge? iv) how do architectures vary in learning different linguistic properties? Our data-driven, quantitative analysis illuminates interesting findings: (i) we found small subsets of neuro",
    "path": "papers/22/06/2206.13288.json",
    "total_tokens": 911,
    "translated_title": "深度NLP模型中突出神经元的发现",
    "translated_abstract": "在理解深度NLP模型中学到的表示和它们所捕捉到的知识方面，已经做了很多工作，但对于个别神经元的关注很少。我们提出了一种称为语言相关分析的技术来提取模型中的突出神经元，以了解这种知识在神经元中是如何保留的。我们进行了精细的分析来回答以下问题：（i）我们能否识别出网络中捕捉特定语言属性的神经元子集？（ii）神经元在网络中是如何分布的？（iii）信息保存得有多冗余？（iv）通过将预训练模型调整到下游的NLP任务，如何影响所学的语言知识？（v）不同的架构在学习不同的语言属性方面有何不同？我们的数据驱动、定量分析揭示了一些有趣的发现：",
    "tldr": "使用语言相关分析技术，我们发现在深度NLP模型中存在一些突出的神经元，这些神经元能够捕捉特定的语言属性，而且信息保存冗余程度较高。调整预训练模型对下游NLP任务的影响以及不同架构学习不同语言属性的差异也得到了研究。",
    "en_tdlr": "Using Linguistic Correlation Analysis, we discovered salient neurons in deep NLP models that capture specific linguistic properties with high redundancy in information preservation. The impact of fine-tuning pre-trained models on downstream NLP tasks and the differences in learning different linguistic properties across architectures have also been investigated."
}