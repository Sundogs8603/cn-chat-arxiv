{
    "title": "Guillotine Regularization: Why removing layers is needed to improve generalization in Self-Supervised Learning. (arXiv:2206.13378v2 [cs.LG] UPDATED)",
    "abstract": "One unexpected technique that emerged in recent years consists in training a Deep Network (DN) with a Self-Supervised Learning (SSL) method, and using this network on downstream tasks but with its last few projector layers entirely removed. This trick of throwing away the projector is actually critical for SSL methods to display competitive performances on ImageNet for which more than 30 percentage points can be gained that way. This is a little vexing, as one would hope that the network layer at which invariance is explicitly enforced by the SSL criterion during training (the last projector layer) should be the one to use for best generalization performance downstream. But it seems not to be, and this study sheds some light on why. This trick, which we name Guillotine Regularization (GR), is in fact a generically applicable method that has been used to improve generalization performance in transfer learning scenarios. In this work, we identify the underlying reasons behind its success",
    "link": "http://arxiv.org/abs/2206.13378",
    "context": "Title: Guillotine Regularization: Why removing layers is needed to improve generalization in Self-Supervised Learning. (arXiv:2206.13378v2 [cs.LG] UPDATED)\nAbstract: One unexpected technique that emerged in recent years consists in training a Deep Network (DN) with a Self-Supervised Learning (SSL) method, and using this network on downstream tasks but with its last few projector layers entirely removed. This trick of throwing away the projector is actually critical for SSL methods to display competitive performances on ImageNet for which more than 30 percentage points can be gained that way. This is a little vexing, as one would hope that the network layer at which invariance is explicitly enforced by the SSL criterion during training (the last projector layer) should be the one to use for best generalization performance downstream. But it seems not to be, and this study sheds some light on why. This trick, which we name Guillotine Regularization (GR), is in fact a generically applicable method that has been used to improve generalization performance in transfer learning scenarios. In this work, we identify the underlying reasons behind its success",
    "path": "papers/22/06/2206.13378.json",
    "total_tokens": 849,
    "translated_title": "断头台正则化：为什么在自监督学习中需要移除网络层以提高泛化性能",
    "translated_abstract": "近年来出现了一种出人意料的技术，即使用自监督学习方法训练深度神经网络（DN），并在下游任务中使用这个网络，但将其最后几个投影层完全去除。对于在ImageNet上展现竞争性能的自监督学习方法，这个去除投影器的技巧实际上是至关重要的，这样可以获得超过30个百分点的性能提升。然而，这个技巧似乎与在自监督训练中显式强制执行不变性的最后一个投影层不符，我们称之为Guillotine Regularization（GR），这是一种通用的方法，已用于改善转移学习情况下的泛化性能，并在这项工作中，我们确定了其成功背后的原因。",
    "tldr": "这篇论文研究了自监督学习领域中的一种通用技巧——通过移除网络层来改善泛化性能，并解释了为什么这种技巧能够奏效。",
    "en_tdlr": "This paper investigates a generic technique in self-supervised learning called Guillotine Regularization, which improves generalization performance by removing layers from deep neural networks, and explains why this technique is effective."
}