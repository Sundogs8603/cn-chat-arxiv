{
    "title": "Evaluating Generative Patent Language Models. (arXiv:2206.14578v2 [cs.CL] UPDATED)",
    "abstract": "Generative language models are promising for assisting human writing in various domains. This manuscript aims to build generative language models in the patent domain and evaluate model performance from a human-centric perspective. The perspective is to measure the ratio of keystrokes that can be saved by autocompletion based on generative patent language models. A higher ratio means a more effective model which can save more keystrokes. This metric can be used to benchmark model performance. The metric is different from conventional machine-centric metrics that are token-based instead of keystroke-based. In terms of model size, the largest model built in this manuscript is 6B, which is state-of-the-art in the patent domain. Based on the metric, it is found that the largest model is not necessarily the best for the human-centric metric. The finding means that keeping increasing model sizes in the patent domain might be unnecessary if the purpose is to assist human writing with autocomp",
    "link": "http://arxiv.org/abs/2206.14578",
    "context": "Title: Evaluating Generative Patent Language Models. (arXiv:2206.14578v2 [cs.CL] UPDATED)\nAbstract: Generative language models are promising for assisting human writing in various domains. This manuscript aims to build generative language models in the patent domain and evaluate model performance from a human-centric perspective. The perspective is to measure the ratio of keystrokes that can be saved by autocompletion based on generative patent language models. A higher ratio means a more effective model which can save more keystrokes. This metric can be used to benchmark model performance. The metric is different from conventional machine-centric metrics that are token-based instead of keystroke-based. In terms of model size, the largest model built in this manuscript is 6B, which is state-of-the-art in the patent domain. Based on the metric, it is found that the largest model is not necessarily the best for the human-centric metric. The finding means that keeping increasing model sizes in the patent domain might be unnecessary if the purpose is to assist human writing with autocomp",
    "path": "papers/22/06/2206.14578.json",
    "total_tokens": 827,
    "translated_title": "评估生成专利语言模型",
    "translated_abstract": "生成性语言模型在各个领域辅助人类写作方面具有很大的潜力。本文旨在构建专利领域的生成性语言模型，并从人类的角度评估模型的表现。我们通过度量基于生成性专利语言模型的自动补全所能节省的按键比率来衡量模型的表现，这种以按键为基础的度量方法不同于传统的基于标记的机器-centric的度量方法。本文中构建的最大模型大小为6B，在专利领域是最先进的。基于此度量方法，我们发现最大的模型并不一定是人类-centric度量方法的最佳选择，这意味着如果目的是通过自动补全来辅助人类写作，那么在专利领域继续增加模型大小可能是不必要的。",
    "tldr": "本文构建了生成性专利语言模型，并通过基于人类节省按键数量的度量方法评估了其表现，发现在专利领域继续增加模型大小可能是不必要的。",
    "en_tdlr": "This paper builds generative patent language models and evaluates their performance from a human-centric perspective by measuring the ratio of keystrokes saved by autocompletion. The finding suggests that increasing model size in the patent domain may not be necessary to assist human writing with autocompletion."
}