{
    "title": "FreeKD: Free-direction Knowledge Distillation for Graph Neural Networks. (arXiv:2206.06561v4 [cs.LG] UPDATED)",
    "abstract": "Knowledge distillation (KD) has demonstrated its effectiveness to boost the performance of graph neural networks (GNNs), where its goal is to distill knowledge from a deeper teacher GNN into a shallower student GNN. However, it is actually difficult to train a satisfactory teacher GNN due to the well-known over-parametrized and over-smoothing issues, leading to invalid knowledge transfer in practical applications. In this paper, we propose the first Free-direction Knowledge Distillation framework via Reinforcement learning for GNNs, called FreeKD, which is no longer required to provide a deeper well-optimized teacher GNN. The core idea of our work is to collaboratively build two shallower GNNs in an effort to exchange knowledge between them via reinforcement learning in a hierarchical way. As we observe that one typical GNN model often has better and worse performances at different nodes during training, we devise a dynamic and free-direction knowledge transfer strategy that consists o",
    "link": "http://arxiv.org/abs/2206.06561",
    "context": "Title: FreeKD: Free-direction Knowledge Distillation for Graph Neural Networks. (arXiv:2206.06561v4 [cs.LG] UPDATED)\nAbstract: Knowledge distillation (KD) has demonstrated its effectiveness to boost the performance of graph neural networks (GNNs), where its goal is to distill knowledge from a deeper teacher GNN into a shallower student GNN. However, it is actually difficult to train a satisfactory teacher GNN due to the well-known over-parametrized and over-smoothing issues, leading to invalid knowledge transfer in practical applications. In this paper, we propose the first Free-direction Knowledge Distillation framework via Reinforcement learning for GNNs, called FreeKD, which is no longer required to provide a deeper well-optimized teacher GNN. The core idea of our work is to collaboratively build two shallower GNNs in an effort to exchange knowledge between them via reinforcement learning in a hierarchical way. As we observe that one typical GNN model often has better and worse performances at different nodes during training, we devise a dynamic and free-direction knowledge transfer strategy that consists o",
    "path": "papers/22/06/2206.06561.json",
    "total_tokens": 976,
    "translated_title": "FreeKD：用于图神经网络的自由方向知识蒸馏",
    "translated_abstract": "知识蒸馏已经证明了一定的效果，可以提高图神经网络的性能，其目标是将来自深度教师GNN的知识浓缩到较浅的学生GNN中。然而，由于已知的过度参数化和过度平滑问题，构建一个令人满意的深层教师GNN实际上是困难的，这导致在实际应用中的无效知识传递。在本文中，我们提出了第一个自由方向知识蒸馏框架FreeKD，通过强化学习针对GNN，其不再需要提供深度优化的教师GNN。我们的核心思想是通过强化学习以分层方式协作地构建两个较浅的GNN，以在它们之间交换知识。我们观察到，一个典型的GNN模型通常在训练过程中在不同节点有较好和较差的表现，因此设计了一种动态和自由方向的知识转移策略，包括〇",
    "tldr": "本文提出了一种称为FreeKD的自由方向知识蒸馏框架，通过强化学习针对图神经网络构建两个较浅的GNN，使其在它们之间交换知识，无需提供深度优化的教师GNN，并且采用动态和自由方向的知识转移策略。",
    "en_tdlr": "This paper proposes a Free-direction Knowledge Distillation framework, called FreeKD, for graph neural networks, which collaboratively builds two shallower GNNs via reinforcement learning to exchange knowledge between them, without requiring a well-optimized teacher GNN, and adopts a dynamic and free-direction knowledge transfer strategy."
}