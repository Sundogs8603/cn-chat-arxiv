{
    "title": "LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models. (arXiv:2206.09557v3 [cs.DC] UPDATED)",
    "abstract": "The recent advancements in self-supervised learning, combined with the Transformer architecture, have enabled natural language processing (NLP) to achieve remarkably low perplexity. However, powerful NLP models necessitate increasing model size, leading to substantial computational and memory requirements. In this paper, we introduce an efficient inference framework tailored for large-scale generative language models. To reduce the model size, we employ a weight-only quantization strategy while preserving full precision for activations. As a result, we attain sub-4-bit quantization for each weight through non-uniform or uniform quantization techniques. Our proposed kernel, called LUT-GEMM, then accelerates quantized matrix multiplications, offering a flexible balance between compression ratio and accuracy. Unlike earlier matrix multiplication kernels that accommodated weight-only quantization, LUT-GEMM efficiently eliminates the resource-demanding dequantization process for both unifor",
    "link": "http://arxiv.org/abs/2206.09557",
    "context": "Title: LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models. (arXiv:2206.09557v3 [cs.DC] UPDATED)\nAbstract: The recent advancements in self-supervised learning, combined with the Transformer architecture, have enabled natural language processing (NLP) to achieve remarkably low perplexity. However, powerful NLP models necessitate increasing model size, leading to substantial computational and memory requirements. In this paper, we introduce an efficient inference framework tailored for large-scale generative language models. To reduce the model size, we employ a weight-only quantization strategy while preserving full precision for activations. As a result, we attain sub-4-bit quantization for each weight through non-uniform or uniform quantization techniques. Our proposed kernel, called LUT-GEMM, then accelerates quantized matrix multiplications, offering a flexible balance between compression ratio and accuracy. Unlike earlier matrix multiplication kernels that accommodated weight-only quantization, LUT-GEMM efficiently eliminates the resource-demanding dequantization process for both unifor",
    "path": "papers/22/06/2206.09557.json",
    "total_tokens": 954,
    "translated_title": "LUT-GEMM：基于LUT的量化矩阵乘法，用于大规模生成式语言模型的高效推理",
    "translated_abstract": "最近自监督学习的先进技术与Transformer架构的结合极大地提高了自然语言处理（NLP）的性能。然而，强大的NLP模型需要越来越大的模型尺寸，从而导致大量的计算和内存需求。本文介绍了一种针对大规模生成式语言模型的高效推理框架。为了减少模型大小，我们采用了一种仅针对权重的量化策略，同时保留了激活函数的完整精度。因此，我们通过非均匀或均匀量化技术获得每个权重的低于4位的量化。我们提出的LUT-GEMM内核加速了量化矩阵乘法，提供了压缩比和准确性之间的灵活平衡。与早期仅适用于权重量化的矩阵乘法内核不同，LUT-GEMM有效地消除了资源消耗更大的反量化过程。",
    "tldr": "本文介绍了一种基于LUT的量化矩阵乘法，用于大规模生成式语言模型的高效推理。采用仅针对权重的量化策略，并提出了LUT-GEMM内核加速量化矩阵乘法，实现压缩比和准确性之间的灵活平衡。",
    "en_tdlr": "This paper introduces an efficient inference framework based on LUT-quantized matrix multiplication for large-scale generative language models. With a weight-only quantization strategy and proposed LUT-GEMM kernel, the framework achieves flexible balance between compression ratio and accuracy."
}