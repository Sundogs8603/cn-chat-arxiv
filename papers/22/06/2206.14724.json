{
    "title": "Private Graph Extraction via Feature Explanations. (arXiv:2206.14724v2 [cs.LG] UPDATED)",
    "abstract": "Privacy and interpretability are two important ingredients for achieving trustworthy machine learning. We study the interplay of these two aspects in graph machine learning through graph reconstruction attacks. The goal of the adversary here is to reconstruct the graph structure of the training data given access to model explanations. Based on the different kinds of auxiliary information available to the adversary, we propose several graph reconstruction attacks. We show that additional knowledge of post-hoc feature explanations substantially increases the success rate of these attacks. Further, we investigate in detail the differences between attack performance with respect to three different classes of explanation methods for graph neural networks: gradient-based, perturbation-based, and surrogate model-based methods. While gradient-based explanations reveal the most in terms of the graph structure, we find that these explanations do not always score high in utility. For the other tw",
    "link": "http://arxiv.org/abs/2206.14724",
    "context": "Title: Private Graph Extraction via Feature Explanations. (arXiv:2206.14724v2 [cs.LG] UPDATED)\nAbstract: Privacy and interpretability are two important ingredients for achieving trustworthy machine learning. We study the interplay of these two aspects in graph machine learning through graph reconstruction attacks. The goal of the adversary here is to reconstruct the graph structure of the training data given access to model explanations. Based on the different kinds of auxiliary information available to the adversary, we propose several graph reconstruction attacks. We show that additional knowledge of post-hoc feature explanations substantially increases the success rate of these attacks. Further, we investigate in detail the differences between attack performance with respect to three different classes of explanation methods for graph neural networks: gradient-based, perturbation-based, and surrogate model-based methods. While gradient-based explanations reveal the most in terms of the graph structure, we find that these explanations do not always score high in utility. For the other tw",
    "path": "papers/22/06/2206.14724.json",
    "total_tokens": 911,
    "translated_title": "通过特征解释实现私有图提取",
    "translated_abstract": "隐私和可解释性是实现可信机器学习的两个重要因素。我们通过图重构攻击研究了这两个方面在图机器学习中的相互作用。攻击者的目标是在访问模型解释的情况下重建训练数据的图结构。根据攻击者可用的不同辅助信息，我们提出了几种图重建攻击方法。我们发现，对于后续特征解释的附加知识显著提高了这些攻击的成功率。此外，我们详细研究了针对图神经网络的三种不同解释方法（基于梯度、扰动和替代模型的方法）在攻击性能方面的差异。尽管基于梯度的解释在揭示图结构方面最为出色，但我们发现这些解释在效用方面并不总是得分较高。其他两种方法的效果较好。",
    "tldr": "本论文研究了隐私与可解释性在图机器学习中的相互作用，提出了几种图重构攻击方法，并发现后续特征解释对攻击的成功率有显著影响。对于图神经网络的不同解释方法，基于梯度的解释揭示了最多的图结构信息，但效用并不总是最高。",
    "en_tdlr": "This paper investigates the interplay of privacy and interpretability in graph machine learning, proposes several graph reconstruction attacks, and finds that post-hoc feature explanations significantly impact the success rate of these attacks. Among different explanation methods for graph neural networks, gradient-based explanations reveal the most about the graph structure, but do not always score highest in utility."
}