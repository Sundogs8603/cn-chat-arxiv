{
    "title": "Tensor-on-Tensor Regression: Riemannian Optimization, Over-parameterization, Statistical-computational Gap, and Their Interplay. (arXiv:2206.08756v3 [math.ST] UPDATED)",
    "abstract": "We study the tensor-on-tensor regression, where the goal is to connect tensor responses to tensor covariates with a low Tucker rank parameter tensor/matrix without the prior knowledge of its intrinsic rank. We propose the Riemannian gradient descent (RGD) and Riemannian Gauss-Newton (RGN) methods and cope with the challenge of unknown rank by studying the effect of rank over-parameterization. We provide the first convergence guarantee for the general tensor-on-tensor regression by showing that RGD and RGN respectively converge linearly and quadratically to a statistically optimal estimate in both rank correctly-parameterized and over-parameterized settings. Our theory reveals an intriguing phenomenon: Riemannian optimization methods naturally adapt to over-parameterization without modifications to their implementation. We also prove the statistical-computational gap in scalar-on-tensor regression by a direct low-degree polynomial argument. Our theory demonstrates a \"blessing of statist",
    "link": "http://arxiv.org/abs/2206.08756",
    "context": "Title: Tensor-on-Tensor Regression: Riemannian Optimization, Over-parameterization, Statistical-computational Gap, and Their Interplay. (arXiv:2206.08756v3 [math.ST] UPDATED)\nAbstract: We study the tensor-on-tensor regression, where the goal is to connect tensor responses to tensor covariates with a low Tucker rank parameter tensor/matrix without the prior knowledge of its intrinsic rank. We propose the Riemannian gradient descent (RGD) and Riemannian Gauss-Newton (RGN) methods and cope with the challenge of unknown rank by studying the effect of rank over-parameterization. We provide the first convergence guarantee for the general tensor-on-tensor regression by showing that RGD and RGN respectively converge linearly and quadratically to a statistically optimal estimate in both rank correctly-parameterized and over-parameterized settings. Our theory reveals an intriguing phenomenon: Riemannian optimization methods naturally adapt to over-parameterization without modifications to their implementation. We also prove the statistical-computational gap in scalar-on-tensor regression by a direct low-degree polynomial argument. Our theory demonstrates a \"blessing of statist",
    "path": "papers/22/06/2206.08756.json",
    "total_tokens": 958,
    "translated_title": "张量对张量回归: 黎曼优化，过参数化，统计计算差异及其相互作用",
    "translated_abstract": "我们研究了张量对张量回归问题，其目标是在不知道其内在秩的情况下，将张量响应与张量协变量相连接，通过低Tucker秩参数张量/矩阵。我们提出了黎曼梯度下降（RGD）和黎曼高斯牛顿（RGN）方法，并通过研究秩过参数化的影响来应对未知秩的挑战。我们首次提供了关于一般张量对张量回归的收敛性保证，表明RGD和RGN分别在正确参数化和过参数化设置下线性和二次收敛到统计上的最优估计。我们的理论揭示了一个有趣的现象：黎曼优化方法在不修改实现方式的情况下自然地适应过参数化。我们还通过直接的低次多项式论证证明了标量对张量回归中的统计计算差异。我们的理论证明了统计学的福音。",
    "tldr": "张量对张量回归问题中，我们提出了黎曼优化方法和秩过参数化的研究，并展示了黎曼优化方法的线性和二次收敛性以及适应过参数化的能力。同时，我们证明了标量对张量回归中的统计计算差异。"
}