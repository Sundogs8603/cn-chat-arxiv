{
    "title": "Boosting Cross-Domain Speech Recognition with Self-Supervision. (arXiv:2206.09783v2 [eess.AS] UPDATED)",
    "abstract": "The cross-domain performance of automatic speech recognition (ASR) could be severely hampered due to the mismatch between training and testing distributions. Since the target domain usually lacks labeled data, and domain shifts exist at acoustic and linguistic levels, it is challenging to perform unsupervised domain adaptation (UDA) for ASR. Previous work has shown that self-supervised learning (SSL) or pseudo-labeling (PL) is effective in UDA by exploiting the self-supervisions of unlabeled data. However, these self-supervisions also face performance degradation in mismatched domain distributions, which previous work fails to address. This work presents a systematic UDA framework to fully utilize the unlabeled data with self-supervision in the pre-training and fine-tuning paradigm. On the one hand, we apply continued pre-training and data replay techniques to mitigate the domain mismatch of the SSL pre-trained model. On the other hand, we propose a domain-adaptive fine-tuning approach",
    "link": "http://arxiv.org/abs/2206.09783",
    "context": "Title: Boosting Cross-Domain Speech Recognition with Self-Supervision. (arXiv:2206.09783v2 [eess.AS] UPDATED)\nAbstract: The cross-domain performance of automatic speech recognition (ASR) could be severely hampered due to the mismatch between training and testing distributions. Since the target domain usually lacks labeled data, and domain shifts exist at acoustic and linguistic levels, it is challenging to perform unsupervised domain adaptation (UDA) for ASR. Previous work has shown that self-supervised learning (SSL) or pseudo-labeling (PL) is effective in UDA by exploiting the self-supervisions of unlabeled data. However, these self-supervisions also face performance degradation in mismatched domain distributions, which previous work fails to address. This work presents a systematic UDA framework to fully utilize the unlabeled data with self-supervision in the pre-training and fine-tuning paradigm. On the one hand, we apply continued pre-training and data replay techniques to mitigate the domain mismatch of the SSL pre-trained model. On the other hand, we propose a domain-adaptive fine-tuning approach",
    "path": "papers/22/06/2206.09783.json",
    "total_tokens": 906,
    "translated_title": "通过自监督提升跨领域语音识别能力",
    "translated_abstract": "自动语音识别（ASR）的跨领域性能可能受到训练和测试分布之间的不匹配的严重阻碍。由于目标领域通常缺乏标记数据，并且声学和语言层面存在领域转移，因此对ASR进行无监督领域自适应（UDA）是具有挑战性的。先前的研究表明，通过利用无标签数据的自监督学习（SSL）或伪标签（PL），可以在UDA中发挥作用。然而，这些自监督也面临着不匹配领域分布的性能降低问题，这是之前的研究没有解决的。本文提出了一个系统的UDA框架，以充分利用预训练和微调范式中的无标签数据的自监督。一方面，我们应用了持续的预训练和数据重放技术来减轻SSL预训练模型的领域不匹配。另一方面，我们提出了一种领域自适应微调方法。",
    "tldr": "本研究提出了一个系统的无监督领域自适应框架，通过使用自监督和伪标签方法来提升跨领域语音识别的性能。",
    "en_tdlr": "This paper presents a systematic framework for unsupervised domain adaptation in automatic speech recognition (ASR) by utilizing self-supervised learning and pseudo-labeling methods to improve cross-domain performance."
}