{
    "title": "Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent. (arXiv:2206.02617v6 [cs.LG] UPDATED)",
    "abstract": "Differentially private stochastic gradient descent (DP-SGD) is the workhorse algorithm for recent advances in private deep learning. It provides a single privacy guarantee to all datapoints in the dataset. We propose output-specific $(\\varepsilon,\\delta)$-DP to characterize privacy guarantees for individual examples when releasing models trained by DP-SGD. We also design an efficient algorithm to investigate individual privacy across a number of datasets. We find that most examples enjoy stronger privacy guarantees than the worst-case bound. We further discover that the training loss and the privacy parameter of an example are well-correlated. This implies groups that are underserved in terms of model utility simultaneously experience weaker privacy guarantees. For example, on CIFAR-10, the average $\\varepsilon$ of the class with the lowest test accuracy is 44.2\\% higher than that of the class with the highest accuracy.",
    "link": "http://arxiv.org/abs/2206.02617",
    "context": "Title: Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent. (arXiv:2206.02617v6 [cs.LG] UPDATED)\nAbstract: Differentially private stochastic gradient descent (DP-SGD) is the workhorse algorithm for recent advances in private deep learning. It provides a single privacy guarantee to all datapoints in the dataset. We propose output-specific $(\\varepsilon,\\delta)$-DP to characterize privacy guarantees for individual examples when releasing models trained by DP-SGD. We also design an efficient algorithm to investigate individual privacy across a number of datasets. We find that most examples enjoy stronger privacy guarantees than the worst-case bound. We further discover that the training loss and the privacy parameter of an example are well-correlated. This implies groups that are underserved in terms of model utility simultaneously experience weaker privacy guarantees. For example, on CIFAR-10, the average $\\varepsilon$ of the class with the lowest test accuracy is 44.2\\% higher than that of the class with the highest accuracy.",
    "path": "papers/22/06/2206.02617.json",
    "total_tokens": 959,
    "translated_title": "个体隐私会计对差分隐私随机梯度下降的影响",
    "translated_abstract": "差分隐私随机梯度下降是最近私有深度学习的前沿算法。它为数据集中的所有数据点提供了单一的隐私保证。我们提出了针对个例的输出特定$(\\varepsilon,\\delta)$-DP，以刻画通过DP-SGD训练的模型对个别示例的隐私保证。我们还设计了一种高效算法来研究跨多个数据集的个体隐私。我们发现大多数示例都享有比最坏情况边界更强的隐私保证。我们进一步发现训练损失和示例的隐私参数之间存在很强的相关性。这意味着在模型效用方面受到不足的群体同时经历较弱的隐私保证。例如，在CIFAR-10上，最低测试准确率类别的平均$\\varepsilon$比最高准确率类别高44.2%。",
    "tldr": "本文研究了通过差分隐私随机梯度下降训练的模型对个体示例的隐私保证，并发现大多数示例享有较强的隐私保证。此外，我们还发现训练损失和示例的隐私参数存在很强的相关性。最低准确率类别的平均隐私参数比最高准确率类别高44.2%。",
    "en_tdlr": "This study investigates the individual privacy guarantees of models trained by differentially private stochastic gradient descent (DP-SGD) and finds that most examples enjoy strong privacy guarantees. The study also discovers a strong correlation between the training loss and the privacy parameter of examples. The average privacy parameter of the class with the lowest test accuracy is 44.2% higher than that of the class with the highest accuracy."
}