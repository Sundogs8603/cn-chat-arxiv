{
    "title": "ResAct: Reinforcing Long-term Engagement in Sequential Recommendation with Residual Actor. (arXiv:2206.02620v2 [cs.IR] UPDATED)",
    "abstract": "Long-term engagement is preferred over immediate engagement in sequential recommendation as it directly affects product operational metrics such as daily active users (DAUs) and dwell time. Meanwhile, reinforcement learning (RL) is widely regarded as a promising framework for optimizing long-term engagement in sequential recommendation. However, due to expensive online interactions, it is very difficult for RL algorithms to perform state-action value estimation, exploration and feature extraction when optimizing long-term engagement. In this paper, we propose ResAct which seeks a policy that is close to, but better than, the online-serving policy. In this way, we can collect sufficient data near the learned policy so that state-action values can be properly estimated, and there is no need to perform online exploration. ResAct optimizes the policy by first reconstructing the online behaviors and then improving it via a Residual Actor. To extract long-term information, ResAct utilizes tw",
    "link": "http://arxiv.org/abs/2206.02620",
    "context": "Title: ResAct: Reinforcing Long-term Engagement in Sequential Recommendation with Residual Actor. (arXiv:2206.02620v2 [cs.IR] UPDATED)\nAbstract: Long-term engagement is preferred over immediate engagement in sequential recommendation as it directly affects product operational metrics such as daily active users (DAUs) and dwell time. Meanwhile, reinforcement learning (RL) is widely regarded as a promising framework for optimizing long-term engagement in sequential recommendation. However, due to expensive online interactions, it is very difficult for RL algorithms to perform state-action value estimation, exploration and feature extraction when optimizing long-term engagement. In this paper, we propose ResAct which seeks a policy that is close to, but better than, the online-serving policy. In this way, we can collect sufficient data near the learned policy so that state-action values can be properly estimated, and there is no need to perform online exploration. ResAct optimizes the policy by first reconstructing the online behaviors and then improving it via a Residual Actor. To extract long-term information, ResAct utilizes tw",
    "path": "papers/22/06/2206.02620.json",
    "total_tokens": 874,
    "tldr": "本文提出了ResAct算法，通过重构在线行为和利用残差Actor进行改进，来优化长期参与的顺序推荐问题。算法可以避免昂贵的在线交互，并且能够正确估计状态-动作值。"
}