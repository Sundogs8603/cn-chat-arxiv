{
    "title": "FixEval: Execution-based Evaluation of Program Fixes for Programming Problems. (arXiv:2206.07796v4 [cs.SE] UPDATED)",
    "abstract": "The complexity of modern software has led to a drastic increase in the time and cost associated with detecting and rectifying software bugs. In response, researchers have explored various methods to automatically generate fixes for buggy code. However, due to the large combinatorial space of possible fixes for any given bug, few tools and datasets are available to evaluate model-generated fixes effectively. To address this issue, we introduce FixEval, a benchmark comprising of buggy code submissions to competitive programming problems and their corresponding fixes. FixEval offers an extensive collection of unit tests to evaluate the correctness of model-generated program fixes and assess further information regarding time, memory constraints, and acceptance based on a verdict. We consider two Transformer language models pretrained on programming languages as our baseline and compare them using match-based and execution-based evaluation metrics. Our experiments show that match-based met",
    "link": "http://arxiv.org/abs/2206.07796",
    "context": "Title: FixEval: Execution-based Evaluation of Program Fixes for Programming Problems. (arXiv:2206.07796v4 [cs.SE] UPDATED)\nAbstract: The complexity of modern software has led to a drastic increase in the time and cost associated with detecting and rectifying software bugs. In response, researchers have explored various methods to automatically generate fixes for buggy code. However, due to the large combinatorial space of possible fixes for any given bug, few tools and datasets are available to evaluate model-generated fixes effectively. To address this issue, we introduce FixEval, a benchmark comprising of buggy code submissions to competitive programming problems and their corresponding fixes. FixEval offers an extensive collection of unit tests to evaluate the correctness of model-generated program fixes and assess further information regarding time, memory constraints, and acceptance based on a verdict. We consider two Transformer language models pretrained on programming languages as our baseline and compare them using match-based and execution-based evaluation metrics. Our experiments show that match-based met",
    "path": "papers/22/06/2206.07796.json",
    "total_tokens": 1015,
    "translated_title": "FixEval: 针对编程问题的程序修复的执行评估",
    "translated_abstract": "现代软件的复杂性导致了检测和纠正软件 Bug 所需的时间和成本急剧增加。为此，研究人员探索了各种方法来自动生成有缺陷代码的修复程序。然而，由于给定任何 Bug 可能的修复的组合空间很大，因此很少有工具和数据集可用于有效评估生成的修复。为了解决这个问题，我们介绍了 FixEval，这是一个由竞技编程问题的有缺陷代码提交和它们对应的修复组成的基准。FixEval 提供了大量的单元测试来评估模型生成的程序修复的正确性，并根据判决进行时间、内存限制和接受性的进一步信息评估。我们将两个编程语言的 Transformer 语言模型作为基线，并使用基于匹配和执行的评估指标进行比较。我们的实验表明，基于匹配的指标可能会提供误导性的程序修复评估，而基于执行的指标更准确，有助于识别生成的修复的弱点。",
    "tldr": "本文介绍了 FixEval，这是一个由竞技编程问题的有缺陷代码提交和它们对应的修复组成的基准。FixEval 提供了大量的单元测试来评估模型生成的程序修复的正确性，并根据判决进行时间、内存限制和接受性的进一步信息评估。基于实验结果，我们发现基于执行的指标更准确，有助于识别生成的修复的弱点。",
    "en_tdlr": "This paper introduces FixEval, a benchmark comprising of buggy code submissions to competitive programming problems and their corresponding fixes. FixEval offers an extensive collection of unit tests to evaluate the correctness of model-generated program fixes and assess further information regarding time, memory constraints, and acceptance based on a verdict. Based on experimental results, the paper concludes that execution-based evaluation metrics are more accurate and helpful in identifying the weaknesses of generated fixes."
}