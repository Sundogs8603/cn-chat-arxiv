{
    "title": "Spatial Entropy as an Inductive Bias for Vision Transformers. (arXiv:2206.04636v3 [cs.CV] UPDATED)",
    "abstract": "Recent work on Vision Transformers (VTs) showed that introducing a local inductive bias in the VT architecture helps reducing the number of samples necessary for training. However, the architecture modifications lead to a loss of generality of the Transformer backbone, partially contradicting the push towards the development of uniform architectures, shared, e.g., by both the Computer Vision and the Natural Language Processing areas. In this work, we propose a different and complementary direction, in which a local bias is introduced using an auxiliary self-supervised task, performed jointly with standard supervised training. Specifically, we exploit the observation that the attention maps of VTs, when trained with self-supervision, can contain a semantic segmentation structure which does not spontaneously emerge when training is supervised. Thus, we explicitly encourage the emergence of this spatial clustering as a form of training regularization. In more detail, we exploit the assump",
    "link": "http://arxiv.org/abs/2206.04636",
    "context": "Title: Spatial Entropy as an Inductive Bias for Vision Transformers. (arXiv:2206.04636v3 [cs.CV] UPDATED)\nAbstract: Recent work on Vision Transformers (VTs) showed that introducing a local inductive bias in the VT architecture helps reducing the number of samples necessary for training. However, the architecture modifications lead to a loss of generality of the Transformer backbone, partially contradicting the push towards the development of uniform architectures, shared, e.g., by both the Computer Vision and the Natural Language Processing areas. In this work, we propose a different and complementary direction, in which a local bias is introduced using an auxiliary self-supervised task, performed jointly with standard supervised training. Specifically, we exploit the observation that the attention maps of VTs, when trained with self-supervision, can contain a semantic segmentation structure which does not spontaneously emerge when training is supervised. Thus, we explicitly encourage the emergence of this spatial clustering as a form of training regularization. In more detail, we exploit the assump",
    "path": "papers/22/06/2206.04636.json",
    "total_tokens": 885,
    "translated_title": "空间熵作为视觉Transformers的归纳偏置",
    "translated_abstract": "近期关于视觉Transformers（VT）的研究表明，引入VT架构中的局部归纳偏置有助于减少训练所需的样本数量。然而，架构修改会导致Transformer骨干的通用性的损失，这部分违背了推动开发像计算机视觉和自然语言处理领域共享的统一架构的趋势。本文提出了一个不同的补充方向，即使用辅助自监督任务来引入局部偏置，同时与标准的有监督训练一起进行。具体来说，我们利用了VT的注意力映射在进行自监督训练时可能包含的语义分割结构，在有监督训练时不会自动出现。因此，我们明确地鼓励空间聚类作为训练正则化的一种形式。更详细地说，我们利用了假设",
    "tldr": "本文提出在视觉Transformers中引入局部偏置的不同方法，使用一个自监督任务来鼓励空间聚类作为训练正则化。通过利用注意力映射的语义分割结构，可以有效地减少样本数量，提高性能。",
    "en_tdlr": "This paper proposes a different approach to introducing local bias in Vision Transformers by using an auxiliary self-supervised task to encourage spatial clustering as a form of training regularization. By exploiting the semantic segmentation structure in attention maps, it effectively reduces the number of samples required for training and improves performance."
}