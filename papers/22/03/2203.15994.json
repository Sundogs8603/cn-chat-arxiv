{
    "title": "Optimal Learning. (arXiv:2203.15994v2 [cs.LG] UPDATED)",
    "abstract": "This paper studies the problem of learning an unknown function $f$ from given data about $f$. The learning problem is to give an approximation $\\hat f$ to $f$ that predicts the values of $f$ away from the data. There are numerous settings for this learning problem depending on (i) what additional information we have about $f$ (known as a model class assumption), (ii) how we measure the accuracy of how well $\\hat f$ predicts $f$, (iii) what is known about the data and data sites, (iv) whether the data observations are polluted by noise. A mathematical description of the optimal performance possible (the smallest possible error of recovery) is known in the presence of a model class assumption. Under standard model class assumptions, it is shown in this paper that a near optimal $\\hat f$ can be found by solving a certain discrete over-parameterized optimization problem with a penalty term. Here, near optimal means that the error is bounded by a fixed constant times the optimal error. This",
    "link": "http://arxiv.org/abs/2203.15994",
    "context": "Title: Optimal Learning. (arXiv:2203.15994v2 [cs.LG] UPDATED)\nAbstract: This paper studies the problem of learning an unknown function $f$ from given data about $f$. The learning problem is to give an approximation $\\hat f$ to $f$ that predicts the values of $f$ away from the data. There are numerous settings for this learning problem depending on (i) what additional information we have about $f$ (known as a model class assumption), (ii) how we measure the accuracy of how well $\\hat f$ predicts $f$, (iii) what is known about the data and data sites, (iv) whether the data observations are polluted by noise. A mathematical description of the optimal performance possible (the smallest possible error of recovery) is known in the presence of a model class assumption. Under standard model class assumptions, it is shown in this paper that a near optimal $\\hat f$ can be found by solving a certain discrete over-parameterized optimization problem with a penalty term. Here, near optimal means that the error is bounded by a fixed constant times the optimal error. This",
    "path": "papers/22/03/2203.15994.json",
    "total_tokens": 812,
    "translated_title": "最优学习",
    "translated_abstract": "本文研究了从关于 $f$ 的给定数据学习未知函数 $f$ 的问题。学习问题是给出一个近似值 $ \\hat f $，用于预测数据外的 $f$ 值。这个学习问题的准确性取决于：（i）我们对 $f$ 有什么额外的信息（称为模型类假设），（ii）我们如何度量 $\\hat f$ 的预测准确性，（iii）数据和数据站点的情况，（iv）数据观察是否受到噪声污染。在存在模型类假设的情况下，已知最优恢复性能的数学描述。在标准模型类假设下，本文证明，通过解决一个带有惩罚项的离散过度参数化优化问题，可以找到近乎最优的 $\\hat f$。最优指的是误差受到固定倍数的限制。",
    "tldr": "本文证明，通过解决一个带有惩罚项的离散过度参数化优化问题，可以找到近乎最优的 $\\hat f$。",
    "en_tdlr": "This paper proves that a near optimal $\\hat f$ can be found by solving a certain discrete over-parameterized optimization problem with a penalty term."
}