{
    "title": "Neural Galerkin Schemes with Active Learning for High-Dimensional Evolution Equations",
    "abstract": "arXiv:2203.01360v4 Announce Type: replace-cross  Abstract: Deep neural networks have been shown to provide accurate function approximations in high dimensions. However, fitting network parameters requires informative training data that are often challenging to collect in science and engineering applications. This work proposes Neural Galerkin schemes based on deep learning that generate training data with active learning for numerically solving high-dimensional partial differential equations. Neural Galerkin schemes build on the Dirac-Frenkel variational principle to train networks by minimizing the residual sequentially over time, which enables adaptively collecting new training data in a self-informed manner that is guided by the dynamics described by the partial differential equations. This is in contrast to other machine learning methods that aim to fit network parameters globally in time without taking into account training data acquisition. Our finding is that the active form of ",
    "link": "https://arxiv.org/abs/2203.01360",
    "context": "Title: Neural Galerkin Schemes with Active Learning for High-Dimensional Evolution Equations\nAbstract: arXiv:2203.01360v4 Announce Type: replace-cross  Abstract: Deep neural networks have been shown to provide accurate function approximations in high dimensions. However, fitting network parameters requires informative training data that are often challenging to collect in science and engineering applications. This work proposes Neural Galerkin schemes based on deep learning that generate training data with active learning for numerically solving high-dimensional partial differential equations. Neural Galerkin schemes build on the Dirac-Frenkel variational principle to train networks by minimizing the residual sequentially over time, which enables adaptively collecting new training data in a self-informed manner that is guided by the dynamics described by the partial differential equations. This is in contrast to other machine learning methods that aim to fit network parameters globally in time without taking into account training data acquisition. Our finding is that the active form of ",
    "path": "papers/22/03/2203.01360.json",
    "total_tokens": 791,
    "translated_title": "具有主动学习的神经Galerkin方案用于高维演化方程",
    "translated_abstract": "深度神经网络已被证明能够在高维度中提供准确的函数逼近。然而，拟合网络参数需要信息丰富的训练数据，在科学和工程应用中往往难以收集。本文提出了基于深度学习的神经Galerkin方案，通过主动学习生成训练数据，用于数值求解高维偏微分方程。神经Galerkin方案基于Dirac-Frenkel变分原理，通过随时间顺序最小化残差来训练网络，这使得能够以自主、动态描述的方式收集新的训练数据，以指导偏微分方程描述的动态。这与其他机器学习方法形成对比，其他方法旨在全局时间内拟合网络参数，而不考虑训练数据获取。我们的发现是主动形式的",
    "tldr": "通过神经Galerkin方案结合深度学习和主动学习，能够自主生成训练数据用于高维偏微分方程的数值求解",
    "en_tdlr": "By combining Neural Galerkin schemes with deep learning and active learning, this work autonomously generates training data for numerically solving high-dimensional partial differential equations."
}