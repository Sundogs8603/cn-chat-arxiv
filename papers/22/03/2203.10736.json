{
    "title": "The activity-weight duality in feed forward neural networks: The geometric determinants of generalization. (arXiv:2203.10736v3 [cs.LG] UPDATED)",
    "abstract": "One of the fundamental problems in machine learning is generalization. In neural network models with a large number of weights (parameters), many solutions can be found to fit the training data equally well. The key question is which solution can describe testing data not in the training set. Here, we report the discovery of an exact duality (equivalence) between changes in activities in a given layer of neurons and changes in weights that connect to the next layer of neurons in a densely connected layer in any feed forward neural network. The activity-weight (A-W) duality allows us to map variations in inputs (data) to variations of the corresponding dual weights. By using this mapping, we show that the generalization loss can be decomposed into a sum of contributions from different eigen-directions of the Hessian matrix of the loss function at the solution in weight space. The contribution from a given eigen-direction is the product of two geometric factors (determinants): the sharpn",
    "link": "http://arxiv.org/abs/2203.10736",
    "context": "Title: The activity-weight duality in feed forward neural networks: The geometric determinants of generalization. (arXiv:2203.10736v3 [cs.LG] UPDATED)\nAbstract: One of the fundamental problems in machine learning is generalization. In neural network models with a large number of weights (parameters), many solutions can be found to fit the training data equally well. The key question is which solution can describe testing data not in the training set. Here, we report the discovery of an exact duality (equivalence) between changes in activities in a given layer of neurons and changes in weights that connect to the next layer of neurons in a densely connected layer in any feed forward neural network. The activity-weight (A-W) duality allows us to map variations in inputs (data) to variations of the corresponding dual weights. By using this mapping, we show that the generalization loss can be decomposed into a sum of contributions from different eigen-directions of the Hessian matrix of the loss function at the solution in weight space. The contribution from a given eigen-direction is the product of two geometric factors (determinants): the sharpn",
    "path": "papers/22/03/2203.10736.json",
    "total_tokens": 1008,
    "translated_title": "前馈神经网络中的活动-权重对偶性：泛化性的几何决定因素",
    "translated_abstract": "机器学习中一个基本的问题是泛化性。在具有大量权重（参数）的神经网络模型中，可以找到很多解来很好地拟合训练数据。关键问题是哪个解能够描述不在训练集中的测试数据。在这里，我们报告了在任何前馈神经网络的密集连接层中，给定层神经元活动的变化与连接到下一层神经元的权重变化之间的确切对偶（等价）关系的发现。活动-权重（A-W）对偶性使我们能够将输入（数据）的变化映射到相应的对偶权重的变化。通过使用这种映射，我们表明泛化损失可以分解为在权重空间中的解的损失函数的Hessian矩阵的不同特征方向的贡献之和。给定特征方向的贡献是两个几何因子（行列式）的乘积：尖锐度",
    "tldr": "这项研究发现了在前馈神经网络中，神经元活动的变化与连接到下一层神经元的权重变化之间的准确对偶关系。通过这种对偶性，我们能够将输入数据的变化映射到对应的权重变化，并发现泛化损失可以通过解的损失函数的Hessian矩阵的特征方向的几何因子的乘积来表示。",
    "en_tdlr": "This study reveals an exact duality between changes in neural activities and changes in weights connecting to the next layer in feedforward neural networks. By leveraging this duality, the researchers demonstrate that the generalization loss in neural networks can be decomposed into contributions from different eigen-directions of the loss function's Hessian matrix in weight space. The contribution from each eigen-direction is determined by two geometric factors."
}