{
    "title": "Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v2 [cs.CL] UPDATED)",
    "abstract": "We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs). TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters. We propose a multi-armed bandit framework for the sequential selection of TLM pre-training hyperparameters, aimed at optimizing language model performance, in a resource efficient manner. We design a Thompson sampling algorithm, with a surrogate Gaussian process reward model of the Masked Language Model (MLM) pre-training objective, for its sequential minimization. Instead of MLM pre-training with fixed masking probabilities, the proposed Gaussian process-based Thompson sampling (GP-TS) accelerates pre-training by sequentially selecting masking hyperparameters that improve performance. We empirically demonstrate how GP-TS pre-trains language models efficiently, i.e., it achieves lower MLM loss in fe",
    "link": "http://arxiv.org/abs/2203.13151",
    "context": "Title: Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking. (arXiv:2203.13151v2 [cs.CL] UPDATED)\nAbstract: We design and evaluate a Bayesian optimization framework for resource efficient pre-training of Transformer-based language models (TLMs). TLM pre-training requires high computational resources and introduces many unresolved design choices, such as selecting its pre-training hyperparameters. We propose a multi-armed bandit framework for the sequential selection of TLM pre-training hyperparameters, aimed at optimizing language model performance, in a resource efficient manner. We design a Thompson sampling algorithm, with a surrogate Gaussian process reward model of the Masked Language Model (MLM) pre-training objective, for its sequential minimization. Instead of MLM pre-training with fixed masking probabilities, the proposed Gaussian process-based Thompson sampling (GP-TS) accelerates pre-training by sequentially selecting masking hyperparameters that improve performance. We empirically demonstrate how GP-TS pre-trains language models efficiently, i.e., it achieves lower MLM loss in fe",
    "path": "papers/22/03/2203.13151.json",
    "total_tokens": 962,
    "translated_title": "多臂老虎机用于语言模型预训练的资源高效、在线优化：动态遮盖的使用案例",
    "translated_abstract": "我们设计并评估了一种贝叶斯优化框架，以资源高效的方式预训练基于Transformer的语言模型（TLM）。 TLM预训练需要高计算资源，并引入许多未解决的设计选择，例如选择其预训练超参数。 我们提出了一个多臂老虎机框架，用于顺序选择TLM预训练超参数，旨在以资源高效的方式优化语言模型性能。 我们设计了一个Thompson抽样算法，用于其顺序最小化的带有掩码语言模型（MLM）预训练目标的代理高斯过程奖励模型。 提出的基于高斯过程的Thompson抽样（GP-TS）不是使用固定掩码概率进行MLM预训练，而是通过顺序选择改善性能的掩码超参数来加速预训练。 我们通过实验证明了GP-TS如何高效进行语言模型的预训练，即在少量迭代中实现更低的MLM损失。",
    "tldr": "本文提出了一种多臂老虎机框架，用于顺序选择TLM预训练超参数，旨在以资源高效的方式优化语言模型性能。并设计了基于高斯过程的Thompson抽样（GP-TS）算法，加速Pre-training过程并降低MLM损失。"
}