{
    "title": "DePA: Improving Non-autoregressive Machine Translation with Dependency-Aware Decoder. (arXiv:2203.16266v2 [cs.CL] UPDATED)",
    "abstract": "Non-autoregressive machine translation (NAT) models have lower translation quality than autoregressive translation (AT) models because NAT decoders do not depend on previous target tokens in the decoder input. We propose a novel and general Dependency-Aware Decoder (DePA) to enhance target dependency modeling in the decoder of fully NAT models from two perspectives: decoder self-attention and decoder input. First, we propose an autoregressive forward-backward pre-training phase before NAT training, which enables the NAT decoder to gradually learn bidirectional target dependencies for the final NAT training. Second, we transform the decoder input from the source language representation space to the target language representation space through a novel attentive transformation process, which enables the decoder to better capture target dependencies. DePA can be applied to any fully NAT models. Extensive experiments show that DePA consistently improves highly competitive and state-of-the-a",
    "link": "http://arxiv.org/abs/2203.16266",
    "context": "Title: DePA: Improving Non-autoregressive Machine Translation with Dependency-Aware Decoder. (arXiv:2203.16266v2 [cs.CL] UPDATED)\nAbstract: Non-autoregressive machine translation (NAT) models have lower translation quality than autoregressive translation (AT) models because NAT decoders do not depend on previous target tokens in the decoder input. We propose a novel and general Dependency-Aware Decoder (DePA) to enhance target dependency modeling in the decoder of fully NAT models from two perspectives: decoder self-attention and decoder input. First, we propose an autoregressive forward-backward pre-training phase before NAT training, which enables the NAT decoder to gradually learn bidirectional target dependencies for the final NAT training. Second, we transform the decoder input from the source language representation space to the target language representation space through a novel attentive transformation process, which enables the decoder to better capture target dependencies. DePA can be applied to any fully NAT models. Extensive experiments show that DePA consistently improves highly competitive and state-of-the-a",
    "path": "papers/22/03/2203.16266.json",
    "total_tokens": 888,
    "translated_title": "DePA: 使用依赖感知解码器改进非自回归机器翻译",
    "translated_abstract": "非自回归机器翻译（NAT）模型与自回归翻译（AT）模型相比，翻译质量较低，因为NAT解码器在解码器输入中不依赖于之前的目标标记。我们提出了一种新颖且通用的依赖感知解码器（DePA），从解码器的自注意力和解码器输入两个方面来增强完全NAT模型中的目标依赖建模。首先，我们提出了一个自回归的前向-后向预训练阶段，在NAT训练之前，使NAT解码器能够逐渐学习双向目标依赖关系，以用于最终的NAT训练。其次，我们通过一种新颖的关注转换过程，将解码器输入从源语言表示空间转换到目标语言表示空间，从而使解码器能够更好地捕捉目标依赖关系。DePA可以应用于任何全NAT模型。大量实验表明，DePA在竞争激烈且具有领先水平的模型上都有稳定的改进效果。",
    "tldr": "DePA是一种依赖感知解码器，通过自回归预训练和关注转换两个步骤来改进非自回归机器翻译。实验证明，DePA能够显著提高翻译质量。"
}