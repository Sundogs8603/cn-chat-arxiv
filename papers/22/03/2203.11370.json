{
    "title": "Language modeling via stochastic processes. (arXiv:2203.11370v2 [cs.CL] UPDATED)",
    "abstract": "Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. Recent work in self-supervised learning suggests that models can learn good latent representations via contrastive learning, which can be effective for discriminative tasks. Our work analyzes the application of contrastive representations for generative tasks, like long text generation. We propose one approach for leveraging constrastive representations, which we call Time Control (TC). TC first learns a contrastive representation of the target text domain, then generates text by decoding from these representations. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC performs competitively to methods specific for learning sentence representations on discourse coherence. On long text generation settings, TC preserves the text structure bo",
    "link": "http://arxiv.org/abs/2203.11370",
    "context": "Title: Language modeling via stochastic processes. (arXiv:2203.11370v2 [cs.CL] UPDATED)\nAbstract: Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. Recent work in self-supervised learning suggests that models can learn good latent representations via contrastive learning, which can be effective for discriminative tasks. Our work analyzes the application of contrastive representations for generative tasks, like long text generation. We propose one approach for leveraging constrastive representations, which we call Time Control (TC). TC first learns a contrastive representation of the target text domain, then generates text by decoding from these representations. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC performs competitively to methods specific for learning sentence representations on discourse coherence. On long text generation settings, TC preserves the text structure bo",
    "path": "papers/22/03/2203.11370.json",
    "total_tokens": 839,
    "translated_title": "基于随机过程的语言建模",
    "translated_abstract": "现代语言模型能够生成高质量的短文本，但是当它们生成较长文本时，往往显得冗杂或者不连贯。这些问题源自next-token-only的语言建模目标。最近的自监督学习工作表明，模型可以通过对比学习学习到好的潜在表征，这对于区分性任务是有效的。我们的工作分析了对比表示应用于生成任务（如长文本生成）的情况。我们提出了一种利用对比表示的方法，称为Time Control (TC)。TC首先学习目标文本领域的对比表示，然后通过这些表示解码生成文本。与特定于领域的方法和跨越各种文本领域的fine-tuning GPT2相比，TC在学习句子表示以获得话语连贯性方面表现竞争力。在长文本生成设置中，TC保留了文本结构。",
    "tldr": "本篇论文探究了对比表示在生成任务中的应用，提出了Time Control(TC)方法，可以保留长文本的结构，并在学习句子表示以获得话语连贯性方面表现竞争力。"
}