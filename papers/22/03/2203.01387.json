{
    "title": "A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems. (arXiv:2203.01387v3 [cs.LG] UPDATED)",
    "abstract": "With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field usin",
    "link": "http://arxiv.org/abs/2203.01387",
    "context": "Title: A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems. (arXiv:2203.01387v3 [cs.LG] UPDATED)\nAbstract: With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field usin",
    "path": "papers/22/03/2203.01387.json",
    "total_tokens": 876,
    "translated_title": "离线强化学习综述：分类、回顾和未解决问题",
    "translated_abstract": "随着深度学习的广泛应用，强化学习（RL）在解决以往无法处理的问题方面取得了显著进展，如从像素观察中玩复杂游戏、与人类进行对话以及控制机器人智能体。然而，仍有许多领域由于与环境互动的高成本和危险而无法用RL解决。离线RL是一种范式，它仅从以前收集的交互的静态数据集中学习，因此可以从大型和多样化的培训数据集中提取策略。有效的离线RL算法比在线RL算法具有更广泛的应用，尤其适用于教育、医疗保健和机器人等实际应用。在本文中，我们提出了一个统一的分类法，对离线RL方法进行分类。此外，我们还对该领域最新的算法突破进行了全面回顾。",
    "tldr": "本文综述了离线强化学习中的分类与最新算法突破，离线RL算法具有更广泛的应用，尤其适用于教育、医疗保健和机器人等实际应用。",
    "en_tdlr": "This paper provides a survey of the taxonomy and latest algorithmic breakthroughs in offline reinforcement learning, which has wider applications than online RL, especially in education, healthcare, and robotics."
}