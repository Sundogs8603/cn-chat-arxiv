{
    "title": "When AUC meets DRO: Optimizing Partial AUC for Deep Learning with Non-Convex Convergence Guarantee. (arXiv:2203.00176v5 [cs.LG] UPDATED)",
    "abstract": "In this paper, we propose systematic and efficient gradient-based methods for both one-way and two-way partial AUC (pAUC) maximization that are applicable to deep learning. We propose new formulations of pAUC surrogate objectives by using the distributionally robust optimization (DRO) to define the loss for each individual positive data. We consider two formulations of DRO, one of which is based on conditional-value-at-risk (CVaR) that yields a non-smooth but exact estimator for pAUC, and another one is based on a KL divergence regularized DRO that yields an inexact but smooth (soft) estimator for pAUC. For both one-way and two-way pAUC maximization, we propose two algorithms and prove their convergence for optimizing their two formulations, respectively. Experiments demonstrate the effectiveness of the proposed algorithms for pAUC maximization for deep learning on various datasets.",
    "link": "http://arxiv.org/abs/2203.00176",
    "context": "Title: When AUC meets DRO: Optimizing Partial AUC for Deep Learning with Non-Convex Convergence Guarantee. (arXiv:2203.00176v5 [cs.LG] UPDATED)\nAbstract: In this paper, we propose systematic and efficient gradient-based methods for both one-way and two-way partial AUC (pAUC) maximization that are applicable to deep learning. We propose new formulations of pAUC surrogate objectives by using the distributionally robust optimization (DRO) to define the loss for each individual positive data. We consider two formulations of DRO, one of which is based on conditional-value-at-risk (CVaR) that yields a non-smooth but exact estimator for pAUC, and another one is based on a KL divergence regularized DRO that yields an inexact but smooth (soft) estimator for pAUC. For both one-way and two-way pAUC maximization, we propose two algorithms and prove their convergence for optimizing their two formulations, respectively. Experiments demonstrate the effectiveness of the proposed algorithms for pAUC maximization for deep learning on various datasets.",
    "path": "papers/22/03/2203.00176.json",
    "total_tokens": 907,
    "translated_title": "当AUC遇上DRO：基于非凸收敛保证的深度学习局部AUC优化",
    "translated_abstract": "本文提出了用于深度学习的一种系统且高效的基于梯度的一次性和二次性局部AUC（pAUC）最大化方法。我们通过使用分布鲁棒优化（DRO）来为每个单独的正数据定义损失，提出了pAUC替代目标的新公式。我们考虑了两种DRO的形式，一种基于条件风险值（CVaR），产生非平滑但准确的pAUC估计量；另一种基于KL散度正则化的DRO，产生不准确但平滑（软）的pAUC估计量。对于一次性和二次性pAUC最大化，我们分别提出了两种算法，并证明了它们对于优化各自的两种形式的收敛性。实验证明了提出的算法在各种数据集上对于深度学习中的pAUC最大化的有效性。",
    "tldr": "本文提出了一种基于梯度的方法，通过分布鲁棒优化（DRO）来最大化深度学习中的局部AUC（pAUC），并提出了准确和平滑的pAUC估计量。实验证明了该方法在各种数据集上的有效性。",
    "en_tdlr": "This paper proposes a gradient-based method for maximizing partial AUC (pAUC) in deep learning, using distributionally robust optimization (DRO), and introduces accurate and smooth estimators for pAUC. Experiments demonstrate the effectiveness of the proposed method on various datasets."
}