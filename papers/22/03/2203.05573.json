{
    "title": "Self Pre-training with Masked Autoencoders for Medical Image Classification and Segmentation. (arXiv:2203.05573v2 [eess.IV] UPDATED)",
    "abstract": "Masked Autoencoder (MAE) has recently been shown to be effective in pre-training Vision Transformers (ViT) for natural image analysis. By reconstructing full images from partially masked inputs, a ViT encoder aggregates contextual information to infer masked image regions. We believe that this context aggregation ability is particularly essential to the medical image domain where each anatomical structure is functionally and mechanically connected to other structures and regions. Because there is no ImageNet-scale medical image dataset for pre-training, we investigate a self pre-training paradigm with MAE for medical image analysis tasks. Our method pre-trains a ViT on the training set of the target data instead of another dataset. Thus, self pre-training can benefit more scenarios where pre-training data is hard to acquire. Our experimental results show that MAE self pre-training markedly improves diverse medical image tasks including chest X-ray disease classification, abdominal CT m",
    "link": "http://arxiv.org/abs/2203.05573",
    "context": "Title: Self Pre-training with Masked Autoencoders for Medical Image Classification and Segmentation. (arXiv:2203.05573v2 [eess.IV] UPDATED)\nAbstract: Masked Autoencoder (MAE) has recently been shown to be effective in pre-training Vision Transformers (ViT) for natural image analysis. By reconstructing full images from partially masked inputs, a ViT encoder aggregates contextual information to infer masked image regions. We believe that this context aggregation ability is particularly essential to the medical image domain where each anatomical structure is functionally and mechanically connected to other structures and regions. Because there is no ImageNet-scale medical image dataset for pre-training, we investigate a self pre-training paradigm with MAE for medical image analysis tasks. Our method pre-trains a ViT on the training set of the target data instead of another dataset. Thus, self pre-training can benefit more scenarios where pre-training data is hard to acquire. Our experimental results show that MAE self pre-training markedly improves diverse medical image tasks including chest X-ray disease classification, abdominal CT m",
    "path": "papers/22/03/2203.05573.json",
    "total_tokens": 930,
    "translated_title": "用被遮蔽的自编码器进行自预训练用于医学图像分类和分割",
    "translated_abstract": "最近研究表明，被遮蔽的自编码器 (MAE) 在自然图像分析中为视觉变换器 (ViT) 的预训练是有效的。通过从部分遮蔽的输入中重建完整的图像，ViT 编码器聚合上下文信息以推断被遮蔽的图像区域。我们认为，这种上下文聚合能力在医学图像领域尤其重要，因为每个解剖结构在功能和机械上都与其他结构和区域相连。由于缺乏 ImageNet 规模的医学图像数据集进行预训练，因此我们研究了用 MAE 进行自预训练的方法以用于医学图像分析任务。我们的方法在目标数据的训练集上对 ViT 进行预训练，而不是使用另一个数据集。因此，自预训练可以使更多难以获取预训练数据的情况受益。我们的实验结果表明，MAE 自预训练显著改进了包括胸部 X 光疾病分类、腹部 CT 在内的各种医学图像任务。",
    "tldr": "该论文提出了在医学图像领域中用被遮蔽的自编码器进行自预训练的方法，并利用该方法显著改进了医学图像分类和分割等任务。",
    "en_tdlr": "This paper proposes a method of self pre-training with Masked Autoencoder for medical image analysis and significantly improves medical image tasks including classification and segmentation."
}