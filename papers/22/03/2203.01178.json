{
    "title": "DCT-Former: Efficient Self-Attention with Discrete Cosine Transform. (arXiv:2203.01178v3 [cs.LG] UPDATED)",
    "abstract": "Since their introduction the Trasformer architectures emerged as the dominating architectures for both natural language processing and, more recently, computer vision applications. An intrinsic limitation of this family of \"fully-attentive\" architectures arises from the computation of the dot-product attention, which grows both in memory consumption and number of operations as $O(n^2)$ where $n$ stands for the input sequence length, thus limiting the applications that require modeling very long sequences. Several approaches have been proposed so far in the literature to mitigate this issue, with varying degrees of success. Our idea takes inspiration from the world of lossy data compression (such as the JPEG algorithm) to derive an approximation of the attention module by leveraging the properties of the Discrete Cosine Transform. An extensive section of experiments shows that our method takes up less memory for the same performance, while also drastically reducing inference time. This ",
    "link": "http://arxiv.org/abs/2203.01178",
    "context": "Title: DCT-Former: Efficient Self-Attention with Discrete Cosine Transform. (arXiv:2203.01178v3 [cs.LG] UPDATED)\nAbstract: Since their introduction the Trasformer architectures emerged as the dominating architectures for both natural language processing and, more recently, computer vision applications. An intrinsic limitation of this family of \"fully-attentive\" architectures arises from the computation of the dot-product attention, which grows both in memory consumption and number of operations as $O(n^2)$ where $n$ stands for the input sequence length, thus limiting the applications that require modeling very long sequences. Several approaches have been proposed so far in the literature to mitigate this issue, with varying degrees of success. Our idea takes inspiration from the world of lossy data compression (such as the JPEG algorithm) to derive an approximation of the attention module by leveraging the properties of the Discrete Cosine Transform. An extensive section of experiments shows that our method takes up less memory for the same performance, while also drastically reducing inference time. This ",
    "path": "papers/22/03/2203.01178.json",
    "total_tokens": 747,
    "translated_title": "DCT-Former: 采用离散余弦变换的高效自注意力模型",
    "translated_abstract": "自注意力模型是目前自然语言处理和计算机视觉领域中占据统治地位的模型结构之一。然而，由于点积注意力的计算需求成倍增加，时间复杂度高达O(n^2)，这在建模长序列时具有天然的限制。此论文提出一种以离散余弦变换为基础的自注意力近似方法，取得更小的内存占用和更短的推理时间，实验验证表明该方法具有较好性能。",
    "tldr": "本文提出使用离散余弦变换的自注意力模型，有效缓解了点积注意力计算苛刻的内存和时间复杂度限制，具有更小的内存占用和更短的推理时间，表现良好。",
    "en_tdlr": "This paper proposes an efficient self-attention model based on discrete cosine transform to alleviate the memory and time complexity limitations of dot-product attention in modeling long sequences. The method achieves better results with smaller memory usage and shorter inference time."
}