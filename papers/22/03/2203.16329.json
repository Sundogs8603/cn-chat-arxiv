{
    "title": "Parameter-efficient Model Adaptation for Vision Transformers. (arXiv:2203.16329v3 [cs.CV] UPDATED)",
    "abstract": "In computer vision, it has achieved great transfer learning performance via adapting large-scale pretrained vision models (e.g., vision transformers) to downstream tasks. Common approaches for model adaptation either update all model parameters or leverage linear probes. In this paper, we aim to study parameter-efficient model adaptation strategies for vision transformers on the image classification task. We formulate efficient model adaptation as a subspace training problem and perform a comprehensive benchmarking over different efficient adaptation methods. We conduct an empirical study on each efficient model adaptation method focusing on its performance alongside parameter cost. Furthermore, we propose a parameter-efficient model adaptation framework, which first selects submodules by measuring local intrinsic dimensions and then projects them into subspace for further decomposition via a novel Kronecker Adaptation (KAdaptation) method. We analyze and compare our method with a dive",
    "link": "http://arxiv.org/abs/2203.16329",
    "context": "Title: Parameter-efficient Model Adaptation for Vision Transformers. (arXiv:2203.16329v3 [cs.CV] UPDATED)\nAbstract: In computer vision, it has achieved great transfer learning performance via adapting large-scale pretrained vision models (e.g., vision transformers) to downstream tasks. Common approaches for model adaptation either update all model parameters or leverage linear probes. In this paper, we aim to study parameter-efficient model adaptation strategies for vision transformers on the image classification task. We formulate efficient model adaptation as a subspace training problem and perform a comprehensive benchmarking over different efficient adaptation methods. We conduct an empirical study on each efficient model adaptation method focusing on its performance alongside parameter cost. Furthermore, we propose a parameter-efficient model adaptation framework, which first selects submodules by measuring local intrinsic dimensions and then projects them into subspace for further decomposition via a novel Kronecker Adaptation (KAdaptation) method. We analyze and compare our method with a dive",
    "path": "papers/22/03/2203.16329.json",
    "total_tokens": 839,
    "translated_title": "视觉Transformer的参数高效模型适应",
    "translated_abstract": "在计算机视觉领域，通过将大规模预训练的视觉模型（例如，视觉Transformer）适应到下游任务中，实现了很好的迁移学习性能。常见的模型适应方法要么更新所有模型参数，要么利用线性探测器。本文旨在研究针对图像分类任务的视觉Transformer的参数高效模型适应策略。我们将高效模型适应形式化为一个子空间训练问题，并对不同的高效适应方法进行全面的基准测试。我们对每种高效模型适应方法进行了实证研究，重点关注其性能与参数成本的关系。此外，我们提出了一个参数高效模型适应框架，首先通过度量局部内在维度来选择子模块，然后通过一种新颖的Kronecker适应方法将它们投影到子空间进行进一步分解。我们对我们的方法与一个相似的方法进行了分析和比较。",
    "tldr": "本文研究了针对图像分类任务的视觉Transformer的参数高效模型适应策略，通过将模型子模块投影到子空间进行分解，实现了性能与参数成本的平衡。",
    "en_tdlr": "This paper investigates parameter-efficient model adaptation strategies for vision transformers in image classification tasks, achieving a balance between performance and parameter cost by projecting model submodules into a subspace for decomposition."
}