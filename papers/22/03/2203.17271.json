{
    "title": "Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?. (arXiv:2203.17271v3 [cs.CV] UPDATED)",
    "abstract": "Vision-language (VL) pretrained models have achieved impressive performance on multimodal reasoning and zero-shot recognition tasks. Many of these VL models are pretrained on unlabeled image and caption pairs from the internet. In this paper, we study whether representations of primitive concepts--such as colors, shapes, or the attributes of object parts--emerge automatically within these pretrained VL models. We propose a two-step framework, Compositional Concept Mapping (CompMap), to investigate this. CompMap first asks a VL model to generate primitive concept activations with text prompts, and then learns to construct a composition model that maps the primitive concept activations (e.g. the likelihood of black tail or red wing) to composite concepts (e.g. a red-winged blackbird). We show that a composition model can be reliably learn from ground truth primitive concepts. We thus hypothesize that if primitive concepts indeed emerge in a VL pretrained model, its primitive concept acti",
    "link": "http://arxiv.org/abs/2203.17271",
    "context": "Title: Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?. (arXiv:2203.17271v3 [cs.CV] UPDATED)\nAbstract: Vision-language (VL) pretrained models have achieved impressive performance on multimodal reasoning and zero-shot recognition tasks. Many of these VL models are pretrained on unlabeled image and caption pairs from the internet. In this paper, we study whether representations of primitive concepts--such as colors, shapes, or the attributes of object parts--emerge automatically within these pretrained VL models. We propose a two-step framework, Compositional Concept Mapping (CompMap), to investigate this. CompMap first asks a VL model to generate primitive concept activations with text prompts, and then learns to construct a composition model that maps the primitive concept activations (e.g. the likelihood of black tail or red wing) to composite concepts (e.g. a red-winged blackbird). We show that a composition model can be reliably learn from ground truth primitive concepts. We thus hypothesize that if primitive concepts indeed emerge in a VL pretrained model, its primitive concept acti",
    "path": "papers/22/03/2203.17271.json",
    "total_tokens": 975,
    "translated_title": "视觉语言预训练模型是否学习到组合原始概念？",
    "translated_abstract": "视觉语言（VL）预训练模型在多模态推理和零样本识别任务中取得了令人瞩目的性能。许多VL模型是在互联网上无标注的图像和标题对上预训练的。本文研究了这些预训练的VL模型是否自动产生原始概念的表示，例如颜色、形状或物体部分的属性。我们提出了一个两步框架，组合概念映射（CompMap）来研究这个问题。CompMap首先请求VL模型使用文本提示生成原始概念激活，然后学习构建一个组合模型，将原始概念激活（例如黑色尾巴或红色翅膀的可能性）映射到组合概念（例如红翅黑鸟）。我们展示了可以从真实的原始概念稳定地学习到组合模型。因此，我们假设如果预训练的VL模型确实产生了原始概念，则其原始概念激活...",
    "tldr": "本文研究了视觉语言预训练模型是否自动产生原始概念的表示，提出组合概念映射（CompMap）框架来研究，认为如果模型确实产生了原始概念，其激活应符合先前人类对原始概念的观察结果。",
    "en_tdlr": "This paper investigates whether primitive concept representations, such as color, shape, or object part attributes, emerge automatically within vision-language (VL) pretrained models using the Compositional Concept Mapping (CompMap) framework. It is hypothesized that if primitive concepts do indeed emerge in a VL pretrained model, its activations should correspond to previously observed results from humans."
}