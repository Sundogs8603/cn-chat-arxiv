{
    "title": "Randomized Sharpness-Aware Training for Boosting Computational Efficiency in Deep Learning. (arXiv:2203.09962v2 [cs.LG] UPDATED)",
    "abstract": "By driving models to converge to flat minima, sharpness-aware learning algorithms (such as SAM) have shown the power to achieve state-of-the-art performances. However, these algorithms will generally incur one extra forward-backward propagation at each training iteration, which largely burdens the computation especially for scalable models. To this end, we propose a simple yet efficient training scheme, called Randomized Sharpness-Aware Training (RST). Optimizers in RST would perform a Bernoulli trial at each iteration to choose randomly from base algorithms (SGD) and sharpness-aware algorithms (SAM) with a probability arranged by a predefined scheduling function. Due to the mixture of base algorithms, the overall count of propagation pairs could be largely reduced. Also, we give theoretical analysis on the convergence of RST. Then, we empirically study the computation cost and effect of various types of scheduling functions, and give directions on setting appropriate scheduling functi",
    "link": "http://arxiv.org/abs/2203.09962",
    "context": "Title: Randomized Sharpness-Aware Training for Boosting Computational Efficiency in Deep Learning. (arXiv:2203.09962v2 [cs.LG] UPDATED)\nAbstract: By driving models to converge to flat minima, sharpness-aware learning algorithms (such as SAM) have shown the power to achieve state-of-the-art performances. However, these algorithms will generally incur one extra forward-backward propagation at each training iteration, which largely burdens the computation especially for scalable models. To this end, we propose a simple yet efficient training scheme, called Randomized Sharpness-Aware Training (RST). Optimizers in RST would perform a Bernoulli trial at each iteration to choose randomly from base algorithms (SGD) and sharpness-aware algorithms (SAM) with a probability arranged by a predefined scheduling function. Due to the mixture of base algorithms, the overall count of propagation pairs could be largely reduced. Also, we give theoretical analysis on the convergence of RST. Then, we empirically study the computation cost and effect of various types of scheduling functions, and give directions on setting appropriate scheduling functi",
    "path": "papers/22/03/2203.09962.json",
    "total_tokens": 929,
    "translated_title": "面向深度学习计算效率提升的随机锐度感知训练方法研究",
    "translated_abstract": "通过使模型收敛于平坦的极小值，SAM等锐度感知的学习算法已显示出实现最先进性能的能力。然而，这些算法通常会在每次训练迭代中多进行一次前向-反向传播，从而大大增加了计算量，特别是在可扩展模型中。为此，我们提出了一种名为随机锐度感知训练(RST)的简单而高效的训练方案。RST中的优化器每次迭代都会进行伯努利实验，以由预定义的调度函数安排的概率随机选择基本算法（SGD）和锐度感知算法（SAM）之一。由于基本算法的混合，传播对的总数可以大大减少。此外，我们还对RST的收敛性进行了理论分析。然后，我们通过实验研究了各种调度函数的计算成本和效果，并提供了设置适当调度函数的方向。",
    "tldr": "本文提出了一种名为随机锐度感知训练（RST）的深度学习训练方法，通过在SGD和SAM之间随机选择来减少计算量，同时保证模型收敛。同时，我们对各种调度函数的效果和计算成本进行了实验研究。",
    "en_tdlr": "This paper proposes a training method called Randomized Sharpness-Aware Training (RST) for deep learning, which reduces computation by randomly choosing either SGD or SAM and ensuring convergence. We also experimentally study the effect and computation cost of various scheduling functions."
}