{
    "title": "Text Adversarial Purification as Defense against Adversarial Attacks. (arXiv:2203.14207v2 [cs.CL] UPDATED)",
    "abstract": "Adversarial purification is a successful defense mechanism against adversarial attacks without requiring knowledge of the form of the incoming attack. Generally, adversarial purification aims to remove the adversarial perturbations therefore can make correct predictions based on the recovered clean samples. Despite the success of adversarial purification in the computer vision field that incorporates generative models such as energy-based models and diffusion models, using purification as a defense strategy against textual adversarial attacks is rarely explored. In this work, we introduce a novel adversarial purification method that focuses on defending against textual adversarial attacks. With the help of language models, we can inject noise by masking input texts and reconstructing the masked texts based on the masked language models. In this way, we construct an adversarial purification process for textual models against the most widely used word-substitution adversarial attacks. We",
    "link": "http://arxiv.org/abs/2203.14207",
    "context": "Title: Text Adversarial Purification as Defense against Adversarial Attacks. (arXiv:2203.14207v2 [cs.CL] UPDATED)\nAbstract: Adversarial purification is a successful defense mechanism against adversarial attacks without requiring knowledge of the form of the incoming attack. Generally, adversarial purification aims to remove the adversarial perturbations therefore can make correct predictions based on the recovered clean samples. Despite the success of adversarial purification in the computer vision field that incorporates generative models such as energy-based models and diffusion models, using purification as a defense strategy against textual adversarial attacks is rarely explored. In this work, we introduce a novel adversarial purification method that focuses on defending against textual adversarial attacks. With the help of language models, we can inject noise by masking input texts and reconstructing the masked texts based on the masked language models. In this way, we construct an adversarial purification process for textual models against the most widely used word-substitution adversarial attacks. We",
    "path": "papers/22/03/2203.14207.json",
    "total_tokens": 920,
    "translated_title": "文本对抗净化作为对抗性攻击的防御",
    "translated_abstract": "对抗性净化是一种成功的防御机制，可以在不需要了解入侵攻击形式的情况下对抗对抗性攻击。通常，对抗性净化旨在消除对抗扰动，从而可以基于恢复的干净样本进行正确的预测。尽管对抗净化在计算机视觉领域中包括基于能量模型和扩散模型的生成模型方面取得了成功，但将净化作为针对文本对抗性攻击的防御策略却很少被探索。在这项工作中，我们介绍了一种针对文本对抗性攻击的新型对抗净化方法。借助语言模型的帮助，我们可以通过掩盖输入文本和基于掩盖的语言模型重构掩盖文本来注入噪声。通过这种方式，我们针对最常用的单词替换对抗攻击构建了针对文本模型的对抗净化过程。",
    "tldr": "该文章介绍了一种新型的针对文本对抗性攻击的对抗净化方法，通过注入噪声、掩盖输入文本并基于语言模型重构掩盖文本，成功地在不需要了解攻击形式的情况下对抗单词替换对抗攻击。",
    "en_tdlr": "This paper introduces a novel adversarial purification method for defending against textual adversarial attacks, which successfully combats word-substitution attacks without requiring knowledge of the form of the incoming attack by injecting noise, masking input texts, and reconstructing them based on masked language models."
}