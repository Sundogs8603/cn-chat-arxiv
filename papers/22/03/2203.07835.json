{
    "title": "Better Uncertainty Calibration via Proper Scores for Classification and Beyond",
    "abstract": "arXiv:2203.07835v4 Announce Type: replace  Abstract: With model trustworthiness being crucial for sensitive real-world applications, practitioners are putting more and more focus on improving the uncertainty calibration of deep neural networks. Calibration errors are designed to quantify the reliability of probabilistic predictions but their estimators are usually biased and inconsistent. In this work, we introduce the framework of proper calibration errors, which relates every calibration error to a proper score and provides a respective upper bound with optimal estimation properties. This relationship can be used to reliably quantify the model calibration improvement. We theoretically and empirically demonstrate the shortcomings of commonly used estimators compared to our approach. Due to the wide applicability of proper scores, this gives a natural extension of recalibration beyond classification.",
    "link": "https://arxiv.org/abs/2203.07835",
    "context": "Title: Better Uncertainty Calibration via Proper Scores for Classification and Beyond\nAbstract: arXiv:2203.07835v4 Announce Type: replace  Abstract: With model trustworthiness being crucial for sensitive real-world applications, practitioners are putting more and more focus on improving the uncertainty calibration of deep neural networks. Calibration errors are designed to quantify the reliability of probabilistic predictions but their estimators are usually biased and inconsistent. In this work, we introduce the framework of proper calibration errors, which relates every calibration error to a proper score and provides a respective upper bound with optimal estimation properties. This relationship can be used to reliably quantify the model calibration improvement. We theoretically and empirically demonstrate the shortcomings of commonly used estimators compared to our approach. Due to the wide applicability of proper scores, this gives a natural extension of recalibration beyond classification.",
    "path": "papers/22/03/2203.07835.json",
    "total_tokens": 771,
    "translated_title": "通过正确得分改进分类及其他任务的更好不确定性校准",
    "translated_abstract": "随着模型的可信度对于敏感的现实应用非常重要，从业者越来越关注改进深度神经网络的不确定性校准。校准误差旨在量化概率预测的可靠性，但它们的估计通常是有偏差且不一致的。在这项工作中，我们介绍了正确校准误差的框架，它将每个校准误差与正确得分联系起来，并提供了一个具有最佳估计特性的相应上界。这种关系可用于可靠地量化模型校准的改进。我们在理论上和实证上展示了与我们的方法相比常用估计器的缺陷。由于正确得分的广泛适用性，这为超出分类的重新校准提供了自然延伸。",
    "tldr": "介绍了正确校准误差框架，通过将每个校准误差与正确得分相关联，提供了最佳估计特性的上界，可靠量化模型校准改进。",
    "en_tdlr": "Introducing a framework for proper calibration errors, relating each calibration error to a proper score with optimal estimation properties, to reliably quantify model calibration improvement."
}