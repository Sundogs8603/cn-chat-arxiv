{
    "title": "Universal Regression with Adversarial Responses. (arXiv:2203.05067v3 [cs.LG] UPDATED)",
    "abstract": "We provide algorithms for regression with adversarial responses under large classes of non-i.i.d. instance sequences, on general separable metric spaces, with provably minimal assumptions. We also give characterizations of learnability in this regression context. We consider universal consistency which asks for strong consistency of a learner without restrictions on the value responses. Our analysis shows that such an objective is achievable for a significantly larger class of instance sequences than stationary processes, and unveils a fundamental dichotomy between value spaces: whether finite-horizon mean estimation is achievable or not. We further provide optimistically universal learning rules, i.e., such that if they fail to achieve universal consistency, any other algorithms will fail as well. For unbounded losses, we propose a mild integrability condition under which there exist algorithms for adversarial regression under large classes of non-i.i.d. instance sequences. In additio",
    "link": "http://arxiv.org/abs/2203.05067",
    "context": "Title: Universal Regression with Adversarial Responses. (arXiv:2203.05067v3 [cs.LG] UPDATED)\nAbstract: We provide algorithms for regression with adversarial responses under large classes of non-i.i.d. instance sequences, on general separable metric spaces, with provably minimal assumptions. We also give characterizations of learnability in this regression context. We consider universal consistency which asks for strong consistency of a learner without restrictions on the value responses. Our analysis shows that such an objective is achievable for a significantly larger class of instance sequences than stationary processes, and unveils a fundamental dichotomy between value spaces: whether finite-horizon mean estimation is achievable or not. We further provide optimistically universal learning rules, i.e., such that if they fail to achieve universal consistency, any other algorithms will fail as well. For unbounded losses, we propose a mild integrability condition under which there exist algorithms for adversarial regression under large classes of non-i.i.d. instance sequences. In additio",
    "path": "papers/22/03/2203.05067.json",
    "total_tokens": 894,
    "translated_title": "具有对抗回应的通用回归算法",
    "translated_abstract": "本文提出了在通用可分离指标空间上，对于大类非独立同分布实例序列的对抗回归算法，并给出了关于可学习性的特征说明。我们考虑强一致性的通用一致性学习，无需对值回应进行限制。我们的分析表明：这种目标可在比平稳过程更大的实例序列类中实现，并揭示了值空间之间的根本二分法：是否可以实现有限时间段均值估计。我们进一步提供了乐观的通用性学习规则，即如果它们未能实现通用一致性，则其他任何算法也将失败。对于未界限损失，我们提出了一种温和的可积条件，其下有对抗性回归的算法结论。此外，我们还展示了如何将相同的工具应用于带有对抗性误差的通用预测问题。",
    "tldr": "本文提出了一种通用回归算法，可针对大类非独立同分布实例序列的对抗性回应，在通用可分离指标空间上实现强一致性的通用一致性学习。",
    "en_tdlr": "The paper proposes a universal regression algorithm for adversarial responses under large classes of non-i.i.d. instance sequences in general separable metric spaces with provably minimal assumptions and unveils a fundamental dichotomy between value spaces. It also provides optimistically universal learning rules and applies the same tools to the setting of universal prediction with adversarial errors."
}