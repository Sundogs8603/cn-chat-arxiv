{
    "title": "Gradient Correction beyond Gradient Descent. (arXiv:2203.08345v2 [cs.LG] UPDATED)",
    "abstract": "The great success neural networks have achieved is inseparable from the application of gradient-descent (GD) algorithms. Based on GD, many variant algorithms have emerged to improve the GD optimization process. The gradient for back-propagation is apparently the most crucial aspect for the training of a neural network. The quality of the calculated gradient can be affected by multiple aspects, e.g., noisy data, calculation error, algorithm limitation, and so on. To reveal gradient information beyond gradient descent, we introduce a framework (\\textbf{GCGD}) to perform gradient correction. GCGD consists of two plug-in modules: 1) inspired by the idea of gradient prediction, we propose a \\textbf{GC-W} module for weight gradient correction; 2) based on Neural ODE, we propose a \\textbf{GC-ODE} module for hidden states gradient correction. Experiment results show that our gradient correction framework can effectively improve the gradient quality to reduce training epochs by $\\sim$ 20\\% and ",
    "link": "http://arxiv.org/abs/2203.08345",
    "context": "Title: Gradient Correction beyond Gradient Descent. (arXiv:2203.08345v2 [cs.LG] UPDATED)\nAbstract: The great success neural networks have achieved is inseparable from the application of gradient-descent (GD) algorithms. Based on GD, many variant algorithms have emerged to improve the GD optimization process. The gradient for back-propagation is apparently the most crucial aspect for the training of a neural network. The quality of the calculated gradient can be affected by multiple aspects, e.g., noisy data, calculation error, algorithm limitation, and so on. To reveal gradient information beyond gradient descent, we introduce a framework (\\textbf{GCGD}) to perform gradient correction. GCGD consists of two plug-in modules: 1) inspired by the idea of gradient prediction, we propose a \\textbf{GC-W} module for weight gradient correction; 2) based on Neural ODE, we propose a \\textbf{GC-ODE} module for hidden states gradient correction. Experiment results show that our gradient correction framework can effectively improve the gradient quality to reduce training epochs by $\\sim$ 20\\% and ",
    "path": "papers/22/03/2203.08345.json",
    "total_tokens": 885,
    "translated_title": "超越梯度下降的梯度修正方法",
    "translated_abstract": "神经网络的成功与梯度下降算法的应用密不可分。基于梯度下降，已经出现了许多变种算法来改善梯度下降的优化过程。反向传播的梯度显然是神经网络训练过程中最重要的一环，然而计算梯度的质量受多种因素影响，如噪声数据、计算误差、算法限制等。为了揭示超越梯度下降的梯度信息，我们引入了一个名为GCGD的框架进行梯度修正，它由两个插件模块组成：1）受梯度预测思想的启发，我们提出了一个名为GC-W的权重梯度修正模块；2）基于神经ODE，我们提出了一个名为GC-ODE的隐藏状态梯度修正模块。实验结果表明，我们的梯度修正框架能够有效地提高梯度质量，并将训练轮数减少约20％。",
    "tldr": "本论文提出了一种名为GCGD的梯度修正框架，可以有效提高梯度质量，从而将训练轮数减少约20％。",
    "en_tdlr": "This paper proposes a gradient correction framework called GCGD, which can effectively improve the quality of gradients and reduce training epochs by about 20%."
}