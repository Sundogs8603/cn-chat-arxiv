{
    "title": "Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning. (arXiv:2203.09249v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) is an emerging distributed learning paradigm under privacy constraint. Data heterogeneity is one of the main challenges in FL, which results in slow convergence and degraded performance. Most existing approaches only tackle the heterogeneity challenge by restricting the local model update in client, ignoring the performance drop caused by direct global model aggregation. Instead, we propose a data-free knowledge distillation method to fine-tune the global model in the server (FedFTG), which relieves the issue of direct model aggregation. Concretely, FedFTG explores the input space of local models through a generator, and uses it to transfer the knowledge from local models to the global model. Besides, we propose a hard sample mining scheme to achieve effective knowledge distillation throughout the training. In addition, we develop customized label sampling and class-level ensemble to derive maximum utilization of knowledge, which implicitly mitigates the distrib",
    "link": "http://arxiv.org/abs/2203.09249",
    "context": "Title: Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID Federated Learning. (arXiv:2203.09249v2 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) is an emerging distributed learning paradigm under privacy constraint. Data heterogeneity is one of the main challenges in FL, which results in slow convergence and degraded performance. Most existing approaches only tackle the heterogeneity challenge by restricting the local model update in client, ignoring the performance drop caused by direct global model aggregation. Instead, we propose a data-free knowledge distillation method to fine-tune the global model in the server (FedFTG), which relieves the issue of direct model aggregation. Concretely, FedFTG explores the input space of local models through a generator, and uses it to transfer the knowledge from local models to the global model. Besides, we propose a hard sample mining scheme to achieve effective knowledge distillation throughout the training. In addition, we develop customized label sampling and class-level ensemble to derive maximum utilization of knowledge, which implicitly mitigates the distrib",
    "path": "papers/22/03/2203.09249.json",
    "total_tokens": 958,
    "translated_title": "通过无数据知识蒸馏对全局模型进行微调，用于非独立同分布联邦学习",
    "translated_abstract": "联邦学习是一种在隐私约束下的新兴分布式学习范 paradigm，数据异构性是联邦学习中的主要挑战之一，导致了收敛速度慢和性能下降。大部分现有方法只通过限制客户端的局部模型更新来解决异构性挑战，忽视了直接全局模型聚合所导致的性能下降问题。相反，我们提出了一种无数据知识蒸馏方法来对服务器上的全局模型进行微调(FedFTG)，从而缓解了直接模型聚合问题。具体来说，FedFTG通过生成器探索本地模型的输入空间，并使用它将本地模型的知识转移到全局模型上。此外，我们提出了一种硬样本挖掘方案，实现了在训练过程中的有效知识蒸馏。此外，我们开发了定制的标签采样和类级集成方法，以最大程度地利用知识，从而隐式地缓解了分布问题。",
    "tldr": "本文针对联邦学习中的数据异构性问题，提出了一种无数据知识蒸馏方法，通过在服务器上对全局模型进行微调，可以有效地缓解模型聚合导致的性能下降，并提高联邦学习的效果。"
}