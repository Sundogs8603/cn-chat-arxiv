{
    "title": "Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v5 [eess.AS] UPDATED)",
    "abstract": "We present a method for visually-grounded spoken term discovery. After training either a HuBERT or wav2vec2.0 model to associate spoken captions with natural images, we show that powerful word segmentation and clustering capability emerges within the model's self-attention heads. Our experiments reveal that this ability is not present to nearly the same extent in the base HuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a crucial component of the word discovery capability we observe. We also evaluate our method on the Buckeye word segmentation and ZeroSpeech spoken term discovery tasks, where we perform on par with or better than currently published methods on several metrics. Code and model weights are available at https://github.com/jasonppy/word-discovery.",
    "link": "http://arxiv.org/abs/2203.15081",
    "context": "Title: Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v5 [eess.AS] UPDATED)\nAbstract: We present a method for visually-grounded spoken term discovery. After training either a HuBERT or wav2vec2.0 model to associate spoken captions with natural images, we show that powerful word segmentation and clustering capability emerges within the model's self-attention heads. Our experiments reveal that this ability is not present to nearly the same extent in the base HuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a crucial component of the word discovery capability we observe. We also evaluate our method on the Buckeye word segmentation and ZeroSpeech spoken term discovery tasks, where we perform on par with or better than currently published methods on several metrics. Code and model weights are available at https://github.com/jasonppy/word-discovery.",
    "path": "papers/22/03/2203.15081.json",
    "total_tokens": 732,
    "translated_title": "视觉-语音自监督模型中的词语发现",
    "translated_abstract": "我们提出了一种基于图像-语音联合训练的自监督模型，能够实现词语的自动分割和聚类，并在 Buckeye 词分割和 ZeroSpeech 任务中展现了与当前已发表的方法相当的甚至更好的表现。实验表明，这种能力并未出现在基本的 HuBERT 和 wav2vec2.0 模型中，视觉联结任务是我们观察到的词语发现能力的重要组成部分。",
    "tldr": "这篇论文介绍了一种基于图像-语音联合训练的自监督模型，在模型训练后实现了自动词语分割和聚类的能力，并在两个任务中表现优异。",
    "en_tdlr": "This paper proposes a visually-grounded self-supervised speech model that can automatically segment and cluster words, achieving comparable or even better performance than current methods on the Buckeye word segmentation and ZeroSpeech tasks. The visual grounding task is found to be a crucial component of the observed word discovery capability, which is not present in the base HuBERT and wav2vec2.0 models."
}