{
    "title": "A Closer Look at Rehearsal-Free Continual Learning. (arXiv:2203.17269v2 [cs.LG] UPDATED)",
    "abstract": "Continual learning is a setting where machine learning models learn novel concepts from continuously shifting training data, while simultaneously avoiding degradation of knowledge on previously seen classes which may disappear from the training data for extended periods of time (a phenomenon known as the catastrophic forgetting problem). Current approaches for continual learning of a single expanding task (aka class-incremental continual learning) require extensive rehearsal of previously seen data to avoid this degradation of knowledge. Unfortunately, rehearsal comes at a cost to memory, and it may also violate data-privacy. Instead, we explore combining knowledge distillation and parameter regularization in new ways to achieve strong continual learning performance without rehearsal. Specifically, we take a deep dive into common continual learning techniques: prediction distillation, feature distillation, L2 parameter regularization, and EWC parameter regularization. We first disprove",
    "link": "http://arxiv.org/abs/2203.17269",
    "context": "Title: A Closer Look at Rehearsal-Free Continual Learning. (arXiv:2203.17269v2 [cs.LG] UPDATED)\nAbstract: Continual learning is a setting where machine learning models learn novel concepts from continuously shifting training data, while simultaneously avoiding degradation of knowledge on previously seen classes which may disappear from the training data for extended periods of time (a phenomenon known as the catastrophic forgetting problem). Current approaches for continual learning of a single expanding task (aka class-incremental continual learning) require extensive rehearsal of previously seen data to avoid this degradation of knowledge. Unfortunately, rehearsal comes at a cost to memory, and it may also violate data-privacy. Instead, we explore combining knowledge distillation and parameter regularization in new ways to achieve strong continual learning performance without rehearsal. Specifically, we take a deep dive into common continual learning techniques: prediction distillation, feature distillation, L2 parameter regularization, and EWC parameter regularization. We first disprove",
    "path": "papers/22/03/2203.17269.json",
    "total_tokens": 750,
    "translated_title": "《深入研究无需重复训练的渐进式学习》",
    "translated_abstract": "渐进式学习是机器学习模型在不断变化的训练数据中学习新概念的一种环境，同时避免以前学习的类别出现“灾难性遗忘”现象。当前的单任务扩展性渐进式学习方法需要大量重复训练以避免知识退化，但重复训练会占用大量内存，并可能违反数据隐私。相反，我们探索了将知识蒸馏和参数正则化以新的方式结合起来，以在不进行重复训练的情况下实现强大的渐进式学习性能。",
    "tldr": "本文介绍了一种新的渐进式学习方法，使用知识蒸馏和参数正则化以避免重复训练，并在不会退化已学数据的情况下实现了强大的性能。",
    "en_tdlr": "This paper introduces a novel approach for continual learning that combines knowledge distillation and parameter regularization to avoid rehearsal and achieve strong performance without degradation of previously learned data."
}