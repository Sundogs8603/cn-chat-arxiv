{
    "title": "Reachability In Simple Neural Networks. (arXiv:2203.07941v3 [cs.CC] UPDATED)",
    "abstract": "We investigate the complexity of the reachability problem for (deep) neural networks: does it compute valid output given some valid input? It was recently claimed that the problem is NP-complete for general neural networks and specifications over the input/output dimension given by conjunctions of linear inequalities. We recapitulate the proof and repair some flaws in the original upper and lower bound proofs. Motivated by the general result, we show that NP-hardness already holds for restricted classes of simple specifications and neural networks. Allowing for a single hidden layer and an output dimension of one as well as neural networks with just one negative, zero and one positive weight or bias is sufficient to ensure NP-hardness. Additionally, we give a thorough discussion and outlook of possible extensions for this direction of research on neural network verification.",
    "link": "http://arxiv.org/abs/2203.07941",
    "context": "Title: Reachability In Simple Neural Networks. (arXiv:2203.07941v3 [cs.CC] UPDATED)\nAbstract: We investigate the complexity of the reachability problem for (deep) neural networks: does it compute valid output given some valid input? It was recently claimed that the problem is NP-complete for general neural networks and specifications over the input/output dimension given by conjunctions of linear inequalities. We recapitulate the proof and repair some flaws in the original upper and lower bound proofs. Motivated by the general result, we show that NP-hardness already holds for restricted classes of simple specifications and neural networks. Allowing for a single hidden layer and an output dimension of one as well as neural networks with just one negative, zero and one positive weight or bias is sufficient to ensure NP-hardness. Additionally, we give a thorough discussion and outlook of possible extensions for this direction of research on neural network verification.",
    "path": "papers/22/03/2203.07941.json",
    "total_tokens": 821,
    "translated_title": "简单神经网络中的可达性问题研究",
    "translated_abstract": "我们研究了（深度）神经网络的可达性问题的复杂性：在给定一些有效输入的情况下，它是否计算出有效输出？最近有人声称，对于一般的神经网络和由线性不等式的合取组成的输入/输出维度的规范，该问题是NP完全问题。 我们总结了证明并修复了原始上界和下界证明中的一些缺陷。受到通用结果的启发，我们展示了NP难度已经适用于简单规范和神经网络的受限类。允许一个隐藏层和一个输出维数以及仅具有一个负、零和一个正权重或偏置的神经网络就足以确保NP难度。此外，我们对神经网络验证研究的这个方向进行了全面的讨论和展望。",
    "tldr": "本研究研究了简单神经网络中的可达性问题，并证明了对于仅具有一个隐含层和一个输出维度以及仅具有一个负、零和一个正权重或偏置的神经网络来说，它是NP难度问题。",
    "en_tdlr": "This study investigates the reachability problem in simple neural networks, proving NP-hardness for networks with a single hidden layer, one output dimension, and weights or biases limited to one negative, one zero, and one positive."
}