{
    "title": "Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks. (arXiv:2203.02928v3 [cs.LG] UPDATED)",
    "abstract": "Despite excellent performance of deep neural networks (DNNs) in image classification, detection, and prediction, characterizing how DNNs make a given decision remains an open problem, resulting in a number of interpretability methods. Post-hoc interpretability methods primarily aim to quantify the importance of input features with respect to the class probabilities. However, due to the lack of ground truth and the existence of interpretability methods with diverse operating characteristics, evaluating these methods is a crucial challenge. A popular approach to evaluate interpretability methods is to perturb input features deemed important for a given prediction and observe the decrease in accuracy. However, perturbation itself may introduce artifacts. We propose a method for estimating the impact of such artifacts on the fidelity estimation by utilizing model accuracy curves from perturbing input features according to the Most Import First (MIF) and Least Import First (LIF) orders. Usi",
    "link": "http://arxiv.org/abs/2203.02928",
    "context": "Title: Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks. (arXiv:2203.02928v3 [cs.LG] UPDATED)\nAbstract: Despite excellent performance of deep neural networks (DNNs) in image classification, detection, and prediction, characterizing how DNNs make a given decision remains an open problem, resulting in a number of interpretability methods. Post-hoc interpretability methods primarily aim to quantify the importance of input features with respect to the class probabilities. However, due to the lack of ground truth and the existence of interpretability methods with diverse operating characteristics, evaluating these methods is a crucial challenge. A popular approach to evaluate interpretability methods is to perturb input features deemed important for a given prediction and observe the decrease in accuracy. However, perturbation itself may introduce artifacts. We propose a method for estimating the impact of such artifacts on the fidelity estimation by utilizing model accuracy curves from perturbing input features according to the Most Import First (MIF) and Least Import First (LIF) orders. Usi",
    "path": "papers/22/03/2203.02928.json",
    "total_tokens": 850,
    "translated_title": "深度神经网络的可解释性方法和扰动伪影的评估",
    "translated_abstract": "深度神经网络在图像分类、检测和预测方面表现出色，但如何解释其决策仍然是一个未解决的问题，因此出现了许多可解释性方法。评估这些方法是一个重要的挑战，其中一种流行的方法是通过扰动输入特征来评估可解释性方法，但是扰动本身可能会引入伪影。本文提出了一种估计伪影影响的方法，使用此方法评估了几种流行的可解释性方法在不同数据集上的表现，并展示了扰动伪影对可解释性方法评估的影响。我们的结果突出了在评估可解释性方法时考虑伪影存在的重要性。",
    "tldr": "本文评估了几种常见的深度神经网络可解释性方法，展示了扰动伪影对可解释性方法评估的影响，强调在评估中需要考虑伪影的存在。",
    "en_tdlr": "This paper evaluates several common interpretability methods for deep neural networks, demonstrates the impact of perturbation artifacts on the evaluation of interpretability methods, and emphasizes the importance of considering the presence of artifacts in the evaluation process."
}