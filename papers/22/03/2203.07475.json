{
    "title": "Invariance in Policy Optimisation and Partial Identifiability in Reward Learning. (arXiv:2203.07475v2 [cs.LG] UPDATED)",
    "abstract": "It is often very challenging to manually design reward functions for complex, real-world tasks. To solve this, one can instead use reward learning to infer a reward function from data. However, there are often multiple reward functions that fit the data equally well, even in the infinite-data limit. This means that the reward function is only partially identifiable. In this work, we formally characterise the partial identifiability of the reward function given several popular reward learning data sources, including expert demonstrations and trajectory comparisons. We also analyse the impact of this partial identifiability for several downstream tasks, such as policy optimisation. We unify our results in a framework for comparing data sources and downstream tasks by their invariances, with implications for the design and selection of data sources for reward learning.",
    "link": "http://arxiv.org/abs/2203.07475",
    "context": "Title: Invariance in Policy Optimisation and Partial Identifiability in Reward Learning. (arXiv:2203.07475v2 [cs.LG] UPDATED)\nAbstract: It is often very challenging to manually design reward functions for complex, real-world tasks. To solve this, one can instead use reward learning to infer a reward function from data. However, there are often multiple reward functions that fit the data equally well, even in the infinite-data limit. This means that the reward function is only partially identifiable. In this work, we formally characterise the partial identifiability of the reward function given several popular reward learning data sources, including expert demonstrations and trajectory comparisons. We also analyse the impact of this partial identifiability for several downstream tasks, such as policy optimisation. We unify our results in a framework for comparing data sources and downstream tasks by their invariances, with implications for the design and selection of data sources for reward learning.",
    "path": "papers/22/03/2203.07475.json",
    "total_tokens": 896,
    "translated_title": "政策优化中的不变性及奖励学习中的部分可识别性",
    "translated_abstract": "对于复杂的现实任务，手动设计奖励函数通常是非常具有挑战性的。为了解决这个问题，可以使用奖励学习从数据中推断奖励函数。然而，即使在无限数据的情况下，通常也会有多个奖励函数可以很好地拟合数据。这意味着奖励函数只能被部分地识别。在这项工作中，我们正式描述了在几种流行的奖励学习数据源（包括专家演示和轨迹比较）下奖励函数的部分可识别性。我们还分析了这种部分可识别性对于几项下游任务（例如政策优化）的影响。我们在一个框架中统一了我们的结果，该框架通过其不变性对比数据源和下游任务，并对奖励学习的数据源的设计和选择产生影响。",
    "tldr": "本文探讨了奖励学习中奖励函数的部分可识别性，并分析了这种部分可识别性对政策优化等下游任务的影响。同时提出了一个框架，对比奖励学习的数据源和下游任务，以其不变性为依据，对奖励学习的数据源的设计和选择产生影响。",
    "en_tdlr": "This paper explores the partial identifiability of reward functions in reward learning, analyzing its impact on downstream tasks such as policy optimization. A framework for comparing data sources and downstream tasks by their invariances is proposed, with implications for the design and selection of data sources for reward learning."
}