{
    "title": "Fail-Safe Adversarial Generative Imitation Learning. (arXiv:2203.01696v2 [cs.LG] UPDATED)",
    "abstract": "For flexible yet safe imitation learning (IL), we propose theory and a modular method, with a safety layer that enables a closed-form probability density/gradient of the safe generative continuous policy, end-to-end generative adversarial training, and worst-case safety guarantees. The safety layer maps all actions into a set of safe actions, and uses the change-of-variables formula plus additivity of measures for the density. The set of safe actions is inferred by first checking safety of a finite sample of actions via adversarial reachability analysis of fallback maneuvers, and then concluding on the safety of these actions' neighborhoods using, e.g., Lipschitz continuity. We provide theoretical analysis showing the robustness advantage of using the safety layer already during training (imitation error linear in the horizon) compared to only using it at test time (up to quadratic error). In an experiment on real-world driver interaction data, we empirically demonstrate tractability, ",
    "link": "http://arxiv.org/abs/2203.01696",
    "context": "Title: Fail-Safe Adversarial Generative Imitation Learning. (arXiv:2203.01696v2 [cs.LG] UPDATED)\nAbstract: For flexible yet safe imitation learning (IL), we propose theory and a modular method, with a safety layer that enables a closed-form probability density/gradient of the safe generative continuous policy, end-to-end generative adversarial training, and worst-case safety guarantees. The safety layer maps all actions into a set of safe actions, and uses the change-of-variables formula plus additivity of measures for the density. The set of safe actions is inferred by first checking safety of a finite sample of actions via adversarial reachability analysis of fallback maneuvers, and then concluding on the safety of these actions' neighborhoods using, e.g., Lipschitz continuity. We provide theoretical analysis showing the robustness advantage of using the safety layer already during training (imitation error linear in the horizon) compared to only using it at test time (up to quadratic error). In an experiment on real-world driver interaction data, we empirically demonstrate tractability, ",
    "path": "papers/22/03/2203.01696.json",
    "total_tokens": 1068,
    "translated_title": "安全可靠的对抗生成模仿学习",
    "translated_abstract": "为了实现灵活而安全的模仿学习（IL），我们提出了一种理论和模块化方法，其中包括一个安全层，该层能够使安全生成连续策略的概率密度/梯度成为闭合形式，并提供端到端的生成对抗训练和最坏情况下的安全保证。安全层将所有行动映射到一组安全行动，并使用变量转换公式和度量的可加性来计算密度。通过对回退操作的对抗可达性分析，我们首先检查有限样本的行动安全性，然后通过利普希茨连续性等方法来推断这些行动邻域的安全性。我们提供了理论分析，表明与仅在测试时使用安全层（最多二次误差）相比，在训练过程中使用安全层的鲁棒性优势（模仿误差与时间序列线性相关）。在实际驾驶员交互数据的实验中，我们经验性地证明了该方法的可行性。",
    "tldr": "提出了一种灵活而安全的模仿学习方法，包括一个安全层，使得生成连续策略的概率密度/梯度成为闭合形式，提供了端到端的生成对抗训练和最坏情况下的安全保证。采用对抗可达性分析和利普希茨连续性等方法，通过推断行动邻域的安全性来确定一组安全行动。在实际驾驶员交互数据的实验中，展示了该方法的可行性和鲁棒性优势。",
    "en_tdlr": "A flexible and safe imitation learning method is proposed, which includes a safety layer that enables the closed-form probability density/gradient of the safe generative continuous policy, end-to-end generative adversarial training, and worst-case safety guarantees. The method infers a set of safe actions by analyzing the safety of a finite sample of actions using adversarial reachability analysis and determining the safety of the action neighborhoods using techniques such as Lipschitz continuity. The experimental results on real-world driver interaction data demonstrate the feasibility and robustness advantage of the proposed method."
}