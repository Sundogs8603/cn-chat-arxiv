{
    "title": "HUMUS-Net: Hybrid unrolled multi-scale network architecture for accelerated MRI reconstruction. (arXiv:2203.08213v2 [eess.IV] UPDATED)",
    "abstract": "In accelerated MRI reconstruction, the anatomy of a patient is recovered from a set of under-sampled and noisy measurements. Deep learning approaches have been proven to be successful in solving this ill-posed inverse problem and are capable of producing very high quality reconstructions. However, current architectures heavily rely on convolutions, that are content-independent and have difficulties modeling long-range dependencies in images. Recently, Transformers, the workhorse of contemporary natural language processing, have emerged as powerful building blocks for a multitude of vision tasks. These models split input images into non-overlapping patches, embed the patches into lower-dimensional tokens and utilize a self-attention mechanism that does not suffer from the aforementioned weaknesses of convolutional architectures. However, Transformers incur extremely high compute and memory cost when 1) the input image resolution is high and 2) when the image needs to be split into a lar",
    "link": "http://arxiv.org/abs/2203.08213",
    "context": "Title: HUMUS-Net: Hybrid unrolled multi-scale network architecture for accelerated MRI reconstruction. (arXiv:2203.08213v2 [eess.IV] UPDATED)\nAbstract: In accelerated MRI reconstruction, the anatomy of a patient is recovered from a set of under-sampled and noisy measurements. Deep learning approaches have been proven to be successful in solving this ill-posed inverse problem and are capable of producing very high quality reconstructions. However, current architectures heavily rely on convolutions, that are content-independent and have difficulties modeling long-range dependencies in images. Recently, Transformers, the workhorse of contemporary natural language processing, have emerged as powerful building blocks for a multitude of vision tasks. These models split input images into non-overlapping patches, embed the patches into lower-dimensional tokens and utilize a self-attention mechanism that does not suffer from the aforementioned weaknesses of convolutional architectures. However, Transformers incur extremely high compute and memory cost when 1) the input image resolution is high and 2) when the image needs to be split into a lar",
    "path": "papers/22/03/2203.08213.json",
    "total_tokens": 896,
    "translated_title": "HUMUS-Net: 混合展开多尺度网络结构用于加速MRI重建",
    "translated_abstract": "在加速MRI重建中，需要从一组欠采样和噪声测量中恢复患者的解剖结构。深度学习方法已被证明可以成功解决这个逆问题，并能够产生非常高质量的重建。然而，当前的架构严重依赖于卷积，这些卷积是内容无关的，并且难以模拟图像中的长程依赖关系。最近，变压器作为当代自然语言处理的核心，已经成为多种视觉任务的强大构建模块。这些模型将输入图像分割为非重叠的块，并将块嵌入到较低维的令牌中，利用自我注意机制，不受卷积架构的前述缺点的影响。然而，当输入图像分辨率较高且需要将图像分割为较大的块时，变压器会产生极高的计算和内存成本。",
    "tldr": "本文提出了一种混合展开多尺度网络结构用于加速MRI重建，该结构将变压器和卷积技术相结合，有效地解决了处理高分辨率图像和大块图像时计算和内存成本过高的问题。",
    "en_tdlr": "The paper proposes a hybrid unrolled multi-scale network architecture, named HUMUS-Net, for accelerated MRI reconstruction, which combines transformers and convolution techniques to effectively solve the problem of high computational and memory costs when processing high-resolution and large-block images."
}