{
    "title": "Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional Reasoning Approach. (arXiv:2203.15925v3 [cs.RO] UPDATED)",
    "abstract": "Cooperative multi-agent problems often require coordination between agents, which can be achieved through a centralized policy that considers the global state. Multi-agent policy gradient (MAPG) methods are commonly used to learn such policies, but they are often limited to problems with low-level action spaces. In complex problems with large state and action spaces, it is advantageous to extend MAPG methods to use higher-level actions, also known as options, to improve the policy search efficiency. However, multi-robot option executions are often asynchronous, that is, agents may select and complete their options at different time steps. This makes it difficult for MAPG methods to derive a centralized policy and evaluate its gradient, as centralized policy always select new options at the same time. In this work, we propose a novel, conditional reasoning approach to address this problem and demonstrate its effectiveness on representative option-based multi-agent cooperative tasks thro",
    "link": "http://arxiv.org/abs/2203.15925",
    "context": "Title: Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional Reasoning Approach. (arXiv:2203.15925v3 [cs.RO] UPDATED)\nAbstract: Cooperative multi-agent problems often require coordination between agents, which can be achieved through a centralized policy that considers the global state. Multi-agent policy gradient (MAPG) methods are commonly used to learn such policies, but they are often limited to problems with low-level action spaces. In complex problems with large state and action spaces, it is advantageous to extend MAPG methods to use higher-level actions, also known as options, to improve the policy search efficiency. However, multi-robot option executions are often asynchronous, that is, agents may select and complete their options at different time steps. This makes it difficult for MAPG methods to derive a centralized policy and evaluate its gradient, as centralized policy always select new options at the same time. In this work, we propose a novel, conditional reasoning approach to address this problem and demonstrate its effectiveness on representative option-based multi-agent cooperative tasks thro",
    "path": "papers/22/03/2203.15925.json",
    "total_tokens": 875,
    "translated_title": "异步、基于选项的多智能体策略梯度：一种条件推理方法",
    "translated_abstract": "合作的多智能体问题通常需要在智能体之间进行协调，可以通过考虑全局状态的中央策略来实现。多智能体策略梯度（MAPG）方法通常用于学习这种策略，但通常仅适用于具有低级动作空间的问题。在具有大规模状态和动作空间的复杂问题中，将MAPG方法扩展为使用更高级别的动作（也称为选项）以提高策略搜索效率是有优势的。然而，多机器人选项执行通常是异步的，也就是说，智能体可能在不同的时间步骤选择并完成它们的选项。这使得MAPG方法很难推导出一个中央策略并评估其梯度，因为中央策略总是在相同的时间选择新的选项。在这项工作中，我们提出了一种新颖的条件推理方法来解决这个问题，并在代表性的基于选项的多智能体合作任务上证明了其有效性。",
    "tldr": "本文提出了一种条件推理方法来解决多智能体策略梯度方法在异步选项执行中的问题，并在基于选项的多智能体合作任务上取得有效结果。"
}