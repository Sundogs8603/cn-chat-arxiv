{
    "title": "On the Neural Tangent Kernel Analysis of Randomly Pruned Neural Networks. (arXiv:2203.14328v3 [cs.LG] UPDATED)",
    "abstract": "Motivated by both theory and practice, we study how random pruning of the weights affects a neural network's neural tangent kernel (NTK). In particular, this work establishes an equivalence of the NTKs between a fully-connected neural network and its randomly pruned version. The equivalence is established under two cases. The first main result studies the infinite-width asymptotic. It is shown that given a pruning probability, for fully-connected neural networks with the weights randomly pruned at the initialization, as the width of each layer grows to infinity sequentially, the NTK of the pruned neural network converges to the limiting NTK of the original network with some extra scaling. If the network weights are rescaled appropriately after pruning, this extra scaling can be removed. The second main result considers the finite-width case. It is shown that to ensure the NTK's closeness to the limit, the dependence of width on the sparsity parameter is asymptotically linear, as the NT",
    "link": "http://arxiv.org/abs/2203.14328",
    "context": "Title: On the Neural Tangent Kernel Analysis of Randomly Pruned Neural Networks. (arXiv:2203.14328v3 [cs.LG] UPDATED)\nAbstract: Motivated by both theory and practice, we study how random pruning of the weights affects a neural network's neural tangent kernel (NTK). In particular, this work establishes an equivalence of the NTKs between a fully-connected neural network and its randomly pruned version. The equivalence is established under two cases. The first main result studies the infinite-width asymptotic. It is shown that given a pruning probability, for fully-connected neural networks with the weights randomly pruned at the initialization, as the width of each layer grows to infinity sequentially, the NTK of the pruned neural network converges to the limiting NTK of the original network with some extra scaling. If the network weights are rescaled appropriately after pruning, this extra scaling can be removed. The second main result considers the finite-width case. It is shown that to ensure the NTK's closeness to the limit, the dependence of width on the sparsity parameter is asymptotically linear, as the NT",
    "path": "papers/22/03/2203.14328.json",
    "total_tokens": 998,
    "translated_title": "随机预剪枝神经网络的神经切向核分析",
    "translated_abstract": "本文研究了预剪枝在神经网络的神经切向核（NTK）中的应用。通过证明，我们建立了预剪枝的权重和原始网络的NTK等价，得出了两种情况下的结论：当每层的宽度按顺序增长到无限大时，预剪枝的神经网络的NTK会渐近于原始网络的极限NTK，而宽度对稀疏参数的依赖是渐近线性的。",
    "tldr": "本文研究了预剪枝在神经网络的神经切向核中的应用，并得出了两种情况下的结论。",
    "en_tdlr": "This paper studies the application of random pruning in neural networks' neural tangent kernel (NTK), and establishes the equivalence of the pruned weights and the original network's NTK. The study also comes up with two main results that show the convergence of the pruned network's NTK to the original network's limiting NTK under infinite-width asymptotic and the asymptotically linear dependence of width on the sparsity parameter in finite-width case."
}