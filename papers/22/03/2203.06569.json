{
    "title": "SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization. (arXiv:2203.06569v2 [cs.CL] UPDATED)",
    "abstract": "Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through fine-tuning large pre-trained language models on the downstream dataset. These models are typically decoded with beam search to generate a unique summary. However, the search space is very large, and with the exposure bias, such decoding is not optimal. In this paper, we show that it is possible to directly train a second-stage model performing re-ranking on a set of summary candidates. Our mixture-of-experts SummaReranker learns to select a better candidate and consistently improves the performance of the base model. With a base PEGASUS, we push ROUGE scores by 5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34% on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and checkpoints will be available at https://github.com/ntunlp/SummaReranker.",
    "link": "http://arxiv.org/abs/2203.06569",
    "context": "Title: SummaReranker: A Multi-Task Mixture-of-Experts Re-ranking Framework for Abstractive Summarization. (arXiv:2203.06569v2 [cs.CL] UPDATED)\nAbstract: Sequence-to-sequence neural networks have recently achieved great success in abstractive summarization, especially through fine-tuning large pre-trained language models on the downstream dataset. These models are typically decoded with beam search to generate a unique summary. However, the search space is very large, and with the exposure bias, such decoding is not optimal. In this paper, we show that it is possible to directly train a second-stage model performing re-ranking on a set of summary candidates. Our mixture-of-experts SummaReranker learns to select a better candidate and consistently improves the performance of the base model. With a base PEGASUS, we push ROUGE scores by 5.44% on CNN-DailyMail (47.16 ROUGE-1), 1.31% on XSum (48.12 ROUGE-1) and 9.34% on Reddit TIFU (29.83 ROUGE-1), reaching a new state-of-the-art. Our code and checkpoints will be available at https://github.com/ntunlp/SummaReranker.",
    "path": "papers/22/03/2203.06569.json",
    "total_tokens": 977,
    "translated_title": "SummaReranker：一种多任务专家混合二次排序框架，用于抽象摘要",
    "translated_abstract": "最近，序列到序列的神经网络在抽象摘要方面取得了巨大成功，尤其是通过在下游数据集上微调大型预训练语言模型。这些模型通常使用波束搜索进行解码以生成唯一的摘要。然而，搜索空间非常大，并且由于曝光偏差，这种解码不是最优的。在本文中，我们展示了可以直接训练第二阶段模型在一组摘要候选上执行重新排序。我们的专家混合框架SummaReranker学习选择更好的候选者，并持续提高基本模型的性能。以基本PEGASUS为例，我们在CNN-DailyMail（47.16 ROUGE-1）上提高了5.44％，在XSum（48.12 ROUGE-1）上提高了1.31％，在Reddit TIFU（29.83 ROUGE-1）上提高了9.34％，达到了新的最先进水平。我们的代码和检查点将在https://github.com/ntunlp/SummaReranker上提供。",
    "tldr": "SummaReranker是一种专家混合的二次排序框架，可用于抽象摘要。它能够直接在一组摘要候选上进行重新排序，从而优化基本模型的ROUGE分数，实现了新的SOTA。",
    "en_tdlr": "SummaReranker is a mixture-of-experts framework for abstractive summarization, which can directly re-rank a set of summary candidates to optimize the ROUGE scores of the base model, achieving a new SOTA result on CNN-DailyMail, XSum, and Reddit TIFU datasets."
}