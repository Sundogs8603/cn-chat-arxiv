{
    "title": "Personalized incentives as feedback design in generalized Nash equilibrium problems. (arXiv:2203.12948v3 [math.OC] UPDATED)",
    "abstract": "We investigate both stationary and time-varying, nonmonotone generalized Nash equilibrium problems that exhibit symmetric interactions among the agents, which are known to be potential. As may happen in practical cases, however, we envision a scenario in which the formal expression of the underlying potential function is not available, and we design a semi-decentralized Nash equilibrium seeking algorithm. In the proposed two-layer scheme, a coordinator iteratively integrates the (possibly noisy and sporadic) agents' feedback to learn the pseudo-gradients of the agents, and then design personalized incentives for them. On their side, the agents receive those personalized incentives, compute a solution to an extended game, and then return feedback measurements to the coordinator. In the stationary setting, our algorithm returns a Nash equilibrium in case the coordinator is endowed with standard learning policies, while it returns a Nash equilibrium up to a constant, yet adjustable, error",
    "link": "http://arxiv.org/abs/2203.12948",
    "context": "Title: Personalized incentives as feedback design in generalized Nash equilibrium problems. (arXiv:2203.12948v3 [math.OC] UPDATED)\nAbstract: We investigate both stationary and time-varying, nonmonotone generalized Nash equilibrium problems that exhibit symmetric interactions among the agents, which are known to be potential. As may happen in practical cases, however, we envision a scenario in which the formal expression of the underlying potential function is not available, and we design a semi-decentralized Nash equilibrium seeking algorithm. In the proposed two-layer scheme, a coordinator iteratively integrates the (possibly noisy and sporadic) agents' feedback to learn the pseudo-gradients of the agents, and then design personalized incentives for them. On their side, the agents receive those personalized incentives, compute a solution to an extended game, and then return feedback measurements to the coordinator. In the stationary setting, our algorithm returns a Nash equilibrium in case the coordinator is endowed with standard learning policies, while it returns a Nash equilibrium up to a constant, yet adjustable, error",
    "path": "papers/22/03/2203.12948.json",
    "total_tokens": 1003,
    "translated_title": "个性化激励作为广义纳什均衡问题中的反馈设计",
    "translated_abstract": "本文研究了对称互动的静态和动态非单调广义纳什均衡问题，这些问题已知具有潜势。然而，在实际情况下可能存在这样一种情况，即底层潜在函数的正式表达式不可用，并设计了一种半分散纳什均衡寻找算法。在所提出的两层方案中，协调员迭代地整合代理的（可能是噪声和零散的）反馈以学习代理的伪梯度，然后为他们设计个性化的激励。在代理方面，代理接收到个性化的激励，计算扩展博弈的解，然后向协调员返回反馈测量。在静态设置下，我们的算法在协调员拥有标准学习策略的情况下返回纳什均衡，而在可调整的常数误差内返回纳什均衡。",
    "tldr": "本文研究了对称互动的非单调广义纳什均衡问题，并提出了一种半分散纳什均衡寻找算法，其中，协调员通过整合代理的反馈来学习代理的伪梯度并为其设计个性化的激励，代理计算扩展博弈的解并反馈测量给协调员，该算法可返回静态设置下的纳什均衡。"
}