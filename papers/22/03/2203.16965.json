{
    "title": "PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v4 [cs.CL] UPDATED)",
    "abstract": "While self-supervised speech representation learning (SSL) models serve a variety of downstream tasks, these models have been observed to overfit to the domain from which the unlabelled data originates. To alleviate this issue, we propose PADA (Pruning Assisted Domain Adaptation) and zero out redundant weights from models pre-trained on large amounts of out-of-domain (OOD) data. Intuitively, this helps to make space for the target-domain ASR finetuning. The redundant weights can be identified through various pruning strategies which have been discussed in detail as a part of this work. Specifically, we investigate the effect of the recently discovered Task-Agnostic and Task-Aware pruning on PADA and propose a new pruning paradigm based on the latter, which we call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial pruning mask from a well fine-tuned OOD model, which makes it starkly different from the rest of the pruning strategies discussed in the paper. Our proposed",
    "link": "http://arxiv.org/abs/2203.16965",
    "context": "Title: PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v4 [cs.CL] UPDATED)\nAbstract: While self-supervised speech representation learning (SSL) models serve a variety of downstream tasks, these models have been observed to overfit to the domain from which the unlabelled data originates. To alleviate this issue, we propose PADA (Pruning Assisted Domain Adaptation) and zero out redundant weights from models pre-trained on large amounts of out-of-domain (OOD) data. Intuitively, this helps to make space for the target-domain ASR finetuning. The redundant weights can be identified through various pruning strategies which have been discussed in detail as a part of this work. Specifically, we investigate the effect of the recently discovered Task-Agnostic and Task-Aware pruning on PADA and propose a new pruning paradigm based on the latter, which we call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial pruning mask from a well fine-tuned OOD model, which makes it starkly different from the rest of the pruning strategies discussed in the paper. Our proposed",
    "path": "papers/22/03/2203.16965.json",
    "total_tokens": 1074,
    "translated_title": "PADA: 基于剪枝的自监督语音表示学习领域自适应",
    "translated_abstract": "自监督语音表示学习模型可用于多种下游任务，但这些模型常常会出现对未标注数据来源领域的过拟合现象。为缓解这个问题，本文提出了PADA (Pruning Assisted Domain Adaptation)，并从经过大量OOT(Out-of-domain)数据预训练的模型中减去多余的权重，为目标领域的ASR微调腾出空间。可以通过各种剪枝策略来识别冗余权重，本文详细讨论了最近发现的Task-Agnostic和Task-Aware剪枝对PADA的影响，并提出了一种新的基于后者的剪枝范式，称为Cross-Domain Task-Aware Pruning(CD-TAW)。CD-TAW从精细调整的OOT模型中获得初始剪枝掩码，这使其与本文中讨论的剪枝策略截然不同。我们提出的PADA方法可以通过微调模型和利用剪枝技术来消除不必要的权重，从而提高自监督语音表示学习模型的泛化能力。使用CD-TAW剪枝策略可以显著提高模型性能。",
    "tldr": "本文提出了PADA方法，在自监督语音表示学习领域自适应方面加入剪枝策略，使用CD-TAW方法从精细调整的OOT模型中获得初始剪枝掩码，并取得良好效果。",
    "en_tdlr": "This paper proposes a method called PADA, which introduces pruning techniques for domain adaptation in self-supervised speech representation learning. The CD-TAW pruning strategy, which uses a well fine-tuned OOD model to initialize the pruning mask, is shown to be particularly effective. The proposed PADA method can improve the generalization of self-supervised speech representation models."
}