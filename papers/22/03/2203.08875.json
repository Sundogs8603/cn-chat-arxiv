{
    "title": "SC2 Benchmark: Supervised Compression for Split Computing. (arXiv:2203.08875v2 [cs.LG] UPDATED)",
    "abstract": "With the increasing demand for deep learning models on mobile devices, splitting neural network computation between the device and a more powerful edge server has become an attractive solution. However, existing split computing approaches often underperform compared to a naive baseline of remote computation on compressed data. Recent studies propose learning compressed representations that contain more relevant information for supervised downstream tasks, showing improved tradeoffs between compressed data size and supervised performance. However, existing evaluation metrics only provide an incomplete picture of split computing. This study introduces supervised compression for split computing (SC2) and proposes new evaluation criteria: minimizing computation on the mobile device, minimizing transmitted data size, and maximizing model accuracy. We conduct a comprehensive benchmark study using 10 baseline methods, three computer vision tasks, and over 180 trained models, and discuss vario",
    "link": "http://arxiv.org/abs/2203.08875",
    "context": "Title: SC2 Benchmark: Supervised Compression for Split Computing. (arXiv:2203.08875v2 [cs.LG] UPDATED)\nAbstract: With the increasing demand for deep learning models on mobile devices, splitting neural network computation between the device and a more powerful edge server has become an attractive solution. However, existing split computing approaches often underperform compared to a naive baseline of remote computation on compressed data. Recent studies propose learning compressed representations that contain more relevant information for supervised downstream tasks, showing improved tradeoffs between compressed data size and supervised performance. However, existing evaluation metrics only provide an incomplete picture of split computing. This study introduces supervised compression for split computing (SC2) and proposes new evaluation criteria: minimizing computation on the mobile device, minimizing transmitted data size, and maximizing model accuracy. We conduct a comprehensive benchmark study using 10 baseline methods, three computer vision tasks, and over 180 trained models, and discuss vario",
    "path": "papers/22/03/2203.08875.json",
    "total_tokens": 917,
    "translated_title": "SC2基准测试：面向划分计算的监督式压缩",
    "translated_abstract": "随着在移动设备上使用深度学习模型的需求增加，将神经网络计算分配给设备和更强大的边缘服务器已成为一种吸引人的解决方案。但是，现有的划分计算方法通常表现不如对压缩数据进行远程计算的朴素基线。最近的研究提出了学习压缩表示，这些表示包含更多用于监督下游任务的相关信息，展示了在压缩数据大小和受监督性能之间改进的权衡。然而，现有的评估指标只提供了划分计算的不完整图像。本研究介绍了面向划分计算的监督式压缩（SC2），并提出了新的评估标准：最小化移动设备上的计算，最小化传输数据大小，最大化模型准确性。我们使用10种基准方法、三个计算机视觉任务和180多个训练模型进行了全面的基准测试研究，并讨论了各种约束条件下方法的比较。",
    "tldr": "该研究提出了一种基于监督式压缩的划分式计算方案（SC2），以有效地将神经网络计算分配给移动设备和边缘服务器；使用新的度量标准对其进行全面评估，发现在压缩数据大小和性能之间获得了更好的平衡。"
}