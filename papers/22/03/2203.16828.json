{
    "title": "Rethinking Portrait Matting with Privacy Preserving. (arXiv:2203.16828v2 [cs.CV] UPDATED)",
    "abstract": "Recently, there has been an increasing concern about the privacy issue raised by identifiable information in machine learning. However, previous portrait matting methods were all based on identifiable images. To fill the gap, we present P3M-10k, which is the first large-scale anonymized benchmark for Privacy-Preserving Portrait Matting (P3M). P3M-10k consists of 10,421 high resolution face-blurred portrait images along with high-quality alpha mattes, which enables us to systematically evaluate both trimap-free and trimap-based matting methods and obtain some useful findings about model generalization ability under the privacy preserving training (PPT) setting. We also present a unified matting model dubbed P3M-Net that is compatible with both CNN and transformer backbones. To further mitigate the cross-domain performance gap issue under the PPT setting, we devise a simple yet effective Copy and Paste strategy (P3M-CP), which borrows facial information from public celebrity images and d",
    "link": "http://arxiv.org/abs/2203.16828",
    "context": "Title: Rethinking Portrait Matting with Privacy Preserving. (arXiv:2203.16828v2 [cs.CV] UPDATED)\nAbstract: Recently, there has been an increasing concern about the privacy issue raised by identifiable information in machine learning. However, previous portrait matting methods were all based on identifiable images. To fill the gap, we present P3M-10k, which is the first large-scale anonymized benchmark for Privacy-Preserving Portrait Matting (P3M). P3M-10k consists of 10,421 high resolution face-blurred portrait images along with high-quality alpha mattes, which enables us to systematically evaluate both trimap-free and trimap-based matting methods and obtain some useful findings about model generalization ability under the privacy preserving training (PPT) setting. We also present a unified matting model dubbed P3M-Net that is compatible with both CNN and transformer backbones. To further mitigate the cross-domain performance gap issue under the PPT setting, we devise a simple yet effective Copy and Paste strategy (P3M-CP), which borrows facial information from public celebrity images and d",
    "path": "papers/22/03/2203.16828.json",
    "total_tokens": 954,
    "translated_title": "保护隐私的肖像抠图研究",
    "translated_abstract": "机器学习中个人信息的识别问题引起了越来越多人的关注。然而，以前的肖像抠图方法都是基于可识别的图像。为了填补这一空白，我们提出了P3M-10k，这是首个用于隐私保护肖像抠图(P3M)的大规模匿名基准测试。P3M-10k包括10,421张高分辨率模糊的人像图像以及高质量的alpha抠图，这使我们能够系统地评估基于Trimap和非Trimap的抠图方法，并在隐私保护训练（PPT）环境下获得有用的结果。我们还提出了一种统一的抠图模型P3M-Net，它兼容CNN和transformer骨干网络。为了进一步减轻在PPT设置下跨域性能差距问题，我们设计了一种简单但有效的Copy and Paste策略(P3M-CP)，它借鉴了公共名人图像的面部信息。",
    "tldr": "本论文提出了P3M-10k，这是首个用于隐私保护肖像抠图的大规模匿名基准测试。同时，本研究提出了统一的抠图模型P3M-Net和有效的跨域性能提升策略P3M-CP。",
    "en_tdlr": "This paper proposes a large-scale anonymous benchmark for privacy-preserving portrait matting (P3M), named P3M-10k. The study also presents a unified matting model compatible with both CNN and transformer backbones, and an effective strategy called P3M-CP to improve cross-domain performance under the privacy-preserving training (PPT) setting."
}