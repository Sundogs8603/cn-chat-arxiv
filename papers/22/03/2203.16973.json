{
    "title": "Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition. (arXiv:2203.16973v4 [cs.CL] UPDATED)",
    "abstract": "Self-supervised learning (SSL) to learn high-level speech representations has been a popular approach to building Automatic Speech Recognition (ASR) systems in low-resource settings. However, the common assumption made in literature is that a considerable amount of unlabeled data is available for the same domain or language that can be leveraged for SSL pre-training, which we acknowledge is not feasible in a real-world setting. In this paper, as part of the Interspeech Gram Vaani ASR challenge, we try to study the effect of domain, language, dataset size, and other aspects of our upstream pre-training SSL data on the final performance low-resource downstream ASR task. We also build on the continued pre-training paradigm to study the effect of prior knowledge possessed by models trained using SSL. Extensive experiments and studies reveal that the performance of ASR systems is susceptible to the data used for SSL pre-training. Their performance improves with an increase in similarity and",
    "link": "http://arxiv.org/abs/2203.16973",
    "context": "Title: Analyzing the factors affecting usefulness of Self-Supervised Pre-trained Representations for Speech Recognition. (arXiv:2203.16973v4 [cs.CL] UPDATED)\nAbstract: Self-supervised learning (SSL) to learn high-level speech representations has been a popular approach to building Automatic Speech Recognition (ASR) systems in low-resource settings. However, the common assumption made in literature is that a considerable amount of unlabeled data is available for the same domain or language that can be leveraged for SSL pre-training, which we acknowledge is not feasible in a real-world setting. In this paper, as part of the Interspeech Gram Vaani ASR challenge, we try to study the effect of domain, language, dataset size, and other aspects of our upstream pre-training SSL data on the final performance low-resource downstream ASR task. We also build on the continued pre-training paradigm to study the effect of prior knowledge possessed by models trained using SSL. Extensive experiments and studies reveal that the performance of ASR systems is susceptible to the data used for SSL pre-training. Their performance improves with an increase in similarity and",
    "path": "papers/22/03/2203.16973.json",
    "total_tokens": 948,
    "translated_title": "自监督预训练的表示对语音识别有用的因素分析",
    "translated_abstract": "自监督学习（SSL）学习高级语音表示已成为在低资源环境中构建自动语音识别（ASR）系统的流行方法。然而，文献中通常假设大量未标记的同一领域或语言的数据可用于SSL预训练，我们认为这在实际情况下不可行。本文作为 Interspeech Gram Vaani ASR 挑战的一部分，试图研究领域、语言、数据集大小和其他上游预训练 SSL 数据方面对低资源下游 ASR 任务最终性能的影响。我们还在继续预训练范式的基础上研究了使用 SSL 训练的模型所拥有的先前知识的影响。广泛的实验和研究揭示了 ASR 系统的性能易受用于 SSL 预训练的数据的影响，并且它们的性能随着相似度和",
    "tldr": "本论文试图研究自监督预训练的语音表示对于低资源语音识别系统在领域、语言、数据集大小以及先前知识方面的影响，揭示了用于自监督预训练的数据对 ASR 系统性能的影响以及相似度和数据的数量等因素对其性能的影响。",
    "en_tdlr": "This paper analyzes the impact of self-supervised pre-trained speech representations on low-resource automatic speech recognition systems in terms of domain, language, dataset size, and prior knowledge. It reveals how the data used for pre-training affects the performance of ASR systems and how factors such as similarity and dataset size affect their performance."
}