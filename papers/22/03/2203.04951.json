{
    "title": "Learning from Physical Human Feedback: An Object-Centric One-Shot Adaptation Method. (arXiv:2203.04951v2 [cs.RO] UPDATED)",
    "abstract": "For robots to be effectively deployed in novel environments and tasks, they must be able to understand the feedback expressed by humans during intervention. This can either correct undesirable behavior or indicate additional preferences. Existing methods either require repeated episodes of interactions or assume prior known reward features, which is data-inefficient and can hardly transfer to new tasks. We relax these assumptions by describing human tasks in terms of object-centric sub-tasks and interpreting physical interventions in relation to specific objects. Our method, Object Preference Adaptation (OPA), is composed of two key stages: 1) pre-training a base policy to produce a wide variety of behaviors, and 2) online-updating according to human feedback. The key to our fast, yet simple adaptation is that general interaction dynamics between agents and objects are fixed, and only object-specific preferences are updated. Our adaptation occurs online, requires only one human interve",
    "link": "http://arxiv.org/abs/2203.04951",
    "context": "Title: Learning from Physical Human Feedback: An Object-Centric One-Shot Adaptation Method. (arXiv:2203.04951v2 [cs.RO] UPDATED)\nAbstract: For robots to be effectively deployed in novel environments and tasks, they must be able to understand the feedback expressed by humans during intervention. This can either correct undesirable behavior or indicate additional preferences. Existing methods either require repeated episodes of interactions or assume prior known reward features, which is data-inefficient and can hardly transfer to new tasks. We relax these assumptions by describing human tasks in terms of object-centric sub-tasks and interpreting physical interventions in relation to specific objects. Our method, Object Preference Adaptation (OPA), is composed of two key stages: 1) pre-training a base policy to produce a wide variety of behaviors, and 2) online-updating according to human feedback. The key to our fast, yet simple adaptation is that general interaction dynamics between agents and objects are fixed, and only object-specific preferences are updated. Our adaptation occurs online, requires only one human interve",
    "path": "papers/22/03/2203.04951.json",
    "total_tokens": 887,
    "translated_title": "从人的物理反馈中学习：一种面向物体的单次自适应方法。",
    "translated_abstract": "为了让机器人能够在新环境和任务中有效地部署，它们必须能够理解人类在干预过程中表达的反馈。这可以纠正不良行为或指出其他偏好。现有的方法要么需要重复的交互回合，要么假定先前已知的奖励特征，这是数据效率低下并且很难转移到新任务的。我们通过在术语上将人类任务描述为以物体为中心的子任务，并将物理干预解释为与特定物体的关系来放松这些假设。我们的方法，Object Preference Adaptation (OPA)，由两个关键阶段组成：1）预训练基础策略以产生各种行为，以及2）根据人类反馈在线更新。我们快速而简单的自适应的关键在于固定了代理和物体之间的一般交互动态，只更新特定于物体的偏好。我们的自适应在线进行，只需要一个人类干预。",
    "tldr": "本文提出的Object Preference Adaptation (OPA)方法通过学习人类针对特定物体的反馈，在单次干预后进行机器人行为的自适应，提高了数据效率和适用性。",
    "en_tdlr": "This paper proposes the Object Preference Adaptation (OPA) method, which learns from human feedback and adapts robot behavior in a single intervention based on object-centric sub-tasks, improving data efficiency and applicability."
}