{
    "title": "Computationally Efficient and Statistically Optimal Robust Low-rank Matrix and Tensor Estimation. (arXiv:2203.00953v4 [math.ST] UPDATED)",
    "abstract": "Low-rank matrix estimation under heavy-tailed noise is challenging, both computationally and statistically. Convex approaches have been proven statistically optimal but suffer from high computational costs, especially since robust loss functions are usually non-smooth. More recently, computationally fast non-convex approaches via sub-gradient descent are proposed, which, unfortunately, fail to deliver a statistically consistent estimator even under sub-Gaussian noise. In this paper, we introduce a novel Riemannian sub-gradient (RsGrad) algorithm which is not only computationally efficient with linear convergence but also is statistically optimal, be the noise Gaussian or heavy-tailed. Convergence theory is established for a general framework and specific applications to absolute loss, Huber loss, and quantile loss are investigated. Compared with existing non-convex methods, ours reveals a surprising phenomenon of dual-phase convergence. In phase one, RsGrad behaves as in a typical non-",
    "link": "http://arxiv.org/abs/2203.00953",
    "context": "Title: Computationally Efficient and Statistically Optimal Robust Low-rank Matrix and Tensor Estimation. (arXiv:2203.00953v4 [math.ST] UPDATED)\nAbstract: Low-rank matrix estimation under heavy-tailed noise is challenging, both computationally and statistically. Convex approaches have been proven statistically optimal but suffer from high computational costs, especially since robust loss functions are usually non-smooth. More recently, computationally fast non-convex approaches via sub-gradient descent are proposed, which, unfortunately, fail to deliver a statistically consistent estimator even under sub-Gaussian noise. In this paper, we introduce a novel Riemannian sub-gradient (RsGrad) algorithm which is not only computationally efficient with linear convergence but also is statistically optimal, be the noise Gaussian or heavy-tailed. Convergence theory is established for a general framework and specific applications to absolute loss, Huber loss, and quantile loss are investigated. Compared with existing non-convex methods, ours reveals a surprising phenomenon of dual-phase convergence. In phase one, RsGrad behaves as in a typical non-",
    "path": "papers/22/03/2203.00953.json",
    "total_tokens": 1070,
    "translated_title": "计算高效且统计最优的鲁棒低秩矩阵和张量估计",
    "translated_abstract": "在重尾噪声下进行低秩矩阵估计既具有计算复杂性又具有统计挑战。凸方法已被证明在统计学上是最优的，但由于鲁棒损失函数通常是非光滑的，因此遭受高计算成本的困扰。最近，通过次梯度下降提出了计算快速的非凸方法，但不幸的是，即使在次高斯噪声下，也无法提供统计一致的估计。在本文中，我们介绍了一种新的变曲率次梯度算法（RsGrad），它不仅计算高效，而且收敛线性，并且在噪声为高斯或重尾分布时也具有统计优良性。我们建立了一个通用框架和具体的应用于绝对损失、Huber损失和分位损失的收敛理论，并对比现有非凸方法，我们发现它的对双相收敛的现象进行了惊人的揭示。在阶段一中，RsGrad的行为类似于典型的非凸方法，而在阶段二中，它实现了类凸收敛，并且具有快速的收敛速率和稳健性。我们的数值实验表明，在多种设置下，RsGrad优于现有方法，适用于矩阵和张量估计问题。",
    "tldr": "提出了一种新的计算高效且统计最优的鲁棒低秩矩阵和张量估计算法，可以在重尾噪声下实现准确估计，实验结果证明其优于现有方法。",
    "en_tdlr": "A novel computationally efficient and statistically optimal algorithm for robust low-rank matrix and tensor estimation is proposed, which can achieve accurate estimation under heavy-tailed noise, and numerical experiments demonstrate its superiority over existing methods."
}