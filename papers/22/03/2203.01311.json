{
    "title": "High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning. (arXiv:2203.01311v4 [cs.LG] UPDATED)",
    "abstract": "Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalitie",
    "link": "http://arxiv.org/abs/2203.01311",
    "context": "Title: High-Modality Multimodal Transformer: Quantifying Modality & Interaction Heterogeneity for High-Modality Representation Learning. (arXiv:2203.01311v4 [cs.LG] UPDATED)\nAbstract: Many real-world problems are inherently multimodal, from spoken language, gestures, and paralinguistics humans use to communicate, to force, proprioception, and visual sensors on robots. While there has been an explosion of interest in multimodal learning, these methods are focused on a small set of modalities primarily in language, vision, and audio. In order to accelerate generalization towards diverse and understudied modalities, this paper studies efficient representation learning for high-modality scenarios involving a large set of diverse modalities. Since adding new models for every new modality becomes prohibitively expensive, a critical technical challenge is heterogeneity quantification: how can we measure which modalities encode similar information and interactions in order to permit parameter sharing with previous modalities? This paper proposes two new information theoretic metrics for heterogeneity quantification: (1) modality heterogeneity studies how similar 2 modalitie",
    "path": "papers/22/03/2203.01311.json",
    "total_tokens": 933,
    "translated_title": "高模态多模态Transformer：量化模态与交互异质性以进行高模态表示学习",
    "translated_abstract": "许多现实世界的问题本质上是多模态的，例如人类用于交流的口语、手势和语用学，以及机器人上的力、本体感和视觉传感器。虽然多模态学习引起了广泛的兴趣，但这些方法主要关注一小组模态，主要是语言、视觉和音频。为了加速向多样化和少被研究的模态推广，本文研究了高模态场景下的高效表示学习，涉及一个大量的不同模态。由于为每个新模态添加新模型变得代价过高，关键的技术挑战是异质性量化：我们如何衡量哪些模态编码了类似的信息和交互，以便允许与先前的模态共享参数？本文提出了两种新的信息论度量方法来量化异质性：(1)模态异质性研究了两个模态之间的相似性。",
    "tldr": "本文研究了高模态场景下的高效表示学习，提出了两种新的信息论度量方法来量化模态和交互的异质性，以加速对多样化和少被研究的模态的推广。 (arXiv:2203.01311v4 [cs.LG] UPDATED)"
}