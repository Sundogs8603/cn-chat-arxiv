{
    "title": "On Embeddings for Numerical Features in Tabular Deep Learning. (arXiv:2203.05556v3 [cs.LG] UPDATED)",
    "abstract": "Recently, Transformer-like deep architectures have shown strong performance on tabular data problems. Unlike traditional models, e.g., MLP, these architectures map scalar values of numerical features to high-dimensional embeddings before mixing them in the main backbone. In this work, we argue that embeddings for numerical features are an underexplored degree of freedom in tabular DL, which allows constructing more powerful DL models and competing with GBDT on some traditionally GBDT-friendly benchmarks. We start by describing two conceptually different approaches to building embedding modules: the first one is based on a piecewise linear encoding of scalar values, and the second one utilizes periodic activations. Then, we empirically demonstrate that these two approaches can lead to significant performance boosts compared to the embeddings based on conventional blocks such as linear layers and ReLU activations. Importantly, we also show that embedding numerical features is beneficial ",
    "link": "http://arxiv.org/abs/2203.05556",
    "context": "Title: On Embeddings for Numerical Features in Tabular Deep Learning. (arXiv:2203.05556v3 [cs.LG] UPDATED)\nAbstract: Recently, Transformer-like deep architectures have shown strong performance on tabular data problems. Unlike traditional models, e.g., MLP, these architectures map scalar values of numerical features to high-dimensional embeddings before mixing them in the main backbone. In this work, we argue that embeddings for numerical features are an underexplored degree of freedom in tabular DL, which allows constructing more powerful DL models and competing with GBDT on some traditionally GBDT-friendly benchmarks. We start by describing two conceptually different approaches to building embedding modules: the first one is based on a piecewise linear encoding of scalar values, and the second one utilizes periodic activations. Then, we empirically demonstrate that these two approaches can lead to significant performance boosts compared to the embeddings based on conventional blocks such as linear layers and ReLU activations. Importantly, we also show that embedding numerical features is beneficial ",
    "path": "papers/22/03/2203.05556.json",
    "total_tokens": 957,
    "translated_title": "关于数值特征在表格深度学习中的嵌入",
    "translated_abstract": "最近，类似Transformer的深度架构在表格数据问题上展现出了强大的性能。与传统模型（如MLP）不同，这些架构将数值特征的标量值映射到高维嵌入中，然后在主干网络中将它们混合。本文认为，数值特征的嵌入在表格深度学习中是一个未充分探索的自由度，它允许构建更强大的深度学习模型，并与传统上适用于GBDT的基准进行竞争。我们首先描述了构建嵌入模块的两种概念上不同的方法：第一种基于标量值的分段线性编码，第二种利用周期性激活函数。然后，我们凭经验证明，与基于线性层和ReLU激活的传统模块相比，这两种方法可以显著提高模型性能。重要的是，我们还展示了嵌入数值特征的益处。",
    "tldr": "本文研究了表格深度学习中关于数值特征的嵌入方法，提出了两种不同的构建嵌入模块的方法，并通过实验证明，与传统模块相比，这些方法可以显著提升模型性能。这对于构建更强大的深度学习模型并在一些传统上适用于GBDT的基准上与之竞争具有重要的益处。",
    "en_tdlr": "This paper investigates embedding methods for numerical features in tabular deep learning, proposing two different approaches for constructing embedding modules. It empirically demonstrates that these approaches can significantly improve model performance compared to traditional modules, highlighting the benefits for building more powerful deep learning models and competing with GBDT on traditionally GBDT-friendly benchmarks."
}