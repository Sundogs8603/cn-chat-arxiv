{
    "title": "Representative Subset Selection for Efficient Fine-Tuning in Self-Supervised Speech Recognition. (arXiv:2203.09829v3 [cs.LG] UPDATED)",
    "abstract": "Self-supervised speech recognition models require considerable labeled training data for learning high-fidelity representations for Automatic Speech Recognition (ASR) which is computationally demanding and time-consuming. We consider the task of identifying an optimal subset of data for efficient fine-tuning in self-supervised speech models for ASR. We discover that the dataset pruning strategies used in vision tasks for sampling the most informative examples do not perform better than random subset selection on fine-tuning self-supervised ASR. We then present the COWERAGE algorithm for representative subset selection in self-supervised ASR. COWERAGE is based on our finding that ensuring the coverage of examples based on training Word Error Rate (WER) in the early training epochs leads to better generalization performance. Extensive experiments with the wav2vec 2.0 and HuBERT model on TIMIT, Librispeech, and LJSpeech datasets show the effectiveness of COWERAGE and its transferability a",
    "link": "http://arxiv.org/abs/2203.09829",
    "context": "Title: Representative Subset Selection for Efficient Fine-Tuning in Self-Supervised Speech Recognition. (arXiv:2203.09829v3 [cs.LG] UPDATED)\nAbstract: Self-supervised speech recognition models require considerable labeled training data for learning high-fidelity representations for Automatic Speech Recognition (ASR) which is computationally demanding and time-consuming. We consider the task of identifying an optimal subset of data for efficient fine-tuning in self-supervised speech models for ASR. We discover that the dataset pruning strategies used in vision tasks for sampling the most informative examples do not perform better than random subset selection on fine-tuning self-supervised ASR. We then present the COWERAGE algorithm for representative subset selection in self-supervised ASR. COWERAGE is based on our finding that ensuring the coverage of examples based on training Word Error Rate (WER) in the early training epochs leads to better generalization performance. Extensive experiments with the wav2vec 2.0 and HuBERT model on TIMIT, Librispeech, and LJSpeech datasets show the effectiveness of COWERAGE and its transferability a",
    "path": "papers/22/03/2203.09829.json",
    "total_tokens": 845,
    "translated_title": "自监督语音识别中高效微调的代表性子集选择",
    "translated_abstract": "自监督语音识别模型需要大量标注数据才能学习高保真的语音识别表征，这需要很大的计算量和时间成本。本文研究了在自监督语音模型中识别最佳数据子集以实现高效微调的任务。我们发现，在视觉任务中用于抽样最具信息的例子的数据集修剪策略不如随机子集选择在自监督语音识别中效果好。我们提出COWERAGE算法，以在自监督语音识别中选择代表性子集。COWERAGE基于我们的发现，在早期的训练轮数中基于训练字错误率(WER)保证例子覆盖度可以实现更好的泛化表现。对TIMIT、Librispeech和LJSpeech数据集上使用wav2vec 2.0和HuBERT模型进行的大量实验表明了COWERAGE的有效性和可传递性。",
    "tldr": "COWERAGE算法提出，通过训练错误率保证样本覆盖度，实现在自监督语音识别中高效微调。",
    "en_tdlr": "COWERAGE algorithm is proposed to achieve efficient fine-tuning in self-supervised speech recognition by ensuring the coverage of sampled examples based on training Word Error Rate (WER), which achieves better generalization performance than the dataset pruning strategies used in vision tasks."
}