{
    "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap. (arXiv:2203.07378v3 [eess.AS] UPDATED)",
    "abstract": "Recent advances in transformer-based architectures which are pre-trained in self-supervised manner have shown great promise in several machine learning tasks. In the audio domain, such architectures have also been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of .638 on MSP-Podcast. Further",
    "link": "http://arxiv.org/abs/2203.07378",
    "context": "Title: Dawn of the transformer era in speech emotion recognition: closing the valence gap. (arXiv:2203.07378v3 [eess.AS] UPDATED)\nAbstract: Recent advances in transformer-based architectures which are pre-trained in self-supervised manner have shown great promise in several machine learning tasks. In the audio domain, such architectures have also been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of .638 on MSP-Podcast. Further",
    "path": "papers/22/03/2203.07378.json",
    "total_tokens": 907,
    "translated_title": "语音情绪识别中的Transformer时代的黎明：弥合情感价值差距",
    "translated_abstract": "最近的Transformer架构在自监督预训练方面取得了重大突破，并在多个机器学习任务中表现出了巨大的潜力。在音频领域，这种架构也已成功应用于语音情绪识别(SER)领域。然而，现有的研究还没有评估模型大小和预训练数据对下游性能的影响，并且对泛化能力、稳健性、公平性和效率方面的关注有限。本文在几种预训练变体的wav2vec 2.0和HuBERT上进行了详细分析，并在MSP-Podcast的唤起、控制和价值维度上进行了微调，同时使用IEMOCAP和MOSI进行跨语料库泛化测试。据我们所知，在不使用显式语言信息的情况下，我们在MSP-Podcast上获得了最佳的价值预测性能，相关性系数为0.638。",
    "tldr": "本文通过对几种预训练变体的分析，发现在语音情绪识别领域，使用Transformer架构能够在没有使用显式语言信息的情况下获得最佳的价值预测性能，相关性系数为0.638。",
    "en_tdlr": "This paper explores the use of Transformer-based architectures in speech emotion recognition (SER) and demonstrates that these architectures can achieve top performance in valence prediction without the use of explicit linguistic information, obtaining a concordance correlation coefficient of 0.638 on MSP-Podcast."
}