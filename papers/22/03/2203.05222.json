{
    "title": "Similarity-based Label Inference Attack against Training and Inference of Split Learning",
    "abstract": "arXiv:2203.05222v2 Announce Type: replace-cross  Abstract: Split learning is a promising paradigm for privacy-preserving distributed learning. The learning model can be cut into multiple portions to be collaboratively trained at the participants by exchanging only the intermediate results at the cut layer. Understanding the security performance of split learning is critical for many privacy-sensitive applications. This paper shows that the exchanged intermediate results, including the smashed data (i.e., extracted features from the raw data) and gradients during training and inference of split learning, can already reveal the private labels. We mathematically analyze the potential label leakages and propose the cosine and Euclidean similarity measurements for gradients and smashed data, respectively. Then, the two similarity measurements are shown to be unified in Euclidean space. Based on the similarity metric, we design three label inference attacks to efficiently recover the private",
    "link": "https://arxiv.org/abs/2203.05222",
    "context": "Title: Similarity-based Label Inference Attack against Training and Inference of Split Learning\nAbstract: arXiv:2203.05222v2 Announce Type: replace-cross  Abstract: Split learning is a promising paradigm for privacy-preserving distributed learning. The learning model can be cut into multiple portions to be collaboratively trained at the participants by exchanging only the intermediate results at the cut layer. Understanding the security performance of split learning is critical for many privacy-sensitive applications. This paper shows that the exchanged intermediate results, including the smashed data (i.e., extracted features from the raw data) and gradients during training and inference of split learning, can already reveal the private labels. We mathematically analyze the potential label leakages and propose the cosine and Euclidean similarity measurements for gradients and smashed data, respectively. Then, the two similarity measurements are shown to be unified in Euclidean space. Based on the similarity metric, we design three label inference attacks to efficiently recover the private",
    "path": "papers/22/03/2203.05222.json",
    "total_tokens": 829,
    "translated_title": "基于相似性的标签推断攻击：针对分布式学习的训练和推理",
    "translated_abstract": "Split learning是一种有望实现隐私保护的分布式学习范式。学习模型可以被切割成多个部分，在参与者之间进行协作训练，只交换切割层的中间结果。了解Split learning的安全性能对许多隐私敏感应用至关重要。本文表明，在Split learning的训练和推理过程中，交换的中间结果，包括破碎的数据（即从原始数据提取的特征）和梯度，已经可以透露出私密标签。我们对潜在的标签泄漏进行了数学分析，并针对梯度和破碎的数据提出了余弦相似度和欧氏相似度测量。然后，这两种相似度测量显示可以在欧氏空间中统一。基于相似性度量，我们设计了三种标签推断攻击，可以高效地恢复私密",
    "tldr": "本文研究了在Split learning的训练和推理过程中基于相似性的标签推断攻击，提出了相似性度量并设计了三种标签推断攻击。",
    "en_tdlr": "This paper investigates similarity-based label inference attacks during training and inference of Split learning, proposes similarity measurements, and designs three label inference attacks."
}