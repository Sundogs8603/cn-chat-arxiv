{
    "title": "CoCoFL: Communication- and Computation-Aware Federated Learning via Partial NN Freezing and Quantization. (arXiv:2203.05468v3 [cs.LG] UPDATED)",
    "abstract": "Devices participating in federated learning (FL) typically have heterogeneous communication, computation, and memory resources. However, in synchronous FL, all devices need to finish training by the same deadline dictated by the server. Our results show that training a smaller subset of the neural network (NN) at constrained devices, i.e., dropping neurons/filters as proposed by state of the art, is inefficient, preventing these devices to make an effective contribution to the model. This causes unfairness w.r.t the achievable accuracies of constrained devices, especially in cases with a skewed distribution of class labels across devices. We present a novel FL technique, CoCoFL, which maintains the full NN structure on all devices. To adapt to the devices' heterogeneous resources, CoCoFL freezes and quantizes selected layers, reducing communication, computation, and memory requirements, whereas other layers are still trained in full precision, enabling to reach a high accuracy. Thereby",
    "link": "http://arxiv.org/abs/2203.05468",
    "context": "Title: CoCoFL: Communication- and Computation-Aware Federated Learning via Partial NN Freezing and Quantization. (arXiv:2203.05468v3 [cs.LG] UPDATED)\nAbstract: Devices participating in federated learning (FL) typically have heterogeneous communication, computation, and memory resources. However, in synchronous FL, all devices need to finish training by the same deadline dictated by the server. Our results show that training a smaller subset of the neural network (NN) at constrained devices, i.e., dropping neurons/filters as proposed by state of the art, is inefficient, preventing these devices to make an effective contribution to the model. This causes unfairness w.r.t the achievable accuracies of constrained devices, especially in cases with a skewed distribution of class labels across devices. We present a novel FL technique, CoCoFL, which maintains the full NN structure on all devices. To adapt to the devices' heterogeneous resources, CoCoFL freezes and quantizes selected layers, reducing communication, computation, and memory requirements, whereas other layers are still trained in full precision, enabling to reach a high accuracy. Thereby",
    "path": "papers/22/03/2203.05468.json",
    "total_tokens": 885,
    "translated_title": "CoCoFL: 基于部分神经网络冻结和量化的通信和计算感知联邦学习",
    "translated_abstract": "参与联邦学习的设备通常具有异构的通信、计算和内存资源。然而，在同步联邦学习中，所有设备需要在由服务器规定的相同截止日期之前完成训练。我们的研究结果表明，在受限设备上训练较小的神经网络子集（即通过删除神经元/滤波器）是低效的，阻止了这些设备对模型的有效贡献。这导致了针对受限设备的可达准确度的不公平性，特别是在设备之间存在类标签分布不均匀的情况下。我们提出了一种新颖的联邦学习技术CoCoFL，该技术在所有设备上保持完整的神经网络结构。为了适应设备的异构资源，CoCoFL冻结和量化选择的层，降低通信、计算和内存需求，而其他层仍然以全精度进行训练，实现高准确度。",
    "tldr": "CoCoFL是一种基于神经网络冻结和量化的通信和计算感知的联邦学习技术，可以适应设备的异构资源，并提高准确度。"
}