{
    "title": "A Survey of Adversarial Defences and Robustness in NLP. (arXiv:2203.06414v4 [cs.CL] UPDATED)",
    "abstract": "In the past few years, it has become increasingly evident that deep neural networks are not resilient enough to withstand adversarial perturbations in input data, leaving them vulnerable to attack. Various authors have proposed strong adversarial attacks for computer vision and Natural Language Processing (NLP) tasks. As a response, many defense mechanisms have also been proposed to prevent these networks from failing. The significance of defending neural networks against adversarial attacks lies in ensuring that the model's predictions remain unchanged even if the input data is perturbed. Several methods for adversarial defense in NLP have been proposed, catering to different NLP tasks such as text classification, named entity recognition, and natural language inference. Some of these methods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training, saving the model from overfitting. This survey aims to review the various m",
    "link": "http://arxiv.org/abs/2203.06414",
    "context": "Title: A Survey of Adversarial Defences and Robustness in NLP. (arXiv:2203.06414v4 [cs.CL] UPDATED)\nAbstract: In the past few years, it has become increasingly evident that deep neural networks are not resilient enough to withstand adversarial perturbations in input data, leaving them vulnerable to attack. Various authors have proposed strong adversarial attacks for computer vision and Natural Language Processing (NLP) tasks. As a response, many defense mechanisms have also been proposed to prevent these networks from failing. The significance of defending neural networks against adversarial attacks lies in ensuring that the model's predictions remain unchanged even if the input data is perturbed. Several methods for adversarial defense in NLP have been proposed, catering to different NLP tasks such as text classification, named entity recognition, and natural language inference. Some of these methods not only defend neural networks against adversarial attacks but also act as a regularization mechanism during training, saving the model from overfitting. This survey aims to review the various m",
    "path": "papers/22/03/2203.06414.json",
    "total_tokens": 903,
    "translated_title": "自然语言处理中对抗防御和鲁棒性的综述",
    "translated_abstract": "在过去的几年中，越来越明显的是，深度神经网络无法抵御输入数据中的对抗扰动，使其容易受到攻击。各种作者已经针对计算机视觉和自然语言处理任务提出了有效的对抗性攻击。作为回应，也提出了许多防御机制，以防止这些网络失败。保护神经网络免受对抗性攻击的重要性在于确保模型的预测即使输入数据发生扰动也能保持不变。已经提出了多种自然语言处理中的对抗防御方法，适用于不同的自然语言处理任务，如文本分类，实体识别和自然语言推理。其中一些方法不仅可以防御神经网络受到对抗性攻击，而且还可以在训练过程中作为正则化机制，防止模型过度拟合。本综述旨在回顾各种在自然语言处理中的对抗防御方法。",
    "tldr": "本文综述了自然语言处理中的对抗防御方法，这些方法不仅可以防御神经网络受到对抗性攻击，而且还可以在训练过程中作为正则化机制，防止模型过度拟合。"
}