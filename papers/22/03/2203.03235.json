{
    "title": "Pre-trained Token-replaced Detection Model as Few-shot Learner. (arXiv:2203.03235v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained masked language models have demonstrated remarkable ability as few-shot learners. In this paper, as an alternative, we propose a novel approach to few-shot learning with pre-trained token-replaced detection models like ELECTRA. In this approach, we reformulate a classification or a regression task as a token-replaced detection problem. Specifically, we first define a template and label description words for each task and put them into the input to form a natural language prompt. Then, we employ the pre-trained token-replaced detection model to predict which label description word is the most original (i.e., least replaced) among all label description words in the prompt. A systematic evaluation on 16 datasets demonstrates that our approach outperforms few-shot learners with pre-trained masked language models in both one-sentence and two-sentence learning tasks.",
    "link": "http://arxiv.org/abs/2203.03235",
    "context": "Title: Pre-trained Token-replaced Detection Model as Few-shot Learner. (arXiv:2203.03235v2 [cs.CL] UPDATED)\nAbstract: Pre-trained masked language models have demonstrated remarkable ability as few-shot learners. In this paper, as an alternative, we propose a novel approach to few-shot learning with pre-trained token-replaced detection models like ELECTRA. In this approach, we reformulate a classification or a regression task as a token-replaced detection problem. Specifically, we first define a template and label description words for each task and put them into the input to form a natural language prompt. Then, we employ the pre-trained token-replaced detection model to predict which label description word is the most original (i.e., least replaced) among all label description words in the prompt. A systematic evaluation on 16 datasets demonstrates that our approach outperforms few-shot learners with pre-trained masked language models in both one-sentence and two-sentence learning tasks.",
    "path": "papers/22/03/2203.03235.json",
    "total_tokens": 823,
    "translated_title": "预训练的token-replaced检测模型作为少样本学习器",
    "translated_abstract": "预训练的遮蔽语言模型已经展示出在少样本学习方面具有非凡的能力。本文提出了一种新的方法，使用预训练的token-replaced检测模型（比如ELECTRA）作为少样本学习器。该方法将分类或回归任务重新定义为token-replaced检测问题。具体来说，我们首先为每个任务定义一个模板和标签描述词，并将它们放入输入中形成一个自然语言提示。然后，我们使用预训练的token-replaced检测模型来预测哪个标签描述词在提示中是最原始的（即最少更改的）。对16个数据集的系统评估表明，我们的方法在一句话和两句话的学习任务中，都优于使用预训练的遮蔽语言模型的少样本学习器。",
    "tldr": "本文提出了一种使用预训练的token-replaced检测模型的少样本学习器方法，将任务重新定义为token-replaced检测问题，能够优于使用预训练的遮蔽语言模型的少样本学习器。",
    "en_tdlr": "This paper proposes a few-shot learning approach using pre-trained token-replaced detection models, outperforming few-shot learners using pre-trained masked language models by reformulating the task as a token-replaced detection problem."
}