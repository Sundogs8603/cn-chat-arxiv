{
    "title": "Dimensionality Reduction and Wasserstein Stability for Kernel Regression. (arXiv:2203.09347v2 [stat.ML] UPDATED)",
    "abstract": "In a high-dimensional regression framework, we study consequences of the naive two-step procedure where first the dimension of the input variables is reduced and second, the reduced input variables are used to predict the output variable with kernel regression. In order to analyze the resulting regression errors, a novel stability result for kernel regression with respect to the Wasserstein distance is derived. This allows us to bound errors that occur when perturbed input data is used to fit the regression function. We apply the general stability result to principal component analysis (PCA). Exploiting known estimates from the literature on both principal component analysis and kernel regression, we deduce convergence rates for the two-step procedure. The latter turns out to be particularly useful in a semi-supervised setting.",
    "link": "http://arxiv.org/abs/2203.09347",
    "context": "Title: Dimensionality Reduction and Wasserstein Stability for Kernel Regression. (arXiv:2203.09347v2 [stat.ML] UPDATED)\nAbstract: In a high-dimensional regression framework, we study consequences of the naive two-step procedure where first the dimension of the input variables is reduced and second, the reduced input variables are used to predict the output variable with kernel regression. In order to analyze the resulting regression errors, a novel stability result for kernel regression with respect to the Wasserstein distance is derived. This allows us to bound errors that occur when perturbed input data is used to fit the regression function. We apply the general stability result to principal component analysis (PCA). Exploiting known estimates from the literature on both principal component analysis and kernel regression, we deduce convergence rates for the two-step procedure. The latter turns out to be particularly useful in a semi-supervised setting.",
    "path": "papers/22/03/2203.09347.json",
    "total_tokens": 853,
    "translated_title": "降维与Wasserstein稳定性在核回归中的应用",
    "translated_abstract": "在高维回归框架中，我们研究了一个朴素的两步法，首先降低输入变量的维数，再使用核回归来预测输出变量。为了分析由此产生的回归误差，我们推导了一个针对Wasserstein距离的新的核回归稳定性结果。这使我们能够限制当扰动输入数据用于拟合回归函数时出现的误差。我们将通用的稳定性结果应用于主成分分析(PCA)，利用已知的主成分分析和核回归文献中的估计，推导出了两步法的收敛速度。后者在半监督设置中特别有用。",
    "tldr": "本文研究了在高维回归框架中的降维与Wasserstein稳定性应用，针对在扰动输入数据用于拟合回归函数时出现的误差推导了稳定性结果，并利用主成分分析和核回归文献中的估计，推导了两步法的收敛速度。",
    "en_tdlr": "This paper studies the application of dimensionality reduction and Wasserstein stability in kernel regression in high-dimensional regression framework, and derives a novel stability result for kernel regression with respect to the Wasserstein distance to bound errors that occur when perturbed input data is used to fit the regression function. The convergence rates for the two-step procedure are deduced by applying the general stability result to principal component analysis and exploiting the known estimates from literature on both principal component analysis and kernel regression, which turns out to be particularly useful in a semi-supervised setting."
}