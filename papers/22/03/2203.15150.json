{
    "title": "Tight Bounds on the Hardness of Learning Simple Nonparametric Mixtures. (arXiv:2203.15150v3 [cs.LG] UPDATED)",
    "abstract": "We study the problem of learning nonparametric distributions in a finite mixture, and establish tight bounds on the sample complexity for learning the component distributions in such models. Namely, we are given i.i.d. samples from a pdf $f$ where $$ f=w_1f_1+w_2f_2, \\quad w_1+w_2=1, \\quad w_1,w_2>0 $$ and we are interested in learning each component $f_i$. Without any assumptions on $f_i$, this problem is ill-posed. In order to identify the components $f_i$, we assume that each $f_i$ can be written as a convolution of a Gaussian and a compactly supported density $\\nu_i$ with $\\text{supp}(\\nu_1)\\cap \\text{supp}(\\nu_2)=\\emptyset$.  Our main result shows that $(\\frac{1}{\\varepsilon})^{\\Omega(\\log\\log \\frac{1}{\\varepsilon})}$ samples are required for estimating each $f_i$. The proof relies on a quantitative Tauberian theorem that yields a fast rate of approximation with Gaussians, which may be of independent interest. To show this is tight, we also propose an algorithm that uses $(\\frac{1",
    "link": "http://arxiv.org/abs/2203.15150",
    "context": "Title: Tight Bounds on the Hardness of Learning Simple Nonparametric Mixtures. (arXiv:2203.15150v3 [cs.LG] UPDATED)\nAbstract: We study the problem of learning nonparametric distributions in a finite mixture, and establish tight bounds on the sample complexity for learning the component distributions in such models. Namely, we are given i.i.d. samples from a pdf $f$ where $$ f=w_1f_1+w_2f_2, \\quad w_1+w_2=1, \\quad w_1,w_2>0 $$ and we are interested in learning each component $f_i$. Without any assumptions on $f_i$, this problem is ill-posed. In order to identify the components $f_i$, we assume that each $f_i$ can be written as a convolution of a Gaussian and a compactly supported density $\\nu_i$ with $\\text{supp}(\\nu_1)\\cap \\text{supp}(\\nu_2)=\\emptyset$.  Our main result shows that $(\\frac{1}{\\varepsilon})^{\\Omega(\\log\\log \\frac{1}{\\varepsilon})}$ samples are required for estimating each $f_i$. The proof relies on a quantitative Tauberian theorem that yields a fast rate of approximation with Gaussians, which may be of independent interest. To show this is tight, we also propose an algorithm that uses $(\\frac{1",
    "path": "papers/22/03/2203.15150.json",
    "total_tokens": 1116,
    "translated_title": "学习简单非参数混合模型的难度的严格界限",
    "translated_abstract": "我们研究了在有限混合模型中学习非参数分布的问题，并对学习这些模型中的组件分布的样本复杂性建立了严格界限。具体而言，我们给定了来自概率密度函数$f$的i.i.d.样本，其中$$ f=w_1f_1+w_2f_2，\\quad w_1+w_2=1，\\quad w_1,w_2>0 $$我们感兴趣的是学习每个组件$f_i$。在对$f_i$没有任何假设的情况下，这个问题是无法解决的。为了识别组件$f_i$，我们假设每个$f_i$可以写成一个高斯函数和一个紧支撑密度$\\nu_i$的卷积形式，其中$\\text{supp}(\\nu_1)\\cap \\text{supp}(\\nu_2)=\\emptyset$。我们的主要结果表明，为了估计每个$f_i$，需要$(\\frac{1}{\\varepsilon})^{\\Omega(\\log\\log \\frac{1}{\\varepsilon})}$个样本。证明依赖于一个量化的塔伯利安定理，该定理提供了与高斯函数的快速逼近速度，这可能是独立感兴趣的。为了证明这是紧确的，我们还提出了一种使用$(\\frac{1",
    "tldr": "我们研究了学习非参数混合模型中组件分布的难度，并建立了学习每个组件分布的样本复杂性的严格界限。我们的主要结果是需要$(\\frac{1}{\\varepsilon})^{\\Omega(\\log\\log \\frac{1}{\\varepsilon})}$个样本来估计每个组件分布。",
    "en_tdlr": "We study the hardness of learning component distributions in nonparametric mixture models and establish tight bounds on the sample complexity for learning each component distribution. Our main result shows that $(\\frac{1}{\\varepsilon})^{\\Omega(\\log\\log \\frac{1}{\\varepsilon})}$ samples are required to estimate each component distribution."
}