{
    "title": "Convergence of First-Order Methods for Constrained Nonconvex Optimization with Dependent Data. (arXiv:2203.15797v2 [math.OC] UPDATED)",
    "abstract": "We focus on analyzing the classical stochastic projected gradient methods under a general dependent data sampling scheme for constrained smooth nonconvex optimization. We show the worst-case rate of convergence $\\tilde{O}(t^{-1/4})$ and complexity $\\tilde{O}(\\varepsilon^{-4})$ for achieving an $\\varepsilon$-near stationary point in terms of the norm of the gradient of Moreau envelope and gradient mapping. While classical convergence guarantee requires i.i.d. data sampling from the target distribution, we only require a mild mixing condition of the conditional distribution, which holds for a wide class of Markov chain sampling algorithms. This improves the existing complexity for the constrained smooth nonconvex optimization with dependent data from $\\tilde{O}(\\varepsilon^{-8})$ to $\\tilde{O}(\\varepsilon^{-4})$ with a significantly simpler analysis. We illustrate the generality of our approach by deriving convergence results with dependent data for stochastic proximal gradient methods, ",
    "link": "http://arxiv.org/abs/2203.15797",
    "context": "Title: Convergence of First-Order Methods for Constrained Nonconvex Optimization with Dependent Data. (arXiv:2203.15797v2 [math.OC] UPDATED)\nAbstract: We focus on analyzing the classical stochastic projected gradient methods under a general dependent data sampling scheme for constrained smooth nonconvex optimization. We show the worst-case rate of convergence $\\tilde{O}(t^{-1/4})$ and complexity $\\tilde{O}(\\varepsilon^{-4})$ for achieving an $\\varepsilon$-near stationary point in terms of the norm of the gradient of Moreau envelope and gradient mapping. While classical convergence guarantee requires i.i.d. data sampling from the target distribution, we only require a mild mixing condition of the conditional distribution, which holds for a wide class of Markov chain sampling algorithms. This improves the existing complexity for the constrained smooth nonconvex optimization with dependent data from $\\tilde{O}(\\varepsilon^{-8})$ to $\\tilde{O}(\\varepsilon^{-4})$ with a significantly simpler analysis. We illustrate the generality of our approach by deriving convergence results with dependent data for stochastic proximal gradient methods, ",
    "path": "papers/22/03/2203.15797.json",
    "total_tokens": 962,
    "translated_title": "受限非凸优化中具有相关数据的一阶方法的收敛性分析",
    "translated_abstract": "本文针对受限光滑非凸优化问题，分析了在一般的相关数据采样方案下的经典随机投影梯度方法。我们证明了利用Moreau包络和梯度映射范数实现$\\varepsilon$-近似稳定点的最坏情况收敛速率为$\\tilde{O}(t^{-1/4})$，复杂度为$\\tilde{O}(\\varepsilon^{-4})$。传统的收敛保证需要从目标分布中进行i.i.d.数据采样，而我们只需要对条件分布进行一种较温和的混合条件即可，该条件适用于广泛的马尔可夫链采样算法。相较于现有的受限光滑非凸优化和相关数据的复杂度为$\\tilde{O}(\\varepsilon^{-8})$的情况，我们提出的方法经过简化分析后复杂度为$\\tilde{O}(\\varepsilon^{-4})$。最后，我们演示了相关数据情况下随机近端梯度方法的收敛性。",
    "tldr": "本文提出了一种可用于受限非凸优化中相关数据采样的一阶方法，并通过采用更加温和的混合条件，将复杂度从$\\tilde{O}(\\varepsilon^{-8})$提升至$\\tilde{O}(\\varepsilon^{-4})$。"
}