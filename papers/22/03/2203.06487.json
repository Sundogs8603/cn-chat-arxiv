{
    "title": "Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?. (arXiv:2203.06487v2 [cs.CV] UPDATED)",
    "abstract": "Being able to explain the prediction to clinical end-users is a necessity to leverage the power of artificial intelligence (AI) models for clinical decision support. For medical images, a feature attribution map, or heatmap, is the most common form of explanation that highlights important features for AI models' prediction. However, it is unknown how well heatmaps perform on explaining decisions on multi-modal medical images, where each image modality or channel visualizes distinct clinical information of the same underlying biomedical phenomenon. Understanding such modality-dependent features is essential for clinical users' interpretation of AI decisions. To tackle this clinically important but technically ignored problem, we propose the modality-specific feature importance (MSFI) metric. It encodes clinical image and explanation interpretation patterns of modality prioritization and modality-specific feature localization. We conduct a clinical requirement-grounded, systematic evalua",
    "link": "http://arxiv.org/abs/2203.06487",
    "context": "Title: Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?. (arXiv:2203.06487v2 [cs.CV] UPDATED)\nAbstract: Being able to explain the prediction to clinical end-users is a necessity to leverage the power of artificial intelligence (AI) models for clinical decision support. For medical images, a feature attribution map, or heatmap, is the most common form of explanation that highlights important features for AI models' prediction. However, it is unknown how well heatmaps perform on explaining decisions on multi-modal medical images, where each image modality or channel visualizes distinct clinical information of the same underlying biomedical phenomenon. Understanding such modality-dependent features is essential for clinical users' interpretation of AI decisions. To tackle this clinically important but technically ignored problem, we propose the modality-specific feature importance (MSFI) metric. It encodes clinical image and explanation interpretation patterns of modality prioritization and modality-specific feature localization. We conduct a clinical requirement-grounded, systematic evalua",
    "path": "papers/22/03/2203.06487.json",
    "total_tokens": 928,
    "translated_title": "在多模式医学图像任务中评估可解释的人工智能：现有算法是否能满足临床需求？",
    "translated_abstract": "能够向临床终端用户解释预测结果是利用人工智能模型进行临床决策支持的必要条件。对于医学图像，特征归因图或热图是最常见的解释形式，它突出了人工智能模型预测的重要特征。然而，尚不清楚热图在解释多模式医学图像上的决策时的效果如何，其中每个图像模式或通道可视化相同底层生物医学现象的不同临床信息。了解这样的模态相关特征对临床用户解释人工智能决策至关重要。为了解决这个在临床上重要但在技术上被忽视的问题，我们提出了模态特定的特征重要性（MSFI）度量指标。它包含了模态优先级和模态特定特征定位的临床图像和解释解释模式。我们进行了一个以临床需求为基础的系统评价。",
    "tldr": "本论文提出了一种模态特定的特征重要性（MSFI）度量指标，用于解释多模式医学图像上的决策。这有助于临床用户理解人工智能模型的预测结果和重要特征，从而提供临床决策支持。",
    "en_tdlr": "This paper proposes a modality-specific feature importance (MSFI) metric to explain decisions on multi-modal medical images, which helps clinical users understand the predictions and important features of AI models, providing clinical decision support."
}