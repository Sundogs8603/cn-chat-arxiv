{
    "title": "Generalized Bandit Regret Minimizer Framework in Imperfect Information Extensive-Form Game. (arXiv:2203.05920v4 [cs.LG] UPDATED)",
    "abstract": "Regret minimization methods are a powerful tool for learning approximate Nash equilibrium (NE) in two-player zero-sum imperfect information extensive-form games (IIEGs). We consider the problem in the interactive bandit-feedback setting where we don't know the dynamics of the IIEG. In general, only the interactive trajectory and the reached terminal node value $v(z^t)$ are revealed. To learn NE, the regret minimizer is required to estimate the full-feedback loss gradient $\\ell^t$ by $v(z^t)$ and minimize the regret. In this paper, we propose a generalized framework for this learning setting. It presents a theoretical framework for the design and the modular analysis of the bandit regret minimization methods. We demonstrate that the most recent bandit regret minimization methods can be analyzed as a particular case of our framework. Following this framework, we describe a novel method SIX-OMD to learn approximate NE. It is model-free and extremely improves the best existing convergence ",
    "link": "http://arxiv.org/abs/2203.05920",
    "context": "Title: Generalized Bandit Regret Minimizer Framework in Imperfect Information Extensive-Form Game. (arXiv:2203.05920v4 [cs.LG] UPDATED)\nAbstract: Regret minimization methods are a powerful tool for learning approximate Nash equilibrium (NE) in two-player zero-sum imperfect information extensive-form games (IIEGs). We consider the problem in the interactive bandit-feedback setting where we don't know the dynamics of the IIEG. In general, only the interactive trajectory and the reached terminal node value $v(z^t)$ are revealed. To learn NE, the regret minimizer is required to estimate the full-feedback loss gradient $\\ell^t$ by $v(z^t)$ and minimize the regret. In this paper, we propose a generalized framework for this learning setting. It presents a theoretical framework for the design and the modular analysis of the bandit regret minimization methods. We demonstrate that the most recent bandit regret minimization methods can be analyzed as a particular case of our framework. Following this framework, we describe a novel method SIX-OMD to learn approximate NE. It is model-free and extremely improves the best existing convergence ",
    "path": "papers/22/03/2203.05920.json",
    "total_tokens": 992,
    "translated_title": "在不完全信息广义展开形式博弈中的广义赌博遗憾最小化框架",
    "translated_abstract": "遗憾最小化方法是学习近似纳什平衡解的强大工具，用于两人零和不完全信息广义展开形式博弈。我们考虑交互赌博反馈设置下的问题，其中我们不知道不完全信息广义展开形式博弈的动态。一般来说，只有交互轨迹和到达终止节点的价值$v(z^t)$被揭示。为了学习纳什平衡解，遗憾最小化器需要通过$v(z^t)$估计完全反馈损失梯度$\\ell^t$并最小化遗憾。本文提出了一个针对这种学习设置的广义框架。它为赌博遗憾最小化方法的设计和模块化分析提供了一个理论框架。我们证明了最新的赌博遗憾最小化方法可以作为我们框架的一个特例进行分析。在按照该框架进行操作的基础上，我们描述了一种名为SIX-OMD的新方法来学习近似纳什平衡解。它是无模型的，并且极大地改善了最好的现有收敛率。",
    "tldr": "本论文提出了一个广义框架，用于在不完全信息广义展开形式博弈中学习近似纳什平衡解。通过该框架，我们设计了一种新方法SIX-OMD来最小化遗憾并改善收敛率。"
}