{
    "title": "Stealing and Evading Malware Classifiers and Antivirus at Low False Positive Conditions. (arXiv:2204.06241v2 [cs.CR] UPDATED)",
    "abstract": "Model stealing attacks have been successfully used in many machine learning domains, but there is little understanding of how these attacks work against models that perform malware detection. Malware detection and, in general, security domains have unique conditions. In particular, there are very strong requirements for low false positive rates (FPR). Antivirus products (AVs) that use machine learning are very complex systems to steal, malware binaries continually change, and the whole environment is adversarial by nature. This study evaluates active learning model stealing attacks against publicly available stand-alone machine learning malware classifiers and also against antivirus products. The study proposes a new neural network architecture for surrogate models (dualFFNN) and a new model stealing attack that combines transfer and active learning for surrogate creation (FFNN-TL). We achieved good surrogates of the stand-alone classifiers with up to 99\\% agreement with the target mod",
    "link": "http://arxiv.org/abs/2204.06241",
    "context": "Title: Stealing and Evading Malware Classifiers and Antivirus at Low False Positive Conditions. (arXiv:2204.06241v2 [cs.CR] UPDATED)\nAbstract: Model stealing attacks have been successfully used in many machine learning domains, but there is little understanding of how these attacks work against models that perform malware detection. Malware detection and, in general, security domains have unique conditions. In particular, there are very strong requirements for low false positive rates (FPR). Antivirus products (AVs) that use machine learning are very complex systems to steal, malware binaries continually change, and the whole environment is adversarial by nature. This study evaluates active learning model stealing attacks against publicly available stand-alone machine learning malware classifiers and also against antivirus products. The study proposes a new neural network architecture for surrogate models (dualFFNN) and a new model stealing attack that combines transfer and active learning for surrogate creation (FFNN-TL). We achieved good surrogates of the stand-alone classifiers with up to 99\\% agreement with the target mod",
    "path": "papers/22/04/2204.06241.json",
    "total_tokens": 861,
    "translated_title": "在低误报率条件下窃取和逃避恶意软件分类器和防病毒软件",
    "translated_abstract": "模型窃取攻击已成功用于许多机器学习领域，但对于执行恶意软件检测的模型，这些攻击的工作原理尚不清楚。恶意软件检测和安全领域具有独特的条件，特别是对低误报率有着极强的要求。本研究评估了针对公开可用的独立机器学习恶意软件分类器和防病毒产品的主动学习模型窃取攻击。本研究提出了一种新的神经网络体系结构用于代理模型（dualFFNN），并设计了一种新的模型窃取攻击方法，该攻击方法将转移学习和主动学习结合起来用于代理创建（FFNN-TL）。",
    "tldr": "本研究尝试针对恶意软件分类器和防病毒产品进行了模型窃取攻击，并提出了一种新的神经网络体系结构和模型窃取攻击方法。最终实现了高达99%的代理模型与目标模型的一致性。",
    "en_tdlr": "This study conducts model stealing attacks against standalone machine learning malware classifiers and antivirus products, proposing a new neural network architecture and attack method that achieves up to 99% surrogate agreement with the target model under low false positive rates."
}