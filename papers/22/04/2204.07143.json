{
    "title": "Neighborhood Attention Transformer. (arXiv:2204.07143v5 [cs.CV] UPDATED)",
    "abstract": "We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on M",
    "link": "http://arxiv.org/abs/2204.07143",
    "context": "Title: Neighborhood Attention Transformer. (arXiv:2204.07143v5 [cs.CV] UPDATED)\nAbstract: We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on M",
    "path": "papers/22/04/2204.07143.json",
    "total_tokens": 975,
    "translated_title": "《邻域注意力变换器》",
    "translated_abstract": "我们提出了“邻域关注”（NA），这是第一种针对视觉任务的高效和可扩展的滑动窗口注意力机制。NA是一种像素级运算，将自注意力（SA）局限于最近的相邻像素，因此与SA的二次复杂度相比，具有线性的时间和空间复杂度。滑动窗口模式使NA的感受野能够增长而不需要额外的像素移位，并且保留了平移等变性，这与Swin Transformer的窗口自注意力（WSA）不同。我们开发了NATTEN（邻域关注扩展），这是一个具有高效的C++和CUDA内核的Python包，使NA的运行速度比Swin的WSA快高达40％，同时使用的内存少了25％。我们进一步提出了基于NA的新层次结构变换器设计——邻域关注变换器（NAT），以提高图像分类和下游视觉性能。在NAT上的实验结果表明具有竞争力，NAT-Tiny在ImageNet上达到了83.2％的top-1准确率和51.4％的mAP。",
    "tldr": "提出了针对视觉任务的高效和可扩展的滑动窗口注意力机制——邻域关注（NA）。基于NA，开发了NAT，NAT-Tiny在ImageNet上达到了83.2％的top-1准确率，能够提高图像分类和下游视觉性能。",
    "en_tdlr": "The paper proposes an efficient and scalable sliding-window attention mechanism called Neighborhood Attention (NA) for vision tasks. Based on NA, Neighborhood Attention Transformer (NAT) is developed to boost image classification and downstream vision performance. NAT-Tiny achieves 83.2% top-1 accuracy on ImageNet. The proposed method improves the efficiency and effectiveness of attention mechanisms in computer vision tasks."
}