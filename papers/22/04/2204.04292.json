{
    "title": "Evolving Pareto-Optimal Actor-Critic Algorithms for Generalizability and Stability. (arXiv:2204.04292v3 [cs.LG] UPDATED)",
    "abstract": "Generalizability and stability are two key objectives for operating reinforcement learning (RL) agents in the real world. Designing RL algorithms that optimize these objectives can be a costly and painstaking process. This paper presents MetaPG, an evolutionary method for automated design of actor-critic loss functions. MetaPG explicitly optimizes for generalizability and performance, and implicitly optimizes the stability of both metrics. We initialize our loss function population with Soft Actor-Critic (SAC) and perform multi-objective optimization using fitness metrics encoding single-task performance, zero-shot generalizability to unseen environment configurations, and stability across independent runs with different random seeds. On a set of continuous control tasks from the Real-World RL Benchmark Suite, we find that our method, using a single environment during evolution, evolves algorithms that improve upon SAC's performance and generalizability by 4% and 20%, respectively, and",
    "link": "http://arxiv.org/abs/2204.04292",
    "context": "Title: Evolving Pareto-Optimal Actor-Critic Algorithms for Generalizability and Stability. (arXiv:2204.04292v3 [cs.LG] UPDATED)\nAbstract: Generalizability and stability are two key objectives for operating reinforcement learning (RL) agents in the real world. Designing RL algorithms that optimize these objectives can be a costly and painstaking process. This paper presents MetaPG, an evolutionary method for automated design of actor-critic loss functions. MetaPG explicitly optimizes for generalizability and performance, and implicitly optimizes the stability of both metrics. We initialize our loss function population with Soft Actor-Critic (SAC) and perform multi-objective optimization using fitness metrics encoding single-task performance, zero-shot generalizability to unseen environment configurations, and stability across independent runs with different random seeds. On a set of continuous control tasks from the Real-World RL Benchmark Suite, we find that our method, using a single environment during evolution, evolves algorithms that improve upon SAC's performance and generalizability by 4% and 20%, respectively, and",
    "path": "papers/22/04/2204.04292.json",
    "total_tokens": 902,
    "translated_title": "进化帕累托最优的演员-评论家算法以实现泛化和稳定性",
    "translated_abstract": "泛化性和稳定性是在现实世界中操作强化学习代理的两个关键目标。设计优化这些目标的强化学习算法可能是一个昂贵而费时的过程。本文提出了MetaPG，一种用于自动设计演员-评论家损失函数的进化方法。MetaPG明确地优化泛化性和性能，并隐式地优化两个指标的稳定性。我们使用软演员评论家（SAC）初始化我们的损失函数种群，并使用编码单任务性能、对未见过的环境配置的零样本泛化性和在不同随机种子下独立运行时的稳定性的适应性度量进行多目标优化。在来自真实世界强化学习基准套件的一组连续控制任务上，我们发现使用单个环境进行进化的方法可以使演变出来的算法在性能和泛化性上比SAC有4%和20%的改进，同时将性能的标准差减少了近一半。",
    "tldr": "本文提出了MetaPG，一种自动设计演员-评论家损失函数的进化方法，该方法明确地优化泛化性和性能，并隐式地优化这两个指标的稳定性。",
    "en_tdlr": "This paper presents MetaPG, an evolutionary method for automated design of actor-critic loss functions that explicitly optimizes for generalizability and performance, and implicitly optimizes the stability of both metrics."
}