{
    "title": "An Algebraically Converging Stochastic Gradient Descent Algorithm for Global Optimization. (arXiv:2204.05923v3 [math.OC] UPDATED)",
    "abstract": "We propose a new gradient descent algorithm with added stochastic terms for finding the global optimizers of nonconvex optimization problems. A key component in the algorithm is the adaptive tuning of the randomness based on the value of the objective function. In the language of simulated annealing, the temperature is state-dependent. With this, we prove the global convergence of the algorithm with an algebraic rate both in probability and in the parameter space. This is a significant improvement over the classical rate from using a more straightforward control of the noise term. The convergence proof is based on the actual discrete setup of the algorithm, not just its continuous limit as often done in the literature. We also present several numerical examples to demonstrate the efficiency and robustness of the algorithm for reasonably complex objective functions.",
    "link": "http://arxiv.org/abs/2204.05923",
    "context": "Title: An Algebraically Converging Stochastic Gradient Descent Algorithm for Global Optimization. (arXiv:2204.05923v3 [math.OC] UPDATED)\nAbstract: We propose a new gradient descent algorithm with added stochastic terms for finding the global optimizers of nonconvex optimization problems. A key component in the algorithm is the adaptive tuning of the randomness based on the value of the objective function. In the language of simulated annealing, the temperature is state-dependent. With this, we prove the global convergence of the algorithm with an algebraic rate both in probability and in the parameter space. This is a significant improvement over the classical rate from using a more straightforward control of the noise term. The convergence proof is based on the actual discrete setup of the algorithm, not just its continuous limit as often done in the literature. We also present several numerical examples to demonstrate the efficiency and robustness of the algorithm for reasonably complex objective functions.",
    "path": "papers/22/04/2204.05923.json",
    "total_tokens": 810,
    "translated_title": "一种代数收敛的随机梯度下降算法用于全局优化",
    "translated_abstract": "我们提出了一种新的梯度下降算法，通过添加随机项来找到非凸优化问题的全局最优解。算法的一个关键组成部分是根据目标函数的值自适应调整随机性。在模拟退火的术语中，温度是与状态相关的。借此，我们证明了算法在概率和参数空间中具有代数收敛性，这比仅使用更简单的噪声项控制的经典收敛速率有了显著改进。收敛证明基于算法的实际离散设置，而不仅仅是如文献中通常做的连续极限。我们还提供了几个数值示例，以展示算法对于相对复杂的目标函数的效率和鲁棒性。",
    "tldr": "本文提出了一种改进的随机梯度下降算法，通过自适应调整随机性来找到非凸优化问题的全局最优解，并通过代数收敛性证明了算法的优越性能。",
    "en_tdlr": "This paper proposes an improved stochastic gradient descent algorithm for finding the global optimizers of nonconvex optimization problems, with adaptive tuning of randomness, and provides a convergence proof based on the algorithm's actual discrete setup, showing its superiority in terms of algebraic convergence."
}