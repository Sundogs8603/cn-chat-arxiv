{
    "title": "Causal Confusion and Reward Misidentification in Preference-Based Reward Learning. (arXiv:2204.06601v4 [cs.LG] UPDATED)",
    "abstract": "Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states -- resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optim",
    "link": "http://arxiv.org/abs/2204.06601",
    "context": "Title: Causal Confusion and Reward Misidentification in Preference-Based Reward Learning. (arXiv:2204.06601v4 [cs.LG] UPDATED)\nAbstract: Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states -- resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optim",
    "path": "papers/22/04/2204.06601.json",
    "total_tokens": 1066,
    "translated_title": "基于偏好的奖励学习中的因果混淆和奖励误识别",
    "translated_abstract": "通过基于偏好的奖励学习来学习策略是定制智能体行为的一种越来越流行的方法，但据说会容易出现虚假相关性和奖励欺骗行为。本文研究了从偏好中学习时因果混淆和奖励误识别的系统性研究，而现有的大部分工作则关注于强化学习和行为克隆中的因果混淆。我们在几个基准领域上进行了一系列的敏感性和消融分析，结果表明，从偏好中学习到的奖励虽然可以在学习环境中获得最小的测试误差，但在分布不同的状态下无法进行泛化，从而导致策略在优化时表现较差。我们发现非因果分散特征的存在、陈述偏好中的噪声以及局部状态的可观察性都可能加剧奖励误识别。我们还确定了一组方法来解释被误识别的学习奖励。总的来说，我们观察到偏好奖励学习中优化的策略对奖励函数的选择非常敏感，必须注意确保学习到的奖励的可识别性和因果性。",
    "tldr": "基于偏好的奖励学习中存在因果混淆和奖励误识别，非因果分散特征、偏好中的噪声以及状态的局部可观察性可能加剧奖励误识别，必须注意保证学习到的奖励的可识别性和因果性。",
    "en_tdlr": "There exist causal confusion and reward misidentification in preference-based reward learning. Non-causal distractors, noise in stated preferences, and partial state observability can exacerbate reward misidentification. It is important to ensure the identifiability and causality of learned rewards."
}