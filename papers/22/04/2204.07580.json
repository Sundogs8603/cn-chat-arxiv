{
    "title": "mGPT: Few-Shot Learners Go Multilingual. (arXiv:2204.07580v2 [cs.CL] UPDATED)",
    "abstract": "Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero- and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model t",
    "link": "http://arxiv.org/abs/2204.07580",
    "context": "Title: mGPT: Few-Shot Learners Go Multilingual. (arXiv:2204.07580v2 [cs.CL] UPDATED)\nAbstract: Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero- and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model t",
    "path": "papers/22/04/2204.07580.json",
    "total_tokens": 929,
    "translated_title": "mGPT: 少样本学习者走向多语言（arXiv:2204.07580v2 [cs.CL] 更新）",
    "translated_abstract": "最近的研究报告称，自回归语言模型可以通过零样本和少样本学习范式成功解决许多自然语言处理任务，这为使用预训练语言模型开辟了新的可能性。本文介绍了两种自回归GPT样式模型，其参数分别为13亿和130亿，使用维基百科和巨大干净爬取的语料库训练了25个语系中的60种语言。我们使用GPT-2源代码和稀疏注意机制复现了GPT-3架构；Deepspeed和Megatron框架使我们能够有效地并行化训练和推断步骤。所得到的模型在性能上与Facebook最近发布的XGLM模型相当，在覆盖更多语言的同时增强了独联体国家和俄罗斯小国家等低资源语言的自然语言处理可能性。我们详细说明了架构设计的动机，详细描述了数据准备流程，并训练了五个小版本的模型。",
    "tldr": "本文介绍了两种自回归GPT样式模型，分别使用13亿和130亿个参数，在60种语言中训练，并展示了与Facebook最近发布的XGLM模型性能相当的结果。这为低资源语言的自然语言处理提供了更多可能性。"
}