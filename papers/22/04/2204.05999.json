{
    "title": "InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v3 [cs.SE] UPDATED)",
    "abstract": "Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. htt",
    "link": "http://arxiv.org/abs/2204.05999",
    "context": "Title: InCoder: A Generative Model for Code Infilling and Synthesis. (arXiv:2204.05999v3 [cs.SE] UPDATED)\nAbstract: Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. htt",
    "path": "papers/22/04/2204.05999.json",
    "total_tokens": 843,
    "translated_title": "InCoder：一种代码填充和合成的生成模型",
    "translated_abstract": "代码往往不是一次从左到右的写作过程，而是反复编辑和改进的过程。我们引入了InCoder，一种统一的生成模型，可以通过从左到右的生成进行程序合成，也可以进行编辑（通过填充）。InCoder通过从一个大型开源代码库中随机屏蔽代码块并将其移动到每个文件末尾的方式进行训练，使其可以进行双向上下文的代码填充。我们的模型是第一个能够直接进行零样本代码填充的生成模型，我们在类型推断、注释生成和变量重命名等具有挑战性的任务上进行了评估。我们发现，在具有双向上下文的条件下，能够显著改善执行这些任务的性能，而在标准程序合成基准测试中，与相似规模的仅从左到右预先训练的模型相比，性能相当。InCoder模型和代码已经公开发布。",
    "tldr": "InCoder是一种统一的生成模型，可以进行程序合成和双向上下文的代码填充，是第一个能够直接进行零样本代码填充的生成模型。",
    "en_tdlr": "InCoder is a unified generative model that can perform program synthesis and bidirectional context code infilling, which is the first generative model that is able to directly perform zero-shot code infilling."
}