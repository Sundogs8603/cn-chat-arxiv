{
    "title": "Federated Progressive Sparsification (Purge, Merge, Tune)+. (arXiv:2204.12430v2 [cs.LG] UPDATED)",
    "abstract": "To improve federated training of neural networks, we develop FedSparsify, a sparsification strategy based on progressive weight magnitude pruning. Our method has several benefits. First, since the size of the network becomes increasingly smaller, computation and communication costs during training are reduced. Second, the models are incrementally constrained to a smaller set of parameters, which facilitates alignment/merging of the local models and improved learning performance at high sparsification rates. Third, the final sparsified model is significantly smaller, which improves inference efficiency and optimizes operations latency during encrypted communication. We show experimentally that FedSparsify learns a subnetwork of both high sparsity and learning performance. Our sparse models can reach a tenth of the size of the original model with the same or better accuracy compared to existing pruning and nonpruning baselines.",
    "link": "http://arxiv.org/abs/2204.12430",
    "context": "Title: Federated Progressive Sparsification (Purge, Merge, Tune)+. (arXiv:2204.12430v2 [cs.LG] UPDATED)\nAbstract: To improve federated training of neural networks, we develop FedSparsify, a sparsification strategy based on progressive weight magnitude pruning. Our method has several benefits. First, since the size of the network becomes increasingly smaller, computation and communication costs during training are reduced. Second, the models are incrementally constrained to a smaller set of parameters, which facilitates alignment/merging of the local models and improved learning performance at high sparsification rates. Third, the final sparsified model is significantly smaller, which improves inference efficiency and optimizes operations latency during encrypted communication. We show experimentally that FedSparsify learns a subnetwork of both high sparsity and learning performance. Our sparse models can reach a tenth of the size of the original model with the same or better accuracy compared to existing pruning and nonpruning baselines.",
    "path": "papers/22/04/2204.12430.json",
    "total_tokens": 944,
    "translated_title": "联邦递进稀疏化（清理、合并、调整）+：一种用于联邦学习的稀疏化策略",
    "translated_abstract": "为了改进神经网络的联邦训练，我们开发了FedSparsify，一种基于渐进权重幅度剪枝的稀疏化策略。我们的方法有几个好处。首先，由于网络的大小越来越小，在训练期间计算和通信成本得到降低。其次，模型逐步限制为较小的参数集，有利于合并本地模型，提高高级稀疏率下的学习性能。第三，最终稀疏模型显著缩小，改善了推理效率，并优化了加密通信期间的操作延迟。我们的实验表明，FedSparsify可以学习具有高稀疏性和学习性能的子网络。相比于现有的剪枝和非剪枝基线，我们的稀疏模型在达到相同或更好的准确性的情况下，可以缩小原始模型的十分之一。",
    "tldr": "本文提出了一种联邦学习的稀疏化策略，可以逐步约束模型的参数集，降低计算和通信成本，同时达到高性能的稀疏化率。实验表明，稀疏模型可以缩小原始模型的十分之一，而准确率不降低或更高。"
}