{
    "title": "COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks. (arXiv:2204.09593v2 [cs.CL] UPDATED)",
    "abstract": "Vision outlooker improves the performance of vision transformers, which implements a self-attention mechanism by adding an outlook attention, a form of local attention.  In natural language processing, as has been the case in computer vision and other domains, transformer-based models constitute the state-of-the-art for most processing tasks. In this domain, too, many authors have argued and demonstrated the importance of local context.  We present an outlook attention mechanism, COOL, for natural language processing. COOL, added on top of the self-attention layers of a transformer-based model, encodes local syntactic context considering word proximity and more pair-wise constraints than dynamic convolution used by existing approaches.  A comparative empirical performance evaluation of an implementation of COOL with different transformer-based models confirms the opportunity for improvement over a baseline using the original models alone for various natural language processing tasks, i",
    "link": "http://arxiv.org/abs/2204.09593",
    "context": "Title: COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks. (arXiv:2204.09593v2 [cs.CL] UPDATED)\nAbstract: Vision outlooker improves the performance of vision transformers, which implements a self-attention mechanism by adding an outlook attention, a form of local attention.  In natural language processing, as has been the case in computer vision and other domains, transformer-based models constitute the state-of-the-art for most processing tasks. In this domain, too, many authors have argued and demonstrated the importance of local context.  We present an outlook attention mechanism, COOL, for natural language processing. COOL, added on top of the self-attention layers of a transformer-based model, encodes local syntactic context considering word proximity and more pair-wise constraints than dynamic convolution used by existing approaches.  A comparative empirical performance evaluation of an implementation of COOL with different transformer-based models confirms the opportunity for improvement over a baseline using the original models alone for various natural language processing tasks, i",
    "path": "papers/22/04/2204.09593.json",
    "total_tokens": 793,
    "translated_title": "COOL：一种上下文观察器及其在问答和其他自然语言处理任务中的应用",
    "translated_abstract": "视觉上下文观察器通过添加一个局部注意力形式的观察注意力改进了视觉Transformer的性能。与计算机视觉和其他领域一样，在自然语言处理中，基于Transformer的模型构成了大多数处理任务的最先进技术。我们提出了一种自然语言处理的观察注意力机制COOL。COOL添加到基于Transformer的模型的自我注意力层之上，考虑单词的接近性和更多的成对约束，编码局部句法上下文。使用不同的基于Transformer的模型对COOL的实现进行比较实证性能评估，证实其在各种自然语言处理任务中改进基线模型的机会，包括问答。",
    "tldr": "COOL是一种上下文观察器，用于编码局部句法上下文，能够改进基于Transformer的模型在自然语言处理任务中的性能，包括问答。",
    "en_tdlr": "COOL is an outlook attention mechanism for natural language processing, which encodes local syntactic context and can improve the performance of transformer-based models in various tasks, including question answering."
}