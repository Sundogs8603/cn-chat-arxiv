{
    "title": "A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability. (arXiv:2204.08570v2 [cs.LG] UPDATED)",
    "abstract": "Graph Neural Networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trustworthy GNNs in various aspects are emerging to prevent the harm from GNN models a",
    "link": "http://arxiv.org/abs/2204.08570",
    "context": "Title: A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability. (arXiv:2204.08570v2 [cs.LG] UPDATED)\nAbstract: Graph Neural Networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trustworthy GNNs in various aspects are emerging to prevent the harm from GNN models a",
    "path": "papers/22/04/2204.08570.json",
    "total_tokens": 1037,
    "translated_title": "关于可信图神经网络的综合调查：隐私、鲁棒性、公平性和可解释性",
    "translated_abstract": "近年来，图神经网络（GNNs）取得了快速的发展。由于它们在建模图结构数据方面的强大能力，GNNs被广泛应用于各种应用中，包括高风险场景，如金融分析、交通预测和药物发现。尽管GNNs在真实世界中对人类有巨大潜力带来好处，但最近的研究表明，GNNs可能会泄露个人信息，容易受到对抗性攻击，可能从训练数据中继承和放大社会偏见，并且缺乏可解释性，这可能会给用户和社会带来无意中的伤害风险。例如，现有的研究表明，攻击者可以通过对训练图进行不可察觉的扰动来欺骗GNNs以给出他们想要的结果。在社交网络上训练的GNNs可能在其决策过程中嵌入歧视，加强不可取的社会偏见。因此，可信赖的GNNs在各个方面正在出现以防止GNN模型造成的伤害。",
    "tldr": "这篇论文综合调查了可信图神经网络的隐私、鲁棒性、公平性和可解释性问题。它指出了GNNs可能泄露个人信息、容易受到对抗性攻击以及可能放大社会偏见的风险。为了避免这些伤害，一些可信赖的GNNs方法正在发展中。",
    "en_tdlr": "This paper presents a comprehensive survey on the issues of privacy, robustness, fairness, and explainability in trustworthy graph neural networks (GNNs). It highlights the risks of GNNs leaking private information, being vulnerable to adversarial attacks, and amplifying societal bias. In order to prevent such harm, trustworthy GNNs approaches are emerging."
}