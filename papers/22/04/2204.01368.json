{
    "title": "Training Fully Connected Neural Networks is $\\exists\\mathbb{R}$-Complete",
    "abstract": "arXiv:2204.01368v3 Announce Type: replace-cross  Abstract: We consider the problem of finding weights and biases for a two-layer fully connected neural network to fit a given set of data points as well as possible, also known as EmpiricalRiskMinimization. Our main result is that the associated decision problem is $\\exists\\mathbb{R}$-complete, that is, polynomial-time equivalent to determining whether a multivariate polynomial with integer coefficients has any real roots. Furthermore, we prove that algebraic numbers of arbitrarily large degree are required as weights to be able to train some instances to optimality, even if all data points are rational. Our result already applies to fully connected instances with two inputs, two outputs, and one hidden layer of ReLU neurons. Thereby, we strengthen a result by Abrahamsen, Kleist and Miltzow [NeurIPS 2021]. A consequence of this is that a combinatorial search algorithm like the one by Arora, Basu, Mianjy and Mukherjee [ICLR 2018] is impos",
    "link": "https://arxiv.org/abs/2204.01368",
    "context": "Title: Training Fully Connected Neural Networks is $\\exists\\mathbb{R}$-Complete\nAbstract: arXiv:2204.01368v3 Announce Type: replace-cross  Abstract: We consider the problem of finding weights and biases for a two-layer fully connected neural network to fit a given set of data points as well as possible, also known as EmpiricalRiskMinimization. Our main result is that the associated decision problem is $\\exists\\mathbb{R}$-complete, that is, polynomial-time equivalent to determining whether a multivariate polynomial with integer coefficients has any real roots. Furthermore, we prove that algebraic numbers of arbitrarily large degree are required as weights to be able to train some instances to optimality, even if all data points are rational. Our result already applies to fully connected instances with two inputs, two outputs, and one hidden layer of ReLU neurons. Thereby, we strengthen a result by Abrahamsen, Kleist and Miltzow [NeurIPS 2021]. A consequence of this is that a combinatorial search algorithm like the one by Arora, Basu, Mianjy and Mukherjee [ICLR 2018] is impos",
    "path": "papers/22/04/2204.01368.json",
    "total_tokens": 909,
    "translated_title": "训练全连接神经网络是$\\exists\\mathbb{R}$-完全的",
    "translated_abstract": "我们考虑在一个两层全连接神经网络中找到适合给定数据点集的权重和偏置，也称为经验风险最小化的问题。我们的主要结果是相关的决策问题是$\\exists\\mathbb{R}$-完全的，即，多项式时间等价于确定多变量整系数多项式是否有任何实根。此外，我们证明需要具有任意大阶的代数数作为权重，才能训练某些实例达到最优，即使所有数据点都是有理数。我们的结果已经适用于具有两个输入、两个输出和一个使用ReLU神经元的隐藏层的全连接实例。因此，我们加强了Abrahamsen、Kleist和Miltzow在NeurIPS 2021中的一个结果。由此带来的一个结论是像Arora、Basu、Mianjy和Mukherjee在ICLR 2018中提出的一种组合搜索算法是不可能实现的。",
    "tldr": "该论文证明了训练全连接神经网络的权重和偏置的相关决策问题是$\\exists\\mathbb{R}$-完全的，同时指出必须使用任意大阶的代数数作为权重才能训练一些实例达到最优。",
    "en_tdlr": "This paper proves that the decision problem of finding weights and biases for training a fully connected neural network is $\\exists\\mathbb{R}$-complete, and shows that algebraic numbers of arbitrarily large degree are required as weights to train some instances to optimality."
}