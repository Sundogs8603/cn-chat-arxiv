{
    "title": "Jump-Start Reinforcement Learning. (arXiv:2204.02372v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tas",
    "link": "http://arxiv.org/abs/2204.02372",
    "context": "Title: Jump-Start Reinforcement Learning. (arXiv:2204.02372v2 [cs.LG] UPDATED)\nAbstract: Reinforcement learning (RL) provides a theoretical framework for continuously improving an agent's behavior via trial and error. However, efficiently learning policies from scratch can be very difficult, particularly for tasks with exploration challenges. In such settings, it might be desirable to initialize RL with an existing policy, offline data, or demonstrations. However, naively performing such initialization in RL often works poorly, especially for value-based methods. In this paper, we present a meta algorithm that can use offline data, demonstrations, or a pre-existing policy to initialize an RL policy, and is compatible with any RL approach. In particular, we propose Jump-Start Reinforcement Learning (JSRL), an algorithm that employs two policies to solve tasks: a guide-policy, and an exploration-policy. By using the guide-policy to form a curriculum of starting states for the exploration-policy, we are able to efficiently improve performance on a set of simulated robotic tas",
    "path": "papers/22/04/2204.02372.json",
    "total_tokens": 915,
    "translated_title": "启动强化学习",
    "translated_abstract": "强化学习（RL）通过试错的方式提供了一个理论框架，从而不断改进智能体的行为。然而，从零开始高效地学习策略可能非常困难，特别是对于具有探索挑战的任务来说。在这种情况下，使用现有策略、离线数据或演示来初始化RL可能是可行的。然而，对RL进行这种初始化常常效果不佳，特别是对于基于值的方法。本文提出了一种可以使用离线数据、演示或现有策略来初始化RL策略，并且与任何RL方法兼容的元算法。具体而言，我们提出了一种名为Jump-Start Reinforcement Learning (JSRL)的算法，它利用两个策略来解决任务：一个导引策略和一个探索策略。通过使用导引策略为探索策略形成一个初始状态的课程，我们能够在一组模拟机器人任务中高效地提高性能。",
    "tldr": "本文介绍了一种元算法，可以使用离线数据、演示或现有策略来初始化强化学习策略，并且与任何强化学习方法兼容。通过使用导引策略来形成探索策略的初始状态课程，能够提高在模拟机器人任务中的性能。",
    "en_tdlr": "This paper presents a meta algorithm that can initialize reinforcement learning (RL) policy using offline data, demonstrations, or a pre-existing policy, and is compatible with any RL approach. By using a guide-policy to form a curriculum of starting states for the exploration-policy, it efficiently improves performance on simulated robotic tasks."
}