{
    "title": "PALBERT: Teaching ALBERT to Ponder. (arXiv:2204.03276v4 [cs.LG] UPDATED)",
    "abstract": "Currently, pre-trained models can be considered the default choice for a wide range of NLP tasks. Despite their SoTA results, there is practical evidence that these models may require a different number of computing layers for different input sequences, since evaluating all layers leads to overconfidence in wrong predictions (namely overthinking). This problem can potentially be solved by implementing adaptive computation time approaches, which were first designed to improve inference speed. Recently proposed PonderNet may be a promising solution for performing an early exit by treating the exit layer's index as a latent variable. However, the originally proposed exit criterion, relying on sampling from trained posterior distribution on the probability of exiting from the $i$-th layer, introduces major variance in exit layer indices, significantly reducing the resulting model's performance. In this paper, we propose improving PonderNet with a novel deterministic Q-exit criterion and a ",
    "link": "http://arxiv.org/abs/2204.03276",
    "context": "Title: PALBERT: Teaching ALBERT to Ponder. (arXiv:2204.03276v4 [cs.LG] UPDATED)\nAbstract: Currently, pre-trained models can be considered the default choice for a wide range of NLP tasks. Despite their SoTA results, there is practical evidence that these models may require a different number of computing layers for different input sequences, since evaluating all layers leads to overconfidence in wrong predictions (namely overthinking). This problem can potentially be solved by implementing adaptive computation time approaches, which were first designed to improve inference speed. Recently proposed PonderNet may be a promising solution for performing an early exit by treating the exit layer's index as a latent variable. However, the originally proposed exit criterion, relying on sampling from trained posterior distribution on the probability of exiting from the $i$-th layer, introduces major variance in exit layer indices, significantly reducing the resulting model's performance. In this paper, we propose improving PonderNet with a novel deterministic Q-exit criterion and a ",
    "path": "papers/22/04/2204.03276.json",
    "total_tokens": 1037,
    "tldr": "本文提出了一种新的自适应计算时间方法 PALBERT，通过教授 ALBERT 模型思考，使用经过改进的 PonderNet 来选择退出层，以提高计算效率和准确性。",
    "en_tdlr": "This paper proposes a new adaptive computation time approach, PALBERT, which teaches the ALBERT model to ponder and improves computational efficiency and accuracy by using an improved PonderNet for exit layer selection."
}