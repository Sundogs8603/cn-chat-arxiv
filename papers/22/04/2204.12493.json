{
    "title": "One-shot Federated Learning without Server-side Training. (arXiv:2204.12493v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) has recently made significant progress as a new machine learning paradigm for privacy protection. Due to the high communication cost of traditional FL, one-shot federated learning is gaining popularity as a way to reduce communication cost between clients and the server. Most of the existing one-shot FL methods are based on Knowledge Distillation; however, {distillation based approach requires an extra training phase and depends on publicly available data sets or generated pseudo samples.} In this work, we consider a novel and challenging cross-silo setting: performing a single round of parameter aggregation on the local models without server-side training. In this setting, we propose an effective algorithm for Model Aggregation via Exploring Common Harmonized Optima (MA-Echo), which iteratively updates the parameters of all local models to bring them close to a common low-loss area on the loss surface, without harming performance on their own data sets at the s",
    "link": "http://arxiv.org/abs/2204.12493",
    "context": "Title: One-shot Federated Learning without Server-side Training. (arXiv:2204.12493v2 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) has recently made significant progress as a new machine learning paradigm for privacy protection. Due to the high communication cost of traditional FL, one-shot federated learning is gaining popularity as a way to reduce communication cost between clients and the server. Most of the existing one-shot FL methods are based on Knowledge Distillation; however, {distillation based approach requires an extra training phase and depends on publicly available data sets or generated pseudo samples.} In this work, we consider a novel and challenging cross-silo setting: performing a single round of parameter aggregation on the local models without server-side training. In this setting, we propose an effective algorithm for Model Aggregation via Exploring Common Harmonized Optima (MA-Echo), which iteratively updates the parameters of all local models to bring them close to a common low-loss area on the loss surface, without harming performance on their own data sets at the s",
    "path": "papers/22/04/2204.12493.json",
    "total_tokens": 828,
    "translated_title": "无服务器端训练的一次性联邦学习",
    "translated_abstract": "联邦学习是一种新的机器学习范式，旨在保护隐私。最近，由于传统联邦学习的高通信代价，一次性联邦学习开始受到关注，以减少客户端和服务器之间的通信成本。本文提出了一种无服务器端训练的交叉场所的有效算法MA-Echo，通过探索公共调和最优区域，迭代更新所有本地模型的参数，将它们拉近到损失表面上的一个低损失区域，而不会影响它们在其自身数据集上的性能。",
    "tldr": "本文提出了一种无需额外训练阶段和公共数据集的交叉场所的一次性联邦学习算法MA-Echo，该算法通过调和最优区域迭代更新所有本地模型的参数，从而将它们拉近到一个低损失区域，而不会影响模型在自身数据集上的性能。",
    "en_tdlr": "This paper proposes a one-shot federated learning algorithm, MA-Echo, which is designed for cross-silo setting without server-side training. It updates the parameters of all local models to bring them close to a common low-loss area on the loss surface, without harming performance on their own data sets, thus eliminating the need for extra training phase and publicly available datasets."
}