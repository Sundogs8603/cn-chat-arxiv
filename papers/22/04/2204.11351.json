{
    "title": "An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models. (arXiv:2204.11351v3 [cs.LG] UPDATED)",
    "abstract": "Nowadays, the interpretation of why a machine learning (ML) model makes certain inferences is as crucial as the accuracy of such inferences. Some ML models like the decision tree possess inherent interpretability that can be directly comprehended by humans. Others like artificial neural networks (ANN), however, rely on external methods to uncover the deduction mechanism. SHapley Additive exPlanations (SHAP) is one of such external methods, which requires a background dataset when interpreting ANNs. Generally, a background dataset consists of instances randomly sampled from the training dataset. However, the sampling size and its effect on SHAP remain to be unexplored. In our empirical study on the MIMIC-III dataset, we show that the two core explanations - SHAP values and variable rankings fluctuate when using different background datasets acquired from random sampling, indicating that users cannot unquestioningly trust the one-shot interpretation from SHAP. Luckily, such fluctuation d",
    "link": "http://arxiv.org/abs/2204.11351",
    "context": "Title: An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models. (arXiv:2204.11351v3 [cs.LG] UPDATED)\nAbstract: Nowadays, the interpretation of why a machine learning (ML) model makes certain inferences is as crucial as the accuracy of such inferences. Some ML models like the decision tree possess inherent interpretability that can be directly comprehended by humans. Others like artificial neural networks (ANN), however, rely on external methods to uncover the deduction mechanism. SHapley Additive exPlanations (SHAP) is one of such external methods, which requires a background dataset when interpreting ANNs. Generally, a background dataset consists of instances randomly sampled from the training dataset. However, the sampling size and its effect on SHAP remain to be unexplored. In our empirical study on the MIMIC-III dataset, we show that the two core explanations - SHAP values and variable rankings fluctuate when using different background datasets acquired from random sampling, indicating that users cannot unquestioningly trust the one-shot interpretation from SHAP. Luckily, such fluctuation d",
    "path": "papers/22/04/2204.11351.json",
    "total_tokens": 993,
    "translated_title": "背景数据规模对深度学习模型 SHAP 解释稳定性的影响的实证研究",
    "translated_abstract": "如今，机器学习模型的推断结果准确性同样重要的是其可解释性。某些机器学习模型如决策树拥有天然的可解释性，而其他模型如人工神经网络则需要外部方法揭示其推断机制。SHapley Additive exPlanations (SHAP)就是一种外部解释方式，它需要一个背景数据集对人工神经网络进行解释。通常，背景数据集由从训练数据集中随机抽样的实例组成。然而，背景数据集的抽样规模及其对 SHAP 的影响仍未被探讨。在我们对 MIMIC-III 数据集进行的实证研究中，我们发现使用不同的随机抽样得到的背景数据集会导致核心解释—— SHAP 值和变量排序值的波动，这表明用户不能轻信 SHAP 提供的一次性解释结果。幸运的是，这样的波动并不意味着 SHAP 失败，而是表明选择合适的背景数据集对于确保 SHAP 解释的稳定性至关重要。",
    "tldr": "本研究发现 SHapley Additive exPlanations (SHAP) 对于机器学习模型的解释受到背景数据集规模的影响，而选择合适的背景数据集能够确保 SHAP 解释的稳定性。",
    "en_tdlr": "This empirical study shows that SHapley Additive exPlanations (SHAP) for interpreting machine learning models is affected by the size of the background dataset, and selecting a suitable background dataset is crucial for ensuring the stability of SHAP interpretation."
}