{
    "title": "Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base. (arXiv:2204.07994v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained language models (PLMs) like BERT have made significant progress in various downstream NLP tasks. However, by asking models to do cloze-style tests, recent work finds that PLMs are short in acquiring knowledge from unstructured text. To understand the internal behaviour of PLMs in retrieving knowledge, we first define knowledge-baring (K-B) tokens and knowledge-free (K-F) tokens for unstructured text and ask professional annotators to label some samples manually. Then, we find that PLMs are more likely to give wrong predictions on K-B tokens and attend less attention to those tokens inside the self-attention module. Based on these observations, we develop two solutions to help the model learn more knowledge from unstructured text in a fully self-supervised manner. Experiments on knowledge-intensive tasks show the effectiveness of the proposed methods. To our best knowledge, we are the first to explore fully self-supervised learning of knowledge in continual pre-training.",
    "link": "http://arxiv.org/abs/2204.07994",
    "context": "Title: Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base. (arXiv:2204.07994v2 [cs.CL] UPDATED)\nAbstract: Pre-trained language models (PLMs) like BERT have made significant progress in various downstream NLP tasks. However, by asking models to do cloze-style tests, recent work finds that PLMs are short in acquiring knowledge from unstructured text. To understand the internal behaviour of PLMs in retrieving knowledge, we first define knowledge-baring (K-B) tokens and knowledge-free (K-F) tokens for unstructured text and ask professional annotators to label some samples manually. Then, we find that PLMs are more likely to give wrong predictions on K-B tokens and attend less attention to those tokens inside the self-attention module. Based on these observations, we develop two solutions to help the model learn more knowledge from unstructured text in a fully self-supervised manner. Experiments on knowledge-intensive tasks show the effectiveness of the proposed methods. To our best knowledge, we are the first to explore fully self-supervised learning of knowledge in continual pre-training.",
    "path": "papers/22/04/2204.07994.json",
    "total_tokens": 923,
    "translated_title": "知识显著跨度掩码：增强语言模型作为知识库",
    "translated_abstract": "预训练语言模型（PLMs）如BERT在各种下游NLP任务中取得了显著进展。然而，通过要求模型进行填空式测试，最近的研究发现PLMs在从非结构化文本中获取知识方面存在不足。为了了解PLMs在检索知识时的内部行为，我们首先为非结构化文本定义了知识可见（K-B）标记和无知识（K-F）标记，并请专业标注员手动标记了一些样本。然后，我们发现PLMs更有可能对K-B标记给出错误预测，并且在自注意力模块内部对这些标记关注的更少。基于这些观察结果，我们以全自监督的方式开发了两种解决方案，帮助模型从非结构化文本中学习更多的知识。在知识密集型任务上的实验证明了所提方法的有效性。据我们所知，我们是第一个探索全自监督学习持续预训练知识的研究者。",
    "tldr": "本文提出了一个名为知识显著跨度掩码的方法，通过全自监督学习帮助语言模型从非结构化文本中获取更多的知识。实验证明了该方法在知识密集型任务中的有效性。",
    "en_tdlr": "This paper proposes a method called Knowledgeable Salient Span Mask to help language models acquire more knowledge from unstructured text through fully self-supervised learning. The effectiveness of the proposed method is demonstrated through experiments on knowledge-intensive tasks."
}