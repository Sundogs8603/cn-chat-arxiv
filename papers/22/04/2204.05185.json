{
    "title": "Uniform Complexity for Text Generation. (arXiv:2204.05185v3 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) have shown promising results in a wide array of generative NLP tasks, such as summarization and machine translation. In the context of narrative generation, however, existing models still do not capture factors that contribute to producing consistent text. For instance, it is logical that a piece of text or a story should be uniformly readable throughout and that this form of complexity should be controllable. As such, if the complexity of an input text prompt is rated first-grade reading level in the Flesch Reading Ease test, then the generated text continuing the plot should also be within this range of complexity. With this in mind, we introduce Uniform Complexity for Text Generation (UCTG), a new benchmark test which raises the challenge of making generative models observe uniform linguistic properties with respect to prompts. We experiment with over 150+ linguistically and cognitively motivated features for evaluating text complexity in humans and gene",
    "link": "http://arxiv.org/abs/2204.05185",
    "context": "Title: Uniform Complexity for Text Generation. (arXiv:2204.05185v3 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) have shown promising results in a wide array of generative NLP tasks, such as summarization and machine translation. In the context of narrative generation, however, existing models still do not capture factors that contribute to producing consistent text. For instance, it is logical that a piece of text or a story should be uniformly readable throughout and that this form of complexity should be controllable. As such, if the complexity of an input text prompt is rated first-grade reading level in the Flesch Reading Ease test, then the generated text continuing the plot should also be within this range of complexity. With this in mind, we introduce Uniform Complexity for Text Generation (UCTG), a new benchmark test which raises the challenge of making generative models observe uniform linguistic properties with respect to prompts. We experiment with over 150+ linguistically and cognitively motivated features for evaluating text complexity in humans and gene",
    "path": "papers/22/04/2204.05185.json",
    "total_tokens": 860,
    "translated_title": "文本生成的统一复杂度",
    "translated_abstract": "大型语言模型（LLMs）在多个生成型自然语言处理任务中取得了令人期待的结果，比如总结和机器翻译。然而，在叙述生成的背景下，现有模型仍未捕捉到产生一致文本所涉及的因素。例如，逻辑上讲，一段文本或一个故事应该在始终可读，这种复杂度是可控制的。因此，如果输入文本提示在弗列许读易度测试中评为一年级阅读水平，那么延续情节的生成文本也应在此复杂度范围内。基于此，我们引入了文本生成的统一复杂度（UCTG），这是一个新的基准测试，提出了使生成模型在提示方面遵循统一语言属性的挑战。我们通过使用150多个与语言和认知相关的特征来评估人类和生成文本的复杂度进行实验。",
    "tldr": "提出了一种新的基准测试，称为统一复杂度文本生成（UCTG），要求生成模型在不同的文本提示情况下遵循统一的语言属性。该测试使用了150多个与人类和生成文本复杂度相关的特征进行评估。",
    "en_tdlr": "Introducing Uniform Complexity for Text Generation (UCTG), a new benchmark test that challenges generative models to adhere to consistent linguistic properties across different text prompts. The test utilizes over 150 features related to human and generated text complexity for evaluation."
}