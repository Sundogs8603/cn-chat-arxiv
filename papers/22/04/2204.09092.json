{
    "title": "Indiscriminate Data Poisoning Attacks on Neural Networks",
    "abstract": "arXiv:2204.09092v2 Announce Type: replace  Abstract: Data poisoning attacks, in which a malicious adversary aims to influence a model by injecting \"poisoned\" data into the training process, have attracted significant recent attention. In this work, we take a closer look at existing poisoning attacks and connect them with old and new algorithms for solving sequential Stackelberg games. By choosing an appropriate loss function for the attacker and optimizing with algorithms that exploit second-order information, we design poisoning attacks that are effective on neural networks. We present efficient implementations that exploit modern auto-differentiation packages and allow simultaneous and coordinated generation of tens of thousands of poisoned points, in contrast to existing methods that generate poisoned points one by one. We further perform extensive experiments that empirically explore the effect of data poisoning attacks on deep neural networks.",
    "link": "https://arxiv.org/abs/2204.09092",
    "context": "Title: Indiscriminate Data Poisoning Attacks on Neural Networks\nAbstract: arXiv:2204.09092v2 Announce Type: replace  Abstract: Data poisoning attacks, in which a malicious adversary aims to influence a model by injecting \"poisoned\" data into the training process, have attracted significant recent attention. In this work, we take a closer look at existing poisoning attacks and connect them with old and new algorithms for solving sequential Stackelberg games. By choosing an appropriate loss function for the attacker and optimizing with algorithms that exploit second-order information, we design poisoning attacks that are effective on neural networks. We present efficient implementations that exploit modern auto-differentiation packages and allow simultaneous and coordinated generation of tens of thousands of poisoned points, in contrast to existing methods that generate poisoned points one by one. We further perform extensive experiments that empirically explore the effect of data poisoning attacks on deep neural networks.",
    "path": "papers/22/04/2204.09092.json",
    "total_tokens": 785,
    "translated_title": "对神经网络的任意数据污染攻击",
    "translated_abstract": "数据污染攻击是指恶意对手通过将“污染”的数据注入到训练过程中来影响模型的攻击，近年来引起了广泛的关注。本研究对现有的污染攻击进行了详细研究，并将其与解决顺序斯塔克伯格博弈的新老算法联系起来。通过为攻击者选择适当的损失函数，并利用利用二阶信息的算法进行优化，我们设计了对神经网络有效的污染攻击。我们提出了高效的实现方法，利用现代自动微分软件包同时、协调地生成数万个污染点，与现有的逐个生成污染点的方法相比。此外，我们还进行了大量实验证明了数据污染攻击对深度神经网络的影响。",
    "tldr": "本研究关注对神经网络的任意数据污染攻击，利用二阶信息进行优化设计出了有效的攻击方法，并通过大量实验证明了对深度神经网络的影响。"
}