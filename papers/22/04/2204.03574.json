{
    "title": "Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v3 [cs.LG] UPDATED)",
    "abstract": "We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) like CLIP. We develop CSP for compositional zero-shot learning, the task of predicting unseen attribute-object compositions (e.g., old cat and young tiger). VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets. CSP treats the attributes and objects that define classes as learnable tokens of vocabulary. During training, the vocabulary is tuned to recognize classes that compose tokens in multiple ways (e.g., old cat and white cat). At test time, we recompose the learned attribute-object vocabulary in new combinations to recognize novel classes. We show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9 percentage points on AUC. CSP",
    "link": "http://arxiv.org/abs/2204.03574",
    "context": "Title: Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. (arXiv:2204.03574v3 [cs.LG] UPDATED)\nAbstract: We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) like CLIP. We develop CSP for compositional zero-shot learning, the task of predicting unseen attribute-object compositions (e.g., old cat and young tiger). VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets. CSP treats the attributes and objects that define classes as learnable tokens of vocabulary. During training, the vocabulary is tuned to recognize classes that compose tokens in multiple ways (e.g., old cat and white cat). At test time, we recompose the learned attribute-object vocabulary in new combinations to recognize novel classes. We show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9 percentage points on AUC. CSP",
    "path": "papers/22/04/2204.03574.json",
    "total_tokens": 928,
    "translated_title": "学习组合软提示以用于组合式零样本学习",
    "translated_abstract": "我们提出了组合软提示（CSP），这是一种参数高效的学习技术，用于改善大规模预训练的视觉语言模型（VLMs）如CLIP的零样本组合性。我们将CSP开发用于组合式零样本学习，也就是预测看不见的属性-对象组合（例如老猫和小老虎）。VLM具有灵活的文本编码器，可以用自然语言提示表示任意类别，但它们通常在组合零样本基准数据集上表现不如特定任务的体系结构。CSP将定义类别的属性和对象视为可学习的词汇标记。在训练期间，词汇表被调整为识别以多种方式组成令牌的类别（例如老猫和白猫）。在测试时，我们将所学的属性-对象词汇表以新的组合方式重新组合，以识别新的类别。我们展示了CSP在基准数据集上比CLIP平均高10.9个百分点的AUC（曲线下面积指标）。",
    "tldr": "通过学习组合式软提示实现了预测看不见的属性-对象组合，超过了基准数据集上的特定体系结构，并在曲线下面积上平均高10.9％。",
    "en_tdlr": "By using compositional soft prompting (CSP), this paper improves the zero-shot compositionality of pretrained vision-language models (VLMs) like CLIP. CSP allows for predicting unseen attribute-object compositions and outperforms task-specific architectures on benchmark datasets by an average of 10.9 percentage points on AUC."
}