{
    "title": "Off-Policy Evaluation with Online Adaptation for Robot Exploration in Challenging Environments. (arXiv:2204.03140v3 [cs.RO] UPDATED)",
    "abstract": "Autonomous exploration has many important applications. However, classic information gain-based or frontier-based exploration only relies on the robot current state to determine the immediate exploration goal, which lacks the capability of predicting the value of future states and thus leads to inefficient exploration decisions. This paper presents a method to learn how \"good\" states are, measured by the state value function, to provide a guidance for robot exploration in real-world challenging environments. We formulate our work as an off-policy evaluation (OPE) problem for robot exploration (OPERE). It consists of offline Monte-Carlo training on real-world data and performs Temporal Difference (TD) online adaptation to optimize the trained value estimator. We also design an intrinsic reward function based on sensor information coverage to enable the robot to gain more information with sparse extrinsic rewards. Results show that our method enables the robot to predict the value of fut",
    "link": "http://arxiv.org/abs/2204.03140",
    "context": "Title: Off-Policy Evaluation with Online Adaptation for Robot Exploration in Challenging Environments. (arXiv:2204.03140v3 [cs.RO] UPDATED)\nAbstract: Autonomous exploration has many important applications. However, classic information gain-based or frontier-based exploration only relies on the robot current state to determine the immediate exploration goal, which lacks the capability of predicting the value of future states and thus leads to inefficient exploration decisions. This paper presents a method to learn how \"good\" states are, measured by the state value function, to provide a guidance for robot exploration in real-world challenging environments. We formulate our work as an off-policy evaluation (OPE) problem for robot exploration (OPERE). It consists of offline Monte-Carlo training on real-world data and performs Temporal Difference (TD) online adaptation to optimize the trained value estimator. We also design an intrinsic reward function based on sensor information coverage to enable the robot to gain more information with sparse extrinsic rewards. Results show that our method enables the robot to predict the value of fut",
    "path": "papers/22/04/2204.03140.json",
    "total_tokens": 979,
    "translated_title": "在挑战性环境中进行机器人探索的在线自适应离策略评估方法",
    "translated_abstract": "自主探索具有许多重要应用。但是，基于信息增益或基于前沿的传统探索仅依赖于机器人当前状态来确定即时探索目标，缺乏预测未来状态价值的能力，因此导致探索决策低效。本文提出了一种方法，学习如何衡量“好”状态（以状态价值函数衡量），为机器人在真实挑战性环境中探索提供指导。我们将我们的工作制定为机器人探索的离策略评估（OPERE）问题。它包括对真实世界数据的离线蒙特卡罗训练，并执行时间差分（TD）在线自适应来优化经过训练的价值估计器。我们还设计了一种基于传感器信息覆盖率的内在奖励函数，以使机器人在稀疏外部奖励的情况下获得更多信息。结果表明，我们的方法使得机器人能够预测未来状态的价值，并在挑战性环境中做出高效的探索决策。",
    "tldr": "本文提出了一种在线自适应离策略评估方法，通过学习状态价值函数来为机器人在真实挑战性环境中探索提供指导，并设计了一种内在奖励函数，使机器人能够在稀疏外部奖励的情况下获得更多信息。"
}