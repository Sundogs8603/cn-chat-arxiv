{
    "title": "Learning to Modulate Random Weights: Neuromodulation-inspired Neural Networks For Efficient Continual Learning. (arXiv:2204.04297v2 [cs.LG] UPDATED)",
    "abstract": "Existing Continual Learning (CL) approaches have focused on addressing catastrophic forgetting by leveraging regularization methods, replay buffers, and task-specific components. However, realistic CL solutions must be shaped not only by metrics of catastrophic forgetting but also by computational efficiency and running time. Here, we introduce a novel neural network architecture inspired by neuromodulation in biological nervous systems to economically and efficiently address catastrophic forgetting and provide new avenues for interpreting learned representations. Neuromodulation is a biological mechanism that has received limited attention in machine learning; it dynamically controls and fine tunes synaptic dynamics in real time to track the demands of different behavioral contexts. Inspired by this, our proposed architecture learns a relatively small set of parameters per task context that \\emph{neuromodulates} the activity of unchanging, randomized weights that transform the input. ",
    "link": "http://arxiv.org/abs/2204.04297",
    "context": "Title: Learning to Modulate Random Weights: Neuromodulation-inspired Neural Networks For Efficient Continual Learning. (arXiv:2204.04297v2 [cs.LG] UPDATED)\nAbstract: Existing Continual Learning (CL) approaches have focused on addressing catastrophic forgetting by leveraging regularization methods, replay buffers, and task-specific components. However, realistic CL solutions must be shaped not only by metrics of catastrophic forgetting but also by computational efficiency and running time. Here, we introduce a novel neural network architecture inspired by neuromodulation in biological nervous systems to economically and efficiently address catastrophic forgetting and provide new avenues for interpreting learned representations. Neuromodulation is a biological mechanism that has received limited attention in machine learning; it dynamically controls and fine tunes synaptic dynamics in real time to track the demands of different behavioral contexts. Inspired by this, our proposed architecture learns a relatively small set of parameters per task context that \\emph{neuromodulates} the activity of unchanging, randomized weights that transform the input. ",
    "path": "papers/22/04/2204.04297.json",
    "total_tokens": 910,
    "translated_title": "学习调节随机权重：受神经调节启发的神经网络用于高效的连续学习",
    "translated_abstract": "现有的连续学习方法主要解决了灾难性遗忘问题，利用正则化方法、回放缓冲区和任务特定的组件。然而，实际连续学习解决方案必须不仅考虑灾难性遗忘的度量指标，还要考虑计算效率和运行时间。在这里，我们引入了一种受生物神经系统中的神经调节启发的新型神经网络架构，以经济高效地解决灾难性遗忘问题，并为解释学习表示提供了新的途径。神经调节是一种生物机制，在机器学习中受到了有限的关注；它以实时动态控制和微调突触动力学来跟踪不同行为背景的需求。受此启发，我们提出的架构学习每个任务环境下的相对较小的一组参数，这些参数对转换输入的不变的、随机化的权重活动进行\\emph{神经调节}。",
    "tldr": "本论文提出了一种受生物神经调节启发的神经网络架构，通过学习调节随机权重的活动来解决连续学习中的灾难性遗忘问题，并提供了新的学习表示解释方法。",
    "en_tdlr": "This paper introduces a novel neural network architecture inspired by neural modulation in biological nervous systems, which learns to modulate the activity of random weights to address catastrophic forgetting in continual learning and provides new ways to interpret learned representations."
}