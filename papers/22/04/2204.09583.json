{
    "title": "Improved Group Robustness via Classifier Retraining on Independent Splits. (arXiv:2204.09583v3 [cs.LG] UPDATED)",
    "abstract": "Deep neural networks trained by minimizing the average risk can achieve strong average performance. Still, their performance for a subgroup may degrade if the subgroup is underrepresented in the overall data population. Group distributionally robust optimization (Sagawa et al., 2020a), or group DRO in short, is a widely used baseline for learning models with strong worst-group performance. We note that this method requires group labels for every example at training time and can overfit to small groups, requiring strong regularization. Given a limited amount of group labels at training time, Just Train Twice (Liu et al., 2021), or JTT in short, is a two-stage method that infers a pseudo group label for every unlabeled example first, then applies group DRO based on the inferred group labels. The inference process is also sensitive to overfitting, sometimes involving additional hyperparameters. This paper designs a simple method based on the idea of classifier retraining on independent sp",
    "link": "http://arxiv.org/abs/2204.09583",
    "context": "Title: Improved Group Robustness via Classifier Retraining on Independent Splits. (arXiv:2204.09583v3 [cs.LG] UPDATED)\nAbstract: Deep neural networks trained by minimizing the average risk can achieve strong average performance. Still, their performance for a subgroup may degrade if the subgroup is underrepresented in the overall data population. Group distributionally robust optimization (Sagawa et al., 2020a), or group DRO in short, is a widely used baseline for learning models with strong worst-group performance. We note that this method requires group labels for every example at training time and can overfit to small groups, requiring strong regularization. Given a limited amount of group labels at training time, Just Train Twice (Liu et al., 2021), or JTT in short, is a two-stage method that infers a pseudo group label for every unlabeled example first, then applies group DRO based on the inferred group labels. The inference process is also sensitive to overfitting, sometimes involving additional hyperparameters. This paper designs a simple method based on the idea of classifier retraining on independent sp",
    "path": "papers/22/04/2204.09583.json",
    "total_tokens": 899,
    "translated_title": "在独立分割上重新训练分类器以提高群体鲁棒性",
    "translated_abstract": "通过最小化平均风险进行训练的深度神经网络可以达到强大的平均性能。然而，如果某个子群在整体数据中代表性不足，它们的性能可能会下降。群体分布鲁棒优化（Sagawa等人，2020a），简称群体DRO，是学习具有强大最差群体性能模型的基准方法。我们注意到该方法在训练时需要为每个示例提供群体标签，并且可能对小群体过拟合，需要强正则化。在训练时只有有限数量的群体标签时，Just Train Twice（Liu等人，2021），简称JTT，是一种两阶段方法，首先为每个无标签示例推断出伪群体标签，然后根据推断的群体标签应用群体DRO。推断过程对过拟合也很敏感，有时涉及额外的超参数。本文设计了一种简单的方法，基于独立分割上的分类器重新训练的思想。",
    "tldr": "通过在独立的分割上重新训练分类器，该方法改善了群体鲁棒性，有助于提高模型在具有少见特征的子群上的性能。"
}