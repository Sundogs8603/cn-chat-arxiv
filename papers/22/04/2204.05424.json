{
    "title": "A Call for Clarity in Beam Search: How It Works and When It Stops",
    "abstract": "arXiv:2204.05424v3 Announce Type: replace  Abstract: Text generation with beam search has proven successful in a wide range of applications. We point out that, though largely overlooked in the literature, the commonly-used implementation of beam decoding (e.g., Hugging Face Transformers and fairseq) uses a first come, first served heuristic: it keeps a set of already completed sequences over time steps and stops when the size of this set reaches the beam size. Based on this finding, we introduce a patience factor, a simple modification to this beam decoding implementation, that generalizes the stopping criterion and provides flexibility to the depth of search. Empirical results demonstrate that adjusting this patience factor improves decoding performance of strong pretrained models on news text summarization and machine translation over diverse language pairs, with a negligible inference slowdown. Our approach only modifies one line of code and can be thus readily incorporated in any i",
    "link": "https://arxiv.org/abs/2204.05424",
    "context": "Title: A Call for Clarity in Beam Search: How It Works and When It Stops\nAbstract: arXiv:2204.05424v3 Announce Type: replace  Abstract: Text generation with beam search has proven successful in a wide range of applications. We point out that, though largely overlooked in the literature, the commonly-used implementation of beam decoding (e.g., Hugging Face Transformers and fairseq) uses a first come, first served heuristic: it keeps a set of already completed sequences over time steps and stops when the size of this set reaches the beam size. Based on this finding, we introduce a patience factor, a simple modification to this beam decoding implementation, that generalizes the stopping criterion and provides flexibility to the depth of search. Empirical results demonstrate that adjusting this patience factor improves decoding performance of strong pretrained models on news text summarization and machine translation over diverse language pairs, with a negligible inference slowdown. Our approach only modifies one line of code and can be thus readily incorporated in any i",
    "path": "papers/22/04/2204.05424.json",
    "total_tokens": 805,
    "translated_title": "对束搜索在文本生成中的工作原理及停止时机的明确要求",
    "translated_abstract": "使用束搜索进行文本生成在各种应用中已被证明成功。我们指出，尽管在文献中被大多数忽视，但常用的束解码实现（例如Hugging Face Transformers和fairseq）使用了先到先服务的启发式方法：它在时间步长上保留一组已完成的序列，并在此集合大小达到束大小时停止。基于这一发现，我们引入了一个耐心因子，对束解码实现进行了简单修改，使停止条件更一般化，并为搜索深度提供了灵活性。实证结果表明，调整这个耐心因子改善了强预训练模型在新闻文本摘要和各种语言对的机器翻译中的解码性能，而推理速度下降微不足道。我们的方法只修改了一行代码，因此可以很容易地在任何i中被纳入。",
    "tldr": "提出了一个简单修改的耐心因子来改善束搜索解码算法，提高了强预训练模型在文本摘要和机器翻译任务上的性能。",
    "en_tdlr": "Introducing a simple modification with a patience factor to improve beam search decoding algorithm, enhancing the performance of strong pretrained models in text summarization and machine translation tasks."
}