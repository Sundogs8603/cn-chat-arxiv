{
    "title": "Finite-Time Analysis of Temporal Difference Learning: Discrete-Time Linear System Perspective. (arXiv:2204.10479v6 [cs.LG] UPDATED)",
    "abstract": "TD-learning is a fundamental algorithm in the field of reinforcement learning (RL), that is employed to evaluate a given policy by estimating the corresponding value function for a Markov decision process. While significant progress has been made in the theoretical analysis of TD-learning, recent research has uncovered guarantees concerning its statistical efficiency by developing finite-time error bounds. This paper aims to contribute to the existing body of knowledge by presenting a novel finite-time analysis of tabular temporal difference (TD) learning, which makes direct and effective use of discrete-time stochastic linear system models and leverages Schur matrix properties. The proposed analysis can cover both on-policy and off-policy settings in a unified manner. By adopting this approach, we hope to offer new and straightforward templates that not only shed further light on the analysis of TD-learning and related RL algorithms but also provide valuable insights for future resear",
    "link": "http://arxiv.org/abs/2204.10479",
    "context": "Title: Finite-Time Analysis of Temporal Difference Learning: Discrete-Time Linear System Perspective. (arXiv:2204.10479v6 [cs.LG] UPDATED)\nAbstract: TD-learning is a fundamental algorithm in the field of reinforcement learning (RL), that is employed to evaluate a given policy by estimating the corresponding value function for a Markov decision process. While significant progress has been made in the theoretical analysis of TD-learning, recent research has uncovered guarantees concerning its statistical efficiency by developing finite-time error bounds. This paper aims to contribute to the existing body of knowledge by presenting a novel finite-time analysis of tabular temporal difference (TD) learning, which makes direct and effective use of discrete-time stochastic linear system models and leverages Schur matrix properties. The proposed analysis can cover both on-policy and off-policy settings in a unified manner. By adopting this approach, we hope to offer new and straightforward templates that not only shed further light on the analysis of TD-learning and related RL algorithms but also provide valuable insights for future resear",
    "path": "papers/22/04/2204.10479.json",
    "total_tokens": 889,
    "translated_title": "基于离散时间线性系统的有限时间分析：时序差分学习",
    "translated_abstract": "TD-learning是强化学习领域中一种基本的算法，它被用于通过估计马尔可夫决策过程的相应价值函数来评估给定策略。尽管TD-learning的理论分析取得了显著进展，但最近的研究通过开发有限时间误差界定来揭示了其统计效率的保证。本文旨在通过提出一种新的表格型时序差分(TD)学习的有限时间分析，直接有效地利用离散时间随机线性系统模型并利用Schur矩阵特性，为现有知识贡献新的方法。所提出的分析方法可以统一地覆盖基于策略和基于价值的两种情况。通过采用这种方法，我们希望能够提供新的、简单的模板，不仅可以进一步阐明TD-learning和相关RL算法的分析，而且为未来的研究提供有价值的见解。",
    "tldr": "本文提出一种基于离散时间线性系统模型和Schur矩阵特性的有限时间分析方法，针对表格型时序差分学习，能够统一地覆盖基于策略和基于价值的两种情况。",
    "en_tdlr": "This paper proposes a novel finite-time analysis method for tabular temporal difference learning using discrete-time stochastic linear system models and Schur matrix properties. The proposed analysis covers both on-policy and off-policy settings in a unified manner."
}