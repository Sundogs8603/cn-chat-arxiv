{
    "title": "The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer. (arXiv:2204.06457v2 [cs.CL] UPDATED)",
    "abstract": "Large multilingual language models such as mBERT or XLM-R enable zero-shot cross-lingual transfer in various IR and NLP tasks. Cao et al. (2020) proposed a data- and compute-efficient method for cross-lingual adjustment of mBERT that uses a small parallel corpus to make embeddings of related words across languages similar to each other. They showed it to be effective in NLI for five European languages. In contrast we experiment with a typologically diverse set of languages (Spanish, Russian, Vietnamese, and Hindi) and extend their original implementations to new tasks (XSR, NER, and QA) and an additional training regime (continual learning). Our study reproduced gains in NLI for four languages, showed improved NER, XSR, and cross-lingual QA results in three languages (though some cross-lingual QA gains were not statistically significant), while mono-lingual QA performance never improved and sometimes degraded. Analysis of distances between contextualized embeddings of related and unrel",
    "link": "http://arxiv.org/abs/2204.06457",
    "context": "Title: The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer. (arXiv:2204.06457v2 [cs.CL] UPDATED)\nAbstract: Large multilingual language models such as mBERT or XLM-R enable zero-shot cross-lingual transfer in various IR and NLP tasks. Cao et al. (2020) proposed a data- and compute-efficient method for cross-lingual adjustment of mBERT that uses a small parallel corpus to make embeddings of related words across languages similar to each other. They showed it to be effective in NLI for five European languages. In contrast we experiment with a typologically diverse set of languages (Spanish, Russian, Vietnamese, and Hindi) and extend their original implementations to new tasks (XSR, NER, and QA) and an additional training regime (continual learning). Our study reproduced gains in NLI for four languages, showed improved NER, XSR, and cross-lingual QA results in three languages (though some cross-lingual QA gains were not statistically significant), while mono-lingual QA performance never improved and sometimes degraded. Analysis of distances between contextualized embeddings of related and unrel",
    "path": "papers/22/04/2204.06457.json",
    "total_tokens": 995,
    "translated_title": "跨语言调整上下文词表示对零翻译转移的影响",
    "translated_abstract": "大型多语言语言模型如mBERT或XLM-R可以在各种IR和NLP任务中实现零翻译跨语言转移。Cao等人提出了一种数据和计算高效的mBERT跨语言调整方法，利用小型平行语料库使不同语言中相关单词的嵌入相似。他们在五种欧洲语言的NLI中证明了其有效性。与此相反，我们在包含西班牙语、俄语、越南语和印地语在内的语言集上进行了实验，并扩展了他们的原始实现来适应新的任务（XSR、NER和QA）和额外的训练机制（持续学习）。我们的研究为四种语言重现了NLI的增益，在三种语言上显示出了改进的NER、XSR和跨语言QA结果（尽管某些跨语言QA的增益在统计上不显著），而单语QA性能从未提高，有时甚至下降。对相关和不相关上下文词嵌入的距离进行了分析。",
    "tldr": "本文研究了跨语言调整上下文词表示在多种语言和任务中的影响，结论是该方法对多语言NLI有益，同时对NER、XSR和跨语言QA也有改进，尤其对某些语言更为明显。而单语QA性能没有改善，有时甚至下降。",
    "en_tdlr": "This paper investigates the impact of cross-lingual adjustment of contextual word representations on various languages and tasks, finding that it is beneficial for multilingual NLI, and shows improvements in NER, XSR, and cross-lingual QA, particularly for certain languages. However, it does not improve monolingual QA performance and sometimes even degrades it."
}