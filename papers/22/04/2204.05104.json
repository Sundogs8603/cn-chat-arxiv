{
    "title": "Self-Supervised Graph Neural Network for Multi-Source Domain Adaptation. (arXiv:2204.05104v2 [cs.LG] UPDATED)",
    "abstract": "Domain adaptation (DA) tries to tackle the scenarios when the test data does not fully follow the same distribution of the training data, and multi-source domain adaptation (MSDA) is very attractive for real world applications. By learning from large-scale unlabeled samples, self-supervised learning has now become a new trend in deep learning. It is worth noting that both self-supervised learning and multi-source domain adaptation share a similar goal: they both aim to leverage unlabeled data to learn more expressive representations. Unfortunately, traditional multi-task self-supervised learning faces two challenges: (1) the pretext task may not strongly relate to the downstream task, thus it could be difficult to learn useful knowledge being shared from the pretext task to the target task; (2) when the same feature extractor is shared between the pretext task and the downstream one and only different prediction heads are used, it is ineffective to enable inter-task information exchang",
    "link": "http://arxiv.org/abs/2204.05104",
    "context": "Title: Self-Supervised Graph Neural Network for Multi-Source Domain Adaptation. (arXiv:2204.05104v2 [cs.LG] UPDATED)\nAbstract: Domain adaptation (DA) tries to tackle the scenarios when the test data does not fully follow the same distribution of the training data, and multi-source domain adaptation (MSDA) is very attractive for real world applications. By learning from large-scale unlabeled samples, self-supervised learning has now become a new trend in deep learning. It is worth noting that both self-supervised learning and multi-source domain adaptation share a similar goal: they both aim to leverage unlabeled data to learn more expressive representations. Unfortunately, traditional multi-task self-supervised learning faces two challenges: (1) the pretext task may not strongly relate to the downstream task, thus it could be difficult to learn useful knowledge being shared from the pretext task to the target task; (2) when the same feature extractor is shared between the pretext task and the downstream one and only different prediction heads are used, it is ineffective to enable inter-task information exchang",
    "path": "papers/22/04/2204.05104.json",
    "total_tokens": 935,
    "translated_title": "自监督图神经网络用于多源域适应",
    "translated_abstract": "域适应试图解决测试数据与训练数据不完全遵循相同分布的情况，而多源域适应对于实际应用非常有吸引力。通过从大规模无标签样本中学习，自监督学习现在已经成为深度学习中的一个新趋势。值得注意的是，自监督学习和多源域适应有一个相似的目标：它们都旨在利用无标签数据学习更具表现力的表示。不幸的是，传统的多任务自监督学习面临两个挑战：（1）预训练任务可能与下游任务关联不强，因此很难从预训练任务中学习到与目标任务共享的有用知识；（2）当相同的特征提取器在预训练任务和下游任务之间共享，并且只使用不同的预测头时，使任务间的信息交换变得无效。",
    "tldr": "该论文介绍了一种自监督图神经网络方法，用于解决多源域适应问题。该方法利用大规模无标签样本进行学习，并尝试在预训练任务和下游任务之间共享有用知识。然而，传统方法面临预训练任务与下游任务关联性不强以及信息交换无效的挑战。",
    "en_tdlr": "This paper presents a self-supervised graph neural network approach for addressing multi-source domain adaptation. The method leverages large-scale unlabeled samples for learning and aims to share useful knowledge between the pre-training task and the downstream task. However, traditional methods face challenges of weak correlation between pre-training and downstream tasks and ineffective information exchange."
}