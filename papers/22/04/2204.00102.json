{
    "title": "Dynamic Multimodal Fusion. (arXiv:2204.00102v2 [cs.CV] UPDATED)",
    "abstract": "Deep multimodal learning has achieved great progress in recent years. However, current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data. In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses multimodal data and generates data-dependent forward paths during inference. To this end, we propose a gating function to provide modality-level or fusion-level decisions on-the-fly based on multimodal features and a resource-aware loss function that encourages computational efficiency. Results on various multimodal tasks demonstrate the efficiency and wide applicability of our approach. For instance, DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic s",
    "link": "http://arxiv.org/abs/2204.00102",
    "context": "Title: Dynamic Multimodal Fusion. (arXiv:2204.00102v2 [cs.CV] UPDATED)\nAbstract: Deep multimodal learning has achieved great progress in recent years. However, current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data. In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses multimodal data and generates data-dependent forward paths during inference. To this end, we propose a gating function to provide modality-level or fusion-level decisions on-the-fly based on multimodal features and a resource-aware loss function that encourages computational efficiency. Results on various multimodal tasks demonstrate the efficiency and wide applicability of our approach. For instance, DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic s",
    "path": "papers/22/04/2204.00102.json",
    "total_tokens": 907,
    "translated_title": "动态多模态融合",
    "translated_abstract": "深度多模态学习近年来取得了长足的进步。然而，当前的融合方法在本质上是静态的，即它们使用相同的计算处理和融合多模态输入，而不考虑不同多模态数据的不同计算需求。在这项工作中，我们提出了动态多模态融合（DynMM），一种新的方法，它自适应地融合多模态数据并在推理期间生成数据相关的前向路径。为此，我们提出了一个门控函数，根据多模态特征提供模态级或融合级决策，以及一个资源感知的损失函数，以鼓励计算效率。对各种多模态任务的结果表明了我们方法的有效性和广泛适用性。例如，DynMM可以在只有微不足道的准确度损失的情况下将计算成本降低46.5%（CMU-MOSEI情感分析），并在计算中节省超过21%的开销以提高分段性能（NYU Depth V2语义分割）。",
    "tldr": "本论文提出了一种动态多模态融合的新方法，可以自适应地融合多模态数据并根据计算需求生成数据相关的前向路径，在提高计算效率的同时兼顾准确度和性能。",
    "en_tdlr": "This paper proposes a new approach of dynamic multimodal fusion, which adaptively fuses multimodal data and generates data-dependent forward paths during inference, and a gating function based on multimodal features and a resource-aware loss function that encourages computational efficiency, achieving high efficiency and wide applicability on various multimodal tasks."
}