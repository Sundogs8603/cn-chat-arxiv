{
    "title": "Settling the Sample Complexity of Model-Based Offline Reinforcement Learning",
    "abstract": "arXiv:2204.05275v3 Announce Type: replace-cross  Abstract: This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications.   We demonstrate that the model-based (or \"plug-in\") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\\frac{1}{1-\\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\\star}_{\\text{clipped}}$. We p",
    "link": "https://arxiv.org/abs/2204.05275",
    "context": "Title: Settling the Sample Complexity of Model-Based Offline Reinforcement Learning\nAbstract: arXiv:2204.05275v3 Announce Type: replace-cross  Abstract: This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications.   We demonstrate that the model-based (or \"plug-in\") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\\frac{1}{1-\\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\\star}_{\\text{clipped}}$. We p",
    "path": "papers/22/04/2204.05275.json",
    "total_tokens": 731,
    "translated_title": "解决模型为基础的离线强化学习的样本复杂性问题",
    "translated_abstract": "本文关注离线强化学习（RL），它利用预先收集的数据进行学习，无需进一步探索。有效的离线RL应能适应分布转移和有限的数据覆盖。然而，先前的算法或分析要么受到次优样本复杂性的困扰，要么产生高昂的烧录成本以达到样本最优性，从而对样本匮乏应用中的高效离线RL构成障碍。",
    "tldr": "该论文展示了基于模型的（或“插件”）方法在标签化马尔可夫决策过程（MDPs）中实现了无烧录成本的极小极优样本复杂性。",
    "en_tdlr": "This paper demonstrates that the model-based approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs)."
}