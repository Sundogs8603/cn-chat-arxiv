{
    "title": "Reinforcement Learning from Partial Observation: Linear Function Approximation with Provable Sample Efficiency",
    "abstract": "arXiv:2204.09787v3 Announce Type: replace  Abstract: We study reinforcement learning for partially observed Markov decision processes (POMDPs) with infinite observation and state spaces, which remains less investigated theoretically. To this end, we make the first attempt at bridging partial observability and function approximation for a class of POMDPs with a linear structure. In detail, we propose a reinforcement learning algorithm (Optimistic Exploration via Adversarial Integral Equation or OP-TENET) that attains an $\\epsilon$-optimal policy within $O(1/\\epsilon^2)$ episodes. In particular, the sample complexity scales polynomially in the intrinsic dimension of the linear structure and is independent of the size of the observation and state spaces.   The sample efficiency of OP-TENET is enabled by a sequence of ingredients: (i) a Bellman operator with finite memory, which represents the value function in a recursive manner, (ii) the identification and estimation of such an operator ",
    "link": "https://arxiv.org/abs/2204.09787",
    "context": "Title: Reinforcement Learning from Partial Observation: Linear Function Approximation with Provable Sample Efficiency\nAbstract: arXiv:2204.09787v3 Announce Type: replace  Abstract: We study reinforcement learning for partially observed Markov decision processes (POMDPs) with infinite observation and state spaces, which remains less investigated theoretically. To this end, we make the first attempt at bridging partial observability and function approximation for a class of POMDPs with a linear structure. In detail, we propose a reinforcement learning algorithm (Optimistic Exploration via Adversarial Integral Equation or OP-TENET) that attains an $\\epsilon$-optimal policy within $O(1/\\epsilon^2)$ episodes. In particular, the sample complexity scales polynomially in the intrinsic dimension of the linear structure and is independent of the size of the observation and state spaces.   The sample efficiency of OP-TENET is enabled by a sequence of ingredients: (i) a Bellman operator with finite memory, which represents the value function in a recursive manner, (ii) the identification and estimation of such an operator ",
    "path": "papers/22/04/2204.09787.json",
    "total_tokens": 943,
    "translated_title": "基于线性函数逼近的部分观测强化学习及其可证明的样本效率",
    "translated_abstract": "我们研究具有无限观测和状态空间的部分观测马尔可夫决策过程（POMDP）的强化学习，在理论上仍然受到较少的研究。为此，我们首次尝试将部分可观测性与具有线性结构的一类POMDP的函数逼近联系起来。具体来说，我们提出了一种强化学习算法（乐观探索通过对抗积分方程或OP-TENET），在$ O（1 / \\ epsilon ^ 2）$个情节内实现了$\\ epsilon $-最优策略。特别地，样本复杂度在线性结构的本征维度多项式地缩放，并且与观测和状态空间的大小无关。OP-TENET的样本效率由一系列因素实现：（i）具有有限记忆的Bellman算子，以递归方式表示值函数，（ii）识别和估计这样一个算子",
    "tldr": "该研究提出了一种基于线性函数逼近的部分观测强化学习算法（OP-TENET），在有限的情节数内实现了$\\epsilon$-最优策略，样本复杂度与线性结构的本征维度多项式缩放，与观测和状态空间的大小无关。",
    "en_tdlr": "The paper introduces a reinforcement learning algorithm (OP-TENET) based on linear function approximation, achieving an $\\epsilon$-optimal policy within a finite number of episodes, with sample complexity scaling polynomially in the intrinsic dimension of the linear structure and independent of the size of observation and state spaces."
}