{
    "title": "Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks. (arXiv:2204.00846v2 [cs.LG] UPDATED)",
    "abstract": "Lipschitz constants of neural networks allow for guarantees of robustness in image classification, safety in controller design, and generalizability beyond the training data. As calculating Lipschitz constants is NP-hard, techniques for estimating Lipschitz constants must navigate the trade-off between scalability and accuracy. In this work, we significantly push the scalability frontier of a semidefinite programming technique known as LipSDP while achieving zero accuracy loss. We first show that LipSDP has chordal sparsity, which allows us to derive a chordally sparse formulation that we call Chordal-LipSDP. The key benefit is that the main computational bottleneck of LipSDP, a large semidefinite constraint, is now decomposed into an equivalent collection of smaller ones: allowing Chordal-LipSDP to outperform LipSDP particularly as the network depth grows. Moreover, our formulation uses a tunable sparsity parameter that enables one to gain tighter estimates without incurring a signifi",
    "link": "http://arxiv.org/abs/2204.00846",
    "context": "Title: Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks. (arXiv:2204.00846v2 [cs.LG] UPDATED)\nAbstract: Lipschitz constants of neural networks allow for guarantees of robustness in image classification, safety in controller design, and generalizability beyond the training data. As calculating Lipschitz constants is NP-hard, techniques for estimating Lipschitz constants must navigate the trade-off between scalability and accuracy. In this work, we significantly push the scalability frontier of a semidefinite programming technique known as LipSDP while achieving zero accuracy loss. We first show that LipSDP has chordal sparsity, which allows us to derive a chordally sparse formulation that we call Chordal-LipSDP. The key benefit is that the main computational bottleneck of LipSDP, a large semidefinite constraint, is now decomposed into an equivalent collection of smaller ones: allowing Chordal-LipSDP to outperform LipSDP particularly as the network depth grows. Moreover, our formulation uses a tunable sparsity parameter that enables one to gain tighter estimates without incurring a signifi",
    "path": "papers/22/04/2204.00846.json",
    "total_tokens": 982,
    "translated_title": "基于和弦稀疏性的深度神经网络Lipschitz常数估计",
    "translated_abstract": "神经网络的Lipschitz常数可以保证图像分类的鲁棒性、控制器设计的安全性以及训练数据之外的泛化能力。由于计算Lipschitz常数是NP难的，估计Lipschitz常数的技术必须在可扩展性和准确性之间权衡。本文在保持零准确度损失的同时，显著推动了半定规划技术LipSDP的可扩展性。我们首先证明LipSDP具有和弦稀疏性，从而推导出一种称为Chordal-LipSDP的和弦稀疏表述。其主要优势在于将LipSDP的主要计算瓶颈，即一个大型半定约束，分解为等效的多个较小半定约束：使得Chordal-LipSDP在网络深度增加时能够优于LipSDP。此外，我们的表述使用了一个可调的稀疏参数，可以在不增加显著误差的情况下获得更紧的估计。",
    "tldr": "本文提出了一种基于和弦稀疏性的Chordal-LipSDP方法，用于估计深度神经网络的Lipschitz常数。通过将大型半定约束分解为多个较小的约束，该方法在网络深度增加时能够在准确性和可扩展性之间取得更好的权衡。",
    "en_tdlr": "This paper presents a Chordal-LipSDP method based on chordal sparsity for estimating the Lipschitz constant of deep neural networks. By decomposing a large semidefinite constraint into smaller ones, this method achieves a better trade-off between accuracy and scalability as the network depth increases."
}