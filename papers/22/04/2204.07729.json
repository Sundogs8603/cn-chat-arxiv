{
    "title": "Efficient Bayesian Policy Reuse with a Scalable Observation Model in Deep Reinforcement Learning. (arXiv:2204.07729v3 [cs.LG] UPDATED)",
    "abstract": "Bayesian policy reuse (BPR) is a general policy transfer framework for selecting a source policy from an offline library by inferring the task belief based on some observation signals and a trained observation model. In this paper, we propose an improved BPR method to achieve more efficient policy transfer in deep reinforcement learning (DRL). First, most BPR algorithms use the episodic return as the observation signal that contains limited information and cannot be obtained until the end of an episode. Instead, we employ the state transition sample, which is informative and instantaneous, as the observation signal for faster and more accurate task inference. Second, BPR algorithms usually require numerous samples to estimate the probability distribution of the tabular-based observation model, which may be expensive and even infeasible to learn and maintain, especially when using the state transition sample as the signal. Hence, we propose a scalable observation model based on fitting ",
    "link": "http://arxiv.org/abs/2204.07729",
    "context": "Title: Efficient Bayesian Policy Reuse with a Scalable Observation Model in Deep Reinforcement Learning. (arXiv:2204.07729v3 [cs.LG] UPDATED)\nAbstract: Bayesian policy reuse (BPR) is a general policy transfer framework for selecting a source policy from an offline library by inferring the task belief based on some observation signals and a trained observation model. In this paper, we propose an improved BPR method to achieve more efficient policy transfer in deep reinforcement learning (DRL). First, most BPR algorithms use the episodic return as the observation signal that contains limited information and cannot be obtained until the end of an episode. Instead, we employ the state transition sample, which is informative and instantaneous, as the observation signal for faster and more accurate task inference. Second, BPR algorithms usually require numerous samples to estimate the probability distribution of the tabular-based observation model, which may be expensive and even infeasible to learn and maintain, especially when using the state transition sample as the signal. Hence, we propose a scalable observation model based on fitting ",
    "path": "papers/22/04/2204.07729.json",
    "total_tokens": 968,
    "translated_title": "在深度强化学习中，具有可扩展观测模型的高效贝叶斯策略复用",
    "translated_abstract": "贝叶斯策略复用（BPR）是一种从离线库中选择源策略的通用策略转移框架，它通过推断基于一些观测信号和训练好的观测模型的任务信念来实现。在本文中，我们提出了一种改进的BPR方法，以实现在深度强化学习中更高效的策略转移。首先，大多数BPR算法使用回合返回作为观测信号，但它包含的信息有限，并且直到回合结束才能获得。相反，我们使用具有信息量和即时性的状态转换样本作为观测信号，以实现更快速和更准确的任务推断。其次，BPR算法通常需要大量样本来估计基于表格的观测模型的概率分布，这可能是昂贵的甚至不可行的，特别是当使用状态转换样本作为信号时。因此，我们提出了一种基于拟合的可扩展观测模型，并使用此模型来加速任务推断和策略复用。",
    "tldr": "本研究提出了一种改进的贝叶斯策略复用方法，在深度强化学习中实现更高效的策略转移。该方法使用即时且信息丰富的状态转换样本作为观测信号，并提出了一个可扩展的观测模型来加速任务推断和策略复用。",
    "en_tdlr": "This paper proposes an improved Bayesian policy reuse method for efficient policy transfer in deep reinforcement learning. It uses instantaneous and informative state transition samples as observation signals and introduces a scalable observation model to accelerate task inference and policy reuse."
}