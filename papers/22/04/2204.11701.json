{
    "title": "Tac2Pose: Tactile Object Pose Estimation from the First Touch. (arXiv:2204.11701v3 [cs.CV] UPDATED)",
    "abstract": "In this paper, we present Tac2Pose, an object-specific approach to tactile pose estimation from the first touch for known objects. Given the object geometry, we learn a tailored perception model in simulation that estimates a probability distribution over possible object poses given a tactile observation. To do so, we simulate the contact shapes that a dense set of object poses would produce on the sensor. Then, given a new contact shape obtained from the sensor, we match it against the pre-computed set using an object-specific embedding learned using contrastive learning. We obtain contact shapes from the sensor with an object-agnostic calibration step that maps RGB tactile observations to binary contact shapes. This mapping, which can be reused across object and sensor instances, is the only step trained with real sensor data. This results in a perception model that localizes objects from the first real tactile observation. Importantly, it produces pose distributions and can incorpor",
    "link": "http://arxiv.org/abs/2204.11701",
    "context": "Title: Tac2Pose: Tactile Object Pose Estimation from the First Touch. (arXiv:2204.11701v3 [cs.CV] UPDATED)\nAbstract: In this paper, we present Tac2Pose, an object-specific approach to tactile pose estimation from the first touch for known objects. Given the object geometry, we learn a tailored perception model in simulation that estimates a probability distribution over possible object poses given a tactile observation. To do so, we simulate the contact shapes that a dense set of object poses would produce on the sensor. Then, given a new contact shape obtained from the sensor, we match it against the pre-computed set using an object-specific embedding learned using contrastive learning. We obtain contact shapes from the sensor with an object-agnostic calibration step that maps RGB tactile observations to binary contact shapes. This mapping, which can be reused across object and sensor instances, is the only step trained with real sensor data. This results in a perception model that localizes objects from the first real tactile observation. Importantly, it produces pose distributions and can incorpor",
    "path": "papers/22/04/2204.11701.json",
    "total_tokens": 917,
    "translated_title": "Tac2Pose：从第一次接触中的触觉对象姿态估计",
    "translated_abstract": "本文介绍了Tac2Pose，一种针对已知对象的从第一次接触中估计触觉姿态的对象特定方法。给定物体几何形状，我们在仿真中学习了一个量身定制的感知模型，可以根据触觉观测来估计可能的物体姿态的概率分布。为此，我们模拟了一组密集的物体姿态在传感器上产生的接触形状。然后，给定从传感器中获得的新接触形状，我们使用对比学习学习的对象特定嵌入将其与预先计算的集合进行匹配。我们使用针对对象无关的校准步骤将RGB触觉观测映射到二值接触形状，从传感器中获得接触形状。这个映射可以在对象和传感器实例之间重复使用，是唯一使用真实传感器数据进行训练的步骤。这样就可以通过第一次真实触觉观测来定位物体。重要的是，它可以产生姿态分布，并可以将其他传感器数据整合到姿态估计中。",
    "tldr": "Tac2Pose是一种从第一次触觉中估计物体姿态的方法，通过在仿真中学习物体的感知模型，根据触觉观测估计可能的物体姿态，并通过对比学习进行匹配。这种方法只需要一次真实触觉观测即可定位物体。"
}