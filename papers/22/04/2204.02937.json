{
    "title": "Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. (arXiv:2204.02937v2 [cs.LG] UPDATED)",
    "abstract": "Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.",
    "link": "http://arxiv.org/abs/2204.02937",
    "context": "Title: Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. (arXiv:2204.02937v2 [cs.LG] UPDATED)\nAbstract: Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.",
    "path": "papers/22/04/2204.02937.json",
    "total_tokens": 953,
    "translated_title": "最后一层重新训练足以提高对虚假相关性的鲁棒性",
    "translated_abstract": "神经网络分类器可以主要依靠简单的虚假特征（如背景）进行预测。然而，即使在这些情况下，我们展示了它们仍然经常学习与数据所需属性相关的核心特征，与最近的研究结果相反。受到这一启示的启发，我们证明了简单的最后一层重新训练可以在虚假相关性基准测试中与甚至胜过最先进的方法，但其复杂度和计算开销显著较低。此外，我们还展示了对于在ImageNet训练的大型模型上进行最后一层重新训练，仅经过几分钟的单GPU训练，也可以显著降低对背景和纹理信息的依赖，提高对协变量转变的鲁棒性。",
    "tldr": "本文研究表明，简单的最后一层重新训练足以提高神经网络分类器对虚假相关性的鲁棒性，可以在虚假相关性基准测试中与最先进的方法相媲美或胜过，但其复杂度和计算开销较低。此外，对于在ImageNet训练的大型模型进行最后一层重新训练，仅几分钟的训练时间就可以显著降低对背景和纹理信息的依赖，提高对协变量转变的鲁棒性。"
}