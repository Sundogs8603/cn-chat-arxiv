{
    "title": "Sliced gradient-enhanced Kriging for high-dimensional function approximation. (arXiv:2204.03562v3 [stat.ML] UPDATED)",
    "abstract": "Gradient-enhanced Kriging (GE-Kriging) is a well-established surrogate modelling technique for approximating expensive computational models. However, it tends to get impractical for high-dimensional problems due to the size of the inherent correlation matrix and the associated high-dimensional hyper-parameter tuning problem. To address these issues, a new method, called sliced GE-Kriging (SGE-Kriging), is developed in this paper for reducing both the size of the correlation matrix and the number of hyper-parameters. We first split the training sample set into multiple slices, and invoke Bayes' theorem to approximate the full likelihood function via a sliced likelihood function, in which multiple small correlation matrices are utilized to describe the correlation of the sample set rather than one large one. Then, we replace the original high-dimensional hyper-parameter tuning problem with a low-dimensional counterpart by learning the relationship between the hyper-parameters and the der",
    "link": "http://arxiv.org/abs/2204.03562",
    "context": "Title: Sliced gradient-enhanced Kriging for high-dimensional function approximation. (arXiv:2204.03562v3 [stat.ML] UPDATED)\nAbstract: Gradient-enhanced Kriging (GE-Kriging) is a well-established surrogate modelling technique for approximating expensive computational models. However, it tends to get impractical for high-dimensional problems due to the size of the inherent correlation matrix and the associated high-dimensional hyper-parameter tuning problem. To address these issues, a new method, called sliced GE-Kriging (SGE-Kriging), is developed in this paper for reducing both the size of the correlation matrix and the number of hyper-parameters. We first split the training sample set into multiple slices, and invoke Bayes' theorem to approximate the full likelihood function via a sliced likelihood function, in which multiple small correlation matrices are utilized to describe the correlation of the sample set rather than one large one. Then, we replace the original high-dimensional hyper-parameter tuning problem with a low-dimensional counterpart by learning the relationship between the hyper-parameters and the der",
    "path": "papers/22/04/2204.03562.json",
    "total_tokens": 839,
    "translated_title": "高维函数逼近的切片梯度增强克里金方法",
    "translated_abstract": "梯度增强克里金（GE-Kriging）是一种用于逼近昂贵计算模型的已建立的代理建模技术。然而，由于固有相关矩阵的大小以及相关的高维超参数调整问题，它在高维问题上变得不实用。为了解决这些问题，在本文中提出了一种名为切片GE-Kriging（SGE-Kriging）的新方法，用于减小相关矩阵的大小和超参数的数量。我们首先将训练样本集拆分为多个切片，并通过贝叶斯定理使用切片似然函数来逼近完整的似然函数，在切片似然函数中，我们使用多个小的相关矩阵来描述样本集的相关性，而不是一个大的矩阵。然后，我们通过学习超参数与导数之间的关系，将原始的高维超参数调整问题替换为一个低维问题。",
    "tldr": "本文提出了一种名为切片GE-Kriging（SGE-Kriging）的新方法，用于解决高维函数逼近中的相关矩阵和超参数调整问题。"
}