{
    "title": "Statistical-Computational Trade-offs in Tensor PCA and Related Problems via Communication Complexity. (arXiv:2204.07526v2 [math.ST] UPDATED)",
    "abstract": "Tensor PCA is a stylized statistical inference problem introduced by Montanari and Richard to study the computational difficulty of estimating an unknown parameter from higher-order moment tensors. Unlike its matrix counterpart, Tensor PCA exhibits a statistical-computational gap, i.e., a sample size regime where the problem is information-theoretically solvable but conjectured to be computationally hard. This paper derives computational lower bounds on the run-time of memory bounded algorithms for Tensor PCA using communication complexity. These lower bounds specify a trade-off among the number of passes through the data sample, the sample size, and the memory required by any algorithm that successfully solves Tensor PCA. While the lower bounds do not rule out polynomial-time algorithms, they do imply that many commonly-used algorithms, such as gradient descent and power method, must have a higher iteration count when the sample size is not large enough. Similar lower bounds are obtai",
    "link": "http://arxiv.org/abs/2204.07526",
    "context": "Title: Statistical-Computational Trade-offs in Tensor PCA and Related Problems via Communication Complexity. (arXiv:2204.07526v2 [math.ST] UPDATED)\nAbstract: Tensor PCA is a stylized statistical inference problem introduced by Montanari and Richard to study the computational difficulty of estimating an unknown parameter from higher-order moment tensors. Unlike its matrix counterpart, Tensor PCA exhibits a statistical-computational gap, i.e., a sample size regime where the problem is information-theoretically solvable but conjectured to be computationally hard. This paper derives computational lower bounds on the run-time of memory bounded algorithms for Tensor PCA using communication complexity. These lower bounds specify a trade-off among the number of passes through the data sample, the sample size, and the memory required by any algorithm that successfully solves Tensor PCA. While the lower bounds do not rule out polynomial-time algorithms, they do imply that many commonly-used algorithms, such as gradient descent and power method, must have a higher iteration count when the sample size is not large enough. Similar lower bounds are obtai",
    "path": "papers/22/04/2204.07526.json",
    "total_tokens": 959,
    "translated_title": "在张量主成分分析和相关问题中的统计计算权衡：通过通信复杂度",
    "translated_abstract": "张量主成分分析是Montanari和Richard引入的一种风格化统计推断问题，用于研究从高阶矩张量中估计未知参数的计算难度。与其矩阵对应问题不同，张量主成分分析展现了一种统计计算差距，即在样本大小范围内，问题在信息理论上可解，但被认为在计算上较困难。本文利用通信复杂度推导出了内存受限算法在张量主成分分析中的计算下界。这些下界指定了成功解决张量主成分分析的任何算法在数据样本经过次数、样本大小和所需内存之间的权衡。尽管下界不能排除多项式时间算法，但它们意味着许多常用的算法，如梯度下降和幂迭代方法，在样本大小不够大时必须有更高的迭代次数。类似的下界还可以使用通信复杂度获得。",
    "tldr": "本文通过通信复杂度推导出了对于内存受限算法在张量主成分分析中的计算下界，并且指定了解决该问题的算法必须在数据样本经过次数、样本大小和所需内存之间进行权衡。这些下界暗示了许多常用算法在样本大小不够大时需要更多的迭代次数。",
    "en_tdlr": "This paper derives computational lower bounds on the run-time of memory bounded algorithms for Tensor PCA using communication complexity, indicating a trade-off among the number of passes through the data sample, the sample size, and the required memory for any algorithm that successfully solves Tensor PCA. These lower bounds imply that many commonly-used algorithms may require more iterations when the sample size is not large enough."
}