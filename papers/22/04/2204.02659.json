{
    "title": "Towards Better Understanding of User Satisfaction in Open-Domain Conversational Search. (arXiv:2204.02659v2 [cs.IR] UPDATED)",
    "abstract": "With the increasing popularity of conversational search, how to evaluate the performance of conversational search systems has become an important question in the IR community. Existing works on conversational search evaluation can mainly be categorized into two streams: (1) constructing metrics based on semantic similarity (e.g. BLUE, METEOR and BERTScore), or (2) directly evaluating the response ranking performance of the system using traditional search methods (e.g. nDCG, RBP and nERR). However, these methods either ignore the information need of the user or ignore the mixed-initiative property of conversational search. This raises the question of how to accurately model user satisfaction in conversational search scenarios. Since explicitly asking users to provide satisfaction feedback is difficult, traditional IR studies often rely on the Cranfield paradigm (i.e., third-party annotation) and user behavior modeling to estimate user satisfaction in search. However, the feasibility and",
    "link": "http://arxiv.org/abs/2204.02659",
    "context": "Title: Towards Better Understanding of User Satisfaction in Open-Domain Conversational Search. (arXiv:2204.02659v2 [cs.IR] UPDATED)\nAbstract: With the increasing popularity of conversational search, how to evaluate the performance of conversational search systems has become an important question in the IR community. Existing works on conversational search evaluation can mainly be categorized into two streams: (1) constructing metrics based on semantic similarity (e.g. BLUE, METEOR and BERTScore), or (2) directly evaluating the response ranking performance of the system using traditional search methods (e.g. nDCG, RBP and nERR). However, these methods either ignore the information need of the user or ignore the mixed-initiative property of conversational search. This raises the question of how to accurately model user satisfaction in conversational search scenarios. Since explicitly asking users to provide satisfaction feedback is difficult, traditional IR studies often rely on the Cranfield paradigm (i.e., third-party annotation) and user behavior modeling to estimate user satisfaction in search. However, the feasibility and",
    "path": "papers/22/04/2204.02659.json",
    "total_tokens": 903,
    "translated_title": "在开放领域的对话式搜索中更好地理解用户满意度",
    "translated_abstract": "随着对话式搜索的普及，如何评估对话式搜索系统的性能成为信息检索领域的一个重要问题。现有对话式搜索评估的工作主要可分为两类：基于语义相似性构建度量标准（如BLUE、METEOR和BERTScore），或直接使用传统搜索方法（如nDCG、RBP和nERR）评估系统的响应排名性能。然而，这些方法要么忽视用户的信息需求，要么忽视对话式搜索的混合主动属性。这引发了一个问题：如何在对话式搜索场景中准确地建模用户满意度。由于明确要求用户提供满意度反馈很困难，传统的IR研究往往依赖Cranfield范式（即第三方注释）和用户行为建模来估计搜索中的用户满意度。然而，实现这种模型在实践中很有挑战性。",
    "tldr": "在对话式搜索中，如何准确地建模用户满意度是一个重要问题。现有方法要么忽视用户信息需求，要么忽视混合主动属性。传统的IR研究使用Cranfield范式和用户行为建模来估计用户满意度，但实践中具有挑战性。"
}