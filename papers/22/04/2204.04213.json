{
    "title": "Structure-aware Protein Self-supervised Learning. (arXiv:2204.04213v4 [cs.LG] UPDATED)",
    "abstract": "Protein representation learning methods have shown great potential to yield useful representation for many downstream tasks, especially on protein classification. Moreover, a few recent studies have shown great promise in addressing insufficient labels of proteins with self-supervised learning methods. However, existing protein language models are usually pretrained on protein sequences without considering the important protein structural information. To this end, we propose a novel structure-aware protein self-supervised learning method to effectively capture structural information of proteins. In particular, a well-designed graph neural network (GNN) model is pretrained to preserve the protein structural information with self-supervised tasks from a pairwise residue distance perspective and a dihedral angle perspective, respectively. Furthermore, we propose to leverage the available protein language model pretrained on protein sequences to enhance the self-supervised learning. Specif",
    "link": "http://arxiv.org/abs/2204.04213",
    "context": "Title: Structure-aware Protein Self-supervised Learning. (arXiv:2204.04213v4 [cs.LG] UPDATED)\nAbstract: Protein representation learning methods have shown great potential to yield useful representation for many downstream tasks, especially on protein classification. Moreover, a few recent studies have shown great promise in addressing insufficient labels of proteins with self-supervised learning methods. However, existing protein language models are usually pretrained on protein sequences without considering the important protein structural information. To this end, we propose a novel structure-aware protein self-supervised learning method to effectively capture structural information of proteins. In particular, a well-designed graph neural network (GNN) model is pretrained to preserve the protein structural information with self-supervised tasks from a pairwise residue distance perspective and a dihedral angle perspective, respectively. Furthermore, we propose to leverage the available protein language model pretrained on protein sequences to enhance the self-supervised learning. Specif",
    "path": "papers/22/04/2204.04213.json",
    "total_tokens": 1186,
    "translated_title": "结构感知的蛋白自监督学习",
    "translated_abstract": "蛋白质表示学习方法在许多下游任务，尤其是蛋白质分类任务中，展现出巨大的潜力。最近的一些研究还利用自监督学习方法解决了蛋白质标签数量不足的问题。然而，现有的蛋白质语言模型通常在蛋白质序列上进行预训练，而忽略了重要的蛋白质结构信息。因此，我们提出了一种新颖的结构感知的蛋白自监督学习方法，来有效地捕获蛋白质的结构信息。具体而言，我们设计了一个优秀的图神经网络（GNN）模型，通过对残基间距和二面角的自监督任务进行预训练，来保留蛋白质的结构信息。此外，我们还提出了利用已有的在蛋白质序列上预训练的语言模型，来增强自监督学习的方法。我们提出的框架 GP-SSL，通过联合训练基于GNN的结构模型和基于语言模型的序列模型，并通过fine-tuning将从GNN模型学到的有用的结构感知表示转移到下游任务中。我们在两个蛋白质分类基准测试上的实验表明，GP-SSL在下游任务性能和结构信息保留方面均优于现有方法和预训练语言模型BERT。我们的代码可在https://github.com/microsoft/GP-SSL上找到。",
    "tldr": "我们提出了一种结构感知的蛋白自监督学习方法，利用预训练的图神经网络模型保留重要的蛋白质结构信息，并结合预训练语言模型来提高下游任务性能。在两个蛋白质分类测试中均表现出了优异的结果。",
    "en_tdlr": "We propose a structure-aware protein self-supervised learning method, which effectively captures important protein structural information by pretraining a well-designed graph neural network model, and leverages pretrained language models to enhance downstream task performance. Our approach, named GP-SSL, outperforms state-of-the-art methods and pretrained language model BERT in terms of both downstream task performance and structural information preservation, as demonstrated on two protein classification benchmarks."
}