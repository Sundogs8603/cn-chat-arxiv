{
    "title": "Universum-inspired Supervised Contrastive Learning. (arXiv:2204.10695v3 [cs.LG] UPDATED)",
    "abstract": "As an effective data augmentation method, Mixup synthesizes an extra amount of samples through linear interpolations. Despite its theoretical dependency on data properties, Mixup reportedly performs well as a regularizer and calibrator contributing reliable robustness and generalization to deep model training. In this paper, inspired by Universum Learning which uses out-of-class samples to assist the target tasks, we investigate Mixup from a largely under-explored perspective - the potential to generate in-domain samples that belong to none of the target classes, that is, universum. We find that in the framework of supervised contrastive learning, Mixup-induced universum can serve as surprisingly high-quality hard negatives, greatly relieving the need for large batch sizes in contrastive learning. With these findings, we propose Universum-inspired supervised Contrastive learning (UniCon), which incorporates Mixup strategy to generate Mixup-induced universum as universum negatives and p",
    "link": "http://arxiv.org/abs/2204.10695",
    "context": "Title: Universum-inspired Supervised Contrastive Learning. (arXiv:2204.10695v3 [cs.LG] UPDATED)\nAbstract: As an effective data augmentation method, Mixup synthesizes an extra amount of samples through linear interpolations. Despite its theoretical dependency on data properties, Mixup reportedly performs well as a regularizer and calibrator contributing reliable robustness and generalization to deep model training. In this paper, inspired by Universum Learning which uses out-of-class samples to assist the target tasks, we investigate Mixup from a largely under-explored perspective - the potential to generate in-domain samples that belong to none of the target classes, that is, universum. We find that in the framework of supervised contrastive learning, Mixup-induced universum can serve as surprisingly high-quality hard negatives, greatly relieving the need for large batch sizes in contrastive learning. With these findings, we propose Universum-inspired supervised Contrastive learning (UniCon), which incorporates Mixup strategy to generate Mixup-induced universum as universum negatives and p",
    "path": "papers/22/04/2204.10695.json",
    "total_tokens": 918,
    "translated_title": "受Universum学习启发的监督对比学习",
    "translated_abstract": "作为一种有效的数据增强方法，Mixup通过线性插值合成了大量的样本。尽管 Mixup 在理论上依赖于数据属性，但据报道，它作为一种规则化器和校准器在深度模型训练中表现良好，为深度模型的鲁棒性和泛化性提供了可靠的贡献。在本文中，受到使用带有课外样本辅助目标任务的 Universum 学习的启发，我们从一个被广泛忽视的角度对 Mixup 进行了研究 - 利用其生成不属于目标类别的域内样本，即 universum。我们发现，在监督对比学习的框架下，通过 Mixup 产生的 universum 可以作为高质量的困难负样本，极大地减轻了对大批次大小在对比学习中的需要。基于这些发现，我们提出了受Universum学习启发的监督对比学习（UniCon），它将Mixup策略应用于产生Mixup诱导的universum作为负向样本。",
    "tldr": "Mixup is a popular data augmentation method that synthesizes extra samples. This paper explores the potential of Mixup to generate in-domain samples as universum negatives in supervised contrastive learning, providing high-quality hard negatives and reducing the need for large batch sizes. The proposed UniCon incorporates Mixup strategy to generate Mixup-induced universum as negatives.",
    "en_tdlr": "Mixup is explored as a method to generate universum negatives in supervised contrastive learning, providing high-quality hard negatives and reducing the need for large batch sizes. The proposed UniCon incorporates Mixup strategy to generate Mixup-induced universum as negatives."
}