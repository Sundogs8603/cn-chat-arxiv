{
    "title": "Interval Bound Interpolation for Few-shot Learning with Few Tasks. (arXiv:2204.03511v3 [cs.LG] UPDATED)",
    "abstract": "Few-shot learning aims to transfer the knowledge acquired from training on a diverse set of tasks to unseen tasks from the same task distribution with a limited amount of labeled data. The underlying requirement for effective few-shot generalization is to learn a good representation of the task manifold. This becomes more difficult when only a limited number of tasks are available for training. In such a few-task few-shot setting, it is beneficial to explicitly preserve the local neighborhoods from the task manifold and exploit this to generate artificial tasks for training. To this end, we introduce the notion of interval bounds from the provably robust training literature to few-shot learning. The interval bounds are used to characterize neighborhoods around the training tasks. These neighborhoods can then be preserved by minimizing the distance between a task and its respective bounds. We then use a novel strategy to artificially form new tasks for training by interpolating between ",
    "link": "http://arxiv.org/abs/2204.03511",
    "context": "Title: Interval Bound Interpolation for Few-shot Learning with Few Tasks. (arXiv:2204.03511v3 [cs.LG] UPDATED)\nAbstract: Few-shot learning aims to transfer the knowledge acquired from training on a diverse set of tasks to unseen tasks from the same task distribution with a limited amount of labeled data. The underlying requirement for effective few-shot generalization is to learn a good representation of the task manifold. This becomes more difficult when only a limited number of tasks are available for training. In such a few-task few-shot setting, it is beneficial to explicitly preserve the local neighborhoods from the task manifold and exploit this to generate artificial tasks for training. To this end, we introduce the notion of interval bounds from the provably robust training literature to few-shot learning. The interval bounds are used to characterize neighborhoods around the training tasks. These neighborhoods can then be preserved by minimizing the distance between a task and its respective bounds. We then use a novel strategy to artificially form new tasks for training by interpolating between ",
    "path": "papers/22/04/2204.03511.json",
    "total_tokens": 860,
    "translated_title": "微小样本学习中的区间界插值算法",
    "translated_abstract": "微小样本学习旨在将通过对多样化任务进行训练所获取的知识转移到具有有限标记数据的相同任务分布中的新任务。实现有效的少次学习泛化的基本前提是学习任务流形的好的表示方法。在仅有受限数量任务的情况下，这变得更加困难。在这种少任务少学习情况下，显式地保留任务流形中的本地邻域并利用其生成训练人工任务可以协助提高性能。为此，我们将完全强韧性训练文献中的区间界概念引入到了微小样本学习中。区间界用于描述训练任务周围的领域。这些邻域可以通过最小化与任务及其相应边界之间的距离来保留。然后利用一种新颖的策略通过插值来人为形成新任务进行训练。",
    "tldr": "在微小样本学习中引入了区间界概念，通过最小化任务及其相应边界之间的距离来保留训练任务周围的领域，并通过插值来人为形成新任务进行训练。",
    "en_tdlr": "Interval bounds are introduced into few-shot learning for preserving local neighborhoods of the task manifold in the few-task few-shot setting. By minimizing the distance between a task and its respective bounds, new tasks can be generated for training through interpolation."
}