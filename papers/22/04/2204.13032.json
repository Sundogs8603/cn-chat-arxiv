{
    "title": "BiTimeBERT: Extending Pre-Trained Language Representations with Bi-Temporal Information. (arXiv:2204.13032v4 [cs.CL] UPDATED)",
    "abstract": "Time is an important aspect of documents and is used in a range of NLP and IR tasks. In this work, we investigate methods for incorporating temporal information during pre-training to further improve the performance on time-related tasks. Compared with common pre-trained language models like BERT which utilize synchronic document collections (e.g., BookCorpus and Wikipedia) as the training corpora, we use long-span temporal news article collection for building word representations. We introduce BiTimeBERT, a novel language representation model trained on a temporal collection of news articles via two new pre-training tasks, which harnesses two distinct temporal signals to construct time-aware language representations. The experimental results show that BiTimeBERT consistently outperforms BERT and other existing pre-trained models with substantial gains on different downstream NLP tasks and applications for which time is of importance (e.g., the accuracy improvement over BERT is 155\\% o",
    "link": "http://arxiv.org/abs/2204.13032",
    "context": "Title: BiTimeBERT: Extending Pre-Trained Language Representations with Bi-Temporal Information. (arXiv:2204.13032v4 [cs.CL] UPDATED)\nAbstract: Time is an important aspect of documents and is used in a range of NLP and IR tasks. In this work, we investigate methods for incorporating temporal information during pre-training to further improve the performance on time-related tasks. Compared with common pre-trained language models like BERT which utilize synchronic document collections (e.g., BookCorpus and Wikipedia) as the training corpora, we use long-span temporal news article collection for building word representations. We introduce BiTimeBERT, a novel language representation model trained on a temporal collection of news articles via two new pre-training tasks, which harnesses two distinct temporal signals to construct time-aware language representations. The experimental results show that BiTimeBERT consistently outperforms BERT and other existing pre-trained models with substantial gains on different downstream NLP tasks and applications for which time is of importance (e.g., the accuracy improvement over BERT is 155\\% o",
    "path": "papers/22/04/2204.13032.json",
    "total_tokens": 894,
    "translated_title": "BiTimeBERT: 利用双重时间信息扩展预训练语言表示",
    "translated_abstract": "时间是文档的重要方面，并在各种NLP和IR任务中使用。本文研究了在预训练期间引入时间信息的方法，以进一步提高在时间相关任务上的性能。与利用同步文档集合（例如BookCorpus和Wikipedia）作为训练语料库的常见预训练语言模型BERT相比，我们使用长跨度时间新闻文章集合来构建单词表示。我们介绍了一种新颖的语言表示模型BiTimeBERT，在时间新闻文章集合上通过两个新的预训练任务进行训练，利用两个不同的时间信号构建时间感知的语言表示。实验结果表明，BiTimeBERT始终优于BERT和其他现有的预训练模型，在时间重要的不同下游NLP任务和应用中获得了显著的性能提升（例如与BERT相比，准确性提高了155％）。",
    "tldr": "本论文介绍了一种新颖的语言表示模型BiTimeBERT，使用长跨度时间新闻文章集合构建单词表示，并通过两个新的预训练任务，利用两个不同的时间信号构建时间感知的语言表示。BiTimeBERT在时间相关任务中具有显著性能优势。",
    "en_tdlr": "This paper introduces a novel language representation model, BiTimeBERT, which uses long-span temporal news article collection for building word representations and constructs time-aware language representations via two new pre-training tasks using two distinct temporal signals. BiTimeBERT shows significant performance advantages in time-related tasks."
}