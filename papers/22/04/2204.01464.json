{
    "title": "Value Gradient weighted Model-Based Reinforcement Learning. (arXiv:2204.01464v2 [cs.LG] UPDATED)",
    "abstract": "Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the perfo",
    "link": "http://arxiv.org/abs/2204.01464",
    "context": "Title: Value Gradient weighted Model-Based Reinforcement Learning. (arXiv:2204.01464v2 [cs.LG] UPDATED)\nAbstract: Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the perfo",
    "path": "papers/22/04/2204.01464.json",
    "total_tokens": 744,
    "translated_title": "基于价值梯度加权的模型驱动强化学习",
    "translated_abstract": "模型驱动强化学习（MBRL）是一种高效获取控制策略的技术，但模型误差往往会导致性能下降。MBRL中的模型通常仅用于重建动态，特别是状态观察值，而模型误差对策略的影响不会被训练目标捕捉到。这导致MBRL的目标与实际使用的损失函数的目标不匹配，从而影响策略和价值的学习效果。本文提出了一种基于价值梯度加权的模型驱动强化学习方法（VaGraM），用于解决这个问题。",
    "tldr": "本文提出了一种基于价值梯度加权的模型驱动强化学习方法，用于提高模型学习的性能。",
    "en_tdlr": "This paper proposes a novel method for value-aware model learning in model-based reinforcement learning, which improves the performance of model learning by using value gradient weighting."
}