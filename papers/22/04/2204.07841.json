{
    "title": "Multi-Modal Few-Shot Object Detection with Meta-Learning-Based Cross-Modal Prompting. (arXiv:2204.07841v3 [cs.CV] UPDATED)",
    "abstract": "We study multi-modal few-shot object detection (FSOD) in this paper, using both few-shot visual examples and class semantic information for detection, which are complementary to each other by definition. Most of the previous works on multi-modal FSOD are fine-tuning-based which are inefficient for online applications. Moreover, these methods usually require expertise like class names to extract class semantic embedding, which are hard to get for rare classes. Our approach is motivated by the high-level conceptual similarity of (metric-based) meta-learning and prompt-based learning to learn generalizable few-shot and zero-shot object detection models respectively without fine-tuning. Specifically, we combine the few-shot visual classifier and text classifier learned via meta-learning and prompt-based learning respectively to build the multi-modal classifier and detection models. In addition, to fully exploit the pre-trained language models, we propose meta-learning-based cross-modal pro",
    "link": "http://arxiv.org/abs/2204.07841",
    "context": "Title: Multi-Modal Few-Shot Object Detection with Meta-Learning-Based Cross-Modal Prompting. (arXiv:2204.07841v3 [cs.CV] UPDATED)\nAbstract: We study multi-modal few-shot object detection (FSOD) in this paper, using both few-shot visual examples and class semantic information for detection, which are complementary to each other by definition. Most of the previous works on multi-modal FSOD are fine-tuning-based which are inefficient for online applications. Moreover, these methods usually require expertise like class names to extract class semantic embedding, which are hard to get for rare classes. Our approach is motivated by the high-level conceptual similarity of (metric-based) meta-learning and prompt-based learning to learn generalizable few-shot and zero-shot object detection models respectively without fine-tuning. Specifically, we combine the few-shot visual classifier and text classifier learned via meta-learning and prompt-based learning respectively to build the multi-modal classifier and detection models. In addition, to fully exploit the pre-trained language models, we propose meta-learning-based cross-modal pro",
    "path": "papers/22/04/2204.07841.json",
    "total_tokens": 994,
    "translated_title": "基于元学习的跨模态提示的多模态小样本目标检测",
    "translated_abstract": "本文研究了多模态小样本目标检测的问题，在检测中使用了视觉样本和语义信息，这些信息的使用是互补的。当前的多模态小样本目标检测方法大多是基于微调的，这对于在线应用来说效率较低。此外，这些方法通常需要专业知识，如类名等，以提取类语义嵌入，对于罕见类来说比较困难。我们的方法受到度量学习和基于提示的学习的高层概念相似性的启发，分别通过元学习和基于提示的学习来学习通用的小样本和零样本目标检测模型，而无需微调。具体来说，我们结合元学习和基于提示的学习分别学得的小样本视觉分类器和文本分类器来建立多模态分类器和检测模型。此外，为了充分利用预训练的语言模型，我们提出了基于元学习的跨模态提示来辅助检测器生成更全面的建议。在三个公共数据集上的实验结果表明，我们提出的方法在小样本和零样本检测任务方面均达到了最先进的性能。",
    "tldr": "本文提出了一种基于元学习的跨模态提示的多模态小样本目标检测方法，该方法不需要微调，结合了视觉样本和语义信息，并取得了最先进的性能。",
    "en_tdlr": "This paper proposes a meta-learning-based cross-modal prompting method for multi-modal few-shot object detection, which integrates visual examples and semantic information and achieves state-of-the-art performance in both few-shot and zero-shot detection tasks without fine-tuning."
}