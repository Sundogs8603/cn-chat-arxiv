{
    "title": "Adversarial Scratches: Deployable Attacks to CNN Classifiers. (arXiv:2204.09397v3 [cs.LG] UPDATED)",
    "abstract": "A growing body of work has shown that deep neural networks are susceptible to adversarial examples. These take the form of small perturbations applied to the model's input which lead to incorrect predictions. Unfortunately, most literature focuses on visually imperceivable perturbations to be applied to digital images that often are, by design, impossible to be deployed to physical targets. We present Adversarial Scratches: a novel L0 black-box attack, which takes the form of scratches in images, and which possesses much greater deployability than other state-of-the-art attacks. Adversarial Scratches leverage B\\'ezier Curves to reduce the dimension of the search space and possibly constrain the attack to a specific location. We test Adversarial Scratches in several scenarios, including a publicly available API and images of traffic signs. Results show that, often, our attack achieves higher fooling rate than other deployable state-of-the-art methods, while requiring significantly fewer",
    "link": "http://arxiv.org/abs/2204.09397",
    "context": "Title: Adversarial Scratches: Deployable Attacks to CNN Classifiers. (arXiv:2204.09397v3 [cs.LG] UPDATED)\nAbstract: A growing body of work has shown that deep neural networks are susceptible to adversarial examples. These take the form of small perturbations applied to the model's input which lead to incorrect predictions. Unfortunately, most literature focuses on visually imperceivable perturbations to be applied to digital images that often are, by design, impossible to be deployed to physical targets. We present Adversarial Scratches: a novel L0 black-box attack, which takes the form of scratches in images, and which possesses much greater deployability than other state-of-the-art attacks. Adversarial Scratches leverage B\\'ezier Curves to reduce the dimension of the search space and possibly constrain the attack to a specific location. We test Adversarial Scratches in several scenarios, including a publicly available API and images of traffic signs. Results show that, often, our attack achieves higher fooling rate than other deployable state-of-the-art methods, while requiring significantly fewer",
    "path": "papers/22/04/2204.09397.json",
    "total_tokens": 956,
    "translated_title": "对CNN分类器的对抗性痕迹攻击：可部署的攻击",
    "translated_abstract": "越来越多的研究表明，深度神经网络容易受到对抗性示例的攻击，这些示例是应用于模型输入的小扰动，导致模型做出错误的预测。本文提出了一种新型的L0黑盒攻击——对抗性痕迹攻击：通过在图像中制造痕迹的方式进行攻击，它比其他最先进的攻击方法更具有可部署性。对抗性痕迹利用Bezier曲线来减少搜索空间的维度，并可能将攻击约束到特定位置。我们在几种情况下测试了对抗性痕迹攻击，包括公开API和交通标志的图像。结果表明，我们的攻击通常比其他部署最先进的方法实现更高的破解率，同时需要明显更少的攻击次数。",
    "tldr": "本文提出了一种新型的对抗性攻击方法——对抗性痕迹攻击。通过在图像中制造痕迹的方式进行攻击，该方法比其他最先进的攻击方法更加可部署，并在公开API和交通标志的图像测试中表现出更高的破解率，攻击次数更少。",
    "en_tdlr": "This paper proposes a novel adversarial attack method named Adversarial Scratches, which takes the form of scratches in images and possesses much greater deployability than other state-of-the-art attacks. The attack leverages Bézier Curves to reduce the dimension of the search space and possibly constrain the attack to a specific location. The proposed method achieves higher fooling rate than other deployable state-of-the-art methods in testing on publicly available APIs and images of traffic signs, while requiring significantly fewer attack attempts."
}