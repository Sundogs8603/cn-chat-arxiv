{
    "title": "AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. (arXiv:2204.13170v4 [cs.LG] UPDATED)",
    "abstract": "In Federated Learning (FL), a number of clients or devices collaborate to train a model without sharing their data. Models are optimized locally at each client and further communicated to a central hub for aggregation. While FL is an appealing decentralized training paradigm, heterogeneity among data from different clients can cause the local optimization to drift away from the global objective. In order to estimate and therefore remove this drift, variance reduction techniques have been incorporated into FL optimization recently. However, these approaches inaccurately estimate the clients' drift and ultimately fail to remove it properly. In this work, we propose an adaptive algorithm that accurately estimates drift across clients. In comparison to previous works, our approach necessitates less storage and communication bandwidth, as well as lower compute costs. Additionally, our proposed methodology induces stability by constraining the norm of estimates for client drift, making it mo",
    "link": "http://arxiv.org/abs/2204.13170",
    "context": "Title: AdaBest: Minimizing Client Drift in Federated Learning via Adaptive Bias Estimation. (arXiv:2204.13170v4 [cs.LG] UPDATED)\nAbstract: In Federated Learning (FL), a number of clients or devices collaborate to train a model without sharing their data. Models are optimized locally at each client and further communicated to a central hub for aggregation. While FL is an appealing decentralized training paradigm, heterogeneity among data from different clients can cause the local optimization to drift away from the global objective. In order to estimate and therefore remove this drift, variance reduction techniques have been incorporated into FL optimization recently. However, these approaches inaccurately estimate the clients' drift and ultimately fail to remove it properly. In this work, we propose an adaptive algorithm that accurately estimates drift across clients. In comparison to previous works, our approach necessitates less storage and communication bandwidth, as well as lower compute costs. Additionally, our proposed methodology induces stability by constraining the norm of estimates for client drift, making it mo",
    "path": "papers/22/04/2204.13170.json",
    "total_tokens": 925,
    "translated_title": "AdaBest: 通过自适应偏差估计最小化联邦学习中的客户漂移",
    "translated_abstract": "在联邦学习中，许多客户端或设备在不共享数据的情况下协作训练模型。模型在每个客户端进行本地优化，然后传输到集中中心进行聚合。尽管联邦学习是一种吸引人的分散式训练范式，但来自不同客户端的数据的异质性可能导致局部优化偏离全局目标。为了估计和消除这种偏离，近期在联邦学习优化中引入了方差减少技术。然而，这些方法对客户端漂移进行了不准确的估计，并最终未能正确地消除它。在这项工作中，我们提出了一种精确估计客户端漂移的自适应算法。与之前的方法相比，我们的方法需要更少的存储和通信带宽，以及更低的计算成本。此外，我们提议的方法通过限制客户端漂移估计的范数来引入稳定性。",
    "tldr": "AdaBest提出了一种自适应算法，用于准确估计联邦学习中的客户端漂移。与之前的方法相比，AdaBest所需的存储和通信带宽较少，计算成本也较低。此外，AdaBest通过限制估计值的范数来提供稳定性。"
}