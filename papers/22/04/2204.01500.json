{
    "title": "Which Tricks Are Important for Learning to Rank?. (arXiv:2204.01500v2 [cs.LG] UPDATED)",
    "abstract": "Nowadays, state-of-the-art learning-to-rank methods are based on gradient-boosted decision trees (GBDT). The most well-known algorithm is LambdaMART which was proposed more than a decade ago. Recently, several other GBDT-based ranking algorithms were proposed. In this paper, we thoroughly analyze these methods in a unified setup. In particular, we address the following questions. Is direct optimization of a smoothed ranking loss preferable over optimizing a convex surrogate? How to properly construct and smooth surrogate ranking losses? To address these questions, we compare LambdaMART with YetiRank and StochasticRank methods and their modifications. We also propose a simple improvement of the YetiRank approach that allows for optimizing specific ranking loss functions. As a result, we gain insights into learning-to-rank techniques and obtain a new state-of-the-art algorithm.",
    "link": "http://arxiv.org/abs/2204.01500",
    "context": "Title: Which Tricks Are Important for Learning to Rank?. (arXiv:2204.01500v2 [cs.LG] UPDATED)\nAbstract: Nowadays, state-of-the-art learning-to-rank methods are based on gradient-boosted decision trees (GBDT). The most well-known algorithm is LambdaMART which was proposed more than a decade ago. Recently, several other GBDT-based ranking algorithms were proposed. In this paper, we thoroughly analyze these methods in a unified setup. In particular, we address the following questions. Is direct optimization of a smoothed ranking loss preferable over optimizing a convex surrogate? How to properly construct and smooth surrogate ranking losses? To address these questions, we compare LambdaMART with YetiRank and StochasticRank methods and their modifications. We also propose a simple improvement of the YetiRank approach that allows for optimizing specific ranking loss functions. As a result, we gain insights into learning-to-rank techniques and obtain a new state-of-the-art algorithm.",
    "path": "papers/22/04/2204.01500.json",
    "total_tokens": 767,
    "translated_title": "重要性排序学习中哪些技巧是重要的？",
    "translated_abstract": "如今，最先进的重要性排序学习方法基于梯度提升决策树（GBDT）。最著名的算法是LambdaMART，提出已有十多年。最近，提出了几种其他基于GBDT的排序算法。本文在统一的设置下全面分析这些方法。具体而言，我们解决以下问题。直接优化平滑排序损失是否优于优化凸似代理？如何正确构建和平滑代理排序损失？为了解决这些问题，我们将LambdaMART与YetiRank和StochasticRank方法及其修改进行比较。我们还提出了YetiRank方法的简单改进，允许优化特定的排序损失函数。结果，我们对重要性排序技术有了深入的理解，并获得了一种新的最先进算法。",
    "tldr": "本文全面分析了重要性排序学习方法，比较了LambdaMART、YetiRank和StochasticRank等方法，并提出了一种新的最先进算法。",
    "en_tdlr": "This paper thoroughly analyzes learning-to-rank methods, compares LambdaMART, YetiRank, and StochasticRank, and proposes a new state-of-the-art algorithm."
}