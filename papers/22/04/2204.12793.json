{
    "title": "Modern Baselines for SPARQL Semantic Parsing. (arXiv:2204.12793v3 [cs.IR] UPDATED)",
    "abstract": "In this work, we focus on the task of generating SPARQL queries from natural language questions, which can then be executed on Knowledge Graphs (KGs). We assume that gold entity and relations have been provided, and the remaining task is to arrange them in the right order along with SPARQL vocabulary, and input tokens to produce the correct SPARQL query. Pre-trained Language Models (PLMs) have not been explored in depth on this task so far, so we experiment with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings, looking for new baselines in the PLM era for this task, on DBpedia and Wikidata KGs. We show that T5 requires special input tokenisation, but produces state of the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms task-specific models from previous works. Moreover, the methods enable semantic parsing for questions where a part of the input needs to be copied to the output query, thus enabling a new paradigm in KG semantic parsing.",
    "link": "http://arxiv.org/abs/2204.12793",
    "context": "Title: Modern Baselines for SPARQL Semantic Parsing. (arXiv:2204.12793v3 [cs.IR] UPDATED)\nAbstract: In this work, we focus on the task of generating SPARQL queries from natural language questions, which can then be executed on Knowledge Graphs (KGs). We assume that gold entity and relations have been provided, and the remaining task is to arrange them in the right order along with SPARQL vocabulary, and input tokens to produce the correct SPARQL query. Pre-trained Language Models (PLMs) have not been explored in depth on this task so far, so we experiment with BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings, looking for new baselines in the PLM era for this task, on DBpedia and Wikidata KGs. We show that T5 requires special input tokenisation, but produces state of the art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms task-specific models from previous works. Moreover, the methods enable semantic parsing for questions where a part of the input needs to be copied to the output query, thus enabling a new paradigm in KG semantic parsing.",
    "path": "papers/22/04/2204.12793.json",
    "total_tokens": 1051,
    "translated_title": "SPARQL语义解析的现代基准模型",
    "translated_abstract": "本文关注于从自然语言问题生成SPARQL查询的任务，这些查询可以在知识图谱上执行。我们假设已经提供了黄金实体和关系，剩下的任务是将它们与SPARQL词汇和输入标记一起按正确的顺序排列，以生成正确的SPARQL查询。到目前为止，预训练语言模型（PLMs）在这个任务上尚未深入研究，因此我们尝试了使用BART、T5和PGNs（指针生成网络）与BERT嵌入来寻找这个任务在PLM时代的新基准，在DBpedia和Wikidata知识图谱上进行了实验。我们展示了T5需要特殊的输入标记化，但在LC-QuAD 1.0和LC-QuAD 2.0数据集上表现出最先进的性能，并超过了以前工作中的任务特定模型。此外，这些方法使得对问题进行语义解析成为可能，其中输入的一部分需要复制到输出查询中，从而实现了知识图谱语义解析的新范式。",
    "tldr": "本文探讨了从自然语言问题生成SPARQL查询的任务，使用预训练语言模型作为新的基准模型，并在DBpedia和Wikidata知识图谱上进行了实验。我们展示了T5模型在LC-QuAD 1.0和LC-QuAD 2.0数据集上表现出最先进的性能，并且能够解析需要将一部分输入复制到输出查询中的问题，这为知识图谱语义解析带来了新的可能性。"
}