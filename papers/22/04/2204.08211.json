{
    "title": "How to Attain Communication-Efficient DNN Training? Convert, Compress, Correct. (arXiv:2204.08211v2 [cs.LG] UPDATED)",
    "abstract": "This paper introduces CO3 -- an algorithm for communication-efficient federated Deep Neural Network (DNN) training. CO3 takes its name from three processing applied which reduce the communication load when transmitting the local DNN gradients from the remote users to the Parameter Server. Namely: (i) gradient quantization through floating-point conversion, (ii) lossless compression of the quantized gradient, and (iii) quantization error correction. We carefully design each of the steps above to assure good training performance under a constraint on the communication rate. In particular, in steps (i) and (ii), we adopt the assumption that DNN gradients are distributed according to a generalized normal distribution, which is validated numerically in the paper. For step (iii), we utilize an error feedback with memory decay mechanism to correct the quantization error introduced in step (i). We argue that the memory decay coefficient, similarly to the learning rate, can be optimally tuned t",
    "link": "http://arxiv.org/abs/2204.08211",
    "context": "Title: How to Attain Communication-Efficient DNN Training? Convert, Compress, Correct. (arXiv:2204.08211v2 [cs.LG] UPDATED)\nAbstract: This paper introduces CO3 -- an algorithm for communication-efficient federated Deep Neural Network (DNN) training. CO3 takes its name from three processing applied which reduce the communication load when transmitting the local DNN gradients from the remote users to the Parameter Server. Namely: (i) gradient quantization through floating-point conversion, (ii) lossless compression of the quantized gradient, and (iii) quantization error correction. We carefully design each of the steps above to assure good training performance under a constraint on the communication rate. In particular, in steps (i) and (ii), we adopt the assumption that DNN gradients are distributed according to a generalized normal distribution, which is validated numerically in the paper. For step (iii), we utilize an error feedback with memory decay mechanism to correct the quantization error introduced in step (i). We argue that the memory decay coefficient, similarly to the learning rate, can be optimally tuned t",
    "path": "papers/22/04/2204.08211.json",
    "total_tokens": 1046,
    "translated_abstract": "本文介绍了一种名为CO3的算法，用于通信高效的联邦深度神经网络（DNN）训练。CO3取其名自三个处理技术：（i）通过浮点转换梯度量化，（ii）无损压缩量化梯度和（iii）量化误差校正。我们精心设计了每个步骤，以确保在通信速率约束下具有良好的训练性能。特别是在步骤（i）和（ii）中，我们采用了假定DNN梯度服从广义正态分布的方法，这在论文中得到了验证。对于步骤（iii），我们利用具有内存衰减机制的误差反馈来校正步骤（i）中引入的量化误差。我们认为，与学习率类似，可以最优地调整记忆衰减系数。",
    "tldr": "本文介绍了一种名为CO3的算法，用于通信高效的联邦深度神经网络（DNN）训练。CO3通过三个处理技术来减少梯度传输时的通信负载，具体来说，包括梯度量化、无损压缩和量化误差校正。作者通过验证DNN梯度服从广义正态分布，精心设计每个步骤以满足训练性能与通信速率的约束。为了校正量化误差，作者还采用了具有内存衰减机制的误差反馈。",
    "en_tdlr": "This paper introduces an algorithm called CO3 for communication-efficient federated DNN training. CO3 reduces communication load by applying three techniques: gradient quantization, lossless compression, and quantization error correction. The authors carefully design each step to ensure good training performance under a communication rate constraint, and validate the assumption that DNN gradients follow a generalized normal distribution. They also use an error feedback mechanism with memory decay to correct the quantization error introduced during gradient quantization."
}