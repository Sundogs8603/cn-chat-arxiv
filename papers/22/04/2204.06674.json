{
    "title": "GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v4 [cs.CL] UPDATED)",
    "abstract": "Recent improvements in KG-to-text generation are due to additional auxiliary pre-training tasks designed to give the fine-tune task a boost in performance. These tasks require extensive computational resources while only suggesting marginal improvements. Here, we demonstrate that by fusing graph-aware elements into existing pre-trained language models, we are able to outperform state-of-the-art models and close the gap imposed by additional pre-training tasks. We do so by proposing a mask structure to capture neighborhood information and a novel type encoder that adds a bias to the graph-attention weights depending on the connection type. Experiments on two KG-to-text benchmark datasets show our models are competitive while involving fewer parameters and no additional pre-training tasks. By formulating the problem as a framework, we can interchange the various proposed components and begin interpreting KG-to-text generative models based on the topological and type information found in ",
    "link": "http://arxiv.org/abs/2204.06674",
    "context": "Title: GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v4 [cs.CL] UPDATED)\nAbstract: Recent improvements in KG-to-text generation are due to additional auxiliary pre-training tasks designed to give the fine-tune task a boost in performance. These tasks require extensive computational resources while only suggesting marginal improvements. Here, we demonstrate that by fusing graph-aware elements into existing pre-trained language models, we are able to outperform state-of-the-art models and close the gap imposed by additional pre-training tasks. We do so by proposing a mask structure to capture neighborhood information and a novel type encoder that adds a bias to the graph-attention weights depending on the connection type. Experiments on two KG-to-text benchmark datasets show our models are competitive while involving fewer parameters and no additional pre-training tasks. By formulating the problem as a framework, we can interchange the various proposed components and begin interpreting KG-to-text generative models based on the topological and type information found in ",
    "path": "papers/22/04/2204.06674.json",
    "total_tokens": 929,
    "translated_title": "GAP: 一种面向图知识库到文本生成的图感知语言模型框架",
    "translated_abstract": "最近，知识库到文本生成的改进是由于增加了辅助预训练任务来提高微调任务的性能。这些任务需要大量的计算资源，只提供了小幅改进。本文提出将图感知元素融入现有的预训练语言模型中，通过提出一个掩码结构来捕获邻域信息和一种新的类型编码器，添加图注意权重偏差，从而可以超越现有的最先进模型，并消除了额外预训练任务带来的差距。在两个知识库到文本的基准数据集上的实验证明，我们的模型是具有竞争力的，同时涉及更少的参数和没有额外的预训练任务。通过将问题制定为一个框架，我们可以交换各种提出的组件，并基于拓扑和类型信息解释基于知识库到文本的生成模型。",
    "tldr": "本文提出了一种面向知识库到文本生成的图感知语言模型框架，通过将图感知元素融入预训练语言模型中，提出了掩码结构和新的类型编码器，超越了现有的最先进模型，并消除了额外预训练任务所带来的差距。",
    "en_tdlr": "This paper proposes a graph-aware language model framework for KG-to-text generation, which outperforms state-of-the-art models and eliminates the gap imposed by additional pre-training tasks by fusing graph-aware elements into existing pre-trained language models, proposing a mask structure to capture neighborhood information and a novel type encoder that adds a bias to the graph-attention weights depending on the connection type, while involving fewer parameters and no additional pre-training tasks."
}