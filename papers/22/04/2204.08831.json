{
    "title": "Probing for the Usage of Grammatical Number. (arXiv:2204.08831v3 [cs.CL] UPDATED)",
    "abstract": "A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations. An encoding, however, might be spurious-i.e., the model might not rely on it when making predictions. In this paper, we try to find encodings that the model actually uses, introducing a usage-based probing setup. We first choose a behavioral task which cannot be solved without using the linguistic property. Then, we attempt to remove the property by intervening on the model's representations. We contend that, if an encoding is used by the model, its removal should harm the performance on the chosen behavioral task. As a case study, we focus on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task. Experimentally, we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output. We also find that BERT uses a separate encoding of grammatical number for nouns and verbs. Fina",
    "link": "http://arxiv.org/abs/2204.08831",
    "context": "Title: Probing for the Usage of Grammatical Number. (arXiv:2204.08831v3 [cs.CL] UPDATED)\nAbstract: A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations. An encoding, however, might be spurious-i.e., the model might not rely on it when making predictions. In this paper, we try to find encodings that the model actually uses, introducing a usage-based probing setup. We first choose a behavioral task which cannot be solved without using the linguistic property. Then, we attempt to remove the property by intervening on the model's representations. We contend that, if an encoding is used by the model, its removal should harm the performance on the chosen behavioral task. As a case study, we focus on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task. Experimentally, we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output. We also find that BERT uses a separate encoding of grammatical number for nouns and verbs. Fina",
    "path": "papers/22/04/2204.08831.json",
    "total_tokens": 912,
    "translated_title": "探究语法数的使用情况",
    "translated_abstract": "探究的核心问题是揭示预训练模型如何在其表示中编码语言属性。然而，编码可能是虚假的，即模型在进行预测时可能不依赖于它。在本文中，我们尝试寻找模型实际使用的编码，引入一种基于使用的探测设置。我们首先选择一个行为任务，该任务在不使用语言属性的情况下无法解决。然后，我们试图通过干预模型的表示来去除属性。我们认为，如果模型使用了某种编码，去除该编码应该会损害所选择的行为任务的性能。以BERT如何编码语法数以及如何利用该编码解决数的一致性任务为案例研究。实验结果显示，BERT依赖于语法数的线性编码来产生正确的行为输出。我们还发现BERT对名词和动词的语法数使用了不同的编码。",
    "tldr": "本文介绍了一种基于使用的探测设置，通过干预模型的表示来去除属性，从而发现模型实际使用的编码。以BERT如何编码语法数为例研究，结果显示BERT依赖于语法数的线性编码来产生正确的行为输出，并对名词和动词的语法数使用了不同的编码。",
    "en_tdlr": "This paper presents a usage-based probing setup to uncover the encodings that pre-trained models actually use. Taking BERT's encoding of grammatical number as a case study, the study finds that BERT relies on a linear encoding of grammatical number and uses different encodings for nouns and verbs."
}