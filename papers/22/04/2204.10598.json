{
    "title": "Sparsely-gated Mixture-of-Expert Layers for CNN Interpretability. (arXiv:2204.10598v3 [cs.CV] UPDATED)",
    "abstract": "Sparsely-gated Mixture of Expert (MoE) layers have been recently successfully applied for scaling large transformers, especially for language modeling tasks. An intriguing side effect of sparse MoE layers is that they convey inherent interpretability to a model via natural expert specialization. In this work, we apply sparse MoE layers to CNNs for computer vision tasks and analyze the resulting effect on model interpretability. To stabilize MoE training, we present both soft and hard constraint-based approaches. With hard constraints, the weights of certain experts are allowed to become zero, while soft constraints balance the contribution of experts with an additional auxiliary loss. As a result, soft constraints handle expert utilization better and support the expert specialization process, while hard constraints maintain more generalized experts and increase overall model performance. Our findings demonstrate that experts can implicitly focus on individual sub-domains of the input s",
    "link": "http://arxiv.org/abs/2204.10598",
    "context": "Title: Sparsely-gated Mixture-of-Expert Layers for CNN Interpretability. (arXiv:2204.10598v3 [cs.CV] UPDATED)\nAbstract: Sparsely-gated Mixture of Expert (MoE) layers have been recently successfully applied for scaling large transformers, especially for language modeling tasks. An intriguing side effect of sparse MoE layers is that they convey inherent interpretability to a model via natural expert specialization. In this work, we apply sparse MoE layers to CNNs for computer vision tasks and analyze the resulting effect on model interpretability. To stabilize MoE training, we present both soft and hard constraint-based approaches. With hard constraints, the weights of certain experts are allowed to become zero, while soft constraints balance the contribution of experts with an additional auxiliary loss. As a result, soft constraints handle expert utilization better and support the expert specialization process, while hard constraints maintain more generalized experts and increase overall model performance. Our findings demonstrate that experts can implicitly focus on individual sub-domains of the input s",
    "path": "papers/22/04/2204.10598.json",
    "total_tokens": 952,
    "translated_title": "稀疏门控专家层用于CNN可解释性",
    "translated_abstract": "稀疏门控专家(MoE)层最近成功应用于大规模变换器中，特别是用于语言模型任务。 稀疏MoE层的一个有趣的副作用是，通过自然的专家特化，它们为模型提供内在的可解释性。在本文中，我们将稀疏MoE层应用于计算机视觉任务中的CNN，并分析这对模型可解释性产生的影响。为了稳定MoE的训练，我们提出了软约束和硬约束两种方法。在硬约束中，某些专家的权重被允许变为零，而软约束则通过额外的辅助损失平衡专家的贡献。因此，软约束更好地处理了专家利用，并支持专家专业化进程，而硬约束保持了更广义的专家并增加了整个模型的性能。我们的研究结果表明，专家可以隐式地关注输入的各个子领域。",
    "tldr": "本文介绍了将稀疏门控专家层应用于计算机视觉中的CNN，并探究了这对模型可解释性的影响，同时也提出了软约束与硬约束两种方法来稳定MoE的训练。研究表明，该方法使专家可以有效关注输入的各个子领域，提高模型的性能。",
    "en_tdlr": "This paper introduces the application of sparsely-gated mixture-of-expert layers to CNNs for computer vision tasks, exploring its impact on model interpretability. Soft and hard constraint-based approaches are proposed to stabilize the training of the MoE layers, with findings suggesting that this method enables experts to effectively focus on individual sub-domains of the input, improving model performance."
}