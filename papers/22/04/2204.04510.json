{
    "title": "Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning",
    "abstract": "Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w",
    "link": "https://arxiv.org/abs/2204.04510",
    "context": "Title: Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning\nAbstract: Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w",
    "path": "papers/22/04/2204.04510.json",
    "total_tokens": 870,
    "translated_title": "将子图转化为节点让简单的图神经网络在子图表示学习上更强大和高效",
    "translated_abstract": "子图表示学习已经成为一个重要的问题，并且通常使用专门的图神经网络来处理大型全局图。这些模型需要大量的内存和计算资源，但挑战子图的层次结构建模。在本文中，我们提出了子图到节点（S2N）转换的新颖公式，用于学习子图的表示。具体而言，给定全局图中的一组子图，我们通过粗略地将子图转换成节点来构建一个新的图。通过理论和实证证据，S2N不仅相比最先进的模型显著减少了内存和计算成本，而且通过捕捉子图的局部和全局结构也在性能上超过了它们。通过利用图粗化方法，我们的方法甚至在数据稀缺的情况下也优于基线模型。我们在八个基准测试上的实验表明，调整模型后效果出色。",
    "tldr": "提出了一种将子图转化为节点的方法来学习子图的表示，该方法不仅显著降低了内存和计算成本，还捕捉了子图的局部和全局结构，并在多个基准测试上表现出色。",
    "en_tdlr": "A novel method is proposed to learn subgraph representations by translating subgraphs to nodes, which significantly reduces memory and computational costs and captures both local and global structures of the subgraph, outperforming state-of-the-art models on multiple benchmarks."
}