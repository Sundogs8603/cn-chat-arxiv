{
    "title": "Fast Inference from Transformers via Speculative Decoding. (arXiv:2211.17192v2 [cs.LG] UPDATED)",
    "abstract": "Inference from large autoregressive models like Transformers is slow decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.",
    "link": "http://arxiv.org/abs/2211.17192",
    "context": "Title: Fast Inference from Transformers via Speculative Decoding. (arXiv:2211.17192v2 [cs.LG] UPDATED)\nAbstract: Inference from large autoregressive models like Transformers is slow decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.",
    "path": "papers/22/11/2211.17192.json",
    "total_tokens": 751,
    "translated_title": "基于投机解码的Transformer快速推理",
    "translated_abstract": "从Transformer等大型自回归模型中进行推理是缓慢的，因为解码K个标记需要运行K次模型。本文介绍了一种名为“投机解码”的算法，它可以在不改变输出的情况下更快地从自回归模型中采样，通过并行计算多个标记实现。我们的方法可以加速现有的模型，而无需重新训练或进行架构更改。我们在T5-XXL上进行了演示，并显示相对于标准T5X实现，其加速了2X-3X，输出相同。",
    "tldr": "本文介绍了一种基于投机解码的算法，可以在不更改输出的情况下更快地从大型自回归模型（如Transformer）中采样，加速了现有的模型，而无需重新训练或进行架构更改。",
    "en_tdlr": "This paper introduces a speculative decoding algorithm, which can sample from large autoregressive models (such as Transformers) faster without changing the outputs, and accelerate existing models without retraining or architecture changes."
}