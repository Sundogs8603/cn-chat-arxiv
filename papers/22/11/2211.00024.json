{
    "title": "A robust estimator of mutual information for deep learning interpretability. (arXiv:2211.00024v2 [physics.data-an] UPDATED)",
    "abstract": "We develop the use of mutual information (MI), a well-established metric in information theory, to interpret the inner workings of deep learning models. To accurately estimate MI from a finite number of samples, we present GMM-MI (pronounced $``$Jimmie$\"$), an algorithm based on Gaussian mixture models that can be applied to both discrete and continuous settings. GMM-MI is computationally efficient, robust to the choice of hyperparameters and provides the uncertainty on the MI estimate due to the finite sample size. We extensively validate GMM-MI on toy data for which the ground truth MI is known, comparing its performance against established mutual information estimators. We then demonstrate the use of our MI estimator in the context of representation learning, working with synthetic data and physical datasets describing highly non-linear processes. We train deep learning models to encode high-dimensional data within a meaningful compressed (latent) representation, and use GMM-MI to q",
    "link": "http://arxiv.org/abs/2211.00024",
    "context": "Title: A robust estimator of mutual information for deep learning interpretability. (arXiv:2211.00024v2 [physics.data-an] UPDATED)\nAbstract: We develop the use of mutual information (MI), a well-established metric in information theory, to interpret the inner workings of deep learning models. To accurately estimate MI from a finite number of samples, we present GMM-MI (pronounced $``$Jimmie$\"$), an algorithm based on Gaussian mixture models that can be applied to both discrete and continuous settings. GMM-MI is computationally efficient, robust to the choice of hyperparameters and provides the uncertainty on the MI estimate due to the finite sample size. We extensively validate GMM-MI on toy data for which the ground truth MI is known, comparing its performance against established mutual information estimators. We then demonstrate the use of our MI estimator in the context of representation learning, working with synthetic data and physical datasets describing highly non-linear processes. We train deep learning models to encode high-dimensional data within a meaningful compressed (latent) representation, and use GMM-MI to q",
    "path": "papers/22/11/2211.00024.json",
    "total_tokens": 904,
    "translated_title": "一种深度学习可解释性的鲁棒互信息估计器",
    "translated_abstract": "我们发展了互信息（MI）在信息论中广泛应用的度量方式，用于解释深度学习模型内部的工作机制。为了准确地从有限数量的样本中估计MI，我们提出了GMM-MI（读作“Jimmie”），这是一种基于高斯混合模型的算法，可用于离散和连续设置。GMM-MI在计算效率上是高效的，对于超参数的选择具有鲁棒性，并提供由于有限样本大小而引起的MI估计的不确定性。我们广泛地验证了GMM-MI在玩具数据上的表现，对比了已确定的互信息估计器的性能。然后，在表示学习的情境下，我们使用合成数据和描述高度非线性过程的物理数据集，训练深度学习模型，将高维数据编码为有意义的压缩（潜在）表示，并使用GMM-MI进行解释。",
    "tldr": "该论文提出了一种鲁棒的互信息（MI）估计器，名为GMM-MI，它可以用于解释深度学习模型内部的工作机制，并在表示学习的情境下进行了验证。"
}