{
    "title": "AdaFocal: Calibration-aware Adaptive Focal Loss. (arXiv:2211.11838v2 [cs.LG] UPDATED)",
    "abstract": "Much recent work has been devoted to the problem of ensuring that a neural network's confidence scores match the true probability of being correct, i.e. the calibration problem. Of note, it was found that training with focal loss leads to better calibration than cross-entropy while achieving similar level of accuracy \\cite{mukhoti2020}. This success stems from focal loss regularizing the entropy of the model's prediction (controlled by the parameter $\\gamma$), thereby reining in the model's overconfidence. Further improvement is expected if $\\gamma$ is selected independently for each training sample (Sample-Dependent Focal Loss (FLSD-53) \\cite{mukhoti2020}). However, FLSD-53 is based on heuristics and does not generalize well. In this paper, we propose a calibration-aware adaptive focal loss called AdaFocal that utilizes the calibration properties of focal (and inverse-focal) loss and adaptively modifies $\\gamma_t$ for different groups of samples based on $\\gamma_{t-1}$ from the previo",
    "link": "http://arxiv.org/abs/2211.11838",
    "context": "Title: AdaFocal: Calibration-aware Adaptive Focal Loss. (arXiv:2211.11838v2 [cs.LG] UPDATED)\nAbstract: Much recent work has been devoted to the problem of ensuring that a neural network's confidence scores match the true probability of being correct, i.e. the calibration problem. Of note, it was found that training with focal loss leads to better calibration than cross-entropy while achieving similar level of accuracy \\cite{mukhoti2020}. This success stems from focal loss regularizing the entropy of the model's prediction (controlled by the parameter $\\gamma$), thereby reining in the model's overconfidence. Further improvement is expected if $\\gamma$ is selected independently for each training sample (Sample-Dependent Focal Loss (FLSD-53) \\cite{mukhoti2020}). However, FLSD-53 is based on heuristics and does not generalize well. In this paper, we propose a calibration-aware adaptive focal loss called AdaFocal that utilizes the calibration properties of focal (and inverse-focal) loss and adaptively modifies $\\gamma_t$ for different groups of samples based on $\\gamma_{t-1}$ from the previo",
    "path": "papers/22/11/2211.11838.json",
    "total_tokens": 836,
    "translated_title": "AdaFocal：校准感知自适应Focal Loss",
    "translated_abstract": "近期的工作致力于解决神经网络的置信度得分与正确概率不相符的校准问题。研究发现，相较于交叉熵，使用Focal Loss进行训练能更好地解决校准问题并保持相同的准确性。Focal Loss通过调控参数γ来规范模型预测的熵，从而抑制模型的过度自信。本文提出了一种新的校准感知自适应Focal Loss，称为AdaFocal，它利用了Focal Loss和Inverse-Focal Loss的校准性质，并根据前一次迭代中的γt自适应地修改不同样本组的γt。实验证明，所提出的方法在多个数据集上优于最先进的方法解决校准问题。",
    "tldr": "本文介绍了AdaFocal，一种新的校准感知自适应Focal Loss。与其他方法相比，在多个数据集上表现出更好的校准性能。",
    "en_tdlr": "This paper introduces AdaFocal, a novel calibration-aware adaptive Focal Loss. Compared to other methods, it shows better calibration performance on multiple datasets."
}