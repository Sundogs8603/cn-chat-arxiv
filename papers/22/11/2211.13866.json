{
    "title": "Minimal Width for Universal Property of Deep RNN. (arXiv:2211.13866v2 [stat.ML] UPDATED)",
    "abstract": "A recurrent neural network (RNN) is a widely used deep-learning network for dealing with sequential data. Imitating a dynamical system, an infinite-width RNN can approximate any open dynamical system in a compact domain. In general, deep networks with bounded widths are more effective than wide networks in practice; however, the universal approximation theorem for deep narrow structures has yet to be extensively studied. In this study, we prove the universality of deep narrow RNNs and show that the upper bound of the minimum width for universality can be independent of the length of the data. Specifically, we show that a deep RNN with ReLU activation can approximate any continuous function or $L^p$ function with the widths $d_x+d_y+2$ and $\\max\\{d_x+1,d_y\\}$, respectively, where the target function maps a finite sequence of vectors in $\\mathbb{R}^{d_x}$ to a finite sequence of vectors in $\\mathbb{R}^{d_y}$. We also compute the additional width required if the activation function is $\\t",
    "link": "http://arxiv.org/abs/2211.13866",
    "context": "Title: Minimal Width for Universal Property of Deep RNN. (arXiv:2211.13866v2 [stat.ML] UPDATED)\nAbstract: A recurrent neural network (RNN) is a widely used deep-learning network for dealing with sequential data. Imitating a dynamical system, an infinite-width RNN can approximate any open dynamical system in a compact domain. In general, deep networks with bounded widths are more effective than wide networks in practice; however, the universal approximation theorem for deep narrow structures has yet to be extensively studied. In this study, we prove the universality of deep narrow RNNs and show that the upper bound of the minimum width for universality can be independent of the length of the data. Specifically, we show that a deep RNN with ReLU activation can approximate any continuous function or $L^p$ function with the widths $d_x+d_y+2$ and $\\max\\{d_x+1,d_y\\}$, respectively, where the target function maps a finite sequence of vectors in $\\mathbb{R}^{d_x}$ to a finite sequence of vectors in $\\mathbb{R}^{d_y}$. We also compute the additional width required if the activation function is $\\t",
    "path": "papers/22/11/2211.13866.json",
    "total_tokens": 867,
    "translated_abstract": "循环神经网络（RNN）是处理序列数据的常用深度学习网络。类比于动力系统，无穷宽的RNN可以在紧致区域中逼近任何开放动力系统。在实践中，具有有限宽度的深度网络通常比宽网络更有效，但深窄结构的通用逼近定理尚未被广泛研究。在本研究中，我们证明了深窄RNN的普适性，并展示了最小宽度的上界可以独立于数据长度。具体而言，我们证明了带有ReLU激活函数的深度RNN可以逼近任何定义在$\\mathbb{R}^{d_x}$有限向量序列到$\\mathbb{R}^{d_y}$有限向量序列的连续函数或$L^p$函数，其宽度分别为$d_x+d_y+2$和$\\max\\{d_x+1,d_y\\}$。我们还计算了激活函数为$\\t$",
    "tldr": "本文证明了深度窄循环神经网络的普适性，并找到了最小宽度的上界，与数据长度无关。",
    "en_tdlr": "This article proves the universality of deep narrow recurrent neural networks and finds an upper bound for the minimum width that is independent of data length."
}