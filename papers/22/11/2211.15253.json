{
    "title": "Lipschitz constant estimation for 1D convolutional neural networks. (arXiv:2211.15253v2 [cs.LG] UPDATED)",
    "abstract": "In this work, we propose a dissipativity-based method for Lipschitz constant estimation of 1D convolutional neural networks (CNNs). In particular, we analyze the dissipativity properties of convolutional, pooling, and fully connected layers making use of incremental quadratic constraints for nonlinear activation functions and pooling operations. The Lipschitz constant of the concatenation of these mappings is then estimated by solving a semidefinite program which we derive from dissipativity theory. To make our method as efficient as possible, we exploit the structure of convolutional layers by realizing these finite impulse response filters as causal dynamical systems in state space and carrying out the dissipativity analysis for the state space realizations. The examples we provide show that our Lipschitz bounds are advantageous in terms of accuracy and scalability.",
    "link": "http://arxiv.org/abs/2211.15253",
    "context": "Title: Lipschitz constant estimation for 1D convolutional neural networks. (arXiv:2211.15253v2 [cs.LG] UPDATED)\nAbstract: In this work, we propose a dissipativity-based method for Lipschitz constant estimation of 1D convolutional neural networks (CNNs). In particular, we analyze the dissipativity properties of convolutional, pooling, and fully connected layers making use of incremental quadratic constraints for nonlinear activation functions and pooling operations. The Lipschitz constant of the concatenation of these mappings is then estimated by solving a semidefinite program which we derive from dissipativity theory. To make our method as efficient as possible, we exploit the structure of convolutional layers by realizing these finite impulse response filters as causal dynamical systems in state space and carrying out the dissipativity analysis for the state space realizations. The examples we provide show that our Lipschitz bounds are advantageous in terms of accuracy and scalability.",
    "path": "papers/22/11/2211.15253.json",
    "total_tokens": 816,
    "translated_title": "一维卷积神经网络的Lipschitz常数估计",
    "translated_abstract": "本文提出了一种基于耗散方法的一维卷积神经网络（CNNs）Lipschitz常数估计方法。具体来说，我们利用非线性激活函数和池化操作的增量二次约束分析了卷积、池化和全连接层的耗散特性。然后，通过解决从耗散理论中推导出的半定规划问题来估计这些映射的拼接的Lipschitz常数。为了使我们的方法尽可能高效，我们利用卷积层的结构，将这些有限冲激响应滤波器表示为状态空间中的因果动态系统，并对状态空间实现进行耗散性分析。我们提供的示例表明，我们的Lipschitz界在准确性和可扩展性方面具有优势。",
    "tldr": "本文提出了一种利用耗散理论估计一维卷积神经网络Lipschitz常数的方法，具有高效、准确和可扩展性的特点。",
    "en_tdlr": "This paper proposes a dissipativity-based method for estimating the Lipschitz constant of 1D convolutional neural networks (CNNs), which is efficient, accurate, and scalable, by analyzing the dissipativity properties of convolutional, pooling, and fully connected layers using incremental quadratic constraints and solving a semidefinite program."
}