{
    "title": "Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces. (arXiv:2211.14400v3 [stat.ML] UPDATED)",
    "abstract": "Let $\\Omega = [0,1]^d$ be the unit cube in $\\mathbb{R}^d$. We study the problem of how efficiently, in terms of the number of parameters, deep neural networks with the ReLU activation function can approximate functions in the Sobolev spaces $W^s(L_q(\\Omega))$ and Besov spaces $B^s_r(L_q(\\Omega))$, with error measured in the $L_p(\\Omega)$ norm. This problem is important when studying the application of neural networks in a variety of fields, including scientific computing and signal processing, and has previously been completely solved only when $p=q=\\infty$. Our contribution is to provide a complete solution for all $1\\leq p,q\\leq \\infty$ and $s > 0$, including asymptotically matching upper and lower bounds. The key technical tool is a novel bit-extraction technique which gives an optimal encoding of sparse vectors. This enables us to obtain sharp upper bounds in the non-linear regime where $p > q$. We also provide a novel method for deriving $L_p$-approximation lower bounds based upon",
    "link": "http://arxiv.org/abs/2211.14400",
    "context": "Title: Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces. (arXiv:2211.14400v3 [stat.ML] UPDATED)\nAbstract: Let $\\Omega = [0,1]^d$ be the unit cube in $\\mathbb{R}^d$. We study the problem of how efficiently, in terms of the number of parameters, deep neural networks with the ReLU activation function can approximate functions in the Sobolev spaces $W^s(L_q(\\Omega))$ and Besov spaces $B^s_r(L_q(\\Omega))$, with error measured in the $L_p(\\Omega)$ norm. This problem is important when studying the application of neural networks in a variety of fields, including scientific computing and signal processing, and has previously been completely solved only when $p=q=\\infty$. Our contribution is to provide a complete solution for all $1\\leq p,q\\leq \\infty$ and $s > 0$, including asymptotically matching upper and lower bounds. The key technical tool is a novel bit-extraction technique which gives an optimal encoding of sparse vectors. This enables us to obtain sharp upper bounds in the non-linear regime where $p > q$. We also provide a novel method for deriving $L_p$-approximation lower bounds based upon",
    "path": "papers/22/11/2211.14400.json",
    "total_tokens": 946,
    "translated_title": "在Sobolev和Besov空间上，关于深度ReLU神经网络的最佳逼近速率研究",
    "translated_abstract": "本文研究了使用ReLU激活函数的深度神经网络在Sobolev空间$W^s(L_q(\\Omega))$和Besov空间$B^s_r(L_q(\\Omega))$中以$L_p(\\Omega)$范数度量误差的参数效率问题。我们的研究对于在科学计算和信号处理等领域中应用神经网络非常重要，在过去只有当$p=q=\\infty$时才完全解决。我们的贡献是提供了所有$1\\leq p,q\\leq \\infty$和$s>0$的完整解决方案，包括渐近匹配的上下界。关键的技术工具是一种新的位提取技术，它提供了稀疏向量的最佳编码。这使我们能够在$p>q$的非线性区域获得尖锐的上界。我们还提供了一种基于的$L_p$逼近下界推导的新方法。",
    "tldr": "该论文研究了在Sobolev和Besov空间中，使用ReLU激活函数的深度神经网络能够以怎样的参数效率逼近函数，包括$L_p(\\Omega)$范数下的误差度量。我们提供了所有$1\\leq p,q \\leq \\infty$和$s>0$的完整解决方案，并引入了一种新的位提取技术来获得尖锐的上界。"
}