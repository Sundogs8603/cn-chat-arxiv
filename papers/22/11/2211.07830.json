{
    "title": "Prompting Language Models for Linguistic Structure. (arXiv:2211.07830v2 [cs.CL] UPDATED)",
    "abstract": "Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.",
    "link": "http://arxiv.org/abs/2211.07830",
    "context": "Title: Prompting Language Models for Linguistic Structure. (arXiv:2211.07830v2 [cs.CL] UPDATED)\nAbstract: Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.",
    "path": "papers/22/11/2211.07830.json",
    "total_tokens": 924,
    "translated_title": "挑战语言模型的语言结构推理能力",
    "translated_abstract": "尽管预训练语言模型（PLMs）可以完成各种各样的语言任务，但基于可推广性的语言理解能力与基于表面级别的词汇模式有多少关系仍然是个未解之谜。为了测试这个问题，我们提出了一种针对语言结构预测任务的结构化提示方法，允许我们在自回归PLMs的零样本和少样本情况下进行序列标注。我们将其用于词性标注、命名实体识别和句子分块，并证明了它在所有情况下的强大少样本性能。我们发现，虽然由于任务泄露到预训练语料库中，PLMs包含了重要的任务标签先验知识，但结构化提示也可以检索任意标签的语言结构。这些发现表明，PLMs的上下文学习能力和语言知识具有推广性，不仅局限于其训练数据的记忆。",
    "tldr": "本文探究了预训练语言模型（PLMs）对于语言结构的推理能力，通过结构化提示方法，在零样本和少样本情况下进行了序列标注实验，并证明了 PLMs 的上下文学习能力和语言知识的推广性，这能够帮助检索任意标签的语言结构。",
    "en_tdlr": "This paper investigates the linguistic structure inference ability of pretrained language models (PLMs) and proposes a structured prompting approach for zero- and few-shot sequence tagging. The experiments on part-of-speech tagging, named entity recognition, and sentence chunking demonstrate the generalization of the in-context learning ability and linguistic knowledge of PLMs beyond memorization of their training data. This structured prompting approach can also retrieve linguistic structure with arbitrary labels."
}