{
    "title": "Diffusion Denoising Process for Perceptron Bias in Out-of-distribution Detection. (arXiv:2211.11255v2 [cs.CV] UPDATED)",
    "abstract": "Out-of-distribution (OOD) detection is a crucial task for ensuring the reliability and safety of deep learning. Currently, discriminator models outperform other methods in this regard. However, the feature extraction process used by discriminator models suffers from the loss of critical information, leaving room for bad cases and malicious attacks. In this paper, we introduce a new perceptron bias assumption that suggests discriminator models are more sensitive to certain features of the input, leading to the overconfidence problem. To address this issue, we propose a novel framework that combines discriminator and generation models and integrates diffusion models (DMs) into OOD detection. We demonstrate that the diffusion denoising process (DDP) of DMs serves as a novel form of asymmetric interpolation, which is well-suited to enhance the input and mitigate the overconfidence problem. The discriminator model features of OOD data exhibit sharp changes under DDP, and we utilize the norm",
    "link": "http://arxiv.org/abs/2211.11255",
    "context": "Title: Diffusion Denoising Process for Perceptron Bias in Out-of-distribution Detection. (arXiv:2211.11255v2 [cs.CV] UPDATED)\nAbstract: Out-of-distribution (OOD) detection is a crucial task for ensuring the reliability and safety of deep learning. Currently, discriminator models outperform other methods in this regard. However, the feature extraction process used by discriminator models suffers from the loss of critical information, leaving room for bad cases and malicious attacks. In this paper, we introduce a new perceptron bias assumption that suggests discriminator models are more sensitive to certain features of the input, leading to the overconfidence problem. To address this issue, we propose a novel framework that combines discriminator and generation models and integrates diffusion models (DMs) into OOD detection. We demonstrate that the diffusion denoising process (DDP) of DMs serves as a novel form of asymmetric interpolation, which is well-suited to enhance the input and mitigate the overconfidence problem. The discriminator model features of OOD data exhibit sharp changes under DDP, and we utilize the norm",
    "path": "papers/22/11/2211.11255.json",
    "total_tokens": 903,
    "translated_title": "基于扩散去噪过程的感知器偏置在异常检测中的应用",
    "translated_abstract": "异常检测对于保证深度学习模型的可靠性和安全性至关重要。目前，判别器模型在这方面的表现超过其他方法。然而，判别器模型使用的特征提取过程容易丢失关键信息，留下不良情况和恶意攻击的空间。在本文中，我们引入了一个新的感知器偏置假设，它表明判别器模型对输入的某些特征更为敏感，导致过度自信的问题。为了解决这个问题，我们提出了一个新的框架，它结合了判别器和生成模型，并将扩散模型(DMs)集成到OOD检测中。我们证明了扩散去噪过程(DDP)作为一种新形式的非对称插值，很适合增强输入并减轻过度自信的问题。在DDP下，OOD数据的判别器模型特征表现为尖锐的变化，我们利用范数...",
    "tldr": "本文针对深度学习中的异常检测问题，提出了一种新的方法——使用扩散模型作为非对称插值的方法来增强输入并减轻过度自信的问题，从而提高判别器模型在异常检测方面的性能。",
    "en_tdlr": "This paper proposes a novel method to enhance the performance of discriminator models in out-of-distribution detection by using diffusion models as asymmetric interpolation to enhance input and alleviate overconfidence problems."
}