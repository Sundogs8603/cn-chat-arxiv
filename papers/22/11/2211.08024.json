{
    "title": "NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction. (arXiv:2211.08024v3 [cs.CV] UPDATED)",
    "abstract": "With the wide and deep adoption of deep learning models in real applications, there is an increasing need to model and learn the representations of the neural networks themselves. These models can be used to estimate attributes of different neural network architectures such as the accuracy and latency, without running the actual training or inference tasks. In this paper, we propose a neural architecture representation model that can be used to estimate these attributes holistically. Specifically, we first propose a simple and effective tokenizer to encode both the operation and topology information of a neural network into a single sequence. Then, we design a multi-stage fusion transformer to build a compact vector representation from the converted sequence. For efficient model training, we further propose an information flow consistency augmentation and correspondingly design an architecture consistency loss, which brings more benefits with less augmentation samples compared with pre",
    "link": "http://arxiv.org/abs/2211.08024",
    "context": "Title: NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction. (arXiv:2211.08024v3 [cs.CV] UPDATED)\nAbstract: With the wide and deep adoption of deep learning models in real applications, there is an increasing need to model and learn the representations of the neural networks themselves. These models can be used to estimate attributes of different neural network architectures such as the accuracy and latency, without running the actual training or inference tasks. In this paper, we propose a neural architecture representation model that can be used to estimate these attributes holistically. Specifically, we first propose a simple and effective tokenizer to encode both the operation and topology information of a neural network into a single sequence. Then, we design a multi-stage fusion transformer to build a compact vector representation from the converted sequence. For efficient model training, we further propose an information flow consistency augmentation and correspondingly design an architecture consistency loss, which brings more benefits with less augmentation samples compared with pre",
    "path": "papers/22/11/2211.08024.json",
    "total_tokens": 1083,
    "translated_title": "NAR-Former：面向全面属性预测的神经架构表示学习",
    "translated_abstract": "随着深度学习模型在实际应用中的广泛和深入采用，越来越需要对神经网络本身的表示进行建模和学习。这些模型可以用于估计不同神经网络架构的属性，例如准确性和延迟，而不需要运行实际的训练或推断任务。本文提出了一种神经架构表示模型，可用于全面估计这些属性。具体地，我们首先提出了一个简单而有效的标记器，将神经网络的操作和拓扑信息编码为一个单一的序列。然后，我们设计了一个多阶段融合变压器，从转换后的序列中构建紧凑的向量表示。为了有效的模型训练，我们进一步提出了信息流一致性增强，并相应地设计了一个架构一致性损失，与预测的损失相比，更少的增广样本带来了更多的好处。在各种基准数据集上的实验证实了所提出的 NAR-Former 模型的有效性，在不同神经架构的准确性和延迟估计方面，与最先进的方法相比取得了有竞争力的性能。",
    "tldr": "本文提出了一种神经架构表示模型NAR-Former来全面估计神经网络架构的属性，通过标记器和多阶段融合变压器构建紧凑的向量表示，并采用信息流一致性增强和架构一致性损失进行有效的模型训练。实验证实了该模型在准确性和延迟估计方面与最先进的方法相比取得了有竞争力的性能。",
    "en_tdlr": "The paper proposes the NAR-Former model for comprehensive attribute estimation of neural network architecture, leveraging a tokenizer and multi-stage fusion transformer to build a compact vector representation, and adopting information flow consistency augmentation and architecture consistency loss for efficient model training. Experiments show that the proposed model achieves competitive performance compared with state-of-the-art approaches in terms of accuracy and latency estimation of different neural architectures."
}