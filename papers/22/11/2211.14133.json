{
    "title": "PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices. (arXiv:2211.14133v2 [cs.LG] UPDATED)",
    "abstract": "Pipeline parallelism enables efficient training of Large Language Models (LLMs) on large-scale distributed accelerator clusters. Yet, pipeline bubbles during startup and tear-down reduce the utilization of accelerators. Although efficient pipeline schemes with micro-batching and bidirectional pipelines have been proposed to maximize utilization, a significant number of bubbles cannot be filled using synchronous forward and backward passes. To address this problem, we suggest that extra work be assigned to the bubbles to gain auxiliary benefits in LLM training. As an example in this direction, we propose PipeFisher, which assigns the work of K-FAC, a second-order optimization method based on the Fisher information matrix, to the bubbles to accelerate convergence. In Phase 1 pretraining of BERT-Base and -Large models, PipeFisher reduces the (simulated) training time to 50-75% compared to training with a first-order optimizer by greatly improving the accelerator utilization and benefiting",
    "link": "http://arxiv.org/abs/2211.14133",
    "context": "Title: PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices. (arXiv:2211.14133v2 [cs.LG] UPDATED)\nAbstract: Pipeline parallelism enables efficient training of Large Language Models (LLMs) on large-scale distributed accelerator clusters. Yet, pipeline bubbles during startup and tear-down reduce the utilization of accelerators. Although efficient pipeline schemes with micro-batching and bidirectional pipelines have been proposed to maximize utilization, a significant number of bubbles cannot be filled using synchronous forward and backward passes. To address this problem, we suggest that extra work be assigned to the bubbles to gain auxiliary benefits in LLM training. As an example in this direction, we propose PipeFisher, which assigns the work of K-FAC, a second-order optimization method based on the Fisher information matrix, to the bubbles to accelerate convergence. In Phase 1 pretraining of BERT-Base and -Large models, PipeFisher reduces the (simulated) training time to 50-75% compared to training with a first-order optimizer by greatly improving the accelerator utilization and benefiting",
    "path": "papers/22/11/2211.14133.json",
    "total_tokens": 907,
    "translated_title": "PipeFisher: 利用管道并结合Fisher信息矩阵的高效大语言模型训练方法",
    "translated_abstract": "管道并行技术可以在大规模分布式加速器集群上高效地训练大型语言模型。然而，在启动和关闭期间的管道间隙会降低加速器的利用率。虽然已经提出了一些高效的管道方案来最大化利用率，如微批处理和双向管道，但仍有大量间隙无法使用同步前向和后向传递填充。为了解决这个问题，我们建议将额外的工作分配给管道间隙，在大语言模型训练中获得辅助效益。作为这种方向的一个例子，我们提出了PipeFisher，它将基于Fisher信息矩阵的二阶优化方法K-FAC的工作分配给管道间隙，加速收敛。在BERT-Base和-Large模型的第一阶段预训练中，PipeFisher将（模拟的）训练时间缩短了50-75％，相比于使用一阶优化器，大大提高了加速器的利用率并获得更好的训练效果。",
    "tldr": "PipeFisher提出了一种新的方法，将二阶优化方法K-FAC的工作分配给管道间隙，以加速收敛和提高大语言模型训练的有效性。",
    "en_tdlr": "PipeFisher proposes a new approach to accelerate convergence and improve the efficiency of large language model training by assigning the work of the second-order optimization method K-FAC to the pipeline bubbles."
}