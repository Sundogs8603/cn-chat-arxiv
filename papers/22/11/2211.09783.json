{
    "title": "UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization. (arXiv:2211.09783v6 [cs.CL] UPDATED)",
    "abstract": "The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose \\textsc{UniSumm}, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark \\textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of few-shot samples for each task, covering diverse domains. Experimental results and analysis show that \\textsc{UniSumm} outperforms strong baselines by a large margin across all sub-tasks in \\textsc{SummZoo} under both automatic and human evaluations",
    "link": "http://arxiv.org/abs/2211.09783",
    "context": "Title: UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization. (arXiv:2211.09783v6 [cs.CL] UPDATED)\nAbstract: The high annotation costs and diverse demands of various summarization tasks motivate the development of few-shot summarization. However, despite the emergence of many summarization tasks and datasets, the current training paradigm for few-shot summarization systems ignores potentially shareable knowledge in heterogeneous datasets. To this end, we propose \\textsc{UniSumm}, a unified few-shot summarization model pre-trained with multiple summarization tasks and can be prefix-tuned to excel at any few-shot summarization task. Meanwhile, to better evaluate few-shot summarizers, under the principles of diversity and robustness, we assemble and release a new benchmark \\textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of few-shot samples for each task, covering diverse domains. Experimental results and analysis show that \\textsc{UniSumm} outperforms strong baselines by a large margin across all sub-tasks in \\textsc{SummZoo} under both automatic and human evaluations",
    "path": "papers/22/11/2211.09783.json",
    "total_tokens": 967,
    "translated_title": "UniSumm和SummZoo：少样本摘要的统一模型和多样化基准",
    "translated_abstract": "高昂的注释成本和各种摘要任务的多样化需求推动了少样本摘要的发展。然而，尽管涌现了许多摘要任务和数据集，目前少样本摘要系统的训练范式忽略了异构数据集中可能共享的知识。为此，我们提出了UniSumm，一种统一的少样本摘要模型，预先训练了多项摘要任务，并可以进行前缀调整，以在任何少样本摘要任务中表现出色。同时，为了更好地评估少样本摘要器，根据多样性和鲁棒性原则，我们组装并发布了一个名为SummZoo的新基准。它包括8个摘要任务，每个任务有多个少样本样本集，涵盖了多种领域。实验结果和分析表明，在自动和人工评估下，UniSumm在SummZoo的所有子任务中均大幅优于强基线模型。",
    "tldr": "该论文提出了UniSumm统一的少样本摘要模型，可以通过前缀调整应对任何少样本摘要任务，同时，他们也发布了一个新的基准SummZoo，其由8个摘要任务组成，每个任务都涵盖了多个少样本样本集，以此更好地评估少样本摘要器。",
    "en_tdlr": "This paper proposes a unified few-shot summarization model called UniSumm, which can be adjusted to excel at any few-shot summarization tasks, and introduces a new benchmark called SummZoo consisting of 8 summarization tasks with multiple sets of few-shot samples for each task, which lead to improved evaluation of few-shot summarizers."
}