{
    "title": "Entry Dependent Expert Selection in Distributed Gaussian Processes Using Multilabel Classification. (arXiv:2211.09940v2 [cs.LG] UPDATED)",
    "abstract": "By distributing the training process, local approximation reduces the cost of the standard Gaussian Process. An ensemble technique combines local predictions from Gaussian experts trained on different partitions of the data. Ensemble methods aggregate models' predictions by assuming a perfect diversity of local predictors. Although it keeps the aggregation tractable, this assumption is often violated in practice. Even though ensemble methods provide consistent results by assuming dependencies between experts, they have a high computational cost, which is cubic in the number of experts involved. By implementing an expert selection strategy, the final aggregation step uses fewer experts and is more efficient. However, a selection approach that assigns a fixed set of experts to each new data point cannot encode the specific properties of each unique data point. This paper proposes a flexible expert selection approach based on the characteristics of entry data points. To this end, we inves",
    "link": "http://arxiv.org/abs/2211.09940",
    "context": "Title: Entry Dependent Expert Selection in Distributed Gaussian Processes Using Multilabel Classification. (arXiv:2211.09940v2 [cs.LG] UPDATED)\nAbstract: By distributing the training process, local approximation reduces the cost of the standard Gaussian Process. An ensemble technique combines local predictions from Gaussian experts trained on different partitions of the data. Ensemble methods aggregate models' predictions by assuming a perfect diversity of local predictors. Although it keeps the aggregation tractable, this assumption is often violated in practice. Even though ensemble methods provide consistent results by assuming dependencies between experts, they have a high computational cost, which is cubic in the number of experts involved. By implementing an expert selection strategy, the final aggregation step uses fewer experts and is more efficient. However, a selection approach that assigns a fixed set of experts to each new data point cannot encode the specific properties of each unique data point. This paper proposes a flexible expert selection approach based on the characteristics of entry data points. To this end, we inves",
    "path": "papers/22/11/2211.09940.json",
    "total_tokens": 842,
    "translated_title": "使用多标签分类的分布式高斯过程中的基于输入的专家选择",
    "translated_abstract": "通过分布式训练过程，局部近似降低了标准高斯过程的成本。集成技术将不同数据分区上训练的高斯专家的局部预测进行组合。集成方法通过假设局部预测器之间的完美差异性来聚合模型的预测结果。虽然它保持了可处理的聚合性质，但这个假设在实践中经常被违反。尽管集成方法通过假设专家之间存在依赖关系提供了一致的结果，但其计算成本很高，与所涉及的专家数量成立方关联。通过实施一种专家选择策略，最终的聚合步骤使用较少的专家并且更高效。然而，将一个固定的专家集分配给每个新数据点的选择方法不能编码每个唯一数据点的特定属性。本文提出了一种基于入口数据点特征的灵活的专家选择方法。为此，我们研究了",
    "tldr": "本文提出了一种基于输入数据点特征的灵活的专家选择方法，以减少在分布式高斯过程中的专家数目并提高效率。",
    "en_tdlr": "This paper proposes a flexible expert selection approach based on the characteristics of input data points to reduce the number of experts and improve efficiency in distributed Gaussian processes."
}