{
    "title": "A Survey of Knowledge Enhanced Pre-trained Language Models. (arXiv:2211.05994v4 [cs.CL] UPDATED)",
    "abstract": "Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, kn",
    "link": "http://arxiv.org/abs/2211.05994",
    "context": "Title: A Survey of Knowledge Enhanced Pre-trained Language Models. (arXiv:2211.05994v4 [cs.CL] UPDATED)\nAbstract: Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, kn",
    "path": "papers/22/11/2211.05994.json",
    "total_tokens": 986,
    "translated_title": "知识增强的预训练语言模型综述",
    "translated_abstract": "预训练语言模型(PLMs)通过自监督学习方法在大规模文本语料上进行训练，在自然语言处理(NLP)的各种任务中取得了令人期待的性能。然而，尽管具有大量参数的PLMs可以在精调阶段有效地获得从大规模训练文本中学到的丰富知识，并对下游任务产生好处，但它们仍然存在一些限制，如缺乏外部知识导致推理能力较差。研究人员致力于将知识融入PLMs以解决这些问题。本文综述了知识增强的预训练语言模型(KE-PLMs)，以提供对这一蓬勃发展领域的清晰认识。我们分别介绍了适用于自然语言理解(NLU)和自然语言生成(NLG)的合适分类法，以突出NLP的这两个主要任务。对于NLU，我们将知识类型划分为四个类别：语言知识，文本知识，知识库知识和常识知识。",
    "tldr": "本文综述了知识增强的预训练语言模型(KE-PLMs)，这是一种将知识融入预训练语言模型的方法，以解决模型推理能力不足的问题。该综述提供了对该领域的全面了解，并介绍了适用于自然语言理解和自然语言生成的分类法。",
    "en_tdlr": "This paper presents a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs), which is a method of incorporating knowledge into pre-trained language models to address the issue of limited reasoning ability. The review provides a clear insight into this thriving field, and introduces appropriate taxonomies for both Natural Language Understanding (NLU) and Natural Language Generation (NLG)."
}