{
    "title": "Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks. (arXiv:2211.00692v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we study the OOD generalization of neural algorithmic reasoning tasks, where the goal is to learn an algorithm (e.g., sorting, breadth-first search, and depth-first search) from input-output pairs using deep neural networks. First, we argue that OOD generalization in this setting is significantly different than common OOD settings. For example, some phenomena in OOD generalization of image classifications such as \\emph{accuracy on the line} are not observed here, and techniques such as data augmentation methods do not help as assumptions underlying many augmentation techniques are often violated. Second, we analyze the main challenges (e.g., input distribution shift, non-representative data generation, and uninformative validation metrics) of the current leading benchmark, i.e., CLRS \\citep{deepmind2021clrs}, which contains 30 algorithmic reasoning tasks. We propose several solutions, including a simple-yet-effective fix to the input distribution shift and improved data ",
    "link": "http://arxiv.org/abs/2211.00692",
    "context": "Title: Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks. (arXiv:2211.00692v2 [cs.LG] UPDATED)\nAbstract: In this paper, we study the OOD generalization of neural algorithmic reasoning tasks, where the goal is to learn an algorithm (e.g., sorting, breadth-first search, and depth-first search) from input-output pairs using deep neural networks. First, we argue that OOD generalization in this setting is significantly different than common OOD settings. For example, some phenomena in OOD generalization of image classifications such as \\emph{accuracy on the line} are not observed here, and techniques such as data augmentation methods do not help as assumptions underlying many augmentation techniques are often violated. Second, we analyze the main challenges (e.g., input distribution shift, non-representative data generation, and uninformative validation metrics) of the current leading benchmark, i.e., CLRS \\citep{deepmind2021clrs}, which contains 30 algorithmic reasoning tasks. We propose several solutions, including a simple-yet-effective fix to the input distribution shift and improved data ",
    "path": "papers/22/11/2211.00692.json",
    "total_tokens": 878,
    "translated_title": "为改进神经算法推理任务的非分布式泛化能力而努力",
    "translated_abstract": "本文研究了神经算法推理任务的非分布式泛化能力，其中目标是使用深度神经网络从输入输出对中学习算法（例如排序、广度优先搜索和深度优先搜索）。首先，我们认为在这种情况下，非分布式泛化与常见的非分布式设置显着不同。例如，通常在图像分类的非分布式泛化中观察到的一些现象，例如\\emph{直线上的准确度}在这里没有观察到，而数据增强方法等技术不起作用，因为许多增强技术的假设通常被违反。其次，我们分析了当前领先的基准测试CLRS\\citep{deepmind2021clrs}的主要挑战（例如，输入分布偏移、非代表性数据生成和无信息的验证指标），该测试包含30个算法推理任务。我们提出了几个解决方案，包括一个简单而有效的修复输入分布偏移和改进数据的方法。",
    "tldr": "本文研究神经算法推理任务的非分布式泛化能力，分析当前基准测试的挑战并提出了解决方案。",
    "en_tdlr": "This paper studies the out-of-distribution generalization of neural algorithmic reasoning tasks, analyzes the challenges of the current benchmark, and proposes several solutions."
}