{
    "title": "Equivariance with Learned Canonicalization Functions. (arXiv:2211.06489v3 [cs.LG] UPDATED)",
    "abstract": "Symmetry-based neural networks often constrain the architecture in order to achieve invariance or equivariance to a group of transformations. In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data. These canonicalization functions can readily be plugged into non-equivariant backbone architectures. We offer explicit ways to implement them for some groups of interest. We show that this approach enjoys universality while providing interpretable insights. Our main hypothesis, supported by our empirical results, is that learning a small neural network to perform canonicalization is better than using predefined heuristics. Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including image classification, $N$-body dynamics prediction, point cloud classification and part segmentation, while being fas",
    "link": "http://arxiv.org/abs/2211.06489",
    "context": "Title: Equivariance with Learned Canonicalization Functions. (arXiv:2211.06489v3 [cs.LG] UPDATED)\nAbstract: Symmetry-based neural networks often constrain the architecture in order to achieve invariance or equivariance to a group of transformations. In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data. These canonicalization functions can readily be plugged into non-equivariant backbone architectures. We offer explicit ways to implement them for some groups of interest. We show that this approach enjoys universality while providing interpretable insights. Our main hypothesis, supported by our empirical results, is that learning a small neural network to perform canonicalization is better than using predefined heuristics. Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including image classification, $N$-body dynamics prediction, point cloud classification and part segmentation, while being fas",
    "path": "papers/22/11/2211.06489.json",
    "total_tokens": 850,
    "translated_title": "使用学习的规范化函数实现等变性",
    "translated_abstract": "基于对称性的神经网络通常通过限制架构来实现对一组变换的不变性或等变性。在本文中，我们提出了一种替代方法，通过学习产生数据的规范表示来避免这种架构约束。这些规范化函数可以直接插入非等变主干架构中。我们提供了一些感兴趣的群组的明确实现方法。我们表明，这种方法具有普适性，同时提供可解释的洞察力。我们的主要假设得到了我们的实证结果的支持，即学习一个小的神经网络来执行规范化优于使用预定义的启发式。我们的实验结果表明，学习规范化函数在许多任务中，包括图像分类、$N$体动力学预测、点云分类和部分分割等学习等变函数的现有技术相比具有竞争力，同时速度更快。",
    "tldr": "本文提出了一种使用学习的规范化函数来实现等变性的方法，避免了对神经网络架构的限制。实验结果表明，学习规范化函数可以在多个任务中与现有技术相比具有竞争力，并且速度更快。"
}