{
    "title": "Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory. (arXiv:2211.10586v2 [cs.CV] UPDATED)",
    "abstract": "Dataset distillation methods aim to compress a large dataset into a small set of synthetic samples, such that when being trained on, competitive performances can be achieved compared to regular training on the entire dataset. Among recently proposed methods, Matching Training Trajectories (MTT) achieves state-of-the-art performance on CIFAR-10/100, while having difficulty scaling to ImageNet-1k dataset due to the large memory requirement when performing unrolled gradient computation through back-propagation. Surprisingly, we show that there exists a procedure to exactly calculate the gradient of the trajectory matching loss with constant GPU memory requirement (irrelevant to the number of unrolled steps). With this finding, the proposed memory-efficient trajectory matching method can easily scale to ImageNet-1K with 6x memory reduction while introducing only around 2% runtime overhead than original MTT. Further, we find that assigning soft labels for synthetic images is crucial for the",
    "link": "http://arxiv.org/abs/2211.10586",
    "context": "Title: Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory. (arXiv:2211.10586v2 [cs.CV] UPDATED)\nAbstract: Dataset distillation methods aim to compress a large dataset into a small set of synthetic samples, such that when being trained on, competitive performances can be achieved compared to regular training on the entire dataset. Among recently proposed methods, Matching Training Trajectories (MTT) achieves state-of-the-art performance on CIFAR-10/100, while having difficulty scaling to ImageNet-1k dataset due to the large memory requirement when performing unrolled gradient computation through back-propagation. Surprisingly, we show that there exists a procedure to exactly calculate the gradient of the trajectory matching loss with constant GPU memory requirement (irrelevant to the number of unrolled steps). With this finding, the proposed memory-efficient trajectory matching method can easily scale to ImageNet-1K with 6x memory reduction while introducing only around 2% runtime overhead than original MTT. Further, we find that assigning soft labels for synthetic images is crucial for the",
    "path": "papers/22/11/2211.10586.json",
    "total_tokens": 958,
    "translated_title": "利用恒定内存将数据集精简扩展到ImageNet-1K",
    "translated_abstract": "数据集精简方法旨在将大型数据集压缩成一小组合成样本，使得在训练时，与在整个数据集上进行常规训练相比，可以获得竞争性的性能。在最近提出的方法中，匹配训练轨迹（MTT）在CIFAR-10/100上实现了最先进的性能，但由于在反向传播过程中执行展开梯度计算时需要大量内存，因此很难扩展到ImageNet-1k数据集。令人惊讶的是，我们发现存在一种方法，可以使用恒定的GPU内存需求（与展开步骤的数量无关）精确计算轨迹匹配损失函数的梯度。有了这一发现，所提出的内存高效的轨迹匹配方法只需要比原始MTT多约2％的运行时开销，即可轻松扩展到具有6倍内存缩减的ImageNet-1K。此外，我们发现为合成图像分配软标签对于实现良好的性能至关重要。",
    "tldr": "本文提出了一种利用恒定内存需求扩展数据集精简的方法，可将Matching Training Trajectories（MTT）应用于ImageNet-1K数据集，达到6倍的内存降低，同时增加了约2%的运行时开销。同时也发现，为合成图像分配软标签对于实现良好的性能非常重要。",
    "en_tdlr": "This paper proposes a memory-efficient trajectory matching method to scale up dataset distillation to ImageNet-1K, achieving 6x memory reduction with only around 2% runtime overhead than original Matching Training Trajectories (MTT). Soft labels assignment to synthetic images is crucial for achieving good performance."
}