{
    "title": "AugOp: Inject Transformation into Neural Operator. (arXiv:2211.12514v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we propose a simple and general approach to augment regular convolution operator by injecting extra group-wise transformation during training and recover it during inference. Extra transformation is carefully selected to ensure it can be merged with regular convolution in each group and will not change the topological structure of regular convolution during inference. Compared with regular convolution operator, our approach (AugConv) can introduce larger learning capacity to improve model performance during training but will not increase extra computational overhead for model deployment. Based on ResNet, we utilize AugConv to build convolutional neural networks named AugResNet. Result on image classification dataset Cifar-10 shows that AugResNet outperforms its baseline in terms of model performance.",
    "link": "http://arxiv.org/abs/2211.12514",
    "context": "Title: AugOp: Inject Transformation into Neural Operator. (arXiv:2211.12514v2 [cs.LG] UPDATED)\nAbstract: In this paper, we propose a simple and general approach to augment regular convolution operator by injecting extra group-wise transformation during training and recover it during inference. Extra transformation is carefully selected to ensure it can be merged with regular convolution in each group and will not change the topological structure of regular convolution during inference. Compared with regular convolution operator, our approach (AugConv) can introduce larger learning capacity to improve model performance during training but will not increase extra computational overhead for model deployment. Based on ResNet, we utilize AugConv to build convolutional neural networks named AugResNet. Result on image classification dataset Cifar-10 shows that AugResNet outperforms its baseline in terms of model performance.",
    "path": "papers/22/11/2211.12514.json",
    "total_tokens": 782,
    "translated_title": "AugOp：将变换注入神经算子",
    "translated_abstract": "本文提出了一种简单而通用的方法，通过在训练期间注入额外的分组变换来增强常规卷积算子，并在推理期间恢复它。精心选择的额外变换确保其可以与每个组中的常规卷积合并，并且在推理期间不会改变常规卷积的拓扑结构。与常规卷积算子相比，我们的方法（AugConv）可以引入更大的学习能力，以改善模型在训练期间的性能，但不会增加模型部署的额外计算开销。基于 ResNet，我们利用 AugConv 构建卷积神经网络，称为 AugResNet。图像分类数据集 Cifar-10 上的结果显示，AugResNet 在模型性能方面优于其基线。",
    "tldr": "本文提出了一种基于注入额外的分组变换来增强常规卷积算子的方法，名为 AugConv，可以引入更大的学习能力，提升模型性能，而不会增加部署时的额外计算开销。",
    "en_tdlr": "This paper proposes a method named AugConv, which injects extra group-wise transformation to augment regular convolution operator to introduce larger learning capacity and improve model performance without increasing extra computational overhead for model deployment. Results show that AugResNet outperforms its baseline on Cifar-10 dataset."
}