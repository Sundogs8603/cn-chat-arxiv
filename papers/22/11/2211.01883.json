{
    "title": "Faster Adaptive Momentum-Based Federated Methods for Distributed Composition Optimization. (arXiv:2211.01883v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning is a popular distributed learning paradigm in machine learning. Meanwhile, composition optimization is an effective hierarchical learning model, which appears in many machine learning applications such as meta learning and robust learning. More recently, although a few federated composition optimization algorithms have been proposed, they still suffer from high sample and communication complexities. In the paper, thus, we propose a class of faster federated compositional optimization algorithms (i.e., MFCGD and AdaMFCGD) to solve the nonconvex distributed composition problems, which builds on the momentum-based variance reduced and local-SGD techniques. In particular, our adaptive algorithm (i.e., AdaMFCGD) uses a unified adaptive matrix to flexibly incorporate various adaptive learning rates. Moreover, we provide a solid theoretical analysis for our algorithms under non-i.i.d. setting, and prove our algorithms obtain a lower sample and communication complexities sim",
    "link": "http://arxiv.org/abs/2211.01883",
    "context": "Title: Faster Adaptive Momentum-Based Federated Methods for Distributed Composition Optimization. (arXiv:2211.01883v2 [cs.LG] UPDATED)\nAbstract: Federated Learning is a popular distributed learning paradigm in machine learning. Meanwhile, composition optimization is an effective hierarchical learning model, which appears in many machine learning applications such as meta learning and robust learning. More recently, although a few federated composition optimization algorithms have been proposed, they still suffer from high sample and communication complexities. In the paper, thus, we propose a class of faster federated compositional optimization algorithms (i.e., MFCGD and AdaMFCGD) to solve the nonconvex distributed composition problems, which builds on the momentum-based variance reduced and local-SGD techniques. In particular, our adaptive algorithm (i.e., AdaMFCGD) uses a unified adaptive matrix to flexibly incorporate various adaptive learning rates. Moreover, we provide a solid theoretical analysis for our algorithms under non-i.i.d. setting, and prove our algorithms obtain a lower sample and communication complexities sim",
    "path": "papers/22/11/2211.01883.json",
    "total_tokens": 936,
    "translated_title": "适应性动量的快速联邦学习方法用于分布式组合优化",
    "translated_abstract": "联邦学习是机器学习中一种流行的分布式学习范式。组合优化是一种有效的分层学习模型，出现在许多机器学习应用中，如元学习和鲁棒学习。最近，虽然已经提出了一些联邦组合优化算法，但它们仍然存在高采样和通信复杂度的问题。因此，本文提出了一类更快的联邦组合优化算法（即MFCGD和AdaMFCGD），用于解决非凸分布式组合问题，其基于动量的方差缩减和本地SGD技术。特别地，我们的自适应算法（即AdaMFCGD）使用统一的自适应矩阵，灵活地结合各种自适应学习率。此外，我们对我们的算法在非独立同分布设置下进行了坚实的理论分析，并证明了我们的算法获得更低的采样和通信复杂度。",
    "tldr": "本文提出了一类基于动量的方差缩减和本地SGD技术的更快的联邦组合优化算法，用于解决非凸分布式组合问题，并使用自适应矩阵灵活地结合各种自适应学习率。",
    "en_tdlr": "This paper proposes a faster federated compositional optimization algorithm based on momentum-based variance reduced and local-SGD techniques, called MFCGD and AdaMFCGD, which solves nonconvex distributed composition problems and flexibly incorporates various adaptive learning rates using an adaptive matrix. A solid theoretical analysis is provided under non-i.i.d. setting, and our algorithms achieve lower sample and communication complexities."
}