{
    "title": "Inapplicable Actions Learning for Knowledge Transfer in Reinforcement Learning. (arXiv:2211.15589v3 [cs.LG] UPDATED)",
    "abstract": "Reinforcement Learning (RL) algorithms are known to scale poorly to environments with many available actions, requiring numerous samples to learn an optimal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as $\\textit{inapplicable actions}$ (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. While this technique has been formalized for quite some time within the Automated Planning community with the concept of precondition in the STRIPS language, RL algorithms have never formally taken advantage of this information to prune the search space to explore. This is typically done in an ",
    "link": "http://arxiv.org/abs/2211.15589",
    "context": "Title: Inapplicable Actions Learning for Knowledge Transfer in Reinforcement Learning. (arXiv:2211.15589v3 [cs.LG] UPDATED)\nAbstract: Reinforcement Learning (RL) algorithms are known to scale poorly to environments with many available actions, requiring numerous samples to learn an optimal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as $\\textit{inapplicable actions}$ (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. While this technique has been formalized for quite some time within the Automated Planning community with the concept of precondition in the STRIPS language, RL algorithms have never formally taken advantage of this information to prune the search space to explore. This is typically done in an ",
    "path": "papers/22/11/2211.15589.json",
    "total_tokens": 1002,
    "translated_title": "强化学习中的知识转移——不适用动作学习",
    "translated_abstract": "强化学习算法在具有许多可能动作的环境中很难扩展，需要大量的样本才能学习到最优策略。传统的方法是在每个可能的状态下考虑相同的固定动作空间，这意味着智能体必须理解、同时学习如何最大化其奖励，忽略无关的动作，比如“不适用动作”（即在给定状态下执行时不会影响环境的动作）。了解这些信息可以通过屏蔽与寻找最优策略相关的动作，从策略分布中去探索无关的动作，从而帮助降低强化学习算法的样本复杂度。本文提出了一种新的方法，在学习阶段通过一种新颖的奖励设计方法学习屏蔽不适用的动作，智能体可以学习预测一个动作是否适用，并根据其预测获得奖励。实验结果表明，相比于传统的RL方法，我们的方法可以使用较少的样本学习到最优策略。",
    "tldr": "本文提出了一种新的强化学习方法，在学习过程中通过奖励设计让智能体学习屏蔽无关的动作，从而降低了样本数量，并获得了比传统RL方法更好的最优策略。"
}