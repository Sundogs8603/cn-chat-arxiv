{
    "title": "Offline Policy Evaluation and Optimization under Confounding. (arXiv:2211.16583v3 [stat.ML] UPDATED)",
    "abstract": "Evaluating and optimizing policies in the presence of unobserved confounders is a problem of growing interest in offline reinforcement learning. Using conventional methods for offline RL in the presence of confounding can not only lead to poor decisions and poor policies, but can also have disastrous effects in critical applications such as healthcare and education. We map out the landscape of offline policy evaluation for confounded MDPs, distinguishing assumptions on confounding based on their time-evolution and effect on the data-collection policies. We determine when consistent value estimates are not achievable, providing and discussing algorithms to estimate lower bounds with guarantees in those cases. When consistent estimates are achievable, we provide sample complexity guarantees. We also present new algorithms for offline policy improvement and prove local convergence guarantees. Finally, we experimentally evaluate our algorithms on gridworld and a simulated healthcare settin",
    "link": "http://arxiv.org/abs/2211.16583",
    "context": "Title: Offline Policy Evaluation and Optimization under Confounding. (arXiv:2211.16583v3 [stat.ML] UPDATED)\nAbstract: Evaluating and optimizing policies in the presence of unobserved confounders is a problem of growing interest in offline reinforcement learning. Using conventional methods for offline RL in the presence of confounding can not only lead to poor decisions and poor policies, but can also have disastrous effects in critical applications such as healthcare and education. We map out the landscape of offline policy evaluation for confounded MDPs, distinguishing assumptions on confounding based on their time-evolution and effect on the data-collection policies. We determine when consistent value estimates are not achievable, providing and discussing algorithms to estimate lower bounds with guarantees in those cases. When consistent estimates are achievable, we provide sample complexity guarantees. We also present new algorithms for offline policy improvement and prove local convergence guarantees. Finally, we experimentally evaluate our algorithms on gridworld and a simulated healthcare settin",
    "path": "papers/22/11/2211.16583.json",
    "total_tokens": 934,
    "translated_title": "在混淆下的离线策略评估和优化。",
    "translated_abstract": "在离线强化学习中，评估和优化策略在存在未观察到的混淆变量时是一个备受关注的问题。使用传统的离线强化学习方法来处理混淆问题不仅可能导致糟糕的决策和策略，而且在关键应用领域如医疗和教育中可能会产生灾难性的影响。我们勾勒了混淆的 MDP 离线策略评估的面貌，并根据混淆对数据收集策略的时间演变和影响来区分混淆的假设。在一些情况下，我们确定了一些无法获得一致价值估计的情况，并提供和讨论了计算具有保证的下限的算法。当一致的估计可行时，我们提供了样本复杂度的保证。我们还提出了新的离线策略改进算法，并证明了局部收敛的保证。最后，我们在格子世界和模拟医疗场景中对算法进行了实验评估。",
    "tldr": "该论文致力于解决离线强化学习中混淆变量导致策略评估和优化存在挑战的问题，包括无法获得一致价值估计和样本复杂度的保证，作者提出了具有保证的下限算法和局部收敛的改进算法。",
    "en_tdlr": "This paper aims to address the challenges of policy evaluation and optimization in offline reinforcement learning with confounding variables, including the inability to obtain consistent value estimates and sample complexity guarantees. The authors propose guaranteed lower bound algorithms and locally convergent improvement algorithms."
}