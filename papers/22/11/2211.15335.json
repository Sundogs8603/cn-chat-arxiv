{
    "title": "You Can Have Better Graph Neural Networks by Not Training Weights at All: Finding Untrained GNNs Tickets",
    "abstract": "Recent works have impressively demonstrated that there exists a subnetwork in randomly initialized convolutional neural networks (CNNs) that can match the performance of the fully trained dense networks at initialization, without any optimization of the weights of the network (i.e., untrained networks). However, the presence of such untrained subnetworks in graph neural networks (GNNs) still remains mysterious. In this paper we carry out the first-of-its-kind exploration of discovering matching untrained GNNs. With sparsity as the core tool, we can find \\textit{untrained sparse subnetworks} at the initialization, that can match the performance of \\textit{fully trained dense} GNNs. Besides this already encouraging finding of comparable performance, we show that the found untrained subnetworks can substantially mitigate the GNN over-smoothing problem, hence becoming a powerful tool to enable deeper GNNs without bells and whistles. We also observe that such sparse untrained subnetworks ha",
    "link": "https://arxiv.org/abs/2211.15335",
    "context": "Title: You Can Have Better Graph Neural Networks by Not Training Weights at All: Finding Untrained GNNs Tickets\nAbstract: Recent works have impressively demonstrated that there exists a subnetwork in randomly initialized convolutional neural networks (CNNs) that can match the performance of the fully trained dense networks at initialization, without any optimization of the weights of the network (i.e., untrained networks). However, the presence of such untrained subnetworks in graph neural networks (GNNs) still remains mysterious. In this paper we carry out the first-of-its-kind exploration of discovering matching untrained GNNs. With sparsity as the core tool, we can find \\textit{untrained sparse subnetworks} at the initialization, that can match the performance of \\textit{fully trained dense} GNNs. Besides this already encouraging finding of comparable performance, we show that the found untrained subnetworks can substantially mitigate the GNN over-smoothing problem, hence becoming a powerful tool to enable deeper GNNs without bells and whistles. We also observe that such sparse untrained subnetworks ha",
    "path": "papers/22/11/2211.15335.json",
    "total_tokens": 944,
    "translated_title": "不训练权重就能拥有更高效的图神经网络：发现未训练的GNN表现",
    "translated_abstract": "最近的研究令人印象深刻地证明，在随机初始化的卷积神经网络（CNN）中存在一个子网络，它在初始化时无需对网络的权重进行任何优化（即未训练网络）就能达到完全训练的稠密网络的性能。然而，在图神经网络（GNNs）中是否存在这样的未训练子网络仍然是个谜。在这篇论文中，我们首次探索了发现匹配的未训练GNNs的方法。通过稀疏性作为核心工具，我们可以在初始化时找到与完全训练的稠密GNNs性能匹配的未训练稀疏子网络。除了这个令人鼓舞的结果，我们还展示了发现的未训练子网络可以显著减轻GNNs过度平滑的问题，从而成为实现更深层GNNs而无需繁琐的强大工具。我们还观察到这样的稀疏未训练子网络具有较高的广泛适用性。",
    "tldr": "通过发现未经训练的稀疏子网络，我们可以在初始化时实现与完全训练的GNNs相媲美的性能，同时有效缓解GNNs过度平滑问题，为实现更深层GNNs提供了强大的工具。"
}