{
    "title": "Prototypical quadruplet for few-shot class incremental learning. (arXiv:2211.02947v3 [cs.CV] UPDATED)",
    "abstract": "Scarcity of data and incremental learning of new tasks pose two major bottlenecks for many modern computer vision algorithms. The phenomenon of catastrophic forgetting, i.e., the model's inability to classify previously learned data after training with new batches of data, is a major challenge. Conventional methods address catastrophic forgetting while compromising the current session's training. Generative replay-based approaches, such as generative adversarial networks (GANs), have been proposed to mitigate catastrophic forgetting, but training GANs with few samples may lead to instability. To address these challenges, we propose a novel method that improves classification robustness by identifying a better embedding space using an improved contrasting loss. Our approach retains previously acquired knowledge in the embedding space, even when trained with new classes, by updating previous session class prototypes to represent the true class mean, which is crucial for our nearest class",
    "link": "http://arxiv.org/abs/2211.02947",
    "context": "Title: Prototypical quadruplet for few-shot class incremental learning. (arXiv:2211.02947v3 [cs.CV] UPDATED)\nAbstract: Scarcity of data and incremental learning of new tasks pose two major bottlenecks for many modern computer vision algorithms. The phenomenon of catastrophic forgetting, i.e., the model's inability to classify previously learned data after training with new batches of data, is a major challenge. Conventional methods address catastrophic forgetting while compromising the current session's training. Generative replay-based approaches, such as generative adversarial networks (GANs), have been proposed to mitigate catastrophic forgetting, but training GANs with few samples may lead to instability. To address these challenges, we propose a novel method that improves classification robustness by identifying a better embedding space using an improved contrasting loss. Our approach retains previously acquired knowledge in the embedding space, even when trained with new classes, by updating previous session class prototypes to represent the true class mean, which is crucial for our nearest class",
    "path": "papers/22/11/2211.02947.json",
    "total_tokens": 905,
    "translated_title": "少样本类别增量学习中的原型四元组",
    "translated_abstract": "在许多现代计算机视觉算法中，数据稀缺和新任务的增量学习构成了两个主要瓶颈。灾难性遗忘现象，即模型在使用新批次数据训练后无法对先前学习的数据进行分类，是一个重大挑战。传统方法解决灾难性遗忘的同时会牺牲当前会话的训练。生成复盘方法，如生成敌对网络（GAN），已被提出以减轻灾难性遗忘，但使用少量样本训练GAN可能导致不稳定性。为解决这些挑战，我们提出了一种新方法，通过使用改进的对比损失来识别更好的嵌入空间，从而提高分类鲁棒性。我们的方法通过更新先前会话类原型以表示真正的类平均值，在嵌入空间中保留先前已获得的知识，即使训练新类时也是如此，这对于我们的最近类别分类器至关重要。",
    "tldr": "本文提出了一种解决灾难性遗忘的少样本类别增量学习方法，通过更新先前会话类原型以表示真正的类平均值，在嵌入空间中保留先前已获得的知识，即使训练新类时也是如此。"
}