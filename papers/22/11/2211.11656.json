{
    "title": "Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization. (arXiv:2211.11656v2 [cs.LG] UPDATED)",
    "abstract": "The aim of Machine Unlearning (MU) is to provide theoretical guarantees on the removal of the contribution of a given data point from a training procedure. Federated Unlearning (FU) consists in extending MU to unlearn a given client's contribution from a federated training routine. Current FU approaches are generally not scalable, and do not come with sound theoretical quantification of the effectiveness of unlearning. In this work we present Informed Federated Unlearning (IFU), a novel efficient and quantifiable FU approach. Upon unlearning request from a given client, IFU identifies the optimal FL iteration from which FL has to be reinitialized, with unlearning guarantees obtained through a randomized perturbation mechanism. The theory of IFU is also extended to account for sequential unlearning requests. Experimental results on different tasks and dataset show that IFU leads to more efficient unlearning procedures as compared to basic re-training and state-of-the-art FU approaches.",
    "link": "http://arxiv.org/abs/2211.11656",
    "context": "Title: Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization. (arXiv:2211.11656v2 [cs.LG] UPDATED)\nAbstract: The aim of Machine Unlearning (MU) is to provide theoretical guarantees on the removal of the contribution of a given data point from a training procedure. Federated Unlearning (FU) consists in extending MU to unlearn a given client's contribution from a federated training routine. Current FU approaches are generally not scalable, and do not come with sound theoretical quantification of the effectiveness of unlearning. In this work we present Informed Federated Unlearning (IFU), a novel efficient and quantifiable FU approach. Upon unlearning request from a given client, IFU identifies the optimal FL iteration from which FL has to be reinitialized, with unlearning guarantees obtained through a randomized perturbation mechanism. The theory of IFU is also extended to account for sequential unlearning requests. Experimental results on different tasks and dataset show that IFU leads to more efficient unlearning procedures as compared to basic re-training and state-of-the-art FU approaches.",
    "path": "papers/22/11/2211.11656.json",
    "total_tokens": 880,
    "translated_title": "顺序知情联合消除：联邦优化中高效且可证明的客户端消除",
    "translated_abstract": "机器消除（MU）旨在提供有关从训练过程中删除给定数据点的贡献的理论保证。联邦消除（FU）是将MU扩展到从联合训练过程中消除给定客户端的贡献。当前的FU方法通常不具有可扩展性，并且没有对消除效果的有效性进行合理的理论量化。在本文中，我们提出了知情联合消除(IFU)，这是一种新颖的高效且可量化的FU方法。在接收到给定客户端的消除请求后，IFU通过随机扰动机制确定了重新初始化FL所需的最佳FL迭代，可以获得消除保证。IFU的理论也可以扩展以解决顺序消除请求。不同任务和数据集上的实验结果表明，与基本重新训练和最先进的FU方法相比，IFU可以实现更高效的消除过程。",
    "tldr": "本文提出一种名为知情联合消除（IFU）的新颖联邦优化方法，可实现有效且可量化的客户端消除请求，实验结果表明其效率较基本方法和最先进的FU方法更高。",
    "en_tdlr": "This paper proposes a novel federated optimization approach named Informed Federated Unlearning (IFU) that enables effective and quantifiable client unlearning requests, with experimental results showing higher efficiency compared to basic methods and state-of-art FU approaches."
}