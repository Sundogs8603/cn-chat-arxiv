{
    "title": "When is Realizability Sufficient for Off-Policy Reinforcement Learning?. (arXiv:2211.05311v2 [cs.LG] UPDATED)",
    "abstract": "Model-free algorithms for reinforcement learning typically require a condition called Bellman completeness in order to successfully operate off-policy with function approximation, unless additional conditions are met. However, Bellman completeness is a requirement that is much stronger than realizability and that is deemed to be too strong to hold in practice. In this work, we relax this structural assumption and analyze the statistical complexity of off-policy reinforcement learning when only realizability holds for the prescribed function class.  We establish finite-sample guarantees for off-policy reinforcement learning that are free of the approximation error term known as inherent Bellman error, and that depend on the interplay of three factors. The first two are well known: they are the metric entropy of the function class and the concentrability coefficient that represents the cost of learning off-policy. The third factor is new, and it measures the violation of Bellman complete",
    "link": "http://arxiv.org/abs/2211.05311",
    "context": "Title: When is Realizability Sufficient for Off-Policy Reinforcement Learning?. (arXiv:2211.05311v2 [cs.LG] UPDATED)\nAbstract: Model-free algorithms for reinforcement learning typically require a condition called Bellman completeness in order to successfully operate off-policy with function approximation, unless additional conditions are met. However, Bellman completeness is a requirement that is much stronger than realizability and that is deemed to be too strong to hold in practice. In this work, we relax this structural assumption and analyze the statistical complexity of off-policy reinforcement learning when only realizability holds for the prescribed function class.  We establish finite-sample guarantees for off-policy reinforcement learning that are free of the approximation error term known as inherent Bellman error, and that depend on the interplay of three factors. The first two are well known: they are the metric entropy of the function class and the concentrability coefficient that represents the cost of learning off-policy. The third factor is new, and it measures the violation of Bellman complete",
    "path": "papers/22/11/2211.05311.json",
    "total_tokens": 1115,
    "translated_title": "基于策略外学习的可实现性何时足够？",
    "translated_abstract": "对于强化学习而言，基于模型的算法通常需要满足贝尔曼完整性条件，以便在函数逼近的情况下成功地进行基于策略外的操作。但贝尔曼完整性是一种比实际要求更强的条件，难以在实践中得到满足。本文放松了这一条件，研究了仅在所规定的函数类具有可实现性时，基于策略外强化学习的统计复杂性，提出了基于策略外强化学习的有限样本保证，不受逼近误差影响，取决于三个因素的相互作用。",
    "tldr": "本文研究了基于模型的算法对于满足贝尔曼完整性条件的要求，提出了新的基于策略外强化学习的有限样本保证。"
}