{
    "title": "ZigZag: Universal Sampling-free Uncertainty Estimation Through Two-Step Inference. (arXiv:2211.11435v2 [cs.LG] UPDATED)",
    "abstract": "Whereas the ability of deep networks to produce useful predictions has been amply demonstrated, estimating the reliability of these predictions remains challenging. Sampling approaches such as MC-Dropout and Deep Ensembles have emerged as the most popular ones for this purpose. Unfortunately, they require many forward passes at inference time, which slows them down. Sampling-free approaches can be faster but suffer from other drawbacks, such as lower reliability of uncertainty estimates, difficulty of use, and limited applicability to different types of tasks and data.  In this work, we introduce a sampling-free approach that is generic and easy to deploy, while producing reliable uncertainty estimates on par with state-of-the-art methods at a significantly lower computational cost. It is predicated on training the network to produce the same output with and without additional information about it. At inference time, when no prior information is given, we use the network's own predicti",
    "link": "http://arxiv.org/abs/2211.11435",
    "context": "Title: ZigZag: Universal Sampling-free Uncertainty Estimation Through Two-Step Inference. (arXiv:2211.11435v2 [cs.LG] UPDATED)\nAbstract: Whereas the ability of deep networks to produce useful predictions has been amply demonstrated, estimating the reliability of these predictions remains challenging. Sampling approaches such as MC-Dropout and Deep Ensembles have emerged as the most popular ones for this purpose. Unfortunately, they require many forward passes at inference time, which slows them down. Sampling-free approaches can be faster but suffer from other drawbacks, such as lower reliability of uncertainty estimates, difficulty of use, and limited applicability to different types of tasks and data.  In this work, we introduce a sampling-free approach that is generic and easy to deploy, while producing reliable uncertainty estimates on par with state-of-the-art methods at a significantly lower computational cost. It is predicated on training the network to produce the same output with and without additional information about it. At inference time, when no prior information is given, we use the network's own predicti",
    "path": "papers/22/11/2211.11435.json",
    "total_tokens": 906,
    "translated_title": "ZigZag: 通过两步推理实现的通用无采样不确定性估计",
    "translated_abstract": "尽管深度网络产生有用的预测的能力已经被充分证明，但是估计这些预测的可靠性仍然具有挑战性。诸如MC-Dropout和Deep Ensembles之类的采样方法已经成为最流行的用于此目的的方法。不幸的是，它们在推理时需要进行许多前向传递，这会减慢速度。无采样方法可能更快，但存在其他缺点，例如不确定性估计的可信度较低、使用困难以及适用于不同类型的任务和数据的能力有限。在这项工作中，我们介绍了一种通用且易于部署的无采样方法，它以明显较低的计算成本获得与最先进方法相媲美的可靠性估计。其基本原理是训练网络在有和没有额外信息的情况下产生相同的输出。在推理时，当没有提供先验信息时，我们使用网络的自身预测。",
    "tldr": "本研究提出了一种通用的无采样不确定性估计方法，通过训练网络在有和没有额外信息的情况下产生相同的输出，实现了与最先进方法相媲美的可靠性估计，同时显著降低了计算成本。",
    "en_tdlr": "This research proposes a universal sampling-free uncertainty estimation method, which trains the network to produce the same output with and without additional information, achieving reliable estimates comparable to state-of-the-art methods at a significantly lower computational cost."
}