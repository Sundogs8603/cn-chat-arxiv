{
    "title": "Holistic Evaluation of Language Models. (arXiv:2211.09110v2 [cs.CL] UPDATED)",
    "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze speci",
    "link": "http://arxiv.org/abs/2211.09110",
    "context": "Title: Holistic Evaluation of Language Models. (arXiv:2211.09110v2 [cs.CL] UPDATED)\nAbstract: Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze speci",
    "path": "papers/22/11/2211.09110.json",
    "total_tokens": 937,
    "translated_title": "语言模型的整体评估",
    "translated_abstract": "语言模型（LMs）正在成为几乎所有主要语言技术的基础，但它们的能力、限制和风险并不被很好地理解。我们提出了语言模型的整体评估（HELM），以提高语言模型的透明度。首先，我们对感兴趣的潜在场景（即用例）和度量（即期望）的广阔空间进行分类。然后，我们选择了一个宽泛的子集，基于覆盖范围和可行性，注意到了缺失或未充分代表的内容（例如，为被忽视的英语方言进行问答，用于可信度的度量）。其次，我们采用多度量方法：我们分别针对每个核心场景测量了准确度、校准度、鲁棒性、公平性、偏见、有毒性和效率这7个度量指标（在87.5%的时间内）。这确保了准确度以外的度量不会被忽视，并且权衡清晰。我们还进行了7个针对性评估，基于26个针对性场景，以分析特定场景下的性能。",
    "tldr": "我们提出了语言模型的整体评估（HELM），通过对潜在场景和度量进行分类并采用多度量方法，提高语言模型的透明度和可信度。",
    "en_tdlr": "We propose Holistic Evaluation of Language Models (HELM) to improve the transparency and trustworthiness of language models. By taxonomizing potential scenarios and adopting a multi-metric approach, we ensure that metrics beyond accuracy are considered and trade-offs are clearly identified."
}