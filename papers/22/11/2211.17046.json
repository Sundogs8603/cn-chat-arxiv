{
    "title": "Rationale-Guided Few-Shot Classification to Detect Abusive Language. (arXiv:2211.17046v2 [cs.CL] UPDATED)",
    "abstract": "Abusive language is a concerning problem in online social media. Past research on detecting abusive language covers different platforms, languages, demographies, etc. However, models trained using these datasets do not perform well in cross-domain evaluation settings. To overcome this, a common strategy is to use a few samples from the target domain to train models to get better performance in that domain (cross-domain few-shot training). However, this might cause the models to overfit the artefacts of those samples. A compelling solution could be to guide the models toward rationales, i.e., spans of text that justify the text's label. This method has been found to improve model performance in the in-domain setting across various NLP tasks. In this paper, we propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language detection. We first build a multitask learning setup to jointly learn rationales, targets, and labels, and find a significant improvement of 6% macro F1 o",
    "link": "http://arxiv.org/abs/2211.17046",
    "context": "Title: Rationale-Guided Few-Shot Classification to Detect Abusive Language. (arXiv:2211.17046v2 [cs.CL] UPDATED)\nAbstract: Abusive language is a concerning problem in online social media. Past research on detecting abusive language covers different platforms, languages, demographies, etc. However, models trained using these datasets do not perform well in cross-domain evaluation settings. To overcome this, a common strategy is to use a few samples from the target domain to train models to get better performance in that domain (cross-domain few-shot training). However, this might cause the models to overfit the artefacts of those samples. A compelling solution could be to guide the models toward rationales, i.e., spans of text that justify the text's label. This method has been found to improve model performance in the in-domain setting across various NLP tasks. In this paper, we propose RGFS (Rationale-Guided Few-Shot Classification) for abusive language detection. We first build a multitask learning setup to jointly learn rationales, targets, and labels, and find a significant improvement of 6% macro F1 o",
    "path": "papers/22/11/2211.17046.json",
    "total_tokens": 975,
    "translated_title": "有理指导下的少样本分类用于检测辱骂语言",
    "translated_abstract": "在在线社交媒体中，辱骂语言是一个令人担忧的问题。过去关于检测辱骂语言的研究涵盖了不同的平台、语言、人口统计等。然而，在交叉领域评估设置中，使用这些数据集训练的模型表现不佳。为了克服这个问题，一种常见策略是使用目标领域中的少量样本来训练模型，以获得更好的性能（交叉领域少样本训练）。然而，这可能导致模型过度拟合这些样本的特征。一个引人注目的解决方案可能是指导模型朝向有理的方向，即可以证明文本标签的文本段落。已经发现这种方法可以提高各种自然语言处理任务中在领域内的模型性能。在本文中，我们提出了用于检测辱骂语言的有理指导下的少样本分类（RGFS）。我们首先建立一个多任务学习框架，共同学习有理、目标和标签，并实现了显著的6%宏F1改进。",
    "tldr": "本文提出了一种有理指导下的少样本分类（RGFS）方法，用于检测辱骂语言。通过多任务学习框架，该方法可以共同学习有理、目标和标签，并在性能上实现了显著的6%宏F1改进。",
    "en_tdlr": "This paper proposes a rationale-guided few-shot classification (RGFS) approach for detecting abusive language. By utilizing a multitask learning framework to jointly learn rationales, targets, and labels, this method achieves a significant 6% macro F1 improvement in performance."
}