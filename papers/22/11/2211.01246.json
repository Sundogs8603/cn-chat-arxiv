{
    "title": "data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup. (arXiv:2211.01246v2 [eess.AS] UPDATED)",
    "abstract": "In this paper, we propose a new Self-Supervised Learning (SSL) algorithm called data2vec-aqc, for speech representation learning from unlabeled speech data. Our goal is to improve SSL for speech in domains where both unlabeled and labeled data are limited. Building on the recently introduced data2vec, we introduce additional modules to the data2vec framework that leverage the benefit of data augmentations, quantized representations, and clustering. The interaction between these modules helps solve the cross-contrastive loss as an additional self-supervised objective. data2vec-aqc achieves up to 14.1% and 20.9% relative WER improvement over the existing state-of-the-art data2vec system over the test-clean and test-other sets, respectively of LibriSpeech, without the use of any language model (LM). Our proposed model also achieves up to 17.8\\% relative WER gains over the baseline data2vec when fine-tuned on a subset of the Switchboard dataset. Code: https://github.com/Speech-Lab-IITM/dat",
    "link": "http://arxiv.org/abs/2211.01246",
    "context": "Title: data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup. (arXiv:2211.01246v2 [eess.AS] UPDATED)\nAbstract: In this paper, we propose a new Self-Supervised Learning (SSL) algorithm called data2vec-aqc, for speech representation learning from unlabeled speech data. Our goal is to improve SSL for speech in domains where both unlabeled and labeled data are limited. Building on the recently introduced data2vec, we introduce additional modules to the data2vec framework that leverage the benefit of data augmentations, quantized representations, and clustering. The interaction between these modules helps solve the cross-contrastive loss as an additional self-supervised objective. data2vec-aqc achieves up to 14.1% and 20.9% relative WER improvement over the existing state-of-the-art data2vec system over the test-clean and test-other sets, respectively of LibriSpeech, without the use of any language model (LM). Our proposed model also achieves up to 17.8\\% relative WER gains over the baseline data2vec when fine-tuned on a subset of the Switchboard dataset. Code: https://github.com/Speech-Lab-IITM/dat",
    "path": "papers/22/11/2211.01246.json",
    "total_tokens": 922,
    "translated_title": "data2vec-aqc：在师生训练中寻找合适的助教。",
    "translated_abstract": "本文提出了一种新的自我监督学习算法——data2vec-aqc，用于从未标记的语音数据中学习语音表示。我们的目标是改进在标记和未标记的数据都很有限的语音领域的自我监督学习。在最近引入的data2vec的基础上，我们引入了额外的模块来利用数据增强、量化表示和聚类的优势。这些模块之间的交互帮助解决了交叉对比损失作为额外的自我监督目标。在没有使用任何语言模型(LM)的情况下，data2vec-aqc在LibriSpeech的test-clean集和test-other集上相对于现有最先进的data2vec系统分别取得了14.1％和20.9％的相对WER提高。我们提出的模型在Switchboard数据集的子集上微调时也可以获得高达17.8％的相对WER收益。",
    "tldr": "本文提出了一种新的自我监督学习算法data2vec-aqc，用于从未标记的语音数据中学习语音表示，该算法在语音识别领域取得了显著的性能提高。",
    "en_tdlr": "This paper proposes a new self-supervised learning algorithm, data2vec-aqc, for learning speech representation from unlabeled speech data. The algorithm achieves significant performance improvement in the field of speech recognition and outperforms the state-of-the-art data2vec system."
}