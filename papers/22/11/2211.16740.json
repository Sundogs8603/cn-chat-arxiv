{
    "title": "Explicit Knowledge Transfer for Weakly-Supervised Code Generation. (arXiv:2211.16740v3 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) can acquire strong code-generation capabilities through few-shot learning. In contrast, supervised fine-tuning is still needed for smaller models to achieve good performance. Such fine-tuning demands a large number of task-specific NL-code pairs, which are expensive to obtain. In this paper, we attempt to transfer the code generation ability of an LLM to a smaller model with the aid of weakly-supervised data. More specifically, we propose explicit knowledge transfer (EKT), which uses the few-shot capabilities of a teacher LLM to create NL-code pairs that we then filter for correctness and fine-tune the student on. We evaluate EKT on the task of generating code solutions to math word problems from the GSM8k dataset. We find that EKT not only yields better performance than training with expert iteration, but also outperforms knowledge distillation, another form of knowledge transfer. A GPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%",
    "link": "http://arxiv.org/abs/2211.16740",
    "context": "Title: Explicit Knowledge Transfer for Weakly-Supervised Code Generation. (arXiv:2211.16740v3 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) can acquire strong code-generation capabilities through few-shot learning. In contrast, supervised fine-tuning is still needed for smaller models to achieve good performance. Such fine-tuning demands a large number of task-specific NL-code pairs, which are expensive to obtain. In this paper, we attempt to transfer the code generation ability of an LLM to a smaller model with the aid of weakly-supervised data. More specifically, we propose explicit knowledge transfer (EKT), which uses the few-shot capabilities of a teacher LLM to create NL-code pairs that we then filter for correctness and fine-tune the student on. We evaluate EKT on the task of generating code solutions to math word problems from the GSM8k dataset. We find that EKT not only yields better performance than training with expert iteration, but also outperforms knowledge distillation, another form of knowledge transfer. A GPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%",
    "path": "papers/22/11/2211.16740.json",
    "total_tokens": 977,
    "translated_title": "弱监督条件下的显式知识转移用于代码生成",
    "translated_abstract": "大型语言模型 (LLMs) 可以通过少量的学习获取强大的代码生成能力。相比之下，小型模型仍需要监督微调才能实现良好的性能。这种微调需要大量的任务特定的自然语言代码对，昂贵且难以获取。本文尝试利用弱监督数据将 LLM 的代码生成能力转移到小型模型。我们提出了显式知识转移 (EKT)，它使用教师 LLM 的少量样本能力来创建 NL-code 对，然后我们过滤出正确的对并对学生模型进行微调。我们评估了 EKT 在从 GSM8k 数据集中生成数学问题代码解时的任务上。我们发现，EKT 不仅比使用专家迭代训练的方法产生更好的性能，而且优于知识蒸馏，另一种知识转移形式。使用 GPT-J 作为教师模型，采用 EKT 对 GPT-Neo 1.3B 模型进行训练可获得 12.4% 的性能提升。",
    "tldr": "本文提出了显式知识转移 (EKT) 方法，将大型语言模型 (LLMs) 的代码生成能力转移到小型模型，将微调所需昂贵且难以获取的任务特定自然语言代码对改为通过过滤大量 NL-code 对来实现，优于知识蒸馏。",
    "en_tdlr": "This paper proposes explicit knowledge transfer (EKT) to transfer the code generation ability of large language models (LLMs) to smaller models under weakly-supervised conditions, and achieves the task of generating code solutions to math word problems from the GSM8k dataset by filtering NL-code pairs instead of requiring a large number of task-specific NL-code pairs, which outperforms knowledge distillation."
}