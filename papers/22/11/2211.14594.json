{
    "title": "Direct-Effect Risk Minimization for Domain Generalization. (arXiv:2211.14594v4 [cs.LG] UPDATED)",
    "abstract": "We study the problem of out-of-distribution (o.o.d.) generalization where spurious correlations of attributes vary across training and test domains. This is known as the problem of correlation shift and has posed concerns on the reliability of machine learning. In this work, we introduce the concepts of direct and indirect effects from causal inference to the domain generalization problem. We argue that models that learn direct effects minimize the worst-case risk across correlation-shifted domains. To eliminate the indirect effects, our algorithm consists of two stages: in the first stage, we learn an indirect-effect representation by minimizing the prediction error of domain labels using the representation and the class labels; in the second stage, we remove the indirect effects learned in the first stage by matching each data with another data of similar indirect-effect representation but of different class labels in the training and validation phase. Our approach is shown to be com",
    "link": "http://arxiv.org/abs/2211.14594",
    "context": "Title: Direct-Effect Risk Minimization for Domain Generalization. (arXiv:2211.14594v4 [cs.LG] UPDATED)\nAbstract: We study the problem of out-of-distribution (o.o.d.) generalization where spurious correlations of attributes vary across training and test domains. This is known as the problem of correlation shift and has posed concerns on the reliability of machine learning. In this work, we introduce the concepts of direct and indirect effects from causal inference to the domain generalization problem. We argue that models that learn direct effects minimize the worst-case risk across correlation-shifted domains. To eliminate the indirect effects, our algorithm consists of two stages: in the first stage, we learn an indirect-effect representation by minimizing the prediction error of domain labels using the representation and the class labels; in the second stage, we remove the indirect effects learned in the first stage by matching each data with another data of similar indirect-effect representation but of different class labels in the training and validation phase. Our approach is shown to be com",
    "path": "papers/22/11/2211.14594.json",
    "total_tokens": 859,
    "translated_title": "针对域泛化的直接影响风险最小化",
    "translated_abstract": "本文研究了域外推 generalization 中属性虚假相关性在训练和测试域间变化的问题，即相关性转移问题，该问题对机器学习的可靠性造成了影响。我们引入了因果推断中的直接和间接影响概念来解决域泛化问题。我们认为能够学习到直接影响的模型可以使最坏情况下的风险最小化。我们的算法分为两个阶段：第一阶段，我们通过最小化使用表示和类标签预测域标签的错误来学习间接影响表示；第二阶段，我们通过将训练和验证阶段中具有相似间接影响表示但标签不同的数据相匹配来消除在第一阶段中学习到的间接影响。我们的方法证明了其可与现有的域泛化算法相比较。",
    "tldr": "本文提出了一种针对域泛化的直接影响风险最小化算法，通过引入因果推断中的直接和间接影响概念解决了相关性转移问题。该算法与现有域泛化算法具有可比性。"
}