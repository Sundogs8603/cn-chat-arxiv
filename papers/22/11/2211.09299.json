{
    "title": "FedFA: Federated Learning with Feature Anchors to Align Features and Classifiers for Heterogeneous Data. (arXiv:2211.09299v3 [cs.LG] UPDATED)",
    "abstract": "Federated learning allows multiple clients to collaboratively train a model without exchanging their data, thus preserving data privacy. Unfortunately, it suffers significant performance degradation under heterogeneous data at clients. Common solutions in local training involve designing a specific auxiliary loss to regularize weight divergence or feature inconsistency. However, we discover that these approaches fall short of the expected performance because they ignore the existence of a vicious cycle between classifier divergence and feature mapping inconsistency across clients, such that client models are updated in inconsistent feature space with diverged classifiers. We then propose a simple yet effective framework named Federated learning with Feature Anchors (FedFA) to align the feature mappings and calibrate classifier across clients during local training, which allows client models updating in a shared feature space with consistent classifiers. We demonstrate that this modific",
    "link": "http://arxiv.org/abs/2211.09299",
    "context": "Title: FedFA: Federated Learning with Feature Anchors to Align Features and Classifiers for Heterogeneous Data. (arXiv:2211.09299v3 [cs.LG] UPDATED)\nAbstract: Federated learning allows multiple clients to collaboratively train a model without exchanging their data, thus preserving data privacy. Unfortunately, it suffers significant performance degradation under heterogeneous data at clients. Common solutions in local training involve designing a specific auxiliary loss to regularize weight divergence or feature inconsistency. However, we discover that these approaches fall short of the expected performance because they ignore the existence of a vicious cycle between classifier divergence and feature mapping inconsistency across clients, such that client models are updated in inconsistent feature space with diverged classifiers. We then propose a simple yet effective framework named Federated learning with Feature Anchors (FedFA) to align the feature mappings and calibrate classifier across clients during local training, which allows client models updating in a shared feature space with consistent classifiers. We demonstrate that this modific",
    "path": "papers/22/11/2211.09299.json",
    "total_tokens": 969,
    "translated_title": "FedFA: 针对异构数据的特征锚定联邦学习",
    "translated_abstract": "联邦学习允许多个客户端在不交换数据的情况下协作训练模型，从而保护数据隐私。然而，在客户端存在异构数据时，它会遭受明显的性能下降。本文发现，常见的本地训练解决方案通过设计特定的辅助损失函数来规范权重差异或特征不一致性，但这些方法忽略了分类器和特征映射不一致之间的恶性循环，导致客户端模型在特征空间和分类器差异的不一致特征空间中更新。我们提出了一个称为 FedFA 的简单而有效的框架，在本地训练过程中通过特征锚定来对齐客户端之间的特征映射并校准分类器，从而使客户端模型在共享的特征空间和一致的分类器下更新。我们证明，与先前的方法相比，在异构数据情况下，这种修改后的联邦学习方法提高了准确性和收敛速度。",
    "tldr": "本文提出了一种名为 FedFA 的联邦学习框架，通过特征锚定来对齐特征映射并校准分类器，解决了在异构数据时分类器和特征映射之间的恶性循环问题。在实验中表明，该方法能够提高准确性和收敛速度。",
    "en_tdlr": "This paper proposes a federated learning framework, named FedFA, that aligns feature mappings and calibrates classifiers across clients using feature anchors, solving the vicious cycle problem between classifier divergence and feature mapping inconsistency in heterogeneous data. Experimental results show that FedFA outperforms previous methods in accuracy and convergence speed."
}