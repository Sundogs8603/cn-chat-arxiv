{
    "title": "QAmeleon: Multilingual QA with Only 5 Examples. (arXiv:2211.08264v2 [cs.CL] UPDATED)",
    "abstract": "The availability of large, high-quality datasets has been one of the main drivers of recent progress in question answering (QA). Such annotated datasets however are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages. An alternative to building large monolingual training datasets is to leverage pre-trained language models (PLMs) under a few-shot learning setting. Our approach, QAmeleon, uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation. Prompt tuning the PLM for data synthesis with only five examples per language delivers accuracy superior to translation-based baselines, bridges nearly 60% of the gap between an English-only baseline and a fully supervised upper bound trained on almost 50,000 hand labeled examples, and always leads to substantial improvements compared to fine-tuning a QA model directly on labeled example",
    "link": "http://arxiv.org/abs/2211.08264",
    "context": "Title: QAmeleon: Multilingual QA with Only 5 Examples. (arXiv:2211.08264v2 [cs.CL] UPDATED)\nAbstract: The availability of large, high-quality datasets has been one of the main drivers of recent progress in question answering (QA). Such annotated datasets however are difficult and costly to collect, and rarely exist in languages other than English, rendering QA technology inaccessible to underrepresented languages. An alternative to building large monolingual training datasets is to leverage pre-trained language models (PLMs) under a few-shot learning setting. Our approach, QAmeleon, uses a PLM to automatically generate multilingual data upon which QA models are trained, thus avoiding costly annotation. Prompt tuning the PLM for data synthesis with only five examples per language delivers accuracy superior to translation-based baselines, bridges nearly 60% of the gap between an English-only baseline and a fully supervised upper bound trained on almost 50,000 hand labeled examples, and always leads to substantial improvements compared to fine-tuning a QA model directly on labeled example",
    "path": "papers/22/11/2211.08264.json",
    "total_tokens": 950,
    "translated_title": "QAmeleon: 仅需5个示例的多语言问答",
    "translated_abstract": "大规模高质量的数据集的可用性是近期问答（QA）技术进展的主要驱动因素之一。然而，这样的标注数据集难以收集且成本高昂，且很少存在于非英语语言中，使得QA技术对于少数语言不可访问。与构建大型单语言训练数据集相比，一种替代方案是在少样本学习环境下利用预训练语言模型（PLM）。我们的方法QAmeleon使用PLM自动生成跨多语言的数据，然后用于训练QA模型，从而避免了昂贵的标注过程。通过仅在每种语言中使用5个示例对PLM进行提示调整，可以获得优于基于翻译的基线方法的准确性，弥合了仅英语的基线模型与近50,000个手工标记示例训练的完全监督上界之间的差距约60％，并且始终相比于直接在标记示例上微调QA模型，都能带来实质性的改进。",
    "tldr": "QAmeleon是一种通过仅使用5个示例进行多语言问答的方法，通过预训练语言模型自动生成数据并进行训练，避免了昂贵的标注过程。该方法在准确性和性能方面优于传统的基于翻译的方法，并能弥合英语和完全监督方法之间的差距。",
    "en_tdlr": "QAmeleon is a method for multilingual question answering that uses only 5 examples. It leverages pre-trained language models to generate data and achieve better performance without costly annotation. It outperforms translation-based baselines and bridges the gap between English-only and fully supervised approaches."
}