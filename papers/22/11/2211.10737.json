{
    "title": "Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training. (arXiv:2211.10737v3 [cs.LG] UPDATED)",
    "abstract": "The unprecedented growth in DNN model complexity, size, and amount of training data has led to a commensurate increase in demand for computing and a search for minimal encoding. Recent research advocates Hybrid Block Floating Point (HBFP) to minimize silicon provisioning in accelerators by converting the majority of arithmetic operations in training to 8-bit fixed point. In this paper, we perform a full-scale exploration of the HBFP design space using mathematical tools to study the interplay among various parameters and identify opportunities for even smaller encodings across layers and epochs. Based on our findings, we propose Accuracy Boosters, an epoch-driven mixed-mantissa HBFP technique that uses 6-bit mantissas only in the last epoch and first/last layers, and 4-bit mantissas for $99.7\\%$ of all other arithmetic operations in training. Using analytic models, we show Accuracy Boosters enable increasing arithmetic density for an HBFP training accelerator by up to $21.3\\times$ comp",
    "link": "http://arxiv.org/abs/2211.10737",
    "context": "Title: Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training. (arXiv:2211.10737v3 [cs.LG] UPDATED)\nAbstract: The unprecedented growth in DNN model complexity, size, and amount of training data has led to a commensurate increase in demand for computing and a search for minimal encoding. Recent research advocates Hybrid Block Floating Point (HBFP) to minimize silicon provisioning in accelerators by converting the majority of arithmetic operations in training to 8-bit fixed point. In this paper, we perform a full-scale exploration of the HBFP design space using mathematical tools to study the interplay among various parameters and identify opportunities for even smaller encodings across layers and epochs. Based on our findings, we propose Accuracy Boosters, an epoch-driven mixed-mantissa HBFP technique that uses 6-bit mantissas only in the last epoch and first/last layers, and 4-bit mantissas for $99.7\\%$ of all other arithmetic operations in training. Using analytic models, we show Accuracy Boosters enable increasing arithmetic density for an HBFP training accelerator by up to $21.3\\times$ comp",
    "path": "papers/22/11/2211.10737.json",
    "total_tokens": 974,
    "translated_title": "提高准确性: 基于时代驱动的混合尾数块浮点方法用于DNN训练",
    "translated_abstract": "DNN模型复杂性、规模和训练数据量的前所未有增长导致了对计算的巨大需求和对最小编码的搜索。最近的研究主张使用混合块浮点(HBFP)来最小化加速器中的硅配备，通过将大部分训练中的算术操作转换为8位定点。本文通过数学工具对HBFP设计空间进行了全面的探索，研究了各种参数之间的相互作用，并确定了在各层和各个时代中更小编码的机会。基于我们的发现，我们提出了Accuracy Boosters，一种基于时代驱动的混合尾数HBFP技术，只在最后一个时代和第一个/最后一层中使用6位尾数，在训练中的其他算术操作中使用4位尾数达到$99.7\\%$。使用分析模型，我们展示了Accuracy Boosters可以使HBFP训练加速器的算术密度增加高达$21.3\\times$。",
    "tldr": "本文提出了一种基于时代驱动的混合尾数HBFP技术，通过对不同参数的探索和优化，实现了对DNN训练中算术操作的更小编码。使用分析模型表明，该方法能够将HBFP训练加速器的算术密度增加高达$21.3\\times$。",
    "en_tdlr": "This paper proposes an epoch-driven mixed-mantissa HBFP technique that achieves smaller encodings for arithmetic operations in DNN training through exploration and optimization of various parameters. Analytic models show that this method can increase the arithmetic density of an HBFP training accelerator by up to 21.3 times."
}