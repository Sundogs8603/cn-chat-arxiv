{
    "title": "PAC-Bayes Bounds for Bandit Problems: A Survey and Experimental Comparison. (arXiv:2211.16110v2 [cs.LG] UPDATED)",
    "abstract": "PAC-Bayes has recently re-emerged as an effective theory with which one can derive principled learning algorithms with tight performance guarantees. However, applications of PAC-Bayes to bandit problems are relatively rare, which is a great misfortune. Many decision-making problems in healthcare, finance and natural sciences can be modelled as bandit problems. In many of these applications, principled algorithms with strong performance guarantees would be very much appreciated. This survey provides an overview of PAC-Bayes bounds for bandit problems and an experimental comparison of these bounds. On the one hand, we found that PAC-Bayes bounds are a useful tool for designing offline bandit algorithms with performance guarantees. In our experiments, a PAC-Bayesian offline contextual bandit algorithm was able to learn randomised neural network polices with competitive expected reward and non-vacuous performance guarantees. On the other hand, the PAC-Bayesian online bandit algorithms that",
    "link": "http://arxiv.org/abs/2211.16110",
    "context": "Title: PAC-Bayes Bounds for Bandit Problems: A Survey and Experimental Comparison. (arXiv:2211.16110v2 [cs.LG] UPDATED)\nAbstract: PAC-Bayes has recently re-emerged as an effective theory with which one can derive principled learning algorithms with tight performance guarantees. However, applications of PAC-Bayes to bandit problems are relatively rare, which is a great misfortune. Many decision-making problems in healthcare, finance and natural sciences can be modelled as bandit problems. In many of these applications, principled algorithms with strong performance guarantees would be very much appreciated. This survey provides an overview of PAC-Bayes bounds for bandit problems and an experimental comparison of these bounds. On the one hand, we found that PAC-Bayes bounds are a useful tool for designing offline bandit algorithms with performance guarantees. In our experiments, a PAC-Bayesian offline contextual bandit algorithm was able to learn randomised neural network polices with competitive expected reward and non-vacuous performance guarantees. On the other hand, the PAC-Bayesian online bandit algorithms that",
    "path": "papers/22/11/2211.16110.json",
    "total_tokens": 988,
    "translated_title": "PAC-Bayes定理在Bandit问题中的应用：一项调查与实验比较",
    "translated_abstract": "PAC-Bayes最近重新出现作为一种有效的理论，可以用来推导出具有紧密性能保证的有原则的学习算法。然而，PAC-Bayes在Bandit问题中的应用相对较少，这是一个很大的遗憾。在医疗保健、金融和自然科学等许多决策问题中，都可以将其建模为Bandit问题。在许多这些应用中，带有强大性能保证的有原则算法将会受到很高的赞赏。本调查提供了关于Bandit问题的PAC-Bayes界限的概述，并进行了这些界限的实验比较。一方面，我们发现PAC-Bayes界限是设计具有性能保证的离线Bandit算法的有用工具。在我们的实验中，一种PAC-Bayesian离线上下文Bandit算法能够学习具有竞争性预期奖励和非空性能保证的随机化神经网络策略。另一方面，PAC-Bayesian在线Bandit算法则缺乏足够的数据以产生强大的性能保证。",
    "tldr": "这项调查研究了PAC-Bayes在Bandit问题中的应用，提供了界限的概述，并进行了实验比较。研究发现，PAC-Bayes界限是设计具有性能保证的离线Bandit算法的有用工具，但在线Bandit算法缺乏足够的数据以产生强大的性能保证。"
}