{
    "title": "OCTET: Object-aware Counterfactual Explanations. (arXiv:2211.12380v2 [cs.CV] UPDATED)",
    "abstract": "Nowadays, deep vision models are being widely deployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counterfactual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the decision of the model. However, previous methods struggle to explain decision models trained on images with many objects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain. In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, encodes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over w",
    "link": "http://arxiv.org/abs/2211.12380",
    "context": "Title: OCTET: Object-aware Counterfactual Explanations. (arXiv:2211.12380v2 [cs.CV] UPDATED)\nAbstract: Nowadays, deep vision models are being widely deployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counterfactual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the decision of the model. However, previous methods struggle to explain decision models trained on images with many objects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain. In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, encodes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over w",
    "path": "papers/22/11/2211.12380.json",
    "total_tokens": 807,
    "translated_title": "OCTET: 对象感知的反事实解释",
    "translated_abstract": "当前，深度视觉模型被广泛应用于安全关键应用，如自动驾驶。模型的可解释性正在成为一个紧迫的问题。在众多解释方法中，反事实解释旨在找到最小和可解释的变化来改变模型输出结果的输入图像。这样的解释使最终用户了解影响模型决策的主要因素。然而，以前的方法很难解释训练有多个物体图像（例如城市场景）的决策模型，这些图像更难处理，但也可能更关键。本文提出了一种基于对象的框架来生成反事实解释，旨在解决这个问题。我们的方法受到最近生成建模工作的启发，将查询图像编码为一个结构化的潜空间，以便进行基于对象的操作。这样做，可以为最终用户提供对模型的控制。",
    "tldr": "本文提出了一种基于对象的框架来生成反事实解释，以解释训练有多个物体图像的决策模型。",
    "en_tdlr": "This paper proposes an object-centric framework for generating counterfactual explanations to explain decision models trained on images with multiple objects."
}