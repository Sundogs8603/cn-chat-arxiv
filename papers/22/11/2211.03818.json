{
    "title": "Retrieval augmentation of large language models for lay language generation. (arXiv:2211.03818v2 [cs.CL] UPDATED)",
    "abstract": "Recent lay language generation systems have used Transformer models trained on a parallel corpus to increase health information accessibility. However, the applicability of these models is constrained by the limited size and topical breadth of available corpora. We introduce CELLS, the largest (63k pairs) and broadest-ranging (12 journals) parallel corpus for lay language generation. The abstract and the corresponding lay language summary are written by domain experts, assuring the quality of our dataset. Furthermore, qualitative evaluation of expert-authored plain language summaries has revealed background explanation as a key strategy to increase accessibility. Such explanation is challenging for neural models to generate because it goes beyond simplification by adding content absent from the source. We derive two specialized paired corpora from CELLS to address key challenges in lay language generation: generating background explanations and simplifying the original abstract. We ado",
    "link": "http://arxiv.org/abs/2211.03818",
    "context": "Title: Retrieval augmentation of large language models for lay language generation. (arXiv:2211.03818v2 [cs.CL] UPDATED)\nAbstract: Recent lay language generation systems have used Transformer models trained on a parallel corpus to increase health information accessibility. However, the applicability of these models is constrained by the limited size and topical breadth of available corpora. We introduce CELLS, the largest (63k pairs) and broadest-ranging (12 journals) parallel corpus for lay language generation. The abstract and the corresponding lay language summary are written by domain experts, assuring the quality of our dataset. Furthermore, qualitative evaluation of expert-authored plain language summaries has revealed background explanation as a key strategy to increase accessibility. Such explanation is challenging for neural models to generate because it goes beyond simplification by adding content absent from the source. We derive two specialized paired corpora from CELLS to address key challenges in lay language generation: generating background explanations and simplifying the original abstract. We ado",
    "path": "papers/22/11/2211.03818.json",
    "total_tokens": 851,
    "translated_title": "大型语言模型在普通语言生成中的检索增强",
    "translated_abstract": "最近的普通语言生成系统利用在平行语料库上训练的Transformer模型增加了健康信息的可访问性。然而，这些模型的适用性受到可用语料库的规模和主题广度的限制。我们介绍了CELLS，这是用于普通语言生成的最大（63k对）和最广泛涉及的（12个期刊）平行语料库。摘要和相应的普通语言摘要由领域专家撰写，确保了我们数据集的质量。此外，专家撰写的常规语言摘要的定性评估揭示了背景解释作为增加可访问性的关键策略。这种解释对于神经模型的生成来说具有挑战性，因为它不仅仅是简化，还添加了源数据中缺少的内容。我们从CELLS中衍生出两个专门的配对语料库，以解决普通语言生成中的关键挑战：生成背景解释和简化原始摘要。",
    "tldr": "CELLS是用于普通语言生成的最大最广泛的平行语料库，通过生成背景解释和简化原始摘要来解决普通语言生成中的关键挑战。",
    "en_tdlr": "CELLS is the largest and broadest parallel corpus for lay language generation, addressing key challenges in lay language generation by generating background explanations and simplifying the original abstract."
}