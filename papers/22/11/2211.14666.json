{
    "title": "Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning. (arXiv:2211.14666v2 [cs.LG] UPDATED)",
    "abstract": "Although disentangled representations are often said to be beneficial for downstream tasks, current empirical and theoretical understanding is limited. In this work, we provide evidence that disentangled representations coupled with sparse base-predictors improve generalization. In the context of multi-task learning, we prove a new identifiability result that provides conditions under which maximally sparse base-predictors yield disentangled representations. Motivated by this theoretical result, we propose a practical approach to learn disentangled representations based on a sparsity-promoting bi-level optimization problem. Finally, we explore a meta-learning version of this algorithm based on group Lasso multiclass SVM base-predictors, for which we derive a tractable dual formulation. It obtains competitive results on standard few-shot classification benchmarks, while each task is using only a fraction of the learned representations.",
    "link": "http://arxiv.org/abs/2211.14666",
    "context": "Title: Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning. (arXiv:2211.14666v2 [cs.LG] UPDATED)\nAbstract: Although disentangled representations are often said to be beneficial for downstream tasks, current empirical and theoretical understanding is limited. In this work, we provide evidence that disentangled representations coupled with sparse base-predictors improve generalization. In the context of multi-task learning, we prove a new identifiability result that provides conditions under which maximally sparse base-predictors yield disentangled representations. Motivated by this theoretical result, we propose a practical approach to learn disentangled representations based on a sparsity-promoting bi-level optimization problem. Finally, we explore a meta-learning version of this algorithm based on group Lasso multiclass SVM base-predictors, for which we derive a tractable dual formulation. It obtains competitive results on standard few-shot classification benchmarks, while each task is using only a fraction of the learned representations.",
    "path": "papers/22/11/2211.14666.json",
    "total_tokens": 917,
    "translated_title": "脱耦表示与稀疏性之间的协同作用：多任务学习中的泛化和可识别性",
    "translated_abstract": "尽管脱耦表示经常被认为对下游任务有益，但目前的经验和理论理解仍然有限。本文提供证据表明，与稀疏基预测器相结合的脱耦表示可提高泛化能力。在多任务学习的背景下，我们证明了一个新的可识别性结果，该结果提供了最大稀疏基预测器产生脱耦表示的条件。受这个理论结果的启发，我们提出了一种基于稀疏促进双层优化问题学习脱耦表示的实用方法。最后，我们探索了一个基于组 Lasso 多类 SVM 基预测器的元学习版本的算法，为此，我们推导出了一个可行的对偶公式。它在标准的少样本分类基准测试上获得了竞争性的结果，而每个任务仅使用了一小部分学习到的表示。",
    "tldr": "本文提供了证据表明，脱耦表示与稀疏基预测器相结合可提高泛化能力。我们提出了一个实用方法来学习这种表示，并在少样本分类基准测试中取得了竞争性的结果。",
    "en_tdlr": "This paper provides evidence that coupled disentangled representations with sparse base-predictors can improve generalization, and proposes a practical approach to learn these representations based on a sparsity-promoting bi-level optimization problem. The paper also explores a meta-learning version of this algorithm that obtains competitive results on standard few-shot classification benchmarks while only using a fraction of the learned representations for each task."
}