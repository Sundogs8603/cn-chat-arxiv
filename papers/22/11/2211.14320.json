{
    "title": "Bidirectional Representations for Low Resource Spoken Language Understanding. (arXiv:2211.14320v2 [cs.CL] UPDATED)",
    "abstract": "Most spoken language understanding systems use a pipeline approach composed of an automatic speech recognition interface and a natural language understanding module. This approach forces hard decisions when converting continuous inputs into discrete language symbols. Instead, we propose a representation model to encode speech in rich bidirectional encodings that can be used for downstream tasks such as intent prediction. The approach uses a masked language modelling objective to learn the representations, and thus benefits from both the left and right contexts. We show that the performance of the resulting encodings before fine-tuning is better than comparable models on multiple datasets, and that fine-tuning the top layers of the representation model improves the current state of the art on the Fluent Speech Command dataset, also in a low-data regime, when a limited amount of labelled data is used for training. Furthermore, we propose class attention as a spoken language understanding",
    "link": "http://arxiv.org/abs/2211.14320",
    "context": "Title: Bidirectional Representations for Low Resource Spoken Language Understanding. (arXiv:2211.14320v2 [cs.CL] UPDATED)\nAbstract: Most spoken language understanding systems use a pipeline approach composed of an automatic speech recognition interface and a natural language understanding module. This approach forces hard decisions when converting continuous inputs into discrete language symbols. Instead, we propose a representation model to encode speech in rich bidirectional encodings that can be used for downstream tasks such as intent prediction. The approach uses a masked language modelling objective to learn the representations, and thus benefits from both the left and right contexts. We show that the performance of the resulting encodings before fine-tuning is better than comparable models on multiple datasets, and that fine-tuning the top layers of the representation model improves the current state of the art on the Fluent Speech Command dataset, also in a low-data regime, when a limited amount of labelled data is used for training. Furthermore, we propose class attention as a spoken language understanding",
    "path": "papers/22/11/2211.14320.json",
    "total_tokens": 850,
    "translated_title": "低资源语音理解的双向表示",
    "translated_abstract": "大多数语音理解系统使用由自动语音识别接口和自然语言理解模块组成的流水线方法。这种方法在将连续输入转换为离散语言符号时会产生困难决策。相反，我们提出了一种表示模型，用于将语音编码为丰富的双向编码，可用于诸如意图预测之类的下游任务。该方法使用掩蔽语言建模目标来学习表示，因此从左右上下文中受益。我们表明，未经微调的生成的编码性能优于多个数据集上的可比模型，并且在低数据情况下，通过微调表示模型的顶层，改进了流畅语音命令数据集上的当前技术水平，当使用有限的标记数据进行训练时。此外，我们提出了类别注意力作为一种语音理解方法。",
    "tldr": "本文提出了一种用于低资源语音理解的双向表示模型，通过学习丰富的双向编码，可以在意图预测等任务上取得较好性能，并通过微调顶层改进了流畅语音命令数据集上的当前技术水平。",
    "en_tdlr": "This paper proposes a bidirectional representation model for low resource spoken language understanding, which achieves better performance in tasks like intent prediction by learning rich bidirectional encodings. Fine-tuning the top layers of the model also improves the current state of the art on the Fluent Speech Command dataset."
}