{
    "title": "Empirical Study on Optimizer Selection for Out-of-Distribution Generalization. (arXiv:2211.08583v3 [cs.LG] UPDATED)",
    "abstract": "Modern deep learning systems do not generalize well when the test data distribution is slightly different to the training data distribution. While much promising work has been accomplished to address this fragility, a systematic study of the role of optimizers and their out-of-distribution generalization performance has not been undertaken. In this study, we examine the performance of popular first-order optimizers for different classes of distributional shift under empirical risk minimization and invariant risk minimization. We address this question for image and text classification using DomainBed, WILDS, and Backgrounds Challenge as testbeds for studying different types of shifts -- namely correlation and diversity shift. We search over a wide range of hyperparameters and examine classification accuracy (in-distribution and out-of-distribution) for over 20,000 models. We arrive at the following findings, which we expect to be helpful for practitioners: i) adaptive optimizers (e.g., ",
    "link": "http://arxiv.org/abs/2211.08583",
    "context": "Title: Empirical Study on Optimizer Selection for Out-of-Distribution Generalization. (arXiv:2211.08583v3 [cs.LG] UPDATED)\nAbstract: Modern deep learning systems do not generalize well when the test data distribution is slightly different to the training data distribution. While much promising work has been accomplished to address this fragility, a systematic study of the role of optimizers and their out-of-distribution generalization performance has not been undertaken. In this study, we examine the performance of popular first-order optimizers for different classes of distributional shift under empirical risk minimization and invariant risk minimization. We address this question for image and text classification using DomainBed, WILDS, and Backgrounds Challenge as testbeds for studying different types of shifts -- namely correlation and diversity shift. We search over a wide range of hyperparameters and examine classification accuracy (in-distribution and out-of-distribution) for over 20,000 models. We arrive at the following findings, which we expect to be helpful for practitioners: i) adaptive optimizers (e.g., ",
    "path": "papers/22/11/2211.08583.json",
    "total_tokens": 961,
    "translated_title": "关于优化器选择提高模型对分布似然小变化的泛化性能的实证研究",
    "translated_abstract": "当测试数据分布与训练数据分布稍有不同时，现代深度学习系统的泛化能力不佳。虽然已经有了很多有前途的工作来解决这个问题，但对优化器及其在分布外泛化性能中的作用进行系统研究尚未进行。本研究通过在经验风险最小化和不变风险最小化下，使用DomainBed，WILDS和Backgrounds Challenge分别作为测试平台研究不同类型偏移（即相关性和多样性变化）对图像和文本分类的影响，并使用广泛的超参数范围搜索并测试超过20,000个模型的分类准确性（分布内和分布外）。我们的研究得出以下结论，我们相信对实践者是有帮助的，i) 自适应优化器（例如 Adam 和 Adagrad）具有泛化性能更好。ii) 当偏移有很强的时间局部性时，能够减少时间相关特征的优化器具有泛化性能更好，而当偏移有强的时间整体性时则没有性能优势。",
    "tldr": "本实证研究通过测试不同分布偏移下超过20,000个模型的表现，发现自适应优化器（如Adam和Adagrad）及可以减少时间相关特征的优化器具有更好的分布外泛化性能。",
    "en_tdlr": "This empirical study examines the role of optimizers in out-of-distribution generalization and finds that adaptive optimizers like Adam and Adagrad perform better, and optimizers that can reduce time-correlated features perform better when the shift has strong temporal locality, while there is no performance advantage when the shift has strong temporal coherence."
}