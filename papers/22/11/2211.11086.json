{
    "title": "An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning. (arXiv:2211.11086v2 [cs.CV] UPDATED)",
    "abstract": "Semi-supervised learning (SSL) has shown great promise in leveraging unlabeled data to improve model performance. While standard SSL assumes uniform data distribution, we consider a more realistic and challenging setting called imbalanced SSL, where imbalanced class distributions occur in both labeled and unlabeled data. Although there are existing endeavors to tackle this challenge, their performance degenerates when facing severe imbalance since they can not reduce the class imbalance sufficiently and effectively. In this paper, we study a simple yet overlooked baseline -- SimiS -- which tackles data imbalance by simply supplementing labeled data with pseudo-labels, according to the difference in class distribution from the most frequent class. Such a simple baseline turns out to be highly effective in reducing class imbalance. It outperforms existing methods by a significant margin, e.g., 12.8%, 13.6%, and 16.7% over previous SOTA on CIFAR100-LT, FOOD101-LT, and ImageNet127 respecti",
    "link": "http://arxiv.org/abs/2211.11086",
    "context": "Title: An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning. (arXiv:2211.11086v2 [cs.CV] UPDATED)\nAbstract: Semi-supervised learning (SSL) has shown great promise in leveraging unlabeled data to improve model performance. While standard SSL assumes uniform data distribution, we consider a more realistic and challenging setting called imbalanced SSL, where imbalanced class distributions occur in both labeled and unlabeled data. Although there are existing endeavors to tackle this challenge, their performance degenerates when facing severe imbalance since they can not reduce the class imbalance sufficiently and effectively. In this paper, we study a simple yet overlooked baseline -- SimiS -- which tackles data imbalance by simply supplementing labeled data with pseudo-labels, according to the difference in class distribution from the most frequent class. Such a simple baseline turns out to be highly effective in reducing class imbalance. It outperforms existing methods by a significant margin, e.g., 12.8%, 13.6%, and 16.7% over previous SOTA on CIFAR100-LT, FOOD101-LT, and ImageNet127 respecti",
    "path": "papers/22/11/2211.11086.json",
    "total_tokens": 1022,
    "translated_title": "一种用于不平衡半监督学习的令人尴尬简单基准",
    "translated_abstract": "半监督学习（SSL）在利用无标签数据改善模型性能方面表现出巨大潜力。然而，标准的SSL假设数据分布均匀，我们考虑了一个更加真实和具有挑战性的情境，即不平衡半监督学习（imbalanced SSL），其中标签数据和无标签数据都出现了不平衡的类别分布。尽管已有努力解决这一挑战的方法，但当遇到严重不平衡时，它们的性能会退化，因为它们无法对类别不平衡进行足够和有效的减少。在本文中，我们研究了一个简单但被忽视的基准方法--SimiS，通过简单地根据与最频繁类别的类别分布差异，将伪标签作为标签数据的补充。这样一个简单的基准方法在减少类别不平衡方面非常有效。它在CIFAR100-LT，FOOD101-LT和ImageNet127上相对于先前最佳方法的性能提升显著，分别提升了12.8％，13.6％和16.7％。",
    "tldr": "本文研究了一种名为SimiS的简单但被忽视的基准方法，通过将伪标签作为标签数据的补充，根据与最频繁类别的类别分布差异，有效地减少了不平衡半监督学习中的类别不平衡，相对于现有方法有显著的性能提升。",
    "en_tdlr": "This paper studies a simple yet overlooked baseline method called SimiS, which effectively reduces class imbalance in imbalanced semi-supervised learning by supplementing labeled data with pseudo-labels based on the difference in class distribution from the most frequent class. It outperforms existing methods significantly in terms of performance improvement."
}