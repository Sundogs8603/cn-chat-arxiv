{
    "title": "Leveraging Offline Data in Online Reinforcement Learning. (arXiv:2211.04974v2 [cs.LG] UPDATED)",
    "abstract": "Two central paradigms have emerged in the reinforcement learning (RL) community: online RL and offline RL. In the online RL setting, the agent has no prior knowledge of the environment, and must interact with it in order to find an $\\epsilon$-optimal policy. In the offline RL setting, the learner instead has access to a fixed dataset to learn from, but is unable to otherwise interact with the environment, and must obtain the best policy it can from this offline data. Practical scenarios often motivate an intermediate setting: if we have some set of offline data and, in addition, may also interact with the environment, how can we best use the offline data to minimize the number of online interactions necessary to learn an $\\epsilon$-optimal policy?  In this work, we consider this setting, which we call the \\textsf{FineTuneRL} setting, for MDPs with linear structure. We characterize the necessary number of online samples needed in this setting given access to some offline dataset, and de",
    "link": "http://arxiv.org/abs/2211.04974",
    "context": "Title: Leveraging Offline Data in Online Reinforcement Learning. (arXiv:2211.04974v2 [cs.LG] UPDATED)\nAbstract: Two central paradigms have emerged in the reinforcement learning (RL) community: online RL and offline RL. In the online RL setting, the agent has no prior knowledge of the environment, and must interact with it in order to find an $\\epsilon$-optimal policy. In the offline RL setting, the learner instead has access to a fixed dataset to learn from, but is unable to otherwise interact with the environment, and must obtain the best policy it can from this offline data. Practical scenarios often motivate an intermediate setting: if we have some set of offline data and, in addition, may also interact with the environment, how can we best use the offline data to minimize the number of online interactions necessary to learn an $\\epsilon$-optimal policy?  In this work, we consider this setting, which we call the \\textsf{FineTuneRL} setting, for MDPs with linear structure. We characterize the necessary number of online samples needed in this setting given access to some offline dataset, and de",
    "path": "papers/22/11/2211.04974.json",
    "total_tokens": 898,
    "translated_title": "在在线强化学习中利用离线数据",
    "translated_abstract": "强化学习领域出现了两个核心范式：在线强化学习和离线强化学习。在线强化学习中，智能体对环境没有先验知识，必须与环境交互以找到一个 ε-最优策略。离线强化学习中，学习器可以从一个固定的数据集中学习，但无法与环境进行交互，必须通过离线数据获取最佳策略。实际情况通常需要一个中间的设置：如果我们有一些离线数据，并且还可以与环境进行交互，我们如何最好地利用离线数据来减少学习一个 ε-最优策略所需的在线交互次数？在这项工作中，我们考虑了这个设置，我们称之为FineTuneRL设置，用于具有线性结构的MDPs。我们确定了在给定一些离线数据的情况下，在这个设置中需要的在线样本数量，并提供了一些性质和算法来实现这个目标。",
    "tldr": "在这项工作中，我们研究了在线强化学习中利用离线数据的设置，为具有线性结构的MDPs确定了所需的在线样本数量，并提供了实现这一目标的性质和算法。"
}