{
    "title": "FsaNet: Frequency Self-attention for Semantic Segmentation. (arXiv:2211.15595v2 [cs.CV] UPDATED)",
    "abstract": "Considering the spectral properties of images, we propose a new self-attention mechanism with highly reduced computational complexity, up to a linear rate. To better preserve edges while promoting similarity within objects, we propose individualized processes over different frequency bands. In particular, we study a case where the process is merely over low-frequency components. By ablation study, we show that low frequency self-attention can achieve very close or better performance relative to full frequency even without retraining the network. Accordingly, we design and embed novel plug-and-play modules to the head of a CNN network that we refer to as FsaNet. The frequency self-attention 1) requires only a few low frequency coefficients as input, 2) can be mathematically equivalent to spatial domain self-attention with linear structures, 3) simplifies token mapping ($1\\times1$ convolution) stage and token mixing stage simultaneously. We show that frequency self-attention requires $87",
    "link": "http://arxiv.org/abs/2211.15595",
    "context": "Title: FsaNet: Frequency Self-attention for Semantic Segmentation. (arXiv:2211.15595v2 [cs.CV] UPDATED)\nAbstract: Considering the spectral properties of images, we propose a new self-attention mechanism with highly reduced computational complexity, up to a linear rate. To better preserve edges while promoting similarity within objects, we propose individualized processes over different frequency bands. In particular, we study a case where the process is merely over low-frequency components. By ablation study, we show that low frequency self-attention can achieve very close or better performance relative to full frequency even without retraining the network. Accordingly, we design and embed novel plug-and-play modules to the head of a CNN network that we refer to as FsaNet. The frequency self-attention 1) requires only a few low frequency coefficients as input, 2) can be mathematically equivalent to spatial domain self-attention with linear structures, 3) simplifies token mapping ($1\\times1$ convolution) stage and token mixing stage simultaneously. We show that frequency self-attention requires $87",
    "path": "papers/22/11/2211.15595.json",
    "total_tokens": 989,
    "translated_title": "FsaNet: 频率自注意力用于语义分割",
    "translated_abstract": "考虑到图像的频谱特性，我们提出了一种新的带有高度降低计算复杂性的自注意机制。为了更好地保留边缘并促进对象内的相似性，我们提出了在不同频段上个性化处理的方法。特别是，我们研究了仅在低频分量上进行处理的情况。通过消融研究，我们展示了即使在不重新训练网络的情况下，低频自注意力也可以达到非常接近甚至更好的性能。因此，我们设计并嵌入了新的即插即用模块到CNN网络的头部，我们将其称为FsaNet。频率自注意力1）仅需要几个低频系数作为输入，2）在数学上可以等效于具有线性结构的空间域自注意力，3）同时简化了令牌映射（$1\\times1$卷积）阶段和令牌混合阶段。我们展示了频率自注意力只需要87",
    "tldr": "FsaNet是一种用于语义分割的新型自注意机制，通过在不同频段上进行个性化处理，可以在保留边缘的同时促进对象内的相似性。通过消融研究表明，即使不重新训练网络，低频自注意力也可以达到接近或更好的性能。频率自注意力还简化了令牌映射和令牌混合阶段，具有较低的计算复杂性。",
    "en_tdlr": "FsaNet is a novel self-attention mechanism for semantic segmentation that achieves edge preservation and promotes similarity within objects by personalized processing in different frequency bands. Ablation study shows that even without retraining, low frequency self-attention can achieve comparable or better performance. Frequency self-attention also simplifies the token mapping and token mixing stages with lower computational complexity."
}