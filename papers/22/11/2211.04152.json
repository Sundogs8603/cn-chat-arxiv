{
    "title": "Federated Learning Using Three-Operator ADMM",
    "abstract": "arXiv:2211.04152v3 Announce Type: replace  Abstract: Federated learning (FL) has emerged as an instance of distributed machine learning paradigm that avoids the transmission of data generated on the users' side. Although data are not transmitted, edge devices have to deal with limited communication bandwidths, data heterogeneity, and straggler effects due to the limited computational resources of users' devices. A prominent approach to overcome such difficulties is FedADMM, which is based on the classical two-operator consensus alternating direction method of multipliers (ADMM). The common assumption of FL algorithms, including FedADMM, is that they learn a global model using data only on the users' side and not on the edge server. However, in edge learning, the server is expected to be near the base station and have direct access to rich datasets. In this paper, we argue that leveraging the rich data on the edge server is much more beneficial than utilizing only user datasets. Specifi",
    "link": "https://arxiv.org/abs/2211.04152",
    "context": "Title: Federated Learning Using Three-Operator ADMM\nAbstract: arXiv:2211.04152v3 Announce Type: replace  Abstract: Federated learning (FL) has emerged as an instance of distributed machine learning paradigm that avoids the transmission of data generated on the users' side. Although data are not transmitted, edge devices have to deal with limited communication bandwidths, data heterogeneity, and straggler effects due to the limited computational resources of users' devices. A prominent approach to overcome such difficulties is FedADMM, which is based on the classical two-operator consensus alternating direction method of multipliers (ADMM). The common assumption of FL algorithms, including FedADMM, is that they learn a global model using data only on the users' side and not on the edge server. However, in edge learning, the server is expected to be near the base station and have direct access to rich datasets. In this paper, we argue that leveraging the rich data on the edge server is much more beneficial than utilizing only user datasets. Specifi",
    "path": "papers/22/11/2211.04152.json",
    "total_tokens": 803,
    "translated_title": "使用三算子ADMM的联邦学习",
    "translated_abstract": "联邦学习（FL）已经成为分布式机器学习范式的一个实例，避免了在用户端生成数据的传输。尽管数据不被传输，边缘设备仍然必须处理有限的通信带宽、数据异构性和由于用户设备的有限计算资源引起的“straggler”效应。FedADMM是克服这些困难的一个著名方法，它基于经典的双算子共识交替方向乘数法（ADMM）。FL算法的一个普遍假设是，它们仅使用用户端数据而不使用边缘服务器上的数据来学习全局模型。然而，在边缘学习中，服务器预计会靠近基站并直接访问丰富的数据集。本文认为，利用边缘服务器上的丰富数据比仅利用用户数据更有益处。",
    "tldr": "联邦学习中引入了三算子ADMM算法，提出了利用边缘服务器丰富数据的优势，与仅使用用户数据相比有更大益处。",
    "en_tdlr": "Federated learning introduces a three-operator ADMM algorithm and highlights the advantages of leveraging rich data on the edge server over using only user data."
}