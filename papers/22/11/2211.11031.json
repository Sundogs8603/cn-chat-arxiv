{
    "title": "Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters. (arXiv:2211.11031v3 [cs.LG] UPDATED)",
    "abstract": "Deployed models decay over time due to shifting inputs, changing user needs, or emergent knowledge gaps. When harmful behaviors are identified, targeted edits are required. However, current model editors, which adjust specific behaviors of pre-trained models, degrade model performance over multiple edits. We propose GRACE, a Lifelong Model Editing method, which implements spot-fixes on streaming errors of a deployed model, ensuring minimal impact on unrelated inputs. GRACE writes new mappings into a pre-trained model's latent space, creating a discrete, local codebook of edits without altering model weights. This is the first method enabling thousands of sequential edits using only streaming errors. Our experiments on T5, BERT, and GPT models show GRACE's state-of-the-art performance in making and retaining edits, while generalizing to unseen inputs. Our code is available at https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace}.",
    "link": "http://arxiv.org/abs/2211.11031",
    "context": "Title: Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adapters. (arXiv:2211.11031v3 [cs.LG] UPDATED)\nAbstract: Deployed models decay over time due to shifting inputs, changing user needs, or emergent knowledge gaps. When harmful behaviors are identified, targeted edits are required. However, current model editors, which adjust specific behaviors of pre-trained models, degrade model performance over multiple edits. We propose GRACE, a Lifelong Model Editing method, which implements spot-fixes on streaming errors of a deployed model, ensuring minimal impact on unrelated inputs. GRACE writes new mappings into a pre-trained model's latent space, creating a discrete, local codebook of edits without altering model weights. This is the first method enabling thousands of sequential edits using only streaming errors. Our experiments on T5, BERT, and GPT models show GRACE's state-of-the-art performance in making and retaining edits, while generalizing to unseen inputs. Our code is available at https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace}.",
    "path": "papers/22/11/2211.11031.json",
    "total_tokens": 935,
    "translated_title": "GRACE：离散键值适配器实现的终身模型编辑",
    "translated_abstract": "部署的模型随时间推移会衰退，原因是输入的变化、用户需求不断改变、或由于出现知识空缺。当发现有害行为时，需要进行有针对性的编辑。然而，当前的模型编辑器在多次编辑中会降低模型的性能。我们提出了GRACE，一种终身模型编辑方法，它在部署模型的流式错误上实现了问题修补，确保对不相关的输入的影响最小化。GRACE将新的映射项写入预训练模型的潜在空间，创建了一个离散的、本地的编码本，而不会改变模型权重。这是第一种只使用流式错误实现数千个顺序编辑的方法。我们在T5、BERT和GPT模型上进行了实验，结果表明GRACE在进行并保留编辑方面的性能处于最先进水平，同时可以推广到未见过的输入。我们的代码可在https://www.github.com/thartvigsen/grace}{github.com/thartvigsen/grace中获得。",
    "tldr": "本论文提出了一种名为GRACE的方法来实现终身模型编辑，它通过在流式错误上执行目标编辑来修复部署模型的问题，生成一个离散、本地的编辑编码本，而不会改变模型权重，在进行数千个顺序编辑时表现为最先进的性能。"
}