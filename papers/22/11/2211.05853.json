{
    "title": "Measuring Reliability of Large Language Models through Semantic Consistency. (arXiv:2211.05853v2 [cs.CL] UPDATED)",
    "abstract": "While large pretrained language models (PLMs) demonstrate incredible fluency and performance on many natural language tasks, recent work has shown that well-performing PLMs are very sensitive to what prompts are feed into them. Even when prompts are semantically identical, language models may give very different answers. When considering safe and trustworthy deployments of PLMs we would like their outputs to be consistent under prompts that mean the same thing or convey the same intent. While some work has looked into how state-of-the-art PLMs address this need, they have been limited to only evaluating lexical equality of single- or multi-word answers and do not address consistency of generative text sequences. In order to understand consistency of PLMs under text generation settings, we develop a measure of semantic consistency that allows the comparison of open-ended text outputs. We implement several versions of this consistency metric to evaluate the performance of a number of PLM",
    "link": "http://arxiv.org/abs/2211.05853",
    "context": "Title: Measuring Reliability of Large Language Models through Semantic Consistency. (arXiv:2211.05853v2 [cs.CL] UPDATED)\nAbstract: While large pretrained language models (PLMs) demonstrate incredible fluency and performance on many natural language tasks, recent work has shown that well-performing PLMs are very sensitive to what prompts are feed into them. Even when prompts are semantically identical, language models may give very different answers. When considering safe and trustworthy deployments of PLMs we would like their outputs to be consistent under prompts that mean the same thing or convey the same intent. While some work has looked into how state-of-the-art PLMs address this need, they have been limited to only evaluating lexical equality of single- or multi-word answers and do not address consistency of generative text sequences. In order to understand consistency of PLMs under text generation settings, we develop a measure of semantic consistency that allows the comparison of open-ended text outputs. We implement several versions of this consistency metric to evaluate the performance of a number of PLM",
    "path": "papers/22/11/2211.05853.json",
    "total_tokens": 901,
    "translated_title": "通过语义一致性测量大型语言模型的可靠性。",
    "translated_abstract": "虽然大型预训练语言模型（PLMs）在许多自然语言处理任务中表现出极高的流畅性和性能，但最近的研究表明，表现良好的PLMs非常敏感，对于输入的提示非常敏感。即使提示在语义上是相同的，语言模型也可能给出非常不同的答案。当考虑PLMs的安全和可信赖部署时，我们希望它们在意思相同或表达相同意图的提示下的输出是一致的。虽然一些研究已经探讨了最先进的PLMs如何解决这个需求，但它们仅限于评估单个或多个单词答案的词汇等价性，而不涉及生成文本序列的一致性。为了理解在生成文本设置下PLMs的一致性，我们开发了一个语义一致性度量，允许比较开放式文本输出的一致性。我们实现了几个版本的一致性度量来评估多个PLM的性能。",
    "tldr": "本研究通过开发语义一致性度量来评估大型预训练语言模型的性能，以比较不同PLM的可靠性，保证其在相同或相似的提示下生成的输出一致。"
}