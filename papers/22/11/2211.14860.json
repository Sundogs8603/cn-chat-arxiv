{
    "title": "Foiling Explanations in Deep Neural Networks. (arXiv:2211.14860v2 [cs.CV] UPDATED)",
    "abstract": "Deep neural networks (DNNs) have greatly impacted numerous fields over the past decade. Yet despite exhibiting superb performance over many problems, their black-box nature still poses a significant challenge with respect to explainability. Indeed, explainable artificial intelligence (XAI) is crucial in several fields, wherein the answer alone -- sans a reasoning of how said answer was derived -- is of little value. This paper uncovers a troubling property of explanation methods for image-based DNNs: by making small visual changes to the input image -- hardly influencing the network's output -- we demonstrate how explanations may be arbitrarily manipulated through the use of evolution strategies. Our novel algorithm, AttaXAI, a model-agnostic, adversarial attack on XAI algorithms, only requires access to the output logits of a classifier and to the explanation map; these weak assumptions render our approach highly useful where real-world models and data are concerned. We compare our me",
    "link": "http://arxiv.org/abs/2211.14860",
    "context": "Title: Foiling Explanations in Deep Neural Networks. (arXiv:2211.14860v2 [cs.CV] UPDATED)\nAbstract: Deep neural networks (DNNs) have greatly impacted numerous fields over the past decade. Yet despite exhibiting superb performance over many problems, their black-box nature still poses a significant challenge with respect to explainability. Indeed, explainable artificial intelligence (XAI) is crucial in several fields, wherein the answer alone -- sans a reasoning of how said answer was derived -- is of little value. This paper uncovers a troubling property of explanation methods for image-based DNNs: by making small visual changes to the input image -- hardly influencing the network's output -- we demonstrate how explanations may be arbitrarily manipulated through the use of evolution strategies. Our novel algorithm, AttaXAI, a model-agnostic, adversarial attack on XAI algorithms, only requires access to the output logits of a classifier and to the explanation map; these weak assumptions render our approach highly useful where real-world models and data are concerned. We compare our me",
    "path": "papers/22/11/2211.14860.json",
    "total_tokens": 832,
    "translated_title": "深度神经网络中的解释方法破解",
    "translated_abstract": "深度神经网络的黑盒特性对于可解释性仍然存在重大挑战，因为仅仅获得其输出是不够有用的。本文发现了图像 DNN 的解释方法的一个令人担忧的属性：通过对输入图像进行微小的视觉更改，我们演示了如何通过进化策略任意操纵解释。我们提出了一个新的算法 AttaXAI，一个针对 XAI 算法的模型无关的敌对攻击，只需要访问分类器的输出信息和解释图，这些弱假设使我们的方法在实际应用中非常有用。",
    "tldr": "本文发现了解释图像 DNN 的一个令人担忧的属性：通过微小视觉更改，我们演示了解释可以通过进化策略任意操纵。我们提出了 AttaXAI，一个针对 XAI 算法的敌对攻击，可访问分类器输出信息和解释图，这使得我们的方法在实际应用中非常有用。",
    "en_tdlr": "This paper uncovers a troubling property of explanation methods for image-based DNNs: explanations may be arbitrarily manipulated through the use of evolution strategies. The authors propose AttaXAI, a model-agnostic, adversarial attack on XAI algorithms that only requires access to the output logits of a classifier and to the explanation map."
}