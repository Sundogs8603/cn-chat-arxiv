{
    "title": "SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation. (arXiv:2211.14813v2 [cs.CV] UPDATED)",
    "abstract": "Recently, the contrastive language-image pre-training, e.g., CLIP, has demonstrated promising results on various downstream tasks. The pre-trained model can capture enriched visual concepts for images by learning from a large scale of text-image data. However, transferring the learned visual knowledge to open-vocabulary semantic segmentation is still under-explored. In this paper, we propose a CLIP-based model named SegCLIP for the topic of open-vocabulary segmentation in an annotation-free manner. The SegCLIP achieves segmentation based on ViT and the main idea is to gather patches with learnable centers to semantic regions through training on text-image pairs. The gathering operation can dynamically capture the semantic groups, which can be used to generate the final segmentation results. We further propose a reconstruction loss on masked patches and a superpixel-based KL loss with pseudo-labels to enhance the visual representation. Experimental results show that our model achieves c",
    "link": "http://arxiv.org/abs/2211.14813",
    "context": "Title: SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation. (arXiv:2211.14813v2 [cs.CV] UPDATED)\nAbstract: Recently, the contrastive language-image pre-training, e.g., CLIP, has demonstrated promising results on various downstream tasks. The pre-trained model can capture enriched visual concepts for images by learning from a large scale of text-image data. However, transferring the learned visual knowledge to open-vocabulary semantic segmentation is still under-explored. In this paper, we propose a CLIP-based model named SegCLIP for the topic of open-vocabulary segmentation in an annotation-free manner. The SegCLIP achieves segmentation based on ViT and the main idea is to gather patches with learnable centers to semantic regions through training on text-image pairs. The gathering operation can dynamically capture the semantic groups, which can be used to generate the final segmentation results. We further propose a reconstruction loss on masked patches and a superpixel-based KL loss with pseudo-labels to enhance the visual representation. Experimental results show that our model achieves c",
    "path": "papers/22/11/2211.14813.json",
    "total_tokens": 849,
    "translated_title": "SegCLIP：基于可学习中心的补丁聚合用于开放词汇语义分割",
    "translated_abstract": "最近，对比语言-图像预训练 (如CLIP) 在各种下游任务上取得了很有希望的结果。预训练模型可以通过学习大量的文本-图像数据来捕捉图像中丰富的视觉概念。然而，将所学的视觉知识转移到开放词汇语义分割仍然不够充分。本文提出了一种基于CLIP的模型SegCLIP，用于无注释的开放词汇分割。SegCLIP利用可学习中心聚集补丁到语义区域通过文本-图像对的训练。聚集操作可以动态地捕捉语义组，用于产生最终的分割结果。我们还提出了一个对遮罩补丁的重构损失和一个基于超像素的KL损失，以增强视觉表示。实验结果表明，我们的模型实现了",
    "tldr": "SegCLIP是一种用于开放词汇语义分割的模型，通过聚集补丁到语义区域进行分割，具有动态捕捉语义组的特点。",
    "en_tdlr": "SegCLIP is a model for open-vocabulary semantic segmentation, which achieves segmentation by aggregating patches to semantic regions and dynamically captures semantic groups."
}