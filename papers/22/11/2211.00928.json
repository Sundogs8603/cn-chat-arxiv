{
    "title": "Neural Active Learning on Heteroskedastic Distributions. (arXiv:2211.00928v2 [cs.LG] UPDATED)",
    "abstract": "Models that can actively seek out the best quality training data hold the promise of more accurate, adaptable, and efficient machine learning. Active learning techniques often tend to prefer examples that are the most difficult to classify. While this works well on homogeneous datasets, we find that it can lead to catastrophic failures when performed on multiple distributions with different degrees of label noise or heteroskedasticity. These active learning algorithms strongly prefer to draw from the distribution with more noise, even if their examples have no informative structure (such as solid color images with random labels). To this end, we demonstrate the catastrophic failure of these active learning algorithms on heteroskedastic distributions and propose a fine-tuning-based approach to mitigate these failures. Further, we propose a new algorithm that incorporates a model difference scoring function for each data point to filter out the noisy examples and sample clean examples th",
    "link": "http://arxiv.org/abs/2211.00928",
    "context": "Title: Neural Active Learning on Heteroskedastic Distributions. (arXiv:2211.00928v2 [cs.LG] UPDATED)\nAbstract: Models that can actively seek out the best quality training data hold the promise of more accurate, adaptable, and efficient machine learning. Active learning techniques often tend to prefer examples that are the most difficult to classify. While this works well on homogeneous datasets, we find that it can lead to catastrophic failures when performed on multiple distributions with different degrees of label noise or heteroskedasticity. These active learning algorithms strongly prefer to draw from the distribution with more noise, even if their examples have no informative structure (such as solid color images with random labels). To this end, we demonstrate the catastrophic failure of these active learning algorithms on heteroskedastic distributions and propose a fine-tuning-based approach to mitigate these failures. Further, we propose a new algorithm that incorporates a model difference scoring function for each data point to filter out the noisy examples and sample clean examples th",
    "path": "papers/22/11/2211.00928.json",
    "total_tokens": 845,
    "translated_title": "异方差分布上的神经主动学习",
    "translated_abstract": "能够主动寻找最佳质量训练数据的模型承诺着更准确、适应性强和高效的机器学习。主动学习技术通常倾向于选择最难分类的例子。尽管这在同质数据集上效果良好，但我们发现在多个具有不同程度标签噪声或异方差性的分布上进行主动学习可能会导致灾难性失败。这些主动学习算法强烈倾向于从噪声更大的分布中选择，即使这些例子没有信息结构（例如具有随机标签的纯色图像）。为此，我们展示了这些主动学习算法在异方差分布上的灾难性失败，并提出了一种基于微调的方法来减轻这些失败。此外，我们还提出了一种新的算法，该算法为每个数据点引入了模型差异评分函数，用于去除噪声例子并采样清晰的例子。",
    "tldr": "这项研究展示了在异方差分布上进行神经主动学习可能导致灾难性失败，并提出了一种利用微调来减轻这种失败的方法。",
    "en_tdlr": "This research demonstrates that performing neural active learning on heteroskedastic distributions can lead to catastrophic failures, and proposes a fine-tuning-based approach to mitigate these failures."
}