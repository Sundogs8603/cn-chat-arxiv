{
    "title": "Transformers are Short Text Classifiers: A Study of Inductive Short Text Classifiers on Benchmarks and Real-world Datasets. (arXiv:2211.16878v3 [cs.CL] UPDATED)",
    "abstract": "Short text classification is a crucial and challenging aspect of Natural Language Processing. For this reason, there are numerous highly specialized short text classifiers. However, in recent short text research, State of the Art (SOTA) methods for traditional text classification, particularly the pure use of Transformers, have been unexploited. In this work, we examine the performance of a variety of short text classifiers as well as the top performing traditional text classifier. We further investigate the effects on two new real-world short text datasets in an effort to address the issue of becoming overly dependent on benchmark datasets with a limited number of characteristics. Our experiments unambiguously demonstrate that Transformers achieve SOTA accuracy on short text classification tasks, raising the question of whether specialized short text techniques are necessary.",
    "link": "http://arxiv.org/abs/2211.16878",
    "context": "Title: Transformers are Short Text Classifiers: A Study of Inductive Short Text Classifiers on Benchmarks and Real-world Datasets. (arXiv:2211.16878v3 [cs.CL] UPDATED)\nAbstract: Short text classification is a crucial and challenging aspect of Natural Language Processing. For this reason, there are numerous highly specialized short text classifiers. However, in recent short text research, State of the Art (SOTA) methods for traditional text classification, particularly the pure use of Transformers, have been unexploited. In this work, we examine the performance of a variety of short text classifiers as well as the top performing traditional text classifier. We further investigate the effects on two new real-world short text datasets in an effort to address the issue of becoming overly dependent on benchmark datasets with a limited number of characteristics. Our experiments unambiguously demonstrate that Transformers achieve SOTA accuracy on short text classification tasks, raising the question of whether specialized short text techniques are necessary.",
    "path": "papers/22/11/2211.16878.json",
    "total_tokens": 822,
    "translated_title": "Transformers是短文本分类器：基于基准和实际数据集的归纳短文本分类器研究",
    "translated_abstract": "短文本分类是自然语言处理中关键且具有挑战性的任务。因此，存在许多高度专门化的短文本分类器。然而，在最近的短文本研究中，传统文本分类的最先进方法，特别是纯粹使用Transformers的方法，尚未被充分利用。在本研究中，我们检查了多种短文本分类器的性能，以及排名靠前的传统文本分类器。我们进一步研究了两个新的实际短文本数据集对模型的影响，以解决对于仅具有有限特征的基准数据集过度依赖的问题。我们的实验证明，Transformers在短文本分类任务中实现了最先进的准确率，引发了是否需要专门的短文本技术的问题。",
    "tldr": "本研究探讨了短文本分类器的性能，并发现Transformers在短文本分类任务中达到了最先进的准确率，引发了是否需要专门的短文本技术的讨论。"
}