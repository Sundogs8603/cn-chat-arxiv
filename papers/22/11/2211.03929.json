{
    "title": "Comparative layer-wise analysis of self-supervised speech models. (arXiv:2211.03929v3 [cs.CL] UPDATED)",
    "abstract": "Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose laye",
    "link": "http://arxiv.org/abs/2211.03929",
    "context": "Title: Comparative layer-wise analysis of self-supervised speech models. (arXiv:2211.03929v3 [cs.CL] UPDATED)\nAbstract: Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose laye",
    "path": "papers/22/11/2211.03929.json",
    "total_tokens": 920,
    "translated_title": "自监督语音模型的逐层比较分析",
    "translated_abstract": "在过去几年中，许多不同预训练目标、输入形式和预训练数据的自监督语音模型被提出。尽管在下游任务中取得了惊人的成功，我们仍然对这些模型编码的属性及其差异了解有限。在本研究中，我们使用基于规范相关分析(CCA)的轻量级分析工具，检查了多个最近模型的中间表示。具体而言，我们测量了单个层次中编码的声学、语音和单词级属性，发现这些属性在不同模型中的层次演变方式不同，且变化与预训练目标的选择相关。我们通过比较属性趋势和语音识别和口语理解任务的性能，进一步研究了我们的分析在下游任务中的应用价值。发现CCA趋势为选择层次提供了可靠的指导。",
    "tldr": "本论文通过比较不同自监督语音模型的逐层中间表示，发现了不同模型在编码声学、语音和单词级属性上的差异，并发现这些差异与预训练目标的选择相关。通过比较属性趋势和语音识别和口语理解任务的性能，我们发现CCA趋势为选择层次提供了可靠的指导。",
    "en_tdlr": "This paper compares the intermediate representation of different self-supervised speech models layer by layer and finds differences in the encoded acoustic, phonetic, and word-level properties, which are related to the choice of pre-training objectives. The study shows that the CCA trends provide reliable guidance for choosing the appropriate layers by comparing the property trends with performance on speech recognition and spoken language understanding tasks."
}