{
    "title": "CACTO: Continuous Actor-Critic with Trajectory Optimization -- Towards global optimality. (arXiv:2211.06625v3 [cs.RO] UPDATED)",
    "abstract": "This paper presents a novel algorithm for the continuous control of dynamical systems that combines Trajectory Optimization (TO) and Reinforcement Learning (RL) in a single framework. The motivations behind this algorithm are the two main limitations of TO and RL when applied to continuous nonlinear systems to minimize a non-convex cost function. Specifically, TO can get stuck in poor local minima when the search is not initialized close to a \"good\" minimum. On the other hand, when dealing with continuous state and control spaces, the RL training process may be excessively long and strongly dependent on the exploration strategy. Thus, our algorithm learns a \"good\" control policy via TO-guided RL policy search that, when used as initial guess provider for TO, makes the trajectory optimization process less prone to converge to poor local optima. Our method is validated on several reaching problems featuring non-convex obstacle avoidance with different dynamical systems, including a car m",
    "link": "http://arxiv.org/abs/2211.06625",
    "context": "Title: CACTO: Continuous Actor-Critic with Trajectory Optimization -- Towards global optimality. (arXiv:2211.06625v3 [cs.RO] UPDATED)\nAbstract: This paper presents a novel algorithm for the continuous control of dynamical systems that combines Trajectory Optimization (TO) and Reinforcement Learning (RL) in a single framework. The motivations behind this algorithm are the two main limitations of TO and RL when applied to continuous nonlinear systems to minimize a non-convex cost function. Specifically, TO can get stuck in poor local minima when the search is not initialized close to a \"good\" minimum. On the other hand, when dealing with continuous state and control spaces, the RL training process may be excessively long and strongly dependent on the exploration strategy. Thus, our algorithm learns a \"good\" control policy via TO-guided RL policy search that, when used as initial guess provider for TO, makes the trajectory optimization process less prone to converge to poor local optima. Our method is validated on several reaching problems featuring non-convex obstacle avoidance with different dynamical systems, including a car m",
    "path": "papers/22/11/2211.06625.json",
    "total_tokens": 851,
    "translated_title": "CACTO：连续动态系统的轨迹优化Actor-Critic算法——走向全局最优。（arXiv:2211.06625v3 [cs.RO] UPDATED）",
    "translated_abstract": "本文提出了一种新的算法，将轨迹优化（TO）和强化学习（RL）结合在一个框架中，用于连续动态系统的控制。该算法的动机是TO和RL在应用于非凸代价函数的连续非线性系统时存在的两个主要限制。本文方法利用TO引导的RL策略搜索来学习“好”的控制策略，并用作TO的初始猜测提供者，使轨迹优化过程不容易收敛到贫乏的局部最优解。我们的方法在几个不同的动态系统，包括具有非凸障碍物避免的到达问题上进行了验证。",
    "tldr": "本研究提出了一种连续动态系统的控制算法，结合了轨迹优化和强化学习，通过TO引导的RL策略搜索学习控制策略来避免在控制过程中陷入贫乏的局部最优解。",
    "en_tdlr": "This paper proposes an algorithm for the continuous control of dynamic systems, which combines trajectory optimization (TO) and reinforcement learning (RL) and learns a good control policy through TO-guided RL policy search, avoiding poor local optimal solutions during control processes."
}