{
    "title": "Towards Abstractive Timeline Summarisation using Preference-based Reinforcement Learning. (arXiv:2211.07596v2 [cs.LG] UPDATED)",
    "abstract": "This paper introduces a novel pipeline for summarising timelines of events reported by multiple news sources. Transformer-based models for abstractive summarisation generate coherent and concise summaries of long documents but can fail to outperform established extractive methods on specialised tasks such as timeline summarisation (TLS). While extractive summaries are more faithful to their sources, they may be less readable and contain redundant or unnecessary information. This paper proposes a preference-based reinforcement learning (PBRL) method for adapting pretrained abstractive summarisers to TLS, which can overcome the drawbacks of extractive timeline summaries. We define a compound reward function that learns from keywords of interest and pairwise preference labels, which we use to fine-tune a pretrained abstractive summariser via offline reinforcement learning. We carry out both automated and human evaluation on three datasets, finding that our method outperforms a comparable ",
    "link": "http://arxiv.org/abs/2211.07596",
    "context": "Title: Towards Abstractive Timeline Summarisation using Preference-based Reinforcement Learning. (arXiv:2211.07596v2 [cs.LG] UPDATED)\nAbstract: This paper introduces a novel pipeline for summarising timelines of events reported by multiple news sources. Transformer-based models for abstractive summarisation generate coherent and concise summaries of long documents but can fail to outperform established extractive methods on specialised tasks such as timeline summarisation (TLS). While extractive summaries are more faithful to their sources, they may be less readable and contain redundant or unnecessary information. This paper proposes a preference-based reinforcement learning (PBRL) method for adapting pretrained abstractive summarisers to TLS, which can overcome the drawbacks of extractive timeline summaries. We define a compound reward function that learns from keywords of interest and pairwise preference labels, which we use to fine-tune a pretrained abstractive summariser via offline reinforcement learning. We carry out both automated and human evaluation on three datasets, finding that our method outperforms a comparable ",
    "path": "papers/22/11/2211.07596.json",
    "total_tokens": 849,
    "translated_title": "使用基于偏好的强化学习实现抽象时间线摘要",
    "translated_abstract": "本文介绍了一种新颖的流程，用于总结多个新闻来源报道的时间线。基于Transformer的抽象摘要模型可以生成连贯且简洁的长文档摘要，但在时间线摘要（TLS）等特定任务上可能无法超越已建立的抽取方法。虽然抽取摘要更忠实于来源，但可能不太易读且包含冗余或不必要的信息。本文提出了一种基于偏好的强化学习（PBRL）方法，用于将预训练的抽象摘要器适应于TLS，可以克服抽取时间线摘要的缺点。我们定义了一个复合奖励函数，通过关键词和偏好标签学习，并将其用于通过线下强化学习对预训练的抽象摘要器进行微调。我们对三个数据集进行了自动化和人工评估，发现我们的方法优于可比较的方法。",
    "tldr": "本文提出了一种基于偏好的强化学习方法，在抽象摘要的基础上优化时间线摘要的质量和可读性，通过自动化和人工评估验证了方法的有效性。",
    "en_tdlr": "This paper proposes a preference-based reinforcement learning method to improve the quality and readability of timeline summarization based on the abstract summarization. The effectiveness of the method is verified through automated and human evaluations."
}