{
    "title": "ViT-CX: Causal Explanation of Vision Transformers. (arXiv:2211.03064v3 [cs.CV] UPDATED)",
    "abstract": "Despite the popularity of Vision Transformers (ViTs) and eXplainable AI (XAI), only a few explanation methods have been designed specially for ViTs thus far. They mostly use attention weights of the [CLS] token on patch embeddings and often produce unsatisfactory saliency maps. This paper proposes a novel method for explaining ViTs called ViT-CX. It is based on patch embeddings, rather than attentions paid to them, and their causal impacts on the model output. Other characteristics of ViTs such as causal overdetermination are also considered in the design of ViT-CX. The empirical results show that ViT-CX produces more meaningful saliency maps and does a better job revealing all important evidence for the predictions than previous methods. The explanation generated by ViT-CX also shows significantly better faithfulness to the model. The codes and appendix are available at https://github.com/vaynexie/CausalX-ViT.",
    "link": "http://arxiv.org/abs/2211.03064",
    "context": "Title: ViT-CX: Causal Explanation of Vision Transformers. (arXiv:2211.03064v3 [cs.CV] UPDATED)\nAbstract: Despite the popularity of Vision Transformers (ViTs) and eXplainable AI (XAI), only a few explanation methods have been designed specially for ViTs thus far. They mostly use attention weights of the [CLS] token on patch embeddings and often produce unsatisfactory saliency maps. This paper proposes a novel method for explaining ViTs called ViT-CX. It is based on patch embeddings, rather than attentions paid to them, and their causal impacts on the model output. Other characteristics of ViTs such as causal overdetermination are also considered in the design of ViT-CX. The empirical results show that ViT-CX produces more meaningful saliency maps and does a better job revealing all important evidence for the predictions than previous methods. The explanation generated by ViT-CX also shows significantly better faithfulness to the model. The codes and appendix are available at https://github.com/vaynexie/CausalX-ViT.",
    "path": "papers/22/11/2211.03064.json",
    "total_tokens": 898,
    "translated_title": "ViT-CX: 视觉Transformer的因果解释",
    "translated_abstract": "尽管视觉Transformer(ViTs)和可解释人工智能(XAI)很受欢迎，但目前为止只有少数专门为ViTs设计的解释方法。它们主要使用对补丁嵌入的[CLS]令牌的注意权重，通常会产生不令人满意的显著性图。本文提出了一种称为ViT-CX的解释ViTs的新方法。它基于补丁嵌入，而不是注重它们的注意力，以及它们对模型输出的因果影响。ViTs的其他特性，如因果过决定性，在ViT-CX的设计中也考虑了。实证结果表明，ViT-CX产生的显著性图更有意义，能更好地揭示预测的所有重要证据比以前的方法。ViT-CX产生的解释也显示出与模型显著更好的忠实性。代码和附录可在https://github.com/vaynexie/CausalX-ViT上找到。",
    "tldr": "本文提出了一种新方法ViT-CX，通过基于补丁嵌入的因果影响来解释视觉Transformer(ViTs)，并考虑了ViTs的因果过决定性等特性，相比以前的方法更有意义且更忠实于模型。",
    "en_tdlr": "This paper proposes a novel method called ViT-CX for explaining Vision Transformers (ViTs) based on the causal impacts of patch embeddings, while considering characteristics such as causal overdetermination. The method produces more meaningful and faithful saliency maps than previous methods."
}