{
    "title": "GENNAPE: Towards Generalized Neural Architecture Performance Estimators. (arXiv:2211.17226v2 [cs.LG] UPDATED)",
    "abstract": "Predicting neural architecture performance is a challenging task and is crucial to neural architecture design and search. Existing approaches either rely on neural performance predictors which are limited to modeling architectures in a predefined design space involving specific sets of operators and connection rules, and cannot generalize to unseen architectures, or resort to zero-cost proxies which are not always accurate. In this paper, we propose GENNAPE, a Generalized Neural Architecture Performance Estimator, which is pretrained on open neural architecture benchmarks, and aims to generalize to completely unseen architectures through combined innovations in network representation, contrastive pretraining, and fuzzy clustering-based predictor ensemble. Specifically, GENNAPE represents a given neural network as a Computation Graph (CG) of atomic operations which can model an arbitrary architecture. It first learns a graph encoder via Contrastive Learning to encourage network separati",
    "link": "http://arxiv.org/abs/2211.17226",
    "context": "Title: GENNAPE: Towards Generalized Neural Architecture Performance Estimators. (arXiv:2211.17226v2 [cs.LG] UPDATED)\nAbstract: Predicting neural architecture performance is a challenging task and is crucial to neural architecture design and search. Existing approaches either rely on neural performance predictors which are limited to modeling architectures in a predefined design space involving specific sets of operators and connection rules, and cannot generalize to unseen architectures, or resort to zero-cost proxies which are not always accurate. In this paper, we propose GENNAPE, a Generalized Neural Architecture Performance Estimator, which is pretrained on open neural architecture benchmarks, and aims to generalize to completely unseen architectures through combined innovations in network representation, contrastive pretraining, and fuzzy clustering-based predictor ensemble. Specifically, GENNAPE represents a given neural network as a Computation Graph (CG) of atomic operations which can model an arbitrary architecture. It first learns a graph encoder via Contrastive Learning to encourage network separati",
    "path": "papers/22/11/2211.17226.json",
    "total_tokens": 851,
    "translated_title": "GENNAPE：面向通用神经架构性能估计的方法",
    "translated_abstract": "预测神经网络架构性能是一项具有挑战性且关键的任务，对于神经架构设计与搜索十分重要。现有方法要么依赖神经网络性能预测器，在预先定义好的设计空间内建模架构，难以推广到未知的架构，要么采用零成本代理，但不总是准确。为此，本文提出了GENNAPE，一种预先训练于开放神经架构基准的通用神经架构性能估计器，旨在通过网络表示、对比预训练和基于模糊聚类的预测器集成等创新实现对完全未知架构的泛化。具体来说，GENNAPE将给定的神经网络表示为原子操作的计算图，可以模拟任意架构。它首先通过对比学习学习图表编码器，以鼓励网络分离",
    "tldr": "本文提出了一个名为GENNAPE的通用神经架构性能估计器，可以泛化到完全未知的架构中，通过创新的网络表示、对比预训练和基于模糊聚类的预测器集成方法实现。",
    "en_tdlr": "This paper proposes a Generalized Neural Architecture Performance Estimator (GENNAPE) that can generalize to completely unseen architectures, achieved by innovative network representation, contrastive pretraining, and fuzzy clustering-based predictor ensemble."
}