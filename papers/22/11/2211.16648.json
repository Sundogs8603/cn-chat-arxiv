{
    "title": "COMET: A Comprehensive Cluster Design Methodology for Distributed Deep Learning Training",
    "abstract": "arXiv:2211.16648v2 Announce Type: replace-cross  Abstract: Modern Deep Learning (DL) models have grown to sizes requiring massive clusters of specialized, high-end nodes to train. Designing such clusters to maximize both performance and utilization--to amortize their steep cost--is a challenging task requiring careful balance of compute, memory, and network resources. Moreover, a plethora of each model's tuning knobs drastically affect the performance, with optimal values often depending on the underlying cluster's characteristics, which necessitates a complex cluster-workload co-design process. To facilitate the design space exploration of such massive DL training clusters, we introduce COMET, a holistic cluster design methodology and workflow to jointly study the impact of parallelization strategies and key cluster resource provisioning on the performance of distributed DL training. We develop a step-by-step process to establish a reusable and flexible methodology, and demonstrate it",
    "link": "https://arxiv.org/abs/2211.16648",
    "context": "Title: COMET: A Comprehensive Cluster Design Methodology for Distributed Deep Learning Training\nAbstract: arXiv:2211.16648v2 Announce Type: replace-cross  Abstract: Modern Deep Learning (DL) models have grown to sizes requiring massive clusters of specialized, high-end nodes to train. Designing such clusters to maximize both performance and utilization--to amortize their steep cost--is a challenging task requiring careful balance of compute, memory, and network resources. Moreover, a plethora of each model's tuning knobs drastically affect the performance, with optimal values often depending on the underlying cluster's characteristics, which necessitates a complex cluster-workload co-design process. To facilitate the design space exploration of such massive DL training clusters, we introduce COMET, a holistic cluster design methodology and workflow to jointly study the impact of parallelization strategies and key cluster resource provisioning on the performance of distributed DL training. We develop a step-by-step process to establish a reusable and flexible methodology, and demonstrate it",
    "path": "papers/22/11/2211.16648.json",
    "total_tokens": 837,
    "translated_title": "COMET：用于分布式深度学习训练的全面集群设计方法论",
    "translated_abstract": "现代深度学习（DL）模型已经发展到需要大规模专门的、高端节点进行训练的大小。设计这样的集群以最大限度地提高性能和利用率--以摊销其高昂成本--是一项具有挑战性的任务，需要仔细平衡计算、内存和网络资源。此外，每个模型的众多调整旋钮极大地影响性能，最佳值往往取决于底层集群的特征，这要求进行复杂的集群-工作负载协同设计过程。为了促进这些大规模DL训练集群的设计空间探索，我们引入了COMET，这是一种综合的集群设计方法和工作流程，用于共同研究并行化策略和关键集群资源配置对分布式DL训练性能的影响。我们开发了一个逐步的过程，建立一种可重用和灵活的方法论，并加以演示。",
    "tldr": "COMET提出了一种全面的集群设计方法论，用于研究并行化策略和关键集群资源配置对分布式DL训练性能的影响"
}