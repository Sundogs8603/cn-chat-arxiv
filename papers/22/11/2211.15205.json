{
    "title": "CIM: Constrained Intrinsic Motivation for Sparse-Reward Continuous Control. (arXiv:2211.15205v2 [cs.LG] UPDATED)",
    "abstract": "Intrinsic motivation is a promising exploration technique for solving reinforcement learning tasks with sparse or absent extrinsic rewards. There exist two technical challenges in implementing intrinsic motivation: 1) how to design a proper intrinsic objective to facilitate efficient exploration; and 2) how to combine the intrinsic objective with the extrinsic objective to help find better solutions. In the current literature, the intrinsic objectives are all designed in a task-agnostic manner and combined with the extrinsic objective via simple addition (or used by itself for reward-free pre-training). In this work, we show that these designs would fail in typical sparse-reward continuous control tasks. To address the problem, we propose Constrained Intrinsic Motivation (CIM) to leverage readily attainable task priors to construct a constrained intrinsic objective, and at the same time, exploit the Lagrangian method to adaptively balance the intrinsic and extrinsic objectives via a si",
    "link": "http://arxiv.org/abs/2211.15205",
    "context": "Title: CIM: Constrained Intrinsic Motivation for Sparse-Reward Continuous Control. (arXiv:2211.15205v2 [cs.LG] UPDATED)\nAbstract: Intrinsic motivation is a promising exploration technique for solving reinforcement learning tasks with sparse or absent extrinsic rewards. There exist two technical challenges in implementing intrinsic motivation: 1) how to design a proper intrinsic objective to facilitate efficient exploration; and 2) how to combine the intrinsic objective with the extrinsic objective to help find better solutions. In the current literature, the intrinsic objectives are all designed in a task-agnostic manner and combined with the extrinsic objective via simple addition (or used by itself for reward-free pre-training). In this work, we show that these designs would fail in typical sparse-reward continuous control tasks. To address the problem, we propose Constrained Intrinsic Motivation (CIM) to leverage readily attainable task priors to construct a constrained intrinsic objective, and at the same time, exploit the Lagrangian method to adaptively balance the intrinsic and extrinsic objectives via a si",
    "path": "papers/22/11/2211.15205.json",
    "total_tokens": 1030,
    "translated_title": "CIM：基于约束的内在动机方法应对稀疏奖励连续控制问题",
    "translated_abstract": "内在动机是一种有前途的探索技术，可用于解决具有稀疏或没有显式奖励的强化学习任务。内在动机的设计存在两个技术难点：1）如何设计适当的内在目标来促进有效探索；2）如何与显式奖励进行有效整合，以帮助找到更好的解决方案。在目前的文献中，内在目标都是以任务无关的方式设计，并通过简单的加法与显式奖励进行整合（或在无奖励的预训练中单独使用）。在本文中，我们表明这些设计在典型的稀疏奖励连续控制任务中将会失败。为了应对这个问题，我们提出了基于约束的内在动机（CIM）方法，利用现有可获得的任务先验信息来构建约束的内在目标，并利用Lagrangian方法通过简单、高效和原则性的框架适应性地平衡内在和显式目标。我们在各种具有挑战性的稀疏奖励连续控制任务上评估了CIM，并展示了其明显优于现有内在动机方法的性能。",
    "tldr": "本文提出了一种基于约束的内在动机方法（CIM），它利用任务先验信息构建内在目标，并适应性地平衡内在和显式目标。在稀疏奖励连续控制任务上，CIM显著优于现有内在动机方法。",
    "en_tdlr": "The paper proposes a constrained intrinsic motivation method (CIM) that leverages task priors to construct intrinsic objectives and adaptively balances intrinsic and extrinsic objectives using Lagrangian method. CIM significantly outperforms existing intrinsic motivation methods in sparse-reward continuous control tasks."
}