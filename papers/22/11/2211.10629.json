{
    "title": "Unifying Label-inputted Graph Neural Networks with Deep Equilibrium Models. (arXiv:2211.10629v2 [cs.LG] UPDATED)",
    "abstract": "The success of Graph Neural Networks (GNN) in learning on non-Euclidean data arouses many subtopics, such as Label-inputted GNN (LGNN) and Implicit GNN (IGNN). LGNN, explicitly inputting supervising information (a.k.a. labels) in GNN, integrates label propagation to achieve superior performance, but with the dilemma between its propagating distance and adaptiveness. IGNN, outputting an equilibrium point by iterating its network infinite times, exploits information in the entire graph to capture long-range dependencies, but with its network constrained to guarantee the existence of the equilibrium. This work unifies the two subdomains by interpreting LGNN in the theory of IGNN and reducing prevailing LGNNs to the form of IGNN. The unification facilitates the exchange between the two subdomains and inspires more studies. Specifically, implicit differentiation of IGNN is introduced to LGNN to differentiate its infinite-range label propagation with constant memory, making the propagation b",
    "link": "http://arxiv.org/abs/2211.10629",
    "context": "Title: Unifying Label-inputted Graph Neural Networks with Deep Equilibrium Models. (arXiv:2211.10629v2 [cs.LG] UPDATED)\nAbstract: The success of Graph Neural Networks (GNN) in learning on non-Euclidean data arouses many subtopics, such as Label-inputted GNN (LGNN) and Implicit GNN (IGNN). LGNN, explicitly inputting supervising information (a.k.a. labels) in GNN, integrates label propagation to achieve superior performance, but with the dilemma between its propagating distance and adaptiveness. IGNN, outputting an equilibrium point by iterating its network infinite times, exploits information in the entire graph to capture long-range dependencies, but with its network constrained to guarantee the existence of the equilibrium. This work unifies the two subdomains by interpreting LGNN in the theory of IGNN and reducing prevailing LGNNs to the form of IGNN. The unification facilitates the exchange between the two subdomains and inspires more studies. Specifically, implicit differentiation of IGNN is introduced to LGNN to differentiate its infinite-range label propagation with constant memory, making the propagation b",
    "path": "papers/22/11/2211.10629.json",
    "total_tokens": 753,
    "translated_title": "用深度均衡模型统一标签输入图神经网络",
    "translated_abstract": "图神经网络在学习非欧几里得数据方面的成功引发了许多子课题，例如标签输入的GNN(LGNN)和隐式GNN(IGNN)。本文通过将LGNN解释为IGNN理论并将流行的LGNN归约为IGNN的形式来统一这两个子域。该统一简化了两个子域之间的交流并启发了更多研究。具体来说，介绍了IGNN的隐式微分到LGNN中，以常数内存微分其无限范围的标签传播，使传播成为可行的选择。",
    "tldr": "本文将标签输入的GNN和隐式GNN统一起来，并提出了一种IGNN中的隐式微分方法，使得标签无限传播变得可行。",
    "en_tdlr": "This paper unifies Label-inputted Graph Neural Networks (LGNN) and Implicit GNN (IGNN), and proposes an implicit differentiation method from IGNN to LGNN to enable infinite-range label propagation with constant memory."
}