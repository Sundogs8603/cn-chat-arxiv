{
    "title": "Leveraging per Image-Token Consistency for Vision-Language Pre-training. (arXiv:2211.15398v2 [cs.CV] UPDATED)",
    "abstract": "Most existing vision-language pre-training (VLP) approaches adopt cross-modal masked language modeling (CMLM) to learn vision-language associations. However, we find that CMLM is insufficient for this purpose according to our observations: (1) Modality bias: a considerable amount of masked tokens in CMLM can be recovered with only the language information, ignoring the visual inputs. (2) Under-utilization of the unmasked tokens: CMLM primarily focuses on the masked token but it cannot simultaneously leverage other tokens to learn vision-language associations. To handle those limitations, we propose EPIC (lEveraging Per Image-Token Consistency for vision-language pre-training). In EPIC, for each image-sentence pair, we mask tokens that are salient to the image (i.e., Saliency-based Masking Strategy) and replace them with alternatives sampled from a language model (i.e., Inconsistent Token Generation Procedure), and then the model is required to determine for each token in the sentence w",
    "link": "http://arxiv.org/abs/2211.15398",
    "context": "Title: Leveraging per Image-Token Consistency for Vision-Language Pre-training. (arXiv:2211.15398v2 [cs.CV] UPDATED)\nAbstract: Most existing vision-language pre-training (VLP) approaches adopt cross-modal masked language modeling (CMLM) to learn vision-language associations. However, we find that CMLM is insufficient for this purpose according to our observations: (1) Modality bias: a considerable amount of masked tokens in CMLM can be recovered with only the language information, ignoring the visual inputs. (2) Under-utilization of the unmasked tokens: CMLM primarily focuses on the masked token but it cannot simultaneously leverage other tokens to learn vision-language associations. To handle those limitations, we propose EPIC (lEveraging Per Image-Token Consistency for vision-language pre-training). In EPIC, for each image-sentence pair, we mask tokens that are salient to the image (i.e., Saliency-based Masking Strategy) and replace them with alternatives sampled from a language model (i.e., Inconsistent Token Generation Procedure), and then the model is required to determine for each token in the sentence w",
    "path": "papers/22/11/2211.15398.json",
    "total_tokens": 958,
    "translated_title": "利用图像-标记一致性进行视觉-语言预训练",
    "translated_abstract": "大多数现有的视觉-语言预训练（VLP）方法采用跨模态掩码语言建模（CMLM）来学习视觉-语言关联。然而，根据我们的观察，我们发现CMLM对于此目的来说是不足够的：（1）模态偏差：CMLM中相当数量的掩码标记可以仅通过语言信息恢复，忽略了视觉输入。（2）未被充分利用的未被掩码标记：CMLM主要关注被掩码的标记，但不能同时利用其他标记来学习视觉-语言关联。为了解决这些限制，我们提出了 EPIC（利用图像-标记一致性进行视觉-语言预训练）。在EPIC中，对于每个图像-句子对，我们掩盖与图像相关的标记（即基于显著性的掩码策略），并用从语言模型中采样的替代标记（即不一致的标记生成过程）进行替换，然后模型需要确定每个标记在句子中的位置。",
    "tldr": "EPIC 提出了一种利用图像-标记一致性的视觉-语言预训练方法，通过基于显著性的掩码策略和不一致的标记生成过程来克服CMLM的限制，并增强了视觉-语言关联的学习能力。"
}