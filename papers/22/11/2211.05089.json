{
    "title": "Sparse Bayesian Lasso via a Variable-Coefficient $\\ell_1$ Penalty. (arXiv:2211.05089v3 [stat.ME] UPDATED)",
    "abstract": "Modern statistical learning algorithms are capable of amazing flexibility, but struggle with interpretability. One possible solution is sparsity: making inference such that many of the parameters are estimated as being identically 0, which may be imposed through the use of nonsmooth penalties such as the $\\ell_1$ penalty. However, the $\\ell_1$ penalty introduces significant bias when high sparsity is desired. In this article, we retain the $\\ell_1$ penalty, but define learnable penalty weights $\\lambda_p$ endowed with hyperpriors. We start the article by investigating the optimization problem this poses, developing a proximal operator associated with the $\\ell_1$ norm. We then study the theoretical properties of this variable-coefficient $\\ell_1$ penalty in the context of penalized likelihood. Next, we investigate application of this penalty to Variational Bayes, developing a model we call the Sparse Bayesian Lasso which allows for behavior qualitatively like Lasso regression to be app",
    "link": "http://arxiv.org/abs/2211.05089",
    "context": "Title: Sparse Bayesian Lasso via a Variable-Coefficient $\\ell_1$ Penalty. (arXiv:2211.05089v3 [stat.ME] UPDATED)\nAbstract: Modern statistical learning algorithms are capable of amazing flexibility, but struggle with interpretability. One possible solution is sparsity: making inference such that many of the parameters are estimated as being identically 0, which may be imposed through the use of nonsmooth penalties such as the $\\ell_1$ penalty. However, the $\\ell_1$ penalty introduces significant bias when high sparsity is desired. In this article, we retain the $\\ell_1$ penalty, but define learnable penalty weights $\\lambda_p$ endowed with hyperpriors. We start the article by investigating the optimization problem this poses, developing a proximal operator associated with the $\\ell_1$ norm. We then study the theoretical properties of this variable-coefficient $\\ell_1$ penalty in the context of penalized likelihood. Next, we investigate application of this penalty to Variational Bayes, developing a model we call the Sparse Bayesian Lasso which allows for behavior qualitatively like Lasso regression to be app",
    "path": "papers/22/11/2211.05089.json",
    "total_tokens": 972,
    "translated_title": "变系数$\\ell_1$惩罚的稀疏贝叶斯Lasso。",
    "translated_abstract": "现代统计学习算法具有惊人的灵活性，但解释性较差。稀疏性是一种可能的解决方案：通过估计许多参数为0，可以通过使用不光滑的$\\ell_1$惩罚来实现这一点。然而，当需要高度稀疏性时，$\\ell_1$惩罚会引入显着的偏差。在本文中，我们保留了$\\ell_1$惩罚，但定义了可学习的惩罚权重$\\lambda_p$并赋予了超先验知识。我们首先研究了这个优化问题，并开发了与$\\ell_1$范数相关的近端算子。然后，我们在惩罚似然的背景下研究了这个变系数$\\ell_1$惩罚的理论性质。接下来，我们研究了将该惩罚应用于变分贝叶斯的方法，开发了一种模型，称为稀疏贝叶斯Lasso，允许表现出类似于Lasso回归的行为。",
    "tldr": "本文提出一种解决高度稀疏性的统计学习算法，即变系数$\\ell_1$惩罚的稀疏贝叶斯Lasso，并通过定义可学习的惩罚权重$\\lambda_p$及超先验知识来达到目的。",
    "en_tdlr": "This paper proposes a statistical learning algorithm, Sparse Bayesian Lasso via a Variable-Coefficient $\\ell_1$ Penalty, which solves the problem of high sparsity through the use of a variable-coefficient $\\ell_1$ penalty and by defining learnable penalty weights $\\lambda_p$ endowed with hyperpriors."
}