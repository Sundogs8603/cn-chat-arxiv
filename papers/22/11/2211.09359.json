{
    "title": "How to Fine-Tune Vision Models with SGD. (arXiv:2211.09359v2 [cs.CV] UPDATED)",
    "abstract": "SGD and AdamW are the two most used optimizers for fine-tuning large neural networks in computer vision. When the two methods perform the same, SGD is preferable because it uses less memory (12 bytes/parameter with momentum and 8 bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite of downstream tasks, especially those with distribution shifts, we find that fine-tuning with AdamW performs substantially better than SGD on modern Vision Transformer and ConvNeXt models. We find that large gaps in performance between SGD and AdamW occur when the fine-tuning gradients in the first \"embedding\" layer are much larger than in the rest of the model. Our analysis suggests an easy fix that works consistently across datasets and models: freezing the embedding layer (less than 1% of the parameters) leads to SGD with or without momentum performing slightly better than AdamW while using less memory (e.g., on ViT-L, SGD uses 33% less GPU memory). Our insights result in state-of",
    "link": "http://arxiv.org/abs/2211.09359",
    "context": "Title: How to Fine-Tune Vision Models with SGD. (arXiv:2211.09359v2 [cs.CV] UPDATED)\nAbstract: SGD and AdamW are the two most used optimizers for fine-tuning large neural networks in computer vision. When the two methods perform the same, SGD is preferable because it uses less memory (12 bytes/parameter with momentum and 8 bytes/parameter without) than AdamW (16 bytes/parameter). However, on a suite of downstream tasks, especially those with distribution shifts, we find that fine-tuning with AdamW performs substantially better than SGD on modern Vision Transformer and ConvNeXt models. We find that large gaps in performance between SGD and AdamW occur when the fine-tuning gradients in the first \"embedding\" layer are much larger than in the rest of the model. Our analysis suggests an easy fix that works consistently across datasets and models: freezing the embedding layer (less than 1% of the parameters) leads to SGD with or without momentum performing slightly better than AdamW while using less memory (e.g., on ViT-L, SGD uses 33% less GPU memory). Our insights result in state-of",
    "path": "papers/22/11/2211.09359.json",
    "total_tokens": 1006,
    "translated_title": "如何使用 SGD 调整视觉模型",
    "translated_abstract": "SGD 和 AdamW 是计算机视觉中用于调整大型神经网络的两种最常用的优化器。当这两种方法表现相同时，SGD 更受青睐，因为它比 AdamW 使用更少的内存（具有动量时，每个参数占用 12 字节，而不具有动量时每个参数占用 8 字节），而 AdamW 则需要更多的内存（每个参数占用 16 字节）。然而，在一系列下游任务中，尤其是在具有分布偏移的任务中，我们发现在现代的 Vision Transformer 和 ConvNeXt 模型上，使用 AdamW 进行微调的表现明显优于使用 SGD。我们发现，在微调梯度在第一个“嵌入”层中比其余模型的梯度大得多时，SGD 和 AdamW 之间存在很大的性能差距。我们的分析提出了一个简单的修正方法，可以在不同数据集和模型上始终保持一致：冻结嵌入层（参数的不到 1%），这样 SGD（具有或不具有动量）的表现略优于 AdamW，并且使用更少的内存（例如，在 ViT-L 上，SGD 使用的 GPU 内存较少 33%）。我们的观点导致了最新的技术进展。",
    "tldr": "SGD 和 AdamW 是用于调整视觉模型的两种常见优化器，当细调梯度在嵌入层中较大时，AdamW 的性能优于 SGD。我们提出了冻结嵌入层的简单修正方法，使得 SGD 表现略好于 AdamW 并且使用更少的内存。"
}