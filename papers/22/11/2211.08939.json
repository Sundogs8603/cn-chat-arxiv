{
    "title": "Augmented Physics-Informed Neural Networks (APINNs): A gating network-based soft domain decomposition methodology. (arXiv:2211.08939v3 [cs.LG] UPDATED)",
    "abstract": "In this paper, we propose the augmented physics-informed neural network (APINN), which adopts soft and trainable domain decomposition and flexible parameter sharing to further improve the extended PINN (XPINN) as well as the vanilla PINN methods. In particular, a trainable gate network is employed to mimic the hard decomposition of XPINN, which can be flexibly fine-tuned for discovering a potentially better partition. It weight-averages several sub-nets as the output of APINN. APINN does not require complex interface conditions, and its sub-nets can take advantage of all training samples rather than just part of the training data in their subdomains. Lastly, each sub-net shares part of the common parameters to capture the similar components in each decomposed function. Furthermore, following the PINN generalization theory in Hu et al. [2021], we show that APINN can improve generalization by proper gate network initialization and general domain & function decomposition. Extensive experi",
    "link": "http://arxiv.org/abs/2211.08939",
    "context": "Title: Augmented Physics-Informed Neural Networks (APINNs): A gating network-based soft domain decomposition methodology. (arXiv:2211.08939v3 [cs.LG] UPDATED)\nAbstract: In this paper, we propose the augmented physics-informed neural network (APINN), which adopts soft and trainable domain decomposition and flexible parameter sharing to further improve the extended PINN (XPINN) as well as the vanilla PINN methods. In particular, a trainable gate network is employed to mimic the hard decomposition of XPINN, which can be flexibly fine-tuned for discovering a potentially better partition. It weight-averages several sub-nets as the output of APINN. APINN does not require complex interface conditions, and its sub-nets can take advantage of all training samples rather than just part of the training data in their subdomains. Lastly, each sub-net shares part of the common parameters to capture the similar components in each decomposed function. Furthermore, following the PINN generalization theory in Hu et al. [2021], we show that APINN can improve generalization by proper gate network initialization and general domain & function decomposition. Extensive experi",
    "path": "papers/22/11/2211.08939.json",
    "total_tokens": 1004,
    "translated_title": "增强型物理知识编码神经网络 (APINNs)：基于门控网络的软领域分解方法",
    "translated_abstract": "本文提出了增强型物理知识编码神经网络 (APINN)，采用软可训练的领域分解和灵活的参数共享以进一步改进扩展物理知识编码神经网络 (XPINN) 和基本物理知识编码神经网络 (PINN) 方法。具体而言，使用可训练的门控网络来模拟 XPINN 的硬分解，可以灵活地微调以发现更好的分区。APINN的输出是几个子网络的权重平均值。APINN不需要复杂的界面条件，并且其子网络可以利用所有训练样本，而不仅仅是其子域中的一部分训练数据。最后，每个子网络共享一部分共同参数，以捕捉每个分解函数中的相似组件。此外，根据胡等人[2021]的PINN泛化理论，我们展示了APINN可以通过适当的门控网络初始化和一般领域和函数分解来改善泛化能力。大量实验证明了APINN方法的有效性和优越性。",
    "tldr": "本文提出了增强型物理知识编码神经网络(APINN)，采用软领域分解和参数共享，通过门控网络初始化和一般领域和函数分解来改进了扩展物理知识编码神经网络(XPINN)和基本物理知识编码神经网络(PINN)的泛化能力。"
}