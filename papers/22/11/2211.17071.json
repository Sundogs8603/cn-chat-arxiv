{
    "title": "Interpreting Vulnerabilities of Multi-Instance Learning to Adversarial Perturbations. (arXiv:2211.17071v3 [cs.CV] UPDATED)",
    "abstract": "Multi-Instance Learning (MIL) is a recent machine learning paradigm which is immensely useful in various real-life applications, like image analysis, video anomaly detection, text classification, etc. It is well known that most of the existing machine learning classifiers are highly vulnerable to adversarial perturbations. Since MIL is a weakly supervised learning, where information is available for a set of instances, called bag and not for every instances, adversarial perturbations can be fatal. In this paper, we have proposed two adversarial perturbation methods to analyze the effect of adversarial perturbations to interpret the vulnerability of MIL methods. Out of the two algorithms, one can be customized for every bag, and the other is a universal one, which can affect all bags in a given data set and thus has some generalizability. Through simulations, we have also shown the effectiveness of the proposed algorithms to fool the state-of-the-art (SOTA) MIL methods. Finally, we have",
    "link": "http://arxiv.org/abs/2211.17071",
    "context": "Title: Interpreting Vulnerabilities of Multi-Instance Learning to Adversarial Perturbations. (arXiv:2211.17071v3 [cs.CV] UPDATED)\nAbstract: Multi-Instance Learning (MIL) is a recent machine learning paradigm which is immensely useful in various real-life applications, like image analysis, video anomaly detection, text classification, etc. It is well known that most of the existing machine learning classifiers are highly vulnerable to adversarial perturbations. Since MIL is a weakly supervised learning, where information is available for a set of instances, called bag and not for every instances, adversarial perturbations can be fatal. In this paper, we have proposed two adversarial perturbation methods to analyze the effect of adversarial perturbations to interpret the vulnerability of MIL methods. Out of the two algorithms, one can be customized for every bag, and the other is a universal one, which can affect all bags in a given data set and thus has some generalizability. Through simulations, we have also shown the effectiveness of the proposed algorithms to fool the state-of-the-art (SOTA) MIL methods. Finally, we have",
    "path": "papers/22/11/2211.17071.json",
    "total_tokens": 931,
    "translated_title": "多实例学习对抗扰动的漏洞解释",
    "translated_abstract": "多实例学习（MIL）是最近一种非常有用的机器学习范式，广泛应用于图像分析、视频异常检测、文本分类等各种实际应用中。众所周知，现有的大多数机器学习分类器都极易受到对抗性扰动的攻击。由于MIL是弱监督学习，只有一个袋子中的实例集合是可访问的，而每个实例都无法获取，对抗性扰动可能是致命的。为了解释MIL方法的漏洞，我们提出了两种对抗性扰动方法。其中一种可以针对每个袋子进行定制，另一种是通用的，可以影响给定数据集中的所有袋子，因此具有一定的泛化性。通过模拟，我们还展示了所提出算法欺骗最先进的MIL方法的有效性。最后，我们使用所提出的技术分析了流行的MIL方法的对抗性漏洞。",
    "tldr": "本文提出了两种对抗性扰动方法以解释多实例学习在该方面的漏洞。其中一种针对每个袋子进行定制，另一种是通用的，可以影响给定数据集中的所有袋子，并且证明了这些方法的有效性。",
    "en_tdlr": "This paper proposes two adversarial perturbation methods to interpret the vulnerability of Multi-Instance Learning. One method is customized for each bag, and the other is universal, and the effectiveness of the proposed algorithms is demonstrated through simulations."
}