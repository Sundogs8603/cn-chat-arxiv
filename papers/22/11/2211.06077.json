{
    "title": "Overparameterized random feature regression with nearly orthogonal data. (arXiv:2211.06077v3 [math.ST] UPDATED)",
    "abstract": "We investigate the properties of random feature ridge regression (RFRR) given by a two-layer neural network with random Gaussian initialization. We study the non-asymptotic behaviors of the RFRR with nearly orthogonal deterministic unit-length input data vectors in the overparameterized regime, where the width of the first layer is much larger than the sample size. Our analysis shows high-probability non-asymptotic concentration results for the training errors, cross-validations, and generalization errors of RFRR centered around their respective values for a kernel ridge regression (KRR). This KRR is derived from an expected kernel generated by a nonlinear random feature map. We then approximate the performance of the KRR by a polynomial kernel matrix obtained from the Hermite polynomial expansion of the activation function, whose degree only depends on the orthogonality among different data points. This polynomial kernel determines the asymptotic behavior of the RFRR and the KRR. Our ",
    "link": "http://arxiv.org/abs/2211.06077",
    "context": "Title: Overparameterized random feature regression with nearly orthogonal data. (arXiv:2211.06077v3 [math.ST] UPDATED)\nAbstract: We investigate the properties of random feature ridge regression (RFRR) given by a two-layer neural network with random Gaussian initialization. We study the non-asymptotic behaviors of the RFRR with nearly orthogonal deterministic unit-length input data vectors in the overparameterized regime, where the width of the first layer is much larger than the sample size. Our analysis shows high-probability non-asymptotic concentration results for the training errors, cross-validations, and generalization errors of RFRR centered around their respective values for a kernel ridge regression (KRR). This KRR is derived from an expected kernel generated by a nonlinear random feature map. We then approximate the performance of the KRR by a polynomial kernel matrix obtained from the Hermite polynomial expansion of the activation function, whose degree only depends on the orthogonality among different data points. This polynomial kernel determines the asymptotic behavior of the RFRR and the KRR. Our ",
    "path": "papers/22/11/2211.06077.json",
    "total_tokens": 956,
    "translated_title": "过度参数化的随机特征回归与几乎正交数据",
    "translated_abstract": "本文研究了随机特征岭回归 (RFRR)，其中包括一个具有随机高斯初始化的两层神经网络。我们研究了在过度参数化情况下，第一层的宽度远大于样本大小的条件下，采用几乎正交确定性单位长度输入数据向量进行 RFRR 的非渐近行为。我们的分析显示了 RFRR 训练误差、交叉验证和泛化误差的非渐近集中结果，在核岭回归 (KRR) 的相应值周围有高概率出现。该 KRR 是由非线性随机特征映射生成的期望核导出的。然后，我们通过激活函数的 Hermite 多项式展开获得的多项式核矩阵来近似 KRR 的性能，其次数仅取决于不同数据点之间的正交性。这个多项式核确定了 RFRR 和 KRR 的渐近行为。",
    "tldr": "本文研究了过度参数化的随机特征回归(RFRR)在几乎正交数据上的行为，证明了在第一层宽度大于样本大小的情况下，RFRR的训练误差、交叉验证和泛化误差可以高概率集中在核岭回归(KRR)的相应值周围，同时给出了用多项式核近似KRR性能的方法。",
    "en_tdlr": "This paper investigates the behavior of overparameterized random feature regression (RFRR) on nearly orthogonal data, showing that the training errors, cross-validations, and generalization errors of RFRR can concentrate around the respective values of kernel ridge regression (KRR) with high probability, and proposes a method to approximate the performance of KRR using a polynomial kernel."
}