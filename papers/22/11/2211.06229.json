{
    "title": "Improving word mover's distance by leveraging self-attention matrix. (arXiv:2211.06229v2 [cs.CL] UPDATED)",
    "abstract": "Measuring the semantic similarity between two sentences is still an important task. The word mover's distance (WMD) computes the similarity via the optimal alignment between the sets of word embeddings. However, WMD does not utilize word order, making it challenging to distinguish sentences with significant overlaps of similar words, even if they are semantically very different. Here, we attempt to improve WMD by incorporating the sentence structure represented by BERT's self-attention matrix (SAM). The proposed method is based on the Fused Gromov-Wasserstein distance, which simultaneously considers the similarity of the word embedding and the SAM for calculating the optimal transport between two sentences. Experiments demonstrate the proposed method enhances WMD and its variants in paraphrase identification with near-equivalent performance in semantic textual similarity. Our code is available at \\url{https://github.com/ymgw55/WSMD}.",
    "link": "http://arxiv.org/abs/2211.06229",
    "context": "Title: Improving word mover's distance by leveraging self-attention matrix. (arXiv:2211.06229v2 [cs.CL] UPDATED)\nAbstract: Measuring the semantic similarity between two sentences is still an important task. The word mover's distance (WMD) computes the similarity via the optimal alignment between the sets of word embeddings. However, WMD does not utilize word order, making it challenging to distinguish sentences with significant overlaps of similar words, even if they are semantically very different. Here, we attempt to improve WMD by incorporating the sentence structure represented by BERT's self-attention matrix (SAM). The proposed method is based on the Fused Gromov-Wasserstein distance, which simultaneously considers the similarity of the word embedding and the SAM for calculating the optimal transport between two sentences. Experiments demonstrate the proposed method enhances WMD and its variants in paraphrase identification with near-equivalent performance in semantic textual similarity. Our code is available at \\url{https://github.com/ymgw55/WSMD}.",
    "path": "papers/22/11/2211.06229.json",
    "total_tokens": 884,
    "translated_title": "通过利用自注意力矩阵提高词移距离的性能",
    "translated_abstract": "在衡量两个句子之间的语义相似性仍然是一个重要任务，词移距离（WMD）通过计算词向量集之间的最优对齐来计算相似性。然而，WMD没有利用词序，这使得它难以区分具有相似词汇重叠的句子，即使它们在语义上非常不同。在这里，我们尝试通过融合BERT的自注意力矩阵（SAM）来改进WMD。所提出的方法基于融合Gromov-Wasserstein距离，同时考虑了词嵌入和SAM的相似度，以计算两个句子之间的最优转运。实验证明，所提出的方法在近义词识别方面提高了WMD及其变体，与语义文本相似性中的几乎相等的性能。我们的代码可在\\url{https://github.com/ymgw55/WSMD}获得。",
    "tldr": "本研究利用自注意力矩阵改进了词移距离（Word Mover's Distance，WMD）的性能，通过考虑句子结构和词嵌入的相似度，实现了在近义词识别和语义文本相似度中较好的表现。",
    "en_tdlr": "This paper improves the performance of Word Mover's Distance (WMD) by leveraging the self-attention matrix (SAM) of BERT to consider sentence structure and word embedding similarity, achieving better performance in paraphrase identification and semantic textual similarity."
}