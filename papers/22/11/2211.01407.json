{
    "title": "On the Informativeness of Supervision Signals. (arXiv:2211.01407v2 [cs.LG] UPDATED)",
    "abstract": "Supervised learning typically focuses on learning transferable representations from training examples annotated by humans. While rich annotations (like soft labels) carry more information than sparse annotations (like hard labels), they are also more expensive to collect. For example, while hard labels only provide information about the closest class an object belongs to (e.g., \"this is a dog\"), soft labels provide information about the object's relationship with multiple classes (e.g., \"this is most likely a dog, but it could also be a wolf or a coyote\"). We use information theory to compare how a number of commonly-used supervision signals contribute to representation-learning performance, as well as how their capacity is affected by factors such as the number of labels, classes, dimensions, and noise. Our framework provides theoretical justification for using hard labels in the big-data regime, but richer supervision signals for few-shot learning and out-of-distribution generalizati",
    "link": "http://arxiv.org/abs/2211.01407",
    "context": "Title: On the Informativeness of Supervision Signals. (arXiv:2211.01407v2 [cs.LG] UPDATED)\nAbstract: Supervised learning typically focuses on learning transferable representations from training examples annotated by humans. While rich annotations (like soft labels) carry more information than sparse annotations (like hard labels), they are also more expensive to collect. For example, while hard labels only provide information about the closest class an object belongs to (e.g., \"this is a dog\"), soft labels provide information about the object's relationship with multiple classes (e.g., \"this is most likely a dog, but it could also be a wolf or a coyote\"). We use information theory to compare how a number of commonly-used supervision signals contribute to representation-learning performance, as well as how their capacity is affected by factors such as the number of labels, classes, dimensions, and noise. Our framework provides theoretical justification for using hard labels in the big-data regime, but richer supervision signals for few-shot learning and out-of-distribution generalizati",
    "path": "papers/22/11/2211.01407.json",
    "total_tokens": 802,
    "translated_title": "关于监督信号的信息量",
    "translated_abstract": "监督学习通常侧重于从人类标注的训练示例中学习可转移的表示。虽然丰富的注释（如软标签）比稀疏的注释（如硬标签）提供更多信息，但它们的收集成本也更高。我们使用信息论比较了许多常用的监督信号对于表示学习性能的贡献，以及它们的能力如何受到标签数、类别、维度和噪声等因素的影响。我们的框架为在大数据时代使用硬标签提供了理论上的证明，但对于少样本学习和分布外泛化，需要使用更丰富的监督信号。",
    "tldr": "本文使用信息论比较了常用的监督信号对表示学习性能的贡献，并为在大数据时代使用硬标签提供了理论上的证明，但对于少样本学习和分布外泛化，需要使用更丰富的监督信号。",
    "en_tdlr": "This paper uses information theory to compare commonly used supervision signals for representation-learning performance and provides theoretical justification for using hard labels in the big-data regime, but suggests using richer supervision signals for few-shot learning and out-of-distribution generalization."
}