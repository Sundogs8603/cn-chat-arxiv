{
    "title": "Adaptation Approaches for Nearest Neighbor Language Models. (arXiv:2211.07828v2 [cs.CL] UPDATED)",
    "abstract": "Semi-parametric Nearest Neighbor Language Models ($k$NN-LMs) have produced impressive gains over purely parametric LMs, by leveraging large-scale neighborhood retrieval over external memory datastores. However, there has been little investigation into adapting such models for new domains. This work attempts to fill that gap and suggests the following approaches for adapting $k$NN-LMs -- 1) adapting the underlying LM (using Adapters), 2) expanding neighborhood retrieval over an additional adaptation datastore, and 3) adapting the weights (scores) of retrieved neighbors using a learned Rescorer module. We study each adaptation strategy separately, as well as the combined performance improvement through ablation experiments and an extensive set of evaluations run over seven adaptation domains. Our combined adaptation approach consistently outperforms purely parametric adaptation and zero-shot ($k$NN-LM) baselines that construct datastores from the adaptation data. On average, we see perpl",
    "link": "http://arxiv.org/abs/2211.07828",
    "context": "Title: Adaptation Approaches for Nearest Neighbor Language Models. (arXiv:2211.07828v2 [cs.CL] UPDATED)\nAbstract: Semi-parametric Nearest Neighbor Language Models ($k$NN-LMs) have produced impressive gains over purely parametric LMs, by leveraging large-scale neighborhood retrieval over external memory datastores. However, there has been little investigation into adapting such models for new domains. This work attempts to fill that gap and suggests the following approaches for adapting $k$NN-LMs -- 1) adapting the underlying LM (using Adapters), 2) expanding neighborhood retrieval over an additional adaptation datastore, and 3) adapting the weights (scores) of retrieved neighbors using a learned Rescorer module. We study each adaptation strategy separately, as well as the combined performance improvement through ablation experiments and an extensive set of evaluations run over seven adaptation domains. Our combined adaptation approach consistently outperforms purely parametric adaptation and zero-shot ($k$NN-LM) baselines that construct datastores from the adaptation data. On average, we see perpl",
    "path": "papers/22/11/2211.07828.json",
    "total_tokens": 1042,
    "translated_title": "最近邻语言模型的适应性方法",
    "translated_abstract": "半参数最近邻语言模型（$k$NN-LMs）通过利用对外部内存数据存储器的大规模邻域检索，比纯参数模型取得了惊人的提升。然而，对于如何适应新领域的模型，目前研究还很少。本文试图填补这一空白，并建议以下适应$k$NN-LMs的方法——1）通过使用转换器来适应底层LM，2）扩展邻域检索到另一个适应数据存储，3）使用学习的重计分模块来适应检索邻居的权重（分数）。我们分别研究了每种适应策略，以及通过消融实验和对7个适应字段运行的广泛评估来组合性能的提高。我们的组合适应方法始终优于纯参数适应和零样本（$k$NN-LM）基线，这些基线从适应数据构建数据存储。平均而言，我们看到相对于基线的困惑度降低最多20％，其中我们的重新计分-$k$NN-LM方法在所有适应场景中提供了最大的改进。",
    "tldr": "本论文提出了三种方法来适应半参数最近邻语言模型（$k$NN-LMs），并运用消融实验和对多个适应领域的广泛评估，发现组合适应方法 consistently outperforms单一适应策略和零样本（$k$NN-LM）基线，重计分模块使得性能提高最多。",
    "en_tdlr": "This paper proposes three methods to adapt semi-parametric nearest neighbor language models ($k$NN-LMs) and shows through ablation experiments and extensive evaluations in multiple adaptation domains that the combined adaptation approach consistently outperforms single adaptation strategies and zero-shot ($ k $NN-LM) baselines, with the rescored module providing the greatest improvement in performance across all adaptation scenarios."
}