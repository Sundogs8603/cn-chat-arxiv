{
    "title": "Generalizable Implicit Neural Representations via Instance Pattern Composers. (arXiv:2211.13223v2 [cs.CV] UPDATED)",
    "abstract": "Despite recent advances in implicit neural representations (INRs), it remains challenging for a coordinate-based multi-layer perceptron (MLP) of INRs to learn a common representation across data instances and generalize it for unseen instances. In this work, we introduce a simple yet effective framework for generalizable INRs that enables a coordinate-based MLP to represent complex data instances by modulating only a small set of weights in an early MLP layer as an instance pattern composer; the remaining MLP weights learn pattern composition rules for common representations across instances. Our generalizable INR framework is fully compatible with existing meta-learning and hypernetworks in learning to predict the modulated weight for unseen instances. Extensive experiments demonstrate that our method achieves high performance on a wide range of domains such as an audio, image, and 3D object, while the ablation study validates our weight modulation.",
    "link": "http://arxiv.org/abs/2211.13223",
    "context": "Title: Generalizable Implicit Neural Representations via Instance Pattern Composers. (arXiv:2211.13223v2 [cs.CV] UPDATED)\nAbstract: Despite recent advances in implicit neural representations (INRs), it remains challenging for a coordinate-based multi-layer perceptron (MLP) of INRs to learn a common representation across data instances and generalize it for unseen instances. In this work, we introduce a simple yet effective framework for generalizable INRs that enables a coordinate-based MLP to represent complex data instances by modulating only a small set of weights in an early MLP layer as an instance pattern composer; the remaining MLP weights learn pattern composition rules for common representations across instances. Our generalizable INR framework is fully compatible with existing meta-learning and hypernetworks in learning to predict the modulated weight for unseen instances. Extensive experiments demonstrate that our method achieves high performance on a wide range of domains such as an audio, image, and 3D object, while the ablation study validates our weight modulation.",
    "path": "papers/22/11/2211.13223.json",
    "total_tokens": 930,
    "translated_title": "通过实例模式组合器实现具有泛化性的隐式神经表示",
    "translated_abstract": "尽管隐式神经表示(INR)取得了一些进展，然而，对于一个基于坐标的多层感知器(INR MLP)来说，在学习跨数据实例的共同表示并将其推广到看不见的实例方面仍然很有挑战性。在本文中，我们提出了一种简单而有效的通用INR框架，它使得基于坐标的MLP仅通过调制早期MLP层中的一小组权重即实例模式组合器，即能够表示复杂的数据实例，而其余的MLP权重则学习适用于实例共同表示的模式组成规则。我们的通用INR框架与现有的元学习和超网络相兼容，可以学习预测看不见实例的调制权重。广泛的实验表明，我们的方法在各种领域(例如音频、图像和3D物体)上都取得了高性能，而消融研究验证了我们的权重调制方法。",
    "tldr": "本文提出了一种使用实例模式组合器实现具有泛化性的隐式神经表示的方法，该方法通过调制早期的多层感知器层中的一小组权重来表示复杂的数据实例，并学习模式组合规则以实现实例的共同表示。",
    "en_tdlr": "This paper proposes a method for achieving generalizable implicit neural representations through the use of an instance pattern composer that modulates a small set of weights in an early MLP layer to represent complex data instances, while the remaining weights learn pattern composition rules for a common representation across instances. The method shows high performance on various domains and is compatible with existing meta-learning and hypernetworks for predicting modulated weights for unseen instances."
}