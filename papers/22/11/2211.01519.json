{
    "title": "SLICER: Learning universal audio representations using low-resource self-supervised pre-training. (arXiv:2211.01519v2 [eess.AS] UPDATED)",
    "abstract": "We present a new Self-Supervised Learning (SSL) approach to pre-train encoders on unlabeled audio data that reduces the need for large amounts of labeled data for audio and speech classification. Our primary aim is to learn audio representations that can generalize across a large variety of speech and non-speech tasks in a low-resource un-labeled audio pre-training setting. Inspired by the recent success of clustering and contrasting learning paradigms for SSL-based speech representation learning, we propose SLICER (Symmetrical Learning of Instance and Cluster-level Efficient Representations), which brings together the best of both clustering and contrasting learning paradigms. We use a symmetric loss between latent representations from student and teacher encoders and simultaneously solve instance and cluster-level contrastive learning tasks. We obtain cluster representations online by just projecting the input spectrogram into an output subspace with dimensions equal to the number of",
    "link": "http://arxiv.org/abs/2211.01519",
    "context": "Title: SLICER: Learning universal audio representations using low-resource self-supervised pre-training. (arXiv:2211.01519v2 [eess.AS] UPDATED)\nAbstract: We present a new Self-Supervised Learning (SSL) approach to pre-train encoders on unlabeled audio data that reduces the need for large amounts of labeled data for audio and speech classification. Our primary aim is to learn audio representations that can generalize across a large variety of speech and non-speech tasks in a low-resource un-labeled audio pre-training setting. Inspired by the recent success of clustering and contrasting learning paradigms for SSL-based speech representation learning, we propose SLICER (Symmetrical Learning of Instance and Cluster-level Efficient Representations), which brings together the best of both clustering and contrasting learning paradigms. We use a symmetric loss between latent representations from student and teacher encoders and simultaneously solve instance and cluster-level contrastive learning tasks. We obtain cluster representations online by just projecting the input spectrogram into an output subspace with dimensions equal to the number of",
    "path": "papers/22/11/2211.01519.json",
    "total_tokens": 873,
    "translated_title": "SLICER: 利用低资源自监督预训练学习通用音频表示",
    "translated_abstract": "我们提出了一种新的自监督学习方法，对未标记的音频数据进行编码器预训练，从而减少音频和语音分类所需的大量标记数据。我们的主要目标是在低资源未标记的音频预训练环境中学习可以概括大量语音和非语音任务的音频表示。受到聚类和对比学习范式在基于自监督学习的语音表示学习中的最近成功启发，我们提出了SLICER（实例和聚类级别高效表征的对称学习），将聚类和对比学习范式的优点结合起来。我们使用学生和教师编码器之间潜在表示之间的对称损失，并同时解决实例和聚类级别的对比学习任务。我们通过将输入的频谱图投影到与聚类数目相同的输出子空间中来在线获得聚类表示。",
    "tldr": "本文提出了一种名为SLICER的自监督学习方法，通过聚类和对比学习相结合的方式进行编码器预训练，从而得到可以广泛适用于语音和非语音任务的音频表示。"
}