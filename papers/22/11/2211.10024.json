{
    "title": "Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks. (arXiv:2211.10024v3 [cs.LG] UPDATED)",
    "abstract": "This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue",
    "link": "http://arxiv.org/abs/2211.10024",
    "context": "Title: Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks. (arXiv:2211.10024v3 [cs.LG] UPDATED)\nAbstract: This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue",
    "path": "papers/22/11/2211.10024.json",
    "total_tokens": 872,
    "translated_title": "自动化复制/粘贴攻击对深度神经网络的诊断",
    "translated_abstract": "本文研究了如何帮助人类监督深度神经网络（DNN）。对抗样本可以通过揭示DNN的弱点来帮助，但很难解释或从中得出可实施的结论。先前的工作提出了使用人类可解释的对抗攻击，包括复制/粘贴攻击，其中一张自然图像粘贴到另一张图像会导致意外的分类错误。本文在此基础上提出了两个贡献。首先，我们介绍了使用嵌入的SNAFUE（Search for Natural Adversarial Features Using Embeddings）寻找复制/粘贴攻击的全自动方法。其次，我们使用SNAFUE来测试ImageNet分类器。我们重现了先前工作中的复制/粘贴攻击，并发现了数百个其他易于描述的漏洞，全过程无需人为干预。代码已在 https://github.com/thestephencasper/snafue 上公开。",
    "tldr": "本文提出了一种基于嵌入方法的SNAFUE技术，用于寻找复制/粘贴攻击，并使用该技术对ImageNet分类器进行了测试并发现许多易于描述的漏洞。"
}