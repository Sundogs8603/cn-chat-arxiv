{
    "title": "A Simple, Yet Effective Approach to Finding Biases in Code Generation. (arXiv:2211.00609v2 [cs.AI] UPDATED)",
    "abstract": "Recently, high-performing code generation systems based on large language models have surfaced. They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.  To investigate the effect, we propose the \"block of influence\" concept, which enables a modular decomposition and analysis of the coding challenges. We introduce an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test. Finally, we demonstrate how our framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.",
    "link": "http://arxiv.org/abs/2211.00609",
    "context": "Title: A Simple, Yet Effective Approach to Finding Biases in Code Generation. (arXiv:2211.00609v2 [cs.AI] UPDATED)\nAbstract: Recently, high-performing code generation systems based on large language models have surfaced. They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.  To investigate the effect, we propose the \"block of influence\" concept, which enables a modular decomposition and analysis of the coding challenges. We introduce an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test. Finally, we demonstrate how our framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.",
    "path": "papers/22/11/2211.00609.json",
    "total_tokens": 855,
    "translated_title": "一种简单但有效的检测代码生成偏差的方法",
    "translated_abstract": "最近，基于大型语言模型的高性能代码生成系统出现了。它们经过大规模的语料库训练，而这些语料库中大多是自然文本，而不是计算机可执行代码。本文表明，目前的代码生成系统存在来自其庞大语言模型的不良偏差，这些偏差在特定情况下会降低生成代码的质量。为了调查这种影响，我们提出“影响块”概念，它能够对编码挑战进行模块化分解和分析。我们引入了一种自动化干预机制，类似于对抗性测试，通过测试模型的失效模式来暴露不良偏差。最后，我们展示了如何在微调期间将我们的框架用作数据转换技术，作为这些偏差的缓解策略。",
    "tldr": "本文介绍了一种简单而有效的方法，能够检测代码生成系统中的偏差，通过对编码挑战进行模块化分解和分析，并通过自动化干预机制来暴露不良偏差，最终作为缓解策略进行数据转换技术的微调。",
    "en_tdlr": "This paper introduces a simple and effective approach to detecting biases in code generation systems by modular decomposition and analysis of coding challenges using the \"block of influence\" concept and an automated intervention mechanism reminiscent of adversarial testing. The framework can also be used as a data transformation technique during fine-tuning to mitigate biases."
}