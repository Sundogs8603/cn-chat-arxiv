{
    "title": "ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning",
    "abstract": "arXiv:2211.04118v3 Announce Type: replace-cross  Abstract: The prompt has become an effective linguistic tool for utilizing pre-trained language models. However, in few-shot scenarios, subtle changes in the prompt design always make the result widely different, and the prompt learning methods also make it easy to overfit the limited samples. To alleviate this, we explore utilizing suitable contrastive samples and multi-degree contrastive learning methods to improve the robustness of the prompt representation. Therefore, the proposed Consprompt combined with the prompt encoding network, contrastive sampling modules, and contrastive scoring modules, is introduced to realize differential contrastive learning. Our results exhibit state-of-the-art performance in different few-shot settings, and the ablation experiments also certify the effectiveness of utilizing multi-degree contrastive learning in the prompt-based fine-tuning process.",
    "link": "https://arxiv.org/abs/2211.04118",
    "context": "Title: ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning\nAbstract: arXiv:2211.04118v3 Announce Type: replace-cross  Abstract: The prompt has become an effective linguistic tool for utilizing pre-trained language models. However, in few-shot scenarios, subtle changes in the prompt design always make the result widely different, and the prompt learning methods also make it easy to overfit the limited samples. To alleviate this, we explore utilizing suitable contrastive samples and multi-degree contrastive learning methods to improve the robustness of the prompt representation. Therefore, the proposed Consprompt combined with the prompt encoding network, contrastive sampling modules, and contrastive scoring modules, is introduced to realize differential contrastive learning. Our results exhibit state-of-the-art performance in different few-shot settings, and the ablation experiments also certify the effectiveness of utilizing multi-degree contrastive learning in the prompt-based fine-tuning process.",
    "path": "papers/22/11/2211.04118.json",
    "total_tokens": 805,
    "translated_title": "利用对比样本进行少样本提示学习",
    "translated_abstract": "提示已成为利用预训练语言模型的有效语言工具。然而，在少样本场景中，提示设计中的细微变化总是导致结果差异很大，并且提示学习方法也很容易过拟合有限的样本。为了缓解这一问题，我们探索利用合适的对比样本和多级对比学习方法来改进提示表示的鲁棒性。因此，引入了提出的Consprompt与提示编码网络、对比采样模块和对比评分模块相结合，实现了差异对比学习。我们的结果在不同少样本设置下展现了最先进的性能，消融实验也证明了在基于提示的微调过程中利用多级对比学习的有效性。",
    "tldr": "提出的Consprompt结合了提示编码网络、对比采样模块和对比评分模块，实现了差异对比学习，在不同少样本设置下表现出最先进的性能，证实了利用多级对比学习在基于提示的微调过程中的有效性。",
    "en_tdlr": "The proposed Consprompt combines prompt encoding network, contrastive sampling modules, and contrastive scoring modules to achieve differential contrastive learning, exhibiting state-of-the-art performance in various few-shot settings and confirming the effectiveness of utilizing multi-degree contrastive learning in the prompt-based fine-tuning process."
}