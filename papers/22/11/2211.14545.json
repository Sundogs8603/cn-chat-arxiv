{
    "title": "Ensemble Multi-Quantiles: Adaptively Flexible Distribution Prediction for Uncertainty Quantification. (arXiv:2211.14545v2 [cs.LG] UPDATED)",
    "abstract": "We propose a novel, succinct, and effective approach to quantify uncertainty in machine learning. It incorporates adaptively flexible distribution prediction for $\\mathbb{P}(\\mathbf{y}|\\mathbf{X}=x)$ in regression tasks. For predicting this conditional distribution, its quantiles of probability levels spreading the interval $(0,1)$ are boosted by additive models which are designed by us with intuitions and interpretability. We seek an adaptive balance between the structural integrity and the flexibility for $\\mathbb{P}(\\mathbf{y}|\\mathbf{X}=x)$, while Gaussian assumption results in a lack of flexibility for real data and highly flexible approaches (e.g., estimating the quantiles separately without a distribution structure) inevitably have drawbacks and may not lead to good generalization. This ensemble multi-quantiles approach called EMQ proposed by us is totally data-driven, and can gradually depart from Gaussian and discover the optimal conditional distribution in the boosting. On ex",
    "link": "http://arxiv.org/abs/2211.14545",
    "context": "Title: Ensemble Multi-Quantiles: Adaptively Flexible Distribution Prediction for Uncertainty Quantification. (arXiv:2211.14545v2 [cs.LG] UPDATED)\nAbstract: We propose a novel, succinct, and effective approach to quantify uncertainty in machine learning. It incorporates adaptively flexible distribution prediction for $\\mathbb{P}(\\mathbf{y}|\\mathbf{X}=x)$ in regression tasks. For predicting this conditional distribution, its quantiles of probability levels spreading the interval $(0,1)$ are boosted by additive models which are designed by us with intuitions and interpretability. We seek an adaptive balance between the structural integrity and the flexibility for $\\mathbb{P}(\\mathbf{y}|\\mathbf{X}=x)$, while Gaussian assumption results in a lack of flexibility for real data and highly flexible approaches (e.g., estimating the quantiles separately without a distribution structure) inevitably have drawbacks and may not lead to good generalization. This ensemble multi-quantiles approach called EMQ proposed by us is totally data-driven, and can gradually depart from Gaussian and discover the optimal conditional distribution in the boosting. On ex",
    "path": "papers/22/11/2211.14545.json",
    "total_tokens": 931,
    "translated_title": "集成多分位数算法：自适应灵活的分布预测用于不确定性量化",
    "translated_abstract": "本文提出了一种新颖、简洁而有效的方法来量化机器学习中的不确定性。它结合了自适应灵活的分布预测，用于回归任务中的条件分布$\\mathbb{P}(\\mathbf{y}|\\mathbf{X}=x)$预测。通过将概率水平的分位数（覆盖区间$(0,1)$）用由我们设计的加法模型提升，来预测这个条件分布。我们寻求$\\mathbb{P}(\\mathbf{y}|\\mathbf{X}=x)$的结构完整性和灵活性之间的自适应平衡，而高斯假设对于真实数据的灵活性不足，高度灵活的方法（例如在没有分布结构的情况下分别估计分位数）不可避免地具有缺陷，并且可能导致无法很好地概括。我们提出的集成多分位数方法EMQ完全是数据驱动的，可以逐步偏离高斯分布并在提升中发现最优条件分布。",
    "tldr": "本文提出了一种自适应灵活的分布预测方法EMQ，用于量化机器学习中的不确定性。该方法逐步偏离高斯分布并在提升中发现最优条件分布，因此具有较好的实用性。",
    "en_tdlr": "This paper proposes an adaptive and flexible approach named EMQ for quantifying uncertainties in machine learning, which gradually departs from Gaussian and discovers the optimal conditional distribution in the boosting process, providing better practicality."
}