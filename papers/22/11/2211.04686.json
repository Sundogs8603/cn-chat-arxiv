{
    "title": "Directional Privacy for Deep Learning. (arXiv:2211.04686v2 [cs.LG] UPDATED)",
    "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD) is a key method for applying privacy in the training of deep learning models. This applies isotropic Gaussian noise to gradients during training, which can perturb these gradients in any direction, damaging utility. Metric DP, however, can provide alternative mechanisms based on arbitrary metrics that might be more suitable for preserving utility. In this paper, we apply \\textit{directional privacy}, via a mechanism based on the von Mises-Fisher (VMF) distribution, to perturb gradients in terms of \\textit{angular distance} so that gradient direction is broadly preserved. We show that this provides both $\\epsilon$-DP and $\\epsilon d$-privacy for deep learning training, rather than the $(\\epsilon, \\delta)$-privacy of the Gaussian mechanism; we observe that the $\\epsilon d$-privacy guarantee does not require a $\\delta>0$ term but degrades smoothly according to the dissimilarity of the input gradients.  As $\\epsilon$s between thes",
    "link": "http://arxiv.org/abs/2211.04686",
    "context": "Title: Directional Privacy for Deep Learning. (arXiv:2211.04686v2 [cs.LG] UPDATED)\nAbstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is a key method for applying privacy in the training of deep learning models. This applies isotropic Gaussian noise to gradients during training, which can perturb these gradients in any direction, damaging utility. Metric DP, however, can provide alternative mechanisms based on arbitrary metrics that might be more suitable for preserving utility. In this paper, we apply \\textit{directional privacy}, via a mechanism based on the von Mises-Fisher (VMF) distribution, to perturb gradients in terms of \\textit{angular distance} so that gradient direction is broadly preserved. We show that this provides both $\\epsilon$-DP and $\\epsilon d$-privacy for deep learning training, rather than the $(\\epsilon, \\delta)$-privacy of the Gaussian mechanism; we observe that the $\\epsilon d$-privacy guarantee does not require a $\\delta>0$ term but degrades smoothly according to the dissimilarity of the input gradients.  As $\\epsilon$s between thes",
    "path": "papers/22/11/2211.04686.json",
    "total_tokens": 1011,
    "translated_title": "深度学习的方向隐私",
    "translated_abstract": "差分隐私随机梯度下降（DP-SGD）是保护深度学习模型训练隐私的关键方法。它在训练过程中向梯度加入各向同性高斯噪声，可能会破坏其效用。度量差分隐私可以提供基于任意度量的替代机制，这可能更适合于维护其效用。本文通过基于von Mises-Fisher（VMF）分布的机制，采用\\textit{角距离} 扰动梯度，从而广泛保留梯度方向，应用\\textit{方向隐私}。我们证明，这提供深度学习训练的$\\epsilon$-DP和$\\epsilon d$-隐私，而不是高斯机制的$(\\epsilon,\\delta)$-隐私。我们观察到$\\epsilon d$-隐私保证不需要$\\delta>0$项，但会根据输入梯度的差异而平滑退化。随着$\\epsilon$在其中的变化，我们展示了使用方向隐私的深度学习训练的实验结果和隐私分析，并且我们探究了这种技术的某些应用。",
    "tldr": "本文采用基于von Mises-Fisher分布的机制应用方向隐私来保护深度学习模型训练隐私，并提供了$\\epsilon d$-隐私保证，可根据输入梯度的差异平滑退化。",
    "en_tdlr": "This paper proposes an approach called directional privacy, which leverages the von Mises-Fisher distribution to perturb the gradients of deep learning models in terms of angular distance, providing both $\\epsilon$-differential privacy and $\\epsilon d$-privacy. This method offers an alternative to isotropic Gaussian noise, which can damage the utility of gradients. The $\\epsilon d$-privacy guarantee smoothly degrades based on the similarity of input gradients."
}