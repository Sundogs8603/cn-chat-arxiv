{
    "title": "Understanding Sparse Feature Updates in Deep Networks using Iterative Linearisation. (arXiv:2211.12345v2 [cs.LG] UPDATED)",
    "abstract": "Larger and deeper networks generalise well despite their increased capacity to overfit. Understanding why this happens is theoretically and practically important. One approach has been to look at the infinitely wide limits of such networks. However, these cannot fully explain finite networks as they do not learn features and the empirical kernel changes significantly during training in contrast to infinite networks. In this work, we derive an iterative linearised training method to investigate this distinction, allowing us to control for sparse (i.e. infrequent) feature updates and quantify the frequency of feature learning needed to achieve comparable performance. We justify iterative linearisation as an interpolation between a finite analog of the infinite width regime, which does not learn features, and standard gradient descent training, which does. We also show that it is analogous to a damped version of the Gauss-Newton algorithm -- a second-order method. We show that in a variet",
    "link": "http://arxiv.org/abs/2211.12345",
    "context": "Title: Understanding Sparse Feature Updates in Deep Networks using Iterative Linearisation. (arXiv:2211.12345v2 [cs.LG] UPDATED)\nAbstract: Larger and deeper networks generalise well despite their increased capacity to overfit. Understanding why this happens is theoretically and practically important. One approach has been to look at the infinitely wide limits of such networks. However, these cannot fully explain finite networks as they do not learn features and the empirical kernel changes significantly during training in contrast to infinite networks. In this work, we derive an iterative linearised training method to investigate this distinction, allowing us to control for sparse (i.e. infrequent) feature updates and quantify the frequency of feature learning needed to achieve comparable performance. We justify iterative linearisation as an interpolation between a finite analog of the infinite width regime, which does not learn features, and standard gradient descent training, which does. We also show that it is analogous to a damped version of the Gauss-Newton algorithm -- a second-order method. We show that in a variet",
    "path": "papers/22/11/2211.12345.json",
    "total_tokens": 837,
    "tldr": "通过迭代线性化训练方法，研究大型深度网络与有限网络之间的差异，量化特征学习的频率以实现相同的性能，更好地理解深度网络的性质。",
    "en_tdlr": "This paper proposes an iterative linearized training method to investigate the differences between large deep networks and finite networks, and quantifies the frequency of feature learning needed to achieve comparable performance. The method offers additional understanding to help understand the nature of deep networks."
}