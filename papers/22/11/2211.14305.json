{
    "title": "SpaText: Spatio-Textual Representation for Controllable Image Generation. (arXiv:2211.14305v2 [cs.CV] UPDATED)",
    "abstract": "Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText - a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition",
    "link": "http://arxiv.org/abs/2211.14305",
    "context": "Title: SpaText: Spatio-Textual Representation for Controllable Image Generation. (arXiv:2211.14305v2 [cs.CV] UPDATED)\nAbstract: Recent text-to-image diffusion models are able to generate convincing results of unprecedented quality. However, it is nearly impossible to control the shapes of different regions/objects or their layout in a fine-grained fashion. Previous attempts to provide such controls were hindered by their reliance on a fixed set of labels. To this end, we present SpaText - a new method for text-to-image generation using open-vocabulary scene control. In addition to a global text prompt that describes the entire scene, the user provides a segmentation map where each region of interest is annotated by a free-form natural language description. Due to lack of large-scale datasets that have a detailed textual description for each region in the image, we choose to leverage the current large-scale text-to-image datasets and base our approach on a novel CLIP-based spatio-textual representation, and show its effectiveness on two state-of-the-art diffusion models: pixel-based and latent-based. In addition",
    "path": "papers/22/11/2211.14305.json",
    "total_tokens": 883,
    "translated_title": "SpaText: 基于空间文本表示的可控图像生成",
    "translated_abstract": "最近的文本到图像扩散模型能够生成前所未有的令人信服的高质量结果。然而，几乎不可能以细粒度的方式控制不同区域/对象的形状或其布局。以往试图提供此类控制的尝试受到其依赖于固定标签集的限制。为此，我们提出了SpaText-一种利用开放词汇场景控制的文本到图像生成新方法。除了描述整个场景的全局文本提示外，用户还提供一个分割地图，其中每个感兴趣区域都由自由形式的自然语言描述进行注释。由于缺乏具有详细文本描述的大规模数据集以描述图像中每个区域，我们选择利用当前的大规模文本到图像数据集，并基于一种新颖的基于CLIP的空间文本表示来实现，同时展示其在基于像素和隐变量的两个最先进的扩散模型上的有效性。",
    "tldr": "SpaText提出了一种新方法，利用开放词汇场景以及基于CLIP的空间文本表示进行文本到图像生成，能够实现对不同区域/对象的形状或布局的细粒度控制。",
    "en_tdlr": "SpaText presents a novel method for text-to-image generation using open-vocabulary scene control and CLIP-based spatio-textual representation, enabling fine-grained control over the shapes and layout of different regions/objects."
}