{
    "title": "Streaming Joint Speech Recognition and Disfluency Detection. (arXiv:2211.08726v2 [cs.CL] UPDATED)",
    "abstract": "Disfluency detection has mainly been solved in a pipeline approach, as post-processing of speech recognition. In this study, we propose Transformer-based encoder-decoder models that jointly solve speech recognition and disfluency detection, which work in a streaming manner. Compared to pipeline approaches, the joint models can leverage acoustic information that makes disfluency detection robust to recognition errors and provide non-verbal clues. Moreover, joint modeling results in low-latency and lightweight inference. We investigate two joint model variants for streaming disfluency detection: a transcript-enriched model and a multi-task model. The transcript-enriched model is trained on text with special tags indicating the starting and ending points of the disfluent part. However, it has problems with latency and standard language model adaptation, which arise from the additional disfluency tags. We propose a multi-task model to solve such problems, which has two output layers at the",
    "link": "http://arxiv.org/abs/2211.08726",
    "context": "Title: Streaming Joint Speech Recognition and Disfluency Detection. (arXiv:2211.08726v2 [cs.CL] UPDATED)\nAbstract: Disfluency detection has mainly been solved in a pipeline approach, as post-processing of speech recognition. In this study, we propose Transformer-based encoder-decoder models that jointly solve speech recognition and disfluency detection, which work in a streaming manner. Compared to pipeline approaches, the joint models can leverage acoustic information that makes disfluency detection robust to recognition errors and provide non-verbal clues. Moreover, joint modeling results in low-latency and lightweight inference. We investigate two joint model variants for streaming disfluency detection: a transcript-enriched model and a multi-task model. The transcript-enriched model is trained on text with special tags indicating the starting and ending points of the disfluent part. However, it has problems with latency and standard language model adaptation, which arise from the additional disfluency tags. We propose a multi-task model to solve such problems, which has two output layers at the",
    "path": "papers/22/11/2211.08726.json",
    "total_tokens": 1086,
    "translated_title": "实时联合语音识别和语言不流畅检测",
    "translated_abstract": "语言不流畅检测主要采用管道处理，作为语音识别的后处理步骤。本研究提出基于 Transformer 的编码器-解码器模型，联合解决语音识别和语言不流畅检测，以流式方式工作。与管道处理相比，联合模型可以利用声学信息，使语言不流畅检测对识别错误具有鲁棒性，并提供非语言线索。此外，联合建模有低延迟和轻量级推理的特点。我们研究了两种流式语言不流畅检测联合模型：文本增强模型和多任务模型。文本增强模型是在带有特殊标记的文本上训练的，指示不流畅部分的起点和终点。然而，它存在来自额外的不流畅标记的延迟和标准语言模型适应的问题。为解决这些问题，我们提出了一个多任务模型，同时具有语音识别和语言不流畅检测两个输出层。多任务模型通过避免由于不流畅而引起的识别错误，实现了高准确率的不流畅检测，并改善了整体的语音识别。",
    "tldr": "本研究提出了一种实时联合语音识别和语言不流畅检测的方法，可以利用声学信息使不流畅检测具有鲁棒性，并提供非语言线索，降低了延迟和推理负担。多任务模型的输出可以实现高准确率的不流畅检测，并避免由于不流畅引起的识别错误，提高了整体语音识别的准确度。"
}