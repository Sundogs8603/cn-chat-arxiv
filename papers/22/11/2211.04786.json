{
    "title": "Leveraging Sequentiality in Reinforcement Learning from a Single Demonstration. (arXiv:2211.04786v2 [cs.RO] UPDATED)",
    "abstract": "Deep Reinforcement Learning has been successfully applied to learn robotic control. However, the corresponding algorithms struggle when applied to problems where the agent is only rewarded after achieving a complex task. In this context, using demonstrations can significantly speed up the learning process, but demonstrations can be costly to acquire. In this paper, we propose to leverage a sequential bias to learn control policies for complex robotic tasks using a single demonstration. To do so, our method learns a goal-conditioned policy to control a system between successive low-dimensional goals. This sequential goal-reaching approach raises a problem of compatibility between successive goals: we need to ensure that the state resulting from reaching a goal is compatible with the achievement of the following goals. To tackle this problem, we present a new algorithm called DCIL-II. We show that DCIL-II can solve with unprecedented sample efficiency some challenging simulated tasks suc",
    "link": "http://arxiv.org/abs/2211.04786",
    "context": "Title: Leveraging Sequentiality in Reinforcement Learning from a Single Demonstration. (arXiv:2211.04786v2 [cs.RO] UPDATED)\nAbstract: Deep Reinforcement Learning has been successfully applied to learn robotic control. However, the corresponding algorithms struggle when applied to problems where the agent is only rewarded after achieving a complex task. In this context, using demonstrations can significantly speed up the learning process, but demonstrations can be costly to acquire. In this paper, we propose to leverage a sequential bias to learn control policies for complex robotic tasks using a single demonstration. To do so, our method learns a goal-conditioned policy to control a system between successive low-dimensional goals. This sequential goal-reaching approach raises a problem of compatibility between successive goals: we need to ensure that the state resulting from reaching a goal is compatible with the achievement of the following goals. To tackle this problem, we present a new algorithm called DCIL-II. We show that DCIL-II can solve with unprecedented sample efficiency some challenging simulated tasks suc",
    "path": "papers/22/11/2211.04786.json",
    "total_tokens": 886,
    "translated_title": "利用单个演示中的连续性来增强强化学习",
    "translated_abstract": "深度强化学习已成功应用于学习机器人控制。然而，当代理只在完成复杂任务后得到奖励时，相应的算法会遇到困难。在这种情况下，使用演示可以显著加速学习过程，但演示可能很难获得。本文提出利用连续性偏见来学习控制复杂机器人任务的策略，只需要一个演示。为此，我们的方法学习了一个目标条件策略，以控制系统在连续的低维度目标之间移动。这种连续目标达成方法引起了一个连续目标间的兼容性问题：我们需要确保达成目标后的状态与后续目标的实现相兼容。为了解决这个问题，我们提出了一种名为DCIL-II的新算法。我们展示了DCIL-II能够以前所未有的样本效率解决一些挑战性的模拟任务。",
    "tldr": "本文提出用单个演示来增强强化学习，利用连续性偏见来学习控制复杂机器人任务，学习一个目标条件策略，通过DCIL-II解决连续目标间的兼容性问题。",
    "en_tdlr": "This paper proposes to enhance reinforcement learning using a single demonstration, by leveraging a sequential bias and learning a goal-conditioned policy. The proposed algorithm, DCIL-II, tackles the challenge of compatibility between successive goals and achieves unprecedented sample efficiency in solving challenging simulated tasks."
}