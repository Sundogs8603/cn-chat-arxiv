{
    "title": "Efficient Transformers with Dynamic Token Pooling. (arXiv:2211.09761v2 [cs.CL] UPDATED)",
    "abstract": "Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vani",
    "link": "http://arxiv.org/abs/2211.09761",
    "context": "Title: Efficient Transformers with Dynamic Token Pooling. (arXiv:2211.09761v2 [cs.CL] UPDATED)\nAbstract: Transformers achieve unrivalled performance in modelling language, but remain inefficient in terms of memory and time complexity. A possible remedy is to reduce the sequence length in the intermediate layers by pooling fixed-length segments of tokens. Nevertheless, natural units of meaning, such as words or phrases, display varying sizes. To address this mismatch, we equip language models with a dynamic-pooling mechanism, which predicts segment boundaries in an autoregressive fashion. We compare several methods to infer boundaries, including end-to-end learning through stochastic re-parameterisation, supervised learning (based on segmentations from subword tokenizers or spikes in conditional entropy), as well as linguistically motivated boundaries. We perform character-level evaluation on texts from multiple datasets and morphologically diverse languages. The results demonstrate that dynamic pooling, which jointly segments and models language, is both faster and more accurate than vani",
    "path": "papers/22/11/2211.09761.json",
    "total_tokens": 897,
    "translated_title": "动态记号池化的高效Transformer",
    "translated_abstract": "Transformer模型在自然语言处理方面的性能表现卓越，但在内存和时间复杂度方面效率低下。一种可能的解决方法是通过固定长度的片段池化记号来缩短中间层的序列长度。然而，诸如单词或短语之类的自然意义单位具有不同的大小。为解决这种不匹配，我们为语言模型配备了一种动态池化机制，可以以自回归的方式预测段边界。我们比较了多种推断边界的方法，包括通过随机重新参数化进行端对端学习、基于子词分词器或条件熵峰值的监督学习，以及基于语言学原理的边界。我们对来自多个数据集和形态不同的语言的文本进行字符级别的评估。结果表明，动态池化机制既比具有固定池化的普通Transformer更快，更准确地进行分段和语言建模。",
    "tldr": "本论文提出了一种动态记号池化机制，可以在自然意义单位具有不同长度的情况下，缩短中间层的序列长度。实验证明，相比具有固定池化的普通Transformer，动态池化机制既更快，更准确地进行分段和语言建模。"
}