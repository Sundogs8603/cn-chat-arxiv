{
    "title": "The Effectiveness of World Models for Continual Reinforcement Learning. (arXiv:2211.15944v2 [cs.LG] UPDATED)",
    "abstract": "World models power some of the most efficient reinforcement learning algorithms. In this work, we showcase that they can be harnessed for continual learning - a situation when the agent faces changing environments. World models typically employ a replay buffer for training, which can be naturally extended to continual learning. We systematically study how different selective experience replay methods affect performance, forgetting, and transfer. We also provide recommendations regarding various modeling options for using world models. The best set of choices is called Continual-Dreamer, it is task-agnostic and utilizes the world model for continual exploration. Continual-Dreamer is sample efficient and outperforms state-of-the-art task-agnostic continual reinforcement learning methods on Minigrid and Minihack benchmarks.",
    "link": "http://arxiv.org/abs/2211.15944",
    "context": "Title: The Effectiveness of World Models for Continual Reinforcement Learning. (arXiv:2211.15944v2 [cs.LG] UPDATED)\nAbstract: World models power some of the most efficient reinforcement learning algorithms. In this work, we showcase that they can be harnessed for continual learning - a situation when the agent faces changing environments. World models typically employ a replay buffer for training, which can be naturally extended to continual learning. We systematically study how different selective experience replay methods affect performance, forgetting, and transfer. We also provide recommendations regarding various modeling options for using world models. The best set of choices is called Continual-Dreamer, it is task-agnostic and utilizes the world model for continual exploration. Continual-Dreamer is sample efficient and outperforms state-of-the-art task-agnostic continual reinforcement learning methods on Minigrid and Minihack benchmarks.",
    "path": "papers/22/11/2211.15944.json",
    "total_tokens": 835,
    "translated_title": "世界模型在连续强化学习中的有效性",
    "translated_abstract": "世界模型是一些高效强化学习算法的基础。在这项工作中，我们展示了它们可以用于连续学习，即智能体面对不断变化的环境情况。世界模型通常使用回放缓冲区进行训练，这可以自然地扩展到连续学习中。我们系统地研究了不同的选择性经验回放方法对性能、遗忘和迁移的影响。我们还提供了关于使用世界模型的各种建模选项的建议。最佳选择是称为Continual-Dreamer的模型，它是任务不可知的，并利用世界模型进行连续探索。Continual-Dreamer具有高样本效率，并在Minigrid和Minihack基准测试中胜过最先进的任务不可知连续强化学习方法。",
    "tldr": "本论文展示了世界模型在连续学习中的应用，通过研究选择性经验回放方法的影响，提出了Continual-Dreamer模型，该模型在Minigrid和Minihack基准测试中表现优于最先进的任务不可知连续强化学习方法。",
    "en_tdlr": "This paper demonstrates the application of world models in continual learning. By studying the impact of selective experience replay methods, the authors propose the Continual-Dreamer model, which outperforms state-of-the-art task-agnostic continual reinforcement learning methods on Minigrid and Minihack benchmarks."
}