{
    "title": "eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers. (arXiv:2211.01324v5 [cs.CV] UPDATED)",
    "abstract": "Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion model",
    "link": "http://arxiv.org/abs/2211.01324",
    "context": "Title: eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers. (arXiv:2211.01324v5 [cs.CV] UPDATED)\nAbstract: Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion model",
    "path": "papers/22/11/2211.01324.json",
    "total_tokens": 916,
    "translated_title": "eDiff-I: 一种使用专家去噪模型集合的文本到图像扩散模型",
    "translated_abstract": "大规模的基于扩散的生成模型在文本相关高分辨率图像合成方面取得了突破。这些文本-图像扩散模型从随机噪声开始，通过迭代地在文本提示的条件下逐步合成图像。我们发现，这种合成行为在生成过程中会定性地改变：在采样早期，生成强烈依赖于文本提示以生成针对文本的内容，而在后期，文本条件几乎被忽略了。这表明在整个生成过程中共享模型参数可能并不理想。因此，与现有工作不同，我们提出训练一组针对不同合成阶段专门的文本-图像扩散模型的集合。为了保持训练效率，我们最初训练单个模型，然后将其分成专门的模型，为迭代生成过程的特定阶段进行训练。我们的扩散模型集合",
    "tldr": "本文提出一种使用专家去噪模型集合的文本到图像扩散模型。文本-图像合成过程中的生成是一个渐进的过程，而在生成的不同阶段，它的合成行为会有所不同。因此本文提出针对不同阶段的专门模型的构想。"
}