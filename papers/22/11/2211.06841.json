{
    "title": "Point-MA2E: Masked and Affine Transformed AutoEncoder for Self-supervised Point Cloud Learning. (arXiv:2211.06841v2 [cs.CV] UPDATED)",
    "abstract": "Masked modeling has demonstrated its effectiveness in self-supervised point cloud learning by reconstructing the complete point cloud from its masked counterpart. Considering that masking only corrupts partial points of the input, in this paper, we promote the affine transformation, which corrupts all input points with certain rules, to complement the popular masking strategy, leading to the Masked and Affine transformed AutoEncoder for point cloud learning (Point-MA2E). Generally, we corrupt the point cloud with affine transformation and masking as input and learn an encoder-decoder model to reconstruct the original point cloud from its corrupted version. Various point cloud encoders are explored in this study. For non-Transformer encoders, we follow the common practice to reconstruct the uncorrupted point cloud directly. For Transformer-based encoders, we decompose the reconstruction of the complete point cloud into the reconstructions of detailed local patches and rough global shape",
    "link": "http://arxiv.org/abs/2211.06841",
    "context": "Title: Point-MA2E: Masked and Affine Transformed AutoEncoder for Self-supervised Point Cloud Learning. (arXiv:2211.06841v2 [cs.CV] UPDATED)\nAbstract: Masked modeling has demonstrated its effectiveness in self-supervised point cloud learning by reconstructing the complete point cloud from its masked counterpart. Considering that masking only corrupts partial points of the input, in this paper, we promote the affine transformation, which corrupts all input points with certain rules, to complement the popular masking strategy, leading to the Masked and Affine transformed AutoEncoder for point cloud learning (Point-MA2E). Generally, we corrupt the point cloud with affine transformation and masking as input and learn an encoder-decoder model to reconstruct the original point cloud from its corrupted version. Various point cloud encoders are explored in this study. For non-Transformer encoders, we follow the common practice to reconstruct the uncorrupted point cloud directly. For Transformer-based encoders, we decompose the reconstruction of the complete point cloud into the reconstructions of detailed local patches and rough global shape",
    "path": "papers/22/11/2211.06841.json",
    "total_tokens": 918,
    "translated_title": "Point-MA2E:自监督点云学习的掩膜和仿射变换自编码器",
    "translated_abstract": "在自监督点云学习中，掩膜建模已经证明了其有效性，通过从其掩膜对应部分重建完整点云。考虑到掩膜只会损坏输入的一部分点，本文推广仿射变换策略，通过特定规则破坏所有输入点，以补充流行的掩膜策略，从而实现点云学习的掩膜和仿射变换自编码器（Point-MA2E）。在此研究中，我们对点云进行仿射变换和掩膜，使用编码器-解码器模型从其损坏版本中重建原始点云。探索了各种点云编码器。对于非Transformer编码器，按照常见做法直接重建未损坏的点云。对于基于Transformer的编码器，我们将重建完整点云分解为详细的局部补丁和粗略的全局形状的重建。",
    "tldr": "本文介绍了一种点云学习的自监督方法Point-MA2E，通过同时采用掩膜和仿射变换策略，实现了从损坏点云到还原点云的重建，扩展了目前掩膜方法的不足。",
    "en_tdlr": "This paper describes a self-supervised method for point cloud learning called Point-MA2E, which uses both masking and affine transformation strategies to achieve reconstruction from corrupted point clouds to original ones, expanding the limitations of current masking methods."
}