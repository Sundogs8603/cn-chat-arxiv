{
    "title": "Large-Scale Bidirectional Training for Zero-Shot Image Captioning. (arXiv:2211.06774v3 [cs.CV] UPDATED)",
    "abstract": "When trained on large-scale datasets, image captioning models can understand the content of images from a general domain but often fail to generate accurate, detailed captions. To improve performance, pretraining-and-finetuning has been a key strategy for image captioning. However, we find that large-scale bidirectional training between image and text enables zero-shot image captioning. In this paper, we introduce Bidirectional Image Text Training in largER Scale, BITTERS, an efficient training and inference framework for zero-shot image captioning. We also propose a new evaluation benchmark which comprises of high quality datasets and an extensive set of metrics to properly evaluate zero-shot captioning accuracy and societal bias. We additionally provide an efficient finetuning approach for keyword extraction. We show that careful selection of large-scale training set and model architecture is the key to achieving zero-shot image captioning.",
    "link": "http://arxiv.org/abs/2211.06774",
    "context": "Title: Large-Scale Bidirectional Training for Zero-Shot Image Captioning. (arXiv:2211.06774v3 [cs.CV] UPDATED)\nAbstract: When trained on large-scale datasets, image captioning models can understand the content of images from a general domain but often fail to generate accurate, detailed captions. To improve performance, pretraining-and-finetuning has been a key strategy for image captioning. However, we find that large-scale bidirectional training between image and text enables zero-shot image captioning. In this paper, we introduce Bidirectional Image Text Training in largER Scale, BITTERS, an efficient training and inference framework for zero-shot image captioning. We also propose a new evaluation benchmark which comprises of high quality datasets and an extensive set of metrics to properly evaluate zero-shot captioning accuracy and societal bias. We additionally provide an efficient finetuning approach for keyword extraction. We show that careful selection of large-scale training set and model architecture is the key to achieving zero-shot image captioning.",
    "path": "papers/22/11/2211.06774.json",
    "total_tokens": 968,
    "translated_title": "大规模双向训练用于零样本图像字幕生成",
    "translated_abstract": "当在大规模数据集上训练时，图像字幕生成模型可以理解通用领域内图像的内容，但往往无法生成准确、详细的字幕。为了提高性能，预训练和微调一直是图像字幕生成的关键策略。然而，我们发现大规模图像和文本之间的双向训练使得零样本图像字幕生成成为可能。在本文中，我们介绍了一种名为BITTERS（Bidirectional Image Text Training in largER Scale）的高效训练和推理框架，用于零样本图像字幕生成。我们还提出了一个新的评估基准，其中包括高质量的数据集和广泛的评估指标，以正确评估零样本字幕的准确性和社会偏差。我们还提供了一种高效的微调方法来提取关键词。我们展示了精选大规模训练集和模型架构是实现零样本图像字幕生成的关键。",
    "tldr": "本文介绍了一个名为BITTERS的高效训练和推理框架，通过大规模图像和文本之间的双向训练实现了零样本图像字幕生成。作者还提出了新的评估基准和微调方法，以提高准确性和降低社会偏差。在实现零样本图像字幕生成方面，精选训练集和模型架构至关重要。",
    "en_tdlr": "This paper presents an efficient training and inference framework called BITTERS for zero-shot image captioning, achieved through large-scale bidirectional training between image and text. The authors also propose a new evaluation benchmark and a finetuning approach to improve accuracy and reduce societal bias. Careful selection of training set and model architecture is crucial for achieving zero-shot image captioning."
}