{
    "title": "Optimal Extended Neighbourhood Rule $k$ Nearest Neighbours Ensemble",
    "abstract": "arXiv:2211.11278v2 Announce Type: replace-cross  Abstract: The traditional k nearest neighbor (kNN) approach uses a distance formula within a spherical region to determine the k closest training observations to a test sample point. However, this approach may not work well when test point is located outside this region. Moreover, aggregating many base kNN learners can result in poor ensemble performance due to high classification errors. To address these issues, a new optimal extended neighborhood rule based ensemble method is proposed in this paper. This rule determines neighbors in k steps starting from the closest sample point to the unseen observation and selecting subsequent nearest data points until the required number of observations is reached. Each base model is constructed on a bootstrap sample with a random subset of features, and optimal models are selected based on out-of-bag performance after building a sufficient number of models. The proposed ensemble is compared with st",
    "link": "https://arxiv.org/abs/2211.11278",
    "context": "Title: Optimal Extended Neighbourhood Rule $k$ Nearest Neighbours Ensemble\nAbstract: arXiv:2211.11278v2 Announce Type: replace-cross  Abstract: The traditional k nearest neighbor (kNN) approach uses a distance formula within a spherical region to determine the k closest training observations to a test sample point. However, this approach may not work well when test point is located outside this region. Moreover, aggregating many base kNN learners can result in poor ensemble performance due to high classification errors. To address these issues, a new optimal extended neighborhood rule based ensemble method is proposed in this paper. This rule determines neighbors in k steps starting from the closest sample point to the unseen observation and selecting subsequent nearest data points until the required number of observations is reached. Each base model is constructed on a bootstrap sample with a random subset of features, and optimal models are selected based on out-of-bag performance after building a sufficient number of models. The proposed ensemble is compared with st",
    "path": "papers/22/11/2211.11278.json",
    "total_tokens": 835,
    "translated_title": "最优扩展邻域规则$k$最近邻集成",
    "translated_abstract": "传统的$k$最近邻($k$NN)方法使用一个球形区域内的距离公式来确定训练观测中与测试样本点最接近的$k$个观测。然而，当测试点位于该区域之外时，这种方法可能不起作用。此外，聚合许多基础$k$NN学习器可能会导致由于高分类误差而表现不佳的集成性能。为解决这些问题，本文提出了一种新的基于最优扩展邻域规则的集成方法。该规则从距离未见观测最近的样本点开始，经过$k$步确定邻居，并选择直到达到所需数量的观测数据点。每个基础模型都是在一个随机特征子集上的自举样本上构建的，并且在构建足够数量的模型后基于袋外表现选择最优模型。提出的集成方法与st进行了比较",
    "tldr": "提出了一种基于最优扩展邻域规则的集成方法，通过新规则确定邻居和模型选择策略来解决传统$k$最近邻方法的局限性和提升集成性能。",
    "en_tdlr": "Proposed an ensemble method based on optimal extended neighborhood rule which addresses limitations of traditional k nearest neighbor method and improves ensemble performance by determining neighbors with a new rule and model selection strategy."
}