{
    "title": "Impact of Adversarial Training on Robustness and Generalizability of Language Models. (arXiv:2211.05523v2 [cs.CL] UPDATED)",
    "abstract": "Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveals that the improved generalization is due to 'more specialized' neurons. To the best of our knowledge,",
    "link": "http://arxiv.org/abs/2211.05523",
    "context": "Title: Impact of Adversarial Training on Robustness and Generalizability of Language Models. (arXiv:2211.05523v2 [cs.CL] UPDATED)\nAbstract: Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveals that the improved generalization is due to 'more specialized' neurons. To the best of our knowledge,",
    "path": "papers/22/11/2211.05523.json",
    "total_tokens": 1052,
    "translated_title": "对抗训练对语言模型的鲁棒性和泛化性的影响",
    "translated_abstract": "对抗训练被广泛认为是防御对抗攻击的最有效手段。但是，已经确认对抗训练模型同时实现鲁棒性和泛化性需要进行权衡。本研究旨在深入比较语言模型中不同的对抗训练方法。具体而言，我们研究了在变压器语言模型中预训练数据增强、训练时间输入扰动和嵌入空间扰动对鲁棒性和泛化性的影响。我们的研究结果表明，通过预训练数据增强或训练时间输入扰动可以实现更好的鲁棒性。然而，通过嵌入空间扰动进行训练可以显著地提高泛化性。学习模型神经元的语言相关性分析表明，改善泛化性是由于存在“更专业”的神经元。据我们所知，这是首个对语言模型对抗训练不同方法进行全面比较的工作。",
    "tldr": "本研究比较了在变压器语言模型中不同的对抗训练方法，并发现预训练数据增强或训练时间输入扰动可以实现更好的鲁棒性，而训练中使用的嵌入空间扰动可以显著提高泛化性。神经元的语言相关性分析表明，这些改进是由于存在“更专业”的神经元。这是首个对语言模型对抗训练不同方法进行全面比较的工作。",
    "en_tdlr": "This study compares different adversarial training approaches in transformer-based language models and finds that pre-training data augmentation or training with input space perturbation can achieve better robustness, while training with embedding space perturbation can significantly improve generalization. Linguistic correlation analysis of neurons of the learned models reveals that these improvements are due to 'more specialized' neurons. This is the first comprehensive comparison of different adversarial training methods for language models."
}