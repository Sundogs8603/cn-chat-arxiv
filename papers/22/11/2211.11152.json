{
    "title": "You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model. (arXiv:2211.11152v2 [cs.CV] UPDATED)",
    "abstract": "Large-scale Transformer models bring significant improvements for various downstream vision language tasks with a unified architecture. The performance improvements come with increasing model size, resulting in slow inference speed and increased cost for severing. While some certain predictions benefit from the full complexity of the large-scale model, not all of inputs need the same amount of computation to conduct, potentially leading to computation resource waste. To handle this challenge, early exiting is proposed to adaptively allocate computational power in term of input complexity to improve inference efficiency. The existing early exiting strategies usually adopt output confidence based on intermediate layers as a proxy of input complexity to incur the decision of skipping following layers. However, such strategies cannot apply to encoder in the widely-used unified architecture with both encoder and decoder due to difficulty of output confidence estimation in the encoder. It is",
    "link": "http://arxiv.org/abs/2211.11152",
    "context": "Title: You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model. (arXiv:2211.11152v2 [cs.CV] UPDATED)\nAbstract: Large-scale Transformer models bring significant improvements for various downstream vision language tasks with a unified architecture. The performance improvements come with increasing model size, resulting in slow inference speed and increased cost for severing. While some certain predictions benefit from the full complexity of the large-scale model, not all of inputs need the same amount of computation to conduct, potentially leading to computation resource waste. To handle this challenge, early exiting is proposed to adaptively allocate computational power in term of input complexity to improve inference efficiency. The existing early exiting strategies usually adopt output confidence based on intermediate layers as a proxy of input complexity to incur the decision of skipping following layers. However, such strategies cannot apply to encoder in the widely-used unified architecture with both encoder and decoder due to difficulty of output confidence estimation in the encoder. It is",
    "path": "papers/22/11/2211.11152.json",
    "total_tokens": 709,
    "tldr": "论文提出动态Early Exiting策略，以适应地分配计算资源，加速视觉语言模型的推理效率。",
    "en_tdlr": "The paper proposes a dynamic Early Exiting strategy to adaptively allocate computational power for accelerating inference efficiency of unified vision language model."
}