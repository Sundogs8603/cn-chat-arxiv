{
    "title": "Clustered Federated Learning based on Nonconvex Pairwise Fusion. (arXiv:2211.04218v2 [cs.LG] UPDATED)",
    "abstract": "This study investigates clustered federated learning (FL), one of the formulations of FL with non-i.i.d. data, where the devices are partitioned into clusters and each cluster optimally fits its data with a localized model. We propose a clustered FL framework that incorporates a nonconvex penalty to pairwise differences of parameters. This framework can automatically identify cluster structures without a priori knowledge of the number of clusters and the set of devices in each cluster. To implement the proposed framework, we introduce a novel clustered FL method called Fusion Penalized Federated Clustering (FPFC). Building upon the standard alternating direction method of multipliers (ADMM), FPFC is implemented in parallel, updates only a subset of devices at each communication round, and allows for variable workload per device. These strategies significantly reduce the communication cost while ensuring privacy, making it practical for FL. We also propose a new warmup strategy for hype",
    "link": "http://arxiv.org/abs/2211.04218",
    "context": "Title: Clustered Federated Learning based on Nonconvex Pairwise Fusion. (arXiv:2211.04218v2 [cs.LG] UPDATED)\nAbstract: This study investigates clustered federated learning (FL), one of the formulations of FL with non-i.i.d. data, where the devices are partitioned into clusters and each cluster optimally fits its data with a localized model. We propose a clustered FL framework that incorporates a nonconvex penalty to pairwise differences of parameters. This framework can automatically identify cluster structures without a priori knowledge of the number of clusters and the set of devices in each cluster. To implement the proposed framework, we introduce a novel clustered FL method called Fusion Penalized Federated Clustering (FPFC). Building upon the standard alternating direction method of multipliers (ADMM), FPFC is implemented in parallel, updates only a subset of devices at each communication round, and allows for variable workload per device. These strategies significantly reduce the communication cost while ensuring privacy, making it practical for FL. We also propose a new warmup strategy for hype",
    "path": "papers/22/11/2211.04218.json",
    "total_tokens": 913,
    "translated_title": "基于非凸配对融合的聚类联邦学习",
    "translated_abstract": "本研究探讨了聚类联邦学习（FL），即具有非i.i.d.数据的FL的一种形式。在聚类联邦学习中，设备被分成聚类，并且每个聚类通过使用本地模型来最优地匹配其数据。我们提出了一种结合了非凸惩罚以配对参数差异的聚类联邦学习框架。该框架可以自动识别聚类结构，无需预先知道聚类的数量和每个聚类中的设备集合。为了实现所提出的框架，我们引入了一种称为融合惩罚联邦聚类（FPFC）的新型聚类联邦学习方法。基于标准的交替方向乘子方法（ADMM），FPFC在并行中实施，仅在每轮通信中更新设备的子集，并允许每个设备的可变工作负载。这些策略显著降低了通信成本，同时确保隐私性，使其在FL中实际可行。我们还提出了一种新的预热策略以提高性能。",
    "tldr": "本研究提出了一种基于非凸配对融合的聚类联邦学习框架，能够自动识别聚类结构，降低通信成本，并确保隐私性。",
    "en_tdlr": "This study proposes a clustered federated learning framework based on nonconvex pairwise fusion, which can automatically identify cluster structures, reduce communication costs, and ensure privacy."
}