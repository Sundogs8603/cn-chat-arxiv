{
    "title": "GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost. (arXiv:2211.06993v3 [cs.CL] UPDATED)",
    "abstract": "Large pre-trained models have revolutionized natural language processing (NLP) research and applications, but high training costs and limited data resources have prevented their benefits from being shared equally amongst speakers of all the world's languages. To address issues of cross-linguistic access to such models and reduce energy consumption for sustainability during large-scale model training, this study proposes an effective and energy-efficient framework called GreenPLM that uses bilingual lexicons to directly \"translate\" pre-trained language models of one language into another at almost no additional cost. We validate this approach in 18 languages' BERT models and show that this framework is comparable to, if not better than, other heuristics with high training costs. In addition, given lightweight continued pre-training on limited data where available, this framework outperforms the original monolingual language models in six out of seven tested languages with up to 200x les",
    "link": "http://arxiv.org/abs/2211.06993",
    "context": "Title: GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost. (arXiv:2211.06993v3 [cs.CL] UPDATED)\nAbstract: Large pre-trained models have revolutionized natural language processing (NLP) research and applications, but high training costs and limited data resources have prevented their benefits from being shared equally amongst speakers of all the world's languages. To address issues of cross-linguistic access to such models and reduce energy consumption for sustainability during large-scale model training, this study proposes an effective and energy-efficient framework called GreenPLM that uses bilingual lexicons to directly \"translate\" pre-trained language models of one language into another at almost no additional cost. We validate this approach in 18 languages' BERT models and show that this framework is comparable to, if not better than, other heuristics with high training costs. In addition, given lightweight continued pre-training on limited data where available, this framework outperforms the original monolingual language models in six out of seven tested languages with up to 200x les",
    "path": "papers/22/11/2211.06993.json",
    "total_tokens": 1017,
    "translated_title": "GreenPLM：几乎不需要成本的跨语言预训练语言模型的跨语言转移",
    "translated_abstract": "大型预训练模型已经改变了自然语言处理的研究和应用，但高昂的训练成本和有限的数据资源阻碍了所有世界语言的使用者平等分享其中的好处。为了解决这些问题，减少大规模模型训练的能源消耗，本研究提出了一种有效的、节能的框架——GreenPLM。该框架利用双语词典直接“翻译”一个语言的预训练语言模型到另一种语言，几乎不需要额外的费用。我们验证了这种方法在18种语言的BERT模型中的有效性，并显示该框架与其他训练成本高的启发式算法相似甚至更优。此外，针对有限数据的轻量级持续预训练，这个框架在七种被测试的语言中有六种比原来的单语言模型表现更好，效率高达200倍。",
    "tldr": "GreenPLM是一个节能有效的框架，可利用双语词典实现几乎无成本的跨语言转移预训练语言模型，有效解决了跨语言访问预训练模型和减少大规模模型训练能源消耗的问题，并在18种语言的BERT模型中验证了其效果。",
    "en_tdlr": "GreenPLM is an energy-efficient framework that allows for almost cost-free cross-lingual transfer of pre-trained language models using bilingual lexicons, effectively addressing issues of cross-linguistic access to pre-trained models and reducing energy consumption during large-scale model training. This study validates the approach in 18 languages' BERT models and shows that the framework is comparable to or better than other heuristics with high training costs. In addition, with lightweight continued pre-training on limited data where available, the framework outperforms the original monolingual language models in six out of seven tested languages with up to 200x improved efficiency."
}