{
    "title": "Is Conditional Generative Modeling all you need for Decision-Making?. (arXiv:2211.15657v4 [cs.LG] UPDATED)",
    "abstract": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that ",
    "link": "http://arxiv.org/abs/2211.15657",
    "context": "Title: Is Conditional Generative Modeling all you need for Decision-Making?. (arXiv:2211.15657v4 [cs.LG] UPDATED)\nAbstract: Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that ",
    "path": "papers/22/11/2211.15657.json",
    "total_tokens": 889,
    "translated_title": "条件生成建模是否足以解决决策问题？",
    "translated_abstract": "最近条件生成建模的改进使得仅凭语言描述就能生成高质量的图像成为可能。我们探讨这些方法是否可以直接解决顺序决策问题。我们从条件生成建模的视角来看待决策，而非强化学习。令人惊讶的是，我们发现我们的方法在标准基准测试中可以优于现有的离线强化学习方法。通过将策略建模为回报条件扩散模型，我们展示了如何避免动态规划的需要，进而消除传统离线强化学习所带来的许多复杂性。通过考虑两个其他的条件变量：约束和技能，我们进一步证明了将策略建模为条件扩散模型的优势。在训练过程中仅对单个约束或技能进行条件设置，导致测试时的行为表现。",
    "tldr": "该论文研究了条件生成建模方法在解决顺序决策问题方面的应用，发现该方法在标准基准测试中表现优于传统离线强化学习方法。通过建模为回报条件扩散模型，可以避免动态规划的复杂性，并通过考虑约束和技能作为条件变量进一步提高性能。",
    "en_tdlr": "This paper investigates the application of conditional generative modeling methods for sequential decision-making and finds that they outperform traditional offline reinforcement learning methods in standard benchmarks. By modeling as a return-conditional diffusion model, the complexities of dynamic programming can be avoided, and further performance improvement can be achieved by considering constraints and skills as conditional variables."
}