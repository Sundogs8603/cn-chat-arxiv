{
    "title": "Whose Emotion Matters? Speaking Activity Localisation without Prior Knowledge. (arXiv:2211.15377v3 [eess.AS] UPDATED)",
    "abstract": "The task of emotion recognition in conversations (ERC) benefits from the availability of multiple modalities, as provided, for example, in the video-based Multimodal EmotionLines Dataset (MELD). However, only a few research approaches use both acoustic and visual information from the MELD videos. There are two reasons for this: First, label-to-video alignments in MELD are noisy, making those videos an unreliable source of emotional speech data. Second, conversations can involve several people in the same scene, which requires the localisation of the utterance source. In this paper, we introduce MELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using recent active speaker detection and automatic speech recognition models, we are able to realign the videos of MELD and capture the facial expressions from speakers in 96.92% of the utterances provided in MELD. Experiments with a self-supervised voice recognition model indicate that the realigned MELD-FAIR videos more cl",
    "link": "http://arxiv.org/abs/2211.15377",
    "context": "Title: Whose Emotion Matters? Speaking Activity Localisation without Prior Knowledge. (arXiv:2211.15377v3 [eess.AS] UPDATED)\nAbstract: The task of emotion recognition in conversations (ERC) benefits from the availability of multiple modalities, as provided, for example, in the video-based Multimodal EmotionLines Dataset (MELD). However, only a few research approaches use both acoustic and visual information from the MELD videos. There are two reasons for this: First, label-to-video alignments in MELD are noisy, making those videos an unreliable source of emotional speech data. Second, conversations can involve several people in the same scene, which requires the localisation of the utterance source. In this paper, we introduce MELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using recent active speaker detection and automatic speech recognition models, we are able to realign the videos of MELD and capture the facial expressions from speakers in 96.92% of the utterances provided in MELD. Experiments with a self-supervised voice recognition model indicate that the realigned MELD-FAIR videos more cl",
    "path": "papers/22/11/2211.15377.json",
    "total_tokens": 939,
    "translated_title": "谁的情绪更重要？没有先前知识的说话活动定位。",
    "translated_abstract": "会话情感识别（ERC）的任务受益于多种模态的可用性，例如在基于视频的多模态情感线数据集（MELD）中提供的信息。然而，只有少数研究方法使用了MELD视频中的声学和视觉信息。这有两个原因：首先，MELD中的标签到视频的对齐是有噪声的，这使得那些视频成为了情感语音数据的不可靠来源。其次，会话可以涉及到同一场景中的几个人，这需要定位话语来源。在本文中，我们通过使用最近的主动说话者检测和自动语音识别模型，引入了带有固定音频视觉信息的MELD-FAIR，能够重新对齐MELD视频并捕获96.92％的MELD中提供的话语的讲话者面部表情。使用自我监督的声音识别模型进行的实验表明，重新对齐的MELD-FAIR视频更清晰地显示了讲话者的面部表情。",
    "tldr": "本文介绍了一种新方法MELD-FAIR来解决情感识别中的挑战，通过使用主动说话者检测和自动语音识别模型，重新对齐了MELD视频，并成功捕获了讲话者的面部表情。"
}