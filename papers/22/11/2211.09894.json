{
    "title": "Supervised Feature Compression based on Counterfactual Analysis. (arXiv:2211.09894v3 [cs.LG] UPDATED)",
    "abstract": "Counterfactual Explanations are becoming a de-facto standard in post-hoc interpretable machine learning. For a given classifier and an instance classified in an undesired class, its counterfactual explanation corresponds to small perturbations of that instance that allows changing the classification outcome. This work aims to leverage Counterfactual Explanations to detect the important decision boundaries of a pre-trained black-box model. This information is used to build a supervised discretization of the features in the dataset with a tunable granularity. Using the discretized dataset, an optimal Decision Tree can be trained that resembles the black-box model, but that is interpretable and compact. Numerical results on real-world datasets show the effectiveness of the approach in terms of accuracy and sparsity.",
    "link": "http://arxiv.org/abs/2211.09894",
    "context": "Title: Supervised Feature Compression based on Counterfactual Analysis. (arXiv:2211.09894v3 [cs.LG] UPDATED)\nAbstract: Counterfactual Explanations are becoming a de-facto standard in post-hoc interpretable machine learning. For a given classifier and an instance classified in an undesired class, its counterfactual explanation corresponds to small perturbations of that instance that allows changing the classification outcome. This work aims to leverage Counterfactual Explanations to detect the important decision boundaries of a pre-trained black-box model. This information is used to build a supervised discretization of the features in the dataset with a tunable granularity. Using the discretized dataset, an optimal Decision Tree can be trained that resembles the black-box model, but that is interpretable and compact. Numerical results on real-world datasets show the effectiveness of the approach in terms of accuracy and sparsity.",
    "path": "papers/22/11/2211.09894.json",
    "total_tokens": 765,
    "translated_title": "基于反事实分析的监督特征压缩",
    "translated_abstract": "反事实解释已成为事后可解释机器学习的事实标准。该工作旨在利用反事实解释识别预训练黑盒模型的重要决策边界。该信息用于在数据集中构建一种可调整细度的特征的监督离散化。使用离散化的数据集，可以训练出类似于黑盒模型的最优决策树，但具有可解释性和紧凑性。在真实数据集上的数值实验表明了该方法在准确性和稀疏性方面的有效性。",
    "tldr": "该论文提出了一种基于反事实分析的监督特征压缩方法，利用此方法可以构建出类似于黑盒模型最优决策树，该决策树具备可解释性和紧凑性，并在真实数据集上有效。",
    "en_tdlr": "This paper proposes a supervised feature compression method based on counterfactual analysis, which identifies important decision boundaries of a pre-trained black-box model. By discretizing features in the dataset with tunable granularity, an optimal decision tree can be trained and achieve interpretability and sparsity. Numerical experiments show its effectiveness on real-world datasets."
}