{
    "title": "Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration. (arXiv:2211.12735v2 [cs.CV] UPDATED)",
    "abstract": "We propose integrally pre-trained transformer pyramid network (iTPN), towards jointly optimizing the network backbone and the neck, so that transfer gap between representation models and downstream tasks is minimal. iTPN is born with two elaborated designs: 1) The first pre-trained feature pyramid upon vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing computational memory overhead and accelerating inference through two flexible designs. 1) Token migration: dropping redundant tokens of the backbone while replenishing them in the feature pyramid without attention operations. 2) Token gathering: reducing computation cost caused by global attention by introducing few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1 accuracy on ImageNet-1K. With 1x training schedule using DINO, the base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object detection, an",
    "link": "http://arxiv.org/abs/2211.12735",
    "context": "Title: Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration. (arXiv:2211.12735v2 [cs.CV] UPDATED)\nAbstract: We propose integrally pre-trained transformer pyramid network (iTPN), towards jointly optimizing the network backbone and the neck, so that transfer gap between representation models and downstream tasks is minimal. iTPN is born with two elaborated designs: 1) The first pre-trained feature pyramid upon vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing computational memory overhead and accelerating inference through two flexible designs. 1) Token migration: dropping redundant tokens of the backbone while replenishing them in the feature pyramid without attention operations. 2) Token gathering: reducing computation cost caused by global attention by introducing few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1 accuracy on ImageNet-1K. With 1x training schedule using DINO, the base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object detection, an",
    "path": "papers/22/11/2211.12735.json",
    "total_tokens": 1039,
    "translated_title": "快速iTPN：具有令牌迁移的整体预训练Transformer金字塔网络",
    "translated_abstract": "我们提出了一种整体预训练的Transformer金字塔网络（iTPN），旨在共同优化网络骨干和瓶颈，以使表示模型与下游任务之间的转移差距最小化。iTPN具有两个精心设计：1）基于视觉Transformer（ViT）的第一个预训练特征金字塔。2）使用遮蔽特征建模（MFM）对特征金字塔进行多阶段监督。Fast-iTPN是iTPN的升级版，通过两个灵活的设计减少了计算内存开销并加速推断。1）令牌迁移：舍弃骨干的冗余标记，同时在特征金字塔中进行补充，无需注意力操作。2）令牌聚集：通过引入少量的聚集标记减少全局注意力引起的计算成本。基于ImageNet-1K数据集，基本/高级的Fast-iTPN分别达到88.75%/89.5%的top-1准确率。在COCO目标检测任务中，使用DINO的1x训练计划，基本/高级的Fast-iTPN分别达到58.4%/58.8%的框AP。",
    "tldr": "我们提出了Fast-iTPN，这是一个整体预训练Transformer金字塔网络，通过令牌迁移和令牌聚集等灵活设计来减少计算内存开销和加速推断。基于ImageNet-1K和COCO数据集，Fast-iTPN在图像分类和目标检测任务中达到了很好的性能。",
    "en_tdlr": "We propose Fast-iTPN, an integrally pre-trained Transformer pyramid network, which reduces computational memory overhead and accelerates inference through flexible designs such as token migration and token gathering. Fast-iTPN achieves high performance on image classification and object detection tasks on the ImageNet-1K and COCO datasets."
}