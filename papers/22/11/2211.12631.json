{
    "title": "A Generic Approach for Reproducible Model Distillation. (arXiv:2211.12631v3 [stat.ML] UPDATED)",
    "abstract": "Model distillation has been a popular method for producing interpretable machine learning. It uses an interpretable \"student\" model to mimic the predictions made by the black box \"teacher\" model. However, when the student model is sensitive to the variability of the data sets used for training even when keeping the teacher fixed, the corresponded interpretation is not reliable. Existing strategies stabilize model distillation by checking whether a large enough corpus of pseudo-data is generated to reliably reproduce student models, but methods to do so have so far been developed for a specific student model. In this paper, we develop a generic approach for stable model distillation based on central limit theorem for the average loss. We start with a collection of candidate student models and search for candidates that reasonably agree with the teacher. Then we construct a multiple testing framework to select a corpus size such that the consistent student model would be selected under d",
    "link": "http://arxiv.org/abs/2211.12631",
    "context": "Title: A Generic Approach for Reproducible Model Distillation. (arXiv:2211.12631v3 [stat.ML] UPDATED)\nAbstract: Model distillation has been a popular method for producing interpretable machine learning. It uses an interpretable \"student\" model to mimic the predictions made by the black box \"teacher\" model. However, when the student model is sensitive to the variability of the data sets used for training even when keeping the teacher fixed, the corresponded interpretation is not reliable. Existing strategies stabilize model distillation by checking whether a large enough corpus of pseudo-data is generated to reliably reproduce student models, but methods to do so have so far been developed for a specific student model. In this paper, we develop a generic approach for stable model distillation based on central limit theorem for the average loss. We start with a collection of candidate student models and search for candidates that reasonably agree with the teacher. Then we construct a multiple testing framework to select a corpus size such that the consistent student model would be selected under d",
    "path": "papers/22/11/2211.12631.json",
    "total_tokens": 906,
    "translated_title": "可复现模型蒸馏的通用方法",
    "translated_abstract": "模型蒸馏是一种流行的生成可解释机器学习模型的方法。它使用一个可解释的“学生”模型去模仿黑盒“老师”模型产生的预测结果。然而，当学生模型对于用于训练的数据集的变异性敏感时，关于学生模型的解释就不可靠了。现有的稳定模型蒸馏策略通过检查是否生成了足够大的伪数据来稳定模型蒸馏，但目前为止，这些方法只适用于特定的学生模型。在本文中，我们提出了一种基于中心极限定理的通用稳定模型蒸馏方法。我们首先从候选的学生模型集合开始，搜索与老师模型相一致的合理候选模型。然后，我们构建一个多重检验框架，选择一组可以使得一致的学生模型被选中的伪数据集。",
    "tldr": "本文提出了一种用中心极限定理为基础的通用稳定模型蒸馏方法，能够从候选学生模型中搜索与老师模型相一致的合理模型，使用一个多重检验框架来选择一组足够大的伪数据集，以选出一致的学生模型。",
    "en_tdlr": "This paper presents a generic and stable approach for model distillation based on the central limit theorem. A candidate student model search process is used to identify reasonable student models that agree with the teacher model, followed by a multiple testing framework to select a large enough corpus of pseudo-data to ensure selection of consistent student models."
}