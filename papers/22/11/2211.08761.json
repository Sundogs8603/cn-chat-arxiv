{
    "title": "Separable PINN: Mitigating the Curse of Dimensionality in Physics-Informed Neural Networks. (arXiv:2211.08761v3 [cs.LG] UPDATED)",
    "abstract": "Physics-informed neural networks (PINNs) have emerged as new data-driven PDE solvers for both forward and inverse problems. While promising, the expensive computational costs to obtain solutions often restrict their broader applicability. We demonstrate that the computations in automatic differentiation (AD) can be significantly reduced by leveraging forward-mode AD when training PINN. However, a naive application of forward-mode AD to conventional PINNs results in higher computation, losing its practical benefit. Therefore, we propose a network architecture, called separable PINN (SPINN), which can facilitate forward-mode AD for more efficient computation. SPINN operates on a per-axis basis instead of point-wise processing in conventional PINNs, decreasing the number of network forward passes. Besides, while the computation and memory costs of standard PINNs grow exponentially along with the grid resolution, that of our model is remarkably less susceptible, mitigating the curse of dim",
    "link": "http://arxiv.org/abs/2211.08761",
    "context": "Title: Separable PINN: Mitigating the Curse of Dimensionality in Physics-Informed Neural Networks. (arXiv:2211.08761v3 [cs.LG] UPDATED)\nAbstract: Physics-informed neural networks (PINNs) have emerged as new data-driven PDE solvers for both forward and inverse problems. While promising, the expensive computational costs to obtain solutions often restrict their broader applicability. We demonstrate that the computations in automatic differentiation (AD) can be significantly reduced by leveraging forward-mode AD when training PINN. However, a naive application of forward-mode AD to conventional PINNs results in higher computation, losing its practical benefit. Therefore, we propose a network architecture, called separable PINN (SPINN), which can facilitate forward-mode AD for more efficient computation. SPINN operates on a per-axis basis instead of point-wise processing in conventional PINNs, decreasing the number of network forward passes. Besides, while the computation and memory costs of standard PINNs grow exponentially along with the grid resolution, that of our model is remarkably less susceptible, mitigating the curse of dim",
    "path": "papers/22/11/2211.08761.json",
    "total_tokens": 890,
    "translated_title": "可分离PINN：减轻物理信息神经网络中的维度诅咒",
    "translated_abstract": "物理信息神经网络（PINNs）已成为前向和反向问题的新型数据驱动PDE求解器。尽管具有潜力，但获得解的昂贵计算成本通常限制其更广泛的应用。我们证明，在训练PINN时，通过利用前向模式自动微分（AD），可以显著减少计算量。然而，对传统PINN进行简单的前向模式AD应用会导致更高的计算量，失去其实际效益。因此，我们提出了一种名为可分离PINN（SPINN）的网络架构，它可以促进更高效的计算的前向模式AD。SPINN在每个轴上操作，而不是传统PINN中的逐点处理，减少了网络正向传递的次数。此外，标准PINN的计算和内存成本随着网格分辨率的增加呈指数级增长，而我们模型的成本相对较低，减轻了维度诅咒。",
    "tldr": "提出了一种名为可分离PINN（SPINN）的网络架构来减轻物理信息神经网络中的维度诅咒，并通过利用前向模式自动微分实现了更高效的计算。",
    "en_tdlr": "A network architecture called separable PINN (SPINN) is proposed to mitigate the curse of dimensionality in physics-informed neural networks, achieving more efficient computation by leveraging forward-mode automatic differentiation."
}