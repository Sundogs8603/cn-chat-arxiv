{
    "title": "FedCL: Federated Multi-Phase Curriculum Learning to Synchronously Correlate User Heterogeneity. (arXiv:2211.07248v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) is a decentralized learning method used to train machine learning algorithms. In FL, a global model iteratively collects the parameters of local models without accessing their local data. However, a significant challenge in FL is handling the heterogeneity of local data distribution, which often results in a drifted global model that is difficult to converge. To address this issue, current methods employ different strategies such as knowledge distillation, weighted model aggregation, and multi-task learning. These approaches are referred to as asynchronous FL, as they align user models either locally or post-hoc, where model drift has already occurred or has been underestimated. In this paper, we propose an active and synchronous correlation approach to address the challenge of user heterogeneity in FL. Specifically, our approach aims to approximate FL as standard deep learning by actively and synchronously scheduling user learning pace in each round with a dyna",
    "link": "http://arxiv.org/abs/2211.07248",
    "context": "Title: FedCL: Federated Multi-Phase Curriculum Learning to Synchronously Correlate User Heterogeneity. (arXiv:2211.07248v2 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) is a decentralized learning method used to train machine learning algorithms. In FL, a global model iteratively collects the parameters of local models without accessing their local data. However, a significant challenge in FL is handling the heterogeneity of local data distribution, which often results in a drifted global model that is difficult to converge. To address this issue, current methods employ different strategies such as knowledge distillation, weighted model aggregation, and multi-task learning. These approaches are referred to as asynchronous FL, as they align user models either locally or post-hoc, where model drift has already occurred or has been underestimated. In this paper, we propose an active and synchronous correlation approach to address the challenge of user heterogeneity in FL. Specifically, our approach aims to approximate FL as standard deep learning by actively and synchronously scheduling user learning pace in each round with a dyna",
    "path": "papers/22/11/2211.07248.json",
    "total_tokens": 939,
    "translated_title": "FedCL：联邦多阶段课程学习，以同步相关用户异质性",
    "translated_abstract": "联邦学习是一种分散式的学习方法，用于训练机器学习算法。在联邦学习中，全局模型迭代地收集本地模型的参数，而不访问其本地数据。然而，在联邦学习中，处理本地数据分布的异质性是一个重大挑战，通常会导致模型漂移，从而难以收敛。为了解决这个问题，当前方法采用不同的策略，如知识蒸馏、加权模型聚合和多任务学习。这些方法被称为异步联邦学习，因为它们在已经发生或被低估模型漂移的情况下校准用户模型，而我们提出一种主动而同步的相关方法来解决联邦学习中用户异质性的挑战。具体而言，我们的方法旨在通过在每轮中主动和同步地安排用户学习速度，将联邦学习近似为标准深度学习。",
    "tldr": "本论文提出了一种主动而同步的联邦学习方法，名为FedCL，以解决联邦学习中用户异质性的挑战。该方法通过在每轮中主动和同步地安排用户学习速度，将异步联邦学习近似为标准深度学习。"
}