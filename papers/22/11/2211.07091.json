{
    "title": "BiViT: Extremely Compressed Binary Vision Transformer. (arXiv:2211.07091v2 [cs.CV] UPDATED)",
    "abstract": "Model binarization can significantly compress model size, reduce energy consumption, and accelerate inference through efficient bit-wise operations. Although binarizing convolutional neural networks have been extensively studied, there is little work on exploring binarization of vision Transformers which underpin most recent breakthroughs in visual recognition. To this end, we propose to solve two fundamental challenges to push the horizon of Binary Vision Transformers (BiViT). First, the traditional binary method does not take the long-tailed distribution of softmax attention into consideration, bringing large binarization errors in the attention module. To solve this, we propose Softmax-aware Binarization, which dynamically adapts to the data distribution and reduces the error caused by binarization. Second, to better preserve the information of the pretrained model and restore accuracy, we propose a Cross-layer Binarization scheme that decouples the binarization of self-attention an",
    "link": "http://arxiv.org/abs/2211.07091",
    "context": "Title: BiViT: Extremely Compressed Binary Vision Transformer. (arXiv:2211.07091v2 [cs.CV] UPDATED)\nAbstract: Model binarization can significantly compress model size, reduce energy consumption, and accelerate inference through efficient bit-wise operations. Although binarizing convolutional neural networks have been extensively studied, there is little work on exploring binarization of vision Transformers which underpin most recent breakthroughs in visual recognition. To this end, we propose to solve two fundamental challenges to push the horizon of Binary Vision Transformers (BiViT). First, the traditional binary method does not take the long-tailed distribution of softmax attention into consideration, bringing large binarization errors in the attention module. To solve this, we propose Softmax-aware Binarization, which dynamically adapts to the data distribution and reduces the error caused by binarization. Second, to better preserve the information of the pretrained model and restore accuracy, we propose a Cross-layer Binarization scheme that decouples the binarization of self-attention an",
    "path": "papers/22/11/2211.07091.json",
    "total_tokens": 876,
    "translated_title": "BiViT: 极度压缩的二进制视觉Transformer",
    "translated_abstract": "模型二进制化可以通过高效的位运算显著压缩模型大小，减少能耗并加速推理过程。虽然对卷积神经网络进行二进制化的研究已经很多，但对于在视觉识别领域取得突破的视觉Transformer的二进制化研究却很少。为此，我们提出了解决二进制视觉Transformer（BiViT）的两个基本挑战。首先，传统的二进制方法没有考虑到softmax注意力的长尾分布，导致注意力模块中存在较大的二进制化误差。为了解决这个问题，我们提出了适应数据分布并减少因二进制化导致的误差的Softmax-aware 二进制化方法。其次，为了更好地保留预训练模型的信息并恢复准确性，我们提出了一种跨层二进制化方案，将自注意力和交叉注意力层的二进制化解耦。",
    "tldr": "BiViT是一种极度压缩的二进制视觉Transformer，通过Softmax-aware二进制化和跨层二进制化方案解决了在二进制化过程中存在的注意力误差和信息损失的问题。",
    "en_tdlr": "BiViT is an extremely compressed binary vision Transformer that solves the issues of attention errors and information loss during the binarization process through Softmax-aware binarization and a cross-layer binarization scheme."
}