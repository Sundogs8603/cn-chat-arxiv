{
    "title": "Multi-Head Adapter Routing for Cross-Task Generalization. (arXiv:2211.03831v2 [cs.AI] UPDATED)",
    "abstract": "Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists in pre-training adapters on a multi-task training set before few-shot adaptation to test tasks. Polytropon [Ponti et al., 2023] ($\\texttt{Poly}$) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation. In this paper, we investigate the role that adapter routing plays in its success and design new variants based on our findings. First, we build on the intuition that finer-grained routing provides more expressivity. Hence, we propose $\\texttt{MHR}$ (Multi-Head Routing), which combines $\\textit{subsets}$ of adapter parameters and outperforms $\\texttt{Poly}$ under a comparable parameter budget; by only fine-tuning the routing function and not the adapters ($\\texttt{MHR}$-$z$), we achieve competitive performance with extreme parameter efficiency. Second, we find that $\\texttt{Poly}$/$\\texttt{MHR",
    "link": "http://arxiv.org/abs/2211.03831",
    "context": "Title: Multi-Head Adapter Routing for Cross-Task Generalization. (arXiv:2211.03831v2 [cs.AI] UPDATED)\nAbstract: Parameter-efficient fine-tuning (PEFT) for cross-task generalization consists in pre-training adapters on a multi-task training set before few-shot adaptation to test tasks. Polytropon [Ponti et al., 2023] ($\\texttt{Poly}$) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation. In this paper, we investigate the role that adapter routing plays in its success and design new variants based on our findings. First, we build on the intuition that finer-grained routing provides more expressivity. Hence, we propose $\\texttt{MHR}$ (Multi-Head Routing), which combines $\\textit{subsets}$ of adapter parameters and outperforms $\\texttt{Poly}$ under a comparable parameter budget; by only fine-tuning the routing function and not the adapters ($\\texttt{MHR}$-$z$), we achieve competitive performance with extreme parameter efficiency. Second, we find that $\\texttt{Poly}$/$\\texttt{MHR",
    "path": "papers/22/11/2211.03831.json",
    "total_tokens": 929,
    "translated_title": "跨任务泛化的多头适配器路由",
    "translated_abstract": "跨任务泛化的参数高效微调(PEFT)在少样本任务适配之前通过在多任务训练集上预训练适配器来实现。Ponti等人的Polytropon [Ponti et al., 2023] ($\\texttt{Poly}$)在预训练和少样本适配期间共同学习了一组适配器和选择每个任务的适配器子集的路由函数。在本文中，我们研究了适配器路由在其成功中的作用，并根据我们的发现设计了基于此的新变体。首先，我们建立在更精细的路由能提供更多表达性的直觉上。因此，我们提出了Multi-Head Routing (MHR)，它结合了适配器参数的子集，并在可比较的参数预算下表现更好；通过仅微调路由函数而不是适配器(MHR-z)，我们实现了与极高参数效率相媲美的性能。其次，我们发现Poly/MHR的交叉模型适配在少样本任务上可以实现更好的性能。",
    "tldr": "本文研究了跨任务泛化中适配器路由的作用，并设计了基于此的新变体Multi-Head Routing (MHR)，通过精细的路由和参数高效的微调实现了更好的性能和更高的参数效率。",
    "en_tdlr": "This paper investigates the role of adapter routing in cross-task generalization and proposes a new variant called Multi-Head Routing (MHR), which achieves better performance and higher parameter efficiency through fine-grained routing and parameter-efficient fine-tuning."
}