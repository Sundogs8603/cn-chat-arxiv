{
    "title": "Retrieval-Augmented Multimodal Language Modeling. (arXiv:2211.12561v2 [cs.CV] UPDATED)",
    "abstract": "Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate both text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption gen",
    "link": "http://arxiv.org/abs/2211.12561",
    "context": "Title: Retrieval-Augmented Multimodal Language Modeling. (arXiv:2211.12561v2 [cs.CV] UPDATED)\nAbstract: Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate both text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption gen",
    "path": "papers/22/11/2211.12561.json",
    "total_tokens": 969,
    "translated_title": "基于检索的多模态语言建模",
    "translated_abstract": "最近出现的多模态模型如DALL-E和CM3在文本与图像生成方面取得了显著进展。但是，这些模型将所有学习到的知识（例如埃菲尔铁塔的外观）存储在模型参数中，需要越来越大的模型和训练数据来捕捉更多的知识。为了以更可扩展和模块化的方式集成知识，我们提出了一种检索增强的多模态模型，使基本的多模态模型（生成器）能够参考由检索器从外部记忆（例如网上的文本和图像）中提取的相关信息。具体来说，我们使用预训练的CLIP作为检索器，使用在LAION数据集上训练的CM3 Transformer作为生成器。我们得到的模型称为检索增强CM3（RA-CM3），是第一个可以检索和生成文本和图像的多模态模型。我们证明RA-CM3在图像和图片生成方面均显著优于基线多模态模型如DALL-E和CM3，并且其检索模块可以有效地整合外部知识，以提高所生成结果的质量。",
    "tldr": "本文提出了一种新的基于检索的多模态模型，使其可以从外部记忆检索相关图片和文本信息，以更可扩展和模块化的方式集成知识，提供更高质量的生成结果。",
    "en_tdlr": "This paper proposes a retrieval-augmented multimodal model that enables a base multimodal model to refer to relevant text and images fetched by a retriever from external memory, providing higher quality generated outputs in a more scalable and modular way."
}