{
    "title": "Token Turing Machines. (arXiv:2211.09119v2 [cs.LG] UPDATED)",
    "abstract": "We propose Token Turing Machines (TTM), a sequential, autoregressive Transformer model with memory for real-world sequential visual understanding. Our model is inspired by the seminal Neural Turing Machine, and has an external memory consisting of a set of tokens which summarise the previous history (i.e., frames). This memory is efficiently addressed, read and written using a Transformer as the processing unit/controller at each step. The model's memory module ensures that a new observation will only be processed with the contents of the memory (and not the entire history), meaning that it can efficiently process long sequences with a bounded computational cost at each step. We show that TTM outperforms other alternatives, such as other Transformer models designed for long sequences and recurrent neural networks, on two real-world sequential visual understanding tasks: online temporal activity detection from videos and vision-based robot action policy learning.  Code is publicly avail",
    "link": "http://arxiv.org/abs/2211.09119",
    "context": "Title: Token Turing Machines. (arXiv:2211.09119v2 [cs.LG] UPDATED)\nAbstract: We propose Token Turing Machines (TTM), a sequential, autoregressive Transformer model with memory for real-world sequential visual understanding. Our model is inspired by the seminal Neural Turing Machine, and has an external memory consisting of a set of tokens which summarise the previous history (i.e., frames). This memory is efficiently addressed, read and written using a Transformer as the processing unit/controller at each step. The model's memory module ensures that a new observation will only be processed with the contents of the memory (and not the entire history), meaning that it can efficiently process long sequences with a bounded computational cost at each step. We show that TTM outperforms other alternatives, such as other Transformer models designed for long sequences and recurrent neural networks, on two real-world sequential visual understanding tasks: online temporal activity detection from videos and vision-based robot action policy learning.  Code is publicly avail",
    "path": "papers/22/11/2211.09119.json",
    "total_tokens": 863,
    "translated_title": "令牌图灵机",
    "translated_abstract": "我们提出了令牌图灵机（TTM），它是一种带有存储器的序列自回归Transformer模型，用于实际世界的序列视觉理解。我们的模型受到 Neural Turing Machine 的启发，并具有由一组令牌组成的外部存储器，用于总结先前的历史（即帧）。该存储器使用Transformer作为每个步骤的处理单元/控制器进行高效地址寻找、读取和写入。模型的存储器模块确保新的观察结果仅使用存储器的内容（而不是整个历史记录）进行处理，这意味着它可以以有限的计算成本每步高效处理长序列。我们展示了TTM在两个真实世界的序列视觉理解任务上（在线时间活动检测和基于视觉的机器人行动策略学习）表现优于其他替代方法，例如为长序列设计的其他Transformer模型和递归神经网络。",
    "tldr": "令牌图灵机是一种序列自回归Transformer模型，具有的外部存储器可以总结先前的历史并以有限的计算成本处理长序列，相比其他替代方法在序列视觉理解任务中表现更优。",
    "en_tdlr": "Token Turing Machines(TTM), a sequential, autoregressive Transformer model, have an external memory of tokens to summarize the previous history and can efficiently process long sequences. TTM outperforms other alternatives on real-world sequential visual understanding tasks."
}