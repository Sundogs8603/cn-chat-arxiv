{
    "title": "REPAIR: REnormalizing Permuted Activations for Interpolation Repair. (arXiv:2211.08403v3 [cs.LG] UPDATED)",
    "abstract": "In this paper we look into the conjecture of Entezari et al. (2021) which states that if the permutation invariance of neural networks is taken into account, then there is likely no loss barrier to the linear interpolation between SGD solutions. First, we observe that neuron alignment methods alone are insufficient to establish low-barrier linear connectivity between SGD solutions due to a phenomenon we call variance collapse: interpolated deep networks suffer a collapse in the variance of their activations, causing poor performance. Next, we propose REPAIR (REnormalizing Permuted Activations for Interpolation Repair) which mitigates variance collapse by rescaling the preactivations of such interpolated networks. We explore the interaction between our method and the choice of normalization layer, network width, and depth, and demonstrate that using REPAIR on top of neuron alignment methods leads to 60%-100% relative barrier reduction across a wide variety of architecture families and t",
    "link": "http://arxiv.org/abs/2211.08403",
    "context": "Title: REPAIR: REnormalizing Permuted Activations for Interpolation Repair. (arXiv:2211.08403v3 [cs.LG] UPDATED)\nAbstract: In this paper we look into the conjecture of Entezari et al. (2021) which states that if the permutation invariance of neural networks is taken into account, then there is likely no loss barrier to the linear interpolation between SGD solutions. First, we observe that neuron alignment methods alone are insufficient to establish low-barrier linear connectivity between SGD solutions due to a phenomenon we call variance collapse: interpolated deep networks suffer a collapse in the variance of their activations, causing poor performance. Next, we propose REPAIR (REnormalizing Permuted Activations for Interpolation Repair) which mitigates variance collapse by rescaling the preactivations of such interpolated networks. We explore the interaction between our method and the choice of normalization layer, network width, and depth, and demonstrate that using REPAIR on top of neuron alignment methods leads to 60%-100% relative barrier reduction across a wide variety of architecture families and t",
    "path": "papers/22/11/2211.08403.json",
    "total_tokens": 928,
    "translated_title": "REPAIR: 修复插值的归一化置换激活",
    "translated_abstract": "本文探讨了Entezari等人（2021）的猜想，即如果考虑神经网络的置换不变性，那么线性插值之间可能没有损失障碍。首先，我们观察到仅使用神经元对齐方法无法建立低障碍线性连接的原因是一种我们称之为方差坍缩的现象：插值深层网络的激活方差崩溃，导致性能较差。其次，我们提出了REPAIR（修复插值的归一化置换激活）方法，通过重新缩放这些插值网络的预激活来缓解方差崩溃。我们探讨了我们方法与归一化层、网络宽度和深度选择之间的相互作用，并演示了在各种架构族中使用REPAIR作为神经元对齐方法的扩展，可以将障碍降低60%至100%。",
    "tldr": "作者发现仅使用神经元对齐方法不能有效解决线性插值中激活方差坍缩的问题，因此提出了REPAIR方法来修复插值的归一化置换激活。实验证明，在各种架构中将REPAIR与神经元对齐方法结合使用可以大幅降低障碍。",
    "en_tdlr": "The authors find that neuron alignment methods alone are insufficient to address the issue of activation variance collapse in linear interpolation, so they propose the REPAIR method to repair the renormalized permuted activations for interpolation. Experimental results demonstrate that combining REPAIR with neuron alignment methods significantly reduces the barrier across various architectures."
}