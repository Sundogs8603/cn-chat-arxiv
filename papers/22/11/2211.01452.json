{
    "title": "MPCFormer: fast, performant and private Transformer inference with MPC. (arXiv:2211.01452v2 [cs.LG] UPDATED)",
    "abstract": "Enabling private inference is crucial for many cloud inference services that are based on Transformer models. However, existing private inference solutions can increase the inference latency by more than 60x or significantly compromise the inference quality. In this paper, we design the framework MPCFORMER as a practical solution, using Secure Multi-Party Computation (MPC) and Knowledge Distillation (KD). Through extensive evaluations, we show that MPCFORMER significantly speeds up Transformer inference in MPC settings while achieving similar ML performance to the input model. On the IMDb dataset, it achieves similar performance to BERTBASE, while being 5.3x faster. On the GLUE benchmark, it achieves 97% performance of BERTBASE with a 2.2x speedup. MPCFORMER remains effective with different trained Transformer weights such as ROBERTABASE and larger models including BERTLarge. Code is available at https://github.com/MccRee177/MPCFormer.",
    "link": "http://arxiv.org/abs/2211.01452",
    "context": "Title: MPCFormer: fast, performant and private Transformer inference with MPC. (arXiv:2211.01452v2 [cs.LG] UPDATED)\nAbstract: Enabling private inference is crucial for many cloud inference services that are based on Transformer models. However, existing private inference solutions can increase the inference latency by more than 60x or significantly compromise the inference quality. In this paper, we design the framework MPCFORMER as a practical solution, using Secure Multi-Party Computation (MPC) and Knowledge Distillation (KD). Through extensive evaluations, we show that MPCFORMER significantly speeds up Transformer inference in MPC settings while achieving similar ML performance to the input model. On the IMDb dataset, it achieves similar performance to BERTBASE, while being 5.3x faster. On the GLUE benchmark, it achieves 97% performance of BERTBASE with a 2.2x speedup. MPCFORMER remains effective with different trained Transformer weights such as ROBERTABASE and larger models including BERTLarge. Code is available at https://github.com/MccRee177/MPCFormer.",
    "path": "papers/22/11/2211.01452.json",
    "total_tokens": 832,
    "translated_title": "MPCFormer: 基于MPC的快速、高性能和私密的Transformer推理",
    "translated_abstract": "实现私密的推理对于基于Transformer模型的云推理服务非常重要。然而，现有的私密推理解决方案可能会增加推理延迟60倍以上，或显著损害推理质量。在本文中，我们设计了MPCFORMER框架作为实用的解决方案，使用安全多方计算（MPC）和知识蒸馏（KD）。通过广泛的评估，我们证明了MPCFORMER在MPC设置中显著加速了Transformer推理，同时实现了类似于输入模型的ML性能。在IMDb数据集上，它实现了类似于BERTBASE的性能，同时速度快5.3倍。在GLUE基准测试中，它以2.2倍的速度提高了97%的BERTBASE性能。MPCFORMER对不同的经过训练的Transformer权重，如ROBERTABASE和更大的模型，如BERTLarge也保持有效。代码可在https://github.com/MccRee177/MPCFormer获取。",
    "tldr": "MPCFormer使用MPC和KD技术实现私密的Transformer模型推理，并在速度和性能方面显著提升。",
    "en_tdlr": "MPCFormer achieves private inference for Transformer models using MPC and KD techniques, significantly improving both speed and performance."
}