{
    "title": "What learning algorithm is in-context learning? Investigations with linear models. (arXiv:2211.15661v3 [cs.LG] UPDATED)",
    "abstract": "Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noi",
    "link": "http://arxiv.org/abs/2211.15661",
    "context": "Title: What learning algorithm is in-context learning? Investigations with linear models. (arXiv:2211.15661v3 [cs.LG] UPDATED)\nAbstract: Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noi",
    "path": "papers/22/11/2211.15661.json",
    "total_tokens": 1096,
    "translated_title": "什么是上下文学习算法？使用线性模型进行调查",
    "translated_abstract": "神经序列模型，特别是转换器，展现了一种非凡的上下文学习能力。它们可以在输入中呈现的标记示例序列$(x,f(x))$构建新的预测器，而无需进一步的参数更新。我们调查假设：基于转换器的上下文学习器通过在其激活中编码较小的模型并根据上下文中出现的新示例更新这些隐式模型，实现了标准的学习算法。以线性回归作为原型问题，我们提供了三条这个假设的证据来源。首先，我们通过构造证明了转换器可以在梯度下降和闭形式的岭回归的基础上实现线性模型的学习算法。其次，我们展示了通过上下文学习训练出来的学习器与梯度下降、岭回归以及精确最小二乘回归所计算的预测器非常相似，在转换器的深度和数据集的噪声水平变化时，能够在不同的预测器之间进行转换。第三，我们提出了一种度量学习器预测器和从显式学习算法中获得的预测器之间相似程度的方法。这种度量表明，上下文学习可以隐含地编码学习算法，并在问题和上下文中根据需要进行切换。",
    "tldr": "研究者提出了一种假设，即基于转换器的上下文学习器可以隐含地编码学习算法，并根据上下文中出现的新示例更新这些隐式模型。通过构造和比较性质证明了这个假设，并提出了一种度量学习器预测器和从显式学习算法中获得的预测器之间相似程度的方法。",
    "en_tdlr": "The researchers propose a hypothesis that transformer-based in-context learners can implicitly encode learning algorithms and update these implicit models as new examples appear in the context. The hypothesis is proved by construction and comparison, and a measure of similarity between learned predictors and explicit learning algorithms is proposed."
}