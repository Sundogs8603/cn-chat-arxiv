{
    "title": "Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions. (arXiv:2211.12316v2 [cs.LG] UPDATED)",
    "abstract": "Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to generalize better than recurrent models. In this work, we conduct an extensive empirical study on Boolean functions to demonstrate the following: (i) Random Transformers are relatively more biased towards functions of low sensitivity. (ii) When trained on Boolean functions, both Transformers and LSTMs prioritize learning functions of low sensitivity, with Transformers ultimately converging to functions of lower sensitivity. (iii) On sparse Boolean functions which have low sensitivity, we find that Transformers generalize near perfectly even in the presence of noisy labels whereas LSTMs overfit and achieve poor generalization accuracy. Overall, our results provide strong quantifiable evidence",
    "link": "http://arxiv.org/abs/2211.12316",
    "context": "Title: Simplicity Bias in Transformers and their Ability to Learn Sparse Boolean Functions. (arXiv:2211.12316v2 [cs.LG] UPDATED)\nAbstract: Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to generalize better than recurrent models. In this work, we conduct an extensive empirical study on Boolean functions to demonstrate the following: (i) Random Transformers are relatively more biased towards functions of low sensitivity. (ii) When trained on Boolean functions, both Transformers and LSTMs prioritize learning functions of low sensitivity, with Transformers ultimately converging to functions of lower sensitivity. (iii) On sparse Boolean functions which have low sensitivity, we find that Transformers generalize near perfectly even in the presence of noisy labels whereas LSTMs overfit and achieve poor generalization accuracy. Overall, our results provide strong quantifiable evidence",
    "path": "papers/22/11/2211.12316.json",
    "total_tokens": 925,
    "translated_title": "Transformers中的简洁偏见及其学习稀疏布尔函数的能力",
    "translated_abstract": "尽管Transformers在自然语言处理任务上取得了广泛的成功，但最近的研究发现，与循环模型相比，它们在建模几种形式语言时遇到困难。这引发了一个问题，为什么Transformers在实践中表现良好，它们是否具有任何能使它们比循环模型更好地泛化的属性。在这项工作中，我们对布尔函数进行了广泛的实证研究，以证明以下内容：(i) 随机Transformers相对更偏向于具有低敏感度的函数。(ii) 当训练布尔函数时，Transformers和LSTMs都优先学习具有低敏感度的函数，最终Transformers收敛到具有更低敏感度的函数。(iii) 在具有低敏感度的稀疏布尔函数上，我们发现Transformers在存在噪音标签的情况下能够近乎完美地泛化，而LSTMs过拟合并且泛化精度较低。总体而言，我们的结果提供了强有力的可量化证据。",
    "tldr": "Transformers相对于循环模型更偏向于学习具有低敏感度的函数，尤其在稀疏布尔函数上，Transformers能够实现近乎完美的泛化，而LSTMs则表现出过拟合和较低的泛化精度。"
}