{
    "title": "Dual Class-Aware Contrastive Federated Semi-Supervised Learning. (arXiv:2211.08914v2 [cs.LG] UPDATED)",
    "abstract": "Federated semi-supervised learning (FSSL), facilitates labeled clients and unlabeled clients jointly training a global model without sharing private data. Existing FSSL methods predominantly employ pseudo-labeling and consistency regularization to exploit the knowledge of unlabeled data, achieving notable success in raw data utilization. However, these training processes are hindered by large deviations between uploaded local models of labeled and unlabeled clients, as well as confirmation bias introduced by noisy pseudo-labels, both of which negatively affect the global model's performance. In this paper, we present a novel FSSL method called Dual Class-aware Contrastive Federated Semi-Supervised Learning (DCCFSSL). This method accounts for both the local class-aware distribution of each client's data and the global class-aware distribution of all clients' data within the feature space. By implementing a dual class-aware contrastive module, DCCFSSL establishes a unified training objec",
    "link": "http://arxiv.org/abs/2211.08914",
    "context": "Title: Dual Class-Aware Contrastive Federated Semi-Supervised Learning. (arXiv:2211.08914v2 [cs.LG] UPDATED)\nAbstract: Federated semi-supervised learning (FSSL), facilitates labeled clients and unlabeled clients jointly training a global model without sharing private data. Existing FSSL methods predominantly employ pseudo-labeling and consistency regularization to exploit the knowledge of unlabeled data, achieving notable success in raw data utilization. However, these training processes are hindered by large deviations between uploaded local models of labeled and unlabeled clients, as well as confirmation bias introduced by noisy pseudo-labels, both of which negatively affect the global model's performance. In this paper, we present a novel FSSL method called Dual Class-aware Contrastive Federated Semi-Supervised Learning (DCCFSSL). This method accounts for both the local class-aware distribution of each client's data and the global class-aware distribution of all clients' data within the feature space. By implementing a dual class-aware contrastive module, DCCFSSL establishes a unified training objec",
    "path": "papers/22/11/2211.08914.json",
    "total_tokens": 905,
    "translated_title": "双类别感知对比联邦半监督学习",
    "translated_abstract": "联邦半监督学习（FSSL）旨在训练一个全局模型而不共享私有数据，它利用伪标签和一致性正则化来利用未标记数据的知识，在未经处理的数据利用方面取得了显著的成功。然而，这些训练过程受到标记和未标记客户端上传的本地模型之间的大偏差以及伪标签引入的确认偏差的阻碍，这两者都对全局模型的性能产生负面影响。在本文中，我们提出了一种名为双类别感知对比联邦半监督学习（DCCFSSL）的新型FSSL方法。该方法考虑了每个客户端数据的本地类别感知分布和所有客户端数据在特征空间中的全局类别感知分布。通过实现双类别感知对比模块，DCCFSSL建立了一个统一的训练目标",
    "tldr": "本论文提出了一种双类别感知对比联邦半监督学习方法，该方法考虑了数据的本地类别感知分布和全局类别感知分布，通过实现双类别感知对比模块，提高了联邦学习的性能表现。"
}