{
    "title": "Quantifying the Impact of Label Noise on Federated Learning. (arXiv:2211.07816v7 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) is a distributed machine learning paradigm where clients collaboratively train a model using their local (human-generated) datasets. While existing studies focus on FL algorithm development to tackle data heterogeneity across clients, the important issue of data quality (e.g., label noise) in FL is overlooked. This paper aims to fill this gap by providing a quantitative study on the impact of label noise on FL. We derive an upper bound for the generalization error that is linear in the clients' label noise level. Then we conduct experiments on MNIST and CIFAR-10 datasets using various FL algorithms. Our empirical results show that the global model accuracy linearly decreases as the noise level increases, which is consistent with our theoretical analysis. We further find that label noise slows down the convergence of FL training, and the global model tends to overfit when the noise level is high.",
    "link": "http://arxiv.org/abs/2211.07816",
    "context": "Title: Quantifying the Impact of Label Noise on Federated Learning. (arXiv:2211.07816v7 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) is a distributed machine learning paradigm where clients collaboratively train a model using their local (human-generated) datasets. While existing studies focus on FL algorithm development to tackle data heterogeneity across clients, the important issue of data quality (e.g., label noise) in FL is overlooked. This paper aims to fill this gap by providing a quantitative study on the impact of label noise on FL. We derive an upper bound for the generalization error that is linear in the clients' label noise level. Then we conduct experiments on MNIST and CIFAR-10 datasets using various FL algorithms. Our empirical results show that the global model accuracy linearly decreases as the noise level increases, which is consistent with our theoretical analysis. We further find that label noise slows down the convergence of FL training, and the global model tends to overfit when the noise level is high.",
    "path": "papers/22/11/2211.07816.json",
    "total_tokens": 914,
    "translated_title": "论文标题：量化标签噪声对联邦学习的影响",
    "translated_abstract": "联邦学习是一种分布式机器学习范例，客户端可以使用本地（人为生成的）数据集协同训练模型。然而，现有研究集中在FL算法的开发上以解决客户端之间的数据异质性，而在FL中数据质量（如标签噪声）这一重要问题被忽视。本文旨在通过对标签噪声对FL的影响进行定量研究来填补这一空白。我们推导了一种上界来衡量客户端标签噪声水平对泛化误差的影响，并使用各种FL算法在MNIST和CIFAR-10数据集上进行实验。我们的实证结果表明，随着噪声水平的增加，全局模型准确性会线性下降，这与我们的理论分析相一致。我们进一步发现，在标签噪声较高时，标签噪声会减缓FL训练的收敛速度，并导致全局模型过拟合。",
    "tldr": "本文量化了标签噪声对FL的影响。实验结果表明，随着噪声水平的增加，全局模型的准确性会线性下降，同时会导致FL训练的收敛速度减缓和全局模型过拟合。",
    "en_tdlr": "This paper quantifies the impact of label noise on Federated Learning (FL), and finds that the global model accuracy decreases linearly as the noise level increases, while the training convergence slows down and the global model tends to overfit when the noise level is high."
}