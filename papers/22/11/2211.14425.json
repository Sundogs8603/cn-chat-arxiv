{
    "title": "PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations. (arXiv:2211.14425v2 [cs.LG] UPDATED)",
    "abstract": "Recently the Transformer structure has shown good performances in graph learning tasks. However, these Transformer models directly work on graph nodes and may have difficulties learning high-level information. Inspired by the vision transformer, which applies to image patches, we propose a new Transformer-based graph neural network: Patch Graph Transformer (PatchGT). Unlike previous transformer-based models for learning graph representations, PatchGT learns from non-trainable graph patches, not from nodes directly. It can help save computation and improve the model performance. The key idea is to segment a graph into patches based on spectral clustering without any trainable parameters, with which the model can first use GNN layers to learn patch-level representations and then use Transformer to obtain graph-level representations. The architecture leverages the spectral information of graphs and combines the strengths of GNNs and Transformers. Further, we show the limitations of previo",
    "link": "http://arxiv.org/abs/2211.14425",
    "context": "Title: PatchGT: Transformer over Non-trainable Clusters for Learning Graph Representations. (arXiv:2211.14425v2 [cs.LG] UPDATED)\nAbstract: Recently the Transformer structure has shown good performances in graph learning tasks. However, these Transformer models directly work on graph nodes and may have difficulties learning high-level information. Inspired by the vision transformer, which applies to image patches, we propose a new Transformer-based graph neural network: Patch Graph Transformer (PatchGT). Unlike previous transformer-based models for learning graph representations, PatchGT learns from non-trainable graph patches, not from nodes directly. It can help save computation and improve the model performance. The key idea is to segment a graph into patches based on spectral clustering without any trainable parameters, with which the model can first use GNN layers to learn patch-level representations and then use Transformer to obtain graph-level representations. The architecture leverages the spectral information of graphs and combines the strengths of GNNs and Transformers. Further, we show the limitations of previo",
    "path": "papers/22/11/2211.14425.json",
    "total_tokens": 932,
    "translated_title": "PatchGT：基于非可训练聚类的Transformer用于学习图表示",
    "translated_abstract": "最近，Transformer结构在图学习任务中表现出良好的性能。然而，这些Transformer模型直接处理图节点，可能难以学习高层次的信息。受图像patch的启发，我们提出了一种新的基于Transformer的图神经网络：Patch Graph Transformer（PatchGT）。与先前用于学习图表示的基于Transformer的模型不同，PatchGT从非可训练图patch中学习，而不是直接从节点学习。它可以帮助节省计算量并提高模型性能。其关键思想是使用谱聚类将图分割为patch，并使用GNN层首先学习patch级别的表示，然后使用Transformer获取图级别的表示。该架构利用了图的谱信息，结合了GNN和Transformer的优点。进一步，我们展示了先前方法的局限性，并在包括节点分类、链路预测和图分类在内的各种基于图的任务上演示了我们提出的方法的有效性。实验结果表明，PatchGT在几个基准数据集上实现了最先进的性能。",
    "tldr": "PatchGT是一种基于非可训练聚类的Transformer图神经网络，通过将图分割为patch学习图表示，兼具GNN和Transformer的优点，可在多种图学习任务上实现最先进的性能。",
    "en_tdlr": "PatchGT is a Transformer-based graph neural network that utilizes non-trainable clustering to segment graphs into patches for representation learning. It combines the strengths of GNNs and Transformers and achieves state-of-the-art performance on various graph-based tasks."
}