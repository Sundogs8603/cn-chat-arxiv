{
    "title": "RUST: Latent Neural Scene Representations from Unposed Imagery. (arXiv:2211.14306v2 [cs.CV] UPDATED)",
    "abstract": "Inferring the structure of 3D scenes from 2D observations is a fundamental challenge in computer vision. Recently popularized approaches based on neural scene representations have achieved tremendous impact and have been applied across a variety of applications. One of the major remaining challenges in this space is training a single model which can provide latent representations which effectively generalize beyond a single scene. Scene Representation Transformer (SRT) has shown promise in this direction, but scaling it to a larger set of diverse scenes is challenging and necessitates accurately posed ground truth data. To address this problem, we propose RUST (Really Unposed Scene representation Transformer), a pose-free approach to novel view synthesis trained on RGB images alone. Our main insight is that one can train a Pose Encoder that peeks at the target image and learns a latent pose embedding which is used by the decoder for view synthesis. We perform an empirical investigation",
    "link": "http://arxiv.org/abs/2211.14306",
    "context": "Title: RUST: Latent Neural Scene Representations from Unposed Imagery. (arXiv:2211.14306v2 [cs.CV] UPDATED)\nAbstract: Inferring the structure of 3D scenes from 2D observations is a fundamental challenge in computer vision. Recently popularized approaches based on neural scene representations have achieved tremendous impact and have been applied across a variety of applications. One of the major remaining challenges in this space is training a single model which can provide latent representations which effectively generalize beyond a single scene. Scene Representation Transformer (SRT) has shown promise in this direction, but scaling it to a larger set of diverse scenes is challenging and necessitates accurately posed ground truth data. To address this problem, we propose RUST (Really Unposed Scene representation Transformer), a pose-free approach to novel view synthesis trained on RGB images alone. Our main insight is that one can train a Pose Encoder that peeks at the target image and learns a latent pose embedding which is used by the decoder for view synthesis. We perform an empirical investigation",
    "path": "papers/22/11/2211.14306.json",
    "total_tokens": 1029,
    "translated_title": "RUST：从未摆姿势的图像中学习潜在的神经场景表示",
    "translated_abstract": "推断三维场景的结构是计算机视觉中的一个基本挑战。基于神经场景表示的方法近年来广受关注，并被应用于各种应用中。这个领域中的一个主要的挑战是训练一个能够提供潜在表示并且能够有效地推广到多个场景的单一模型。场景表示变换器（SRT）在这方面显示出了潜力，但将其扩展到更大数量的多样化场景是具有挑战性的，并需要准确的摆姿数据来进行训练。为了解决这个问题，我们提出了RUST（真正的未摆姿势场景表示变换器），这是一种基于RGB图像进行训练的无姿势新视角合成方法。我们的主要见解是，可以训练一个姿势编码器，它可以查看目标图像并学习一个潜在的姿势嵌入，该姿势被解码器用于视角合成。我们在三个具有挑战性的数据集上对RUST进行了实证研究，并证明我们的方法在视觉质量和泛化到新场景方面均显著优于最先进的无监督方法。",
    "tldr": "该论文提出了一种无需摆姿势的新视角合成方法RUST，通过训练一个姿势编码器学习图像中的潜在姿态嵌入，可有效地推广到多个场景和提高视觉质量，优于最先进的无监督方法。",
    "en_tdlr": "This paper proposes a pose-free approach to novel view synthesis called RUST, which uses a Pose Encoder to learn a latent pose embedding from RGB images alone and achieves better visual quality and generalization performance than state-of-the-art unsupervised methods on challenging datasets."
}