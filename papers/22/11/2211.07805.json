{
    "title": "Agent-State Construction with Auxiliary Inputs. (arXiv:2211.07805v3 [cs.LG] UPDATED)",
    "abstract": "In many, if not every realistic sequential decision-making task, the decision-making agent is not able to model the full complexity of the world. The environment is often much larger and more complex than the agent, a setting also known as partial observability. In such settings, the agent must leverage more than just the current sensory inputs; it must construct an agent state that summarizes previous interactions with the world. Currently, a popular approach for tackling this problem is to learn the agent-state function via a recurrent network from the agent's sensory stream as input. Many impressive reinforcement learning applications have instead relied on environment-specific functions to aid the agent's inputs for history summarization. These augmentations are done in multiple ways, from simple approaches like concatenating observations to more complex ones such as uncertainty estimates. Although ubiquitous in the field, these additional inputs, which we term auxiliary inputs, ar",
    "link": "http://arxiv.org/abs/2211.07805",
    "context": "Title: Agent-State Construction with Auxiliary Inputs. (arXiv:2211.07805v3 [cs.LG] UPDATED)\nAbstract: In many, if not every realistic sequential decision-making task, the decision-making agent is not able to model the full complexity of the world. The environment is often much larger and more complex than the agent, a setting also known as partial observability. In such settings, the agent must leverage more than just the current sensory inputs; it must construct an agent state that summarizes previous interactions with the world. Currently, a popular approach for tackling this problem is to learn the agent-state function via a recurrent network from the agent's sensory stream as input. Many impressive reinforcement learning applications have instead relied on environment-specific functions to aid the agent's inputs for history summarization. These augmentations are done in multiple ways, from simple approaches like concatenating observations to more complex ones such as uncertainty estimates. Although ubiquitous in the field, these additional inputs, which we term auxiliary inputs, ar",
    "path": "papers/22/11/2211.07805.json",
    "total_tokens": 1031,
    "translated_title": "带辅助输入的Agent-State构建",
    "translated_abstract": "在许多现实的序列决策任务中，决策代理往往无法模拟世界的全部复杂性。环境往往比代理更大更复杂，这也称为部分观测性。在这种情况下，代理必须利用不仅仅是当前的感官输入; 它必须构建一个代理状态，以总结与世界的先前交互。目前，应对这个问题的流行方法是通过一个循环网络从代理的感官流作为输入来学习Agent-State函数。许多强大的强化学习应用程序实际上依赖于特定于环境的函数来帮助代理输入历史摘要。这些增强有多种方式，从简单的方法，如连接观察，到更复杂的方法，如不确定性估计。尽管它们在领域中普遍存在，但我们称之为辅助输入的这些附加输入通常以一种特殊的方式处理。在本文中，我们探索了一种将辅助输入纳入Agent-State构建过程的原则方法。我们的方法基于信息瓶颈原理，其中辅助输入用于调整代理状态的信息内容，同时保留与决策制定相关的信息。",
    "tldr": "本文提出了一种基于信息瓶颈原理的方法，将辅助输入纳入到Agent-State构建过程中，以构建一个代理状态，总结与世界的先前交互，有效地解决了部分观测性问题。"
}