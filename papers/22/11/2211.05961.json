{
    "title": "Inverse Kernel Decomposition. (arXiv:2211.05961v2 [cs.LG] UPDATED)",
    "abstract": "The state-of-the-art dimensionality reduction approaches largely rely on complicated optimization procedures. On the other hand, closed-form approaches requiring merely eigen-decomposition do not have enough sophistication and nonlinearity. In this paper, we propose a novel nonlinear dimensionality reduction method -- Inverse Kernel Decomposition (IKD) -- based on an eigen-decomposition of the sample covariance matrix of data. The method is inspired by Gaussian process latent variable models (GPLVMs) and has comparable performance with GPLVMs. To deal with very noisy data with weak correlations, we propose two solutions -- blockwise and geodesic -- to make use of locally correlated data points and provide better and numerically more stable latent estimations. We use synthetic datasets and four real-world datasets to show that IKD is a better dimensionality reduction method than other eigen-decomposition-based methods, and achieves comparable performance against optimization-based metho",
    "link": "http://arxiv.org/abs/2211.05961",
    "context": "Title: Inverse Kernel Decomposition. (arXiv:2211.05961v2 [cs.LG] UPDATED)\nAbstract: The state-of-the-art dimensionality reduction approaches largely rely on complicated optimization procedures. On the other hand, closed-form approaches requiring merely eigen-decomposition do not have enough sophistication and nonlinearity. In this paper, we propose a novel nonlinear dimensionality reduction method -- Inverse Kernel Decomposition (IKD) -- based on an eigen-decomposition of the sample covariance matrix of data. The method is inspired by Gaussian process latent variable models (GPLVMs) and has comparable performance with GPLVMs. To deal with very noisy data with weak correlations, we propose two solutions -- blockwise and geodesic -- to make use of locally correlated data points and provide better and numerically more stable latent estimations. We use synthetic datasets and four real-world datasets to show that IKD is a better dimensionality reduction method than other eigen-decomposition-based methods, and achieves comparable performance against optimization-based metho",
    "path": "papers/22/11/2211.05961.json",
    "total_tokens": 887,
    "translated_title": "反向核分解",
    "translated_abstract": "最先进的降维方法在很大程度上依赖于复杂的优化过程。而仅需特征值分解的闭式方法在复杂性和非线性方面不够。在本文中，我们提出了一种新的非线性降维方法——逆核分解（IKD），基于数据的样本协方差矩阵的特征值分解。该方法受到高斯过程潜变量模型（GPLVMs）的启发，具有与GPLVMs相当的性能。为了处理具有较弱相关性的噪声数据，我们提出了两种解决方案——分块和测地线——以利用局部相关的数据点并提供更好和数值上更稳定的潜变量估计。我们使用合成数据集和四个真实数据集表明，IKD是一种比其他基于特征值分解的方法更好的降维方法，并且在优化方法方面具有相当的性能。",
    "tldr": "本文提出了一种新的非线性降维方法——逆核分解（IKD），通过特征值分解样本协方差矩阵实现。该方法受到高斯过程潜变量模型（GPLVMs）的启发，并在处理噪声数据方面提供了两种解决方案，具有良好的性能。",
    "en_tdlr": "This paper introduces a novel nonlinear dimensionality reduction method, Inverse Kernel Decomposition (IKD), based on eigen-decomposition of the sample covariance matrix. Inspired by Gaussian process latent variable models (GPLVMs), IKD provides two solutions for handling noisy data and achieves good performance."
}