{
    "title": "Reason from Context with Self-supervised Learning. (arXiv:2211.12817v2 [cs.CV] UPDATED)",
    "abstract": "Self-supervised learning (SSL) learns to capture discriminative visual features useful for knowledge transfers. To better accommodate the object-centric nature of current downstream tasks such as object recognition and detection, various methods have been proposed to suppress contextual biases or disentangle objects from contexts. Nevertheless, these methods may prove inadequate in situations where object identity needs to be reasoned from associated context, such as recognizing or inferring tiny or obscured objects. As an initial effort in the SSL literature, we investigate whether and how contextual associations can be enhanced for visual reasoning within SSL regimes, by (a) proposing a new Self-supervised method with external memories for Context Reasoning (SeCo), and (b) introducing two new downstream tasks, lift-the-flap and object priming, addressing the problems of \"what\" and \"where\" in context reasoning. In both tasks, SeCo outperformed all state-of-the-art (SOTA) SSL methods b",
    "link": "http://arxiv.org/abs/2211.12817",
    "context": "Title: Reason from Context with Self-supervised Learning. (arXiv:2211.12817v2 [cs.CV] UPDATED)\nAbstract: Self-supervised learning (SSL) learns to capture discriminative visual features useful for knowledge transfers. To better accommodate the object-centric nature of current downstream tasks such as object recognition and detection, various methods have been proposed to suppress contextual biases or disentangle objects from contexts. Nevertheless, these methods may prove inadequate in situations where object identity needs to be reasoned from associated context, such as recognizing or inferring tiny or obscured objects. As an initial effort in the SSL literature, we investigate whether and how contextual associations can be enhanced for visual reasoning within SSL regimes, by (a) proposing a new Self-supervised method with external memories for Context Reasoning (SeCo), and (b) introducing two new downstream tasks, lift-the-flap and object priming, addressing the problems of \"what\" and \"where\" in context reasoning. In both tasks, SeCo outperformed all state-of-the-art (SOTA) SSL methods b",
    "path": "papers/22/11/2211.12817.json",
    "total_tokens": 907,
    "tldr": "提出了一种利用自监督学习进行上下文推断的新方法SeCo，并引入了两个新的下游任务，翻转卡片和对象引导，用于解决上下文推理中的 \"什么\" 和 \"在哪里\" 问题。"
}