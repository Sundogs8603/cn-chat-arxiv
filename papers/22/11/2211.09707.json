{
    "title": "Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models. (arXiv:2211.09707v2 [cs.LG] UPDATED)",
    "abstract": "Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-ex",
    "link": "http://arxiv.org/abs/2211.09707",
    "context": "Title: Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models. (arXiv:2211.09707v2 [cs.LG] UPDATED)\nAbstract: Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-ex",
    "path": "papers/22/11/2211.09707.json",
    "total_tokens": 949,
    "translated_title": "听、去噪、行动！基于扩散模型的音频驱动动作合成。",
    "translated_abstract": "扩散模型作为高度表现力但训练高效的概率模型，近年来受到了广泛的关注。我们展示了这些模型非常适合用于合成与音频同时发生的人体运动，例如跳舞和共同语音手势。由于给定音频时运动复杂且高度模糊，需要对其进行概率描述。具体而言，我们将DiffWave结构用于建模3D姿势序列，将Conformers用于替代膨胀卷积以提高建模能力。我们还展示了对运动风格的控制，使用无分类器的引导来调整风格表达的强度。手势和舞蹈生成实验证实了该方法实现了最高水平的运动质量，具有独特的风格表达，其表达的形式可以更或者更少地突出。我们还使用相同的模型体系结构合成了路径驱动的运动。最后，我们将引导过程推广到多个音频源的期望控制，从而实现了更细粒度的运动控制。",
    "tldr": "该论文展示了使用Diffusion Models来进行音频驱动的人体运动合成的有效性和适用性，具备极高的运动质量，可以实现独特的风格表达控制，还可以应用于多源音频下的运动控制。",
    "en_tdlr": "This paper demonstrates the effectiveness and applicability of using Diffusion Models for synthesizing audio-driven human motion, achieving high-quality motion with unique style expression control and can be applied to motion control in multi-source audio."
}