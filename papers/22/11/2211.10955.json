{
    "title": "When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method. (arXiv:2211.10955v2 [cs.LG] UPDATED)",
    "abstract": "Real-world large-scale datasets are both noisily labeled and class-imbalanced. The issues seriously hurt the generalization of trained models. It is hence significant to address the simultaneous incorrect labeling and class-imbalance, i.e., the problem of learning with noisy labels on long-tailed data. Previous works develop several methods for the problem. However, they always rely on strong assumptions that are invalid or hard to be checked in practice. In this paper, to handle the problem and address the limitations of prior works, we propose a representation calibration method RCAL. Specifically, RCAL works with the representations extracted by unsupervised contrastive learning. We assume that without incorrect labeling and class imbalance, the representations of instances in each class conform to a multivariate Gaussian distribution, which is much milder and easier to be checked. Based on the assumption, we recover underlying representation distributions from polluted ones resulti",
    "link": "http://arxiv.org/abs/2211.10955",
    "context": "Title: When Noisy Labels Meet Long Tail Dilemmas: A Representation Calibration Method. (arXiv:2211.10955v2 [cs.LG] UPDATED)\nAbstract: Real-world large-scale datasets are both noisily labeled and class-imbalanced. The issues seriously hurt the generalization of trained models. It is hence significant to address the simultaneous incorrect labeling and class-imbalance, i.e., the problem of learning with noisy labels on long-tailed data. Previous works develop several methods for the problem. However, they always rely on strong assumptions that are invalid or hard to be checked in practice. In this paper, to handle the problem and address the limitations of prior works, we propose a representation calibration method RCAL. Specifically, RCAL works with the representations extracted by unsupervised contrastive learning. We assume that without incorrect labeling and class imbalance, the representations of instances in each class conform to a multivariate Gaussian distribution, which is much milder and easier to be checked. Based on the assumption, we recover underlying representation distributions from polluted ones resulti",
    "path": "papers/22/11/2211.10955.json",
    "total_tokens": 935,
    "translated_title": "当嘈杂标签遇上长尾困境：一种表示校准方法",
    "translated_abstract": "真实世界的大规模数据集既存在嘈杂的标签，又存在类别不平衡的问题。这些问题严重影响了训练模型的泛化能力。因此，解决同时存在错误标记和类别不平衡的问题，即噪声标签在长尾数据上的学习问题非常重要。先前的工作开发了一些方法来解决这个问题。然而，它们总是依赖于在实践中无效或难以检查的强假设。在本文中，为了解决这个问题并解决先前工作的局限性，我们提出了一种表示校准方法RCAL。具体而言，RCAL使用无监督对比学习提取的表示进行工作。我们假设在没有错误标记和类别不平衡的情况下，每个类别中实例的表示符合多变量高斯分布，这种分布更加温和且更容易检查。基于该假设，我们从被污染的表示中恢复出潜在的表示分布。",
    "tldr": "本文提出了一种表示校准方法RCAL来解决在嘈杂标签和长尾数据上学习的问题。RCAL通过使用无监督对比学习提取的表示，并假设实例的表示符合多变量高斯分布，从而解决了先前方法的局限性。",
    "en_tdlr": "This paper proposes a representation calibration method, RCAL, to address the learning problem on noisy labels and long-tailed data. By using representations extracted through unsupervised contrastive learning, RCAL assumes that the representations of instances follow a multivariate Gaussian distribution, overcoming the limitations of previous methods."
}