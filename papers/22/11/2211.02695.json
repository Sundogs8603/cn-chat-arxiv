{
    "title": "WaveNets: Wavelet Channel Attention Networks",
    "abstract": "arXiv:2211.02695v2 Announce Type: replace-cross  Abstract: Channel Attention reigns supreme as an effective technique in the field of computer vision. However, the proposed channel attention by SENet suffers from information loss in feature learning caused by the use of Global Average Pooling (GAP) to represent channels as scalars. Thus, designing effective channel attention mechanisms requires finding a solution to enhance features preservation in modeling channel inter-dependencies. In this work, we utilize Wavelet transform compression as a solution to the channel representation problem. We first test wavelet transform as an Auto-Encoder model equipped with conventional channel attention module. Next, we test wavelet transform as a standalone channel compression method. We prove that global average pooling is equivalent to the recursive approximate Haar wavelet transform. With this proof, we generalize channel attention using Wavelet compression and name it WaveNet. Implementation o",
    "link": "https://arxiv.org/abs/2211.02695",
    "context": "Title: WaveNets: Wavelet Channel Attention Networks\nAbstract: arXiv:2211.02695v2 Announce Type: replace-cross  Abstract: Channel Attention reigns supreme as an effective technique in the field of computer vision. However, the proposed channel attention by SENet suffers from information loss in feature learning caused by the use of Global Average Pooling (GAP) to represent channels as scalars. Thus, designing effective channel attention mechanisms requires finding a solution to enhance features preservation in modeling channel inter-dependencies. In this work, we utilize Wavelet transform compression as a solution to the channel representation problem. We first test wavelet transform as an Auto-Encoder model equipped with conventional channel attention module. Next, we test wavelet transform as a standalone channel compression method. We prove that global average pooling is equivalent to the recursive approximate Haar wavelet transform. With this proof, we generalize channel attention using Wavelet compression and name it WaveNet. Implementation o",
    "path": "papers/22/11/2211.02695.json",
    "total_tokens": 807,
    "translated_title": "WaveNets：小波通道注意力网络",
    "translated_abstract": "通道注意力作为计算机视觉领域的一种有效技术至高无上。然而，由于SENet提出的通道注意力在特征学习中存在信息损失，导致使用全局平均池化（GAP）表示通道为标量。因此，设计有效的通道注意力机制需要找到一种解决方案，以增强特征在建模通道相互依赖性中的保留。在这项工作中，我们利用小波变换压缩作为解决通道表示问题的方案。我们首先测试小波变换作为配备传统通道注意力模块的自动编码器模型。接下来，我们测试小波变换作为独立的通道压缩方法。我们证明全局平均池化等效于递归近似的Haar小波变换。有了这个证明，我们推广了使用小波压缩的通道注意力，将其命名为WaveNet。",
    "tldr": "WaveNets利用小波变换压缩作为通道表示的解决方案，提出了一种通道注意力机制，名为WaveNet。",
    "en_tdlr": "WaveNets propose a channel attention mechanism named WaveNet that uses wavelet transform compression as a solution for channel representation, addressing the information loss issue in feature learning and enhancing the preservation of features in modeling channel inter-dependencies."
}