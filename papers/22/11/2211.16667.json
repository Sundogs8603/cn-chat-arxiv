{
    "title": "Dynamic Sparse Training via Balancing the Exploration-Exploitation Trade-off. (arXiv:2211.16667v3 [cs.LG] UPDATED)",
    "abstract": "Over-parameterization of deep neural networks (DNNs) has shown high prediction accuracy for many applications. Although effective, the large number of parameters hinders its popularity on resource-limited devices and has an outsize environmental impact. Sparse training (using a fixed number of nonzero weights in each iteration) could significantly mitigate the training costs by reducing the model size. However, existing sparse training methods mainly use either random-based or greedy-based drop-and-grow strategies, resulting in local minimal and low accuracy. In this work, we consider the dynamic sparse training as a sparse connectivity search problem and design an exploitation and exploration acquisition function to escape from local optima and saddle points. We further design an acquisition function and provide the theoretical guarantees for the proposed method and clarify its convergence property. Experimental results show that sparse models (up to 98\\% sparsity) obtained by our pro",
    "link": "http://arxiv.org/abs/2211.16667",
    "context": "Title: Dynamic Sparse Training via Balancing the Exploration-Exploitation Trade-off. (arXiv:2211.16667v3 [cs.LG] UPDATED)\nAbstract: Over-parameterization of deep neural networks (DNNs) has shown high prediction accuracy for many applications. Although effective, the large number of parameters hinders its popularity on resource-limited devices and has an outsize environmental impact. Sparse training (using a fixed number of nonzero weights in each iteration) could significantly mitigate the training costs by reducing the model size. However, existing sparse training methods mainly use either random-based or greedy-based drop-and-grow strategies, resulting in local minimal and low accuracy. In this work, we consider the dynamic sparse training as a sparse connectivity search problem and design an exploitation and exploration acquisition function to escape from local optima and saddle points. We further design an acquisition function and provide the theoretical guarantees for the proposed method and clarify its convergence property. Experimental results show that sparse models (up to 98\\% sparsity) obtained by our pro",
    "path": "papers/22/11/2211.16667.json",
    "total_tokens": 1012,
    "translated_title": "平衡探索和开发权衡的动态稀疏训练",
    "translated_abstract": "深度神经网络的超参数化已经在许多应用中表现出高预测准确性。虽然有效，但大量参数阻碍了它在资源受限设备上的普及，并对环境产生不良影响。使用固定数量的非零权重来进行稀疏训练可以显着减轻训练成本，减小模型大小，但现有的稀疏训练方法主要使用基于随机或贪婪的减少和增长策略，导致局部最小值和低精度问题。在本文中，我们将动态稀疏训练视为稀疏连通性搜索问题，并设计一个开发和探索收购函数来摆脱局部最优和鞍点。我们进一步设计了一种收购功能，并提供了所提方法的理论保证并阐明其收敛性质。实验结果表明，我们的程序获得了多种深度神经网络上与密集模型可比的精度（高达98％的稀疏度），并且在精度和收敛速度方面超过了现有的稀疏训练方法。",
    "tldr": "本文提出了一种动态稀疏训练方法，采用开发和探索收购函数来平衡探索和开发之间的权衡，从而摆脱了局部最优和鞍点问题，实验结果表明，该方法在精度和收敛速度方面超过了现有的稀疏训练方法。",
    "en_tdlr": "This paper proposes a dynamic sparse training method that uses an exploitation and exploration acquisition function to balance the exploration-exploitation trade-off, avoiding local optima and saddle points. The experimental results show that the proposed method outperforms existing sparse training methods in terms of both accuracy and convergence speed."
}