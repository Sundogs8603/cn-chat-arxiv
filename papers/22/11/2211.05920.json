{
    "title": "When Less is More: On the Value of \"Co-training\" for Semi-Supervised Software Defect Predictors",
    "abstract": "arXiv:2211.05920v2 Announce Type: replace-cross  Abstract: Labeling a module defective or non-defective is an expensive task. Hence, there are often limits on how much-labeled data is available for training. Semi-supervised classifiers use far fewer labels for training models. However, there are numerous semi-supervised methods, including self-labeling, co-training, maximal-margin, and graph-based methods, to name a few. Only a handful of these methods have been tested in SE for (e.g.) predicting defects and even there, those methods have been tested on just a handful of projects.   This paper applies a wide range of 55 semi-supervised learners to over 714 projects. We find that semi-supervised \"co-training methods\" work significantly better than other approaches. Specifically, after labeling, just   2.5% of data, then make predictions that are competitive to those using 100% of the data.   That said, co-training needs to be used cautiously since the specific choice of co-training meth",
    "link": "https://arxiv.org/abs/2211.05920",
    "context": "Title: When Less is More: On the Value of \"Co-training\" for Semi-Supervised Software Defect Predictors\nAbstract: arXiv:2211.05920v2 Announce Type: replace-cross  Abstract: Labeling a module defective or non-defective is an expensive task. Hence, there are often limits on how much-labeled data is available for training. Semi-supervised classifiers use far fewer labels for training models. However, there are numerous semi-supervised methods, including self-labeling, co-training, maximal-margin, and graph-based methods, to name a few. Only a handful of these methods have been tested in SE for (e.g.) predicting defects and even there, those methods have been tested on just a handful of projects.   This paper applies a wide range of 55 semi-supervised learners to over 714 projects. We find that semi-supervised \"co-training methods\" work significantly better than other approaches. Specifically, after labeling, just   2.5% of data, then make predictions that are competitive to those using 100% of the data.   That said, co-training needs to be used cautiously since the specific choice of co-training meth",
    "path": "papers/22/11/2211.05920.json",
    "total_tokens": 922,
    "translated_title": "当少即是多：关于半监督软件缺陷预测器\"共同训练\"的价值",
    "translated_abstract": "在标记模块为缺陷或非缺陷的任务中，标记数据是一项昂贵的任务。因此，可用于训练的标记数据往往受到限制。半监督分类器使用较少的标记数据来训练模型。然而，有许多半监督方法，包括自标签、共同训练、最大间隔和基于图的方法等。在软件工程领域，只有少数几种方法被用于测试（例如预测缺陷），而且这些方法只在少数几个项目上进行了测试。本文将55种半监督学习器应用于714个项目上，发现半监督的\"共同训练方法\"比其他方法表现更好。具体而言，在标记了仅2.5%的数据后，使用共同训练方法的预测结果与使用100%的数据进行预测的结果相媲美。然而，需要谨慎使用共同训练方法，因为共同训练方法的选择具体取决于...",
    "tldr": "该论文研究了半监督软件缺陷预测器中的\"共同训练\"方法的价值，并发现这种方法可以利用少量标记数据取得与使用全部数据相媲美的预测结果。",
    "en_tdlr": "This paper examines the value of \"co-training\" in semi-supervised software defect predictors and finds that this method can achieve competitive predictions using a small amount of labeled data compared to using all the data."
}