{
    "title": "Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation. (arXiv:2211.11004v3 [cs.LG] UPDATED)",
    "abstract": "Model-based deep learning has achieved astounding successes due in part to the availability of large-scale real-world data. However, processing such massive amounts of data comes at a considerable cost in terms of computations, storage, training and the search for good neural architectures. Dataset distillation has thus recently come to the fore. This paradigm involves distilling information from large real-world datasets into tiny and compact synthetic datasets such that processing the latter ideally yields similar performances as the former. State-of-the-art methods primarily rely on learning the synthetic dataset by matching the gradients obtained during training between the real and synthetic data. However, these gradient-matching methods suffer from the so-called accumulated trajectory error caused by the discrepancy between the distillation and subsequent evaluation. To mitigate the adverse impact of this accumulated trajectory error, we propose a novel approach that encourages t",
    "link": "http://arxiv.org/abs/2211.11004",
    "context": "Title: Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation. (arXiv:2211.11004v3 [cs.LG] UPDATED)\nAbstract: Model-based deep learning has achieved astounding successes due in part to the availability of large-scale real-world data. However, processing such massive amounts of data comes at a considerable cost in terms of computations, storage, training and the search for good neural architectures. Dataset distillation has thus recently come to the fore. This paradigm involves distilling information from large real-world datasets into tiny and compact synthetic datasets such that processing the latter ideally yields similar performances as the former. State-of-the-art methods primarily rely on learning the synthetic dataset by matching the gradients obtained during training between the real and synthetic data. However, these gradient-matching methods suffer from the so-called accumulated trajectory error caused by the discrepancy between the distillation and subsequent evaluation. To mitigate the adverse impact of this accumulated trajectory error, we propose a novel approach that encourages t",
    "path": "papers/22/11/2211.11004.json",
    "total_tokens": 1079,
    "translated_title": "通过减少累积轨迹误差来提高数据集精炼效果的论文翻译",
    "translated_abstract": "由于大型真实世界数据集的可用性，基于模型的深度学习取得了惊人的成功。然而，处理如此大量的数据在计算、存储、训练和搜寻良好的神经结构等方面成本相当高昂，因此数据集精炼近期成为焦点。这种范式涉及获取真实世界数据集中的信息并将其提炼为微小紧凑的合成数据集，以便在理想情况下处理后者的表现类似于前者。当前最先进的方法主要依靠学习通过在真实数据和合成数据之间匹配训练期间获得的梯度的合成数据集。然而，这些梯度匹配方法受到所谓的累积轨迹误差的影响，此误差是由于精炼和后续评估之间不一致导致的。为了减轻这种累积轨迹误差的不利影响，我们提出了一种新方法，鼓励将累积轨迹误差从训练阶段转移到精炼阶段。具体而言，我们引入了一种损失项，在精炼过程中最小化累积轨迹误差。我们在不同数据集和网络架构上的实验表明，我们的方法优于当前最先进的方法并显著减少累积轨迹误差。",
    "tldr": "通过新增损失项最小化累积轨迹误差，从而提高数据集精炼效果，并且在多种数据集和网络架构上的实验证明该方法优于当前最先进的方法。",
    "en_tdlr": "This paper proposes a novel approach to improve dataset distillation by minimizing the accumulated trajectory error using a loss term during distillation. The experiments on various datasets and network architectures show that this method outperforms state-of-the-art approaches."
}