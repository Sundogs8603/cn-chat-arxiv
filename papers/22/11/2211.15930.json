{
    "title": "Offline Supervised Learning V.S. Online Direct Policy Optimization: A Comparative Study and A Unified Training Paradigm for Neural Network-Based Optimal Feedback Control. (arXiv:2211.15930v2 [math.OC] UPDATED)",
    "abstract": "This work is concerned with solving neural network-based feedback controllers efficiently for optimal control problems. We first conduct a comparative study of two mainstream approaches: offline supervised learning and online direct policy optimization. Albeit the training part of the supervised learning approach is relatively easy, the success of the method heavily depends on the optimal control dataset generated by open-loop optimal control solvers. In contrast, direct optimization turns the optimal control problem into an optimization problem directly without any requirement of pre-computing, but the dynamics-related objective can be hard to optimize when the problem is complicated. Our results highlight the priority of offline supervised learning in terms of both optimality and training time. To overcome the main challenges, dataset, and optimization, in the two approaches respectively, we complement them and propose the Pre-train and Fine-tune strategy as a unified training paradi",
    "link": "http://arxiv.org/abs/2211.15930",
    "context": "Title: Offline Supervised Learning V.S. Online Direct Policy Optimization: A Comparative Study and A Unified Training Paradigm for Neural Network-Based Optimal Feedback Control. (arXiv:2211.15930v2 [math.OC] UPDATED)\nAbstract: This work is concerned with solving neural network-based feedback controllers efficiently for optimal control problems. We first conduct a comparative study of two mainstream approaches: offline supervised learning and online direct policy optimization. Albeit the training part of the supervised learning approach is relatively easy, the success of the method heavily depends on the optimal control dataset generated by open-loop optimal control solvers. In contrast, direct optimization turns the optimal control problem into an optimization problem directly without any requirement of pre-computing, but the dynamics-related objective can be hard to optimize when the problem is complicated. Our results highlight the priority of offline supervised learning in terms of both optimality and training time. To overcome the main challenges, dataset, and optimization, in the two approaches respectively, we complement them and propose the Pre-train and Fine-tune strategy as a unified training paradi",
    "path": "papers/22/11/2211.15930.json",
    "total_tokens": 885,
    "translated_title": "离线监督学习与在线直接策略优化的比较研究及神经网络优化反馈控制的统一训练范式",
    "translated_abstract": "本文旨在高效解决基于神经网络的最优反馈控制问题。首先对离线监督学习和在线直接策略优化这两种主流方法进行了比较研究。虽然离线监督学习方法的训练部分相对容易，但其成功与否严重依赖于由开环最优控制求解器生成的最优控制数据集。相反，直接优化将最优控制问题直接转化为优化问题，无需预先计算，但在问题复杂时，与动力学相关的目标可能难以优化。我们的结果突出了离线监督学习在最优性和训练时间方面的优势。为了克服两种方法中的主要挑战，即数据集和优化问题，我们互补它们，并提出了预训练和微调策略作为一种统一的训练范式。",
    "tldr": "本文对离线监督学习和在线直接策略优化进行了比较研究，结果突出了离线监督学习在最优性和训练时间方面的优势。为了克服两种方法中的主要挑战，我们提出了预训练和微调策略作为一种统一的训练范式。"
}