{
    "title": "Late Audio-Visual Fusion for In-The-Wild Speaker Diarization. (arXiv:2211.01299v2 [eess.AS] UPDATED)",
    "abstract": "Speaker diarization is well studied for constrained audios but little explored for challenging in-the-wild videos, which have more speakers, shorter utterances, and inconsistent on-screen speakers. We address this gap by proposing an audio-visual diarization model which combines audio-only and visual-centric sub-systems via late fusion. For audio, we show that an attractor-based end-to-end system (EEND-EDA) performs remarkably well when trained with our proposed recipe of a simulated proxy dataset, and propose an improved version, EEND-EDA++, that uses attention in decoding and a speaker recognition loss during training to better handle the larger number of speakers. The visual-centric sub-system leverages facial attributes and lip-audio synchrony for identity and speech activity estimation of on-screen speakers. Both sub-systems surpass the state of the art (SOTA) by a large margin, with the fused audio-visual system achieving a new SOTA on the AVA-AVD benchmark.",
    "link": "http://arxiv.org/abs/2211.01299",
    "context": "Title: Late Audio-Visual Fusion for In-The-Wild Speaker Diarization. (arXiv:2211.01299v2 [eess.AS] UPDATED)\nAbstract: Speaker diarization is well studied for constrained audios but little explored for challenging in-the-wild videos, which have more speakers, shorter utterances, and inconsistent on-screen speakers. We address this gap by proposing an audio-visual diarization model which combines audio-only and visual-centric sub-systems via late fusion. For audio, we show that an attractor-based end-to-end system (EEND-EDA) performs remarkably well when trained with our proposed recipe of a simulated proxy dataset, and propose an improved version, EEND-EDA++, that uses attention in decoding and a speaker recognition loss during training to better handle the larger number of speakers. The visual-centric sub-system leverages facial attributes and lip-audio synchrony for identity and speech activity estimation of on-screen speakers. Both sub-systems surpass the state of the art (SOTA) by a large margin, with the fused audio-visual system achieving a new SOTA on the AVA-AVD benchmark.",
    "path": "papers/22/11/2211.01299.json",
    "total_tokens": 1093,
    "translated_title": "在野外的说话者分辨中的后期音频-视觉融合",
    "translated_abstract": "音频视觉融合是为了应对具有更多说话者、较短发言和不一致的屏幕说话者的具有挑战性的野外视频而提出的。我们通过提出一种音频视觉分辨模型，通过后期融合将仅包含音频和以视觉为中心的子系统相结合。对于音频，我们展示了一个基于吸引子的端到端系统（EEND-EDA）在使用我们提出的一个模拟代理数据集的训练配方时表现出色，并提出了改进版的EEND-EDA++，在解码过程中使用注意力和在训练过程中使用说话者识别损失来更好地处理更多的说话者。以视觉为中心的子系统利用面部属性和唇音同步来估计屏幕上说话者的身份和语音活动。两个子系统都大幅超过了当前最先进技术 (SOTA)，通过融合音频-视觉系统在AVA-AVD基准测试中取得了新的SOTA。",
    "tldr": "本论文提出了一种音频-视觉融合的模型，通过后期融合将音频和视觉信息相结合，用于解决在野外视频中具有挑战性的说话者分辨问题。通过使用模拟代理数据集进行训练，并引入注意力机制和说话者识别损失，我们提出的模型在处理更多说话者时表现更好。此外，利用面部属性和唇音同步的视觉子系统能够估计屏幕上说话者的身份和语音活动。整体而言，我们的模型在AVA-AVD基准测试上取得了新的最先进结果。"
}