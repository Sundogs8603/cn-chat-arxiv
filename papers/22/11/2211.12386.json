{
    "title": "A Recursively Recurrent Neural Network (R2N2) Architecture for Learning Iterative Algorithms. (arXiv:2211.12386v2 [cs.LG] UPDATED)",
    "abstract": "Meta-learning of numerical algorithms for a given task consists of the data-driven identification and adaptation of an algorithmic structure and the associated hyperparameters. To limit the complexity of the meta-learning problem, neural architectures with a certain inductive bias towards favorable algorithmic structures can, and should, be used. We generalize our previously introduced Runge-Kutta neural network to a recursively recurrent neural network (R2N2) superstructure for the design of customized iterative algorithms. In contrast to off-the-shelf deep learning approaches, it features a distinct division into modules for generation of information and for the subsequent assembly of this information towards a solution. Local information in the form of a subspace is generated by subordinate, inner, iterations of recurrent function evaluations starting at the current outer iterate. The update to the next outer iterate is computed as a linear combination of these evaluations, reducing",
    "link": "http://arxiv.org/abs/2211.12386",
    "context": "Title: A Recursively Recurrent Neural Network (R2N2) Architecture for Learning Iterative Algorithms. (arXiv:2211.12386v2 [cs.LG] UPDATED)\nAbstract: Meta-learning of numerical algorithms for a given task consists of the data-driven identification and adaptation of an algorithmic structure and the associated hyperparameters. To limit the complexity of the meta-learning problem, neural architectures with a certain inductive bias towards favorable algorithmic structures can, and should, be used. We generalize our previously introduced Runge-Kutta neural network to a recursively recurrent neural network (R2N2) superstructure for the design of customized iterative algorithms. In contrast to off-the-shelf deep learning approaches, it features a distinct division into modules for generation of information and for the subsequent assembly of this information towards a solution. Local information in the form of a subspace is generated by subordinate, inner, iterations of recurrent function evaluations starting at the current outer iterate. The update to the next outer iterate is computed as a linear combination of these evaluations, reducing",
    "path": "papers/22/11/2211.12386.json",
    "total_tokens": 929,
    "translated_title": "用于学习迭代算法的递归循环神经网络 (R2N2) 架构",
    "translated_abstract": "数值算法的元学习包括对算法结构和相关超参数的数据驱动识别和适应。为了限制元学习问题的复杂性，可以使用具有对有利算法结构的某种归纳偏差的神经网络架构。我们将之前介绍的龙格－库塔神经网络推广为一种递归循环神经网络 (R2N2) 超结构，用于设计定制的迭代算法。与现成的深度学习方法不同，它具有明确的模块划分，用于生成信息和将该信息组装成解决方案。通过从当前外部迭代开始进行从属内部迭代的循环函数评估，可以生成局部信息，以子空间的形式呈现。下一次外部迭代的更新是通过这些评估的线性组合计算得出的，从而减少算法的复杂性。",
    "tldr": "本文提出了一个名为 R2N2 的递归循环神经网络结构，用于学习定制的迭代算法。与传统的深度学习方法不同的是，R2N2 将生成信息和组装信息的过程划分为不同的模块，通过在每次迭代中评估函数来生成局部信息，并将这些评估的线性组合用于更新下一次迭代的结果，从而降低算法的复杂性。",
    "en_tdlr": "This paper introduces a recursively recurrent neural network (R2N2) architecture for learning customized iterative algorithms, which features distinct division of generation and assembly of information and reduces algorithm complexity by evaluating functions and updating results in each iteration."
}