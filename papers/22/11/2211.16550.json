{
    "title": "Soft Alignment Objectives for Robust Adaptation of Language Generation. (arXiv:2211.16550v2 [cs.CL] UPDATED)",
    "abstract": "Domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application. However, the traditional adaptation by further training on in-domain data rapidly weakens the model's ability to generalize to other domains, making the open-ended deployments of the adapted models prone to errors. This work introduces novel training objectives built upon a semantic similarity of the predicted tokens to the reference.  Our results show that (1) avoiding the common assumption of a single correct prediction by constructing the training target from tokens' semantic similarity can mitigate catastrophic forgetting during domain adaptation, while (2) preserving the quality of the adaptation, (3) with negligible additions to compute costs.  In the broader context, the objectives grounded in a continuous token similarity pioneer the exploration of the middle ground between the efficient but na\\\"{\\i}ve exact-match token-level objectives and expressive b",
    "link": "http://arxiv.org/abs/2211.16550",
    "context": "Title: Soft Alignment Objectives for Robust Adaptation of Language Generation. (arXiv:2211.16550v2 [cs.CL] UPDATED)\nAbstract: Domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application. However, the traditional adaptation by further training on in-domain data rapidly weakens the model's ability to generalize to other domains, making the open-ended deployments of the adapted models prone to errors. This work introduces novel training objectives built upon a semantic similarity of the predicted tokens to the reference.  Our results show that (1) avoiding the common assumption of a single correct prediction by constructing the training target from tokens' semantic similarity can mitigate catastrophic forgetting during domain adaptation, while (2) preserving the quality of the adaptation, (3) with negligible additions to compute costs.  In the broader context, the objectives grounded in a continuous token similarity pioneer the exploration of the middle ground between the efficient but na\\\"{\\i}ve exact-match token-level objectives and expressive b",
    "path": "papers/22/11/2211.16550.json",
    "total_tokens": 905,
    "translated_title": "用于生成语言的软对齐目标的鲁棒适应",
    "translated_abstract": "领域自适应允许生成语言模型解决应用领域转移造成的特定缺陷。然而，通过在领域内数据上进行进一步训练来进行传统适应会迅速削弱模型推广到其他领域的能力，使得调整后模型的无限部署容易出现错误。本工作介绍了建立在预测令牌与参考语义相似性的新型训练目标。我们的结果表明，避免单个正确预测的常见假设，通过构建来自令牌语义相似性的训练目标可以缓解领域适应期间的灾难性遗忘，同时在保持调整质量方面具有可忽略的计算成本增加。在更广泛的背景下，基于连续的令牌相似度的目标引领了高效但显式令牌级目标和具有表现力的基于连续令牌表示的目标之间中间地带的探索。",
    "tldr": "本研究提出了一种基于预测令牌与参考语义相似性的新型训练目标，可以在领域自适应中缓解灾难性遗忘，同时又可以保持调整质量，并且计算成本增加可忽略不计。",
    "en_tdlr": "This study proposes a novel training objective based on the semantic similarity between predicted tokens and reference, which mitigates catastrophic forgetting during domain adaptation while preserving adaptation quality and with negligible additional compute costs."
}