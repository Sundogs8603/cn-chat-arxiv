{
    "title": "Dense Hebbian neural networks: a replica symmetric picture of unsupervised learning. (arXiv:2211.14067v2 [cond-mat.dis-nn] UPDATED)",
    "abstract": "We consider dense, associative neural-networks trained with no supervision and we investigate their computational capabilities analytically, via a statistical-mechanics approach, and numerically, via Monte Carlo simulations. In particular, we obtain a phase diagram summarizing their performance as a function of the control parameters such as the quality and quantity of the training dataset and the network storage, valid in the limit of large network size and structureless datasets. Moreover, we establish a bridge between macroscopic observables standardly used in statistical mechanics and loss functions typically used in the machine learning. As technical remarks, from the analytic side, we implement large deviations and stability analysis within Guerra's interpolation to tackle the not-Gaussian distributions involved in the post-synaptic potentials while, from the computational counterpart, we insert Plefka approximation in the Monte Carlo scheme, to speed up the evaluation of the syn",
    "link": "http://arxiv.org/abs/2211.14067",
    "context": "Title: Dense Hebbian neural networks: a replica symmetric picture of unsupervised learning. (arXiv:2211.14067v2 [cond-mat.dis-nn] UPDATED)\nAbstract: We consider dense, associative neural-networks trained with no supervision and we investigate their computational capabilities analytically, via a statistical-mechanics approach, and numerically, via Monte Carlo simulations. In particular, we obtain a phase diagram summarizing their performance as a function of the control parameters such as the quality and quantity of the training dataset and the network storage, valid in the limit of large network size and structureless datasets. Moreover, we establish a bridge between macroscopic observables standardly used in statistical mechanics and loss functions typically used in the machine learning. As technical remarks, from the analytic side, we implement large deviations and stability analysis within Guerra's interpolation to tackle the not-Gaussian distributions involved in the post-synaptic potentials while, from the computational counterpart, we insert Plefka approximation in the Monte Carlo scheme, to speed up the evaluation of the syn",
    "path": "papers/22/11/2211.14067.json",
    "total_tokens": 948,
    "translated_title": "密集的贺维模型神经网络：无监督学习的对称副本描述",
    "translated_abstract": "我们研究了无监督训练的密集关联神经网络，并通过统计力学方法和蒙特卡洛模拟进行了计算能力的分析。特别地，我们在大网络规模和无结构数据集的极限情况下获得了一个相图，总结了网络性能与训练数据集的质量、数量以及网络存储等控制参数之间的关系。此外，我们建立了统计力学中常用的宏观可观测量与机器学习中常用的损失函数之间的联系。在技术上，从解析的角度，我们运用Guerra的插值实现了大偏差和稳定性分析，用于处理与突触后电位相关的非高斯分布；从计算的角度，我们将Plefka近似插入到蒙特卡洛方案中，以加速突触强度的评估。",
    "tldr": "本文研究了无监督训练的密集贺维神经网络，并通过统计力学方法和蒙特卡洛模拟分析了其计算能力。我们得到了一个相图，总结了网络性能与训练数据集质量、数量和网络存储之间的关系，并建立了统计力学中的宏观可观测量与机器学习中的损失函数的联系。"
}