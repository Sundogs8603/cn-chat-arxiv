{
    "title": "Realization of Causal Representation Learning to Adjust Confounding Bias in Latent Space. (arXiv:2211.08573v5 [cs.LG] UPDATED)",
    "abstract": "Causal DAGs(Directed Acyclic Graphs) are usually considered in a 2D plane. Edges indicate causal effects' directions and imply their corresponding time-passings. Due to the natural restriction of statistical models, effect estimation is usually approximated by averaging the individuals' correlations, i.e., observational changes over a specific time. However, in the context of Machine Learning on large-scale questions with complex DAGs, such slight biases can snowball to distort global models - More importantly, it has practically impeded the development of AI, for instance, the weak generalizability of causal models. In this paper, we redefine causal DAG as \\emph{do-DAG}, in which variables' values are no longer time-stamp-dependent, and timelines can be seen as axes. By geometric explanation of multi-dimensional do-DAG, we identify the \\emph{Causal Representation Bias} and its necessary factors, differentiated from common confounding biases. Accordingly, a DL(Deep Learning)-based fram",
    "link": "http://arxiv.org/abs/2211.08573",
    "context": "Title: Realization of Causal Representation Learning to Adjust Confounding Bias in Latent Space. (arXiv:2211.08573v5 [cs.LG] UPDATED)\nAbstract: Causal DAGs(Directed Acyclic Graphs) are usually considered in a 2D plane. Edges indicate causal effects' directions and imply their corresponding time-passings. Due to the natural restriction of statistical models, effect estimation is usually approximated by averaging the individuals' correlations, i.e., observational changes over a specific time. However, in the context of Machine Learning on large-scale questions with complex DAGs, such slight biases can snowball to distort global models - More importantly, it has practically impeded the development of AI, for instance, the weak generalizability of causal models. In this paper, we redefine causal DAG as \\emph{do-DAG}, in which variables' values are no longer time-stamp-dependent, and timelines can be seen as axes. By geometric explanation of multi-dimensional do-DAG, we identify the \\emph{Causal Representation Bias} and its necessary factors, differentiated from common confounding biases. Accordingly, a DL(Deep Learning)-based fram",
    "path": "papers/22/11/2211.08573.json",
    "total_tokens": 925,
    "tldr": "本文重新定义因果DAG为“do-DAG”，提出了一种基于DL的框架，利用多维do-DAG几何解释确定了“因果表示偏差”及其必要因素，从而调整混淆偏差。",
    "en_tdlr": "This paper redefines causal DAG as \"do-DAG\", proposes a DL-based framework, and uses multi-dimensional do-DAG geometry to identify \"Causal Representation Bias\" and its necessary factors, thus adjusting confounding bias."
}