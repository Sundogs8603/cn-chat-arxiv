{
    "title": "X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks. (arXiv:2211.12402v2 [cs.CV] UPDATED)",
    "abstract": "Vision language pre-training aims to learn alignments between vision and language from a large amount of data. Most existing methods only learn image-text alignments. Some others utilize pre-trained object detectors to leverage vision language alignments at the object level. In this paper, we propose to learn multi-grained vision language alignments by a unified pre-training framework that learns multi-grained aligning and multi-grained localization simultaneously. Based on it, we present X$^2$-VLM, an all-in-one model with a flexible modular architecture, in which we further unify image-text pre-training and video-text pre-training in one model. X$^2$-VLM is able to learn unlimited visual concepts associated with diverse text descriptions. Experiment results show that X$^2$-VLM performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale. Moreover, we show that the modular design of X$^2$-VLM results in",
    "link": "http://arxiv.org/abs/2211.12402",
    "context": "Title: X$^2$-VLM: All-In-One Pre-trained Model For Vision-Language Tasks. (arXiv:2211.12402v2 [cs.CV] UPDATED)\nAbstract: Vision language pre-training aims to learn alignments between vision and language from a large amount of data. Most existing methods only learn image-text alignments. Some others utilize pre-trained object detectors to leverage vision language alignments at the object level. In this paper, we propose to learn multi-grained vision language alignments by a unified pre-training framework that learns multi-grained aligning and multi-grained localization simultaneously. Based on it, we present X$^2$-VLM, an all-in-one model with a flexible modular architecture, in which we further unify image-text pre-training and video-text pre-training in one model. X$^2$-VLM is able to learn unlimited visual concepts associated with diverse text descriptions. Experiment results show that X$^2$-VLM performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale. Moreover, we show that the modular design of X$^2$-VLM results in",
    "path": "papers/22/11/2211.12402.json",
    "total_tokens": 1057,
    "translated_title": "X$^2$-VLM: 全能的视觉-语言任务预训练模型",
    "translated_abstract": "视觉-语言预训练旨在从大量数据中学习视觉和语言之间的对齐。大多数现有的方法只学习图像-文本对齐。其他一些方法利用预先训练的物体检测器在对象级别上利用视觉-语言对齐。本文提出了一种统一的预训练框架，同时学习多粒度的视觉-语言对齐和多粒度的定位，从而实现多粒度的视觉-语言对齐。基于此，我们提出了全能模型X$^2$-VLM，具有灵活的模块化架构，在一个模型中进一步统一了图像-文本预训练和视频-文本预训练。X$^2$-VLM能够学习与多样的文本描述相关的无限视觉概念。实验结果显示，X$^2$-VLM在图像-文本和视频-文本任务的基础和大规模上表现最好，在性能和模型规模之间取得了良好的平衡。此外，我们还展示了X$^2$-VLM的模块化设计导致了...",
    "tldr": "X$^2$-VLM是一个全能的视觉-语言任务预训练模型，利用统一的框架实现了多粒度的视觉-语言对齐和定位，并在一个模型中统一了图像-文本和视频-文本预训练。实验结果显示，X$^2$-VLM在各种任务上表现最好，并且在性能和模型规模之间取得了良好的平衡。",
    "en_tdlr": "X$^2$-VLM is an all-in-one pre-trained model for vision-language tasks, which achieves multi-grained vision-language alignments and localization in a unified framework, and unifies image-text and video-text pre-training in one model. Experimental results demonstrate that X$^2$-VLM performs the best across various tasks, striking a good balance between performance and model scale."
}