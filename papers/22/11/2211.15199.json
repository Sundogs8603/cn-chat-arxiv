{
    "title": "Large Pre-Trained Models with Extra-Large Vocabularies: A Contrastive Analysis of Hebrew BERT Models and a New One to Outperform Them All. (arXiv:2211.15199v2 [cs.CL] UPDATED)",
    "abstract": "We present a new pre-trained language model (PLM) for modern Hebrew, termed AlephBERTGimmel, which employs a much larger vocabulary (128K items) than standard Hebrew PLMs before. We perform a contrastive analysis of this model against all previous Hebrew PLMs (mBERT, heBERT, AlephBERT) and assess the effects of larger vocabularies on task performance. Our experiments show that larger vocabularies lead to fewer splits, and that reducing splits is better for model performance, across different tasks. All in all this new model achieves new SOTA on all available Hebrew benchmarks, including Morphological Segmentation, POS Tagging, Full Morphological Analysis, NER, and Sentiment Analysis. Subsequently we advocate for PLMs that are larger not only in terms of number of layers or training data, but also in terms of their vocabulary. We release the new model publicly for unrestricted use.",
    "link": "http://arxiv.org/abs/2211.15199",
    "context": "Title: Large Pre-Trained Models with Extra-Large Vocabularies: A Contrastive Analysis of Hebrew BERT Models and a New One to Outperform Them All. (arXiv:2211.15199v2 [cs.CL] UPDATED)\nAbstract: We present a new pre-trained language model (PLM) for modern Hebrew, termed AlephBERTGimmel, which employs a much larger vocabulary (128K items) than standard Hebrew PLMs before. We perform a contrastive analysis of this model against all previous Hebrew PLMs (mBERT, heBERT, AlephBERT) and assess the effects of larger vocabularies on task performance. Our experiments show that larger vocabularies lead to fewer splits, and that reducing splits is better for model performance, across different tasks. All in all this new model achieves new SOTA on all available Hebrew benchmarks, including Morphological Segmentation, POS Tagging, Full Morphological Analysis, NER, and Sentiment Analysis. Subsequently we advocate for PLMs that are larger not only in terms of number of layers or training data, but also in terms of their vocabulary. We release the new model publicly for unrestricted use.",
    "path": "papers/22/11/2211.15199.json",
    "total_tokens": 970,
    "translated_title": "带着额外大词汇量的大型预训练模型：希伯来语BERT模型对比分析和一种新的模型来超越它们。",
    "translated_abstract": "本文介绍了一种名为AlephBERTGimmel的现代希伯来语新预训练语言模型（PLM），其使用比标准希伯来PLMs更大的词汇量（128K项目）。我们对这个模型进行对比分析，与所有以前的希伯来PLMs（mBERT，heBERT，AlephBERT）进行比较，并评估更大词汇量对任务性能的影响。我们的实验表明，更大的词汇量导致更少的分裂，而减少分裂对于模型性能更好，适用于不同任务。总的来说，这个新模型在包括分词，词性标注，全形态分析，命名实体识别和情感分析在内的所有可用的希伯来语基准测试上都达到了最先进的水平。因此，我们提倡不仅以层数或训练数据数量为衡量标准，而且还以词汇量为标准的更大PLMs。我们公开发布这个新模型，可以不受限制地使用。",
    "tldr": "阐述了新的预训练语言模型AlephBERTGimmel在希伯来语基准测试上的表现，其使用更高的词汇量，达到了新的最新最好的性能。",
    "en_tdlr": "This paper presents a new pre-trained language model, AlephBERTGimmel, with a larger vocabulary than traditional Hebrew models. The study shows that larger vocabularies lead to improved task performance, and AlephBERTGimmel outperforms previous Hebrew models on various benchmarks, including NER and Sentiment Analysis, advocating for the use of larger vocabularies in PLMs."
}