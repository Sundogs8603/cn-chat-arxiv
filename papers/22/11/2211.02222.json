{
    "title": "The Benefits of Model-Based Generalization in Reinforcement Learning. (arXiv:2211.02222v3 [cs.LG] UPDATED)",
    "abstract": "Model-Based Reinforcement Learning (RL) is widely believed to have the potential to improve sample efficiency by allowing an agent to synthesize large amounts of imagined experience. Experience Replay (ER) can be considered a simple kind of model, which has proved effective at improving the stability and efficiency of deep RL. In principle, a learned parametric model could improve on ER by generalizing from real experience to augment the dataset with additional plausible experience. However, given that learned value functions can also generalize, it is not immediately obvious why model generalization should be better. Here, we provide theoretical and empirical insight into when, and how, we can expect data generated by a learned model to be useful. First, we provide a simple theorem motivating how learning a model as an intermediate step can narrow down the set of possible value functions more than learning a value function directly from data using the Bellman equation. Second, we prov",
    "link": "http://arxiv.org/abs/2211.02222",
    "context": "Title: The Benefits of Model-Based Generalization in Reinforcement Learning. (arXiv:2211.02222v3 [cs.LG] UPDATED)\nAbstract: Model-Based Reinforcement Learning (RL) is widely believed to have the potential to improve sample efficiency by allowing an agent to synthesize large amounts of imagined experience. Experience Replay (ER) can be considered a simple kind of model, which has proved effective at improving the stability and efficiency of deep RL. In principle, a learned parametric model could improve on ER by generalizing from real experience to augment the dataset with additional plausible experience. However, given that learned value functions can also generalize, it is not immediately obvious why model generalization should be better. Here, we provide theoretical and empirical insight into when, and how, we can expect data generated by a learned model to be useful. First, we provide a simple theorem motivating how learning a model as an intermediate step can narrow down the set of possible value functions more than learning a value function directly from data using the Bellman equation. Second, we prov",
    "path": "papers/22/11/2211.02222.json",
    "total_tokens": 914,
    "translated_title": "模型驱动泛化在强化学习中的效益",
    "translated_abstract": "模型驱动的强化学习（RL）被广泛认为具有提高样本效率的潜力，因为它允许代理器合成大量的想象经验。经验回放（ER）可以被视为一种简单的模型，已被证明能够提高深度强化学习的稳定性和效率。理论上，一个学习的参数化模型可以通过从真实经验中泛化来增添数据集中的其他可信经验，从而改进ER。然而，考虑到学习的值函数也具有泛化能力，模型泛化为何更好并不明显。在这篇论文中，我们提供了理论和实证研究结果，揭示了学习模型生成的数据何时以及如何能够有效。首先，我们提供了一个简单的定理，解释了通过学习模型作为中间步骤比直接使用贝尔曼方程从数据中学习值函数如何缩小可能值函数集合的范围。其次，我们提供实证结果，证明通过学习模型生成的数据可以提供更多有用的信息，从而提高强化学习的效果。",
    "tldr": "这篇论文研究了模型驱动泛化在强化学习中的效益，通过学习模型生成的数据可以提供更多有用的信息，从而提高强化学习的效果。",
    "en_tdlr": "This paper investigates the benefits of model-based generalization in reinforcement learning, showing that the data generated by a learned model can provide more useful information to improve the effectiveness of reinforcement learning."
}