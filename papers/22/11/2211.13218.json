{
    "title": "CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning. (arXiv:2211.13218v2 [cs.CV] UPDATED)",
    "abstract": "Computer vision models suffer from a phenomenon known as catastrophic forgetting when learning novel concepts from continuously shifting training data. Typical solutions for this continual learning problem require extensive rehearsal of previously seen data, which increases memory costs and may violate data privacy. Recently, the emergence of large-scale pre-trained vision transformer models has enabled prompting approaches as an alternative to data-rehearsal. These approaches rely on a key-query mechanism to generate prompts and have been found to be highly resistant to catastrophic forgetting in the well-established rehearsal-free continual learning setting. However, the key mechanism of these methods is not trained end-to-end with the task sequence. Our experiments show that this leads to a reduction in their plasticity, hence sacrificing new task accuracy, and inability to benefit from expanded parameter capacity. We instead propose to learn a set of prompt components which are ass",
    "link": "http://arxiv.org/abs/2211.13218",
    "context": "Title: CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning. (arXiv:2211.13218v2 [cs.CV] UPDATED)\nAbstract: Computer vision models suffer from a phenomenon known as catastrophic forgetting when learning novel concepts from continuously shifting training data. Typical solutions for this continual learning problem require extensive rehearsal of previously seen data, which increases memory costs and may violate data privacy. Recently, the emergence of large-scale pre-trained vision transformer models has enabled prompting approaches as an alternative to data-rehearsal. These approaches rely on a key-query mechanism to generate prompts and have been found to be highly resistant to catastrophic forgetting in the well-established rehearsal-free continual learning setting. However, the key mechanism of these methods is not trained end-to-end with the task sequence. Our experiments show that this leads to a reduction in their plasticity, hence sacrificing new task accuracy, and inability to benefit from expanded parameter capacity. We instead propose to learn a set of prompt components which are ass",
    "path": "papers/22/11/2211.13218.json",
    "total_tokens": 1042,
    "translated_title": "CODA-Prompt：基于分解注意力提示的无重训练连续学习方法",
    "translated_abstract": "计算机视觉模型在学习不断变化的训练数据中的新概念时容易产生所谓的灾难性遗忘现象。解决这个连续学习问题的典型方法需要对先前已经见过的数据进行大量的重复训练，这增加了内存成本并可能违反数据隐私。最近，大规模预训练视觉变换器模型的出现使提示方法成为一种替代数据重复训练的方法。这些方法依靠关键查询机制生成提示，并已被发现在已经建立的无重训练连续学习设置中高度抵抗灾难性遗忘。然而，这些方法的关键机制没有与任务序列一起进行端到端训练。我们的实验表明，这会导致它们的可塑性降低，从而牺牲新任务准确性，并无法从扩展的参数容量中受益。我们提出了一种新的连续学习方法CODA-Prompt，它使用分解注意力提示机制结合蒸馏损失来训练提示组件，从而实现与任务序列的端到端训练。我们在一系列视觉数据集上的实验表明，CODA-Prompt优于最近的提示方法，而不需要重复训练或额外的资源。",
    "tldr": "CODA-Prompt是一种基于分解注意力提示，无需重复训练即可连续学习的方法，相比于其他提示方法具有更好的性能和效率。",
    "en_tdlr": "CODA-Prompt is a rehearsal-free continual learning approach based on a decomposed attention-based prompting mechanism, which achieves superior performance and efficiency compared to other prompting methods."
}