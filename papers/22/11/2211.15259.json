{
    "title": "A Call to Reflect on Evaluation Practices for Failure Detection in Image Classification. (arXiv:2211.15259v2 [cs.CV] UPDATED)",
    "abstract": "Reliable application of machine learning-based decision systems in the wild is one of the major challenges currently investigated by the field. A large portion of established approaches aims to detect erroneous predictions by means of assigning confidence scores. This confidence may be obtained by either quantifying the model's predictive uncertainty, learning explicit scoring functions, or assessing whether the input is in line with the training distribution. Curiously, while these approaches all state to address the same eventual goal of detecting failures of a classifier upon real-life application, they currently constitute largely separated research fields with individual evaluation protocols, which either exclude a substantial part of relevant methods or ignore large parts of relevant failure sources. In this work, we systematically reveal current pitfalls caused by these inconsistencies and derive requirements for a holistic and realistic evaluation of failure detection. To demon",
    "link": "http://arxiv.org/abs/2211.15259",
    "context": "Title: A Call to Reflect on Evaluation Practices for Failure Detection in Image Classification. (arXiv:2211.15259v2 [cs.CV] UPDATED)\nAbstract: Reliable application of machine learning-based decision systems in the wild is one of the major challenges currently investigated by the field. A large portion of established approaches aims to detect erroneous predictions by means of assigning confidence scores. This confidence may be obtained by either quantifying the model's predictive uncertainty, learning explicit scoring functions, or assessing whether the input is in line with the training distribution. Curiously, while these approaches all state to address the same eventual goal of detecting failures of a classifier upon real-life application, they currently constitute largely separated research fields with individual evaluation protocols, which either exclude a substantial part of relevant methods or ignore large parts of relevant failure sources. In this work, we systematically reveal current pitfalls caused by these inconsistencies and derive requirements for a holistic and realistic evaluation of failure detection. To demon",
    "path": "papers/22/11/2211.15259.json",
    "total_tokens": 900,
    "translated_title": "在图像分类的失败检测评估实践上进行反思的呼吁",
    "translated_abstract": "在野外可靠地应用基于机器学习的决策系统是当前领域正在研究的主要挑战之一。许多现有的方法旨在通过分配置信度得到检测到错误预测的结果。这种置信度可以通过量化模型的预测不确定性、学习明确的评分函数或评估输入是否符合训练分布来获得。尽管这些方法都声明要解决在实际应用中检测分类器失败的相同最终目标，但它们目前在很大程度上构成了各自独立的研究领域，具有单独的评估协议，这些协议可能排除了大量相关方法或忽略了大量相关的失败来源。在这项工作中，我们系统地揭示了这些不一致性带来的当前缺陷，并提出了对于全面和现实的失败检测评估的要求。",
    "tldr": "本篇论文呼吁对图像分类中失败检测的评估实践进行反思。虽然现有的技术都在尝试通过置信度检测错误预测，但是它们目前构成了独立的研究领域，存在评估标准的不一致性。因此，我们需要进行综合和现实的评估，以解决当前存在的问题。",
    "en_tdlr": "This paper calls for reflection on evaluation practices for failure detection in image classification. Despite existing techniques attempting to detect erroneous predictions through confidence scores, they currently constitute separate research fields with inconsistent evaluation protocols. A holistic and realistic evaluation is needed to address current issues."
}