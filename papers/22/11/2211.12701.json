{
    "title": "Continual Learning of Natural Language Processing Tasks: A Survey. (arXiv:2211.12701v2 [cs.CL] UPDATED)",
    "abstract": "Continual learning (CL) is a learning paradigm that emulates the human capability of learning and accumulating knowledge continually without forgetting the previously learned knowledge and also transferring the learned knowledge to help learn new tasks better. This survey presents a comprehensive review and analysis of the recent progress of CL in NLP, which has significant differences from CL in computer vision and machine learning. It covers (1) all CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting (CF) prevention, (3) knowledge transfer (KT), which is particularly important for NLP tasks; and (4) some theory and the hidden challenge of inter-task class separation (ICS). (1), (3) and (4) have not been included in the existing survey. Finally, a list of future directions is discussed.",
    "link": "http://arxiv.org/abs/2211.12701",
    "context": "Title: Continual Learning of Natural Language Processing Tasks: A Survey. (arXiv:2211.12701v2 [cs.CL] UPDATED)\nAbstract: Continual learning (CL) is a learning paradigm that emulates the human capability of learning and accumulating knowledge continually without forgetting the previously learned knowledge and also transferring the learned knowledge to help learn new tasks better. This survey presents a comprehensive review and analysis of the recent progress of CL in NLP, which has significant differences from CL in computer vision and machine learning. It covers (1) all CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting (CF) prevention, (3) knowledge transfer (KT), which is particularly important for NLP tasks; and (4) some theory and the hidden challenge of inter-task class separation (ICS). (1), (3) and (4) have not been included in the existing survey. Finally, a list of future directions is discussed.",
    "path": "papers/22/11/2211.12701.json",
    "total_tokens": 772,
    "translated_title": "自然语言处理任务的持续学习：一项调查",
    "translated_abstract": "持续学习是一种学习范式，模拟人类不断学习和积累知识的能力，不会忘记之前学过的知识，并将学到的知识传递给新任务更好地学习。本文调查了NLP中CL的最新进展，它与计算机视觉和机器学习中的CL有显着区别。它涵盖了（1）所有CL设置及现有技术分类；（2）防止灾难性遗忘（CF）；（3）知识迁移（KT），对NLP任务尤其重要；以及（4）一些理论和交任务类分离（ICS）的隐含挑战。本文还讨论了未来方向的一些列表。",
    "tldr": "本文综述了NLP中持续学习的最新进展，其中CF预防、知识迁移和跨任务类分离等方面对NLP任务至关重要，并讨论了未来研究方向。",
    "en_tdlr": "This survey presents recent progress of continual learning (CL) in NLP, covering CF prevention, knowledge transfer (KT) crucial for NLP, inter-task class separation (ICS), and future directions."
}