{
    "title": "BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder. (arXiv:2211.00792v2 [eess.AS] UPDATED)",
    "abstract": "We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech recognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced encoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR has been actively studied, aiming to utilize versatile linguistic knowledge for generating accurate text. One crucial factor that makes this integration challenging lies in the vocabulary mismatch; the vocabulary constructed for a pre-trained LM is generally too large for E2E-ASR training and is likely to have a mismatch against a target ASR domain. To overcome such an issue, we propose BECTRA, an extended version of our previous BERT-CTC, that realizes BERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based model, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder using a vocabulary suitable for a target task. With the combination of the transducer and BERT-CTC, we also propose a novel inference algorithm for",
    "link": "http://arxiv.org/abs/2211.00792",
    "context": "Title: BECTRA: Transducer-based End-to-End ASR with BERT-Enhanced Encoder. (arXiv:2211.00792v2 [eess.AS] UPDATED)\nAbstract: We present BERT-CTC-Transducer (BECTRA), a novel end-to-end automatic speech recognition (E2E-ASR) model formulated by the transducer with a BERT-enhanced encoder. Integrating a large-scale pre-trained language model (LM) into E2E-ASR has been actively studied, aiming to utilize versatile linguistic knowledge for generating accurate text. One crucial factor that makes this integration challenging lies in the vocabulary mismatch; the vocabulary constructed for a pre-trained LM is generally too large for E2E-ASR training and is likely to have a mismatch against a target ASR domain. To overcome such an issue, we propose BECTRA, an extended version of our previous BERT-CTC, that realizes BERT-based E2E-ASR using a vocabulary of interest. BECTRA is a transducer-based model, which adopts BERT-CTC for its encoder and trains an ASR-specific decoder using a vocabulary suitable for a target task. With the combination of the transducer and BERT-CTC, we also propose a novel inference algorithm for",
    "path": "papers/22/11/2211.00792.json",
    "total_tokens": 1001,
    "tldr": "本文提出了BECTRA，一种采用BERT增强编码器的转录器端到端语音识别模型，通过使用一个适合目标任务的词汇进行训练，解决了大规模预训练语言模型与目标领域之间的词汇不匹配问题。",
    "en_tdlr": "This paper proposes BECTRA, a transducer-based end-to-end automatic speech recognition model with a BERT-enhanced encoder, which addresses the vocabulary mismatch issue between large-scale pre-trained language models and target ASR domains by using a vocabulary suitable for the target task during training."
}