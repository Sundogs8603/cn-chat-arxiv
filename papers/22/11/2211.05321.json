{
    "title": "Fairness and bias correction in machine learning for depression prediction: results from four different study populations. (arXiv:2211.05321v2 [cs.LG] UPDATED)",
    "abstract": "A significant level of stigma and inequality exists in mental healthcare, especially in under-served populations, which spreads through collected data. When not properly accounted for, machine learning (ML) models learned from data can reinforce the structural biases already present in society. Here, we present a systematic study of bias in ML models designed to predict depression in four different case studies covering different countries and populations. We find that standard ML approaches show regularly biased behaviors. However, we show that standard mitigation techniques, and our own post-hoc method, can be effective in reducing the level of unfair bias. We provide practical recommendations to develop ML models for depression risk prediction with increased fairness and trust in the real world. No single best ML model for depression prediction provides equality of outcomes. This emphasizes the importance of analyzing fairness during model selection and transparent reporting about t",
    "link": "http://arxiv.org/abs/2211.05321",
    "context": "Title: Fairness and bias correction in machine learning for depression prediction: results from four different study populations. (arXiv:2211.05321v2 [cs.LG] UPDATED)\nAbstract: A significant level of stigma and inequality exists in mental healthcare, especially in under-served populations, which spreads through collected data. When not properly accounted for, machine learning (ML) models learned from data can reinforce the structural biases already present in society. Here, we present a systematic study of bias in ML models designed to predict depression in four different case studies covering different countries and populations. We find that standard ML approaches show regularly biased behaviors. However, we show that standard mitigation techniques, and our own post-hoc method, can be effective in reducing the level of unfair bias. We provide practical recommendations to develop ML models for depression risk prediction with increased fairness and trust in the real world. No single best ML model for depression prediction provides equality of outcomes. This emphasizes the importance of analyzing fairness during model selection and transparent reporting about t",
    "path": "papers/22/11/2211.05321.json",
    "total_tokens": 947,
    "translated_title": "机器学习在抑郁症预测中的公平性与偏差矫正：来自四个不同研究人群的结果。",
    "translated_abstract": "在心理保健中，尤其是在不受关注的人群中，存在着相当程度的污名化和不平等，这种不平等会扩散到收集的数据中。如果不适当地考虑机器学习(ML)模型所学的数据，这些模型就会强化已经存在于社会中的结构性偏差。在这里，我们对设计用于预测抑郁症的ML模型的偏差进行了有系统的研究，并在四个不同的案例研究中涵盖了不同的国家和人群。我们发现标准的ML方法显示出常规偏差行为。然而，我们展示了标准的缓解技术以及我们自己的事后方法可以有效地降低不公平偏差的级别。我们提供了实用的建议，以开发预测抑郁症风险的ML模型，在真实世界中提高公平性和信任度。没有单一最好的预测抑郁症的ML模型可以提供结果的平等。这强调了在模型选择过程中分析公平性以及透明报告的重要性。",
    "tldr": "本文研究了设计用于预测抑郁症的机器学习模型的公平性问题，并给出了有效的偏差矫正方法。这项研究强调了分析公平性以及透明报告的重要性。",
    "en_tdlr": "This paper studies the issue of fairness in the design of machine learning models for depression prediction, and presents effective bias correction methods. The study highlights the importance of analyzing fairness and transparent reporting."
}