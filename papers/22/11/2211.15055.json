{
    "title": "AdaTask: A Task-aware Adaptive Learning Rate Approach to Multi-task Learning. (arXiv:2211.15055v2 [cs.LG] UPDATED)",
    "abstract": "Multi-task learning (MTL) models have demonstrated impressive results in computer vision, natural language processing, and recommender systems. Even though many approaches have been proposed, how well these approaches balance different tasks on each parameter still remains unclear. In this paper, we propose to measure the task dominance degree of a parameter by the total updates of each task on this parameter. Specifically, we compute the total updates by the exponentially decaying Average of the squared Updates (AU) on a parameter from the corresponding task.Based on this novel metric, we observe that many parameters in existing MTL methods, especially those in the higher shared layers, are still dominated by one or several tasks. The dominance of AU is mainly due to the dominance of accumulative gradients from one or several tasks. Motivated by this, we propose a Task-wise Adaptive learning rate approach, AdaTask in short, to separate the \\emph{accumulative gradients} and hence the l",
    "link": "http://arxiv.org/abs/2211.15055",
    "context": "Title: AdaTask: A Task-aware Adaptive Learning Rate Approach to Multi-task Learning. (arXiv:2211.15055v2 [cs.LG] UPDATED)\nAbstract: Multi-task learning (MTL) models have demonstrated impressive results in computer vision, natural language processing, and recommender systems. Even though many approaches have been proposed, how well these approaches balance different tasks on each parameter still remains unclear. In this paper, we propose to measure the task dominance degree of a parameter by the total updates of each task on this parameter. Specifically, we compute the total updates by the exponentially decaying Average of the squared Updates (AU) on a parameter from the corresponding task.Based on this novel metric, we observe that many parameters in existing MTL methods, especially those in the higher shared layers, are still dominated by one or several tasks. The dominance of AU is mainly due to the dominance of accumulative gradients from one or several tasks. Motivated by this, we propose a Task-wise Adaptive learning rate approach, AdaTask in short, to separate the \\emph{accumulative gradients} and hence the l",
    "path": "papers/22/11/2211.15055.json",
    "total_tokens": 1042,
    "translated_title": "AdaTask: 一种面向多任务学习的任务感知自适应学习率方法",
    "translated_abstract": "多任务学习（MTL）模型已在计算机视觉、自然语言处理和推荐系统等领域展现出令人瞩目的结果。尽管已经提出了许多方法，但这些方法如何在每个参数上平衡不同任务仍然不清楚。在本文中，我们提出通过每个任务对该参数进行的总更新来衡量参数的任务优势度。具体而言，我们通过指数衰减的平均更新（AU）来计算每个任务在该参数上的总更新数。基于这一新颖的度量标准，我们观察到现有MTL方法中的许多参数，尤其是在较高的共享层中的参数，仍然受到一个或几个任务的支配。AU的支配主要是由于一个或几个任务的梯度累积导致的。受此启发，我们提出了一种名为AdaTask的任务感知自适应学习率方法，以分离不同任务之间的累积梯度，从而平衡不同任务的重要性。AdaTask根据AU值自适应地调整不同任务的学习率，以平衡不同任务的重要性。我们在各种基准测试上评估了AdaTask，并证明它始终优于现有的MTL方法。",
    "tldr": "提出了一种名为AdaTask的任务感知自适应学习率方法，通过自适应地调整不同任务的学习率，以平衡不同任务的重要性，从而在各种基准测试上始终优于现有的MTL方法。",
    "en_tdlr": "AdaTask is a task-aware adaptive learning rate approach proposed for multi-task learning, which balances the importance of different tasks through adaptively adjusting their learning rates based on a novel metric, and consistently outperforms existing MTL methods on various benchmarks."
}