{
    "title": "Latent Video Diffusion Models for High-Fidelity Long Video Generation. (arXiv:2211.13221v2 [cs.CV] UPDATED)",
    "abstract": "AI-generated content has attracted lots of attention recently, but photo-realistic video synthesis is still challenging. Although many attempts using GANs and autoregressive models have been made in this area, the visual quality and length of generated videos are far from satisfactory. Diffusion models have shown remarkable results recently but require significant computational resources. To address this, we introduce lightweight video diffusion models by leveraging a low-dimensional 3D latent space, significantly outperforming previous pixel-space video diffusion models under a limited computational budget. In addition, we propose hierarchical diffusion in the latent space such that longer videos with more than one thousand frames can be produced. To further overcome the performance degradation issue for long video generation, we propose conditional latent perturbation and unconditional guidance that effectively mitigate the accumulated errors during the extension of video length. Ext",
    "link": "http://arxiv.org/abs/2211.13221",
    "context": "Title: Latent Video Diffusion Models for High-Fidelity Long Video Generation. (arXiv:2211.13221v2 [cs.CV] UPDATED)\nAbstract: AI-generated content has attracted lots of attention recently, but photo-realistic video synthesis is still challenging. Although many attempts using GANs and autoregressive models have been made in this area, the visual quality and length of generated videos are far from satisfactory. Diffusion models have shown remarkable results recently but require significant computational resources. To address this, we introduce lightweight video diffusion models by leveraging a low-dimensional 3D latent space, significantly outperforming previous pixel-space video diffusion models under a limited computational budget. In addition, we propose hierarchical diffusion in the latent space such that longer videos with more than one thousand frames can be produced. To further overcome the performance degradation issue for long video generation, we propose conditional latent perturbation and unconditional guidance that effectively mitigate the accumulated errors during the extension of video length. Ext",
    "path": "papers/22/11/2211.13221.json",
    "total_tokens": 871,
    "translated_abstract": "近期，AI生成内容引起了广泛关注，但逼真的视频合成仍然具有挑战性。虽然这个领域已经尝试使用许多GAN和自回归模型，但生成视频的视觉质量和长度仍然难以令人满意。最近，扩散模型显示出了显着的结果，但需要显着的计算资源。为了解决这个问题，我们引入了轻量级视频扩散模型，通过利用低维3D潜在空间，在有限的计算预算下显著优于以前的像素空间视频扩散模型。此外，我们提出了基于潜在空间的分层扩散，这样可以生成超过一千帧的长视频。为了进一步克服长视频生成的性能下降问题，我们提出了条件潜在扰动和无条件指导，有效地减轻了视频长度扩展过程中累积错误的影响。",
    "tldr": "本文提出了高保真长视频生成的潜在视频扩散模型，该模型利用低维3D潜在空间实现了轻量级视频扩散，并通过条件潜在扰动和无条件指导有效减轻了视频长度扩展过程中累积错误的影响。",
    "en_tdlr": "The paper proposes latent video diffusion models for high-fidelity long video generation, which leverage a low-dimensional 3D latent space for lightweight diffusion and mitigate accumulated errors through conditional perturbation and unconditional guidance."
}