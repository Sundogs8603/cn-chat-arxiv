{
    "title": "UGIF: UI Grounded Instruction Following. (arXiv:2211.07615v2 [cs.CL] UPDATED)",
    "abstract": "Smartphone users often find it difficult to navigate myriad menus to perform common tasks such as \"How to block calls from unknown numbers?\". Currently, help documents with step-by-step instructions are manually written to aid the user. The user experience can be further enhanced by grounding the instructions in the help document to the UI and overlaying a tutorial on the phone UI. To build such tutorials, several natural language processing components including retrieval, parsing, and grounding are necessary, but there isn't any relevant dataset for such a task. Thus, we introduce UGIF-DataSet, a multi-lingual, multi-modal UI grounded dataset for step-by-step task completion on the smartphone containing 4,184 tasks across 8 languages. As an initial approach to this problem, we propose retrieving the relevant instruction steps based on the user's query and parsing the steps using Large Language Models (LLMs) to generate macros that can be executed on-device. The instruction steps are o",
    "link": "http://arxiv.org/abs/2211.07615",
    "context": "Title: UGIF: UI Grounded Instruction Following. (arXiv:2211.07615v2 [cs.CL] UPDATED)\nAbstract: Smartphone users often find it difficult to navigate myriad menus to perform common tasks such as \"How to block calls from unknown numbers?\". Currently, help documents with step-by-step instructions are manually written to aid the user. The user experience can be further enhanced by grounding the instructions in the help document to the UI and overlaying a tutorial on the phone UI. To build such tutorials, several natural language processing components including retrieval, parsing, and grounding are necessary, but there isn't any relevant dataset for such a task. Thus, we introduce UGIF-DataSet, a multi-lingual, multi-modal UI grounded dataset for step-by-step task completion on the smartphone containing 4,184 tasks across 8 languages. As an initial approach to this problem, we propose retrieving the relevant instruction steps based on the user's query and parsing the steps using Large Language Models (LLMs) to generate macros that can be executed on-device. The instruction steps are o",
    "path": "papers/22/11/2211.07615.json",
    "total_tokens": 758,
    "translated_title": "UGIF：UI 视觉引导下的指令跟踪",
    "translated_abstract": "智能手机用户经常会发现导航复杂的菜单执行常见任务变得困难，例如“如何屏蔽未知号码的来电？”目前，人工编写的逐步说明文件可帮助用户。我们提出了 UGIF-DataSet，一个多语言、多模态的 UI 视觉引导数据集，其中包含了 8 种语言的 4,184 个常用操作。作为解决这个问题的初步方法，我们提出基于用户查询检索相关指令步骤，使用大型语言模型 (LLM) 分析步骤并生成可在设备上执行的宏。",
    "tldr": "该论文提出了一个多语言、多模态的 UI 视觉引导数据集，旨在通过将指令步骤与 UI 视频相结合，帮助智能手机用户更轻松地完成任务。"
}