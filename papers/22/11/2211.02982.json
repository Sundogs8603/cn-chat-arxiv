{
    "title": "Event and Entity Extraction from Generated Video Captions. (arXiv:2211.02982v2 [cs.CV] UPDATED)",
    "abstract": "Annotation of multimedia data by humans is time-consuming and costly, while reliable automatic generation of semantic metadata is a major challenge. We propose a framework to extract semantic metadata from automatically generated video captions. As metadata, we consider entities, the entities' properties, relations between entities, and the video category. We employ two state-of-the-art dense video captioning models with masked transformer (MT) and parallel decoding (PVDC) to generate captions for videos of the ActivityNet Captions dataset. Our experiments show that it is possible to extract entities, their properties, relations between entities, and the video category from the generated captions. We observe that the quality of the extracted information is mainly influenced by the quality of the event localization in the video as well as the performance of the event caption generation.",
    "link": "http://arxiv.org/abs/2211.02982",
    "context": "Title: Event and Entity Extraction from Generated Video Captions. (arXiv:2211.02982v2 [cs.CV] UPDATED)\nAbstract: Annotation of multimedia data by humans is time-consuming and costly, while reliable automatic generation of semantic metadata is a major challenge. We propose a framework to extract semantic metadata from automatically generated video captions. As metadata, we consider entities, the entities' properties, relations between entities, and the video category. We employ two state-of-the-art dense video captioning models with masked transformer (MT) and parallel decoding (PVDC) to generate captions for videos of the ActivityNet Captions dataset. Our experiments show that it is possible to extract entities, their properties, relations between entities, and the video category from the generated captions. We observe that the quality of the extracted information is mainly influenced by the quality of the event localization in the video as well as the performance of the event caption generation.",
    "path": "papers/22/11/2211.02982.json",
    "total_tokens": 807,
    "translated_title": "生成视频字幕中的事件和实体提取",
    "translated_abstract": "由人工进行多媒体数据注释耗时且昂贵，而可靠的自动生成语义元数据是一个重大挑战。我们提出了一个从自动生成的视频字幕中提取语义元数据的框架。作为元数据，我们考虑实体、实体属性、实体之间的关系以及视频分类。我们使用两种最先进的密集视频字幕模型，即遮蔽转换器（MT）和并行解码（PVDC），为ActivityNet Captions数据集的视频生成字幕。我们的实验证明，从生成的字幕中提取实体、实体属性、实体之间的关系和视频分类是可能的。我们观察到，提取信息的质量主要受到视频中事件定位的质量以及事件字幕生成的性能的影响。",
    "tldr": "该论文提出了一个从生成的视频字幕中提取语义元数据的框架，通过使用密集视频字幕模型，可以提取实体、实体属性、实体之间的关系和视频分类。提取信息的质量受到事件定位质量和字幕生成性能的影响。",
    "en_tdlr": "This paper proposes a framework to extract semantic metadata from automatically generated video captions. By using dense video captioning models, entities, their properties, relations between entities, and the video category can be extracted. The quality of the extracted information is influenced by the quality of event localization and event caption generation."
}