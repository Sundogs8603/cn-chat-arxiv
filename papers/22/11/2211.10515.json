{
    "title": "Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments. (arXiv:2211.10515v2 [stat.ML] UPDATED)",
    "abstract": "Consider the problem of exploration in sparse-reward or reward-free environments, such as in Montezuma's Revenge. In the curiosity-driven paradigm, the agent is rewarded for how much each realized outcome differs from their predicted outcome. But using predictive error as intrinsic motivation is fragile in stochastic environments, as the agent may become trapped by high-entropy areas of the state-action space, such as a \"noisy TV\". In this work, we study a natural solution derived from structural causal models of the world: Our key idea is to learn representations of the future that capture precisely the unpredictable aspects of each outcome -- which we use as additional input for predictions, such that intrinsic rewards only reflect the predictable aspects of world dynamics. First, we propose incorporating such hindsight representations into models to disentangle \"noise\" from \"novelty\", yielding Curiosity in Hindsight: a simple and scalable generalization of curiosity that is robust t",
    "link": "http://arxiv.org/abs/2211.10515",
    "context": "Title: Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments. (arXiv:2211.10515v2 [stat.ML] UPDATED)\nAbstract: Consider the problem of exploration in sparse-reward or reward-free environments, such as in Montezuma's Revenge. In the curiosity-driven paradigm, the agent is rewarded for how much each realized outcome differs from their predicted outcome. But using predictive error as intrinsic motivation is fragile in stochastic environments, as the agent may become trapped by high-entropy areas of the state-action space, such as a \"noisy TV\". In this work, we study a natural solution derived from structural causal models of the world: Our key idea is to learn representations of the future that capture precisely the unpredictable aspects of each outcome -- which we use as additional input for predictions, such that intrinsic rewards only reflect the predictable aspects of world dynamics. First, we propose incorporating such hindsight representations into models to disentangle \"noise\" from \"novelty\", yielding Curiosity in Hindsight: a simple and scalable generalization of curiosity that is robust t",
    "path": "papers/22/11/2211.10515.json",
    "total_tokens": 961,
    "translated_title": "回顾中的好奇心：随机环境中的内在探索",
    "translated_abstract": "本文考虑在稀疏奖励或无奖励环境中的探索问题，如Montezuma's Revenge。在好奇心驱动范式中，代理被奖励实际结果与预测结果的差异。但在随机环境中，使用预测误差作为内在动机是脆弱的，因为代理可能被状态-动作空间中高熵区域（如“噪声电视”）所困住。本文提出了一种基于结构因果模型的自然解决方案：学习未来的表征，精确地捕捉每个结果的不可预测方面，并将其用作预测的额外输入，从而使内在奖励仅反映世界动态的可预测方面。首先，我们提出将这种回顾表征结合到模型中，以将“噪声”与“新奇”区分开来，得到了回顾中的好奇心：一种简单而可扩展的好奇心泛化方法，具有鲁棒性。",
    "tldr": "本文提出了一种基于结构因果模型的回顾中的好奇心方法，用于稀疏奖励或无奖励环境中的探索问题。该方法学习未来的表征，以捕捉每个结果的不可预测部分，并将其用作预测的额外输入，从而获得鲁棒的内在奖励。"
}