{
    "title": "Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules. (arXiv:2211.16691v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement Learning (RL) generally suffers from poor sample complexity, mostly due to the need to exhaustively explore the state-action space to find well-performing policies. On the other hand, we postulate that expert knowledge of the system often allows us to design simple rules we expect good policies to follow at all times. In this work, we hence propose a simple yet effective modification of continuous actor-critic frameworks to incorporate such rules and avoid regions of the state-action space that are known to be suboptimal, thereby significantly accelerating the convergence of RL agents. Concretely, we saturate the actions chosen by the agent if they do not comply with our intuition and, critically, modify the gradient update step of the policy to ensure the learning process is not affected by the saturation step. On a room temperature control case study, it allows agents to converge to well-performing policies up to 6-7x faster than classical agents without computational o",
    "link": "http://arxiv.org/abs/2211.16691",
    "context": "Title: Computationally Efficient Reinforcement Learning: Targeted Exploration leveraging simple Rules. (arXiv:2211.16691v2 [cs.LG] UPDATED)\nAbstract: Reinforcement Learning (RL) generally suffers from poor sample complexity, mostly due to the need to exhaustively explore the state-action space to find well-performing policies. On the other hand, we postulate that expert knowledge of the system often allows us to design simple rules we expect good policies to follow at all times. In this work, we hence propose a simple yet effective modification of continuous actor-critic frameworks to incorporate such rules and avoid regions of the state-action space that are known to be suboptimal, thereby significantly accelerating the convergence of RL agents. Concretely, we saturate the actions chosen by the agent if they do not comply with our intuition and, critically, modify the gradient update step of the policy to ensure the learning process is not affected by the saturation step. On a room temperature control case study, it allows agents to converge to well-performing policies up to 6-7x faster than classical agents without computational o",
    "path": "papers/22/11/2211.16691.json",
    "total_tokens": 981,
    "translated_title": "计算效率高的强化学习：基于简单规则的有针对性探索",
    "translated_abstract": "强化学习通常由于需要穷举探索状态-动作空间以找到表现良好的策略而导致样本复杂度不太好。然而，我们认为系统的专家知识通常允许我们设计简单规则，我们期望良好的策略始终遵循这些规则。因此，在本研究中，我们提出了一种简单而有效的连续演员-评论家框架的修改版本，以纳入这些规则并避免已知子优的状态-动作空间区域，从而显着加速强化学习代理程序的改进。具体而言，如果代理程序选择的动作不符合我们的直觉，我们会饱和这些动作，关键是修改策略的梯度更新步骤，以确保学习流程不受饱和步骤的影响。在一个房间温度控制案例研究中，它使代理程序以比传统代理程序快6-7倍的速度收敛到表现良好的策略，而不需要消耗额外的计算资源。",
    "tldr": "本研究提出了一种基于简单规则的有针对性探索方法，通过避免已知子优的状态-动作空间区域来更快地加速强化学习代理程序的收敛，并在一个房间温度控制案例研究中实现了比传统方法快6-7倍的速度收敛。",
    "en_tdlr": "This paper proposes a computationally efficient reinforcement learning method that incorporates simple rules to accelerate convergence of agents by avoiding suboptimal state-action space, achieving up to 6-7x faster convergence compared to traditional methods on a room temperature control case study."
}