{
    "title": "Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity. (arXiv:2211.02139v2 [cs.LG] UPDATED)",
    "abstract": "Existing regulations prohibit model developers from accessing protected attributes (gender, race, etc.), often resulting in fairness assessments on populations without knowing their protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset for auditing purposes). However, the model developers might be allowed to test their models for bias by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. In particular, we show that one can rec",
    "link": "http://arxiv.org/abs/2211.02139",
    "context": "Title: Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity. (arXiv:2211.02139v2 [cs.LG] UPDATED)\nAbstract: Existing regulations prohibit model developers from accessing protected attributes (gender, race, etc.), often resulting in fairness assessments on populations without knowing their protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset for auditing purposes). However, the model developers might be allowed to test their models for bias by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. In particular, we show that one can rec",
    "path": "papers/22/11/2211.02139.json",
    "total_tokens": 835,
    "translated_title": "通过平滑敏感度实现隐私保护的偏差查询",
    "translated_abstract": "现有法规禁止模型开发人员访问受保护属性（性别，种族等），这经常导致在不知道他们的受保护组的情况下对人群进行公平性评估。在这种情况下，机构通常采用模型开发人员（不可访问受保护属性训练模型）和合规团队（可能全面访问数据集以用于审计目的）之间的分离。但是，模型开发人员可能被允许通过查询合规团队获取组公平性指标来测试其模型的偏差。本文首先证明了仅仅查询公平指标（例如统计平等和平等赔率）可能会泄露个人的受保护属性给模型开发人员。我们证明了模型开发人员总是可以通过单个查询从测试数据集中识别目标个体的受保护属性。我们特别展示了一种方法，通过它可以平滑地减小敏感度来解决这个问题并保护隐私。",
    "tldr": "本文论证了查询公平指标可能会泄露受保护属性，提出了解决方案以保护隐私。"
}