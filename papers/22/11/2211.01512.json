{
    "title": "Convergence of the Inexact Langevin Algorithm and Score-based Generative Models in KL Divergence. (arXiv:2211.01512v2 [cs.LG] UPDATED)",
    "abstract": "We study the Inexact Langevin Dynamics (ILD), Inexact Langevin Algorithm (ILA), and Score-based Generative Modeling (SGM) when utilizing estimated score functions for sampling. Our focus lies in establishing stable biased convergence guarantees in terms of the Kullback-Leibler (KL) divergence. To achieve these guarantees, we impose two key assumptions: 1) the target distribution satisfies the log-Sobolev inequality (LSI), and 2) the score estimator exhibits a bounded Moment Generating Function (MGF) error. Notably, the MGF error assumption we adopt is more lenient compared to the $L^\\infty$ error assumption used in existing literature. However, it is stronger than the $L^2$ error assumption utilized in recent works, which often leads to unstable bounds. We explore the question of how to obtain a provably accurate score estimator that satisfies the MGF error assumption. Specifically, we demonstrate that a simple estimator based on kernel density estimation fulfills the MGF error assumpt",
    "link": "http://arxiv.org/abs/2211.01512",
    "context": "Title: Convergence of the Inexact Langevin Algorithm and Score-based Generative Models in KL Divergence. (arXiv:2211.01512v2 [cs.LG] UPDATED)\nAbstract: We study the Inexact Langevin Dynamics (ILD), Inexact Langevin Algorithm (ILA), and Score-based Generative Modeling (SGM) when utilizing estimated score functions for sampling. Our focus lies in establishing stable biased convergence guarantees in terms of the Kullback-Leibler (KL) divergence. To achieve these guarantees, we impose two key assumptions: 1) the target distribution satisfies the log-Sobolev inequality (LSI), and 2) the score estimator exhibits a bounded Moment Generating Function (MGF) error. Notably, the MGF error assumption we adopt is more lenient compared to the $L^\\infty$ error assumption used in existing literature. However, it is stronger than the $L^2$ error assumption utilized in recent works, which often leads to unstable bounds. We explore the question of how to obtain a provably accurate score estimator that satisfies the MGF error assumption. Specifically, we demonstrate that a simple estimator based on kernel density estimation fulfills the MGF error assumpt",
    "path": "papers/22/11/2211.01512.json",
    "total_tokens": 1091,
    "translated_title": "不精确 Langevin 算法与基于得分的生成模型在 KL 散度中的收敛性",
    "translated_abstract": "本文研究了不精确 Langevin 动力学（ILD）、不精确 Langevin 算法（ILA）和基于得分的生成建模（SGM）在利用估计得分函数进行采样时的情况。我们的重点在于建立关于 Kullback-Leibler（KL）散度的稳定偏差收敛保证。为了实现这些保证，我们采用了两个关键假设：1）目标分布满足对数 Sobolev 不等式（LSI），2）分数估计器展示出一个有界的矩阵生成函数（MGF）误差。值得注意的是，我们采用的 MGF 误差假设相比现有文献中使用的 $L^\\infty$ 误差假设更为宽松。然而，它比最近的作品中使用的 $L^2$ 误差假设更强，后者常常导致不稳定的边界。我们探讨了如何获得满足 MGF 误差假设的可靠分数估计器的问题。具体来说，我们证明了一种基于核密度估计的简单估计器满足 MGF 误差假设。",
    "tldr": "本文研究了不精确 Langevin 算法与基于得分的生成模型在 KL 散度中的收敛性，提出了建立稳定偏差收敛保证的两个关键假设：目标分布满足对数 Sobolev 不等式和分数估计器展示出有界的矩阵生成函数误差。作者探讨了如何获得可靠的分数估计器，并证明了基于核密度估计的简单估计器满足假设。",
    "en_tdlr": "This paper studies the convergence of the Inexact Langevin Algorithm and Score-based Generative Models in KL divergence and proposes two key assumptions of log-Sobolev inequality and bounded Moment Generating Function error to establish stable biased convergence guarantees. The authors explore how to obtain a reliable score estimator and prove that a simple estimator based on kernel density estimation satisfies the MGF error assumption."
}