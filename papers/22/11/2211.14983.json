{
    "title": "Multiagent Reinforcement Learning for Autonomous Routing and Pickup Problem with Adaptation to Variable Demand. (arXiv:2211.14983v2 [cs.MA] UPDATED)",
    "abstract": "We derive a learning framework to generate routing/pickup policies for a fleet of autonomous vehicles tasked with servicing stochastically appearing requests on a city map. We focus on policies that 1) give rise to coordination amongst the vehicles, thereby reducing wait times for servicing requests, 2) are non-myopic, and consider a-priori potential future requests, 3) can adapt to changes in the underlying demand distribution. Specifically, we are interested in policies that are adaptive to fluctuations of actual demand conditions in urban environments, such as on-peak vs. off-peak hours. We achieve this through a combination of (i) an online play algorithm that improves the performance of an offline-trained policy, and (ii) an offline approximation scheme that allows for adapting to changes in the underlying demand model. In particular, we achieve adaptivity of our learned policy to different demand distributions by quantifying a region of validity using the q-valid radius of a Wass",
    "link": "http://arxiv.org/abs/2211.14983",
    "context": "Title: Multiagent Reinforcement Learning for Autonomous Routing and Pickup Problem with Adaptation to Variable Demand. (arXiv:2211.14983v2 [cs.MA] UPDATED)\nAbstract: We derive a learning framework to generate routing/pickup policies for a fleet of autonomous vehicles tasked with servicing stochastically appearing requests on a city map. We focus on policies that 1) give rise to coordination amongst the vehicles, thereby reducing wait times for servicing requests, 2) are non-myopic, and consider a-priori potential future requests, 3) can adapt to changes in the underlying demand distribution. Specifically, we are interested in policies that are adaptive to fluctuations of actual demand conditions in urban environments, such as on-peak vs. off-peak hours. We achieve this through a combination of (i) an online play algorithm that improves the performance of an offline-trained policy, and (ii) an offline approximation scheme that allows for adapting to changes in the underlying demand model. In particular, we achieve adaptivity of our learned policy to different demand distributions by quantifying a region of validity using the q-valid radius of a Wass",
    "path": "papers/22/11/2211.14983.json",
    "total_tokens": 1026,
    "translated_title": "多智能体强化学习在自主路由和接送问题中的应用，可适应需求的变化",
    "translated_abstract": "我们推导出一个学习框架，用于为一组自主车辆在城市地图上服务于随机出现的请求时生成路由/接送策略。我们着重研究的策略是：1）产生协调作用，从而减少为服务请求等待的时间；2）是非近视策略，并考虑先前可能出现的未来请求；3）可以适应基础需求分布的变化。具体来说，我们感兴趣的策略是适应城市环境中实际需求条件的波动，例如高峰时间和非高峰时间等。我们通过以下方式实现：(i)能够改进离线训练策略性能的在线玩算法，和(ii)一种离线逼近方案，允许适应基于需求模型的变化。特别地，我们通过计算Wasserstein距离的q-valid半径来量化有效区域，从而实现对我们已学习策略对不同需求分布的适应性。我们的实验结果表明，我们提出的框架可以比基线启发式策略改善平均等待时间，并能够适应不断变化的需求模型。",
    "tldr": "本论文提出了一种多智能体强化学习的学习框架，用于在城市地图上服务于随机出现的请求，可以产生协调作用并考虑先前可能出现的未来请求，能够适应不同需求分布的变化。"
}