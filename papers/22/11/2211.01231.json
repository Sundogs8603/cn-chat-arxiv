{
    "title": "Interval Markov Decision Processes with Continuous Action-Spaces. (arXiv:2211.01231v2 [eess.SY] UPDATED)",
    "abstract": "Interval Markov Decision Processes (IMDPs) are finite-state uncertain Markov models, where the transition probabilities belong to intervals. Recently, there has been a surge of research on employing IMDPs as abstractions of stochastic systems for control synthesis. However, due to the absence of algorithms for synthesis over IMDPs with continuous action-spaces, the action-space is assumed discrete a-priori, which is a restrictive assumption for many applications. Motivated by this, we introduce continuous-action IMDPs (caIMDPs), where the bounds on transition probabilities are functions of the action variables, and study value iteration for maximizing expected cumulative rewards. Specifically, we decompose the max-min problem associated to value iteration to $|\\mathcal{Q}|$ max problems, where $|\\mathcal{Q}|$ is the number of states of the caIMDP. Then, exploiting the simple form of these max problems, we identify cases where value iteration over caIMDPs can be solved efficiently (e.g.",
    "link": "http://arxiv.org/abs/2211.01231",
    "context": "Title: Interval Markov Decision Processes with Continuous Action-Spaces. (arXiv:2211.01231v2 [eess.SY] UPDATED)\nAbstract: Interval Markov Decision Processes (IMDPs) are finite-state uncertain Markov models, where the transition probabilities belong to intervals. Recently, there has been a surge of research on employing IMDPs as abstractions of stochastic systems for control synthesis. However, due to the absence of algorithms for synthesis over IMDPs with continuous action-spaces, the action-space is assumed discrete a-priori, which is a restrictive assumption for many applications. Motivated by this, we introduce continuous-action IMDPs (caIMDPs), where the bounds on transition probabilities are functions of the action variables, and study value iteration for maximizing expected cumulative rewards. Specifically, we decompose the max-min problem associated to value iteration to $|\\mathcal{Q}|$ max problems, where $|\\mathcal{Q}|$ is the number of states of the caIMDP. Then, exploiting the simple form of these max problems, we identify cases where value iteration over caIMDPs can be solved efficiently (e.g.",
    "path": "papers/22/11/2211.01231.json",
    "total_tokens": 922,
    "translated_title": "连续动作空间的区间马尔可夫决策过程(IMDPs)",
    "translated_abstract": "区间马尔可夫决策过程(IMDPs)是有限状态不确定马尔可夫模型，其转移概率属于区间。最近，人们研究了将IMDPs作为随机系统的抽象以进行控制合成的方法。然而，由于缺乏适用于连续动作空间的IMDPs合成算法，因此先前一直假设动作空间是离散的，这对许多实际应用而言是一种限制性假设。因此，本文提出了连续动作IMDPs (caIMDPs)，其中转移概率的上下界是动作变量的函数，并研究了最大化期望累积奖励的值迭代算法。具体地，我们将与值迭代相关的max-min问题分解成$|\\mathcal{Q}|$ 个max问题，其中$|\\mathcal{Q}|$是caIMDP的状态数量。我们利用这些max问题的简单形式，确定了caIMDPs值迭代能够有效求解的情况（例如...）",
    "tldr": "本文介绍了连续动作IMDPs (caIMDPs)，研究了该过程上最大化期望累积奖励的值迭代算法，其中转移概率的上下界是动作变量的函数。",
    "en_tdlr": "This paper introduces continuous-action Interval Markov Decision Processes (caIMDPs) and studies value iteration for maximizing expected cumulative rewards, where the bounds on transition probabilities are functions of the action variables."
}