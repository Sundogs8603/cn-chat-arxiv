{
    "title": "UMFuse: Unified Multi View Fusion for Human Editing applications. (arXiv:2211.10157v4 [cs.CV] UPDATED)",
    "abstract": "Numerous pose-guided human editing methods have been explored by the vision community due to their extensive practical applications. However, most of these methods still use an image-to-image formulation in which a single image is given as input to produce an edited image as output. This objective becomes ill-defined in cases when the target pose differs significantly from the input pose. Existing methods then resort to in-painting or style transfer to handle occlusions and preserve content. In this paper, we explore the utilization of multiple views to minimize the issue of missing information and generate an accurate representation of the underlying human model. To fuse knowledge from multiple viewpoints, we design a multi-view fusion network that takes the pose key points and texture from multiple source images and generates an explainable per-pixel appearance retrieval map. Thereafter, the encodings from a separate network (trained on a single-view human reposing task) are merged i",
    "link": "http://arxiv.org/abs/2211.10157",
    "context": "Title: UMFuse: Unified Multi View Fusion for Human Editing applications. (arXiv:2211.10157v4 [cs.CV] UPDATED)\nAbstract: Numerous pose-guided human editing methods have been explored by the vision community due to their extensive practical applications. However, most of these methods still use an image-to-image formulation in which a single image is given as input to produce an edited image as output. This objective becomes ill-defined in cases when the target pose differs significantly from the input pose. Existing methods then resort to in-painting or style transfer to handle occlusions and preserve content. In this paper, we explore the utilization of multiple views to minimize the issue of missing information and generate an accurate representation of the underlying human model. To fuse knowledge from multiple viewpoints, we design a multi-view fusion network that takes the pose key points and texture from multiple source images and generates an explainable per-pixel appearance retrieval map. Thereafter, the encodings from a separate network (trained on a single-view human reposing task) are merged i",
    "path": "papers/22/11/2211.10157.json",
    "total_tokens": 908,
    "translated_title": "UMFuse：用于人体编辑应用的统一多视图融合技术",
    "translated_abstract": "对于人体编辑技术，视觉社区已经探讨了众多的姿势引导方法，因为它们有广泛的实际应用。然而，大多数方法仍然使用图像到图像的公式，其中一个单一的图像被给定为输入，以产生一个编辑过的图像为输出。当目标姿势与输入姿势有显著差异时，这一目标变得不明确。现有的方法则采用修补或样式转移来处理遮挡并保留内容。在本文中，我们探讨了利用多个视图来最小化信息丢失并生成下层人体模型的准确表示。为了融合多个视点的知识，我们设计了一个多视图融合网络，它从多个源图像中获取姿势关键点和纹理，并生成可解释的每像素外观检索映射。此后，从单视角人体换装任务中训练得到的编码被合并起来。",
    "tldr": "本文提出了UMFuse，一种利用多视图融合处理人体编辑任务的方法，通过设计一个多视图融合网络，利用多源图像中的关键点和纹理生成每像素外观检索映射，从而最小化信息丢失并生成准确的下层人体模型。",
    "en_tdlr": "In this paper, we propose UMFuse, a method that utilizes multi-view fusion to handle human editing by generating an explainable per-pixel appearance retrieval map using key points and texture from multiple source images, minimizing information loss and accurately representing the underlying human model."
}