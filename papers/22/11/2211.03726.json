{
    "title": "TAP-Vid: A Benchmark for Tracking Any Point in a Video. (arXiv:2211.03726v2 [cs.CV] UPDATED)",
    "abstract": "Generic motion understanding from video involves not only tracking objects, but also perceiving how their surfaces deform and move. This information is useful to make inferences about 3D shape, physical properties and object interactions. While the problem of tracking arbitrary physical points on surfaces over longer video clips has received some attention, no dataset or benchmark for evaluation existed, until now. In this paper, we first formalize the problem, naming it tracking any point (TAP). We introduce a companion benchmark, TAP-Vid, which is composed of both real-world videos with accurate human annotations of point tracks, and synthetic videos with perfect ground-truth point tracks. Central to the construction of our benchmark is a novel semi-automatic crowdsourced pipeline which uses optical flow estimates to compensate for easier, short-term motion like camera shake, allowing annotators to focus on harder sections of video. We validate our pipeline on synthetic data and prop",
    "link": "http://arxiv.org/abs/2211.03726",
    "context": "Title: TAP-Vid: A Benchmark for Tracking Any Point in a Video. (arXiv:2211.03726v2 [cs.CV] UPDATED)\nAbstract: Generic motion understanding from video involves not only tracking objects, but also perceiving how their surfaces deform and move. This information is useful to make inferences about 3D shape, physical properties and object interactions. While the problem of tracking arbitrary physical points on surfaces over longer video clips has received some attention, no dataset or benchmark for evaluation existed, until now. In this paper, we first formalize the problem, naming it tracking any point (TAP). We introduce a companion benchmark, TAP-Vid, which is composed of both real-world videos with accurate human annotations of point tracks, and synthetic videos with perfect ground-truth point tracks. Central to the construction of our benchmark is a novel semi-automatic crowdsourced pipeline which uses optical flow estimates to compensate for easier, short-term motion like camera shake, allowing annotators to focus on harder sections of video. We validate our pipeline on synthetic data and prop",
    "path": "papers/22/11/2211.03726.json",
    "total_tokens": 1090,
    "translated_title": "TAP-Vid：在视频中跟踪任何点的基准数据集",
    "translated_abstract": "从视频中获取普适的运动理解不仅涉及追踪物体，还需要感知它们的表面变形和运动。这些信息对于推断 3D 形状、物理属性和物体交互非常有用。虽然在较长的视频片段中追踪任意物理点的问题已经引起了一些关注，但直到现在还没有可用于评估的数据集或基准。在本文中，我们首先将这个问题形式化，并将其命名为跟踪任意点 (TAP)。我们引入了一个伴随数据集 TAP-Vid，它由具有准确人工标注的点迹的真实世界视频和具有完美地面实况点迹的合成视频组成。我们构建基准的关键是一个新颖的半自动众包流水线，它使用光流估计来弥补摄像机抖动等简单短期运动，让注释者专注于视频的更难部分。我们在合成数据上验证了我们的流水线，并提出了评估指标来衡量在 TAP-Vid 上的跟踪表现。我们的基准包括具有非刚性运动和遮挡的具有挑战性的序列，以及广泛的物体类别和摄像机运动。我们希望 TAP-Vid 能够鼓励研究这个重要而困难的问题，推动更好的算法来跟踪视频中任意物理点。",
    "tldr": "TAP-Vid是一个跟踪任何点在视频中的基准数据集，包含真实世界视频和合成视频，有助于推动解决跟踪任意物理点在视频中的难题。",
    "en_tdlr": "TAP-Vid is a benchmark dataset for tracking any point in a video, including real-world and synthetic videos, which aims to promote the development of better algorithms for solving the challenging problem of tracking arbitrary physical points in videos."
}