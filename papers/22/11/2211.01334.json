{
    "title": "MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction. (arXiv:2211.01334v3 [cs.IR] UPDATED)",
    "abstract": "New findings in natural language processing (NLP) demonstrate that the strong memorization capability contributes a lot to the success of Large Language Models (LLM). This inspires us to explicitly bring an independent memory mechanism into CTR ranking model to learn and memorize cross features' representations. In this paper, we propose multi-Hash Codebook NETwork (HCNet) as the memory mechanism for efficiently learning and memorizing representations of cross features in CTR tasks. HCNet uses a multi-hash codebook as the main memory place and the whole memory procedure consists of three phases: multi-hash addressing, memory restoring, and feature shrinking. We also propose a new CTR model named MemoNet which combines HCNet with a DNN backbone. Extensive experimental results on three public datasets and online test show that MemoNet reaches superior performance over state-of-the-art approaches. Besides, MemoNet shows scaling law of large language model in NLP, which means we can enlarg",
    "link": "http://arxiv.org/abs/2211.01334",
    "context": "Title: MemoNet: Memorizing All Cross Features' Representations Efficiently via Multi-Hash Codebook Network for CTR Prediction. (arXiv:2211.01334v3 [cs.IR] UPDATED)\nAbstract: New findings in natural language processing (NLP) demonstrate that the strong memorization capability contributes a lot to the success of Large Language Models (LLM). This inspires us to explicitly bring an independent memory mechanism into CTR ranking model to learn and memorize cross features' representations. In this paper, we propose multi-Hash Codebook NETwork (HCNet) as the memory mechanism for efficiently learning and memorizing representations of cross features in CTR tasks. HCNet uses a multi-hash codebook as the main memory place and the whole memory procedure consists of three phases: multi-hash addressing, memory restoring, and feature shrinking. We also propose a new CTR model named MemoNet which combines HCNet with a DNN backbone. Extensive experimental results on three public datasets and online test show that MemoNet reaches superior performance over state-of-the-art approaches. Besides, MemoNet shows scaling law of large language model in NLP, which means we can enlarg",
    "path": "papers/22/11/2211.01334.json",
    "total_tokens": 967,
    "translated_title": "MemoNet: 通过多哈希码本网络高效地记忆所有交叉特征表示以实现CTR预测",
    "translated_abstract": "自然语言处理（NLP）中的新发现表明，强大的记忆能力对大型语言模型（LLM）的成功起到了很大作用。这启发我们将独立的记忆机制引入CTR排名模型，以学习和记忆交叉特征的表示。本文提出了多哈希码本网络（HCNet）作为CTR任务中高效学习和记忆交叉特征表示的记忆机制。HCNet使用多哈希码本作为主要的记忆位置，并由多哈希寻址、记忆恢复和特征缩减三个阶段组成。我们还提出了一种名为MemoNet的新型CTR模型，将HCNet与DNN骨干网络相结合。广泛的实验结果在三个公共数据集和在线测试中表明，MemoNet在性能上优于最先进的方法。此外，MemoNet展现出NLP中的大型语言模型的扩展规律，这意味着我们可以扩大模型规模来提高性能。",
    "tldr": "本文提出了一种名为MemoNet的CTR模型，通过引入多哈希码本网络（HCNet）作为记忆机制，高效地学习和记忆交叉特征的表示。实验结果表明MemoNet在性能上优于最先进的方法，并且展现出NLP中的大型语言模型的扩展规律。",
    "en_tdlr": "This paper proposes a CTR model called MemoNet, which introduces a memory mechanism called multi-Hash Codebook Network (HCNet) to efficiently learn and memorize cross features' representations. Experimental results demonstrate the superior performance of MemoNet over state-of-the-art approaches, and it shows the scaling law of large language models in NLP."
}