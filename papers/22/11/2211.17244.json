{
    "title": "Tight Certification of Adversarially Trained Neural Networks via Nonconvex Low-Rank Semidefinite Relaxations. (arXiv:2211.17244v3 [cs.LG] UPDATED)",
    "abstract": "Adversarial training is well-known to produce high-quality neural network models that are empirically robust against adversarial perturbations. Nevertheless, once a model has been adversarially trained, one often desires a certification that the model is truly robust against all future attacks. Unfortunately, when faced with adversarially trained models, all existing approaches have significant trouble making certifications that are strong enough to be practically useful. Linear programming (LP) techniques in particular face a \"convex relaxation barrier\" that prevent them from making high-quality certifications, even after refinement with mixed-integer linear programming (MILP) and branch-and-bound (BnB) techniques. In this paper, we propose a nonconvex certification technique, based on a low-rank restriction of a semidefinite programming (SDP) relaxation. The nonconvex relaxation makes strong certifications comparable to much more expensive SDP methods, while optimizing over dramatica",
    "link": "http://arxiv.org/abs/2211.17244",
    "context": "Title: Tight Certification of Adversarially Trained Neural Networks via Nonconvex Low-Rank Semidefinite Relaxations. (arXiv:2211.17244v3 [cs.LG] UPDATED)\nAbstract: Adversarial training is well-known to produce high-quality neural network models that are empirically robust against adversarial perturbations. Nevertheless, once a model has been adversarially trained, one often desires a certification that the model is truly robust against all future attacks. Unfortunately, when faced with adversarially trained models, all existing approaches have significant trouble making certifications that are strong enough to be practically useful. Linear programming (LP) techniques in particular face a \"convex relaxation barrier\" that prevent them from making high-quality certifications, even after refinement with mixed-integer linear programming (MILP) and branch-and-bound (BnB) techniques. In this paper, we propose a nonconvex certification technique, based on a low-rank restriction of a semidefinite programming (SDP) relaxation. The nonconvex relaxation makes strong certifications comparable to much more expensive SDP methods, while optimizing over dramatica",
    "path": "papers/22/11/2211.17244.json",
    "total_tokens": 932,
    "translated_title": "通过非凸低秩半正定松弛实现对对抗训练神经网络的严格认证",
    "translated_abstract": "众所周知，对抗训练可以产生高质量的神经网络模型，这些模型在经验上对抗性扰动具有鲁棒性。然而，一旦进行了对抗性训练，人们通常希望证明该模型在未来的所有攻击中真正具有鲁棒性。不幸的是，面对对抗训练模型时，所有现有方法都难以做出足够有效的证明。特别是线性规划（LP）技术，即使经过混合整数线性规划（MILP）和分支定界（BnB）技术的改进，也会面临\"凸松弛壁垒\"，使得它们难以进行高质量的证明。因此，本文提出了一种基于低秩半正定松弛的非凸认证技术。非凸松弛可以进行与更昂贵的半正定规划（SDP）方法相媲美的强认证，同时优化范围更广。",
    "tldr": "本文提出了一种新的，基于低秩半正定松弛技术实现对对抗性训练神经网络的严格认证方法，它能够实现采用更便宜的SDP方法相当的强认证。"
}