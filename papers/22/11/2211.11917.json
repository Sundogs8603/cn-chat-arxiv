{
    "title": "Latent Iterative Refinement for Modular Source Separation. (arXiv:2211.11917v2 [cs.SD] UPDATED)",
    "abstract": "Traditional source separation approaches train deep neural network models end-to-end with all the data available at once by minimizing the empirical risk on the whole training set. On the inference side, after training the model, the user fetches a static computation graph and runs the full model on some specified observed mixture signal to get the estimated source signals. Additionally, many of those models consist of several basic processing blocks which are applied sequentially. We argue that we can significantly increase resource efficiency during both training and inference stages by reformulating a model's training and inference procedures as iterative mappings of latent signal representations. First, we can apply the same processing block more than once on its output to refine the input signal and consequently improve parameter efficiency. During training, we can follow a block-wise procedure which enables a reduction on memory requirements. Thus, one can train a very complicate",
    "link": "http://arxiv.org/abs/2211.11917",
    "context": "Title: Latent Iterative Refinement for Modular Source Separation. (arXiv:2211.11917v2 [cs.SD] UPDATED)\nAbstract: Traditional source separation approaches train deep neural network models end-to-end with all the data available at once by minimizing the empirical risk on the whole training set. On the inference side, after training the model, the user fetches a static computation graph and runs the full model on some specified observed mixture signal to get the estimated source signals. Additionally, many of those models consist of several basic processing blocks which are applied sequentially. We argue that we can significantly increase resource efficiency during both training and inference stages by reformulating a model's training and inference procedures as iterative mappings of latent signal representations. First, we can apply the same processing block more than once on its output to refine the input signal and consequently improve parameter efficiency. During training, we can follow a block-wise procedure which enables a reduction on memory requirements. Thus, one can train a very complicate",
    "path": "papers/22/11/2211.11917.json",
    "total_tokens": 797,
    "translated_title": "模块化源分离的潜在迭代优化",
    "translated_abstract": "传统的源分离方法通过最小化整个训练集上的经验风险，将深度神经网络模型端到端地训练。在推理阶段，用户通过获取静态计算图，并在特定的混合信号上运行完整的模型，来获取估计的源信号。此外，许多模型由多个基本处理块组成，这些处理块按顺序应用。我们认为，通过将模型的训练和推理过程重新定义为潜在信号表示的迭代映射，我们可以显著提高资源利用效率。首先，我们可以多次对输出应用相同的处理块，以改进输入信号并提高参数效率。在训练过程中，我们可以采用基块级的程序，这样可以减少内存需求。因此，可以训练一个非常复杂的模型...",
    "tldr": "通过将深度神经网络模型的训练和推理过程重新定义为潜在信号表示的迭代映射，能够显著提高模型的资源利用效率。",
    "en_tdlr": "By reformulating the training and inference procedures of deep neural network models as iterative mappings of latent signal representations, significant resource efficiency improvements can be achieved."
}