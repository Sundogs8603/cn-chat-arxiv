{
    "title": "A Close Look into the Calibration of Pre-trained Language Models. (arXiv:2211.00151v3 [cs.CL] UPDATED)",
    "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs' calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don't learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining ",
    "link": "http://arxiv.org/abs/2211.00151",
    "context": "Title: A Close Look into the Calibration of Pre-trained Language Models. (arXiv:2211.00151v3 [cs.CL] UPDATED)\nAbstract: Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs' calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don't learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining ",
    "path": "papers/22/11/2211.00151.json",
    "total_tokens": 970,
    "tldr": "本文对预训练语言模型校准问题进行了深入研究，发现预训练语言模型无法在训练过程中学习校准，不同的训练条件都会对校准性能产生影响。",
    "en_tdlr": "This paper conducts a detailed study on the calibration of pre-trained language models (PLMs), finding that PLMs do not learn to become calibrated during training, and that the calibration performance can be affected by various training conditions."
}