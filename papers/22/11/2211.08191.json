{
    "title": "Improved disentangled speech representations using contrastive learning in factorized hierarchical variational autoencoder. (arXiv:2211.08191v2 [eess.AS] UPDATED)",
    "abstract": "Leveraging the fact that speaker identity and content vary on different time scales, \\acrlong{fhvae} (\\acrshort{fhvae}) uses different latent variables to symbolize these two attributes. Disentanglement of these attributes is carried out by different prior settings of the corresponding latent variables. For the prior of speaker identity variable, \\acrshort{fhvae} assumes it is a Gaussian distribution with an utterance-scale varying mean and a fixed variance. By setting a small fixed variance, the training process promotes identity variables within one utterance gathering close to the mean of their prior. However, this constraint is relatively weak, as the mean of the prior changes between utterances. Therefore, we introduce contrastive learning into the \\acrshort{fhvae} framework, to make the speaker identity variables gathering when representing the same speaker, while distancing themselves as far as possible from those of other speakers. The model structure has not been changed in th",
    "link": "http://arxiv.org/abs/2211.08191",
    "context": "Title: Improved disentangled speech representations using contrastive learning in factorized hierarchical variational autoencoder. (arXiv:2211.08191v2 [eess.AS] UPDATED)\nAbstract: Leveraging the fact that speaker identity and content vary on different time scales, \\acrlong{fhvae} (\\acrshort{fhvae}) uses different latent variables to symbolize these two attributes. Disentanglement of these attributes is carried out by different prior settings of the corresponding latent variables. For the prior of speaker identity variable, \\acrshort{fhvae} assumes it is a Gaussian distribution with an utterance-scale varying mean and a fixed variance. By setting a small fixed variance, the training process promotes identity variables within one utterance gathering close to the mean of their prior. However, this constraint is relatively weak, as the mean of the prior changes between utterances. Therefore, we introduce contrastive learning into the \\acrshort{fhvae} framework, to make the speaker identity variables gathering when representing the same speaker, while distancing themselves as far as possible from those of other speakers. The model structure has not been changed in th",
    "path": "papers/22/11/2211.08191.json",
    "total_tokens": 1013,
    "translated_title": "基于因式分层变分自编码器的对比学习改进语音表征",
    "translated_abstract": "利用说话者身份和语音内容在不同时间尺度上变化的事实，因式分层变分自编码器使用不同的潜在变量来表示这两个属性。通过不同潜在变量的先验设置来实现这些属性的分离。对于说话者身份变量的先验设置，因式分层变分自编码器假设它是具有变化的均值和固定方差的高斯分布。通过设置一个较小的固定方差，训练过程促进了相同句子内的身份变量靠近其先验均值。为了使同一说话者的表示时身份变量相聚而与其他说话者的身份变量相隔，我们在因式分层变分自编码器框架中引入对比学习。模型结构没有发生改变。引入对比损失后，属性分离表示更有效地捕获了说话者身份。在TIMIT数据集和VCTK数据集上的实验表明，相对于传统的没有对比学习的因式分层变分自编码器，在说话者识别任务上，所提出的方法表现优异，达到了最新水平。",
    "tldr": "本文引入对比学习在因式分层变分自编码器中，实现更好的语音表征，通过捕获说话者身份改善模型性能，达到最新水平。",
    "en_tdlr": "This paper proposes using contrastive learning in factorized hierarchical variational autoencoder to improve disentangled speech representations. By introducing contrastive loss, the model can better capture speaker identity and achieve state-of-the-art performance on speaker identification tasks."
}