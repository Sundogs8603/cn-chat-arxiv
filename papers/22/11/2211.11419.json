{
    "title": "Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR. (arXiv:2211.11419v3 [cs.SD] UPDATED)",
    "abstract": "This paper presents an in-depth study on a Sequentially Sampled Chunk Conformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer first demonstrates the significant performance gains from using the sequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the Conformer encoder by allowing efficient cross-chunk interactions while keeping linear complexities. Furthermore, it explores taking advantage of chunked convolution to make use of the chunk-wise future context and integrates with casual convolution in the convolution layers to further reduce CER. We verify the proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results show that a state-of-the-art performance for streaming E2E ASR is achieved with CER 5.33% without LM rescoring. And, owing to its linear complexity, the SSC-Conformer can train with large batch sizes and infer more efficiently.",
    "link": "http://arxiv.org/abs/2211.11419",
    "context": "Title: Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR. (arXiv:2211.11419v3 [cs.SD] UPDATED)\nAbstract: This paper presents an in-depth study on a Sequentially Sampled Chunk Conformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer first demonstrates the significant performance gains from using the sequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the Conformer encoder by allowing efficient cross-chunk interactions while keeping linear complexities. Furthermore, it explores taking advantage of chunked convolution to make use of the chunk-wise future context and integrates with casual convolution in the convolution layers to further reduce CER. We verify the proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results show that a state-of-the-art performance for streaming E2E ASR is achieved with CER 5.33% without LM rescoring. And, owing to its linear complexity, the SSC-Conformer can train with large batch sizes and infer more efficiently.",
    "path": "papers/22/11/2211.11419.json",
    "total_tokens": 956,
    "translated_title": "串行采样块式Conformer网络在流式端到端ASR中的应用",
    "translated_abstract": "本文针对流式端到端语音识别 (E2E ASR) 提出了一种名为 SSC-Conformer 的串行采样块式 Conformer 模型。该模型使用串行采样块多头自注意力机制 (SSC-MHSA) 来提高跨块交互的效率，同时保持线性复杂度。此外，本文还提出利用块卷积来增加块级未来上下文，并将其与卷积层的因果卷积相结合以进一步降低 CER。在 AISHELL-1 基准测试中，实验结果表明 SSC-Conformer 在无语言模型重打分的情况下可以实现 CER 5.33%，达到了流式 E2E ASR 的最新性能水平，并且由于其线性复杂度，可以使用更大的批量进行训练并更高效地推理。",
    "tldr": "本论文提出了一种称为 SSC-Conformer 的块式模型，利用串行采样块自注意力机制提高块间交互效率，同时保持线性复杂度，将块卷积与因果卷积相结合以达到更好的 CER 表现，实验结果表明 SSC-Conformer 在 AISHELL-1 基准测试中取得了最新的流式 E2E ASR 性能水平。",
    "en_tdlr": "This paper proposes an SSC-Conformer model which utilizes sequentially sampled chunk-wise multi-head self-attention to improve inter-chunk interaction efficiency while maintaining linear complexity. The proposed model also incorporates chunked convolution to increase chunk-level future context and combines it with causal convolution for better CER performance. Experimental results show that the SSC-Conformer achieves the state-of-the-art performance for streaming E2E ASR on the AISHELL-1 benchmark without LM rescoring, and its linear complexity allows for efficient training and inference."
}