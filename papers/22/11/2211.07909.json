{
    "title": "Selective Memory Recursive Least Squares: Recast Forgetting into Memory in RBF Neural Network Based Real-Time Learning. (arXiv:2211.07909v2 [eess.SY] UPDATED)",
    "abstract": "In radial basis function neural network (RBFNN) based real-time learning tasks, forgetting mechanisms are widely used such that the neural network can keep its sensitivity to new data. However, with forgetting mechanisms, some useful knowledge will get lost simply because they are learned a long time ago, which we refer to as the passive knowledge forgetting phenomenon. To address this problem, this paper proposes a real-time training method named selective memory recursive least squares (SMRLS) in which the classical forgetting mechanisms are recast into a memory mechanism. Different from the forgetting mechanism, which mainly evaluates the importance of samples according to the time when samples are collected, the memory mechanism evaluates the importance of samples through both temporal and spatial distribution of samples. With SMRLS, the input space of the RBFNN is evenly divided into a finite number of partitions and a synthesized objective function is developed using synthesized ",
    "link": "http://arxiv.org/abs/2211.07909",
    "context": "Title: Selective Memory Recursive Least Squares: Recast Forgetting into Memory in RBF Neural Network Based Real-Time Learning. (arXiv:2211.07909v2 [eess.SY] UPDATED)\nAbstract: In radial basis function neural network (RBFNN) based real-time learning tasks, forgetting mechanisms are widely used such that the neural network can keep its sensitivity to new data. However, with forgetting mechanisms, some useful knowledge will get lost simply because they are learned a long time ago, which we refer to as the passive knowledge forgetting phenomenon. To address this problem, this paper proposes a real-time training method named selective memory recursive least squares (SMRLS) in which the classical forgetting mechanisms are recast into a memory mechanism. Different from the forgetting mechanism, which mainly evaluates the importance of samples according to the time when samples are collected, the memory mechanism evaluates the importance of samples through both temporal and spatial distribution of samples. With SMRLS, the input space of the RBFNN is evenly divided into a finite number of partitions and a synthesized objective function is developed using synthesized ",
    "path": "papers/22/11/2211.07909.json",
    "total_tokens": 935,
    "translated_title": "选择性记忆递归最小二乘法：将遗忘转化为径向基神经网络实时学习中的记忆 (arXiv:2211.07909v2 [eess.SY] UPDATED)",
    "translated_abstract": "在基于径向基函数神经网络（RBFNN）的实时学习任务中，广泛使用遗忘机制以使神经网络对新数据保持敏感性。然而，通过遗忘机制，一些有用的知识会因为它们学习很久以前而被遗忘，这被称为被动知识遗忘现象。为了解决这个问题，本文提出了一种实时训练方法，名为选择性记忆递归最小二乘法（SMRLS），其中将经典遗忘机制转化为一种记忆机制。与遗忘机制不同，该记忆机制主要通过样本的采集时间以及时空分布评估样本的重要性。通过SMRLS，RBFNN的输入空间被均匀划分为有限数量的分区，并使用综合目标函数进行综合开发。",
    "tldr": "本文提出了一种名为选择性记忆递归最小二乘法（SMRLS）的实时训练方法，将经典遗忘机制转化为记忆机制，通过综合评估样本的采集时间和时空分布来评估样本的重要性。",
    "en_tdlr": "This paper proposes a real-time training method called Selective Memory Recursive Least Squares (SMRLS), which recasts classical forgetting mechanisms into a memory mechanism. It evaluates the importance of samples by considering both temporal and spatial distribution, addressing the issue of passive knowledge forgetting in radial basis function neural network (RBFNN) based real-time learning."
}