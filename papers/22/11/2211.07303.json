{
    "title": "Adaptive Federated Minimax Optimization with Lower complexities. (arXiv:2211.07303v3 [cs.LG] UPDATED)",
    "abstract": "Federated learning is a popular distributed and privacy-preserving machine learning paradigm. Meanwhile, minimax optimization, as an effective hierarchical optimization, is widely applied in machine learning. Recently, some federated optimization methods have been proposed to solve the distributed minimax problems. However, these federated minimax methods still suffer from high gradient and communication complexities. Meanwhile, few algorithm focuses on using adaptive learning rate to accelerate algorithms. To fill this gap, in the paper, we study a class of nonconvex minimax optimization, and propose an efficient adaptive federated minimax optimization algorithm (i.e., AdaFGDA) to solve these distributed minimax problems. Specifically, our AdaFGDA builds on the momentum-based variance reduced and local-SGD techniques, and it can flexibly incorporate various adaptive learning rates by using the unified adaptive matrix. Theoretically, we provide a solid convergence analysis framework fo",
    "link": "http://arxiv.org/abs/2211.07303",
    "context": "Title: Adaptive Federated Minimax Optimization with Lower complexities. (arXiv:2211.07303v3 [cs.LG] UPDATED)\nAbstract: Federated learning is a popular distributed and privacy-preserving machine learning paradigm. Meanwhile, minimax optimization, as an effective hierarchical optimization, is widely applied in machine learning. Recently, some federated optimization methods have been proposed to solve the distributed minimax problems. However, these federated minimax methods still suffer from high gradient and communication complexities. Meanwhile, few algorithm focuses on using adaptive learning rate to accelerate algorithms. To fill this gap, in the paper, we study a class of nonconvex minimax optimization, and propose an efficient adaptive federated minimax optimization algorithm (i.e., AdaFGDA) to solve these distributed minimax problems. Specifically, our AdaFGDA builds on the momentum-based variance reduced and local-SGD techniques, and it can flexibly incorporate various adaptive learning rates by using the unified adaptive matrix. Theoretically, we provide a solid convergence analysis framework fo",
    "path": "papers/22/11/2211.07303.json",
    "total_tokens": 915,
    "translated_title": "自适应联邦式最小最大优化及其较低复杂度",
    "translated_abstract": "联邦学习是一种流行的分布式和隐私保护的机器学习范例。同时，作为一种有效的层次优化方法，最小最大优化广泛应用于机器学习中。最近，已经提出了一些联邦式最小最大优化方法来解决分布式最小最大问题。然而，这些联邦式最小最大方法仍然受到梯度和通信复杂性高的问题的困扰。同时，很少有算法专注于使用自适应学习率来加速算法。为了填补这一空白，在本文中，我们研究了一类非凸最小最大优化问题，并提出了一种高效的自适应联邦式最小最大优化算法（即AdaFGDA）来解决这些分布式最小最大问题。具体而言，我们的AdaFGDA建立在基于动量的方差减少和局部SGD技术的基础上，通过使用统一的自适应矩阵可以灵活地结合各种自适应学习率。从理论上讲，我们提供了一个坚实的收敛性分析框架……",
    "tldr": "本文提出了一种自适应联邦式最小最大优化算法（AdaFGDA），用于解决分布式最小最大问题，在梯度和通信复杂性较低的情况下取得了良好的效果。"
}