{
    "title": "Particle-based Variational Inference with Preconditioned Functional Gradient Flow. (arXiv:2211.13954v2 [stat.ML] UPDATED)",
    "abstract": "Particle-based variational inference (VI) minimizes the KL divergence between model samples and the target posterior with gradient flow estimates. With the popularity of Stein variational gradient descent (SVGD), the focus of particle-based VI algorithms has been on the properties of functions in Reproducing Kernel Hilbert Space (RKHS) to approximate the gradient flow. However, the requirement of RKHS restricts the function class and algorithmic flexibility. This paper offers a general solution to this problem by introducing a functional regularization term that encompasses the RKHS norm as a special case. This allows us to propose a new particle-based VI algorithm called preconditioned functional gradient flow (PFG). Compared to SVGD, PFG has several advantages. It has a larger function class, improved scalability in large particle-size scenarios, better adaptation to ill-conditioned distributions, and provable continuous-time convergence in KL divergence. Additionally, non-linear fun",
    "link": "http://arxiv.org/abs/2211.13954",
    "context": "Title: Particle-based Variational Inference with Preconditioned Functional Gradient Flow. (arXiv:2211.13954v2 [stat.ML] UPDATED)\nAbstract: Particle-based variational inference (VI) minimizes the KL divergence between model samples and the target posterior with gradient flow estimates. With the popularity of Stein variational gradient descent (SVGD), the focus of particle-based VI algorithms has been on the properties of functions in Reproducing Kernel Hilbert Space (RKHS) to approximate the gradient flow. However, the requirement of RKHS restricts the function class and algorithmic flexibility. This paper offers a general solution to this problem by introducing a functional regularization term that encompasses the RKHS norm as a special case. This allows us to propose a new particle-based VI algorithm called preconditioned functional gradient flow (PFG). Compared to SVGD, PFG has several advantages. It has a larger function class, improved scalability in large particle-size scenarios, better adaptation to ill-conditioned distributions, and provable continuous-time convergence in KL divergence. Additionally, non-linear fun",
    "path": "papers/22/11/2211.13954.json",
    "total_tokens": 949,
    "translated_title": "基于粒子的预处理函数梯度流变分推断",
    "translated_abstract": "基于粒子的变分推断通过梯度流估计最小化模型样本与目标后验之间的KL散度。随着Stein变分梯度下降（SVGD）的流行，基于粒子的VI算法的重点已经转向在重现核希尔伯特空间（RKHS）中逼近梯度流的函数的特性。然而，RKHS的要求限制了函数类和算法的灵活性。本文通过引入包含了RKHS范数的函数正则化项，提供了这个问题的通用解决方案。这使得我们可以提出一个新的基于粒子的VI算法，叫做预处理函数梯度流（PFG）。与SVGD相比，PFG具有更大的函数类，改进了大量粒子场景的可扩展性，更适应病态分布，并在KL散度上提供了可证明的连续时间收敛。此外，非线性函数规范也可以轻松地并入到所提出的算法中。",
    "tldr": "本文提出了一种新的基于粒子的变分推断算法PFG，通过引入包含RKHS范数的函数正则项实现更大的函数类和更好的适应性，解决了RKHS要求限制函数类和算法灵活性的问题，并在KL散度上提供了可证明的连续时间收敛。",
    "en_tdlr": "This paper proposes a new particle-based VI algorithm called PFG, which solves the problem of restricting function class and algorithmic flexibility caused by the requirement of RKHS by introducing a functional regularization term that encompasses the RKHS norm as a special case, achieving a larger function class and better adaptability, and providing provable continuous-time convergence in KL divergence."
}