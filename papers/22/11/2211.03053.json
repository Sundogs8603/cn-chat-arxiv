{
    "title": "Suffix Retrieval-Augmented Language Modeling. (arXiv:2211.03053v2 [cs.CL] UPDATED)",
    "abstract": "Causal language modeling (LM) uses word history to predict the next word. BERT, on the other hand, makes use of bi-directional word information in a sentence to predict words at masked positions. While BERT is effective in sequence encoding, it is non-causal by nature and is not designed for sequence generation. In this paper, we propose a novel language model, SUffix REtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual effect in an autoregressive manner. SUREALM employs an embedding retriever to search for training sentences in a data store that share similar word history during sequence generation. In particular, the suffix portions of the retrieved sentences mimick the \"future\" context. We evaluated our proposed model on the DSTC9 spoken dialogue corpus and showed promising word perplexity reduction on the validation and test set compared to competitive baselines.",
    "link": "http://arxiv.org/abs/2211.03053",
    "context": "Title: Suffix Retrieval-Augmented Language Modeling. (arXiv:2211.03053v2 [cs.CL] UPDATED)\nAbstract: Causal language modeling (LM) uses word history to predict the next word. BERT, on the other hand, makes use of bi-directional word information in a sentence to predict words at masked positions. While BERT is effective in sequence encoding, it is non-causal by nature and is not designed for sequence generation. In this paper, we propose a novel language model, SUffix REtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual effect in an autoregressive manner. SUREALM employs an embedding retriever to search for training sentences in a data store that share similar word history during sequence generation. In particular, the suffix portions of the retrieved sentences mimick the \"future\" context. We evaluated our proposed model on the DSTC9 spoken dialogue corpus and showed promising word perplexity reduction on the validation and test set compared to competitive baselines.",
    "path": "papers/22/11/2211.03053.json",
    "total_tokens": 925,
    "translated_title": "后缀检索增强语言建模",
    "translated_abstract": "因果语言建模（LM）使用单词历史来预测下一个单词。相反，BERT利用句子中的双向单词信息来预测蒙面位置的单词。虽然BERT在序列编码方面很有效，但它本质上是非因果的，不适用于序列生成。在本文中，我们提出了一种新颖的语言模型，后缀检索增强语言模型（SUREALM），它以自回归的方式模拟双向上下文效应。SUREALM采用嵌入检索器，在序列生成过程中搜索具有相似单词历史的训练句子数据存储。特别是，检索到的句子的后缀部分模拟了“未来”上下文。我们在DSTC9口语对话语料库上评估了我们的模型，并显示了与竞争基线相比，在验证集和测试集上有良好的单词困惑度降低。",
    "tldr": "提出一种后缀检索增强语言模型 (SUREALM)，该模型以自回归的方式模拟双向上下文效应。采用嵌入检索器,在序列生成过程中搜索具有相似单词历史的训练句子数据存储。在DSTC9口语对话语料库上的表现优于竞争基线。",
    "en_tdlr": "SUREALM, a novel language model that simulates a bi-directional contextual effect in an autoregressive manner using an embedding retriever to search for training sentences in a data store that share similar word history during sequence generation, showed promising results on the DSTC9 spoken dialogue corpus in reducing word perplexity compared to competitive baselines."
}