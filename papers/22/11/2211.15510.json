{
    "title": "Localized Shortcut Removal. (arXiv:2211.15510v2 [cs.CV] UPDATED)",
    "abstract": "Machine learning is a data-driven field, and the quality of the underlying datasets plays a crucial role in learning success. However, high performance on held-out test data does not necessarily indicate that a model generalizes or learns anything meaningful. This is often due to the existence of machine learning shortcuts - features in the data that are predictive but unrelated to the problem at hand. To address this issue for datasets where the shortcuts are smaller and more localized than true features, we propose a novel approach to detect and remove them. We use an adversarially trained lens to detect and eliminate highly predictive but semantically unconnected clues in images. In our experiments on both synthetic and real-world data, we show that our proposed approach reliably identifies and neutralizes such shortcuts without causing degradation of model performance on clean data. We believe that our approach can lead to more meaningful and generalizable machine learning models, ",
    "link": "http://arxiv.org/abs/2211.15510",
    "context": "Title: Localized Shortcut Removal. (arXiv:2211.15510v2 [cs.CV] UPDATED)\nAbstract: Machine learning is a data-driven field, and the quality of the underlying datasets plays a crucial role in learning success. However, high performance on held-out test data does not necessarily indicate that a model generalizes or learns anything meaningful. This is often due to the existence of machine learning shortcuts - features in the data that are predictive but unrelated to the problem at hand. To address this issue for datasets where the shortcuts are smaller and more localized than true features, we propose a novel approach to detect and remove them. We use an adversarially trained lens to detect and eliminate highly predictive but semantically unconnected clues in images. In our experiments on both synthetic and real-world data, we show that our proposed approach reliably identifies and neutralizes such shortcuts without causing degradation of model performance on clean data. We believe that our approach can lead to more meaningful and generalizable machine learning models, ",
    "path": "papers/22/11/2211.15510.json",
    "total_tokens": 884,
    "translated_title": "本地化的快捷方式移除",
    "translated_abstract": "机器学习是一个数据驱动的领域，底层数据集的质量在学习成功中起着至关重要的作用。然而，在测试数据上表现优秀并不一定表示模型具有泛化性或学习到了有意义的东西。这往往是由于存在机器学习快捷方式-数据中与问题无关但具有预测性的特征。针对这个问题，我们提出了一种新方法来检测和移除局部化的快捷方式，而非真正的特征。我们使用对抗性训练的方法，使用一个\"镜头\"来检测和消除图像中高度预测但语义上不相关的线索。在合成数据和真实数据上的实验中，我们展示了我们的方法可靠地识别并消除这类快捷方式，而不会导致模型在干净数据上的性能下降。我们相信，我们的方法可以产生更有意义和泛化性好的机器学习模型。",
    "tldr": "提出了一种新颖的方法来检测和移除数据中局部化的快捷方式，而非真正的特征。该方法使用对抗性训练来识别和消除语义不相关的线索。实验证明该方法可靠地提高了机器学习模型的泛化能力。",
    "en_tdlr": "A novel approach is proposed to detect and remove localized shortcuts in data, rather than true features. The method uses adversarial training to identify and eliminate semantically unconnected clues. Experiments show that this approach reliably improves the generalizability of machine learning models."
}