{
    "title": "Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning. (arXiv:2211.07591v2 [cs.CL] UPDATED)",
    "abstract": "Inspired by the curvature of space-time (Einstein, 1921), we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking model towards a goal in a zero-shot fashion by projecting the goal utterance and the corresponding reply candidates into a latent space. Here the cosine similarity indicates the distance/reachability of a candidate utterance toward the corresponding goal. Furthermore, we explore how these forward-entailing language representations can be utilized for assessing the likelihood of sequences by the entailment strength i.e. through the cosine similarity of its individual members (encoded separately) as an emergent property in the curved space. These non-local properties allow us to imagine the likelihood of future patterns in dialogues, specifically by ordering/identifying future go",
    "link": "http://arxiv.org/abs/2211.07591",
    "context": "Title: Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning. (arXiv:2211.07591v2 [cs.CL] UPDATED)\nAbstract: Inspired by the curvature of space-time (Einstein, 1921), we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking model towards a goal in a zero-shot fashion by projecting the goal utterance and the corresponding reply candidates into a latent space. Here the cosine similarity indicates the distance/reachability of a candidate utterance toward the corresponding goal. Furthermore, we explore how these forward-entailing language representations can be utilized for assessing the likelihood of sequences by the entailment strength i.e. through the cosine similarity of its individual members (encoded separately) as an emergent property in the curved space. These non-local properties allow us to imagine the likelihood of future patterns in dialogues, specifically by ordering/identifying future go",
    "path": "papers/22/11/2211.07591.json",
    "total_tokens": 1048,
    "translated_title": "想象力是你所需要的一切！用于长期对话规划的曲线对比学习的抽象序列建模",
    "translated_abstract": "受时空曲率启发（爱因斯坦，1921），我们引入了曲线对比学习（CCL），这是一种新颖的表示学习技术，用于学习多轮对话中发言对之间的相对转弯距离。通过将目标发言和相应的回复候选项投影到潜在空间中，得到的双编码器模型可以引导变压器作为响应排序模型以零-shot方式朝着目标前进。这里的余弦相似性表示了候选发言朝着相应目标的距离/可达性。此外，我们探讨了如何利用这些前向启示性语言表示来评估序列的可能性，即通过其各个成员（分别编码）的余弦相似度作为曲线空间中的新兴属性。这些非局部属性使我们能够想象未来对话模式的可能性，具体是通过排序/识别未来的go",
    "tldr": "本文介绍了一种新颖的表示学习技术——曲线对比学习（CCL），用于学习多轮对话中发言对之间的相对转弯距离。利用这种技术可以将目标发言和回复候选项映射到潜在空间中，通过余弦相似性评估候选发言和目标之间的距离。此外，该方法还探索了如何通过曲线空间中的余弦相似度评估序列的可能性，从而想象未来对话模式的可能性。",
    "en_tdlr": "This paper introduces a novel representation learning technique called Curved Contrastive Learning (CCL), which is used to learn the relative turn distance between utterance pairs in multi-turn dialogues. By mapping the goal utterance and reply candidates into a latent space, the technique enables the evaluation of the distance between candidates and the goal using cosine similarity. Additionally, it explores the use of cosine similarity in curved space to assess the likelihood of sequences and imagine future dialogue patterns."
}