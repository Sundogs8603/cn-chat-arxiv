{
    "title": "A Universal Discriminator for Zero-Shot Generalization. (arXiv:2211.08099v2 [cs.CL] UPDATED)",
    "abstract": "Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a single discriminator to predict whether a text sample comes from the true data distribution, similar to GANs. Since many NLP tasks can be formulated as selecting from a few options, we use this discriminator to predict the concatenation of input and which option has the highest probability of coming from the true data distribution. This simple formulation achieves state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by 16.0\\%, 7.8\\%, and 11.5\\% respectively on different scales. In the finetuning setting, our approach also achieves new state-of-the-art results on a wide range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile, our approach requires minim",
    "link": "http://arxiv.org/abs/2211.08099",
    "context": "Title: A Universal Discriminator for Zero-Shot Generalization. (arXiv:2211.08099v2 [cs.CL] UPDATED)\nAbstract: Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a single discriminator to predict whether a text sample comes from the true data distribution, similar to GANs. Since many NLP tasks can be formulated as selecting from a few options, we use this discriminator to predict the concatenation of input and which option has the highest probability of coming from the true data distribution. This simple formulation achieves state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by 16.0\\%, 7.8\\%, and 11.5\\% respectively on different scales. In the finetuning setting, our approach also achieves new state-of-the-art results on a wide range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile, our approach requires minim",
    "path": "papers/22/11/2211.08099.json",
    "total_tokens": 928,
    "translated_title": "一种用于零样本泛化的通用鉴别器",
    "translated_abstract": "生成建模一直是大规模预训练和零样本泛化的主要方法。本研究通过展示在大量NLP任务中，鉴别方法比生成方法表现更好，挑战这种惯例。我们训练一个单一的鉴别器来预测文本样本是否来自真实数据分布，类似于GAN。由于许多NLP任务可以表示为从几个选项中选择，因此我们使用这个鉴别器来预测输入和哪个选项与真实数据分布的概率最大。这种简单的公式在T0基准测试中实现了最先进的零样本结果，分别在不同规模上比T0高16.0％，7.8％和11.5％。在微调设置中，我们的方法还在各种NLP任务上取得了新的最先进结果，仅占之前方法的1/4参数。同时，我们的方法需要最小的噪声和架构工程。",
    "tldr": "该论文提出了一种用于零样本泛化的通用鉴别器，通过在NLP任务中使用单一的鉴别器，可实现比生成方法更好的表现，取得了在T0基准测试中最先进的零样本结果，同时在各种NLP任务上实现了新的最先进结果。",
    "en_tdlr": "This paper proposes a universal discriminator for zero-shot generalization, which achieves better performance than generative approaches in NLP tasks and obtains state-of-the-art zero-shot results on the T0 benchmark. It also achieves new state-of-the-art results on various NLP tasks with only a quarter of parameters compared to previous methods."
}