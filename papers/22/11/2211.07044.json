{
    "title": "SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation. (arXiv:2211.07044v2 [cs.CV] UPDATED)",
    "abstract": "Self-supervised pre-training bears potential to generate expressive representations without human annotation. Most pre-training in Earth observation (EO) are based on ImageNet or medium-size, labeled remote sensing (RS) datasets. We share an unlabeled RS dataset SSL4EO-S12 (Self-Supervised Learning for Earth Observation - Sentinel-1/2) to assemble a large-scale, global, multimodal, and multi-seasonal corpus of satellite imagery from the ESA Sentinel-1 \\& -2 satellite missions. For EO applications we demonstrate SSL4EO-S12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec. Resulting models yield downstream performance close to, or surpassing accuracy measures of supervised learning. In addition, pre-training on SSL4EO-S12 excels compared to existing datasets. We make openly available the dataset, related source code, and pre-trained models at https://github.com/zhu-xlab/SSL4EO-S12.",
    "link": "http://arxiv.org/abs/2211.07044",
    "context": "Title: SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation. (arXiv:2211.07044v2 [cs.CV] UPDATED)\nAbstract: Self-supervised pre-training bears potential to generate expressive representations without human annotation. Most pre-training in Earth observation (EO) are based on ImageNet or medium-size, labeled remote sensing (RS) datasets. We share an unlabeled RS dataset SSL4EO-S12 (Self-Supervised Learning for Earth Observation - Sentinel-1/2) to assemble a large-scale, global, multimodal, and multi-seasonal corpus of satellite imagery from the ESA Sentinel-1 \\& -2 satellite missions. For EO applications we demonstrate SSL4EO-S12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec. Resulting models yield downstream performance close to, or surpassing accuracy measures of supervised learning. In addition, pre-training on SSL4EO-S12 excels compared to existing datasets. We make openly available the dataset, related source code, and pre-trained models at https://github.com/zhu-xlab/SSL4EO-S12.",
    "path": "papers/22/11/2211.07044.json",
    "total_tokens": 966,
    "translated_title": "SSL4EO-S12:自监督学习在地球观测中的大规模多模态、多时相数据集",
    "translated_abstract": "自我监督预训练有望生成表现力强的表征，而地球观测领域的大多数预训练都基于ImageNet或中等规模的标记的遥感数据集。我们分享了一个未标记的地球观测数据集SSL4EO-S12，从欧空局Sentinel-1和-2卫星任务中装配了一个大规模、全球、多模态和多季节的卫星图像语料库。对于EO应用，我们展示SSL4EO-S12在自监督预训练方面取得了成功，并成功地应用于几种方法：MoCo-v2，DINO，MAE和data2vec。结果模型的下游性能接近或超过了监督学习的准确性度量。此外，与现有数据集相比，对SSL4EO-S12的预训练表现出色。我们在https://github.com/zhu-xlab/SSL4EO-S12上公开了数据集、相关源代码和预训练模型。",
    "tldr": "该论文介绍了一种名为SSL4EO-S12的大规模、全球、多模态、多季度的自监督学习地球观测数据集，证明了对该数据集进行自监督预训练可以产生准确性与监督学习相当的模型，在该领域具有很高的应用价值。",
    "en_tdlr": "This paper introduces a large-scale, global, multi-modal, and multi-seasonal self-supervised learning dataset for Earth observation called SSL4EO-S12, demonstrating that self-supervised pre-training on this dataset can generate models with accuracy close to supervised learning, and has high application value in the field."
}