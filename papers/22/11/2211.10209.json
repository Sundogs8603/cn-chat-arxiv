{
    "title": "Leveraging Algorithmic Fairness to Mitigate Blackbox Attribute Inference Attacks",
    "abstract": "arXiv:2211.10209v2 Announce Type: replace  Abstract: Machine learning (ML) models have been deployed for high-stakes applications, e.g., healthcare and criminal justice. Prior work has shown that ML models are vulnerable to attribute inference attacks where an adversary, with some background knowledge, trains an ML attack model to infer sensitive attributes by exploiting distinguishable model predictions. However, some prior attribute inference attacks have strong assumptions about adversary's background knowledge (e.g., marginal distribution of sensitive attribute) and pose no more privacy risk than statistical inference. Moreover, none of the prior attacks account for class imbalance of sensitive attribute in datasets coming from real-world applications (e.g., Race and Sex). In this paper, we propose an practical and effective attribute inference attack that accounts for this imbalance using an adaptive threshold over the attack model's predictions. We exhaustively evaluate our propo",
    "link": "https://arxiv.org/abs/2211.10209",
    "context": "Title: Leveraging Algorithmic Fairness to Mitigate Blackbox Attribute Inference Attacks\nAbstract: arXiv:2211.10209v2 Announce Type: replace  Abstract: Machine learning (ML) models have been deployed for high-stakes applications, e.g., healthcare and criminal justice. Prior work has shown that ML models are vulnerable to attribute inference attacks where an adversary, with some background knowledge, trains an ML attack model to infer sensitive attributes by exploiting distinguishable model predictions. However, some prior attribute inference attacks have strong assumptions about adversary's background knowledge (e.g., marginal distribution of sensitive attribute) and pose no more privacy risk than statistical inference. Moreover, none of the prior attacks account for class imbalance of sensitive attribute in datasets coming from real-world applications (e.g., Race and Sex). In this paper, we propose an practical and effective attribute inference attack that accounts for this imbalance using an adaptive threshold over the attack model's predictions. We exhaustively evaluate our propo",
    "path": "papers/22/11/2211.10209.json",
    "total_tokens": 830,
    "translated_title": "利用算法公平性减轻黑盒属性推断攻击",
    "translated_abstract": "机器学习（ML）模型已经被部署在高风险应用中，例如医疗保健和刑事司法。先前的研究表明，ML模型容易受到属性推断攻击的影响，攻击者利用一些背景知识训练ML攻击模型，通过利用可区分的模型预测来推断敏感属性。然而，一些先前的属性推断攻击对攻击者的背景知识（例如，敏感属性的边际分布）有很强的假设，并且不会带来比统计推断更多的隐私风险。此外，先前的攻击并未考虑来自真实应用程序的数据集中敏感属性的类别不平衡（例如，种族和性别）。在本文中，我们提出了一个考虑此不平衡的实用且有效的属性推断攻击，使用攻击模型预测的自适应阈值。我们对我们的方法进行了全面评估。",
    "tldr": "通过使用自适应阈值来考虑数据集中敏感属性类别不平衡，我们提出了一种实用且有效的属性推断攻击方法。",
    "en_tdlr": "We propose a practical and effective attribute inference attack by considering the imbalance of sensitive attribute categories in datasets through the use of adaptive threshold over the attack model's predictions."
}