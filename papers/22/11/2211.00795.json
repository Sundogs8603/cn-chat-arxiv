{
    "title": "InterMPL: Momentum Pseudo-Labeling with Intermediate CTC Loss. (arXiv:2211.00795v2 [eess.AS] UPDATED)",
    "abstract": "This paper presents InterMPL, a semi-supervised learning method of end-to-end automatic speech recognition (ASR) that performs pseudo-labeling (PL) with intermediate supervision. Momentum PL (MPL) trains a connectionist temporal classification (CTC)-based model on unlabeled data by continuously generating pseudo-labels on the fly and improving their quality. In contrast to autoregressive formulations, such as the attention-based encoder-decoder and transducer, CTC is well suited for MPL, or PL-based semi-supervised ASR in general, owing to its simple/fast inference algorithm and robustness against generating collapsed labels. However, CTC generally yields inferior performance than the autoregressive models due to the conditional independence assumption, thereby limiting the performance of MPL. We propose to enhance MPL by introducing intermediate loss, inspired by the recent advances in CTC-based modeling. Specifically, we focus on self-conditional and hierarchical conditional CTC, tha",
    "link": "http://arxiv.org/abs/2211.00795",
    "context": "Title: InterMPL: Momentum Pseudo-Labeling with Intermediate CTC Loss. (arXiv:2211.00795v2 [eess.AS] UPDATED)\nAbstract: This paper presents InterMPL, a semi-supervised learning method of end-to-end automatic speech recognition (ASR) that performs pseudo-labeling (PL) with intermediate supervision. Momentum PL (MPL) trains a connectionist temporal classification (CTC)-based model on unlabeled data by continuously generating pseudo-labels on the fly and improving their quality. In contrast to autoregressive formulations, such as the attention-based encoder-decoder and transducer, CTC is well suited for MPL, or PL-based semi-supervised ASR in general, owing to its simple/fast inference algorithm and robustness against generating collapsed labels. However, CTC generally yields inferior performance than the autoregressive models due to the conditional independence assumption, thereby limiting the performance of MPL. We propose to enhance MPL by introducing intermediate loss, inspired by the recent advances in CTC-based modeling. Specifically, we focus on self-conditional and hierarchical conditional CTC, tha",
    "path": "papers/22/11/2211.00795.json",
    "total_tokens": 899,
    "tldr": "本文提出了一种半监督学习方法InterMPL，通过中间监督来增强动量伪标签，实现无监督数据的自动语音识别模型训练，该方法主要集中在加强CTC。"
}