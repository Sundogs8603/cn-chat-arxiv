{
    "title": "Human-in-the-Loop Mixup. (arXiv:2211.01202v3 [cs.LG] UPDATED)",
    "abstract": "Aligning model representations to humans has been found to improve robustness and generalization. However, such methods often focus on standard observational data. Synthetic data is proliferating and powering many advances in machine learning; yet, it is not always clear whether synthetic labels are perceptually aligned to humans -- rendering it likely model representations are not human aligned. We focus on the synthetic data used in mixup: a powerful regularizer shown to improve model robustness, generalization, and calibration. We design a comprehensive series of elicitation interfaces, which we release as HILL MixE Suite, and recruit 159 participants to provide perceptual judgments along with their uncertainties, over mixup examples. We find that human perceptions do not consistently align with the labels traditionally used for synthetic points, and begin to demonstrate the applicability of these findings to potentially increase the reliability of downstream models, particularly wh",
    "link": "http://arxiv.org/abs/2211.01202",
    "context": "Title: Human-in-the-Loop Mixup. (arXiv:2211.01202v3 [cs.LG] UPDATED)\nAbstract: Aligning model representations to humans has been found to improve robustness and generalization. However, such methods often focus on standard observational data. Synthetic data is proliferating and powering many advances in machine learning; yet, it is not always clear whether synthetic labels are perceptually aligned to humans -- rendering it likely model representations are not human aligned. We focus on the synthetic data used in mixup: a powerful regularizer shown to improve model robustness, generalization, and calibration. We design a comprehensive series of elicitation interfaces, which we release as HILL MixE Suite, and recruit 159 participants to provide perceptual judgments along with their uncertainties, over mixup examples. We find that human perceptions do not consistently align with the labels traditionally used for synthetic points, and begin to demonstrate the applicability of these findings to potentially increase the reliability of downstream models, particularly wh",
    "path": "papers/22/11/2211.01202.json",
    "total_tokens": 837,
    "translated_title": "人机协作Mixup",
    "translated_abstract": "将模型表示与人类对齐已被发现能提高鲁棒性和泛化性。然而，这类方法通常专注于标准的观测数据。合成数据正在蓬勃发展并推动机器学习的许多进展，但往往不清楚合成标签是否与人类的知觉保持一致，这可能导致模型表示不与人类对齐。我们专注于Mixup中使用的合成数据：一种强大的正则化方法，已被证明可以提高模型的鲁棒性、泛化性和校准性。我们设计了一系列全面的启发式接口，并将其发布为HILL MixE Suite，并招募了159名参与者对Mixup示例进行了感知判断及其不确定性。我们发现人类知觉不一致地与传统上用于合成数据的标签对齐，并开始展示这些发现的适用性，以潜在地提高下游模型的可靠性。",
    "tldr": "本研究针对使用Mixup的合成数据进行人机协作研究，发现人类知觉与合成数据标签不一致，这些发现有助于提高下游模型的可靠性。",
    "en_tdlr": "In this study, we investigate human-in-the-loop mixup using synthetic data and find that human perceptions do not align consistently with synthetic labels. These findings contribute to improving the reliability of downstream models."
}