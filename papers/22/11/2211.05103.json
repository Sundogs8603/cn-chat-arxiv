{
    "title": "Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models. (arXiv:2211.05103v2 [eess.AS] UPDATED)",
    "abstract": "In this paper, we extend previous self-supervised approaches for language identification by experimenting with Conformer based architecture in a multilingual pre-training paradigm. We find that pre-trained speech models optimally encode language discriminatory information in lower layers. Further, we demonstrate that the embeddings obtained from these layers are significantly robust to classify unseen languages and different acoustic environments without additional training. After fine-tuning a pre-trained Conformer model on the VoxLingua107 dataset, we achieve results similar to current state-of-the-art systems for language identification. More, our model accomplishes this with 5x less parameters. We open-source the model through the NVIDIA NeMo toolkit.",
    "link": "http://arxiv.org/abs/2211.05103",
    "total_tokens": 956,
    "translated_title": "意外学习者：自监督多语言模型中的口语语言识别",
    "translated_abstract": "本文通过在多语言预训练范式中尝试Conformer架构，扩展了先前的自监督语言识别方法。我们发现，预训练的语音模型在较低层中最优地编码了语言区分信息。此外，我们证明了从这些层获得的嵌入在没有额外训练的情况下，能够显著地稳健地分类未见过的语言和不同的声学环境。在对预训练的Conformer模型在VoxLingua107数据集上进行微调后，我们实现了与当前最先进的语言识别系统类似的结果。此外，我们的模型使用的参数量仅为其它模型的五分之一。我们通过NVIDIA NeMo工具包开源了该模型。",
    "tldr": "本文通过在多语言预训练范式中尝试Conformer架构，扩展了先前的自监督语言识别方法。预训练的语音模型在较低层中最优地编码了语言区分信息，从这些层获得的嵌入能够显著地稳健地分类未见过的语言和不同的声学环境。在对预训练的Conformer模型在VoxLingua107数据集上进行微调后，我们实现了与当前最先进的语言识别系统类似的结果，且使用的参数量仅为其它模型的五分之一。",
    "en_tldr": "This paper extends previous self-supervised approaches for language identification by experimenting with Conformer based architecture in a multilingual pre-training paradigm. The pre-trained speech models optimally encode language discriminatory information in lower layers, and the embeddings obtained from these layers are significantly robust to classify unseen languages and different acoustic environments without additional training. After fine-tuning a pre-trained Conformer model on the VoxLingua107 dataset, the authors achieve results similar to current state-of-the-art systems for language identification, with 5x less parameters. The model is open-sourced through the NVIDIA NeMo toolkit."
}