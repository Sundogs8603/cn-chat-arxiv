{
    "title": "Block size estimation for data partitioning in HPC applications using machine learning techniques",
    "abstract": "The extensive use of HPC infrastructures and frameworks for running dataintensive applications has led to a growing interest in data partitioning techniques and strategies. In fact, application performance can be heavily affected by how data are partitioned, which in turn depends on the selected size for data blocks, i.e. the block size. Therefore, finding an effective partitioning, i.e. a suitable block size, is a key strategy to speed-up parallel data-intensive applications and increase scalability. This paper describes a methodology, namely BLEST-ML (BLock size ESTimation through Machine Learning), for block size estimation that relies on supervised machine learning techniques. The proposed methodology was evaluated by designing an implementation tailored to dislib, a distributed computing library highly focused on machine learning algorithms built on top of the PyCOMPSs framework. We assessed the effectiveness of the provided implementation through an extensive experimental evaluat",
    "link": "https://arxiv.org/abs/2211.10819",
    "context": "Title: Block size estimation for data partitioning in HPC applications using machine learning techniques\nAbstract: The extensive use of HPC infrastructures and frameworks for running dataintensive applications has led to a growing interest in data partitioning techniques and strategies. In fact, application performance can be heavily affected by how data are partitioned, which in turn depends on the selected size for data blocks, i.e. the block size. Therefore, finding an effective partitioning, i.e. a suitable block size, is a key strategy to speed-up parallel data-intensive applications and increase scalability. This paper describes a methodology, namely BLEST-ML (BLock size ESTimation through Machine Learning), for block size estimation that relies on supervised machine learning techniques. The proposed methodology was evaluated by designing an implementation tailored to dislib, a distributed computing library highly focused on machine learning algorithms built on top of the PyCOMPSs framework. We assessed the effectiveness of the provided implementation through an extensive experimental evaluat",
    "path": "papers/22/11/2211.10819.json",
    "total_tokens": 838,
    "translated_title": "使用机器学习技术进行高性能计算应用中的数据分区的块大小估计",
    "translated_abstract": "高性能计算基础设施和框架的广泛使用促使对数据分区技术和策略的兴趣不断增长。实际上，应用性能可能受数据分区方式的影响，而这又取决于选择的数据块大小，即块大小。因此，找到一种有效的分区即合适的块大小是加速并行数据密集型应用和提高可伸缩性的关键策略。本文描述了一种名为BLEST-ML（通过机器学习的块大小估计）的方法，该方法依赖于监督机器学习技术进行块大小估计。提出的方法通过针对基于PyCOMPSs框架的高度专注于机器学习算法的分布式计算库dislib设计的实现进行评估。我们通过广泛的实验评估来评估提供的实现的有效性。",
    "tldr": "这项工作介绍了一种名为BLEST-ML的方法，通过机器学习技术进行块大小估计，以加速并行数据密集型应用和提高可伸缩性。这种方法在分布式计算库dislib上进行了评估。",
    "en_tdlr": "This work presents BLEST-ML, a method for block size estimation using machine learning techniques, to accelerate parallel data-intensive applications and improve scalability. The proposed methodology was evaluated on the distributed computing library dislib."
}