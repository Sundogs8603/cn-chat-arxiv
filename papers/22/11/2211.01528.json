{
    "title": "Fair and Optimal Classification via Post-Processing. (arXiv:2211.01528v3 [cs.LG] UPDATED)",
    "abstract": "To mitigate the bias exhibited by machine learning models, fairness criteria can be integrated into the training process to ensure fair treatment across all demographics, but it often comes at the expense of model performance. Understanding such tradeoffs, therefore, underlies the design of fair algorithms. To this end, this paper provides a complete characterization of the inherent tradeoff of demographic parity on classification problems, under the most general multi-group, multi-class, and noisy setting. Specifically, we show that the minimum error rate achievable by randomized and attribute-aware fair classifiers is given by the optimal value of a Wasserstein-barycenter problem. On the practical side, our findings lead to a simple post-processing algorithm that derives fair classifiers from score functions, which yields the optimal fair classifier when the score is Bayes optimal. We provide suboptimality analysis and sample complexity for our algorithm, and demonstrate its effectiv",
    "link": "http://arxiv.org/abs/2211.01528",
    "context": "Title: Fair and Optimal Classification via Post-Processing. (arXiv:2211.01528v3 [cs.LG] UPDATED)\nAbstract: To mitigate the bias exhibited by machine learning models, fairness criteria can be integrated into the training process to ensure fair treatment across all demographics, but it often comes at the expense of model performance. Understanding such tradeoffs, therefore, underlies the design of fair algorithms. To this end, this paper provides a complete characterization of the inherent tradeoff of demographic parity on classification problems, under the most general multi-group, multi-class, and noisy setting. Specifically, we show that the minimum error rate achievable by randomized and attribute-aware fair classifiers is given by the optimal value of a Wasserstein-barycenter problem. On the practical side, our findings lead to a simple post-processing algorithm that derives fair classifiers from score functions, which yields the optimal fair classifier when the score is Bayes optimal. We provide suboptimality analysis and sample complexity for our algorithm, and demonstrate its effectiv",
    "path": "papers/22/11/2211.01528.json",
    "total_tokens": 799,
    "translated_title": "通过后处理实现公平和最优分类",
    "translated_abstract": "为了减轻机器学习模型所呈现的偏见，公平性标准可以整合到训练过程中，以确保在所有人口统计学中实现公平对待，然而这往往是以模型表现为代价的。因此，了解这种权衡是公平算法设计的基础。本文在最普遍的多组、多类别和嘈杂设置下，完整地表征了公平、达摩尔平等在分类问题中的内在权衡。具体而言，我们表明，通过随机化和属性感知公平分类器实现的最小错误率是由沃瑟斯坦重心问题的最优值给出的。在实践方面，我们的发现可以产生一个简单的后处理算法，从评分函数中推导出公平分类器，并在评分为贝叶斯最优时得到最优公平分类器。我们为我们的算法提供了次优性分析和样本复杂性，并展示了它的有效性。",
    "tldr": "本文提出了一个后处理算法，通过评分函数推导公平分类器，达到公平对待不同群体的目的。",
    "en_tdlr": "This paper proposes a post-processing algorithm that derives fair classifiers from score functions, achieving fairness for different demographic groups."
}