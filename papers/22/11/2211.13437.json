{
    "title": "Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning. (arXiv:2211.13437v2 [cs.CV] UPDATED)",
    "abstract": "Cross-modal alignment is essential for vision-language pre-training (VLP) models to learn the correct corresponding information across different modalities. For this purpose, inspired by the success of masked language modeling (MLM) tasks in the NLP pre-training area, numerous masked modeling tasks have been proposed for VLP to further promote cross-modal interactions. The core idea of previous masked modeling tasks is to focus on reconstructing the masked tokens based on visible context for learning local-to-local alignment. However, most of them pay little attention to the global semantic features generated for the masked data, resulting in a limited cross-modal alignment ability of global representations. Therefore, in this paper, we propose a novel Semantic Completion Learning (SCL) task, complementary to existing masked modeling tasks, to facilitate global-to-local alignment. Specifically, the SCL task complements the missing semantics of masked data by capturing the corresponding",
    "link": "http://arxiv.org/abs/2211.13437",
    "context": "Title: Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning. (arXiv:2211.13437v2 [cs.CV] UPDATED)\nAbstract: Cross-modal alignment is essential for vision-language pre-training (VLP) models to learn the correct corresponding information across different modalities. For this purpose, inspired by the success of masked language modeling (MLM) tasks in the NLP pre-training area, numerous masked modeling tasks have been proposed for VLP to further promote cross-modal interactions. The core idea of previous masked modeling tasks is to focus on reconstructing the masked tokens based on visible context for learning local-to-local alignment. However, most of them pay little attention to the global semantic features generated for the masked data, resulting in a limited cross-modal alignment ability of global representations. Therefore, in this paper, we propose a novel Semantic Completion Learning (SCL) task, complementary to existing masked modeling tasks, to facilitate global-to-local alignment. Specifically, the SCL task complements the missing semantics of masked data by capturing the corresponding",
    "path": "papers/22/11/2211.13437.json",
    "total_tokens": 762,
    "translated_title": "见识你所错过的：语义完成学习的视觉-语言预训练",
    "translated_abstract": "跨模态对齐对于视觉-语言预训练（VLP）模型学习不同模态下的正确相关信息至关重要。本文针对以前的蒙面建模任务主要关注重构蒙面标记而忽略了蒙面数据生成的全局语义特征的问题，提出了一种新的“语义完成学习”任务，以便全局对局部的对齐。具体来说，SCL任务通过捕捉相应蒙面数据的缺失语义来补充它们的信息。",
    "tldr": "本文提出了一种新的“语义完成学习”任务，以改善现有蒙面建模任务忽略全局语义特征的问题，通过捕捉相应蒙面数据的缺失语义来补充它们的信息。"
}