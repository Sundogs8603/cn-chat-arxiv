{
    "title": "Estimating the minimizer and the minimum value of a regression function under passive design. (arXiv:2211.16457v2 [math.ST] UPDATED)",
    "abstract": "We propose a new method for estimating the minimizer $\\boldsymbol{x}^*$ and the minimum value $f^*$ of a smooth and strongly convex regression function $f$ from the observations contaminated by random noise. Our estimator $\\boldsymbol{z}_n$ of the minimizer $\\boldsymbol{x}^*$ is based on a version of the projected gradient descent with the gradient estimated by a regularized local polynomial algorithm. Next, we propose a two-stage procedure for estimation of the minimum value $f^*$ of regression function $f$. At the first stage, we construct an accurate enough estimator of $\\boldsymbol{x}^*$, which can be, for example, $\\boldsymbol{z}_n$. At the second stage, we estimate the function value at the point obtained in the first stage using a rate optimal nonparametric procedure. We derive non-asymptotic upper bounds for the quadratic risk and optimization error of $\\boldsymbol{z}_n$, and for the risk of estimating $f^*$. We establish minimax lower bounds showing that, under certain choice ",
    "link": "http://arxiv.org/abs/2211.16457",
    "context": "Title: Estimating the minimizer and the minimum value of a regression function under passive design. (arXiv:2211.16457v2 [math.ST] UPDATED)\nAbstract: We propose a new method for estimating the minimizer $\\boldsymbol{x}^*$ and the minimum value $f^*$ of a smooth and strongly convex regression function $f$ from the observations contaminated by random noise. Our estimator $\\boldsymbol{z}_n$ of the minimizer $\\boldsymbol{x}^*$ is based on a version of the projected gradient descent with the gradient estimated by a regularized local polynomial algorithm. Next, we propose a two-stage procedure for estimation of the minimum value $f^*$ of regression function $f$. At the first stage, we construct an accurate enough estimator of $\\boldsymbol{x}^*$, which can be, for example, $\\boldsymbol{z}_n$. At the second stage, we estimate the function value at the point obtained in the first stage using a rate optimal nonparametric procedure. We derive non-asymptotic upper bounds for the quadratic risk and optimization error of $\\boldsymbol{z}_n$, and for the risk of estimating $f^*$. We establish minimax lower bounds showing that, under certain choice ",
    "path": "papers/22/11/2211.16457.json",
    "total_tokens": 968,
    "translated_title": "估计在被动设计下回归函数的极小值点和最小值",
    "translated_abstract": "我们提出了一种新的方法，用于从带有随机噪声的观测中估计一个平滑且强凸的回归函数$f$的极小值点$x^*$和最小值$f^*$。我们的估计器$z_n$基于一个投影梯度下降的版本，并使用通过正则化局部多项式算法估计的梯度。接下来，我们提出了一个两阶段的估计过程，用于估计回归函数$f$的最小值$f^*$。在第一阶段，我们构建了一个足够准确的$x^*$的估计器，可以是$z_n$。在第二阶段，我们使用一个速率最优的非参数过程来估计在第一阶段得到的点的函数值。我们推导了$z_n$的二次风险和优化误差的非渐进上界，以及估计$f^*$的风险的非渐进上界。我们建立了最小化下界，证明在某些选择下，这些上界是渐进最优的。",
    "tldr": "本论文提出了一种方法，可以在带有随机噪声的观测中估计回归函数的极小值点和最小值。方法基于投影梯度下降和非参数过程，通过推导上界和建立最小化下界证明了方法的渐进最优性。",
    "en_tdlr": "This paper proposes a method for estimating the minimizer and the minimum value of a regression function from observations contaminated by random noise. The method is based on projected gradient descent and non-parametric procedures, and its asymptotic optimality is demonstrated by deriving upper bounds and establishing minimax lower bounds."
}