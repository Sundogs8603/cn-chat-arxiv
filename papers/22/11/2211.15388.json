{
    "title": "Shifted Diffusion for Text-to-image Generation. (arXiv:2211.15388v2 [cs.CV] UPDATED)",
    "abstract": "We present Corgi, a novel method for text-to-image generation. Corgi is based on our proposed shifted diffusion model, which achieves better image embedding generation from input text. Unlike the baseline diffusion model used in DALL-E 2, our method seamlessly encodes prior knowledge of the pre-trained CLIP model in its diffusion process by designing a new initialization distribution and a new transition step of the diffusion. Compared to the strong DALL-E 2 baseline, our method performs better in generating image embedding from the text in terms of both efficiency and effectiveness, resulting in better text-to-image generation. Extensive large-scale experiments are conducted and evaluated in terms of both quantitative measures and human evaluation, indicating a stronger generation ability of our method compared to existing ones. Furthermore, our model enables semi-supervised and language-free training for text-to-image generation, where only part or none of the images in the training ",
    "link": "http://arxiv.org/abs/2211.15388",
    "context": "Title: Shifted Diffusion for Text-to-image Generation. (arXiv:2211.15388v2 [cs.CV] UPDATED)\nAbstract: We present Corgi, a novel method for text-to-image generation. Corgi is based on our proposed shifted diffusion model, which achieves better image embedding generation from input text. Unlike the baseline diffusion model used in DALL-E 2, our method seamlessly encodes prior knowledge of the pre-trained CLIP model in its diffusion process by designing a new initialization distribution and a new transition step of the diffusion. Compared to the strong DALL-E 2 baseline, our method performs better in generating image embedding from the text in terms of both efficiency and effectiveness, resulting in better text-to-image generation. Extensive large-scale experiments are conducted and evaluated in terms of both quantitative measures and human evaluation, indicating a stronger generation ability of our method compared to existing ones. Furthermore, our model enables semi-supervised and language-free training for text-to-image generation, where only part or none of the images in the training ",
    "path": "papers/22/11/2211.15388.json",
    "total_tokens": 915,
    "translated_title": "Shifted Diffusion用于文本到图像生成",
    "translated_abstract": "我们提出了Corgi，一种新颖的文本到图像生成方法。Corgi基于我们提出的Shifted Diffusion模型，可以更好地生成来自输入文本的图像嵌入。与DALL-E 2中使用的基线扩散模型不同，我们的方法通过设计新的初始化分布和扩散的新过渡步骤，在其扩散过程中无缝地编码了预训练CLIP模型的先前知识。与强劲的DALL-E 2基准相比，我们的方法在从文本中生成图像嵌入方面表现更好，具有更高的效率和有效性，从而实现更好的文本到图像生成。进行了大量的大规模实验，并在定量指标和人类评估方面进行了评估，结果表明与现有方法相比，我们的方法具有更强的生成能力。此外，我们的模型实现了文本到图像生成的半监督和无语言训练，只需要部分或不需要训练数据集中的图像与输入文本对齐。",
    "tldr": "本文提出了一种新的文本到图像生成方法，使用Shifted Diffusion模型更好地生成来自输入文本的图像嵌入，并通过大量实验和评估证明了其在效率和有效性方面的优势，同时支持半监督和无语言训练。",
    "en_tdlr": "This paper proposes a novel text-to-image generation method based on Shifted Diffusion model, which achieves better image embedding generation from input text and enables semi-supervised and language-free training. Extensive experiments and evaluation show its superiority in efficiency and effectiveness compared to existing methods."
}