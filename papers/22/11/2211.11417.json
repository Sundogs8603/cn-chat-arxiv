{
    "title": "DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata. (arXiv:2211.11417v2 [cs.CV] UPDATED)",
    "abstract": "Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic videos. However, they require a slow iterative optimization process to synthesize a single fixed-size short video, and they do not offer any post-training control over the synthesis process. We propose Dynamic Neural Cellular Automata (DyNCA), a framework for real-time and controllable dynamic texture synthesis. Our method is built upon the recently introduced NCA models and can synthesize infinitely long and arbitrary-sized realistic video textures in real time. We quantitatively and qualitatively evaluate our model and show that our synthesized videos appear more realistic than the existing results. We improve the SOTA DyTS performance by $2\\sim 4$ orders of magnitude. Moreover, our model offers several real-time video controls including motion speed, motion direction, and an editing brush tool. We exhibit our trained models in an online interactive demo that runs on local hardware and is accessible on personal ",
    "link": "http://arxiv.org/abs/2211.11417",
    "context": "Title: DyNCA: Real-time Dynamic Texture Synthesis Using Neural Cellular Automata. (arXiv:2211.11417v2 [cs.CV] UPDATED)\nAbstract: Current Dynamic Texture Synthesis (DyTS) models can synthesize realistic videos. However, they require a slow iterative optimization process to synthesize a single fixed-size short video, and they do not offer any post-training control over the synthesis process. We propose Dynamic Neural Cellular Automata (DyNCA), a framework for real-time and controllable dynamic texture synthesis. Our method is built upon the recently introduced NCA models and can synthesize infinitely long and arbitrary-sized realistic video textures in real time. We quantitatively and qualitatively evaluate our model and show that our synthesized videos appear more realistic than the existing results. We improve the SOTA DyTS performance by $2\\sim 4$ orders of magnitude. Moreover, our model offers several real-time video controls including motion speed, motion direction, and an editing brush tool. We exhibit our trained models in an online interactive demo that runs on local hardware and is accessible on personal ",
    "path": "papers/22/11/2211.11417.json",
    "total_tokens": 917,
    "translated_title": "DyNCA：使用神经元自动机的实时动态纹理合成",
    "translated_abstract": "目前的动态纹理合成 (DyTS) 模型可以合成逼真的视频，但它们需要缓慢的迭代优化过程来合成单个固定大小的短视频，并且在合成过程中没有后期控制的功能。我们提出了动态神经元自动机 (DyNCA)，这是一个实时可控的动态纹理合成框架。我们的方法建立在最近引入的 NCA 模型之上，可以实时合成无限长和任意大小的逼真视频纹理。我们定量和定性地评估了我们的模型，并展示了我们合成的视频比现有结果更逼真。我们将 SOTA DyTS 的性能提高了 $2\\sim 4$ 个数量级。此外，我们的模型提供了几个实时视频控制功能，包括运动速度、运动方向以及编辑刷工具。我们在一个在线交互式演示中展示了我们训练好的模型，该演示在本地硬件上运行，并可在个人电脑和移动设备上访问。",
    "tldr": "提出了一种名为 DyNCA 的实时可控动态纹理合成框架，可以合成无限长和任意大小的逼真视频纹理，提高了现有结果的逼真程度，并提供了多种实时视频控制功能。",
    "en_tdlr": "Proposed a real-time and controllable dynamic texture synthesis framework called DyNCA, which can synthesize infinitely long and arbitrary-sized realistic video textures, improving the realism of existing results, and providing multiple real-time video control functions."
}