{
    "title": "Data Poisoning Attack Aiming the Vulnerability of Continual Learning. (arXiv:2211.15875v2 [cs.LG] UPDATED)",
    "abstract": "Generally, regularization-based continual learning models limit access to the previous task data to imitate the real-world constraints related to memory and privacy. However, this introduces a problem in these models by not being able to track the performance on each task. In essence, current continual learning methods are susceptible to attacks on previous tasks. We demonstrate the vulnerability of regularization-based continual learning methods by presenting a simple task-specific data poisoning attack that can be used in the learning process of a new task. Training data generated by the proposed attack causes performance degradation on a specific task targeted by the attacker. We experiment with the attack on the two representative regularization-based continual learning methods, Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI), trained with variants of MNIST dataset. The experiment results justify the vulnerability proposed in this paper and demonstrate the importa",
    "link": "http://arxiv.org/abs/2211.15875",
    "context": "Title: Data Poisoning Attack Aiming the Vulnerability of Continual Learning. (arXiv:2211.15875v2 [cs.LG] UPDATED)\nAbstract: Generally, regularization-based continual learning models limit access to the previous task data to imitate the real-world constraints related to memory and privacy. However, this introduces a problem in these models by not being able to track the performance on each task. In essence, current continual learning methods are susceptible to attacks on previous tasks. We demonstrate the vulnerability of regularization-based continual learning methods by presenting a simple task-specific data poisoning attack that can be used in the learning process of a new task. Training data generated by the proposed attack causes performance degradation on a specific task targeted by the attacker. We experiment with the attack on the two representative regularization-based continual learning methods, Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI), trained with variants of MNIST dataset. The experiment results justify the vulnerability proposed in this paper and demonstrate the importa",
    "path": "papers/22/11/2211.15875.json",
    "total_tokens": 900,
    "translated_title": "针对持续学习脆弱性的数据污染攻击",
    "translated_abstract": "通常，基于正则化的持续学习模型限制对先前任务数据的访问，以模拟与内存和隐私有关的真实世界约束。然而，这在这些模型中引入了一个问题，即无法跟踪每个任务的性能。实质上，当前的持续学习方法容易受到对先前任务的攻击。我们通过提出一种简单的任务特定数据污染攻击来展示基于正则化的持续学习方法的脆弱性，该攻击可以在新任务的学习过程中使用。所提攻击生成的训练数据导致攻击者针对的特定任务性能下降。我们在两个具有代表性的基于正则化的持续学习方法（弹性权重整合（EWC）和突触智能（SI））上进行了攻击实验，这两种方法是使用MNIST数据集的变体进行训练的。实验结果证明了本文提出的脆弱性，并展示了其重要性。",
    "tldr": "该论文针对基于正则化的持续学习方法的脆弱性，通过提出一种简单的任务特定数据污染攻击来展示其脆弱性。该攻击导致特定任务的性能下降，并且实验证明了这种脆弱性的重要性。"
}