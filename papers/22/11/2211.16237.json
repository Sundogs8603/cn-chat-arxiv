{
    "title": "Closing the gap between SVRG and TD-SVRG with Gradient Splitting. (arXiv:2211.16237v2 [cs.LG] UPDATED)",
    "abstract": "Temporal difference (TD) learning is a policy evaluation in reinforcement learning whose performance can be enhanced by variance reduction techniques. Recently, multiple works have sought to fuse TD learning with SVRG to obtain a policy evaluation method with a geometric rate of convergence. However, the resulting convergence rate is significantly weaker than what is achieved by SVRG in the setting of convex optimization. In this work we utilize a recent interpretation of TD-learning as the splitting of the gradient of an appropriately chosen function, thus simplifying the algorithm and fusing TD with SVRG. Our main result is a geometric convergence bound with predetermined learning rate of $1/8$, which is identical to the convergence bound available for SVRG in the convex setting. Our theoretical findings are supported by a set of experiments.",
    "link": "http://arxiv.org/abs/2211.16237",
    "context": "Title: Closing the gap between SVRG and TD-SVRG with Gradient Splitting. (arXiv:2211.16237v2 [cs.LG] UPDATED)\nAbstract: Temporal difference (TD) learning is a policy evaluation in reinforcement learning whose performance can be enhanced by variance reduction techniques. Recently, multiple works have sought to fuse TD learning with SVRG to obtain a policy evaluation method with a geometric rate of convergence. However, the resulting convergence rate is significantly weaker than what is achieved by SVRG in the setting of convex optimization. In this work we utilize a recent interpretation of TD-learning as the splitting of the gradient of an appropriately chosen function, thus simplifying the algorithm and fusing TD with SVRG. Our main result is a geometric convergence bound with predetermined learning rate of $1/8$, which is identical to the convergence bound available for SVRG in the convex setting. Our theoretical findings are supported by a set of experiments.",
    "path": "papers/22/11/2211.16237.json",
    "total_tokens": 805,
    "translated_title": "用梯度分割方法缩小SVRG与TD-SVRG之间的差距",
    "translated_abstract": "TD（时序差分）学习是一种增强学习中的策略评估方法，其性能可以通过方差缩减技术进行增强。最近，多个工作尝试将TD学习与SVRG相结合，以获得一种具有几何收敛速度的策略评估方法。然而，在凸优化设置下，所得到的收敛速度明显不及SVRG。本研究利用最近对TD学习的解释，将其视为一个适当选择函数的梯度的分割，从而简化了算法，并将TD与SVRG相结合。我们的主要结果是一个具有预定学习速率为1/8的几何收敛界限，与凸设置下SVRG的收敛界限相同。我们的理论发现得到了一系列实验证明。",
    "tldr": "本论文通过将TD学习视为适当选择函数的梯度分割，将TD和SVRG相结合，实现了具有几何收敛速度的策略评估方法，并在理论和实验上得到了支持。"
}