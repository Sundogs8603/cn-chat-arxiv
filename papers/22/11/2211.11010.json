{
    "title": "Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric. (arXiv:2211.11010v2 [cs.CV] UPDATED)",
    "abstract": "Combining the Color and Event cameras (also called Dynamic Vision Sensors, DVS) for robust object tracking is a newly emerging research topic in recent years. Existing color-event tracking framework usually contains multiple scattered modules which may lead to low efficiency and high computational complexity, including feature extraction, fusion, matching, interactive learning, etc. In this paper, we propose a single-stage backbone network for Color-Event Unified Tracking (CEUTrack), which achieves the above functions simultaneously. Given the event points and RGB frames, we first transform the points into voxels and crop the template and search regions for both modalities, respectively. Then, these regions are projected into tokens and parallelly fed into the unified Transformer backbone network. The output features will be fed into a tracking head for target object localization. Our proposed CEUTrack is simple, effective, and efficient, which achieves over 75 FPS and new SOTA perform",
    "link": "http://arxiv.org/abs/2211.11010",
    "context": "Title: Revisiting Color-Event based Tracking: A Unified Network, Dataset, and Metric. (arXiv:2211.11010v2 [cs.CV] UPDATED)\nAbstract: Combining the Color and Event cameras (also called Dynamic Vision Sensors, DVS) for robust object tracking is a newly emerging research topic in recent years. Existing color-event tracking framework usually contains multiple scattered modules which may lead to low efficiency and high computational complexity, including feature extraction, fusion, matching, interactive learning, etc. In this paper, we propose a single-stage backbone network for Color-Event Unified Tracking (CEUTrack), which achieves the above functions simultaneously. Given the event points and RGB frames, we first transform the points into voxels and crop the template and search regions for both modalities, respectively. Then, these regions are projected into tokens and parallelly fed into the unified Transformer backbone network. The output features will be fed into a tracking head for target object localization. Our proposed CEUTrack is simple, effective, and efficient, which achieves over 75 FPS and new SOTA perform",
    "path": "papers/22/11/2211.11010.json",
    "total_tokens": 957,
    "translated_title": "重新审视基于颜色事件的跟踪：一个统一的网络、数据集和度量",
    "translated_abstract": "近年来，将颜色和事件相机（也称为动态视觉传感器，DVS）结合起来进行鲁棒的目标跟踪是一个新兴的研究课题。现有的颜色-事件跟踪框架通常包含多个离散的模块，可能导致低效率和高计算复杂度，包括特征提取、融合、匹配、交互学习等。本文提出了一个用于颜色-事件统一跟踪（CEUTrack）的单阶段骨干网络，该网络同时实现了上述功能。给定事件点和RGB帧，我们首先将点转换为体素，并分别裁剪模板和搜索区域的两种模态。然后，这些区域被投影到令牌中，并并行输入到统一的Transformer骨干网络。输出特征将被输入到跟踪头部进行目标对象定位。我们提出的CEUTrack简单、有效、高效，达到了超过75 FPS和新的SOTA性能。",
    "tldr": "本文提出了一个用于颜色-事件统一跟踪的单阶段骨干网络（CEUTrack），通过将事件点和RGB帧转换为体素，将裁剪后的模板和搜索区域投影到令牌中，并通过统一的Transformer骨干网络实现了目标对象的定位。这一方法简单、有效且高效。"
}