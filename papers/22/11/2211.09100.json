{
    "title": "Global Optimization with Parametric Function Approximation. (arXiv:2211.09100v3 [cs.LG] UPDATED)",
    "abstract": "We consider the problem of global optimization with noisy zeroth order oracles - a well-motivated problem useful for various applications ranging from hyper-parameter tuning for deep learning to new material design. Existing work relies on Gaussian processes or other non-parametric family, which suffers from the curse of dimensionality. In this paper, we propose a new algorithm GO-UCB that leverages a parametric family of functions (e.g., neural networks) instead. Under a realizable assumption and a few other mild geometric conditions, we show that GO-UCB achieves a cumulative regret of \\~O$(\\sqrt{T})$ where $T$ is the time horizon. At the core of GO-UCB is a carefully designed uncertainty set over parameters based on gradients that allows optimistic exploration. Synthetic and real-world experiments illustrate GO-UCB works better than popular Bayesian optimization approaches, even if the model is misspecified.",
    "link": "http://arxiv.org/abs/2211.09100",
    "context": "Title: Global Optimization with Parametric Function Approximation. (arXiv:2211.09100v3 [cs.LG] UPDATED)\nAbstract: We consider the problem of global optimization with noisy zeroth order oracles - a well-motivated problem useful for various applications ranging from hyper-parameter tuning for deep learning to new material design. Existing work relies on Gaussian processes or other non-parametric family, which suffers from the curse of dimensionality. In this paper, we propose a new algorithm GO-UCB that leverages a parametric family of functions (e.g., neural networks) instead. Under a realizable assumption and a few other mild geometric conditions, we show that GO-UCB achieves a cumulative regret of \\~O$(\\sqrt{T})$ where $T$ is the time horizon. At the core of GO-UCB is a carefully designed uncertainty set over parameters based on gradients that allows optimistic exploration. Synthetic and real-world experiments illustrate GO-UCB works better than popular Bayesian optimization approaches, even if the model is misspecified.",
    "path": "papers/22/11/2211.09100.json",
    "total_tokens": 890,
    "translated_title": "具有参数函数逼近的全局优化问题",
    "translated_abstract": "本文研究了具有噪声零阶访问器的全局优化问题，这是一个有用于从深度学习的超参数调整到新材料设计等各种应用的问题。现有的工作依赖于高斯过程或其他非参数家族，这些方法受到维度灾难的影响。本文提出了一种新的算法GO-UCB，它采用参数化函数家族（如神经网络）。在一个可实现的假设和一些其他温和的几何条件下，我们证明GO-UCB实现了一个累积遗憾度$\\sim O(\\sqrt{T})$，其中$T$是时间跨度。GO-UCB的核心是一个基于梯度的精心设计的参数不确定性集合，可以进行乐观的探索。合成和真实世界的实验证明，即使模型被错误指定，GO-UCB的效果也优于流行的贝叶斯优化方法。",
    "tldr": "本文提出了一种具有参数函数逼近的全局优化算法GO-UCB，采用基于梯度的参数不确定性集合进行乐观探索，实验证明其在处理具有噪声零阶访问器的全局优化问题上优于传统的贝叶斯优化方法。",
    "en_tdlr": "This paper proposes a global optimization algorithm, GO-UCB, with parametric function approximation. It leverages a carefully designed uncertainty set based on gradients for optimistic exploration and outperforms traditional Bayesian optimization methods in handling global optimization problems with noisy zeroth order oracles."
}