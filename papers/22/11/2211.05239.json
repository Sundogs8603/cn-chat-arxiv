{
    "title": "RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure. (arXiv:2211.05239v3 [cs.LG] UPDATED)",
    "abstract": "We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. Re",
    "link": "http://arxiv.org/abs/2211.05239",
    "context": "Title: RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure. (arXiv:2211.05239v3 [cs.LG] UPDATED)\nAbstract: We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. Re",
    "path": "papers/22/11/2211.05239.json",
    "total_tokens": 971,
    "translated_title": "RecD：为端到端深度学习推荐模型训练基础设施提供去重功能",
    "translated_abstract": "我们提出了 RecD（推荐去重），它是一组针对深度学习推荐模型 (DLRM) 训练流程的端到端基础设施优化。RecD解决了由于特征重复造成的海量存储、预处理和训练开销，这是大规模 DLRM 训练数据集内在的问题，因为 DLRM 数据集是从交互中生成的。我们展示了 RecD 如何利用此属性来优化生产数据的流程，减少数据集存储和预处理需求，并最大限度地在训练批次中重复。RecD 引入了一种新的张量格式 InverseKeyedJaggedTensors (IKJTs)，以在每个批次中去除特征值的重复。我们展示了 DLRM 模型架构如何利用 IKJTs 来显著提高训练吞吐量。",
    "tldr": "RecD 是一种为 DLRM 训练提供去重功能的端到端基础设施优化，解决了由于特征重复造成的海量存储、预处理和训练开销，引入了新的张量格式 InverseKeyedJaggedTensors (IKJTs) 来去除特征值的重复，使 DLRM 模型架构能够更好地利用数据的重复性提高训练吞吐量。",
    "en_tdlr": "RecD is an end-to-end infrastructure optimization for Deep Learning Recommendation Model (DLRM) training, providing deduplication to address the storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. It introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to remove feature value duplication within training batches and leverage data repetition to drastically increase training throughput of DLRM model architectures."
}