{
    "title": "Unlearning Graph Classifiers with Limited Data Resources. (arXiv:2211.03216v2 [cs.LG] UPDATED)",
    "abstract": "As the demand for user privacy grows, controlled data removal (machine unlearning) is becoming an important feature of machine learning models for data-sensitive Web applications such as social networks and recommender systems. Nevertheless, at this point it is still largely unknown how to perform efficient machine unlearning of graph neural networks (GNNs); this is especially the case when the number of training samples is small, in which case unlearning can seriously compromise the performance of the model. To address this issue, we initiate the study of unlearning the Graph Scattering Transform (GST), a mathematical framework that is efficient, provably stable under feature or graph topology perturbations, and offers graph classification performance comparable to that of GNNs. Our main contribution is the first known nonlinear approximate graph unlearning method based on GSTs. Our second contribution is a theoretical analysis of the computational complexity of the proposed unlearnin",
    "link": "http://arxiv.org/abs/2211.03216",
    "context": "Title: Unlearning Graph Classifiers with Limited Data Resources. (arXiv:2211.03216v2 [cs.LG] UPDATED)\nAbstract: As the demand for user privacy grows, controlled data removal (machine unlearning) is becoming an important feature of machine learning models for data-sensitive Web applications such as social networks and recommender systems. Nevertheless, at this point it is still largely unknown how to perform efficient machine unlearning of graph neural networks (GNNs); this is especially the case when the number of training samples is small, in which case unlearning can seriously compromise the performance of the model. To address this issue, we initiate the study of unlearning the Graph Scattering Transform (GST), a mathematical framework that is efficient, provably stable under feature or graph topology perturbations, and offers graph classification performance comparable to that of GNNs. Our main contribution is the first known nonlinear approximate graph unlearning method based on GSTs. Our second contribution is a theoretical analysis of the computational complexity of the proposed unlearnin",
    "path": "papers/22/11/2211.03216.json",
    "total_tokens": 853,
    "translated_title": "有限数据资源下的图分类器遗忘方法",
    "translated_abstract": "随着用户隐私需求的增加，对机器学习模型进行受控数据删除（机器遗忘）成为数据敏感的网络应用（如社交网络和推荐系统）中的重要特性。然而，目前还不清楚如何在图神经网络（GNNs）中进行高效的机器遗忘，尤其是在训练样本数量较少的情况下，此时遗忘可能严重损害模型性能。为解决这个问题，我们首次研究了遗忘图散射变换（GST），这是一个高效的数学框架，对特征或图拓扑扰动具有可证明的稳定性，并且在图分类性能方面与GNNs相当。我们的主要贡献是基于GSTs的第一个已知的非线性近似图遗忘方法。第二个贡献是对所提出的遗忘方法的计算复杂性进行理论分析。",
    "tldr": "本文研究了有限数据资源下的图分类器遗忘方法，提出了一种基于图散射变换的非线性近似图遗忘方法，并进行了计算复杂性的理论分析。",
    "en_tdlr": "This paper investigates the method of unlearning graph classifiers with limited data resources, proposing a nonlinear approximate graph unlearning method based on graph scattering transforms (GSTs) and conducting theoretical analysis on its computational complexity."
}