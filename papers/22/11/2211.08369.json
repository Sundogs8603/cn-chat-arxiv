{
    "title": "Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods. (arXiv:2211.08369v3 [cs.CL] UPDATED)",
    "abstract": "A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful has been to use evaluation-by-agreement -- if multiple methods agree on an explanation, its credibility increases. However, recent work has found that saliency methods exhibit weak rank correlations even when applied to the same model instance and advocated for the use of alternative diagnostic methods. In our work, we demonstrate that rank correlation is not a good fit for evaluating agreement and argue that Pearson-$r$ is a better-suited alternative. We further show that regularization techniques that increase faithfulness of attention explanations also increase agreement between saliency methods. By connecting our findings to instance categories based on training dynamics, we show that the agreement of saliency method explanations is very low f",
    "link": "http://arxiv.org/abs/2211.08369",
    "context": "Title: Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods. (arXiv:2211.08369v3 [cs.CL] UPDATED)\nAbstract: A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful has been to use evaluation-by-agreement -- if multiple methods agree on an explanation, its credibility increases. However, recent work has found that saliency methods exhibit weak rank correlations even when applied to the same model instance and advocated for the use of alternative diagnostic methods. In our work, we demonstrate that rank correlation is not a good fit for evaluating agreement and argue that Pearson-$r$ is a better-suited alternative. We further show that regularization techniques that increase faithfulness of attention explanations also increase agreement between saliency methods. By connecting our findings to instance categories based on training dynamics, we show that the agreement of saliency method explanations is very low f",
    "path": "papers/22/11/2211.08369.json",
    "total_tokens": 1009,
    "translated_title": "易于决定，难以达成一致：减少显著性方法之间的分歧",
    "translated_abstract": "揭示神经NLP模型黑匣子的流行方法是利用显著性方法，它们为每个输入组件分配标量重要性分数。评估解释性方法是否忠实的常见做法是使用“协议评估”——如果多种方法对解释达成一致，其可信度就会增加。然而，最近的研究发现，即使应用于同一模型实例，显著性方法也表现出较弱的秩相关性，并倡导使用替代诊断方法。在我们的工作中，我们证明了秩相关性不适合评估一致性，并认为Pearson-r是更适合的替代方法。我们进一步表明，增加注意力解释的忠实度的正则化技术也会增加显著性方法之间的一致性。通过将我们的发现与基于培训动态的实例类别相连接，我们展示了标准基准数据集中某些类别的显著性方法解释的一致性非常低，例如细粒度情感分类，并提出未来研究的潜在方向。",
    "tldr": "本文介绍了利用显著性方法揭示神经NLP模型黑匣子，协议评估并不能保证可靠性，通过使用Pearson-r更适合的替代方案实现一致性，同时通过正则化技术提高注意力解释的忠实度。",
    "en_tdlr": "This paper presents the use of saliency methods to unveil the black box of neural NLP models and argues that evaluation-by-agreement is not reliable. The authors propose Pearson-r as a better alternative for evaluating agreement and show that regularization techniques can increase the faithfulness of attention explanations and the agreement between saliency methods. The paper also suggests potential directions for future research based on the connection between findings and instance categories."
}