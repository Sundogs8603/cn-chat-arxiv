{
    "title": "PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales. (arXiv:2211.01562v3 [cs.CL] UPDATED)",
    "abstract": "Neural language models (LMs) have achieved impressive results on various language-based reasoning tasks by utilizing latent knowledge encoded in their own pretrained parameters. To make this reasoning process more explicit, recent works retrieve a rationalizing LM's internal knowledge by training or prompting it to generate free-text rationales, which can be used to guide task predictions made by either the same LM or a separate reasoning LM. However, rationalizing LMs require expensive rationale annotation and/or computation, without any assurance that their generated rationales improve LM task performance or faithfully reflect LM decision-making. In this paper, we propose PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns to faithfully reason over rationales via counterfactual regularization. First, PINTO maps out a suitable reasoning process for the task input by prompting a frozen rationalizing LM to generate a free-text rationale. Second, PINTO's reasoni",
    "link": "http://arxiv.org/abs/2211.01562",
    "context": "Title: PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales. (arXiv:2211.01562v3 [cs.CL] UPDATED)\nAbstract: Neural language models (LMs) have achieved impressive results on various language-based reasoning tasks by utilizing latent knowledge encoded in their own pretrained parameters. To make this reasoning process more explicit, recent works retrieve a rationalizing LM's internal knowledge by training or prompting it to generate free-text rationales, which can be used to guide task predictions made by either the same LM or a separate reasoning LM. However, rationalizing LMs require expensive rationale annotation and/or computation, without any assurance that their generated rationales improve LM task performance or faithfully reflect LM decision-making. In this paper, we propose PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns to faithfully reason over rationales via counterfactual regularization. First, PINTO maps out a suitable reasoning process for the task input by prompting a frozen rationalizing LM to generate a free-text rationale. Second, PINTO's reasoni",
    "path": "papers/22/11/2211.01562.json",
    "total_tokens": 1107,
    "translated_title": "使用提示生成的原理实现忠实的语言推理——PINTO",
    "translated_abstract": "神经语言模型（LM）利用自己预训练参数中的潜在知识在各种基于语言的推理任务上取得了令人瞩目的成果。为了使这个推理过程更加明确，最近的研究利用训练或提示的方式检索LM的内部知识生成自由文本原理，可以用于引导相同LM或单独的推理LM进行任务预测。然而，理性化的LM需要昂贵的原理注释和（或）计算，并不能保证它们生成的原理会改善LM的任务表现或忠实地反映LM的决策。本文提出了PINTO，一种通过提示生成的学习来理性化的LM管道，并通过反事实正则化学习忠实地推理原理。首先，PINTO通过提示冻结的理性化LM生成自由文本的原理，为任务输入制定了合适的推理过程。其次，PINTO的推理LM使用生成的原理进行预测，同时被反事实正则化项指导，该项鼓励即使对原理进行微小的更改，预测也相同。我们在各种语言推理数据集上的实验表明，PINTO显著优于使用外部原理的基线，同时提供了可理解的、忠实反映LM决策过程的理性。",
    "tldr": "本文提出了PINTO，基于提示生成的方式实现LM的忠实推理，并通过反事实正则化来学习。实验表明，PINTO显著优于使用外部原理的基线，同时提供了可理解的、忠实反映LM决策过程的理性。"
}