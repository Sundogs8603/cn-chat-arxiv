{
    "title": "Data Level Lottery Ticket Hypothesis for Vision Transformers. (arXiv:2211.01484v2 [cs.CV] UPDATED)",
    "abstract": "The conventional lottery ticket hypothesis (LTH) claims that there exists a sparse subnetwork within a dense neural network and a proper random initialization method called the winning ticket, such that it can be trained from scratch to almost as good as the dense counterpart. Meanwhile, the research of LTH in vision transformers (ViTs) is scarcely evaluated. In this paper, we first show that the conventional winning ticket is hard to find at the weight level of ViTs by existing methods. Then, we generalize the LTH for ViTs to input data consisting of image patches inspired by the input dependence of ViTs. That is, there exists a subset of input image patches such that a ViT can be trained from scratch by using only this subset of patches and achieve similar accuracy to the ViTs trained by using all image patches. We call this subset of input patches the em winning tickets, which represent a significant amount of information in the input data. We use a ticket selector to generate the w",
    "link": "http://arxiv.org/abs/2211.01484",
    "context": "Title: Data Level Lottery Ticket Hypothesis for Vision Transformers. (arXiv:2211.01484v2 [cs.CV] UPDATED)\nAbstract: The conventional lottery ticket hypothesis (LTH) claims that there exists a sparse subnetwork within a dense neural network and a proper random initialization method called the winning ticket, such that it can be trained from scratch to almost as good as the dense counterpart. Meanwhile, the research of LTH in vision transformers (ViTs) is scarcely evaluated. In this paper, we first show that the conventional winning ticket is hard to find at the weight level of ViTs by existing methods. Then, we generalize the LTH for ViTs to input data consisting of image patches inspired by the input dependence of ViTs. That is, there exists a subset of input image patches such that a ViT can be trained from scratch by using only this subset of patches and achieve similar accuracy to the ViTs trained by using all image patches. We call this subset of input patches the em winning tickets, which represent a significant amount of information in the input data. We use a ticket selector to generate the w",
    "path": "papers/22/11/2211.01484.json",
    "total_tokens": 1035,
    "translated_title": "面向视觉Transformer的数据级彩票假设",
    "translated_abstract": "传统彩票假说（LTH）声称在密集神经网络中存在着一个稀疏的子网络和一个称为“获奖彩票”的适当随机初始化方法，以便可以从头开始训练它，使其几乎像密集网络一样好。与此同时，对于视觉Transformer（ViTs）中LTH的研究却很少被评估。本文首先表明了在现有方法下，在ViTs的权重级别上寻找传统的获奖彩票是困难的。然后，我们将ViTs的LTH推广到由图像补丁组成的输入数据中，受ViTs输入依赖启发。也就是说，存在一个输入图像补丁的子集，使得通过仅使用该子集，可以从头开始训练ViT，并达到与使用所有图像补丁训练的ViTs相似的精度。我们将这个输入补丁子集称为em获奖彩票，它代表了输入数据中的大量信息。我们使用票选择器生成带有em获奖彩票的权重子集，并使用EMD训练这些“获奖”子集，可以证明这种方法在各种视觉任务中都是可行的和有效的。",
    "tldr": "本文将传统的彩票假说（LTH）扩展到由图像补丁组成的输入数据中，证明存在一个子集的输入图像补丁使得可以从头开始训练视觉Transformer（ViT），并且达到与使用所有图像补丁训练的ViTs相似的精度，这种方法在各种视觉任务中都是可行的和有效的。",
    "en_tdlr": "This paper extends the conventional lottery ticket hypothesis (LTH) to input data consisting of image patches and demonstrates that a subset of input image patches exists such that a Vision Transformer (ViT) can be trained from scratch by using only this subset of patches and achieve similar accuracy to the ViTs trained by using all image patches. This method is proven feasible and effective in various visual tasks."
}