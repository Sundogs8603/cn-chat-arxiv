{
    "title": "MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. (arXiv:2211.07321v3 [cs.CL] UPDATED)",
    "abstract": "In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend.",
    "link": "http://arxiv.org/abs/2211.07321",
    "context": "Title: MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. (arXiv:2211.07321v3 [cs.CL] UPDATED)\nAbstract: In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend.",
    "path": "papers/22/11/2211.07321.json",
    "total_tokens": 964,
    "translated_title": "MT4SSL：通过集成多个目标来提升自监督语音表示学习",
    "translated_abstract": "本文从训练目标的获取方式提出了自监督语音模型的新视角，并将目标提取器概括为离线目标提取器和在线目标提取器。基于此，我们提出了一种新的自监督多任务学习框架MT4SSL，它使用K均值算法作为离线目标提取器，使用没有梯度的教师网络作为在线目标提取器。实验结果表明，我们的模型在LibriSpeech基准测试上优于以前的方法，并且与使用更少数据的最佳模型相当甚至更好。此外，我们发现在预训练阶段同时使用离线和在线目标提取器可以得到更好的收敛性。因此，我们认为从我们的角度进行自监督语音模型上的多任务学习是一种有前途的趋势。",
    "tldr": "本文提出了一个新的自监督多任务学习框架MT4SSL，通过同时使用K均值算法作为离线目标提取器和没有梯度的教师网络作为在线目标提取器，取得了比以前更好的表现。同时，使用离线和在线目标提取器可以得到更好的收敛性，我们认为这是自监督语音模型上的多任务学习有前途的趋势。",
    "en_tdlr": "This paper proposes a new multi-tasking learning framework for self-supervised speech representation learning, called MT4SSL, which uses both K-means algorithm and teacher network as target extractors and achieves better performance than previous SSL methods. The use of both offline and online target extractors leads to better convergence in the pre-training phase, indicating the promising trend of multi-task learning on self-supervised speech models."
}