{
    "title": "VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval. (arXiv:2211.12764v3 [cs.CV] UPDATED)",
    "abstract": "Many recent studies leverage the pre-trained CLIP for text-video cross-modal retrieval by tuning the backbone with additional heavy modules, which not only brings huge computational burdens with much more parameters, but also leads to the knowledge forgetting from upstream models. In this work, we propose the VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the text-video retrieval task. The proposed VoP is an end-to-end framework with both video & text prompts introducing, which can be regarded as a powerful baseline with only 0.1% trainable parameters. Further, based on the spatio-temporal characteristics of videos, we develop three novel video prompt mechanisms to improve the performance with different scales of trainable parameters. The basic idea of the VoP enhancement is to model the frame position, frame context, and layer function with specific trainable prompts, respectively. Extensive experiments show that compared to full fine-tuning, the enhanced VoP achie",
    "link": "http://arxiv.org/abs/2211.12764",
    "context": "Title: VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval. (arXiv:2211.12764v3 [cs.CV] UPDATED)\nAbstract: Many recent studies leverage the pre-trained CLIP for text-video cross-modal retrieval by tuning the backbone with additional heavy modules, which not only brings huge computational burdens with much more parameters, but also leads to the knowledge forgetting from upstream models. In this work, we propose the VoP: Text-Video Co-operative Prompt Tuning for efficient tuning on the text-video retrieval task. The proposed VoP is an end-to-end framework with both video & text prompts introducing, which can be regarded as a powerful baseline with only 0.1% trainable parameters. Further, based on the spatio-temporal characteristics of videos, we develop three novel video prompt mechanisms to improve the performance with different scales of trainable parameters. The basic idea of the VoP enhancement is to model the frame position, frame context, and layer function with specific trainable prompts, respectively. Extensive experiments show that compared to full fine-tuning, the enhanced VoP achie",
    "path": "papers/22/11/2211.12764.json",
    "total_tokens": 977,
    "translated_title": "VoP：用于文本视频跨模态检索的协作提示调整",
    "translated_abstract": "许多最近的研究利用预训练的CLIP来通过使用额外的重模块来调整backbone从而实现文本视频跨模态检索，这不仅带来了大量的计算负担和更多的参数，也导致了上游模型的知识遗忘。本文提出了VoP：文本视频协作提示调整，以实现对文本视频检索任务的高效调整。所提出的VoP是一个端到端框架，具有引入视频和文本提示的能力，可视为具有仅0.1％可训练参数的强大基线。此外，基于视频的时空特征，我们开发了三种新型视频提示机制，以提高不同可训练参数规模的性能。VoP增强的基本思想是分别利用特定的可训练提示来模拟帧位置，帧上下文和层函数。大量实验表明，与完全微调相比，增强的VoP实现了相似甚至更好的效果，并且具有更少的可训练参数和更低的计算复杂度。",
    "tldr": "本论文提出了一种用于文本视频跨模态检索的协作提示调整（VoP）框架，与传统方法相比，VoP具有更少的可训练参数和更低的计算复杂度，同时实现了相似甚至更好的性能。"
}