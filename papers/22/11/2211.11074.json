{
    "title": "Frozen Overparameterization: A Double Descent Perspective on Transfer Learning of Deep Neural Networks. (arXiv:2211.11074v2 [cs.LG] UPDATED)",
    "abstract": "We study the generalization behavior of transfer learning of deep neural networks (DNNs). We adopt the overparameterization perspective -- featuring interpolation of the training data (i.e., approximately zero train error) and the double descent phenomenon -- to explain the delicate effect of the transfer learning setting on generalization performance. We study how the generalization behavior of transfer learning is affected by the dataset size in the source and target tasks, the number of transferred layers that are kept frozen in the target DNN training, and the similarity between the source and target tasks. We show that the test error evolution during the target DNN training has a more significant double descent effect when the target training dataset is sufficiently large. In addition, a larger source training dataset can yield a slower target DNN training. Moreover, we demonstrate that the number of frozen layers can determine whether the transfer learning is effectively underpar",
    "link": "http://arxiv.org/abs/2211.11074",
    "context": "Title: Frozen Overparameterization: A Double Descent Perspective on Transfer Learning of Deep Neural Networks. (arXiv:2211.11074v2 [cs.LG] UPDATED)\nAbstract: We study the generalization behavior of transfer learning of deep neural networks (DNNs). We adopt the overparameterization perspective -- featuring interpolation of the training data (i.e., approximately zero train error) and the double descent phenomenon -- to explain the delicate effect of the transfer learning setting on generalization performance. We study how the generalization behavior of transfer learning is affected by the dataset size in the source and target tasks, the number of transferred layers that are kept frozen in the target DNN training, and the similarity between the source and target tasks. We show that the test error evolution during the target DNN training has a more significant double descent effect when the target training dataset is sufficiently large. In addition, a larger source training dataset can yield a slower target DNN training. Moreover, we demonstrate that the number of frozen layers can determine whether the transfer learning is effectively underpar",
    "path": "papers/22/11/2211.11074.json",
    "total_tokens": 958,
    "translated_title": "冻结过度参数化：对深度神经网络转移学习的双峰透视研究",
    "translated_abstract": "本文研究了深度神经网络（DNN）转移学习的泛化行为。我们采用过度参数化的视角——特征插值（即近似于零的训练误差）和双峰现象——来解释转移学习设置对泛化性能的微妙影响。我们研究了源任务和目标任务的数据集大小、在目标DNN训练中保持冻结的转移层数量以及源和目标任务之间的相似度对转移学习泛化行为的影响。我们表明，当目标训练数据集足够大时，在目标DNN训练过程中测试误差演变会有更明显的双峰效应。此外，更大的源训练数据集可能导致较慢的目标DNN训练。此外，我们证明了冻结层数量可以确定转移学习是否有效地低于...",
    "tldr": "本文研究了深度神经网络（DNN）转移学习的泛化行为。我们发现，目标数据集大时，测试误差演变会有更明显的双峰效应。更大的源数据集可能会导致较慢的目标DNN训练。此外，我们证明了冻结层数量可以确定转移学习是否有效地低于..."
}