{
    "title": "An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding. (arXiv:2211.08161v2 [eess.AS] UPDATED)",
    "abstract": "Continual learning refers to a dynamical framework in which a model receives a stream of non-stationary data over time and must adapt to new data while preserving previously acquired knowledge. Unluckily, neural networks fail to meet these two desiderata, incurring the so-called catastrophic forgetting phenomenon. Whereas a vast array of strategies have been proposed to attenuate forgetting in the computer vision domain, for speech-related tasks, on the other hand, there is a dearth of works. In this paper, we consider the joint use of rehearsal and knowledge distillation (KD) approaches for spoken language understanding under a class-incremental learning scenario. We report on multiple KD combinations at different levels in the network, showing that combining feature-level and predictions-level KDs leads to the best results. Finally, we provide an ablation study on the effect of the size of the rehearsal memory that corroborates the efficacy of our approach for low-resource devices.",
    "link": "http://arxiv.org/abs/2211.08161",
    "context": "Title: An Investigation of the Combination of Rehearsal and Knowledge Distillation in Continual Learning for Spoken Language Understanding. (arXiv:2211.08161v2 [eess.AS] UPDATED)\nAbstract: Continual learning refers to a dynamical framework in which a model receives a stream of non-stationary data over time and must adapt to new data while preserving previously acquired knowledge. Unluckily, neural networks fail to meet these two desiderata, incurring the so-called catastrophic forgetting phenomenon. Whereas a vast array of strategies have been proposed to attenuate forgetting in the computer vision domain, for speech-related tasks, on the other hand, there is a dearth of works. In this paper, we consider the joint use of rehearsal and knowledge distillation (KD) approaches for spoken language understanding under a class-incremental learning scenario. We report on multiple KD combinations at different levels in the network, showing that combining feature-level and predictions-level KDs leads to the best results. Finally, we provide an ablation study on the effect of the size of the rehearsal memory that corroborates the efficacy of our approach for low-resource devices.",
    "path": "papers/22/11/2211.08161.json",
    "total_tokens": 926,
    "translated_title": "串讲和知识蒸馏在言语理解的渐进学习中的联合应用调查",
    "translated_abstract": "渐进学习是一个动态框架，模型随着时间接收非平稳的数据流，并在保留以前习得的知识的同时适应新数据。不幸的是，神经网络无法满足这两个期望，从而导致所谓的灾难性遗忘现象。虽然已经提出了大量策略以减轻计算机视觉领域中的遗忘，但在涉及语音的任务中，却缺乏这方面的工作。在本文中，我们考虑了串讲和知识蒸馏（KD）方法在渐进学习情况下的联合使用以进行言语理解。我们报告了多个级别的KD组合，并显示组合特征级别和预测级别的KD会带来最好的结果。最后，我们对串讲记忆大小的影响进行了消融研究，证实了我们的方法对低资源设备的有效性。",
    "tldr": "本文考虑了串讲和知识蒸馏（KD）方法在渐进学习情况下的联合应用，证实组合特征级别和预测级别的KD会带来最好的结果，并证实了这种方法对低资源设备的有效性。",
    "en_tdlr": "This paper investigates the joint use of rehearsal and knowledge distillation (KD) approaches in continual learning for spoken language understanding. The results show that combining feature-level and predictions-level KDs leads to the best results, and also provide evidence for the efficacy of this approach for low-resource devices."
}