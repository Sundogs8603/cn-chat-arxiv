{
    "title": "Event Tables for Efficient Experience Replay. (arXiv:2211.00576v2 [cs.LG] UPDATED)",
    "abstract": "Experience replay (ER) is a crucial component of many deep reinforcement learning (RL) systems. However, uniform sampling from an ER buffer can lead to slow convergence and unstable asymptotic behaviors. This paper introduces Stratified Sampling from Event Tables (SSET), which partitions an ER buffer into Event Tables, each capturing important subsequences of optimal behavior. We prove a theoretical advantage over the traditional monolithic buffer approach and combine SSET with an existing prioritized sampling strategy to further improve learning speed and stability. Empirical results in challenging MiniGrid domains, benchmark RL environments, and a high-fidelity car racing simulator demonstrate the advantages and versatility of SSET over existing ER buffer sampling approaches.",
    "link": "http://arxiv.org/abs/2211.00576",
    "context": "Title: Event Tables for Efficient Experience Replay. (arXiv:2211.00576v2 [cs.LG] UPDATED)\nAbstract: Experience replay (ER) is a crucial component of many deep reinforcement learning (RL) systems. However, uniform sampling from an ER buffer can lead to slow convergence and unstable asymptotic behaviors. This paper introduces Stratified Sampling from Event Tables (SSET), which partitions an ER buffer into Event Tables, each capturing important subsequences of optimal behavior. We prove a theoretical advantage over the traditional monolithic buffer approach and combine SSET with an existing prioritized sampling strategy to further improve learning speed and stability. Empirical results in challenging MiniGrid domains, benchmark RL environments, and a high-fidelity car racing simulator demonstrate the advantages and versatility of SSET over existing ER buffer sampling approaches.",
    "path": "papers/22/11/2211.00576.json",
    "total_tokens": 776,
    "translated_title": "有效经验回放的事件表",
    "translated_abstract": "经验回放(ER)是许多深度强化学习(RL)系统的关键组成部分。然而，从ER缓冲区进行统一采样可能导致缓慢的收敛和不稳定的渐近行为。本文介绍了事件表的分层采样（SSET），将ER缓冲区分为事件表，每个事件表捕获了最优行为的重要子序列。我们证明相对于传统的统一缓冲区方法，这种方法具有理论优势，并将SSET与现有的优先采样策略结合起来，进一步提高了学习速度和稳定性。在具有挑战性的MiniGrid领域，基准RL环境以及高保真度的汽车赛车模拟器中的实证结果表明了SSET相对于现有ER缓冲区采样方法的优势和多功能性。",
    "tldr": "本文提出了一种叫做SSET的经验回放采样方法，将缓冲区分为事件表，并采用现有的优先采样策略，大大提高了学习速度和稳定性。",
    "en_tdlr": "This paper proposes an experience replay sampling method called SSET, which partitions the buffer into event tables and combines with existing prioritized sampling strategy to significantly improve learning speed and stability in challenging RL environments."
}