{
    "title": "Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes. (arXiv:2211.15144v2 [cs.LG] UPDATED)",
    "abstract": "The potential of offline reinforcement learning (RL) is that high-capacity models trained on large, heterogeneous datasets can lead to agents that generalize broadly, analogously to similar advances in vision and NLP. However, recent works argue that offline RL methods encounter unique challenges to scaling up model capacity. Drawing on the learnings from these works, we re-examine previous design choices and find that with appropriate choices: ResNets, cross-entropy based distributional backups, and feature normalization, offline Q-learning algorithms exhibit strong performance that scales with model capacity. Using multi-task Atari as a testbed for scaling and generalization, we train a single policy on 40 games with near-human performance using up-to 80 million parameter networks, finding that model performance scales favorably with capacity. In contrast to prior work, we extrapolate beyond dataset performance even when trained entirely on a large (400M transitions) but highly subop",
    "link": "http://arxiv.org/abs/2211.15144",
    "context": "Title: Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes. (arXiv:2211.15144v2 [cs.LG] UPDATED)\nAbstract: The potential of offline reinforcement learning (RL) is that high-capacity models trained on large, heterogeneous datasets can lead to agents that generalize broadly, analogously to similar advances in vision and NLP. However, recent works argue that offline RL methods encounter unique challenges to scaling up model capacity. Drawing on the learnings from these works, we re-examine previous design choices and find that with appropriate choices: ResNets, cross-entropy based distributional backups, and feature normalization, offline Q-learning algorithms exhibit strong performance that scales with model capacity. Using multi-task Atari as a testbed for scaling and generalization, we train a single policy on 40 games with near-human performance using up-to 80 million parameter networks, finding that model performance scales favorably with capacity. In contrast to prior work, we extrapolate beyond dataset performance even when trained entirely on a large (400M transitions) but highly subop",
    "path": "papers/22/11/2211.15144.json",
    "total_tokens": 916,
    "translated_title": "离线强化学习在多样性多任务数据上进行，具有扩展性和推广性。",
    "translated_abstract": "离线强化学习具有很高的潜力，通过在大型异构数据集上训练高容量模型可以产生广义智能体，类似于视觉和自然语言处理领域的类似进展。然而，最近的研究表明，离线强化学习方法在扩展模型容量方面遇到了独特的挑战。本研究基于这些工作得出的结论重新审视以前的设计选择，并发现通过适当的选择，离线Q-learning算法表现出强大的性能和容量扩展。我们以多任务Atari作为扩展和推广的测试平台，使用高达8000万个参数网络在40个游戏上训练单个策略，实现接近人类水平的性能，发现模型性能与容量的关系呈正比。与以往研究不同，即使完全在大型（400M过渡）但高度难以抽象的子集上进行训练，我们也可以推广到数据集性能之外。",
    "tldr": "本文研究离线Q-learning算法在多任务Atari游戏上的表现，结果表明合适的选择能够提高扩展性和推广性，使模型性能与容量正相关，并且可以推广到数据集性能之外。",
    "en_tdlr": "This paper explores the performance of offline Q-learning algorithm on multi-task Atari games and finds that appropriate choices can improve scalability and generalization, making model performance positively correlated with capacity and able to extrapolate beyond dataset performance."
}