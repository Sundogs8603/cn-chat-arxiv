{
    "title": "On the Sample Complexity of Two-Layer Networks: Lipschitz vs. Element-Wise Lipschitz Activation. (arXiv:2211.09634v3 [cs.LG] UPDATED)",
    "abstract": "We investigate the sample complexity of bounded two-layer neural networks using different activation functions.  In particular, we consider the class  $$ \\mathcal{H} = \\left\\{\\textbf{x}\\mapsto \\langle \\textbf{v}, \\sigma \\circ W\\textbf{b} + \\textbf{b} \\rangle : \\textbf{b}\\in\\mathbb{R}^d, W \\in \\mathbb{R}^{\\mathcal{T}\\times d}, \\textbf{v} \\in \\mathbb{R}^{\\mathcal{T}}\\right\\} $$  where the spectral norm of $W$ and $\\textbf{v}$ is bounded by $O(1)$, the Frobenius norm of $W$ is bounded from its initialization by $R > 0$, and $\\sigma$ is a Lipschitz activation function.  We prove that if $\\sigma$ is element-wise, then the sample complexity of $\\mathcal{H}$ has only logarithmic dependency in width and that this complexity is tight, up to logarithmic factors.  We further show that the element-wise property of $\\sigma$ is essential for a logarithmic dependency bound in width, in the sense that there exist non-element-wise activation functions whose sample complexity is linear in width, for wid",
    "link": "http://arxiv.org/abs/2211.09634",
    "raw_ret": "{\n    \"translated_title\": \"关于两层网络的样本复杂度：Lipschitz与逐元素Lipschitz激活的比较\",\n    \"translated_abstract\": \"本文研究了使用不同激活函数的有界两层神经网络的样本复杂度。特别地，我们考虑类$$\\mathcal{H}=\\left\\{\\textbf{x}\\mapsto\\langle\\textbf{v},\\sigma\\circ W\\textbf{b}+\\textbf{b}\\rangle:\\textbf{b}\\in\\mathbb{R}^d,W\\in\\mathbb{R}^{\\mathcal{T}\\times d},\\textbf{v}\\in\\mathbb{R}^{\\mathcal{T}}\\right\\}$$其中$W$和$\\textbf{v}$的谱范数被限制在$O(1)$，$W$的Frobenius范数在初始化时被限制为$R>0$，$\\sigma$是Lipschitz激活函数。我们证明了如果$\\sigma$是逐元素的，那么$\\mathcal{H}$的样本复杂度仅在宽度上有对数依赖，并且这种复杂度是紧密的，直到对数因子。我们进一步表明，$\\sigma$的逐元素属性对于宽度的对数依赖边界是必不可少的，因为存在非逐元素激活函数，其样本复杂度在宽度上是线性的，对于wid\",\n    \"tldr\": \"本文研究了使用不同激活函数的有界两层神经网络的样本复杂度。如果激活函数是逐元素的，那么这种复杂度仅在宽度上有对数依赖，且是紧密的，直到对数因子。逐元素激活函数的属性对于宽度的对数依赖边界是必不可少的，因为存在非逐元素激活函数，其样本复杂度在宽度上是线性的。\"\n}<|im_sep|>",
    "total_tokens": 1112
}