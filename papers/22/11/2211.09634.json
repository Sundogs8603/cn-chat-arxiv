{
    "title": "On the Sample Complexity of Two-Layer Networks: Lipschitz vs. Element-Wise Lipschitz Activation. (arXiv:2211.09634v4 [cs.LG] UPDATED)",
    "abstract": "We investigate the sample complexity of bounded two-layer neural networks using different activation functions.  In particular, we consider the class  $$ \\mathcal{H} = \\left\\{\\textbf{x}\\mapsto \\langle \\textbf{v}, \\sigma \\circ W\\textbf{b} + \\textbf{b} \\rangle : \\textbf{b}\\in\\mathbb{R}^d, W \\in \\mathbb{R}^{\\mathcal{T}\\times d}, \\textbf{v} \\in \\mathbb{R}^{\\mathcal{T}}\\right\\} $$  where the spectral norm of $W$ and $\\textbf{v}$ is bounded by $O(1)$, the Frobenius norm of $W$ is bounded from its initialization by $R > 0$, and $\\sigma$ is a Lipschitz activation function.  We prove that if $\\sigma$ is element-wise, then the sample complexity of $\\mathcal{H}$ has only logarithmic dependency in width and that this complexity is tight, up to logarithmic factors.  We further show that the element-wise property of $\\sigma$ is essential for a logarithmic dependency bound in width, in the sense that there exist non-element-wise activation functions whose sample complexity is linear in width, for wid",
    "link": "http://arxiv.org/abs/2211.09634",
    "context": "Title: On the Sample Complexity of Two-Layer Networks: Lipschitz vs. Element-Wise Lipschitz Activation. (arXiv:2211.09634v4 [cs.LG] UPDATED)\nAbstract: We investigate the sample complexity of bounded two-layer neural networks using different activation functions.  In particular, we consider the class  $$ \\mathcal{H} = \\left\\{\\textbf{x}\\mapsto \\langle \\textbf{v}, \\sigma \\circ W\\textbf{b} + \\textbf{b} \\rangle : \\textbf{b}\\in\\mathbb{R}^d, W \\in \\mathbb{R}^{\\mathcal{T}\\times d}, \\textbf{v} \\in \\mathbb{R}^{\\mathcal{T}}\\right\\} $$  where the spectral norm of $W$ and $\\textbf{v}$ is bounded by $O(1)$, the Frobenius norm of $W$ is bounded from its initialization by $R > 0$, and $\\sigma$ is a Lipschitz activation function.  We prove that if $\\sigma$ is element-wise, then the sample complexity of $\\mathcal{H}$ has only logarithmic dependency in width and that this complexity is tight, up to logarithmic factors.  We further show that the element-wise property of $\\sigma$ is essential for a logarithmic dependency bound in width, in the sense that there exist non-element-wise activation functions whose sample complexity is linear in width, for wid",
    "path": "papers/22/11/2211.09634.json",
    "total_tokens": 1108,
    "translated_title": "关于两层网络样本复杂度的研究：Lipschitz与逐元素Lipschitz激活的比较",
    "translated_abstract": "我们研究了使用不同激活函数的有界两层神经网络的样本复杂度。特别地，我们考虑了类$$\\mathcal{H} = \\{\\textbf{x}\\mapsto \\langle \\textbf{v}, \\sigma \\circ W\\textbf{b} + \\textbf{b} \\rangle : \\textbf{b}\\in\\mathbb{R}^d, W \\in \\mathbb{R}^{\\mathcal{T}\\times d}, \\textbf{v} \\in \\mathbb{R}^{\\mathcal{T}}\\}$$其中$W$和$\\textbf{v}$的谱范数被$O(1)$限制，$W$的Frobenius范数从初始化开始被$R>0$限制，$\\sigma$是一个Lipschitz激活函数。我们证明了如果$\\sigma$是逐元素的，则$\\mathcal{H}$的样本复杂度仅仅在宽度上有对数依赖，并且这种复杂度是紧致的，仅有对数因子的差异。我们进一步展示了$\\sigma$的逐元素性质对于宽度上的对数依赖界限是至关重要的，也就是说存在不是逐元素激活函数的样本复杂度在宽度上是线性的，针对宽度，样本复杂度介于对数和线性之间。",
    "tldr": "本文研究了使用不同激活函数的有界两层神经网络的样本复杂度，证明了当激活函数为逐元素Lipschitz时，样本复杂度在宽度上仅有对数依赖，并且这种依赖是紧致的。",
    "en_tdlr": "This paper investigates the sample complexity of bounded two-layer neural networks using different activation functions and proves that when the activation function is element-wise Lipschitz, the sample complexity only has logarithmic dependency in width, which is tight."
}