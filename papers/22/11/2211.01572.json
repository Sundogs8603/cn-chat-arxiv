{
    "title": "FedTP: Federated Learning by Transformer Personalization. (arXiv:2211.01572v2 [cs.LG] UPDATED)",
    "abstract": "Federated learning is an emerging learning paradigm where multiple clients collaboratively train a machine learning model in a privacy-preserving manner. Personalized federated learning extends this paradigm to overcome heterogeneity across clients by learning personalized models. Recently, there have been some initial attempts to apply Transformers to federated learning. However, the impacts of federated learning algorithms on self-attention have not yet been studied. This paper investigates this relationship and reveals that federated averaging algorithms actually have a negative impact on self-attention where there is data heterogeneity. These impacts limit the capabilities of the Transformer model in federated learning settings. Based on this, we propose FedTP, a novel Transformer-based federated learning framework that learns personalized self-attention for each client while aggregating the other parameters among the clients. Instead of using a vanilla personalization mechanism th",
    "link": "http://arxiv.org/abs/2211.01572",
    "context": "Title: FedTP: Federated Learning by Transformer Personalization. (arXiv:2211.01572v2 [cs.LG] UPDATED)\nAbstract: Federated learning is an emerging learning paradigm where multiple clients collaboratively train a machine learning model in a privacy-preserving manner. Personalized federated learning extends this paradigm to overcome heterogeneity across clients by learning personalized models. Recently, there have been some initial attempts to apply Transformers to federated learning. However, the impacts of federated learning algorithms on self-attention have not yet been studied. This paper investigates this relationship and reveals that federated averaging algorithms actually have a negative impact on self-attention where there is data heterogeneity. These impacts limit the capabilities of the Transformer model in federated learning settings. Based on this, we propose FedTP, a novel Transformer-based federated learning framework that learns personalized self-attention for each client while aggregating the other parameters among the clients. Instead of using a vanilla personalization mechanism th",
    "path": "papers/22/11/2211.01572.json",
    "total_tokens": 875,
    "translated_title": "FedTP: 联邦学习中的Transformer个性化",
    "translated_abstract": "联邦学习是一种新兴的学习范式，多个客户端协作地进行隐私保护的机器学习模型训练。个性化联邦学习通过学习个性化模型，克服客户端之间的异质性。最近，已经有一些尝试将Transformer应用于联邦学习。然而，联邦学习算法对自注意力的影响尚未被研究。本文研究了这种关系，并揭示了在存在数据异质性时，联邦平均算法实际上会对自注意力产生负面影响。这些影响限制了Transformer模型在联邦学习环境中的能力。基于此，我们提出了FedTP，一种新的基于Transformer的联邦学习框架，它为每个客户端学习个性化的自注意力，同时将其他参数聚合在客户端之间而不是使用纯个性化机制。",
    "tldr": "本文研究发现联邦平均算法对Transformer模型中的自注意力存在负面影响，限制了联邦学习的能力。为此提出了FedTP，在学习客户端个性化自注意力的同时，将其他参数聚合在客户端之间。",
    "en_tdlr": "This paper finds that federated averaging algorithms have a negative impact on self-attention in Transformer models, which limits the capabilities of federated learning. Therefore, FedTP is proposed to learn personalized self-attention for each client while aggregating the other parameters among the clients."
}