{
    "title": "Generalized Balancing Weights via Deep Neural Networks. (arXiv:2211.07533v5 [stat.ML] UPDATED)",
    "abstract": "Estimating causal effects from observational data is a central problem in many domains. A general approach is to balance covariates with weights such that the distribution of the data mimics randomization. We present generalized balancing weights, Neural Balancing Weights (NBW), to estimate the causal effects of an arbitrary mixture of discrete and continuous interventions. The weights were obtained through direct estimation of the density ratio between the source and balanced distributions by optimizing the variational representation of $f$-divergence. For this, we selected $\\alpha$-divergence as it presents efficient optimization because it has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients; moreover, it is advantageous for the vanishing-gradient problem. In addition, we provide the following two methods for estimating the balancing weights: improving the generalization performance of the balancing weights and checking ",
    "link": "http://arxiv.org/abs/2211.07533",
    "context": "Title: Generalized Balancing Weights via Deep Neural Networks. (arXiv:2211.07533v5 [stat.ML] UPDATED)\nAbstract: Estimating causal effects from observational data is a central problem in many domains. A general approach is to balance covariates with weights such that the distribution of the data mimics randomization. We present generalized balancing weights, Neural Balancing Weights (NBW), to estimate the causal effects of an arbitrary mixture of discrete and continuous interventions. The weights were obtained through direct estimation of the density ratio between the source and balanced distributions by optimizing the variational representation of $f$-divergence. For this, we selected $\\alpha$-divergence as it presents efficient optimization because it has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients; moreover, it is advantageous for the vanishing-gradient problem. In addition, we provide the following two methods for estimating the balancing weights: improving the generalization performance of the balancing weights and checking ",
    "path": "papers/22/11/2211.07533.json",
    "total_tokens": 881,
    "translated_title": "基于深度神经网络的广义平衡权重",
    "translated_abstract": "从观测数据中估计因果效应是许多领域中的一个中心问题。一种广泛使用的方法是平衡协变量的权重，使得数据的分布类似于随机化。我们提出了一种称为神经平衡权重（NBW）的广义平衡权重，以估计任意混合离散和连续干预的因果效应。通过优化 $f$ -分布的变分表示，直接估计源和平衡分布之间的密度比，获得了权重。为此，我们选择了 $\\alpha$-差异作为优化的目标函数，因为它具有样本复杂度独立于其地面实况值和无偏小批量梯度的估计器，而且对于梯度消失问题具有优势。此外，我们提供了以下两种方法来估计平衡权重：提高平衡权重的泛化性能和检查其效果。",
    "tldr": "本文提出了一种广义平衡权重方法（NBW），通过优化 $f$ -分布的变分表示，直接估计源和平衡分布之间的密度比，获得了权重，用于估计任意混合离散和连续干预的因果效应。",
    "en_tdlr": "The paper proposes a generalized balancing weights method (NBW), which estimates the causal effects of arbitrary mixtures of discrete and continuous interventions by directly estimating the density ratio between the source and balanced distributions through optimizing the variational representation of f-divergence."
}