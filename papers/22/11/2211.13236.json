{
    "title": "MEGAN: Multi-Explanation Graph Attention Network. (arXiv:2211.13236v2 [cs.LG] UPDATED)",
    "abstract": "We propose a multi-explanation graph attention network (MEGAN). Unlike existing graph explainability methods, our network can produce node and edge attributional explanations along multiple channels, the number of which is independent of task specifications. This proves crucial to improve the interpretability of graph regression predictions, as explanations can be split into positive and negative evidence w.r.t to a reference value. Additionally, our attention-based network is fully differentiable and explanations can actively be trained in an explanation-supervised manner. We first validate our model on a synthetic graph regression dataset with known ground-truth explanations. Our network outperforms existing baseline explainability methods for the single- as well as the multi-explanation case, achieving near-perfect explanation accuracy during explanation supervision. Finally, we demonstrate our model's capabilities on multiple real-world datasets. We find that our model produces spa",
    "link": "http://arxiv.org/abs/2211.13236",
    "context": "Title: MEGAN: Multi-Explanation Graph Attention Network. (arXiv:2211.13236v2 [cs.LG] UPDATED)\nAbstract: We propose a multi-explanation graph attention network (MEGAN). Unlike existing graph explainability methods, our network can produce node and edge attributional explanations along multiple channels, the number of which is independent of task specifications. This proves crucial to improve the interpretability of graph regression predictions, as explanations can be split into positive and negative evidence w.r.t to a reference value. Additionally, our attention-based network is fully differentiable and explanations can actively be trained in an explanation-supervised manner. We first validate our model on a synthetic graph regression dataset with known ground-truth explanations. Our network outperforms existing baseline explainability methods for the single- as well as the multi-explanation case, achieving near-perfect explanation accuracy during explanation supervision. Finally, we demonstrate our model's capabilities on multiple real-world datasets. We find that our model produces spa",
    "path": "papers/22/11/2211.13236.json",
    "total_tokens": 830,
    "translated_title": "MEGAN: 多解释图注意力网络",
    "translated_abstract": "我们提出了一种多解释图注意力网络（MEGAN）。与现有的图可解释方法不同，我们的网络可以沿多个通道产生节点和边的说明性解释，其数量独立于任务规格说明。这对于改进图回归预测的可解释性至关重要，因为可以将解释分为相对于参考值的正面和负面证据。此外，我们的基于注意力的网络是完全可微的，可以在解释监督方式下主动地训练说明。我们首先在已知地面真相说明的合成图回归数据集上验证了我们的模型。我们的网络在单一和多解释情况下均优于现有的基准可解释方法，在解释监督期间实现了接近完美的解释准确性。最后，我们展示了我们的模型在多个真实世界数据集上的能力。",
    "tldr": "MEGAN是一种可以在多个通道中产生节点和边的说明性解释的图注意力网络，对于改进图回归预测的可解释性至关重要。此外，它是完全可微的，可以在解释监督方式下主动地训练说明。",
    "en_tdlr": "MEGAN is a graph attention network that produces node and edge attributional explanations along multiple channels, which is crucial for improving the interpretability of graph regression predictions. It is also fully differentiable and can be actively trained in an explanation-supervised manner."
}