{
    "title": "Linear RNNs Provably Learn Linear Dynamic Systems. (arXiv:2211.10582v2 [cs.LG] UPDATED)",
    "abstract": "We study the learning ability of linear recurrent neural networks with Gradient Descent. We prove the first theoretical guarantee on linear RNNs to learn any stable linear dynamic system using any a large type of loss functions. For an arbitrary stable linear system with a parameter $\\rho_C$ related to the transition matrix $C$, we show that despite the non-convexity of the parameter optimization loss if the width of the RNN is large enough (and the required width in hidden layers does not rely on the length of the input sequence), a linear RNN can provably learn any stable linear dynamic system with the sample and time complexity polynomial in $\\frac{1}{1-\\rho_C}$. Our results provide the first theoretical guarantee to learn a linear RNN and demonstrate how can the recurrent structure help to learn a dynamic system.",
    "link": "http://arxiv.org/abs/2211.10582",
    "context": "Title: Linear RNNs Provably Learn Linear Dynamic Systems. (arXiv:2211.10582v2 [cs.LG] UPDATED)\nAbstract: We study the learning ability of linear recurrent neural networks with Gradient Descent. We prove the first theoretical guarantee on linear RNNs to learn any stable linear dynamic system using any a large type of loss functions. For an arbitrary stable linear system with a parameter $\\rho_C$ related to the transition matrix $C$, we show that despite the non-convexity of the parameter optimization loss if the width of the RNN is large enough (and the required width in hidden layers does not rely on the length of the input sequence), a linear RNN can provably learn any stable linear dynamic system with the sample and time complexity polynomial in $\\frac{1}{1-\\rho_C}$. Our results provide the first theoretical guarantee to learn a linear RNN and demonstrate how can the recurrent structure help to learn a dynamic system.",
    "path": "papers/22/11/2211.10582.json",
    "total_tokens": 819,
    "translated_title": "线性循环神经网络能够证明性地学习线性动态系统",
    "translated_abstract": "我们研究了使用梯度下降法对线性循环神经网络进行学习的能力。我们提供了首个理论保证，证明了线性循环神经网络可以使用各种大范围的损失函数学习任何稳定的线性动态系统。对于任意稳定的线性系统，我们通过展示，尽管参数优化的损失函数是非凸的，但只要RNN的宽度足够大（并且隐藏层所需的宽度不依赖于输入序列的长度），则线性RNN可以证明地学习任何稳定的线性动态系统，其样本和时间复杂度多项式地与$\\frac{1}{1-\\rho_C}$相关。我们的结果提供了学习线性RNN的首个理论保证，并演示了循环结构如何帮助学习动态系统。",
    "tldr": "本文证明了线性循环神经网络可使用梯度下降法学习任意稳定的线性动态系统，为其提供了第一个理论保证，并演示了循环结构如何帮助学习动态系统。",
    "en_tdlr": "This paper proves that linear recurrent neural networks can learn any stable linear dynamic system using gradient descent, providing the first theoretical guarantee and demonstrating how the recurrent structure helps in learning dynamic systems."
}