{
    "title": "A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors. (arXiv:2211.10773v2 [cs.LG] UPDATED)",
    "abstract": "$k$-nearest neighbor classification is a popular non-parametric method because of desirable properties like automatic adaption to distributional scale changes. Unfortunately, it has thus far proved difficult to design active learning strategies for the training of local voting-based classifiers that naturally retain these desirable properties, and hence active learning strategies for $k$-nearest neighbor classification have been conspicuously missing from the literature. In this work, we introduce a simple and intuitive active learning algorithm for the training of $k$-nearest neighbor classifiers, the first in the literature which retains the concept of the $k$-nearest neighbor vote at prediction time. We provide consistency guarantees for a modified $k$-nearest neighbors classifier trained on samples acquired via our scheme, and show that when the conditional probability function $\\mathbb{P}(Y=y|X=x)$ is sufficiently smooth and the Tsybakov noise condition holds, our actively trained",
    "link": "http://arxiv.org/abs/2211.10773",
    "context": "Title: A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors. (arXiv:2211.10773v2 [cs.LG] UPDATED)\nAbstract: $k$-nearest neighbor classification is a popular non-parametric method because of desirable properties like automatic adaption to distributional scale changes. Unfortunately, it has thus far proved difficult to design active learning strategies for the training of local voting-based classifiers that naturally retain these desirable properties, and hence active learning strategies for $k$-nearest neighbor classification have been conspicuously missing from the literature. In this work, we introduce a simple and intuitive active learning algorithm for the training of $k$-nearest neighbor classifiers, the first in the literature which retains the concept of the $k$-nearest neighbor vote at prediction time. We provide consistency guarantees for a modified $k$-nearest neighbors classifier trained on samples acquired via our scheme, and show that when the conditional probability function $\\mathbb{P}(Y=y|X=x)$ is sufficiently smooth and the Tsybakov noise condition holds, our actively trained",
    "path": "papers/22/11/2211.10773.json",
    "total_tokens": 916,
    "translated_title": "一种 $k$ 最近邻的两阶段主动学习算法",
    "translated_abstract": "$k$ 最近邻分类是一种流行的非参数方法，因为它具有自动适应分布比例变化等优点。然而，迄今为止，针对自然保留这些优秀特性的本地投票分类器的训练，设计主动学习策略一直是困难的，因此 $k$ 最近邻分类的主动学习策略在文献中一直缺失。在本文中，我们介绍了一种简单直观的主动学习算法，用于训练 $k$ 最近邻分类器，这是文献中第一次保持了 $k$ 最近邻投票概念的预测时间。我们为通过我们方案获取的样本提供了一种修改的 $k$ 最近邻分类器的一致性保证，并且当条件概率函数 $\\mathbb{P}(Y=y|X=x)$ 足够平滑并且 Tsybakov 噪声条件成立时，我们的主动训练的方法表现良好。",
    "tldr": "本文提出了一种用于训练 $k$ 最近邻分类器的简单直观的主动学习算法，首次保持了 $k$ 最近邻投票概念的预测时间，并提供了一致性保证。",
    "en_tdlr": "A simple and intuitive active learning algorithm is introduced for training $k$-nearest neighbor classifiers, which is the first in the literature that retains the concept of the $k$-nearest neighbor vote at prediction time, and consistency guarantees are provided for the modified $k$-nearest neighbors classifier trained on samples acquired via the proposed scheme."
}