{
    "title": "Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification. (arXiv:2211.11158v2 [cs.CV] UPDATED)",
    "abstract": "Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into human-readable concepts. They allow people to easily understand why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and often under-perform their black box counterparts, preventing their broad adoption. We address these shortcomings and are first to show how to construct high-performance CBMs without manual specification of similar accuracy to black box models. Our approach, Language Guided Bottlenecks (LaBo), leverages a language model, GPT-3, to define a large space of possible bottlenecks. Given a problem domain, LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse information. Ultimately, GPT-3's sentential concepts can be aligned to images using C",
    "link": "http://arxiv.org/abs/2211.11158",
    "context": "Title: Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification. (arXiv:2211.11158v2 [cs.CV] UPDATED)\nAbstract: Concept Bottleneck Models (CBM) are inherently interpretable models that factor model decisions into human-readable concepts. They allow people to easily understand why a model is failing, a critical feature for high-stakes applications. CBMs require manually specified concepts and often under-perform their black box counterparts, preventing their broad adoption. We address these shortcomings and are first to show how to construct high-performance CBMs without manual specification of similar accuracy to black box models. Our approach, Language Guided Bottlenecks (LaBo), leverages a language model, GPT-3, to define a large space of possible bottlenecks. Given a problem domain, LaBo uses GPT-3 to produce factual sentences about categories to form candidate concepts. LaBo efficiently searches possible bottlenecks through a novel submodular utility that promotes the selection of discriminative and diverse information. Ultimately, GPT-3's sentential concepts can be aligned to images using C",
    "path": "papers/22/11/2211.11158.json",
    "total_tokens": 970,
    "translated_title": "瓶中语言：语言模型引导的概念瓶颈用于可解释的图像分类",
    "translated_abstract": "概念瓶颈模型是一种本质上可解释的模型，可以将模型决策分解为可读的概念，使人们能够轻松理解模型失败的原因，这对于高风险应用非常重要。概念瓶颈模型需要手动指定概念，通常表现不如黑盒子模型，限制了它们的广泛采用。我们解决了这些缺点，并首次展示了如何构建高性能的概念瓶颈模型，而不需要手动指定，其准确性类似于黑盒子模型。我们的方法“Language Guided Bottlenecks”（LaBo）利用一种语言模型GPT-3来定义可能的瓶颈组成的大型空间。在给定问题域的情况下，LaBo使用GPT-3生成有关类别的事实句子，形成候选概念。LaBo通过一种新颖的子模块效用高效搜索可能的瓶颈，促进选择有区分力和多样性的信息。最终，GPT-3的句子概念可以使用C与图像对齐。",
    "tldr": "本文提出了一种名为LaBo的语言模型引导的概念瓶颈模型，能够不需要手动指定关键概念并实现与黑盒子模型相似的性能，在图像分类中具有重要的可解释性。",
    "en_tdlr": "This paper proposes a Language Guided Bottlenecks (LaBo) model that leverages a language model to define a large space of possible bottlenecks which does not require manual specification of key concepts, achieves similar performance to black box models and provides interpretability for image classification tasks."
}