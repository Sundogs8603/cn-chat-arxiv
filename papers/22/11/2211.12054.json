{
    "title": "Visually Grounded Commonsense Knowledge Acquisition. (arXiv:2211.12054v2 [cs.CV] UPDATED)",
    "abstract": "Large-scale commonsense knowledge bases empower a broad range of AI applications, where the automatic extraction of commonsense knowledge (CKE) is a fundamental and challenging problem. CKE from text is known for suffering from the inherent sparsity and reporting bias of commonsense in text. Visual perception, on the other hand, contains rich commonsense knowledge about real-world entities, e.g., (person, can_hold, bottle), which can serve as promising sources for acquiring grounded commonsense knowledge. In this work, we present CLEVER, which formulates CKE as a distantly supervised multi-instance learning problem, where models learn to summarize commonsense relations from a bag of images about an entity pair without any human annotation on image instances. To address the problem, CLEVER leverages vision-language pre-training models for deep understanding of each image in the bag, and selects informative instances from the bag to summarize commonsense entity relations via a novel cont",
    "link": "http://arxiv.org/abs/2211.12054",
    "context": "Title: Visually Grounded Commonsense Knowledge Acquisition. (arXiv:2211.12054v2 [cs.CV] UPDATED)\nAbstract: Large-scale commonsense knowledge bases empower a broad range of AI applications, where the automatic extraction of commonsense knowledge (CKE) is a fundamental and challenging problem. CKE from text is known for suffering from the inherent sparsity and reporting bias of commonsense in text. Visual perception, on the other hand, contains rich commonsense knowledge about real-world entities, e.g., (person, can_hold, bottle), which can serve as promising sources for acquiring grounded commonsense knowledge. In this work, we present CLEVER, which formulates CKE as a distantly supervised multi-instance learning problem, where models learn to summarize commonsense relations from a bag of images about an entity pair without any human annotation on image instances. To address the problem, CLEVER leverages vision-language pre-training models for deep understanding of each image in the bag, and selects informative instances from the bag to summarize commonsense entity relations via a novel cont",
    "path": "papers/22/11/2211.12054.json",
    "total_tokens": 898,
    "translated_title": "视觉基础下的常识知识获取",
    "translated_abstract": "大规模的常识知识库为广泛的AI应用提供动力，其中自动提取常识知识（CKE）是一个关键且具有挑战性的问题。由文本提取CKE知识是已知的受限于本质的稀疏性和推理偏差。另一方面，视觉知觉包含有关真实世界实体的丰富常识知识，例如（人，可以持有，瓶子），这些知识可作为获取基于常识的知识的有希望来源。我们提出CLEVER，将CKE作为一种远距离监督多实例学习问题进行了规定，模型通过关于实体对的图像包汇总出常识关系而无需对图像实例进行人工注释。CLEVER使用了基于视觉-语言预训练模型深入理解图像包中的每个图像，并从中选择信息性实例，以通过新颖的关系汇总方式概括常识知识实体关系信息。",
    "tldr": "本文介绍了CLEVER，一种以视觉-语言预训练模型为基础的常识知识提取方法，通过包含有关实体对的图像包汇总出常识关系，避免了对图像实例进行人工注释的问题。",
    "en_tdlr": "This paper introduces CLEVER, a visually grounded approach to extract commonsense knowledge by summarizing commonsense relations from a bag of images about an entity pair, without human annotation on image instances, using vision-language pre-training models."
}