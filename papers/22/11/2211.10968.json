{
    "title": "Statistical Optimality of Divide and Conquer Kernel-based Functional Linear Regression",
    "abstract": "arXiv:2211.10968v3 Announce Type: replace  Abstract: Previous analysis of regularized functional linear regression in a reproducing kernel Hilbert space (RKHS) typically requires the target function to be contained in this kernel space. This paper studies the convergence performance of divide-and-conquer estimators in the scenario that the target function does not necessarily reside in the underlying RKHS. As a decomposition-based scalable approach, the divide-and-conquer estimators of functional linear regression can substantially reduce the algorithmic complexities in time and memory. We develop an integral operator approach to establish sharp finite sample upper bounds for prediction with divide-and-conquer estimators under various regularity conditions of explanatory variables and target function. We also prove the asymptotic optimality of the derived rates by building the mini-max lower bounds. Finally, we consider the convergence of noiseless estimators and show that the rates ca",
    "link": "https://arxiv.org/abs/2211.10968",
    "context": "Title: Statistical Optimality of Divide and Conquer Kernel-based Functional Linear Regression\nAbstract: arXiv:2211.10968v3 Announce Type: replace  Abstract: Previous analysis of regularized functional linear regression in a reproducing kernel Hilbert space (RKHS) typically requires the target function to be contained in this kernel space. This paper studies the convergence performance of divide-and-conquer estimators in the scenario that the target function does not necessarily reside in the underlying RKHS. As a decomposition-based scalable approach, the divide-and-conquer estimators of functional linear regression can substantially reduce the algorithmic complexities in time and memory. We develop an integral operator approach to establish sharp finite sample upper bounds for prediction with divide-and-conquer estimators under various regularity conditions of explanatory variables and target function. We also prove the asymptotic optimality of the derived rates by building the mini-max lower bounds. Finally, we consider the convergence of noiseless estimators and show that the rates ca",
    "path": "papers/22/11/2211.10968.json",
    "total_tokens": 770,
    "translated_title": "分治核函数的功能线性回归的统计优化性",
    "translated_abstract": "先前对于正则化的核函数空间中的功能线性回归的分析通常要求目标函数包含在这个核空间中。本文研究了在目标函数不一定驻留在基本RKHS中的情况下，分治估计器的收敛性能。作为一种基于分解的可扩展方法，功能线性回归的分治估计器可以大幅减少时间和内存中的算法复杂性。我们采用积分算子方法建立了针对分治估计器在解释变量和目标函数的各种正则性条件下的预测的尖锐有限样本上界。通过建立最小-最大下界，我们还证明了导出率的渐近最优性。最后，我们考虑了无噪声估计器的收敛性，并展示了这些率能够",
    "tldr": "研究了在目标函数不一定包含在核空间中的情况下，分治核函数的功能线性回归算法的统计优化性。",
    "en_tdlr": "Investigated the statistical optimality of divide-and-conquer kernel-based functional linear regression when the target function may not necessarily reside in the kernel space."
}