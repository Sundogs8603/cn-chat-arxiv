{
    "title": "Gamma-convergence of a nonlocal perimeter arising in adversarial machine learning. (arXiv:2211.15223v4 [math.AP] UPDATED)",
    "abstract": "In this paper we prove Gamma-convergence of a nonlocal perimeter of Minkowski type to a local anisotropic perimeter. The nonlocal model describes the regularizing effect of adversarial training in binary classifications. The energy essentially depends on the interaction between two distributions modelling likelihoods for the associated classes. We overcome typical strict regularity assumptions for the distributions by only assuming that they have bounded $BV$ densities. In the natural topology coming from compactness, we prove Gamma-convergence to a weighted perimeter with weight determined by an anisotropic function of the two densities. Despite being local, this sharp interface limit reflects classification stability with respect to adversarial perturbations. We further apply our results to deduce Gamma-convergence of the associated total variations, to study the asymptotics of adversarial training, and to prove Gamma-convergence of graph discretizations for the nonlocal perimeter.",
    "link": "http://arxiv.org/abs/2211.15223",
    "context": "Title: Gamma-convergence of a nonlocal perimeter arising in adversarial machine learning. (arXiv:2211.15223v4 [math.AP] UPDATED)\nAbstract: In this paper we prove Gamma-convergence of a nonlocal perimeter of Minkowski type to a local anisotropic perimeter. The nonlocal model describes the regularizing effect of adversarial training in binary classifications. The energy essentially depends on the interaction between two distributions modelling likelihoods for the associated classes. We overcome typical strict regularity assumptions for the distributions by only assuming that they have bounded $BV$ densities. In the natural topology coming from compactness, we prove Gamma-convergence to a weighted perimeter with weight determined by an anisotropic function of the two densities. Despite being local, this sharp interface limit reflects classification stability with respect to adversarial perturbations. We further apply our results to deduce Gamma-convergence of the associated total variations, to study the asymptotics of adversarial training, and to prove Gamma-convergence of graph discretizations for the nonlocal perimeter.",
    "path": "papers/22/11/2211.15223.json",
    "total_tokens": 931,
    "translated_title": "一种在对抗机器学习中出现的非局部周长的伽玛收敛性",
    "translated_abstract": "本文证明了一种Minkowski类型的非局部周长对于一个局部各向异性周长的伽玛收敛。这种非局部模型描述了对抗性训练在二元分类中的正则化效果。能量的实质依赖于两个分布之间的相互作用，这两个分布模拟了关联类别的可能性。我们通过假设它们具有有界的BV密度，克服了对分布的典型严格正则性假设。在紧致度量空间自然拓扑上，我们证明了它与两个密度的各向异性函数确定的加权周长的伽玛收敛。尽管是局部的，这个锐利界面极限反映了对抗性扰动下的分类稳定性。我们进一步应用我们的结果推导出了关联总变差的伽玛收敛，研究了对抗性训练的渐近行为，并证明了非局部周长的图离散的伽玛收敛。",
    "tldr": "本文研究了在对抗机器学习中出现的非局部周长的伽玛收敛性问题，证明了其与局部各向异性周长的伽玛收敛。该工作对于理解对抗性训练在二元分类中的正则化效果以及相关优化问题具有重要意义。"
}