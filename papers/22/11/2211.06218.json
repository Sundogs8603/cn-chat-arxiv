{
    "title": "Total Variation Graph Neural Networks. (arXiv:2211.06218v2 [cs.LG] UPDATED)",
    "abstract": "Recently proposed Graph Neural Networks (GNNs) for vertex clustering are trained with an unsupervised minimum cut objective, approximated by a Spectral Clustering (SC) relaxation. However, the SC relaxation is loose and, while it offers a closed-form solution, it also yields overly smooth cluster assignments that poorly separate the vertices. In this paper, we propose a GNN model that computes cluster assignments by optimizing a tighter relaxation of the minimum cut based on graph total variation (GTV). The cluster assignments can be used directly to perform vertex clustering or to implement graph pooling in a graph classification framework. Our model consists of two core components: i) a message-passing layer that minimizes the $\\ell_1$ distance in the features of adjacent vertices, which is key to achieving sharp transitions between clusters; ii) an unsupervised loss function that minimizes the GTV of the cluster assignments while ensuring balanced partitions. Experimental results sh",
    "link": "http://arxiv.org/abs/2211.06218",
    "context": "Title: Total Variation Graph Neural Networks. (arXiv:2211.06218v2 [cs.LG] UPDATED)\nAbstract: Recently proposed Graph Neural Networks (GNNs) for vertex clustering are trained with an unsupervised minimum cut objective, approximated by a Spectral Clustering (SC) relaxation. However, the SC relaxation is loose and, while it offers a closed-form solution, it also yields overly smooth cluster assignments that poorly separate the vertices. In this paper, we propose a GNN model that computes cluster assignments by optimizing a tighter relaxation of the minimum cut based on graph total variation (GTV). The cluster assignments can be used directly to perform vertex clustering or to implement graph pooling in a graph classification framework. Our model consists of two core components: i) a message-passing layer that minimizes the $\\ell_1$ distance in the features of adjacent vertices, which is key to achieving sharp transitions between clusters; ii) an unsupervised loss function that minimizes the GTV of the cluster assignments while ensuring balanced partitions. Experimental results sh",
    "path": "papers/22/11/2211.06218.json",
    "total_tokens": 936,
    "translated_title": "总变分图神经网络",
    "translated_abstract": "最近提出的基于图的神经网络(GNN)用于顶点聚类是通过一个无监督的最小割目标进行训练的，这个目标是通过谱聚类(SC)松弛来近似的。然而，SC松弛是宽松的，虽然它提供了一个闭式解，但它也产生了过于平滑的集群分配，无法很好地将顶点分隔开。在本文中，我们提出了一个GNN模型，通过优化基于图总变差(GTV)的最小割的一个更紧密的松弛来计算集群分配。这些集群分配可以直接用于执行顶点聚类或在图分类框架中实现图池化。我们的模型包含两个核心组件：i)一层消息传递，该层最小化相邻顶点之间特征的$\\ell_1$距离，这对实现集群之间的锐利转换至关重要；ii)一种无监督的损失函数，最小化了集群分配的GTV，同时确保了平衡的分区。",
    "tldr": "本论文提出一种新型的GNN模型，通过基于图总变差的松弛来计算集群分配，并通过最小化相邻顶点之间特征的$\\ell_1$距离来实现锐利转换，实现顶点聚类和图池化。",
    "en_tdlr": "This paper proposes a new model of Graph Neural Networks that computes cluster assignments by optimizing a tighter relaxation of the minimum cut based on graph total variation (GTV), achieving sharp transitions between clusters by minimizing the $\\ell_1$ distance between adjacent vertex features, and allowing for vertex clustering and graph pooling in a graph classification framework."
}