{
    "title": "Fine-Tuning Language Models via Epistemic Neural Networks. (arXiv:2211.01568v2 [cs.CL] UPDATED)",
    "abstract": "Language models often pre-train on large unsupervised text corpora, then fine-tune on additional task-specific data. However, typical fine-tuning schemes do not prioritize the examples that they tune on. We show that, if you can prioritize informative training data, you can achieve better performance while using fewer labels. To do this we augment a language model with an epinet: a small additional network that helps to estimate model uncertainty and forms an \\textit{epistemic neural network} (ENN). ENNs are neural networks that can know what they don't know. Using an epinet to prioritize uncertain data, we can fine-tune BERT on GLUE tasks to the same performance while using 2x less data than training without prioritization. We also investigate performance in synthetic neural network generative models designed to build understanding. In each setting, using an epinet outperforms heuristic active learning schemes.",
    "link": "http://arxiv.org/abs/2211.01568",
    "context": "Title: Fine-Tuning Language Models via Epistemic Neural Networks. (arXiv:2211.01568v2 [cs.CL] UPDATED)\nAbstract: Language models often pre-train on large unsupervised text corpora, then fine-tune on additional task-specific data. However, typical fine-tuning schemes do not prioritize the examples that they tune on. We show that, if you can prioritize informative training data, you can achieve better performance while using fewer labels. To do this we augment a language model with an epinet: a small additional network that helps to estimate model uncertainty and forms an \\textit{epistemic neural network} (ENN). ENNs are neural networks that can know what they don't know. Using an epinet to prioritize uncertain data, we can fine-tune BERT on GLUE tasks to the same performance while using 2x less data than training without prioritization. We also investigate performance in synthetic neural network generative models designed to build understanding. In each setting, using an epinet outperforms heuristic active learning schemes.",
    "path": "papers/22/11/2211.01568.json",
    "total_tokens": 934,
    "translated_title": "通过启示神经网络微调语言模型",
    "translated_abstract": "语言模型通常会在大规模的无监督文本语料库上进行预训练，然后在特定任务的数据上进行微调。然而，通常的微调方法并不重视所微调的示例。我们展示了如果你能够将有信息价值的训练数据放在优先考虑的位置上，就可以在使用更少标签的情况下获得更好的性能表现。我们将语言模型增广了一个 epinet，这是一个辅助估算模型不确定性并形成一个启示神经网络（ENN）的小型额外网络。ENN是能够知道自己的不足的神经网络。通过使用一个 epinet 来优先考虑不确定数据，我们可以将 BERT 对 GLUE 任务的微调性能提高到与不进行优先考虑训练相同的性能，同时使用的数据标签数量减半。我们还研究了合成神经网络生成模型的表现。在每种情况下，使用 epinet 都优于启发式主动学习方案。",
    "tldr": "本文介绍了一种使用启示神经网络（ENN）来优先考虑有信息价值数据进行微调语言模型的方法，这种方法可以使用更少的标签数据实现更好的性能表现，同时在各种类型的神经网络模型中使用 ENN 都比常规的启发式主动学习方案表现更优。",
    "en_tdlr": "This paper presents a method of fine-tuning language models by prioritizing informative training data using epistemic neural networks (ENNs), which achieve better performance with fewer labels. The use of an epinet to prioritize uncertain data can improve fine-tuning on GLUE tasks and synthetic neural network generative models, and outperforms heuristic active learning schemes in each setting."
}