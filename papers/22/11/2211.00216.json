{
    "title": "Distributed Graph Neural Network Training: A Survey. (arXiv:2211.00216v2 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks (GNNs) are a type of deep learning models that are trained on graphs and have been successfully applied in various domains. Despite the effectiveness of GNNs, it is still challenging for GNNs to efficiently scale to large graphs. As a remedy, distributed computing becomes a promising solution of training large-scale GNNs, since it is able to provide abundant computing resources. However, the dependency of graph structure increases the difficulty of achieving high-efficiency distributed GNN training, which suffers from the massive communication and workload imbalance. In recent years, many efforts have been made on distributed GNN training, and an array of training algorithms and systems have been proposed. Yet, there is a lack of systematic review on the optimization techniques for the distributed execution of GNN training. In this survey, we analyze three major challenges in distributed GNN training that are massive feature communication, the loss of model accura",
    "link": "http://arxiv.org/abs/2211.00216",
    "context": "Title: Distributed Graph Neural Network Training: A Survey. (arXiv:2211.00216v2 [cs.LG] UPDATED)\nAbstract: Graph neural networks (GNNs) are a type of deep learning models that are trained on graphs and have been successfully applied in various domains. Despite the effectiveness of GNNs, it is still challenging for GNNs to efficiently scale to large graphs. As a remedy, distributed computing becomes a promising solution of training large-scale GNNs, since it is able to provide abundant computing resources. However, the dependency of graph structure increases the difficulty of achieving high-efficiency distributed GNN training, which suffers from the massive communication and workload imbalance. In recent years, many efforts have been made on distributed GNN training, and an array of training algorithms and systems have been proposed. Yet, there is a lack of systematic review on the optimization techniques for the distributed execution of GNN training. In this survey, we analyze three major challenges in distributed GNN training that are massive feature communication, the loss of model accura",
    "path": "papers/22/11/2211.00216.json",
    "total_tokens": 841,
    "translated_title": "分布式图神经网络训练：一项调查",
    "translated_abstract": "图神经网络（GNNs）是一种在图上进行训练的深度学习模型，在多个领域取得了成功应用。尽管GNNs的有效性，但是将其扩展到大规模图依然具有挑战性。分布式计算成为训练大规模GNNs的有希望的解决方案，因为它能提供丰富的计算资源。然而，图结构的依赖性使得实现高效的分布式GNN训练变得困难，存在大量的通信和负载不平衡。近年来，人们对分布式GNN训练进行了许多努力，并提出了一系列训练算法和系统。然而，在分布式执行GNN训练的优化技术方面缺乏系统性的综述。本调查分析了分布式GNN训练中的三个主要挑战：大规模特征通信、模型精度损失和分布式同步。",
    "tldr": "这项调查研究了分布式图神经网络训练中的挑战，并提出了解决方案来优化特征通信、模型精度和分布式同步。"
}