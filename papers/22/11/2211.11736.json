{
    "title": "Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models. (arXiv:2211.11736v3 [cs.RO] UPDATED)",
    "abstract": "In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration",
    "link": "http://arxiv.org/abs/2211.11736",
    "context": "Title: Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models. (arXiv:2211.11736v3 [cs.RO] UPDATED)\nAbstract: In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration",
    "path": "papers/22/11/2211.11736.json",
    "total_tokens": 861,
    "translated_title": "通过视觉-语言模型的指令增强实现机器人技能获取",
    "translated_abstract": "最近几年，在学习遵循自然语言指令的机器人操作策略方面取得了很大进展。这些方法通常从机器人-语言数据语料库中学习，该数据要么是为特定任务而收集的，要么是在事后由人工昂贵地重新标注的，带有丰富的语言描述。最近，大规模预训练的视觉-语言模型（VLMs）如CLIP或ViLD已应用于机器人学习表示和场景描述。这些预训练模型能否作为机器人数据的自动标注工具，将互联网规模的知识导入现有数据集，使其对于未在其地面真实注释中反映的任务也能发挥作用？为了实现这一目标，我们引入了数据驱动的指令增强（DIAL）来用于基于语言的控制：我们利用CLIP的语义理解，利用半监督的语言标签将知识传递到大规模无标签演示数据集上。",
    "tldr": "本文提出了一种通过视觉-语言模型的指令增强方法，利用预训练模型将互联网规模的知识导入现有机器人数据集，实现机器人技能的获取。",
    "en_tdlr": "This paper introduces an approach to robotic skill acquisition by augmenting instructions with vision-language models, utilizing pretrained models to import Internet-scale knowledge into existing robot datasets."
}