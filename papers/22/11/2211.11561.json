{
    "title": "SAMSON: Sharpness-Aware Minimization Scaled by Outlier Normalization for Improving DNN Generalization and Robustness. (arXiv:2211.11561v2 [cs.LG] UPDATED)",
    "abstract": "Energy-efficient deep neural network (DNN) accelerators are prone to non-idealities that degrade DNN performance at inference time. To mitigate such degradation, existing methods typically add perturbations to the DNN weights during training to simulate inference on noisy hardware. However, this often requires knowledge about the target hardware and leads to a trade-off between DNN performance and robustness, decreasing the former to increase the latter. In this work, we show that applying sharpness-aware training, by optimizing for both the loss value and loss sharpness, significantly improves robustness to noisy hardware at inference time without relying on any assumptions about the target hardware. In particular, we propose a new adaptive sharpness-aware method that conditions the worst-case perturbation of a given weight not only on its magnitude but also on the range of the weight distribution. This is achieved by performing sharpness-aware minimization scaled by outlier minimizat",
    "link": "http://arxiv.org/abs/2211.11561",
    "context": "Title: SAMSON: Sharpness-Aware Minimization Scaled by Outlier Normalization for Improving DNN Generalization and Robustness. (arXiv:2211.11561v2 [cs.LG] UPDATED)\nAbstract: Energy-efficient deep neural network (DNN) accelerators are prone to non-idealities that degrade DNN performance at inference time. To mitigate such degradation, existing methods typically add perturbations to the DNN weights during training to simulate inference on noisy hardware. However, this often requires knowledge about the target hardware and leads to a trade-off between DNN performance and robustness, decreasing the former to increase the latter. In this work, we show that applying sharpness-aware training, by optimizing for both the loss value and loss sharpness, significantly improves robustness to noisy hardware at inference time without relying on any assumptions about the target hardware. In particular, we propose a new adaptive sharpness-aware method that conditions the worst-case perturbation of a given weight not only on its magnitude but also on the range of the weight distribution. This is achieved by performing sharpness-aware minimization scaled by outlier minimizat",
    "path": "papers/22/11/2211.11561.json",
    "total_tokens": 963,
    "translated_title": "SAMSON：针对深度神经网络泛化和鲁棒性问题的异常归一化尺度下锐度感知最小化方法",
    "translated_abstract": "能效较高的深度神经网络（DNN）加速器容易出现非理想情况，从而降低DNN的推断性能。为了缓解这种情况，现有的方法通常在训练期间向DNN权重添加扰动，以模拟噪声硬件上的推断过程。然而，这通常需要关于目标硬件的知识，并且会导致在DNN性能和鲁棒性之间进行权衡，降低前者以提高后者。在本文中，我们展示了应用锐度感知训练，在优化损失值和损失锐度的同时，可以显著提高对噪声硬件的推断鲁棒性，而无需依赖于有关目标硬件的任何假设。特别地，我们提出了一种新的自适应锐度感知方法，它不仅将给定权重的最坏情况扰动取决于其大小，而且还取决于权重分布的范围。这是通过执行在异常值归一化尺度下的锐度感知最小化来实现的。",
    "tldr": "通过应用锐度感知训练并将其优化为损失值和损失锐度，SAMSON方法显著提高了对噪声硬件的推断鲁棒性，而无需关于目标硬件的任何假设。",
    "en_tdlr": "The SAMSON method improves the robustness of deep neural networks to noisy hardware during inference by applying sharpness-aware training and optimizing for loss value and loss sharpness without relying on any assumptions about target hardware."
}