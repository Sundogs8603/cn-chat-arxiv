{
    "title": "Behavior Estimation from Multi-Source Data for Offline Reinforcement Learning. (arXiv:2211.16078v2 [cs.LG] UPDATED)",
    "abstract": "Offline reinforcement learning (RL) have received rising interest due to its appealing data efficiency. The present study addresses behavior estimation, a task that lays the foundation of many offline RL algorithms. Behavior estimation aims at estimating the policy with which training data are generated. In particular, this work considers a scenario where the data are collected from multiple sources. In this case, neglecting data heterogeneity, existing approaches for behavior estimation suffers from behavior misspecification. To overcome this drawback, the present study proposes a latent variable model to infer a set of policies from data, which allows an agent to use as behavior policy the policy that best describes a particular trajectory. This model provides with a agent fine-grained characterization for multi-source data and helps it overcome behavior misspecification. This work also proposes a learning algorithm for this model and illustrates its practical usage via extending an ",
    "link": "http://arxiv.org/abs/2211.16078",
    "context": "Title: Behavior Estimation from Multi-Source Data for Offline Reinforcement Learning. (arXiv:2211.16078v2 [cs.LG] UPDATED)\nAbstract: Offline reinforcement learning (RL) have received rising interest due to its appealing data efficiency. The present study addresses behavior estimation, a task that lays the foundation of many offline RL algorithms. Behavior estimation aims at estimating the policy with which training data are generated. In particular, this work considers a scenario where the data are collected from multiple sources. In this case, neglecting data heterogeneity, existing approaches for behavior estimation suffers from behavior misspecification. To overcome this drawback, the present study proposes a latent variable model to infer a set of policies from data, which allows an agent to use as behavior policy the policy that best describes a particular trajectory. This model provides with a agent fine-grained characterization for multi-source data and helps it overcome behavior misspecification. This work also proposes a learning algorithm for this model and illustrates its practical usage via extending an ",
    "path": "papers/22/11/2211.16078.json",
    "total_tokens": 940,
    "translated_title": "多源数据用于离线强化学习的行为估计",
    "translated_abstract": "离线强化学习由于数据效率高而备受关注。本文关注于行为估计，这是许多离线强化学习算法的基础任务。行为估计的目标是估计生成训练数据的策略。本文考虑了从多个来源收集数据的场景。在这种情况下，忽略数据异构性，现有的行为估计方法会出现行为错误。为了克服这一缺陷，本文提出了一个潜变量模型来从数据中推断一组策略，使代理能够使用最能描述特定轨迹的策略作为行为策略。该模型为多源数据提供了代理的精细化描述，并帮助它克服行为错误。本文还为该模型提出了一个学习算法，并通过扩展现有的离线强化学习算法Soft Actor-Critic（SAC）来处理多源数据，说明了它的实际用途。实验结果表明，与现有最先进的替代方法相比，所提出的方法是有效的。",
    "tldr": "本文提出了一种用于离线强化学习的多源数据行为估计方法，通过潜变量模型推断策略来克服数据异构性导致的行为错误。",
    "en_tdlr": "This paper proposes a multi-source data behavior estimation method for offline reinforcement learning, which overcomes behavior misspecification due to data heterogeneity by inferring policies through a latent variable model. The proposed method extends the Soft Actor-Critic algorithm to handle multi-source data and achieves improved performance compared to state-of-the-art."
}