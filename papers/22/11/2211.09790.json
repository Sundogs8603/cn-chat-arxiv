{
    "title": "ConStruct-VL: Data-Free Continual Structured VL Concepts Learning. (arXiv:2211.09790v2 [cs.LG] UPDATED)",
    "abstract": "Recently, large-scale pre-trained Vision-and-Language (VL) foundation models have demonstrated remarkable capabilities in many zero-shot downstream tasks, achieving competitive results for recognizing objects defined by as little as short text prompts. However, it has also been shown that VL models are still brittle in Structured VL Concept (SVLC) reasoning, such as the ability to recognize object attributes, states, and inter-object relations. This leads to reasoning mistakes, which need to be corrected as they occur by teaching VL models the missing SVLC skills; often this must be done using private data where the issue was found, which naturally leads to a data-free continual (no task-id) VL learning setting. In this work, we introduce the first Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it is challenging for many existing data-free CL strategies. We, therefore, propose a data-free method comprised of a new approach of Adversarial Pseudo-Re",
    "link": "http://arxiv.org/abs/2211.09790",
    "context": "Title: ConStruct-VL: Data-Free Continual Structured VL Concepts Learning. (arXiv:2211.09790v2 [cs.LG] UPDATED)\nAbstract: Recently, large-scale pre-trained Vision-and-Language (VL) foundation models have demonstrated remarkable capabilities in many zero-shot downstream tasks, achieving competitive results for recognizing objects defined by as little as short text prompts. However, it has also been shown that VL models are still brittle in Structured VL Concept (SVLC) reasoning, such as the ability to recognize object attributes, states, and inter-object relations. This leads to reasoning mistakes, which need to be corrected as they occur by teaching VL models the missing SVLC skills; often this must be done using private data where the issue was found, which naturally leads to a data-free continual (no task-id) VL learning setting. In this work, we introduce the first Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it is challenging for many existing data-free CL strategies. We, therefore, propose a data-free method comprised of a new approach of Adversarial Pseudo-Re",
    "path": "papers/22/11/2211.09790.json",
    "total_tokens": 887,
    "translated_title": "ConStruct-VL: 无需数据的持续结构化视觉语言概念学习",
    "translated_abstract": "最近，大规模预先训练的视觉语言基础模型在许多零样本下游任务中展示出了非凡的能力，能够通过仅包含短文本提示的定义来识别物体，并取得了竞争性的结果。然而，也已经表明，VL模型在结构化VL概念推理方面仍然很脆弱，例如识别物体属性、状态和物体间关系的能力。这导致推理错误，需要通过教授VL模型缺失的SVLC技能来进行更正；通常必须使用发现问题的私有数据来完成这一点，这自然而然地导致了一个无需任务ID的无数据持续VL学习设置。在这项工作中，我们介绍了第一个持续的无数据结构化VL概念学习（ConStruct-VL）基准，并表明它对许多现有的无数据CL策略都很具有挑战性。",
    "tldr": "该论文介绍了第一个持续的无数据结构化VL概念学习（ConStruct-VL）基准，旨在解决VL模型在结构化VL概念推理方面的瓶颈问题，并提出了一种数据-free的方法。",
    "en_tdlr": "This paper presents the first benchmark for continual data-free structured VL concepts learning, which addresses the bottleneck issue of VL models in structured VL concept reasoning, and proposes a data-free method to solve it."
}