{
    "title": "Learning Heterogeneous Agent Cooperation via Multiagent League Training. (arXiv:2211.11616v2 [cs.LG] UPDATED)",
    "abstract": "Many multiagent systems in the real world include multiple types of agents with different abilities and functionality. Such heterogeneous multiagent systems have significant practical advantages. However, they also come with challenges compared with homogeneous systems for multiagent reinforcement learning, such as the non-stationary problem and the policy version iteration issue. This work proposes a general-purpose reinforcement learning algorithm named Heterogeneous League Training (HLT) to address heterogeneous multiagent problems. HLT keeps track of a pool of policies that agents have explored during training, gathering a league of heterogeneous policies to facilitate future policy optimization. Moreover, a hyper-network is introduced to increase the diversity of agent behaviors when collaborating with teammates having different levels of cooperation skills. We use heterogeneous benchmark tasks to demonstrate that (1) HLT promotes the success rate in cooperative heterogeneous task",
    "link": "http://arxiv.org/abs/2211.11616",
    "context": "Title: Learning Heterogeneous Agent Cooperation via Multiagent League Training. (arXiv:2211.11616v2 [cs.LG] UPDATED)\nAbstract: Many multiagent systems in the real world include multiple types of agents with different abilities and functionality. Such heterogeneous multiagent systems have significant practical advantages. However, they also come with challenges compared with homogeneous systems for multiagent reinforcement learning, such as the non-stationary problem and the policy version iteration issue. This work proposes a general-purpose reinforcement learning algorithm named Heterogeneous League Training (HLT) to address heterogeneous multiagent problems. HLT keeps track of a pool of policies that agents have explored during training, gathering a league of heterogeneous policies to facilitate future policy optimization. Moreover, a hyper-network is introduced to increase the diversity of agent behaviors when collaborating with teammates having different levels of cooperation skills. We use heterogeneous benchmark tasks to demonstrate that (1) HLT promotes the success rate in cooperative heterogeneous task",
    "path": "papers/22/11/2211.11616.json",
    "total_tokens": 868,
    "translated_title": "通过多智能体联赛训练实现异质智能体的协作学习",
    "translated_abstract": "现实世界中的许多多智能体系统包括多种能力和功能不同的智能体。这样的异质多智能体系统具有重要的实用优势。然而，与同质系统相比，它们也带来了多智能体强化学习面临的一些挑战，如非稳态问题和策略版本迭代问题。本文提出了一种名为异质联赛训练（HLT）的通用强化学习算法，以解决异质多智能体问题。HLT跟踪代理人在训练期间探索的一组策略，收集异质策略联盟以促进未来的策略优化。此外，引入了超网络，以增加代理人的行为多样性，从而在与具有不同级别的合作技能的队友合作时进行协作。我们使用异质基准任务来证明HLT能够提高协作异质任务的成功率。",
    "tldr": "本文提出了一种名为异质联赛训练（HLT）的通用强化学习算法，为解决异质多智能体问题，使用策略池和超网络，提高了异质智能体的合作效率。",
    "en_tdlr": "This paper proposes a general-purpose reinforcement learning algorithm named Heterogeneous League Training (HLT) to address heterogeneous multiagent problems by tracking a pool of policies and introducing a hyper-network to improve cooperation efficiency of heterogeneous agents."
}