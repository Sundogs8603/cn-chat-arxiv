{
    "title": "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning. (arXiv:2211.11275v2 [eess.AS] UPDATED)",
    "abstract": "Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this paper, we propose a unified cross-modal representation learning framework VATLM (Visual-Audio-Text Language Model). The proposed VATLM employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, VATLM is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on audio-vis",
    "link": "http://arxiv.org/abs/2211.11275",
    "context": "Title: VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning. (arXiv:2211.11275v2 [eess.AS] UPDATED)\nAbstract: Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this paper, we propose a unified cross-modal representation learning framework VATLM (Visual-Audio-Text Language Model). The proposed VATLM employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, VATLM is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on audio-vis",
    "path": "papers/22/11/2211.11275.json",
    "total_tokens": 955,
    "translated_title": "VATLM: 使用统一的遮蔽预测进行视听文本预训练的语音表示学习",
    "translated_abstract": "虽然语音是人类与外界交流的一种简单而有效的方式，但更真实的语音交互包含多模式信息，例如视觉、文本。如何设计一个统一的框架来整合不同的模态信息，利用不同的资源（例如视听对、音频文本对、未标记的语音和未标记的文本）促进语音表示学习还没有被很好地探索。本文提出了一个统一跨模式表示学习框架VATLM（Visual-Audio-Text语言模型）。所提出的VATLM采用统一的骨干网络来建模模态独立信息，并利用三个简单的模态依赖模块对视觉、语音和文本输入进行预处理。为了将这三种模态集成到一个共享语义空间中，VATLM使用我们所提出的统一分词器给出的统一令牌的遮蔽预测任务进行优化。我们在音频-视觉检索和口语理解任务上评估了预训练的VATLM，并证明了它在学习联合多模态表示方面的有效性。",
    "tldr": "本文提出了一个名为VATLM的统一跨模式表示学习框架，利用视听文本资料的预处理与一种统一的遮蔽预测任务进行优化，以达到优秀的联合多模态表示效果。",
    "en_tdlr": "This paper proposes a unified cross-modal representation learning framework called VATLM, which utilizes the pre-processing of visual-audio-text data and optimizes a masked prediction task using a unified tokenizer to achieve excellent joint multimodal representation learning."
}