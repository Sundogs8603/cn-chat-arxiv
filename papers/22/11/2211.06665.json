{
    "title": "A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges. (arXiv:2211.06665v4 [cs.LG] UPDATED)",
    "abstract": "Reinforcement Learning (RL) is a popular machine learning paradigm where intelligent agents interact with the environment to fulfill a long-term goal. Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great success over a wide spectrum of complex control tasks. Despite the encouraging results achieved, the deep neural network-based backbone is widely deemed as a black box that impedes practitioners to trust and employ trained agents in realistic scenarios where high security and reliability are essential. To alleviate this issue, a large volume of literature devoted to shedding light on the inner workings of the intelligent agents has been proposed, by constructing intrinsic interpretability or post-hoc explainability. In this survey, we provide a comprehensive review of existing works on eXplainable RL (XRL) and introduce a new taxonomy where prior works are clearly categorized into model-explaining, reward-explaining, state-explaining, and task-explaining methods",
    "link": "http://arxiv.org/abs/2211.06665",
    "context": "Title: A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges. (arXiv:2211.06665v4 [cs.LG] UPDATED)\nAbstract: Reinforcement Learning (RL) is a popular machine learning paradigm where intelligent agents interact with the environment to fulfill a long-term goal. Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great success over a wide spectrum of complex control tasks. Despite the encouraging results achieved, the deep neural network-based backbone is widely deemed as a black box that impedes practitioners to trust and employ trained agents in realistic scenarios where high security and reliability are essential. To alleviate this issue, a large volume of literature devoted to shedding light on the inner workings of the intelligent agents has been proposed, by constructing intrinsic interpretability or post-hoc explainability. In this survey, we provide a comprehensive review of existing works on eXplainable RL (XRL) and introduce a new taxonomy where prior works are clearly categorized into model-explaining, reward-explaining, state-explaining, and task-explaining methods",
    "path": "papers/22/11/2211.06665.json",
    "total_tokens": 897,
    "translated_title": "关于可解释性强化学习的综述：概念、算法和挑战",
    "translated_abstract": "强化学习是一种流行的机器学习范式，智能代理与环境进行交互以实现长期目标。在深度学习的复兴推动下，深度强化学习在各种复杂控制任务中取得了巨大成功。尽管取得了令人鼓舞的结果，基于深度神经网络的主干结构被普遍视为黑盒子，阻碍了从业者在安全性和可靠性至关重要的真实场景中信任和使用训练代理。为了缓解这个问题，大量的文献致力于揭示智能代理的内部工作原理，通过构建内在可解释性或事后可解释性。在本综述中，我们对现有的可解释性强化学习方法进行了全面的回顾，并引入了一个新的分类法，将先前的工作明确地分为模型解释、奖励解释、状态解释和任务解释方法。",
    "tldr": "该综述调查了可解释性强化学习方法，介绍了模型解释、奖励解释、状态解释和任务解释方法，并探讨了解释强化学习的概念、算法和挑战。",
    "en_tdlr": "This survey reviews the methods of explainable reinforcement learning, introduces model-explaining, reward-explaining, state-explaining, and task-explaining methods, and explores the concepts, algorithms, and challenges of explainable RL."
}