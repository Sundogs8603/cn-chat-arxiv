{
    "title": "SimVP: Towards Simple yet Powerful Spatiotemporal Predictive Learning. (arXiv:2211.12509v3 [cs.LG] UPDATED)",
    "abstract": "Recent years have witnessed remarkable advances in spatiotemporal predictive learning, incorporating auxiliary inputs, elaborate neural architectures, and sophisticated training strategies. Although impressive, the system complexity of mainstream methods is increasing as well, which may hinder the convenient applications. This paper proposes SimVP, a simple spatiotemporal predictive baseline model that is completely built upon convolutional networks without recurrent architectures and trained by common mean squared error loss in an end-to-end fashion. Without introducing any extra tricks and strategies, SimVP can achieve superior performance on various benchmark datasets. To further improve the performance, we derive variants with the gated spatiotemporal attention translator from SimVP that can achieve better performance. We demonstrate that SimVP has strong generalization and extensibility on real-world datasets through extensive experiments. The significant reduction in training cos",
    "link": "http://arxiv.org/abs/2211.12509",
    "context": "Title: SimVP: Towards Simple yet Powerful Spatiotemporal Predictive Learning. (arXiv:2211.12509v3 [cs.LG] UPDATED)\nAbstract: Recent years have witnessed remarkable advances in spatiotemporal predictive learning, incorporating auxiliary inputs, elaborate neural architectures, and sophisticated training strategies. Although impressive, the system complexity of mainstream methods is increasing as well, which may hinder the convenient applications. This paper proposes SimVP, a simple spatiotemporal predictive baseline model that is completely built upon convolutional networks without recurrent architectures and trained by common mean squared error loss in an end-to-end fashion. Without introducing any extra tricks and strategies, SimVP can achieve superior performance on various benchmark datasets. To further improve the performance, we derive variants with the gated spatiotemporal attention translator from SimVP that can achieve better performance. We demonstrate that SimVP has strong generalization and extensibility on real-world datasets through extensive experiments. The significant reduction in training cos",
    "path": "papers/22/11/2211.12509.json",
    "total_tokens": 939,
    "translated_title": "SimVP: 较为简单却强大的时空预测学习的探索",
    "translated_abstract": "近年来，时空预测学习在辅助输入、复杂神经结构和精细化训练策略等方面取得了显著进展。然而，目前主流方法的系统复杂性不断增加，这可能会妨碍它们的便捷应用。本文提出SimVP，一种完全基于卷积神经网络构建且没有循环结构、以均方误差损失为训练目标的简单时空预测基准模型。不需要任何额外的技巧和策略，SimVP在各种基准数据集上都能取得优异的表现。为了进一步提高性能，我们从SimVP中提取了具有门式时空注意力翻译器的变体，可以实现更好的性能。通过大量实验，我们证明了SimVP在真实世界数据集上具有强大的泛化性和可扩展性。 而且训练成本显著降低了。",
    "tldr": "SimVP是一种简单且强大的时空预测学习基准模型，完全基于卷积神经网络构建且没有循环结构，并且使用均方误差损失为训练目标。SimVP在各种基准数据集上均能取得优异的表现，并且具有强大的泛化性和可扩展性。",
    "en_tdlr": "SimVP is a simple yet powerful baseline model for spatiotemporal predictive learning, completely built upon convolutional networks without recurrent architectures, and trained by mean squared error loss. SimVP achieves superior performance on various benchmark datasets without introducing any extra tricks or strategies, and demonstrates strong generalization and extensibility on real-world datasets."
}