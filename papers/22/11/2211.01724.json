{
    "title": "Learning Control by Iterative Inversion. (arXiv:2211.01724v2 [cs.LG] UPDATED)",
    "abstract": "We propose $\\textit{iterative inversion}$ -- an algorithm for learning an inverse function without input-output pairs, but only with samples from the desired output distribution and access to the forward function. The key challenge is a $\\textit{distribution shift}$ between the desired outputs and the outputs of an initial random guess, and we prove that iterative inversion can steer the learning correctly, under rather strict conditions on the function. We apply iterative inversion to learn control. Our input is a set of demonstrations of desired behavior, given as video embeddings of trajectories (without actions), and our method iteratively learns to imitate trajectories generated by the current policy, perturbed by random exploration noise. Our approach does not require rewards, and only employs supervised learning, which can be easily scaled to use state-of-the-art trajectory embedding techniques and policy representations. Indeed, with a VQ-VAE embedding, and a transformer-based ",
    "link": "http://arxiv.org/abs/2211.01724",
    "context": "Title: Learning Control by Iterative Inversion. (arXiv:2211.01724v2 [cs.LG] UPDATED)\nAbstract: We propose $\\textit{iterative inversion}$ -- an algorithm for learning an inverse function without input-output pairs, but only with samples from the desired output distribution and access to the forward function. The key challenge is a $\\textit{distribution shift}$ between the desired outputs and the outputs of an initial random guess, and we prove that iterative inversion can steer the learning correctly, under rather strict conditions on the function. We apply iterative inversion to learn control. Our input is a set of demonstrations of desired behavior, given as video embeddings of trajectories (without actions), and our method iteratively learns to imitate trajectories generated by the current policy, perturbed by random exploration noise. Our approach does not require rewards, and only employs supervised learning, which can be easily scaled to use state-of-the-art trajectory embedding techniques and policy representations. Indeed, with a VQ-VAE embedding, and a transformer-based ",
    "path": "papers/22/11/2211.01724.json",
    "total_tokens": 934,
    "translated_title": "学习通过迭代反演控制",
    "translated_abstract": "我们提出了一种名为“迭代反演”的算法，用于学习无输入输出对的反函数，只使用所需输出分布的样本和对前向函数的访问。关键挑战是所期望输出与初始随机猜测的输出之间的分布偏移。我们证明了在函数的相当严格的条件下，迭代反演可以正确地引导学习。我们将迭代反演应用于学习控制。我们的输入是一组所需行为的演示，以轨迹的视频嵌入（没有动作）的形式给出，我们的方法迭代地学习模仿由当前策略生成的轨迹，受到随机探索噪声的扰动。我们的方法不需要奖励，仅使用监督学习，可以轻松扩大使用最先进的轨迹嵌入技术和策略表示。事实上，使用VQ-VAE嵌入和基于变压器的策略表示，我们在几个控制任务上获得了良好的表现。",
    "tldr": "迭代反演是一种算法，能够学习无输入输出对的反函数。该算法通过样本学习期望输出分布，可以解决所期望输出与初始随机猜测输出分布之间的偏移问题。此方法被成功应用于学习控制问题，并获得了良好的表现。",
    "en_tdlr": "Iterative inversion is an algorithm that can learn the inverse function without input-output pairs by sampling from the desired output distribution and accessing the forward function. It solves the challenge of distribution shift between desired outputs and initial random guess outputs. The method has been successfully applied to learning control and performs well."
}