{
    "title": "RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation. (arXiv:2211.09869v2 [cs.CV] UPDATED)",
    "abstract": "Diffusion models currently achieve state-of-the-art performance for both conditional and unconditional image generation. However, so far, image diffusion models do not support tasks required for 3D understanding, such as view-consistent 3D generation or single-view object reconstruction. In this paper, we present RenderDiffusion, the first diffusion model for 3D generation and inference, trained using only monocular 2D supervision. Central to our method is a novel image denoising architecture that generates and renders an intermediate three-dimensional representation of a scene in each denoising step. This enforces a strong inductive structure within the diffusion process, providing a 3D consistent representation while only requiring 2D supervision. The resulting 3D representation can be rendered from any view. We evaluate RenderDiffusion on FFHQ, AFHQ, ShapeNet and CLEVR datasets, showing competitive performance for generation of 3D scenes and inference of 3D scenes from 2D images. Ad",
    "link": "http://arxiv.org/abs/2211.09869",
    "context": "Title: RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation. (arXiv:2211.09869v2 [cs.CV] UPDATED)\nAbstract: Diffusion models currently achieve state-of-the-art performance for both conditional and unconditional image generation. However, so far, image diffusion models do not support tasks required for 3D understanding, such as view-consistent 3D generation or single-view object reconstruction. In this paper, we present RenderDiffusion, the first diffusion model for 3D generation and inference, trained using only monocular 2D supervision. Central to our method is a novel image denoising architecture that generates and renders an intermediate three-dimensional representation of a scene in each denoising step. This enforces a strong inductive structure within the diffusion process, providing a 3D consistent representation while only requiring 2D supervision. The resulting 3D representation can be rendered from any view. We evaluate RenderDiffusion on FFHQ, AFHQ, ShapeNet and CLEVR datasets, showing competitive performance for generation of 3D scenes and inference of 3D scenes from 2D images. Ad",
    "path": "papers/22/11/2211.09869.json",
    "total_tokens": 979,
    "translated_title": "RenderDiffusion: 用于3D重建、修复和生成的图像扩散方法",
    "translated_abstract": "目前，扩散模型在有条件和无条件的图像生成方面均达到了最先进的水平。但是，目前的图像扩散模型不支持用于3D理解所需的任务，例如视角一致的3D生成或单视角物体重建。本文提出了RenderDiffusion，这是第一个用于3D生成和推断的扩散模型，只需使用单眼2D监督进行训练。方法的核心是一种新颖的图像去噪架构，在每个去噪步骤中生成和渲染场景的中间三维表示。这在扩散过程中强制实现了一个强的归纳结构，提供了3D一致的表示，同时只需要2D监督。生成的3D表示可以从任何视角渲染。我们评估了RenderDiffusion在FFHQ、AFHQ、ShapeNet和CLEVR数据集上的性能，显示出了在生成3D场景和从2D图像推断3D场景方面具有竞争力的表现。",
    "tldr": "本文提出了第一个支持3D理解任务的扩散模型RenderDiffusion，只需使用单眼2D监督进行训练。它利用一种新颖的图像去噪架构，生成和渲染中间的三维表示，在扩散过程中提供强有力的3D一致性。",
    "en_tdlr": "This paper proposes RenderDiffusion, the first diffusion model for 3D generation and inference, trained using only monocular 2D supervision. It involves a novel image denoising architecture that generates and renders an intermediate 3D representation in each denoising step, enforcing a strong inductive structure within the diffusion process and providing strong 3D consistency while only requiring 2D supervision. Competitive results are shown on various datasets."
}