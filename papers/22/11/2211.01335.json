{
    "title": "Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. (arXiv:2211.01335v3 [cs.CV] UPDATED)",
    "abstract": "The tremendous success of CLIP (Radford et al., 2021) has promoted the research and application of contrastive learning for vision-language pretraining. In this work, we construct a large-scale dataset of image-text pairs in Chinese, where most data are retrieved from publicly available datasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5 Chinese CLIP models of multiple sizes, spanning from 77 to 958 million parameters. Furthermore, we propose a two-stage pretraining method, where the model is first trained with the image encoder frozen and then trained with all parameters being optimized, to achieve enhanced model performance. Our comprehensive experiments demonstrate that Chinese CLIP can achieve the state-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups of zero-shot learning and finetuning, and it is able to achieve competitive performance in zero-shot image classification based on the evaluation on the ELEVATER benchmark (Li et al., ",
    "link": "http://arxiv.org/abs/2211.01335",
    "context": "Title: Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese. (arXiv:2211.01335v3 [cs.CV] UPDATED)\nAbstract: The tremendous success of CLIP (Radford et al., 2021) has promoted the research and application of contrastive learning for vision-language pretraining. In this work, we construct a large-scale dataset of image-text pairs in Chinese, where most data are retrieved from publicly available datasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5 Chinese CLIP models of multiple sizes, spanning from 77 to 958 million parameters. Furthermore, we propose a two-stage pretraining method, where the model is first trained with the image encoder frozen and then trained with all parameters being optimized, to achieve enhanced model performance. Our comprehensive experiments demonstrate that Chinese CLIP can achieve the state-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups of zero-shot learning and finetuning, and it is able to achieve competitive performance in zero-shot image classification based on the evaluation on the ELEVATER benchmark (Li et al., ",
    "path": "papers/22/11/2211.01335.json",
    "total_tokens": 906,
    "translated_title": "中文CLIP: 中文对比视觉语言预训练",
    "translated_abstract": "CLIP的巨大成功推动了对于视觉语言预训练中对比学习的研究和应用。本研究构建了一个大规模的中文图像-文本对数据集，其中大部分数据来源于公开数据集，我们在这个新数据集上对中文CLIP模型进行了预训练，开发了5个多尺寸的中文CLIP模型，范围从7700万到9.58亿参数。此外，我们提出了一个两阶段预训练方法，在该方法中，模型首先使用冻结的图像编码器进行训练，然后使用所有参数进行优化来提高模型性能。综合实验表明，中文CLIP可以在零样本学习和微调设置下在MUGE、Flickr30K-CN和COCO-CN上实现最先进的性能，并且它能够在ELEVATER基准测试上的零样本图像分类中取得竞争性的性能。",
    "tldr": "本研究构建了一个大规模的中文图像-文本对数据集，新提出的两阶段预训练方法提高了模型性能，中文CLIP在多任务图像理解中取得最先进的性能表现，特别在零样本学习和微调设置下表现出色。"
}