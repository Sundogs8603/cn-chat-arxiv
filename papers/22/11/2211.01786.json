{
    "title": "Crosslingual Generalization through Multitask Finetuning. (arXiv:2211.01786v2 [cs.CL] UPDATED)",
    "abstract": "Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find m",
    "link": "http://arxiv.org/abs/2211.01786",
    "context": "Title: Crosslingual Generalization through Multitask Finetuning. (arXiv:2211.01786v2 [cs.CL] UPDATED)\nAbstract: Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find m",
    "path": "papers/22/11/2211.01786.json",
    "total_tokens": 973,
    "translated_title": "通过多任务微调实现跨语言泛化",
    "translated_abstract": "已经证明，多任务微调可以帮助大型语言模型在零-shot场景下推广到新的任务，但目前MTF的研究集中在英语数据和模型上。我们将MTF应用于预训练的多语言BLOOM和mT5模型系列，生成了经过微调的变体BLOOMZ和mT0。我们发现，在英语提示下，对大型多语言语言模型进行英语任务的微调，可以实现对仅出现在预训练语料库中的非英语语言的任务泛化。使用英语提示进行多语言任务的微调进一步提高了在英语和非英语任务上的表现，从而实现了各种零-shot结果的最新水平。我们还研究了在英语翻译为每个数据集的语言的情况下进行多语言任务微调。我们发现，在这些机器翻译提示上训练可以在各自语言中更好地完成人写的提示。令人惊讶的是，我们发现m",
    "tldr": "该论文通过多任务微调实现跨语言泛化。研究表明，在英语提示下，对大型多语言模型进行英语任务的微调，可以实现对仅出现在预训练语料库中的非英语语言的任务泛化，并且使用英语提示进行多语言任务的微调进一步提高了在英语和非英语任务上的表现，从而实现了各种零-shot结果的最新水平。",
    "en_tdlr": "This paper proposes a multi-task learning approach for crosslingual generalization, and shows that fine-tuning multilingual language models on English tasks with English prompts can lead to task generalization to non-English languages. Fine-tuning on multilingual tasks with translated English prompts can also improve performance on human-written prompts in respective languages."
}