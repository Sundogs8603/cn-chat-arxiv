{
    "title": "DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model. (arXiv:2211.16374v2 [cs.CV] UPDATED)",
    "abstract": "Recent 3D generative models have achieved remarkable performance in synthesizing high resolution photorealistic images with view consistency and detailed 3D shapes, but training them for diverse domains is challenging since it requires massive training images and their camera distribution information. Text-guided domain adaptation methods have shown impressive performance on converting the 2D generative model on one domain into the models on other domains with different styles by leveraging the CLIP (Contrastive Language-Image Pre-training), rather than collecting massive datasets for those domains. However, one drawback of them is that the sample diversity in the original generative model is not well-preserved in the domain-adapted generative models due to the deterministic nature of the CLIP text encoder. Text-guided domain adaptation will be even more challenging for 3D generative models not only because of catastrophic diversity loss, but also because of inferior text-image corresp",
    "link": "http://arxiv.org/abs/2211.16374",
    "context": "Title: DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model. (arXiv:2211.16374v2 [cs.CV] UPDATED)\nAbstract: Recent 3D generative models have achieved remarkable performance in synthesizing high resolution photorealistic images with view consistency and detailed 3D shapes, but training them for diverse domains is challenging since it requires massive training images and their camera distribution information. Text-guided domain adaptation methods have shown impressive performance on converting the 2D generative model on one domain into the models on other domains with different styles by leveraging the CLIP (Contrastive Language-Image Pre-training), rather than collecting massive datasets for those domains. However, one drawback of them is that the sample diversity in the original generative model is not well-preserved in the domain-adapted generative models due to the deterministic nature of the CLIP text encoder. Text-guided domain adaptation will be even more challenging for 3D generative models not only because of catastrophic diversity loss, but also because of inferior text-image corresp",
    "path": "papers/22/11/2211.16374.json",
    "total_tokens": 975,
    "translated_title": "DATID-3D: 基于文本图像扩散的多样性保留领域自适应方法应用于3D生成模型",
    "translated_abstract": "近期的3D生成模型在合成高分辨率逼真图像且保持视角一致性和详细的三维形状方面已经取得了显著的成果，但在多种不同领域的训练上仍然具有挑战性，因为需要大量的训练图像及其相机分布信息。指导文本信息的领域自适应方法通过利用CLIP（Contrastive Language-Image Pre-training）将一个2D生成模型转换为具有不同风格的其他领域模型，而不需要收集这些领域的大量数据集。然而，它们的一个缺点是由于CLIP文本编码器的确定性，原始生成模型中的样本多样性在领域适应生成模型中并没有得到很好的保留。对3D生成模型进行指导文本信息的领域自适应不仅由于灾难性多样性损失的原因，还由于图像和文本之间的对应关系不佳而更加具有挑战性。",
    "tldr": "本文提出一种基于文本图像扩散的多样性保留领域自适应方法，用于解决3D生成模型在多种不同领域的训练挑战，该方法应用于将2D生成模型转化为其它风格的领域模型，通过CLIP学习文本和图像之间的关系。"
}