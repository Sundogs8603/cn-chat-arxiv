{
    "title": "miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings. (arXiv:2211.04928v2 [cs.CL] UPDATED)",
    "abstract": "This paper presents miCSE, a mutual information-based contrastive learning framework that significantly advances the state-of-the-art in few-shot sentence embedding. The proposed approach imposes alignment between the attention pattern of different views during contrastive learning. Learning sentence embeddings with miCSE entails enforcing the structural consistency across augmented views for every sentence, making contrastive self-supervised learning more sample efficient. As a result, the proposed approach shows strong performance in the few-shot learning domain. While it achieves superior results compared to state-of-the-art methods on multiple benchmarks in few-shot learning, it is comparable in the full-shot scenario. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods for sentence embedding.",
    "link": "http://arxiv.org/abs/2211.04928",
    "context": "Title: miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings. (arXiv:2211.04928v2 [cs.CL] UPDATED)\nAbstract: This paper presents miCSE, a mutual information-based contrastive learning framework that significantly advances the state-of-the-art in few-shot sentence embedding. The proposed approach imposes alignment between the attention pattern of different views during contrastive learning. Learning sentence embeddings with miCSE entails enforcing the structural consistency across augmented views for every sentence, making contrastive self-supervised learning more sample efficient. As a result, the proposed approach shows strong performance in the few-shot learning domain. While it achieves superior results compared to state-of-the-art methods on multiple benchmarks in few-shot learning, it is comparable in the full-shot scenario. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods for sentence embedding.",
    "path": "papers/22/11/2211.04928.json",
    "total_tokens": 845,
    "translated_title": "miCSE：用于少样本句子嵌入的互信息对比学习框架",
    "translated_abstract": "本文介绍了miCSE，一种基于互信息对比学习的框架，该框架极大地提高了少样本句子嵌入的最新技术水平。所提出的方法在对比学习期间，通过对不同视图的注意力模式进行对齐。使用miCSE学习句子嵌入即对每个句子的增强视图强制实施结构一致性，从而使对比自监督学习更加高效。因此，该方法在少样本学习领域表现出强大的性能。虽然与多个少样本学习基准的最新方法相比表现出卓越的结果，但在全样本情况下具有可比性。这项研究为比当前的句子嵌入对比方法更加鲁棒的高效自监督学习方法开辟了新的途径。",
    "tldr": "本文提出了miCSE框架，使用互信息对比学习在少样本情况下学习句子嵌入，在多个基准测试中均表现出卓越结果，并为更加鲁棒的自监督学习方法开辟了新的途径。",
    "en_tdlr": "This paper proposes the miCSE framework, uses mutual information contrastive learning to learn sentence embeddings in few-shot scenarios, shows superior results compared to multiple benchmarks, and opens up avenues for more robust self-supervised learning methods."
}