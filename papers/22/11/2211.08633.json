{
    "title": "MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation. (arXiv:2211.08633v2 [cs.CL] UPDATED)",
    "abstract": "There have been several meta-evaluation studies on the correlation between human ratings and offline machine translation (MT) evaluation metrics such as BLEU, chrF2, BertScore and COMET. These metrics have been used to evaluate simultaneous speech translation (SST) but their correlations with human ratings of SST, which has been recently collected as Continuous Ratings (CR), are unclear. In this paper, we leverage the evaluations of candidate systems submitted to the English-German SST task at IWSLT 2022 and conduct an extensive correlation analysis of CR and the aforementioned metrics. Our study reveals that the offline metrics are well correlated with CR and can be reliably used for evaluating machine translation in simultaneous mode, with some limitations on the test set size. We conclude that given the current quality levels of SST, these metrics can be used as proxies for CR, alleviating the need for large scale human evaluation. Additionally, we observe that correlations of the m",
    "link": "http://arxiv.org/abs/2211.08633",
    "context": "Title: MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation. (arXiv:2211.08633v2 [cs.CL] UPDATED)\nAbstract: There have been several meta-evaluation studies on the correlation between human ratings and offline machine translation (MT) evaluation metrics such as BLEU, chrF2, BertScore and COMET. These metrics have been used to evaluate simultaneous speech translation (SST) but their correlations with human ratings of SST, which has been recently collected as Continuous Ratings (CR), are unclear. In this paper, we leverage the evaluations of candidate systems submitted to the English-German SST task at IWSLT 2022 and conduct an extensive correlation analysis of CR and the aforementioned metrics. Our study reveals that the offline metrics are well correlated with CR and can be reliably used for evaluating machine translation in simultaneous mode, with some limitations on the test set size. We conclude that given the current quality levels of SST, these metrics can be used as proxies for CR, alleviating the need for large scale human evaluation. Additionally, we observe that correlations of the m",
    "path": "papers/22/11/2211.08633.json",
    "total_tokens": 909,
    "translated_title": "机器翻译的衡量指标与人工评分相符，在同时翻译中也适用",
    "translated_abstract": "在人工评分和离线机器翻译评估指标（如BLEU、chrF2、BertScore和COMET）之间有几个元评估研究。这些指标已经用于同时语音翻译（SST）的评估，但它们与最近收集的连续评分（CR）的SST的人工评分之间的相关性不清楚。在本文中，我们利用提交给IWSLT 2022年英德SST任务的候选系统的评估，并对CR和上述指标进行了广泛的相关性分析。我们的研究表明，离线指标与CR存在良好的相关性，并可以可靠地用于评估同时翻译模式下的机器翻译，但对测试集大小有一定限制。我们得出结论称，鉴于当前SST的质量水平，这些指标可以用作CR的代理，减轻了大规模人工评估的需要。另外，我们观察到许多指标的相关性在不同的测试集下发生变化。",
    "tldr": "本文表明了离线指标与人工连续评分之间存在良好的相关性，可以在同时翻译模式中可靠地用于机器翻译的评估，从而减轻了大规模人工评估的需要。"
}