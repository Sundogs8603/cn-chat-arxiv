{
    "title": "Weighted Ensemble Self-Supervised Learning. (arXiv:2211.09981v3 [cs.LG] UPDATED)",
    "abstract": "Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses. We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that th",
    "link": "http://arxiv.org/abs/2211.09981",
    "context": "Title: Weighted Ensemble Self-Supervised Learning. (arXiv:2211.09981v3 [cs.LG] UPDATED)\nAbstract: Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses. We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that th",
    "path": "papers/22/11/2211.09981.json",
    "total_tokens": 968,
    "translated_title": "带权重集成的自监督学习",
    "translated_abstract": "集成在监督学习中已被证明是提高模型性能、不确定性估计和健壮性的有效技术。自监督学习的进展使得利用大规模未标记语料库进行最先进的小样本和监督学习成为可能。本文研究了如何通过开发一个允许数据相关的加权交叉熵损失的框架来改进最近的自监督学习技术。我们避免对表示骨干进行集成；这个选择产生了一种高效的集成方法，它的训练成本很小，对下游评估不需要进行架构改变或计算开销。我们的方法在 ImageNet-1K 数据集上使用了两种最先进的自监督学习方法 DINO (Caron 等人，2021) 和 MSN (Assran 等人，2022)，在多个评估指标上均优于它们，尤其在小样本设置中表现最佳。我们探讨了几种加权方案，并发现…（未完成）",
    "tldr": "本文提出了一种带权重集成的自监督学习方法，通过开发允许数据相关的加权交叉熵损失的框架，可以提高最近自监督学习技术的性能，而不需要改变原有的架构，其在 ImageNet-1K 数据集上的表现优于最先进的 DINO 和 MSN 方法，特别是在小样本设置中表现最佳。"
}