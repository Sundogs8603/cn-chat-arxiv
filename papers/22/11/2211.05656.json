{
    "title": "On Proper Learnability between Average- and Worst-case Robustness. (arXiv:2211.05656v5 [cs.LG] UPDATED)",
    "abstract": "Recently, Montasser et al. [2019] showed that finite VC dimension is not sufficient for proper adversarially robust PAC learning. In light of this hardness, there is a growing effort to study what type of relaxations to the adversarially robust PAC learning setup can enable proper learnability. In this work, we initiate the study of proper learning under relaxations of the worst-case robust loss. We give a family of robust loss relaxations under which VC classes are properly PAC learnable with sample complexity close to what one would require in the standard PAC learning setup. On the other hand, we show that for an existing and natural relaxation of the worst-case robust loss, finite VC dimension is not sufficient for proper learning. Lastly, we give new generalization guarantees for the adversarially robust empirical risk minimizer.",
    "link": "http://arxiv.org/abs/2211.05656",
    "context": "Title: On Proper Learnability between Average- and Worst-case Robustness. (arXiv:2211.05656v5 [cs.LG] UPDATED)\nAbstract: Recently, Montasser et al. [2019] showed that finite VC dimension is not sufficient for proper adversarially robust PAC learning. In light of this hardness, there is a growing effort to study what type of relaxations to the adversarially robust PAC learning setup can enable proper learnability. In this work, we initiate the study of proper learning under relaxations of the worst-case robust loss. We give a family of robust loss relaxations under which VC classes are properly PAC learnable with sample complexity close to what one would require in the standard PAC learning setup. On the other hand, we show that for an existing and natural relaxation of the worst-case robust loss, finite VC dimension is not sufficient for proper learning. Lastly, we give new generalization guarantees for the adversarially robust empirical risk minimizer.",
    "path": "papers/22/11/2211.05656.json",
    "total_tokens": 907,
    "translated_title": "在平均鲁棒性和最坏情况下鲁棒性之间的适当可学习性研究",
    "translated_abstract": "最近，Montasser等人[2019]表明，有限VC维度不足以实现适当的对抗鲁棒PAC学习。鉴于这种困难，学界开始研究在对最坏情况下鲁棒损失的放宽下的适当学习。我们给出了一系列鲁棒损失的放宽，使得VC分类可适当地用PAC学习算法进行学习，其样本复杂度接近于标准PAC学习设置所需的复杂度。另一方面，我们证明了对于一种现有的和自然的最坏情况下鲁棒损失的放宽，有限的VC维度不足以实现适当的学习。最后，我们给出了对于对抗鲁棒性经验风险最小化算法的新的泛化保证。",
    "tldr": "本研究研究了在对最坏情况下鲁棒损失的放松下适当的学习问题，提出了鲁棒损失的放宽使得VC分类可适当地用PAC学习算法进行学习，并给出了对于对抗鲁棒性经验风险最小化算法的新的泛化保证。",
    "en_tdlr": "This paper studies the problem of proper learning under relaxations of the worst-case robust loss, proposes a family of robust loss relaxations under which VC classes are properly PAC learnable with close sample complexity, and provides new generalization guarantees for the adversarially robust empirical risk minimizer."
}