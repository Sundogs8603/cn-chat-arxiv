{
    "title": "MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking. (arXiv:2211.05503v3 [cs.CL] UPDATED)",
    "abstract": "Dialogue state tracking (DST) aims to convert the dialogue history into dialogue states which consist of slot-value pairs. As condensed structural information memorizing all history information, the dialogue state in the last turn is typically adopted as the input for predicting the current state by DST models. However, these models tend to keep the predicted slot values unchanged, which is defined as state momentum in this paper. Specifically, the models struggle to update slot values that need to be changed and correct wrongly predicted slot values in the last turn. To this end, we propose MoNET to tackle state momentum via noise-enhanced training. First, the previous state of each turn in the training data is noised via replacing some of its slot values. Then, the noised previous state is used as the input to learn to predict the current state, improving the model's ability to update and correct slot values. Furthermore, a contrastive context matching framework is designed to narrow",
    "link": "http://arxiv.org/abs/2211.05503",
    "context": "Title: MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking. (arXiv:2211.05503v3 [cs.CL] UPDATED)\nAbstract: Dialogue state tracking (DST) aims to convert the dialogue history into dialogue states which consist of slot-value pairs. As condensed structural information memorizing all history information, the dialogue state in the last turn is typically adopted as the input for predicting the current state by DST models. However, these models tend to keep the predicted slot values unchanged, which is defined as state momentum in this paper. Specifically, the models struggle to update slot values that need to be changed and correct wrongly predicted slot values in the last turn. To this end, we propose MoNET to tackle state momentum via noise-enhanced training. First, the previous state of each turn in the training data is noised via replacing some of its slot values. Then, the noised previous state is used as the input to learn to predict the current state, improving the model's ability to update and correct slot values. Furthermore, a contrastive context matching framework is designed to narrow",
    "path": "papers/22/11/2211.05503.json",
    "total_tokens": 845,
    "translated_title": "MoNET：通过噪声增强的训练解决对话状态跟踪中的状态惯性问题",
    "translated_abstract": "对话状态跟踪（DST）旨在将对话历史转换为包含槽-值对的对话状态。作为所有历史信息的结构化概括信息，通常采用上一轮的对话状态作为DST模型预测当前状态的输入。但是，这些模型往往保持预测的槽值不变，这在本文中被定义为状态惯性。为了解决这个问题，我们提出了MoNET，在噪声增强训练的帮助下解决状态惯性问题。具体而言，我们使用替换一些槽值来噪声化每轮的训练数据中的上一个状态。然后，将噪声化的上一个状态作为输入，预测当前状态，从而改善模型更新和修正槽值的能力。此外，我们设计了对比上下文匹配框架来缩小...",
    "tldr": "本研究提出了MoNET，通过噪声增强训练解决了对话状态跟踪中的状态惯性问题，提高了模型修正槽值的能力。",
    "en_tdlr": "This paper proposes MoNET to tackle state momentum via noise-enhanced training in dialogue state tracking, improving the ability to update and correct slot values."
}