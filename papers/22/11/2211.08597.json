{
    "title": "SketchySGD: Reliable Stochastic Optimization via Randomized Curvature Estimates. (arXiv:2211.08597v4 [math.OC] UPDATED)",
    "abstract": "SketchySGD improves upon existing stochastic gradient methods in machine learning by using randomized low-rank approximations to the subsampled Hessian and by introducing an automated stepsize that works well across a wide range of convex machine learning problems. We show theoretically that SketchySGD with a fixed stepsize converges linearly to a small ball around the optimum. Further, in the ill-conditioned setting we show SketchySGD converges at a faster rate than SGD for least-squares problems. We validate this improvement empirically with ridge regression experiments on real data. Numerical experiments on both ridge and logistic regression problems with dense and sparse data, show that SketchySGD equipped with its default hyperparameters can achieve comparable or better results than popular stochastic gradient methods, even when they have been tuned to yield their best performance. In particular, SketchySGD is able to solve an ill-conditioned logistic regression problem with a dat",
    "link": "http://arxiv.org/abs/2211.08597",
    "context": "Title: SketchySGD: Reliable Stochastic Optimization via Randomized Curvature Estimates. (arXiv:2211.08597v4 [math.OC] UPDATED)\nAbstract: SketchySGD improves upon existing stochastic gradient methods in machine learning by using randomized low-rank approximations to the subsampled Hessian and by introducing an automated stepsize that works well across a wide range of convex machine learning problems. We show theoretically that SketchySGD with a fixed stepsize converges linearly to a small ball around the optimum. Further, in the ill-conditioned setting we show SketchySGD converges at a faster rate than SGD for least-squares problems. We validate this improvement empirically with ridge regression experiments on real data. Numerical experiments on both ridge and logistic regression problems with dense and sparse data, show that SketchySGD equipped with its default hyperparameters can achieve comparable or better results than popular stochastic gradient methods, even when they have been tuned to yield their best performance. In particular, SketchySGD is able to solve an ill-conditioned logistic regression problem with a dat",
    "path": "papers/22/11/2211.08597.json",
    "total_tokens": 958,
    "tldr": "SketchySGD是一个使用随机低秩逼近子采样的海森矩阵的机器学习随机梯度方法，引入了一种可靠的自动化步长方法，并具有在稀疏和稠密数据中实现可比或更好结果的能力，甚至在其他流行的随机梯度方法不能解决的病态问题中也能取得成功。",
    "en_tdlr": "SketchySGD is a machine learning stochastic gradient method that uses a randomized low-rank approximation of the subsampled Hessian and introduces a reliable automatic stepsize. It has the ability to achieve comparable or even better results in both sparse and dense data, and is successful in ill-conditioned problems where other popular stochastic gradient methods fail."
}