{
    "title": "Safe and Efficient Reinforcement Learning Using Disturbance-Observer-Based Control Barrier Functions. (arXiv:2211.17250v3 [cs.RO] UPDATED)",
    "abstract": "Safe reinforcement learning (RL) with assured satisfaction of hard state constraints during training has recently received a lot of attention. Safety filters, e.g., based on control barrier functions (CBFs), provide a promising way for safe RL via modifying the unsafe actions of an RL agent on the fly. Existing safety filter-based approaches typically involve learning of uncertain dynamics and quantifying the learned model error, which leads to conservative filters before a large amount of data is collected to learn a good model, thereby preventing efficient exploration. This paper presents a method for safe and efficient RL using disturbance observers (DOBs) and control barrier functions (CBFs). Unlike most existing safe RL methods that deal with hard state constraints, our method does not involve model learning, and leverages DOBs to accurately estimate the pointwise value of the uncertainty, which is then incorporated into a robust CBF condition to generate safe actions. The DOB-bas",
    "link": "http://arxiv.org/abs/2211.17250",
    "context": "Title: Safe and Efficient Reinforcement Learning Using Disturbance-Observer-Based Control Barrier Functions. (arXiv:2211.17250v3 [cs.RO] UPDATED)\nAbstract: Safe reinforcement learning (RL) with assured satisfaction of hard state constraints during training has recently received a lot of attention. Safety filters, e.g., based on control barrier functions (CBFs), provide a promising way for safe RL via modifying the unsafe actions of an RL agent on the fly. Existing safety filter-based approaches typically involve learning of uncertain dynamics and quantifying the learned model error, which leads to conservative filters before a large amount of data is collected to learn a good model, thereby preventing efficient exploration. This paper presents a method for safe and efficient RL using disturbance observers (DOBs) and control barrier functions (CBFs). Unlike most existing safe RL methods that deal with hard state constraints, our method does not involve model learning, and leverages DOBs to accurately estimate the pointwise value of the uncertainty, which is then incorporated into a robust CBF condition to generate safe actions. The DOB-bas",
    "path": "papers/22/11/2211.17250.json",
    "total_tokens": 939,
    "translated_title": "使用扰动观测器和控制屏障函数的安全高效增强学习",
    "translated_abstract": "近期关注越来越多的是在训练过程中保证硬状态约束满足的安全增强学习。安全过滤器，例如基于控制屏障函数(CBFs)的方法，通过即时修改增强学习代理的不安全动作来实现安全的增强学习。现有的基于安全过滤器的方法通常涉及学习不确定动力学和量化学习模型误差，这导致在收集足够的数据学习好模型之前，过滤器过于保守，从而影响了有效的探索。本文提出了一种使用扰动观测器和控制屏障函数进行安全高效增强学习的方法。与大多数现有的处理硬状态约束的安全增强学习方法不同，我们的方法不涉及模型学习，并利用扰动观测器准确估计不确定性的点值，然后将其纳入鲁棒的CBF条件生成安全动作。",
    "tldr": "本论文提出了一种使用扰动观测器和控制屏障函数进行安全高效增强学习的方法，与现有方法相比，该方法不需要模型学习，利用扰动观测器准确估计不确定性的点值，并将其纳入鲁棒的CBF条件中生成安全动作。",
    "en_tdlr": "This paper proposes a method for safe and efficient reinforcement learning using disturbance observers and control barrier functions. Unlike existing methods, this approach does not require model learning and accurately estimates the uncertainty using disturbance observers, which is then used to generate safe actions based on robust CBF conditions."
}