{
    "title": "Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models. (arXiv:2211.15718v2 [cs.CL] UPDATED)",
    "abstract": "In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel classes, then generate examples from each novel class matching the task format. Second, we train a classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on novel class examples over prior methods by",
    "link": "http://arxiv.org/abs/2211.15718",
    "context": "Title: Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models. (arXiv:2211.15718v2 [cs.CL] UPDATED)\nAbstract: In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel classes, then generate examples from each novel class matching the task format. Second, we train a classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on novel class examples over prior methods by",
    "path": "papers/22/11/2211.15718.json",
    "total_tokens": 886,
    "translated_title": "对比新颖性增强学习: 利用大型语言模型预测离群值",
    "translated_abstract": "在许多任务设置中，文本分类模型可能会遇到无法正确预测的新颖类别的示例。有选择地预测在低置信度示例上的模型提供了一种可能的解决方案，但现有模型对未见过的类别常常过于自信。为了纠正这种过度自信，我们引入了对比新颖性增强学习（CoNAL），这是一种两步方法，它生成代表新颖类别的OOD示例，然后训练以降低置信度。首先，通过提示大型语言模型两次，我们生成OOD示例：我们提示它枚举相关的新颖类别，然后生成每个新颖类别匹配任务格式的示例。其次，我们使用新颖的对比目标来训练分类器，该目标鼓励在生成的OOD示例上具有比训练示例更低的置信度。当使用CoNAL训练时，与先前的方法相比，分类器在检测和放弃新颖类别示例方面的能力得到了大幅提高，同时保持对已知类别的性能。",
    "tldr": "CoNAL方法可以帮助分类模型降低在新颖类别上的过度自信，提高检测和放弃这些类别示例的能力。",
    "en_tdlr": "CoNAL approach helps classification models decrease overconfidence on novel classes, improving the ability to detect and abstain from these class examples."
}