{
    "title": "Perturb Initial Features: Generalization of Neural Networks Under Sparse Features for Semi-supervised Node Classification. (arXiv:2211.15081v6 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks (GNNs) are commonly used in semi-supervised settings. Previous research has primarily focused on finding appropriate graph filters (e.g. aggregation methods) to perform well on both homophilic and heterophilic graphs. While these methods are effective, they can still suffer from the sparsity of node features, where the initial data contain few non-zero elements. This can lead to overfitting in certain dimensions in the first projection matrix, as training samples may not cover the entire range of graph filters (hyperplanes). To address this, we propose a novel data augmentation strategy. Specifically, by flipping both the initial features and hyperplane, we create additional space for training, which leads to more precise updates of the learnable parameters and improved robustness for unseen features during inference. To the best of our knowledge, this is the first attempt to mitigate the overfitting caused by the initial features. Extensive experiments on real-wo",
    "link": "http://arxiv.org/abs/2211.15081",
    "context": "Title: Perturb Initial Features: Generalization of Neural Networks Under Sparse Features for Semi-supervised Node Classification. (arXiv:2211.15081v6 [cs.LG] UPDATED)\nAbstract: Graph neural networks (GNNs) are commonly used in semi-supervised settings. Previous research has primarily focused on finding appropriate graph filters (e.g. aggregation methods) to perform well on both homophilic and heterophilic graphs. While these methods are effective, they can still suffer from the sparsity of node features, where the initial data contain few non-zero elements. This can lead to overfitting in certain dimensions in the first projection matrix, as training samples may not cover the entire range of graph filters (hyperplanes). To address this, we propose a novel data augmentation strategy. Specifically, by flipping both the initial features and hyperplane, we create additional space for training, which leads to more precise updates of the learnable parameters and improved robustness for unseen features during inference. To the best of our knowledge, this is the first attempt to mitigate the overfitting caused by the initial features. Extensive experiments on real-wo",
    "path": "papers/22/11/2211.15081.json",
    "total_tokens": 1001,
    "tldr": "该论文提出了一种新的数据增强策略，通过扰动初始特征和超平面，为训练创造额外的空间，从而减轻半监督图神经网络在节点特征稀疏的情况下的过拟合问题。"
}