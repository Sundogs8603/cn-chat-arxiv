{
    "title": "Interpreting Primal-Dual Algorithms for Constrained Multiagent Reinforcement Learning. (arXiv:2211.16069v3 [eess.SY] UPDATED)",
    "abstract": "Constrained multiagent reinforcement learning (C-MARL) is gaining importance as MARL algorithms find new applications in real-world systems ranging from energy systems to drone swarms. Most C-MARL algorithms use a primal-dual approach to enforce constraints through a penalty function added to the reward. In this paper, we study the structural effects of this penalty term on the MARL problem. First, we show that the standard practice of using the constraint function as the penalty leads to a weak notion of safety. However, by making simple modifications to the penalty term, we can enforce meaningful probabilistic (chance and conditional value at risk) constraints. Second, we quantify the effect of the penalty term on the value function, uncovering an improved value estimation procedure. We use these insights to propose a constrained multiagent advantage actor critic (C-MAA2C) algorithm. Simulations in a simple constrained multiagent environment affirm that our reinterpretation of the pr",
    "link": "http://arxiv.org/abs/2211.16069",
    "context": "Title: Interpreting Primal-Dual Algorithms for Constrained Multiagent Reinforcement Learning. (arXiv:2211.16069v3 [eess.SY] UPDATED)\nAbstract: Constrained multiagent reinforcement learning (C-MARL) is gaining importance as MARL algorithms find new applications in real-world systems ranging from energy systems to drone swarms. Most C-MARL algorithms use a primal-dual approach to enforce constraints through a penalty function added to the reward. In this paper, we study the structural effects of this penalty term on the MARL problem. First, we show that the standard practice of using the constraint function as the penalty leads to a weak notion of safety. However, by making simple modifications to the penalty term, we can enforce meaningful probabilistic (chance and conditional value at risk) constraints. Second, we quantify the effect of the penalty term on the value function, uncovering an improved value estimation procedure. We use these insights to propose a constrained multiagent advantage actor critic (C-MAA2C) algorithm. Simulations in a simple constrained multiagent environment affirm that our reinterpretation of the pr",
    "path": "papers/22/11/2211.16069.json",
    "total_tokens": 1010,
    "translated_title": "对约束多智能体强化学习的原始-对偶算法的解释",
    "translated_abstract": "约束多智能体强化学习(C-MARL)在实际系统(从能源系统到无人机群)中的应用日益重要。大多数C-MARL算法通过将惩罚函数添加到奖励中，使用原始-对偶方法来实现约束。本文研究这种惩罚项对MARL问题的结构影响。首先，我们证明了使用约束函数作为惩罚的标准做法可以导致一种较弱的安全性概念。然而，通过对惩罚项进行简单修正，我们可以强制执行有意义的概率约束。其次，我们量化了惩罚项对价值函数的影响，揭示了改进的价值估计过程。我们利用这些见解提出了一种约束多智能体优势演员评论家(C-MAA2C)算法。在简单的约束多智能体环境中的模拟实验证实了我们对原始-对偶方法的重新解释可以在MARL问题中导致更好的安全性和性能。",
    "tldr": "本文研究了对约束多智能体强化学习的原始-对偶算法，证明了使用约束函数作为惩罚的标准做法可以导致较弱的安全性概念，通过对惩罚项进行简单修正，能够强制约束并提高性能。",
    "en_tdlr": "This paper studies the primal-dual algorithms for constrained multiagent reinforcement learning (C-MARL), and shows that using the constraint function as the penalty can lead to weak safety. By modifying the penalty term, meaningful probabilistic constraints can be enforced leading to better performance. The paper proposes a constrained multiagent advantage actor critic (C-MAA2C) algorithm based on these insights, and simulations show improved safety and performance in C-MARL problems."
}