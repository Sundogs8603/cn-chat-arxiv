{
    "title": "Neural Architecture for Online Ensemble Continual Learning. (arXiv:2211.14963v2 [cs.LG] UPDATED)",
    "abstract": "Continual learning with an increasing number of classes is a challenging task. The difficulty rises when each example is presented exactly once, which requires the model to learn online. Recent methods with classic parameter optimization procedures have been shown to struggle in such setups or have limitations like non-differentiable components or memory buffers. For this reason, we present the fully differentiable ensemble method that allows us to efficiently train an ensemble of neural networks in the end-to-end regime. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods. The conducted experiments have also shown a significant increase in the performance for small ensembles, which demonstrates the capability of obtaining relatively high classification accuracy with a reduced number of classifiers.",
    "link": "http://arxiv.org/abs/2211.14963",
    "context": "Title: Neural Architecture for Online Ensemble Continual Learning. (arXiv:2211.14963v2 [cs.LG] UPDATED)\nAbstract: Continual learning with an increasing number of classes is a challenging task. The difficulty rises when each example is presented exactly once, which requires the model to learn online. Recent methods with classic parameter optimization procedures have been shown to struggle in such setups or have limitations like non-differentiable components or memory buffers. For this reason, we present the fully differentiable ensemble method that allows us to efficiently train an ensemble of neural networks in the end-to-end regime. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods. The conducted experiments have also shown a significant increase in the performance for small ensembles, which demonstrates the capability of obtaining relatively high classification accuracy with a reduced number of classifiers.",
    "path": "papers/22/11/2211.14963.json",
    "total_tokens": 775,
    "translated_title": "在线集成持续学习的神经结构",
    "translated_abstract": "随着类别数量的增加，持续学习变得越来越具有挑战性。当每个示例只出现一次时，学习变得更加困难，这要求模型能够在线学习。最近的方法在这种设置下往往面临困难，或者存在无法微分的组件或者内存缓冲区的限制。因此，我们提出了完全可微的集成方法，可以在端到端的方式下高效训练神经网络的集成模型。所提出的技术在没有内存缓冲区的情况下取得了最先进的结果，并明显优于参考方法。进行的实验还表明，对于小型的集成模型，性能有显著提高，这表明可以通过更少的分类器获得相对较高的分类准确性。",
    "tldr": "提出了一种完全可微的在线集成持续学习方法，实验结果表明在没有内存缓冲区的情况下取得了最先进的结果，并且可以通过较少的分类器获得较高的分类准确性。",
    "en_tdlr": "A fully differentiable online ensemble continual learning method is proposed, which achieves state-of-the-art results without a memory buffer and enables higher classification accuracy with a reduced number of classifiers."
}