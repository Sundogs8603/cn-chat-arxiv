{
    "title": "PAD-Net: An Efficient Framework for Dynamic Networks. (arXiv:2211.05528v2 [cs.LG] UPDATED)",
    "abstract": "Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model's representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments wi",
    "link": "http://arxiv.org/abs/2211.05528",
    "context": "Title: PAD-Net: An Efficient Framework for Dynamic Networks. (arXiv:2211.05528v2 [cs.LG] UPDATED)\nAbstract: Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model's representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments wi",
    "path": "papers/22/11/2211.05528.json",
    "total_tokens": 813,
    "translated_title": "PAD-Net：用于动态网络的高效框架",
    "translated_abstract": "动态网络，例如动态卷积（DY-Conv）和专家混合模型（MoE），已被广泛探索，因为它们可以显着提高模型的表示能力，并具有可接受的计算成本。实现动态网络的常见做法是将给定的静态层转换为完全动态的层，其中所有参数都是动态的（至少在单个层内）并随输入变化。但是，这种完全动态的设置可能会导致冗余参数和高部署成本，从而限制了动态网络在更广泛的任务和模型中的适用性。我们工作的主要贡献是挑战动态网络的基本常识，并提出部分动态网络，即PAD-Net，以将冗余动态参数转换为静态参数。此外，我们进一步设计迭代模式分区来有效地分区动态和静态参数。我们的方法受到大规模实验的全面支持。",
    "tldr": "PAD-Net是一个部分动态网络的框架，将冗余的动态参数转换为静态参数，提高了动态网络的效率和适用性。",
    "en_tdlr": "PAD-Net is a framework for partially dynamic networks that transforms redundant dynamic parameters into static ones, improving the efficiency and applicability of dynamic networks."
}