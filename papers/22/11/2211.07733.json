{
    "title": "Speaking Multiple Languages Affects the Moral Bias of Language Models. (arXiv:2211.07733v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained multilingual language models (PMLMs) are commonly used when dealing with data from multiple languages and cross-lingual transfer. However, PMLMs are trained on varying amounts of data for each language. In practice this means their performance is often much better on English than many other languages. We explore to what extent this also applies to moral norms. Do the models capture moral norms from English and impose them on other languages? Do the models exhibit random and thus potentially harmful beliefs in certain languages? Both these issues could negatively impact cross-lingual transfer and potentially lead to harmful outcomes. In this paper, we (1) apply the MoralDirection framework to multilingual models, comparing results in German, Czech, Arabic, Chinese, and English, (2) analyse model behaviour on filtered parallel subtitles corpora, and (3) apply the models to a Moral Foundations Questionnaire, comparing with human responses from different countries. Our experime",
    "link": "http://arxiv.org/abs/2211.07733",
    "context": "Title: Speaking Multiple Languages Affects the Moral Bias of Language Models. (arXiv:2211.07733v2 [cs.CL] UPDATED)\nAbstract: Pre-trained multilingual language models (PMLMs) are commonly used when dealing with data from multiple languages and cross-lingual transfer. However, PMLMs are trained on varying amounts of data for each language. In practice this means their performance is often much better on English than many other languages. We explore to what extent this also applies to moral norms. Do the models capture moral norms from English and impose them on other languages? Do the models exhibit random and thus potentially harmful beliefs in certain languages? Both these issues could negatively impact cross-lingual transfer and potentially lead to harmful outcomes. In this paper, we (1) apply the MoralDirection framework to multilingual models, comparing results in German, Czech, Arabic, Chinese, and English, (2) analyse model behaviour on filtered parallel subtitles corpora, and (3) apply the models to a Moral Foundations Questionnaire, comparing with human responses from different countries. Our experime",
    "path": "papers/22/11/2211.07733.json",
    "total_tokens": 1210,
    "translated_title": "多语言对语言模型的道德偏见产生影响",
    "translated_abstract": "预训练的多语言语言模型广泛应用于处理多种语言的数据和跨语言转移。然而，这些模型对于每种语言的数据训练量不同，使得它们在英语方面的表现往往要比其他语言好得多。我们研究这是否影响了这些模型对道德准则的捕捉和运用。这些模型是否从英语中捕捉到了道德准则并将其施加到其他语言中？这些模型是否在某些语言中表现出随意的、潜在有害的信念？这两个问题都可能对跨语言转移产生负面影响，从而可能导致有害的结果。本文中，我们运用“道德方向”框架比较了多语言模型在德语、捷克语、阿拉伯语、汉语和英语中的结果，分析了模型在过滤后的平行字幕语料库上的行为，并将模型应用于道德基础问卷，与来自不同国家的人类回答进行比较。我们的实验结果表明，预训练的多语言语言模型存在不同程度的道德偏见，且英语方面的偏见比其他语言更强。具体而言，我们发现在英语训练的多语言语言模型更倾向于优先考虑个体化的道德基础而非束缚性的道德基础。这对跨语言转移和设计伦理自然语言处理系统具有重要的影响。",
    "tldr": "本文研究发现，预训练的多语言语言模型具有不同程度的道德偏见，英语方面的偏见比其他语言更强。具体而言，在英语训练的多语言语言模型更倾向于优先考虑个体化的道德基础而非束缚性的道德基础，这对跨语言转移和设计伦理自然语言处理系统具有重要的影响。",
    "en_tdlr": "This paper finds that pre-trained multilingual language models exhibit varying degrees of moral bias, with biases much stronger for English than for other languages. Specifically, models trained on English tend to prioritize individualizing moral foundations over binding ones. This has important implications for cross-lingual transfer and the design of ethical natural language processing systems."
}