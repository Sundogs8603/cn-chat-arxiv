{
    "title": "SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v2 [cs.CV] UPDATED)",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to circumvent this need for fine-tuning still require access to images from the target distribution. In this paper, we pursue a different approach and explore the regime of training-free \"name-only transfer\" in which the only knowledge we possess about the downstream task comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks -- SuS and TIP-X, that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art zero-shot classification results on 19 benchmark datasets. ",
    "link": "http://arxiv.org/abs/2211.16198",
    "context": "Title: SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v2 [cs.CV] UPDATED)\nAbstract: Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to circumvent this need for fine-tuning still require access to images from the target distribution. In this paper, we pursue a different approach and explore the regime of training-free \"name-only transfer\" in which the only knowledge we possess about the downstream task comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks -- SuS and TIP-X, that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art zero-shot classification results on 19 benchmark datasets. ",
    "path": "papers/22/11/2211.16198.json",
    "total_tokens": 842,
    "translated_title": "SuS-X：无需训练的基于名称的视觉语言模型迁移方法",
    "translated_abstract": "对比语言-图像预训练（CLIP）已成为训练大规模视觉语言模型的一种简单而有效的方法。尽管CLIP在多种下游任务的零样本分类和检索方面展示出卓越的性能，但要发挥其全部潜力，微调仍然是必要的。微调整个CLIP模型会消耗资源且不稳定。此外，最近的方法虽然旨在避免对下游任务进行微调，但仍需要访问目标分布中的图像。本文探索了另一种方法——无需训练的“仅基于名称迁移”的方法。我们提出了一种新颖的方法SuS-X，由两个关键构建块——SuS和TIP-X组成，既不需要密集的微调，也不需要昂贵的标记数据。SuS-X在19个基准数据集上实现了最先进的零样本分类结果。",
    "tldr": "本文提出了SuS-X，一种无需训练的基于名称的视觉语言模型迁移方法，具有较高的零样本分类能力。",
    "en_tdlr": "This paper proposes SuS-X, a training-free name-only transfer method for vision-language models, which achieves state-of-the-art zero-shot classification results on 19 benchmark datasets."
}