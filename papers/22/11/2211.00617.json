{
    "title": "Convergence of policy gradient methods for finite-horizon stochastic linear-quadratic control problems. (arXiv:2211.00617v2 [math.OC] UPDATED)",
    "abstract": "We study the global linear convergence of policy gradient (PG) methods for finite-horizon continuous-time exploratory linear-quadratic control (LQC) problems. The setting includes stochastic LQC problems with indefinite costs and allows additional entropy regularisers in the objective. We consider a continuous-time Gaussian policy whose mean is linear in the state variable and whose covariance is state-independent. Contrary to discrete-time problems, the cost is noncoercive in the policy and not all descent directions lead to bounded iterates. We propose geometry-aware gradient descents for the mean and covariance of the policy using the Fisher geometry and the Bures-Wasserstein geometry, respectively. The policy iterates are shown to satisfy an a-priori bound, and converge globally to the optimal policy with a linear rate. We further propose a novel PG method with discrete-time policies. The algorithm leverages the continuous-time analysis, and achieves a robust linear convergence acr",
    "link": "http://arxiv.org/abs/2211.00617",
    "context": "Title: Convergence of policy gradient methods for finite-horizon stochastic linear-quadratic control problems. (arXiv:2211.00617v2 [math.OC] UPDATED)\nAbstract: We study the global linear convergence of policy gradient (PG) methods for finite-horizon continuous-time exploratory linear-quadratic control (LQC) problems. The setting includes stochastic LQC problems with indefinite costs and allows additional entropy regularisers in the objective. We consider a continuous-time Gaussian policy whose mean is linear in the state variable and whose covariance is state-independent. Contrary to discrete-time problems, the cost is noncoercive in the policy and not all descent directions lead to bounded iterates. We propose geometry-aware gradient descents for the mean and covariance of the policy using the Fisher geometry and the Bures-Wasserstein geometry, respectively. The policy iterates are shown to satisfy an a-priori bound, and converge globally to the optimal policy with a linear rate. We further propose a novel PG method with discrete-time policies. The algorithm leverages the continuous-time analysis, and achieves a robust linear convergence acr",
    "path": "papers/22/11/2211.00617.json",
    "total_tokens": 970,
    "translated_title": "这是一个有限时间随机线性二次控制问题的策略梯度方法的收敛性研究",
    "translated_abstract": "本文研究了有限时间连续时间探索性线性二次控制问题中策略梯度方法的全局线性收敛性。该设置包括具有非确定性成本和允许目标中添加额外的熵正则化项的随机线性二次控制问题。我们考虑了一个连续时间的高斯策略，其均值是状态变量的线性函数，方差与状态无关。与离散时间问题相反，策略中的成本函数不是严格凸函数，并且并非所有的下降方向都导致有界的迭代。我们提出了基于Fisher几何和Bures-Wasserstein几何的策略均值和协方差的几何感知梯度下降算法。策略迭代被证明满足先验界，并以线性速率全局收敛到最优策略。我们进一步提出了一种新的具有离散时间策略的PG方法。该算法利用了连续时间的分析结果，并实现了鲁棒的线性收敛。",
    "tldr": "本文研究了有限时间随机线性二次控制问题中策略梯度方法的全局线性收敛性，并提出了基于Fisher几何和Bures-Wasserstein几何的几何感知梯度下降算法，该算法以线性速率全局收敛到最优策略。"
}