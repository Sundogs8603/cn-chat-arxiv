{
    "title": "Towards Understanding Omission in Dialogue Summarization. (arXiv:2211.07145v2 [cs.CL] UPDATED)",
    "abstract": "Dialogue summarization aims to condense the lengthy dialogue into a concise summary, and has recently achieved significant progress. However, the result of existing methods is still far from satisfactory. Previous works indicated that omission is a major factor in affecting the quality of summarization, but few of them have further explored the omission problem, such as how omission affects summarization results and how to detect omission, which is critical for reducing omission and improving summarization quality. Moreover, analyzing and detecting omission relies on summarization datasets with omission labels (i.e., which dialogue utterances are omitted in the summarization), which are not available in the current literature. In this paper, we propose the OLDS dataset, which provides high-quality Omission Labels for Dialogue Summarization. By analyzing this dataset, we find that a large improvement in summarization quality can be achieved by providing ground-truth omission labels for ",
    "link": "http://arxiv.org/abs/2211.07145",
    "context": "Title: Towards Understanding Omission in Dialogue Summarization. (arXiv:2211.07145v2 [cs.CL] UPDATED)\nAbstract: Dialogue summarization aims to condense the lengthy dialogue into a concise summary, and has recently achieved significant progress. However, the result of existing methods is still far from satisfactory. Previous works indicated that omission is a major factor in affecting the quality of summarization, but few of them have further explored the omission problem, such as how omission affects summarization results and how to detect omission, which is critical for reducing omission and improving summarization quality. Moreover, analyzing and detecting omission relies on summarization datasets with omission labels (i.e., which dialogue utterances are omitted in the summarization), which are not available in the current literature. In this paper, we propose the OLDS dataset, which provides high-quality Omission Labels for Dialogue Summarization. By analyzing this dataset, we find that a large improvement in summarization quality can be achieved by providing ground-truth omission labels for ",
    "path": "papers/22/11/2211.07145.json",
    "total_tokens": 961,
    "translated_title": "探究对话摘要中的省略问题",
    "translated_abstract": "对话摘要的目标是将冗长的对话压缩成简练的摘要，并在近期取得了显著的进展。然而，现有方法的结果距离令人满意仍有很大差距。先前的研究表明，省略是影响摘要质量的主要因素之一，但是很少有研究进一步探讨省略问题，例如省略如何影响摘要结果以及如何检测省略问题，这对于减少省略并提高摘要质量至关重要。此外，分析和检测省略依赖于具有省略标签的摘要数据集（即，哪些对话话语在摘要中被省略），而当前文献中并没有这样的数据集。本文提出了OLDS数据集，为对话摘要提供了高质量的省略标签。通过分析该数据集，我们发现，通过为摘要模型提供真实省略标签，并在摘要过程中显式地建模和解决省略问题，可以大幅提高摘要质量。",
    "tldr": "本篇论文提出了OLDS数据集，用于为对话摘要提供高质量的省略标签。通过分析该数据集，发现通过为摘要模型提供真实省略标签，并在摘要过程中显式地建模和解决省略问题，可以大幅提高摘要质量。"
}