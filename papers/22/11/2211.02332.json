{
    "title": "Once-for-All Sequence Compression for Self-Supervised Speech Models. (arXiv:2211.02332v3 [cs.CL] UPDATED)",
    "abstract": "The sequence length along the time axis is often the dominant factor of the computation in speech processing. Works have been proposed to reduce the sequence length for lowering the computational cost in self-supervised speech models. However, different downstream tasks have different tolerance of sequence compressing, so a model that produces a fixed compressing rate may not fit all tasks. In this work, we introduce a once-for-all (OFA) sequence compression framework for self-supervised speech models that supports a continuous range of operating compressing rates. The framework is evaluated on various tasks, showing marginal degradation compared to the fixed compressing rate variants with a smooth performance-efficiency trade-off. We further explore adaptive compressing rate learning, demonstrating the ability to select task-specific preferred frame periods without needing a grid search.",
    "link": "http://arxiv.org/abs/2211.02332",
    "context": "Title: Once-for-All Sequence Compression for Self-Supervised Speech Models. (arXiv:2211.02332v3 [cs.CL] UPDATED)\nAbstract: The sequence length along the time axis is often the dominant factor of the computation in speech processing. Works have been proposed to reduce the sequence length for lowering the computational cost in self-supervised speech models. However, different downstream tasks have different tolerance of sequence compressing, so a model that produces a fixed compressing rate may not fit all tasks. In this work, we introduce a once-for-all (OFA) sequence compression framework for self-supervised speech models that supports a continuous range of operating compressing rates. The framework is evaluated on various tasks, showing marginal degradation compared to the fixed compressing rate variants with a smooth performance-efficiency trade-off. We further explore adaptive compressing rate learning, demonstrating the ability to select task-specific preferred frame periods without needing a grid search.",
    "path": "papers/22/11/2211.02332.json",
    "total_tokens": 803,
    "translated_title": "自监督语音模型一次性序列压缩",
    "translated_abstract": "在语音处理中，时间轴上的序列长度通常是计算的主要因素。为了降低自监督语音模型的计算成本，已经提出了一些方法来减少序列长度。然而，不同的下游任务对序列压缩有不同的容忍度，因此生产固定压缩率的模型可能不适用于所有任务。本文介绍了一种自监督语音模型的一次性序列压缩框架，支持连续的操作压缩率范围。该框架在各种任务上进行了评估，与固定压缩率变体相比，表现出平滑的性能效率权衡。我们进一步探讨了自适应压缩率学习，演示了选择任务特定的优先帧时期的能力，无需进行网格搜索。",
    "tldr": "本文提出了一种自监督语音模型的一次性序列压缩框架，该框架支持连续的操作压缩率范围，并在各种任务上展现出平滑的性能效率权衡。"
}