{
    "title": "SinDDM: A Single Image Denoising Diffusion Model. (arXiv:2211.16582v3 [cs.CV] UPDATED)",
    "abstract": "Denoising diffusion models (DDMs) have led to staggering performance leaps in image generation, editing and restoration. However, existing DDMs use very large datasets for training. Here, we introduce a framework for training a DDM on a single image. Our method, which we coin SinDDM, learns the internal statistics of the training image by using a multi-scale diffusion process. To drive the reverse diffusion process, we use a fully-convolutional light-weight denoiser, which is conditioned on both the noise level and the scale. This architecture allows generating samples of arbitrary dimensions, in a coarse-to-fine manner. As we illustrate, SinDDM generates diverse high-quality samples, and is applicable in a wide array of tasks, including style transfer and harmonization. Furthermore, it can be easily guided by external supervision. Particularly, we demonstrate text-guided generation from a single image using a pre-trained CLIP model.",
    "link": "http://arxiv.org/abs/2211.16582",
    "context": "Title: SinDDM: A Single Image Denoising Diffusion Model. (arXiv:2211.16582v3 [cs.CV] UPDATED)\nAbstract: Denoising diffusion models (DDMs) have led to staggering performance leaps in image generation, editing and restoration. However, existing DDMs use very large datasets for training. Here, we introduce a framework for training a DDM on a single image. Our method, which we coin SinDDM, learns the internal statistics of the training image by using a multi-scale diffusion process. To drive the reverse diffusion process, we use a fully-convolutional light-weight denoiser, which is conditioned on both the noise level and the scale. This architecture allows generating samples of arbitrary dimensions, in a coarse-to-fine manner. As we illustrate, SinDDM generates diverse high-quality samples, and is applicable in a wide array of tasks, including style transfer and harmonization. Furthermore, it can be easily guided by external supervision. Particularly, we demonstrate text-guided generation from a single image using a pre-trained CLIP model.",
    "path": "papers/22/11/2211.16582.json",
    "total_tokens": 930,
    "translated_title": "SinDDM: 一种单图像去噪扩散模型",
    "translated_abstract": "去噪扩散模型（DDMs）在图像生成、编辑和修复方面取得了惊人的性能提升。但是，现有的DDMs需要使用非常大的数据集进行训练。本文介绍一种在单张图像上训练DDM的框架，称之为SinDDM。我们的方法通过使用多尺度扩散过程学习训练图像的内部统计信息。为了推动反向扩散过程，我们使用了一个完全卷积的轻量级去噪器，它受噪声水平和尺度的调节。该体系结构允许以粗略到细节的方式生成任意维度的样本。正如我们所说明的，SinDDM生成多样化的高质量样本，在包括样式转换和谐调在内的广泛任务中都适用。此外，它可以轻松地受到外部监督的指导。特别地，我们展示了如何使用预训练的CLIP模型从单张图像进行文本引导生成。",
    "tldr": "本文介绍了一种称为SinDDM的单图像去噪扩散模型。该模型使用单张图像进行训练，并可生成高质量的任意维度的样本。此外，它还可以通过外部监督进行指导，可用于多种任务，包括样式转换和文本引导生成。"
}