{
    "title": "Nonlinear Advantage: Trained Networks Might Not Be As Complex as You Think. (arXiv:2211.17180v2 [cs.LG] UPDATED)",
    "abstract": "We perform an empirical study of the behaviour of deep networks when fully linearizing some of its feature channels through a sparsity prior on the overall number of nonlinear units in the network. In experiments on image classification and machine translation tasks, we investigate how much we can simplify the network function towards linearity before performance collapses. First, we observe a significant performance gap when reducing nonlinearity in the network function early on as opposed to late in training, in-line with recent observations on the time-evolution of the data-dependent NTK. Second, we find that after training, we are able to linearize a significant number of nonlinear units while maintaining a high performance, indicating that much of a network's expressivity remains unused but helps gradient descent in early stages of training. To characterize the depth of the resulting partially linearized network, we introduce a measure called average path length, representing the ",
    "link": "http://arxiv.org/abs/2211.17180",
    "context": "Title: Nonlinear Advantage: Trained Networks Might Not Be As Complex as You Think. (arXiv:2211.17180v2 [cs.LG] UPDATED)\nAbstract: We perform an empirical study of the behaviour of deep networks when fully linearizing some of its feature channels through a sparsity prior on the overall number of nonlinear units in the network. In experiments on image classification and machine translation tasks, we investigate how much we can simplify the network function towards linearity before performance collapses. First, we observe a significant performance gap when reducing nonlinearity in the network function early on as opposed to late in training, in-line with recent observations on the time-evolution of the data-dependent NTK. Second, we find that after training, we are able to linearize a significant number of nonlinear units while maintaining a high performance, indicating that much of a network's expressivity remains unused but helps gradient descent in early stages of training. To characterize the depth of the resulting partially linearized network, we introduce a measure called average path length, representing the ",
    "path": "papers/22/11/2211.17180.json",
    "total_tokens": 885,
    "tldr": "本文通过实证研究发现在特征通道上施加稀疏性限制后，可以将网络函数线性化，而仍然保持较好的性能，这表明网络的表达能力没有充分发挥。"
}