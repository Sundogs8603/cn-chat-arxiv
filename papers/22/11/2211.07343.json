{
    "title": "Replacing Language Model for Style Transfer",
    "abstract": "arXiv:2211.07343v2 Announce Type: replace  Abstract: We introduce replacing language model (RLM), a sequence-to-sequence language modeling framework for text style transfer (TST). Our method autoregressively replaces each token of the source sentence with a text span that has a similar meaning but in the target style. The new span is generated via a non-autoregressive masked language model, which can better preserve the local-contextual meaning of the replaced token. This RLM generation scheme gathers the flexibility of autoregressive models and the accuracy of non-autoregressive models, which bridges the gap between sentence-level and word-level style transfer methods. To control the generation style more precisely, we conduct a token-level style-content disentanglement on the hidden representations of RLM. Empirical results on real-world text datasets demonstrate the effectiveness of RLM compared with other TST baselines. The code is at https://github.com/Linear95/RLM.",
    "link": "https://arxiv.org/abs/2211.07343",
    "context": "Title: Replacing Language Model for Style Transfer\nAbstract: arXiv:2211.07343v2 Announce Type: replace  Abstract: We introduce replacing language model (RLM), a sequence-to-sequence language modeling framework for text style transfer (TST). Our method autoregressively replaces each token of the source sentence with a text span that has a similar meaning but in the target style. The new span is generated via a non-autoregressive masked language model, which can better preserve the local-contextual meaning of the replaced token. This RLM generation scheme gathers the flexibility of autoregressive models and the accuracy of non-autoregressive models, which bridges the gap between sentence-level and word-level style transfer methods. To control the generation style more precisely, we conduct a token-level style-content disentanglement on the hidden representations of RLM. Empirical results on real-world text datasets demonstrate the effectiveness of RLM compared with other TST baselines. The code is at https://github.com/Linear95/RLM.",
    "path": "papers/22/11/2211.07343.json",
    "total_tokens": 838,
    "translated_title": "替换语言模型用于文本风格转换",
    "translated_abstract": "我们引入了一种称为替换语言模型（RLM）的序列到序列语言建模框架，用于文本风格转换（TST）。我们的方法自回归地将源句子的每个标记替换为具有类似含义但具有目标风格的文本片段。新的片段是通过非自回归掩蔽语言模型生成的，可以更好地保留替换标记的局部上下文含义。这种RLM生成方案汇集了自回归模型的灵活性和非自回归模型的准确性，弥合了句子级和词级风格转换方法之间的差距。为了更精确地控制生成风格，我们在RLM的隐藏表示上进行了标记级风格内容解缠。实证结果表明，与其他TST基线相比，RLM在真实文本数据集上的有效性。代码在https://github.com/Linear95/RLM。",
    "tldr": "提出了一种替换语言模型（RLM），结合了自回归模型的灵活性和非自回归模型的准确性，在文本风格转换中实现了更精确的生成控制。",
    "en_tdlr": "Introducing a replacing language model (RLM) that combines the flexibility of autoregressive models and the accuracy of non-autoregressive models, achieving more precise generation control in text style transfer."
}