{
    "title": "Variants of SGD for Lipschitz Continuous Loss Functions in Low-Precision Environments. (arXiv:2211.04655v4 [math.OC] UPDATED)",
    "abstract": "Motivated by neural network training in low-bit floating and fixed-point environments, this work studies the convergence of variants of SGD using adaptive step sizes with computational error. Considering a general stochastic Lipschitz continuous loss function, an asymptotic convergence result to a Clarke stationary point, and the non-asymptotic convergence to an approximate stationary point are presented assuming that only an approximation of the loss function's stochastic gradient can be computed, as well as error in computing the SGD step itself. Different variants of SGD are tested empirically in a variety of low-precision arithmetic environments, where improved test set accuracy is observed compared to SGD for two image recognition tasks.",
    "link": "http://arxiv.org/abs/2211.04655",
    "context": "Title: Variants of SGD for Lipschitz Continuous Loss Functions in Low-Precision Environments. (arXiv:2211.04655v4 [math.OC] UPDATED)\nAbstract: Motivated by neural network training in low-bit floating and fixed-point environments, this work studies the convergence of variants of SGD using adaptive step sizes with computational error. Considering a general stochastic Lipschitz continuous loss function, an asymptotic convergence result to a Clarke stationary point, and the non-asymptotic convergence to an approximate stationary point are presented assuming that only an approximation of the loss function's stochastic gradient can be computed, as well as error in computing the SGD step itself. Different variants of SGD are tested empirically in a variety of low-precision arithmetic environments, where improved test set accuracy is observed compared to SGD for two image recognition tasks.",
    "path": "papers/22/11/2211.04655.json",
    "total_tokens": 777,
    "translated_title": "低精度环境下利普希茨连续损失函数的SGD变种",
    "translated_abstract": "本文研究了在低位浮点和定点环境下神经网络训练时使用自适应步长的SGD变种的收敛性。考虑到一般随机利普希茨连续的损失函数，假设只能计算损失函数随机梯度的近似值以及计算SGD步骤本身时的误差，给出了一个渐近收敛到Clarke稳定点的结果和到近似稳定点的非渐近收敛。在各种低精度算术环境下经验地测试了不同的SGD变种，在两个图像识别任务中与SGD相比观察到了改进的测试集准确性。",
    "tldr": "本文研究了在低精度环境下神经网络训练的SGD变种，并测试了不同变种的效果。结果表明，相比于传统的SGD方法，在低精度算术环境下使用自适应步长的SGD变种可以获得更好的测试集准确性。",
    "en_tdlr": "The paper studies variants of SGD for neural network training in low-precision environments and presents empirical results showing that using adaptive step sizes with computational error can improve test set accuracy compared to traditional SGD methods."
}