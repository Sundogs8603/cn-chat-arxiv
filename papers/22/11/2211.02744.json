{
    "title": "KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction. (arXiv:2211.02744v2 [cs.CL] UPDATED)",
    "abstract": "The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the sta",
    "link": "http://arxiv.org/abs/2211.02744",
    "context": "Title: KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction. (arXiv:2211.02744v2 [cs.CL] UPDATED)\nAbstract: The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the sta",
    "path": "papers/22/11/2211.02744.json",
    "total_tokens": 911,
    "translated_title": "KGLM: 将知识图谱结构整合进语言模型以进行链接预测",
    "translated_abstract": "知识图谱能够在大规模情况下表示复杂关系，已被广泛应用于知识表示、问答和推荐系统等领域。然而，知识图谱通常存在信息缺失的问题，需要进行知识图谱补全任务。预训练和微调的语言模型已经在这些任务中表现出出色的效果，尽管这些模型忽略了知识图谱所蕴含的固有信息，即实体和关系类型。因此，我们提出了KGLM（Knowledge Graph Language Model）架构，其中引入了一个新的实体/关系嵌入层，它学习区分不同的实体和关系类型，使得模型能够学习知识图谱的结构。在本文中，我们展示了使用从知识图谱提取的三元组进一步预训练这些额外嵌入层的语言模型，然后采用后续的 link prediction 任务表现出了良好的效果。",
    "tldr": "本文提出了 KGLM 架构，将新的实体/关系嵌入层整合进语言模型，使其能够学习知识图谱的结构。使用从知识图谱提取的三元组对模型进行预训练并进行链接预测任务得到了良好效果。",
    "en_tdlr": "This paper proposes the KGLM architecture, which integrates a new entity/relation embedding layer into the language model to enable it to learn the structure of the knowledge graph. Pre-training the model with triples extracted from the knowledge graph, followed by link prediction tasks, showed promising results."
}