{
    "title": "Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors. (arXiv:2211.12005v3 [cs.LG] UPDATED)",
    "abstract": "As data becomes increasingly vital, a company would be very cautious about releasing data, because the competitors could use it to train high-performance models, thereby posing a tremendous threat to the company's commercial competence. To prevent training good models on the data, we could add imperceptible perturbations to it. Since such perturbations aim at hurting the entire training process, they should reflect the vulnerability of DNN training, rather than that of a single model. Based on this new idea, we seek perturbed examples that are always unrecognized (never correctly classified) in training. In this paper, we uncover them by model checkpoints' gradients, forming the proposed self-ensemble protection (SEP), which is very effective because (1) learning on examples ignored during normal training tends to yield DNNs ignoring normal examples; (2) checkpoints' cross-model gradients are close to orthogonal, meaning that they are as diverse as DNNs with different architectures. Th",
    "link": "http://arxiv.org/abs/2211.12005",
    "context": "Title: Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors. (arXiv:2211.12005v3 [cs.LG] UPDATED)\nAbstract: As data becomes increasingly vital, a company would be very cautious about releasing data, because the competitors could use it to train high-performance models, thereby posing a tremendous threat to the company's commercial competence. To prevent training good models on the data, we could add imperceptible perturbations to it. Since such perturbations aim at hurting the entire training process, they should reflect the vulnerability of DNN training, rather than that of a single model. Based on this new idea, we seek perturbed examples that are always unrecognized (never correctly classified) in training. In this paper, we uncover them by model checkpoints' gradients, forming the proposed self-ensemble protection (SEP), which is very effective because (1) learning on examples ignored during normal training tends to yield DNNs ignoring normal examples; (2) checkpoints' cross-model gradients are close to orthogonal, meaning that they are as diverse as DNNs with different architectures. Th",
    "path": "papers/22/11/2211.12005.json",
    "total_tokens": 964,
    "translated_title": "自我集成保护：训练检查点是良好的数据保护者",
    "translated_abstract": "随着数据变得越来越重要，公司在发布数据时通常会非常谨慎，因为竞争对手可以使用它来训练高性能模型，从而对公司的商业竞争力造成巨大威胁。为了防止在数据上训练良好的模型，我们可以对其添加不可感知的扰动。由于这样的扰动旨在伤害整个训练过程，因此它们应该反映DNN训练的脆弱性，而不是单个模型的脆弱性。基于这个新想法，我们寻找在训练中始终无法识别（从未被正确分类）的扰动样本。在本文中，我们通过模型检查点的梯度发现这些样本，形成所提出的自我集成保护（SEP）。该方法非常有效，因为（1）在正常训练过程中忽略的示例上进行学习往往会产生不忽略正常示例的DNN；（2）检查点之间跨模型的梯度与正交接近，表示它们与具有不同体系结构的DNN一样多样化。",
    "tldr": "本文提出自我集成保护技术，通过对训练数据添加不可感知的扰动，通过模型检查点的梯度发现这些样本，可以有效地防止竞争对手在数据上训练高性能模型。",
    "en_tdlr": "This paper proposes a self-ensemble protection technique to effectively prevent competitors from training high-performance models on sensitive data, by adding imperceptible perturbations and using gradients from model checkpoints to discover unrecognized examples, thus reflecting the vulnerability of DNN training."
}