{
    "title": "One-shot Implicit Animatable Avatars with Model-based Priors. (arXiv:2212.02469v2 [cs.CV] UPDATED)",
    "abstract": "Existing neural rendering methods for creating human avatars typically either require dense input signals such as video or multi-view images, or leverage a learned prior from large-scale specific 3D human datasets such that reconstruction can be performed with sparse-view inputs. Most of these methods fail to achieve realistic reconstruction when only a single image is available. To enable the data-efficient creation of realistic animatable 3D humans, we propose ELICIT, a novel method for learning human-specific neural radiance fields from a single image. Inspired by the fact that humans can effortlessly estimate the body geometry and imagine full-body clothing from a single image, we leverage two priors in ELICIT: 3D geometry prior and visual semantic prior. Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned vertex-based template model (i.e., SMPL) and implements the visual clothing semantic prior with the CLIP-based pre-trained models. Both priors are used ",
    "link": "http://arxiv.org/abs/2212.02469",
    "context": "Title: One-shot Implicit Animatable Avatars with Model-based Priors. (arXiv:2212.02469v2 [cs.CV] UPDATED)\nAbstract: Existing neural rendering methods for creating human avatars typically either require dense input signals such as video or multi-view images, or leverage a learned prior from large-scale specific 3D human datasets such that reconstruction can be performed with sparse-view inputs. Most of these methods fail to achieve realistic reconstruction when only a single image is available. To enable the data-efficient creation of realistic animatable 3D humans, we propose ELICIT, a novel method for learning human-specific neural radiance fields from a single image. Inspired by the fact that humans can effortlessly estimate the body geometry and imagine full-body clothing from a single image, we leverage two priors in ELICIT: 3D geometry prior and visual semantic prior. Specifically, ELICIT utilizes the 3D body shape geometry prior from a skinned vertex-based template model (i.e., SMPL) and implements the visual clothing semantic prior with the CLIP-based pre-trained models. Both priors are used ",
    "path": "papers/22/12/2212.02469.json",
    "total_tokens": 956,
    "translated_title": "带有基于模型先验的一次性隐式动画化头像制作方法",
    "translated_abstract": "现有的创建人类头像的神经渲染方法通常要么需要稠密输入信号（如视频或多视角图像），要么利用从大规模特定3D人体数据集中学到的先验，使得可以使用稀疏视角输入进行重建。大多数这些方法在仅有一张图像时无法实现逼真重建。为了实现数据高效的逼真可动3D人体的创建，我们提出了ELICIT，这是一种从一张图片学习人体特定神经辐射场的新方法。受到人类可以轻松估计身体几何形状并从一张图片中想象造型完整的衣柜的启示，我们在ELICIT中利用了两个先验：3D几何先验和视觉语义先验。具体来说，ELICIT利用一个蒙皮顶点模板模型（即SMPL）的3D身体形状几何先验，并通过基于CLIP的预训练模型实现了视觉服装语义先验。这两个先验均用于从单个图像进行逼真的可动3D重建。",
    "tldr": "本文提出了ELICIT，一种从单张图片学习人类特定神经辐射场的方法，同时利用3D几何先验和视觉语义先验实现了一次性数据高效的逼真可动3D人体的创建。",
    "en_tdlr": "This paper proposes ELICIT, a novel method for learning human-specific neural radiance fields from a single image, and utilizes both 3D geometry and visual semantic priors to enable data-efficient creation of realistic animatable 3D humans."
}