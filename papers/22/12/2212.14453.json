{
    "title": "Learning Multimodal Data Augmentation in Feature Space. (arXiv:2212.14453v2 [cs.LG] UPDATED)",
    "abstract": "The ability to jointly learn from multiple modalities, such as text, audio, and visual data, is a defining feature of intelligent systems. While there have been promising advances in designing neural networks to harness multimodal data, the enormous success of data augmentation currently remains limited to single-modality tasks like image classification. Indeed, it is particularly difficult to augment each modality while preserving the overall semantic structure of the data; for example, a caption may no longer be a good description of an image after standard augmentations have been applied, such as translation. Moreover, it is challenging to specify reasonable transformations that are not tailored to a particular modality. In this paper, we introduce LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that automatically learns to jointly augment multimodal data in feature space, with no constraints on the identities of the modalities or the relationship between modalit",
    "link": "http://arxiv.org/abs/2212.14453",
    "context": "Title: Learning Multimodal Data Augmentation in Feature Space. (arXiv:2212.14453v2 [cs.LG] UPDATED)\nAbstract: The ability to jointly learn from multiple modalities, such as text, audio, and visual data, is a defining feature of intelligent systems. While there have been promising advances in designing neural networks to harness multimodal data, the enormous success of data augmentation currently remains limited to single-modality tasks like image classification. Indeed, it is particularly difficult to augment each modality while preserving the overall semantic structure of the data; for example, a caption may no longer be a good description of an image after standard augmentations have been applied, such as translation. Moreover, it is challenging to specify reasonable transformations that are not tailored to a particular modality. In this paper, we introduce LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that automatically learns to jointly augment multimodal data in feature space, with no constraints on the identities of the modalities or the relationship between modalit",
    "path": "papers/22/12/2212.14453.json",
    "total_tokens": 919,
    "translated_title": "在特征空间中学习多模态数据增强",
    "translated_abstract": "能够联合学习多个模态（如文本、音频和视觉数据）是智能系统的一个核心特征。尽管设计神经网络来利用多模态数据取得了令人鼓舞的进展，但数据增强的巨大成功仍然局限于单模态任务，如图像分类。确实，在保留数据的整体语义结构的同时增强每个模态是特别困难的；例如，在应用了标准增强方法（例如翻译）之后，标题可能不再是图像的良好描述。此外，仍然很难指定不针对特定模态的合理变换。本文介绍了一种易于使用的方法，名为LeMDA（Learning Multimodal Data Augmentation），它在特征空间中自动学习联合增强多模态数据，而不限制模态的身份或模态之间的关系。我们在标准基准测试上展示了我们方法的有效性，证明LeMDA在多个领域（包括图像字幕和语音识别）中提高了多模态学习算法的性能。",
    "tldr": "本文介绍了一种名为LeMDA的易于使用的方法，它在特征空间中自动学习联合增强多模态数据，提高了多模态学习算法的性能。"
}