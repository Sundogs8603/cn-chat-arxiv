{
    "title": "What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?. (arXiv:2212.02705v4 [cs.AI] UPDATED)",
    "abstract": "Various methods for Multi-Agent Reinforcement Learning (MARL) have been developed with the assumption that agents' policies are based on accurate state information. However, policies learned through Deep Reinforcement Learning (DRL) are susceptible to adversarial state perturbation attacks. In this work, we propose a State-Adversarial Markov Game (SAMG) and make the first attempt to investigate the fundamental properties of MARL under state uncertainties. Our analysis shows that the commonly used solution concepts of optimal agent policy and robust Nash equilibrium do not always exist in SAMGs. To circumvent this difficulty, we consider a new solution concept called robust agent policy, where agents aim to maximize the worst-case expected state value. We prove the existence of robust agent policy for finite state and finite action SAMGs. Additionally, we propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents under state uncertai",
    "link": "http://arxiv.org/abs/2212.02705",
    "context": "Title: What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?. (arXiv:2212.02705v4 [cs.AI] UPDATED)\nAbstract: Various methods for Multi-Agent Reinforcement Learning (MARL) have been developed with the assumption that agents' policies are based on accurate state information. However, policies learned through Deep Reinforcement Learning (DRL) are susceptible to adversarial state perturbation attacks. In this work, we propose a State-Adversarial Markov Game (SAMG) and make the first attempt to investigate the fundamental properties of MARL under state uncertainties. Our analysis shows that the commonly used solution concepts of optimal agent policy and robust Nash equilibrium do not always exist in SAMGs. To circumvent this difficulty, we consider a new solution concept called robust agent policy, where agents aim to maximize the worst-case expected state value. We prove the existence of robust agent policy for finite state and finite action SAMGs. Additionally, we propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents under state uncertai",
    "path": "papers/22/12/2212.02705.json",
    "total_tokens": 1075,
    "translated_title": "怎样解决面对对抗状态的多智能体强化学习问题？",
    "translated_abstract": "在多智能体强化学习(MARL)中，已经开发了各种方法，假设智能体的策略基于准确的状态信息。然而，通过深度强化学习(DRL)学习的策略容易受到对抗性状态扰动攻击的影响。在这项工作中，我们提出了一种状态对抗性马尔科夫博弈(SAMG)，并首次尝试研究状态不确定性下MARL的基本属性。我们的分析表明，在SAMG中，通常使用的最优智能体策略和鲁棒纳什均衡解决概念并不总是存在的。为了克服这个困难，我们考虑了一种称为鲁棒智能体策略的新解决概念，其中智能体的目标是最大化最坏情况下的预期状态值。我们证明了有限状态和有限动作SAMG中存在鲁棒智能体策略。此外，我们提出了一种名为鲁棒多智能体对抗性演员-评论家(RMA3C)算法，用于学习在状态不确定性下的MARL智能体的鲁棒性策略。",
    "tldr": "本文提出了一种解决面对对抗性状态的多智能体强化学习问题的方法，通过引入状态对抗性马尔科夫博弈，提出了鲁棒智能体策略的概念，并证明了其在有限状态和有限动作情况下的存在性。此外，还提出了一种鲁棒多智能体对抗性演员-评论家算法，用于学习在状态不确定性下的鲁棒性策略。"
}