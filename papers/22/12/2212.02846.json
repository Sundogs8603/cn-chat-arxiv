{
    "title": "Statistical mechanics of continual learning: variational principle and mean-field potential. (arXiv:2212.02846v3 [cond-mat.stat-mech] UPDATED)",
    "abstract": "An obstacle to artificial general intelligence is set by the continual learning of multiple tasks of different nature. Recently, various heuristic tricks, both from machine learning and from neuroscience angles, were proposed, but they lack a unified theory ground. Here, we focus on the continual learning in single-layered and multi-layered neural networks of binary weights. A variational Bayesian learning setting is thus proposed, where the neural network is trained in a field-space, rather than the gradient-ill-defined discrete-weight space, and furthermore, the weight uncertainty is naturally incorporated, and modulates the synaptic resources among tasks. From a physics perspective, we translate the variational continual learning into the Franz-Parisi thermodynamic potential framework, where the previous task knowledge acts as a prior and a reference as well. We thus interprete the continual learning of the binary perceptron in a teacher-student setting as a Franz-Parisi potential c",
    "link": "http://arxiv.org/abs/2212.02846",
    "context": "Title: Statistical mechanics of continual learning: variational principle and mean-field potential. (arXiv:2212.02846v3 [cond-mat.stat-mech] UPDATED)\nAbstract: An obstacle to artificial general intelligence is set by the continual learning of multiple tasks of different nature. Recently, various heuristic tricks, both from machine learning and from neuroscience angles, were proposed, but they lack a unified theory ground. Here, we focus on the continual learning in single-layered and multi-layered neural networks of binary weights. A variational Bayesian learning setting is thus proposed, where the neural network is trained in a field-space, rather than the gradient-ill-defined discrete-weight space, and furthermore, the weight uncertainty is naturally incorporated, and modulates the synaptic resources among tasks. From a physics perspective, we translate the variational continual learning into the Franz-Parisi thermodynamic potential framework, where the previous task knowledge acts as a prior and a reference as well. We thus interprete the continual learning of the binary perceptron in a teacher-student setting as a Franz-Parisi potential c",
    "path": "papers/22/12/2212.02846.json",
    "total_tokens": 930,
    "translated_title": "连续学习的统计力学:变分原理和平均场势",
    "translated_abstract": "人工智能通用性的一个障碍是多种不同任务的连续学习。最近，涉及机器学习和神经科学的各种启发性技巧被提出，但它们缺乏一个统一的理论基础。本文关注二元权重的单层和多层神经网络的连续学习。提出了一个变分贝叶斯学习设置，其中神经网络在场空间而不是渐变未定义的离散权重空间中进行训练，并且自然地将权重不确定性合并，并调节任务间的突触资源。从物理学的角度，我们将变分的连续学习转化为Franz-Parisi热力学势的框架，其中以前的任务知识充当先验和参考。因此，我们将一个教师-学生设置中的二元感知器的连续学习解释为一个Franz-Parisi势。",
    "tldr": "从物理学的角度将连续学习的问题转化为Franz-Parisi热力学势的框架，将之前学习到的任务作为先验和参考，提出了一个在场空间中训练神经网络的变分贝叶斯学习设置，用于调节任务间的突触资源。",
    "en_tdlr": "A variational Bayesian learning setting is proposed to address the problem of continual learning in neural networks, which is translated into the Franz-Parisi thermodynamic potential framework from a physics perspective. The setting incorporates weight uncertainty and modulates the synaptic resources among tasks by training the network in a field-space. The previous task knowledge is applied as a prior and reference in the continual learning process."
}