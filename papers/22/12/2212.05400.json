{
    "title": "How to Backdoor Diffusion Models?. (arXiv:2212.05400v2 [cs.CV] UPDATED)",
    "abstract": "Diffusion models are state-of-the-art deep learning empowered generative models that are trained based on the principle of learning forward and reverse diffusion processes via progressive noise-addition and denoising. To gain a better understanding of the limitations and potential risks, this paper presents the first study on the robustness of diffusion models against backdoor attacks. Specifically, we propose BadDiffusion, a novel attack framework that engineers compromised diffusion processes during model training for backdoor implantation. At the inference stage, the backdoored diffusion model will behave just like an untampered generator for regular data inputs, while falsely generating some targeted outcome designed by the bad actor upon receiving the implanted trigger signal. Such a critical risk can be dreadful for downstream tasks and applications built upon the problematic model. Our extensive experiments on various backdoor attack settings show that BadDiffusion can consisten",
    "link": "http://arxiv.org/abs/2212.05400",
    "context": "Title: How to Backdoor Diffusion Models?. (arXiv:2212.05400v2 [cs.CV] UPDATED)\nAbstract: Diffusion models are state-of-the-art deep learning empowered generative models that are trained based on the principle of learning forward and reverse diffusion processes via progressive noise-addition and denoising. To gain a better understanding of the limitations and potential risks, this paper presents the first study on the robustness of diffusion models against backdoor attacks. Specifically, we propose BadDiffusion, a novel attack framework that engineers compromised diffusion processes during model training for backdoor implantation. At the inference stage, the backdoored diffusion model will behave just like an untampered generator for regular data inputs, while falsely generating some targeted outcome designed by the bad actor upon receiving the implanted trigger signal. Such a critical risk can be dreadful for downstream tasks and applications built upon the problematic model. Our extensive experiments on various backdoor attack settings show that BadDiffusion can consisten",
    "path": "papers/22/12/2212.05400.json",
    "total_tokens": 936,
    "translated_title": "如何后门扩散模型？",
    "translated_abstract": "扩散模型是最先进的基于深度学习的生成模型，其训练原理是通过逐步添加噪声和去噪学习正向和反向的扩散过程。为了更好地了解其限制和潜在风险，本文首次研究了扩散模型对后门攻击的鲁棒性。具体而言，我们提出了BadDiffusion，这是一个新的攻击框架，它在模型训练期间工程化了受损的扩散过程，进行后门植入。在推理阶段，后门扩散模型将像普通数据输入的未篡改生成器一样运行，同时在接收到植入的触发信号后，伪造出一些被坏演员设计的目标结果。这种重大风险可能对建立在有问题的模型之上的下游任务和应用造成严重影响。我们在各种后门攻击设置上进行了广泛的实验，结果表明 BadDiffusion 可以稳定地伪造特定的目标结果。",
    "tldr": "该论文介绍了针对扩散模型的后门攻击框架 BadDiffusion，其可以在模型训练期间实现植入后门，导致模型在正常数据输入时依然表现良好，但接收到触发信号时产生误导性输出。该攻击可能对建立在有问题的模型之上的下游任务和应用造成严重影响。",
    "en_tdlr": "This paper presents a novel attack framework called BadDiffusion, which can engineer compromised diffusion processes during model training for backdoor implantation, leading to misleading outputs upon receiving the implanted trigger signal. The proposed attack may have critical risks for downstream tasks and applications."
}