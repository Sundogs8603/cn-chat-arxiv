{
    "title": "On the Compatibility between Neural Networks and Partial Differential Equations for Physics-informed Learning. (arXiv:2212.00270v2 [physics.comp-ph] UPDATED)",
    "abstract": "We shed light on a pitfall and an opportunity in physics-informed neural networks (PINNs). We prove that a multilayer perceptron (MLP) only with ReLU (Rectified Linear Unit) or ReLU-like Lipschitz activation functions will always lead to a vanished Hessian. Such a network-imposed constraint contradicts any second- or higher-order partial differential equations (PDEs). Therefore, a ReLU-based MLP cannot form a permissible function space for the approximation of their solutions. Inspired by this pitfall, we prove that a linear PDE up to the $n$-th order can be strictly satisfied by an MLP with $C^n$ activation functions when the weights of its output layer lie on a certain hyperplane, as called the out-layer-hyperplane. An MLP equipped with the out-layer-hyperplane becomes \"physics-enforced\", no longer requiring a loss function for the PDE itself (but only those for the initial and boundary conditions). Such a hyperplane exists not only for MLPs but for any network architecture tailed by",
    "link": "http://arxiv.org/abs/2212.00270",
    "context": "Title: On the Compatibility between Neural Networks and Partial Differential Equations for Physics-informed Learning. (arXiv:2212.00270v2 [physics.comp-ph] UPDATED)\nAbstract: We shed light on a pitfall and an opportunity in physics-informed neural networks (PINNs). We prove that a multilayer perceptron (MLP) only with ReLU (Rectified Linear Unit) or ReLU-like Lipschitz activation functions will always lead to a vanished Hessian. Such a network-imposed constraint contradicts any second- or higher-order partial differential equations (PDEs). Therefore, a ReLU-based MLP cannot form a permissible function space for the approximation of their solutions. Inspired by this pitfall, we prove that a linear PDE up to the $n$-th order can be strictly satisfied by an MLP with $C^n$ activation functions when the weights of its output layer lie on a certain hyperplane, as called the out-layer-hyperplane. An MLP equipped with the out-layer-hyperplane becomes \"physics-enforced\", no longer requiring a loss function for the PDE itself (but only those for the initial and boundary conditions). Such a hyperplane exists not only for MLPs but for any network architecture tailed by",
    "path": "papers/22/12/2212.00270.json",
    "total_tokens": 1068,
    "translated_title": "关于神经网络和偏微分方程在物理学习中的兼容性",
    "translated_abstract": "本文探讨了物理知识神经网络的缺陷和机遇。我们证明了一个只使用ReLU（修正线性单元）或类ReLU的Lipschitz激活函数的多层感知器（MLP）将始终导致海森矩阵消失。这种网络所施加的限制与任何二阶或更高阶的偏微分方程（PDE）相矛盾。因此，基于ReLU的MLP不能形成解决方案的合法函数空间。在这个缺陷的启发下，我们证明了一个具有$C^n$激活函数的MLP，当其输出层的权重位于某个超平面（称为输出层超平面）上时，可以严格满足一个线性PDE直到 $n$ 阶。配备输出层超平面的MLP变得“物理强制执行”，不再需要针对PDE本身的损失函数（只需要初边值条件的损失函数）。这样的超平面不仅存在于MLP中，而且存在于任何网络架构的尾部。",
    "tldr": "本文探讨了物理知识神经网络的缺陷和机遇，证明了基于ReLU的MLP不能形成解决方案的合法函数空间，而使用带有输出层超平面的具有$C^n$激活函数的MLP可以严格满足一个线性PDE直到 $n$ 阶。",
    "en_tdlr": "This paper explores the pitfall and opportunity in Physics-informed neural networks (PINNs), proving that an MLP only with ReLU or ReLU-like Lipschitz activation functions will always lead to a vanished Hessian, and can't form a permissible function space for the approximation of solutions of partial differential equations. The paper also proves that a linear PDE up to the n-th order can be strictly satisfied by an MLP with C^n activation functions when the weights of its output layer lie on a certain hyperplane, which is called the out-layer-hyperplane, making the MLP \"physics-enforced\", no longer requiring a loss function for the PDE itself."
}