{
    "title": "Bayesian Interpolation with Deep Linear Networks. (arXiv:2212.14457v3 [stat.ML] UPDATED)",
    "abstract": "Characterizing how neural network depth, width, and dataset size jointly impact model quality is a central problem in deep learning theory. We give here a complete solution in the special case of linear networks with output dimension one trained using zero noise Bayesian inference with Gaussian weight priors and mean squared error as a negative log-likelihood. For any training dataset, network depth, and hidden layer widths, we find non-asymptotic expressions for the predictive posterior and Bayesian model evidence in terms of Meijer-G functions, a class of meromorphic special functions of a single complex variable. Through novel asymptotic expansions of these Meijer-G functions, a rich new picture of the joint role of depth, width, and dataset size emerges. We show that linear networks make provably optimal predictions at infinite depth: the posterior of infinitely deep linear networks with data-agnostic priors is the same as that of shallow networks with evidence-maximizing data-depe",
    "link": "http://arxiv.org/abs/2212.14457",
    "context": "Title: Bayesian Interpolation with Deep Linear Networks. (arXiv:2212.14457v3 [stat.ML] UPDATED)\nAbstract: Characterizing how neural network depth, width, and dataset size jointly impact model quality is a central problem in deep learning theory. We give here a complete solution in the special case of linear networks with output dimension one trained using zero noise Bayesian inference with Gaussian weight priors and mean squared error as a negative log-likelihood. For any training dataset, network depth, and hidden layer widths, we find non-asymptotic expressions for the predictive posterior and Bayesian model evidence in terms of Meijer-G functions, a class of meromorphic special functions of a single complex variable. Through novel asymptotic expansions of these Meijer-G functions, a rich new picture of the joint role of depth, width, and dataset size emerges. We show that linear networks make provably optimal predictions at infinite depth: the posterior of infinitely deep linear networks with data-agnostic priors is the same as that of shallow networks with evidence-maximizing data-depe",
    "path": "papers/22/12/2212.14457.json",
    "total_tokens": 1276,
    "translated_title": "深度线性网络的贝叶斯插值",
    "translated_abstract": "在深度学习理论中，表征神经网络的深度、宽度和数据集大小如何共同影响模型质量是一个核心问题。我们在线性网络的特殊情况下，使用具有高斯权重先验和平均平方误差的贝叶斯推理对单输出维度进行了完整的解决方案。对于任何训练数据集、网络深度和隐藏层宽度，我们找到了预测后验和贝叶斯模型证据的非渐近表达，这些表达式是一类关于Meijer-G函数的亚纯特殊函数。通过这些Meijer-G函数的新型渐近展开，我们得到了深度、宽度和数据集大小的联合作用的丰富新图像。我们表明，线性网络在无限深度时可以提供可证明的最优预测：具有数据不可知先验的无限深度线性网络的后验概率与具有最大化数据依赖先验信息的浅网络的后验概率相同，且后验概率集中于线性函数。当网络是有限的时，我们还推导了后验距离线性函数的尖锐大偏差边界，并表明这些边界以高度错综复杂的方式取决于网络深度、宽度和数据集大小。最后，在大数据集极限下提供了完整的贝叶斯模型证据的渐近展开，并证明了对于固定宽度，证据是深度和数据集大小的多项式。",
    "tldr": "本文在线性网络的情况下，使用贝叶斯推理找到了预测后验和贝叶斯模型证据的非渐近表达，并通过这些表达式得到深度、宽度和数据集大小的联合作用的新图像，同时证明了线性网络在无限深度时提供了可证明的最优预测，并推导了有限网络的尖锐大偏差边界。",
    "en_tdlr": "This paper presents a complete solution to the central problem of how neural network depth, width, and dataset size jointly impact model quality in the special case of linear networks. The authors use Bayesian inference with Gaussian weight priors and mean squared error as a negative log-likelihood to find non-asymptotic expressions for the predictive posterior and Bayesian model evidence in terms of Meijer-G functions, and derive novel asymptotic expansions of these functions, providing a rich new picture of the joint role of depth, width, and dataset size. They also prove the provably optimal predictions of infinitely deep linear networks and derive sharp large deviation bounds on posterior deviations from the linear function when the network is finite."
}