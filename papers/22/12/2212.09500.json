{
    "title": "Exploring Tradeoffs in Spiking Neural Networks. (arXiv:2212.09500v2 [cs.NE] UPDATED)",
    "abstract": "Spiking Neural Networks (SNNs) have emerged as a promising alternative to traditional Deep Neural Networks for low-power computing. However, the effectiveness of SNNs is not solely determined by their performance but also by their energy consumption, prediction speed, and robustness to noise. The recent method Fast \\& Deep, along with others, achieves fast and energy-efficient computation by constraining neurons to fire at most once. Known as Time-To-First-Spike (TTFS), this constraint however restricts the capabilities of SNNs in many aspects. In this work, we explore the relationships between performance, energy consumption, speed and stability when using this constraint. More precisely, we highlight the existence of tradeoffs where performance and robustness are gained at the cost of sparsity and prediction latency. To improve these tradeoffs, we propose a relaxed version of Fast \\& Deep that allows for multiple spikes per neuron. Our experiments show that relaxing the spike constra",
    "link": "http://arxiv.org/abs/2212.09500",
    "context": "Title: Exploring Tradeoffs in Spiking Neural Networks. (arXiv:2212.09500v2 [cs.NE] UPDATED)\nAbstract: Spiking Neural Networks (SNNs) have emerged as a promising alternative to traditional Deep Neural Networks for low-power computing. However, the effectiveness of SNNs is not solely determined by their performance but also by their energy consumption, prediction speed, and robustness to noise. The recent method Fast \\& Deep, along with others, achieves fast and energy-efficient computation by constraining neurons to fire at most once. Known as Time-To-First-Spike (TTFS), this constraint however restricts the capabilities of SNNs in many aspects. In this work, we explore the relationships between performance, energy consumption, speed and stability when using this constraint. More precisely, we highlight the existence of tradeoffs where performance and robustness are gained at the cost of sparsity and prediction latency. To improve these tradeoffs, we propose a relaxed version of Fast \\& Deep that allows for multiple spikes per neuron. Our experiments show that relaxing the spike constra",
    "path": "papers/22/12/2212.09500.json",
    "total_tokens": 1074,
    "translated_title": "探索脉冲神经网络中的权衡",
    "translated_abstract": "脉冲神经网络（SNN）已成为低功耗计算的有前途的替代传统深度神经网络的方法。然而，SNN的效能不仅与性能有关，还与能耗、预测速度和对噪声的鲁棒性有关。最近的方法Fast＆Deep以及其他方法通过限制神经元最多只能发射一次脉冲而实现快速和节能计算。称为\"Time-To-First-Spike（TTFS）\"，但这种约束在许多方面限制了SNN的性能。在这项工作中，我们探讨了在使用此约束时性能、能耗、速度和稳定性之间的关系。更具体地，我们突出了权衡存在的情况，在付出稀疏度和预测延迟的代价下获得性能和稳健性。为了改善这些权衡，我们提出了Fast＆Deep的松弛版本，允许每个神经元发出多个脉冲。我们的实验表明，放松脉冲约束可以提高SNN的性能、能耗和鲁棒性，同时仍保持合理的预测速度。",
    "tldr": "本研究探讨了在使用Time-To-First-Spike（TTFS）约束时，性能、能耗、速度和稳定性之间的权衡，并提出了一种允许多个脉冲的松弛版本的方法来改善这些权衡。实验证明放宽脉冲约束可以提高SNN的性能、能耗和鲁棒性，同时仍保持合理的预测速度。",
    "en_tdlr": "This paper explores the tradeoffs between performance, energy consumption, speed, and stability in using the Time-To-First-Spike (TTFS) constraint in Spiking Neural Networks (SNNs). The authors propose a relaxed version of the Fast \\& Deep method that allows for multiple spikes per neuron to improve these tradeoffs. Experiments show that this relaxation improves the performance, energy consumption, and robustness of SNNs while maintaining reasonable prediction speed."
}