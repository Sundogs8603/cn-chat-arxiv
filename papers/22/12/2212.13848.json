{
    "title": "Learning Lipschitz Functions by GD-trained Shallow Overparameterized ReLU Neural Networks. (arXiv:2212.13848v2 [cs.LG] UPDATED)",
    "abstract": "We explore the ability of overparameterized shallow ReLU neural networks to learn Lipschitz, nondifferentiable, bounded functions with additive noise when trained by Gradient Descent (GD). To avoid the problem that in the presence of noise, neural networks trained to nearly zero training error are inconsistent in this class, we focus on the early-stopped GD which allows us to show consistency and optimal rates. In particular, we explore this problem from the viewpoint of the Neural Tangent Kernel (NTK) approximation of a GD-trained finite-width neural network. We show that whenever some early stopping rule is guaranteed to give an optimal rate (of excess risk) on the Hilbert space of the kernel induced by the ReLU activation function, the same rule can be used to achieve minimax optimal rate for learning on the class of considered Lipschitz functions by neural networks. We discuss several data-free and data-dependent practically appealing stopping rules that yield optimal rates.",
    "link": "http://arxiv.org/abs/2212.13848",
    "context": "Title: Learning Lipschitz Functions by GD-trained Shallow Overparameterized ReLU Neural Networks. (arXiv:2212.13848v2 [cs.LG] UPDATED)\nAbstract: We explore the ability of overparameterized shallow ReLU neural networks to learn Lipschitz, nondifferentiable, bounded functions with additive noise when trained by Gradient Descent (GD). To avoid the problem that in the presence of noise, neural networks trained to nearly zero training error are inconsistent in this class, we focus on the early-stopped GD which allows us to show consistency and optimal rates. In particular, we explore this problem from the viewpoint of the Neural Tangent Kernel (NTK) approximation of a GD-trained finite-width neural network. We show that whenever some early stopping rule is guaranteed to give an optimal rate (of excess risk) on the Hilbert space of the kernel induced by the ReLU activation function, the same rule can be used to achieve minimax optimal rate for learning on the class of considered Lipschitz functions by neural networks. We discuss several data-free and data-dependent practically appealing stopping rules that yield optimal rates.",
    "path": "papers/22/12/2212.13848.json",
    "total_tokens": 983,
    "translated_title": "通过GD训练的过度参数化浅层ReLU神经网络学习Lipschitz函数",
    "translated_abstract": "本研究探究了过度参数化的浅层ReLU神经网络在通过梯度下降（GD）训练时学习具有加性噪声的Lipschitz、不可微分、有界函数的能力。为避免存在噪声时，神经网络训练到接近0的训练误差时不一致的问题，我们专注于停止较早的GD，从而展示了一致性和最优速率。具体来说，我们从GD训练的有限宽度神经网络的神经切向核（NTK）近似的视角探索了这个问题。我们发现，只要在ReLU激活函数引起的核的希尔伯特空间中，某些早停规则保证能够给出最优的超额风险速率，那么相同的规则就可以被用于实现在Lipschitz函数所考虑的类中通过神经网络学习的极小极值速率。我们讨论了几个无需数据和数据相关的实际吸引力停止准则，这些准则产生了最优速率。",
    "tldr": "本论文通过GD训练过程中神经切向核（NTK）近似方法，探究了过度参数化的浅层ReLU神经网络学习Lipschitz函数的能力，提出了一系列能够产生最优速率的实用早停规则。",
    "en_tdlr": "This paper explores the ability of overparameterized shallow ReLU neural networks to learn Lipschitz functions through the Neural Tangent Kernel (NTK) approximation method during the GD training process, proposes a series of practical early stopping rules that can produce optimal rates, and discusses the optimal excess risk rates for early-stopped GD training in the considered class of Lipschitz functions by neural networks."
}