{
    "title": "BLIND: Bias Removal With No Demographics. (arXiv:2212.10563v2 [cs.CL] UPDATED)",
    "abstract": "Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce BLIND, a method for bias removal with no prior knowledge of the demographics in the dataset. While training a model on a downstream task, BLIND detects biased samples using an auxiliary model that predicts the main model's success, and down-weights those samples during the training process. Experiments with racial and gender biases in sentiment classification and occupation classification tasks demonstrate that BLIND mitigates social biases without relying on a costly demographic annotation process. Our method is competitive with other methods that require demographic information and sometimes even surpasses them.",
    "link": "http://arxiv.org/abs/2212.10563",
    "context": "Title: BLIND: Bias Removal With No Demographics. (arXiv:2212.10563v2 [cs.CL] UPDATED)\nAbstract: Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce BLIND, a method for bias removal with no prior knowledge of the demographics in the dataset. While training a model on a downstream task, BLIND detects biased samples using an auxiliary model that predicts the main model's success, and down-weights those samples during the training process. Experiments with racial and gender biases in sentiment classification and occupation classification tasks demonstrate that BLIND mitigates social biases without relying on a costly demographic annotation process. Our method is competitive with other methods that require demographic information and sometimes even surpasses them.",
    "path": "papers/22/12/2212.10563.json",
    "total_tokens": 788,
    "translated_title": "BLIND: 无人口统计学的偏见去除方法",
    "translated_abstract": "在真实世界数据训练的模型往往会模仿和放大社会偏见。常见的减轻偏见的方法需要先了解哪些类型的偏见需要被纠正（例如性别或种族偏见）以及与每个数据样本相关联的社会群体。在本文中，我们介绍了一种名为BLIND的方法，它可以在没有对数据集中人口统计信息的先前了解下进行偏见去除。在训练下游任务模型时，BLIND使用一个辅助模型来预测主模型的成功，并在训练过程中减少这些受到偏见样本的权重。基于对情感分类和职业分类任务中的种族和性别偏见的实验表明，BLIND可以在不依赖昂贵的人口统计注释过程的情况下消除社会偏见。我们的方法与需要人口统计学信息的其他方法相比具有竞争力，有时甚至超过它们。",
    "tldr": "BLIND可以在没有先前了解数据集人口统计信息的情况下消除训练模型中的社会偏见。",
    "en_tdlr": "BLIND can remove social biases in model training without prior knowledge of demographic information in the dataset."
}