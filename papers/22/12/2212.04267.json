{
    "title": "Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval. (arXiv:2212.04267v2 [cs.CV] UPDATED)",
    "abstract": "Vision-Language Pretraining (VLP) and Foundation models have been the go-to recipe for achieving SoTA performance on general benchmarks. However, leveraging these powerful techniques for more complex vision-language tasks, such as cooking applications, with more structured input data, is still little investigated. In this work, we propose to leverage these techniques for structured-text based computational cuisine tasks. Our strategy, dubbed VLPCook, first transforms existing image-text pairs to image and structured-text pairs. This allows to pretrain our VLPCook model using VLP objectives adapted to the strutured data of the resulting datasets, then finetuning it on downstream computational cooking tasks. During finetuning, we also enrich the visual encoder, leveraging pretrained foundation models (e.g. CLIP) to provide local and global textual context. VLPCook outperforms current SoTA by a significant margin (+3.3 Recall@1 absolute improvement) on the task of Cross-Modal Food Retriev",
    "link": "http://arxiv.org/abs/2212.04267",
    "context": "Title: Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval. (arXiv:2212.04267v2 [cs.CV] UPDATED)\nAbstract: Vision-Language Pretraining (VLP) and Foundation models have been the go-to recipe for achieving SoTA performance on general benchmarks. However, leveraging these powerful techniques for more complex vision-language tasks, such as cooking applications, with more structured input data, is still little investigated. In this work, we propose to leverage these techniques for structured-text based computational cuisine tasks. Our strategy, dubbed VLPCook, first transforms existing image-text pairs to image and structured-text pairs. This allows to pretrain our VLPCook model using VLP objectives adapted to the strutured data of the resulting datasets, then finetuning it on downstream computational cooking tasks. During finetuning, we also enrich the visual encoder, leveraging pretrained foundation models (e.g. CLIP) to provide local and global textual context. VLPCook outperforms current SoTA by a significant margin (+3.3 Recall@1 absolute improvement) on the task of Cross-Modal Food Retriev",
    "path": "papers/22/12/2212.04267.json",
    "total_tokens": 967,
    "translated_title": "视觉和结构化语言预训练用于跨模态食物检索",
    "translated_abstract": "视觉-语言预训练（VLP）和基础模型一直是在通用基准上实现最佳结果的常规方法。然而，在更复杂的视觉-语言任务中，比如涉及更结构化输入数据的烹饪应用程序，利用这些强大的技术的研究还很少。在这项工作中，我们提出了一种利用这些技术进行基于结构化文本的计算料理任务的策略。我们的策略被称为VLPCook，首先将现有的图像-文本对转换为图像和结构化文本对。这使得我们可以使用适应于得到的数据集的结构化数据的VLP目标预训练我们的VLPCook模型，然后在下游计算烹饪任务上进行微调。在微调过程中，我们还利用预训练基础模型（例如CLIP）丰富视觉编码器，提供局部和全局文本上下文。VLPCook在跨模态食物检索任务上通过显著的较高比例（+3.3 Recall@1绝对改善）超越了当前的SoTA。",
    "tldr": "本研究提出了一种新的策略VLPCook，利用视觉-语言预训练技术和结构化数据，实现基于结构化文本的计算料理任务，并在跨模态食物检索任务上取得了超越当前SoTA的优异表现。",
    "en_tdlr": "This study proposes a new strategy, VLPCook, which uses vision-language pretraining techniques and structured data to accomplish computational cooking tasks based on structured text, and achieved outstanding performance exceeding the current state-of-the-art on the cross-modal food retrieval task."
}