{
    "title": "Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments. (arXiv:2212.09736v2 [cs.CL] UPDATED)",
    "abstract": "A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient f",
    "link": "http://arxiv.org/abs/2212.09736",
    "context": "Title: Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments. (arXiv:2212.09736v2 [cs.CL] UPDATED)\nAbstract: A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient f",
    "path": "papers/22/12/2212.09736.json",
    "total_tokens": 970,
    "translated_title": "不生成，辨别：一种将语言模型与现实世界环境接轨的方案",
    "translated_abstract": "目前语言模型最缺失的就是现实世界环境的接轨性。已有的相关工作将语言模型用于直接生成计划，以便在环境中执行以达到预期的效果，这使得语言模型负担了确保语法正确性、忠实性和可控性的重担。本文提出了一个泛用的框架Pangu，用于实现语言模型与现实环境的接轨，该框架利用语言模型的辨别能力而非生成能力，由一个符号代理和一个神经语言模型协同工作：代理在环境中探索以逐步构建有效的计划，而语言模型评估备选计划的合理性以引导搜索过程。针对知识库问答（KBQA）这一具有挑战性的问题进行了案例研究，该问题具有庞大的环境，结果表明了Pangu的显著有效性和灵活性：BERT基语言模型已足够应对。",
    "tldr": "Pangu是一个泛用的框架，用于实现语言模型与现实环境的接轨，它利用语言模型的辨别能力而非生成能力，由一个符号代理和一个神经语言模型协同工作。这一方案已经在知识库问答问题中证明了它的有效性和灵活性。",
    "en_tdlr": "Pangu is a generic framework that combines symbolic agents with neural language models to ground language models to real-world environments. By utilizing the discriminative ability of language models, Pangu incrementally constructs valid plans while evaluating the plausibility of candidate plans to guide the search process. A case study on knowledge base question answering demonstrates the effectiveness and flexibility of this approach."
}