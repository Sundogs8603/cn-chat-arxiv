{
    "title": "CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation. (arXiv:2212.09506v3 [cs.CV] UPDATED)",
    "abstract": "Weakly supervised semantic segmentation (WSSS) with image-level labels is a challenging task. Mainstream approaches follow a multi-stage framework and suffer from high training costs. In this paper, we explore the potential of Contrastive Language-Image Pre-training models (CLIP) to localize different categories with only image-level labels and without further training. To efficiently generate high-quality segmentation masks from CLIP, we propose a novel WSSS framework called CLIP-ES. Our framework improves all three stages of WSSS with special designs for CLIP: 1) We introduce the softmax function into GradCAM and exploit the zero-shot ability of CLIP to suppress the confusion caused by non-target classes and backgrounds. Meanwhile, to take full advantage of CLIP, we re-explore text inputs under the WSSS setting and customize two text-driven strategies: sharpness-based prompt selection and synonym fusion. 2) To simplify the stage of CAM refinement, we propose a real-time class-aware a",
    "link": "http://arxiv.org/abs/2212.09506",
    "context": "Title: CLIP is Also an Efficient Segmenter: A Text-Driven Approach for Weakly Supervised Semantic Segmentation. (arXiv:2212.09506v3 [cs.CV] UPDATED)\nAbstract: Weakly supervised semantic segmentation (WSSS) with image-level labels is a challenging task. Mainstream approaches follow a multi-stage framework and suffer from high training costs. In this paper, we explore the potential of Contrastive Language-Image Pre-training models (CLIP) to localize different categories with only image-level labels and without further training. To efficiently generate high-quality segmentation masks from CLIP, we propose a novel WSSS framework called CLIP-ES. Our framework improves all three stages of WSSS with special designs for CLIP: 1) We introduce the softmax function into GradCAM and exploit the zero-shot ability of CLIP to suppress the confusion caused by non-target classes and backgrounds. Meanwhile, to take full advantage of CLIP, we re-explore text inputs under the WSSS setting and customize two text-driven strategies: sharpness-based prompt selection and synonym fusion. 2) To simplify the stage of CAM refinement, we propose a real-time class-aware a",
    "path": "papers/22/12/2212.09506.json",
    "total_tokens": 834,
    "translated_title": "CLIP也是一个高效的分割器：一种基于文本的弱监督语义分割方法",
    "translated_abstract": "利用图像级别标签进行弱监督的语义分割是一项具有挑战性的任务。本文探讨了对比度语言-图像预训练模型（CLIP）在仅使用图像级别标签的情况下本地化不同类别的潜力，并提出了一种名为CLIP-ES的新型WSSS框架，以高效生成高质量的分割掩模。",
    "tldr": "本文提出了一种基于文本输入的新型WSSS框架CLIP-ES，利用图像级别标签进行弱监督语义分割，通过采用softmax函数、锐度驱动提示选择和同义词融合等策略，提高了WSSS的三个阶段的效率，并成功利用了CLIP的零样本能力，从而实现了高质量的分割掩模生成。",
    "en_tdlr": "We propose a novel WSSS framework called CLIP-ES that utilizes the potential of CLIP to generate high-quality segmentation masks with only image-level labels, and achieves efficient weakly supervised semantic segmentation through strategies such as softmax function, sharpness-based prompt selection, and synonym fusion, which greatly improves the efficiency of all three stages of WSSS under the text-driven approach."
}