{
    "title": "How Does Beam Search improve Span-Level Confidence Estimation in Generative Sequence Labeling?. (arXiv:2212.10767v2 [cs.CL] UPDATED)",
    "abstract": "Sequence labeling is a core task in text understanding for IE/IR systems. Text generation models have increasingly become the go-to solution for such tasks (e.g., entity extraction and dialog slot filling). While most research has focused on the labeling accuracy, a key aspect -- of vital practical importance -- has slipped through the cracks: understanding model confidence. More specifically, we lack a principled understanding of how to reliably gauge the confidence of a model in its predictions for each labeled span. This paper aims to provide some empirical insights on estimating model confidence for generative sequence labeling. Most notably, we find that simply using the decoder's output probabilities \\textbf{is not} the best in realizing well-calibrated confidence estimates. As verified over six public datasets of different tasks, we show that our proposed approach -- which leverages statistics from top-$k$ predictions by a beam search -- significantly reduces calibration errors ",
    "link": "http://arxiv.org/abs/2212.10767",
    "context": "Title: How Does Beam Search improve Span-Level Confidence Estimation in Generative Sequence Labeling?. (arXiv:2212.10767v2 [cs.CL] UPDATED)\nAbstract: Sequence labeling is a core task in text understanding for IE/IR systems. Text generation models have increasingly become the go-to solution for such tasks (e.g., entity extraction and dialog slot filling). While most research has focused on the labeling accuracy, a key aspect -- of vital practical importance -- has slipped through the cracks: understanding model confidence. More specifically, we lack a principled understanding of how to reliably gauge the confidence of a model in its predictions for each labeled span. This paper aims to provide some empirical insights on estimating model confidence for generative sequence labeling. Most notably, we find that simply using the decoder's output probabilities \\textbf{is not} the best in realizing well-calibrated confidence estimates. As verified over six public datasets of different tasks, we show that our proposed approach -- which leverages statistics from top-$k$ predictions by a beam search -- significantly reduces calibration errors ",
    "path": "papers/22/12/2212.10767.json",
    "total_tokens": 926,
    "translated_title": "如何在生成序列标记中改善基于跨度级别置信度的束搜索？(arXiv:2212.10767v2 [cs.CL] 更新)",
    "translated_abstract": "序列标记是信息抽取/信息检索系统中的核心任务。文本生成模型越来越成为这类任务的解决方案（例如实体提取和对话槽填充）。虽然大多数研究都集中在标记准确性上，但一个关键的方面——对模型置信度的理解却被忽视了。具体而言，我们缺乏一个能够可靠地衡量模型对每个标记跨度的预测置信度的原则性理解。本文旨在提供一些关于生成序列标记的模型置信度估计的实证见解。值得注意的是，我们发现仅仅使用解码器的输出概率并不是实现良好校准置信度估计的最佳方法。通过对六个不同任务的公共数据集进行验证，我们展示了我们提出的方法——利用束搜索的前k个预测的统计数据——显著降低了校准误差。",
    "tldr": "本文研究了在生成序列标记任务中如何改善对跨度级别置信度的估计。研究发现仅仅使用解码器的输出概率并不是最佳方法，而利用束搜索的前k个预测的统计数据可以显著降低校准误差。"
}