{
    "title": "ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning. (arXiv:2212.01378v2 [cs.LG] UPDATED)",
    "abstract": "We propose a new paradigm to continually evolve pretrained models, denoted ColD Fusion. It provides the benefits of multitask learning but leverages distributed computation with limited communication and eliminates the need for shared data. Consequentially, ColD Fusion can give rise to a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based upon. We show that ColD Fusion yields comparable benefits to multitask training by producing a model that (a) attains strong performance on all of the datasets it was trained on; and (b) is a better starting point for finetuning on unseen datasets. We show that ColD Fusion outperforms RoBERTa and even previous multitask models. Specifically, when training and testing on 35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.33 points on average without any changes to the architecture.",
    "link": "http://arxiv.org/abs/2212.01378",
    "context": "Title: ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning. (arXiv:2212.01378v2 [cs.LG] UPDATED)\nAbstract: We propose a new paradigm to continually evolve pretrained models, denoted ColD Fusion. It provides the benefits of multitask learning but leverages distributed computation with limited communication and eliminates the need for shared data. Consequentially, ColD Fusion can give rise to a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based upon. We show that ColD Fusion yields comparable benefits to multitask training by producing a model that (a) attains strong performance on all of the datasets it was trained on; and (b) is a better starting point for finetuning on unseen datasets. We show that ColD Fusion outperforms RoBERTa and even previous multitask models. Specifically, when training and testing on 35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.33 points on average without any changes to the architecture.",
    "path": "papers/22/12/2212.01378.json",
    "total_tokens": 792,
    "translated_title": "ColD Fusion: 协同下降的分布式多任务微调方法",
    "translated_abstract": "我们提出了一种新的范式来不断演进预训练模型，称为ColD Fusion。它具有多任务学习的优势，但利用有限通信的分布式计算，并且消除了共享数据的需求。因此，ColD Fusion可以形成一个协同循环，其中微调模型可以循环利用，不断改进它们所基于的预训练模型。我们展示了ColD Fusion产生了与多任务训练相当的好处，通过产生一个在所有训练数据集上表现良好并且在未见数据集上进行微调的更好的起点模型。我们展示了ColD Fusion优于RoBERTa甚至以前的多任务模型。具体来说，在使用35个不同数据集进行训练和测试时，ColD Fusion-based模型在不改变架构的情况下平均优于RoBERTa 2.33个点。",
    "tldr": "ColD Fusion是一种协同下降的分布式多任务微调方法，通过利用分布式计算，可以不断改进预训练模型，并在各种数据集上表现良好，优于RoBERTa模型。"
}