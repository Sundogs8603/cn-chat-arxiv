{
    "title": "MiLMo:Minority Multilingual Pre-trained Language Model. (arXiv:2212.01779v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained language models are trained on large-scale unsupervised data, and they can fine-turn the model only on small-scale labeled datasets, and achieve good results. Multilingual pre-trained language models can be trained on multiple languages, and the model can understand multiple languages at the same time. At present, the search on pre-trained models mainly focuses on rich resources, while there is relatively little research on low-resource languages such as minority languages, and the public multilingual pre-trained language model can not work well for minority languages. Therefore, this paper constructs a multilingual pre-trained model named MiLMo that performs better on minority language tasks, including Mongolian, Tibetan, Uyghur, Kazakh and Korean. To solve the problem of scarcity of datasets on minority languages and verify the effectiveness of the MiLMo model, this paper constructs a minority multilingual text classification dataset named MiTC, and trains a word2vec mode",
    "link": "http://arxiv.org/abs/2212.01779",
    "context": "Title: MiLMo:Minority Multilingual Pre-trained Language Model. (arXiv:2212.01779v2 [cs.CL] UPDATED)\nAbstract: Pre-trained language models are trained on large-scale unsupervised data, and they can fine-turn the model only on small-scale labeled datasets, and achieve good results. Multilingual pre-trained language models can be trained on multiple languages, and the model can understand multiple languages at the same time. At present, the search on pre-trained models mainly focuses on rich resources, while there is relatively little research on low-resource languages such as minority languages, and the public multilingual pre-trained language model can not work well for minority languages. Therefore, this paper constructs a multilingual pre-trained model named MiLMo that performs better on minority language tasks, including Mongolian, Tibetan, Uyghur, Kazakh and Korean. To solve the problem of scarcity of datasets on minority languages and verify the effectiveness of the MiLMo model, this paper constructs a minority multilingual text classification dataset named MiTC, and trains a word2vec mode",
    "path": "papers/22/12/2212.01779.json",
    "total_tokens": 900,
    "translated_title": "MiLMo：少数民族多语言预训练语言模型",
    "translated_abstract": "预训练语言模型在大规模无监督数据上进行训练，可以在小规模带标签的数据集上微调模型，并取得良好的效果。多语言预训练语言模型可以在多种语言上进行训练，模型同时能够理解多种语言。目前，预训练模型的研究主要集中在资源丰富的语言上，相对于常见语言而言，对于少数民族等资源稀缺语言的研究却相对较少，公共多语言预训练语言模型在少数民族语言上表现不佳。因此，本文构建了一个名为MiLMo的多语言预训练模型，该模型在包括蒙古语、藏语、维吾尔语、哈萨克语和朝鲜语在内的少数民族语言任务上表现较好。为验证MiLMo模型的有效性并解决少数民族语言数据集稀缺的问题，本文构建了一个名为MiTC的少数民族多语言文本分类数据集，并训练了一个Word2vec模型。",
    "tldr": "本论文构建了一个名为MiLMo的多语言预训练模型，该模型在少数民族语言任务上表现较好，并构建了一个少数民族多语言文本分类数据集。",
    "en_tdlr": "This paper constructs a multilingual pre-trained model named MiLMo that performs better on minority language tasks, and constructs a minority multilingual text classification dataset named MiTC."
}