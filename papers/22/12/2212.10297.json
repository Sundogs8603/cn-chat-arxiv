{
    "title": "Extrinsic Evaluation of Machine Translation Metrics. (arXiv:2212.10297v2 [cs.CL] UPDATED)",
    "abstract": "Automatic machine translation (MT) metrics are widely used to distinguish the translation qualities of machine translation systems across relatively large test sets (system-level evaluation). However, it is unclear if automatic metrics are reliable at distinguishing good translations from bad translations at the sentence level (segment-level evaluation). In this paper, we investigate how useful MT metrics are at detecting the success of a machine translation component when placed in a larger platform with a downstream task. We evaluate the segment-level performance of the most widely used MT metrics (chrF, COMET, BERTScore, etc.) on three downstream cross-lingual tasks (dialogue state tracking, question answering, and semantic parsing). For each task, we only have access to a monolingual task-specific model. We calculate the correlation between the metric's ability to predict a good/bad translation with the success/failure on the final task for the Translate-Test setup. Our experiments",
    "link": "http://arxiv.org/abs/2212.10297",
    "context": "Title: Extrinsic Evaluation of Machine Translation Metrics. (arXiv:2212.10297v2 [cs.CL] UPDATED)\nAbstract: Automatic machine translation (MT) metrics are widely used to distinguish the translation qualities of machine translation systems across relatively large test sets (system-level evaluation). However, it is unclear if automatic metrics are reliable at distinguishing good translations from bad translations at the sentence level (segment-level evaluation). In this paper, we investigate how useful MT metrics are at detecting the success of a machine translation component when placed in a larger platform with a downstream task. We evaluate the segment-level performance of the most widely used MT metrics (chrF, COMET, BERTScore, etc.) on three downstream cross-lingual tasks (dialogue state tracking, question answering, and semantic parsing). For each task, we only have access to a monolingual task-specific model. We calculate the correlation between the metric's ability to predict a good/bad translation with the success/failure on the final task for the Translate-Test setup. Our experiments",
    "path": "papers/22/12/2212.10297.json",
    "total_tokens": 920,
    "translated_title": "机器翻译度量的外在评估",
    "translated_abstract": "自动机器翻译度量通常用于在较大的测试集上比较机器翻译系统的翻译质量（系统级评估），但是，句子级别上自动度量是否能可靠地区分好翻译和差翻译仍不清楚。本文调查了在具有下游任务的大型平台上放置机器翻译组件以检测其成功的MT度量的有用性。我们在三个跨语言下游任务（对话状态跟踪，问题回答和语义解析）上评估了最广泛使用的MT量度（chrF，COMET，BERTScore等）的分段性能。对于每个任务，我们仅能访问单语种任务特定模型。我们计算在Translate-Test设置下，度量预测好/坏翻译能力与最终任务成功/失败之间的相关性。我们的实验表明，尽管某些度量在系统级评估中表现良好，但在分段评估中可能不可靠。此外，某些度量的有用性取决于下游任务。",
    "tldr": "论文研究了机器翻译度量在大型平台和下游任务中的可靠性，发现某些度量在句子级别上表现不佳且其有用性与下游任务有关。",
    "en_tdlr": "This paper investigates the reliability of machine translation metrics in a larger platform and downstream tasks, finding that certain metrics perform poorly at the sentence level and their usefulness depends on the downstream task."
}