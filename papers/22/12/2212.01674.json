{
    "title": "CrossSplit: Mitigating Label Noise Memorization through Data Splitting. (arXiv:2212.01674v2 [cs.CV] UPDATED)",
    "abstract": "We approach the problem of improving robustness of deep learning algorithms in the presence of label noise. Building upon existing label correction and co-teaching methods, we propose a novel training procedure to mitigate the memorization of noisy labels, called CrossSplit, which uses a pair of neural networks trained on two disjoint parts of the labelled dataset. CrossSplit combines two main ingredients: (i) Cross-split label correction. The idea is that, since the model trained on one part of the data cannot memorize example-label pairs from the other part, the training labels presented to each network can be smoothly adjusted by using the predictions of its peer network; (ii) Cross-split semi-supervised training. A network trained on one part of the data also uses the unlabeled inputs of the other part. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of no",
    "link": "http://arxiv.org/abs/2212.01674",
    "context": "Title: CrossSplit: Mitigating Label Noise Memorization through Data Splitting. (arXiv:2212.01674v2 [cs.CV] UPDATED)\nAbstract: We approach the problem of improving robustness of deep learning algorithms in the presence of label noise. Building upon existing label correction and co-teaching methods, we propose a novel training procedure to mitigate the memorization of noisy labels, called CrossSplit, which uses a pair of neural networks trained on two disjoint parts of the labelled dataset. CrossSplit combines two main ingredients: (i) Cross-split label correction. The idea is that, since the model trained on one part of the data cannot memorize example-label pairs from the other part, the training labels presented to each network can be smoothly adjusted by using the predictions of its peer network; (ii) Cross-split semi-supervised training. A network trained on one part of the data also uses the unlabeled inputs of the other part. Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of no",
    "path": "papers/22/12/2212.01674.json",
    "total_tokens": 950,
    "translated_title": "CrossSplit: 通过数据分割缓解标签噪声记忆问题",
    "translated_abstract": "我们的研究旨在解决标签噪声存在情况下深度学习算法鲁棒性不足的问题。基于现有的标签修正和共同教学方法，我们提出了一种新的训练程序——CrossSplit，以缓解噪声标签的记忆问题。CrossSplit使用在两个标记数据集的不相交部分上训练的一对神经网络。该方法组合了两个主要组成部分：(i)交叉分割标签修正：由于在数据集的一部分上训练的模型不能记忆来自其他部分的示例-标签对，因此可以使用对等网络的预测平滑调整每个网络呈现的训练标签；(ii)交叉分割半监督训练：在一个部分的数据上训练的网络也使用另一个部分的未标记输入。在CIFAR-10、CIFAR-100、Tiny-ImageNet和mini-WebVision数据集上的大量实验表明，我们的方法可以在广泛的噪声和干扰下比当前最先进的方法更好地完成任务。",
    "tldr": "本文提出了一种名为CrossSplit的新训练程序，通过使用交叉分割的标签修正和半监督训练两个主要组成部分，缓解了深度学习算法中标签噪声记忆问题，具有良好的效果。",
    "en_tdlr": "This paper proposes a novel training procedure called CrossSplit that mitigates label noise memorization in deep learning algorithms by using cross-split label correction and semi-supervised training. The method shows good performances on various datasets in the presence of noise and interference."
}