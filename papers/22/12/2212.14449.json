{
    "title": "Policy Mirror Ascent for Efficient and Independent Learning in Mean Field Games. (arXiv:2212.14449v2 [math.OC] UPDATED)",
    "abstract": "Mean-field games have been used as a theoretical tool to obtain an approximate Nash equilibrium for symmetric and anonymous $N$-player games. However, limiting applicability, existing theoretical results assume variations of a \"population generative model\", which allows arbitrary modifications of the population distribution by the learning algorithm. Moreover, learning algorithms typically work on abstract simulators with population instead of the $N$-player game. Instead, we show that $N$ agents running policy mirror ascent converge to the Nash equilibrium of the regularized game within $\\widetilde{\\mathcal{O}}(\\varepsilon^{-2})$ samples from a single sample trajectory without a population generative model, up to a standard $\\mathcal{O}(\\frac{1}{\\sqrt{N}})$ error due to the mean field. Taking a divergent approach from the literature, instead of working with the best-response map we first show that a policy mirror ascent map can be used to construct a contractive operator having the Na",
    "link": "http://arxiv.org/abs/2212.14449",
    "context": "Title: Policy Mirror Ascent for Efficient and Independent Learning in Mean Field Games. (arXiv:2212.14449v2 [math.OC] UPDATED)\nAbstract: Mean-field games have been used as a theoretical tool to obtain an approximate Nash equilibrium for symmetric and anonymous $N$-player games. However, limiting applicability, existing theoretical results assume variations of a \"population generative model\", which allows arbitrary modifications of the population distribution by the learning algorithm. Moreover, learning algorithms typically work on abstract simulators with population instead of the $N$-player game. Instead, we show that $N$ agents running policy mirror ascent converge to the Nash equilibrium of the regularized game within $\\widetilde{\\mathcal{O}}(\\varepsilon^{-2})$ samples from a single sample trajectory without a population generative model, up to a standard $\\mathcal{O}(\\frac{1}{\\sqrt{N}})$ error due to the mean field. Taking a divergent approach from the literature, instead of working with the best-response map we first show that a policy mirror ascent map can be used to construct a contractive operator having the Na",
    "path": "papers/22/12/2212.14449.json",
    "total_tokens": 967,
    "translated_title": "《在均场博弈中使用策略镜面上升实现高效独立学习》",
    "translated_abstract": "均场博弈被用作获得对称和匿名的N人博弈的近似纳什均衡的理论工具。然而，现有的理论结果只适用于\"人口生成模型\"的变化，该模型允许学习算法通过人口分布进行任意修改。此外，学习算法通常使用具有人口属性的抽象模拟器，而不是N人博弈。我们展示了N个代理运行策略镜面上升是如何在不使用人口生成模型的情况下，从单个样本轨迹中收敛于规则博弈的纳什均衡，只需要大约$\\widetilde{\\mathcal{O}}(\\varepsilon^{-2})$的样本，由于均场的缘故，误差为$\\mathcal{O}(\\frac{1}{\\sqrt{N}})$。相较于文献的不同方法，我们首先展示了可以使用策略镜面上升映射来构建一个契约算子，而不是与最佳响应映射一起工作。",
    "tldr": "本文提出了在均场博弈中使用策略镜面上升实现高效独立学习的方法，不需要人口生成模型，且只需要约$\\widetilde{\\mathcal{O}}(\\varepsilon^{-2})$个样本即可达到纳什均衡。",
    "en_tdlr": "This paper proposes a method for efficient and independent learning using policy mirror ascent in mean-field games, without the need for a population generative model, and reaches the Nash equilibrium with only about $\\widetilde{\\mathcal{O}}(\\varepsilon^{-2})$ samples."
}