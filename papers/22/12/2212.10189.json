{
    "title": "Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions. (arXiv:2212.10189v2 [cs.CL] UPDATED)",
    "abstract": "When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first identifying various forms of KB incompleteness that make questions unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset with only answerable questions). Experimenting with three state-of-the-art KBQA models, we find that all three models suffer a drop in performance even after suitable adaptation for unanswerable questions. In addition, these often detect unanswerability for wrong reasons and find specific forms of unanswerability particularly difficult to handle. This underscores the need for further research in making KBQA systems robust to unanswerability",
    "link": "http://arxiv.org/abs/2212.10189",
    "context": "Title: Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions. (arXiv:2212.10189v2 [cs.CL] UPDATED)\nAbstract: When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first identifying various forms of KB incompleteness that make questions unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset with only answerable questions). Experimenting with three state-of-the-art KBQA models, we find that all three models suffer a drop in performance even after suitable adaptation for unanswerable questions. In addition, these often detect unanswerability for wrong reasons and find specific forms of unanswerability particularly difficult to handle. This underscores the need for further research in making KBQA systems robust to unanswerability",
    "path": "papers/22/12/2212.10189.json",
    "total_tokens": 977,
    "translated_title": "我有足够的知识回答吗？探究知识库问答的可回答性。",
    "translated_abstract": "在对知识库进行自然语言问答时，缺失的事实、不完整的模式和有限的范围自然地导致许多问题无法回答。虽然在其他问答环境中已经探讨了可回答性，但对于知识库问答（KBQA）尚未进行研究。我们首先识别了各种形式的知识库不完整性，使得问题无法回答，并通过有系统地调整GrailQA（一个仅包含可回答问题的流行KBQA数据集）来创建具有无法回答问题的新的GrailQAbility基准KBQA数据集。在三个最先进的KBQA模型的实验中，我们发现即使在适当调整无法回答的问题后，所有三个模型的性能也会下降。此外，这些模型常常因错误的原因检测出无法回答，并发现特定形式的无法回答尤其难以处理。这强调了进一步研究使KBQA系统对无法回答具有鲁棒性的必要性。",
    "tldr": "本文探究了在知识库问答中的可回答性问题，使用新的GrailQAbility基准KBQA数据集，发现现有的KBQA模型在处理无法回答问题时性能下降，并对无法回答的检测存在问题，需要进一步研究来使KBQA系统对无法回答具有鲁棒性。",
    "en_tdlr": "This paper investigates the issue of answerability in knowledge base question answering. Using a new benchmark KBQA dataset, GrailQAbility, it shows that existing KBQA models suffer a drop in performance when handling unanswerable questions, and often detect unanswerability for wrong reasons. Further research is needed to make KBQA systems robust to unanswerability."
}