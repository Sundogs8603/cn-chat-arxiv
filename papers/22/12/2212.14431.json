{
    "title": "Function Approximation for Solving Stackelberg Equilibrium in Large Perfect Information Games. (arXiv:2212.14431v2 [cs.GT] UPDATED)",
    "abstract": "Function approximation (FA) has been a critical component in solving large zero-sum games. Yet, little attention has been given towards FA in solving \\textit{general-sum} extensive-form games, despite them being widely regarded as being computationally more challenging than their fully competitive or cooperative counterparts. A key challenge is that for many equilibria in general-sum games, no simple analogue to the state value function used in Markov Decision Processes and zero-sum games exists. In this paper, we propose learning the \\textit{Enforceable Payoff Frontier} (EPF) -- a generalization of the state value function for general-sum games. We approximate the optimal \\textit{Stackelberg extensive-form correlated equilibrium} by representing EPFs with neural networks and training them by using appropriate backup operations and loss functions. This is the first method that applies FA to the Stackelberg setting, allowing us to scale to much larger games while still enjoying performa",
    "link": "http://arxiv.org/abs/2212.14431",
    "context": "Title: Function Approximation for Solving Stackelberg Equilibrium in Large Perfect Information Games. (arXiv:2212.14431v2 [cs.GT] UPDATED)\nAbstract: Function approximation (FA) has been a critical component in solving large zero-sum games. Yet, little attention has been given towards FA in solving \\textit{general-sum} extensive-form games, despite them being widely regarded as being computationally more challenging than their fully competitive or cooperative counterparts. A key challenge is that for many equilibria in general-sum games, no simple analogue to the state value function used in Markov Decision Processes and zero-sum games exists. In this paper, we propose learning the \\textit{Enforceable Payoff Frontier} (EPF) -- a generalization of the state value function for general-sum games. We approximate the optimal \\textit{Stackelberg extensive-form correlated equilibrium} by representing EPFs with neural networks and training them by using appropriate backup operations and loss functions. This is the first method that applies FA to the Stackelberg setting, allowing us to scale to much larger games while still enjoying performa",
    "path": "papers/22/12/2212.14431.json",
    "total_tokens": 985,
    "translated_title": "在大型完全信息博弈中解决斯塔克伯格均衡的函数逼近方法",
    "translated_abstract": "函数逼近是解决大型零和游戏的关键组成部分。然而，在解决广义和博弈方面，函数逼近得到的关注却很少。广义和博弈被广泛认为在计算上比它们的完全竞争或合作伙伴更具挑战性。其中一个关键的挑战是对于广义和博弈中的许多均衡，不存在像马尔可夫决策过程和零和游戏中使用的状态价值函数的简单类比物。在本文中，我们提出了学习“可强制履行的收益边界”（EPF）的方法，这是广义和博弈状态价值函数的一般化。我们通过用神经网络表示EPF并使用适当的备份操作和损失函数来训练它们，从而逼近了最优的斯塔克伯格广义和博弈策略均衡。这是第一个将函数逼近应用于斯塔克伯格均衡的方法，使我们能够扩展到更大的博弈并仍然享受良好的性能。",
    "tldr": "该论文提出了一种在大型博弈中解决斯塔克伯格均衡的方法，通过学习“可强制履行的收益边界”（EPF）来逼近最优策略均衡，可以应用于更广泛的广义和博弈。",
    "en_tdlr": "The paper proposes a method for solving Stackelberg equilibrium in large games by approximating the optimal Stackelberg extensive-form correlated equilibrium through learning the Enforceable Payoff Frontier (EPF), allowing the method to be applied to a wider range of general-sum games."
}