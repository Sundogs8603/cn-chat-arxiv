{
    "title": "Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis. (arXiv:2212.10356v2 [cs.CL] UPDATED)",
    "abstract": "Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences. A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create ~\\textbf{Sandwich}, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design.",
    "link": "http://arxiv.org/abs/2212.10356",
    "context": "Title: Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis. (arXiv:2212.10356v2 [cs.CL] UPDATED)\nAbstract: Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences. A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create ~\\textbf{Sandwich}, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design.",
    "path": "papers/22/12/2212.10356.json",
    "total_tokens": 766,
    "translated_title": "通过感受野分析透视Transformer长度外推",
    "translated_abstract": "长度外推允许在短序列上训练Transformer语言模型，当测试较长序列时仍保持困惑度。相对位置嵌入设计ALiBi到目前为止已被广泛使用。我们通过一个新颖的累积标准化梯度工具，通过感受野分析解剖了ALiBi。感受野的概念进一步允许我们修改香草正弦位置嵌入，创建了~\\textbf{Sandwich}，这是第一个真正使用比训练序列长的长度信息的无参数相对位置嵌入设计。Sandwich与KERPLE和T5共享具有可学习相对位置嵌入的对数衰减时间偏差模式；这些揭示了未来可外推的位置嵌入设计。",
    "tldr": "该论文通过感受野分析透视了Transformer长度外推中的相对位置嵌入设计，提出了第一个真正使用比训练序列长的长度信息的无参数相对位置嵌入设计Sandwich。",
    "en_tdlr": "This paper dissects the relative positional embedding designs in Transformer length extrapolation through receptive field analysis and proposes a parameter-free design called Sandwich, which utilizes length information longer than the training sequence."
}