{
    "title": "Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off. (arXiv:2212.08949v3 [cs.LG] UPDATED)",
    "abstract": "A default assumption in reinforcement learning (RL) and optimal control is that observations arrive at discrete time points on a fixed clock cycle. Yet, many applications involve continuous-time systems where the time discretization, in principle, can be managed. The impact of time discretization on RL methods has not been fully characterized in existing theory, but a more detailed analysis of its effect could reveal opportunities for improving data-efficiency. We address this gap by analyzing Monte-Carlo policy evaluation for LQR systems and uncover a fundamental trade-off between approximation and statistical error in value estimation. Importantly, these two errors behave differently to time discretization, leading to an optimal choice of temporal resolution for a given data budget. These findings show that managing the temporal resolution can provably improve policy evaluation efficiency in LQR systems with finite data. Empirically, we demonstrate the trade-off in numerical simulati",
    "link": "http://arxiv.org/abs/2212.08949",
    "context": "Title: Managing Temporal Resolution in Continuous Value Estimation: A Fundamental Trade-off. (arXiv:2212.08949v3 [cs.LG] UPDATED)\nAbstract: A default assumption in reinforcement learning (RL) and optimal control is that observations arrive at discrete time points on a fixed clock cycle. Yet, many applications involve continuous-time systems where the time discretization, in principle, can be managed. The impact of time discretization on RL methods has not been fully characterized in existing theory, but a more detailed analysis of its effect could reveal opportunities for improving data-efficiency. We address this gap by analyzing Monte-Carlo policy evaluation for LQR systems and uncover a fundamental trade-off between approximation and statistical error in value estimation. Importantly, these two errors behave differently to time discretization, leading to an optimal choice of temporal resolution for a given data budget. These findings show that managing the temporal resolution can provably improve policy evaluation efficiency in LQR systems with finite data. Empirically, we demonstrate the trade-off in numerical simulati",
    "path": "papers/22/12/2212.08949.json",
    "total_tokens": 965,
    "translated_title": "在连续值估计中管理时间分辨率: 一项基本权衡的研究",
    "translated_abstract": "强化学习（RL）和最优控制中的默认假设是观测以固定的时钟周期在离散的时间点到达。然而，许多应用涉及连续时间系统，理论上可以对时间离散化进行管理。时间离散化对RL方法的影响尚未在现有理论中完全表征，但对其影响进行更详细的分析可能揭示提高数据效率的机会。我们通过分析LQR系统的Monte-Carlo策略评估来填补这一空白，并发现了估值过程中近似误差和统计误差之间的基本权衡。重要的是，这两种错误对时间离散化的表现不同，这导致了对于给定数据预算的时间分辨率的最佳选择。这些发现表明，在具有有限数据的LQR系统中，管理时间分辨率可以改善策略评估的效率。从实证角度来看，我们在数值模拟中展示了这种权衡。",
    "tldr": "本论文分析了在强化学习和最优控制中观测时间以离散时间点固定周期到达的默认假设与实际情况下的连续时间系统之间的差异，并在LQR系统中揭示了近似误差和统计误差之间的基本权衡。在有限数据的情况下，管理时间分辨率可以显著改善策略评估的效率。",
    "en_tdlr": "This paper analyzes the difference between the default assumption of discrete observations in reinforcement learning and optimal control and the reality of continuous-time systems, and uncovers a fundamental trade-off between approximation error and statistical error in value estimation in LQR systems. Managing the temporal resolution can significantly improve the efficiency of policy evaluation with finite data."
}