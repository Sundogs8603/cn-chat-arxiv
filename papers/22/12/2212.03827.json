{
    "title": "Discovering Latent Knowledge in Language Models Without Supervision",
    "abstract": "arXiv:2212.03827v2 Announce Type: replace-cross  Abstract: Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms ",
    "link": "https://arxiv.org/abs/2212.03827",
    "context": "Title: Discovering Latent Knowledge in Language Models Without Supervision\nAbstract: arXiv:2212.03827v2 Announce Type: replace-cross  Abstract: Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms ",
    "path": "papers/22/12/2212.03827.json",
    "total_tokens": 883,
    "translated_title": "在不需要监督的情况下发现语言模型中的潜在知识",
    "translated_abstract": "训练语言模型的现有技术可能与真相不一致：如果我们用模仿学习训练模型，它们可能会重现人类的错误；如果我们训练它们生成人类评价高的文本，它们可能会输出人类评估者无法检测到的错误。我们提出通过在语言模型的内部激活中直接发现潜在知识的方式来规避这个问题，而且是纯粹无监督的方式。具体来说，我们引入了一种方法，能够准确回答只给定未标记模型激活的是非问题。该方法通过在激活空间中找到满足逻辑一致性属性的方向来工作，例如一个陈述及其否定具有相反的真值。我们展示，尽管没有使用监督和模型输出，我们的方法可以恢复大型语言模型中代表多样知识：在6个模型和10个问答数据集上，它表现优异。",
    "tldr": "通过在语言模型的内部激活中直接发现潜在知识的方式，我们提出了一种纯粹无监督的方法，可以准确回答未标记模型激活的是非问题，并且在大型语言模型中恢复多样知识。",
    "en_tdlr": "We propose a purely unsupervised method to discover latent knowledge inside language models by accurately answering yes-no questions given only unlabeled model activations, showing superior performance across various models and question-answering datasets."
}