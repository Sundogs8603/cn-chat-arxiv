{
    "title": "Autoregressive Bandits",
    "abstract": "arXiv:2212.06251v2 Announce Type: replace  Abstract: Autoregressive processes naturally arise in a large variety of real-world scenarios, including stock markets, sales forecasting, weather prediction, advertising, and pricing. When facing a sequential decision-making problem in such a context, the temporal dependence between consecutive observations should be properly accounted for guaranteeing convergence to the optimal policy. In this work, we propose a novel online learning setting, namely, Autoregressive Bandits (ARBs), in which the observed reward is governed by an autoregressive process of order $k$, whose parameters depend on the chosen action. We show that, under mild assumptions on the reward process, the optimal policy can be conveniently computed. Then, we devise a new optimistic regret minimization algorithm, namely, AutoRegressive Upper Confidence Bound (AR-UCB), that suffers sublinear regret of order $\\widetilde{\\mathcal{O}} \\left( \\frac{(k+1)^{3/2}\\sqrt{nT}}{(1-\\Gamma)^",
    "link": "https://arxiv.org/abs/2212.06251",
    "context": "Title: Autoregressive Bandits\nAbstract: arXiv:2212.06251v2 Announce Type: replace  Abstract: Autoregressive processes naturally arise in a large variety of real-world scenarios, including stock markets, sales forecasting, weather prediction, advertising, and pricing. When facing a sequential decision-making problem in such a context, the temporal dependence between consecutive observations should be properly accounted for guaranteeing convergence to the optimal policy. In this work, we propose a novel online learning setting, namely, Autoregressive Bandits (ARBs), in which the observed reward is governed by an autoregressive process of order $k$, whose parameters depend on the chosen action. We show that, under mild assumptions on the reward process, the optimal policy can be conveniently computed. Then, we devise a new optimistic regret minimization algorithm, namely, AutoRegressive Upper Confidence Bound (AR-UCB), that suffers sublinear regret of order $\\widetilde{\\mathcal{O}} \\left( \\frac{(k+1)^{3/2}\\sqrt{nT}}{(1-\\Gamma)^",
    "path": "papers/22/12/2212.06251.json",
    "total_tokens": 902,
    "translated_title": "自回归赌博机",
    "translated_abstract": "自回归过程在股票市场、销售预测、天气预测、广告和定价等各种实际场景中自然而然地出现。在面对这样的序贯决策问题时，应该正确考虑连续观测之间的时间依赖性，以保证收敛到最优策略。在这项工作中，我们提出了一种新颖的在线学习设置，即自回归赌博机（ARBs），其中观测到的奖励由一个阶数为$k$的自回归过程控制，其参数取决于选择的动作。我们证明，在对奖励过程进行温和假设的情况下，最优策略可以方便地计算出来。然后，我们设计了一种新的乐观遗憾最小化算法，即自回归上置信界（AR-UCB），其遗憾呈现出次线性的阶数$\\widetilde{\\mathcal{O}} \\left( \\frac{(k+1)^{3/2}\\sqrt{nT}}{(1-\\Gamma)^",
    "tldr": "在自回归过程控制的奖励下，提出了自回归赌博机（ARBs）在线学习设置，并设计了AutoRegressive Upper Confidence Bound (AR-UCB)算法，可以方便计算最优策略并具有次线性遗憾。"
}