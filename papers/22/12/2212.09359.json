{
    "title": "WACO: Word-Aligned Contrastive Learning for Speech Translation. (arXiv:2212.09359v2 [cs.CL] UPDATED)",
    "abstract": "End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model's performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data. Code is available at https://github.com/owaski/WACO.",
    "link": "http://arxiv.org/abs/2212.09359",
    "context": "Title: WACO: Word-Aligned Contrastive Learning for Speech Translation. (arXiv:2212.09359v2 [cs.CL] UPDATED)\nAbstract: End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model's performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data. Code is available at https://github.com/owaski/WACO.",
    "path": "papers/22/12/2212.09359.json",
    "total_tokens": 943,
    "translated_title": "WACO: 用于语音翻译的词对齐对比学习",
    "translated_abstract": "端到端语音翻译（E2E ST）旨在直接将源语音转化为目标文本。当仅有极少的语音文本数据用于训练时，现有的ST方法的表现很差。我们观察到ST模型的性能与其语音和源文本之间的嵌入相似度密切相关。在本文中，我们提出了一种名为Word-Aligned COntrastive learning（WACO）的简单有效的极低资源语音到文本翻译方法。我们的关键思想是通过对比学习来建立语音和文本模态的词级表示之间的桥梁。我们在MuST-C数据集上评估了WACO和其他方法，该数据集是一个广泛使用的ST基准，并在从IWSLT 2023获取的低资源方向的马耳他语-英语翻译上进行了评估。我们的实验表明，WACO仅使用1小时的并行ST数据，比最好的基准提高了9+ BLEU分。代码可在https://github.com/owaski/WACO上找到。",
    "tldr": "WACO是一种用于极低资源语音到文本翻译的简单而有效的方法，通过对比学习将语音和文本的词级表示连接起来，实验证明WACO在极低资源条件下比基线方法提高了9+ BLEU分。",
    "en_tdlr": "WACO is a simple and effective method for extremely low-resource speech-to-text translation. It bridges word-level representations for speech and text modalities via contrastive learning, and experiments show that WACO outperforms the baseline by 9+ BLEU points under low-resource conditions."
}