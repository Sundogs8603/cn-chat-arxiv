{
    "title": "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?. (arXiv:2212.09747v2 [cs.CL] UPDATED)",
    "abstract": "The CoNLL-2003 English named entity recognition (NER) dataset has been widely used to train and evaluate NER models for almost 20 years. However, it is unclear how well models that are trained on this 20-year-old data and developed over a period of decades using the same test set will perform when applied on modern data. In this paper, we evaluate the generalization of over 20 different models trained on CoNLL-2003, and show that NER models have very different generalization. Surprisingly, we find no evidence of performance degradation in pre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using decades-old data. We investigate why some models generalize well to new data while others do not, and attempt to disentangle the effects of temporal drift and overfitting due to test reuse. Our analysis suggests that most deterioration is due to temporal mismatch between the pre-training corpora and the downstream test sets. We found that four factors are important for good g",
    "link": "http://arxiv.org/abs/2212.09747",
    "context": "Title: Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?. (arXiv:2212.09747v2 [cs.CL] UPDATED)\nAbstract: The CoNLL-2003 English named entity recognition (NER) dataset has been widely used to train and evaluate NER models for almost 20 years. However, it is unclear how well models that are trained on this 20-year-old data and developed over a period of decades using the same test set will perform when applied on modern data. In this paper, we evaluate the generalization of over 20 different models trained on CoNLL-2003, and show that NER models have very different generalization. Surprisingly, we find no evidence of performance degradation in pre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using decades-old data. We investigate why some models generalize well to new data while others do not, and attempt to disentangle the effects of temporal drift and overfitting due to test reuse. Our analysis suggests that most deterioration is due to temporal mismatch between the pre-training corpora and the downstream test sets. We found that four factors are important for good g",
    "path": "papers/22/12/2212.09747.json",
    "total_tokens": 1046,
    "translated_title": "CoNLL-2003命名实体标注器在2023年仍然有效吗?",
    "translated_abstract": "CoNLL-2003英文命名实体识别（NER）数据集已经被广泛用于训练和评估NER模型几乎20年。然而，目前不清楚在现代数据上应用在此20年前的数据上训练并经过几十年发展的模型的性能如何。本文评估了超过20种在CoNLL-2003上训练的模型的泛化情况，并表明NER模型的泛化能力差异很大。令人惊讶的是，我们发现即使使用几十年前的数据进行微调，如RoBERTa和T5等预训练的Transformer模型仍然没有性能下降的证据。我们研究了为什么一些模型能够很好地泛化到新数据，而其他模型则不能，并试图解释由于测试集重用而导致的时间漂移和过拟合的影响。我们的分析表明，大部分恶化是由于预训练语料库与下游测试集之间的时间不匹配所致。我们发现四个因素对于良好的泛化性能很重要。",
    "tldr": "本文评估了CoNLL-2003上超过20种模型的泛化性能，发现NER模型的泛化能力各异。令人惊讶的是，即使使用几十年前的数据进行微调，预训练的Transformer模型仍然保持着很好的性能。研究还发现，预训练语料库与下游测试集之间的时间不匹配是导致性能恶化的主要因素。",
    "en_tdlr": "This paper evaluates the generalization of over 20 models trained on the CoNLL-2003 dataset and finds that NER models exhibit varying degrees of generalization. Surprisingly, even when fine-tuned with decades-old data, pre-trained Transformer models maintain good performance. The study also reveals that a mismatch in time between pre-training corpora and downstream test sets is the main factor contributing to performance degradation."
}