{
    "title": "Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding. (arXiv:2212.02802v2 [cs.CV] UPDATED)",
    "abstract": "Inspired by the impressive performance of recent face image editing methods, several studies have been naturally proposed to extend these methods to the face video editing task. One of the main challenges here is temporal consistency among edited frames, which is still unresolved. To this end, we propose a novel face video editing framework based on diffusion autoencoders that can successfully extract the decomposed features - for the first time as a face video editing model - of identity and motion from a given video. This modeling allows us to edit the video by simply manipulating the temporally invariant feature to the desired direction for the consistency. Another unique strength of our model is that, since our model is based on diffusion models, it can satisfy both reconstruction and edit capabilities at the same time, and is robust to corner cases in wild face videos (e.g. occluded faces) unlike the existing GAN-based methods.",
    "link": "http://arxiv.org/abs/2212.02802",
    "context": "Title: Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding. (arXiv:2212.02802v2 [cs.CV] UPDATED)\nAbstract: Inspired by the impressive performance of recent face image editing methods, several studies have been naturally proposed to extend these methods to the face video editing task. One of the main challenges here is temporal consistency among edited frames, which is still unresolved. To this end, we propose a novel face video editing framework based on diffusion autoencoders that can successfully extract the decomposed features - for the first time as a face video editing model - of identity and motion from a given video. This modeling allows us to edit the video by simply manipulating the temporally invariant feature to the desired direction for the consistency. Another unique strength of our model is that, since our model is based on diffusion models, it can satisfy both reconstruction and edit capabilities at the same time, and is robust to corner cases in wild face videos (e.g. occluded faces) unlike the existing GAN-based methods.",
    "path": "papers/22/12/2212.02802.json",
    "total_tokens": 825,
    "translated_title": "扩散视频自编码器：通过分解视频特征实现一致的人脸视频编辑",
    "translated_abstract": "受到最近面部图像编辑方法的惊人表现的启发，自然会有几项研究来扩展这些方法以应用于面部视频编辑任务。 这里的一个主要挑战是编辑帧之间的时间一致性，这仍然没有得到解决。 为此，我们提出了一种基于扩散自编码器的新型面部视频编辑框架，该框架可以成功地从给定的视频中提取出分解的特征-首次作为面部视频编辑模型-标识和运动。",
    "tldr": "本文提出了一种基于扩散自编码器的面部视频编辑框架，可以从给定的视频中提取出分解的特征，实现简单的特征调整来确保时间上的一致性。与现有的基于GAN的方法不同，该模型同时满足重建和编辑能力，并且对受野外面部视频的角落情况具有鲁棒性。",
    "en_tdlr": "The paper proposes a novel face video editing framework based on diffusion autoencoders, which can extract decomposed features that allow for easy feature manipulation to ensure temporal consistency. The model satisfies both reconstruction and editing capabilities due to being based on diffusion methods, showing greater robustness to occluded faces compared to existing GAN-based methods."
}