{
    "title": "Data-Efficient Finetuning Using Cross-Task Nearest Neighbors. (arXiv:2212.00196v2 [cs.CL] UPDATED)",
    "abstract": "Obtaining labeled data to train a model for a task of interest is often expensive. Prior work shows training models on multitask data augmented with task descriptions (prompts) effectively transfers knowledge to new tasks. Towards efficiently building task-specific models, we assume access to a small number (32-1000) of unlabeled target-task examples and use those to retrieve the most similar labeled examples from a large pool of multitask data augmented with prompts. Compared to the current practice of finetuning models on uniformly sampled prompted multitask data (e.g.: FLAN, T0), our approach of finetuning on cross-task nearest neighbors is significantly more data-efficient. Using only 2% of the data from the P3 pool without any labeled target-task data, our models outperform strong baselines trained on all available data by 3-30% on 12 out of 14 datasets representing held-out tasks including legal and scientific document QA. Similarly, models trained on cross-task nearest neighbors",
    "link": "http://arxiv.org/abs/2212.00196",
    "context": "Title: Data-Efficient Finetuning Using Cross-Task Nearest Neighbors. (arXiv:2212.00196v2 [cs.CL] UPDATED)\nAbstract: Obtaining labeled data to train a model for a task of interest is often expensive. Prior work shows training models on multitask data augmented with task descriptions (prompts) effectively transfers knowledge to new tasks. Towards efficiently building task-specific models, we assume access to a small number (32-1000) of unlabeled target-task examples and use those to retrieve the most similar labeled examples from a large pool of multitask data augmented with prompts. Compared to the current practice of finetuning models on uniformly sampled prompted multitask data (e.g.: FLAN, T0), our approach of finetuning on cross-task nearest neighbors is significantly more data-efficient. Using only 2% of the data from the P3 pool without any labeled target-task data, our models outperform strong baselines trained on all available data by 3-30% on 12 out of 14 datasets representing held-out tasks including legal and scientific document QA. Similarly, models trained on cross-task nearest neighbors",
    "path": "papers/22/12/2212.00196.json",
    "total_tokens": 927,
    "translated_title": "采用跨任务最近邻的数据高效微调方法",
    "translated_abstract": "获取用于训练感兴趣任务的标记数据通常代价高昂。先前的研究表明，在多任务数据上进行训练并加上任务描述（提示）有效地将知识传递给新任务。为了有效地构建任务特定模型，我们假设可以访问少量（32-1000）未标记的目标任务示例，并使用这些示例从包含提示的大量多任务数据中检索最相似的标记示例。与当前在均匀采样提示任务多任务数据（例如：FLAN、T0）上微调模型的做法相比，我们的方法在数据效率上显著更高。在没有任何标记的目标任务数据的情况下，仅使用 P3 池中 2% 的数据，我们的模型在 12 个代表保留任务的数据集（包括法律和科学文档 QA）中的性能要比在所有可用数据上进行训练的强基准模型高出 3-30%。采用跨任务最近邻训练的模型效果类似。",
    "tldr": "本文提出一种通过跨任务最近邻进行数据高效微调的方法，通过仅使用少量目标任务数据和多任务数据中的最相似标记数据，避免了对大量标记数据的需求，取得了比强基准模型更好的效果。",
    "en_tdlr": "This paper proposes a data-efficient finetuning approach using cross-task nearest neighbors, which achieves better performance than strong baselines by exploiting a small number of target-task examples and retrieving the most similar labeled examples from a large pool of multitask data without requiring a large amount of labeled data."
}