{
    "title": "UnICLAM:Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering. (arXiv:2212.10729v3 [cs.CV] UPDATED)",
    "abstract": "Medical Visual Question Answering (Medical-VQA) aims to to answer clinical questions regarding radiology images, assisting doctors with decision-making options. Nevertheless, current Medical-VQA models learn cross-modal representations through residing vision and texture encoders in dual separate spaces, which lead to indirect semantic alignment. In this paper, we propose UnICLAM, a Unified and Interpretable Medical-VQA model through Contrastive Representation Learning with Adversarial Masking. Specifically, to learn an aligned image-text representation, we first establish a unified dual-stream pre-training structure with the gradually soft-parameter sharing strategy. Technically, the proposed strategy learns a constraint for the vision and texture encoders to be close in a same space, which is gradually loosened as the higher number of layers. Moreover, for grasping the unified semantic representation, we extend the adversarial masking data augmentation to the contrastive representati",
    "link": "http://arxiv.org/abs/2212.10729",
    "context": "Title: UnICLAM:Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering. (arXiv:2212.10729v3 [cs.CV] UPDATED)\nAbstract: Medical Visual Question Answering (Medical-VQA) aims to to answer clinical questions regarding radiology images, assisting doctors with decision-making options. Nevertheless, current Medical-VQA models learn cross-modal representations through residing vision and texture encoders in dual separate spaces, which lead to indirect semantic alignment. In this paper, we propose UnICLAM, a Unified and Interpretable Medical-VQA model through Contrastive Representation Learning with Adversarial Masking. Specifically, to learn an aligned image-text representation, we first establish a unified dual-stream pre-training structure with the gradually soft-parameter sharing strategy. Technically, the proposed strategy learns a constraint for the vision and texture encoders to be close in a same space, which is gradually loosened as the higher number of layers. Moreover, for grasping the unified semantic representation, we extend the adversarial masking data augmentation to the contrastive representati",
    "path": "papers/22/12/2212.10729.json",
    "total_tokens": 915,
    "translated_title": "UnICLAM：对抗性屏蔽的对比表示学习用于统一和可解释的医学视觉问答",
    "translated_abstract": "医学视觉问答（Medical-VQA）旨在回答有关放射学图像的临床问题，为医生提供决策选项。然而，当前的Medical-VQA模型通过将视觉和纹理编码器分别放置在双独立空间中来学习跨模态表示，这导致间接的语义对齐。在本文中，我们提出了UnICLAM，一种通过对比表示学习和对抗性屏蔽实现统一和可解释的医学视觉问答模型。具体来说，为了学习对齐的图像-文本表示，我们首先建立了一个统一的双流预训练结构，采用逐渐软参数共享策略。技术上，所提出的策略学习了一个约束，使得视觉和纹理编码器在同一空间中接近，随着层数的增加，逐渐放松。此外，为了把握统一的语义表示，我们将对抗性屏蔽数据增强拓展到对比表示中。",
    "tldr": "UnICLAM是一种统一和可解释的医学视觉问答模型，通过对比表示学习和对抗性屏蔽，实现了图像和文本之间的对齐和语义表示。"
}