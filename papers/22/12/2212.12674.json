{
    "title": "Data-Driven Linear Complexity Low-Rank Approximation of General Kernel Matrices: A Geometric Approach. (arXiv:2212.12674v2 [math.NA] UPDATED)",
    "abstract": "A general, {\\em rectangular} kernel matrix may be defined as $K_{ij} = \\kappa(x_i,y_j)$ where $\\kappa(x,y)$ is a kernel function and where $X=\\{x_i\\}_{i=1}^m$ and $Y=\\{y_i\\}_{i=1}^n$ are two sets of points. In this paper, we seek a low-rank approximation to a kernel matrix where the sets of points $X$ and $Y$ are large and are arbitrarily distributed, such as away from each other, ``intermingled'', identical, etc. Such rectangular kernel matrices may arise, for example, in Gaussian process regression where $X$ corresponds to the training data and $Y$ corresponds to the test data. In this case, the points are often high-dimensional. Since the point sets are large, we must exploit the fact that the matrix arises from a kernel function, and avoid forming the matrix, and thus ruling out most algebraic techniques. In particular, we seek methods that can scale linearly or nearly linear with respect to the size of data for a fixed approximation rank. The main idea in this paper is to {\\em geo",
    "link": "http://arxiv.org/abs/2212.12674",
    "context": "Title: Data-Driven Linear Complexity Low-Rank Approximation of General Kernel Matrices: A Geometric Approach. (arXiv:2212.12674v2 [math.NA] UPDATED)\nAbstract: A general, {\\em rectangular} kernel matrix may be defined as $K_{ij} = \\kappa(x_i,y_j)$ where $\\kappa(x,y)$ is a kernel function and where $X=\\{x_i\\}_{i=1}^m$ and $Y=\\{y_i\\}_{i=1}^n$ are two sets of points. In this paper, we seek a low-rank approximation to a kernel matrix where the sets of points $X$ and $Y$ are large and are arbitrarily distributed, such as away from each other, ``intermingled'', identical, etc. Such rectangular kernel matrices may arise, for example, in Gaussian process regression where $X$ corresponds to the training data and $Y$ corresponds to the test data. In this case, the points are often high-dimensional. Since the point sets are large, we must exploit the fact that the matrix arises from a kernel function, and avoid forming the matrix, and thus ruling out most algebraic techniques. In particular, we seek methods that can scale linearly or nearly linear with respect to the size of data for a fixed approximation rank. The main idea in this paper is to {\\em geo",
    "path": "papers/22/12/2212.12674.json",
    "total_tokens": 1020,
    "translated_title": "基于数据驱动的一般核矩阵线性复杂度低秩逼近：一种几何方法",
    "translated_abstract": "一般的矩形核矩阵可以定义为 $K_{ij} = \\kappa(x_i,y_j)$，其中 $\\kappa(x,y)$ 是一个核函数，$X=\\{x_i\\}_{i=1}^m$ 和 $Y=\\{y_i\\}_{i=1}^n$ 是两组点集。本文旨在寻找一个核矩阵的低秩逼近，其中点集 $X$ 和 $Y$ 是大规模而任意分布的，比如相距远离、交错分布、相同等等。这样的矩形核矩阵可能出现在高斯过程回归中，其中 $X$ 对应训练数据，$Y$ 对应测试数据。在这种情况下，点集通常是高维的。由于点集很大，我们必须利用矩阵来自于核函数的事实，并避免形成矩阵，从而排除了大多数代数技术。特别地，我们寻求能够以固定逼近秩为代价线性或近乎线性扩展的方法。本文的主要思想是使用几何方法来近似线性或近乎线性地表示矩阵的内部结构和模式。",
    "tldr": "本文提出了一种基于几何方法的数据驱动线性复杂度低秩逼近算法，适用于大规模、任意分布的矩形核矩阵，可用于高斯过程回归等应用场景。",
    "en_tdlr": "This paper presents a geometric approach for data-driven linear complexity low-rank approximation of general kernel matrices, which is suitable for large-scale, arbitrarily distributed rectangular kernel matrices and can be applied in Gaussian process regression and other applications."
}