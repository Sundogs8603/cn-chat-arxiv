{
    "title": "Elixir: Train a Large Language Model on a Small GPU Cluster. (arXiv:2212.05339v3 [cs.DC] UPDATED)",
    "abstract": "In recent years, large language models have achieved great success due to their unprecedented size. However, training these models poses a challenge for most researchers as it requires a substantial number of GPUs. To reduce GPU memory usage, memory partitioning, and memory offloading have been proposed. These approaches eliminate memory redundancies and offload memory usage to the CPU and NVMe memory, respectively, enabling training on small GPU clusters. However, directly deploying these solutions often leads to suboptimal efficiency. Only experienced experts can unleash the full potential of hardware by carefully tuning the distributed configuration. Thus, we present a novel solution, Elixir, which automates efficient large-model training based on pre-runtime model profiling. Elixir aims to identify the optimal combination of partitioning and offloading techniques to maximize training throughput. In our experiments, Elixir significantly outperforms the current state-of-the-art basel",
    "link": "http://arxiv.org/abs/2212.05339",
    "context": "Title: Elixir: Train a Large Language Model on a Small GPU Cluster. (arXiv:2212.05339v3 [cs.DC] UPDATED)\nAbstract: In recent years, large language models have achieved great success due to their unprecedented size. However, training these models poses a challenge for most researchers as it requires a substantial number of GPUs. To reduce GPU memory usage, memory partitioning, and memory offloading have been proposed. These approaches eliminate memory redundancies and offload memory usage to the CPU and NVMe memory, respectively, enabling training on small GPU clusters. However, directly deploying these solutions often leads to suboptimal efficiency. Only experienced experts can unleash the full potential of hardware by carefully tuning the distributed configuration. Thus, we present a novel solution, Elixir, which automates efficient large-model training based on pre-runtime model profiling. Elixir aims to identify the optimal combination of partitioning and offloading techniques to maximize training throughput. In our experiments, Elixir significantly outperforms the current state-of-the-art basel",
    "path": "papers/22/12/2212.05339.json",
    "total_tokens": 926,
    "translated_title": "Elixir: 在小型 GPU 集群上训练大型语言模型",
    "translated_abstract": "近年来，由于其规模前所未有的大小，大型语言模型取得了巨大的成功。然而，训练这些模型对于大多数研究人员来说是一项挑战，因为它需要大量的 GPU。为了减少 GPU 内存使用，提出了内存分区和内存卸载。这些方法消除了内存冗余，并将内存使用卸载到 CPU 和 NVMe 存储器中，使得可以在小型 GPU 集群上进行训练。然而，直接部署这些解决方案通常会导致次优效率。只有经验丰富的专家才能通过仔细调整分布式配置来充分发挥硬件的潜力。因此，我们提出了一种新颖的解决方案 Elixir，它基于预运行模型分析自动化高效的大模型训练。Elixir 的目标是确定分区和卸载技术的最佳组合，以最大化训练吞吐量。在我们的实验中，Elixir 显著优于当前最先进的基准",
    "tldr": "Elixir 提出了一种基于预运行模型分析的自动化高效大模型训练方案，可以将内存使用卸载到 CPU 和 NVMe 存储器中，充分发挥硬件的潜力，实验中表现优于最先进的基准模型。",
    "en_tdlr": "Elixir proposes an automated and efficient solution for large-model training based on pre-runtime model profiling, which offloads memory usage to CPU and NVMe memory, maximizing hardware potential. Elixir outperforms current state-of-the-art baselines in experiments."
}