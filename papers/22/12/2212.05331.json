{
    "title": "Effects of Spectral Normalization in Multi-agent Reinforcement Learning. (arXiv:2212.05331v2 [cs.LG] UPDATED)",
    "abstract": "A reliable critic is central to on-policy actor-critic learning. But it becomes challenging to learn a reliable critic in a multi-agent sparse reward scenario due to two factors: 1) The joint action space grows exponentially with the number of agents 2) This, combined with the reward sparseness and environment noise, leads to large sample requirements for accurate learning. We show that regularising the critic with spectral normalization (SN) enables it to learn more robustly, even in multi-agent on-policy sparse reward scenarios. Our experiments show that the regularised critic is quickly able to learn from the sparse rewarding experience in the complex SMAC and RWARE domains. These findings highlight the importance of regularisation in the critic for stable learning.",
    "link": "http://arxiv.org/abs/2212.05331",
    "context": "Title: Effects of Spectral Normalization in Multi-agent Reinforcement Learning. (arXiv:2212.05331v2 [cs.LG] UPDATED)\nAbstract: A reliable critic is central to on-policy actor-critic learning. But it becomes challenging to learn a reliable critic in a multi-agent sparse reward scenario due to two factors: 1) The joint action space grows exponentially with the number of agents 2) This, combined with the reward sparseness and environment noise, leads to large sample requirements for accurate learning. We show that regularising the critic with spectral normalization (SN) enables it to learn more robustly, even in multi-agent on-policy sparse reward scenarios. Our experiments show that the regularised critic is quickly able to learn from the sparse rewarding experience in the complex SMAC and RWARE domains. These findings highlight the importance of regularisation in the critic for stable learning.",
    "path": "papers/22/12/2212.05331.json",
    "total_tokens": 931,
    "translated_title": "多智能体强化学习中的谱归一化效应",
    "translated_abstract": "在多智能体稀疏奖励场景下，一个可靠的评论者对于在策略上实现演员-评论者学习至关重要。然而，由于两个因素，学习一个可靠的评论者变得具有挑战性：1）随着智能体数量的增加，联合作用空间呈指数增长；2）这个因素结合奖励稀疏和环境噪声，需要大量的样本数才能实现准确的学习。我们表明，用谱归一化(SN)对评论家进行规范化，使它能够在多智能体策略上的稀疏奖励场景中更加稳健地学习。我们的实验表明，规范化的评论家能够快速地从复杂的SMAC和RWARE领域的稀缺奖励经历中学习到。这些发现强调了评论家规范化在稳定学习中的重要性。",
    "tldr": "本文研究多智能体强化学习中评论者的规范化方法，发现使用谱归一化(SN)可以使评论家更稳健地学习，特别是在稀疏奖励场景下。实验结果表明，在规范化评论家的情况下，能够快速轻松地从那些复杂情境中学习，这一点对于实现稳定学习非常重要。",
    "en_tdlr": "This paper investigates the spectral normalization method for regularizing the critic in multi-agent reinforcement learning, and finds that the regularized critic with spectral normalization is more robust in learning, especially in sparse reward scenarios. Experimental results show that the regularized critic can quickly learn from complex scenarios, highlighting the importance of regularization in achieving stable learning."
}