{
    "title": "Pretraining Without Attention. (arXiv:2212.10544v2 [cs.CL] UPDATED)",
    "abstract": "Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT in terms of interactions and syntactic representations. All models from this work are available at https://git",
    "link": "http://arxiv.org/abs/2212.10544",
    "context": "Title: Pretraining Without Attention. (arXiv:2212.10544v2 [cs.CL] UPDATED)\nAbstract: Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT in terms of interactions and syntactic representations. All models from this work are available at https://git",
    "path": "papers/22/12/2212.10544.json",
    "total_tokens": 947,
    "translated_title": "不依赖注意力机制的预训练",
    "translated_abstract": "在自然语言处理中，Transformer模型是预训练中取得成功的关键。虽然也有其他架构被用于预训练，但下游任务的准确率要么显著下降，要么需要注意力机制才能达到标准测试的基准（如GLUE）。本文探讨了一种不依赖注意力机制的预训练方法，采用最近在基于状态空间模型（SSM）的序列路由方面的进展。我们提出的模型Bidirectional Gated SSM（BiGS）结合了SSM层和乘性门控架构，这在简化序列建模架构中已经被证明是有效的。该模型学习不考虑成对交互的静态层。即使如此，BiGS能够达到与BERT预训练准确度相当的GLUE测试结果，并且可以在不进行近似的情况下扩展到4096个标记的长形式预训练。分析表明，尽管这些模型的平均准确率相似，但与BERT相比，这种方法在交互和句法表示方面具有不同的归纳偏差。本文所有模型可在 https://git 上获得。",
    "tldr": "本文通过使用基于状态空间模型的序列路由方法提出了一种不依赖注意力机制的预训练模型BiGS，可以达到与BERT预训练准确度相当的GLUE测试结果，并具有不同的归纳偏差。",
    "en_tdlr": "This paper proposes a pretraining model called BiGS that does not rely on attention mechanism but uses sequence routing based on state-space models. The model achieves similar performance to BERT on GLUE benchmark and has different inductive biases in terms of interactions and syntactic representations."
}