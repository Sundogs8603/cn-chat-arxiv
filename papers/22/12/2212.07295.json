{
    "title": "Maximal Initial Learning Rates in Deep ReLU Networks. (arXiv:2212.07295v2 [stat.ML] UPDATED)",
    "abstract": "Training a neural network requires choosing a suitable learning rate, which involves a trade-off between speed and effectiveness of convergence. While there has been considerable theoretical and empirical analysis of how large the learning rate can be, most prior work focuses only on late-stage training. In this work, we introduce the maximal initial learning rate $\\eta^{\\ast}$ - the largest learning rate at which a randomly initialized neural network can successfully begin training and achieve (at least) a given threshold accuracy. Using a simple approach to estimate $\\eta^{\\ast}$, we observe that in constant-width fully-connected ReLU networks, $\\eta^{\\ast}$ behaves differently from the maximum learning rate later in training. Specifically, we find that $\\eta^{\\ast}$ is well predicted as a power of depth $\\times$ width, provided that (i) the width of the network is sufficiently large compared to the depth, and (ii) the input layer is trained at a relatively small learning rate. We fu",
    "link": "http://arxiv.org/abs/2212.07295",
    "context": "Title: Maximal Initial Learning Rates in Deep ReLU Networks. (arXiv:2212.07295v2 [stat.ML] UPDATED)\nAbstract: Training a neural network requires choosing a suitable learning rate, which involves a trade-off between speed and effectiveness of convergence. While there has been considerable theoretical and empirical analysis of how large the learning rate can be, most prior work focuses only on late-stage training. In this work, we introduce the maximal initial learning rate $\\eta^{\\ast}$ - the largest learning rate at which a randomly initialized neural network can successfully begin training and achieve (at least) a given threshold accuracy. Using a simple approach to estimate $\\eta^{\\ast}$, we observe that in constant-width fully-connected ReLU networks, $\\eta^{\\ast}$ behaves differently from the maximum learning rate later in training. Specifically, we find that $\\eta^{\\ast}$ is well predicted as a power of depth $\\times$ width, provided that (i) the width of the network is sufficiently large compared to the depth, and (ii) the input layer is trained at a relatively small learning rate. We fu",
    "path": "papers/22/12/2212.07295.json",
    "total_tokens": 930,
    "translated_title": "深层ReLU网络中的最大初始学习率",
    "translated_abstract": "训练神经网络需要选择适当的学习率，这涉及到速度和有效收敛之间的权衡。尽管对于学习率可以有多大进行了相当大量的理论和实证分析，但大多数先前的工作只关注于后期训练。在这项工作中，我们引入了最大初始学习率$\\eta^{*}$——在这个学习率下，一个随机初始化的神经网络可以成功地开始训练并达到（至少）一个给定的阈值精度。使用简单的方法估计$\\eta^{*}$，我们观察到，在恒定宽度的完全连接的ReLU网络中，$\\eta^{*}$的行为与训练后期的最大学习率不同。具体而言，我们发现，$\\eta^{*}$可以很好地预测为深度$\\times$宽度的幂次，前提是（i）网络的宽度相对深度足够大，（ii）输入层以相对较小的学习率进行训练。",
    "tldr": "本文针对深度学习中的学习率问题，提出了最大初始学习率的概念，并发现其行为与训练后期的最大学习率不同。我们得出结论：在一定条件下，最大初始学习率可以很好地预测为深度×宽度的幂次。",
    "en_tdlr": "This paper introduces the concept of maximal initial learning rate for deep learning and shows its behavior is different from the maximum learning rate later in training. The conclusion is that, under certain conditions, maximal initial learning rate can be well predicted as a power of depth x width."
}