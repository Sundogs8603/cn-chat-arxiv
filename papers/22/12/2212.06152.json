{
    "title": "Accelerating Dataset Distillation via Model Augmentation. (arXiv:2212.06152v2 [cs.LG] UPDATED)",
    "abstract": "Dataset Distillation (DD), a newly emerging field, aims at generating much smaller but efficient synthetic training datasets from large ones. Existing DD methods based on gradient matching achieve leading performance; however, they are extremely computationally intensive as they require continuously optimizing a dataset among thousands of randomly initialized models. In this paper, we assume that training the synthetic data with diverse models leads to better generalization performance. Thus we propose two model augmentation techniques, i.e. using early-stage models and parameter perturbation to learn an informative synthetic set with significantly reduced training cost. Extensive experiments demonstrate that our method achieves up to 20x speedup and comparable performance on par with state-of-the-art methods.",
    "link": "http://arxiv.org/abs/2212.06152",
    "context": "Title: Accelerating Dataset Distillation via Model Augmentation. (arXiv:2212.06152v2 [cs.LG] UPDATED)\nAbstract: Dataset Distillation (DD), a newly emerging field, aims at generating much smaller but efficient synthetic training datasets from large ones. Existing DD methods based on gradient matching achieve leading performance; however, they are extremely computationally intensive as they require continuously optimizing a dataset among thousands of randomly initialized models. In this paper, we assume that training the synthetic data with diverse models leads to better generalization performance. Thus we propose two model augmentation techniques, i.e. using early-stage models and parameter perturbation to learn an informative synthetic set with significantly reduced training cost. Extensive experiments demonstrate that our method achieves up to 20x speedup and comparable performance on par with state-of-the-art methods.",
    "path": "papers/22/12/2212.06152.json",
    "total_tokens": 776,
    "translated_title": "模型增强下的数据集蒸馏加速",
    "translated_abstract": "数据集蒸馏（DD）是一个新兴的领域，旨在从大型原始数据集中产生更小但高效的合成训练数据集。现有的基于梯度匹配的DD方法达到了领先的性能；但是，它们非常计算密集，因为他们需要在成千上万个随机初始化模型中不断优化数据集。在本文中，我们假设使用多种模型对合成数据进行训练可以获得更好的泛化性能。因此，我们提出了两种模型增强技术，即使用早期模型和参数扰动来学习具有显着降低训练成本的信息合成集。广泛的实验表明，我们的方法实现了高达20倍的加速，并且与最先进的方法具有相当的性能。",
    "tldr": "本文提出了两种模型增强技术，即使用早期模型和参数扰动，以显着降低训练成本的方式优化数据集蒸馏，实现了高达20倍的加速。",
    "en_tdlr": "This paper proposes two model augmentation techniques, using early-stage models and parameter perturbation, to optimize dataset distillation with significantly reduced training cost, achieving up to 20x speedup while maintaining comparable performance with state-of-the-art methods."
}