{
    "title": "IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation metrics for Indian Languages. (arXiv:2212.10180v2 [cs.CL] UPDATED)",
    "abstract": "The rapid growth of machine translation (MT) systems has necessitated comprehensive studies to meta-evaluate evaluation metrics being used, which enables a better selection of metrics that best reflect MT quality. Unfortunately, most of the research focuses on high-resource languages, mainly English, the observations for which may not always apply to other languages. Indian languages, having over a billion speakers, are linguistically different from English, and to date, there has not been a systematic study of evaluating MT systems from English into Indian languages. In this paper, we fill this gap by creating an MQM dataset consisting of 7000 fine-grained annotations, spanning 5 Indian languages and 7 MT systems, and use it to establish correlations between annotator scores and scores obtained using existing automatic metrics. Our results show that pre-trained metrics, such as COMET, have the highest correlations with annotator scores. Additionally, we find that the metrics do not ad",
    "link": "http://arxiv.org/abs/2212.10180",
    "context": "Title: IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation metrics for Indian Languages. (arXiv:2212.10180v2 [cs.CL] UPDATED)\nAbstract: The rapid growth of machine translation (MT) systems has necessitated comprehensive studies to meta-evaluate evaluation metrics being used, which enables a better selection of metrics that best reflect MT quality. Unfortunately, most of the research focuses on high-resource languages, mainly English, the observations for which may not always apply to other languages. Indian languages, having over a billion speakers, are linguistically different from English, and to date, there has not been a systematic study of evaluating MT systems from English into Indian languages. In this paper, we fill this gap by creating an MQM dataset consisting of 7000 fine-grained annotations, spanning 5 Indian languages and 7 MT systems, and use it to establish correlations between annotator scores and scores obtained using existing automatic metrics. Our results show that pre-trained metrics, such as COMET, have the highest correlations with annotator scores. Additionally, we find that the metrics do not ad",
    "path": "papers/22/12/2212.10180.json",
    "total_tokens": 993,
    "translated_title": "IndicMT Eval：一份用于对印度语言机器翻译评价指标进行元评估的数据集",
    "translated_abstract": "机器翻译系统的快速增长需要进行全面的研究来对正在使用的评估指标进行元评估，从而能够更好地选择最能反映机器翻译质量的指标。遗憾的是，大部分研究都集中在高资源语言，主要是英文，其中的观察结果并不总是适用于其他语言。印度语言与英文在语言上存在差异，并且迄今为止尚未对从英文到印度语言的机器翻译系统进行系统性研究。本文通过创建一个包含7000个细粒度注释的MQM数据集，涵盖了5种印度语言和7个机器翻译系统，并使用该数据集建立了注释者分数和现有自动评估指标得分之间的相关性。我们的结果表明，预训练的指标（如COMET）与注释者分数的相关性最高。此外，我们还发现这些评估指标不总是能准确地评估印度语言的机器翻译质量。",
    "tldr": "本文填补了对从英文到印度语言的机器翻译系统进行系统性研究的空白，通过创建包含7000个细粒度注释的MQM数据集，发现预训练的指标与注释者分数具有最高的相关性，并指出现有的评估指标不能准确评估印度语言的机器翻译质量。",
    "en_tdlr": "This paper fills the gap in systematic studies of machine translation systems from English to Indian languages by creating an MQM dataset with 7000 fine-grained annotations. It finds that pre-trained metrics have the highest correlations with annotator scores and highlights the limitations of existing evaluation metrics in accurately evaluating machine translation quality for Indian languages."
}