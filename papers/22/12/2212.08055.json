{
    "title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units. (arXiv:2212.08055v2 [cs.CL] UPDATED)",
    "abstract": "Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However",
    "link": "http://arxiv.org/abs/2212.08055",
    "context": "Title: UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units. (arXiv:2212.08055v2 [cs.CL] UPDATED)\nAbstract: Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However",
    "path": "papers/22/12/2212.08055.json",
    "total_tokens": 882,
    "translated_title": "UnitY: 用离散单元进行两遍直接语音翻译",
    "translated_abstract": "直接语音到语音翻译具有优化的组件和简化的流程，比级联方法更具优势。我们提出了一种新颖的两遍直接语音到语音翻译架构UnitY，首先生成文本表示，其次预测离散的声学单元。通过第一遍解码器的子词预测、高级的两遍解码器架构设计和搜索策略以及更好的训练正则化来提高模型性能。为了利用大量未标记的文本数据，我们基于自监督去噪自编码任务对第一遍文本解码器进行预训练。在各种数据规模的基准数据集上进行实验验证，UnitY比单遍语音到单元翻译模型的ASR-BLEU提高了2.5-4.2个，并且解码速度提高了2.83倍。我们还展示了在第二遍预测频谱时，所提出的方法可以提高性能。",
    "tldr": "UnitY是一种通过两遍翻译生成最佳结果的语音翻译方法，能够比单遍语音到单元翻译模型在ASR-BLEU值和解码速度上表现更好。",
    "en_tdlr": "UnitY is a direct two-pass speech-to-speech translation method that generates the best results, outperforming single-pass speech-to-unit translation models in terms of ASR-BLEU values and decoding speed."
}