{
    "title": "Dense Hebbian neural networks: a replica symmetric picture of supervised learning. (arXiv:2212.00606v2 [cond-mat.dis-nn] UPDATED)",
    "abstract": "We consider dense, associative neural-networks trained by a teacher (i.e., with supervision) and we investigate their computational capabilities analytically, via statistical-mechanics of spin glasses, and numerically, via Monte Carlo simulations. In particular, we obtain a phase diagram summarizing their performance as a function of the control parameters such as quality and quantity of the training dataset, network storage and noise, that is valid in the limit of large network size and structureless datasets: these networks may work in a ultra-storage regime (where they can handle a huge amount of patterns, if compared with shallow neural networks) or in a ultra-detection regime (where they can perform pattern recognition at prohibitive signal-to-noise ratios, if compared with shallow neural networks). Guided by the random theory as a reference framework, we also test numerically learning, storing and retrieval capabilities shown by these networks on structured datasets as MNist and ",
    "link": "http://arxiv.org/abs/2212.00606",
    "context": "Title: Dense Hebbian neural networks: a replica symmetric picture of supervised learning. (arXiv:2212.00606v2 [cond-mat.dis-nn] UPDATED)\nAbstract: We consider dense, associative neural-networks trained by a teacher (i.e., with supervision) and we investigate their computational capabilities analytically, via statistical-mechanics of spin glasses, and numerically, via Monte Carlo simulations. In particular, we obtain a phase diagram summarizing their performance as a function of the control parameters such as quality and quantity of the training dataset, network storage and noise, that is valid in the limit of large network size and structureless datasets: these networks may work in a ultra-storage regime (where they can handle a huge amount of patterns, if compared with shallow neural networks) or in a ultra-detection regime (where they can perform pattern recognition at prohibitive signal-to-noise ratios, if compared with shallow neural networks). Guided by the random theory as a reference framework, we also test numerically learning, storing and retrieval capabilities shown by these networks on structured datasets as MNist and ",
    "path": "papers/22/12/2212.00606.json",
    "total_tokens": 953,
    "translated_title": "密集式Hebbian神经网络：监督学习的对称图片",
    "translated_abstract": "我们考虑由教师（即监督学习）训练的密集的关联神经网络，并通过自旋玻璃的统计力学分析和蒙特卡罗模拟来研究它们的计算能力。特别地，我们得到了一个相图，总结了它们的性能如训练数据集的质量和数量、网络存储和噪声等控制参数的函数，这在网络尺寸大、数据集结构简单的极限下是有效的：这些网络可以在超大存储区域工作（与浅层神经网络相比，它们可以处理大量的模式），或者在超高检测区域工作（与浅层神经网络相比，它们可以在极低的信噪比下进行模式识别）。在以随机理论作为参考框架的指导下，我们还对这些网络在结构化数据集（如MNist）上展示的学习、存储和检索能力进行了数值测试。",
    "tldr": "这篇论文研究了通过教师训练的密集Hebbian神经网络的计算能力，通过统计力学和蒙特卡罗模拟得到了一个相图，指出这些网络在大规模和结构简单的数据集下可以在超大存储或超高检测区域工作。",
    "en_tdlr": "This paper investigates the computational capabilities of dense Hebbian neural networks trained by a teacher, and obtains a phase diagram summarizing their performance in terms of storage and detection capabilities. These networks can work in an ultra-storage regime or an ultra-detection regime, outperforming shallow neural networks in handling large amounts of patterns or performing pattern recognition at low signal-to-noise ratios."
}