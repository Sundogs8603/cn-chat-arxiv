{
    "title": "On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning. (arXiv:2212.09170v2 [cs.CL] UPDATED)",
    "abstract": "Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide future designs of sentence representation learning methods by taking a closer look at contrastive SRL through the lens of isotropy, contextualization and learning dynamics. We interpret its successes through the geometry of the representation shifts and show that contrastive learning brings isotropy, and drives high intra-sentence similarity: when in the same sentence, tokens converge to similar positions in the semantic space. We also find that what we formalize as \"spurious contextualization\" is mitigated for semantically meaningful tokens, while augmented for functional ones. We find that the embedding space is directed towards the origin during training, with more areas now better define",
    "link": "http://arxiv.org/abs/2212.09170",
    "context": "Title: On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning. (arXiv:2212.09170v2 [cs.CL] UPDATED)\nAbstract: Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide future designs of sentence representation learning methods by taking a closer look at contrastive SRL through the lens of isotropy, contextualization and learning dynamics. We interpret its successes through the geometry of the representation shifts and show that contrastive learning brings isotropy, and drives high intra-sentence similarity: when in the same sentence, tokens converge to similar positions in the semantic space. We also find that what we formalize as \"spurious contextualization\" is mitigated for semantically meaningful tokens, while augmented for functional ones. We find that the embedding space is directed towards the origin during training, with more areas now better define",
    "path": "papers/22/12/2212.09170.json",
    "total_tokens": 933,
    "translated_title": "关于对比句子表示学习的各向同性、情境化和学习动态",
    "translated_abstract": "将对比学习目标纳入句子表示学习中，在许多句子级自然语言处理任务中取得了显著的改进。本文通过各向同性、情境化和学习动态的视角来剖析对比句子表示学习的表现，旨在为未来设计句子表示学习方法提供指导。作者通过表示变换的几何学来解释对比学习的成功，并展示对比学习如何带来各向同性并导致同一句子中的标记在语义空间中收敛到相似的位置。研究还发现，对于语义上有意义的标记，\"虚假的情境化\"得到了缓解，而对于功能性标记则被增强。训练过程中，嵌入空间朝向原点并更好地定义了更多区域。",
    "tldr": "本文通过几何学角度在对比句子表示学习中发现，对比学习带来了各向同性，并驱动同一句子中标记在语义空间中收敛到相似的位置。对于语义上有意义的标记，\"虚假的情境化\"得到了缓解，而对于功能性标记则被增强。",
    "en_tdlr": "This paper investigates contrastive-based sentence representation learning from the perspectives of isotropy, contextualization, and learning dynamics. By analyzing the geometry of representation shifts, the authors find that contrastive learning brings isotropy and drives intra-sentence similarity. They also discover that \"spurious contextualization\" is mitigated for semantically meaningful tokens while augmented for functional ones. The embedding space is directed towards the origin during training, with more areas better defined."
}