{
    "title": "Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment. (arXiv:2212.10549v2 [cs.CL] UPDATED)",
    "abstract": "Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., \"mug in grass\") with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the directed language attention from 'mug' to 'grass' (capturing the semantic relation 'in') to match the directed visual attention from the mug to the grass. Tokens and their corresponding objects are softly identified using the cross-modal attention. We prove that this notion of soft relation alignment is equivalent to enforcing congruence between vision and language attention matrices under a 'change of basis' provided by the cross-modal a",
    "link": "http://arxiv.org/abs/2212.10549",
    "context": "Title: Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment. (arXiv:2212.10549v2 [cs.CL] UPDATED)\nAbstract: Despite recent progress towards scaling up multimodal vision-language models, these models are still known to struggle on compositional generalization benchmarks such as Winoground. We find that a critical component lacking from current vision-language models is relation-level alignment: the ability to match directional semantic relations in text (e.g., \"mug in grass\") with spatial relationships in the image (e.g., the position of the mug relative to the grass). To tackle this problem, we show that relation alignment can be enforced by encouraging the directed language attention from 'mug' to 'grass' (capturing the semantic relation 'in') to match the directed visual attention from the mug to the grass. Tokens and their corresponding objects are softly identified using the cross-modal attention. We prove that this notion of soft relation alignment is equivalent to enforcing congruence between vision and language attention matrices under a 'change of basis' provided by the cross-modal a",
    "path": "papers/22/12/2212.10549.json",
    "total_tokens": 954,
    "translated_title": "跨模态注意力一致性正则化用于视觉-语言关系对齐",
    "translated_abstract": "尽管近年来联合视觉-语言模型的规模不断扩大，但这些模型在诸如Winoground等组合概括性基准测试中仍然存在困难。我们发现，当前视觉-语言模型缺乏的关键组成部分是关系级别的对齐：即能够将文本中的定向语义关系（例如，“草坪中的杯子”）与图像中的空间关系（例如，杯子相对于草坪的位置）进行匹配。为了解决这个问题，我们展示了通过鼓励从“杯子”到“草坪”（捕捉语义关系“在”）的定向语言注意力与从杯子到草坪的定向视觉注意力相匹配来实现关系对齐。通过跨模态注意力软性地识别标记及其对应的对象。我们证明了这种软性关系对齐的概念等同于在由跨模态提供的“基底变换”下实施视觉和语言注意力矩阵之间的一致性。",
    "tldr": "本研究提出了一种跨模态注意力一致性正则化方法，用于视觉-语言关系对齐。通过鼓励语言注意力与视觉注意力的一致性来实现关系级别的对齐，从而提高视觉-语言模型在组合概括性基准测试中的性能。"
}