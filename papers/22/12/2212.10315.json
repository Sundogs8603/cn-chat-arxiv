{
    "title": "HINT: Hypernetwork Instruction Tuning for Efficient Zero- & Few-Shot Generalisation. (arXiv:2212.10315v2 [cs.CL] UPDATED)",
    "abstract": "Recent NLP models have shown the remarkable ability to effectively generalise `zero-shot' to new tasks using only natural language instructions as guidance. However, many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every input example, resulting in costly reprocessing of the instruction. To avoid this, we introduce Hypernetworks for INstruction Tuning (HINT), which convert task instructions and examples into parameter-efficient modules inserted into an underlying model using a pretrained text encoder, eliminating the need to include instructions in the model input. The hypernetwork in HINT also produces an encoded instruction, which we concatenate with encoded inputs during decoding to further improve performance. HINT models outperform strong state-of-the-art baselines by over 10% when controlling for compute (measured in FLOPs). By converting instructions into modules, HINT models can effectively disregard ",
    "link": "http://arxiv.org/abs/2212.10315",
    "context": "Title: HINT: Hypernetwork Instruction Tuning for Efficient Zero- & Few-Shot Generalisation. (arXiv:2212.10315v2 [cs.CL] UPDATED)\nAbstract: Recent NLP models have shown the remarkable ability to effectively generalise `zero-shot' to new tasks using only natural language instructions as guidance. However, many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every input example, resulting in costly reprocessing of the instruction. To avoid this, we introduce Hypernetworks for INstruction Tuning (HINT), which convert task instructions and examples into parameter-efficient modules inserted into an underlying model using a pretrained text encoder, eliminating the need to include instructions in the model input. The hypernetwork in HINT also produces an encoded instruction, which we concatenate with encoded inputs during decoding to further improve performance. HINT models outperform strong state-of-the-art baselines by over 10% when controlling for compute (measured in FLOPs). By converting instructions into modules, HINT models can effectively disregard ",
    "path": "papers/22/12/2212.10315.json",
    "total_tokens": 972,
    "translated_title": "HINT：用于高效零及少样本泛化的超网络指令调整",
    "translated_abstract": "最近的NLP模型显示出了在新任务中只使用自然语言指导就能很好地推广“零样本”的非凡能力。然而，由于这些方法依赖于将冗长的指令与每个输入示例连接，导致指令的昂贵重新处理，因此许多方法存在高计算成本的问题。为了避免这一点，我们引入了用于指令调整的超级网络（HINT），它将任务指令和示例转换为参数高效的模块，使用预训练的文本编码器将其插入基础模型中，从而无需将指令包含在模型输入中。HINT中的超网络还产生了一种编码指令，我们在解码期间将其与编码输入连接起来以进一步提高性能。在控制计算（以FLOPs计量）的情况下，HINT模型的表现优于强有力的最新基线模型。通过将指令转换为模块，HINT模型可以有效地忽略指令。",
    "tldr": "本文介绍了一种新的NLP模型HINT，它使用超网络将任务指令和示例转换为参数高效的模块，从而无需将指令包含在模型输入中，并可为解码期间提供编码指令。HINT模型在计算量相等的情况下性能比最新的基线模型强10%以上，解决了高计算成本的问题。",
    "en_tdlr": "This paper introduces a new NLP model, HINT, which uses hypernetworks to convert task instructions and examples into parameter-efficient modules, eliminating the need to include instructions in the model input, and provides encoded instructions for decoding. HINT models outperform strong state-of-the-art baselines by over 10% when controlling for compute and solve the problem of high computational costs."
}