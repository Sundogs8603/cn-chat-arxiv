{
    "title": "Learning to Select from Multiple Options. (arXiv:2212.00301v2 [cs.CL] UPDATED)",
    "abstract": "Many NLP tasks can be regarded as a selection problem from a set of options, such as classification tasks, multi-choice question answering, etc. Textual entailment (TE) has been shown as the state-of-the-art (SOTA) approach to dealing with those selection problems. TE treats input texts as premises (P), options as hypotheses (H), then handles the selection problem by modeling (P, H) pairwise. Two limitations: first, the pairwise modeling is unaware of other options, which is less intuitive since humans often determine the best options by comparing competing candidates; second, the inference process of pairwise TE is time-consuming, especially when the option space is large. To deal with the two issues, this work first proposes a contextualized TE model (Context-TE) by appending other k options as the context of the current (P, H) modeling. Context-TE is able to learn more reliable decision for the H since it considers various context. Second, we speed up Context-TE by coming up with Pa",
    "link": "http://arxiv.org/abs/2212.00301",
    "context": "Title: Learning to Select from Multiple Options. (arXiv:2212.00301v2 [cs.CL] UPDATED)\nAbstract: Many NLP tasks can be regarded as a selection problem from a set of options, such as classification tasks, multi-choice question answering, etc. Textual entailment (TE) has been shown as the state-of-the-art (SOTA) approach to dealing with those selection problems. TE treats input texts as premises (P), options as hypotheses (H), then handles the selection problem by modeling (P, H) pairwise. Two limitations: first, the pairwise modeling is unaware of other options, which is less intuitive since humans often determine the best options by comparing competing candidates; second, the inference process of pairwise TE is time-consuming, especially when the option space is large. To deal with the two issues, this work first proposes a contextualized TE model (Context-TE) by appending other k options as the context of the current (P, H) modeling. Context-TE is able to learn more reliable decision for the H since it considers various context. Second, we speed up Context-TE by coming up with Pa",
    "path": "papers/22/12/2212.00301.json",
    "total_tokens": 913,
    "translated_title": "学习从多个选项中选择",
    "translated_abstract": "许多自然语言处理任务可以看作是从一组选项中进行选择，比如分类任务、多项选择题等。文本蕴涵（TE）被证明是处理这些选择问题的最先进方法。TE将输入文本视为前提（P），选项视为假设（H），然后通过对（P，H）进行配对建模来处理选择问题。然而，TE方法存在两个限制：首先，配对建模无法意识到其他选项，这不够直观，因为人们常常通过比较竞争候选项来确定最佳选项；其次，配对TE的推理过程耗时，特别是当选项空间较大时。为了解决这两个问题，本文首先提出了一种上下文化的TE模型（Context-TE），通过将其他k个选项附加为当前（P，H）建模的上下文来进行建模。Context-TE能够通过考虑不同的上下文来学习到更可靠的H决策。其次，我们通过提出Pa",
    "tldr": "本文提出了一个上下文化的文本蕴涵（TE）模型（Context-TE），通过考虑其他选项作为当前建模的上下文，它能够解决TE方法中的两个限制。这个模型可以学习到更可靠的选项决策，并且通过加速推理过程来提高效率。"
}