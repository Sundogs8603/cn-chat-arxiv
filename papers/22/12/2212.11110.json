{
    "title": "Lifelong Reinforcement Learning with Modulating Masks. (arXiv:2212.11110v3 [cs.LG] UPDATED)",
    "abstract": "Lifelong learning aims to create AI systems that continuously and incrementally learn during a lifetime, similar to biological learning. Attempts so far have met problems, including catastrophic forgetting, interference among tasks, and the inability to exploit previous knowledge. While considerable research has focused on learning multiple supervised classification tasks that involve changes in the input distribution, lifelong reinforcement learning (LRL) must deal with variations in the state and transition distributions, and in the reward functions. Modulating masks with a fixed backbone network, recently developed for classification, are particularly suitable to deal with such a large spectrum of task variations. In this paper, we adapted modulating masks to work with deep LRL, specifically PPO and IMPALA agents. The comparison with LRL baselines in both discrete and continuous RL tasks shows superior performance. We further investigated the use of a linear combination of previousl",
    "link": "http://arxiv.org/abs/2212.11110",
    "context": "Title: Lifelong Reinforcement Learning with Modulating Masks. (arXiv:2212.11110v3 [cs.LG] UPDATED)\nAbstract: Lifelong learning aims to create AI systems that continuously and incrementally learn during a lifetime, similar to biological learning. Attempts so far have met problems, including catastrophic forgetting, interference among tasks, and the inability to exploit previous knowledge. While considerable research has focused on learning multiple supervised classification tasks that involve changes in the input distribution, lifelong reinforcement learning (LRL) must deal with variations in the state and transition distributions, and in the reward functions. Modulating masks with a fixed backbone network, recently developed for classification, are particularly suitable to deal with such a large spectrum of task variations. In this paper, we adapted modulating masks to work with deep LRL, specifically PPO and IMPALA agents. The comparison with LRL baselines in both discrete and continuous RL tasks shows superior performance. We further investigated the use of a linear combination of previousl",
    "path": "papers/22/12/2212.11110.json",
    "total_tokens": 885,
    "translated_title": "使用调整掩码的终身强化学习",
    "translated_abstract": "终身学习旨在创建在其生命周期中持续和逐步学习的人工智能系统，类似生物学习。到目前为止，这方面的尝试遇到了问题，包括灾难性遗忘、任务之间的干扰，以及无法利用先前的知识。虽然已经有相当多的研究集中在学习涉及输入分布变化的多个监督分类任务上，但是终身强化学习必须处理状态和转换分布以及奖励函数的变化。最近针对分类问题开发的使用固定骨干网络的调整掩码对于处理如此大范围的任务变化特别适用。在本文中，我们将调整掩码应用于深层次的终身强化学习，具体包括PPO和IMPALA代理。在离散和连续强化学习任务中与终身强化学习基线进行了比较，结果显示出卓越的性能。我们进一步研究了先前任务的线性组合的使用。",
    "tldr": "本文研究了终身强化学习中使用调整掩码的方法，通过将调整掩码应用于PPO和IMPALA代理，显著提高了在离散和连续强化学习任务中的性能。",
    "en_tdlr": "This paper investigates the use of modulating masks in lifelong reinforcement learning, and adapts them to PPO and IMPALA agents, resulting in improved performance in both discrete and continuous RL tasks."
}