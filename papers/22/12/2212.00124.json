{
    "title": "One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion. (arXiv:2212.00124v2 [cs.LG] UPDATED)",
    "abstract": "Offline reinforcement learning (RL) is suitable for safety-critical domains where online exploration is not feasible. In such domains, decision-making should take into consideration the risk of catastrophic outcomes. In other words, decision-making should be risk-averse. An additional challenge of offline RL is avoiding distributional shift, i.e. ensuring that state-action pairs visited by the policy remain near those in the dataset. Previous works on risk in offline RL combine offline RL techniques (to avoid distributional shift), with risk-sensitive RL algorithms (to achieve risk-aversion). In this work, we propose risk-aversion as a mechanism to jointly address both of these issues. We propose a model-based approach, and use an ensemble of models to estimate epistemic uncertainty, in addition to aleatoric uncertainty. We train a policy that is risk-averse, and avoids high uncertainty actions. Risk-aversion to epistemic uncertainty prevents distributional shift, as areas not covered ",
    "link": "http://arxiv.org/abs/2212.00124",
    "context": "Title: One Risk to Rule Them All: Addressing Distributional Shift in Offline Reinforcement Learning via Risk-Aversion. (arXiv:2212.00124v2 [cs.LG] UPDATED)\nAbstract: Offline reinforcement learning (RL) is suitable for safety-critical domains where online exploration is not feasible. In such domains, decision-making should take into consideration the risk of catastrophic outcomes. In other words, decision-making should be risk-averse. An additional challenge of offline RL is avoiding distributional shift, i.e. ensuring that state-action pairs visited by the policy remain near those in the dataset. Previous works on risk in offline RL combine offline RL techniques (to avoid distributional shift), with risk-sensitive RL algorithms (to achieve risk-aversion). In this work, we propose risk-aversion as a mechanism to jointly address both of these issues. We propose a model-based approach, and use an ensemble of models to estimate epistemic uncertainty, in addition to aleatoric uncertainty. We train a policy that is risk-averse, and avoids high uncertainty actions. Risk-aversion to epistemic uncertainty prevents distributional shift, as areas not covered ",
    "path": "papers/22/12/2212.00124.json",
    "total_tokens": 1046,
    "translated_title": "一种解决离线强化学习分布偏移的风险规避方法",
    "translated_abstract": "离线强化学习（RL）适用于在线探索不可行的安全关键领域。在这种领域中，决策应考虑到灾难性结果的风险。换句话说，决策应该是风险规避的。离线RL的另一个挑战是避免分布偏移，即确保策略访问的状态-操作对靠近数据集中的状态-操作对。以往的研究将离线RL技术（以避免分布偏移）与风险敏感型RL算法（以实现风险规避）相结合。在本文中，我们提出了将风险规避机制作为同时解决这两个问题的方法。我们提出一种基于模型的方法，使用模型集合来估计认知不确定性和随机不确定性。我们训练了一个风险规避的策略，避免高不确定性的行为。对认知不确定性的风险规避可以防止分布偏移，因为避免了数据集中未涵盖的区域。我们在几个标准基准测试中展示了这种方法的有效性，并显示出比先前处理这些挑战的工作明显的改进。",
    "tldr": "本研究提出了一种基于风险规避机制的离线强化学习方法，同时解决了避免分布偏移和避免灾难性结果的风险问题，并取得了明显的改进。",
    "en_tdlr": "This study proposes a risk-averse mechanism for offline reinforcement learning that addresses both distributional shift and catastrophic outcomes risk, achieving significant improvements in several benchmarks. The approach uses a model-based method with an ensemble of models to estimate uncertainty, allowing for the training of a risk-averse policy that avoids high uncertainty actions."
}