{
    "title": "Training Trajectories of Language Models Across Scales. (arXiv:2212.09803v2 [cs.CL] UPDATED)",
    "abstract": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al.,2022)--from 125M to 175B parameters--on next-token prediction, sequence-level generation, and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior; 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; 3) perplexity ",
    "link": "http://arxiv.org/abs/2212.09803",
    "context": "Title: Training Trajectories of Language Models Across Scales. (arXiv:2212.09803v2 [cs.CL] UPDATED)\nAbstract: Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al.,2022)--from 125M to 175B parameters--on next-token prediction, sequence-level generation, and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior; 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; 3) perplexity ",
    "path": "papers/22/12/2212.09803.json",
    "total_tokens": 694,
    "tldr": "本文研究不同规模 OPT 模型（参数从 125M 到 175B 不等）在预训练过程中的训练动态，发现大型语言模型能够解决较小模型难以解决的问题。",
    "en_tdlr": "This paper analyzes the training dynamics of OPT models of different sizes (from 125M to 175B parameters) during pre-training and finds that larger language models are capable of solving problems that smaller models struggle with or cannot solve."
}