{
    "title": "G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks",
    "abstract": "arXiv:2212.03613v3 Announce Type: replace  Abstract: Recently, domain-specific PLMs have been proposed to boost the task performance of specific domains (e.g., biomedical and computer science) by continuing to pre-train general PLMs with domain-specific corpora. However, this Domain-Adaptive Pre-Training (DAPT; Gururangan et al. (2020)) tends to forget the previous general knowledge acquired by general PLMs, which leads to a catastrophic forgetting phenomenon and sub-optimal performance. To alleviate this problem, we propose a new framework of General Memory Augmented Pre-trained Language Model (G-MAP), which augments the domain-specific PLM by a memory representation built from the frozen general PLM without losing any general knowledge. Specifically, we propose a new memory-augmented layer, and based on it, different augmented strategies are explored to build the memory representation and then adaptively fuse it into the domain-specific PLM. We demonstrate the effectiveness of G-MAP ",
    "link": "https://arxiv.org/abs/2212.03613",
    "context": "Title: G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks\nAbstract: arXiv:2212.03613v3 Announce Type: replace  Abstract: Recently, domain-specific PLMs have been proposed to boost the task performance of specific domains (e.g., biomedical and computer science) by continuing to pre-train general PLMs with domain-specific corpora. However, this Domain-Adaptive Pre-Training (DAPT; Gururangan et al. (2020)) tends to forget the previous general knowledge acquired by general PLMs, which leads to a catastrophic forgetting phenomenon and sub-optimal performance. To alleviate this problem, we propose a new framework of General Memory Augmented Pre-trained Language Model (G-MAP), which augments the domain-specific PLM by a memory representation built from the frozen general PLM without losing any general knowledge. Specifically, we propose a new memory-augmented layer, and based on it, different augmented strategies are explored to build the memory representation and then adaptively fuse it into the domain-specific PLM. We demonstrate the effectiveness of G-MAP ",
    "path": "papers/22/12/2212.03613.json",
    "total_tokens": 932,
    "translated_title": "G-MAP：通用记忆增强的面向领域任务的预训练语言模型",
    "translated_abstract": "最近，已经提出了领域特定的PLMs，通过继续使用领域特定语料库对通用PLMs进行预训练，来提高特定领域（例如生物医学和计算机科学）的任务性能。然而，这种领域自适应预训练（DAPT；Gururangan等人（2020））往往会遗忘通用PLMs已经获得的先前通用知识，导致灾难性遗忘现象和次优性能。为了缓解这个问题，我们提出了一个新的通用记忆增强的预训练语言模型框架（G-MAP），它通过从冻结的通用PLM构建的记忆表示来增强领域特定的PLM，而不会丢失任何通用知识。具体来说，我们提出了一种新的记忆增强层，并基于它，探索了不同的增强策略来构建记忆表示，然后将其自适应地融合到领域特定的PLM中。我们展示了G-MAP的有效性",
    "tldr": "G-MAP提出了一种新的通用记忆增强的预训练语言模型框架，能够在增强领域特定PLM时，保留通用知识，缓解灾难性遗忘现象，提高模型性能",
    "en_tdlr": "G-MAP introduces a novel framework of General Memory Augmented Pre-trained Language Model, which maintains general knowledge while enhancing domain-specific PLM, alleviating catastrophic forgetting and improving model performance."
}