{
    "title": "Learning threshold neurons via the \"edge of stability\". (arXiv:2212.07469v2 [cs.LG] UPDATED)",
    "abstract": "Existing analyses of neural network training often operate under the unrealistic assumption of an extremely small learning rate. This lies in stark contrast to practical wisdom and empirical studies, such as the work of J. Cohen et al. (ICLR 2021), which exhibit startling new phenomena (the \"edge of stability\" or \"unstable convergence\") and potential benefits for generalization in the large learning rate regime. Despite a flurry of recent works on this topic, however, the latter effect is still poorly understood. In this paper, we take a step towards understanding genuinely non-convex training dynamics with large learning rates by performing a detailed analysis of gradient descent for simplified models of two-layer neural networks. For these models, we provably establish the edge of stability phenomenon and discover a sharp phase transition for the step size below which the neural network fails to learn \"threshold-like\" neurons (i.e., neurons with a non-zero first-layer bias). This elu",
    "link": "http://arxiv.org/abs/2212.07469",
    "context": "Title: Learning threshold neurons via the \"edge of stability\". (arXiv:2212.07469v2 [cs.LG] UPDATED)\nAbstract: Existing analyses of neural network training often operate under the unrealistic assumption of an extremely small learning rate. This lies in stark contrast to practical wisdom and empirical studies, such as the work of J. Cohen et al. (ICLR 2021), which exhibit startling new phenomena (the \"edge of stability\" or \"unstable convergence\") and potential benefits for generalization in the large learning rate regime. Despite a flurry of recent works on this topic, however, the latter effect is still poorly understood. In this paper, we take a step towards understanding genuinely non-convex training dynamics with large learning rates by performing a detailed analysis of gradient descent for simplified models of two-layer neural networks. For these models, we provably establish the edge of stability phenomenon and discover a sharp phase transition for the step size below which the neural network fails to learn \"threshold-like\" neurons (i.e., neurons with a non-zero first-layer bias). This elu",
    "path": "papers/22/12/2212.07469.json",
    "total_tokens": 921,
    "translated_title": "通过\"稳定边缘\"学习阈值神经元",
    "translated_abstract": "现有的神经网络训练分析通常基于极小学习率的不现实假设。与实际智慧和经验研究相反，例如J. Cohen等人的工作（ICLR 2021），展示了惊人的新现象（\"稳定边缘\"或\"不稳定收敛\"），以及大学习率体制下的潜在泛化效果。然而，尽管最近有大量关于这个主题的研究，但后一种效应仍然理解有限。本文通过对简化的两层神经网络模型的梯度下降进行详细分析，迈出了理解大学习率下真正非凸训练动态的一步。对于这些模型，我们证明了稳定边缘现象，并发现了一个尖锐的阶跃转变，当步长小于此值时，神经网络无法学习到\"阈值样式\"神经元（即具有非零第一层偏置的神经元）。",
    "tldr": "该论文通过对简化的两层神经网络模型的梯度下降进行详细分析，揭示了大学习率下非凸训练动态的稳定边缘现象，并发现了神经网络无法学习阈值样式神经元的临界步长。"
}