{
    "title": "Parallel Context Windows for Large Language Models. (arXiv:2212.10947v2 [cs.CL] UPDATED)",
    "abstract": "When applied for processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (``windows''), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved ",
    "link": "http://arxiv.org/abs/2212.10947",
    "context": "Title: Parallel Context Windows for Large Language Models. (arXiv:2212.10947v2 [cs.CL] UPDATED)\nAbstract: When applied for processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (``windows''), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved ",
    "path": "papers/22/12/2212.10947.json",
    "total_tokens": 848,
    "translated_title": "大型语言模型的并行上下文窗口",
    "translated_abstract": "处理长文本时，大型语言模型（LLM）受到上下文窗口的限制。现有的解决这个问题的方法包括训练专门的架构，但不能轻松地应用于现成的LLM。我们提出了并行上下文窗口（PCW）方法，可以缓解任何现成LLM的上下文窗口限制，而无需进行进一步训练。这种方法的关键在于将长上下文划分为块（“窗口”），限制注意机制仅在每个窗口内应用，并跨窗口重用位置嵌入。我们的主要结果是在750万到1780亿个参数范围内的模型上测试PCW方法，并展示其在输入和输出空间不同的任务中带来了显著的改进。我们在其他需要长上下文窗口的情况下，如多跳问题和使用多个检索的检索增强型问答中展示了额外的好处。",
    "tldr": "PCW方法可以缓解现成LLM的上下文窗口限制，将长上下文划分为块并在每个窗口内重用位置嵌入，提高了处理长文本的性能表现。",
    "en_tdlr": "PCW method alleviates the context window restriction for any off-the-shelf LLM without further training by dividing long context into chunks and reusing positional embeddings within each window, which improves the performance in processing long text."
}