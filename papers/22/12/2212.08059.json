{
    "title": "Rethinking Vision Transformers for MobileNet Size and Speed. (arXiv:2212.08059v2 [cs.CV] UPDATED)",
    "abstract": "With the success of Vision Transformers (ViTs) in computer vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mobile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorporate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as MobileNet and maintain a similar size? We revisit the design choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by opt",
    "link": "http://arxiv.org/abs/2212.08059",
    "context": "Title: Rethinking Vision Transformers for MobileNet Size and Speed. (arXiv:2212.08059v2 [cs.CV] UPDATED)\nAbstract: With the success of Vision Transformers (ViTs) in computer vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mobile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorporate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as MobileNet and maintain a similar size? We revisit the design choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by opt",
    "path": "papers/22/12/2212.08059.json",
    "total_tokens": 830,
    "translated_title": "重新思考适用于MobileNet速度和尺寸的Vision Transformers",
    "translated_abstract": "随着Vision Transformers在计算机视觉任务中取得的成功，最近的研究尝试优化ViTs的性能和复杂性，以实现在移动设备上的高效部署。提出了多种方法来加速注意力机制，改进低效的设计，或将移动设备友好的轻量级卷积与ViTs结合形成混合架构。然而，即使是多年前的MobileNet，ViT及其变种仍然具有更高的延迟或更多的参数。在实践中，延迟和大小对于在资源受限硬件上的高效部署都是至关重要的。在这项工作中，我们对ViTs的设计选择进行了重新审视，并提出了一种具有低延迟和高参数效率的新型超网络。我们进一步引入了一种新颖的细粒度联合搜索策略，用于通过优化来寻找高效的transformer架构。",
    "tldr": "本研究重新思考了Vision Transformers在移动设备上的部署效率，并提出了一种新的超网络设计和搜索策略，以实现与MobileNet类似大小和速度的Transformer模型。"
}