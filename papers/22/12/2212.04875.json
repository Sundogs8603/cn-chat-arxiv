{
    "title": "Expeditious Saliency-guided Mix-up through Random Gradient Thresholding. (arXiv:2212.04875v3 [cs.CV] UPDATED)",
    "abstract": "Mix-up training approaches have proven to be effective in improving the generalization ability of Deep Neural Networks. Over the years, the research community expands mix-up methods into two directions, with extensive efforts to improve saliency-guided procedures but minimal focus on the arbitrary path, leaving the randomization domain unexplored. In this paper, inspired by the superior qualities of each direction over one another, we introduce a novel method that lies at the junction of the two routes. By combining the best elements of randomness and saliency utilization, our method balances speed, simplicity, and accuracy. We name our method R-Mix following the concept of \"Random Mix-up\". We demonstrate its effectiveness in generalization, weakly supervised object localization, calibration, and robustness to adversarial attacks. Finally, in order to address the question of whether there exists a better decision protocol, we train a Reinforcement Learning agent that decides the mix-up",
    "link": "http://arxiv.org/abs/2212.04875",
    "context": "Title: Expeditious Saliency-guided Mix-up through Random Gradient Thresholding. (arXiv:2212.04875v3 [cs.CV] UPDATED)\nAbstract: Mix-up training approaches have proven to be effective in improving the generalization ability of Deep Neural Networks. Over the years, the research community expands mix-up methods into two directions, with extensive efforts to improve saliency-guided procedures but minimal focus on the arbitrary path, leaving the randomization domain unexplored. In this paper, inspired by the superior qualities of each direction over one another, we introduce a novel method that lies at the junction of the two routes. By combining the best elements of randomness and saliency utilization, our method balances speed, simplicity, and accuracy. We name our method R-Mix following the concept of \"Random Mix-up\". We demonstrate its effectiveness in generalization, weakly supervised object localization, calibration, and robustness to adversarial attacks. Finally, in order to address the question of whether there exists a better decision protocol, we train a Reinforcement Learning agent that decides the mix-up",
    "path": "papers/22/12/2212.04875.json",
    "total_tokens": 960,
    "translated_title": "快速基于显著性引导的随机梯度阈值化的混合训练方法",
    "translated_abstract": "混合训练方法已被证明可以提高深度神经网络的泛化能力。多年来，研究界将混合方法扩展为两个方向，旨在改进显著性引导过程并最小化对任意路径的关注，将随机性领域留给进一步探索。在本文中，受到每个方向相互优越的特性的启发，我们引入了一种新颖的方法，位于两个路线的交汇处。通过结合随机性和显著性利用的最佳元素，我们的方法在速度、简单性和准确性之间取得平衡。我们将方法命名为R-Mix，意为“随机混合”。我们证明了该方法在泛化性能、弱监督目标定位、校准和对抗性攻击的鲁棒性方面的有效性。最后，为了回答是否存在更好的决策协议这个问题，我们训练了一个强化学习代理来决定混合训练过程。",
    "tldr": "本文介绍了一种新的混合训练方法，名为R-Mix，通过结合随机性和显著性利用的最佳元素，实现了速度、简单性和准确性的平衡。该方法在泛化能力、弱监督目标定位、校准和对抗性攻击的鲁棒性方面具有有效性。同时，作者还通过训练一个强化学习代理来决定混合训练过程，以探索是否存在更好的决策协议。"
}