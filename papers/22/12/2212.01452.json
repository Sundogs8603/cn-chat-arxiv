{
    "title": "CLIP: Train Faster with Less Data. (arXiv:2212.01452v2 [cs.CV] UPDATED)",
    "abstract": "Deep learning models require an enormous amount of data for training. However, recently there is a shift in machine learning from model-centric to data-centric approaches. In data-centric approaches, the focus is to refine and improve the quality of the data to improve the learning performance of the models rather than redesigning model architectures. In this paper, we propose CLIP i.e., Curriculum Learning with Iterative data Pruning. CLIP combines two data-centric approaches i.e., curriculum learning and dataset pruning to improve the model learning accuracy and convergence speed. The proposed scheme applies loss-aware dataset pruning to iteratively remove the least significant samples and progressively reduces the size of the effective dataset in the curriculum learning training. Extensive experiments performed on crowd density estimation models validate the notion behind combining the two approaches by reducing the convergence time and improving generalization. To our knowledge, th",
    "link": "http://arxiv.org/abs/2212.01452",
    "context": "Title: CLIP: Train Faster with Less Data. (arXiv:2212.01452v2 [cs.CV] UPDATED)\nAbstract: Deep learning models require an enormous amount of data for training. However, recently there is a shift in machine learning from model-centric to data-centric approaches. In data-centric approaches, the focus is to refine and improve the quality of the data to improve the learning performance of the models rather than redesigning model architectures. In this paper, we propose CLIP i.e., Curriculum Learning with Iterative data Pruning. CLIP combines two data-centric approaches i.e., curriculum learning and dataset pruning to improve the model learning accuracy and convergence speed. The proposed scheme applies loss-aware dataset pruning to iteratively remove the least significant samples and progressively reduces the size of the effective dataset in the curriculum learning training. Extensive experiments performed on crowd density estimation models validate the notion behind combining the two approaches by reducing the convergence time and improving generalization. To our knowledge, th",
    "path": "papers/22/12/2212.01452.json",
    "total_tokens": 866,
    "translated_title": "CLIP: 用更少的数据更快地训练模型",
    "translated_abstract": "深度学习模型需要大量的数据进行训练，但近年来机器学习正从以模型为中心转向以数据为中心的方法。在数据为中心的方法中，重点是通过改进和提高数据质量来提高模型的学习性能，而不是重新设计模型架构。在本文中，我们提出了CLIP，即使用迭代数据修剪的课程学习。CLIP结合了课程学习和数据集修剪这两种数据为中心的方法，以提高模型的学习准确性和收敛速度。所提出的方案采用了有损数据集修剪的方法，迭代地去除最不重要的样本，并逐渐减小在课程学习训练中的有效数据集的大小。在众筹密度估计模型上进行的大量实验验证了结合这两种方法的理念，通过减小收敛时间和改进泛化能力。据我们所知，这是第一次将课程学习和数据集修剪结合应用于深度学习的训练中。",
    "tldr": "本文提出了CLIP，通过结合课程学习和数据集修剪的方法，在深度学习模型的训练中使用更少的数据，实现更快的收敛速度和更好的泛化能力。",
    "en_tdlr": "The paper proposes CLIP, which combines curriculum learning and dataset pruning to train deep learning models with less data, resulting in faster convergence and better generalization."
}