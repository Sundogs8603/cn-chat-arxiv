{
    "title": "Reconstructing Kernel-based Machine Learning Force Fields with Super-linear Convergence. (arXiv:2212.12737v2 [physics.chem-ph] UPDATED)",
    "abstract": "Kernel machines have sustained continuous progress in the field of quantum chemistry. In particular, they have proven to be successful in the low-data regime of force field reconstruction. This is because many equivariances and invariances due to physical symmetries can be incorporated into the kernel function to compensate for much larger datasets. So far, the scalability of kernel machines has however been hindered by its quadratic memory and cubical runtime complexity in the number of training points. While it is known, that iterative Krylov subspace solvers can overcome these burdens, their convergence crucially relies on effective preconditioners, which are elusive in practice. Effective preconditioners need to partially pre-solve the learning problem in a computationally cheap and numerically robust manner. Here, we consider the broad class of Nystr\\\"om-type methods to construct preconditioners based on successively more sophisticated low-rank approximations of the original kerne",
    "link": "http://arxiv.org/abs/2212.12737",
    "context": "Title: Reconstructing Kernel-based Machine Learning Force Fields with Super-linear Convergence. (arXiv:2212.12737v2 [physics.chem-ph] UPDATED)\nAbstract: Kernel machines have sustained continuous progress in the field of quantum chemistry. In particular, they have proven to be successful in the low-data regime of force field reconstruction. This is because many equivariances and invariances due to physical symmetries can be incorporated into the kernel function to compensate for much larger datasets. So far, the scalability of kernel machines has however been hindered by its quadratic memory and cubical runtime complexity in the number of training points. While it is known, that iterative Krylov subspace solvers can overcome these burdens, their convergence crucially relies on effective preconditioners, which are elusive in practice. Effective preconditioners need to partially pre-solve the learning problem in a computationally cheap and numerically robust manner. Here, we consider the broad class of Nystr\\\"om-type methods to construct preconditioners based on successively more sophisticated low-rank approximations of the original kerne",
    "path": "papers/22/12/2212.12737.json",
    "total_tokens": 1023,
    "translated_title": "用超线性收敛重构基于核的机器学习力场",
    "translated_abstract": "核机器在量子化学领域持续取得进展，尤其在力场重构的低数据范围内已被证明成功。这是因为可以将许多针对物理对称性的等变性和不变性合并到核函数中以补偿更大的数据集。但是，核机器的可扩展性受到其二次内存和与训练点数成立方关系的限制。虽然已知迭代的Krylov子空间求解器可以克服这些负担，但它们的收敛关键取决于有效的预处理器，这在实践中很难实现。有效的预处理器需要以计算便宜和数值鲁棒的方式部分预解学习问题。在这里，我们考虑了Nyström型方法类的广泛方法，以基于最初核函数的越来越复杂的低秩近似构建预处理器。",
    "tldr": "本文提出了一种基于Nyström型方法的预处理器构建框架，实现了在低数据范围内高效重构核机器力场，并在带有数万个培训点的化学系统中获得了稳定和准确的结果。",
    "en_tdlr": "This paper proposes a framework for constructing preconditions based on Nyström-type methods, which achieves efficient reconstruction of kernel machine force fields in the low-data regime and stable and accurate results in chemical systems with up to tens of thousands of training points."
}