{
    "title": "DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization. (arXiv:2212.10018v2 [cs.CL] UPDATED)",
    "abstract": "Dialogue summarization has recently garnered significant attention due to its wide range of applications. However, existing methods for summarizing dialogues have limitations because they do not take into account the inherent structure of dialogue and rely heavily on labeled data, which can lead to poor performance in new domains. In this work, we propose DIONYSUS (dynamic input optimization in pre-training for dialogue summarization), a pre-trained encoder-decoder model for summarizing dialogues in any new domain. To pre-train DIONYSUS, we create two pseudo summaries for each dialogue example: one is produced by a fine-tuned summarization model, and the other is a collection of dialogue turns that convey important information. We then choose one of these pseudo summaries based on the difference in information distribution across different types of dialogues. This selected pseudo summary serves as the objective for pre-training DIONYSUS using a self-supervised approach on a large dialo",
    "link": "http://arxiv.org/abs/2212.10018",
    "context": "Title: DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization. (arXiv:2212.10018v2 [cs.CL] UPDATED)\nAbstract: Dialogue summarization has recently garnered significant attention due to its wide range of applications. However, existing methods for summarizing dialogues have limitations because they do not take into account the inherent structure of dialogue and rely heavily on labeled data, which can lead to poor performance in new domains. In this work, we propose DIONYSUS (dynamic input optimization in pre-training for dialogue summarization), a pre-trained encoder-decoder model for summarizing dialogues in any new domain. To pre-train DIONYSUS, we create two pseudo summaries for each dialogue example: one is produced by a fine-tuned summarization model, and the other is a collection of dialogue turns that convey important information. We then choose one of these pseudo summaries based on the difference in information distribution across different types of dialogues. This selected pseudo summary serves as the objective for pre-training DIONYSUS using a self-supervised approach on a large dialo",
    "path": "papers/22/12/2212.10018.json",
    "total_tokens": 968,
    "translated_title": "DIONYSUS：用于低资源对话摘要的预训练模型",
    "translated_abstract": "对话摘要由于其广泛的应用受到了近期的极大关注。然而，现有的对话摘要方法存在局限性，因为它们没有考虑对话的固有结构，并且严重依赖于标记数据，这可能导致在新领域中效果欠佳。在这项工作中，我们提出了DIONYSUS（用于任何新领域中对话摘要的预训练编码器-解码器模型），其基于自监督方法，用于预训练模型。我们为每个对话示例创建了两个伪摘要：一个是通过微调摘要模型生成的，另一个是包含重要信息的对话转换集合。然后，我们基于不同类型的对话中的信息分布差异选择其中一个伪摘要。这个所选的伪摘要作为目标，用自监督的方法在大型对话语料库上预训练DIONYSUS。",
    "tldr": "提出了一种名为DIONYSUS的对话摘要模型，它是一个预先训练的编码器-解码器模型，可用于低资源的对话摘要，自监督的方法用于预训练。该模型的创新点在于利用不同的伪摘要，并基于对话中信息分布的分析来选择最佳的伪摘要，提高了在新领域中对话摘要的表现。",
    "en_tdlr": "This paper proposes a pre-trained encoder-decoder model called DIONYSUS for low-resource dialogue summarization. The model uses self-supervised methods for pre-training and makes use of two pseudo summaries created for each dialogue example. The model's innovative approach involves selecting the best pseudo summary based on differences in information distribution across different types of dialogues, resulting in improved performance in new domains."
}