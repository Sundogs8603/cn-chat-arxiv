{
    "title": "Curriculum Learning for Relative Overgeneralization. (arXiv:2212.02733v2 [cs.LG] UPDATED)",
    "abstract": "In multi-agent reinforcement learning (MARL), many popular methods, such as VDN and QMIX, are susceptible to a critical multi-agent pathology known as relative overgeneralization (RO), which arises when the optimal joint action's utility falls below that of a sub-optimal joint action in cooperative tasks. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks that require significant coordination between agents within a given timestep. Recent value-based MARL algorithms such as QPLEX and WQMIX can overcome RO to some extent. However, our experimental results show that they can still fail to solve cooperative tasks that exhibit strong RO. In this work, we propose a novel approach called curriculum learning for relative overgeneralization (CURO) to better overcome RO. To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks that are tailored to the current ability of the ",
    "link": "http://arxiv.org/abs/2212.02733",
    "context": "Title: Curriculum Learning for Relative Overgeneralization. (arXiv:2212.02733v2 [cs.LG] UPDATED)\nAbstract: In multi-agent reinforcement learning (MARL), many popular methods, such as VDN and QMIX, are susceptible to a critical multi-agent pathology known as relative overgeneralization (RO), which arises when the optimal joint action's utility falls below that of a sub-optimal joint action in cooperative tasks. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks that require significant coordination between agents within a given timestep. Recent value-based MARL algorithms such as QPLEX and WQMIX can overcome RO to some extent. However, our experimental results show that they can still fail to solve cooperative tasks that exhibit strong RO. In this work, we propose a novel approach called curriculum learning for relative overgeneralization (CURO) to better overcome RO. To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks that are tailored to the current ability of the ",
    "path": "papers/22/12/2212.02733.json",
    "total_tokens": 924,
    "translated_title": "相对过度泛化的课程学习",
    "translated_abstract": "在多智能体强化学习 (MARL) 中，许多流行方法如 VDN 和 QMIX，都容易受到相对过度泛化 (RO) 这一关键性的多智能体病理的影响。当合作任务中最佳联合行动的效用低于次优联合行动时，就会出现RO。RO可能导致智能体陷入局部最优解或无法解决需要智能体之间在给定时间步长内进行大量协调的合作任务。最近的基于价值的MARL算法，如QPLEX和WQMIX可以在一定程度上克服RO。然而，我们的实验结果表明，它们仍然无法解决展示强RO的合作任务。在这项工作中，我们提出了一种称为相对过度泛化的课程学习（CURO）的新方法，以更好地克服RO。在CURO中，我们首先微调目标任务的奖励函数以生成适合当前能力的源任务来解决展示强RO的目标任务。",
    "tldr": "本论文提出了一种名为相对过度泛化的课程学习（CURO）的新算法来解决多智能体强化学习中存在的相对过度泛化 (RO) 问题，该方法在解决展示强RO的合作任务方面具有很好的表现。",
    "en_tdlr": "This paper proposes a novel approach called Curriculum Learning for Relative Overgeneralization (CURO) to better address the critical multi-agent pathology known as Relative Overgeneralization (RO) in multi-agent reinforcement learning (MARL), which shows great performance in solving cooperative tasks that exhibit strong RO."
}