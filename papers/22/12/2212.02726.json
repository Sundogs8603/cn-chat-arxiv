{
    "title": "Dataset vs Reality: Understanding Model Performance from the Perspective of Information Need. (arXiv:2212.02726v2 [cs.IR] UPDATED)",
    "abstract": "Deep learning technologies have brought us many models that outperform human beings on a few benchmarks. An interesting question is: can these models well solve real-world problems with similar settings (e.g., identical input/output) to the benchmark datasets? We argue that a model is trained to answer the same information need for which the training dataset is created. Although some datasets may share high structural similarities, e.g., question-answer pairs for the question answering (QA) task and image-caption pairs for the image captioning (IC) task, they may represent different research tasks aiming for answering different information needs. To support our argument, we use the QA task and IC task as two case studies and compare their widely used benchmark datasets. From the perspective of information need in the context of information retrieval, we show the differences in the dataset creation processes, and the differences in morphosyntactic properties between datasets. The differ",
    "link": "http://arxiv.org/abs/2212.02726",
    "context": "Title: Dataset vs Reality: Understanding Model Performance from the Perspective of Information Need. (arXiv:2212.02726v2 [cs.IR] UPDATED)\nAbstract: Deep learning technologies have brought us many models that outperform human beings on a few benchmarks. An interesting question is: can these models well solve real-world problems with similar settings (e.g., identical input/output) to the benchmark datasets? We argue that a model is trained to answer the same information need for which the training dataset is created. Although some datasets may share high structural similarities, e.g., question-answer pairs for the question answering (QA) task and image-caption pairs for the image captioning (IC) task, they may represent different research tasks aiming for answering different information needs. To support our argument, we use the QA task and IC task as two case studies and compare their widely used benchmark datasets. From the perspective of information need in the context of information retrieval, we show the differences in the dataset creation processes, and the differences in morphosyntactic properties between datasets. The differ",
    "path": "papers/22/12/2212.02726.json",
    "total_tokens": 845,
    "translated_title": "数据集与现实：从信息需求的角度理解模型性能",
    "translated_abstract": "深度学习技术带来了许多在一些基准测试中胜过人类的模型。一个有趣的问题是：这些模型能否很好地解决与基准数据集相似的真实世界问题（例如，相同的输入/输出）？我们认为一个模型是被训练来回答创建训练数据集时相同的信息需求的。虽然一些数据集可能具有高度结构相似性，例如问答（QA）任务的问答对和图像字幕（IC）任务的图像字幕对，但它们可能代表不同的研究任务，旨在回答不同的信息需求。为了支持我们的论点，我们使用问答任务和图像字幕任务作为两个案例研究，并比较它们的广泛使用的基准数据集。从信息检索的信息需求角度出发，我们展示了数据集创建过程中的差异以及数据集之间形态和语法属性的差异。",
    "tldr": "研究探讨了模型在解决真实世界问题和基准数据集中表现不同的原因，指出模型受到数据集信息需求影响。",
    "en_tdlr": "This study explores the reasons for the different performance of models in solving real-world problems and benchmark datasets and points out that models are influenced by the information needs of the dataset."
}