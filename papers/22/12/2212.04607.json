{
    "title": "Confidence-Conditioned Value Functions for Offline Reinforcement Learning. (arXiv:2212.04607v2 [cs.LG] UPDATED)",
    "abstract": "Offline reinforcement learning (RL) promises the ability to learn effective policies solely using existing, static datasets, without any costly online interaction. To do so, offline RL methods must handle distributional shift between the dataset and the learned policy. The most common approach is to learn conservative, or lower-bound, value functions, which underestimate the return of out-of-distribution (OOD) actions. However, such methods exhibit one notable drawback: policies optimized on such value functions can only behave according to a fixed, possibly suboptimal, degree of conservatism. However, this can be alleviated if we instead are able to learn policies for varying degrees of conservatism at training time and devise a method to dynamically choose one of them during evaluation. To do so, in this work, we propose learning value functions that additionally condition on the degree of conservatism, which we dub confidence-conditioned value functions. We derive a new form of a Be",
    "link": "http://arxiv.org/abs/2212.04607",
    "context": "Title: Confidence-Conditioned Value Functions for Offline Reinforcement Learning. (arXiv:2212.04607v2 [cs.LG] UPDATED)\nAbstract: Offline reinforcement learning (RL) promises the ability to learn effective policies solely using existing, static datasets, without any costly online interaction. To do so, offline RL methods must handle distributional shift between the dataset and the learned policy. The most common approach is to learn conservative, or lower-bound, value functions, which underestimate the return of out-of-distribution (OOD) actions. However, such methods exhibit one notable drawback: policies optimized on such value functions can only behave according to a fixed, possibly suboptimal, degree of conservatism. However, this can be alleviated if we instead are able to learn policies for varying degrees of conservatism at training time and devise a method to dynamically choose one of them during evaluation. To do so, in this work, we propose learning value functions that additionally condition on the degree of conservatism, which we dub confidence-conditioned value functions. We derive a new form of a Be",
    "path": "papers/22/12/2212.04607.json",
    "total_tokens": 871,
    "translated_title": "离线强化学习中的置信度条件值函数",
    "translated_abstract": "离线强化学习（RL）承诺能够仅使用现有的静态数据集学习有效的策略，而不需要任何昂贵的在线交互。为了做到这一点，离线RL方法必须处理数据集和学习策略之间的分布偏移。最常见的方法是学习保守或下限值函数，它们低估了超出分布的行为的回报。然而，这种方法存在一个显著缺点：在这些值函数上优化的策略只能根据固定的、可能是次优的保守程度来行为。然而，如果我们能够在训练时学习不同保守程度的策略，并设计一种方法在评估时动态选择其中之一，这个问题就可以得到缓解。为此，在这项工作中，我们提出学习附加条件的值函数，这些值函数依赖于保守程度，我们将其称为置信度条件值函数。",
    "tldr": "本文提出了一种条件值函数的学习方法，该方法在离线强化学习中处理了数据集和学习策略之间的分布偏移，并可以在训练和评估时根据保守程度动态选择策略。",
    "en_tdlr": "This paper proposes a method for learning conditioned value functions that handle distributional shift between datasets and learned policies in offline reinforcement learning, and enable dynamic policy selection based on conservatism during training and evaluation."
}