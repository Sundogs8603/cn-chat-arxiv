{
    "title": "Python Code Generation by Asking Clarification Questions. (arXiv:2212.09885v2 [cs.CL] UPDATED)",
    "abstract": "Code generation from text requires understanding the user's intent from a natural language description and generating an executable code snippet that satisfies this intent. While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that the under-specification of a natural language description can be resolved by asking clarification questions. Therefore, we collect and introduce a new dataset named CodeClarQA containing pairs of natural language descriptions and code with created synthetic clarification questions and answers. The empirical results of our evaluation of pretrained language model performance on code generation show that clarifications result in more precisely generated code, as shown by the substantial improvement of model performance in all evaluation metrics. Alongside t",
    "link": "http://arxiv.org/abs/2212.09885",
    "context": "Title: Python Code Generation by Asking Clarification Questions. (arXiv:2212.09885v2 [cs.CL] UPDATED)\nAbstract: Code generation from text requires understanding the user's intent from a natural language description and generating an executable code snippet that satisfies this intent. While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that the under-specification of a natural language description can be resolved by asking clarification questions. Therefore, we collect and introduce a new dataset named CodeClarQA containing pairs of natural language descriptions and code with created synthetic clarification questions and answers. The empirical results of our evaluation of pretrained language model performance on code generation show that clarifications result in more precisely generated code, as shown by the substantial improvement of model performance in all evaluation metrics. Alongside t",
    "path": "papers/22/12/2212.09885.json",
    "total_tokens": 841,
    "translated_title": "通过提问澄清问题实现Python代码生成",
    "translated_abstract": "从文本中生成代码需要理解用户的意图，并生成满足此意图的可执行代码片段。在这项任务中，当自然语言描述不够明确时，最近的预训练语言模型表现不佳。本文提出了一种新颖而更现实的任务设置，假设自然语言描述的不明确性可以通过提问澄清问题来解决。因此，我们收集并引入了一个名为 CodeClarQA 的新数据集，其中包含自然语言描述和代码的成对数据以及创建的合成澄清问题和答案。评估预训练语言模型在代码生成上的实证结果表明，在自然语言描述存在歧义时，通过澄清问题可以生成更准确的代码，这体现在所有评估指标的模型性能显著提高。除了翻译外，我们提出了一种新的代码生成方法，其中包括提问澄清问题以消除自然语言描述中的歧义。",
    "tldr": "本文提出了一个新的方法来解决自然语言描述中存在歧义的问题--通过提问澄清问题，本文提出的方法在预训练语言模型性能上取得了显著改善。",
    "en_tdlr": "This paper proposes a novel approach to resolving ambiguity in natural language descriptions by incorporating asking clarification questions, which leads to substantially improved performance of pretrained language models in code generation."
}