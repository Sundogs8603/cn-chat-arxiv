{
    "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. (arXiv:2212.03597v3 [cs.LG] UPDATED)",
    "abstract": "Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focuses on data efficiency capabilities. To this end, we present DeepSpeed Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretr",
    "link": "http://arxiv.org/abs/2212.03597",
    "context": "Title: DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. (arXiv:2212.03597v3 [cs.LG] UPDATED)\nAbstract: Recent advances on deep learning models come at the price of formidable training cost. The increasing model size is one of the root causes, but another less-emphasized fact is that data scale is actually increasing at a similar speed as model scale, and the training cost is proportional to both of them. Compared to the rapidly evolving model architecture, how to efficiently use the training data (especially for the expensive foundation model pretraining) is both less explored and difficult to realize due to the lack of a convenient framework that focuses on data efficiency capabilities. To this end, we present DeepSpeed Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. Specifically, we propose and combine two data efficiency techniques: efficient data sampling via a general curriculum learning library, and efficient data routing via a novel random layerwise token dropping technique. For GPT-3 1.3B language model pretr",
    "path": "papers/22/12/2212.03597.json",
    "total_tokens": 916,
    "translated_title": "DeepSpeed数据效率：通过高效的数据采样和路由改善深度学习模型质量和训练效率",
    "translated_abstract": "近年来，深度学习模型的进步以巨大的训练成本为代价。模型尺寸的增加是一个主要原因，但另一个不太被重视的事实是，数据规模实际上与模型规模以相似的速度增加，而训练成本与两者成比例。与不断发展的模型架构相比，如何高效利用训练数据（特别是对于昂贵的基础模型预训练）在较少被探索且难以实现，这是因为缺乏一个专注于数据效率能力的方便框架。为此，我们提出了DeepSpeed数据效率，一种能更好地利用数据、提高训练效率并改善模型质量的框架。具体而言，我们提出并结合了两种数据效率技术：通过通用课程学习库实现高效数据采样，以及通过一种新颖的随机逐层删除令牌的技术实现高效数据路由。针对GPT-3 1.3B语言模型预训练进行了实验验证。",
    "tldr": "DeepSpeed Data Efficiency提出了两种数据效率技术：高效的数据采样和高效的数据路由，能够提高深度学习模型的训练效率和质量。",
    "en_tdlr": "DeepSpeed Data Efficiency introduces two data efficiency techniques: efficient data sampling and efficient data routing, which improve the training efficiency and quality of deep learning models."
}