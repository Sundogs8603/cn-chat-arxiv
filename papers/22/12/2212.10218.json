{
    "title": "GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator. (arXiv:2212.10218v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model. Our model, named as GanLM, is trained with two pre-training objectives: replaced token detection and replaced token denoising. Specifically, given masked source sentences, the generator outputs the target distribution and the discriminator predicts whether the target sampled tokens from distribution are incorrect. The target sentence is replaced with misclassified tokens to construct noisy previous context, which is used to generate the gold sentence. In general, both tasks improve the ability of language understanding and generation by selectivel",
    "link": "http://arxiv.org/abs/2212.10218",
    "context": "Title: GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator. (arXiv:2212.10218v2 [cs.CL] UPDATED)\nAbstract: Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model. Our model, named as GanLM, is trained with two pre-training objectives: replaced token detection and replaced token denoising. Specifically, given masked source sentences, the generator outputs the target distribution and the discriminator predicts whether the target sampled tokens from distribution are incorrect. The target sentence is replaced with misclassified tokens to construct noisy previous context, which is used to generate the gold sentence. In general, both tasks improve the ability of language understanding and generation by selectivel",
    "path": "papers/22/12/2212.10218.json",
    "total_tokens": 908,
    "translated_title": "GanLM: 带辅助鉴别器的编码器-解码器预训练",
    "translated_abstract": "预训练模型在自然语言处理领域取得了显著的成功。然而，现有的预训练方法未充分利用语言理解对生成的好处。受生成对抗网络（GANs）的思想启发，我们提出了一种GAN风格的编码器-解码器预训练模型，通过引入辅助鉴别器，在单个模型中统一了语言理解和生成的能力。我们的模型名为GanLM，使用两个预训练目标进行训练：替换令牌检测和替换令牌去噪。具体来说，给定掩码源句子，生成器输出目标分布，鉴别器预测从分布中抽样的目标令牌是否不正确。将目标句子替换为错误分类的令牌以构造带噪声的先前上下文，用于生成黄金句子。总的来说，这两个任务通过有选择性地提高语言理解和生成的能力。",
    "tldr": "本文提出了一种名为GanLM的编码器-解码器预训练模型，它引入了辅助鉴别器来统一语言理解和生成能力，并使用两个预训练目标进行训练：替换令牌检测和替换令牌去噪。"
}