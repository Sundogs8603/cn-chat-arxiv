{
    "title": "Hedging Complexity in Generalization via a Parametric Distributionally Robust Optimization Framework. (arXiv:2212.01518v2 [math.OC] UPDATED)",
    "abstract": "Empirical risk minimization (ERM) and distributionally robust optimization (DRO) are popular approaches for solving stochastic optimization problems that appear in operations management and machine learning. Existing generalization error bounds for these methods depend on either the complexity of the cost function or dimension of the random perturbations. Consequently, the performance of these methods can be poor for high-dimensional problems with complex objective functions. We propose a simple approach in which the distribution of random perturbations is approximated using a parametric family of distributions. This mitigates both sources of complexity; however, it introduces a model misspecification error. We show that this new source of error can be controlled by suitable DRO formulations. Our proposed parametric DRO approach has significantly improved generalization bounds over existing ERM and DRO methods and parametric ERM for a wide variety of settings. Our method is particularl",
    "link": "http://arxiv.org/abs/2212.01518",
    "context": "Title: Hedging Complexity in Generalization via a Parametric Distributionally Robust Optimization Framework. (arXiv:2212.01518v2 [math.OC] UPDATED)\nAbstract: Empirical risk minimization (ERM) and distributionally robust optimization (DRO) are popular approaches for solving stochastic optimization problems that appear in operations management and machine learning. Existing generalization error bounds for these methods depend on either the complexity of the cost function or dimension of the random perturbations. Consequently, the performance of these methods can be poor for high-dimensional problems with complex objective functions. We propose a simple approach in which the distribution of random perturbations is approximated using a parametric family of distributions. This mitigates both sources of complexity; however, it introduces a model misspecification error. We show that this new source of error can be controlled by suitable DRO formulations. Our proposed parametric DRO approach has significantly improved generalization bounds over existing ERM and DRO methods and parametric ERM for a wide variety of settings. Our method is particularl",
    "path": "papers/22/12/2212.01518.json",
    "total_tokens": 857,
    "translated_title": "通过参数分布鲁棒优化框架降低泛化复杂度",
    "translated_abstract": "经验风险最小化(ERM)和分布鲁棒优化(DRO)是解决运营管理和机器学习中出现的随机优化问题的流行方法。现有的这些方法的泛化误差界限要么依赖于成本函数的复杂度，要么依赖于随机扰动的维度。因此，在具有复杂目标函数的高维问题中，这些方法的性能可能较差。我们提出了一种简单的方法，该方法通过使用参数分布族来逼近随机扰动的分布。这减轻了两种复杂性来源；然而，它引入了模型未正确建模的误差。我们表明，这种新的误差来源可以通过合适的DRO公式来控制。我们提出的参数DRO方法在各种设置下对现有的ERM和DRO方法以及参数ERM的泛化界限有显著改进。我们的方法特别适用于。。。",
    "tldr": "通过使用参数分布鲁棒优化框架，我们提出了一种降低高维复杂问题泛化误差的简单方法，并且在各种设置下取得了显著改进的效果。",
    "en_tdlr": "We propose a simple approach to reduce the generalization error of high-dimensional complex problems by using a parametric distributionally robust optimization framework, which significantly improves the performance in various settings."
}