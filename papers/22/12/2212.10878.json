{
    "title": "Automatic Network Adaptation for Ultra-Low Uniform-Precision Quantization. (arXiv:2212.10878v3 [cs.CV] UPDATED)",
    "abstract": "Uniform-precision neural network quantization has gained popularity since it simplifies densely packed arithmetic unit for high computing capability. However, it ignores heterogeneous sensitivity to the impact of quantization errors across the layers, resulting in sub-optimal inference accuracy. This work proposes a novel neural architecture search called neural channel expansion that adjusts the network structure to alleviate accuracy degradation from ultra-low uniform-precision quantization. The proposed method selectively expands channels for the quantization sensitive layers while satisfying hardware constraints (e.g., FLOPs, PARAMs). Based on in-depth analysis and experiments, we demonstrate that the proposed method can adapt several popular networks channels to achieve superior 2-bit quantization accuracy on CIFAR10 and ImageNet. In particular, we achieve the best-to-date Top-1/Top-5 accuracy for 2-bit ResNet50 with smaller FLOPs and the parameter size.",
    "link": "http://arxiv.org/abs/2212.10878",
    "context": "Title: Automatic Network Adaptation for Ultra-Low Uniform-Precision Quantization. (arXiv:2212.10878v3 [cs.CV] UPDATED)\nAbstract: Uniform-precision neural network quantization has gained popularity since it simplifies densely packed arithmetic unit for high computing capability. However, it ignores heterogeneous sensitivity to the impact of quantization errors across the layers, resulting in sub-optimal inference accuracy. This work proposes a novel neural architecture search called neural channel expansion that adjusts the network structure to alleviate accuracy degradation from ultra-low uniform-precision quantization. The proposed method selectively expands channels for the quantization sensitive layers while satisfying hardware constraints (e.g., FLOPs, PARAMs). Based on in-depth analysis and experiments, we demonstrate that the proposed method can adapt several popular networks channels to achieve superior 2-bit quantization accuracy on CIFAR10 and ImageNet. In particular, we achieve the best-to-date Top-1/Top-5 accuracy for 2-bit ResNet50 with smaller FLOPs and the parameter size.",
    "path": "papers/22/12/2212.10878.json",
    "total_tokens": 914,
    "translated_title": "超低均匀精度量化的自动网络适应",
    "translated_abstract": "均匀精度神经网络量化因其简化了高计算性能的紧密填充算术单元而变得流行。然而，它忽略了层间对量化误差的异构敏感性，导致推理准确性不佳。本文提出了一种名为神经通道扩展的神经架构搜索方法，调整网络结构以减轻超低均匀精度量化对推理准确性的影响。该方法优化了各层的通道扩展，同时满足硬件约束（例如FLOPs，PARAMs）。基于深入的分析和实验，我们证明了该方法可以调整多个流行网络，以在CIFAR10和ImageNet上实现更好的2位量化准确性。特别是，我们实现了具有更小的FLOPs和参数大小的2bit ResNet50最佳的Top-1/Top-5准确率。",
    "tldr": "本论文提出了一种名为神经通道扩展的神经架构搜索方法，该方法能够自适应多个流行网络，通过选择性扩展通道和满足硬件约束，实现了超低均匀精度量化对推理准确性的减轻和提升，其中2bit ResNet50准确率达到了目前最佳水平。",
    "en_tdlr": "This paper proposes a neural architecture search method called neural channel expansion, which can adapt multiple popular networks to alleviate and improve the inference accuracy degradation from ultra-low uniform-precision quantization. By selectively expanding channels and satisfying hardware constraints, 2bit ResNet50 achieves the best-to-date Top-1/Top-5 accuracy."
}