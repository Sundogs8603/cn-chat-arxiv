{
    "title": "Representation Learning in Deep RL via Discrete Information Bottleneck. (arXiv:2212.13835v2 [cs.LG] UPDATED)",
    "abstract": "Several self-supervised representation learning methods have been proposed for reinforcement learning (RL) with rich observations. For real-world applications of RL, recovering underlying latent states is crucial, particularly when sensory inputs contain irrelevant and exogenous information. In this work, we study how information bottlenecks can be used to construct latent states efficiently in the presence of task-irrelevant information. We propose architectures that utilize variational and discrete information bottlenecks, coined as RepDIB, to learn structured factorized representations. Exploiting the expressiveness bought by factorized representations, we introduce a simple, yet effective, bottleneck that can be integrated with any existing self-supervised objective for RL. We demonstrate this across several online and offline RL benchmarks, along with a real robot arm task, where we find that compressed representations with RepDIB can lead to strong performance improvements, as th",
    "link": "http://arxiv.org/abs/2212.13835",
    "context": "Title: Representation Learning in Deep RL via Discrete Information Bottleneck. (arXiv:2212.13835v2 [cs.LG] UPDATED)\nAbstract: Several self-supervised representation learning methods have been proposed for reinforcement learning (RL) with rich observations. For real-world applications of RL, recovering underlying latent states is crucial, particularly when sensory inputs contain irrelevant and exogenous information. In this work, we study how information bottlenecks can be used to construct latent states efficiently in the presence of task-irrelevant information. We propose architectures that utilize variational and discrete information bottlenecks, coined as RepDIB, to learn structured factorized representations. Exploiting the expressiveness bought by factorized representations, we introduce a simple, yet effective, bottleneck that can be integrated with any existing self-supervised objective for RL. We demonstrate this across several online and offline RL benchmarks, along with a real robot arm task, where we find that compressed representations with RepDIB can lead to strong performance improvements, as th",
    "path": "papers/22/12/2212.13835.json",
    "total_tokens": 760,
    "translated_title": "通过离散信息瓶颈在深度强化学习中进行表征学习",
    "translated_abstract": "针对强化学习中感知信息带有噪声和无关信息的情况，本文提出了一种利用信息瓶颈方法构建隐状态的表征学习方法，名为RepDIB。该方法利用可变离散信息瓶颈学习结构化分解表示，并将简单而有效的瓶颈与现有自监督目标相结合。在多个在线和离线RL基准测试以及真实机器人臂任务中，实验证明使用RepDIB进行压缩表征可以有效提高性能。",
    "tldr": "本研究提出了一种名为RepDIB的表征学习方法，在强化学习中通过离散信息瓶颈构建隐状态，可以有效地去除感知信息中的噪声和无关信息，实验表明该方法能够提高性能。",
    "en_tdlr": "This paper proposes a representation learning method called RepDIB, which constructs latent states through discrete information bottlenecks in reinforcement learning to efficiently remove noise and irrelevant information in sensory inputs, and experiments show that it can improve performance."
}