{
    "title": "LAMBADA: Backward Chaining for Automated Reasoning in Natural Language. (arXiv:2212.13894v2 [cs.AI] UPDATED)",
    "abstract": "Remarkable progress has been made on automated reasoning with natural text, by using Language Models (LMs) and methods such as Chain-of-Thought and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from the intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules. These sub-modules are simply implemented by few-shot prompted LM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on challenging logical reasoning datasets, particularly when ",
    "link": "http://arxiv.org/abs/2212.13894",
    "context": "Title: LAMBADA: Backward Chaining for Automated Reasoning in Natural Language. (arXiv:2212.13894v2 [cs.AI] UPDATED)\nAbstract: Remarkable progress has been made on automated reasoning with natural text, by using Language Models (LMs) and methods such as Chain-of-Thought and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from the intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules. These sub-modules are simply implemented by few-shot prompted LM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on challenging logical reasoning datasets, particularly when ",
    "path": "papers/22/12/2212.13894.json",
    "total_tokens": 840,
    "tldr": "LAMBADA是一种自然语言自动推理技术，使用反向链接算法，在逻辑推理数据集上能够获得比前向推理方法更高的准确度。"
}