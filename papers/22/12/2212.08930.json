{
    "title": "On Noisy Evaluation in Federated Hyperparameter Tuning. (arXiv:2212.08930v4 [cs.LG] UPDATED)",
    "abstract": "Hyperparameter tuning is critical to the success of federated learning applications. Unfortunately, appropriately selecting hyperparameters is challenging in federated networks. Issues of scale, privacy, and heterogeneity introduce noise in the tuning process and make it difficult to evaluate the performance of various hyperparameters. In this work, we perform the first systematic study on the effect of noisy evaluation in federated hyperparameter tuning. We first identify and rigorously explore key sources of noise, including client subsampling, data and systems heterogeneity, and data privacy. Surprisingly, our results indicate that even small amounts of noise can significantly impact tuning methods-reducing the performance of state-of-the-art approaches to that of naive baselines. To address noisy evaluation in such scenarios, we propose a simple and effective approach that leverages public proxy data to boost the evaluation signal. Our work establishes general challenges, baselines",
    "link": "http://arxiv.org/abs/2212.08930",
    "context": "Title: On Noisy Evaluation in Federated Hyperparameter Tuning. (arXiv:2212.08930v4 [cs.LG] UPDATED)\nAbstract: Hyperparameter tuning is critical to the success of federated learning applications. Unfortunately, appropriately selecting hyperparameters is challenging in federated networks. Issues of scale, privacy, and heterogeneity introduce noise in the tuning process and make it difficult to evaluate the performance of various hyperparameters. In this work, we perform the first systematic study on the effect of noisy evaluation in federated hyperparameter tuning. We first identify and rigorously explore key sources of noise, including client subsampling, data and systems heterogeneity, and data privacy. Surprisingly, our results indicate that even small amounts of noise can significantly impact tuning methods-reducing the performance of state-of-the-art approaches to that of naive baselines. To address noisy evaluation in such scenarios, we propose a simple and effective approach that leverages public proxy data to boost the evaluation signal. Our work establishes general challenges, baselines",
    "path": "papers/22/12/2212.08930.json",
    "total_tokens": 952,
    "translated_title": "关于联邦超参数调整中的噪声评估",
    "translated_abstract": "超参数调整对于联邦学习应用的成功至关重要。然而，在联邦网络中适当地选择超参数是具有挑战性的。规模、隐私和异构性等问题会导致调整过程中产生噪声，并使评估各种超参数的性能变得困难。在这项工作中，我们进行了关于联邦超参数调整中噪声评估影响的第一次系统研究。我们首先确定并严格探索关键噪声源，包括客户端子采样、数据和系统的异构性以及数据隐私。令人惊讶的是，我们的结果表明，即使是小量的噪声也会显著影响调整方法，将尖端方法的性能降低到幼稚的基线水平。针对此类情况中的噪声评估问题，我们提出了一种简单有效的方法，利用公共代理数据来提高评估信号。我们的研究为解决联邦超参数调整中的噪声评估问题提供了一般性挑战、基线和解决方案。",
    "tldr": "本论文为联邦超参数调整中的噪声评估问题提供了第一次系统研究，发现即使是小量的噪声也会显著影响现有的方法，提出了一种利用公共代理数据来提高评估信号的简单有效方法。",
    "en_tdlr": "This paper provides the first systematic study on the effect of noisy evaluation in federated hyperparameter tuning, and proposes a simple and effective approach that leverages public proxy data to boost the evaluation signal. It shows that even small amounts of noise can significantly impact existing approaches."
}