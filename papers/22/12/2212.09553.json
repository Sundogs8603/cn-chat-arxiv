{
    "title": "Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models. (arXiv:2212.09553v2 [cs.CL] UPDATED)",
    "abstract": "We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT), in over 100 languages. By leveraging a quantized representation of speech as a target, Mu$^{2}$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling (MLM) objective on the encoder, for both unlabeled speech and text, while utilizing the supervised tasks to improve cross-lingual and cross-modal representation alignment within the model. On CoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR, our model matches the performance of an mSLAM model fine-tuned with an RNN-",
    "link": "http://arxiv.org/abs/2212.09553",
    "context": "Title: Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models. (arXiv:2212.09553v2 [cs.CL] UPDATED)\nAbstract: We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT), in over 100 languages. By leveraging a quantized representation of speech as a target, Mu$^{2}$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling (MLM) objective on the encoder, for both unlabeled speech and text, while utilizing the supervised tasks to improve cross-lingual and cross-modal representation alignment within the model. On CoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR, our model matches the performance of an mSLAM model fine-tuned with an RNN-",
    "path": "papers/22/12/2212.09553.json",
    "total_tokens": 1033,
    "translated_title": "Mu$^{2}$SLAM: 多任务、多语言的语音和语言模型",
    "translated_abstract": "我们提出了Mu$^{2}$SLAM，它是一个多语言序列到序列模型，同时在超过100种语言的未标记语音、未标记文本和监督数据上进行预训练，覆盖了自动语音识别（ASR）、自动语音翻译（AST）和机器翻译（MT）领域。通过利用量化表示的语音作为目标，Mu$^{2}$SLAM使用类似于T5的序列到序列掩蔽去噪目标在解码器上训练语音-文本模型，并在编码器上使用掩蔽语言建模（MLM）目标来训练未标记语音和文本，同时利用监督任务来提高模型内的跨语言和跨模态表示对齐。在CoVoST AST上，Mu$^{2}$SLAM在公共数据集上训练的模型达到了新的最高水平，在XX-EN翻译上比之前最优结果提高了1.9 BLEU分，对于EN-XX翻译提高了1.1 BLEU分。在Voxpopuli ASR上，我们的模型与使用RNN微调的mSLAM模型的性能相当。",
    "tldr": "Mu$^{2}$SLAM是一个多语言的语音和语言模型，通过使用未标记的语音和文本进行预训练，以及利用监督任务提高跨语言和跨模态表示对齐，取得了在CoVoST AST上新的最高性能，并与使用RNN微调的mSLAM模型在Voxpopuli ASR上性能相当。"
}