{
    "title": "Context-aware Fine-tuning of Self-supervised Speech Models. (arXiv:2212.08542v2 [eess.AS] UPDATED)",
    "abstract": "Self-supervised pre-trained transformers have improved the state of the art on a variety of speech tasks. Due to the quadratic time and space complexity of self-attention, they usually operate at the level of relatively short (e.g., utterance) segments. In this paper, we study the use of context, i.e., surrounding segments, during fine-tuning and propose a new approach called context-aware fine-tuning. We attach a context module on top of the last layer of a pre-trained model to encode the whole segment into a context embedding vector which is then used as an additional feature for the final prediction. During the fine-tuning stage, we introduce an auxiliary loss that encourages this context embedding vector to be similar to context vectors of surrounding segments. This allows the model to make predictions without access to these surrounding segments at inference time and requires only a tiny overhead compared to standard fine-tuned models. We evaluate the proposed approach using the S",
    "link": "http://arxiv.org/abs/2212.08542",
    "context": "Title: Context-aware Fine-tuning of Self-supervised Speech Models. (arXiv:2212.08542v2 [eess.AS] UPDATED)\nAbstract: Self-supervised pre-trained transformers have improved the state of the art on a variety of speech tasks. Due to the quadratic time and space complexity of self-attention, they usually operate at the level of relatively short (e.g., utterance) segments. In this paper, we study the use of context, i.e., surrounding segments, during fine-tuning and propose a new approach called context-aware fine-tuning. We attach a context module on top of the last layer of a pre-trained model to encode the whole segment into a context embedding vector which is then used as an additional feature for the final prediction. During the fine-tuning stage, we introduce an auxiliary loss that encourages this context embedding vector to be similar to context vectors of surrounding segments. This allows the model to make predictions without access to these surrounding segments at inference time and requires only a tiny overhead compared to standard fine-tuned models. We evaluate the proposed approach using the S",
    "path": "papers/22/12/2212.08542.json",
    "total_tokens": 928,
    "translated_title": "自监督语音模型的上下文感知微调研究",
    "translated_abstract": "自监督预训练的transformer在各种语音任务方面都取得了突破。由于自注意力的二次时间和空间复杂度，它们通常在相对较短的片段（例如语音）级别上操作。本文研究了在微调期间使用上下文（即周围片段）并提出了一种新方法，称为上下文感知微调。我们在预训练模型的最后一层上附加了一个上下文模块，将整个片段编码为一个上下文嵌入向量，然后将其用作最终预测的另一个特征。在微调阶段，我们引入了一个辅助损失，鼓励该上下文嵌入向量与周围片段的上下文向量相似。这允许模型在推理时进行预测，而无需访问这些周围片段，与标准微调模型相比，仅需要极小的开销。我们使用S数据集完成了所提出方法的评估。",
    "tldr": "本文研究了一种新方法，称为上下文感知微调。在微调期间使用上下文信息可以使模型在推理时进行预测，而无需访问这些周围片段。该方法能够有效减小运行开销，并在S数据集上取得了良好的评估结果。",
    "en_tdlr": "This paper proposes a new approach called context-aware fine-tuning to use context information during the fine-tuning stage, which allows the model to make predictions without accessing surrounding segments at inference time. The proposed method can significantly reduce the computational cost and achieves good evaluation results on the S dataset."
}