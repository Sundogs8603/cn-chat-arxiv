{
    "title": "Relation-Aware Language-Graph Transformer for Question Answering. (arXiv:2212.00975v2 [cs.CL] UPDATED)",
    "abstract": "Question Answering (QA) is a task that entails reasoning over natural language contexts, and many relevant works augment language models (LMs) with graph neural networks (GNNs) to encode the Knowledge Graph (KG) information. However, most existing GNN-based modules for QA do not take advantage of rich relational information of KGs and depend on limited information interaction between the LM and the KG. To address these issues, we propose Question Answering Transformer (QAT), which is designed to jointly reason over language and graphs with respect to entity relations in a unified manner. Specifically, QAT constructs Meta-Path tokens, which learn relation-centric embeddings based on diverse structural and semantic relations. Then, our Relation-Aware Self-Attention module comprehensively integrates different modalities via the Cross-Modal Relative Position Bias, which guides information exchange between relevant entites of different modalities. We validate the effectiveness of QAT on com",
    "link": "http://arxiv.org/abs/2212.00975",
    "context": "Title: Relation-Aware Language-Graph Transformer for Question Answering. (arXiv:2212.00975v2 [cs.CL] UPDATED)\nAbstract: Question Answering (QA) is a task that entails reasoning over natural language contexts, and many relevant works augment language models (LMs) with graph neural networks (GNNs) to encode the Knowledge Graph (KG) information. However, most existing GNN-based modules for QA do not take advantage of rich relational information of KGs and depend on limited information interaction between the LM and the KG. To address these issues, we propose Question Answering Transformer (QAT), which is designed to jointly reason over language and graphs with respect to entity relations in a unified manner. Specifically, QAT constructs Meta-Path tokens, which learn relation-centric embeddings based on diverse structural and semantic relations. Then, our Relation-Aware Self-Attention module comprehensively integrates different modalities via the Cross-Modal Relative Position Bias, which guides information exchange between relevant entites of different modalities. We validate the effectiveness of QAT on com",
    "path": "papers/22/12/2212.00975.json",
    "total_tokens": 846,
    "translated_title": "基于关系感知的语言图转换器用于问答",
    "translated_abstract": "问答是一项需要推理自然语言环境的任务，许多相关工作通过图神经网络(GNN)增强语言模型(LM)，以对知识图谱(KG)信息进行编码。然而，大多数现有的面向问答的基于GNN的模块并未利用KG的丰富关系信息，并且依赖于LM和KG之间的有限信息交互。为了解决这些问题，我们提出了 Question Answering Transformer(QAT)，它旨在以统一的方式联合推理语言和图关于实体关系。具体而言，QAT构建了元路径令牌，这些令牌学习基于不同的结构和语义关系的关系中心嵌入。然后，我们的关系感知自注意力模块通过跨模态相关位置偏差全面整合了不同的模态，以指导不同模态之间相关实体的信息交换。我们验证了QAT在多个QA数据集上的有效性。",
    "tldr": "本论文提出了关系感知语言图转换器，能够以统一的方式联合推理语言和图关于实体关系，并在多个QA数据集上验证了其有效性。",
    "en_tdlr": "This paper proposes a relation-aware language-graph transformer for question answering, which can jointly reason over language and graphs with respect to entity relations in a unified manner, and its effectiveness is validated on multiple QA datasets."
}