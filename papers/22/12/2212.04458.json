{
    "title": "General-Purpose In-Context Learning by Meta-Learning Transformers. (arXiv:2212.04458v2 [cs.LG] UPDATED)",
    "abstract": "Modern machine learning requires system designers to specify aspects of the learning pipeline, such as losses, architectures, and optimizers. Meta-learning, or learning-to-learn, instead aims to learn those aspects, and promises to unlock greater capabilities with less manual effort. One particularly ambitious goal of meta-learning is to train general-purpose in-context learning algorithms from scratch, using only black-box models with minimal inductive bias. Such a model takes in training data, and produces test-set predictions across a wide range of problems, without any explicit definition of an inference model, training loss, or optimization algorithm. In this paper we show that Transformers and other black-box models can be meta-trained to act as general-purpose in-context learners. We characterize transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-opti",
    "link": "http://arxiv.org/abs/2212.04458",
    "context": "Title: General-Purpose In-Context Learning by Meta-Learning Transformers. (arXiv:2212.04458v2 [cs.LG] UPDATED)\nAbstract: Modern machine learning requires system designers to specify aspects of the learning pipeline, such as losses, architectures, and optimizers. Meta-learning, or learning-to-learn, instead aims to learn those aspects, and promises to unlock greater capabilities with less manual effort. One particularly ambitious goal of meta-learning is to train general-purpose in-context learning algorithms from scratch, using only black-box models with minimal inductive bias. Such a model takes in training data, and produces test-set predictions across a wide range of problems, without any explicit definition of an inference model, training loss, or optimization algorithm. In this paper we show that Transformers and other black-box models can be meta-trained to act as general-purpose in-context learners. We characterize transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-opti",
    "path": "papers/22/12/2212.04458.json",
    "total_tokens": 883,
    "translated_title": "通过元学习Transformer实现通用上下文学习",
    "translated_abstract": "现代机器学习要求系统设计者指定学习流程的方方面面，例如损失函数、架构和优化器。而元学习，或者学会学习，目标是学习这些方面，并承诺以更少的手动工作开启更大的能力。元学习的一个特别雄心勃勃的目标是从头开始训练通用的上下文学习算法，仅使用带有最小归纳偏见的黑盒模型。这样的模型接收训练数据，并在各种问题上产生测试集预测，而无需定义推理模型、训练损失或优化算法。在本文中，我们展示了Transformer和其他黑盒模型可以被元训练成为通用的上下文学习器。我们通过模型大小、任务数量和元优化引起的算法之间的转换进行了表征，这些算法可以实现泛化，也可以实现记忆，还有一些算法根本无法进行元训练。",
    "tldr": "本文展示了Transformer和其他黑盒模型可以通过元学习训练成为通用的上下文学习器，该模型可以在各种问题上进行测试集预测，无需定义推理模型、训练损失或优化算法。"
}