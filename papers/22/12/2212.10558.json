{
    "title": "On-the-fly Denoising for Data Augmentation in Natural Language Understanding",
    "abstract": "Data Augmentation (DA) is frequently used to provide additional training data without extra human annotation automatically. However, data augmentation may introduce noisy data that impairs training. To guarantee the quality of augmented data, existing methods either assume no noise exists in the augmented data and adopt consistency training or use simple heuristics such as training loss and diversity constraints to filter out \"noisy\" data. However, those filtered examples may still contain useful information, and dropping them completely causes a loss of supervision signals. In this paper, based on the assumption that the original dataset is cleaner than the augmented data, we propose an on-the-fly denoising technique for data augmentation that learns from soft augmented labels provided by an organic teacher model trained on the cleaner original data. To further prevent overfitting on noisy labels, a simple self-regularization module is applied to force the model prediction to be consi",
    "link": "https://arxiv.org/abs/2212.10558",
    "context": "Title: On-the-fly Denoising for Data Augmentation in Natural Language Understanding\nAbstract: Data Augmentation (DA) is frequently used to provide additional training data without extra human annotation automatically. However, data augmentation may introduce noisy data that impairs training. To guarantee the quality of augmented data, existing methods either assume no noise exists in the augmented data and adopt consistency training or use simple heuristics such as training loss and diversity constraints to filter out \"noisy\" data. However, those filtered examples may still contain useful information, and dropping them completely causes a loss of supervision signals. In this paper, based on the assumption that the original dataset is cleaner than the augmented data, we propose an on-the-fly denoising technique for data augmentation that learns from soft augmented labels provided by an organic teacher model trained on the cleaner original data. To further prevent overfitting on noisy labels, a simple self-regularization module is applied to force the model prediction to be consi",
    "path": "papers/22/12/2212.10558.json",
    "total_tokens": 895,
    "translated_title": "自然语言理解中的即时去噪数据增强",
    "translated_abstract": "数据增强（DA）经常被用来在没有额外的人工注释的情况下提供额外的训练数据。然而，数据增强可能引入噪声数据来干扰训练。为了保证增强数据的质量，现有方法要么假设增强数据中没有噪声，并采用一致性训练，要么使用简单的启发式方法（如训练损失和多样性约束）来过滤掉“嘈杂”的数据。然而，这些被过滤的示例可能仍然包含有用的信息，并且完全丢弃它们会导致监督信号的丧失。在本文中，基于原始数据集比增强数据更干净的假设，我们提出了一种即时去噪的数据增强技术，该技术利用了在更干净的原始数据上训练的有机教师模型提供的软增强标签进行学习。为了进一步防止过度拟合噪声标签，我们还应用了简单的自我正则化模块，强制模型预测保持一致。",
    "tldr": "本文提出了一种即时去噪的数据增强技术，利用软增强标签和自我正则化模块，通过从更干净的原始数据学习来保证增强数据的质量。",
    "en_tdlr": "This paper proposes an on-the-fly denoising technique for data augmentation in natural language understanding, which learns from soft augmented labels provided by an organic teacher model trained on cleaner original data, ensuring the quality of augmented data."
}