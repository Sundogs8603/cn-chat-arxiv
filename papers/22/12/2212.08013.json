{
    "title": "FlexiViT: One Model for All Patch Sizes. (arXiv:2212.08013v2 [cs.CV] UPDATED)",
    "abstract": "Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to a",
    "link": "http://arxiv.org/abs/2212.08013",
    "context": "Title: FlexiViT: One Model for All Patch Sizes. (arXiv:2212.08013v2 [cs.CV] UPDATED)\nAbstract: Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to a",
    "path": "papers/22/12/2212.08013.json",
    "total_tokens": 909,
    "translated_title": "FlexiViT：适用于所有补丁大小的模型",
    "translated_abstract": "Vision Transformer将图像切成补丁以将其转换为序列。这些补丁的大小控制速度/准确性权衡，补丁越小，精度越高，但计算成本也越高，但更改补丁大小通常需要重新训练模型。本文证明，简单随机化训练时的补丁大小会导致一组权重表现良好，可在各种补丁大小范围内使用，从而使其在部署时容易根据不同的计算预算来定制模型。我们在许多任务上对生成的模型进行了全面评估，包括分类，图像-文本检索，开放式检测，全景分割和语义分割，得出结论：它通常可以匹配，并且有时会优于以单个补丁大小训练的标准ViT模型。因此，在ViT中使用FlexiViT进行训练是一种简单易行的改进方法。",
    "tldr": "本文介绍了一种名为FlexiViT的模型，它可以使用一组权重表现良好，并且适用于各种补丁大小，从而使其在部署时容易根据不同的计算预算来定制模型。"
}