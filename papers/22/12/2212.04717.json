{
    "title": "On the Sensitivity of Reward Inference to Misspecified Human Models. (arXiv:2212.04717v2 [cs.LG] UPDATED)",
    "abstract": "Inferring reward functions from human behavior is at the center of value alignment - aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases ",
    "link": "http://arxiv.org/abs/2212.04717",
    "context": "Title: On the Sensitivity of Reward Inference to Misspecified Human Models. (arXiv:2212.04717v2 [cs.LG] UPDATED)\nAbstract: Inferring reward functions from human behavior is at the center of value alignment - aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases ",
    "path": "papers/22/12/2212.04717.json",
    "total_tokens": 893,
    "translated_title": "关于奖励推断对错误人类模型的敏感性",
    "translated_abstract": "从人类行为中推断奖励函数是与价值对齐密切相关的内容 - 确保人工智能的目标与我们人类实际想要的一致。但这需要建立人类的行为模型。经过几十年的认知科学、神经科学和行为经济学研究，获得准确的人类模型仍然是一个开放的研究课题。这引出了一个问题：模型的准确性对于奖励推断的准确性有多重要？一方面，如果模型中存在小错误就会导致推断的灾难性错误，那么奖励学习的整个框架似乎注定失败，因为我们永远无法拥有完美的人类行为模型。另一方面，如果随着模型的改进，可以保证奖励的准确性也会提高，这将证明在模型方面做更多工作的益处。我们在理论和实证方面研究了这个问题。我们确实展示了构建小的对抗性偏差是可能的",
    "tldr": "本研究从理论和实证角度研究了奖励推断对错误人类模型的敏感性，并发现存在可能构建小的对抗性偏差的情况。"
}