{
    "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions. (arXiv:2212.10560v2 [cs.CL] UPDATED)",
    "abstract": "Large \"instruction-tuned\" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written inst",
    "link": "http://arxiv.org/abs/2212.10560",
    "context": "Title: Self-Instruct: Aligning Language Models with Self-Generated Instructions. (arXiv:2212.10560v2 [cs.CL] UPDATED)\nAbstract: Large \"instruction-tuned\" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written inst",
    "path": "papers/22/12/2212.10560.json",
    "total_tokens": 939,
    "translated_title": "自我指导: 用自生成的指示对齐语言模型",
    "translated_abstract": "大型的\"指令调整\"语言模型(即，调整为响应指令)已经展示了惊人的能力，可以零-shot推广到新任务。然而，它们严重依赖于人工编写的指令数据，通常在数量、多样性和创造力方面受到限制，从而阻碍了调整模型的通用性。我们引入了Self-Instruct，这是一个用于改善预训练语言模型遵循指令能力的框架，通过自身的生成来引导它们。我们的工作流程从语言模型中生成指令、输入和输出样本，然后过滤掉无效或相似的样本，然后再将它们用于调整原始模型。将我们的方法应用于普通的GPT3，我们展示了在超自然指令上与InstructGPT-001的性能相媲美，并比原始模型获得了33%的绝对改进。",
    "tldr": "本论文提出了一种名为Self-Instruct的框架，通过自身生成指导信息来提高预训练语言模型的指令遵循能力。在超自然指令上，我们展示了与InstructGPT-001相同的性能表现，并在原始模型上获得了33%的改进。"
}