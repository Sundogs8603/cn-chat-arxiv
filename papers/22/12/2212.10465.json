{
    "title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization. (arXiv:2212.10465v2 [cs.CL] UPDATED)",
    "abstract": "We present SODA: the first publicly available, million-scale high-quality social dialogue dataset. In contrast to most existing crowdsourced, small-scale dialogue corpora, we distill 1.5M socially-grounded dialogues from a large language model (InstructGPT; Ouyang et al., 2022). Dialogues are distilled by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x; West et al., 2022). Human evaluation shows that dialogues in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets.  Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. ",
    "link": "http://arxiv.org/abs/2212.10465",
    "context": "Title: SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization. (arXiv:2212.10465v2 [cs.CL] UPDATED)\nAbstract: We present SODA: the first publicly available, million-scale high-quality social dialogue dataset. In contrast to most existing crowdsourced, small-scale dialogue corpora, we distill 1.5M socially-grounded dialogues from a large language model (InstructGPT; Ouyang et al., 2022). Dialogues are distilled by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x; West et al., 2022). Human evaluation shows that dialogues in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets.  Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. ",
    "path": "papers/22/12/2212.10465.json",
    "total_tokens": 1003,
    "translated_title": "SODA: 具有社交常识语境化的百万规模对话蒸馏",
    "translated_abstract": "我们提出了SODA：第一个公开可用的百万规模高质量社交对话数据集。与大多数现有的众包小规模对话语料库不同，我们通过从知识图谱（Atomic10x; West等人，2022）中的社交常识知识进行上下文化，提炼了150万个社交对话。人类评估表明，SODA中的对话比以前的由人类撰写的数据集更一致、更具体且（令人惊讶地）更自然。我们使用SODA训练了COSMO：一个通用的对话模型，在未知的数据集上比最佳表现的对话模型（例如GODEL、BlenderBot-1、Koala、Vicuna）更自然和一致。实验结果表明，COSMO有时甚至被认为优于原始的人工编写的标准回答。此外，我们的结果揭示了知识丰富型对话和自然社交闲聊之间的区别。",
    "tldr": "SODA是第一个公开发布的百万级别高质量社交对话数据集，通过从知识图谱中上下文化社交常识知识进行蒸馏，我们训练出了COSMO，其比目前最佳表现的对话模型更为自然和一致，这有助于了解知识丰富型对话和自然社交闲聊之间的差异。",
    "en_tdlr": "SODA is the first publicly available, million-scale high-quality social dialogue dataset, distilled by contextualizing social commonsense knowledge from a knowledge graph. Using SODA, COSMO, a generalizable conversation model, is trained and outperforms best-performing conversation models, shedding light on the distinction between knowledge-enriched conversations and natural social chitchats."
}