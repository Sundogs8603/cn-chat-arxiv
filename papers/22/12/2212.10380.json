{
    "title": "What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary. (arXiv:2212.10380v2 [cs.CL] UPDATED)",
    "abstract": "Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model's vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to ",
    "link": "http://arxiv.org/abs/2212.10380",
    "context": "Title: What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary. (arXiv:2212.10380v2 [cs.CL] UPDATED)\nAbstract: Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model's vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to ",
    "path": "papers/22/12/2212.10380.json",
    "total_tokens": 929,
    "translated_title": "你所谓的令牌是关于什么的？稠密检索作为词汇表上的分布。",
    "translated_abstract": "双编码器现在是稠密检索的主要架构。然而，我们对它们如何表示文本以及为什么会导致良好性能知之甚少。在本文中，我们通过词汇表上的分布来阐明这个问题。我们建议通过将双编码器产生的向量表示投影到模型的词汇表空间中来解释它们，我们展示了产生的投影包含丰富的语义信息，并将它们与稀疏检索之间进行联系。我们发现，这种观点可以解释稠密检索器的一些失败案例。例如，我们观察到模型无法处理尾部实体与令牌分布倾向于忘记这些实体的某些令牌之间存在相关性。我们利用了这一洞察，并提出了一种在推理时丰富查询和段落表示与词汇信息的简单方法，并展示了这相比于常规的双编码器有显著的性能提升。",
    "tldr": "本文探讨了双编码器用于稠密检索的机制，通过将向量表示投影到模型的词汇表空间来解释它们，进一步解释了一些失败案例，提出了一种简单的方法在推理时丰富查询和段落表示与词汇信息，显著提高了性能。",
    "en_tdlr": "This paper explores the mechanism of dual encoders for dense retrieval by interpreting vector representations through projecting them into the model's vocabulary space, further explains some failure cases, proposes a simple method to enrich query and passage representations with lexical information at inference time, which significantly improves performance."
}