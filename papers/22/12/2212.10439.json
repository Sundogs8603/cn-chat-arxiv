{
    "title": "Policy Gradient in Robust MDPs with Global Convergence Guarantee. (arXiv:2212.10439v2 [cs.LG] UPDATED)",
    "abstract": "Robust Markov decision processes (RMDPs) provide a promising framework for computing reliable policies in the face of model errors. Many successful reinforcement learning algorithms build on variations of policy-gradient methods, but adapting these methods to RMDPs has been challenging. As a result, the applicability of RMDPs to large, practical domains remains limited. This paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first generic policy gradient method for RMDPs. In contrast with prior robust policy gradient algorithms, DRPG monotonically reduces approximation errors to guarantee convergence to a globally optimal policy in tabular RMDPs. We introduce a novel parametric transition kernel and solve the inner loop robust policy via a gradient-based method. Finally, our numerical results demonstrate the utility of our new algorithm and confirm its global convergence properties.",
    "link": "http://arxiv.org/abs/2212.10439",
    "context": "Title: Policy Gradient in Robust MDPs with Global Convergence Guarantee. (arXiv:2212.10439v2 [cs.LG] UPDATED)\nAbstract: Robust Markov decision processes (RMDPs) provide a promising framework for computing reliable policies in the face of model errors. Many successful reinforcement learning algorithms build on variations of policy-gradient methods, but adapting these methods to RMDPs has been challenging. As a result, the applicability of RMDPs to large, practical domains remains limited. This paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first generic policy gradient method for RMDPs. In contrast with prior robust policy gradient algorithms, DRPG monotonically reduces approximation errors to guarantee convergence to a globally optimal policy in tabular RMDPs. We introduce a novel parametric transition kernel and solve the inner loop robust policy via a gradient-based method. Finally, our numerical results demonstrate the utility of our new algorithm and confirm its global convergence properties.",
    "path": "papers/22/12/2212.10439.json",
    "total_tokens": 923,
    "translated_title": "具有全局收敛保证的鲁棒MDP中的策略梯度",
    "translated_abstract": "鲁棒马尔可夫决策过程（RMDP）提供了一个计算可靠策略以应对模型错误的有前途的框架。许多成功的强化学习算法基于各种变化的策略梯度方法，但将这些方法适用于RMDP是具有挑战性的。因此，RMDP对于大型实际领域的适用性仍然有限。本文提出了一种新的双循环鲁棒策略梯度（DRPG），这是首个通用的RMDP策略梯度方法。与之前的鲁棒策略梯度算法相比，DRPG单调地减少近似误差，以保证在表格RMDP中收敛到全局最优策略。我们介绍了一种新颖的参数转移核，通过基于梯度的方法解决内部循环鲁棒策略。最后，我们的数值实验证明了我们新算法的效用，并确认了它的全局收敛特性。",
    "tldr": "本文提出了一个新的双循环鲁棒策略梯度（DRPG）算法，是首个通用的RMDP策略梯度方法，通过单调地减少近似误差来保证在表格RMDP中全局最优策略的收敛性。",
    "en_tdlr": "This paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first generic policy gradient method for RMDPs, which guarantees convergence to a globally optimal policy in tabular RMDPs by monotonically reducing approximation errors."
}