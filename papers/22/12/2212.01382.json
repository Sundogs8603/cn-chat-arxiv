{
    "title": "Welfare and Fairness in Multi-objective Reinforcement Learning. (arXiv:2212.01382v4 [cs.GT] UPDATED)",
    "abstract": "We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimental",
    "link": "http://arxiv.org/abs/2212.01382",
    "context": "Title: Welfare and Fairness in Multi-objective Reinforcement Learning. (arXiv:2212.01382v4 [cs.GT] UPDATED)\nAbstract: We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimental",
    "path": "papers/22/12/2212.01382.json",
    "total_tokens": 885,
    "translated_title": "多目标强化学习中的福利和公平性",
    "translated_abstract": "我们研究了公平多目标强化学习，其中一个代理必须学习一个能够在多个维度的向量值奖励上同时获得高回报的策略。受公平资源分配文献的启发，我们将其建模为期望福利最大化问题，针对向量的长期累积奖励的非线性公平福利函数。其中一个经典的例子是纳什社会福利函数，或者几何平均数，其对数变换也被称为比例公平目标。我们表明，即使在表格化的情况下，对期望纳什社会福利进行近似最优化也是计算上难以处理的。尽管如此，我们提供了一种创新的Q-learning改进方法，结合非线性标量化学习更新和非稳态动作选择，以学习有效的优化非线性福利函数的策略。我们证明了我们的算法是可收敛的，并进行了实验证明。",
    "tldr": "本论文研究了多目标强化学习中的福利和公平性问题，提出了一种基于非线性福利函数的Q-learning算法，通过非线性标量化学习更新和非稳态动作选择来优化策略。算法被证明是可收敛的。"
}