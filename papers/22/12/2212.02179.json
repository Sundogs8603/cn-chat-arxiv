{
    "title": "Physics-Informed Model-Based Reinforcement Learning. (arXiv:2212.02179v4 [cs.LG] UPDATED)",
    "abstract": "We apply reinforcement learning (RL) to robotics tasks. One of the drawbacks of traditional RL algorithms has been their poor sample efficiency. One approach to improve the sample efficiency is model-based RL. In our model-based RL algorithm, we learn a model of the environment, essentially its transition dynamics and reward function, use it to generate imaginary trajectories and backpropagate through them to update the policy, exploiting the differentiability of the model. Intuitively, learning more accurate models should lead to better model-based RL performance. Recently, there has been growing interest in developing better deep neural network based dynamics models for physical systems, by utilizing the structure of the underlying physics. We focus on robotic systems undergoing rigid body motion without contacts. We compare two versions of our model-based RL algorithm, one which uses a standard deep neural network based dynamics model and the other which uses a much more accurate, p",
    "link": "http://arxiv.org/abs/2212.02179",
    "context": "Title: Physics-Informed Model-Based Reinforcement Learning. (arXiv:2212.02179v4 [cs.LG] UPDATED)\nAbstract: We apply reinforcement learning (RL) to robotics tasks. One of the drawbacks of traditional RL algorithms has been their poor sample efficiency. One approach to improve the sample efficiency is model-based RL. In our model-based RL algorithm, we learn a model of the environment, essentially its transition dynamics and reward function, use it to generate imaginary trajectories and backpropagate through them to update the policy, exploiting the differentiability of the model. Intuitively, learning more accurate models should lead to better model-based RL performance. Recently, there has been growing interest in developing better deep neural network based dynamics models for physical systems, by utilizing the structure of the underlying physics. We focus on robotic systems undergoing rigid body motion without contacts. We compare two versions of our model-based RL algorithm, one which uses a standard deep neural network based dynamics model and the other which uses a much more accurate, p",
    "path": "papers/22/12/2212.02179.json",
    "total_tokens": 904,
    "translated_title": "物理学信解的基于模型的强化学习",
    "translated_abstract": "本文将强化学习算法（RL）应用于机器人任务。传统RL算法的一个缺点是样本效率较低。提高样本效率的一种方法是基于模型的RL。在我们的基于模型的RL算法中，我们学习环境的模型，主要是其转换动态和奖励函数，利用其生成虚拟轨迹并通过其反向传播来更新策略，利用模型的可微性。直观地说，学习更准确的模型应该会导致更好的基于模型的RL性能。近年来，人们越来越关注利用底层物理结构开发更好的基于深度神经网络的物理系统动态模型。我们专注于没有联系的刚体运动的机器人系统。我们比较了我们基于模型的RL算法的两个版本，一个使用标准的基于深度神经网络的动态模型，另一个使用更准确的p型混合自动编码器动态模型。",
    "tldr": "本文介绍了一种应用于机器人任务的基于模型的强化学习算法，利用环境模型生成轨迹来更新策略，其中使用深度神经网络动态模型的性能比使用更准确的p型混合自动编码器动态模型的性能要差。"
}