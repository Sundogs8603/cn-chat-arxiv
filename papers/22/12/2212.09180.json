{
    "title": "Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems. (arXiv:2212.09180v2 [cs.CL] UPDATED)",
    "abstract": "There has been great recent advancement in human-computer chat. However, proper evaluation currently requires human judgements that produce notoriously high-variance metrics due to their inherent subjectivity. Furthermore, there is little standardization in the methods and labels used for evaluation, with an overall lack of work to compare and assess the validity of various evaluation approaches. As a consequence, existing evaluation results likely leave an incomplete picture of the strengths and weaknesses of open-domain chatbots. We aim towards a dimensional evaluation of human-computer chat that can reliably measure several distinct aspects of chat quality. To this end, we present our novel human evaluation method that quantifies the rate of several quality-related chatbot behaviors. Our results demonstrate our method to be more suitable for dimensional chat evaluation than alternative likert-style or comparative methods. We then use our validated method and existing methods to eval",
    "link": "http://arxiv.org/abs/2212.09180",
    "context": "Title: Don't Forget Your ABC's: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems. (arXiv:2212.09180v2 [cs.CL] UPDATED)\nAbstract: There has been great recent advancement in human-computer chat. However, proper evaluation currently requires human judgements that produce notoriously high-variance metrics due to their inherent subjectivity. Furthermore, there is little standardization in the methods and labels used for evaluation, with an overall lack of work to compare and assess the validity of various evaluation approaches. As a consequence, existing evaluation results likely leave an incomplete picture of the strengths and weaknesses of open-domain chatbots. We aim towards a dimensional evaluation of human-computer chat that can reliably measure several distinct aspects of chat quality. To this end, we present our novel human evaluation method that quantifies the rate of several quality-related chatbot behaviors. Our results demonstrate our method to be more suitable for dimensional chat evaluation than alternative likert-style or comparative methods. We then use our validated method and existing methods to eval",
    "path": "papers/22/12/2212.09180.json",
    "total_tokens": 1013,
    "translated_title": "别忘了你的ABC：评估聊天导向对话系统的最新进展",
    "translated_abstract": "人机聊天交互领域近来取得了巨大进展，然而，适当的评估仍需要人类主观判断，因此评测指标易出现高方差问题。此外，当前评估方法和标准缺乏规范性，缺乏用于评估有效性的工作。因此，现有的评估结果可能无法完整反映开放领域聊天机器人的优点和缺陷。我们旨在实现对人机聊天的维度化评估，可可靠地测量聊天质量的几个不同方面。为此，我们提出了一种新颖的人类评估方法，量化了几种与质量相关的机器人聊天行为。我们的结果表明，我们的方法比替代的Likert-style或比较方法更适合评估维度化聊天。然后，我们使用我们验证的方法和现有方法来评估一组最先进的开放领域聊天机器人，并全面比较了它们在几个质量维度上的性能。我们的结果突显了现有开放领域聊天机器人技术的持续优势和限制，并为聊天导向对话系统的未来发展提供了基准。",
    "tldr": "本文提出了一种可靠的维度化评估人机聊天的新方法，并评估了一组最先进的开放领域聊天机器人，为未来聊天导向对话系统的发展提供了基准。",
    "en_tdlr": "This paper proposes a reliable method for dimensionally evaluating human-computer chat and evaluates a set of state-of-the-art open-domain chatbots, serving as a benchmark for future developments in chat-oriented dialogue systems."
}