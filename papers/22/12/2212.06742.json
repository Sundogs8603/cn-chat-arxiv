{
    "title": "ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages. (arXiv:2212.06742v2 [cs.CL] UPDATED)",
    "abstract": "Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translation language modeling that relies on parallel data of many NLs and PLs. Extensive results show that ERNIE-Code outperforms previous multilingual LLMs for PL or NL across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-",
    "link": "http://arxiv.org/abs/2212.06742",
    "context": "Title: ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages. (arXiv:2212.06742v2 [cs.CL] UPDATED)\nAbstract: Software engineers working with the same programming language (PL) may speak different natural languages (NLs) and vice versa, erecting huge barriers to communication and working efficiency. Recent studies have demonstrated the effectiveness of generative pre-training in computer programs, yet they are always English-centric. In this work, we step towards bridging the gap between multilingual NLs and multilingual PLs for large language models (LLMs). We release ERNIE-Code, a unified pre-trained language model for 116 NLs and 6 PLs. We employ two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translation language modeling that relies on parallel data of many NLs and PLs. Extensive results show that ERNIE-Code outperforms previous multilingual LLMs for PL or NL across a wide range of end tasks of code intelligence, including multilingual code-to-text, text-to-code, code-to-code, and text-",
    "path": "papers/22/12/2212.06742.json",
    "total_tokens": 1010,
    "translated_title": "ERNIE-Code: 超越英语为中心的跨语言编程预训练",
    "translated_abstract": "软件工程师使用同一种编程语言可能使用不同的自然语言，这会导致沟通和工作效率的巨大障碍。最近的研究表明，在计算机程序中使用生成式预训练是有效的，然而它们总是以英语为中心。在这项工作中，我们迈出了迈向为大型语言模型（LLM）建立多语言自然语言和多语言编程语言之间桥梁的一步。我们发布了ERNIE-Code，这是一个适用于116种自然语言和6种编程语言的统一预训练语言模型。我们采用两种普遍的跨语言预训练方法：跨度损坏语言建模从单语言自然语言或编程语言中学习模式；基于桥接的翻译语言建模依靠多种自然语言和编程语言的平行数据。广泛的结果表明，ERNIE-Code在代码智能的广泛终端任务中优于以前的多语言LLMs，包括多语言代码到文本，文本到代码，代码到代码和文本到文本的转换。",
    "tldr": "ERNIE-Code是一个适用于116种自然语言和6种编程语言的统一预训练语言模型，采用了跨度损坏语言建模和基于桥接的翻译语言建模两种跨语言预训练方法，并在广泛的代码智能终端任务中优于以前的多语言LLMs。",
    "en_tdlr": "ERNIE-Code is a unified pre-trained language model for 116 natural languages and 6 programming languages that employs two methods for universal cross-lingual pre-training and outperforms previous multilingual LLMs for code intelligence across a wide range of end tasks."
}