{
    "title": "P{\\O}DA: Prompt-driven Zero-shot Domain Adaptation. (arXiv:2212.03241v2 [cs.CV] UPDATED)",
    "abstract": "Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of `Prompt-driven Zero-shot Domain Adaptation', where we adapt a model trained on a source domain using only a single general textual description of the target domain, i.e., a prompt. First, we leverage a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Second, we show that augmented features can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand. Our prompt-driven approach even outperforms one-shot unsupervised domain adaptation on some datasets, and gi",
    "link": "http://arxiv.org/abs/2212.03241",
    "total_tokens": 978,
    "translated_title": "P{\\O}DA: 基于提示的零样本领域自适应",
    "translated_abstract": "领域自适应在计算机视觉领域得到了广泛的研究，但仍需要在训练时访问目标图像，这在某些不常见的情况下可能是不可行的。本文提出了“基于提示的零样本领域自适应”任务，其中我们仅使用目标域的单个通用文本描述（即提示）来调整在源域上训练的模型。首先，我们利用预训练的对比视觉语言模型（CLIP）来优化源特征的仿射变换，将其引导到目标文本嵌入中，同时保留其内容和语义。其次，我们展示了增强的特征可以用于执行语义分割的零样本领域自适应。实验表明，我们的方法在几个数据集上显著优于基于CLIP的风格转移基线，用于下游任务。我们的基于提示的方法甚至在某些数据集上优于一次性无监督领域自适应，并且gi",
    "tldr": "本文提出了一种基于提示的零样本领域自适应方法，通过利用预训练的对比视觉语言模型（CLIP）来优化源特征的仿射变换，将其引导到目标文本嵌入中，同时保留其内容和语义。实验表明，该方法在几个数据集上显著优于基于CLIP的风格转移基线，用于下游任务。",
    "en_tldr": "This paper proposes a prompt-driven zero-shot domain adaptation method, which leverages a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Experiments demonstrate that the method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand."
}