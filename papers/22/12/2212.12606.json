{
    "title": "A Convergence Rate for Manifold Neural Networks. (arXiv:2212.12606v2 [cs.LG] UPDATED)",
    "abstract": "High-dimensional data arises in numerous applications, and the rapidly developing field of geometric deep learning seeks to develop neural network architectures to analyze such data in non-Euclidean domains, such as graphs and manifolds. Recent work by Z. Wang, L. Ruiz, and A. Ribeiro has introduced a method for constructing manifold neural networks using the spectral decomposition of the Laplace Beltrami operator. Moreover, in this work, the authors provide a numerical scheme for implementing such neural networks when the manifold is unknown and one only has access to finitely many sample points. The authors show that this scheme, which relies upon building a data-driven graph, converges to the continuum limit as the number of sample points tends to infinity. Here, we build upon this result by establishing a rate of convergence that depends on the intrinsic dimension of the manifold but is independent of the ambient dimension. We also discuss how the rate of convergence depends on the",
    "link": "http://arxiv.org/abs/2212.12606",
    "context": "Title: A Convergence Rate for Manifold Neural Networks. (arXiv:2212.12606v2 [cs.LG] UPDATED)\nAbstract: High-dimensional data arises in numerous applications, and the rapidly developing field of geometric deep learning seeks to develop neural network architectures to analyze such data in non-Euclidean domains, such as graphs and manifolds. Recent work by Z. Wang, L. Ruiz, and A. Ribeiro has introduced a method for constructing manifold neural networks using the spectral decomposition of the Laplace Beltrami operator. Moreover, in this work, the authors provide a numerical scheme for implementing such neural networks when the manifold is unknown and one only has access to finitely many sample points. The authors show that this scheme, which relies upon building a data-driven graph, converges to the continuum limit as the number of sample points tends to infinity. Here, we build upon this result by establishing a rate of convergence that depends on the intrinsic dimension of the manifold but is independent of the ambient dimension. We also discuss how the rate of convergence depends on the",
    "path": "papers/22/12/2212.12606.json",
    "total_tokens": 888,
    "translated_title": "流形神经网络的收敛速率",
    "translated_abstract": "高维数据在许多应用中产生，并且几何深度学习这一快速发展的领域致力于开发神经网络架构，以便在非欧几里德领域（如图和流形）中分析此类数据。弗丁·王、唐坤·卢以及亚历山大·里贝罗最近的工作引入了一种利用拉普拉斯-贝尔特拉米算子的谱分解构建流形神经网络的方法。此外，本文作者提供了一种数值方案，以在流形未知且仅有有限数量样本点可获得的情况下实施此类神经网络。作者证明了这种方案，依靠构建数据驱动的图，当样本点数量趋于无穷大时，收敛到连续极限。在这里，我们在此结果的基础上建立了一个收敛速率，该收敛速率取决于流形的内在维度，但与环境维度无关。我们还讨论了收敛速率如何依赖于...",
    "tldr": "该论文研究了流形神经网络的收敛速率，通过建立一个与环境维度无关但与流形内在维度相关的收敛速率，对先前的工作进行了改进。",
    "en_tdlr": "This paper investigates the convergence rate of manifold neural networks and improves upon previous work by establishing a convergence rate that is independent of the ambient dimension but dependent on the intrinsic dimension of the manifold."
}