{
    "title": "Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study",
    "abstract": "arXiv:2212.10233v2 Announce Type: replace  Abstract: Neural models that do not rely on pre-training have excelled in the keyphrase generation task with large annotated datasets. Meanwhile, new approaches have incorporated pre-trained language models (PLMs) for their data efficiency. However, there lacks a systematic study of how the two types of approaches compare and how different design choices can affect the performance of PLM-based models. To fill in this knowledge gap and facilitate a more informed use of PLMs for keyphrase extraction and keyphrase generation, we present an in-depth empirical study. Formulating keyphrase extraction as sequence labeling and keyphrase generation as sequence-to-sequence generation, we perform extensive experiments in three domains. After showing that PLMs have competitive high-resource performance and state-of-the-art low-resource performance, we investigate important design choices including in-domain PLMs, PLMs with different pre-training objective",
    "link": "https://arxiv.org/abs/2212.10233",
    "context": "Title: Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study\nAbstract: arXiv:2212.10233v2 Announce Type: replace  Abstract: Neural models that do not rely on pre-training have excelled in the keyphrase generation task with large annotated datasets. Meanwhile, new approaches have incorporated pre-trained language models (PLMs) for their data efficiency. However, there lacks a systematic study of how the two types of approaches compare and how different design choices can affect the performance of PLM-based models. To fill in this knowledge gap and facilitate a more informed use of PLMs for keyphrase extraction and keyphrase generation, we present an in-depth empirical study. Formulating keyphrase extraction as sequence labeling and keyphrase generation as sequence-to-sequence generation, we perform extensive experiments in three domains. After showing that PLMs have competitive high-resource performance and state-of-the-art low-resource performance, we investigate important design choices including in-domain PLMs, PLMs with different pre-training objective",
    "path": "papers/22/12/2212.10233.json",
    "total_tokens": 946,
    "translated_title": "面向关键词生成的预训练语言模型：一项深入的实证研究",
    "translated_abstract": "不依赖预训练的神经模型在拥有大量注释数据集的情况下在关键词生成任务中表现出色。与此同时，新方法已将预训练语言模型（PLMs）纳入其数据效率。然而，目前缺乏对比这两种方法以及不同设计选择如何影响基于PLM模型性能的系统研究。为填补这一知识空白，并促进更加明智地使用PLMs进行关键词提取和关键词生成，我们提出了一个深入的实证研究。我们将关键词提取构建为序列标注，关键词生成构建为序列到序列生成，并在三个领域进行了广泛实验。在展示PLMs具有具有竞争力的高资源性能和最先进的低资源性能后，我们研究了重要的设计选择，包括领域内PLMs，具有不同预训练目标的PLMs等。",
    "tldr": "通过深入的实证研究，本文研究了不依赖预训练的神经模型与整合预训练语言模型（PLMs）在关键词生成任务中的性能比较，揭示了PLMs具有具有竞争力的高资源性能和最先进的低资源性能，并探讨了不同设计选择对基于PLM模型性能的影响。"
}