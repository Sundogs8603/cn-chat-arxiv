{
    "title": "Data Curation Alone Can Stabilize In-context Learning. (arXiv:2212.10378v2 [cs.CL] UPDATED)",
    "abstract": "In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm (e.g., prompt retrieval or calibration). We introduce two methods to choose training subsets -- both score training examples individually, then select the highest-scoring ones. CondAcc scores a training example by its average dev-set ICL accuracy when combined with random training examples, while Datamodels learns linear regressors that estimate how the presence of each training example influences LLM outputs. Across five tasks and two LLMs, sampling from stable subsets selected by CondAcc and Datamodels improves average accuracy",
    "link": "http://arxiv.org/abs/2212.10378",
    "context": "Title: Data Curation Alone Can Stabilize In-context Learning. (arXiv:2212.10378v2 [cs.CL] UPDATED)\nAbstract: In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm (e.g., prompt retrieval or calibration). We introduce two methods to choose training subsets -- both score training examples individually, then select the highest-scoring ones. CondAcc scores a training example by its average dev-set ICL accuracy when combined with random training examples, while Datamodels learns linear regressors that estimate how the presence of each training example influences LLM outputs. Across five tasks and two LLMs, sampling from stable subsets selected by CondAcc and Datamodels improves average accuracy",
    "path": "papers/22/12/2212.10378.json",
    "total_tokens": 963,
    "translated_title": "仅通过数据整理可以稳定上下文学习",
    "translated_abstract": "上下文学习使得大型语言模型（LLM）通过一系列训练样例可以执行新任务。然而，已知ICL对训练样例的选择非常敏感：从训练集中随机抽样导致性能高度变异。在本文中，我们展示了精心整理训练数据子集可以极大地稳定ICL表现，而不需要ICL算法的其他更改（例如提示检索或校准）。我们引入了两种选择培训子集的方法——两者都单独评分培训示例，然后选择得分最高的示例。CondAcc通过将训练示例与随机训练示例组合时的平均开发集ICL准确性来评分训练示例，而DataModels学习线性回归器，估计每个训练示例的存在如何影响LLM输出。在五个任务和两个LLM的情况下，从由CondAcc和DataModels选择的稳定子集中抽样可以提高平均准确性。",
    "tldr": "精心整理训练数据子集可以极大地稳定上下文学习表现，而不需要对ICL算法进行其他更改。CondAcc通过将训练示例与随机训练示例组合时的平均开发集ICL准确性来评分训练示例，而DataModels学习线性回归器，估计每个训练示例的存在如何影响LLM输出。",
    "en_tdlr": "Carefully curating a subset of training data can greatly stabilize in-context learning performance without any other changes to the algorithm. CondAcc scores a training example by its average dev-set ICL accuracy when combined with random training examples, while DataModels learns linear regressors that estimate how the presence of each training example influences LLM outputs."
}