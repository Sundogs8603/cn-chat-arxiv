{
    "title": "NeRN -- Learning Neural Representations for Neural Networks. (arXiv:2212.13554v2 [cs.LG] UPDATED)",
    "abstract": "Neural Representations have recently been shown to effectively reconstruct a wide range of signals from 3D meshes and shapes to images and videos. We show that, when adapted correctly, neural representations can be used to directly represent the weights of a pre-trained convolutional neural network, resulting in a Neural Representation for Neural Networks (NeRN). Inspired by coordinate inputs of previous neural representation methods, we assign a coordinate to each convolutional kernel in our network based on its position in the architecture, and optimize a predictor network to map coordinates to their corresponding weights. Similarly to the spatial smoothness of visual scenes, we show that incorporating a smoothness constraint over the original network's weights aids NeRN towards a better reconstruction. In addition, since slight perturbations in pre-trained model weights can result in a considerable accuracy loss, we employ techniques from the field of knowledge distillation to stabi",
    "link": "http://arxiv.org/abs/2212.13554",
    "context": "Title: NeRN -- Learning Neural Representations for Neural Networks. (arXiv:2212.13554v2 [cs.LG] UPDATED)\nAbstract: Neural Representations have recently been shown to effectively reconstruct a wide range of signals from 3D meshes and shapes to images and videos. We show that, when adapted correctly, neural representations can be used to directly represent the weights of a pre-trained convolutional neural network, resulting in a Neural Representation for Neural Networks (NeRN). Inspired by coordinate inputs of previous neural representation methods, we assign a coordinate to each convolutional kernel in our network based on its position in the architecture, and optimize a predictor network to map coordinates to their corresponding weights. Similarly to the spatial smoothness of visual scenes, we show that incorporating a smoothness constraint over the original network's weights aids NeRN towards a better reconstruction. In addition, since slight perturbations in pre-trained model weights can result in a considerable accuracy loss, we employ techniques from the field of knowledge distillation to stabi",
    "path": "papers/22/12/2212.13554.json",
    "total_tokens": 947,
    "translated_title": "NeRN——学习神经网络的神经表示法",
    "translated_abstract": "最近，神经表示法已被证明可以有效地重建广泛的信号，从3D网格和形状到图像和视频。我们展示了当神经表示法适当地被调整时，可以用于直接表示预训练卷积神经网络的权重，从而得到一个神经网络的神经表示法（NeRN）。受以前神经表示法方法的坐标输入的启发，我们为网络中的每个卷积核分配一个坐标，基于其在体系结构中的位置，并优化一个预测器网络来将坐标映射到相应的权重。与视觉场景的空间平滑性类似，我们显示在原始网络的权重上加入平滑约束有助于NeRN实现更好的重建。此外，由于预训练模型权重的轻微扰动可能导致相当大的精度损失，我们采用知识蒸馏领域的技术来稳定模型。",
    "tldr": "该论文提出了一种新的神经网络表示方法NeRN，可以直接用于表示预训练卷积神经网络的权重。该方法通过为网络中的每个卷积核分配一个坐标，并将其映射到相应的权重来实现重建。此外，该论文使用知识蒸馏技术稳定模型，并加入平滑约束有助于实现更好的重建效果。",
    "en_tdlr": "This paper proposes a new neural representation method, NeRN, which can be directly used to represent the weights of a pre-trained convolutional neural network. By assigning a coordinate to each convolutional kernel and mapping it to the corresponding weight, this method achieves reconstruction. In addition, knowledge distillation is used to stabilize the model, and a smoothness constraint is added to improve the reconstruction effect."
}