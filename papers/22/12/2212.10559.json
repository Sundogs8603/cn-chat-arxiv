{
    "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. (arXiv:2212.10559v3 [cs.CL] UPDATED)",
    "abstract": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspect",
    "link": "http://arxiv.org/abs/2212.10559",
    "context": "Title: Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. (arXiv:2212.10559v3 [cs.CL] UPDATED)\nAbstract: Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspect",
    "path": "papers/22/12/2212.10559.json",
    "total_tokens": 868,
    "translated_title": "为什么GPT能够学习上下文？语言模型隐式地作为元优化器执行梯度下降。",
    "translated_abstract": "大型预训练语言模型展示了惊人的上下文学习能力。它们可以通过几个演示的输入-标签对，在没有参数更新的情况下预测未见过的输入的标签。尽管在性能方面取得了巨大的成功，但其工作机制仍然是一个未解决的问题。本文将语言模型解释为元优化器，并将上下文学习理解为隐式微调。理论上，我们发现Transformer注意力有梯度下降的双重形式。在此基础上，我们将ICL如下理解：GPT首先根据演示示例产生元梯度，然后将这些元梯度应用于原始GPT，构建ICL模型。我们全面比较了实际任务中上下文学习和显式微调的行为，提供了支持我们理解的经验证据。实验结果表明，上下文学习在多个方面表现出与显式微调类似的行为。",
    "tldr": "本文将语言模型解释为元优化器，并将上下文学习理解为隐式微调，通过实验证据支持了这一理解。",
    "en_tdlr": "This paper explains language models as meta-optimizers and interprets in-context learning as implicit fine-tuning, and provides empirical evidence to support this understanding."
}