{
    "title": "Convolution-enhanced Evolving Attention Networks. (arXiv:2212.08330v2 [cs.LG] UPDATED)",
    "abstract": "Attention-based neural networks, such as Transformers, have become ubiquitous in numerous applications, including computer vision, natural language processing, and time-series analysis. In all kinds of attention networks, the attention maps are crucial as they encode semantic dependencies between input tokens. However, most existing attention networks perform modeling or reasoning based on representations , wherein the attention maps of different layers are learned separately without explicit interactions. In this paper, we propose a novel and generic evolving attention mechanism, which directly models the evolution of inter-token relationships through a chain of residual convolutional modules. The major motivations are twofold. On the one hand, the attention maps in different layers share transferable knowledge, thus adding a residual connection can facilitate the information flow of inter-token relationships across layers. On the other hand, there is naturally an evolutionary trend a",
    "link": "http://arxiv.org/abs/2212.08330",
    "context": "Title: Convolution-enhanced Evolving Attention Networks. (arXiv:2212.08330v2 [cs.LG] UPDATED)\nAbstract: Attention-based neural networks, such as Transformers, have become ubiquitous in numerous applications, including computer vision, natural language processing, and time-series analysis. In all kinds of attention networks, the attention maps are crucial as they encode semantic dependencies between input tokens. However, most existing attention networks perform modeling or reasoning based on representations , wherein the attention maps of different layers are learned separately without explicit interactions. In this paper, we propose a novel and generic evolving attention mechanism, which directly models the evolution of inter-token relationships through a chain of residual convolutional modules. The major motivations are twofold. On the one hand, the attention maps in different layers share transferable knowledge, thus adding a residual connection can facilitate the information flow of inter-token relationships across layers. On the other hand, there is naturally an evolutionary trend a",
    "path": "papers/22/12/2212.08330.json",
    "total_tokens": 872,
    "translated_title": "卷积增强的不断进化的注意力网络",
    "translated_abstract": "基于注意力机制的神经网络，比如Transformers，在许多应用中变得无处不在，包括计算机视觉、自然语言处理和时间序列分析。在所有种类的注意力网络中，注意力图是至关重要的，因为它们编码了输入标记之间的语义依赖关系。然而，大多数现有的注意力网络是基于表示进行建模或推理的，不同层次的注意力图在学习时是分别学习而不是进行显式交互。本文提出了一种新颖且通用的进化注意力机制，该机制通过一系列残差卷积模块直接模拟标记间关系的演变。其主要动机有两方面。一方面，不同层次的注意力图分享可转移知识，因此添加残差连接可以促进标记之间关系在不同层次之间的信息流动。另一方面，自然存在着演化趋势。",
    "tldr": "本文提出了一种新颖且通用的卷积增强进化注意力机制，通过一系列残差卷积模块直接模拟标记间关系的演变，在不同层次之间促进信息流动。",
    "en_tdlr": "This paper proposes a novel and generic convolution-enhanced evolving attention mechanism, which directly models the evolution of inter-token relationships through a chain of residual convolutional modules and facilitates the information flow across layers."
}