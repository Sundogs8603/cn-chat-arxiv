{
    "title": "A Natural Bias for Language Generation Models. (arXiv:2212.09686v2 [cs.CL] UPDATED)",
    "abstract": "After just a few hundred training updates, a standard probabilistic model for language generation has likely not yet learnt many semantic or syntactic rules of natural language, making it difficult to estimate the probability distribution over next tokens. Yet around this point, these models have identified a simple, loss-minimising behaviour: to output the unigram distribution of the target training corpus. The use of such a heuristic raises the question: Can we initialise our models with this behaviour and save precious compute resources and model capacity? Here we show that we can effectively endow standard neural language generation models with a separate module that reflects unigram frequency statistics as prior knowledge, simply by initialising the bias term in a model's final linear layer with the log-unigram distribution. We use neural machine translation as a test bed for this simple technique and observe that it: (i) improves learning efficiency; (ii) achieves better overall ",
    "link": "http://arxiv.org/abs/2212.09686",
    "context": "Title: A Natural Bias for Language Generation Models. (arXiv:2212.09686v2 [cs.CL] UPDATED)\nAbstract: After just a few hundred training updates, a standard probabilistic model for language generation has likely not yet learnt many semantic or syntactic rules of natural language, making it difficult to estimate the probability distribution over next tokens. Yet around this point, these models have identified a simple, loss-minimising behaviour: to output the unigram distribution of the target training corpus. The use of such a heuristic raises the question: Can we initialise our models with this behaviour and save precious compute resources and model capacity? Here we show that we can effectively endow standard neural language generation models with a separate module that reflects unigram frequency statistics as prior knowledge, simply by initialising the bias term in a model's final linear layer with the log-unigram distribution. We use neural machine translation as a test bed for this simple technique and observe that it: (i) improves learning efficiency; (ii) achieves better overall ",
    "path": "papers/22/12/2212.09686.json",
    "total_tokens": 947,
    "translated_title": "语言生成模型的自然倾向",
    "translated_abstract": "经过几百个训练循环之后，一个标准的语言生成概率模型可能还没有学会自然语言的许多语义或句法规则，这使得难以估计下一个令牌的概率分布。但在这一点左右，这些模型已经确定了一种简单的最小化损失的行为：输出目标训练语料库的单元分布。使用这种启发式方法引出了一个问题：我们可以初始化我们的模型，使用这种行为并节省宝贵的计算资源和模型容量吗？在这里，我们展示了一个方法，可以有效地为标准神经语言生成模型赋予反映单元频率统计作为先验知识的单独模块，只需通过将模型的最终线性层的偏差项初始化为log-unigram分布来实现。我们以神经机器翻译为测试基础，观察到：（i）提高了学习效率；（ii）实现了更好的整体翻译质量；（iii）能够更准确地翻译不经常出现的单词和短语。",
    "tldr": "通过初始化偏差项为log-unigram分布，可为神经语言生成模型赋予单元频率统计的先验知识，从而提高学习效率和翻译质量，准确翻译不经常出现的单词和短语。",
    "en_tdlr": "By initializing the bias term with the log-unigram distribution, a separate module reflecting unigram frequency statistics can be effectively endowed for standard neural language generation models, which improves learning efficiency and translation quality, leading to more accurate translations of infrequent words and phrases."
}