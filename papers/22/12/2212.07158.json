{
    "title": "Establishing a stronger baseline for lightweight contrastive models. (arXiv:2212.07158v2 [cs.CV] UPDATED)",
    "abstract": "Recent research has reported a performance degradation in self-supervised contrastive learning for specially designed efficient networks, such as MobileNet and EfficientNet. A common practice to address this problem is to introduce a pretrained contrastive teacher model and train the lightweight networks with distillation signals generated by the teacher. However, it is time and resource consuming to pretrain a teacher model when it is not available. In this work, we aim to establish a stronger baseline for lightweight contrastive models without using a pretrained teacher model. Specifically, we show that the optimal recipe for efficient models is different from that of larger models, and using the same training settings as ResNet50, as previous research does, is inappropriate. Additionally, we observe a common issu e in contrastive learning where either the positive or negative views can be noisy, and propose a smoothed version of InfoNCE loss to alleviate this problem. As a result, w",
    "link": "http://arxiv.org/abs/2212.07158",
    "context": "Title: Establishing a stronger baseline for lightweight contrastive models. (arXiv:2212.07158v2 [cs.CV] UPDATED)\nAbstract: Recent research has reported a performance degradation in self-supervised contrastive learning for specially designed efficient networks, such as MobileNet and EfficientNet. A common practice to address this problem is to introduce a pretrained contrastive teacher model and train the lightweight networks with distillation signals generated by the teacher. However, it is time and resource consuming to pretrain a teacher model when it is not available. In this work, we aim to establish a stronger baseline for lightweight contrastive models without using a pretrained teacher model. Specifically, we show that the optimal recipe for efficient models is different from that of larger models, and using the same training settings as ResNet50, as previous research does, is inappropriate. Additionally, we observe a common issu e in contrastive learning where either the positive or negative views can be noisy, and propose a smoothed version of InfoNCE loss to alleviate this problem. As a result, w",
    "path": "papers/22/12/2212.07158.json",
    "total_tokens": 941,
    "translated_title": "为轻量级对比模型建立更强的基准线",
    "translated_abstract": "最近的研究报告称，在诸如MobileNet和EfficientNet等特定设计的高效网络中，自监督对比学习的性能出现下降。解决这个问题的常见做法是引入预训练的对比教师模型，并使用教师生成的蒸馏信号来训练轻量级网络。然而，当教师模型不可用时，预训练教师模型是一项耗时和资源消耗大的工作。在这项工作中，我们的目标是在不使用预训练教师模型的情况下为轻量级对比模型建立更强的基准线。具体来说，我们展示了高效模型的最佳训练配置与较大模型不同，使用先前研究中的与ResNet50相同的训练设置是不合适的。此外，我们观察到在对比学习中存在一种常见问题，即正或负视图中的一个可能会有噪声，并提出了一种平滑的InfoNCE损失函数来减轻这个问题。结果，我们的方法在轻量级对比模型上取得了更好的性能。",
    "tldr": "本论文旨在为轻量级对比模型建立更强的基准线，解决了在自监督对比学习中使用效率型网络的性能下降问题。通过优化训练配置和引入平滑的损失函数，实现了更好的性能。",
    "en_tdlr": "This paper aims to establish a stronger baseline for lightweight contrastive models, addressing the performance degradation in self-supervised contrastive learning for efficient networks. By optimizing the training settings and introducing a smoothed loss function, the authors achieve improved performance."
}