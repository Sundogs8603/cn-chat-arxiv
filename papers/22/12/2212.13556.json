{
    "title": "Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization. (arXiv:2212.13556v3 [cs.LG] UPDATED)",
    "abstract": "To date, no \"information-theoretic\" frameworks for reasoning about generalization error have been shown to establish minimax rates for gradient descent in the setting of stochastic convex optimization. In this work, we consider the prospect of establishing such rates via several existing information-theoretic frameworks: input-output mutual information bounds, conditional mutual information bounds and variants, PAC-Bayes bounds, and recent conditional variants thereof. We prove that none of these bounds are able to establish minimax rates. We then consider a common tactic employed in studying gradient methods, whereby the final iterate is corrupted by Gaussian noise, producing a noisy \"surrogate\" algorithm. We prove that minimax rates cannot be established via the analysis of such surrogates. Our results suggest that new ideas are required to analyze gradient descent using information-theoretic techniques.",
    "link": "http://arxiv.org/abs/2212.13556",
    "context": "Title: Limitations of Information-Theoretic Generalization Bounds for Gradient Descent Methods in Stochastic Convex Optimization. (arXiv:2212.13556v3 [cs.LG] UPDATED)\nAbstract: To date, no \"information-theoretic\" frameworks for reasoning about generalization error have been shown to establish minimax rates for gradient descent in the setting of stochastic convex optimization. In this work, we consider the prospect of establishing such rates via several existing information-theoretic frameworks: input-output mutual information bounds, conditional mutual information bounds and variants, PAC-Bayes bounds, and recent conditional variants thereof. We prove that none of these bounds are able to establish minimax rates. We then consider a common tactic employed in studying gradient methods, whereby the final iterate is corrupted by Gaussian noise, producing a noisy \"surrogate\" algorithm. We prove that minimax rates cannot be established via the analysis of such surrogates. Our results suggest that new ideas are required to analyze gradient descent using information-theoretic techniques.",
    "path": "papers/22/12/2212.13556.json",
    "total_tokens": 937,
    "translated_title": "《随机凸优化中梯度下降方法的信息论泛化界限的局限性》",
    "translated_abstract": "至今为止，在随机凸优化设置中，没有证明“信息论”框架能够建立梯度下降的最小最大速率以推断泛化误差。在本研究中，我们考虑通过几种现有的信息论框架来建立这样的速率：输入-输出互信息界限、条件互信息界限及其变体、PAC-Bayes界限和最近的条件变体。我们证明了这些界限均无法建立最小最大速率。然后，我们考虑了在研究梯度方法中常用的一种策略，即通过高斯噪声破坏最终的迭代，从而产生噪声的“代理”算法。我们证明无法通过对这些代理算法的分析建立最小最大速率。我们的结果表明，需要新的思路来分析使用信息论技术的梯度下降方法。",
    "tldr": "本研究通过考虑多种信息论框架，证明了在随机凸优化领域中，没有一个\"信息论\"框架能够建立梯度下降的最小最大速率。同时，通过分析一种常见的策略，我们证明了高斯噪声破坏迭代的方法也无法建立最小最大速率。这些结果表明，需要新的思路来分析使用信息论技术的梯度下降方法。",
    "en_tdlr": "This paper demonstrates the limitations of existing information-theoretic frameworks in establishing minimax rates for gradient descent in stochastic convex optimization. The analysis of noisy surrogate algorithms also fails to establish these rates. New ideas are needed in analyzing gradient descent using information-theoretic techniques."
}