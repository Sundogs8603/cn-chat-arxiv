{
    "title": "Scalable and Sample Efficient Distributed Policy Gradient Algorithms in Multi-Agent Networked Systems. (arXiv:2212.06357v2 [cs.MA] UPDATED)",
    "abstract": "This paper studies a class of multi-agent reinforcement learning (MARL) problems where the reward that an agent receives depends on the states of other agents, but the next state only depends on the agent's own current state and action. We name it REC-MARL standing for REward-Coupled Multi-Agent Reinforcement Learning. REC-MARL has a range of important applications such as real-time access control and distributed power control in wireless networks. This paper presents a distributed policy gradient algorithm for REC-MARL. The proposed algorithm is distributed in two aspects: (i) the learned policy is a distributed policy that maps a local state of an agent to its local action and (ii) the learning/training is distributed, during which each agent updates its policy based on its own and neighbors' information. The learned algorithm achieves a stationary policy and its iterative complexity bounds depend on the dimension of local states and actions. The experimental results of our algorithm",
    "link": "http://arxiv.org/abs/2212.06357",
    "context": "Title: Scalable and Sample Efficient Distributed Policy Gradient Algorithms in Multi-Agent Networked Systems. (arXiv:2212.06357v2 [cs.MA] UPDATED)\nAbstract: This paper studies a class of multi-agent reinforcement learning (MARL) problems where the reward that an agent receives depends on the states of other agents, but the next state only depends on the agent's own current state and action. We name it REC-MARL standing for REward-Coupled Multi-Agent Reinforcement Learning. REC-MARL has a range of important applications such as real-time access control and distributed power control in wireless networks. This paper presents a distributed policy gradient algorithm for REC-MARL. The proposed algorithm is distributed in two aspects: (i) the learned policy is a distributed policy that maps a local state of an agent to its local action and (ii) the learning/training is distributed, during which each agent updates its policy based on its own and neighbors' information. The learned algorithm achieves a stationary policy and its iterative complexity bounds depend on the dimension of local states and actions. The experimental results of our algorithm",
    "path": "papers/22/12/2212.06357.json",
    "total_tokens": 906,
    "translated_title": "多智能体网络系统中的可扩展和高效分布式政策梯度算法",
    "translated_abstract": "本文研究了一类多智能体强化学习（MARL）问题，其中代理接收到的奖励取决于其他代理的状态，但下一个状态仅取决于代理自己的当前状态和动作。我们称其为REC-MARL，代表奖励耦合多智能体强化学习。REC-MARL具有一系列重要应用，如无线网络中的实时访问控制和分布式功率控制。本文提出了一种针对REC-MARL的分布式政策梯度算法。该算法在两个方面上是分布式的：（i）学习的策略是分布式策略，将代理的本地状态映射到其本地动作，（ii）训练是分布式的，每个代理基于自己和邻居的信息更新其策略。所学算法实现了一个稳定策略，其迭代复杂度边界取决于本地状态和动作的维数。实验结果表明了我们算法的有效性。",
    "tldr": "本文介绍了一种分布式政策梯度算法，用于解决具有奖励耦合的多智能体强化学习问题，具有广泛的应用价值。",
    "en_tdlr": "This paper presents a distributed policy gradient algorithm for solving reward-coupled multi-agent reinforcement learning problems, which has a range of important applications and achieves a stationary policy with iterative complexity bounds depending on the dimension of local states and actions."
}