{
    "title": "Evaluating Human-Language Model Interaction. (arXiv:2212.09746v3 [cs.CL] UPDATED)",
    "abstract": "Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurass",
    "link": "http://arxiv.org/abs/2212.09746",
    "context": "Title: Evaluating Human-Language Model Interaction. (arXiv:2212.09746v3 [cs.CL] UPDATED)\nAbstract: Many real-world applications of language models (LMs), such as writing assistance and code autocomplete, involve human-LM interaction. However, most benchmarks are non-interactive in that a model produces output without human involvement. To evaluate human-LM interaction, we develop a new framework, Human-AI Language-based Interaction Evaluation (HALIE), that defines the components of interactive systems and dimensions to consider when designing evaluation metrics. Compared to standard, non-interactive evaluation, HALIE captures (i) the interactive process, not only the final output; (ii) the first-person subjective experience, not just a third-party assessment; and (iii) notions of preference beyond quality (e.g., enjoyment and ownership). We then design five tasks to cover different forms of interaction: social dialogue, question answering, crossword puzzles, summarization, and metaphor generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3 and AI21 Labs' Jurass",
    "path": "papers/22/12/2212.09746.json",
    "total_tokens": 862,
    "translated_title": "评估人机语言模型交互",
    "translated_abstract": "许多语言模型（LM）的实际应用，例如写作辅助和代码自动完成，涉及到人机交互。然而，大多数基准测试都是非交互式的，模型在没有人类参与的情况下产生输出。为了评估人机交互，我们开发了一个新的框架，人机语言交互评估（HALIE），该框架定义了交互式系统的组成部分和设计评估指标时要考虑的维度。与标准的非交互式评估相比，HALIE捕捉到了（i）交互过程，而不仅仅是最终输出；（ii）第一人称主观体验，而不仅仅是第三方评估；（iii）除了质量之外的偏好概念（例如享受和所有权）。然后，我们设计了五个任务，涵盖不同形式的交互：社交对话、问答、填字游戏、摘要和隐喻生成。使用四个最先进的LM（OpenAI的GPT-3的三个变体和AI21 Labs的Jurass）",
    "tldr": "为了评估人机交互，研究人员开发了一个框架HALIE，该框架捕捉了交互过程、主观体验和偏好概念，并设计了五个任务来涵盖不同形式的交互。"
}