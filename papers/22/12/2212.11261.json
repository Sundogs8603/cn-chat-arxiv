{
    "title": "Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias. (arXiv:2212.11261v2 [cs.CY] UPDATED)",
    "abstract": "Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. We replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in AI. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d >0.80) and sadness (d >0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP ge",
    "link": "http://arxiv.org/abs/2212.11261",
    "context": "Title: Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias. (arXiv:2212.11261v2 [cs.CY] UPDATED)\nAbstract: Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. We replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in AI. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d >0.80) and sadness (d >0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP ge",
    "path": "papers/22/12/2212.11261.json",
    "total_tokens": 978,
    "translated_title": "使用网页抓取的多模态数据预训练的对比语言-视觉AI模型存在性物化偏见",
    "translated_abstract": "本文评估了使用对比语言图像预训练（CLIP）目标进行网页抓取的多模态数据的九种语言-视觉AI模型，以寻找心理学家研究的偏见的证据：女孩和女性的性物化现象。我们复制了三个心理实验，并显示这种偏见在AI中依然存在。第一个实验使用Sexual OBjectification and EMotion Database中的标准女性图像，并发现情感状态的识别是由主体是否全身或部分穿着进行介导的。嵌入关联测试返回了愤怒(d>0.80)和悲伤(d>0.50)的显着效应大小，将完全穿着的主体的图像与情感相关联。GRAD-CAM显著性图突出显示，CLIP生成的模型存在性物化偏见，即使使用最先进的对比多模态预训练和网页抓取的数据也是如此。",
    "tldr": "本文使用对比语言图像预训练的多模态AI模型，使用网页抓取的数据训练，发现这些模型存在性物化偏见，即人的情感状态与身体的呈现相关，表现出对女性的性别偏见。"
}