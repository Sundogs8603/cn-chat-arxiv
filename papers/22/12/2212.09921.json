{
    "title": "Input Normalized Stochastic Gradient Descent Training of Deep Neural Networks. (arXiv:2212.09921v2 [cs.LG] UPDATED)",
    "abstract": "In this paper, we propose a novel optimization algorithm for training machine learning models called Input Normalized Stochastic Gradient Descent (INSGD), inspired by the Normalized Least Mean Squares (NLMS) algorithm used in adaptive filtering. When training complex models on large datasets, the choice of optimizer parameters, particularly the learning rate, is crucial to avoid divergence. Our algorithm updates the network weights using stochastic gradient descent with $\\ell_1$ and $\\ell_2$-based normalizations applied to the learning rate, similar to NLMS. However, unlike existing normalization methods, we exclude the error term from the normalization process and instead normalize the update term using the input vector to the neuron. Our experiments demonstrate that our optimization algorithm achieves higher accuracy levels compared to different initialization settings. We evaluate the efficiency of our training algorithm on benchmark datasets using ResNet-18, WResNet-20, ResNet-50, ",
    "link": "http://arxiv.org/abs/2212.09921",
    "context": "Title: Input Normalized Stochastic Gradient Descent Training of Deep Neural Networks. (arXiv:2212.09921v2 [cs.LG] UPDATED)\nAbstract: In this paper, we propose a novel optimization algorithm for training machine learning models called Input Normalized Stochastic Gradient Descent (INSGD), inspired by the Normalized Least Mean Squares (NLMS) algorithm used in adaptive filtering. When training complex models on large datasets, the choice of optimizer parameters, particularly the learning rate, is crucial to avoid divergence. Our algorithm updates the network weights using stochastic gradient descent with $\\ell_1$ and $\\ell_2$-based normalizations applied to the learning rate, similar to NLMS. However, unlike existing normalization methods, we exclude the error term from the normalization process and instead normalize the update term using the input vector to the neuron. Our experiments demonstrate that our optimization algorithm achieves higher accuracy levels compared to different initialization settings. We evaluate the efficiency of our training algorithm on benchmark datasets using ResNet-18, WResNet-20, ResNet-50, ",
    "path": "papers/22/12/2212.09921.json",
    "total_tokens": 925,
    "translated_title": "输入归一化随机梯度下降训练深度神经网络",
    "translated_abstract": "本文提出了一种用于训练机器学习模型的新型优化算法，称为输入归一化随机梯度下降（INSGD），受到自适应滤波中使用的归一化最小均方（NLMS）算法的启发。当在大型数据集上训练复杂模型时，优化器参数的选择，尤其是学习率，对于避免发散是至关重要的。我们的算法使用随机梯度下降更新网络权重，其中学习率使用基于$\\ell_1$和$\\ell_2$的归一化，类似于NLMS。然而，与现有的归一化方法不同的是，我们在归一化过程中排除了误差项，而是使用输入向量对更新项进行归一化。我们的实验证明，相比于不同的初始化设置，我们的优化算法实现了更高的准确性水平。我们使用ResNet-18、WResNet-20和ResNet-50等基准数据集评估了我们的训练算法的效率。",
    "tldr": "本文提出了一种名为输入归一化随机梯度下降（INSGD）的优化算法，通过在学习率上应用归一化来更新网络权重，以实现在大型数据集上训练复杂模型时避免发散并提高准确性水平。"
}