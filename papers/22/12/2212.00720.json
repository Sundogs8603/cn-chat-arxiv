{
    "title": "A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks",
    "abstract": "Predictive coding networks are neuroscience-inspired models with roots in both Bayesian statistics and neuroscience. Training such models, however, is quite inefficient and unstable. In this work, we show how by simply changing the temporal scheduling of the update rule for the synaptic weights leads to an algorithm that is much more efficient and stable than the original one, and has theoretical guarantees in terms of convergence. The proposed algorithm, that we call incremental predictive coding (iPC) is also more biologically plausible than the original one, as it it fully automatic. In an extensive set of experiments, we show that iPC constantly performs better than the original formulation on a large number of benchmarks for image classification, as well as for the training of both conditional and masked language models, in terms of test accuracy, efficiency, and convergence with respect to a large set of hyperparameters.",
    "link": "https://arxiv.org/abs/2212.00720",
    "context": "Title: A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks\nAbstract: Predictive coding networks are neuroscience-inspired models with roots in both Bayesian statistics and neuroscience. Training such models, however, is quite inefficient and unstable. In this work, we show how by simply changing the temporal scheduling of the update rule for the synaptic weights leads to an algorithm that is much more efficient and stable than the original one, and has theoretical guarantees in terms of convergence. The proposed algorithm, that we call incremental predictive coding (iPC) is also more biologically plausible than the original one, as it it fully automatic. In an extensive set of experiments, we show that iPC constantly performs better than the original formulation on a large number of benchmarks for image classification, as well as for the training of both conditional and masked language models, in terms of test accuracy, efficiency, and convergence with respect to a large set of hyperparameters.",
    "path": "papers/22/12/2212.00720.json",
    "total_tokens": 837,
    "translated_title": "一种稳定、快速和完全自动的预测编码网络学习算法",
    "translated_abstract": "预测编码网络是一种受神经科学启发的模型，根植于贝叶斯统计学和神经科学。然而，训练这样的模型非常低效且不稳定。在本文中，我们展示了通过简单改变突触权重的更新规则的时间调度，可以得到比原始算法更高效、更稳定且具有收敛性保证的算法。我们提出的算法被称为递增预测编码(iPC)，也比原始算法更符合生物学可行性，因为它是完全自动的。通过一系列广泛的实验，我们展示了iPC在图像分类的许多基准测试以及条件语言模型和掩蔽语言模型的训练中，在测试准确性、效率和收敛性方面始终优于原始方法，并且对于大量超参数具有收敛性。",
    "tldr": "本文提出了一种稳定、快速且完全自动的预测编码网络学习算法(iPC)，通过改变突触权重的更新规则的时间调度，提高了训练效率和稳定性，并具有收敛性保证。实验证明，在图像分类和语言模型训练等多个任务上，iPC相比原始算法在测试准确性、效率和收敛性方面表现更好。"
}