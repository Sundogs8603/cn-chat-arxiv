{
    "title": "Thinking Fast and Slow in Large Language Models. (arXiv:2212.05206v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.",
    "link": "http://arxiv.org/abs/2212.05206",
    "context": "Title: Thinking Fast and Slow in Large Language Models. (arXiv:2212.05206v2 [cs.CL] UPDATED)\nAbstract: Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.",
    "path": "papers/22/12/2212.05206.json",
    "total_tokens": 906,
    "translated_title": "在大型语言模型中进行快速和慢速思考",
    "translated_abstract": "大型语言模型（LLMs）目前处于将AI系统与人类交流和日常生活结合的前沿。因此，评估它们新兴的能力非常重要。在这项研究中，我们展示了像GPT-3这样的LLMs表现出与人类直觉惊人相似的行为，以及由此带来的认知错误。然而，具有更高认知能力的LLMs，特别是ChatGPT和GPT-4，学会了避免陷入这些错误，表现出超理性的方式。在我们的实验中，我们使用认知反思测试（CRT）以及最初设计用于研究人类直觉决策的语义错觉来探索LLMs。我们的研究表明，利用心理学方法研究LLMs有助于揭示其他未知的新特征。",
    "tldr": "本研究发现，大型语言模型（LLMs）如GPT-3在行为上与人类直觉相似，但可能带有认知错误。然而，具有更高认知能力的LLMs，如ChatGPT和GPT-4，学会了避免这些错误，表现出超理性的方式。通过在心理学方法的帮助下研究LLMs，我们可以揭示出其它未知的新特征。",
    "en_tdlr": "This study finds that large language models (LLMs) like GPT-3 exhibit behaviors similar to human intuition but with cognitive errors. However, LLMs with higher cognitive capabilities, such as ChatGPT and GPT-4, have learned to avoid these errors and perform in a hyper-rational manner. By using psychological methods to study LLMs, we can uncover other unknown emergent traits."
}