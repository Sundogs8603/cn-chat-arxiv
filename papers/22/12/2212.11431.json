{
    "title": "Local Policy Improvement for Recommender Systems. (arXiv:2212.11431v2 [cs.LG] UPDATED)",
    "abstract": "Recommender systems predict what items a user will interact with next, based on their past interactions. The problem is often approached through supervised learning, but recent advancements have shifted towards policy optimization of rewards (e.g., user engagement). One challenge with the latter is policy mismatch: we are only able to train a new policy given data collected from a previously-deployed policy. The conventional way to address this problem is through importance sampling correction, but this comes with practical limitations. We suggest an alternative approach of local policy improvement without off-policy correction. Our method computes and optimizes a lower bound of expected reward of the target policy, which is easy to estimate from data and does not involve density ratios (such as those appearing in importance sampling correction). This local policy improvement paradigm is ideal for recommender systems, as previous policies are typically of decent quality and policies ar",
    "link": "http://arxiv.org/abs/2212.11431",
    "context": "Title: Local Policy Improvement for Recommender Systems. (arXiv:2212.11431v2 [cs.LG] UPDATED)\nAbstract: Recommender systems predict what items a user will interact with next, based on their past interactions. The problem is often approached through supervised learning, but recent advancements have shifted towards policy optimization of rewards (e.g., user engagement). One challenge with the latter is policy mismatch: we are only able to train a new policy given data collected from a previously-deployed policy. The conventional way to address this problem is through importance sampling correction, but this comes with practical limitations. We suggest an alternative approach of local policy improvement without off-policy correction. Our method computes and optimizes a lower bound of expected reward of the target policy, which is easy to estimate from data and does not involve density ratios (such as those appearing in importance sampling correction). This local policy improvement paradigm is ideal for recommender systems, as previous policies are typically of decent quality and policies ar",
    "path": "papers/22/12/2212.11431.json",
    "total_tokens": 871,
    "translated_title": "推荐系统的本地策略改进",
    "translated_abstract": "推荐系统基于用户过去的互动行为而预测他们可能会与哪些项目交互。解决该问题的常用方法是通过监督学习，但最近的进展转向了基于奖励（例如用户参与度）的策略优化。后者面临的挑战之一是策略不匹配：我们只能基于以前部署策略收集到的数据来训练新的策略。传统的方法是通过重要性采样校正来解决这个问题，但这种方法存在实际限制。我们建议一种不需要现场校正的本地策略改进方法。我们的方法计算和优化目标策略预期奖励的下限，这易于从数据中估计并且不涉及密度比（例如在重要性采样校正中出现的比率）。这种本地策略改进范例非常适用于推荐系统，因为以前的策略通常质量较高，策略的数量也很少。",
    "tldr": "该论文介绍了一种针对推荐系统的本地策略改进方法，不需要现场校正，易于从数据中估计，适用于以前的策略质量较高但数量较少的情况。",
    "en_tdlr": "This paper introduces a local policy improvement method for recommender systems, which does not require off-policy correction, is easy to estimate from data, and is suitable for situations where previous policies are of high quality but there are few of them."
}