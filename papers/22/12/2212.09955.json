{
    "title": "BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics. (arXiv:2212.09955v2 [cs.CL] UPDATED)",
    "abstract": "The proliferation of automatic faithfulness metrics for summarization has produced a need for benchmarks to evaluate them. While existing benchmarks measure the correlation with human judgements of faithfulness on model-generated summaries, they are insufficient for diagnosing whether metrics are: 1) consistent, i.e., indicate lower faithfulness as errors are introduced into a summary, 2) effective on human-written texts, and 3) sensitive to different error types (as summaries can contain multiple errors). To address these needs, we present a benchmark of unfaithful minimal pairs (BUMP), a dataset of 889 human-written, minimally different summary pairs, where a single error is introduced to a summary from the CNN/DailyMail dataset to produce an unfaithful summary. We find BUMP complements existing benchmarks in a number of ways: 1) the summaries in BUMP are harder to discriminate and less probable under SOTA summarization models, 2) unlike non-pair-based datasets, BUMP can be used to m",
    "link": "http://arxiv.org/abs/2212.09955",
    "context": "Title: BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of Faithfulness Metrics. (arXiv:2212.09955v2 [cs.CL] UPDATED)\nAbstract: The proliferation of automatic faithfulness metrics for summarization has produced a need for benchmarks to evaluate them. While existing benchmarks measure the correlation with human judgements of faithfulness on model-generated summaries, they are insufficient for diagnosing whether metrics are: 1) consistent, i.e., indicate lower faithfulness as errors are introduced into a summary, 2) effective on human-written texts, and 3) sensitive to different error types (as summaries can contain multiple errors). To address these needs, we present a benchmark of unfaithful minimal pairs (BUMP), a dataset of 889 human-written, minimally different summary pairs, where a single error is introduced to a summary from the CNN/DailyMail dataset to produce an unfaithful summary. We find BUMP complements existing benchmarks in a number of ways: 1) the summaries in BUMP are harder to discriminate and less probable under SOTA summarization models, 2) unlike non-pair-based datasets, BUMP can be used to m",
    "path": "papers/22/12/2212.09955.json",
    "total_tokens": 989,
    "translated_title": "BUMP：一种不忠实最小对比基准，用于评估忠实性度量的超验评估。",
    "translated_abstract": "自动忠实性度量在摘要文本中的广泛应用产生了对基准的需求。虽然现有的基准测量了模型生成的摘要与人类忠实性判断的相关性，但它们不足以诊断指标是否：1）一致，即当错误被引入到摘要中时指出较低的忠实性；2）在人工写作文本中有效；3）对不同的错误类型敏感（因为摘要可以包含多个错误）。为了解决这些需求，我们提出了一个不忠实最小对比基准（BUMP），这是一个由889个对CNN / DailyMail数据集中的摘要进行微小差异处理而得到的，其中摘要中引入了单个错误的人写的摘要对。我们发现BUMP在许多方面补充了现有的基准：1）BUMP中的摘要更难区别，并且在SOTA摘要模型下不太可能；2）与非对比基准数据集不同，BUMP可用于衡量指标是否良好地识别不忠实性。",
    "tldr": "BUMP是一个不忠实最小对比基准，由889个人写的对比CNN / DailyMail数据集中的摘要进行微小差异处理而得。BUMP可以帮助评估自动忠实性度量，其优于现有基准数据集，因为更具挑战性，且可以表征不同类型的不忠实性。",
    "en_tdlr": "BUMP is a benchmark of unfaithful minimal pairs that introduces a single error to a human-written summary from the CNN/DailyMail dataset. It helps evaluate automatic faithfulness metrics and is superior to existing benchmarks because it is more challenging and characterizes different types of unfaithfulness."
}