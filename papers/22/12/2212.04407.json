{
    "title": "Variable Decision-Frequency Option Critic. (arXiv:2212.04407v3 [cs.LG] UPDATED)",
    "abstract": "In classic reinforcement learning algorithms, agents make decisions at discrete and fixed time intervals. The duration between decisions becomes a crucial hyperparameter, as setting it too short may increase the difficulty of the problem by requiring the agent to make numerous decisions to achieve its goal, while setting it too long can result in the agent losing control over the system. However, physical systems do not necessarily require a constant control frequency, and for learning agents, it is often preferable to operate with a low frequency when possible and a high frequency when necessary. We propose a framework called Continuous-Time Continuous-Options (CTCO), where the agent chooses options as sub-policies of variable durations. These options are time-continuous and can interact with the system at any desired frequency providing a smooth change of actions. We demonstrate the effectiveness of CTCO by comparing its performance to classical RL and temporal-abstraction RL methods",
    "link": "http://arxiv.org/abs/2212.04407",
    "context": "Title: Variable Decision-Frequency Option Critic. (arXiv:2212.04407v3 [cs.LG] UPDATED)\nAbstract: In classic reinforcement learning algorithms, agents make decisions at discrete and fixed time intervals. The duration between decisions becomes a crucial hyperparameter, as setting it too short may increase the difficulty of the problem by requiring the agent to make numerous decisions to achieve its goal, while setting it too long can result in the agent losing control over the system. However, physical systems do not necessarily require a constant control frequency, and for learning agents, it is often preferable to operate with a low frequency when possible and a high frequency when necessary. We propose a framework called Continuous-Time Continuous-Options (CTCO), where the agent chooses options as sub-policies of variable durations. These options are time-continuous and can interact with the system at any desired frequency providing a smooth change of actions. We demonstrate the effectiveness of CTCO by comparing its performance to classical RL and temporal-abstraction RL methods",
    "path": "papers/22/12/2212.04407.json",
    "total_tokens": 896,
    "translated_title": "可变化决策频率的选项评论者",
    "translated_abstract": "在传统的强化学习算法中，代理在离散和固定的时间间隔内做出决策。决策之间的持续时间变成了一个关键的超参数，因为设置得太短可能会增加问题的难度，需要代理进行多次决策才能实现其目标，而设置得太长会导致代理失去对系统的控制。然而，物理系统不一定需要恒定的控制频率，对于学习代理来说，一般情况下，当需要时以高频率运行，而在可能时以低频率运行更好。我们提出了一个名为连续时间连续选项 (CTCO) 的框架，其中代理选择选项作为可变持续时间的子策略。这些选项是时间连续的，可以以任何所需频率与系统交互，从而提供平滑的动作变化。我们通过将其性能与传统 RL 和时间抽象 RL 方法进行比较，展示了 CTCO 的有效性。",
    "tldr": "这篇论文提出了一个名为CTCO的框架，其中代理选择选项作为可变持续时间的子策略。这个框架可以以任何所需频率与系统交互，从而提供平滑的动作变化，相比传统RL和时间抽象RL方法，其性能更好。",
    "en_tdlr": "This paper proposes a framework called CTCO, where agents choose options as sub-policies of variable durations and can interact with the system at any desired frequency, providing smooth action changes. CTCO demonstrates better performance compared to classical RL and temporal-abstraction RL methods."
}