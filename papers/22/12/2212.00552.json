{
    "title": "An Effective Employment of Contrastive Learning in Multi-label Text Classification. (arXiv:2212.00552v2 [cs.CL] UPDATED)",
    "abstract": "The effectiveness of contrastive learning technology in natural language processing tasks is yet to be explored and analyzed. How to construct positive and negative samples correctly and reasonably is the core challenge of contrastive learning. It is even harder to discover contrastive objects in multi-label text classification tasks. There are very few contrastive losses proposed previously. In this paper, we investigate the problem from a different angle by proposing five novel contrastive losses for multi-label text classification tasks. These are Strict Contrastive Loss (SCL), Intra-label Contrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL), Jaccard Similarity Probability Contrastive Loss (JSPCL), and Stepwise Label Contrastive Loss (SLCL). We explore the effectiveness of contrastive learning for multi-label text classification tasks by the employment of these novel losses and provide a set of baseline models for deploying contrastive learning techniques on specific t",
    "link": "http://arxiv.org/abs/2212.00552",
    "context": "Title: An Effective Employment of Contrastive Learning in Multi-label Text Classification. (arXiv:2212.00552v2 [cs.CL] UPDATED)\nAbstract: The effectiveness of contrastive learning technology in natural language processing tasks is yet to be explored and analyzed. How to construct positive and negative samples correctly and reasonably is the core challenge of contrastive learning. It is even harder to discover contrastive objects in multi-label text classification tasks. There are very few contrastive losses proposed previously. In this paper, we investigate the problem from a different angle by proposing five novel contrastive losses for multi-label text classification tasks. These are Strict Contrastive Loss (SCL), Intra-label Contrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL), Jaccard Similarity Probability Contrastive Loss (JSPCL), and Stepwise Label Contrastive Loss (SLCL). We explore the effectiveness of contrastive learning for multi-label text classification tasks by the employment of these novel losses and provide a set of baseline models for deploying contrastive learning techniques on specific t",
    "path": "papers/22/12/2212.00552.json",
    "total_tokens": 861,
    "translated_title": "在多标签文本分类中有效地使用对比学习",
    "translated_abstract": "对比学习技术在自然语言处理任务中的有效性尚待探索和分析。如何正确合理地构建正负样本是对比学习的核心挑战，而在多标签文本分类任务中，发现对比对象更加困难。之前提出的对比损失函数很少。在本文中，我们从不同的角度探讨这个问题，提出了五个新颖的对比损失函数，用于多标签文本分类任务，包括严格对比损失 (SCL)、内标签对比损失 (ICL)、Jaccard相似度对比损失(JSCL)、Jaccard相似度概率对比损失(JSPCL)和逐步标签对比损失(SLCL)。我们通过使用这些新颖的损失函数，探索了对比学习在多标签文本分类任务中的有效性，并为在特定任务上部署对比学习技术提供了一套基准模型。",
    "tldr": "本论文提出了五个新的对比损失函数，用于多标签文本分类任务。通过对比学习技术的应用，探索了其在多标签文本分类任务中的有效性，并提供了一套基准模型。",
    "en_tdlr": "This paper proposes five novel contrastive losses for multi-label text classification tasks. By exploring the effectiveness of contrastive learning techniques, it provides a set of baseline models for deploying contrastive learning on specific tasks."
}