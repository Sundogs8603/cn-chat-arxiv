{
    "title": "Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval. (arXiv:2212.10726v2 [cs.CL] UPDATED)",
    "abstract": "Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in $N$ languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation. We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval -- the last of which w",
    "link": "http://arxiv.org/abs/2212.10726",
    "context": "Title: Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval. (arXiv:2212.10726v2 [cs.CL] UPDATED)\nAbstract: Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in $N$ languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation. We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval -- the last of which w",
    "path": "papers/22/12/2212.10726.json",
    "total_tokens": 877,
    "translated_title": "超越对比学习：一种多语言检索的变分生成模型",
    "translated_abstract": "对比学习已被成功用于检索语义对齐的句子，但往往需要大批量处理或精心的工程才能奏效。本文提出了一种生成模型，用于学习多语言文本嵌入，可以用于检索或评分句子对。我们的模型在$N$种语言的并行数据上操作，并通过我们引入的一种近似方法，在这个多语言环境中有效地鼓励源分离，将翻译之间共享的语义信息与语体或语言特定变化分开。我们展示了对比和基于生成的方法在学习多语言文本嵌入方面的大规模仔细比较，这是我们所知道的尽管这些方法十分流行却从未比较过的。我们在一系列任务上评估了这种方法，包括语义相似性，双语挖掘和跨语言问题检索——最后一个任务将展示我们的方法在实际检索任务中的有效性。",
    "tldr": "本文提出了一种多语言检索的变分生成模型，可以有效地鼓励源分离，并展示其与对比学习方法的比较结果，该方法在实际检索任务中表现出效果。",
    "en_tdlr": "This paper proposes a variational generative model for multilingual retrieval, which can efficiently encourage source separation and shows careful comparisons with contrastive learning methods, demonstrating its effectiveness in practical retrieval tasks."
}