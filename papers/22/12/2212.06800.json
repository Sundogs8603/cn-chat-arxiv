{
    "title": "Diverse Demonstrations Improve In-context Compositional Generalization. (arXiv:2212.06800v3 [cs.CL] UPDATED)",
    "abstract": "In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input. In this work, we propose a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations. We empirically show that combining diverse demonstrations with in-context learning substantially improves performance across three compositional generalization semantic parsing datasets in the pure in-context learning setup and when combined with fi",
    "link": "http://arxiv.org/abs/2212.06800",
    "context": "Title: Diverse Demonstrations Improve In-context Compositional Generalization. (arXiv:2212.06800v3 [cs.CL] UPDATED)\nAbstract: In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input. In this work, we propose a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations. We empirically show that combining diverse demonstrations with in-context learning substantially improves performance across three compositional generalization semantic parsing datasets in the pure in-context learning setup and when combined with fi",
    "path": "papers/22/12/2212.06800.json",
    "total_tokens": 831,
    "translated_title": "多样的演示提高了上下文组合泛化能力",
    "translated_abstract": "在 i.i.d 语义解析拆分中，上下文学习表现出了很大的成功，其中训练集和测试集是从同一分布中抽取的。在这个设置中，模型通常会使用与输入话语类似的演示来提示。然而，在组合泛化的设置中，模型在输出具有训练集中不存在的结构的测试集上进行测试时，选择类似的演示是不够的，因为通常没有任何示例足够接近于输入。本文提出了一种选择多样的演示的方法，旨在共同涵盖输出程序中所需的所有结构，以鼓励模型从这些演示中推广到新的结构。我们在三个组合泛化语义解析数据集中进行了实证研究，结果表明，在纯上下文学习设置中，结合多样的演示可以显著提高模型的性能，同时还可以与fi进行组合。",
    "tldr": "本文提出了一种选择多样化演示来鼓励模型在组合泛化的测试集上表现良好的方法，并在三个组合泛化语义解析数据集中取得了显著的提高。",
    "en_tdlr": "The paper proposes a method of selecting diverse demonstrations to encourage model's performance on compositional generalization test sets, and achieves substantial improvement on three semantic parsing datasets."
}