{
    "title": "MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers. (arXiv:2212.07841v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained Transformers (\\eg BERT) have been commonly used in existing dense retrieval methods for parameter initialization, and recent studies are exploring more effective pre-training tasks for further improving the quality of dense vectors. Although various novel and effective tasks have been proposed, their different input formats and learning objectives make them hard to be integrated for jointly improving the model performance. In this work, we aim to unify a variety of pre-training tasks into the bottlenecked masked autoencoder manner, and integrate them into a multi-task pre-trained model, namely MASTER. Concretely, MASTER utilizes a shared-encoder multi-decoder architecture that can construct a representation bottleneck to compress the abundant semantic information across tasks into dense vectors. Based on it, we integrate three types of representative pre-training tasks: corrupted passages recovering, related passages recovering and PLMs outputs recovering, to characterize t",
    "link": "http://arxiv.org/abs/2212.07841",
    "context": "Title: MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers. (arXiv:2212.07841v2 [cs.CL] UPDATED)\nAbstract: Pre-trained Transformers (\\eg BERT) have been commonly used in existing dense retrieval methods for parameter initialization, and recent studies are exploring more effective pre-training tasks for further improving the quality of dense vectors. Although various novel and effective tasks have been proposed, their different input formats and learning objectives make them hard to be integrated for jointly improving the model performance. In this work, we aim to unify a variety of pre-training tasks into the bottlenecked masked autoencoder manner, and integrate them into a multi-task pre-trained model, namely MASTER. Concretely, MASTER utilizes a shared-encoder multi-decoder architecture that can construct a representation bottleneck to compress the abundant semantic information across tasks into dense vectors. Based on it, we integrate three types of representative pre-training tasks: corrupted passages recovering, related passages recovering and PLMs outputs recovering, to characterize t",
    "path": "papers/22/12/2212.07841.json",
    "total_tokens": 1067,
    "translated_title": "MASTER:多任务预训练的瓶颈掩蔽自编码器比密集型检索器更好",
    "translated_abstract": "现有的密集检索方法中，预训练的Transformer（如BERT）通常用于参数初始化，最近的研究正在探索更有效的预训练任务，以进一步提高密集向量的质量。虽然已经提出了各种新颖而有效的任务，但它们不同的输入格式和学习目标使它们难以被整合起来共同提高模型性能。本文旨在将各种预训练任务统一成瓶颈掩蔽自编码器，将它们整合到一个多任务预训练模型中，名为MASTER。具体来说，MASTER利用共享编码器多解码器架构，可以构造表示瓶颈，将跨各种任务的丰富语义信息压缩成密集向量。基于此，我们整合了三种代表性的预训练任务：破损段落恢复、相关段落恢复和PLMs输出恢复，以同时进行转录、索引和QA。我们在两个广泛使用的数据集上的实验表明，MASTER在相同的模型大小和预训练资源下优于最先进的密集检索模型，表明了将各种预训练任务以统一的格式集成的有效性。",
    "tldr": "本文提出了一个名为MASTER的多任务预训练模型，利用瓶颈掩蔽自编码器统一各种预训练任务，并将其集成到一个模型中。该模型在两个广泛使用的数据集上的实验表明，相比同等模型大小和预训练资源的最先进密集检索模型，MASTER表现更好。"
}