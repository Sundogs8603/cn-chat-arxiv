{
    "title": "SERENGETI: Massively Multilingual Language Models for Africa. (arXiv:2212.10785v2 [cs.CL] UPDATED)",
    "abstract": "Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research.\\footnote{\\href{https://github.com/UBC-NLP/serengeti}{https://g",
    "link": "http://arxiv.org/abs/2212.10785",
    "context": "Title: SERENGETI: Massively Multilingual Language Models for Africa. (arXiv:2212.10785v2 [cs.CL] UPDATED)\nAbstract: Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research.\\footnote{\\href{https://github.com/UBC-NLP/serengeti}{https://g",
    "path": "papers/22/12/2212.10785.json",
    "total_tokens": 861,
    "translated_title": "SERENGETI：为非洲而设计的大规模多语言语言模型",
    "translated_abstract": "多语言预训练语言模型（mPLM）在预训练期间获得有价值的语言信息，可推动特定任务的微调。目前，现有语言模型仅覆盖了大约2,000种非洲语言中的约31种。我们开发了SERENGETI，一种大规模多语言语言模型，覆盖了517种非洲语言和语言方言，以改善这种限制。我们在20个数据集上评估了我们的新型模型在八个自然语言理解任务上的表现，并将其与覆盖4-23种非洲语言的4个mPLM进行比较。SERENGETI在八个任务中的11个数据集上表现优异，实现了82.27的平均F_1。我们还进行了模型错误分析，以探究在零-shot情况下应用模型时语言系谱和语言相似性的影响。我们将向公众发布我们的研究模型。",
    "tldr": "SERENGETI是一个大规模多语言语言模型，覆盖了517种非洲语言和语言方言。在自然语言理解任务中，它的表现优于其他在非洲语言上的语言模型，能够提供有价值的语言信息。"
}