{
    "title": "Visually-augmented pretrained language models for NLP tasks without images. (arXiv:2212.07937v2 [cs.CL] UPDATED)",
    "abstract": "Although pre-trained language models~(PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or generation), and they also conduct the augmentation for the whole input text, without considering whether it is actually needed in specific inputs or tasks. To address these issues, we propose a novel \\textbf{V}isually-\\textbf{A}ugmented fine-tuning approach that can be generally applied to various PLMs or NLP tasks, \\textbf{W}ithout using any retrieved or generated \\textbf{I}mages, namely \\textbf{VAWI}. Experimental results show that our approach can consistently improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and outperform several competitive baselines on ten tasks. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/VAWI}.",
    "link": "http://arxiv.org/abs/2212.07937",
    "context": "Title: Visually-augmented pretrained language models for NLP tasks without images. (arXiv:2212.07937v2 [cs.CL] UPDATED)\nAbstract: Although pre-trained language models~(PLMs) have shown impressive performance by text-only self-supervised training, they are found lack of visual semantics or commonsense. Existing solutions often rely on explicit images for visual knowledge augmentation (requiring time-consuming retrieval or generation), and they also conduct the augmentation for the whole input text, without considering whether it is actually needed in specific inputs or tasks. To address these issues, we propose a novel \\textbf{V}isually-\\textbf{A}ugmented fine-tuning approach that can be generally applied to various PLMs or NLP tasks, \\textbf{W}ithout using any retrieved or generated \\textbf{I}mages, namely \\textbf{VAWI}. Experimental results show that our approach can consistently improve the performance of BERT, RoBERTa, BART, and T5 at different scales, and outperform several competitive baselines on ten tasks. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/VAWI}.",
    "path": "papers/22/12/2212.07937.json",
    "total_tokens": 959,
    "translated_title": "无需图像的视觉增强预训练语言模型用于自然语言处理任务",
    "translated_abstract": "尽管预训练语言模型(PLMs)通过纯文本自监督训练表现出色，但他们缺乏视觉语义或常识。现有的解决方案常常依赖于明确的图像来进行视觉知识增强(需要耗费时间的检索或生成)，并且他们也会为整个输入文本进行增强，而不考虑是否实际上需要在特定的输入或任务中进行增强。为了解决这些问题，我们提出了一种新颖的视觉增强微调方法，可以普遍地应用于各种PLM或NLP任务，无需使用任何检索或生成的图像，称为VAWI。实验结果表明，我们的方法可以在不同规模的BERT、RoBERTa、BART和T5上持续地提高性能，并在十个任务上胜过几个竞争基线。我们的代码和数据公开可用于\\url{https://github.com/RUCAIBox/VAWI}。",
    "tldr": "该研究提出了一种无需检索或生成图像的视觉增强微调方法，可以普遍地应用于各种PLM或NLP任务，并在不同规模的BERT、RoBERTa、BART和T5上持续地提高性能。",
    "en_tdlr": "This paper proposes a novel visually-augmented fine-tuning approach, without using any retrieved or generated images, for various PLMs or NLP tasks, and achieves consistently improved performance on different scales of BERT, RoBERTa, BART, and T5, outperforming several competitive baselines on ten tasks. The codes and data are publicly available at https://github.com/RUCAIBox/VAWI."
}