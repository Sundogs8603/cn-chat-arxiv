{
    "title": "PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v2 [cs.CL] UPDATED)",
    "abstract": "We investigate response generation for multi-turn dialogue in generative-based chatbots. Existing generative models based on RNNs (Recurrent Neural Networks) usually employ the last hidden state to summarize the sequences, which makes models unable to capture the subtle variability observed in different dialogues and cannot distinguish the differences between dialogues that are similar in composition. In this paper, we propose a Pseudo-Variational Gated Recurrent Unit (PVGRU) component without posterior knowledge through introducing a recurrent summarizing variable into the GRU, which can aggregate the accumulated distribution variations of subsequences. PVGRU can perceive the subtle semantic variability through summarizing variables that are optimized by the devised distribution consistency and reconstruction objectives. In addition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve the dive",
    "link": "http://arxiv.org/abs/2212.09086",
    "context": "Title: PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v2 [cs.CL] UPDATED)\nAbstract: We investigate response generation for multi-turn dialogue in generative-based chatbots. Existing generative models based on RNNs (Recurrent Neural Networks) usually employ the last hidden state to summarize the sequences, which makes models unable to capture the subtle variability observed in different dialogues and cannot distinguish the differences between dialogues that are similar in composition. In this paper, we propose a Pseudo-Variational Gated Recurrent Unit (PVGRU) component without posterior knowledge through introducing a recurrent summarizing variable into the GRU, which can aggregate the accumulated distribution variations of subsequences. PVGRU can perceive the subtle semantic variability through summarizing variables that are optimized by the devised distribution consistency and reconstruction objectives. In addition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve the dive",
    "path": "papers/22/12/2212.09086.json",
    "total_tokens": 904,
    "translated_title": "PVGRU：通过Pseudo-Variational机制生成多样且相关的对话回复",
    "translated_abstract": "我们研究了基于生成的聊天机器人中用于多轮对话的回复生成。现有的基于RNN（循环神经网络）的生成模型通常使用最后隐藏的状态来汇总序列，这使得模型无法捕捉不同对话中观察到的微妙变化，并且不能区分在构成方面相似的对话之间的差异。在本文中，我们提出了一种Pseudo-Variational Gated Recurrent Unit（PVGRU）组件，无需后验知识即可将汇总变量引入GRU，其可以聚合子序列的累积分布变化。 PVGRU可以通过总结变量感知微妙的语义变化，这些变化是通过设计的分布一致性和重构目标进行优化的。此外，我们基于PVGRU构建了Pseudo-Variational Hierarchical Dialogue（PVHD）模型。实验结果表明，PVGRU可以广泛提高对话模型的多样性和相关性。",
    "tldr": "该论文提出了一个名为PVGRU的组件，可以通过引入汇总变量来聚合子序列的累积分布变化，从而优化基于生成的聊天机器人的多轮对话回复，提高对话模型的多样性和相关性。",
    "en_tdlr": "This paper proposes a PVGRU component that can optimize response generation for multi-turn dialogue in generative-based chatbots by introducing a recurrent summarizing variable into the GRU, which can aggregate the accumulated distribution variations of subsequences and perceive the subtle semantic variability. It can broadly improve the diversity and relevance of dialogue models."
}