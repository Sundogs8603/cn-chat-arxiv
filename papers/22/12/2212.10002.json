{
    "title": "Defending Against Misinformation Attacks in Open-Domain Question Answering. (arXiv:2212.10002v2 [cs.CL] UPDATED)",
    "abstract": "Recent work in open-domain question answering (ODQA) has shown that adversarial poisoning of the search collection can cause large drops in accuracy for production systems. However, little to no work has proposed methods to defend against these attacks. To do so, we rely on the intuition that redundant information often exists in large corpora. To find it, we introduce a method that uses query augmentation to search for a diverse set of passages that could answer the original question but are less likely to have been poisoned. We integrate these new passages into the model through the design of a novel confidence method, comparing the predicted answer to its appearance in the retrieved contexts (what we call \\textit{Confidence from Answer Redundancy}, i.e. CAR). Together these methods allow for a simple but effective way to defend against poisoning attacks that provides gains of nearly 20\\% exact match across varying levels of data poisoning/knowledge conflicts.",
    "link": "http://arxiv.org/abs/2212.10002",
    "context": "Title: Defending Against Misinformation Attacks in Open-Domain Question Answering. (arXiv:2212.10002v2 [cs.CL] UPDATED)\nAbstract: Recent work in open-domain question answering (ODQA) has shown that adversarial poisoning of the search collection can cause large drops in accuracy for production systems. However, little to no work has proposed methods to defend against these attacks. To do so, we rely on the intuition that redundant information often exists in large corpora. To find it, we introduce a method that uses query augmentation to search for a diverse set of passages that could answer the original question but are less likely to have been poisoned. We integrate these new passages into the model through the design of a novel confidence method, comparing the predicted answer to its appearance in the retrieved contexts (what we call \\textit{Confidence from Answer Redundancy}, i.e. CAR). Together these methods allow for a simple but effective way to defend against poisoning attacks that provides gains of nearly 20\\% exact match across varying levels of data poisoning/knowledge conflicts.",
    "path": "papers/22/12/2212.10002.json",
    "total_tokens": 906,
    "translated_title": "在开放域问答中防御误导性攻击",
    "translated_abstract": "最近在开放域问答领域中的研究表明，对于搜索集合进行的敌对污染可能会导致生产系统的精度大幅下降。然而，几乎没有工作提出防御这些攻击的方法。为了解决这个问题，我们依赖于大型语料库中存在冗余信息的直觉。为了找到这些信息，我们引入了一种使用查询扩充来搜索可能回答原始问题的多样化段落集合的方法，但是不太可能被污染。我们通过设计一种新型的置信度方法（比较预测答案与其在检索到的上下文中出现的情况——我们称之为答案冗余置信度，即CAR）将这些新段落集成到模型中。这些方法共同构成了一种简单但有效的方式，用于防御污染攻击，可在不同水平的数据污染/知识冲突下提供近20％的精确匹配增益。",
    "tldr": "本文提出了一种使用查询扩充来搜索冗余信息、并通过新颖的置信度方法将其集成到模型中的方法，可以有效防御开放域问答系统中的污染攻击，精确匹配率可提高近20%。",
    "en_tdlr": "This paper proposes a method that uses query augmentation to search for redundant information and integrates it into the model through a novel confidence method, which can effectively defend against poisoning attacks in open-domain question answering systems and increase the exact match rate by nearly 20%."
}