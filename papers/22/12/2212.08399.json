{
    "title": "Assessing the Impact of Sequence Length Learning on Classification Tasks for Transformer Encoder Models",
    "abstract": "arXiv:2212.08399v2 Announce Type: replace  Abstract: Classification algorithms using Transformer architectures can be affected by the sequence length learning problem whenever observations from different classes have a different length distribution. This problem causes models to use sequence length as a predictive feature instead of relying on important textual information. Although most public datasets are not affected by this problem, privately owned corpora for fields such as medicine and insurance may carry this data bias. The exploitation of this sequence length feature poses challenges throughout the value chain as these machine learning models can be used in critical applications. In this paper, we empirically expose this problem and present approaches to minimize its impacts.",
    "link": "https://arxiv.org/abs/2212.08399",
    "context": "Title: Assessing the Impact of Sequence Length Learning on Classification Tasks for Transformer Encoder Models\nAbstract: arXiv:2212.08399v2 Announce Type: replace  Abstract: Classification algorithms using Transformer architectures can be affected by the sequence length learning problem whenever observations from different classes have a different length distribution. This problem causes models to use sequence length as a predictive feature instead of relying on important textual information. Although most public datasets are not affected by this problem, privately owned corpora for fields such as medicine and insurance may carry this data bias. The exploitation of this sequence length feature poses challenges throughout the value chain as these machine learning models can be used in critical applications. In this paper, we empirically expose this problem and present approaches to minimize its impacts.",
    "path": "papers/22/12/2212.08399.json",
    "total_tokens": 733,
    "translated_title": "评估序列长度学习对Transformer编码器模型分类任务的影响",
    "translated_abstract": "使用Transformer架构的分类算法在观测来自不同类的序列具有不同长度分布时可能受到序列长度学习问题的影响。这个问题导致模型将序列长度作为一个预测特征，而不是依赖于重要的文本信息。虽然大多数公共数据集不受此问题影响，但对于医学和保险等领域的私人拥有语料库可能存在这种数据偏差。利用这种序列长度特征会在整个价值链中带来挑战，因为这些机器学习模型可以用于关键应用。在本文中，我们从经验上揭示了这个问题，并提出了减少其影响的方法。",
    "tldr": "Transformer编码器模型在处理分类任务时，受到序列长度学习问题影响，可能导致模型过度依赖序列长度作为预测特征而非文本信息，本文提出了一些方法来减少这种影响。",
    "en_tdlr": "Transformer encoder models in classification tasks are impacted by the sequence length learning problem, leading to the models relying excessively on sequence length as a predictive feature rather than textual information. This paper presents approaches to mitigate this impact."
}