{
    "title": "Benchmarking Self-Supervised Learning on Diverse Pathology Datasets. (arXiv:2212.04690v2 [cs.CV] UPDATED)",
    "abstract": "Computational pathology can lead to saving human lives, but models are annotation hungry and pathology images are notoriously expensive to annotate. Self-supervised learning has shown to be an effective method for utilizing unlabeled data, and its application to pathology could greatly benefit its downstream tasks. Yet, there are no principled studies that compare SSL methods and discuss how to adapt them for pathology. To address this need, we execute the largest-scale study of SSL pre-training on pathology image data, to date. Our study is conducted using 4 representative SSL methods on diverse downstream tasks. We establish that large-scale domain-aligned pre-training in pathology consistently out-performs ImageNet pre-training in standard SSL settings such as linear and fine-tuning evaluations, as well as in low-label regimes. Moreover, we propose a set of domain-specific techniques that we experimentally show leads to a performance boost. Lastly, for the first time, we apply SSL t",
    "link": "http://arxiv.org/abs/2212.04690",
    "context": "Title: Benchmarking Self-Supervised Learning on Diverse Pathology Datasets. (arXiv:2212.04690v2 [cs.CV] UPDATED)\nAbstract: Computational pathology can lead to saving human lives, but models are annotation hungry and pathology images are notoriously expensive to annotate. Self-supervised learning has shown to be an effective method for utilizing unlabeled data, and its application to pathology could greatly benefit its downstream tasks. Yet, there are no principled studies that compare SSL methods and discuss how to adapt them for pathology. To address this need, we execute the largest-scale study of SSL pre-training on pathology image data, to date. Our study is conducted using 4 representative SSL methods on diverse downstream tasks. We establish that large-scale domain-aligned pre-training in pathology consistently out-performs ImageNet pre-training in standard SSL settings such as linear and fine-tuning evaluations, as well as in low-label regimes. Moreover, we propose a set of domain-specific techniques that we experimentally show leads to a performance boost. Lastly, for the first time, we apply SSL t",
    "path": "papers/22/12/2212.04690.json",
    "total_tokens": 1127,
    "translated_title": "多样化病理数据集上的自监督学习对比研究",
    "translated_abstract": "计算病理学可以挽救人类生命，但是模型需要大量标注数据，病理图像非常昂贵。自监督学习已经证明是利用无标签数据的有效方法，其在病理学中的应用可以极大地改善其下游任务。然而，没有一项基于原则的研究来比较自监督学习方法并讨论如何适应病理学。为了解决这个问题，我们进行了迄今为止规模最大的一项自监督预训练病理图像数据的研究，使用了4种代表性自监督学习方法进行了测试，并对不同下游任务进行了评估。我们发现在病理学中大规模领域对齐的预训练方法在标准的自监督学习设置（如线性和微调评估）以及在低标签环境下始终优于在ImageNet上的预训练方法。此外，我们提出了一套领域特定技术，我们通过实验表明这些技术可以提高模型性能。最后，我们首次将自监督学习转移学习应用于月经周期病理数据集，这是一个挑战性的未见过的领域，我们已经展示大规模领域对齐预训练可以显著提高该领域下游任务的性能。",
    "tldr": "本文针对计算病理学所需的大量标注数据问题，进行了最大规模的自监督预训练研究，发现大规模领域对齐的预训练方法在病理学中始终优于ImageNet上的预训练方法，并提出一套领域特定技术来提高模型性能，在月经周期病理数据集中也表现出良好性能。",
    "en_tdlr": "This study presents the largest-scale research on self-supervised pre-training with pathology image data, proposing domain-specific techniques to enhance the performance of the models. The study establishes that large-scale domain-aligned pre-training consistently outperforms ImageNet pre-training in pathology, and demonstrates the effectiveness of SSL transfer learning on the menstrual cycle pathology dataset."
}