{
    "title": "Generalizing DP-SGD with Shuffling and Batch Clipping. (arXiv:2212.05796v3 [cs.LG] UPDATED)",
    "abstract": "Classical differential private DP-SGD implements individual clipping with random subsampling, which forces a mini-batch SGD approach. We provide a general differential private algorithmic framework that goes beyond DP-SGD and allows any possible first order optimizers (e.g., classical SGD and momentum based SGD approaches) in combination with batch clipping, which clips an aggregate of computed gradients rather than summing clipped gradients (as is done in individual clipping). The framework also admits sampling techniques beyond random subsampling such as shuffling. Our DP analysis follows the $f$-DP approach and introduces a new proof technique which allows us to derive simple closed form expressions and to also analyse group privacy. In particular, for $E$ epochs work and groups of size $g$, we show a $\\sqrt{g E}$ DP dependency for batch clipping with shuffling.",
    "link": "http://arxiv.org/abs/2212.05796",
    "context": "Title: Generalizing DP-SGD with Shuffling and Batch Clipping. (arXiv:2212.05796v3 [cs.LG] UPDATED)\nAbstract: Classical differential private DP-SGD implements individual clipping with random subsampling, which forces a mini-batch SGD approach. We provide a general differential private algorithmic framework that goes beyond DP-SGD and allows any possible first order optimizers (e.g., classical SGD and momentum based SGD approaches) in combination with batch clipping, which clips an aggregate of computed gradients rather than summing clipped gradients (as is done in individual clipping). The framework also admits sampling techniques beyond random subsampling such as shuffling. Our DP analysis follows the $f$-DP approach and introduces a new proof technique which allows us to derive simple closed form expressions and to also analyse group privacy. In particular, for $E$ epochs work and groups of size $g$, we show a $\\sqrt{g E}$ DP dependency for batch clipping with shuffling.",
    "path": "papers/22/12/2212.05796.json",
    "total_tokens": 985,
    "translated_title": "使用打乱和批量剪切的方法推广DP-SGD算法",
    "translated_abstract": "传统的差分隐私DP-SGD算法使用随机子采样实现个体剪切，从而强制实现小批量SGD方法。我们提供了一个通用的差分隐私算法框架，超越了DP-SGD算法，允许任何可能的一阶优化器（如经典的SGD和基于动量的SGD方法）与批量剪切相结合，它剪切了计算梯度的聚合结果，而不是对剪切梯度求和（个体剪切）。该框架还允许使用除随机子采样之外的采样技术，如打乱。我们的DP分析遵循$f$-DP的方法，并引入了一种新的证明技术，使我们能够推导出简单的闭式表达式，并对群体隐私进行分析。特别地，对于$E$个时期的工作和大小为$g$的群体，我们展示了具有打乱和批量剪切的$\\sqrt{g E}$ DP依赖关系。",
    "tldr": "本文提出了一个通用的差分隐私算法框架，允许将任何一阶优化器与批量剪切相结合，并使用打乱等采样技术。作者的DP分析使用了$f$-DP方法，并推导出了简单的闭式表达式，同时还考虑了群体隐私。实验证明，对于特定的工作时期和群体大小，框架具有$\\sqrt{g E}$ 的差分隐私依赖关系。"
}