{
    "title": "Cognitive Accident Prediction in Driving Scenes: A Multimodality Benchmark. (arXiv:2212.09381v2 [cs.CV] UPDATED)",
    "abstract": "Traffic accident prediction in driving videos aims to provide an early warning of the accident occurrence, and supports the decision making of safe driving systems. Previous works usually concentrate on the spatial-temporal correlation of object-level context, while they do not fit the inherent long-tailed data distribution well and are vulnerable to severe environmental change. In this work, we propose a Cognitive Accident Prediction (CAP) method that explicitly leverages human-inspired cognition of text description on the visual observation and the driver attention to facilitate model training. In particular, the text description provides a dense semantic description guidance for the primary context of the traffic scene, while the driver attention provides a traction to focus on the critical region closely correlating with safe driving. CAP is formulated by an attentive text-to-vision shift fusion module, an attentive scene context transfer module, and the driver attention guided acc",
    "link": "http://arxiv.org/abs/2212.09381",
    "context": "Title: Cognitive Accident Prediction in Driving Scenes: A Multimodality Benchmark. (arXiv:2212.09381v2 [cs.CV] UPDATED)\nAbstract: Traffic accident prediction in driving videos aims to provide an early warning of the accident occurrence, and supports the decision making of safe driving systems. Previous works usually concentrate on the spatial-temporal correlation of object-level context, while they do not fit the inherent long-tailed data distribution well and are vulnerable to severe environmental change. In this work, we propose a Cognitive Accident Prediction (CAP) method that explicitly leverages human-inspired cognition of text description on the visual observation and the driver attention to facilitate model training. In particular, the text description provides a dense semantic description guidance for the primary context of the traffic scene, while the driver attention provides a traction to focus on the critical region closely correlating with safe driving. CAP is formulated by an attentive text-to-vision shift fusion module, an attentive scene context transfer module, and the driver attention guided acc",
    "path": "papers/22/12/2212.09381.json",
    "total_tokens": 914,
    "translated_title": "行车场景中的认知事故预测:一种多模态基准方法",
    "translated_abstract": "行车视频的交通事故预测旨在提早发现事故的发生，支持安全驾驶系统的决策。以往的研究通常关注物体级上下文的时空相关性，而不太适合内在的长尾数据分布，并且容易受到严重环境变化的影响。在本研究中，我们提出了一种认知事故预测（CAP）方法，明确地利用了人类启发的对视觉观察和驾驶员注意力上的文本描述的认知来促进模型训练。特别地，文本描述为交通场景的主要上下文提供了密集的语义描述指导，而驾驶员的注意力提供了一个牵引力，使其专注于与安全驾驶密切相关的关键区域。CAP由注重文本到视觉转移融合模块、注重场景上下文转移模块和驾驶员注意力引导的事故预测网络三个部分组成。",
    "tldr": "本篇论文提出了一种名为CAP的方法，它通过文本描述和驾驶员注意力来改善行车视频中的交通事故预测。这个方法考虑了长尾数据分布和环境变化等挑战，希望能为未来的安全驾驶系统提供决策支持。",
    "en_tdlr": "This paper proposes a method called CAP, which improves traffic accident prediction in driving videos by leveraging text description and driver attention. It considers challenges such as long-tailed data distribution and environmental changes, aiming to provide decision support for future safe driving systems."
}