{
    "title": "An Analysis of Attention via the Lens of Exchangeability and Latent Variable Models",
    "abstract": "arXiv:2212.14852v3 Announce Type: replace  Abstract: With the attention mechanism, transformers achieve significant empirical successes. Despite the intuitive understanding that transformers perform relational inference over long sequences to produce desirable representations, we lack a rigorous theory on how the attention mechanism achieves it. In particular, several intriguing questions remain open: (a) What makes a desirable representation? (b) How does the attention mechanism infer the desirable representation within the forward pass? (c) How does a pretraining procedure learn to infer the desirable representation through the backward pass?   We observe that, as is the case in BERT and ViT, input tokens are often exchangeable since they already include positional encodings. The notion of exchangeability induces a latent variable model that is invariant to input sizes, which enables our theoretical analysis.   - To answer (a) on representation, we establish the existence of a suffic",
    "link": "https://arxiv.org/abs/2212.14852",
    "context": "Title: An Analysis of Attention via the Lens of Exchangeability and Latent Variable Models\nAbstract: arXiv:2212.14852v3 Announce Type: replace  Abstract: With the attention mechanism, transformers achieve significant empirical successes. Despite the intuitive understanding that transformers perform relational inference over long sequences to produce desirable representations, we lack a rigorous theory on how the attention mechanism achieves it. In particular, several intriguing questions remain open: (a) What makes a desirable representation? (b) How does the attention mechanism infer the desirable representation within the forward pass? (c) How does a pretraining procedure learn to infer the desirable representation through the backward pass?   We observe that, as is the case in BERT and ViT, input tokens are often exchangeable since they already include positional encodings. The notion of exchangeability induces a latent variable model that is invariant to input sizes, which enables our theoretical analysis.   - To answer (a) on representation, we establish the existence of a suffic",
    "path": "papers/22/12/2212.14852.json",
    "total_tokens": 719,
    "translated_title": "通过可交换性和潜变量模型分析注意力机制",
    "translated_abstract": "利用注意力机制，transformers取得了显著的实证成功。尽管我们直觉地认为transformers通过对长序列进行关系推断来产生理想表示，但我们缺乏对注意力机制如何实现此目标的严格理论。本文观察到，正如在BERT和ViT中一样，输入标记通常是可交换的，因为它们已经包含位置编码。可交换性的概念引入了一个对输入大小不变的潜变量模型，从而使得我们的理论分析成为可能。",
    "tldr": "通过研究可交换性和潜变量模型，我们建立了对注意力机制工作原理的理论分析，从而揭示了transformers如何实现对长序列进行关系推断以产生理想表示的方法。",
    "en_tdlr": "By studying exchangeability and latent variable models, we establish a theoretical analysis of the attention mechanism, revealing how transformers achieve relational inference over long sequences to produce desirable representations."
}