{
    "title": "On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. (arXiv:2212.10020v2 [cs.CL] UPDATED)",
    "abstract": "In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https://github.com/cloudygoose/blindspot_nlg.",
    "link": "http://arxiv.org/abs/2212.10020",
    "context": "Title: On the Blind Spots of Model-Based Evaluation Metrics for Text Generation. (arXiv:2212.10020v2 [cs.CL] UPDATED)\nAbstract: In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore is confused by truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning or middle of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation. We have released our code and data at https://github.com/cloudygoose/blindspot_nlg.",
    "path": "papers/22/12/2212.10020.json",
    "total_tokens": 1005,
    "translated_title": "论基于模型的文本生成评价指标的盲点问题",
    "translated_abstract": "本文探讨了一种有用但常常被忽略的文本生成评价指标鲁棒性分析方法：使用合成数据进行压力测试。我们随机设计并合成了各种可能的误差，并检查它们是否会导致评价指标分数的显著下降。我们研究了基于预训练语言模型的一系列最新评价指标，用于开放式生成、翻译和摘要等任务。实验揭示了现有评价指标中有趣的不敏感、偏见、甚至漏洞。例如，我们发现BERTScore对摘要中的截断误差感到困惑，而在生成的开头或中间存在误差时MAUVE（基于GPT-2）则不敏感。进一步，我们研究了这些盲点背后的原因，并提出了实用的解决方案，以实现更可靠的文本生成评价。我们已在https://github.com/cloudygoose/blindspot_nlg 上发布了我们的代码和数据。",
    "tldr": "本文研究了针对文本生成评价指标的鲁棒性分析方法，使用合成数据进行压力测试，发现现有评价指标存在一些盲点和偏见，例如BERTScore对摘要中的截断误差无法很好地处理，MAUVE对于生成的开头或中间的误差不敏感，本文提出了实用的解决方案以实现更可靠的文本生成评价。",
    "en_tdlr": "This paper explores the blind spots and biases in existing evaluation metrics for open-ended text generation, translation, and summarization, and proposes practical solutions to improve evaluation reliability based on stress tests with synthetic data. The experiments reveal that BERTScore is confused by truncation errors in summarization and MAUVE is insensitive to errors at the beginning or middle of generations. The authors provide a code and data release for replicating their work."
}