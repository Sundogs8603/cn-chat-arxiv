{
    "title": "MobileTL: On-device Transfer Learning with Inverted Residual Blocks. (arXiv:2212.03246v2 [cs.LG] UPDATED)",
    "abstract": "Transfer learning on edge is challenging due to on-device limited resources. Existing work addresses this issue by training a subset of parameters or adding model patches. Developed with inference in mind, Inverted Residual Blocks (IRBs) split a convolutional layer into depthwise and pointwise convolutions, leading to more stacking layers, e.g., convolution, normalization, and activation layers. Though they are efficient for inference, IRBs require that additional activation maps are stored in memory for training weights for convolution layers and scales for normalization layers. As a result, their high memory cost prohibits training IRBs on resource-limited edge devices, and making them unsuitable in the context of transfer learning. To address this issue, we present MobileTL, a memory and computationally efficient on-device transfer learning method for models built with IRBs. MobileTL trains the shifts for internal normalization layers to avoid storing activation maps for the backwar",
    "link": "http://arxiv.org/abs/2212.03246",
    "context": "Title: MobileTL: On-device Transfer Learning with Inverted Residual Blocks. (arXiv:2212.03246v2 [cs.LG] UPDATED)\nAbstract: Transfer learning on edge is challenging due to on-device limited resources. Existing work addresses this issue by training a subset of parameters or adding model patches. Developed with inference in mind, Inverted Residual Blocks (IRBs) split a convolutional layer into depthwise and pointwise convolutions, leading to more stacking layers, e.g., convolution, normalization, and activation layers. Though they are efficient for inference, IRBs require that additional activation maps are stored in memory for training weights for convolution layers and scales for normalization layers. As a result, their high memory cost prohibits training IRBs on resource-limited edge devices, and making them unsuitable in the context of transfer learning. To address this issue, we present MobileTL, a memory and computationally efficient on-device transfer learning method for models built with IRBs. MobileTL trains the shifts for internal normalization layers to avoid storing activation maps for the backwar",
    "path": "papers/22/12/2212.03246.json",
    "total_tokens": 1113,
    "translated_title": "MobileTL: 基于Inverted Residual Blocks的设备本地迁移学习",
    "translated_abstract": "设备本地迁移学习面临有限的设备资源的挑战。现有方法通过训练参数的子集或加入模型补丁来解决这个问题。为了更高效的推理，Inverted Residual Blocks（IRBs）将卷积层分为逐层深度和逐点卷积，从而实现更多卷积、标准化和激活层的堆叠。虽然它们对于推理是高效的，但IRBs需要在内存中存储额外的激活映射来训练卷积层的权重和标准化层的规模。因此，它们的高内存成本阻碍了在资源有限的边缘设备上训练IRBs，使其在迁移学习的情况下不适用。为了解决这个问题，我们提出了MobileTL，一种基于内部标准化层的移动设备上的内存和计算效率高的迁移学习方法，用于构建IRBs模型。MobileTL训练内部规范化层的移位，以避免存储向后传递的激活图。在图像识别任务中，MobileTL显著降低了内存使用率，并保持了竞争性的准确性。我们在ImageNet数据集上验证了我们的方法，并展示了一个小规模数据集上成功的迁移学习结果。",
    "tldr": "本文提出了MobileTL，一种基于内部标准化层具有内存和计算效率的移动设备上的迁移学习方法，用于构建IRBs模型。MobileTL通过训练内部规范化层的移位来避免存储向后传递的激活图，显著降低了内存使用率，并在图像识别任务中保持了竞争性的准确性。",
    "en_tdlr": "This paper proposes MobileTL, an on-device transfer learning method for models built with Inverted Residual Blocks. MobileTL trains the shifts for internal normalization layers to avoid storing activation maps for the backward pass, significantly reducing memory usage while maintaining competitive accuracy."
}