{
    "title": "Towards Holistic Surgical Scene Understanding. (arXiv:2212.04582v3 [cs.CV] UPDATED)",
    "abstract": "Most benchmarks for studying surgical interventions focus on a specific challenge instead of leveraging the intrinsic complementarity among different tasks. In this work, we present a new experimental framework towards holistic surgical scene understanding. First, we introduce the Phase, Step, Instrument, and Atomic Visual Action recognition (PSI-AVA) Dataset. PSI-AVA includes annotations for both long-term (Phase and Step recognition) and short-term reasoning (Instrument detection and novel Atomic Action recognition) in robot-assisted radical prostatectomy videos. Second, we present Transformers for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong baseline for surgical scene understanding. TAPIR leverages our dataset's multi-level annotations as it benefits from the learned representation on the instrument detection task to improve its classification capacity. Our experimental results in both PSI-AVA and other publicly available databases demonstrate the adequacy o",
    "link": "http://arxiv.org/abs/2212.04582",
    "context": "Title: Towards Holistic Surgical Scene Understanding. (arXiv:2212.04582v3 [cs.CV] UPDATED)\nAbstract: Most benchmarks for studying surgical interventions focus on a specific challenge instead of leveraging the intrinsic complementarity among different tasks. In this work, we present a new experimental framework towards holistic surgical scene understanding. First, we introduce the Phase, Step, Instrument, and Atomic Visual Action recognition (PSI-AVA) Dataset. PSI-AVA includes annotations for both long-term (Phase and Step recognition) and short-term reasoning (Instrument detection and novel Atomic Action recognition) in robot-assisted radical prostatectomy videos. Second, we present Transformers for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong baseline for surgical scene understanding. TAPIR leverages our dataset's multi-level annotations as it benefits from the learned representation on the instrument detection task to improve its classification capacity. Our experimental results in both PSI-AVA and other publicly available databases demonstrate the adequacy o",
    "path": "papers/22/12/2212.04582.json",
    "total_tokens": 926,
    "translated_title": "实现对手术场景的整体理解：针对机器人辅助下的前列腺癌根治术视频",
    "translated_abstract": "大多数用于研究手术干预的基准测试都集中在特定挑战上，而忽略了不同任务间内在的互补性。本文提出了一个新的实验框架，以实现对手术场景的整体理解。首先，我们介绍了Phase、Step、Instrument和Atomic Visual Action（PSI-AVA）数据集。PSI-AVA在机器人辅助下的前列腺癌根治术视频中，对长期（阶段和步骤的识别）和短期推理（器械检测和新型原子动作识别）进行注释。其次，我们提出了TAPIR，即Transformers for Action，Phase，Instrument和Steps Recognition，作为手术场景理解的强基准。TAPIR利用我们数据集的多级注释，通过器械检测任务上学习的表示方式，提高了其分类能力。我们在PSI-AVA和其他公开数据库中的实验结果证明，我们提出的整体手术场景理解框架是充分有效的。",
    "tldr": "本研究提出了一个新实验框架，PSI-AVA数据集，以实现对机器人辅助下的前列腺癌根治术视频的整体理解。同时，我们的实验结果证明了基于TAPIR的手术场景理解框架的有效性。",
    "en_tdlr": "This paper proposes a new experimental framework, PSI-AVA dataset, for achieving holistic surgical scene understanding in robot-assisted radical prostatectomy videos. Experimental results demonstrate the effectiveness of the TAPIR-based framework for surgical scene understanding."
}