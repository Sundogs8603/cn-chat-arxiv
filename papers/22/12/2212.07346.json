{
    "title": "Learning useful representations for shifting tasks and distributions. (arXiv:2212.07346v3 [cs.LG] UPDATED)",
    "abstract": "Does the dominant approach to learn representations (as a side effect of optimizing an expected cost for a single training distribution) remain a good approach when we are dealing with multiple distributions? Our thesis is that such scenarios are better served by representations that are richer than those obtained with a single optimization episode. We support this thesis with simple theoretical arguments and with experiments utilizing an apparently na\\\"{\\i}ve ensembling technique: concatenating the representations obtained from multiple training episodes using the same data, model, algorithm, and hyper-parameters, but different random seeds. These independently trained networks perform similarly. Yet, in a number of scenarios involving new distributions, the concatenated representation performs substantially better than an equivalently sized network trained with a single training run. This proves that the representations constructed by multiple training episodes are in fact different.",
    "link": "http://arxiv.org/abs/2212.07346",
    "context": "Title: Learning useful representations for shifting tasks and distributions. (arXiv:2212.07346v3 [cs.LG] UPDATED)\nAbstract: Does the dominant approach to learn representations (as a side effect of optimizing an expected cost for a single training distribution) remain a good approach when we are dealing with multiple distributions? Our thesis is that such scenarios are better served by representations that are richer than those obtained with a single optimization episode. We support this thesis with simple theoretical arguments and with experiments utilizing an apparently na\\\"{\\i}ve ensembling technique: concatenating the representations obtained from multiple training episodes using the same data, model, algorithm, and hyper-parameters, but different random seeds. These independently trained networks perform similarly. Yet, in a number of scenarios involving new distributions, the concatenated representation performs substantially better than an equivalently sized network trained with a single training run. This proves that the representations constructed by multiple training episodes are in fact different.",
    "path": "papers/22/12/2212.07346.json",
    "total_tokens": 765,
    "translated_title": "学习针对不同任务和分布的有用表示",
    "translated_abstract": "当我们处理多个分布时，以优化单个训练分布的预期成本为副作用学习表示的主导方法是否仍然是一个好方法？我们的论点是，在这种情况下，比单个优化过程获得的更丰富的表示更好。我们用简单的理论论证和利用一个表面上天真的集成技术进行的实验来支持这个论点：将从多个训练过程获得的表示连接起来，这些训练过程使用相同的数据、模型、算法和超参数，但使用不同的随机种子进行训练。这些独立训练的网络表现相似。然而，在涉及新分布的一些情况下，连接的表示比用单个训练运行训练的等效大小的网络表现出色。这证明了多个训练过程构建的表示实际上是不同的。",
    "tldr": "处理多个分布时，通过连接多个训练过程得到的表示比单个训练过程得到的表示更好。"
}