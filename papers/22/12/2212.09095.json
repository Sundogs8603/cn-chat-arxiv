{
    "title": "Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale. (arXiv:2212.09095v2 [cs.CL] UPDATED)",
    "abstract": "Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: $\\sim$70% of attention heads and $\\sim$20% of feed forward networks can be removed with minimal decline in task performance. We find substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, prefix matching and copying. These in",
    "link": "http://arxiv.org/abs/2212.09095",
    "context": "Title: Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale. (arXiv:2212.09095v2 [cs.CL] UPDATED)\nAbstract: Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion parameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we find this is indeed the case: $\\sim$70% of attention heads and $\\sim$20% of feed forward networks can be removed with minimal decline in task performance. We find substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, finding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, prefix matching and copying. These in",
    "path": "papers/22/12/2212.09095.json",
    "total_tokens": 970,
    "translated_title": "重新思考上下文学习中规模的作用: 基于可解释性的660亿尺度案例研究",
    "translated_abstract": "研究表明，通过上下文学习范式，语言模型在规模增加时在各种任务上表现更好。本文通过使用一个660亿参数的语言模型（OPT-66B）在14个不同的下游任务中进行研究，探讨了大型语言模型在上下文学习执行任务的能力是否均匀分布在其所有的组件上。结果发现，约70%的注意力头和约20%的前馈网路可以移除而任务表现仅有轻微下降。在不同任务和上下文示例数量中，我们发现对上下文学习不重要的注意力头的集合存在较大的重叠。同时，我们通过一种任务无关的方式来验证我们的假设，发现OPT-66B中的一小部分注意力头在执行与上下文学习相关的基础归纳操作（即前缀匹配和复制）方面具有高效的能力。",
    "tldr": "本文通过使用一个660亿参数的语言模型，在多个任务中发现了上下文学习能力并不均匀分布在其各个组件上。通过移除约70%的注意力头和约20%的前馈网络，任务执行表现仅有轻微下降。此外，在OPT-66B中，存在一小部分注意力头对于上下文学习中的基础归纳操作具有高效能力。",
    "en_tdlr": "This paper investigates the uneven distribution of in-context learning abilities across components in a large language model and finds that a significant percentage of attention heads and feed forward networks can be removed with minimal decline in task performance. Additionally, a small set of attention heads in OPT-66B show efficient performance in primitive induction operations associated with in-context learning."
}