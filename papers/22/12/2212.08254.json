{
    "title": "RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers. (arXiv:2212.08254v2 [cs.CV] UPDATED)",
    "abstract": "Post-training quantization (PTQ), which only requires a tiny dataset for calibration without end-to-end retraining, is a light and practical model compression technique. Recently, several PTQ schemes for vision transformers (ViTs) have been presented; unfortunately, they typically suffer from non-trivial accuracy degradation, especially in low-bit cases. In this paper, we propose RepQ-ViT, a novel PTQ framework for ViTs based on quantization scale reparameterization, to address the above issues. RepQ-ViT decouples the quantization and inference processes, where the former employs complex quantizers and the latter employs scale-reparameterized simplified quantizers. This ensures both accurate quantization and efficient inference, which distinguishes it from existing approaches that sacrifice quantization performance to meet the target hardware. More specifically, we focus on two components with extreme distributions: post-LayerNorm activations with severe inter-channel variation and pos",
    "link": "http://arxiv.org/abs/2212.08254",
    "context": "Title: RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers. (arXiv:2212.08254v2 [cs.CV] UPDATED)\nAbstract: Post-training quantization (PTQ), which only requires a tiny dataset for calibration without end-to-end retraining, is a light and practical model compression technique. Recently, several PTQ schemes for vision transformers (ViTs) have been presented; unfortunately, they typically suffer from non-trivial accuracy degradation, especially in low-bit cases. In this paper, we propose RepQ-ViT, a novel PTQ framework for ViTs based on quantization scale reparameterization, to address the above issues. RepQ-ViT decouples the quantization and inference processes, where the former employs complex quantizers and the latter employs scale-reparameterized simplified quantizers. This ensures both accurate quantization and efficient inference, which distinguishes it from existing approaches that sacrifice quantization performance to meet the target hardware. More specifically, we focus on two components with extreme distributions: post-LayerNorm activations with severe inter-channel variation and pos",
    "path": "papers/22/12/2212.08254.json",
    "total_tokens": 908,
    "translated_title": "RepQ-ViT：基于规模重参数化实现对Vision Transformers的后训练量化",
    "translated_abstract": "后训练量化(PTQ)是一种轻量级且实用的模型压缩技术，只需要一个小型数据集进行校准，无需进行端到端的重新训练。最近，针对Vision Transformers (ViTs) 提出了几种PTQ方案；然而，它们通常会在低位情况下出现非常严重的准确性下降。本文提出了一种基于量化规模重参数化的ViTs PTQ框架——RepQ-ViT，以解决上述问题。RepQ-ViT将量化和推断过程分开，前者采用复杂的量化器，后者采用具有规模重参数化的简化量化器。这保证了准确量化和高效推断，与现有方法不同，后者为了满足目标硬件而牺牲了量化性能。更具体地说，我们关注两个具有极端分布的组件：具有严重通道间变异的LayerNorm后激活和pos。",
    "tldr": "本论文提出了一种基于规模重参数化的后训练量化框架RepQ-ViT，用于解决Vision Transformers在低位情况下的准确性下降问题，通过将量化和推断分开处理，实现了准确量化和高效推断。",
    "en_tdlr": "This paper proposes RepQ-ViT, a post-training quantization framework based on scale reparameterization, to address the issue of accuracy degradation in low-bit cases for Vision Transformers. By separating the quantization and inference processes, it achieves accurate quantization and efficient inference."
}