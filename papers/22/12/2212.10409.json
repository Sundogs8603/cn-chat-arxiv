{
    "title": "ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations. (arXiv:2212.10409v2 [cs.CL] UPDATED)",
    "abstract": "Context is everything, even in commonsense moral reasoning. Changing contexts can flip the moral judgment of an action; \"Lying to a friend\" is wrong in general, but may be morally acceptable if it is intended to protect their life.  We present ClarifyDelphi, an interactive system that learns to ask clarification questions (e.g., why did you lie to your friend?) in order to elicit additional salient contexts of a social or moral situation. We posit that questions whose potential answers lead to diverging moral judgments are the most informative. Thus, we propose a reinforcement learning framework with a defeasibility reward that aims to maximize the divergence between moral judgments of hypothetical answers to a question. Human evaluation demonstrates that our system generates more relevant, informative and defeasible questions compared to competitive baselines. Our work is ultimately inspired by studies in cognitive science that have investigated the flexibility in moral cognition (i.e",
    "link": "http://arxiv.org/abs/2212.10409",
    "context": "Title: ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations. (arXiv:2212.10409v2 [cs.CL] UPDATED)\nAbstract: Context is everything, even in commonsense moral reasoning. Changing contexts can flip the moral judgment of an action; \"Lying to a friend\" is wrong in general, but may be morally acceptable if it is intended to protect their life.  We present ClarifyDelphi, an interactive system that learns to ask clarification questions (e.g., why did you lie to your friend?) in order to elicit additional salient contexts of a social or moral situation. We posit that questions whose potential answers lead to diverging moral judgments are the most informative. Thus, we propose a reinforcement learning framework with a defeasibility reward that aims to maximize the divergence between moral judgments of hypothetical answers to a question. Human evaluation demonstrates that our system generates more relevant, informative and defeasible questions compared to competitive baselines. Our work is ultimately inspired by studies in cognitive science that have investigated the flexibility in moral cognition (i.e",
    "path": "papers/22/12/2212.10409.json",
    "total_tokens": 986,
    "translated_title": "ClarifyDelphi：针对社会和道德情境的强化澄清问题与优先考虑对抗的奖励",
    "translated_abstract": "上下文的重要性不言而喻，甚至在常识道德推理中也是如此。改变上下文可能会颠倒一项行为的道德判断;“对朋友撒谎”在一般情况下是不对的，但如果旨在保护他们的生命，就可能是道德上可接受的。我们提出了ClarifyDelphi，一个交互式系统，它学习提出澄清问题（例如，你为什么要对你的朋友撒谎？）以获取社会或道德情境的其他重要信息。我们认为，其潜在答案导致道德判断有所分歧的问题是最有信息价值的。因此，我们提出了一种增强学习框架，该框架具有对抗性奖励，旨在最大化回答问题时的道德判断分歧。人类评估表明，与竞争基线相比，我们的系统生成的问题更相关、更有信息价值和更具优胜性。我们的工作最终受到认知科学研究的启发，该研究调查了道德认知的灵活性（即能够纳入新的上下文信息并相应地修改道德判断）。",
    "tldr": "ClarifyDelphi是一个交互式系统，能够针对社会或道德情境提出最有信息价值的问题，并通过奖励机制最大化回答问题时的道德判断分歧。",
    "en_tdlr": "ClarifyDelphi is an interactive system that generates informative and divergent clarification questions using a reinforcement learning framework with a defeasibility reward, ultimately inspired by studies in cognitive science on the flexibility of moral cognition."
}