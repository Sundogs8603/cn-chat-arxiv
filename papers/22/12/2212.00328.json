{
    "title": "Differentially Private Learning with Per-Sample Adaptive Clipping. (arXiv:2212.00328v3 [cs.LG] UPDATED)",
    "abstract": "Privacy in AI remains a topic that draws attention from researchers and the general public in recent years. As one way to implement privacy-preserving AI, differentially private learning is a framework that enables AI models to use differential privacy (DP). To achieve DP in the learning process, existing algorithms typically limit the magnitude of gradients with a constant clipping, which requires carefully tuned due to its significant impact on model performance. As a solution to this issue, latest works NSGD and Auto-S innovatively propose to use normalization instead of clipping to avoid hyperparameter tuning. However, normalization-based approaches like NSGD and Auto-S rely on a monotonic weight function, which imposes excessive weight on small gradient samples and introduces extra deviation to the update. In this paper, we propose a Differentially Private Per-Sample Adaptive Clipping (DP-PSAC) algorithm based on a non-monotonic adaptive weight function, which guarantees privacy w",
    "link": "http://arxiv.org/abs/2212.00328",
    "context": "Title: Differentially Private Learning with Per-Sample Adaptive Clipping. (arXiv:2212.00328v3 [cs.LG] UPDATED)\nAbstract: Privacy in AI remains a topic that draws attention from researchers and the general public in recent years. As one way to implement privacy-preserving AI, differentially private learning is a framework that enables AI models to use differential privacy (DP). To achieve DP in the learning process, existing algorithms typically limit the magnitude of gradients with a constant clipping, which requires carefully tuned due to its significant impact on model performance. As a solution to this issue, latest works NSGD and Auto-S innovatively propose to use normalization instead of clipping to avoid hyperparameter tuning. However, normalization-based approaches like NSGD and Auto-S rely on a monotonic weight function, which imposes excessive weight on small gradient samples and introduces extra deviation to the update. In this paper, we propose a Differentially Private Per-Sample Adaptive Clipping (DP-PSAC) algorithm based on a non-monotonic adaptive weight function, which guarantees privacy w",
    "path": "papers/22/12/2212.00328.json",
    "total_tokens": 1120,
    "translated_title": "带每个样本自适应裁剪的差分隐私学习",
    "translated_abstract": "近年来，AI中的隐私问题一直是吸引研究者和公众关注的话题。差分隐私学习作为一种实现隐私保护AI的方式，可以使AI模型使用差分隐私。现有算法通常通过常量裁剪来限制梯度幅度以实现学习过程的差分隐私。为了解决裁剪常数对模型性能的巨大影响，最新的作品NSGD和Auto-S提出了使用归一化来替代裁剪的创新方法。然而，NSGD和Auto-S等基于归一化的方法依赖于单调权重函数，对小梯度样本施加过量权重并引入额外偏差。本文提出了一种基于非单调自适应权重函数的差分隐私每个样本自适应裁剪（DP-PSAC）算法，它保证了隐私而不牺牲模型性能。我们的DP-PSAC算法根据该样本的历史敏感性自适应裁剪每个样本的梯度幅度，可以显著减少裁剪梯度的方差并提高模型的效用。实验结果表明，与现有方法相比，我们的DP-PSAC算法在真实数据集上实现了竞争性的实用性。",
    "tldr": "本文提出了一种差分隐私每个样本自适应裁剪算法DP-PSAC，该算法采用非单调自适应权重函数，根据梯度的历史敏感性自适应裁剪每个样本的梯度幅度，提高了模型的实用性，且保证了隐私。",
    "en_tdlr": "This paper proposes a Differentially Private Per-Sample Adaptive Clipping (DP-PSAC) algorithm, which adaptively clips each sample's gradient magnitude based on its historical sensitivity using a non-monotonic adaptive weight function. Our DP-PSAC algorithm improves model utility while ensuring privacy, and achieves competitive performance compared to existing methods on real-world datasets."
}