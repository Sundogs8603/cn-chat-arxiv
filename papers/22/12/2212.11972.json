{
    "title": "Scalable Adaptive Computation for Iterative Generation. (arXiv:2212.11972v2 [cs.LG] UPDATED)",
    "abstract": "Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Networks (RINs), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the rev",
    "link": "http://arxiv.org/abs/2212.11972",
    "context": "Title: Scalable Adaptive Computation for Iterative Generation. (arXiv:2212.11972v2 [cs.LG] UPDATED)\nAbstract: Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Networks (RINs), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the rev",
    "path": "papers/22/12/2212.11972.json",
    "total_tokens": 1147,
    "translated_title": "可伸缩自适应计算用于迭代生成",
    "translated_abstract": "自然数据是冗余的，但主要的结构仍然在其输入和输出空间上统一切割计算。本文提出了循环接口网络（RINs），这是一种基于注意力的结构，它将其核心计算与数据的维数分离，实现了更可伸缩的高维数据生成的自适应计算。RINs将大部分计算（即全局自注意力）集中在一组潜在标记上，使用交叉注意力在潜在标记和数据标记之间读取和写入（即路由）信息。堆叠RIN模块允许自下而上（从数据到潜在）和自上而下（从潜在到数据）反馈，从而实现更深层和更具表现力的路由。虽然这种路由引入了挑战，但在任务（和路由问题）逐渐变化的循环计算设置中，这个问题就不那么棘手了，比如扩散模型的迭代生成。我们展示了如何通过在每次修订过程的前向传递中调节潜在标记，并引入了一种灵活的适配器结构，可以实现高维数据和结构的有效扩展。我们通过生成高质量的图像和多元时间序列展示了RINs的有效性，并表明RINs可以优雅地扩展到具有数十万维度的数据集。",
    "tldr": "本文提出了循环接口网络（RINs）结构，将核心计算与数据维数分离，实现了自适应计算，解决了高维数据生成的可伸缩性问题。这一结构将大部分计算集中在潜在标记上，使用交叉注意力在潜在标记和数据标记之间路由信息，具有很好的生成效果并可扩展到数十万维的数据集上。",
    "en_tdlr": "This paper proposes the Recurrent Interface Networks (RINs) architecture for adaptive computation of high-dimensional data generation, which separates core computation from data dimensionality. RINs focus computation on latent tokens using cross-attention for information routing, and introduce a flexible adapter architecture for efficient scaling. The architecture generates high-quality images and multivariate time-series and scales to datasets with up to hundreds of thousands of dimensions."
}