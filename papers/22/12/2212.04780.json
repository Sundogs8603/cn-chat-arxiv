{
    "title": "Genie: Show Me the Data for Quantization. (arXiv:2212.04780v2 [cs.LG] UPDATED)",
    "abstract": "Zero-shot quantization is a promising approach for developing lightweight deep neural networks when data is inaccessible owing to various reasons, including cost and issues related to privacy. By exploiting the learned parameters ($\\mu$ and $\\sigma$) of batch normalization layers in an FP32-pre-trained model, zero-shot quantization schemes focus on generating synthetic data. Subsequently, they distill knowledge from the pre-trained model (teacher) to the quantized model (student) such that the quantized model can be optimized with the synthetic dataset. However, thus far, zero-shot quantization has primarily been discussed in the context of quantization-aware training methods, which require task-specific losses and long-term optimization as much as retraining. We thus introduce a post-training quantization scheme for zero-shot quantization that produces high-quality quantized networks within a few hours. Furthermore, we propose a framework called \\genie~that generates data suited for q",
    "link": "http://arxiv.org/abs/2212.04780",
    "context": "Title: Genie: Show Me the Data for Quantization. (arXiv:2212.04780v2 [cs.LG] UPDATED)\nAbstract: Zero-shot quantization is a promising approach for developing lightweight deep neural networks when data is inaccessible owing to various reasons, including cost and issues related to privacy. By exploiting the learned parameters ($\\mu$ and $\\sigma$) of batch normalization layers in an FP32-pre-trained model, zero-shot quantization schemes focus on generating synthetic data. Subsequently, they distill knowledge from the pre-trained model (teacher) to the quantized model (student) such that the quantized model can be optimized with the synthetic dataset. However, thus far, zero-shot quantization has primarily been discussed in the context of quantization-aware training methods, which require task-specific losses and long-term optimization as much as retraining. We thus introduce a post-training quantization scheme for zero-shot quantization that produces high-quality quantized networks within a few hours. Furthermore, we propose a framework called \\genie~that generates data suited for q",
    "path": "papers/22/12/2212.04780.json",
    "total_tokens": 859,
    "translated_title": "Genie: 展示我量化的数据",
    "translated_abstract": "当数据因为各种原因（包括成本和隐私问题）无法访问时，零样本量化是开发轻量级深度神经网络的一种有前途的方法。通过利用FP32预训练模型中批归一化层的学习参数（$\\mu$和$\\sigma$），零样本量化方案专注于生成合成数据。随后，它们从预训练模型（教师）中提取知识，传递给量化模型（学生），使得量化模型可以使用合成数据集进行优化。然而，到目前为止，零样本量化主要在量化感知训练方法的上下文中讨论，这些方法需要任务特定的损失和长期的优化，就像需要重新训练一样。因此，我们引入了一个后训练量化方案，用于零样本量化，可以在几小时内生成高质量的量化网络。此外，我们提出了一个名为“Genie”的框架，用于生成适合零样本量化的数据。",
    "tldr": "Genie提出了一个后训练量化方案，用于开发轻量级深度神经网络，并提出了一个用于生成适合零样本量化的数据的框架。",
    "en_tdlr": "Genie proposes a post-training quantization scheme for developing lightweight deep neural networks with zero-shot quantization, and introduces a framework for generating data suited for zero-shot quantization."
}