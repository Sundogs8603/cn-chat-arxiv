{
    "title": "Character-Aware Models Improve Visual Text Rendering. (arXiv:2212.10562v2 [cs.CL] UPDATED)",
    "abstract": "Current image generation models struggle to reliably produce well-formed visual text. In this paper, we investigate a key contributing factor: popular text-to-image models lack character-level input features, making it much harder to predict a word's visual makeup as a series of glyphs. To quantify this effect, we conduct a series of experiments comparing character-aware vs. character-blind text encoders. In the text-only domain, we find that character-aware models provide large gains on a novel spelling task (WikiSpell). Applying our learnings to the visual domain, we train a suite of image generation models, and show that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (our DrawText benchmark). Our models set a much higher state-of-the-art on visual spelling, with 30+ point accuracy gains over competitors on rare words, despite training on far fewer examples.",
    "link": "http://arxiv.org/abs/2212.10562",
    "context": "Title: Character-Aware Models Improve Visual Text Rendering. (arXiv:2212.10562v2 [cs.CL] UPDATED)\nAbstract: Current image generation models struggle to reliably produce well-formed visual text. In this paper, we investigate a key contributing factor: popular text-to-image models lack character-level input features, making it much harder to predict a word's visual makeup as a series of glyphs. To quantify this effect, we conduct a series of experiments comparing character-aware vs. character-blind text encoders. In the text-only domain, we find that character-aware models provide large gains on a novel spelling task (WikiSpell). Applying our learnings to the visual domain, we train a suite of image generation models, and show that character-aware variants outperform their character-blind counterparts across a range of novel text rendering tasks (our DrawText benchmark). Our models set a much higher state-of-the-art on visual spelling, with 30+ point accuracy gains over competitors on rare words, despite training on far fewer examples.",
    "path": "papers/22/12/2212.10562.json",
    "total_tokens": 837,
    "translated_title": "基于字符的模型提升了视觉文本渲染效果",
    "translated_abstract": "当前的图像生成模型在生成良好排版的视觉文本方面存在较大的问题。本文考察了其中一个关键因素：目前主流的文本到图像模型缺乏字符级别的输入特征，使得以字体为基础的视觉文本呈现更加困难。通过一系列的实验比较，我们发现基于字符的文本编码模型对于新颖的拼写任务（WikiSpell）提供了很大的帮助。将这些结果应用于视觉领域后，我们训练了一套图像生成模型，并展示了基于字符的变体在一系列新颖的文本渲染任务（即DrawText benchmark）上表现优于其无字符输入的对应模型。我们的模型在视觉拼写方面取得了远高于同领域对手的成果，在少量样本的情况下提高了超过30个百分点的准确率。",
    "tldr": "本研究证实，以字体为基础的字符级文本编码模型可以显著提高图像生成的视觉文本排版质量，达到更高的拼写准确率和更好的效果。",
    "en_tdlr": "This study confirms that character-level text encoding models based on font can significantly improve the quality of visual text rendering in image generation, achieve higher spelling accuracy and better results."
}