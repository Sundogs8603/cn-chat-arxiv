{
    "title": "Second-order optimization with lazy Hessians. (arXiv:2212.00781v3 [math.OC] UPDATED)",
    "abstract": "We analyze Newton's method with lazy Hessian updates for solving general possibly non-convex optimization problems. We propose to reuse a previously seen Hessian for several iterations while computing new gradients at each step of the method. This significantly reduces the overall arithmetical complexity of second-order optimization schemes. By using the cubic regularization technique, we establish fast global convergence of our method to a second-order stationary point, while the Hessian does not need to be updated each iteration. For convex problems, we justify global and local superlinear rates for lazy Newton steps with quadratic regularization, which is easier to compute. The optimal frequency for updating the Hessian is once every $d$ iterations, where $d$ is the dimension of the problem. This provably improves the total arithmetical complexity of second-order algorithms by a factor $\\sqrt{d}$.",
    "link": "http://arxiv.org/abs/2212.00781",
    "context": "Title: Second-order optimization with lazy Hessians. (arXiv:2212.00781v3 [math.OC] UPDATED)\nAbstract: We analyze Newton's method with lazy Hessian updates for solving general possibly non-convex optimization problems. We propose to reuse a previously seen Hessian for several iterations while computing new gradients at each step of the method. This significantly reduces the overall arithmetical complexity of second-order optimization schemes. By using the cubic regularization technique, we establish fast global convergence of our method to a second-order stationary point, while the Hessian does not need to be updated each iteration. For convex problems, we justify global and local superlinear rates for lazy Newton steps with quadratic regularization, which is easier to compute. The optimal frequency for updating the Hessian is once every $d$ iterations, where $d$ is the dimension of the problem. This provably improves the total arithmetical complexity of second-order algorithms by a factor $\\sqrt{d}$.",
    "path": "papers/22/12/2212.00781.json",
    "total_tokens": 822,
    "translated_title": "懒惰Hessian的二阶优化方法",
    "translated_abstract": "我们分析了用于解决一般可能是非凸优化问题的懒惰Hessian更新的牛顿法。我们建议在计算方法的每个步骤中重复使用先前看到的Hessian，同时计算新的梯度。这显着降低了二阶优化方案的总算术复杂度。通过使用立方正则化技术，我们建立了我们的方法到二阶稳定点的快速全局收敛性，而Hessian在每个迭代中不需要更新。对于凸问题，我们证明了具有二次正则化的懒惰牛顿步骤具有全局和局部超线性速率，这更容易计算。更新Hessian的最佳频率是每$d$个迭代一次，其中$d$是问题的维度。这可以证明将二阶算法的总算术复杂度提高了$\\sqrt{d}$倍。",
    "tldr": "本论文提出了一种用于解决可能为非凸问题的二阶优化算法，使用懒惰Hessian更新和重用先前看到的Hessian，可显著降低总算术复杂度，并提高了效率。",
    "en_tdlr": "This paper proposes a second-order optimization algorithm for solving possibly non-convex problems, which uses lazy Hessian updates and reuses previously seen Hessians to significantly reduce arithmetic complexity and improve efficiency."
}