{
    "title": "A Dynamics Theory of Implicit Regularization in Deep Low-Rank Matrix Factorization. (arXiv:2212.14150v2 [cs.LG] UPDATED)",
    "abstract": "Implicit regularization is an important way to interpret neural networks. Recent theory starts to explain implicit regularization with the model of deep matrix factorization (DMF) and analyze the trajectory of discrete gradient dynamics in the optimization process. These discrete gradient dynamics are relatively small but not infinitesimal, thus fitting well with the practical implementation of neural networks. Currently, discrete gradient dynamics analysis has been successfully applied to shallow networks but encounters the difficulty of complex computation for deep networks. In this work, we introduce another discrete gradient dynamics approach to explain implicit regularization, i.e. landscape analysis. It mainly focuses on gradient regions, such as saddle points and local minima. We theoretically establish the connection between saddle point escaping (SPE) stages and the matrix rank in DMF. We prove that, for a rank-R matrix reconstruction, DMF will converge to a second-order criti",
    "link": "http://arxiv.org/abs/2212.14150",
    "context": "Title: A Dynamics Theory of Implicit Regularization in Deep Low-Rank Matrix Factorization. (arXiv:2212.14150v2 [cs.LG] UPDATED)\nAbstract: Implicit regularization is an important way to interpret neural networks. Recent theory starts to explain implicit regularization with the model of deep matrix factorization (DMF) and analyze the trajectory of discrete gradient dynamics in the optimization process. These discrete gradient dynamics are relatively small but not infinitesimal, thus fitting well with the practical implementation of neural networks. Currently, discrete gradient dynamics analysis has been successfully applied to shallow networks but encounters the difficulty of complex computation for deep networks. In this work, we introduce another discrete gradient dynamics approach to explain implicit regularization, i.e. landscape analysis. It mainly focuses on gradient regions, such as saddle points and local minima. We theoretically establish the connection between saddle point escaping (SPE) stages and the matrix rank in DMF. We prove that, for a rank-R matrix reconstruction, DMF will converge to a second-order criti",
    "path": "papers/22/12/2212.14150.json",
    "total_tokens": 888,
    "translated_title": "深度低秩矩阵分解中隐式正则化的动力学理论",
    "translated_abstract": "隐式正则化是解释神经网络的重要方法。最近的理论开始用深度矩阵分解模型(DMF)来解释隐式正则化，并分析了优化过程中离散梯度动力学的轨迹。这些离散梯度动力学相对较小但不是无穷小，因此很好地适应了神经网络的实际实现。目前，离散梯度动力学分析已成功应用于浅层网络，但在深层网络中遇到了复杂计算的困难。在这项工作中，我们引入了另一种解释隐式正则化的离散梯度动力学方法，即景观分析。它主要关注梯度区域，如鞍点和局部最小值。我们在理论上建立了鞍点逃逸(SPE)阶段和DMF中矩阵秩之间的联系。我们证明，对于秩为R的矩阵重构，DMF将收敛到二阶临界点。",
    "tldr": "本文提出了一种新的解释神经网络隐式正则化的方法，通过离散梯度动力学和景观分析揭示了深度低秩矩阵分解中的隐式正则化机制。",
    "en_tdlr": "This paper introduces a new approach to explain implicit regularization in neural networks, revealing the implicit regularization mechanism in deep low-rank matrix factorization through discrete gradient dynamics and landscape analysis."
}