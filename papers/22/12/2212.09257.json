{
    "title": "PromptBoosting: Black-Box Text Classification with Ten Forward Passes. (arXiv:2212.09257v2 [cs.CL] UPDATED)",
    "abstract": "We describe PromptBoosting, a query-efficient procedure for building a text classifier from a neural language model (LM) without access to the LM's parameters, gradients, or hidden representations. This form of \"black-box\" classifier training has become increasingly important as the cost of training and inference in large-scale LMs grows. But existing black-box LM classifier learning approaches are themselves computationally inefficient, typically specializing LMs to the target task by searching in a large space of (discrete or continuous) prompts using zeroth-order optimization methods. Instead of directly optimizing in prompt space, PromptBoosting obtains a small pool of prompts via a gradient-free approach and then constructs a large pool of weak learners by pairing these prompts with different elements of the LM's output distribution. These weak learners are then ensembled using the AdaBoost algorithm. The entire learning process requires only a small number of forward passes and n",
    "link": "http://arxiv.org/abs/2212.09257",
    "context": "Title: PromptBoosting: Black-Box Text Classification with Ten Forward Passes. (arXiv:2212.09257v2 [cs.CL] UPDATED)\nAbstract: We describe PromptBoosting, a query-efficient procedure for building a text classifier from a neural language model (LM) without access to the LM's parameters, gradients, or hidden representations. This form of \"black-box\" classifier training has become increasingly important as the cost of training and inference in large-scale LMs grows. But existing black-box LM classifier learning approaches are themselves computationally inefficient, typically specializing LMs to the target task by searching in a large space of (discrete or continuous) prompts using zeroth-order optimization methods. Instead of directly optimizing in prompt space, PromptBoosting obtains a small pool of prompts via a gradient-free approach and then constructs a large pool of weak learners by pairing these prompts with different elements of the LM's output distribution. These weak learners are then ensembled using the AdaBoost algorithm. The entire learning process requires only a small number of forward passes and n",
    "path": "papers/22/12/2212.09257.json",
    "total_tokens": 839,
    "translated_title": "PromptBoosting：具有十次前向传递的黑盒文本分类",
    "translated_abstract": "我们描述了PromptBoosting，一种从神经语言模型（LM）构建文本分类器的查询高效过程，其中没有访问LM的参数、梯度或隐藏表示。这种“黑盒”分类器训练形式在大规模LM的训练和推理成本增加时变得越来越重要。但是，现有的黑盒LM分类器学习方法本身计算效率低下，通常使用零阶优化方法在大量的（离散或连续）提示空间中搜索将LM特化到目标任务。PromptBoosting不直接在提示空间进行优化，而是通过无梯度方法获得一小组提示，然后将这些提示与LM输出分布的不同元素配对构建大量弱学习器。然后使用AdaBoost算法对这些弱学习器进行集成。整个学习过程仅需要少量前向传递和n",
    "tldr": "PromptBoosting是一种黑盒文本分类的方法，通过一小组提示和AdaBoost算法将神经语言模型的输出分布构建为大量弱学习器，从而实现了高效的分类器训练过程。"
}