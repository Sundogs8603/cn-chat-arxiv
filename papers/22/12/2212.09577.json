{
    "title": "CiteBench: A benchmark for Scientific Citation Text Generation. (arXiv:2212.09577v2 [cs.CL] UPDATED)",
    "abstract": "Science progresses by incrementally building upon the prior body of knowledge documented in scientific publications. The acceleration of research across many fields makes it hard to stay up-to-date with the recent developments and to summarize the ever-growing body of prior work. To target this issue, the task of citation text generation aims to produce accurate textual summaries given a set of papers-to-cite and the citing paper context. Existing studies in citation text generation are based upon widely diverging task definitions, which makes it hard to study this task systematically. To address this challenge, we propose CiteBench: a benchmark for citation text generation that unifies multiple diverse datasets and enables standardized evaluation of citation text generation models across task designs and domains. Using the new benchmark, we investigate the performance of multiple strong baselines, test their transferability between the datasets, and deliver new insights into the task ",
    "link": "http://arxiv.org/abs/2212.09577",
    "context": "Title: CiteBench: A benchmark for Scientific Citation Text Generation. (arXiv:2212.09577v2 [cs.CL] UPDATED)\nAbstract: Science progresses by incrementally building upon the prior body of knowledge documented in scientific publications. The acceleration of research across many fields makes it hard to stay up-to-date with the recent developments and to summarize the ever-growing body of prior work. To target this issue, the task of citation text generation aims to produce accurate textual summaries given a set of papers-to-cite and the citing paper context. Existing studies in citation text generation are based upon widely diverging task definitions, which makes it hard to study this task systematically. To address this challenge, we propose CiteBench: a benchmark for citation text generation that unifies multiple diverse datasets and enables standardized evaluation of citation text generation models across task designs and domains. Using the new benchmark, we investigate the performance of multiple strong baselines, test their transferability between the datasets, and deliver new insights into the task ",
    "path": "papers/22/12/2212.09577.json",
    "total_tokens": 918,
    "translated_title": "CiteBench：科学引文文本生成基准测试",
    "translated_abstract": "科学通过逐步建立在科学出版物中记录的先前知识体系的基础上提高。许多领域的研究加速使得跟上最新发展并总结不断增长的先前工作的困难。为了解决这个问题，引文文本生成的任务旨在在给定需要引用的论文和引用论文的情境的情况下生成准确的文本摘要。现有的引文文本生成研究基于广泛分歧的任务定义，这使得系统地研究这个任务变得困难。为了应对这个挑战，我们提出了CiteBench：一个引文文本生成基准测试，它统一了多个不同的数据集，使得可以对任务设计和领域中的引文文本生成模型进行标准化评估。使用这个新的基准测试，我们调查了多个强基准的性能，测试了它们在数据集之间的可转移性，并提供了对任务的新见解。",
    "tldr": "CiteBench是一个科学引文文本生成基准测试，旨在解决研究加速导致的解读和总结先前工作的困难。该基准测试可以进行标准化评估，研究不同任务设计和领域的引文文本生成模型。对多个基线模型的大量测试发现了新的见解。",
    "en_tdlr": "CiteBench is a scientific citation text generation benchmark that aims to address the difficulty in interpreting and summarizing prior work caused by research acceleration. The benchmark enables standardized evaluation of citation text generation models across task designs and domains. Multiple strong baselines were tested, leading to new insights into the task."
}