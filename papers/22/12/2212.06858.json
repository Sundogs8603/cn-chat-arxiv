{
    "title": "LidarCLIP or: How I Learned to Talk to Point Clouds. (arXiv:2212.06858v3 [cs.CV] UPDATED)",
    "abstract": "Research connecting text and images has recently seen several breakthroughs, with models like CLIP, DALL-E 2, and Stable Diffusion. However, the connection between text and other visual modalities, such as lidar data, has received less attention, prohibited by the lack of text-lidar datasets. In this work, we propose LidarCLIP, a mapping from automotive point clouds to a pre-existing CLIP embedding space. Using image-lidar pairs, we supervise a point cloud encoder with the image CLIP embeddings, effectively relating text and lidar data with the image domain as an intermediary. We show the effectiveness of LidarCLIP by demonstrating that lidar-based retrieval is generally on par with image-based retrieval, but with complementary strengths and weaknesses. By combining image and lidar features, we improve upon both single-modality methods and enable a targeted search for challenging detection scenarios under adverse sensor conditions. We also explore zero-shot classification and show that",
    "link": "http://arxiv.org/abs/2212.06858",
    "context": "Title: LidarCLIP or: How I Learned to Talk to Point Clouds. (arXiv:2212.06858v3 [cs.CV] UPDATED)\nAbstract: Research connecting text and images has recently seen several breakthroughs, with models like CLIP, DALL-E 2, and Stable Diffusion. However, the connection between text and other visual modalities, such as lidar data, has received less attention, prohibited by the lack of text-lidar datasets. In this work, we propose LidarCLIP, a mapping from automotive point clouds to a pre-existing CLIP embedding space. Using image-lidar pairs, we supervise a point cloud encoder with the image CLIP embeddings, effectively relating text and lidar data with the image domain as an intermediary. We show the effectiveness of LidarCLIP by demonstrating that lidar-based retrieval is generally on par with image-based retrieval, but with complementary strengths and weaknesses. By combining image and lidar features, we improve upon both single-modality methods and enable a targeted search for challenging detection scenarios under adverse sensor conditions. We also explore zero-shot classification and show that",
    "path": "papers/22/12/2212.06858.json",
    "total_tokens": 922,
    "translated_title": "LidarCLIP：如何与点云交互",
    "translated_abstract": "最近，将文本和图像联系起来的研究取得了几项重大突破，例如 CLIP、DALL-E 2 和 Stable Diffusion 等模型。然而，文本与其他视觉模态(如激光雷达数据)之间的联系却没有得到足够的关注，这是由于缺少文本-激光雷达数据集。在这项工作中，我们提出了 LidarCLIP，它将汽车点云映射到预先存在的 CLIP 嵌入空间中。使用图像-点云对，我们使用图像 CLIP 嵌入监督点云编码器，有效地将文本和激光雷达数据与图像域作为中介联系起来。我们展示了 LidarCLIP 的有效性，通过演示基于激光雷达的检索一般与基于图像的检索相当，但具有互补的优点和缺点。通过结合图像和激光雷达特征，我们改进了单模态方法，实现了针对在不良传感器条件下具有挑战性的检测场景的有针对性搜索。我们还探讨了零样本分类，并展示了...",
    "tldr": "LidarCLIP可以将文本和激光雷达数据联系起来，达到有效的检索效果，而且能够在不良传感器条件下实现对具有挑战性的检测场景的有针对性搜索。"
}