{
    "title": "Improving Uncertainty Quantification of Variance Networks by Tree-Structured Learning. (arXiv:2212.12658v2 [cs.LG] UPDATED)",
    "abstract": "To improve the uncertainty quantification of variance networks, we propose a novel tree-structured local neural network model that partitions the feature space into multiple regions based on uncertainty heterogeneity. A tree is built upon giving the training data, whose leaf nodes represent different regions where region-specific neural networks are trained to predict both the mean and the variance for quantifying uncertainty. The proposed Uncertainty-Splitting Neural Regression Tree (USNRT) employs novel splitting criteria. At each node, a neural network is trained on the full data first, and a statistical test for the residuals is conducted to find the best split, corresponding to the two sub-regions with the most significant uncertainty heterogeneity between them. USNRT is computationally friendly because very few leaf nodes are sufficient and pruning is unnecessary. Furthermore, an ensemble version can be easily constructed to estimate the total uncertainty including the aleatory a",
    "link": "http://arxiv.org/abs/2212.12658",
    "context": "Title: Improving Uncertainty Quantification of Variance Networks by Tree-Structured Learning. (arXiv:2212.12658v2 [cs.LG] UPDATED)\nAbstract: To improve the uncertainty quantification of variance networks, we propose a novel tree-structured local neural network model that partitions the feature space into multiple regions based on uncertainty heterogeneity. A tree is built upon giving the training data, whose leaf nodes represent different regions where region-specific neural networks are trained to predict both the mean and the variance for quantifying uncertainty. The proposed Uncertainty-Splitting Neural Regression Tree (USNRT) employs novel splitting criteria. At each node, a neural network is trained on the full data first, and a statistical test for the residuals is conducted to find the best split, corresponding to the two sub-regions with the most significant uncertainty heterogeneity between them. USNRT is computationally friendly because very few leaf nodes are sufficient and pruning is unnecessary. Furthermore, an ensemble version can be easily constructed to estimate the total uncertainty including the aleatory a",
    "path": "papers/22/12/2212.12658.json",
    "total_tokens": 972,
    "translated_title": "提高方差网络不确定性量化的树状结构学习方法",
    "translated_abstract": "为了提高方差网络的不确定性量化，我们提出了一种新颖的树状结构局部神经网络模型，该模型根据不确定性的异质性将特征空间划分为多个区域。根据训练数据，建立一棵树，其叶节点代表不同的区域，在这些区域特定的神经网络中进行训练，以预测均值和方差以量化不确定性。所提出的不确定性分裂神经回归树 (USNRT)采用了新颖的分裂准则。在每个节点上，首先对全数据进行神经网络训练，然后对残差进行统计检验，找到最佳分裂，对应具有最显著不确定性异质性的两个子区域。USNRT在计算上友好，只需很少的叶节点即可满足要求，无需进行修剪。此外，还可以轻松构建集合版本以估计包括 aleatory 的总不确定性。",
    "tldr": "提出了一种改进方差网络不确定性量化的方法，通过树状结构学习将特征空间分割为多个区域，并使用区域特定的神经网络预测均值和方差来量化不确定性。该方法利用新的分裂准则，在计算上友好且不需要修剪，还可以构建集合版本来估计总不确定性。"
}