{
    "title": "TIDE: Time Derivative Diffusion for Deep Learning on Graphs. (arXiv:2212.02483v2 [cs.LG] UPDATED)",
    "abstract": "A prominent paradigm for graph neural networks is based on the message-passing framework. In this framework, information communication is realized only between neighboring nodes. The challenge of approaches that use this paradigm is to ensure efficient and accurate long-distance communication between nodes, as deep convolutional networks are prone to oversmoothing. In this paper, we present a novel method based on time derivative graph diffusion (TIDE) to overcome these structural limitations of the message-passing framework. Our approach allows for optimizing the spatial extent of diffusion across various tasks and network channels, thus enabling medium and long-distance communication efficiently. Furthermore, we show that our architecture design also enables local message-passing and thus inherits from the capabilities of local message-passing approaches. We show that on both widely used graph benchmarks and synthetic mesh and graph datasets, the proposed framework outperforms state-",
    "link": "http://arxiv.org/abs/2212.02483",
    "context": "Title: TIDE: Time Derivative Diffusion for Deep Learning on Graphs. (arXiv:2212.02483v2 [cs.LG] UPDATED)\nAbstract: A prominent paradigm for graph neural networks is based on the message-passing framework. In this framework, information communication is realized only between neighboring nodes. The challenge of approaches that use this paradigm is to ensure efficient and accurate long-distance communication between nodes, as deep convolutional networks are prone to oversmoothing. In this paper, we present a novel method based on time derivative graph diffusion (TIDE) to overcome these structural limitations of the message-passing framework. Our approach allows for optimizing the spatial extent of diffusion across various tasks and network channels, thus enabling medium and long-distance communication efficiently. Furthermore, we show that our architecture design also enables local message-passing and thus inherits from the capabilities of local message-passing approaches. We show that on both widely used graph benchmarks and synthetic mesh and graph datasets, the proposed framework outperforms state-",
    "path": "papers/22/12/2212.02483.json",
    "total_tokens": 877,
    "translated_title": "TIDE：用于图上深度学习的时间导数扩散",
    "translated_abstract": "图神经网络的一个重要范式是基于消息传递框架的。在这个框架中，信息通信仅在相邻节点之间实现。使用这种范式的方法的挑战是确保节点之间的高效和准确的长距离通信，因为深度卷积网络容易产生过度平滑。在本文中，我们提出了一种基于时间导数图扩散（TIDE）的新方法，以克服消息传递框架的这些结构限制。我们的方法允许优化扩散的空间范围，适用于各种任务和网络通道，从而实现中长距离通信的高效率。此外，我们还展示了我们的架构设计也使本地消息传递成为可能，从而继承了本地消息传递方法的能力。我们在广泛使用的图基准和合成网格和图数据集上展示，所提出的框架优于\tstate-of-the-art 方法。",
    "tldr": "本文提出了一种新方法 TIDE，通过时间导数图扩散克服了图神经网络中消息传递框架的结构限制，实现了高效地中长距离通信，并在图神经网络任务中达到了 state-of-the-art 的性能表现。",
    "en_tdlr": "This paper proposes a new method TIDE which overcomes the structural limitations of the message-passing framework in graph neural networks through time derivative graph diffusion, achieving efficient medium and long-distance communication and state-of-the-art performance on graph neural network tasks."
}