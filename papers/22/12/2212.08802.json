{
    "title": "Relational Sentence Embedding for Flexible Semantic Matching. (arXiv:2212.08802v2 [cs.CL] UPDATED)",
    "abstract": "We present Relational Sentence Embedding (RSE), a new paradigm to further discover the potential of sentence embeddings. Prior work mainly models the similarity between sentences based on their embedding distance. Because of the complex semantic meanings conveyed, sentence pairs can have various relation types, including but not limited to entailment, paraphrasing, and question-answer. It poses challenges to existing embedding methods to capture such relational information. We handle the problem by learning associated relational embeddings. Specifically, a relation-wise translation operation is applied to the source sentence to infer the corresponding target sentence with a pre-trained Siamese-based encoder. The fine-grained relational similarity scores can be computed from learned embeddings. We benchmark our method on 19 datasets covering a wide range of tasks, including semantic textual similarity, transfer, and domain-specific tasks. Experimental results show that our method is eff",
    "link": "http://arxiv.org/abs/2212.08802",
    "context": "Title: Relational Sentence Embedding for Flexible Semantic Matching. (arXiv:2212.08802v2 [cs.CL] UPDATED)\nAbstract: We present Relational Sentence Embedding (RSE), a new paradigm to further discover the potential of sentence embeddings. Prior work mainly models the similarity between sentences based on their embedding distance. Because of the complex semantic meanings conveyed, sentence pairs can have various relation types, including but not limited to entailment, paraphrasing, and question-answer. It poses challenges to existing embedding methods to capture such relational information. We handle the problem by learning associated relational embeddings. Specifically, a relation-wise translation operation is applied to the source sentence to infer the corresponding target sentence with a pre-trained Siamese-based encoder. The fine-grained relational similarity scores can be computed from learned embeddings. We benchmark our method on 19 datasets covering a wide range of tasks, including semantic textual similarity, transfer, and domain-specific tasks. Experimental results show that our method is eff",
    "path": "papers/22/12/2212.08802.json",
    "total_tokens": 896,
    "translated_title": "关于灵活语义匹配的关系句嵌入",
    "translated_abstract": "我们提出了一种新的关系句嵌入 (RSE) 范式，以进一步发掘句子嵌入的潜力。先前的工作主要是基于嵌入距离建模句子间的相似度。由于表达的复杂语义含义，句子对可能具有各种关系类型，包括但不限于蕴涵、释义和问答。对于现有的嵌入方法来捕捉这种关系信息提出了挑战。我们通过学习相关的关系嵌入来解决这个问题。具体地，我们通过一个预训练的共生中心编码器，对源句子应用关系转换操作来推断对应的目标句子。通过学习的嵌入，可以计算出细粒度的关系相似度分数。我们在19个数据集上对我们的方法进行了基准测试，涵盖了广泛的任务范围，包括语义文本相似性、转移和领域特定任务。实验结果表明，我们的方法是有效的。",
    "tldr": "本文提出了一种新的关系句嵌入方法，通过学习相关的关系嵌入来解决句子间复杂语义含义的关系捕捉问题。实验结果表明该方法在多个任务中均具有高效性能。",
    "en_tdlr": "This paper proposes a new paradigm of Relational Sentence Embedding (RSE) to capture the complex semantic relations between sentence pairs. By learning associated relational embeddings, the proposed method achieves high efficiency in a wide range of tasks according to experimental results on 19 datasets."
}