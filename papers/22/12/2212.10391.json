{
    "title": "Empowering Sentence Encoders with Prompting and Label Retrieval for Zero-shot Text Classification. (arXiv:2212.10391v2 [cs.CL] UPDATED)",
    "abstract": "With contrastive pre-training, sentence encoders are generally optimized to locate semantically similar samples closer to each other in their embedding spaces. In this work, we focus on the potential of their embedding spaces to be readily adapted to zero-shot text classification, as semantically distinct samples are already well-separated. Our framework, RaLP (Retrieval augmented Label Prompts for sentence encoder), encodes prompted label candidates with a sentence encoder, then assigns the label whose prompt embedding has the highest similarity with the input text embedding. In order to compensate for the potentially poorly descriptive labels in their original format, RaLP retrieves sentences that are semantically similar to the original label prompt from external corpora and use them as additional pseudo-label prompts. RaLP achieves competitive or stronger performance than much larger baselines on various closed-set classification and multiple-choice QA datasets under zero-shot sett",
    "link": "http://arxiv.org/abs/2212.10391",
    "context": "Title: Empowering Sentence Encoders with Prompting and Label Retrieval for Zero-shot Text Classification. (arXiv:2212.10391v2 [cs.CL] UPDATED)\nAbstract: With contrastive pre-training, sentence encoders are generally optimized to locate semantically similar samples closer to each other in their embedding spaces. In this work, we focus on the potential of their embedding spaces to be readily adapted to zero-shot text classification, as semantically distinct samples are already well-separated. Our framework, RaLP (Retrieval augmented Label Prompts for sentence encoder), encodes prompted label candidates with a sentence encoder, then assigns the label whose prompt embedding has the highest similarity with the input text embedding. In order to compensate for the potentially poorly descriptive labels in their original format, RaLP retrieves sentences that are semantically similar to the original label prompt from external corpora and use them as additional pseudo-label prompts. RaLP achieves competitive or stronger performance than much larger baselines on various closed-set classification and multiple-choice QA datasets under zero-shot sett",
    "path": "papers/22/12/2212.10391.json",
    "total_tokens": 930,
    "translated_title": "利用提示和标签检索增强句子编码器进行零样本文本分类",
    "translated_abstract": "使用对比性预训练，句子编码器通常会优化以便在它们的嵌入空间中将语义相似的样本靠近彼此定位。本文关注其嵌入空间可以便于零样本文本分类的潜力，因为语义不同的样本已经被很好地分离。我们提出的框架 RaLP（使用句子编码器的标签检索增强提示）使用句子编码器对提示标签候选进行编码，然后将具有最高相似性的提示嵌入与输入文本嵌入关联为标签。为了补偿其原始格式中可能描述不当的标签，RaLP从外部语料库中检索与原始标签提示语义相似的句子，并将它们用作附加的伪标签提示。RaLP在零样本设置下的各种封闭集分类和多选题 QA 数据集上取得了至少与较大基线竞争性能或更强的性能。",
    "tldr": "本文提出了一种 RaLP 框架，其使用提示和标签检索增强句子编码器进行零样本文本分类。该框架可以处理描述不当的标签，同时将提示与输入文本嵌入相似性相对较高的标签关联，取得了与较大基线竞争性能或更强的性能。",
    "en_tdlr": "This paper proposes the RaLP framework which empowers sentence encoders with prompting and label retrieval for zero-shot text classification. The framework handles poorly descriptive labels by retrieving similar sentences to the prompt, and associates labels with the highest similarity to the input text embedding, achieving competitive or stronger performance than larger baselines on various datasets."
}