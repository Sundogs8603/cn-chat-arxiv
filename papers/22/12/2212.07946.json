{
    "title": "Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability. (arXiv:2212.07946v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning (RL) has garnered significant attention for developing decision-making agents that aim to maximize rewards, specified by an external supervisor, within fully observable environments. However, many real-world problems involve partial observations, formulated as partially observable Markov decision processes (POMDPs). Previous studies have tackled RL in POMDPs by either incorporating the memory of past actions and observations or by inferring the true state of the environment from observed data. However, aggregating observed data over time becomes impractical in continuous spaces. Moreover, inference-based RL approaches often require many samples to perform well, as they focus solely on reward maximization and neglect uncertainty in the inferred state. Active inference (AIF) is a framework formulated in POMDPs and directs agents to select actions by minimizing a function called expected free energy (EFE). This supplies reward-maximizing (exploitative) behaviour, as",
    "link": "http://arxiv.org/abs/2212.07946",
    "context": "Title: Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability. (arXiv:2212.07946v2 [cs.LG] UPDATED)\nAbstract: Reinforcement learning (RL) has garnered significant attention for developing decision-making agents that aim to maximize rewards, specified by an external supervisor, within fully observable environments. However, many real-world problems involve partial observations, formulated as partially observable Markov decision processes (POMDPs). Previous studies have tackled RL in POMDPs by either incorporating the memory of past actions and observations or by inferring the true state of the environment from observed data. However, aggregating observed data over time becomes impractical in continuous spaces. Moreover, inference-based RL approaches often require many samples to perform well, as they focus solely on reward maximization and neglect uncertainty in the inferred state. Active inference (AIF) is a framework formulated in POMDPs and directs agents to select actions by minimizing a function called expected free energy (EFE). This supplies reward-maximizing (exploitative) behaviour, as",
    "path": "papers/22/12/2212.07946.json",
    "total_tokens": 931,
    "translated_title": "主动推理和强化学习：在部分可观测的连续状态和动作空间下的统一推理",
    "translated_abstract": "强化学习在完全可观测环境中开发决策制定代理以最大化由外部监督员指定的奖励引起了极大关注。然而，许多现实世界的问题涉及部分观测，形式化为部分可观测的马尔可夫决策过程（POMDP）。以往的研究通过将过去的行动和观测记忆或通过从观测数据中推断环境的真实状态来解决POMDP中的强化学习问题。然而，在连续空间中随时间聚合观测数据变得不可行。此外，基于推理的强化学习方法通常需要许多样本才能表现良好，因为它们仅关注奖励最大化，忽视了推断状态的不确定性。主动推理（AIF）是在POMDP中制定的一种框架，通过最小化一个称为期望自由能（EFE）的函数指导代理选择动作。这提供了最大化奖励（富有开发性）的行为。",
    "tldr": "本论文提出了一种能够在部分可观测的连续状态和动作空间下进行统一推理的框架，通过最小化期望自由能函数指导代理选择动作，以实现最大化奖励的决策制定。",
    "en_tdlr": "This paper proposes a unified framework for inference in partially observable continuous state and action spaces, using the concept of expected free energy to guide the agent's action selection and maximize reward."
}