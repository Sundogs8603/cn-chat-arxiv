{
    "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. (arXiv:2212.10509v2 [cs.CL] UPDATED)",
    "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, \\textit{what to retrieve} depends on \\textit{what has already been derived}, which in turn may depend on \\textit{what was previously retrieved}. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: Ho",
    "link": "http://arxiv.org/abs/2212.10509",
    "context": "Title: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. (arXiv:2212.10509v2 [cs.CL] UPDATED)\nAbstract: Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, \\textit{what to retrieve} depends on \\textit{what has already been derived}, which in turn may depend on \\textit{what was previously retrieved}. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: Ho",
    "path": "papers/22/12/2212.10509.json",
    "total_tokens": 936,
    "translated_title": "利用思路链条推理交错式检索解决知识密集型多步问题",
    "translated_abstract": "基于提示的大型语言模型在多步问答中生成自然语言推理步骤或思路链条（CoT）时具有出色的强大性能。然而，当所需知识不可用或不在模型参数中更新时，它们可能出错。虽然使用问题从外部知识源检索相关文本有助于大型语言模型，但我们观察到这种一步检索和阅读方法对于多步问题回答不足够。对于多步问题，需要根据先前得出的内容选择检索内容，而这可能依赖于之前检索过的内容。为了解决这个问题，我们提出了 IRCoT，一种新的多步问答方法，它将检索与思路链条中的步骤进行交错，以思路链条引导检索，并使用检索结果来改进思路链条。在四个数据集上使用 IRCoT 与 GPT3 可以大大提高检索（高达 21 点）和下游问答（高达 15 点）的性能。",
    "tldr": "提出了一种名为 IRCoT 的方法，该方法将检索与思路链条步骤交替进行，以引导检索并使用检索结果改进思路链条，从而有效解决了多步问答中先前检索信息不足的问题。",
    "en_tdlr": "IRCoT proposes a method that interleaves retrieval with steps in a Chain-of-Thoughts, guiding the retrieval and using retrieved results to improve the CoT, effectively address the previous insufficiency of information retrieval in multi-step QA."
}