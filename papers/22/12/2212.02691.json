{
    "title": "LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training. (arXiv:2212.02691v2 [cs.CL] UPDATED)",
    "abstract": "Transformers are widely used in NLP tasks. However, current approaches to leveraging transformers to understand language expose one weak spot: Number understanding. In some scenarios, numbers frequently occur, especially in semi-structured data like tables. But current approaches to rich-number tasks with transformer-based language models abandon or lose some of the numeracy information - e.g., breaking numbers into sub-word tokens - which leads to many number-related errors. In this paper, we propose the LUNA framework which improves the numerical reasoning and calculation capabilities of transformer-based language models. With the number plugin of NumTok and NumBed, LUNA represents each number as a whole to model input. With number pre-training, including regression loss and model distillation, LUNA bridges the gap between number and vocabulary embeddings. To the best of our knowledge, this is the first work that explicitly injects numeracy capability into language models using Numbe",
    "link": "http://arxiv.org/abs/2212.02691",
    "context": "Title: LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training. (arXiv:2212.02691v2 [cs.CL] UPDATED)\nAbstract: Transformers are widely used in NLP tasks. However, current approaches to leveraging transformers to understand language expose one weak spot: Number understanding. In some scenarios, numbers frequently occur, especially in semi-structured data like tables. But current approaches to rich-number tasks with transformer-based language models abandon or lose some of the numeracy information - e.g., breaking numbers into sub-word tokens - which leads to many number-related errors. In this paper, we propose the LUNA framework which improves the numerical reasoning and calculation capabilities of transformer-based language models. With the number plugin of NumTok and NumBed, LUNA represents each number as a whole to model input. With number pre-training, including regression loss and model distillation, LUNA bridges the gap between number and vocabulary embeddings. To the best of our knowledge, this is the first work that explicitly injects numeracy capability into language models using Numbe",
    "path": "papers/22/12/2212.02691.json",
    "total_tokens": 846,
    "translated_title": "LUNA：使用数字插件和预训练的Transformers实现语言理解",
    "translated_abstract": "Transformers在NLP任务中被广泛使用。然而，目前利用transformers理解语言的方法存在一个弱点：数字理解。在某些场景下，数字经常出现，特别是在半结构化数据（如表格）中。但是利用基于transformer的语言模型进行数字任务的当前方法会丢失一些数字信息，例如将数字分解为子词标记等，导致许多与数字相关的错误。在本文中，我们提出了LUNA框架，可以显著提高基于transformer的语言模型的数字推理和计算能力。LUNA使用NumTok和NumBed的数字插件将每个数字作为一个整体来表示输入。通过数字预训练，包括回归损失和模型蒸馏，LUNA弥合了数字和词汇嵌入之间的差距。据我们所知，这是第一个明确将数字能力注入语言模型中使用数字插件和预训练的工作。",
    "tldr": "LUNA框架通过数字插件和预训练的方式，解决了当前基于transformer的语言模型无法很好理解数字的问题。",
    "en_tdlr": "The LUNA framework improves the numerical reasoning and calculation capabilities of transformer-based language models via number plugins and pre-training, addressing the issue of current transformer-based language models' inability to understand numbers well."
}