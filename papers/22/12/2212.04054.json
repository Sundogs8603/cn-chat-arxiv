{
    "title": "Learning to Dub Movies via Hierarchical Prosody Models. (arXiv:2212.04054v2 [cs.CL] UPDATED)",
    "abstract": "Given a piece of text, a video clip and a reference audio, the movie dubbing (also known as visual voice clone V2C) task aims to generate speeches that match the speaker's emotion presented in the video using the desired speaker voice as reference. V2C is more challenging than conventional text-to-speech tasks as it additionally requires the generated speech to exactly match the varying emotions and speaking speed presented in the video. Unlike previous works, we propose a novel movie dubbing architecture to tackle these problems via hierarchical prosody modelling, which bridges the visual information to corresponding speech prosody from three aspects: lip, face, and scene. Specifically, we align lip movement to the speech duration, and convey facial expression to speech energy and pitch via attention mechanism based on valence and arousal representations inspired by recent psychology findings. Moreover, we design an emotion booster to capture the atmosphere from global video scenes. A",
    "link": "http://arxiv.org/abs/2212.04054",
    "context": "Title: Learning to Dub Movies via Hierarchical Prosody Models. (arXiv:2212.04054v2 [cs.CL] UPDATED)\nAbstract: Given a piece of text, a video clip and a reference audio, the movie dubbing (also known as visual voice clone V2C) task aims to generate speeches that match the speaker's emotion presented in the video using the desired speaker voice as reference. V2C is more challenging than conventional text-to-speech tasks as it additionally requires the generated speech to exactly match the varying emotions and speaking speed presented in the video. Unlike previous works, we propose a novel movie dubbing architecture to tackle these problems via hierarchical prosody modelling, which bridges the visual information to corresponding speech prosody from three aspects: lip, face, and scene. Specifically, we align lip movement to the speech duration, and convey facial expression to speech energy and pitch via attention mechanism based on valence and arousal representations inspired by recent psychology findings. Moreover, we design an emotion booster to capture the atmosphere from global video scenes. A",
    "path": "papers/22/12/2212.04054.json",
    "total_tokens": 915,
    "translated_title": "基于分层韵律模型的电影配音学习",
    "translated_abstract": "基于一段文本、一个视频片段和一个参考音频，电影配音任务（也称为视觉语音克隆V2C）旨在使用所需的说话者声音作为参考，生成与视频中呈现的说话者情感匹配的语音，而且要求生成的语音恰好匹配视频中呈现的不断变化的情感和说话速度。与以往的工作不同，本文提出了一种新颖的电影配音体系结构，通过分层韵律建模从三个方面将视觉信息与相应的语音韵律联系起来：嘴唇、面部和场景。具体而言，我们将嘴唇运动与语音持续时间对齐，并通过基于最近心理学发现的愉悦和唤起表示的注意机制，将面部表情传达到语音能量和音高上。此外，我们设计了一个情感增强器来捕捉全局视频场景的氛围。",
    "tldr": "本研究提出了一种新的电影配音架构，通过分层韵律建模从嘴唇、面部和场景三个方面将视觉信息与相应的语音韵律联系起来，从而解决了V2C任务中情感变化和说话速度匹配等问题。"
}