{
    "title": "Defense Against Adversarial Attacks on Audio DeepFake Detection. (arXiv:2212.14597v2 [cs.SD] UPDATED)",
    "abstract": "Audio DeepFakes (DF) are artificially generated utterances created using deep learning, with the primary aim of fooling the listeners in a highly convincing manner. Their quality is sufficient to pose a severe threat in terms of security and privacy, including the reliability of news or defamation. Multiple neural network-based methods to detect generated speech have been proposed to prevent the threats. In this work, we cover the topic of adversarial attacks, which decrease the performance of detectors by adding superficial (difficult to spot by a human) changes to input data. Our contribution contains evaluating the robustness of 3 detection architectures against adversarial attacks in two scenarios (white-box and using transferability) and enhancing it later by using adversarial training performed by our novel adaptive training. Moreover, one of the investigated architectures is RawNet3, which, to the best of our knowledge, we adapted for the first time to DeepFake detection.",
    "link": "http://arxiv.org/abs/2212.14597",
    "context": "Title: Defense Against Adversarial Attacks on Audio DeepFake Detection. (arXiv:2212.14597v2 [cs.SD] UPDATED)\nAbstract: Audio DeepFakes (DF) are artificially generated utterances created using deep learning, with the primary aim of fooling the listeners in a highly convincing manner. Their quality is sufficient to pose a severe threat in terms of security and privacy, including the reliability of news or defamation. Multiple neural network-based methods to detect generated speech have been proposed to prevent the threats. In this work, we cover the topic of adversarial attacks, which decrease the performance of detectors by adding superficial (difficult to spot by a human) changes to input data. Our contribution contains evaluating the robustness of 3 detection architectures against adversarial attacks in two scenarios (white-box and using transferability) and enhancing it later by using adversarial training performed by our novel adaptive training. Moreover, one of the investigated architectures is RawNet3, which, to the best of our knowledge, we adapted for the first time to DeepFake detection.",
    "path": "papers/22/12/2212.14597.json",
    "total_tokens": 929,
    "translated_title": "抵御针对音频DeepFake检测的对抗性攻击",
    "translated_abstract": "音频DeepFake是使用深度学习生成的人工语音，其主要目的是以高度令人信服的方式欺骗听众。它们的质量足以在安全和隐私方面构成严重威胁，包括新闻的可靠性或诽谤。已经提出了多种基于神经网络的方法来检测生成的语音，以防止这些威胁。本研究涵盖了针对对抗性攻击的主题，这些攻击通过向输入数据添加表面的更难被人类察觉的变化来降低检测器的性能。我们的贡献包括在两个场景（白盒和使用可传递性）中评估3个检测体系结构对抗攻击的鲁棒性，并通过使用我们的新颖自适应训练进行对抗训练来加强它。此外，我们还首次适应了RawNet3体系结构用于DeepFake检测。",
    "tldr": "本文旨在探讨音频DeepFake检测中对抗性攻击的问题，并通过对三种检测体系结构的鲁棒性评估以及采用自适应训练进行对抗训练来提高检测器的性能。其中，我们还首次适应了RawNet3体系结构用于DeepFake检测。",
    "en_tdlr": "This paper aims to explore adversarial attacks in audio DeepFake detection, evaluate the robustness of three detection architectures and enhance performance using novel adaptive training. Additionally, the RawNet3 architecture is adapted for the first time for DeepFake detection."
}