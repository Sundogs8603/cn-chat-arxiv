{
    "title": "Understanding Translationese in Cross-Lingual Summarization. (arXiv:2212.07220v2 [cs.CL] UPDATED)",
    "abstract": "Given a document in a source language, cross-lingual summarization (CLS) aims at generating a concise summary in a different target language. Unlike monolingual summarization (MS), naturally occurring source-language documents paired with target-language summaries are rare. To collect large-scale CLS data, existing datasets typically involve translation in their creation. However, the translated text is distinguished from the text originally written in that language, i.e., translationese. In this paper, we first confirm that different approaches of constructing CLS datasets will lead to different degrees of translationese. Then we systematically investigate how translationese affects CLS model evaluation and performance when it appears in source documents or target summaries. In detail, we find that (1) the translationese in documents or summaries of test sets might lead to the discrepancy between human judgment and automatic evaluation; (2) the translationese in training sets would ha",
    "link": "http://arxiv.org/abs/2212.07220",
    "context": "Title: Understanding Translationese in Cross-Lingual Summarization. (arXiv:2212.07220v2 [cs.CL] UPDATED)\nAbstract: Given a document in a source language, cross-lingual summarization (CLS) aims at generating a concise summary in a different target language. Unlike monolingual summarization (MS), naturally occurring source-language documents paired with target-language summaries are rare. To collect large-scale CLS data, existing datasets typically involve translation in their creation. However, the translated text is distinguished from the text originally written in that language, i.e., translationese. In this paper, we first confirm that different approaches of constructing CLS datasets will lead to different degrees of translationese. Then we systematically investigate how translationese affects CLS model evaluation and performance when it appears in source documents or target summaries. In detail, we find that (1) the translationese in documents or summaries of test sets might lead to the discrepancy between human judgment and automatic evaluation; (2) the translationese in training sets would ha",
    "path": "papers/22/12/2212.07220.json",
    "total_tokens": 1015,
    "translated_title": "理解跨语言摘要中的“翻译语言”现象",
    "translated_abstract": "在跨语言摘要（CLS）中，给定一篇源语言文档，旨在生成一篇简洁的目标语言摘要。与单语摘要（MS）不同，自然出现的源语言文档配对目标语言摘要并不常见。为了收集大规模的CLS数据，现有数据集通常在创建过程中涉及翻译。然而，翻译文本与原始语言文本之间存在着区别，即翻译语言。本文首先确认了构建CLS数据集的不同方法将导致不同程度的翻译语言现象。然后我们系统地研究了翻译语言在源文档或目标摘要中出现时如何影响CLS模型的评估和性能。具体而言，我们发现（1）测试集中文档或摘要中的翻译语言可能导致人工判断与自动评估之间的差异；（2）训练集中的翻译语言会影响模型的训练表现和性能。",
    "tldr": "本论文研究了跨语言摘要中的“翻译语言”现象对模型的影响，发现不同方法构建数据集会导致不同程度的翻译语言现象。文章还发现翻译语言会对模型的评估和性能产生影响，包括测试集中的翻译语言可能导致人工判断与自动评估之间的差异，以及训练集中的翻译语言会影响模型的训练表现和性能。",
    "en_tdlr": "This paper investigates the impact of \"translationese\" in cross-lingual summarization models, finding that different methods of constructing datasets lead to varying degrees of translationese. It also reveals that translationese affects model evaluation and performance, including the discrepancy between human judgment and automatic evaluation in test sets and the influence of translationese in training sets on model training and performance."
}