{
    "title": "Generalization Bounds for Few-Shot Transfer Learning with Pretrained Classifiers. (arXiv:2212.12532v2 [cs.LG] UPDATED)",
    "abstract": "We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. We offer a theoretical explanation for this behavior based on the recently discovered phenomenon of class-feature-variability collapse, that is, that during the training of deep classification networks the feature embeddings of samples belonging to the same class tend to concentrate around their class means. More specifically, we show that the few-shot error of the learned feature map on new classes (defined as the classification error of the nearest class-center classifier using centers learned from a small number of random samples from each new class) is small in case of class-feature-variability collapse, under the a",
    "link": "http://arxiv.org/abs/2212.12532",
    "context": "Title: Generalization Bounds for Few-Shot Transfer Learning with Pretrained Classifiers. (arXiv:2212.12532v2 [cs.LG] UPDATED)\nAbstract: We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. We offer a theoretical explanation for this behavior based on the recently discovered phenomenon of class-feature-variability collapse, that is, that during the training of deep classification networks the feature embeddings of samples belonging to the same class tend to concentrate around their class means. More specifically, we show that the few-shot error of the learned feature map on new classes (defined as the classification error of the nearest class-center classifier using centers learned from a small number of random samples from each new class) is small in case of class-feature-variability collapse, under the a",
    "path": "papers/22/12/2212.12532.json",
    "total_tokens": 876,
    "translated_title": "预训练分类器的少样本迁移学习的泛化界限",
    "translated_abstract": "我们研究了基础模型学习可传输到新的、未见过的类别的分类表示能力。最近的文献结果表明，由一个单一的分类器学得的表示在少样本学习问题上与专门为这些问题设计的特殊算法学得的表示有竞争力。我们提出了一个理论解释，基于最近发现的类别特征变异崩塌现象，即在深度分类网络训练过程中，属于同一类别的样本的特征嵌入倾向于聚集在它们的类别均值附近。具体地，我们证明了在类别特征变异崩塌的情况下，学得的特征映射在新的类别上的少样本误差小，即使用从每个新类别的少量随机样本学得的中心来进行最近类别中心分类器的分类误差小。",
    "tldr": "该论文研究了基于预训练分类器学习的表示在少样本迁移学习中的通用性界限，提出了类别特征变异崩塌现象的理论解释，该现象使得在新类别上通过少样本学习的特征映射具有较小的误差。",
    "en_tdlr": "This paper investigates the generalization bounds of representations learned through pretrained classifiers in few-shot transfer learning, and provides a theoretical explanation based on the phenomenon of class-feature-variability collapse, which allows for small errors in few-shot learning on new classes."
}