{
    "title": "A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization. (arXiv:2212.02387v2 [cs.LG] UPDATED)",
    "abstract": "This paper studies the stochastic optimization for decentralized nonconvex-strongly-concave minimax problem. We propose a simple and efficient algorithm, called Decentralized Recursive-gradient descEnt Ascent Method (\\texttt{DREAM}), which achieves the best-known theoretical guarantee for finding the $\\epsilon$-stationary point of the primal function. For the online setting, the proposed method requires $\\mathcal{O}(\\kappa^3\\epsilon^{-3})$ stochastic first-order oracle (SFO) calls and $\\mathcal{O}\\big(\\kappa^2\\epsilon^{-2}/\\sqrt{1-\\lambda_2(W)}\\,\\big)$ communication rounds to find an $\\epsilon$-stationary point, where $\\kappa$ is the condition number and $\\lambda_2(W)$ is the second-largest eigenvalue of the gossip matrix~$W$. For the offline setting with totally $N$ component functions, the proposed method requires $\\mathcal{O}\\big(\\kappa^2 \\sqrt{N} \\epsilon^{-2}\\big)$ SFO calls and the same communication complexity as the online setting.",
    "link": "http://arxiv.org/abs/2212.02387",
    "context": "Title: A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization. (arXiv:2212.02387v2 [cs.LG] UPDATED)\nAbstract: This paper studies the stochastic optimization for decentralized nonconvex-strongly-concave minimax problem. We propose a simple and efficient algorithm, called Decentralized Recursive-gradient descEnt Ascent Method (\\texttt{DREAM}), which achieves the best-known theoretical guarantee for finding the $\\epsilon$-stationary point of the primal function. For the online setting, the proposed method requires $\\mathcal{O}(\\kappa^3\\epsilon^{-3})$ stochastic first-order oracle (SFO) calls and $\\mathcal{O}\\big(\\kappa^2\\epsilon^{-2}/\\sqrt{1-\\lambda_2(W)}\\,\\big)$ communication rounds to find an $\\epsilon$-stationary point, where $\\kappa$ is the condition number and $\\lambda_2(W)$ is the second-largest eigenvalue of the gossip matrix~$W$. For the offline setting with totally $N$ component functions, the proposed method requires $\\mathcal{O}\\big(\\kappa^2 \\sqrt{N} \\epsilon^{-2}\\big)$ SFO calls and the same communication complexity as the online setting.",
    "path": "papers/22/12/2212.02387.json",
    "total_tokens": 973,
    "translated_title": "一种简单高效的去中心化非凸极小化问题随机算法",
    "translated_abstract": "本文研究了去中心化非凸极小化问题的随机优化。我们提出了一种简单高效的算法，称为去中心化递归梯度上升法（\\texttt{DREAM}），它实现了寻找原函数的$\\epsilon$-稳定点的最佳已知理论保证。在在线设置下，所提出的方法需要$\\mathcal{O}(\\kappa^3\\epsilon^{-3})$随机一阶预言机（SFO）调用以及$\\mathcal{O}\\big(\\kappa^2\\epsilon^{-2}/\\sqrt{1-\\lambda_2(W)}\\,\\big)$通信轮次来找到$\\epsilon$-稳定点，其中$\\kappa$是条件数，$\\lambda_2(W)$是八卦矩阵$W$的次大特征值。对于完全由$N$个分量函数组成的离线设置，所提出的方法需要$\\mathcal{O}\\big(\\kappa^2 \\sqrt{N} \\epsilon^{-2}\\big)$ SFO 调用和与在线设置相同的通信复杂度。",
    "tldr": "本文提出了一种称为去中心化递归梯度上升法（DREAM）的简单高效算法，用于解决去中心化非凸极小化问题，并实现了寻找原函数的 $\\epsilon$-稳定点的最佳理论保证。",
    "en_tdlr": "This paper proposes a simple and efficient algorithm, called Decentralized Recursive-gradient descEnt Ascent Method (DREAM), for solving decentralized nonconvex-strongly-concave minimax problem and achieves the best-known theoretical guarantee for finding the $\\epsilon$-stationary point of the primal function."
}