{
    "title": "Synthetic Pre-Training Tasks for Neural Machine Translation. (arXiv:2212.09864v2 [cs.CL] UPDATED)",
    "abstract": "Pre-training models with large crawled corpora can lead to issues such as toxicity and bias, as well as copyright and privacy concerns. A promising way of alleviating such concerns is to conduct pre-training with synthetic tasks and data, since no real-world information is ingested by the model. Our goal in this paper is to understand the factors that contribute to the effectiveness of pre-training models when using synthetic resources, particularly in the context of neural machine translation. We propose several novel approaches to pre-training translation models that involve different levels of lexical and structural knowledge, including: 1) generating obfuscated data from a large parallel corpus 2) concatenating phrase pairs extracted from a small word-aligned corpus, and 3) generating synthetic parallel data without real human language corpora. Our experiments on multiple language pairs reveal that pre-training benefits can be realized even with high levels of obfuscation or purely",
    "link": "http://arxiv.org/abs/2212.09864",
    "context": "Title: Synthetic Pre-Training Tasks for Neural Machine Translation. (arXiv:2212.09864v2 [cs.CL] UPDATED)\nAbstract: Pre-training models with large crawled corpora can lead to issues such as toxicity and bias, as well as copyright and privacy concerns. A promising way of alleviating such concerns is to conduct pre-training with synthetic tasks and data, since no real-world information is ingested by the model. Our goal in this paper is to understand the factors that contribute to the effectiveness of pre-training models when using synthetic resources, particularly in the context of neural machine translation. We propose several novel approaches to pre-training translation models that involve different levels of lexical and structural knowledge, including: 1) generating obfuscated data from a large parallel corpus 2) concatenating phrase pairs extracted from a small word-aligned corpus, and 3) generating synthetic parallel data without real human language corpora. Our experiments on multiple language pairs reveal that pre-training benefits can be realized even with high levels of obfuscation or purely",
    "path": "papers/22/12/2212.09864.json",
    "total_tokens": 914,
    "translated_title": "神经机器翻译的合成预训练任务",
    "translated_abstract": "使用大规模抓取的语料库进行预训练模型可能会导致毒性和偏见等问题，以及版权和隐私问题。采用合成任务和数据进行预训练是缓解这些问题的一种有前途的方式，因为模型不会吸收任何真实世界信息。本文旨在了解使用合成资源时对预训练模型有效性的影响因素，特别是在神经机器翻译的背景下。我们提出了几种新颖的预训练翻译模型的方法，包括不同水平的词汇和结构知识，例如：1）从大型平行语料库生成混淆数据，2）连接从小型词对齐语料库提取的短语对，以及3）生成不带真实人类语料库的合成平行数据。我们在多种语言对上的实验表明，即使存在高水平的混淆或纯合成数据，也可以实现预训练的效益。",
    "tldr": "本文提出了一种使用合成任务和数据预训练神经机器翻译模型的方法，其可以缓解大规模抓取的语料库所导致的毒性、偏见和法律隐患，并证明了即使采用高度混淆或纯合成数据，预训练依然有效。"
}