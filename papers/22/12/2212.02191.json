{
    "title": "On the effectiveness of partial variance reduction in federated learning with heterogeneous data. (arXiv:2212.02191v2 [cs.LG] UPDATED)",
    "abstract": "Data heterogeneity across clients is a key challenge in federated learning. Prior works address this by either aligning client and server models or using control variates to correct client model drift. Although these methods achieve fast convergence in convex or simple non-convex problems, the performance in over-parameterized models such as deep neural networks is lacking. In this paper, we first revisit the widely used FedAvg algorithm in a deep neural network to understand how data heterogeneity influences the gradient updates across the neural network layers. We observe that while the feature extraction layers are learned efficiently by FedAvg, the substantial diversity of the final classification layers across clients impedes the performance. Motivated by this, we propose to correct model drift by variance reduction only on the final layers. We demonstrate that this significantly outperforms existing benchmarks at a similar or lower communication cost. We furthermore provide proof",
    "link": "http://arxiv.org/abs/2212.02191",
    "context": "Title: On the effectiveness of partial variance reduction in federated learning with heterogeneous data. (arXiv:2212.02191v2 [cs.LG] UPDATED)\nAbstract: Data heterogeneity across clients is a key challenge in federated learning. Prior works address this by either aligning client and server models or using control variates to correct client model drift. Although these methods achieve fast convergence in convex or simple non-convex problems, the performance in over-parameterized models such as deep neural networks is lacking. In this paper, we first revisit the widely used FedAvg algorithm in a deep neural network to understand how data heterogeneity influences the gradient updates across the neural network layers. We observe that while the feature extraction layers are learned efficiently by FedAvg, the substantial diversity of the final classification layers across clients impedes the performance. Motivated by this, we propose to correct model drift by variance reduction only on the final layers. We demonstrate that this significantly outperforms existing benchmarks at a similar or lower communication cost. We furthermore provide proof",
    "path": "papers/22/12/2212.02191.json",
    "total_tokens": 976,
    "translated_title": "论异构数据中部分方差降低在联邦学习中的有效性",
    "translated_abstract": "客户端数据的异构性是联邦学习中的一个关键挑战，之前的方法通过调整客户端和服务器模型或使用控制变量来纠正客户端模型漂移来解决这个问题。虽然这些方法能在凸或简单的非凸问题中实现快速收敛，但在深度神经网络等过度参数化的模型中性能较差。在本文中，我们首先重新审查了一个深度神经网络中广泛使用的FedAvg算法，以了解数据异构性如何影响神经网络层之间的梯度更新。我们观察到，虽然FedAvg有效地学习了特征提取层，但客户端最终分类层的大量差异会阻碍性能。在此基础上，我们提出仅对最终层的方差进行降低以纠正模型漂移。我们证明了这样做可以在相似或更低的通信成本下显著优于现有的基准方法。此外，我们还提供了证明…",
    "tldr": "本文研究了联邦学习中数据异构性如何影响深度神经网络层之间的梯度更新，发现虽然特征提取层能被有效学习，但客户端最终分类层的大量差异阻碍了该算法的性能，因此提出了仅对最终层的方差进行降低的方法，证明其可以在相似或更低的通信成本下显著优于现有的基准方法。",
    "en_tdlr": "This paper investigates how data heterogeneity affects gradient updates between layers in deep neural networks in federated learning. By proposing variance reduction only on the final layers, it significantly outperforms existing benchmarks at similar or lower communication costs."
}