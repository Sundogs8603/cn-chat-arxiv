{
    "title": "DC-MBR: Distributional Cooling for Minimum Bayesian Risk Decoding. (arXiv:2212.04205v2 [cs.CL] UPDATED)",
    "abstract": "Minimum Bayesian Risk Decoding (MBR) emerges as a promising decoding algorithm in Neural Machine Translation. However, MBR performs poorly with label smoothing, which is surprising as label smoothing provides decent improvement with beam search and improves generality in various tasks. In this work, we show that the issue arises from the un-consistency of label smoothing on the token-level and sequence-level distributions. We demonstrate that even though label smoothing only causes a slight change in the token-level, the sequence-level distribution is highly skewed. We coin the issue \\emph{autoregressive over-smoothness}. To address this issue, we propose a simple and effective method, Distributional Cooling MBR (DC-MBR), which manipulates the entropy of output distributions by tuning down the Softmax temperature. We theoretically prove the equivalence between pre-tuning label smoothing factor and distributional cooling. Extensive experiments on NMT benchmarks validate that distributio",
    "link": "http://arxiv.org/abs/2212.04205",
    "context": "Title: DC-MBR: Distributional Cooling for Minimum Bayesian Risk Decoding. (arXiv:2212.04205v2 [cs.CL] UPDATED)\nAbstract: Minimum Bayesian Risk Decoding (MBR) emerges as a promising decoding algorithm in Neural Machine Translation. However, MBR performs poorly with label smoothing, which is surprising as label smoothing provides decent improvement with beam search and improves generality in various tasks. In this work, we show that the issue arises from the un-consistency of label smoothing on the token-level and sequence-level distributions. We demonstrate that even though label smoothing only causes a slight change in the token-level, the sequence-level distribution is highly skewed. We coin the issue \\emph{autoregressive over-smoothness}. To address this issue, we propose a simple and effective method, Distributional Cooling MBR (DC-MBR), which manipulates the entropy of output distributions by tuning down the Softmax temperature. We theoretically prove the equivalence between pre-tuning label smoothing factor and distributional cooling. Extensive experiments on NMT benchmarks validate that distributio",
    "path": "papers/22/12/2212.04205.json",
    "total_tokens": 1038,
    "translated_title": "DC-MBR：最小贝叶斯风险解码的分布式冷却",
    "translated_abstract": "最小贝叶斯风险解码(MBR)在神经机器翻译中表现出众，然而，在标签平滑的情况下，MBR的表现并不理想。令人惊讶的是，标签平滑在束搜索中提供了不错的改进，并在各种任务中提高了通用性。我们展示了问题出现在标签平滑在令牌级和序列级分布上不一致。我们证明了即使标签平滑只在令牌级上造成轻微变化，序列级分布也会高度倾斜。我们将此问题称为“自回归过度平滑性”。为了解决这个问题，我们提出了一种简单而有效的方法，分布式冷却MBR（DC-MBR），通过调整softmax温度来操纵输出分布的熵。我们在理论上证明了预调节标签平滑因子和分布式冷却之间的等价性。在NMT基准测试中进行的广泛实验验证了分布式冷却显著减轻了过度平滑问题并持续优于现有基线。",
    "tldr": "本研究提出了分布式冷却MBR（DC-MBR）的方法来解决标签平滑对最小贝叶斯风险解码（MBR）造成的自回归过度平滑性问题，该方法通过调整softmax温度来操纵输出分布的熵。最终，DC-MBR在神经机器翻译中表现出众，相较于现有基线持续优化。"
}