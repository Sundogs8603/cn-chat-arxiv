{
    "title": "HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation. (arXiv:2212.08853v2 [cs.CL] UPDATED)",
    "abstract": "Language models with the Transformers structure have shown great performance in natural language processing. However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse. In this work, we propose HyPe, a simple yet effective fine-tuning technique to alleviate such problems by perturbing hidden representations of Transformers layers. Unlike previous works that only add noise to inputs or parameters, we argue that the hidden representations of Transformers layers convey more diverse and meaningful language information. Therefore, making the Transformers layers more robust to hidden representation perturbations can further benefit the fine-tuning of PLMs en bloc. We conduct extensive experiments and analyses on GLUE and other natural language inference datasets. Results demonstrate that HyPe outperforms vanilla fine-tuning and enhances generalization of hidden representations from different layers. In",
    "link": "http://arxiv.org/abs/2212.08853",
    "context": "Title: HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation. (arXiv:2212.08853v2 [cs.CL] UPDATED)\nAbstract: Language models with the Transformers structure have shown great performance in natural language processing. However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse. In this work, we propose HyPe, a simple yet effective fine-tuning technique to alleviate such problems by perturbing hidden representations of Transformers layers. Unlike previous works that only add noise to inputs or parameters, we argue that the hidden representations of Transformers layers convey more diverse and meaningful language information. Therefore, making the Transformers layers more robust to hidden representation perturbations can further benefit the fine-tuning of PLMs en bloc. We conduct extensive experiments and analyses on GLUE and other natural language inference datasets. Results demonstrate that HyPe outperforms vanilla fine-tuning and enhances generalization of hidden representations from different layers. In",
    "path": "papers/22/12/2212.08853.json",
    "total_tokens": 860,
    "translated_title": "HyPe：使用隐藏表示扰动提高预训练语言模型微调效果",
    "translated_abstract": "基于Transformer结构的语言模型在自然语言处理任务中表现出色，但在微调预训练语言模型时仍然存在着过拟合或表示崩溃等问题。本论文提出了HyPe技术，通过扰动Transformer层的隐藏表示来缓解这些问题。与先前只添加噪声到输入或参数的方法不同，我们认为Transformer层的隐藏表示传达了更多不同且有意义的语言信息，因此让Transformer层更加鲁棒于隐藏表示的扰动可以进一步改善PLMs微调的效果。通过在GLUE和其他自然语言推理数据集上进行广泛的实验和分析，结果表明HyPe优于普通的微调，并增强了来自不同层的隐藏表示的泛化性能。",
    "tldr": "本论文提出了一种名为HyPe的微调技术，通过扰动Transformer层的隐藏表示，改善了预训练语言模型微调过拟合或表示崩溃等问题，并在多个自然语言推理数据集上取得了比普通微调更好的效果。",
    "en_tdlr": "This paper presents a fine-tuning technique called HyPe, which perturbs the hidden representations of Transformers layers to alleviate overfitting and representation collapse issues in pre-trained language model fine-tuning. By enhancing the robustness of Transformers layers to hidden representation perturbations, HyPe outperforms vanilla fine-tuning and improves the generalization of hidden representations from different layers on multiple natural language inference datasets."
}