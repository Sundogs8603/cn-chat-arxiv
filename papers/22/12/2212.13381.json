{
    "title": "MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. (arXiv:2212.13381v3 [cs.LG] UPDATED)",
    "abstract": "Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels. This technique is known to improve the generalization performance in many learning paradigms and applications. In this work, we first analyze Mixup and show that it implicitly regularizes infinitely many directional derivatives of all orders. Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup. To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs. Our results show that the proposed method improves Mixup across multiple datasets using a variety of architectures, for instance, exhibiting an improvement over Mixup by 0.8% in ImageNet top-1 accuracy.",
    "link": "http://arxiv.org/abs/2212.13381",
    "context": "Title: MixupE: Understanding and Improving Mixup from Directional Derivative Perspective. (arXiv:2212.13381v3 [cs.LG] UPDATED)\nAbstract: Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels. This technique is known to improve the generalization performance in many learning paradigms and applications. In this work, we first analyze Mixup and show that it implicitly regularizes infinitely many directional derivatives of all orders. Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup. To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs. Our results show that the proposed method improves Mixup across multiple datasets using a variety of architectures, for instance, exhibiting an improvement over Mixup by 0.8% in ImageNet top-1 accuracy.",
    "path": "papers/22/12/2212.13381.json",
    "total_tokens": 930,
    "translated_title": "MixupE：从方向导数角度理解和改进Mixup技术",
    "translated_abstract": "Mixup是一种深度神经网络中流行的数据增强技术，通过线性插值输入和它们的标签生成额外的样本。该技术已被证实在许多学习范式和应用中提高了泛化性能。本文首先对Mixup进行分析，发现它隐含地对所有阶数的无限多个方向导数进行了正则化。基于这一新的洞见，我们提出了一种改进版本的Mixup，理论上证明它可以比原始版本具有更好的泛化性能。为了证明这种方法的有效性，我们在各种领域进行了实验，例如图像、表格数据、语音和图形。我们的结果表明，所提出的方法改进了Mixup在多个数据集上的表现，在使用各种架构时都表现出比Mixup更好的性能，例如在ImageNet的top-1精度上比Mixup提高了0.8%。",
    "tldr": "本文从方向导数的角度分析了深度神经网络中常用的数据增强技术Mixup，发现它隐含地对所有阶数的无限多个方向导数进行了正则化。基于这一思路，作者提出了改进版的Mixup，理论上证明具有更好的泛化性能，并在各种领域的数据集上进行了验证，表现出比Mixup更好的效果。",
    "en_tdlr": "This paper analyzes the widely used data augmentation technique Mixup in deep neural networks from the perspective of directional derivatives and shows that it implicitly regularizes infinitely many directional derivatives of all orders. Based on this insight, the authors propose an improved version of Mixup that is theoretically proven to have better generalization performance and demonstrate its effectiveness on various datasets across different domains."
}