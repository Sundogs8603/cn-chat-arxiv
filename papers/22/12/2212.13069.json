{
    "title": "Homophily modulates double descent generalization in graph convolution networks. (arXiv:2212.13069v2 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks are among the most successful machine learning models for relational datasets like metabolic, transportation, and social networks. Yet the determinants of their strong generalization for diverse interactions encoded in the data are not well understood. Methods from statistical learning theory do not explain emergent phenomena such as double descent or the dependence of risk on the nature of interactions. We use analytical tools from statistical physics and random matrix theory to precisely characterize generalization in simple graph convolution networks on the contextual stochastic block model. The derived curves are phenomenologically rich: they explain the distinction between learning on homophilic and heterophilic and they predict double descent whose existence in GNNs has been questioned by recent work. We show how risk depends on the interplay between the noise in the graph, noise in the features, and the proportion of nodes used for training. Our analysis pr",
    "link": "http://arxiv.org/abs/2212.13069",
    "context": "Title: Homophily modulates double descent generalization in graph convolution networks. (arXiv:2212.13069v2 [cs.LG] UPDATED)\nAbstract: Graph neural networks are among the most successful machine learning models for relational datasets like metabolic, transportation, and social networks. Yet the determinants of their strong generalization for diverse interactions encoded in the data are not well understood. Methods from statistical learning theory do not explain emergent phenomena such as double descent or the dependence of risk on the nature of interactions. We use analytical tools from statistical physics and random matrix theory to precisely characterize generalization in simple graph convolution networks on the contextual stochastic block model. The derived curves are phenomenologically rich: they explain the distinction between learning on homophilic and heterophilic and they predict double descent whose existence in GNNs has been questioned by recent work. We show how risk depends on the interplay between the noise in the graph, noise in the features, and the proportion of nodes used for training. Our analysis pr",
    "path": "papers/22/12/2212.13069.json",
    "total_tokens": 957,
    "translated_title": "同质性在图卷积网络的双下降泛化中的调制作用",
    "translated_abstract": "图神经网络是用于关系数据集（如代谢、交通和社交网络）的最成功的机器学习模型之一。然而，它们对数据中编码的各种交互的强大泛化的决定因素并不为人所知。来自统计学习理论的方法无法解释出现的现象，如双下降或风险取决于交互性质的问题。我们使用统计物理和随机矩阵理论的分析工具来精确地表征简单图卷积网络在背景随机块模型上的泛化。导出的曲线现象学上十分丰富：它们解释了同质性和异质性学习之间的区别，并预测了最近作品所质疑的GNN中双下降现象的存在。我们展示了风险如何取决于图中的噪声、特征中的噪声和用于训练的节点比例之间的相互作用。我们的分析为理解同质性如何调制图神经网络的泛化提供了第一步。",
    "tldr": "本文通过使用统计物理和随机矩阵理论的分析工具，精确地表征了简单图卷积网络在背景随机块模型上的泛化，提出了同质性在图卷积网络的泛化中的调制作用。",
    "en_tdlr": "This paper uses analytical tools from statistical physics and random matrix theory to precisely characterize the generalization of simple graph convolution networks on the contextual stochastic block model, and proposes the modulation effect of homophily on the generalization of graph neural networks."
}