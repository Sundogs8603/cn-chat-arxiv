{
    "title": "Text Embeddings by Weakly-Supervised Contrastive Pre-training",
    "abstract": "arXiv:2212.03533v2 Announce Type: replace  Abstract: This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.",
    "link": "https://arxiv.org/abs/2212.03533",
    "context": "Title: Text Embeddings by Weakly-Supervised Contrastive Pre-training\nAbstract: arXiv:2212.03533v2 Announce Type: replace  Abstract: This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.",
    "path": "papers/22/12/2212.03533.json",
    "total_tokens": 874,
    "translated_title": "用弱监督对比预训练进行文本嵌入",
    "translated_abstract": "本文介绍了E5，一种最先进的文本嵌入模型，可以很好地迁移到各种任务中。该模型以对比方式训练，使用我们精心策划的大规模文本配对数据集（名为CCPairs）的弱监督信号。E5可以作为通用嵌入模型用于任何需要单一文本向量表示的任务，如检索、聚类和分类，在零-shot和微调设置下表现出色。我们在BEIR和MTEB基准测试的56个数据集上进行了广泛评估。在零-shot设置下，E5是第一个在BEIR检索基准测试上击败强大的BM25基线且不使用任何标记数据的模型。在微调后，E5在MTEB基准测试上取得了最佳结果，胜过具有40倍参数的现有嵌入模型。",
    "tldr": "本文提出了一种名为E5的文本嵌入模型，通过弱监督对比训练方式，在未经过标记数据的情况下，在多个任务中表现卓越，是第一个在BEIR检索基准测试上击败BM25基线的模型，在微调后在MTEB基准测试上获得最佳结果。",
    "en_tdlr": "This paper introduces a text embedding model called E5, trained in a weakly supervised contrastive manner, excelling in multiple tasks without labeled data, being the first model to beat the BM25 baseline on the BEIR retrieval benchmark and achieving the best results on the MTEB benchmark after fine-tuning."
}