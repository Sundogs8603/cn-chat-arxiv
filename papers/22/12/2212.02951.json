{
    "title": "State Space Closure: Revisiting Endless Online Level Generation via Reinforcement Learning. (arXiv:2212.02951v2 [cs.AI] UPDATED)",
    "abstract": "In this paper, we revisit endless online level generation with the recently proposed experience-driven procedural content generation via reinforcement learning (EDRL) framework. Inspired by an observation that EDRL tends to generate recurrent patterns, we formulate a notion of state space closure which makes any stochastic state appeared possibly in an infinite-horizon online generation process can be found within a finite-horizon. Through theoretical analysis, we find that even though state space closure arises a concern about diversity, it generalises EDRL trained with a finite-horizon to the infinite-horizon scenario without deterioration of content quality. Moreover, we verify the quality and the diversity of contents generated by EDRL via empirical studies, on the widely used Super Mario Bros. benchmark. Experimental results reveal that the diversity of levels generated by EDRL is limited due to the state space closure, whereas their quality does not deteriorate in a horizon which",
    "link": "http://arxiv.org/abs/2212.02951",
    "context": "Title: State Space Closure: Revisiting Endless Online Level Generation via Reinforcement Learning. (arXiv:2212.02951v2 [cs.AI] UPDATED)\nAbstract: In this paper, we revisit endless online level generation with the recently proposed experience-driven procedural content generation via reinforcement learning (EDRL) framework. Inspired by an observation that EDRL tends to generate recurrent patterns, we formulate a notion of state space closure which makes any stochastic state appeared possibly in an infinite-horizon online generation process can be found within a finite-horizon. Through theoretical analysis, we find that even though state space closure arises a concern about diversity, it generalises EDRL trained with a finite-horizon to the infinite-horizon scenario without deterioration of content quality. Moreover, we verify the quality and the diversity of contents generated by EDRL via empirical studies, on the widely used Super Mario Bros. benchmark. Experimental results reveal that the diversity of levels generated by EDRL is limited due to the state space closure, whereas their quality does not deteriorate in a horizon which",
    "path": "papers/22/12/2212.02951.json",
    "total_tokens": 928,
    "translated_title": "状态空间封闭：通过强化学习重新审视无尽在线关卡生成",
    "translated_abstract": "本文中，我们使用最近提出的基于经验驱动的强化学习程序内容生成（EDRL）框架重新审视了无尽在线关卡生成。通过观察EDRL倾向于生成重复模式的现象，我们提出了状态空间封闭的概念，它使得无限时段的在线生成过程中可能出现的任何随机状态都可以在有限时段内找到。通过理论分析，我们发现状态空间封闭虽然引起了多样性的关注，但它将有限时段训练的EDRL推广到无限时段的情况，而不会降低生成内容的质量。此外，通过实证研究，我们验证了EDRL生成内容的质量和多样性，使用了广泛使用的Super Mario Bros基准测试进行评估。实验结果表明，由于状态空间封闭，EDRL生成的关卡多样性有限，但它们的质量在时段内没有下降。",
    "tldr": "这篇论文重新审视了无尽在线关卡生成，提出了状态空间封闭概念，将有限时段训练的经验驱动强化学习程序内容生成推广到无限时段，而不损失生成内容的质量。通过实证研究，验证了EDRL生成的内容质量和多样性。",
    "en_tdlr": "This paper reexamines endless online level generation with the experience-driven procedural content generation via reinforcement learning (EDRL) framework, proposing a notion of state space closure to generalize EDRL trained with a finite-horizon to the infinite-horizon scenario without deteriorating content quality. Empirical studies confirmed the quality and diversity of content generated by EDRL."
}