{
    "title": "Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs. (arXiv:2212.09034v3 [cs.LG] UPDATED)",
    "abstract": "Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training. This finding sheds new insights into understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting",
    "link": "http://arxiv.org/abs/2212.09034",
    "context": "Title: Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs. (arXiv:2212.09034v3 [cs.LG] UPDATED)\nAbstract: Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs' superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs' performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training. This finding sheds new insights into understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting",
    "path": "papers/22/12/2212.09034.json",
    "total_tokens": 1030,
    "translated_title": "GNN 和 MLP 相互联系揭示 GNN 在本质上是好的泛化器",
    "translated_abstract": "图神经网络（GNNs）是图表示学习的事实上模型类别，它们建立在多层感知器（MLP）体系结构之上，并增加了额外的消息传递层以允许特征在节点之间流动。本文猜测 GNNs 的表现出众不是其高级表现力的主要原因，而是其固有的泛化能力。这项发现提供了一种新的方式来理解 GNNs 的学习行为，并可以用作更深层次的分析工具。",
    "tldr": "本文通过将中间模型命名为 PMLP 并在测试时采用 GNNs 的架构，发现 GNNs 的表现出众不是其高级表现力的主要原因，而是其固有的泛化能力。"
}