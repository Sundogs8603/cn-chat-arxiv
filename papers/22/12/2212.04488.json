{
    "title": "Multi-Concept Customization of Text-to-Image Diffusion. (arXiv:2212.04488v2 [cs.CV] UPDATED)",
    "abstract": "While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (~6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works",
    "link": "http://arxiv.org/abs/2212.04488",
    "context": "Title: Multi-Concept Customization of Text-to-Image Diffusion. (arXiv:2212.04488v2 [cs.CV] UPDATED)\nAbstract: While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (~6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works",
    "path": "papers/22/12/2212.04488.json",
    "total_tokens": 1032,
    "translated_title": "多概念定制化的文本图像扩散",
    "translated_abstract": "尽管生成模型可以从大规模数据库中学习到高质量的概念图像，但用户通常希望合成他们自己的概念的实例（例如，他们的家庭、宠物或物品）。我们能否教会模型快速获取新的概念，只给出几个示例？此外，我们能否组合多个新概念？我们提出了一种高效的方法——定制化扩散，用于增强现有的文本图像模型。我们发现，只优化文本图像调节机制中的几个参数就足够强大，可以表示新概念，并能实现快速调整（约6分钟）。此外，我们通过闭合形式约束优化，可以联合训练多个概念或将多个精调模型合并为一个。我们的精调模型生成多个新概念的变异，并将它们与现有概念无缝地组合，形成新颖的场景。我们的方法优于或与多个基线和并发作品并驾齐驱。",
    "tldr": "该论文提出了一种称为Custom Diffusion的高效方法，用于增强现有的文本图像模型以实现多概念定制化，该方法只需优化文本图像调节机制中的几个参数就可以表示新概念并实现快速调整。同时，该方法可以联合训练多个概念或将多个精调模型合并为一个，并在生成多个新概念的变异并将它们与现有概念无缝地组合方面表现出色。",
    "en_tdlr": "The paper proposes Custom Diffusion, an efficient method for augmenting existing text-to-image models, to achieve multi-concept customization. The method only requires optimizing a few parameters in the text-to-image conditioning mechanism and enables fast tuning (~6 minutes). It can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. The fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings, outperforming or performing on par with several baselines and concurrent works."
}