{
    "title": "Understanding Sinusoidal Neural Networks. (arXiv:2212.01833v2 [cs.LG] UPDATED)",
    "abstract": "In this work, we investigate the structure and representation capacity of sinusoidal MLPs - multilayer perceptron networks that use sine as the activation function. These neural networks (known as neural fields) have become fundamental in representing common signals in computer graphics, such as images, signed distance functions, and radiance fields. This success can be primarily attributed to two key properties of sinusoidal MLPs: smoothness and compactness. These functions are smooth because they arise from the composition of affine maps with the sine function. This work provides theoretical results to justify the compactness property of sinusoidal MLPs and provides control mechanisms in the definition and training of these networks.  We propose to study a sinusoidal MLP by expanding it as a harmonic sum. First, we observe that its first layer can be seen as a harmonic dictionary, which we call the input sinusoidal neurons. Then, a hidden layer combines this dictionary using an affin",
    "link": "http://arxiv.org/abs/2212.01833",
    "context": "Title: Understanding Sinusoidal Neural Networks. (arXiv:2212.01833v2 [cs.LG] UPDATED)\nAbstract: In this work, we investigate the structure and representation capacity of sinusoidal MLPs - multilayer perceptron networks that use sine as the activation function. These neural networks (known as neural fields) have become fundamental in representing common signals in computer graphics, such as images, signed distance functions, and radiance fields. This success can be primarily attributed to two key properties of sinusoidal MLPs: smoothness and compactness. These functions are smooth because they arise from the composition of affine maps with the sine function. This work provides theoretical results to justify the compactness property of sinusoidal MLPs and provides control mechanisms in the definition and training of these networks.  We propose to study a sinusoidal MLP by expanding it as a harmonic sum. First, we observe that its first layer can be seen as a harmonic dictionary, which we call the input sinusoidal neurons. Then, a hidden layer combines this dictionary using an affin",
    "path": "papers/22/12/2212.01833.json",
    "total_tokens": 903,
    "translated_title": "理解正弦神经网络",
    "translated_abstract": "在这项工作中，我们研究了正弦多层感知机网络（MLP）的结构和表示能力，该网络使用正弦函数作为激活函数。这些神经网络（也被称为神经场）在计算机图形中表示常见信号（如图像、有符号距离函数和辐射场）方面具有基础性作用。正弦MLP的成功主要归因于其两个关键性质：平滑性和紧凑性。这些函数是平滑的，因为它们由仿射映射与正弦函数的组合得到。本工作提供了理论结果来证明正弦MLP的紧凑性，并在这些网络的定义和训练中提供了控制机制。我们提出通过将其展开为谐波和来研究正弦MLP。首先，我们观察到其第一层可以被看作是一个谐波字典，我们称之为输入正弦神经元。然后，隐藏层使用仿射变换来结合这个字典。",
    "tldr": "本研究探究了正弦神经网络的结构和表达能力，这些网络使用正弦函数作为激活函数。正弦MLP具有平滑性和紧凑性两个关键性质，并提供了控制机制来定义和训练这些网络。",
    "en_tdlr": "This paper investigates the structure and representation capacity of sinusoidal MLPs and highlights their smoothness and compactness properties. It provides theoretical results to justify the compactness property and offers control mechanisms for the definition and training of these networks."
}