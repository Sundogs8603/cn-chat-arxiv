{
    "title": "CREPE: Can Vision-Language Foundation Models Reason Compositionally?. (arXiv:2212.07796v3 [cs.CL] UPDATED)",
    "abstract": "A fundamental characteristic common to both human vision and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, we find that: across 7 architectures trained with 4 algorithms on massive datasets, they struggle at compositionality. To arrive at this conclusion, we introduce a new compositionality evaluation benchmark, CREPE, which measures two important aspects of compositionality identified by cognitive science literature: systematicity and productivity. To measure systematicity, CREPE consists of a test dataset containing over $370K$ image-text pairs and three different seen-unseen splits. The three splits are designed to test models trained on three popular training datasets: CC-12M, YFCC-15M, and LAION-400M. We also generate $325K$, $316K$, and $309K$ hard negative captions for a subset of the pairs. To test productivity, CREPE contains $17K$ image-text pairs with nine different complexities plus $",
    "link": "http://arxiv.org/abs/2212.07796",
    "context": "Title: CREPE: Can Vision-Language Foundation Models Reason Compositionally?. (arXiv:2212.07796v3 [cs.CL] UPDATED)\nAbstract: A fundamental characteristic common to both human vision and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, we find that: across 7 architectures trained with 4 algorithms on massive datasets, they struggle at compositionality. To arrive at this conclusion, we introduce a new compositionality evaluation benchmark, CREPE, which measures two important aspects of compositionality identified by cognitive science literature: systematicity and productivity. To measure systematicity, CREPE consists of a test dataset containing over $370K$ image-text pairs and three different seen-unseen splits. The three splits are designed to test models trained on three popular training datasets: CC-12M, YFCC-15M, and LAION-400M. We also generate $325K$, $316K$, and $309K$ hard negative captions for a subset of the pairs. To test productivity, CREPE contains $17K$ image-text pairs with nine different complexities plus $",
    "path": "papers/22/12/2212.07796.json",
    "total_tokens": 933,
    "translated_title": "CREPE：视觉-语言基础模型是否能够按组合思考？",
    "translated_abstract": "人类视觉和自然语言的一个共同特征是它们的构成性质。然而，尽管大型视觉和语言预训练为性能提升做出了贡献，但我们发现：在训练了4种算法的7个架构和海量数据集后，它们都难以实现组合性。为了得出这个结论，我们介绍了一个新的组合性评估基准，即CREPE，它衡量了认知科学文献所识别的组合性的两个重要方面：系统性和产出性。为了衡量系统性，CREPE包含一个测试数据集，其中包含超过37万个图像-文本对和三个不同的已看/未看分割。这三个分割是设计用来测试在三个流行的训练数据集CC-12M、YFCC-15M和LAION-400M上训练的模型。我们还为其中的一个子集生成了32.5万、31.6万和30.9万个困难的负面字幕。为了测试产出性能，CREPE包含了17,000个图像-文本对，涵盖了九种不同的复杂性和。",
    "tldr": "CREPE提出了一个新的组合性评估基准，衡量了模型的系统性和产出性能，发现大型预训练的视觉-语言基础模型在组合性上仍存在问题。",
    "en_tdlr": "CREPE introduced a new compositional evaluation benchmark to measure the systematicity and productivity of large vision and language pretraining models, and found that they still struggle with compositionality."
}