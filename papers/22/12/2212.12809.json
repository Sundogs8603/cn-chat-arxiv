{
    "title": "Understanding the Complexity Gains of Single-Task RL with a Curriculum. (arXiv:2212.12809v2 [cs.LG] UPDATED)",
    "abstract": "Reinforcement learning (RL) problems can be challenging without well-shaped rewards. Prior work on provably efficient RL methods generally proposes to address this issue with dedicated exploration strategies. However, another way to tackle this challenge is to reformulate it as a multi-task RL problem, where the task space contains not only the challenging task of interest but also easier tasks that implicitly function as a curriculum. Such a reformulation opens up the possibility of running existing multi-task RL methods as a more efficient alternative to solving a single challenging task from scratch. In this work, we provide a theoretical framework that reformulates a single-task RL problem as a multi-task RL problem defined by a curriculum. Under mild regularity conditions on the curriculum, we show that sequentially solving each task in the multi-task RL problem is more computationally efficient than solving the original single-task problem, without any explicit exploration bonuse",
    "link": "http://arxiv.org/abs/2212.12809",
    "context": "Title: Understanding the Complexity Gains of Single-Task RL with a Curriculum. (arXiv:2212.12809v2 [cs.LG] UPDATED)\nAbstract: Reinforcement learning (RL) problems can be challenging without well-shaped rewards. Prior work on provably efficient RL methods generally proposes to address this issue with dedicated exploration strategies. However, another way to tackle this challenge is to reformulate it as a multi-task RL problem, where the task space contains not only the challenging task of interest but also easier tasks that implicitly function as a curriculum. Such a reformulation opens up the possibility of running existing multi-task RL methods as a more efficient alternative to solving a single challenging task from scratch. In this work, we provide a theoretical framework that reformulates a single-task RL problem as a multi-task RL problem defined by a curriculum. Under mild regularity conditions on the curriculum, we show that sequentially solving each task in the multi-task RL problem is more computationally efficient than solving the original single-task problem, without any explicit exploration bonuse",
    "path": "papers/22/12/2212.12809.json",
    "total_tokens": 905,
    "translated_title": "通过课程理解单任务强化学习中的复杂度收益",
    "translated_abstract": "没有经过良好设计的奖励机制使得强化学习问题变得具有挑战性。现有的高效强化学习方法通常提出使用专门的探索策略来解决此问题。然而，另一种解决这个问题的方法是将其重新构造为一个多任务强化学习问题，其中任务空间不仅包含挑战性任务，还包含隐含的课程作为辅助。这样的重新构造打开了使用现有多任务强化学习方法作为解决单个具有挑战性任务的更高效的替代方法的可能性。本文提供了一个理论框架，将单任务强化学习问题重新构造为由课程定义的多任务强化学习问题。在课程有轻微正则化条件的情况下，我们证明了依次解决多任务RL问题中的每个任务比从头开始解决原始单任务问题更加计算上高效，而无需任何显式的探索奖励。",
    "tldr": "本文提出一个理论框架将单任务强化学习问题重新构造为由课程定义的多任务问题，证明在课程有轻微正则化条件的情况下，依次解决每个任务比直接解决原始单任务更加计算上高效。",
    "en_tdlr": "This paper proposes a theoretical framework to reformulate single-task reinforcement learning as a multi-task problem defined by a curriculum, and proves that sequentially solving each task in the multi-task problem with mild regularization on the curriculum is computationally more efficient than tackling the original single task directly."
}