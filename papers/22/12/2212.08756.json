{
    "title": "Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization. (arXiv:2212.08756v4 [cs.CL] UPDATED)",
    "abstract": "Machine learning models can reach high performance on benchmark natural language processing (NLP) datasets but fail in more challenging settings. We study this issue when a pre-trained model learns dataset artifacts in natural language inference (NLI), the topic of studying the logical relationship between a pair of text sequences. We provide a variety of techniques for analyzing and locating dataset artifacts inside the crowdsourced Stanford Natural Language Inference (SNLI) corpus. We study the stylistic pattern of dataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a unique multi-scale data augmentation technique with two distinct frameworks: a behavioral testing checklist at the sentence level and lexical synonym criteria at the word level. Specifically, our combination method enhances our model's resistance to perturbation testing, enabling it to continuously outperform the pre-trained baseline.",
    "link": "http://arxiv.org/abs/2212.08756",
    "context": "Title: Multi-Scales Data Augmentation Approach In Natural Language Inference For Artifacts Mitigation And Pre-Trained Model Optimization. (arXiv:2212.08756v4 [cs.CL] UPDATED)\nAbstract: Machine learning models can reach high performance on benchmark natural language processing (NLP) datasets but fail in more challenging settings. We study this issue when a pre-trained model learns dataset artifacts in natural language inference (NLI), the topic of studying the logical relationship between a pair of text sequences. We provide a variety of techniques for analyzing and locating dataset artifacts inside the crowdsourced Stanford Natural Language Inference (SNLI) corpus. We study the stylistic pattern of dataset artifacts in the SNLI. To mitigate dataset artifacts, we employ a unique multi-scale data augmentation technique with two distinct frameworks: a behavioral testing checklist at the sentence level and lexical synonym criteria at the word level. Specifically, our combination method enhances our model's resistance to perturbation testing, enabling it to continuously outperform the pre-trained baseline.",
    "path": "papers/22/12/2212.08756.json",
    "total_tokens": 872,
    "translated_title": "自然语言推理中的多尺度数据增强方法用于纠偏和预训练模型优化",
    "translated_abstract": "虽然机器学习模型在基准自然语言处理数据集上表现良好，但在更具挑战性的情境下却表现不佳。本文研究了已预训练模型学习自然语言推理（NLI）中数据集人为制造效应的问题，即逻辑关系在一对文本序列中的学习。我们提供了多种技术来分析和定位Stanford自然语言推理（SNLI）语料库内的数据集人为制造效应。我们研究了SNLI语料库中数据集人为制造效应的风格模式并采用了独特的多尺度数据增强技术，其中包括句子级的行为测试检查表和单词级的词汇同义词标准。具体而言，我们采用的组合方法提高了我们的模型对扰动测试的抵抗力，使其持续优于预训练基线。",
    "tldr": "本文通过多尺度数据增强方法纠偏了预训练模型在自然语言推理中遇到的数据集人为制造效应，提高了模型对扰动测试的抵抗力。",
    "en_tdlr": "This paper mitigates dataset artifacts in natural language inference for a pre-trained model using a unique multi-scale data augmentation technique, including a behavioral testing checklist at sentence-level and lexical synonym criteria at word-level, enhancing the model's resistance to perturbation testing and outperforming the pre-trained baseline."
}