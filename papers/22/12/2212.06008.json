{
    "title": "Who Evaluates the Evaluators? On Automatic Metrics for Assessing AI-based Offensive Code Generators. (arXiv:2212.06008v3 [cs.SE] UPDATED)",
    "abstract": "AI-based code generators are an emerging solution for automatically writing programs starting from descriptions in natural language, by using deep neural networks (Neural Machine Translation, NMT). In particular, code generators have been used for ethical hacking and offensive security testing by generating proof-of-concept attacks. Unfortunately, the evaluation of code generators still faces several issues. The current practice uses output similarity metrics, i.e., automatic metrics that compute the textual similarity of generated code with ground-truth references. However, it is not clear what metric to use, and which metric is most suitable for specific contexts. This work analyzes a large set of output similarity metrics on offensive code generators. We apply the metrics on two state-of-the-art NMT models using two datasets containing offensive assembly and Python code with their descriptions in the English language. We compare the estimates from the automatic metrics with human ev",
    "link": "http://arxiv.org/abs/2212.06008",
    "context": "Title: Who Evaluates the Evaluators? On Automatic Metrics for Assessing AI-based Offensive Code Generators. (arXiv:2212.06008v3 [cs.SE] UPDATED)\nAbstract: AI-based code generators are an emerging solution for automatically writing programs starting from descriptions in natural language, by using deep neural networks (Neural Machine Translation, NMT). In particular, code generators have been used for ethical hacking and offensive security testing by generating proof-of-concept attacks. Unfortunately, the evaluation of code generators still faces several issues. The current practice uses output similarity metrics, i.e., automatic metrics that compute the textual similarity of generated code with ground-truth references. However, it is not clear what metric to use, and which metric is most suitable for specific contexts. This work analyzes a large set of output similarity metrics on offensive code generators. We apply the metrics on two state-of-the-art NMT models using two datasets containing offensive assembly and Python code with their descriptions in the English language. We compare the estimates from the automatic metrics with human ev",
    "path": "papers/22/12/2212.06008.json",
    "total_tokens": 1143,
    "translated_title": "谁来评估评估者？关于用于评估基于人工智能的攻击代码生成器的自动度量的研究",
    "translated_abstract": "基于人工智能的代码生成器是从自然语言描述出发自动编写程序的新兴解决方案，采用深度神经网络（神经机器翻译）。特别地，代码生成器已经被用于进行伦理黑客和攻击性安全测试，以生成攻击的概念证明。不幸的是，代码生成器的评估仍然面临着许多问题。当前的做法是使用输出相似度度量标准，即自动度量标准来计算生成代码与参考翻译之间的文本相似度。然而，并不清楚应该使用哪种度量标准，以及哪种度量标准最适用于特定的情况。本研究分析了用于评估攻击代码生成器的大量输出相似度度量标准。我们将这些度量标准应用于两种最先进的神经机器翻译模型，使用包含英语语言描述的攻击装配代码和Python代码的两个数据集。我们比较了自动度量标准的估计值与生成代码的人工评估结果，以评估两者之间的相关性。我们的结果表明，相似度度量标准的选择对生成器的评估有重要影响，而自动度量标准可能不足以捕捉生成代码的质量。此外，我们提出了选取适当的度量标准的指南，并讨论了这项研究的局限性和未来方向。",
    "tldr": "本研究分析了用于评估攻击代码生成器的大量输出相似度度量标准，判断自动度量标准与人工评估结果之间的相关性，并提出了选取适当度量标准的指南。",
    "en_tdlr": "This paper analyzes a large set of output similarity metrics on offensive code generators, comparing the estimates from automatic metrics with human evaluations of generated code, and proposes guidelines for selecting appropriate metrics. The study highlights the impact of metric choice on code generator evaluation and suggests that automatic metrics might not be sufficient to capture the quality of generated code."
}