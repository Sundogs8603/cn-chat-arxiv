{
    "title": "ISAACS: Iterative Soft Adversarial Actor-Critic for Safety. (arXiv:2212.03228v2 [cs.LG] UPDATED)",
    "abstract": "The deployment of robots in uncontrolled environments requires them to operate robustly under previously unseen scenarios, like irregular terrain and wind conditions. Unfortunately, while rigorous safety frameworks from robust optimal control theory scale poorly to high-dimensional nonlinear dynamics, control policies computed by more tractable \"deep\" methods lack guarantees and tend to exhibit little robustness to uncertain operating conditions. This work introduces a novel approach enabling scalable synthesis of robust safety-preserving controllers for robotic systems with general nonlinear dynamics subject to bounded modeling error by combining game-theoretic safety analysis with adversarial reinforcement learning in simulation. Following a soft actor-critic scheme, a safety-seeking fallback policy is co-trained with an adversarial \"disturbance\" agent that aims to invoke the worst-case realization of model error and training-to-deployment discrepancy allowed by the designer's uncert",
    "link": "http://arxiv.org/abs/2212.03228",
    "context": "Title: ISAACS: Iterative Soft Adversarial Actor-Critic for Safety. (arXiv:2212.03228v2 [cs.LG] UPDATED)\nAbstract: The deployment of robots in uncontrolled environments requires them to operate robustly under previously unseen scenarios, like irregular terrain and wind conditions. Unfortunately, while rigorous safety frameworks from robust optimal control theory scale poorly to high-dimensional nonlinear dynamics, control policies computed by more tractable \"deep\" methods lack guarantees and tend to exhibit little robustness to uncertain operating conditions. This work introduces a novel approach enabling scalable synthesis of robust safety-preserving controllers for robotic systems with general nonlinear dynamics subject to bounded modeling error by combining game-theoretic safety analysis with adversarial reinforcement learning in simulation. Following a soft actor-critic scheme, a safety-seeking fallback policy is co-trained with an adversarial \"disturbance\" agent that aims to invoke the worst-case realization of model error and training-to-deployment discrepancy allowed by the designer's uncert",
    "path": "papers/22/12/2212.03228.json",
    "total_tokens": 1212,
    "translated_title": "ISAACS：安全的迭代软对抗 Actor-Critic 算法",
    "translated_abstract": "在不受控制的环境中部署机器人需要它们能够在之前未见过的情况下稳健地运行，例如不规则的地形和风力条件。由于优化控制理论的严格安全框架难以扩展到高维非线性动态系统，而更易处理的“深度”方法计算出的控制策略缺乏保证，并且往往在不确定的操作条件下表现出很少的稳健性。本文提出了一种新的方法，将博弈论安全分析与仿真中的对抗式强化学习相结合，使具有一般非线性动态的机器人系统能够进行可扩展的鲁棒安全控制器的综合合成，受到有界建模误差的限制。采用软性 actor-critic 方法，同时进行一个安全的回退策略和一个称为“干扰”的对抗智能体的协同训练，该对抗智能体致力于唤起设计者不确定性假设下允许的模型误差和训练-部署偏差的最坏情况实现。同时，一个评价网络被训练来评估主策略和回退策略的安全性，提供一个自适应的软约束来指导探索和限制不良行为。在各种任务的实验中表明，我们的框架可以有效地学习避免碰撞的策略，并超越了标准深度强化学习算法在安全问题上的表现，即使存在显著的建模不确定性。",
    "tldr": "本文提出了一种新的算法，ISAACS，通过将博弈论安全分析与对抗强化学习相结合，使机器人系统能够进行可扩展的鲁棒安全控制。实验结果表明，该算法可以有效地学习并避免碰撞，并在安全问题上超越标准的深度强化学习算法。",
    "en_tdlr": "This paper proposes an iterative soft adversarial actor-critic (ISAACS) algorithm that combines game-theoretic safety analysis and adversarial reinforcement learning to synthesize scalable and robust safety-preserving controllers for robotic systems. Experiments show that ISAACS can efficiently learn policies that avoid collision and outperform standard deep reinforcement learning algorithms for safety problems, even in the presence of significant modeling uncertainty."
}