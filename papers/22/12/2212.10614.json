{
    "title": "MolCPT: Molecule Continuous Prompt Tuning to Generalize Molecular Representation Learning. (arXiv:2212.10614v2 [cs.LG] UPDATED)",
    "abstract": "Molecular representation learning is crucial for the problem of molecular property prediction, where graph neural networks (GNNs) serve as an effective solution due to their structure modeling capabilities. Since labeled data is often scarce and expensive to obtain, it is a great challenge for GNNs to generalize in the extensive molecular space. Recently, the training paradigm of \"pre-train, fine-tune\" has been leveraged to improve the generalization capabilities of GNNs. It uses self-supervised information to pre-train the GNN, and then performs fine-tuning to optimize the downstream task with just a few labels. However, pre-training does not always yield statistically significant improvement, especially for self-supervised learning with random structural masking. In fact, the molecular structure is characterized by motif subgraphs, which are frequently occurring and influence molecular properties. To leverage the task-related motifs, we propose a novel paradigm of \"pre-train, prompt,",
    "link": "http://arxiv.org/abs/2212.10614",
    "context": "Title: MolCPT: Molecule Continuous Prompt Tuning to Generalize Molecular Representation Learning. (arXiv:2212.10614v2 [cs.LG] UPDATED)\nAbstract: Molecular representation learning is crucial for the problem of molecular property prediction, where graph neural networks (GNNs) serve as an effective solution due to their structure modeling capabilities. Since labeled data is often scarce and expensive to obtain, it is a great challenge for GNNs to generalize in the extensive molecular space. Recently, the training paradigm of \"pre-train, fine-tune\" has been leveraged to improve the generalization capabilities of GNNs. It uses self-supervised information to pre-train the GNN, and then performs fine-tuning to optimize the downstream task with just a few labels. However, pre-training does not always yield statistically significant improvement, especially for self-supervised learning with random structural masking. In fact, the molecular structure is characterized by motif subgraphs, which are frequently occurring and influence molecular properties. To leverage the task-related motifs, we propose a novel paradigm of \"pre-train, prompt,",
    "path": "papers/22/12/2212.10614.json",
    "total_tokens": 922,
    "translated_title": "MolCPT：分子连续提示调整以推广分子表示学习",
    "translated_abstract": "分子表示学习对于分子属性预测问题至关重要，而图神经网络(GNNs)由于其结构建模能力成为了一种有效的解决方案。由于标记数据通常稀缺且昂贵，对GNNs在广泛的分子空间中进行推广是一个巨大的挑战。最近，“预训练，微调”训练范式被用来改善GNNs的泛化能力。它使用自监督信息来预训练GNN，然后通过微调来优化下游任务，仅使用少量标签。然而，预训练并不总是能够产生统计上显著的改进，尤其是对于具有随机结构掩码的自监督学习。实际上，分子结构具有频繁出现且影响分子属性的模式子图。为了利用与任务相关的模式子图，我们提出了一种新的“预训练，提示，微调”的训练范式。",
    "tldr": "该论文介绍了一种新的训练范式\"MolCPT\"，用于改善图神经网络在分子表示学习中的泛化能力。该方法利用任务相关的模式子图来进行预训练和微调，以提高对广泛的分子空间的推广能力。",
    "en_tdlr": "This paper presents a new training paradigm, \"MolCPT\", for improving the generalization capabilities of graph neural networks in molecular representation learning. The method utilizes task-related motif subgraphs for pre-training and fine-tuning, aiming to enhance the generalization ability in the extensive molecular space."
}