{
    "title": "Bagging is an Optimal PAC Learner. (arXiv:2212.02264v3 [cs.LG] UPDATED)",
    "abstract": "Determining the optimal sample complexity of PAC learning in the realizable setting was a central open problem in learning theory for decades. Finally, the seminal work by Hanneke (2016) gave an algorithm with a provably optimal sample complexity. His algorithm is based on a careful and structured sub-sampling of the training data and then returning a majority vote among hypotheses trained on each of the sub-samples. While being a very exciting theoretical result, it has not had much impact in practice, in part due to inefficiency, since it constructs a polynomial number of sub-samples of the training data, each of linear size.  In this work, we prove the surprising result that the practical and classic heuristic bagging (a.k.a. bootstrap aggregation), due to Breiman (1996), is in fact also an optimal PAC learner. Bagging pre-dates Hanneke's algorithm by twenty years and is taught in most undergraduate machine learning courses. Moreover, we show that it only requires a logarithmic numb",
    "link": "http://arxiv.org/abs/2212.02264",
    "context": "Title: Bagging is an Optimal PAC Learner. (arXiv:2212.02264v3 [cs.LG] UPDATED)\nAbstract: Determining the optimal sample complexity of PAC learning in the realizable setting was a central open problem in learning theory for decades. Finally, the seminal work by Hanneke (2016) gave an algorithm with a provably optimal sample complexity. His algorithm is based on a careful and structured sub-sampling of the training data and then returning a majority vote among hypotheses trained on each of the sub-samples. While being a very exciting theoretical result, it has not had much impact in practice, in part due to inefficiency, since it constructs a polynomial number of sub-samples of the training data, each of linear size.  In this work, we prove the surprising result that the practical and classic heuristic bagging (a.k.a. bootstrap aggregation), due to Breiman (1996), is in fact also an optimal PAC learner. Bagging pre-dates Hanneke's algorithm by twenty years and is taught in most undergraduate machine learning courses. Moreover, we show that it only requires a logarithmic numb",
    "path": "papers/22/12/2212.02264.json",
    "total_tokens": 818,
    "translated_title": "Bagging是一种最优的PAC学习器",
    "translated_abstract": "在可实现的情况下确定PAC学习的最优样本复杂度是学习理论几十年来的中心难题。Han口的开创性工作给出了一个具有可证明最优样本复杂度的算法。他的算法基于对训练数据的谨慎和结构化子采样，然后返回在每个子样本上训练的假设的多数投票。在这项工作中，我们证明了实用且经典的heuristic bagging (即自助聚合)，是一种最优的PAC学习器。我们的主要技术贡献是对Bagging的Rademacher复杂度的紧密分析。",
    "tldr": "Bagging是一种最优的PAC学习器,具有可证明最优的样本复杂度。相对于Hanneke的算法，Bagging更为高效，且只需要logarithmic数量的子样本。",
    "en_tdlr": "Bagging is an optimal PAC learner, with a provably optimal sample complexity. Bagging is more efficient than Hanneke's algorithm, and only requires a logarithmic number of sub-samples."
}