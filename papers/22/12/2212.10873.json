{
    "title": "Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners. (arXiv:2212.10873v3 [cs.CL] UPDATED)",
    "abstract": "Through in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified th",
    "link": "http://arxiv.org/abs/2212.10873",
    "context": "Title: Prompt-Augmented Linear Probing: Scaling beyond the Limit of Few-shot In-Context Learners. (arXiv:2212.10873v3 [cs.CL] UPDATED)\nAbstract: Through in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified th",
    "path": "papers/22/12/2212.10873.json",
    "total_tokens": 914,
    "translated_title": "提升线性探测：超越少样本学习的极限",
    "translated_abstract": "通过上下文学习，大规模语言模型可以高效地进行少样本学习，而无需进行额外的模型微调。然而，由于底层语言模型的固有输入长度限制，上下文学习的性能在可用训练样本数量上并不具有可伸缩性。与此同时，许多研究表明语言模型也是强大的特征提取器，使其能够以黑匣子的方式使用，并实现线性探测范式，即在预先提取的输入表示之上训练轻量级鉴别器。本文提出了prompt-augmented linear probing（PALP），它是线性探测和上下文学习的混合体，兼具二者的优点。PALP继承了线性探测的可伸缩性和使语言模型通过将输入定制为更易理解的形式来派生更有意义表示的能力。在各种数据集的深入调查中，我们验证了PALP在少样本和零样本情况下均胜过最先进的方法。",
    "tldr": "本论文提出了一种混合线性探测和上下文学习的方法，结合了两者的优点，旨在提高模型在少样本和零样本情况下的性能表现。",
    "en_tdlr": "This paper proposes a hybrid method, prompt-augmented linear probing (PALP), that combines the advantages of linear probing and in-context learning to improve the model's performance in few-shot and zero-shot scenarios."
}