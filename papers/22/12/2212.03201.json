{
    "title": "Misspecification in Inverse Reinforcement Learning. (arXiv:2212.03201v2 [cs.LG] UPDATED)",
    "abstract": "The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function $R$ from a policy $\\pi$. To do this, we need a model of how $\\pi$ relates to $R$. In the current literature, the most common models are optimality, Boltzmann rationality, and causal entropy maximisation. One of the primary motivations behind IRL is to infer human preferences from human behaviour. However, the true relationship between human preferences and human behaviour is much more complex than any of the models currently used in IRL. This means that they are misspecified, which raises the worry that they might lead to unsound inferences if applied to real-world data. In this paper, we provide a mathematical analysis of how robust different IRL models are to misspecification, and answer precisely how the demonstrator policy may differ from each of the standard models before that model leads to faulty inferences about the reward function $R$. We also introduce a framework for reasoning about misspecificatio",
    "link": "http://arxiv.org/abs/2212.03201",
    "context": "Title: Misspecification in Inverse Reinforcement Learning. (arXiv:2212.03201v2 [cs.LG] UPDATED)\nAbstract: The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function $R$ from a policy $\\pi$. To do this, we need a model of how $\\pi$ relates to $R$. In the current literature, the most common models are optimality, Boltzmann rationality, and causal entropy maximisation. One of the primary motivations behind IRL is to infer human preferences from human behaviour. However, the true relationship between human preferences and human behaviour is much more complex than any of the models currently used in IRL. This means that they are misspecified, which raises the worry that they might lead to unsound inferences if applied to real-world data. In this paper, we provide a mathematical analysis of how robust different IRL models are to misspecification, and answer precisely how the demonstrator policy may differ from each of the standard models before that model leads to faulty inferences about the reward function $R$. We also introduce a framework for reasoning about misspecificatio",
    "path": "papers/22/12/2212.03201.json",
    "total_tokens": 782,
    "translated_title": "逆强化学习中的错误规范问题",
    "translated_abstract": "逆强化学习的目标是从策略π中推断出奖励函数R。为此，我们需要一个表明π与R关系的模型。目前文献中最常见的模型是最优性、Boltzmann合理性和因果熵最大化。IRL的一个主要动机是从人类行为中推断出人类偏好。然而，人类偏好与人类行为之间的真实关系比IRL目前使用的任何模型都更加复杂。这意味着它们被规范错误，这引发了这样的担心：如果应用于真实世界的数据，它们可能会导致不良推断。",
    "tldr": "本文研究了逆强化学习中的错误规范问题，探讨了不同模型与演示策略之间的偏差，以及在何种情况下模型会导致误导性推断。",
    "en_tdlr": "This paper examines the misspecification problem in inverse reinforcement learning, exploring the deviation between different models and the demonstrated policy that can lead to misleading inferences."
}