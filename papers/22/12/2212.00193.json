{
    "title": "Distilling Reasoning Capabilities into Smaller Language Models. (arXiv:2212.00193v2 [cs.LG] UPDATED)",
    "abstract": "Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models.  In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP",
    "link": "http://arxiv.org/abs/2212.00193",
    "context": "Title: Distilling Reasoning Capabilities into Smaller Language Models. (arXiv:2212.00193v2 [cs.LG] UPDATED)\nAbstract: Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models.  In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP",
    "path": "papers/22/12/2212.00193.json",
    "total_tokens": 1135,
    "translated_title": "把推理能力压缩到更小的语言模型中",
    "translated_abstract": "逐步推理的方法（如CoT）在具有推理能力的大型语言模型中被证明非常有效。然而，CoT方法的成功基本上是与模型大小密切相关的，并且通常需要十亿级参数规模的模型才能使CoT工作。在本文中，我们提出了一种知识蒸馏方法，利用较大模型的逐步CoT推理能力，并将这些能力蒸馏到更小的模型中。在这项工作中，我们提出了一种替代推理方案：苏格拉底式CoT，它学习将原始问题分解为一系列子问题，并用它来指导中间推理步骤。我们使用苏格拉底式CoT来训练两个小型蒸馏模型的组合：问题分解器和子问题求解器。在实践中，给定一个新问题，这两个蒸馏模型以同步的方式工作，以分解和解决复杂的问题。在多个推理数据集（GSM8K，StrategyQA和SVAMP）上，我们展示了我们的蒸馏模型学会了高精度地执行复杂的推理任务，通常优于没有专门使用CoT推理方法进行训练的大型模型。",
    "tldr": "本文提出了一种知识蒸馏方法，可以把大型语言模型的逐步推理能力蒸馏到更小的模型中，提出了一种替代推理方案，使用苏格拉底式CoT来训练两个小型蒸馏模型的组合，可以用来分解和解决复杂的问题，且在多个推理数据集上表现出高精度的复杂推理能力，经常优于那些没有经过CoT推理方法训练的大模型。",
    "en_tdlr": "This paper proposes a knowledge distillation approach that can distill the step-by-step reasoning capabilities of large language models into smaller models by introducing an alternative reasoning scheme called Socratic CoT. The approach trains a combination of two small distilled models, a problem decomposer and a subproblem solver, which work together to decompose and solve complex problems. Results show that the distilled models achieve high accuracy on multiple reasoning datasets and often outperform large models that were not trained using CoT reasoning approaches."
}