{
    "title": "I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation. (arXiv:2212.09246v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained language models, despite their rapid advancements powered by scale, still fall short of robust commonsense capabilities. And yet, scale appears to be the winning recipe; after all, the largest models seem to have acquired the largest amount of commonsense capabilities. Or is it?  In this paper, we investigate the possibility of a seemingly impossible match: can smaller language models with dismal commonsense capabilities (i.e., GPT-2), ever win over models that are orders of magnitude larger and better (i.e., GPT-3), if the smaller models are powered with novel commonsense distillation algorithms? The key intellectual question we ask here is whether it is possible, if at all, to design a learning algorithm that does not benefit from scale, yet leads to a competitive level of commonsense acquisition. In this work, we study the generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g.",
    "link": "http://arxiv.org/abs/2212.09246",
    "context": "Title: I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation. (arXiv:2212.09246v2 [cs.CL] UPDATED)\nAbstract: Pre-trained language models, despite their rapid advancements powered by scale, still fall short of robust commonsense capabilities. And yet, scale appears to be the winning recipe; after all, the largest models seem to have acquired the largest amount of commonsense capabilities. Or is it?  In this paper, we investigate the possibility of a seemingly impossible match: can smaller language models with dismal commonsense capabilities (i.e., GPT-2), ever win over models that are orders of magnitude larger and better (i.e., GPT-3), if the smaller models are powered with novel commonsense distillation algorithms? The key intellectual question we ask here is whether it is possible, if at all, to design a learning algorithm that does not benefit from scale, yet leads to a competitive level of commonsense acquisition. In this work, we study the generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g.",
    "path": "papers/22/12/2212.09246.json",
    "total_tokens": 892,
    "translated_title": "I2D2: 基于NeuroLogic和自我模仿的归纳知识蒸馏",
    "translated_abstract": "尽管预训练语言模型在规模方面不断强化，但仍缺乏坚实的常识功能。然而，规模似乎是制胜法宝；毕竟，最大的模型似乎已经获得了最多的常识功能。这篇论文探究了似乎不可能实现的匹配：如果小型语言模型（如GPT-2）通过新颖的常识蒸馏算法得到动力，它们是否能赢过比它们大数个数量级并且更优秀的模型（如GPT-3）？我们所提出的关键智力问题是，是否可能设计一种学习算法，它并不受到规模的好处，而却有竞争力的常识获取水平。在本文中，我们研究了常识知识的生成模型，重点关注生成通用语句的任务，即关于日常概念的常识事实陈述。",
    "tldr": "本论文探究了通过常识蒸馏算法强化小型语言模型的能力，挑战大型模型的常识获取能力，提出了一种不依赖规模的学习算法方案。"
}