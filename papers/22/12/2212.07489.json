{
    "title": "SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2212.07489v2 [cs.LG] UPDATED)",
    "abstract": "The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for centralised training with decentralised execution. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex *closed-loop* policies. In particular, we show that an *open-loop* policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation. We also introduce the extended partial observability challenge (EPO), which au",
    "link": "http://arxiv.org/abs/2212.07489",
    "context": "Title: SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2212.07489v2 [cs.LG] UPDATED)\nAbstract: The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for centralised training with decentralised execution. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex *closed-loop* policies. In particular, we show that an *open-loop* policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation. We also introduce the extended partial observability challenge (EPO), which au",
    "path": "papers/22/12/2212.07489.json",
    "total_tokens": 926,
    "translated_title": "SMACv2：一种改进的合作多智能体强化学习基准",
    "translated_abstract": "具有挑战性的基准在机器学习的最新进展中起到了关键作用。在合作多智能体强化学习中，星际争霸多智能体挑战（SMAC）已成为集中训练和分散执行的受欢迎的测试平台。然而，在多年的持续改进后，算法现在已经实现了接近完美的性能。在这项工作中，我们进行了新的分析，证明SMAC缺乏需要复杂的“闭环”策略的随机性和部分可观察性。特别是，我们展示了一个只依赖于时间步骤的“开环”策略可以在许多SMAC场景中实现非平凡的胜率。为了解决这个限制，我们引入了SMACv2，这是基准的一个新版本，其中的场景是程序生成的，并且在评估过程中要求智能体对以前未见过的设置（来自同一分布）进行泛化。我们还引入了扩展的部分可观察性挑战（EPO），它可以测试智能体在部分可观察态下的学习能力。",
    "tldr": "SMACv2是一个改进的合作多智能体强化学习基准，通过引入随机性和部分可观察性的限制，挑战算法在复杂场景中的泛化能力。",
    "en_tdlr": "SMACv2 is an improved benchmark for cooperative multi-agent reinforcement learning, challenging algorithms to generalize in complex scenarios by introducing stochasticity and partial observability limitations."
}