{
    "title": "Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training. (arXiv:2212.10503v2 [cs.CL] UPDATED)",
    "abstract": "Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new embeddings requires a full forward and backward pass over the entire model. We propose mini-model adaptation, a compute-efficient alternative that builds a shallow mini-model from a fraction of a large model's parameters. New language-specific embeddings can then be efficiently trained over the mini-model and plugged into the aligned large model for rapid cross-lingual transfer. We explore two approaches to learn mini-models: MiniJoint, which jointly pretrains the primary model and the mini-model using a single transformer with a secondary MLM head at a middle layer; and MiniPost, where we start from a regular pretrained model, build a mini-model by extracting and freezing a few layers, and learn a ",
    "link": "http://arxiv.org/abs/2212.10503",
    "context": "Title: Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training. (arXiv:2212.10503v2 [cs.CL] UPDATED)\nAbstract: Prior work shows that it is possible to expand pretrained Masked Language Models (MLMs) to new languages by learning a new set of embeddings, while keeping the transformer body frozen. Despite learning a small subset of parameters, this approach is not compute-efficient, as training the new embeddings requires a full forward and backward pass over the entire model. We propose mini-model adaptation, a compute-efficient alternative that builds a shallow mini-model from a fraction of a large model's parameters. New language-specific embeddings can then be efficiently trained over the mini-model and plugged into the aligned large model for rapid cross-lingual transfer. We explore two approaches to learn mini-models: MiniJoint, which jointly pretrains the primary model and the mini-model using a single transformer with a secondary MLM head at a middle layer; and MiniPost, where we start from a regular pretrained model, build a mini-model by extracting and freezing a few layers, and learn a ",
    "path": "papers/22/12/2212.10503.json",
    "total_tokens": 966,
    "translated_title": "迷你模型适应：通过对齐的浅层训练高效地将预训练模型扩展到新语言上",
    "translated_abstract": "先前的研究表明，通过学习一组新的嵌入向量，并保持transformer主体部分冻结，可以将预训练的遮蔽语言模型（MLM）扩展到新语言。尽管只学习了一小部分参数，但这种方法在计算效率上并不高，因为训练新的嵌入向量需要对整个模型进行完整的前向和反向传播。我们提出了迷你模型适应，一种计算高效的替代方案，通过从大型模型的一小部分参数中构建一个浅层迷你模型。然后可以在迷你模型上高效地训练新的语言特定嵌入向量，并将其插入到对齐的大型模型中进行快速的跨语言传输。我们探索了两种学习迷你模型的方法：MiniJoint，它使用一个具有中间层次上辅助MLM头的单个transformer同时预训练主模型和迷你模型。MiniPost则从常规预训练模型开始，通过提取和冻结几层来构建迷你模型，并学习语言特定的嵌入向量。",
    "tldr": "使用迷你模型适应的方法，通过构建浅层迷你模型以及高效训练新的语言特定嵌入向量，实现了将预训练模型扩展到新语言上的快速跨语言传输。",
    "en_tdlr": "Mini-model adaptation proposes a compute-efficient alternative for extending pretrained models to new languages by building shallow mini-models and efficiently training new language-specific embeddings, enabling rapid cross-lingual transfer."
}