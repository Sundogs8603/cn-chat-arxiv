{
    "title": "Privately Estimating a Gaussian: Efficient, Robust and Optimal. (arXiv:2212.08018v2 [cs.DS] UPDATED)",
    "abstract": "In this work, we give efficient algorithms for privately estimating a Gaussian distribution in both pure and approximate differential privacy (DP) models with optimal dependence on the dimension in the sample complexity. In the pure DP setting, we give an efficient algorithm that estimates an unknown $d$-dimensional Gaussian distribution up to an arbitrary tiny total variation error using $\\widetilde{O}(d^2 \\log \\kappa)$ samples while tolerating a constant fraction of adversarial outliers. Here, $\\kappa$ is the condition number of the target covariance matrix. The sample bound matches best non-private estimators in the dependence on the dimension (up to a polylogarithmic factor). We prove a new lower bound on differentially private covariance estimation to show that the dependence on the condition number $\\kappa$ in the above sample bound is also tight. Prior to our work, only identifiability results (yielding inefficient super-polynomial time algorithms) were known for the problem. In",
    "link": "http://arxiv.org/abs/2212.08018",
    "context": "Title: Privately Estimating a Gaussian: Efficient, Robust and Optimal. (arXiv:2212.08018v2 [cs.DS] UPDATED)\nAbstract: In this work, we give efficient algorithms for privately estimating a Gaussian distribution in both pure and approximate differential privacy (DP) models with optimal dependence on the dimension in the sample complexity. In the pure DP setting, we give an efficient algorithm that estimates an unknown $d$-dimensional Gaussian distribution up to an arbitrary tiny total variation error using $\\widetilde{O}(d^2 \\log \\kappa)$ samples while tolerating a constant fraction of adversarial outliers. Here, $\\kappa$ is the condition number of the target covariance matrix. The sample bound matches best non-private estimators in the dependence on the dimension (up to a polylogarithmic factor). We prove a new lower bound on differentially private covariance estimation to show that the dependence on the condition number $\\kappa$ in the above sample bound is also tight. Prior to our work, only identifiability results (yielding inefficient super-polynomial time algorithms) were known for the problem. In",
    "path": "papers/22/12/2212.08018.json",
    "total_tokens": 1058,
    "translated_title": "私下高效、鲁棒且最优地估计高斯分布",
    "translated_abstract": "本文提出了在纯和近似差分隐私模型下，针对高斯分布的高效算法，并且样本复杂度与维度的依赖是最优的。在纯差分隐私设置下，我们提供了一种有效的算法，使用 $\\widetilde{O}(d^2 \\log \\kappa)$ 个样本，可以对未知的 $d$ 维高斯分布进行任意微小差异总变差误差的估计，同时容忍恶意离群值的存在。这里，$\\kappa$ 是目标协方差矩阵的条件数。该样本上界在维度（多对数因子）的依赖上与最佳非隐私估计器相匹配, 我们证明了隐私协方差矩阵估计的一个新的下界，为了表明上文样本上界与目标协方差矩阵的条件数 $\\kappa$ 的依赖关系也是最紧密的。在本工作之前，我們只知道该问题的可识别性结果（导致非常低效的超多项式时间算法）。",
    "tldr": "本论文提出了在纯和近似差分隐私模型下，对于未知的 $d$ 维高斯分布进行任意微小差异总变差误差的高效算法，并容忍恶意离群值的存在， 样本复杂度与维度的依赖是最优的；我们还证明了样本上界与目标协方差矩阵的条件数 $\\kappa$ 的依赖关系也是最紧密的。",
    "en_tdlr": "This paper proposes efficient algorithms for privately estimating a Gaussian distribution in both pure and approximate differential privacy models. The sample complexity has optimal dependence on the dimension and can tolerate a constant fraction of adversarial outliers. The sample bound matches the best non-private estimators in the dependence on the dimension (up to a polylogarithmic factor). A new lower bound on differentially private covariance estimation is also proved to show the sample bound's tight dependence on the condition number of the target covariance matrix."
}