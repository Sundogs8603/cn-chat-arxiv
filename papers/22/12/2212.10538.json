{
    "title": "HyperBO+: Pre-training a universal prior for Bayesian optimization with hierarchical Gaussian processes. (arXiv:2212.10538v2 [cs.LG] UPDATED)",
    "abstract": "Bayesian optimization (BO), while proved highly effective for many black-box function optimization tasks, requires practitioners to carefully select priors that well model their functions of interest. Rather than specifying by hand, researchers have investigated transfer learning based methods to automatically learn the priors, e.g. multi-task BO (Swersky et al., 2013), few-shot BO (Wistuba and Grabocka, 2021) and HyperBO (Wang et al., 2022). However, those prior learning methods typically assume that the input domains are the same for all tasks, weakening their ability to use observations on functions with different domains or generalize the learned priors to BO on different search spaces. In this work, we present HyperBO+: a pre-training approach for hierarchical Gaussian processes that enables the same prior to work universally for Bayesian optimization on functions with different domains. We propose a two-step pre-training method and analyze its appealing asymptotic properties and ",
    "link": "http://arxiv.org/abs/2212.10538",
    "context": "Title: HyperBO+: Pre-training a universal prior for Bayesian optimization with hierarchical Gaussian processes. (arXiv:2212.10538v2 [cs.LG] UPDATED)\nAbstract: Bayesian optimization (BO), while proved highly effective for many black-box function optimization tasks, requires practitioners to carefully select priors that well model their functions of interest. Rather than specifying by hand, researchers have investigated transfer learning based methods to automatically learn the priors, e.g. multi-task BO (Swersky et al., 2013), few-shot BO (Wistuba and Grabocka, 2021) and HyperBO (Wang et al., 2022). However, those prior learning methods typically assume that the input domains are the same for all tasks, weakening their ability to use observations on functions with different domains or generalize the learned priors to BO on different search spaces. In this work, we present HyperBO+: a pre-training approach for hierarchical Gaussian processes that enables the same prior to work universally for Bayesian optimization on functions with different domains. We propose a two-step pre-training method and analyze its appealing asymptotic properties and ",
    "path": "papers/22/12/2212.10538.json",
    "total_tokens": 901,
    "translated_title": "HyperBO+：使用分层高斯过程为贝叶斯优化预先训练通用先验",
    "translated_abstract": "贝叶斯优化在许多黑盒函数优化任务中被证明非常有效，但需要实践者精心选择适合其感兴趣函数的先验概率模型。研究人员已经探索了基于迁移学习的方法，如多任务BO、少样本BO和HyperBO，来自动学习先验概率。然而，这些方法通常假设所有任务的输入空间相同，限制了它们在不同输入空间上使用观测结果或推广学习到的先验概率模型的能力。本文提出了HyperBO+，这是一种基于分层高斯过程的预先训练方法，能够在具有不同输入空间的函数上普遍适用于贝叶斯优化。我们提出了一种两步预先训练方法，并分析了其吸引人的渐近性质及特点。",
    "tldr": "本文提出了一种名为HyperBO+的方法，通过使用分层高斯过程的预先训练，实现了在具有不同输入空间的函数上普适于贝叶斯优化。研究人员设计了一种两步预先训练方法，并分析了其吸引人的渐近性质。",
    "en_tdlr": "This paper proposes a method called HyperBO+ that achieves universality for Bayesian optimization on functions with different input spaces by using pre-training with hierarchical Gaussian processes. The researchers design a two-step pre-training method and analyze its appealing asymptotic properties."
}