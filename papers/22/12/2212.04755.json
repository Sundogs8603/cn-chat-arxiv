{
    "title": "From Clozing to Comprehending: Retrofitting Pre-trained Masked Language Model to Pre-trained Machine Reader. (arXiv:2212.04755v2 [cs.CL] UPDATED)",
    "abstract": "We present Pre-trained Machine Reader (PMR), a novel method for retrofitting pre-trained masked language models (MLMs) to pre-trained machine reading comprehension (MRC) models without acquiring labeled data. PMR can resolve the discrepancy between model pre-training and downstream fine-tuning of existing MLMs. To build the proposed PMR, we constructed a large volume of general-purpose and high-quality MRC-style training data by using Wikipedia hyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style pre-training. Apart from its simplicity, PMR effectively solves extraction tasks, such as Extractive Question Answering and Named Entity Recognition. PMR shows tremendous improvements over existing approaches, especially in low-resource scenarios. When applied to the sequence classification task in the MRC formulation, PMR enables the extraction of high-quality rationales to explain the classification process, thereby providing greater prediction explainability. PMR als",
    "link": "http://arxiv.org/abs/2212.04755",
    "context": "Title: From Clozing to Comprehending: Retrofitting Pre-trained Masked Language Model to Pre-trained Machine Reader. (arXiv:2212.04755v2 [cs.CL] UPDATED)\nAbstract: We present Pre-trained Machine Reader (PMR), a novel method for retrofitting pre-trained masked language models (MLMs) to pre-trained machine reading comprehension (MRC) models without acquiring labeled data. PMR can resolve the discrepancy between model pre-training and downstream fine-tuning of existing MLMs. To build the proposed PMR, we constructed a large volume of general-purpose and high-quality MRC-style training data by using Wikipedia hyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style pre-training. Apart from its simplicity, PMR effectively solves extraction tasks, such as Extractive Question Answering and Named Entity Recognition. PMR shows tremendous improvements over existing approaches, especially in low-resource scenarios. When applied to the sequence classification task in the MRC formulation, PMR enables the extraction of high-quality rationales to explain the classification process, thereby providing greater prediction explainability. PMR als",
    "path": "papers/22/12/2212.04755.json",
    "total_tokens": 1016,
    "translated_title": "从Clozing到理解：将预训练的遮蔽语言模型改造为预训练的机器阅读器",
    "translated_abstract": "本文提出了一种名为“Pre-trained Machine Reader (PMR)”的新方法，用于将预训练的遮蔽语言模型 (MLMs) 改造为预训练的机器阅读理解 (MRC) 模型，无需获取标记数据。通过使用 Wikipedia 超链接构建了大量通用且高质量的 MRC 风格训练数据，并设计了一个 Wiki Anchor Extraction 任务来指导 MRC 风格的预训练，从而解决了现有 MLMs 模型预训练与下游微调之间的差异化问题。除了简单易用，PMR 还能有效解决一些如抽取式问答和命名实体识别等任务。PMR 在现有方法方面显示了巨大的改进，特别是在低资源环境下。当应用于 MRC 公式中的序列分类任务时，PMR 能够提取高质量的证明材料来解释分类过程，从而提供更好的预测可解释性。PMR 在多个基准数据集上也比现有的基于预训练的模型表现更优。",
    "tldr": "本文提出了一种无需标记数据的新方法，能够将预训练的遮蔽语言模型改造为预训练的机器阅读理解模型，解决了现有模型预训练与下游微调之间的差异化问题。PMR 在多个基准数据集上表现优秀，能有效提高预测可解释性。",
    "en_tdlr": "This paper proposes a novel method called Pre-trained Machine Reader (PMR) that can retrofit pre-trained masked language models (MLMs) to pre-trained machine reading comprehension (MRC) models without labeled data, and effectively solves the discrepancy between model pre-training and downstream fine-tuning. PMR improves explanation and achieves superior performance on multiple benchmark datasets."
}