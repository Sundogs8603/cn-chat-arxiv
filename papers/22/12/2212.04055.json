{
    "title": "Mitigating Memorization of Noisy Labels by Clipping the Model Prediction. (arXiv:2212.04055v2 [cs.LG] UPDATED)",
    "abstract": "In the presence of noisy labels, designing robust loss functions is critical for securing the generalization performance of deep neural networks. Cross Entropy (CE) loss has been shown to be not robust to noisy labels due to its unboundedness. To alleviate this issue, existing works typically design specialized robust losses with the symmetric condition, which usually lead to the underfitting issue. In this paper, our key idea is to induce a loss bound at the logit level, thus universally enhancing the noise robustness of existing losses. Specifically, we propose logit clipping (LogitClip), which clamps the norm of the logit vector to ensure that it is upper bounded by a constant. In this manner, CE loss equipped with our LogitClip method is effectively bounded, mitigating the overfitting to examples with noisy labels. Moreover, we present theoretical analyses to certify the noise-tolerant ability of LogitClip. Extensive experiments show that LogitClip not only significantly improves t",
    "link": "http://arxiv.org/abs/2212.04055",
    "context": "Title: Mitigating Memorization of Noisy Labels by Clipping the Model Prediction. (arXiv:2212.04055v2 [cs.LG] UPDATED)\nAbstract: In the presence of noisy labels, designing robust loss functions is critical for securing the generalization performance of deep neural networks. Cross Entropy (CE) loss has been shown to be not robust to noisy labels due to its unboundedness. To alleviate this issue, existing works typically design specialized robust losses with the symmetric condition, which usually lead to the underfitting issue. In this paper, our key idea is to induce a loss bound at the logit level, thus universally enhancing the noise robustness of existing losses. Specifically, we propose logit clipping (LogitClip), which clamps the norm of the logit vector to ensure that it is upper bounded by a constant. In this manner, CE loss equipped with our LogitClip method is effectively bounded, mitigating the overfitting to examples with noisy labels. Moreover, we present theoretical analyses to certify the noise-tolerant ability of LogitClip. Extensive experiments show that LogitClip not only significantly improves t",
    "path": "papers/22/12/2212.04055.json",
    "total_tokens": 1039,
    "translated_abstract": "在噪声标签存在的情况下，设计强健的损失函数对于确保深层神经网络的泛化性能至关重要。交叉熵（CE）损失由于其无界性而被证明在噪声标签存在时不具备强健性。为了缓解这个问题，现有的工作通常设计具有对称条件的专门的强健性损失函数，这通常会导致欠拟合问题。本文的主要思想是在逻辑层面上引入损失边界，从而普遍增强现有损失的噪声稳健性。具体而言，我们提出了对数截断（LogitClip），通过将逻辑向量的范数夹紧以确保其上限被一个常数限制。通过这种方式，配备我们的LogitClip方法的CE损失有效地被限制，从而减轻对带有噪声标签的示例的过度拟合。此外，我们提出了理论分析来认证LogitClip的容错能力。广泛的实验表明，LogitClip不仅显著提高了在噪声标签下的模型性能和鲁棒性，并且可以被无缝地与其他现有的损失函数和正则化方法相结合，从而实现更好的性能。",
    "tldr": "本文提出了一种通过对逻辑层面进行损失边界处理的方法来增强现有损失函数的噪声稳健性。其提出的LogitClip方法可以显著提高模型的鲁棒性，并可以与其他现有的损失函数和正则化方法相结合，实现更好的性能。"
}