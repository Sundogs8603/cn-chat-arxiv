{
    "title": "Contrastive Error Attribution for Finetuned Language Models. (arXiv:2212.10722v2 [cs.CL] UPDATED)",
    "abstract": "Recent work has identified noisy and misannotated data as a core cause of hallucinations and unfaithful outputs in Natural Language Generation (NLG) tasks. Consequently, identifying and removing these examples is a key open challenge in creating reliable NLG systems. In this work, we introduce a framework to identify and remove low-quality training instances that lead to undesirable outputs, such as faithfulness errors in text summarization. We show that existing approaches for error tracing, such as gradient-based influence measures, do not perform reliably for detecting faithfulness errors in NLG datasets. We overcome the drawbacks of existing error tracing methods through a new, contrast-based estimate that compares undesired generations to human-corrected outputs. Our proposed method can achieve a mean average precision of 0.93 at detecting known data errors across synthetic tasks with known ground truth, substantially outperforming existing approaches. Using this approach and re-t",
    "link": "http://arxiv.org/abs/2212.10722",
    "context": "Title: Contrastive Error Attribution for Finetuned Language Models. (arXiv:2212.10722v2 [cs.CL] UPDATED)\nAbstract: Recent work has identified noisy and misannotated data as a core cause of hallucinations and unfaithful outputs in Natural Language Generation (NLG) tasks. Consequently, identifying and removing these examples is a key open challenge in creating reliable NLG systems. In this work, we introduce a framework to identify and remove low-quality training instances that lead to undesirable outputs, such as faithfulness errors in text summarization. We show that existing approaches for error tracing, such as gradient-based influence measures, do not perform reliably for detecting faithfulness errors in NLG datasets. We overcome the drawbacks of existing error tracing methods through a new, contrast-based estimate that compares undesired generations to human-corrected outputs. Our proposed method can achieve a mean average precision of 0.93 at detecting known data errors across synthetic tasks with known ground truth, substantially outperforming existing approaches. Using this approach and re-t",
    "path": "papers/22/12/2212.10722.json",
    "total_tokens": 964,
    "translated_title": "基于对比的错误归因用于调整的语言模型",
    "translated_abstract": "最近的研究发现，噪声和错误标注的数据是自然语言生成（NLG）任务中产生幻觉和不准确输出的核心原因。因此，在创建可靠的NLG系统中，识别和删除这些示例是一个关键的挑战。在本文中，我们介绍了一个框架来识别和删除导致意外输出的低质量训练实例，例如文本摘要中的准确性错误。我们发现，现有的错误追踪方法，如基于梯度的影响度量，在NLG数据集中检测准确性错误时不可靠。我们通过一种新的基于对比的估计方法克服了现有错误追踪方法的缺点，该方法将不期望的生成结果与人工修正的输出进行比较。我们提出的方法在已知基准任务的合成任务中可以达到0.93的平均精度，远远超过现有方法。使用这种方法和重新调整的模型，我们可以获得更具信任度的NLG系统。",
    "tldr": "本文提出了一种基于对比的错误归因框架，用于识别和删除导致文本摘要中准确性错误的低质量训练实例。该方法通过将不期望的生成结果与人工修正的输出进行比较，显著优于现有的错误追踪方法，能够在已知基准任务的合成任务中达到0.93的平均精度。",
    "en_tdlr": "This paper introduces a contrastive error attribution framework to identify and remove low-quality training instances that lead to accuracy errors in text summarization. The proposed method, which compares undesired generations to human-corrected outputs, outperforms existing error tracing approaches and achieves a mean average precision of 0.93 in synthetic tasks with known ground truth."
}