{
    "title": "Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization. (arXiv:2212.10445v3 [cs.LG] UPDATED)",
    "abstract": "Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks.",
    "link": "http://arxiv.org/abs/2212.10445",
    "context": "Title: Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization. (arXiv:2212.10445v3 [cs.LG] UPDATED)\nAbstract: Foundation models are redefining how AI systems are built. Practitioners now follow a standard procedure to build their machine learning solutions: from a pre-trained foundation model, they fine-tune the weights on the target task of interest. So, the Internet is swarmed by a handful of foundation models fine-tuned on many diverse tasks: these individual fine-tunings exist in isolation without benefiting from each other. In our opinion, this is a missed opportunity, as these specialized models contain rich and diverse features. In this paper, we thus propose model ratatouille, a new strategy to recycle the multiple fine-tunings of the same foundation model on diverse auxiliary tasks. Specifically, we repurpose these auxiliary weights as initializations for multiple parallel fine-tunings on the target task; then, we average all fine-tuned weights to obtain the final model. This recycling strategy aims at maximizing the diversity in weights by leveraging the diversity in auxiliary tasks.",
    "path": "papers/22/12/2212.10445.json",
    "total_tokens": 976,
    "translated_title": "模型美食：多样模型的回收利用以实现超出分布的泛化",
    "translated_abstract": "基础模型正在重新定义AI系统的构建方式。从一个预训练的基础模型开始，从业者现在都遵循一个标准的流程来构建他们的机器学习解决方案：在目标任务上微调权重。因此，互联网上充斥着许多在不同任务上微调的基础模型：这些单独的微调过程存在孤立，没有相互受益。在我们看来，这是一个被忽视的机会，因为这些专门的模型包含丰富多样的特征。因此，在本文中，我们提出了模型美食，这是一种在不同辅助任务上回收相同基础模型的多个微调的新策略。具体而言，我们将这些辅助权重重新用于目标任务上的多个并行微调的初始化；然后，我们对所有微调后的权重取平均值，得到最终模型。这种回收策略旨在通过利用辅助任务的多样性来最大化权重的多样性。",
    "tldr": "本论文提出了一种名为模型美食的新策略，通过回收基于同一基础模型在多样辅助任务上的多次微调，实现了在目标任务上的超出分布的泛化能力。通过利用辅助任务的多样性，这种策略旨在最大限度地提高模型权重的多样性。",
    "en_tdlr": "In this paper, a novel strategy called \"Model Ratatouille\" is proposed, which achieves out-of-distribution generalization on the target task by recycling multiple fine-tunings of the same foundation model on diverse auxiliary tasks. By leveraging the diversity in auxiliary tasks, this strategy aims to maximize the diversity in weights."
}