{
    "title": "On the Trade-off between Over-smoothing and Over-squashing in Deep Graph Neural Networks. (arXiv:2212.02374v2 [cs.LG] UPDATED)",
    "abstract": "Graph Neural Networks (GNNs) have succeeded in various computer science applications, yet deep GNNs underperform their shallow counterparts despite deep learning's success in other domains. Over-smoothing and over-squashing are key challenges when stacking graph convolutional layers, hindering deep representation learning and information propagation from distant nodes. Our work reveals that over-smoothing and over-squashing are intrinsically related to the spectral gap of the graph Laplacian, resulting in an inevitable trade-off between these two issues, as they cannot be alleviated simultaneously. To achieve a suitable compromise, we propose adding and removing edges as a viable approach. We introduce the Stochastic Jost and Liu Curvature Rewiring (SJLR) algorithm, which is computationally efficient and preserves fundamental properties compared to previous curvature-based methods. Unlike existing approaches, SJLR performs edge addition and removal during GNN training while maintaining",
    "link": "http://arxiv.org/abs/2212.02374",
    "context": "Title: On the Trade-off between Over-smoothing and Over-squashing in Deep Graph Neural Networks. (arXiv:2212.02374v2 [cs.LG] UPDATED)\nAbstract: Graph Neural Networks (GNNs) have succeeded in various computer science applications, yet deep GNNs underperform their shallow counterparts despite deep learning's success in other domains. Over-smoothing and over-squashing are key challenges when stacking graph convolutional layers, hindering deep representation learning and information propagation from distant nodes. Our work reveals that over-smoothing and over-squashing are intrinsically related to the spectral gap of the graph Laplacian, resulting in an inevitable trade-off between these two issues, as they cannot be alleviated simultaneously. To achieve a suitable compromise, we propose adding and removing edges as a viable approach. We introduce the Stochastic Jost and Liu Curvature Rewiring (SJLR) algorithm, which is computationally efficient and preserves fundamental properties compared to previous curvature-based methods. Unlike existing approaches, SJLR performs edge addition and removal during GNN training while maintaining",
    "path": "papers/22/12/2212.02374.json",
    "total_tokens": 868,
    "translated_title": "深度图神经网络中过度平滑和过度压缩的权衡研究",
    "translated_abstract": "图神经网络（GNN）在各种计算机科学应用中取得了成功，但是深度GNN在应用中表现不佳，尽管深度学习在其他领域取得了成功。当堆叠图卷积层时，过度平滑和过度压缩是深度表示学习和从远处节点传播信息的关键挑战。我们的工作揭示了过度平滑和过度压缩与图拉普拉斯算符的谱间隔有内在联系，导致这两个问题之间存在必然的权衡，无法同时缓解。为了达到合适的折中，我们提出了添加和删除边缘作为可行的方法。我们引入了随机Jost和Liu曲率重连（SJLR）算法，它在速度上具有计算效率，并与以前基于曲率的方法相比保持了基本特性。与现有方法不同，SJLR在GNN训练过程中执行边缘添加和删除，同时保持了基本特性。",
    "tldr": "过度平滑和过度压缩是深度图神经网络中的关键挑战，我们提出了添加和删除边缘的方法来解决这个问题。"
}