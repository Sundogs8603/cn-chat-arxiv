{
    "title": "Temporal Saliency Detection Towards Explainable Transformer-based Timeseries Forecasting. (arXiv:2212.07771v2 [cs.LG] UPDATED)",
    "abstract": "Despite the notable advancements in numerous Transformer-based models, the task of long multi-horizon time series forecasting remains a persistent challenge, especially towards explainability. Focusing on commonly used saliency maps in explaining DNN in general, our quest is to build attention-based architecture that can automatically encode saliency-related temporal patterns by establishing connections with appropriate attention heads. Hence, this paper introduces Temporal Saliency Detection (TSD), an effective approach that builds upon the attention mechanism and applies it to multi-horizon time series prediction. While our proposed architecture adheres to the general encoder-decoder structure, it undergoes a significant renovation in the encoder component, wherein we incorporate a series of information contracting and expanding blocks inspired by the U-Net style architecture. The TSD approach facilitates the multiresolution analysis of saliency patterns by condensing multi-heads, th",
    "link": "http://arxiv.org/abs/2212.07771",
    "context": "Title: Temporal Saliency Detection Towards Explainable Transformer-based Timeseries Forecasting. (arXiv:2212.07771v2 [cs.LG] UPDATED)\nAbstract: Despite the notable advancements in numerous Transformer-based models, the task of long multi-horizon time series forecasting remains a persistent challenge, especially towards explainability. Focusing on commonly used saliency maps in explaining DNN in general, our quest is to build attention-based architecture that can automatically encode saliency-related temporal patterns by establishing connections with appropriate attention heads. Hence, this paper introduces Temporal Saliency Detection (TSD), an effective approach that builds upon the attention mechanism and applies it to multi-horizon time series prediction. While our proposed architecture adheres to the general encoder-decoder structure, it undergoes a significant renovation in the encoder component, wherein we incorporate a series of information contracting and expanding blocks inspired by the U-Net style architecture. The TSD approach facilitates the multiresolution analysis of saliency patterns by condensing multi-heads, th",
    "path": "papers/22/12/2212.07771.json",
    "total_tokens": 845,
    "translated_title": "面向可解释的基于Transformer的时间序列预测的时间显著性检测",
    "translated_abstract": "尽管在许多基于Transformer的模型中取得了显著的进展，但长期多步预测的时间序列预测任务仍然是一个持续的挑战，特别是在可解释性方面。本文的目标是基于常用的显著性图解释DNN的思想，构建一种基于注意力机制的架构，能够通过与适当的注意力头建立连接，自动编码与显著性相关的时间模式。因此，本文介绍了一种名为Temporal Saliency Detection (TSD) 的有效方法，它在多步时间序列预测中利用了注意力机制。虽然我们提出的架构遵循常规的编码器-解码器结构，但在编码器组件中经历了重大的改进，其中我们采用了受U-Net风格架构启发的一系列信息收缩和扩展模块。TSD方法通过压缩多头注意力实现了显著性模式的多分辨率分析。",
    "tldr": "这项研究提出了一种名为Temporal Saliency Detection (TSD)的方法，利用基于注意力机制的架构实现了多步时间序列预测，并通过压缩多头注意力进行显著性模式的多分辨率分析。"
}