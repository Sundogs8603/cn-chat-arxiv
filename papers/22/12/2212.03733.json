{
    "title": "Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior",
    "abstract": "arXiv:2212.03733v2 Announce Type: replace-cross  Abstract: Reinforcement-learning agents seek to maximize a reward signal through environmental interactions. As humans, our job in the learning process is to design reward functions to express desired behavior and enable the agent to learn such behavior swiftly. In this work, we consider the reward-design problem in tasks formulated as reaching desirable states and avoiding undesirable states. To start, we propose a strict partial ordering of the policy space to resolve trade-offs in behavior preference. We prefer policies that reach the good states faster and with higher probability while avoiding the bad states longer. Next, we introduce Tiered Reward, a class of environment-independent reward functions and show it is guaranteed to induce policies that are Pareto-optimal according to our preference relation. Finally, we demonstrate that Tiered Reward can lead to fast learning by evaluating on several environments using multiple tabular",
    "link": "https://arxiv.org/abs/2212.03733",
    "context": "Title: Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior\nAbstract: arXiv:2212.03733v2 Announce Type: replace-cross  Abstract: Reinforcement-learning agents seek to maximize a reward signal through environmental interactions. As humans, our job in the learning process is to design reward functions to express desired behavior and enable the agent to learn such behavior swiftly. In this work, we consider the reward-design problem in tasks formulated as reaching desirable states and avoiding undesirable states. To start, we propose a strict partial ordering of the policy space to resolve trade-offs in behavior preference. We prefer policies that reach the good states faster and with higher probability while avoiding the bad states longer. Next, we introduce Tiered Reward, a class of environment-independent reward functions and show it is guaranteed to induce policies that are Pareto-optimal according to our preference relation. Finally, we demonstrate that Tiered Reward can lead to fast learning by evaluating on several environments using multiple tabular",
    "path": "papers/22/12/2212.03733.json",
    "total_tokens": 881,
    "translated_title": "层级奖励函数：规定和快速学习所需行为",
    "translated_abstract": "强化学习代理通过与环境的交互来最大化奖励信号。在学习过程中，我们作为人类的任务是设计奖励函数，以表达所期望的行为，并使代理能够迅速学习这种行为。在这项工作中，我们考虑了任务中达到良好状态和避免不良状态的奖励设计问题。首先，我们提出了一种严格的策略空间的部分排序，以解决行为偏好中的权衡。我们更倾向于能更快速地到达良好状态并以更高的概率到达，同时能更长时间地避免不良状态的策略。接下来，我们介绍了层级奖励，一类与环境无关的奖励函数，并表明它保证诱导出根据我们的偏好关系是帕累托最优的策略。最后，我们证明了层级奖励可以通过在多个环境上使用多个表格进行评估，从而实现快速学习。",
    "tldr": "层级奖励函数提出了一种解决奖励设计问题的方法，能够保证诱导出根据偏好关系是帕累托最优的策略，并在多个环境中展示其快速学习的能力。",
    "en_tdlr": "Tiered Reward Functions propose a method to solve the reward-design problem, guaranteeing policies that are Pareto-optimal according to preference relation, and demonstrate the ability for fast learning in multiple environments."
}