{
    "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models. (arXiv:2212.14052v3 [cs.LG] UPDATED)",
    "abstract": "State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid ",
    "link": "http://arxiv.org/abs/2212.14052",
    "context": "Title: Hungry Hungry Hippos: Towards Language Modeling with State Space Models. (arXiv:2212.14052v3 [cs.LG] UPDATED)\nAbstract: State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid ",
    "path": "papers/22/12/2212.14052.json",
    "total_tokens": 1101,
    "translated_title": "饥饿的河马：基于状态空间模型的语言建模方法",
    "translated_abstract": "状态空间模型（SSMs）在某些模态下表现出卓越的序列建模性能，但在语言建模方面表现不足。此外，尽管SSMs的序列长度近乎线性地扩展而不是二次方，但由于硬件利用率低下，它们仍然比变压器更慢。在本文中，我们取得了进展，理解了SSMs和建模关注机制之间的表现差距，并降低了SSMs和建模关注机制之间的硬件障碍。首先，我们使用合成的语言建模任务来理解SSMs和建模关注机制之间的差距。我们发现，现有的SSMs在两个方面存在困难：回忆先前的标记和跨序列比较标记。为了理解对语言建模的影响，我们提出了一个新的SSM层，H3，专门设计这些能力。H3在合成语言上与建模关注机制相匹配，并在OpenWebText上比变压器少了0.4 PPL。此外，一种混合模型，将H3和注意力以及硬件优化相结合，实现了语言建模基准测试的最新性能。我们的工作凸显了SSMs在语言建模方面的潜力，并为如何设计更好的SSMs提供了见解。",
    "tldr": "本文针对SSMs在语言建模上表现不足以及硬件利用率低下的问题，提出了一种新的SSM层H3，并将其与建模关注机制相结合，通过硬件优化实现了语言建模基准的最新性能，突出SSMs在语言建模中的潜力。",
    "en_tdlr": "This paper proposes a new SSM layer, H3, to address the shortcomings of SSMs in language modeling, and combines it with attention to achieve state-of-the-art performance on language modeling benchmarks through hardware optimization. The work highlights the potential of SSMs for language modeling and provides insights into how to design better SSMs for this task."
}