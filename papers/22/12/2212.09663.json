{
    "title": "Norm of Word Embedding Encodes Information Gain. (arXiv:2212.09663v2 [cs.CL] UPDATED)",
    "abstract": "Distributed representations of words encode lexical semantic information, but what type of information is encoded, and how? Focusing on the skip-gram with negative-sampling method, we found that the squared norm of static word embedding encodes the information gain conveyed by the word; the information gain is defined by the Kullback-Leibler divergence of the co-occurrence distribution of the word to the unigram distribution of the corpus. Our findings are explained by the theoretical framework of the exponential family of probability distributions and confirmed through precise experiments that remove spurious correlations arising from word frequency. We demonstrate that both the KL divergence and the squared norm of embedding provide a useful metric of a word's informativeness in tasks such as keyword extraction, part-of-speech discrimination, and hypernym classification.",
    "link": "http://arxiv.org/abs/2212.09663",
    "context": "Title: Norm of Word Embedding Encodes Information Gain. (arXiv:2212.09663v2 [cs.CL] UPDATED)\nAbstract: Distributed representations of words encode lexical semantic information, but what type of information is encoded, and how? Focusing on the skip-gram with negative-sampling method, we found that the squared norm of static word embedding encodes the information gain conveyed by the word; the information gain is defined by the Kullback-Leibler divergence of the co-occurrence distribution of the word to the unigram distribution of the corpus. Our findings are explained by the theoretical framework of the exponential family of probability distributions and confirmed through precise experiments that remove spurious correlations arising from word frequency. We demonstrate that both the KL divergence and the squared norm of embedding provide a useful metric of a word's informativeness in tasks such as keyword extraction, part-of-speech discrimination, and hypernym classification.",
    "path": "papers/22/12/2212.09663.json",
    "total_tokens": 854,
    "translated_title": "词向量的范数编码信息增益",
    "translated_abstract": "词的分布式表示编码了词汇语义信息，但是编码了哪些类型的信息？以及如何编码？本文针对跳字模型和负采样方法，发现静态词向量的平方范数编码了词所传达的信息增益；而信息增益是通过词在共现分布和语料库的单词分布之间的KL散度来定义的。我们的发现是通过指数族概率分布的理论框架说明的，并通过消除词频引起的虚假相关性的精密实验进行了确认。我们证明，无论是KL散度还是词嵌入的平方范数，在关键词提取、词性区分和上位词分类等任务中都提供了有用的词信息度量。",
    "tldr": "本文研究发现，跳字模型和负采样方法中静态词向量的平方范数编码了词所传达的信息增益，通过与语料库中单词的分布之间的KL散度来定义，可用于关键词提取、词性区分和上位词分类等任务。",
    "en_tdlr": "This paper finds that the squared norm of static word embedding in skip-gram with negative-sampling method encodes the information gain conveyed by a word, which is defined by the KL divergence of the co-occurrence distribution of the word to the unigram distribution of the corpus. Both the KL divergence and the squared norm of embedding provide a useful metric of a word's informativeness in tasks such as keyword extraction, part-of-speech discrimination, and hypernym classification."
}