{
    "title": "Restricting to the chip architecture maintains the quantum neural network accuracy",
    "abstract": "arXiv:2212.14426v2 Announce Type: replace-cross  Abstract: In the era of noisy intermediate-scale quantum devices, variational quantum algorithms (VQAs) stand as a prominent strategy for constructing quantum machine learning models. These models comprise both a quantum and a classical component. The quantum facet is characterized by a parametrization $U$, typically derived from the composition of various quantum gates. On the other hand, the classical component involves an optimizer that adjusts the parameters of $U$ to minimize a cost function $C$. Despite the extensive applications of VQAs, several critical questions persist, such as determining the optimal gate sequence, devising efficient parameter optimization strategies, selecting appropriate cost functions, and understanding the influence of quantum chip architectures on the final results. This article aims to address the last question, emphasizing that, in general, the cost function tends to converge towards an average value as",
    "link": "https://arxiv.org/abs/2212.14426",
    "context": "Title: Restricting to the chip architecture maintains the quantum neural network accuracy\nAbstract: arXiv:2212.14426v2 Announce Type: replace-cross  Abstract: In the era of noisy intermediate-scale quantum devices, variational quantum algorithms (VQAs) stand as a prominent strategy for constructing quantum machine learning models. These models comprise both a quantum and a classical component. The quantum facet is characterized by a parametrization $U$, typically derived from the composition of various quantum gates. On the other hand, the classical component involves an optimizer that adjusts the parameters of $U$ to minimize a cost function $C$. Despite the extensive applications of VQAs, several critical questions persist, such as determining the optimal gate sequence, devising efficient parameter optimization strategies, selecting appropriate cost functions, and understanding the influence of quantum chip architectures on the final results. This article aims to address the last question, emphasizing that, in general, the cost function tends to converge towards an average value as",
    "path": "papers/22/12/2212.14426.json",
    "total_tokens": 801,
    "translated_title": "限制在芯片架构上保持量子神经网络的准确性",
    "translated_abstract": "在噪声中等规模量子设备的时代，变分量子算法（VQAs）作为构建量子机器学习模型的显著策略。这些模型包括量子部分和经典部分。量子部分通过参数化 $U$ 来表征，通常由各种量子门的组合得到。另一方面，经典部分涉及一个优化器，调节 $U$ 的参数以最小化成本函数 $C$。尽管VQAs有广泛的应用，但仍然存在一些关键问题，比如确定最佳门序列、设计高效的参数优化策略、选择合适的成本函数，以及了解量子芯片架构对最终结果的影响。本文旨在解决最后一个问题，并强调，一般情况下，成本函数倾向于收敛到一个平均值。",
    "tldr": "该研究旨在探讨量子神经网络在量子芯片架构上表现出的准确性，并发现成本函数往往会收敛到一个平均值。",
    "en_tdlr": "This study aims to explore the accuracy of quantum neural networks on quantum chip architectures and finds that the cost function tends to converge to an average value."
}