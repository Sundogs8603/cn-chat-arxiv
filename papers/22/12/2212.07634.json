{
    "title": "Gradient-based Intra-attention Pruning on Pre-trained Language Models. (arXiv:2212.07634v2 [cs.CL] UPDATED)",
    "abstract": "Pre-trained language models achieve superior performance but are computationally expensive. Techniques such as pruning and knowledge distillation have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method GRAIN (Gradient-based Intra-attention pruning), which performs task-specific pruning with knowledge distillation and yields highly effective models. Different from common approaches that prune each attention head as a whole, GRAIN inspects and prunes intra-attention structures, which greatly expands the structure search space and enables more flexible models. We also propose a gradient separation strategy that reduces the interference of distillation on pruning for a better combination of the two approaches. Experiments on GLUE, SQuAD, and CoNLL 2003 show that GRAIN notably outperforms other methods, especially in the high sparsity regime, and achieves $6\\sim7\\times$ speedups while maintaining $93\\%\\sim99\\%$ performance. Under extreme",
    "link": "http://arxiv.org/abs/2212.07634",
    "context": "Title: Gradient-based Intra-attention Pruning on Pre-trained Language Models. (arXiv:2212.07634v2 [cs.CL] UPDATED)\nAbstract: Pre-trained language models achieve superior performance but are computationally expensive. Techniques such as pruning and knowledge distillation have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method GRAIN (Gradient-based Intra-attention pruning), which performs task-specific pruning with knowledge distillation and yields highly effective models. Different from common approaches that prune each attention head as a whole, GRAIN inspects and prunes intra-attention structures, which greatly expands the structure search space and enables more flexible models. We also propose a gradient separation strategy that reduces the interference of distillation on pruning for a better combination of the two approaches. Experiments on GLUE, SQuAD, and CoNLL 2003 show that GRAIN notably outperforms other methods, especially in the high sparsity regime, and achieves $6\\sim7\\times$ speedups while maintaining $93\\%\\sim99\\%$ performance. Under extreme",
    "path": "papers/22/12/2212.07634.json",
    "total_tokens": 1022,
    "translated_title": "基于渐进式自注意力剪枝的预训练语言模型压缩",
    "translated_abstract": "预训练语言模型在性能上表现卓越，但计算代价巨大。为此，人们开发了剪枝和知识蒸馏等技术来减小模型的体积和延迟。本文提出了一种有结构的剪枝方法GRAIN（基于渐进式自注意力剪枝），通过知识蒸馏执行任务特定剪枝，可以得到高效的模型。与通常剪枝每个注意力头的整体不同，GRAIN检查和剪枝内部自注意结构，极大扩展了结构搜索空间，使模型更加灵活。我们还提出了梯度分离策略，以减少蒸馏对剪枝的干扰，更好地结合这两种方法。在GLUE、SQuAD和CoNLL 2003上的实验表明，GRAIN显着优于其他方法，特别是在高稀疏度条件下，并且在保持$93\\%\\sim99\\%$性能的同时实现了$6\\sim7\\times$的加速。在极端条件下，GRAIN仍能保持与高性能LSTM相同的数量级。",
    "tldr": "本文提出了一种基于渐进式自注意力剪枝的压缩预训练语言模型方法GRAIN，通过执行任务特定的剪枝和知识蒸馏，可以得到高效的模型，并在GLUE、SQuAD和CoNLL 2003等任务中表现优异。",
    "en_tdlr": "This paper proposes a structured pruning method called GRAIN (Gradient-based Intra-attention pruning), which performs task-specific pruning with knowledge distillation and achieves highly effective models by inspecting and pruning intra-attention structures. GRAIN notably outperforms other methods, especially in the high sparsity regime, and achieves $6\\sim7\\times$ speedups while maintaining $93\\%\\sim99\\%$ performance on tasks such as GLUE, SQuAD, and CoNLL 2003."
}