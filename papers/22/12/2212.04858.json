{
    "title": "Implicit variance regularization in non-contrastive SSL. (arXiv:2212.04858v2 [cs.LG] UPDATED)",
    "abstract": "Non-contrastive SSL methods like BYOL and SimSiam rely on asymmetric predictor networks to avoid representational collapse without negative samples. Yet, how predictor networks facilitate stable learning is not fully understood. While previous theoretical analyses assumed Euclidean losses, most practical implementations rely on cosine similarity. To gain further theoretical insight into non-contrastive SSL, we analytically study learning dynamics in conjunction with Euclidean and cosine similarity in the eigenspace of closed-form linear predictor networks. We show that both avoid collapse through implicit variance regularization albeit through different dynamical mechanisms. Moreover, we find that the eigenvalues act as effective learning rate multipliers and propose a family of isotropic loss functions (IsoLoss) that equalize convergence rates across eigenmodes. Empirically, IsoLoss speeds up the initial learning dynamics and increases robustness, thereby allowing us to dispense with ",
    "link": "http://arxiv.org/abs/2212.04858",
    "context": "Title: Implicit variance regularization in non-contrastive SSL. (arXiv:2212.04858v2 [cs.LG] UPDATED)\nAbstract: Non-contrastive SSL methods like BYOL and SimSiam rely on asymmetric predictor networks to avoid representational collapse without negative samples. Yet, how predictor networks facilitate stable learning is not fully understood. While previous theoretical analyses assumed Euclidean losses, most practical implementations rely on cosine similarity. To gain further theoretical insight into non-contrastive SSL, we analytically study learning dynamics in conjunction with Euclidean and cosine similarity in the eigenspace of closed-form linear predictor networks. We show that both avoid collapse through implicit variance regularization albeit through different dynamical mechanisms. Moreover, we find that the eigenvalues act as effective learning rate multipliers and propose a family of isotropic loss functions (IsoLoss) that equalize convergence rates across eigenmodes. Empirically, IsoLoss speeds up the initial learning dynamics and increases robustness, thereby allowing us to dispense with ",
    "path": "papers/22/12/2212.04858.json",
    "total_tokens": 951,
    "translated_title": "非对比自监督学习中的隐式方差正则化",
    "translated_abstract": "非对比自监督学习方法（如BYOL和SimSiam）依赖于非对称预测网络来避免表示崩溃而无需负样本。然而，预测网络如何促进稳定学习还不完全清楚。虽然先前的理论分析假设欧几里得损失，但大多数实际实现依赖于余弦相似度。为了进一步理解非对比自监督学习，我们在闭式线性预测网络的特征空间中分析研究学习动力学与欧几里得距离和余弦相似度的关系。我们发现，两者均通过隐式方差正则化来避免崩溃，尽管具有不同的动态机制。此外，我们发现特征值作为有效的学习率乘数，并提出了一族等向性损失函数（IsoLoss），以在特征模式之间平衡收敛速度。实验证明，IsoLoss加速了初始学习动力学并增加了鲁棒性，从而使我们能够摆脱...",
    "tldr": "非对比自监督学习中的预测网络通过隐式方差正则化避免表示崩溃，欧几里得距离和余弦相似度具有不同的动态机制，并且特征值作为学习率乘数。引入一族等向性损失函数可以平衡收敛速度。",
    "en_tdlr": "Predictor networks in non-contrastive SSL avoid representational collapse through implicit variance regularization, with different dynamic mechanisms for Euclidean distance and cosine similarity. Eigenvalues act as learning rate multipliers, and a family of isotropic loss functions is proposed to equalize convergence rates."
}