{
    "title": "Attention in a family of Boltzmann machines emerging from modern Hopfield networks. (arXiv:2212.04692v2 [cs.LG] UPDATED)",
    "abstract": "Hopfield networks and Boltzmann machines (BMs) are fundamental energy-based neural network models. Recent studies on modern Hopfield networks have broaden the class of energy functions and led to a unified perspective on general Hopfield networks including an attention module. In this letter, we consider the BM counterparts of modern Hopfield networks using the associated energy functions, and study their salient properties from a trainability perspective. In particular, the energy function corresponding to the attention module naturally introduces a novel BM, which we refer to as the attentional BM (AttnBM). We verify that AttnBM has a tractable likelihood function and gradient for certain special cases and is easy to train. Moreover, we reveal the hidden connections between AttnBM and some single-layer models, namely the Gaussian--Bernoulli restricted BM and the denoising autoencoder with softmax units coming from denoising score matching. We also investigate BMs introduced by other ",
    "link": "http://arxiv.org/abs/2212.04692",
    "context": "Title: Attention in a family of Boltzmann machines emerging from modern Hopfield networks. (arXiv:2212.04692v2 [cs.LG] UPDATED)\nAbstract: Hopfield networks and Boltzmann machines (BMs) are fundamental energy-based neural network models. Recent studies on modern Hopfield networks have broaden the class of energy functions and led to a unified perspective on general Hopfield networks including an attention module. In this letter, we consider the BM counterparts of modern Hopfield networks using the associated energy functions, and study their salient properties from a trainability perspective. In particular, the energy function corresponding to the attention module naturally introduces a novel BM, which we refer to as the attentional BM (AttnBM). We verify that AttnBM has a tractable likelihood function and gradient for certain special cases and is easy to train. Moreover, we reveal the hidden connections between AttnBM and some single-layer models, namely the Gaussian--Bernoulli restricted BM and the denoising autoencoder with softmax units coming from denoising score matching. We also investigate BMs introduced by other ",
    "path": "papers/22/12/2212.04692.json",
    "total_tokens": 948,
    "tldr": "本文研究了现代 Hopfield 网络的 Boltzmann 机对应物，提出一种新的 Boltzmann 机 AttnBM，利用注意力模块的能量函数实现。该模型易于训练，且与高斯-Bernoulli 限制 BM 和去噪自编码器有联系。",
    "en_tdlr": "This paper studies the Boltzmann machine counterparts of modern Hopfield networks and introduces a new Boltzmann machine called AttnBM, which utilizes the energy function of an attention module. The model is easy to train and has connections with Gaussian-Bernoulli restricted BM and denoising autoencoder."
}