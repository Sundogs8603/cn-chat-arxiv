{
    "title": "Doubly Smoothed GDA: Global Convergent Algorithm for Constrained Nonconvex-Nonconcave Minimax Optimization. (arXiv:2212.12978v3 [math.OC] UPDATED)",
    "abstract": "Nonconvex-nonconcave minimax optimization has received intense attention over the last decade due to its broad applications in machine learning. Unfortunately, most existing algorithms cannot be guaranteed to converge globally and even suffer from limit cycles. To address this issue, we propose a novel single-loop algorithm called doubly smoothed gradient descent ascent method (DSGDA), which naturally balances the primal and dual updates. The proposed DSGDA can get rid of limit cycles in various challenging nonconvex-nonconcave examples in the literature, including Forsaken, Bilinearly-coupled minimax, Sixth-order polynomial, and PolarGame. We further show that under an one-sided Kurdyka-\\L{}ojasiewicz condition with exponent $\\theta\\in(0,1)$ (resp. convex primal/concave dual function), DSGDA can find a game-stationary point with an iteration complexity of $\\mathcal{O}(\\epsilon^{-2\\max\\{2\\theta,1\\}})$ (resp. $\\mathcal{O}(\\epsilon^{-4})$). These match the best results for single-loop al",
    "link": "http://arxiv.org/abs/2212.12978",
    "context": "Title: Doubly Smoothed GDA: Global Convergent Algorithm for Constrained Nonconvex-Nonconcave Minimax Optimization. (arXiv:2212.12978v3 [math.OC] UPDATED)\nAbstract: Nonconvex-nonconcave minimax optimization has received intense attention over the last decade due to its broad applications in machine learning. Unfortunately, most existing algorithms cannot be guaranteed to converge globally and even suffer from limit cycles. To address this issue, we propose a novel single-loop algorithm called doubly smoothed gradient descent ascent method (DSGDA), which naturally balances the primal and dual updates. The proposed DSGDA can get rid of limit cycles in various challenging nonconvex-nonconcave examples in the literature, including Forsaken, Bilinearly-coupled minimax, Sixth-order polynomial, and PolarGame. We further show that under an one-sided Kurdyka-\\L{}ojasiewicz condition with exponent $\\theta\\in(0,1)$ (resp. convex primal/concave dual function), DSGDA can find a game-stationary point with an iteration complexity of $\\mathcal{O}(\\epsilon^{-2\\max\\{2\\theta,1\\}})$ (resp. $\\mathcal{O}(\\epsilon^{-4})$). These match the best results for single-loop al",
    "path": "papers/22/12/2212.12978.json",
    "total_tokens": 1114,
    "translated_title": "双重平滑GDA：用于非凸-非凹极小极大优化的全局收敛算法",
    "translated_abstract": "非凸-非凹极小极大优化近年来受到了广泛的关注，其在机器学习中具有广泛的应用。然而，大多数现有算法不能保证全局收敛，甚至会遭受极限环的困扰。为了解决这个问题，我们提出了一种新颖的单循环算法，称为双重平滑梯度下降上升法 (DSGDA)，它能够自然地平衡原始与对偶更新，并且将极其具有挑战性的非凸-非凹例子中的极限环消除，包括 Forsaken，Bilinearly-coupled minimax，Sixth-order polynomial 和 PolarGame。我们进一步证明，在一个单侧的 $\\theta\\in(0,1)$ Kurdyka-\\L{}ojasiewicz条件（或凸原始/凹对偶函数）下，DSGDA 可以找到一个游戏平衡点，并且具有迭代复杂度 $\\mathcal{O}(\\epsilon^{-2\\max\\{2\\theta,1\\}})$（或 $\\mathcal{O}(\\epsilon^{-4})$），这些与文献中单循环算法的最佳结果相匹配。",
    "tldr": "本文提出了一种双重平滑梯度下降上升法 (DSGDA)，该算法可以应用于非凸-非凹极小极大优化，并且能够全局收敛并消除极限环。在一定条件下，DSGDA 的迭代复杂度达到了文献中单循环算法的最佳结果。",
    "en_tdlr": "The paper proposes a doubly smoothed gradient descent ascent method (DSGDA) for constrained nonconvex-nonconcave minimax optimization which balances primal and dual updates. DSGDA can eliminate limit cycles and converge globally in challenging nonconvex-nonconcave examples. The iteration complexity for finding a game-stationary point is $\\mathcal{O}(\\epsilon^{-2\\max\\{2\\theta,1\\}})$ (or $\\mathcal{O}(\\epsilon^{-4})$), matching the best results for single-loop algorithm in the literature."
}