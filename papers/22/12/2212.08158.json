{
    "title": "MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks. (arXiv:2212.08158v2 [cs.CV] UPDATED)",
    "abstract": "Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality. Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets. Experiments with six VL models -- LXMERT, CLIP and four ALBEF variants -- on four VL tasks highlight that unimodal",
    "link": "http://arxiv.org/abs/2212.08158",
    "context": "Title: MM-SHAP: A Performance-agnostic Metric for Measuring Multimodal Contributions in Vision and Language Models & Tasks. (arXiv:2212.08158v2 [cs.CV] UPDATED)\nAbstract: Vision and language models (VL) are known to exploit unrobust indicators in individual modalities (e.g., introduced by distributional biases) instead of focusing on relevant information in each modality. That a unimodal model achieves similar accuracy on a VL task to a multimodal one, indicates that so-called unimodal collapse occurred. However, accuracy-based tests fail to detect e.g., when the model prediction is wrong, while the model used relevant information from a modality. Instead, we propose MM-SHAP, a performance-agnostic multimodality score based on Shapley values that reliably quantifies in which proportions a multimodal model uses individual modalities. We apply MM-SHAP in two ways: (1) to compare models for their average degree of multimodality, and (2) to measure for individual models the contribution of individual modalities for different tasks and datasets. Experiments with six VL models -- LXMERT, CLIP and four ALBEF variants -- on four VL tasks highlight that unimodal",
    "path": "papers/22/12/2212.08158.json",
    "total_tokens": 1113,
    "translated_title": "MM-SHAP：一种用于衡量视觉与语言模型和任务的多模态贡献的性能不可知度量",
    "translated_abstract": "已知视觉和语言模型（VL）往往利用各自模态中的不稳定指标（例如由分布偏差引入）而不是专注于每个模态中的相关信息。如果单模态模型在VL任务上达到类似多模态模型的准确度，则表明所谓的单模态崩溃已经发生。然而，基于准确度的测试无法检测例如模型预测错误但模型使用了一个模态的相关信息。因此，我们提出了MM-SHAP，一种基于Shapley值的性能不可知多模态得分，可可靠地量化多模态模型使用各自模态的比例。我们将MM-SHAP应用于两种方式：（1）比较模型的平均多模态程度，（2）衡量不同任务和数据集的个体模型对各自模态的贡献。六个VL模型的实验（LXMERT、CLIP和四个ALBEF变体）表明单模态崩溃比我们以前认为的更为普遍。我们的结果还表明，MM-SHAP是揭示和分析VL模型多模态行为的有效工具。",
    "tldr": "该论文提出了一种性能不可知的多模态得分方法MM-SHAP，可以可靠地量化多模态模型使用各自模态的比例，并应用于比较模型的平均多模态程度和衡量个体模型的贡献。实验结果表明单模态崩溃比以前认为的更为普遍，而MM-SHAP是分析VL模型多模态行为的有效工具。",
    "en_tdlr": "This paper proposes a performance-agnostic metric, MM-SHAP, to measure the contributions of individual modalities in vision and language models. MM-SHAP uses Shapley values to reliably quantify the proportions in which a multimodal model uses individual modalities. The experiments with six VL models highlight that unimodal collapse is more prevalent than previously thought and MM-SHAP is a powerful tool in analyzing the multimodal behavior of VL models."
}