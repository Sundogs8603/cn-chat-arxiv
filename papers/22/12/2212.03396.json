{
    "title": "Learning to Select Prototypical Parts for Interpretable Sequential Data Modeling. (arXiv:2212.03396v2 [cs.LG] UPDATED)",
    "abstract": "Prototype-based interpretability methods provide intuitive explanations of model prediction by comparing samples to a reference set of memorized exemplars or typical representatives in terms of similarity. In the field of sequential data modeling, similarity calculations of prototypes are usually based on encoded representation vectors. However, due to highly recursive functions, there is usually a non-negligible disparity between the prototype-based explanations and the original input. In this work, we propose a Self-Explaining Selective Model (SESM) that uses a linear combination of prototypical concepts to explain its own predictions. The model employs the idea of case-based reasoning by selecting sub-sequences of the input that mostly activate different concepts as prototypical parts, which users can compare to sub-sequences selected from different example inputs to understand model decisions. For better interpretability, we design multiple constraints including diversity, stabilit",
    "link": "http://arxiv.org/abs/2212.03396",
    "context": "Title: Learning to Select Prototypical Parts for Interpretable Sequential Data Modeling. (arXiv:2212.03396v2 [cs.LG] UPDATED)\nAbstract: Prototype-based interpretability methods provide intuitive explanations of model prediction by comparing samples to a reference set of memorized exemplars or typical representatives in terms of similarity. In the field of sequential data modeling, similarity calculations of prototypes are usually based on encoded representation vectors. However, due to highly recursive functions, there is usually a non-negligible disparity between the prototype-based explanations and the original input. In this work, we propose a Self-Explaining Selective Model (SESM) that uses a linear combination of prototypical concepts to explain its own predictions. The model employs the idea of case-based reasoning by selecting sub-sequences of the input that mostly activate different concepts as prototypical parts, which users can compare to sub-sequences selected from different example inputs to understand model decisions. For better interpretability, we design multiple constraints including diversity, stabilit",
    "path": "papers/22/12/2212.03396.json",
    "total_tokens": 873,
    "translated_title": "学习选择典型部分以解释序列数据建模",
    "translated_abstract": "原型解释方法通过将样本与参考集中的典型代表进行相似度比较，提供了模型预测的直观解释。在序列数据建模领域，原型的相似性计算通常基于编码表示向量。然而，由于高度递归的函数，原型解释与原始输入之间通常存在明显差异。在本文中，我们提出了一种自解释选择模型（SESM），它使用典型的概念的线性组合来解释其自身的预测。该模型采用基于案例的推理思想，通过选择大部分激活不同概念的子序列作为典型部分来解释模型决策，用户可以将其与选择自不同示例输入的子序列进行比较以理解模型决策。为了更好的解释性，我们设计了多个约束条件，包括多样性，稳定性等。",
    "tldr": "提出了一种自解释选择模型，使用典型的概念的线性组合来解释其自身的预测，通过选择大部分激活不同概念的子序列作为典型部分来解释模型决策，为了更好的解释性，还设计了多个约束条件。",
    "en_tdlr": "A Self-Explaining Selective Model (SESM) is proposed to explain its own predictions by using a linear combination of prototypical concepts and selecting sub-sequences of the input that mostly activate different concepts as prototypical parts, with multiple constraints designed for interpretability."
}