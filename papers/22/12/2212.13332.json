{
    "title": "Development and Evaluation of a Learning-based Model for Real-time Haptic Texture Rendering. (arXiv:2212.13332v2 [cs.RO] UPDATED)",
    "abstract": "Current Virtual Reality (VR) environments lack the rich haptic signals that humans experience during real-life interactions, such as the sensation of texture during lateral movement on a surface. Adding realistic haptic textures to VR environments requires a model that generalizes to variations of a user's interaction and to the wide variety of existing textures in the world. Current methodologies for haptic texture rendering exist, but they usually develop one model per texture, resulting in low scalability. We present a deep learning-based action-conditional model for haptic texture rendering and evaluate its perceptual performance in rendering realistic texture vibrations through a multi part human user study. This model is unified over all materials and uses data from a vision-based tactile sensor (GelSight) to render the appropriate surface conditioned on the user's action in real time. For rendering texture, we use a high-bandwidth vibrotactile transducer attached to a 3D Systems",
    "link": "http://arxiv.org/abs/2212.13332",
    "context": "Title: Development and Evaluation of a Learning-based Model for Real-time Haptic Texture Rendering. (arXiv:2212.13332v2 [cs.RO] UPDATED)\nAbstract: Current Virtual Reality (VR) environments lack the rich haptic signals that humans experience during real-life interactions, such as the sensation of texture during lateral movement on a surface. Adding realistic haptic textures to VR environments requires a model that generalizes to variations of a user's interaction and to the wide variety of existing textures in the world. Current methodologies for haptic texture rendering exist, but they usually develop one model per texture, resulting in low scalability. We present a deep learning-based action-conditional model for haptic texture rendering and evaluate its perceptual performance in rendering realistic texture vibrations through a multi part human user study. This model is unified over all materials and uses data from a vision-based tactile sensor (GelSight) to render the appropriate surface conditioned on the user's action in real time. For rendering texture, we use a high-bandwidth vibrotactile transducer attached to a 3D Systems",
    "path": "papers/22/12/2212.13332.json",
    "total_tokens": 966,
    "translated_title": "实时触觉质地渲染的基于学习的模型的开发和评估",
    "translated_abstract": "当前的虚拟现实（VR）环境缺乏人类在现实生活中的交互中经历到的丰富触觉信号，例如在表面上的横向移动中感受到的质地感。在VR环境中添加逼真的触觉质地需要一个能够推广到用户交互的变化和世界上各种各样的质地的模型。目前存在用于触觉质地渲染的方法，但通常需要为每种质地开发一个模型，导致可扩展性较低。我们提出了一个基于深度学习的动作条件模型，用于触觉质地渲染，并通过多个用户的感知性能评估来呈现逼真的质地振动。该模型统一适用于所有材料，使用来自基于视觉的触觉传感器（GelSight）的数据，在实时条件下呈现适当的表面。在质地渲染方面，我们使用一个高带宽的振触觉传感器连接到一个3D系统。",
    "tldr": "该论文开发了一个基于学习的模型，用于实时触觉质地渲染，在多个用户的研究中评估了其感知性能。这个模型可以推广到各种质地和用户交互的变化，并使用视觉触觉传感器的数据进行实时渲染。",
    "en_tdlr": "This paper presents the development and evaluation of a learning-based model for real-time haptic texture rendering. It addresses the lack of rich haptic signals in current VR environments and the scalability issues of existing rendering methodologies. The model is able to generalize to various textures and user interactions, and utilizes data from a vision-based tactile sensor for real-time rendering."
}