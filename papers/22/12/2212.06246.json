{
    "title": "Jointly Learning Visual and Auditory Speech Representations from Raw Data. (arXiv:2212.06246v2 [cs.LG] UPDATED)",
    "abstract": "We present RAVEn, a self-supervised multi-modal approach to jointly learn visual and auditory speech representations. Our pre-training objective involves encoding masked inputs, and then predicting contextualised targets generated by slowly-evolving momentum encoders. Driven by the inherent differences between video and audio, our design is asymmetric w.r.t. the two modalities' pretext tasks: Whereas the auditory stream predicts both the visual and auditory targets, the visual one predicts only the auditory targets. We observe strong results in low- and high-resource labelled data settings when fine-tuning the visual and auditory encoders resulting from a single pre-training stage, in which the encoders are jointly trained. Notably, RAVEn surpasses all self-supervised methods on visual speech recognition (VSR) on LRS3, and combining RAVEn with self-training using only 30 hours of labelled data even outperforms a recent semi-supervised method trained on 90,000 hours of non-public data. ",
    "link": "http://arxiv.org/abs/2212.06246",
    "context": "Title: Jointly Learning Visual and Auditory Speech Representations from Raw Data. (arXiv:2212.06246v2 [cs.LG] UPDATED)\nAbstract: We present RAVEn, a self-supervised multi-modal approach to jointly learn visual and auditory speech representations. Our pre-training objective involves encoding masked inputs, and then predicting contextualised targets generated by slowly-evolving momentum encoders. Driven by the inherent differences between video and audio, our design is asymmetric w.r.t. the two modalities' pretext tasks: Whereas the auditory stream predicts both the visual and auditory targets, the visual one predicts only the auditory targets. We observe strong results in low- and high-resource labelled data settings when fine-tuning the visual and auditory encoders resulting from a single pre-training stage, in which the encoders are jointly trained. Notably, RAVEn surpasses all self-supervised methods on visual speech recognition (VSR) on LRS3, and combining RAVEn with self-training using only 30 hours of labelled data even outperforms a recent semi-supervised method trained on 90,000 hours of non-public data. ",
    "path": "papers/22/12/2212.06246.json",
    "total_tokens": 1026,
    "translated_title": "从原始数据中共同学习视觉和听觉语音表示的RAVEn算法研究",
    "translated_abstract": "本文提出了一种自监督多模态方法RAVEn，用于共同学习视觉和听觉语音表示。我们的预训练目标包括编码掩蔽输入，然后预测由缓慢演化的动量编码器生成的上下文目标。由于视频和音频之间的固有差异，我们的设计对于两种模态的先前任务是不对称的：听觉流预测视觉和听觉目标，而视觉流只预测听觉目标。当微调来自单个预训练阶段的视觉和听觉编码器时，无论是低资源还是高资源标记数据设置，我们观察到强大的结果。值得注意的是，在LRS3上RAVEn超越了所有自监督方法的视觉语音识别（VSR），并且将RAVEn与仅使用30小时标记数据的自我训练相结合，甚至胜过一个仅在90,000小时非公共数据上训练的最近的半监督方法。",
    "tldr": "本文提出了一种自监督算法RAVEn，能够同时学习视觉和听觉语音表示，无需准备大量标记数据，能在低资源和高资源数据设置下得到显著的结果，相比其他自监督算法在视觉语音识别上表现更好，在结合少量标记数据的情况下仍可超过使用大量非公共数据的半监督算法。",
    "en_tdlr": "This paper proposes a self-supervised algorithm RAVEn that can jointly learn visual and auditory speech representations without requiring large amounts of labeled data. The algorithm shows strong results in low- and high-resource data settings and outperforms other self-supervised methods on visual speech recognition (VSR). Even with only 30 hours of labeled data, combining RAVEn with self-training can still outperform a recent semi-supervised method trained on 90,000 hours of non-public data."
}