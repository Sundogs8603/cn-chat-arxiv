{
    "title": "Losses over Labels: Weakly Supervised Learning via Direct Loss Construction. (arXiv:2212.06921v2 [cs.LG] UPDATED)",
    "abstract": "Owing to the prohibitive costs of generating large amounts of labeled data, programmatic weak supervision is a growing paradigm within machine learning. In this setting, users design heuristics that provide noisy labels for subsets of the data. These weak labels are combined (typically via a graphical model) to form pseudolabels, which are then used to train a downstream model. In this work, we question a foundational premise of the typical weakly supervised learning pipeline: given that the heuristic provides all ``label\" information, why do we need to generate pseudolabels at all? Instead, we propose to directly transform the heuristics themselves into corresponding loss functions that penalize differences between our model and the heuristic. By constructing losses directly from the heuristics, we can incorporate more information than is used in the standard weakly supervised pipeline, such as how the heuristics make their decisions, which explicitly informs feature selection during ",
    "link": "http://arxiv.org/abs/2212.06921",
    "context": "Title: Losses over Labels: Weakly Supervised Learning via Direct Loss Construction. (arXiv:2212.06921v2 [cs.LG] UPDATED)\nAbstract: Owing to the prohibitive costs of generating large amounts of labeled data, programmatic weak supervision is a growing paradigm within machine learning. In this setting, users design heuristics that provide noisy labels for subsets of the data. These weak labels are combined (typically via a graphical model) to form pseudolabels, which are then used to train a downstream model. In this work, we question a foundational premise of the typical weakly supervised learning pipeline: given that the heuristic provides all ``label\" information, why do we need to generate pseudolabels at all? Instead, we propose to directly transform the heuristics themselves into corresponding loss functions that penalize differences between our model and the heuristic. By constructing losses directly from the heuristics, we can incorporate more information than is used in the standard weakly supervised pipeline, such as how the heuristics make their decisions, which explicitly informs feature selection during ",
    "path": "papers/22/12/2212.06921.json",
    "total_tokens": 858,
    "translated_title": "标签损失：通过直接损失构建进行弱监督学习",
    "translated_abstract": "由于生成大量标记数据的成本过高，编程弱监督成为机器学习中的一种新兴范 Paradigm。在这种情况下，用户设计启发式方法为数据子集提供噪声标签。这些弱标签被组合（通常通过图模型）形成伪标签，然后用于训练下游模型。在本文中，我们质疑传统弱监督学习流程的基本前提：既然启发式方法提供了所有的“标签”信息，为什么还需要生成伪标签呢？相反，我们提议直接将启发式方法转化为相应的损失函数，惩罚模型与启发式方法之间的差异。通过直接从启发式方法构建损失函数，我们可以融入比标准弱监督学习流程中使用的更多信息，例如启发式方法如何做出决策，这明确地指导了特征选择过程。",
    "tldr": "本文提出了一种弱监督学习的新方法，通过直接将启发式方法转化为损失函数，来训练模型，并利用启发式方法的决策信息进行特征选择。",
    "en_tdlr": "This paper proposes a new approach for weakly supervised learning by directly transforming heuristics into loss functions, training the model, and incorporating the decision information from heuristics for feature selection."
}