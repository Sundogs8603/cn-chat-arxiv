{
    "title": "Localization vs. Semantics: Visual Representations in Unimodal and Multimodal Models. (arXiv:2212.00281v2 [cs.CV] UPDATED)",
    "abstract": "Despite the impressive advancements achieved through vision-and-language pretraining, it remains unclear whether this joint learning paradigm can help understand each individual modality. In this work, we conduct a comparative analysis of the visual representations in existing vision-and-language models and vision-only models by probing a broad range of tasks, aiming to assess the quality of the learned representations in a nuanced manner. Interestingly, our empirical observations suggest that vision-and-language models are better at label prediction tasks like object and attribute prediction, while vision-only models are stronger at dense prediction tasks that require more localized information. We hope our study sheds light on the role of language in visual learning, and serves as an empirical guide for various pretrained models. Code will be released at https://github.com/Lizw14/visual_probing",
    "link": "http://arxiv.org/abs/2212.00281",
    "context": "Title: Localization vs. Semantics: Visual Representations in Unimodal and Multimodal Models. (arXiv:2212.00281v2 [cs.CV] UPDATED)\nAbstract: Despite the impressive advancements achieved through vision-and-language pretraining, it remains unclear whether this joint learning paradigm can help understand each individual modality. In this work, we conduct a comparative analysis of the visual representations in existing vision-and-language models and vision-only models by probing a broad range of tasks, aiming to assess the quality of the learned representations in a nuanced manner. Interestingly, our empirical observations suggest that vision-and-language models are better at label prediction tasks like object and attribute prediction, while vision-only models are stronger at dense prediction tasks that require more localized information. We hope our study sheds light on the role of language in visual learning, and serves as an empirical guide for various pretrained models. Code will be released at https://github.com/Lizw14/visual_probing",
    "path": "papers/22/12/2212.00281.json",
    "total_tokens": 860,
    "translated_title": "定位 vs. 语义：单模态和多模态模型中的视觉表示",
    "translated_abstract": "尽管通过视觉与语言预训练取得了令人印象深刻的进展，但目前尚不清楚这种联合学习范式是否有助于理解每个个体模态。在本研究中，我们通过探究广泛的任务，对比分析现有的视觉与语言模型和仅视觉模型中的视觉表示，旨在以细致的方式评估学习表示的质量。有趣的是，我们的实证观察表明，视觉与语言模型在标签预测任务（如对象和属性预测）方面表现更好，而仅视觉模型在需要更局部信息的密集预测任务上表现更强。我们希望我们的研究能够阐明语言在视觉学习中的作用，并成为各种预训练模型的经验指南。代码将在https://github.com/Lizw14/visual_probing发布。",
    "tldr": "本文比较分析了现有视觉与语言模型和仅视觉模型中的视觉表示，发现视觉与语言模型在标签预测任务上表现更好，而仅视觉模型在需要更局部信息的密集预测任务上表现更强。",
    "en_tdlr": "This work conducts a comparative analysis to assess the quality of visual representations in vision-and-language models and vision-only models, finding that the former performs better in label prediction tasks while the latter excels in dense prediction tasks requiring localized information."
}