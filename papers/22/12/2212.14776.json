{
    "title": "On the Interpretability of Attention Networks. (arXiv:2212.14776v2 [cs.LG] UPDATED)",
    "abstract": "Attention mechanisms form a core component of several successful deep learning architectures, and are based on one key idea: ''The output depends only on a small (but unknown) segment of the input.'' In several practical applications like image captioning and language translation, this is mostly true. In trained models with an attention mechanism, the outputs of an intermediate module that encodes the segment of input responsible for the output is often used as a way to peek into the `reasoning` of the network. We make such a notion more precise for a variant of the classification problem that we term selective dependence classification (SDC) when used with attention model architectures. Under such a setting, we demonstrate various error modes where an attention model can be accurate but fail to be interpretable, and show that such models do occur as a result of training. We illustrate various situations that can accentuate and mitigate this behaviour. Finally, we use our objective def",
    "link": "http://arxiv.org/abs/2212.14776",
    "context": "Title: On the Interpretability of Attention Networks. (arXiv:2212.14776v2 [cs.LG] UPDATED)\nAbstract: Attention mechanisms form a core component of several successful deep learning architectures, and are based on one key idea: ''The output depends only on a small (but unknown) segment of the input.'' In several practical applications like image captioning and language translation, this is mostly true. In trained models with an attention mechanism, the outputs of an intermediate module that encodes the segment of input responsible for the output is often used as a way to peek into the `reasoning` of the network. We make such a notion more precise for a variant of the classification problem that we term selective dependence classification (SDC) when used with attention model architectures. Under such a setting, we demonstrate various error modes where an attention model can be accurate but fail to be interpretable, and show that such models do occur as a result of training. We illustrate various situations that can accentuate and mitigate this behaviour. Finally, we use our objective def",
    "path": "papers/22/12/2212.14776.json",
    "total_tokens": 956,
    "translated_title": "关于注意力网络的可解释性研究",
    "translated_abstract": "注意力机制是多个成功深度学习架构的核心组件，基于一个关键想法：“输出仅取决于输入的一个小（但未知）部分”。在图像字幕和语言翻译等多个实际应用中，这通常是正确的。在具有注意力机制的训练模型中，编码与输出相关的输入段的中间模块的输出通常被用作窥视网络“推理”的一种方式。本文通过使用注意力模型体系结构解决一个变体分类问题，我们称之为选择依赖分类（SDC），从而更加清晰地阐述了这种概念。在这种情况下，我们演示了多种错误模式，其中注意力模型可以准确无误但不具有可解释性，并表明这种模型因训练而出现。我们还阐述了可以加强和减轻此行为的多种情况。最后，我们使用我们的目标定义了一种对于SDC模型及其解释性的评估指标，并评估了不同架构的模型的解释性。",
    "tldr": "本文研究注意力网络的可解释性，提出了选择依赖分类（SDC）变体分类问题，并演示了注意力模型可以准确无误但不具有可解释性的多种错误模式。该研究为评估SDC模型及其解释性提供了一种评估指标，并评估了不同架构的模型的解释性。",
    "en_tdlr": "This paper studies the interpretability of attention networks, proposes a variant of the classification problem called selective dependence classification (SDC), demonstrates various error modes where attention models can be accurate but fail to be interpretable, and provides an evaluation metric for the interpretability of SDC models. The study also evaluates the interpretability of models with different architectures."
}