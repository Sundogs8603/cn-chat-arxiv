{
    "title": "Shape-Guided Diffusion with Inside-Outside Attention. (arXiv:2212.00210v2 [cs.CV] UPDATED)",
    "abstract": "When manipulating an object, existing text-to-image diffusion models often ignore the shape of the object and generate content that is incorrectly scaled, cut off, or replaced with background content. We propose a training-free method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be sensitive to shape input specified by a user or automatically inferred from text. We use a novel Inside-Outside Attention mechanism during the inversion and generation process to apply this shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits specified by text prompts to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degra",
    "link": "http://arxiv.org/abs/2212.00210",
    "context": "Title: Shape-Guided Diffusion with Inside-Outside Attention. (arXiv:2212.00210v2 [cs.CV] UPDATED)\nAbstract: When manipulating an object, existing text-to-image diffusion models often ignore the shape of the object and generate content that is incorrectly scaled, cut off, or replaced with background content. We propose a training-free method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be sensitive to shape input specified by a user or automatically inferred from text. We use a novel Inside-Outside Attention mechanism during the inversion and generation process to apply this shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits specified by text prompts to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degra",
    "path": "papers/22/12/2212.00210.json",
    "total_tokens": 926,
    "translated_title": "带有内外部注意力的形状引导扩散",
    "translated_abstract": "在操作对象时，现有的文本到图像扩散模型通常忽略对象的形状并生成错误比例、被截断或被背景内容替换的图像。我们提出了一种无需训练的方法 Shape-Guided Diffusion，该方法修改了预训练扩散模型，使之对用户指定的形状输入或从文本中自动推断的形状敏感。我们使用一种新颖的内外部注意机制，在反演和生成过程中将此形状限制应用于跨注意力图和自注意力图上。我们的机制指定空间区域是对象（内部）还是背景（外部），然后将文本提示指定的编辑与正确的区域相关联。我们在形状引导编辑任务上展示了我们方法的有效性，其中模型必须根据文本提示和对象掩码替换对象。我们创建了一个新的从 MS-COCO 衍生的 ShapePrompts 基准，并在形状忠实度方面实现了 SOTA 的结果，而不需要降级。",
    "tldr": "该论文提出了一种无需训练的形状引导扩散方法，使用一种新颖的内外部注意机制将形状限制应用于跨注意力图和自注意力图上，从而在文本到图像扩散模型中考虑到对象形状，进而可以实现对象形状忠实度更高的图像生成。",
    "en_tdlr": "This paper proposes a training-free method called Shape-Guided Diffusion that modifies pretrained diffusion models to consider the shape of objects during text-to-image generation by using a novel Inside-Outside Attention mechanism, achieving improved shape faithfulness without degradation."
}