{
    "title": "Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios. (arXiv:2212.11419v2 [cs.AI] UPDATED)",
    "abstract": "Imitation learning (IL) is a simple and powerful way to use high-quality human driving data, which can be collected at scale, to produce human-like behavior. However, policies based on imitation learning alone often fail to sufficiently account for safety and reliability concerns. In this paper, we show how imitation learning combined with reinforcement learning using simple rewards can substantially improve the safety and reliability of driving policies over those learned from imitation alone. In particular, we train a policy on over 100k miles of urban driving data, and measure its effectiveness in test scenarios grouped by different levels of collision likelihood. Our analysis shows that while imitation can perform well in low-difficulty scenarios that are well-covered by the demonstration data, our proposed approach significantly improves robustness on the most challenging scenarios (over 38% reduction in failures). To our knowledge, this is the first application of a combined imit",
    "link": "http://arxiv.org/abs/2212.11419",
    "context": "Title: Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios. (arXiv:2212.11419v2 [cs.AI] UPDATED)\nAbstract: Imitation learning (IL) is a simple and powerful way to use high-quality human driving data, which can be collected at scale, to produce human-like behavior. However, policies based on imitation learning alone often fail to sufficiently account for safety and reliability concerns. In this paper, we show how imitation learning combined with reinforcement learning using simple rewards can substantially improve the safety and reliability of driving policies over those learned from imitation alone. In particular, we train a policy on over 100k miles of urban driving data, and measure its effectiveness in test scenarios grouped by different levels of collision likelihood. Our analysis shows that while imitation can perform well in low-difficulty scenarios that are well-covered by the demonstration data, our proposed approach significantly improves robustness on the most challenging scenarios (over 38% reduction in failures). To our knowledge, this is the first application of a combined imit",
    "path": "papers/22/12/2212.11419.json",
    "total_tokens": 949,
    "translated_title": "不仅仅是模仿：结合强化学习用于具有挑战性的驾驶情景的鲁棒化模仿",
    "translated_abstract": "模仿学习(imitation learning, IL)是一种简单而强大的方法，利用大规模的高质量人类驾驶数据产生类人驾驶行为。然而，仅基于模仿学习的策略往往无法充分考虑到安全性和可靠性的问题。本文中，我们展示了如何通过结合使用简单奖励的强化学习和模仿学习，可以显著提高驾驶策略的安全性和可靠性，超过仅用模仿学习学到的策略。具体来说，我们在超过10万英里的城市驾驶数据上训练了一个策略，并在不同碰撞可能性水平下的测试情景中测量其效果。我们的分析表明，虽然模仿学习在由示范数据很好覆盖的低难度情景中表现良好，但我们提出的方法在最具挑战性的情景中显著提高了鲁棒性(故障减少了超过38%)。据我们所知，这是第一个将结合imitation learning和reinforcement learning应用于驾驶情景的工作。",
    "tldr": "结合使用简单奖励的强化学习和模仿学习，可以显著提高驾驶策略的安全性和可靠性，超过仅用模仿学习学到的策略。",
    "en_tdlr": "Combining reinforcement learning using simple rewards with imitation learning can significantly improve the safety and reliability of driving policies over those learned from imitation alone."
}