{
    "title": "ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. (arXiv:2212.07919v2 [cs.CL] UPDATED)",
    "abstract": "Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rat",
    "link": "http://arxiv.org/abs/2212.07919",
    "context": "Title: ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning. (arXiv:2212.07919v2 [cs.CL] UPDATED)\nAbstract: Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rat",
    "path": "papers/22/12/2212.07919.json",
    "total_tokens": 893,
    "translated_title": "ROSCOE: 用于评分逐步推理的一套度量指标",
    "translated_abstract": "当大型语言模型被要求生成逐步推理来解释其最终答案时，它们在下游任务性能上显示出了改进。这些推理步骤大大提高了模型的可解释性和验证性，但在没有可靠的自动评估方法的情况下，独立于最终答案来研究它们的正确性是困难的。我们并不知道所述的推理步骤实际上有多少支持最终任务预测结果。在这项工作中，我们提出了ROSCOE，这是一套可解释的无监督自动评分指标，它改进并扩展了先前的文本生成评估指标。为了评估ROSCOE与基线指标的差异，我们设计了一种推理错误的分类，并在常用的推理数据集上收集了合成和人工评估得分。与现有指标相比，ROSCOE可以通过利用逐步推理的特性来衡量语义一致性、逻辑性、信息量、流畅度和事实等特征。",
    "tldr": "ROSCOE是一套度量指标，用于评分逐步推理的正确性和质量。它可以衡量语义一致性、逻辑性、信息量、流畅度和事实等特征，并提供可解释的评估方法。",
    "en_tdlr": "ROSCOE is a suite of metrics for scoring the correctness and quality of step-by-step reasoning. It measures semantic consistency, logicality, informativeness, fluency, factuality, and provides interpretable evaluation methods."
}