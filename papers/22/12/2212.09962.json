{
    "title": "Distributional Robustness Bounds Generalization Errors",
    "abstract": "arXiv:2212.09962v3 Announce Type: replace  Abstract: Bayesian methods, distributionally robust optimization methods, and regularization methods are three pillars of trustworthy machine learning combating distributional uncertainty, e.g., the uncertainty of an empirical distribution compared to the true underlying distribution. This paper investigates the connections among the three frameworks and, in particular, explores why these frameworks tend to have smaller generalization errors. Specifically, first, we suggest a quantitative definition for \"distributional robustness\", propose the concept of \"robustness measure\", and formalize several philosophical concepts in distributionally robust optimization. Second, we show that Bayesian methods are distributionally robust in the probably approximately correct (PAC) sense; in addition, by constructing a Dirichlet-process-like prior in Bayesian nonparametrics, it can be proven that any regularized empirical risk minimization method is equival",
    "link": "https://arxiv.org/abs/2212.09962",
    "context": "Title: Distributional Robustness Bounds Generalization Errors\nAbstract: arXiv:2212.09962v3 Announce Type: replace  Abstract: Bayesian methods, distributionally robust optimization methods, and regularization methods are three pillars of trustworthy machine learning combating distributional uncertainty, e.g., the uncertainty of an empirical distribution compared to the true underlying distribution. This paper investigates the connections among the three frameworks and, in particular, explores why these frameworks tend to have smaller generalization errors. Specifically, first, we suggest a quantitative definition for \"distributional robustness\", propose the concept of \"robustness measure\", and formalize several philosophical concepts in distributionally robust optimization. Second, we show that Bayesian methods are distributionally robust in the probably approximately correct (PAC) sense; in addition, by constructing a Dirichlet-process-like prior in Bayesian nonparametrics, it can be proven that any regularized empirical risk minimization method is equival",
    "path": "papers/22/12/2212.09962.json",
    "total_tokens": 808,
    "translated_title": "分布鲁棒性界定了泛化错误",
    "translated_abstract": "Bayesian methods, distributionally robust optimization methods, and regularization methods是值得信赖的机器学习的基石，用于抵抗分布不确定性，比如经验分布与真实基础分布之间的不确定性。本文研究了这三种框架之间的联系，特别地探讨了为何这些框架倾向于具有更小的泛化错误。具体地，首先，我们提出了“分布鲁棒性”的定量定义，提出了“鲁棒性度量”的概念，并形式化了分布鲁棒性优化中的几个哲学概念。其次，我们表明Bayesian方法在可能近似正确意义上是分布鲁棒的；此外，通过构造类似Dirichlet过程的先验于贝叶斯非参数模型中，可以证明任何正则化的经验风险最小化方法等价于",
    "tldr": "分布鲁棒性界定了泛化错误，Bayesian方法在可能近似正确意义上是分布鲁棒的，同时正则化的经验风险最小化方法也被证明是等价于Bayesian方法的。",
    "en_tdlr": "Distributional robustness bounds generalization errors, Bayesian methods are distributionally robust in the probably approximately correct (PAC) sense, and regularized empirical risk minimization methods are proven to be equivalent to Bayesian methods."
}