{
    "title": "Smooth Sailing: Improving Active Learning for Pre-trained Language Models with Representation Smoothness Analysis. (arXiv:2212.11680v2 [cs.LG] UPDATED)",
    "abstract": "Developed to alleviate prohibitive labeling costs, active learning (AL) methods aim to reduce label complexity in supervised learning. While recent work has demonstrated the benefit of using AL in combination with large pre-trained language models (PLMs), it has often overlooked the practical challenges that hinder the effectiveness of AL. We address these challenges by leveraging representation smoothness analysis to ensure AL is feasible, that is, both effective and practicable. Firstly, we propose an early stopping technique that does not require a validation set -- often unavailable in realistic AL conditions -- and observe significant improvements over random sampling across multiple datasets and AL methods. Further, we find that task adaptation improves AL, whereas standard short fine-tuning in AL does not provide improvements over random sampling. Our work demonstrates the usefulness of representation smoothness analysis for AL and introduces an AL stopping criterion that reduce",
    "link": "http://arxiv.org/abs/2212.11680",
    "context": "Title: Smooth Sailing: Improving Active Learning for Pre-trained Language Models with Representation Smoothness Analysis. (arXiv:2212.11680v2 [cs.LG] UPDATED)\nAbstract: Developed to alleviate prohibitive labeling costs, active learning (AL) methods aim to reduce label complexity in supervised learning. While recent work has demonstrated the benefit of using AL in combination with large pre-trained language models (PLMs), it has often overlooked the practical challenges that hinder the effectiveness of AL. We address these challenges by leveraging representation smoothness analysis to ensure AL is feasible, that is, both effective and practicable. Firstly, we propose an early stopping technique that does not require a validation set -- often unavailable in realistic AL conditions -- and observe significant improvements over random sampling across multiple datasets and AL methods. Further, we find that task adaptation improves AL, whereas standard short fine-tuning in AL does not provide improvements over random sampling. Our work demonstrates the usefulness of representation smoothness analysis for AL and introduces an AL stopping criterion that reduce",
    "path": "papers/22/12/2212.11680.json",
    "total_tokens": 920,
    "translated_title": "平稳航行：用表示平滑度分析改进预训练语言模型中的主动学习",
    "translated_abstract": "主动学习（AL）方法旨在减少监督学习中的标注复杂性，以缓解昂贵的标注成本。最近的研究表明，将AL与大型预训练语言模型（PLM）结合使用具有益处，但往往忽视了影响AL效果的实际挑战。我们通过利用表示平滑度分析来解决这些挑战，以确保AL既有效又实用。首先，我们提出一种无需验证集的早停技术，在多个数据集和AL方法中对比随机抽样，观察到显著的改进。此外，我们发现任务适应改进了AL，而标准的短期微调在AL中并没有比随机抽样提供改进。我们的工作证明了表示平滑度分析在AL中的有用性，并引入了一种减少主动学习停止标准的方法。",
    "tldr": "通过表示平滑度分析，我们改进了预训练语言模型中的主动学习方法，提出了一种无需验证集的早停技术，并发现任务适应对主动学习具有改进作用。这项工作在实际应用中证明了表示平滑度分析对于提高主动学习的有效性和实用性的重要性。",
    "en_tdlr": "By leveraging representation smoothness analysis, we improve active learning methods in pre-trained language models and propose an early stopping technique that doesn't require a validation set. We also find that task adaptation enhances active learning. This work demonstrates the significance of representation smoothness analysis in improving the effectiveness and practicality of active learning."
}