{
    "title": "An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws. (arXiv:2212.01365v2 [cs.LG] UPDATED)",
    "abstract": "We study the compute-optimal trade-off between model and training data set sizes for large neural networks. Our result suggests a linear relation similar to that supported by the empirical analysis of chinchilla. While that work studies transformer-based large language models trained on the MassiveText corpus gopher, as a starting point for development of a mathematical theory, we focus on a simpler learning model and data generating process, each based on a neural network with a sigmoidal output unit and single hidden layer of ReLU activation units. We introduce general error upper bounds for a class of algorithms which incrementally update a statistic (for example gradient descent). For a particular learning model inspired by barron 1993, we establish an upper bound on the minimal information-theoretically achievable expected error as a function of model and data set sizes. We then derive allocations of computation that minimize this bound. We present empirical results which suggest ",
    "link": "http://arxiv.org/abs/2212.01365",
    "context": "Title: An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws. (arXiv:2212.01365v2 [cs.LG] UPDATED)\nAbstract: We study the compute-optimal trade-off between model and training data set sizes for large neural networks. Our result suggests a linear relation similar to that supported by the empirical analysis of chinchilla. While that work studies transformer-based large language models trained on the MassiveText corpus gopher, as a starting point for development of a mathematical theory, we focus on a simpler learning model and data generating process, each based on a neural network with a sigmoidal output unit and single hidden layer of ReLU activation units. We introduce general error upper bounds for a class of algorithms which incrementally update a statistic (for example gradient descent). For a particular learning model inspired by barron 1993, we establish an upper bound on the minimal information-theoretically achievable expected error as a function of model and data set sizes. We then derive allocations of computation that minimize this bound. We present empirical results which suggest ",
    "path": "papers/22/12/2212.01365.json",
    "total_tokens": 879,
    "translated_title": "计算最优神经网络缩放定律的信息论分析",
    "translated_abstract": "我们研究了大型神经网络模型和训练数据集大小之间的计算最优权衡。我们的结果表明，类似于白鼠实验所支持的线性关系。尽管白鼠实验研究的是基于MassiveText语料库的基于变压器的大型语言模型，我们的研究重点是简化的学习模型和数据生成过程，每个都基于一个具有S型输出单元和单隐藏层ReLU激活单元的神经网络。我们引入了一类算法的一般误差上界，这些算法逐步更新一个统计量（例如梯度下降）。针对受到barron 1993启发的特定学习模型，我们建立了一个上界，该上界是作为模型和数据集大小的函数的信息论上可实现的期望误差的最小值。然后，我们推导出最小化这一上界的计算分配方式。我们展示了一些实证结果，这些结果表明...（待续）",
    "tldr": "我们进行了一项关于大型神经网络的计算最优权衡的研究，并提出了一种信息论上可行的期望误差最小化模型和数据集大小的计算分配方法。",
    "en_tdlr": "We conducted a study on the compute-optimal trade-off between model and training data set sizes for large neural networks, and proposed a computation allocation method that minimizes the expected error based on information theory for model and data set sizes."
}