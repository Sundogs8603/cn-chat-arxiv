{
    "title": "ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models. (arXiv:2212.10474v2 [cs.CL] UPDATED)",
    "abstract": "State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably c",
    "link": "http://arxiv.org/abs/2212.10474",
    "context": "Title: ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models. (arXiv:2212.10474v2 [cs.CL] UPDATED)\nAbstract: State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably c",
    "path": "papers/22/12/2212.10474.json",
    "total_tokens": 976,
    "translated_title": "ByGPT5：基于端到端的、面向风格的诗歌生成模型，采用无记号言语模型",
    "translated_abstract": "现有的诗歌生成系统通常非常复杂，要么由特定任务的模型管道构成，要么采用手动创建的约束条件来融入先前的知识，或两者兼而有之。相反，端到端的模型不需要建模先前的知识，并且可以仅从数据中学习诗歌的微妙之处，从而减少需要人类监督的程度。在本研究中，我们研究了以韵律、节律和头韵等风格为条件的端到端诗歌生成。我们确定并解决了过去尝试的数据缺乏和不匹配的记号化算法可能存在的局限性。特别地，我们成功地预训练了ByGPT5，一种新的无记号解码器-仅语言模型，并在我们的风格标注的大型定制英语和德语四行诗语料库上进行微调。我们展示了ByGPT5优于其他模型，如mT5、ByT5、GPT-2和ChatGPT，同时也更具参数效率，性能表现良好。",
    "tldr": "本研究提出了一种基于无记号言语模型的端到端诗歌生成模型ByGPT5，并成功地应用于韵律、节律和头韵等风格的诗歌生成，取得了较好的性能表现。"
}