{
    "title": "Backward Curriculum Reinforcement Learning. (arXiv:2212.14214v3 [cs.AI] UPDATED)",
    "abstract": "Current reinforcement learning algorithms train an agent using forward-generated trajectories, which provide little guidance so that the agent can explore as much as possible. While realizing the value of reinforcement learning results from sufficient exploration, this approach leads to a trade-off in losing sample efficiency, an essential factor impacting algorithm performance. Previous tasks use reward-shaping techniques and network structure modification to increase sample efficiency. However, these methods require many steps to implement. In this work, we propose novel backward curriculum reinforcement learning that begins training the agent using the backward trajectory of the episode instead of the original forward trajectory. This approach provides the agent with a strong reward signal, enabling more sample-efficient learning. Moreover, our method only requires a minor change in the algorithm of reversing the order of the trajectory before agent training, allowing a straightforw",
    "link": "http://arxiv.org/abs/2212.14214",
    "context": "Title: Backward Curriculum Reinforcement Learning. (arXiv:2212.14214v3 [cs.AI] UPDATED)\nAbstract: Current reinforcement learning algorithms train an agent using forward-generated trajectories, which provide little guidance so that the agent can explore as much as possible. While realizing the value of reinforcement learning results from sufficient exploration, this approach leads to a trade-off in losing sample efficiency, an essential factor impacting algorithm performance. Previous tasks use reward-shaping techniques and network structure modification to increase sample efficiency. However, these methods require many steps to implement. In this work, we propose novel backward curriculum reinforcement learning that begins training the agent using the backward trajectory of the episode instead of the original forward trajectory. This approach provides the agent with a strong reward signal, enabling more sample-efficient learning. Moreover, our method only requires a minor change in the algorithm of reversing the order of the trajectory before agent training, allowing a straightforw",
    "path": "papers/22/12/2212.14214.json",
    "total_tokens": 865,
    "translated_title": "反向课程强化学习",
    "translated_abstract": "当前强化学习算法使用前向生成轨迹来训练智能体，这种方法提供的指导不足以使智能体进行尽可能多的探索。尽管我们认识到强化学习结果来自充分的探索，但这种方法在样本效率上存在折衷，这是影响算法性能的重要因素。以往的方法使用奖励塑造技术和网络结构修改来增加样本效率，但这些方法需要很多步骤来实现。在本文中，我们提出了一种新颖的反向课程强化学习方法，即通过使用回放轨迹而不是原始的前向轨迹来训练智能体。这种方法为智能体提供了强有力的奖励信号，从而实现更高效的学习。此外，我们的方法只需要在智能体训练之前对轨迹的顺序进行微小的改变，使得实现起来更加直接。",
    "tldr": "这项工作提出了一种新颖的反向课程强化学习方法，通过使用回放轨迹而不是原始的前向轨迹来训练智能体。这种方法通过提供强有力的奖励信号实现了更高效的学习，而且只需要进行微小的算法改变。",
    "en_tdlr": "This work proposes a novel backward curriculum reinforcement learning approach that trains the agent using backward trajectories instead of forward trajectories. This method achieves more efficient learning by providing a strong reward signal, with only minor algorithm changes required."
}