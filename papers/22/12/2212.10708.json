{
    "title": "Zero-shot Triplet Extraction by Template Infilling. (arXiv:2212.10708v2 [cs.CL] UPDATED)",
    "abstract": "The task of triplet extraction aims to extract pairs of entities and their corresponding relations from unstructured text. Most existing methods train an extraction model on training data involving specific target relations, and are incapable of extracting new relations that were not observed at training time. Generalizing the model to unseen relations typically requires fine-tuning on synthetic training data which is often noisy and unreliable. We show that by reducing triplet extraction to a template infilling task over a pre-trained language model (LM), we can equip the extraction model with zero-shot learning capabilities and eliminate the need for additional training data. We propose a novel framework, ZETT (ZEro-shot Triplet extraction by Template infilling), that aligns the task objective to the pre-training objective of generative transformers to generalize to unseen relations. Experiments on FewRel and Wiki-ZSL datasets demonstrate that ZETT shows consistent and stable perform",
    "link": "http://arxiv.org/abs/2212.10708",
    "context": "Title: Zero-shot Triplet Extraction by Template Infilling. (arXiv:2212.10708v2 [cs.CL] UPDATED)\nAbstract: The task of triplet extraction aims to extract pairs of entities and their corresponding relations from unstructured text. Most existing methods train an extraction model on training data involving specific target relations, and are incapable of extracting new relations that were not observed at training time. Generalizing the model to unseen relations typically requires fine-tuning on synthetic training data which is often noisy and unreliable. We show that by reducing triplet extraction to a template infilling task over a pre-trained language model (LM), we can equip the extraction model with zero-shot learning capabilities and eliminate the need for additional training data. We propose a novel framework, ZETT (ZEro-shot Triplet extraction by Template infilling), that aligns the task objective to the pre-training objective of generative transformers to generalize to unseen relations. Experiments on FewRel and Wiki-ZSL datasets demonstrate that ZETT shows consistent and stable perform",
    "path": "papers/22/12/2212.10708.json",
    "total_tokens": 879,
    "translated_title": "零样本模板填充下的三元组抽取",
    "translated_abstract": "三元组抽取的任务旨在从非结构化文本中提取实体对及其对应的关系。大部分现有方法在特定目标关系的训练数据上训练抽取模型，无法提取训练时未观察到的新关系。将模型推广到未见关系通常需要对常常不可靠且噪声较大的合成训练数据进行微调。我们通过将三元组提取任务转化为基于预训练语言模型（LM）的模板填充任务，展示了可以使抽取模型具备零样本学习能力，并消除对额外训练数据的需求。我们提出了一种新的框架ZETT（Zero-shot Triplet extraction by Template infilling），将任务目标与生成式Transformer的预训练目标进行对齐，以推广到未见关系。在FewRel和Wiki-ZSL数据集上的实验证明，ZETT显示出一致且稳定的性能。",
    "tldr": "本论文提出了一种将三元组抽取任务转化为模板填充任务的框架，通过对预训练语言模型进行零样本学习，实现了在没有额外训练数据的情况下对未见关系的抽取，并在实验证明了其稳定且一致的性能表现。"
}