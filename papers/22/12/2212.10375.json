{
    "title": "Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering. (arXiv:2212.10375v2 [cs.CL] UPDATED)",
    "abstract": "Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example permutation (i.e., selection and ordering) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code is released to facilitate future research in this area: https://githu",
    "link": "http://arxiv.org/abs/2212.10375",
    "context": "Title: Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering. (arXiv:2212.10375v2 [cs.CL] UPDATED)\nAbstract: Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example permutation (i.e., selection and ordering) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code is released to facilitate future research in this area: https://githu",
    "path": "papers/22/12/2212.10375.json",
    "total_tokens": 1001,
    "translated_title": "自适应上下文学习：基于信息压缩视角的上下文示例选取和排序方法",
    "translated_abstract": "尽管在上下文学习 (ICL) 中具有惊人的少样本表现，但仍然普遍采用随机选取样本作为上下文的做法。本文提出了一种新的ICL原则：自适应上下文学习。引入自适应机制来帮助每个样本找到一个能够得到正确预测的上下文示例排列（即选取和排序），从而最大化表现。为了验证自适应ICL的有效性，我们提出了一个通用的选择-排序框架，并将其用新的选择和排序算法实例化。在八个不同的NLP数据集上进行了广泛的评估，我们的自适应ICL方法相对于常规设置提高了40%的相对改进。进一步分析揭示了自适应ICL的巨大潜力，即可能通过更先进的算法来缩小ICL和微调之间的差距。我们发布代码，以促进未来在该领域的研究：https://github.com/jxlr/SAICL_iclr22。",
    "tldr": "这篇论文提出了自适应上下文学习的原则，通过引入自适应机制帮助每个样本找到正确的上下文示例排列，从而最大化表现。通过广泛的评估，在8个不同的NLP数据集上，自适应ICL方法相对于常规设置提高了40%的相对改进。",
    "en_tdlr": "This paper proposes a new principle for in-context learning (ICL) called self-adaptive in-context learning and introduces a self-adaptation mechanism to help each sample find an in-context example permutation that can derive the correct prediction. The self-adaptive ICL method achieved a 40% relative improvement over the common practice setting upon extensive evaluation on eight different NLP datasets."
}