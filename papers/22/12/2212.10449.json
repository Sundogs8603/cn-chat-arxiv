{
    "title": "Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization. (arXiv:2212.10449v2 [cs.CL] UPDATED)",
    "abstract": "In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to user-provided queries and identify relevant content to be summarized. We demonstrate the effectiveness of this approach through extensive experimentation on two summarization domains, short stories and dialogue, and multiple control strategies: keywords, questions, and factoid QA pairs. Our pretraining method relies only on unlabeled documents and a question generation system and outperforms pre-finetuning approaches that use additional supervised data. Furthermore, our results show that Socratic pretra",
    "link": "http://arxiv.org/abs/2212.10449",
    "context": "Title: Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization. (arXiv:2212.10449v2 [cs.CL] UPDATED)\nAbstract: In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to user-provided queries and identify relevant content to be summarized. We demonstrate the effectiveness of this approach through extensive experimentation on two summarization domains, short stories and dialogue, and multiple control strategies: keywords, questions, and factoid QA pairs. Our pretraining method relies only on unlabeled documents and a question generation system and outperforms pre-finetuning approaches that use additional supervised data. Furthermore, our results show that Socratic pretra",
    "path": "papers/22/12/2212.10449.json",
    "total_tokens": 969,
    "translated_title": "Socratic预训练：面向可控摘要的问题驱动预训练",
    "translated_abstract": "在标注数据稀缺的情况下，对于长篇文档的可控摘要，预训练模型很难适应任务并有效地响应用户查询。在本文中，我们介绍了Socratic预训练，一种面向问题驱动的无监督预训练方法，旨在提高摘要任务的可控性。通过训练模型生成和回答给定上下文中的相关问题，Socratic预训练使模型能够更有效地遵循用户提供的查询，并确定需要摘要的相关内容。我们通过在两个摘要域上进行广泛实验，即短篇故事和对话，并使用关键词、问题和事实QA对多个控制策略进行了演示。我们的预训练方法只依赖于无标注文档和问题生成系统，表现优于使用额外的监督数据的预精调方法。此外，我们的结果表明，Socratic预训练可以显著提高模型生成遵守用户指定约束条件的摘要的能力。",
    "tldr": "本论文介绍了一种面向问题驱动的无监督预训练方法，名为Socratic预训练，用于提高摘要任务的可控性，并演示了该方法通过在多个控制策略上进行广泛实验得出的优于其他方法的结果。",
    "en_tdlr": "This paper introduces Socratic pretraining, an unsupervised pretraining method that is question-driven and designed to improve controllability in summarization tasks. The method enables the model to adhere better to user queries and generate more relevant summaries. Extensive experimentation shows that the method outperforms other approaches and significantly improves the model's ability to generate summaries that adhere to user-specified constraints."
}