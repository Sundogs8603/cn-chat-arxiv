{
    "title": "A soft nearest-neighbor framework for continual semi-supervised learning. (arXiv:2212.05102v2 [cs.CV] UPDATED)",
    "abstract": "Despite significant advances, the performance of state-of-the-art continual learning approaches hinges on the unrealistic scenario of fully labeled data. In this paper, we tackle this challenge and propose an approach for continual semi-supervised learning--a setting where not all the data samples are labeled. A primary issue in this scenario is the model forgetting representations of unlabeled data and overfitting the labeled samples. We leverage the power of nearest-neighbor classifiers to nonlinearly partition the feature space and flexibly model the underlying data distribution thanks to its non-parametric nature. This enables the model to learn a strong representation for the current task, and distill relevant information from previous tasks. We perform a thorough experimental evaluation and show that our method outperforms all the existing approaches by large margins, setting a solid state of the art on the continual semi-supervised learning paradigm. For example, on CIFAR-100 we",
    "link": "http://arxiv.org/abs/2212.05102",
    "context": "Title: A soft nearest-neighbor framework for continual semi-supervised learning. (arXiv:2212.05102v2 [cs.CV] UPDATED)\nAbstract: Despite significant advances, the performance of state-of-the-art continual learning approaches hinges on the unrealistic scenario of fully labeled data. In this paper, we tackle this challenge and propose an approach for continual semi-supervised learning--a setting where not all the data samples are labeled. A primary issue in this scenario is the model forgetting representations of unlabeled data and overfitting the labeled samples. We leverage the power of nearest-neighbor classifiers to nonlinearly partition the feature space and flexibly model the underlying data distribution thanks to its non-parametric nature. This enables the model to learn a strong representation for the current task, and distill relevant information from previous tasks. We perform a thorough experimental evaluation and show that our method outperforms all the existing approaches by large margins, setting a solid state of the art on the continual semi-supervised learning paradigm. For example, on CIFAR-100 we",
    "path": "papers/22/12/2212.05102.json",
    "total_tokens": 936,
    "translated_title": "一种用于持续半监督学习的软最近邻框架",
    "translated_abstract": "尽管取得了显着进展，最先进的持续学习方法的表现仍然依赖于完全标记的数据的不现实情况。在本文中，我们解决了这一挑战，提出了一种适用于持续半监督学习的方法——一种未标记的数据样本不全部标记的情况。在这种情况下的一个主要问题是模型会忘记未标记数据的表示，并过度拟合标记的样本。我们利用最近邻分类器的威力来非线性地划分特征空间，并由于其非参数性质而灵活地对潜在的数据分布进行建模。这使得模型能够为当前任务学习强大的表示，并从以前的任务中提炼相关的信息。我们进行了彻底的实验评估，并展示了我们的方法以较大的优势优于所有现有的方法，在持续半监督学习范式上树立了坚实的现有技术。",
    "tldr": "该论文介绍了一种用于持续半监督学习的软最近邻框架，该框架利用最近邻分类器来非线性地划分特征空间并非参数地建模潜在的数据分布，以避免模型忘记对未标记数据表示并过度拟合标记的样本。",
    "en_tdlr": "This paper presents a soft nearest-neighbor framework for continual semi-supervised learning, which leverages the power of nearest-neighbor classifiers to nonlinearly partition the feature space and flexibly model the underlying data distribution thanks to its non-parametric nature, avoiding the model forgetting representations of unlabeled data and overfitting the labeled samples."
}