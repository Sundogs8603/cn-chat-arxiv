{
    "title": "Deep Incubation: Training Large Models by Divide-and-Conquering. (arXiv:2212.04129v2 [cs.CV] UPDATED)",
    "abstract": "Recent years have witnessed a remarkable success of large deep learning models. However, training these models is challenging due to high computational costs, painfully slow convergence, and overfitting issues. In this paper, we present Deep Incubation, a novel approach that enables the efficient and effective training of large models by dividing them into smaller sub-modules that can be trained separately and assembled seamlessly. A key challenge for implementing this idea is to ensure the compatibility of the independently trained sub-modules. To address this issue, we first introduce a global, shared meta model, which is leveraged to implicitly link all the modules together, and can be designed as an extremely small network with negligible computational overhead. Then we propose a module incubation algorithm, which trains each sub-module to replace the corresponding component of the meta model and accomplish a given learning task. Despite the simplicity, our approach effectively enc",
    "link": "http://arxiv.org/abs/2212.04129",
    "context": "Title: Deep Incubation: Training Large Models by Divide-and-Conquering. (arXiv:2212.04129v2 [cs.CV] UPDATED)\nAbstract: Recent years have witnessed a remarkable success of large deep learning models. However, training these models is challenging due to high computational costs, painfully slow convergence, and overfitting issues. In this paper, we present Deep Incubation, a novel approach that enables the efficient and effective training of large models by dividing them into smaller sub-modules that can be trained separately and assembled seamlessly. A key challenge for implementing this idea is to ensure the compatibility of the independently trained sub-modules. To address this issue, we first introduce a global, shared meta model, which is leveraged to implicitly link all the modules together, and can be designed as an extremely small network with negligible computational overhead. Then we propose a module incubation algorithm, which trains each sub-module to replace the corresponding component of the meta model and accomplish a given learning task. Despite the simplicity, our approach effectively enc",
    "path": "papers/22/12/2212.04129.json",
    "total_tokens": 913,
    "translated_title": "深度孵化: 分而治之地训练大型模型",
    "translated_abstract": "近年来，大型深度学习模型取得了显著成功。然而，由于高计算成本、缓慢的收敛速度和过度拟合等问题，训练这些模型仍然具有挑战性。本文提出了一种称为深度孵化的新方法，通过将大型模型分成较小的子模块进行训练并无缝地组装起来，从而实现了大型模型的高效、有效训练。实现这个想法的一个关键挑战是确保独立训练的子模块的兼容性。为了解决这个问题，我们首先介绍了一个全局的共享元模型，它被用来隐式地将所有模块链接在一起，并且可以设计为一个具有可忽略计算开销的极小网络。然后我们提出了一个模块孵化算法，它训练每个子模块来替换元模型的相应部分并完成给定的学习任务。尽管简单，我们的方法有效地提高了大型模型的训练效率和效果。",
    "tldr": "本文提出了一种称为深度孵化的训练大型模型的新方法，通过将大型模型分成较小的子模块进行训练并无缝地组装起来，从而实现了大型模型的高效、有效训练。",
    "en_tdlr": "This paper proposes a novel approach called Deep Incubation for training large models by dividing them into smaller sub-modules that can be trained separately and assembled seamlessly, which effectively improves the training efficiency and effectiveness of large models."
}