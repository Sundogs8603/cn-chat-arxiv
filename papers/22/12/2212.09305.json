{
    "title": "SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes. (arXiv:2212.09305v2 [cs.CL] UPDATED)",
    "abstract": "Is it possible to train a general metric for evaluating text generation quality without human annotated ratings? Existing learned metrics either perform unsatisfactorily across text generation tasks or require human ratings for training on specific tasks. In this paper, we propose SESCORE2, a self-supervised approach for training a model-based metric for text generation evaluation. The key concept is to synthesize realistic model mistakes by perturbing sentences retrieved from a corpus. The primary advantage of the SESCORE2 is its ease of extension to many other languages while providing reliable severity estimation. We evaluate SESCORE2 and previous methods on four text generation tasks across three languages. SESCORE2 outperforms unsupervised metric PRISM on four text generation evaluation benchmarks, with a Kendall improvement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised BLEURT and COMET on multiple text generation tasks. The code and data are available at https:",
    "link": "http://arxiv.org/abs/2212.09305",
    "context": "Title: SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes. (arXiv:2212.09305v2 [cs.CL] UPDATED)\nAbstract: Is it possible to train a general metric for evaluating text generation quality without human annotated ratings? Existing learned metrics either perform unsatisfactorily across text generation tasks or require human ratings for training on specific tasks. In this paper, we propose SESCORE2, a self-supervised approach for training a model-based metric for text generation evaluation. The key concept is to synthesize realistic model mistakes by perturbing sentences retrieved from a corpus. The primary advantage of the SESCORE2 is its ease of extension to many other languages while providing reliable severity estimation. We evaluate SESCORE2 and previous methods on four text generation tasks across three languages. SESCORE2 outperforms unsupervised metric PRISM on four text generation evaluation benchmarks, with a Kendall improvement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised BLEURT and COMET on multiple text generation tasks. The code and data are available at https:",
    "path": "papers/22/12/2212.09305.json",
    "total_tokens": 947,
    "translated_title": "SESCORE2: 通过合成真实错误来学习文本生成评估",
    "translated_abstract": "是否可能在没有人工评分的情况下训练一个用于评估文本生成质量的通用度量标准？现有的学习度量标准在文本生成任务上表现不理想，或者需要人工评分来训练特定任务。在本文中，我们提出了SESCORE2，一种自监督的方法，用于训练基于模型的文本生成评估度量标准。其关键概念是通过扰动从语料库中检索到的句子来合成真实的模型错误。SESCORE2的主要优点是其易于扩展到许多其他语言，并提供可靠的严重性估计。我们在三种语言的四个文本生成任务上评估了SESCORE2和以前的方法。SESCORE2在四个文本生成评估基准测试上优于无监督度量标准PRISM，肯德尔改进为0.078。令人惊讶的是，SESCORE2甚至在多个文本生成任务上优于有监督度量标准BLEURT和COMET。代码和数据可在https:获取。",
    "tldr": "这项研究提出了一种自监督方法SESCORE2，通过合成真实的模型错误来训练用于评估文本生成质量的度量标准。SESCORE2在多个语言和任务上的评估中表现出色，并优于其他无监督和有监督的方法。",
    "en_tdlr": "This research proposes a self-supervised method, SESCORE2, for training a metric to evaluate text generation quality by synthesizing realistic model mistakes. SESCORE2 performs well on multiple languages and tasks, outperforming other unsupervised and supervised methods."
}