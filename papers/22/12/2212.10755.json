{
    "title": "JASMINE: Arabic GPT Models for Few-Shot Learning. (arXiv:2212.10755v2 [cs.CL] UPDATED)",
    "abstract": "Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic, a wide collection of languages and dialectal varieties with more than 400 million population, by introducing JASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million-6.7 billion parameters pretrained on a large and diverse dataset (~ 235 GB of text). We also carefully design and release a comprehensive benchmark for both automated and human evaluation of Arabic autoregressive models, with coverage of potential social biases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINE extensively showing powerful performance intrinsically as well as in few-shot learning on a",
    "link": "http://arxiv.org/abs/2212.10755",
    "context": "Title: JASMINE: Arabic GPT Models for Few-Shot Learning. (arXiv:2212.10755v2 [cs.CL] UPDATED)\nAbstract: Scholarship on generative pretraining (GPT) remains acutely Anglocentric, leaving serious gaps in our understanding of the whole class of autoregressive models. For example, we have little knowledge about the potential of these models and their societal impacts in diverse linguistic and cultural settings. We alleviate this issue for Arabic, a wide collection of languages and dialectal varieties with more than 400 million population, by introducing JASMINE. JASMINE is a suite of powerful Arabic autoregressive Transformer language models ranging in size between 300 million-6.7 billion parameters pretrained on a large and diverse dataset (~ 235 GB of text). We also carefully design and release a comprehensive benchmark for both automated and human evaluation of Arabic autoregressive models, with coverage of potential social biases, harms, and toxicity. Using our novel benchmark, we evaluate JASMINE extensively showing powerful performance intrinsically as well as in few-shot learning on a",
    "path": "papers/22/12/2212.10755.json",
    "total_tokens": 896,
    "translated_title": "JASMINE：用于少样本学习的阿拉伯GPT模型",
    "translated_abstract": "生成预训练（GPT）的学术研究仍然以英语为中心，导致我们对自回归模型整个类别的理解存在严重的空白。例如，我们对这些模型在不同语言和文化环境中的潜力及其对社会的影响知之甚少。为了解决这个问题，我们引入了JASMINE，它针对阿拉伯语这个拥有4亿多人口、包含广泛语言和方言变体的语言集合而设计。JASMINE是一组功能强大的阿拉伯自回归Transformer语言模型，参数范围为3亿-67亿，预训练于一个大而多样的数据集（约235GB的文本）。我们还精心设计并发布了一个综合评估阿拉伯自回归模型的基准，包括潜在的社会偏见、危害和有害性。利用我们的新颖基准，我们对JASMINE进行了广泛评估，展示了其自身强大的性能以及在少样本学习上的表现。",
    "tldr": "JASMINE是一组功能强大的阿拉伯自回归Transformer语言模型，预训练于大规模多样的阿拉伯文本数据集，具有在少样本学习上表现出色的能力。",
    "en_tdlr": "JASMINE is a suite of powerful Arabic autoregressive Transformer language models, pretrained on a large and diverse dataset, showing excellent performance in few-shot learning."
}