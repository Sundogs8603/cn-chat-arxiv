{
    "title": "Pre-trained Language Models Can be Fully Zero-Shot Learners. (arXiv:2212.06950v2 [cs.CL] UPDATED)",
    "abstract": "How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, and paraphrasing. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margi",
    "link": "http://arxiv.org/abs/2212.06950",
    "context": "Title: Pre-trained Language Models Can be Fully Zero-Shot Learners. (arXiv:2212.06950v2 [cs.CL] UPDATED)\nAbstract: How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, and paraphrasing. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margi",
    "path": "papers/22/12/2212.06950.json",
    "total_tokens": 976,
    "translated_title": "预训练语言模型可以成为完全零-shot学习器",
    "translated_abstract": "在没有标记或额外未标记数据的情况下，我们如何将预训练模型扩展到许多语言理解任务中？预训练语言模型（PLMs）已经对各种自然语言处理任务有效。然而，现有方法要么需要对下游标记数据集进行微调，要么需要手动构建适当的提示。在本文中，我们提出了用于完全零-shot语言理解的非参数提示PLM（NPPrompt）。与以前的方法不同，NPPrompt仅使用预训练的语言模型，不需要任何标记数据或额外的原始语料库进行进一步的微调，也不依赖于人类构建全面的提示标签词集。我们在不同的自然语言处理任务上评估了NPPrompt和以前的主要少量样本和零射方法：包括文本分类、文本蕴含、相似文本检索和改写。实验结果表明，我们的NPPrompt显示出大幅优于以前最好的完全零-shot方法的表现。",
    "tldr": "本文提出了一种名为NPPrompt的方法，它可以使预训练语言模型成为完全零-shot学习器。相比于现有方法，NPPrompt不需要使用人工标注数据或者构建提示，只需使用预训练的语言模型。NPPrompt在文本分类、文本蕴含、相似文本检索和改写等任务上的表现大幅优于以前最好的完全零-shot方法。",
    "en_tdlr": "The paper proposes a method called NPPrompt, which enables pre-trained language models to become fully zero-shot learners. Unlike existing methods, NPPrompt doesn't require manual labeling or construction of prompts, only using pre-trained language models. It outperforms previous best fully zero-shot methods on text classification, text entailment, similar text retrieval, and paraphrasing tasks."
}