{
    "title": "FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference. (arXiv:2212.08153v2 [cs.CL] UPDATED)",
    "abstract": "Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.",
    "link": "http://arxiv.org/abs/2212.08153",
    "context": "Title: FiDO: Fusion-in-Decoder optimized for stronger performance and faster inference. (arXiv:2212.08153v2 [cs.CL] UPDATED)\nAbstract: Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.",
    "path": "papers/22/12/2212.08153.json",
    "total_tokens": 919,
    "translated_title": "FiDO：针对更强的性能和更快的推理进行优化的解码器融合模型",
    "translated_abstract": "Fusion-in-Decoder (FiD)是一种强大的检索增强语言模型，在许多知识密集型NLP任务上树立了业界标杆。但是，FiD所使用的架构是通过对标准T5模型做最小修改而选择的，我们的分析表明这对于一个检索增强模型来说是高度不优化的。特别地，FiD将大部分FLOPs分配给了编码器，而大多数推理时间是由于解码器中的内存带宽限制。我们提出了两个简单的更改，以缓解内存带宽约束，并使推理速度提高7倍。这使我们能够以适度的成本使用更大的解码器。我们将经过上述修改的FiD称为FiDO，并展示它在广泛的推理预算范围内比现有的FiD模型显著地提高了性能。例如，FiDO-Large-XXL比FiD-Base进行更快的推理，并实现了比FiD-Large更好的性能。",
    "tldr": "这项研究提出了一种名为FiDO的解码器融合模型，通过两个简单的更改，有效缓解了内存带宽约束，加快了模型的推理速度，大大提高了模型性能，达到了领先水平。",
    "en_tdlr": "This research proposes a decoder fusion model called FiDO, which effectively alleviates memory bandwidth constraints and accelerates the inference speed through two simple modifications, greatly improving the model performance and achieving state-of-the-art results."
}