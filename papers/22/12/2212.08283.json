{
    "title": "SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering. (arXiv:2212.08283v2 [cs.CV] UPDATED)",
    "abstract": "Most TextVQA approaches focus on the integration of objects, scene texts and question words by a simple transformer encoder. But this fails to capture the semantic relations between different modalities. The paper proposes a Scene Graph based co-Attention Network (SceneGATE) for TextVQA, which reveals the semantic relations among the objects, Optical Character Recognition (OCR) tokens and the question words. It is achieved by a TextVQA-based scene graph that discovers the underlying semantics of an image. We created a guided-attention module to capture the intra-modal interplay between the language and the vision as a guidance for inter-modal interactions. To make explicit teaching of the relations between the two modalities, we proposed and integrated two attention modules, namely a scene graph-based semantic relation-aware attention and a positional relation-aware attention. We conducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA. It is shown that our SceneG",
    "link": "http://arxiv.org/abs/2212.08283",
    "context": "Title: SceneGATE: Scene-Graph based co-Attention networks for TExt visual question answering. (arXiv:2212.08283v2 [cs.CV] UPDATED)\nAbstract: Most TextVQA approaches focus on the integration of objects, scene texts and question words by a simple transformer encoder. But this fails to capture the semantic relations between different modalities. The paper proposes a Scene Graph based co-Attention Network (SceneGATE) for TextVQA, which reveals the semantic relations among the objects, Optical Character Recognition (OCR) tokens and the question words. It is achieved by a TextVQA-based scene graph that discovers the underlying semantics of an image. We created a guided-attention module to capture the intra-modal interplay between the language and the vision as a guidance for inter-modal interactions. To make explicit teaching of the relations between the two modalities, we proposed and integrated two attention modules, namely a scene graph-based semantic relation-aware attention and a positional relation-aware attention. We conducted extensive experiments on two benchmark datasets, Text-VQA and ST-VQA. It is shown that our SceneG",
    "path": "papers/22/12/2212.08283.json",
    "total_tokens": 1016,
    "translated_title": "SceneGATE：基于场景图的文本视觉问答中的共同关注网络",
    "translated_abstract": "大多数TextVQA方法都注重通过简单的transformer编码器来整合物体、场景文本和问题词，但这无法捕捉不同模态之间的语义关系。本文提出了一种基于场景图的共同关注网络（SceneGATE）用于TextVQA，该网络揭示了图像中物体、光学字符识别（OCR）标记和问题词之间的语义关系。通过一个基于TextVQA的场景图来发现图像的潜在语义，我们创造了一个引导关注模块来捕获语言和视觉之间的内部交互作为跨模态交互的向导。为了明确教授两种模态之间的关系，我们提出并集成了两个注意模块，即基于场景图的语义关系感知注意和位置关系感知注意。我们在两个基准数据集Text-VQA和ST-VQA上进行了广泛的实验。结果表明，我们的SceneGATE在两个数据集上都比现有的方法表现显著。",
    "tldr": "本文提出了一种基于场景图的共同关注网络（SceneGATE）用于TextVQA，通过发现场景图的潜在语义，捕捉了图像中物体、OCR标记和问题词之间的语义关系，并在Text-VQA和ST-VQA两个基准数据集上表现显著优于现有方法。",
    "en_tdlr": "The paper proposes SceneGATE, a Scene Graph based co-Attention Network for TextVQA, which captures the semantic relationships among objects, Optical Character Recognition (OCR) tokens and question words by discovering the underlying semantics of an image through a TextVQA-based scene graph. The guided-attention module and two attention modules, namely scene graph-based semantic relation-aware attention and positional relation-aware attention, are proposed and integrated to teach the explicit relations between language and vision. Extensive experiments on Text-VQA and ST-VQA datasets show that SceneGATE outperforms the state-of-the-art methods significantly."
}