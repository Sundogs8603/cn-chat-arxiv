{
    "title": "FFNeRV: Flow-Guided Frame-Wise Neural Representations for Videos. (arXiv:2212.12294v2 [cs.CV] UPDATED)",
    "abstract": "Neural fields, also known as coordinate-based or implicit neural representations, have shown a remarkable capability of representing, generating, and manipulating various forms of signals. For video representations, however, mapping pixel-wise coordinates to RGB colors has shown relatively low compression performance and slow convergence and inference speed. Frame-wise video representation, which maps a temporal coordinate to its entire frame, has recently emerged as an alternative method to represent videos, improving compression rates and encoding speed. While promising, it has still failed to reach the performance of state-of-the-art video compression algorithms. In this work, we propose FFNeRV, a novel method for incorporating flow information into frame-wise representations to exploit the temporal redundancy across the frames in videos inspired by the standard video codecs. Furthermore, we introduce a fully convolutional architecture, enabled by one-dimensional temporal grids, imp",
    "link": "http://arxiv.org/abs/2212.12294",
    "context": "Title: FFNeRV: Flow-Guided Frame-Wise Neural Representations for Videos. (arXiv:2212.12294v2 [cs.CV] UPDATED)\nAbstract: Neural fields, also known as coordinate-based or implicit neural representations, have shown a remarkable capability of representing, generating, and manipulating various forms of signals. For video representations, however, mapping pixel-wise coordinates to RGB colors has shown relatively low compression performance and slow convergence and inference speed. Frame-wise video representation, which maps a temporal coordinate to its entire frame, has recently emerged as an alternative method to represent videos, improving compression rates and encoding speed. While promising, it has still failed to reach the performance of state-of-the-art video compression algorithms. In this work, we propose FFNeRV, a novel method for incorporating flow information into frame-wise representations to exploit the temporal redundancy across the frames in videos inspired by the standard video codecs. Furthermore, we introduce a fully convolutional architecture, enabled by one-dimensional temporal grids, imp",
    "path": "papers/22/12/2212.12294.json",
    "total_tokens": 819,
    "translated_title": "FFNeRV：流导引的逐帧神经表示视频",
    "translated_abstract": "神经领域，也被称为基于坐标或隐式神经表示，展示了对各种形式信号的出色表示、生成和操作能力。然而，对于视频表示来说，将像素坐标映射到RGB颜色的压缩性能相对较低，收敛速度慢且推理速度慢。逐帧视频表示最近作为一种替代方法出现，将时间坐标映射到整个帧，提高了压缩率和编码速度。虽然有前景，但它仍然无法达到最先进的视频压缩算法的性能。在这项工作中，我们提出了FFNeRV，一种新颖的方法，将流信息融入帧级表示中，以利用视频中帧之间的时间冗余，受到标准视频编解码器的启发。此外，我们引入了一个完全卷积的架构，利用一维时间网格的能力。",
    "tldr": "FFNeRV是一种新颖的方法，将流信息融入帧级表示中，以利用视频中帧之间的时间冗余，并引入了完全卷积的架构。",
    "en_tdlr": "FFNeRV is a novel method that incorporates flow information into frame-wise representations to exploit temporal redundancy across frames in videos, and introduces a fully convolutional architecture."
}