{
    "title": "Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation. (arXiv:2212.07981v2 [cs.CL] UPDATED)",
    "abstract": "Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the co",
    "link": "http://arxiv.org/abs/2212.07981",
    "context": "Title: Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation. (arXiv:2212.07981v2 [cs.CL] UPDATED)\nAbstract: Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the co",
    "path": "papers/22/12/2212.07981.json",
    "total_tokens": 970,
    "translated_title": "重访黄金标准：以健壮的人类评估为基础的摘要评估",
    "translated_abstract": "人类评估是自动摘要系统和评估指标评估的基础。然而，现有的摘要人类评估研究要么存在很低的评分员一致性，要么规模不足，并且缺乏深入的人类评估分析。因此，我们从以下几个方面解决了现有摘要评估的缺点：（1）我们提出了一种修改过的摘要显要性协议，即原子内容单位（ACUs），它基于细粒度语义单元，允许获得较高的评分员一致性。（2）我们开发了一个大型的人类评估数据集RoSE，其中包括超过28个表现最佳的系统在三个数据集上的22,000个摘要级别注释。（3）我们对四种人类评估协议进行了比较研究，强调了评估设置中可能存在的混淆因素。（4）我们使用共现矩阵作为人类评估的代理，评估了50个自动指标及其变体。",
    "tldr": "该论文提出了一种新的摘要显要性协议，ACUs，解决了现有评估协议低一致性的问题。作者建立了一个超大规模评估数据集RoSE并进行了人类评估和自动评估，为摘要评估的相关研究提供了新的思路。",
    "en_tdlr": "This paper proposes a new summarization salience protocol, ACUs, to solve the problem of low inter-annotator agreement in existing evaluation protocols. The authors establish a large-scale evaluation dataset, RoSE, and conduct human and automatic evaluations, providing new ideas for research on summary evaluation."
}