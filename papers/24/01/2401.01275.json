{
    "title": "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation. (arXiv:2401.01275v1 [cs.CL])",
    "abstract": "Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate th",
    "link": "http://arxiv.org/abs/2401.01275",
    "context": "Title: CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation. (arXiv:2401.01275v1 [cs.CL])\nAbstract: Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate th",
    "path": "papers/24/01/2401.01275.json",
    "total_tokens": 913,
    "translated_title": "CharacterEval: 一种用于角色扮演对话代理评估的中文基准",
    "translated_abstract": "最近，大型语言模型（LLM）的出现彻底改变了生成代理的方式。其中，角色扮演对话代理（RPCA）由于其触发用户情感的能力而引起了广泛关注。然而，缺乏一个全面的基准测试集阻碍了该领域的进展。为了填补这一空白，我们推出了CharacterEval，这是一个用于全面评估RPCA的中文基准测试集，并配有一个定制的高质量数据集。该数据集包括1,785个多轮角色扮演对话，涵盖了23,020个示例，涉及了77个来源于中国小说和剧本的角色。它经过精心构建，首先通过GPT-4进行初始对话提取，然后进行严格的人工质量控制，并通过百度百科获取了深入的角色资料。CharacterEval采用多方面的评估方法，包括四个维度上的十三个有针对性的指标。在CharacterEval上进行的全面实验证明了它的有效性。",
    "tldr": "CharacterEval是一个用于角色扮演对话代理评估的中文基准测试集，包含1,785个多轮对话和23,020个示例，涵盖77个角色。实验结果表明CharacterEval在评估RPCA方面是有效的。",
    "en_tdlr": "CharacterEval is a Chinese benchmark for role-playing conversational agent evaluation, consisting of 1,785 multi-turn dialogues and 23,020 examples with 77 characters. The experiments demonstrate the effectiveness of CharacterEval in assessing RPCAs."
}