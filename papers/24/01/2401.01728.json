{
    "title": "Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices. (arXiv:2401.01728v1 [cs.LG])",
    "abstract": "Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets. This trend is expected to continue. However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales. This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics. Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model. These clusters engage in $\\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively e",
    "link": "http://arxiv.org/abs/2401.01728",
    "context": "Title: Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices. (arXiv:2401.01728v1 [cs.LG])\nAbstract: Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets. This trend is expected to continue. However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales. This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics. Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model. These clusters engage in $\\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively e",
    "path": "papers/24/01/2401.01728.json",
    "total_tokens": 953,
    "translated_title": "Ravnest: 异构设备上的分散式异步训练",
    "translated_abstract": "现代的深度学习模型越来越大、越来越复杂，通过在大型数据集上进行训练，已经展示出了异常的泛化和准确性。这一趋势预计将继续。然而，这些模型的增大使得传统的集中式方法在这种规模上受到内存限制的挑战。本文提出了一种用于大型现代深度学习模型的异步分散式训练范式，利用了连接在互联网上的资源受限的异构个人计算机的计算能力，以实现有利的性能指标。Ravnest通过有效地将计算节点组织成具有类似数据传输速率和计算能力的集群来实现分散式训练，而不需要每个节点承载整个模型。这些集群参与$\\textit{零气泡异步模型并行}$训练，利用$\\textit{并行多环全局汇聚}$方法可以有效地进行通信和模型聚合。",
    "tldr": "本文提出了一种用于大型现代深度学习模型的异步分散式训练范式，通过连接互联网上的资源受限的异构个人计算机，利用计算能力来实现有利的性能指标。Ravnest通过有效地将计算节点组织成具有类似数据传输速率和计算能力的集群，实现了分散式训练，而不需要每个节点承载整个模型。",
    "en_tdlr": "This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models, utilizing the compute power of heterogeneous PCs connected through the internet to achieve favorable performance metrics. Ravnest organizes compute nodes into clusters with similar data transfer rates and compute capabilities to enable decentralized training, without requiring each node to host the entire model."
}