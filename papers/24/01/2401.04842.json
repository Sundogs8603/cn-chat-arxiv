{
    "title": "Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers. (arXiv:2401.04842v1 [cs.IR])",
    "abstract": "Large language models can now directly generate answers to many factual questions without referencing external sources. Unfortunately, relatively little attention has been paid to methods for evaluating the quality and correctness of these answers, for comparing the performance of one model to another, or for comparing one prompt to another. In addition, the quality of generated answers are rarely directly compared to the quality of retrieved answers. As models evolve and prompts are modified, we have no systematic way to measure improvements without resorting to expensive human judgments. To address this problem we adapt standard retrieval benchmarks to evaluate answers generated by large language models. Inspired by the BERTScore metric for summarization, we explore two approaches. In the first, we base our evaluation on the benchmark relevance judgments. We empirically run experiments on how information retrieval relevance judgments can be utilized as an anchor to evaluating the gen",
    "link": "http://arxiv.org/abs/2401.04842",
    "context": "Title: Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers. (arXiv:2401.04842v1 [cs.IR])\nAbstract: Large language models can now directly generate answers to many factual questions without referencing external sources. Unfortunately, relatively little attention has been paid to methods for evaluating the quality and correctness of these answers, for comparing the performance of one model to another, or for comparing one prompt to another. In addition, the quality of generated answers are rarely directly compared to the quality of retrieved answers. As models evolve and prompts are modified, we have no systematic way to measure improvements without resorting to expensive human judgments. To address this problem we adapt standard retrieval benchmarks to evaluate answers generated by large language models. Inspired by the BERTScore metric for summarization, we explore two approaches. In the first, we base our evaluation on the benchmark relevance judgments. We empirically run experiments on how information retrieval relevance judgments can be utilized as an anchor to evaluating the gen",
    "path": "papers/24/01/2401.04842.json",
    "total_tokens": 868,
    "translated_title": "将标准检索基准应用于评估生成的答案的研究",
    "translated_abstract": "目前，大型语言模型能够直接生成许多事实性问题的答案，而无需引用外部来源。然而，对于评估这些答案的质量和正确性，比较一个模型与另一个模型的性能，以及比较一个提示与另一个提示的方法相对较少关注。此外，生成答案的质量很少与检索答案的质量直接进行比较。随着模型的演化和提示的修改，我们没有系统的方法来衡量改进，除非采用昂贵的人为判断。为了解决这个问题，我们将标准检索基准应用于评估大型语言模型生成的答案。受BERTScore摘要评估指标的启发，我们探索了两种方法。第一种方法是基于基准相关性判断进行评估。我们进行了实证实验来研究信息检索相关性判断如何作为评估生成答案的锚点。",
    "tldr": "该研究通过将标准检索基准应用于评估大型语言模型生成的答案，解决了评估答案质量的问题。他们探索并比较了两种评估方法，并以信息检索相关性判断作为评估的锚点。",
    "en_tdlr": "This study addresses the issue of evaluating the quality of answers generated by large language models by adapting standard retrieval benchmarks. They explore and compare two evaluation approaches and utilize information retrieval relevance judgments as anchors for evaluation."
}