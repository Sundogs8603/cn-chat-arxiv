{
    "title": "Communication Efficient and Provable Federated Unlearning. (arXiv:2401.11018v1 [cs.LG])",
    "abstract": "We study federated unlearning, a novel problem to eliminate the impact of specific clients or data points on the global model learned via federated learning (FL). This problem is driven by the right to be forgotten and the privacy challenges in FL. We introduce a new framework for exact federated unlearning that meets two essential criteria: \\textit{communication efficiency} and \\textit{exact unlearning provability}. To our knowledge, this is the first work to tackle both aspects coherently. We start by giving a rigorous definition of \\textit{exact} federated unlearning, which guarantees that the unlearned model is statistically indistinguishable from the one trained without the deleted data. We then pinpoint the key property that enables fast exact federated unlearning: total variation (TV) stability, which measures the sensitivity of the model parameters to slight changes in the dataset. Leveraging this insight, we develop a TV-stable FL algorithm called \\texttt{FATS}, which modifies",
    "link": "http://arxiv.org/abs/2401.11018",
    "context": "Title: Communication Efficient and Provable Federated Unlearning. (arXiv:2401.11018v1 [cs.LG])\nAbstract: We study federated unlearning, a novel problem to eliminate the impact of specific clients or data points on the global model learned via federated learning (FL). This problem is driven by the right to be forgotten and the privacy challenges in FL. We introduce a new framework for exact federated unlearning that meets two essential criteria: \\textit{communication efficiency} and \\textit{exact unlearning provability}. To our knowledge, this is the first work to tackle both aspects coherently. We start by giving a rigorous definition of \\textit{exact} federated unlearning, which guarantees that the unlearned model is statistically indistinguishable from the one trained without the deleted data. We then pinpoint the key property that enables fast exact federated unlearning: total variation (TV) stability, which measures the sensitivity of the model parameters to slight changes in the dataset. Leveraging this insight, we develop a TV-stable FL algorithm called \\texttt{FATS}, which modifies",
    "path": "papers/24/01/2401.11018.json",
    "total_tokens": 963,
    "translated_title": "通信高效、可证明的联邦取消学习",
    "translated_abstract": "我们研究了联邦取消学习，这是一个通过联邦学习（FL）消除特定客户端或数据点对全局模型影响的新问题。该问题源于被遗忘的权利和FL中的隐私挑战。我们引入了一个新的框架，用于确切的联邦取消学习，满足两个基本标准：通信效率和确切取消学习的可证明性。据我们所知，这是第一个一致解决这两个方面的工作。我们首先给出了\"确切\"联邦取消学习的严格定义，确保取消学习后的模型在统计上不可区分于在没有被删除数据的情况下训练的模型。然后我们确定了实现快速确切联邦取消学习的关键属性：总变差（TV）稳定性，该属性衡量了模型参数对数据集细微变化的敏感性。基于这一见解，我们开发了一种名为FATS的TV稳定FL算法，对原有的FL算法进行了修改。",
    "tldr": "这篇论文解决了联邦学习中的一个新问题：通过联邦取消学习，消除特定客户端或数据点对全局模型的影响。作者提出了一个通信高效且可证明的确切联邦取消学习框架，并通过开发一个TV稳定FL算法来实现快速确切联邦取消学习。",
    "en_tdlr": "This paper addresses a novel problem in federated learning - eliminating the impact of specific clients or data points on the global model. The authors propose a communication efficient and provable framework for exact federated unlearning, and develop a TV-stable FL algorithm called FATS to achieve fast exact unlearning."
}