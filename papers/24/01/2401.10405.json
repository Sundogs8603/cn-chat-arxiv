{
    "title": "Differentially Private and Adversarially Robust Machine Learning: An Empirical Evaluation. (arXiv:2401.10405v1 [cs.LG])",
    "abstract": "Malicious adversaries can attack machine learning models to infer sensitive information or damage the system by launching a series of evasion attacks. Although various work addresses privacy and security concerns, they focus on individual defenses, but in practice, models may undergo simultaneous attacks. This study explores the combination of adversarial training and differentially private training to defend against simultaneous attacks. While differentially-private adversarial training, as presented in DP-Adv, outperforms the other state-of-the-art methods in performance, it lacks formal privacy guarantees and empirical validation. Thus, in this work, we benchmark the performance of this technique using a membership inference attack and empirically show that the resulting approach is as private as non-robust private models. This work also highlights the need to explore privacy guarantees in dynamic training paradigms.",
    "link": "http://arxiv.org/abs/2401.10405",
    "context": "Title: Differentially Private and Adversarially Robust Machine Learning: An Empirical Evaluation. (arXiv:2401.10405v1 [cs.LG])\nAbstract: Malicious adversaries can attack machine learning models to infer sensitive information or damage the system by launching a series of evasion attacks. Although various work addresses privacy and security concerns, they focus on individual defenses, but in practice, models may undergo simultaneous attacks. This study explores the combination of adversarial training and differentially private training to defend against simultaneous attacks. While differentially-private adversarial training, as presented in DP-Adv, outperforms the other state-of-the-art methods in performance, it lacks formal privacy guarantees and empirical validation. Thus, in this work, we benchmark the performance of this technique using a membership inference attack and empirically show that the resulting approach is as private as non-robust private models. This work also highlights the need to explore privacy guarantees in dynamic training paradigms.",
    "path": "papers/24/01/2401.10405.json",
    "total_tokens": 952,
    "translated_title": "差分隐私和对抗性鲁棒的机器学习：经验评估",
    "translated_abstract": "恶意对手可以通过发起一系列的逃逸攻击来推断敏感信息或破坏机器学习模型。尽管各种工作致力于解决隐私和安全问题，但它们集中在个别防御上，而实际上，模型可能同时遭受多个攻击。本研究探讨对抗训练和差分隐私训练的组合，以应对同时攻击。虽然DP-Adv中提出的差分隐私对抗训练在性能上优于其他最先进的方法，但它缺乏正式的隐私保证和实证验证。因此，在本文中，我们使用成员推断攻击来基准测试该技术的性能，并经验证明该方法与非鲁棒私有模型一样具有隐私性。本研究还强调了探索动态训练范式中隐私保证的需求。",
    "tldr": "本研究旨在评估差分隐私和对抗性鲁棒机器学习的效果。通过将对抗训练和差分隐私训练相结合来应对同时攻击，实证结果表明该方法在性能上优于其他方法，并且在隐私保证方面与非鲁棒私有模型相当。这项研究还强调了对动态训练范式中隐私保证的需求。",
    "en_tdlr": "This study evaluates the effectiveness of differentially private and adversarially robust machine learning. By combining adversarial training and differentially private training to defend against simultaneous attacks, it outperforms other methods in terms of performance and provides privacy guarantees comparable to non-robust private models. The research also emphasizes the need for privacy guarantees in dynamic training paradigms."
}