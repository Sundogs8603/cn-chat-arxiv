{
    "title": "TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models. (arXiv:2401.06620v1 [cs.CL])",
    "abstract": "There are 293 scripts representing over 7,000 languages in the written form. Due to various reasons, many closely related languages use different scripts, which poses difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a result, mPLMs present a script barrier: representations from different scripts are located in different subspaces, which is a strong indicator of why crosslingual transfer involving languages of different scripts shows sub-optimal performance. To address this problem, we propose a simple framework TransliCo that contains Transliteration Contrastive Modeling (TCM) to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (Latn, in our case), which ensures uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we find-tune it on a small portion (5\\%) of its ",
    "link": "http://arxiv.org/abs/2401.06620",
    "context": "Title: TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models. (arXiv:2401.06620v1 [cs.CL])\nAbstract: There are 293 scripts representing over 7,000 languages in the written form. Due to various reasons, many closely related languages use different scripts, which poses difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a result, mPLMs present a script barrier: representations from different scripts are located in different subspaces, which is a strong indicator of why crosslingual transfer involving languages of different scripts shows sub-optimal performance. To address this problem, we propose a simple framework TransliCo that contains Transliteration Contrastive Modeling (TCM) to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (Latn, in our case), which ensures uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we find-tune it on a small portion (5\\%) of its ",
    "path": "papers/24/01/2401.06620.json",
    "total_tokens": 1027,
    "translated_title": "TransliCo：一种用于解决多语言预训练语言模型中脚本障碍的对比学习框架",
    "translated_abstract": "书面形式中有293种脚本代表着7000多种语言。由于各种原因，许多密切相关的语言使用不同的脚本，这给多语言预训练语言模型（mPLMs）通过词汇重叠学习跨语言知识带来了困难。因此，mPLMs存在脚本障碍：来自不同脚本的表示位于不同子空间中，这是为什么涉及不同脚本语言的跨语言传递显示次优性能的强有力指标。为了解决这个问题，我们提出了一个简单的框架TransliCo，它包含Transliteration Contrastive Modeling（TCM），通过对训练数据中的句子及其在统一脚本（在我们的案例中是拉丁字母）中的音译进行对比，来微调mPLM，从而确保不同脚本的表示空间的一致性。使用Glot500-m作为我们的源模型，它是在500多种语言上预训练的mPLM，我们将其在其5％的小部分上微调",
    "tldr": "本论文提出了TransliCo，一个对比学习框架，用于解决多语言预训练语言模型中的脚本障碍。通过对比不同脚本的句子及其在统一脚本中的音译，实现了不同脚本的统一表示空间。实验证明，这种方法能够改善跨语言传递的性能。",
    "en_tdlr": "This paper introduces TransliCo, a contrastive learning framework to address the script barrier in multilingual pretrained language models. By contrasting sentences from different scripts with their transliterations in a unified script, it achieves a consistent representation space for different scripts. Experimental results demonstrate that this approach improves crosslingual transfer performance."
}