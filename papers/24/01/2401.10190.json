{
    "title": "A Kaczmarz-inspired approach to accelerate the optimization of neural network wavefunctions. (arXiv:2401.10190v1 [physics.comp-ph])",
    "abstract": "Neural network wavefunctions optimized using the variational Monte Carlo method have been shown to produce highly accurate results for the electronic structure of atoms and small molecules, but the high cost of optimizing such wavefunctions prevents their application to larger systems. We propose the Subsampled Projected-Increment Natural Gradient Descent (SPRING) optimizer to reduce this bottleneck. SPRING combines ideas from the recently introduced minimum-step stochastic reconfiguration optimizer (MinSR) and the classical randomized Kaczmarz method for solving linear least-squares problems. We demonstrate that SPRING outperforms both MinSR and the popular Kronecker-Factored Approximate Curvature method (KFAC) across a number of small atoms and molecules, given that the learning rates of all methods are optimally tuned. For example, on the oxygen atom, SPRING attains chemical accuracy after forty thousand training iterations, whereas both MinSR and KFAC fail to do so even after one h",
    "link": "http://arxiv.org/abs/2401.10190",
    "context": "Title: A Kaczmarz-inspired approach to accelerate the optimization of neural network wavefunctions. (arXiv:2401.10190v1 [physics.comp-ph])\nAbstract: Neural network wavefunctions optimized using the variational Monte Carlo method have been shown to produce highly accurate results for the electronic structure of atoms and small molecules, but the high cost of optimizing such wavefunctions prevents their application to larger systems. We propose the Subsampled Projected-Increment Natural Gradient Descent (SPRING) optimizer to reduce this bottleneck. SPRING combines ideas from the recently introduced minimum-step stochastic reconfiguration optimizer (MinSR) and the classical randomized Kaczmarz method for solving linear least-squares problems. We demonstrate that SPRING outperforms both MinSR and the popular Kronecker-Factored Approximate Curvature method (KFAC) across a number of small atoms and molecules, given that the learning rates of all methods are optimally tuned. For example, on the oxygen atom, SPRING attains chemical accuracy after forty thousand training iterations, whereas both MinSR and KFAC fail to do so even after one h",
    "path": "papers/24/01/2401.10190.json",
    "total_tokens": 902,
    "translated_title": "一种启发于Kaczmarz的方法加速神经网络波函数的优化",
    "translated_abstract": "通过变分蒙特卡罗方法优化的神经网络波函数已被证明在原子和小分子的电子结构方面产生高精度结果，但是优化这种波函数的高成本限制了它们在更大系统中的应用。我们提出了Subsampled Projected-Increment Natural Gradient Descent (SPRING)优化器来减少这个瓶颈。SPRING结合了之前引入的最小步长随机重构优化器(MinSR)和经典的随机Kaczmarz方法解决线性最小二乘问题的思想。我们证明了在多个小原子和分子上，SPRING优于MinSR和流行的Kronecker-Factored Approximate Curvature方法(KFAC)，前提是各种方法的学习率都经过了最佳调整。例如，在氧原子上，SPRING在四万次训练迭代之后达到了化学精度，而MinSR和KFAC甚至在一个小时后也无法做到这一点。",
    "tldr": "本论文提出了一种启发于Kaczmarz的方法加速神经网络波函数的优化，该方法结合了最小步长随机重构优化器(MinSR)和随机Kaczmarz方法，相对于其他方法在多个小原子和分子上表现更优。",
    "en_tdlr": "This paper proposes a Kaczmarz-inspired approach to accelerate the optimization of neural network wavefunctions, which outperforms other methods across multiple small atoms and molecules by combining the minimum-step stochastic reconfiguration optimizer (MinSR) and the randomized Kaczmarz method."
}