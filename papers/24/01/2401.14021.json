{
    "title": "Accelerating Retrieval-Augmented Language Model Serving with Speculation. (arXiv:2401.14021v1 [cs.LG])",
    "abstract": "Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit t",
    "link": "http://arxiv.org/abs/2401.14021",
    "context": "Title: Accelerating Retrieval-Augmented Language Model Serving with Speculation. (arXiv:2401.14021v1 [cs.LG])\nAbstract: Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit t",
    "path": "papers/24/01/2401.14021.json",
    "total_tokens": 902,
    "translated_title": "使用推测加速检索增强型语言模型服务",
    "translated_abstract": "检索增强型语言模型（RaLM）通过将非参数的知识库与参数化的语言模型相结合，已经展示出解决知识密集型自然语言处理（NLP）任务的潜力。与对完全参数化模型进行微调不同，RaLM在适应最新数据和更好的来源归属机制方面具有低成本的优势。在众多的RaLM方法中，迭代式RaLM由于检索器与语言模型之间更频繁的互动而具有更好的生成质量。尽管有这些好处，迭代式RaLM通常会因为频繁的检索步骤而遇到高开销。为此，我们提出了RaLMSpec，这是一个基于推测的框架，通过推测式检索和批量验证，能够在保持相同模型输出的同时，提供通用加速的效果。通过进一步结合预取、最佳推测步幅调度器和异步验证，RaLMSpec能够自动利用并发性和并行性来最大程度地提高性能。",
    "tldr": "提出了RaLMSpec，这是一个使用推测加速检索增强型语言模型服务的框架，通过推测式检索和批量验证提供了通用的加速效果，并通过进一步优化和并发处理，提高了性能。"
}