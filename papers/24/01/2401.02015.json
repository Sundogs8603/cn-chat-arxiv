{
    "title": "Improving Diffusion-Based Image Synthesis with Context Prediction. (arXiv:2401.02015v1 [cs.CV] CROSS LISTED)",
    "abstract": "Diffusion models are a new class of generative models, and have dramatically promoted image generation with unprecedented quality and diversity. Existing diffusion models mainly try to reconstruct input image from a corrupted one with a pixel-wise or feature-wise constraint along spatial axes. However, such point-based reconstruction may fail to make each predicted pixel/feature fully preserve its neighborhood context, impairing diffusion-based image synthesis. As a powerful source of automatic supervisory signal, context has been well studied for learning representations. Inspired by this, we for the first time propose ConPreDiff to improve diffusion-based image synthesis with context prediction. We explicitly reinforce each point to predict its neighborhood context (i.e., multi-stride features/tokens/pixels) with a context decoder at the end of diffusion denoising blocks in training stage, and remove the decoder for inference. In this way, each point can better reconstruct itself by ",
    "link": "http://arxiv.org/abs/2401.02015",
    "context": "Title: Improving Diffusion-Based Image Synthesis with Context Prediction. (arXiv:2401.02015v1 [cs.CV] CROSS LISTED)\nAbstract: Diffusion models are a new class of generative models, and have dramatically promoted image generation with unprecedented quality and diversity. Existing diffusion models mainly try to reconstruct input image from a corrupted one with a pixel-wise or feature-wise constraint along spatial axes. However, such point-based reconstruction may fail to make each predicted pixel/feature fully preserve its neighborhood context, impairing diffusion-based image synthesis. As a powerful source of automatic supervisory signal, context has been well studied for learning representations. Inspired by this, we for the first time propose ConPreDiff to improve diffusion-based image synthesis with context prediction. We explicitly reinforce each point to predict its neighborhood context (i.e., multi-stride features/tokens/pixels) with a context decoder at the end of diffusion denoising blocks in training stage, and remove the decoder for inference. In this way, each point can better reconstruct itself by ",
    "path": "papers/24/01/2401.02015.json",
    "total_tokens": 890,
    "translated_title": "改进基于扩散的图像合成与上下文预测",
    "translated_abstract": "扩散模型是一种新的生成模型类别，极大提升了图像生成的质量和多样性。现有的扩散模型主要通过像素或特征约束在空间轴上对损坏图像进行重建。然而，这种点对点的重建可能无法完全保留每个预测像素/特征的邻域上下文，影响了基于扩散的图像合成。为了解决这个问题，我们首次提出了ConPreDiff，通过上下文预测改进基于扩散的图像合成。训练阶段，在扩散去噪块的末端增加了一个上下文解码器，明确地鼓励每个点预测其邻域上下文（即多步长特征/令牌/像素），并在推理阶段去除解码器。通过这种方式，每个点可以更好地重建自身。",
    "tldr": "本研究提出了一种名为ConPreDiff的方法，通过上下文预测来改善基于扩散的图像合成。在训练阶段，我们使用上下文解码器鼓励每个点预测其邻域上下文，并在推理阶段去除解码器。这种方法能够更好地重建图像。",
    "en_tdlr": "This paper proposes a method called ConPreDiff to improve diffusion-based image synthesis by incorporating context prediction. By using a context decoder during training to encourage each point to predict its neighborhood context and removing the decoder during inference, this approach allows for better image reconstruction."
}