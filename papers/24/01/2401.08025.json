{
    "title": "Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination",
    "abstract": "arXiv:2401.08025v2 Announce Type: replace  Abstract: The potential of Vision-Language Models (VLMs) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach on three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on all math tasks (on aver",
    "link": "https://arxiv.org/abs/2401.08025",
    "context": "Title: Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination\nAbstract: arXiv:2401.08025v2 Announce Type: replace  Abstract: The potential of Vision-Language Models (VLMs) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach on three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on all math tasks (on aver",
    "path": "papers/24/01/2401.08025.json",
    "total_tokens": 865,
    "translated_title": "自我想象：利用自我想象进行多模型自然推理",
    "translated_abstract": "Vision-Language模型（VLMs）的潜力在处理复杂基于文本问题时往往未被充分利用，尤其是当这些问题能够从视觉表达中获益时。本文提出了Self-Imagine，与人类通过创建问题的视觉图并推断解决步骤的能力相 resonating。我们利用单一的Vision-Language模型（VLM）使用HTML生成问题的结构化表示，然后将HTML渲染为图像，并最终使用相同的VLM根据问题和图像回答问题。我们的方法不需要任何额外的训练数据或训练。我们使用最先进的（LLAVA-1.5和GEMINI PRO）VLMs在三个数学任务和九个通用推理任务上评估了我们的方法。我们的方法提升了LLAVA-1.5和GEMINI PRO在所有数学任务上的性能。",
    "tldr": "本文提出了Self-Imagine方法，通过利用一种Vision-Language模型生成问题的结构化表示并将其渲染为图像，再使用相同的模型回答问题，从而在数学任务和通用推理任务中提高了模型性能。",
    "en_tdlr": "This paper introduces the Self-Imagine method, which boosts model performance in mathematics and general reasoning tasks by utilizing a Vision-Language model to generate a structured representation of the question, render it as an image, and then answer the question with the same model."
}