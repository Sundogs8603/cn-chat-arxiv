{
    "title": "On the Expressive Power of Graph Neural Networks. (arXiv:2401.01626v1 [cs.LG])",
    "abstract": "The study of Graph Neural Networks has received considerable interest in the past few years. By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine. The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification. However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.  While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives. Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs. Another direction focuses on the extent to which GNNs can distinguish between different graph",
    "link": "http://arxiv.org/abs/2401.01626",
    "context": "Title: On the Expressive Power of Graph Neural Networks. (arXiv:2401.01626v1 [cs.LG])\nAbstract: The study of Graph Neural Networks has received considerable interest in the past few years. By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine. The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification. However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.  While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives. Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs. Another direction focuses on the extent to which GNNs can distinguish between different graph",
    "path": "papers/24/01/2401.01626.json",
    "total_tokens": 865,
    "translated_title": "关于图神经网络的表达能力研究",
    "translated_abstract": "过去几年来，图神经网络的研究引起了相当大的兴趣。通过将深度学习扩展到图结构化数据，GNN可以解决社会科学、化学和医学等领域的各种任务。GNN架构的发展主要集中在改进节点或图分类等任务的实证性性能。然而，最近的一系列工作则寻求找到具有理论特性的GNN架构，通过研究其表达能力并设计最大化这种表达能力的架构。虽然关于如何定义GNN的表达能力还没有共识，但可以从几个有很好动机的角度来看待。也许最自然的方法是研究GNN的通用逼近性质，就像MLP的这种性质一样得到了广泛的研究。另一个方向关注的是GNN在区分不同图之间的能力程度。",
    "tldr": "研究人员对图神经网络的表达能力和设计架构进行了大量工作，以提高其在各领域任务中的性能。主要方法包括研究GNN的通用逼近性质和其在区分不同图之间的能力程度。"
}