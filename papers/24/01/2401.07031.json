{
    "title": "Code Security Vulnerability Repair Using Reinforcement Learning with Large Language Models",
    "abstract": "With the recent advancement of Large Language Models (LLMs), generating functionally correct code has become less complicated for a wide array of developers. While using LLMs has sped up the functional development process, it poses a heavy risk to code security. Code generation with proper security measures using LLM is a significantly more challenging task than functional code generation. Security measures may include adding a pair of lines of code with the original code, consisting of null pointer checking or prepared statements for SQL injection prevention. Currently, available code repair LLMs generate code repair by supervised fine-tuning, where the model looks at cross-entropy loss. However, the original and repaired codes are mostly similar in functionality and syntactically, except for a few (1-2) lines, which act as security measures. This imbalance between the lines needed for security measures and the functional code enforces the supervised fine-tuned model to prioritize gen",
    "link": "https://arxiv.org/abs/2401.07031",
    "context": "Title: Code Security Vulnerability Repair Using Reinforcement Learning with Large Language Models\nAbstract: With the recent advancement of Large Language Models (LLMs), generating functionally correct code has become less complicated for a wide array of developers. While using LLMs has sped up the functional development process, it poses a heavy risk to code security. Code generation with proper security measures using LLM is a significantly more challenging task than functional code generation. Security measures may include adding a pair of lines of code with the original code, consisting of null pointer checking or prepared statements for SQL injection prevention. Currently, available code repair LLMs generate code repair by supervised fine-tuning, where the model looks at cross-entropy loss. However, the original and repaired codes are mostly similar in functionality and syntactically, except for a few (1-2) lines, which act as security measures. This imbalance between the lines needed for security measures and the functional code enforces the supervised fine-tuned model to prioritize gen",
    "path": "papers/24/01/2401.07031.json",
    "total_tokens": 860,
    "translated_title": "使用大型语言模型的强化学习修复代码安全漏洞",
    "translated_abstract": "随着大型语言模型（LLMs）的最新进展，对于各种开发者来说，生成功能正确的代码变得不再那么复杂。虽然使用LLMs加速了功能开发过程，但对代码安全性构成了重大风险。在使用LLMs进行带有安全措施的代码生成时，相较于功能代码生成，任务更为困难。安全措施可能包括在原始代码中添加一对代码行，包括空指针检查或准备好的语句以防止SQL注入。目前，可用的代码修复LLMs通过监督微调生成代码修复，模型通过交叉熵损失进行训练。然而，原始代码和修复后的代码在功能和语法上大致相似，除了少数（1-2）行作为安全措施。这种安全措施所需行数与功能代码之间的不平衡导致监督微调模型优先考虑生成功能代码。",
    "tldr": "本论文研究了使用大型语言模型的强化学习方法，以修复代码中的安全漏洞。目前的模型主要通过监督微调生成修复代码，但存在安全措施代码和功能代码之间的不平衡问题。",
    "en_tdlr": "This paper explores the use of reinforcement learning with large language models to repair code security vulnerabilities. Current models primarily rely on supervised fine-tuning to generate repair code, but struggle with the imbalance between security measures and functional code."
}