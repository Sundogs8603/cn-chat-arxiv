{
    "title": "Attention, Distillation, and Tabularization: Towards Practical Neural Network-Based Prefetching. (arXiv:2401.06362v1 [cs.NE])",
    "abstract": "Attention-based Neural Networks (NN) have demonstrated their effectiveness in accurate memory access prediction, an essential step in data prefetching. However, the substantial computational overheads associated with these models result in high inference latency, limiting their feasibility as practical prefetchers. To close the gap, we propose a new approach based on tabularization that significantly reduces model complexity and inference latency without sacrificing prediction accuracy. Our novel tabularization methodology takes as input a distilled, yet highly accurate attention-based model for memory access prediction and efficiently converts its expensive matrix multiplications into a hierarchy of fast table lookups. As an exemplar of the above approach, we develop DART, a prefetcher comprised of a simple hierarchy of tables. With a modest 0.09 drop in F1-score, DART reduces 99.99% of arithmetic operations from the large attention-based model and 91.83% from the distilled model. DAR",
    "link": "http://arxiv.org/abs/2401.06362",
    "context": "Title: Attention, Distillation, and Tabularization: Towards Practical Neural Network-Based Prefetching. (arXiv:2401.06362v1 [cs.NE])\nAbstract: Attention-based Neural Networks (NN) have demonstrated their effectiveness in accurate memory access prediction, an essential step in data prefetching. However, the substantial computational overheads associated with these models result in high inference latency, limiting their feasibility as practical prefetchers. To close the gap, we propose a new approach based on tabularization that significantly reduces model complexity and inference latency without sacrificing prediction accuracy. Our novel tabularization methodology takes as input a distilled, yet highly accurate attention-based model for memory access prediction and efficiently converts its expensive matrix multiplications into a hierarchy of fast table lookups. As an exemplar of the above approach, we develop DART, a prefetcher comprised of a simple hierarchy of tables. With a modest 0.09 drop in F1-score, DART reduces 99.99% of arithmetic operations from the large attention-based model and 91.83% from the distilled model. DAR",
    "path": "papers/24/01/2401.06362.json",
    "total_tokens": 975,
    "translated_title": "注意力、蒸馏和表格化：走向实用的基于神经网络的预取模型",
    "translated_abstract": "基于注意力的神经网络在准确的内存访问预测中表现出了高效性，这是数据预取的一个关键步骤。然而，这些模型所带来的计算开销造成了高推理延迟，限制了它们作为实际预取模型的可行性。为了弥合这一差距，我们提出了一种基于表格化的新方法，该方法显著降低了模型复杂度和推理延迟，同时又不牺牲预测准确性。我们的新颖的表格化方法将一个经过蒸馏的具有高精确度的注意力模型作为输入，将其昂贵的矩阵乘法转换成快速表格查找的层次结构。作为上述方法的示例，我们开发了DART，一个由简单表格层次结构组成的预取模型。在F1得分下降了0.09的情况下，DART从大型注意力模型中减少了99.99%的算术运算，从蒸馏模型中减少了91.83%的运算量。",
    "tldr": "我们提出了一种基于表格化的方法，通过将注意力模型转换为层次结构的表格查找，显著降低了预取模型的复杂度和推理延迟，同时保持了高准确性。通过我们的方法，我们开发了一个DART预取模型，在减少计算量的情况下只有轻微的性能下降。",
    "en_tdlr": "We propose a tabularization-based approach that significantly reduces the complexity and inference latency of prefetching models while maintaining high accuracy. Using this approach, we develop DART, a prefetcher that achieves a significant reduction in computation while maintaining high performance."
}