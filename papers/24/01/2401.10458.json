{
    "title": "Contrastive Unlearning: A Contrastive Approach to Machine Unlearning. (arXiv:2401.10458v1 [cs.LG])",
    "abstract": "Machine unlearning aims to eliminate the influence of a subset of training samples (i.e., unlearning samples) from a trained model. Effectively and efficiently removing the unlearning samples without negatively impacting the overall model performance is still challenging. In this paper, we propose a contrastive unlearning framework, leveraging the concept of representation learning for more effective unlearning. It removes the influence of unlearning samples by contrasting their embeddings against the remaining samples so that they are pushed away from their original classes and pulled toward other classes. By directly optimizing the representation space, it effectively removes the influence of unlearning samples while maintaining the representations learned from the remaining samples. Experiments on a variety of datasets and models on both class unlearning and sample unlearning showed that contrastive unlearning achieves the best unlearning effects and efficiency with the lowest perfo",
    "link": "http://arxiv.org/abs/2401.10458",
    "context": "Title: Contrastive Unlearning: A Contrastive Approach to Machine Unlearning. (arXiv:2401.10458v1 [cs.LG])\nAbstract: Machine unlearning aims to eliminate the influence of a subset of training samples (i.e., unlearning samples) from a trained model. Effectively and efficiently removing the unlearning samples without negatively impacting the overall model performance is still challenging. In this paper, we propose a contrastive unlearning framework, leveraging the concept of representation learning for more effective unlearning. It removes the influence of unlearning samples by contrasting their embeddings against the remaining samples so that they are pushed away from their original classes and pulled toward other classes. By directly optimizing the representation space, it effectively removes the influence of unlearning samples while maintaining the representations learned from the remaining samples. Experiments on a variety of datasets and models on both class unlearning and sample unlearning showed that contrastive unlearning achieves the best unlearning effects and efficiency with the lowest perfo",
    "path": "papers/24/01/2401.10458.json",
    "total_tokens": 891,
    "translated_title": "对比反训练：一种对比方法用于机器反学习",
    "translated_abstract": "机器反学习旨在消除训练样本子集（即反学习样本）对已训练模型的影响。有效且高效地移除反学习样本，同时不会对整体模型性能产生负面影响仍然具有挑战性。在本文中，我们提出了一种对比反学习框架，利用表示学习的概念来实现更有效的反学习。该方法通过将反学习样本的嵌入与剩余样本进行对比，将它们从原始类别中推开并拉向其他类别，从而消除其影响。通过直接优化表示空间，它有效地去除了反学习样本的影响，同时保持了从剩余样本中学到的表示。在各种数据集和模型上进行的实验证明，对比反学习能够以最低的性能损失实现最好的反学习效果和效率。",
    "tldr": "该论文提出了一种对比反学习的框架，通过对比反学习样本和剩余样本的嵌入，将反学习样本推离原始类别并拉向其他类别，从而有效地消除其影响，同时保持了剩余样本学习到的表示。实验证明，对比反学习在反学习效果和效率方面表现最佳。"
}