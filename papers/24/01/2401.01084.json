{
    "title": "Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction. (arXiv:2401.01084v1 [cs.LG])",
    "abstract": "Natural policy gradient (NPG) and its variants are widely-used policy search methods in reinforcement learning. Inspired by prior work, a new NPG variant coined NPG-HM is developed in this paper, which utilizes the Hessian-aided momentum technique for variance reduction, while the sub-problem is solved via the stochastic gradient descent method. It is shown that NPG-HM can achieve the global last iterate $\\epsilon$-optimality with a sample complexity of $\\mathcal{O}(\\epsilon^{-2})$, which is the best known result for natural policy gradient type methods under the generic Fisher non-degenerate policy parameterizations. The convergence analysis is built upon a relaxed weak gradient dominance property tailored for NPG under the compatible function approximation framework, as well as a neat way to decompose the error when handling the sub-problem. Moreover, numerical experiments on Mujoco-based environments demonstrate the superior performance of NPG-HM over other state-of-the-art policy g",
    "link": "http://arxiv.org/abs/2401.01084",
    "context": "Title: Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction. (arXiv:2401.01084v1 [cs.LG])\nAbstract: Natural policy gradient (NPG) and its variants are widely-used policy search methods in reinforcement learning. Inspired by prior work, a new NPG variant coined NPG-HM is developed in this paper, which utilizes the Hessian-aided momentum technique for variance reduction, while the sub-problem is solved via the stochastic gradient descent method. It is shown that NPG-HM can achieve the global last iterate $\\epsilon$-optimality with a sample complexity of $\\mathcal{O}(\\epsilon^{-2})$, which is the best known result for natural policy gradient type methods under the generic Fisher non-degenerate policy parameterizations. The convergence analysis is built upon a relaxed weak gradient dominance property tailored for NPG under the compatible function approximation framework, as well as a neat way to decompose the error when handling the sub-problem. Moreover, numerical experiments on Mujoco-based environments demonstrate the superior performance of NPG-HM over other state-of-the-art policy g",
    "path": "papers/24/01/2401.01084.json",
    "total_tokens": 999,
    "translated_title": "具有Hessian辅助动量方差减小的自然策略梯度全局收敛",
    "translated_abstract": "自然策略梯度（NPG）及其变体是强化学习中广泛使用的策略搜索方法。本文在之前的工作的基础上，开发了一种新的NPG变体，命名为NPG-HM，该方法利用了Hessian辅助动量技术进行方差减小，并通过随机梯度下降方法解决子问题。结果表明，NPG-HM可以在样本复杂度为$\\mathcal{O}(\\epsilon^{-2})$的情况下实现全局最后迭代的$\\epsilon$-最优性，这是在通用的Fisher非退化策略参数化下自然策略梯度方法中已知的最佳结果。收敛分析建立在针对NPG的松弛弱梯度优势性质以及处理子问题时的错误分解的兼容函数逼近框架下。此外，基于Mujoco环境的数值实验表明NPG-HM相对于其他最先进的策略方法展现出卓越的性能。",
    "tldr": "本文开发了一种新的自然策略梯度变体NPG-HM，采用Hessian辅助动量技术进行方差减小，通过随机梯度下降解决子问题。实验证明NPG-HM在通用Fisher非退化策略参数化下可以实现全局最后迭代的$\\epsilon$-最优性，并且在Mujoco环境中表现出卓越的性能。",
    "en_tdlr": "A new variant of natural policy gradient, NPG-HM, is developed in this paper, which utilizes the Hessian-aided momentum technique for variance reduction and solves the sub-problem via stochastic gradient descent. It is shown that NPG-HM achieves global last iterate $\\epsilon$-optimality and demonstrates superior performance in Mujoco-based environments."
}