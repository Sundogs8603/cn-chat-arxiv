{
    "title": "Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics. (arXiv:2401.13391v1 [cs.LG])",
    "abstract": "Artificial Intelligence (AI) finds widespread applications across various domains, sparking concerns about fairness in its deployment. While fairness in AI remains a central concern, the prevailing discourse often emphasizes outcome-based metrics without a nuanced consideration of the differential impacts within subgroups. Bias mitigation techniques do not only affect the ranking of pairs of instances across sensitive groups, but often also significantly affect the ranking of instances within these groups. Such changes are hard to explain and raise concerns regarding the validity of the intervention. Unfortunately, these effects largely remain under the radar in the accuracy-fairness evaluation framework that is usually applied. This paper challenges the prevailing metrics for assessing bias mitigation techniques, arguing that they do not take into account the changes within-groups and that the resulting prediction labels fall short of reflecting real-world scenarios. We propose a para",
    "link": "http://arxiv.org/abs/2401.13391",
    "context": "Title: Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics. (arXiv:2401.13391v1 [cs.LG])\nAbstract: Artificial Intelligence (AI) finds widespread applications across various domains, sparking concerns about fairness in its deployment. While fairness in AI remains a central concern, the prevailing discourse often emphasizes outcome-based metrics without a nuanced consideration of the differential impacts within subgroups. Bias mitigation techniques do not only affect the ranking of pairs of instances across sensitive groups, but often also significantly affect the ranking of instances within these groups. Such changes are hard to explain and raise concerns regarding the validity of the intervention. Unfortunately, these effects largely remain under the radar in the accuracy-fairness evaluation framework that is usually applied. This paper challenges the prevailing metrics for assessing bias mitigation techniques, arguing that they do not take into account the changes within-groups and that the resulting prediction labels fall short of reflecting real-world scenarios. We propose a para",
    "path": "papers/24/01/2401.13391.json",
    "total_tokens": 866,
    "translated_title": "超越准确性和公平性：停止仅根据群组间指标评估偏见缓解方法",
    "translated_abstract": "人工智能在各个领域得到广泛应用，引发了对其公平性的关注。虽然公平性在人工智能中仍然是一个核心关注点，但目前的讨论往往强调基于结果的指标，而没有对子群体中的差异影响进行细致考虑。偏见缓解技术不仅影响敏感群组之间的实例排序，而且通常还显著影响这些群组内实例的排序。这些变化难以解释，并引起了对干预的有效性的担忧。不幸的是，这些效应在通常应用的准确性-公平性评估框架中往往被忽视。本文对评估偏见缓解技术的流行度指标提出了质疑，认为它们没有考虑到群组内的变化，并且导致的预测标签不能完全反映现实情况。",
    "tldr": "本文对评估偏见缓解技术的流行度指标提出质疑，认为它们没有考虑到群组内的变化，并且导致的预测标签不能完全反映现实情况。",
    "en_tdlr": "This paper challenges the prevailing metrics for assessing bias mitigation techniques, arguing that they do not take into account the changes within-groups and that the resulting prediction labels fall short of reflecting real-world scenarios."
}