{
    "title": "Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models. (arXiv:2401.10440v1 [cs.CL])",
    "abstract": "Despite their popularity in non-English NLP, multilingual language models often underperform monolingual ones due to inter-language competition for model parameters. We propose Cross-lingual Expert Language Models (X-ELM), which mitigate this competition by independently training language models on subsets of the multilingual corpus. This process specializes X-ELMs to different languages while remaining effective as a multilingual ensemble. Our experiments show that when given the same compute budget, X-ELM outperforms jointly trained multilingual models across all considered languages and that these gains transfer to downstream tasks. X-ELM provides additional benefits over performance improvements: new experts can be iteratively added, adapting X-ELM to new languages without catastrophic forgetting. Furthermore, training is asynchronous, reducing the hardware requirements for multilingual training and democratizing multilingual modeling.",
    "link": "http://arxiv.org/abs/2401.10440",
    "context": "Title: Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models. (arXiv:2401.10440v1 [cs.CL])\nAbstract: Despite their popularity in non-English NLP, multilingual language models often underperform monolingual ones due to inter-language competition for model parameters. We propose Cross-lingual Expert Language Models (X-ELM), which mitigate this competition by independently training language models on subsets of the multilingual corpus. This process specializes X-ELMs to different languages while remaining effective as a multilingual ensemble. Our experiments show that when given the same compute budget, X-ELM outperforms jointly trained multilingual models across all considered languages and that these gains transfer to downstream tasks. X-ELM provides additional benefits over performance improvements: new experts can be iteratively added, adapting X-ELM to new languages without catastrophic forgetting. Furthermore, training is asynchronous, reducing the hardware requirements for multilingual training and democratizing multilingual modeling.",
    "path": "papers/24/01/2401.10440.json",
    "total_tokens": 954,
    "translated_title": "用跨语言专家语言模型突破多语言环境的难题",
    "translated_abstract": "尽管多语言语言模型在非英语自然语言处理（NLP）中很受欢迎，但由于模型参数之间的跨语言竞争，它们往往表现不及单语语言模型。我们提出了跨语言专家语言模型（X-ELM），通过对多语言语料库的子集进行独立训练，来减轻这种竞争。这个过程使X-ELM针对不同语言进行专门训练，同时作为一个多语言集合保持有效。我们的实验表明，在给定相同计算预算的情况下，X-ELM在所有考虑的语言上优于联合训练的多语言模型，并且这些收益可以转移到下游任务中。X-ELM在性能改进方面提供了额外的好处：可以迭代地添加新的专家，适应新语言而不会产生灾难性的遗忘。此外，训练是异步进行的，减少了多语言训练的硬件要求，实现多语言建模的民主化。",
    "tldr": "本论文提出了一种称为X-ELM的跨语言专家语言模型，通过独立训练语言模型的子集来减轻多语言竞争，为多语言处理带来提升。实验表明，X-ELM在各种语言上优于联合训练的多语言模型，并且可以适应新语言的迭代添加。",
    "en_tdlr": "This paper proposes a cross-lingual expert language model, X-ELM, which mitigates inter-language competition by independently training language models on subsets of the multilingual corpus. Experimental results show that X-ELM outperforms jointly trained multilingual models and can adapt to new languages iteratively."
}