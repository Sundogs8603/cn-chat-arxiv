{
    "title": "Anisotropy Is Inherent to Self-Attention in Transformers. (arXiv:2401.12143v2 [cs.CL] UPDATED)",
    "abstract": "The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations suggest that anisotropy is actually inherent to Transformers-based models.",
    "link": "http://arxiv.org/abs/2401.12143",
    "context": "Title: Anisotropy Is Inherent to Self-Attention in Transformers. (arXiv:2401.12143v2 [cs.CL] UPDATED)\nAbstract: The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations suggest that anisotropy is actually inherent to Transformers-based models.",
    "path": "papers/24/01/2401.12143.json",
    "total_tokens": 743,
    "translated_title": "自注意力在Transformer中是固有的各向异性",
    "translated_abstract": "表示退化问题是基于Transformer的自监督学习方法中普遍观察到的现象。在自然语言处理中，它以各向异性的形式出现，这是隐藏表示的独特属性，使其在角度距离（余弦相似度）方面意外地彼此靠近。最近的一些研究表明，各向异性是在长尾分布的令牌上优化交叉熵损失的结果。我们在本文中展示，各向异性也可以在具有特定目标的语言模型中经验观察到，这些模型不应直接受到相同后果的影响。我们还展示了各向异性问题扩展到在其他模态上训练的Transformer。我们的观察表明，各向异性实际上是Transformer-based模型固有的。",
    "tldr": "本文发现自注意力在Transformer中是固有的各向异性现象，该现象在各种任务和数据集上都普遍存在，不仅限于长尾分布和语言模型。"
}