{
    "title": "Falcon: Fair Active Learning using Multi-armed Bandits. (arXiv:2401.12722v1 [cs.LG])",
    "abstract": "Biased data can lead to unfair machine learning models, highlighting the importance of embedding fairness at the beginning of data analysis, particularly during dataset curation and labeling. In response, we propose Falcon, a scalable fair active learning framework. Falcon adopts a data-centric approach that improves machine learning model fairness via strategic sample selection. Given a user-specified group fairness measure, Falcon identifies samples from \"target groups\" (e.g., (attribute=female, label=positive)) that are the most informative for improving fairness. However, a challenge arises since these target groups are defined using ground truth labels that are not available during sample selection. To handle this, we propose a novel trial-and-error method, where we postpone using a sample if the predicted label is different from the expected one and falls outside the target group. We also observe the trade-off that selecting more informative samples results in higher likelihood o",
    "link": "http://arxiv.org/abs/2401.12722",
    "context": "Title: Falcon: Fair Active Learning using Multi-armed Bandits. (arXiv:2401.12722v1 [cs.LG])\nAbstract: Biased data can lead to unfair machine learning models, highlighting the importance of embedding fairness at the beginning of data analysis, particularly during dataset curation and labeling. In response, we propose Falcon, a scalable fair active learning framework. Falcon adopts a data-centric approach that improves machine learning model fairness via strategic sample selection. Given a user-specified group fairness measure, Falcon identifies samples from \"target groups\" (e.g., (attribute=female, label=positive)) that are the most informative for improving fairness. However, a challenge arises since these target groups are defined using ground truth labels that are not available during sample selection. To handle this, we propose a novel trial-and-error method, where we postpone using a sample if the predicted label is different from the expected one and falls outside the target group. We also observe the trade-off that selecting more informative samples results in higher likelihood o",
    "path": "papers/24/01/2401.12722.json",
    "total_tokens": 935,
    "translated_title": "Falcon: 使用多臂赌博机进行公平主动学习",
    "translated_abstract": "偏倚数据可能导致不公平的机器学习模型，强调在数据分析的开始阶段嵌入公平性的重要性，特别是在数据集的筛选和标定过程中。为此，我们提出了一种可扩展的公平主动学习框架Falcon。Falcon采用了一种以数据为中心的方法，通过策略性样本选择来改善机器学习模型的公平性。给定用户指定的群体公平度量，Falcon确定了对提高公平性最有信息量的“目标群体”样本（例如（属性=女性，标签=正面））。然而，由于在样本选择过程中不可用ground truth标签来定义这些目标群体，出现了挑战。为了解决这个问题，我们提出了一种新颖的试错方法，在预测标签与期望标签不同时并落在目标群体之外时，我们推迟使用该样本。我们还观察到这样做会产生权衡，选择更有信息量的样本会增加样本进入目标群体之外的可能性。",
    "tldr": "Falcon是一个使用多臂赌博机的公平主动学习框架，通过策略性样本选择来改善机器学习模型的公平性。它通过识别对于提高公平性最具信息量的“目标群体”样本，并采用一种试错方法来解决样本选择中没有ground truth标签的挑战。"
}