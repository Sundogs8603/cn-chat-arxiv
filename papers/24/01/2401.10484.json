{
    "title": "Enhancing Scalability in Recommender Systems through Lottery Ticket Hypothesis and Knowledge Distillation-based Neural Network Pruning. (arXiv:2401.10484v1 [cs.IR])",
    "abstract": "This study introduces an innovative approach aimed at the efficient pruning of neural networks, with a particular focus on their deployment on edge devices. Our method involves the integration of the Lottery Ticket Hypothesis (LTH) with the Knowledge Distillation (KD) framework, resulting in the formulation of three distinct pruning models. These models have been developed to address scalability issue in recommender systems, whereby the complexities of deep learning models have hindered their practical deployment. With judicious application of the pruning techniques, we effectively curtail the power consumption and model dimensions without compromising on accuracy. Empirical evaluation has been performed using two real world datasets from diverse domains against two baselines. Gratifyingly, our approaches yielded a GPU computation-power reduction of up to 66.67%. Notably, our study contributes to the field of recommendation system by pioneering the application of LTH and KD.",
    "link": "http://arxiv.org/abs/2401.10484",
    "context": "Title: Enhancing Scalability in Recommender Systems through Lottery Ticket Hypothesis and Knowledge Distillation-based Neural Network Pruning. (arXiv:2401.10484v1 [cs.IR])\nAbstract: This study introduces an innovative approach aimed at the efficient pruning of neural networks, with a particular focus on their deployment on edge devices. Our method involves the integration of the Lottery Ticket Hypothesis (LTH) with the Knowledge Distillation (KD) framework, resulting in the formulation of three distinct pruning models. These models have been developed to address scalability issue in recommender systems, whereby the complexities of deep learning models have hindered their practical deployment. With judicious application of the pruning techniques, we effectively curtail the power consumption and model dimensions without compromising on accuracy. Empirical evaluation has been performed using two real world datasets from diverse domains against two baselines. Gratifyingly, our approaches yielded a GPU computation-power reduction of up to 66.67%. Notably, our study contributes to the field of recommendation system by pioneering the application of LTH and KD.",
    "path": "papers/24/01/2401.10484.json",
    "total_tokens": 1084,
    "translated_title": "基于彩票票据假设和知识蒸馏的神经网络剪枝技术增强推荐系统的可扩展性",
    "translated_abstract": "本研究介绍了一种创新的方法，旨在高效地剪枝神经网络，特别关注它们在边缘设备上的部署。我们的方法将彩票票据假设（LTH）与知识蒸馏（KD）框架相结合，形成了三种不同的剪枝模型。这些模型旨在解决推荐系统中的可扩展性问题，其中深度学习模型的复杂性妨碍了它们的实际部署。通过巧妙应用剪枝技术，我们有效地降低了功耗和模型尺寸，同时不影响准确性。我们使用两个真实世界数据集对两个基准进行了实证评估。令人满意的是，我们的方法在GPU计算功率上实现了高达66.67%的降低。值得注意的是，我们的研究通过首次应用LTH和KD技术，对推荐系统领域做出了贡献。",
    "tldr": "本研究通过将彩票票据假设和知识蒸馏框架相结合，提出了一种剪枝神经网络的创新方法，以解决推荐系统在边缘设备上的可扩展性问题。经实验证明，该方法能够有效地降低功耗和模型尺寸，并实现高达66.67%的GPU计算功率减少。此外，本研究还首次应用了彩票票据假设和知识蒸馏技术于推荐系统领域，对该领域作出了重要贡献。",
    "en_tdlr": "This study proposes an innovative method for pruning neural networks in recommender systems, aiming to enhance scalability on edge devices. By integrating the Lottery Ticket Hypothesis (LTH) with the Knowledge Distillation (KD) framework, the method effectively reduces power consumption and model dimensions while maintaining high accuracy. The evaluation shows a significant reduction of GPU computation power by up to 66.67%. Additionally, the study contributes to the recommendation system field by pioneering the application of LTH and KD techniques."
}