{
    "title": "ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence Labeling Tasks. (arXiv:2401.16589v1 [cs.CL])",
    "abstract": "Prompt-based methods have been successfully applied to multilingual pretrained language models for zero-shot cross-lingual understanding. However, most previous studies primarily focused on sentence-level classification tasks, and only a few considered token-level labeling tasks such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose Token-Level Prompt Decomposition (ToPro), which facilitates the prompt-based method for token-level sequence labeling tasks. The ToPro method decomposes an input sentence into single tokens and applies one prompt template to each token. Our experiments on multilingual NER and POS tagging datasets demonstrate that ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning in zero-shot cross-lingual transfer, especially for languages that are typologically different from the source language English. Our method also attains state-of-the-art performance when employed with the mT5 model. Besides, our exp",
    "link": "http://arxiv.org/abs/2401.16589",
    "context": "Title: ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence Labeling Tasks. (arXiv:2401.16589v1 [cs.CL])\nAbstract: Prompt-based methods have been successfully applied to multilingual pretrained language models for zero-shot cross-lingual understanding. However, most previous studies primarily focused on sentence-level classification tasks, and only a few considered token-level labeling tasks such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose Token-Level Prompt Decomposition (ToPro), which facilitates the prompt-based method for token-level sequence labeling tasks. The ToPro method decomposes an input sentence into single tokens and applies one prompt template to each token. Our experiments on multilingual NER and POS tagging datasets demonstrate that ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning in zero-shot cross-lingual transfer, especially for languages that are typologically different from the source language English. Our method also attains state-of-the-art performance when employed with the mT5 model. Besides, our exp",
    "path": "papers/24/01/2401.16589.json",
    "total_tokens": 906,
    "translated_title": "ToPro: 跨语言序列标注任务的基于Token级别的提示分解",
    "translated_abstract": "基于提示的方法已成功应用于多语言预训练语言模型的零样本跨语言理解。然而，大多数之前的研究主要关注句子级别的分类任务，只有少数考虑到了词汇级别的标注任务，如命名实体识别（NER）和词性标注（POS）。在本文中，我们提出了基于Token级别的提示分解（ToPro），该方法可用于词汇级别的序列标注任务。ToPro方法将输入句子分解为单个词汇，并对每个词汇应用一个提示模板。我们在多语言NER和POS标注数据集上的实验表明，基于ToPro的微调在零样本跨语言转移中优于Vanilla微调和Prompt-Tuning，尤其对于结构与源语言英语不同的语言。我们的方法在使用mT5模型时也获得了最先进的性能。",
    "tldr": "这项研究提出了一种名为ToPro的方法，用于跨语言序列标注任务。该方法通过将输入句子分解为单个词汇并应用提示模板，在零样本跨语言转移中实现了优于其他微调方法的性能，并在结构不同的语言上取得了最先进的结果。",
    "en_tdlr": "This study proposes a method called ToPro for cross-lingual sequence labeling tasks. By decomposing input sentences into individual tokens and applying prompt templates, the ToPro method achieves superior performance in zero-shot cross-lingual transfer compared to other fine-tuning methods and achieves state-of-the-art results on languages with different structures."
}