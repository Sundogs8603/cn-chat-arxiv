{
    "title": "A Precise Characterization of SGD Stability Using Loss Surface Geometry. (arXiv:2401.12332v1 [cs.LG])",
    "abstract": "Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are",
    "link": "http://arxiv.org/abs/2401.12332",
    "context": "Title: A Precise Characterization of SGD Stability Using Loss Surface Geometry. (arXiv:2401.12332v1 [cs.LG])\nAbstract: Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are",
    "path": "papers/24/01/2401.12332.json",
    "total_tokens": 836,
    "translated_title": "利用损失面几何进行 SGD 稳定性的精确刻画",
    "translated_abstract": "随机梯度下降（SGD）是一种基于经验实证成功的优化算法，但其理论理解相对有限。最近的研究揭示了SGD实际有效性的一个关键因素：它引发的隐式正则化。一些研究探索了在稳定点附近的SGD的线性稳定性属性，作为过参数化神经网络中尖锐度和泛化误差的预测代理（Wu等人，2022；Jastrzebski等人，2019；Cohen等人，2021）。在本文中，我们深入研究了线性稳定性与尖锐度之间的关系。具体而言，我们详细划定了线性稳定性的必要和充分条件，这取决于SGD的超参数和最优解处的尖锐度。为此，我们引入了损失Hessian的新型一致性度量，它包含了损失函数的相关几何属性。",
    "tldr": "本文精确刻画了利用损失面几何分析 SGD 稳定性的关键条件，为理解其实际有效性提供了新的方法。"
}