{
    "title": "Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning. (arXiv:2401.15273v1 [cs.LG])",
    "abstract": "Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of FRL algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' MDPs, and continuous state-action spaces. Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In response, we introduce FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-ti",
    "link": "http://arxiv.org/abs/2401.15273",
    "context": "Title: Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning. (arXiv:2401.15273v1 [cs.LG])\nAbstract: Federated reinforcement learning (FRL) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of FRL algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' MDPs, and continuous state-action spaces. Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In response, we introduce FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-ti",
    "path": "papers/24/01/2401.15273.json",
    "total_tokens": 897,
    "translated_title": "有限时间分析的政策异构联邦强化学习",
    "translated_abstract": "联邦强化学习（FRL）是一种利用不同代理的信息来降低强化学习任务样本复杂性的前景光明的范式。然而，当每个代理与一个可能不同的环境进行交互时，关于FRL算法的非渐进性能几乎没有理论上的了解。这种结果的缺乏可以归因于各种技术挑战及其复杂的相互作用：马尔可夫取样、线性函数逼近、多个本地更新以节省通信、代理的MDP的奖励函数和转移核的异质性以及连续的状态-动作空间。此外，在政策上的设置中，行为政策随时间变化，进一步使分析复杂化。针对这些挑战，我们引入了FedSARSA，一种新颖的带有线性函数逼近的联邦政策强化学习方案，以应对这些挑战并提供全面的有限时间分析。",
    "tldr": "本论文介绍了一种新颖的联邦政策强化学习方案（FedSARSA），利用线性函数逼近来解决马尔可夫取样、多个本地更新等技术挑战，从而提供了关于有限时间性能的全面分析。",
    "en_tdlr": "This paper introduces a novel federated on-policy reinforcement learning scheme (FedSARSA) that addresses technical challenges such as Markovian sampling and multiple local updates using linear function approximation, providing a comprehensive analysis of finite-time performance."
}