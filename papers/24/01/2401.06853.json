{
    "title": "Large Language Models Can Learn Temporal Reasoning",
    "abstract": "arXiv:2401.06853v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal expressions and intricate contextual details. In this paper, we propose TG-LLM, a new framework towards language-based TR. To be specific, we first teach LLM to translate the context into a temporal graph (TG). A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for fine-tuning on this graph translation task. We confirm in experiments that the capability of TG extraction learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we guide LLM to perform symbolic reasoning over the TG via Chain of Thoughts (CoTs) bootstrappin",
    "link": "https://arxiv.org/abs/2401.06853",
    "context": "Title: Large Language Models Can Learn Temporal Reasoning\nAbstract: arXiv:2401.06853v2 Announce Type: replace  Abstract: While large language models (LLMs) have demonstrated remarkable reasoning capabilities, they are not without their flaws and inaccuracies. Recent studies have introduced various methods to mitigate these limitations. Temporal reasoning (TR), in particular, presents a significant challenge for LLMs due to its reliance on diverse temporal expressions and intricate contextual details. In this paper, we propose TG-LLM, a new framework towards language-based TR. To be specific, we first teach LLM to translate the context into a temporal graph (TG). A synthetic dataset, which is fully controllable and requires minimal supervision, is constructed for fine-tuning on this graph translation task. We confirm in experiments that the capability of TG extraction learned on our dataset can be transferred to other TR tasks and benchmarks. On top of that, we guide LLM to perform symbolic reasoning over the TG via Chain of Thoughts (CoTs) bootstrappin",
    "path": "papers/24/01/2401.06853.json",
    "total_tokens": 789,
    "translated_title": "大型语言模型可以学习时间推理",
    "translated_abstract": "尽管大型语言模型（LLMs）展示了出色的推理能力，但它们并非没有缺陷和不准确之处。最近的研究介绍了各种方法来减轻这些局限性。特别是，时间推理（TR）对LLMs提出了重大挑战，因为它依赖于多样的时间表达和复杂的上下文细节。本文中，我们提出了TG-LLM，一个致力于基于语言的时间推理的新框架。具体而言，我们首先教导LLM将上下文翻译成时间图（TG）。我们构建了一个全可控且需要最少监督的合成数据集，用于在这个图翻译任务上进行微调。我们在实验证实，学习在我们数据集上的TG提取能力可以转移到其他TR任务和基准测试上。除此之外，我们使用CoTs引导LLM通过TG进行符号推理。",
    "tldr": "本文提出了一个新的TG-LLM框架，以语言为基础进行时间推理，通过教导LLM将上下文翻译成时间图，并使用CoTs指导LLM进行符号推理。"
}