{
    "title": "APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding. (arXiv:2401.06761v1 [cs.CL])",
    "abstract": "The massive adoption of large language models (LLMs) demands efficient deployment strategies. However, the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving. In this work, we introduce a parallel auto-regressive generation method. By instruct-tuning on general domain data that contains hierarchical structures, we enable LLMs to independently plan their generation process and perform auto-parallel auto-regressive (APAR) generation, significantly reducing the number of generation steps. APAR alone can achieve up to 2x speed-up, and when combined with speculative decoding, the speed-up can reach up to 4x. In addition, APAR reduces the key-value cache consumption and attention computation during generation. This leads to a throughput increase of 20-70% and a latency reduce of 20-35% in high-throughput scenarios, compared to state-of-the-art serving frameworks.",
    "link": "http://arxiv.org/abs/2401.06761",
    "context": "Title: APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding. (arXiv:2401.06761v1 [cs.CL])\nAbstract: The massive adoption of large language models (LLMs) demands efficient deployment strategies. However, the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving. In this work, we introduce a parallel auto-regressive generation method. By instruct-tuning on general domain data that contains hierarchical structures, we enable LLMs to independently plan their generation process and perform auto-parallel auto-regressive (APAR) generation, significantly reducing the number of generation steps. APAR alone can achieve up to 2x speed-up, and when combined with speculative decoding, the speed-up can reach up to 4x. In addition, APAR reduces the key-value cache consumption and attention computation during generation. This leads to a throughput increase of 20-70% and a latency reduce of 20-35% in high-throughput scenarios, compared to state-of-the-art serving frameworks.",
    "path": "papers/24/01/2401.06761.json",
    "total_tokens": 932,
    "translated_title": "APAR: LLMs可以进行自动并行自回归解码",
    "translated_abstract": "大规模语言模型（LLM）的广泛应用要求有效的部署策略。然而，大多数LLM生成文本的基本自回归解码过程对于实现高效服务提出了挑战。在这项工作中，我们介绍了一种并行自回归生成方法。通过以包含层次结构的通用领域数据为指导进行调整，我们使LLM能够独立规划其生成过程，并进行自动化并行自回归（APAR）生成，显著减少了生成步骤的数量。仅APAR就可以实现多达2倍的加速，而当与推测解码结合时，加速度可以达到4倍。此外，APAR在生成过程中减少了键值缓存消耗和注意力计算。与最先进的服务框架相比，在高吞吐量场景中，这导致吞吐量增加了20-70％，延迟降低了20-35％。",
    "tldr": "这项工作介绍了一种名为APAR的并行自回归生成方法，通过对数据进行调整，使得LLMs能够独立规划生成过程以及执行自动并行自回归（APAR）生成，从而显著减少了生成步骤的数量，并在高吞吐量场景中实现了吞吐量增加和延迟降低。"
}