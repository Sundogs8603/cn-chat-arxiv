{
    "title": "MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance. (arXiv:2401.02906v1 [cs.CR])",
    "abstract": "The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. We delve into the novel challenge of defending MLLMs against such attacks. We discovered that images act as a \"foreign language\" that is not considered during alignment, which can make MLLMs prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover the possible scenarios. This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play ",
    "link": "http://arxiv.org/abs/2401.02906",
    "context": "Title: MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance. (arXiv:2401.02906v1 [cs.CR])\nAbstract: The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. We delve into the novel challenge of defending MLLMs against such attacks. We discovered that images act as a \"foreign language\" that is not considered during alignment, which can make MLLMs prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover the possible scenarios. This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play ",
    "path": "papers/24/01/2401.02906.json",
    "total_tokens": 934,
    "translated_title": "MLLM-Protektor: 确保MLLM的安全性而不影响性能",
    "translated_abstract": "多模态大型语言模型(MLLM)的部署带来了一个独特的脆弱性：对视觉输入的恶意攻击易受攻击。我们探讨了保护MLLM免受此类攻击的新挑战。我们发现，图像作为一种“外语”在对齐过程中没有被考虑到，这可能使MLLM更容易产生有害的响应。不幸的是，与文本中所考虑的离散标记不同，图像信号的连续性质在对齐过程中带来了重大挑战，从而使覆盖可能情景变得困难。这种脆弱性加剧了一个事实，即开源的MLLM主要在有限的图像-文本对上进行了微调，远远小于广泛的文本预训练语料库，这使得MLLM在明确对齐调整过程中更容易遗忘其原始能力。为了应对这些挑战，我们介绍了MLLM-Protektor，一个即插即用的",
    "tldr": "MLLM-Protektor是一个即插即用的方法，用于保护多模态大型语言模型免受恶意攻击。通过解决图像与文本对齐的挑战，该方法提供了对MLLM的有效保护，防止其产生有害响应。",
    "en_tdlr": "MLLM-Protector is a plug-and-play approach for safeguarding multimodal large language models from malicious attacks. By addressing the challenge of aligning images with text, this method offers effective protection for MLLMs, preventing them from generating harmful responses."
}