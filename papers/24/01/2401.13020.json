{
    "title": "A Safe Reinforcement Learning Algorithm for Supervisory Control of Power Plants. (arXiv:2401.13020v1 [cs.SY])",
    "abstract": "Traditional control theory-based methods require tailored engineering for each system and constant fine-tuning. In power plant control, one often needs to obtain a precise representation of the system dynamics and carefully design the control scheme accordingly. Model-free Reinforcement learning (RL) has emerged as a promising solution for control tasks due to its ability to learn from trial-and-error interactions with the environment. It eliminates the need for explicitly modeling the environment's dynamics, which is potentially inaccurate. However, the direct imposition of state constraints in power plant control raises challenges for standard RL methods. To address this, we propose a chance-constrained RL algorithm based on Proximal Policy Optimization for supervisory control. Our method employs Lagrangian relaxation to convert the constrained optimization problem into an unconstrained objective, where trainable Lagrange multipliers enforce the state constraints. Our approach achiev",
    "link": "http://arxiv.org/abs/2401.13020",
    "context": "Title: A Safe Reinforcement Learning Algorithm for Supervisory Control of Power Plants. (arXiv:2401.13020v1 [cs.SY])\nAbstract: Traditional control theory-based methods require tailored engineering for each system and constant fine-tuning. In power plant control, one often needs to obtain a precise representation of the system dynamics and carefully design the control scheme accordingly. Model-free Reinforcement learning (RL) has emerged as a promising solution for control tasks due to its ability to learn from trial-and-error interactions with the environment. It eliminates the need for explicitly modeling the environment's dynamics, which is potentially inaccurate. However, the direct imposition of state constraints in power plant control raises challenges for standard RL methods. To address this, we propose a chance-constrained RL algorithm based on Proximal Policy Optimization for supervisory control. Our method employs Lagrangian relaxation to convert the constrained optimization problem into an unconstrained objective, where trainable Lagrange multipliers enforce the state constraints. Our approach achiev",
    "path": "papers/24/01/2401.13020.json",
    "total_tokens": 872,
    "translated_title": "电厂监控的安全强化学习算法",
    "translated_abstract": "传统的控制理论方法需要为每个系统进行定制化工程和不断的微调。在电厂控制中，经常需要精确地获取系统动力学的表示并相应地设计控制方案。无模型强化学习（RL）由于其能够通过与环境的试错交互学习而成为控制任务的有希望解决方案。它消除了对明确建模环境动态的需求，这可能是不准确的。然而，在电厂控制中直接施加状态约束对标准RL方法提出了挑战。为了解决这个问题，我们提出了一种基于近端策略优化的机会约束RL算法，用于监控控制。我们的方法采用拉格朗日松弛，将约束优化问题转化为无约束的目标函数，其中可训练的拉格朗日乘子强制执行状态约束。",
    "tldr": "这篇论文提出了一种基于Proximal Policy Optimization的机会约束强化学习算法，用于电厂监控的监督控制。通过使用Lagrangian relaxation，我们将约束优化问题转化为无约束的目标函数，并通过可训练的拉格朗日乘子实施状态约束。",
    "en_tdlr": "This paper proposes a chance-constrained reinforcement learning algorithm based on Proximal Policy Optimization for supervisory control in power plants. By using Lagrangian relaxation, the constrained optimization problem is converted into an unconstrained objective, and trainable Lagrange multipliers enforce the state constraints."
}