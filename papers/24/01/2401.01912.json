{
    "title": "Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network. (arXiv:2401.01912v1 [cs.CV])",
    "abstract": "Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the pe",
    "link": "http://arxiv.org/abs/2401.01912",
    "context": "Title: Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network. (arXiv:2401.01912v1 [cs.CV])\nAbstract: Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the pe",
    "path": "papers/24/01/2401.01912.json",
    "total_tokens": 966,
    "translated_title": "缩小时间步长：实现具有脉冲神经网络的低延迟神经形态对象识别",
    "translated_abstract": "脉冲神经网络（SNNs）的神经形态对象识别是低功耗神经形态计算的基石。然而，现有的SNNs在识别神经形态对象时存在显著的延迟问题，需要使用10到40个或更多的时间步长。在低延迟情况下，现有SNNs的性能严重降低。在本研究中，我们提出了缩小SNN（SSNN）来实现低延迟的神经形态对象识别，而不降低性能。具体而言，我们通过将SNNs分成多个阶段，逐步缩小时间步长来减少推断延迟。在时间步长缩小过程中，时间变换器平滑地转换时间尺度并最大程度地保留信息。此外，我们在训练过程中向SNN添加多个早期分类器，以减轻替代梯度和真实梯度之间的不匹配，以及梯度消失/爆炸问题，从而消除了性能下降的问题。",
    "tldr": "本研究提出了一种缩小时间步长的脉冲神经网络（SSNN），通过将SNN分成多个阶段，逐步缩小时间步长，实现了低延迟的神经形态对象识别，并通过添加早期分类器解决了性能下降的问题。",
    "en_tdlr": "This paper proposes a Shrinking Spiking Neural Network (SSNN) that achieves low-latency neuromorphic object recognition by dividing the network into multiple stages with progressively shrinking time steps, and mitigating performance degradation through the addition of early classifiers."
}