{
    "title": "Efficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR. (arXiv:2401.08992v1 [cs.CL])",
    "abstract": "The end-to-end ASR model is often desired in the streaming multilingual scenario since it is easier to deploy and can benefit from pre-trained speech models such as powerful foundation models. Meanwhile, the heterogeneous nature and imbalanced data abundance of different languages may cause performance degradation, leading to asynchronous peak performance for different languages during training, especially on tail ones. Sometimes even the data itself may become unavailable as a result of the enhanced privacy protection. Existing work tend to significantly increase the model size or learn language-specific decoders to accommodate each language separately. In this study, we explore simple yet effective Language-Dependent Adapter (LDA) finetuning under a cascaded Conformer transducer framework enhanced by teacher pseudo-labeling for tail languages in the streaming multilingual ASR. The adapter only accounts for 0.4% of the full model per language. It is plugged into the frozen foundation ",
    "link": "http://arxiv.org/abs/2401.08992",
    "context": "Title: Efficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR. (arXiv:2401.08992v1 [cs.CL])\nAbstract: The end-to-end ASR model is often desired in the streaming multilingual scenario since it is easier to deploy and can benefit from pre-trained speech models such as powerful foundation models. Meanwhile, the heterogeneous nature and imbalanced data abundance of different languages may cause performance degradation, leading to asynchronous peak performance for different languages during training, especially on tail ones. Sometimes even the data itself may become unavailable as a result of the enhanced privacy protection. Existing work tend to significantly increase the model size or learn language-specific decoders to accommodate each language separately. In this study, we explore simple yet effective Language-Dependent Adapter (LDA) finetuning under a cascaded Conformer transducer framework enhanced by teacher pseudo-labeling for tail languages in the streaming multilingual ASR. The adapter only accounts for 0.4% of the full model per language. It is plugged into the frozen foundation ",
    "path": "papers/24/01/2401.08992.json",
    "total_tokens": 980,
    "translated_title": "流式多语言ASR中针对边缘语种的高效适配器微调",
    "translated_abstract": "在流式多语言情景中，人们通常希望使用端到端ASR模型，因为这样更容易部署，并且可以从预训练的语音模型中受益，例如强大的基础模型。与此同时，不同语言的异质性和数据丰富度的不平衡可能导致性能下降，从而在训练过程中不同语言的性能出现异步峰值，尤其是对于边缘语种。有时，数据本身甚至可能因为加强的隐私保护而不可用。现有的方法往往倾向于显著增加模型大小或学习语言特定的解码器来单独适应每种语言。本研究中，我们探索了一种简单且有效的Language-Dependent Adapter (LDA)微调方法，该方法运用了级联的Conformer转录器框架，通过教师伪标签增强了对流式多语言ASR中的边缘语种的支持。适配器仅占每种语言的完整模型的0.4%。它被插入到冻结的基础模型中。",
    "tldr": "本研究通过在流式多语言ASR中采用简单而有效的适配器微调方法，提供了对边缘语种的支持。适配器仅占每种语言的模型的0.4%。该方法在级联的Conformer转录器框架下，通过教师伪标签增强了模型性能。",
    "en_tdlr": "This study provides support for tail languages in streaming multilingual ASR through the use of a simple yet effective adapter finetuning method. The adapter only accounts for 0.4% of each language's model. The method enhances model performance through teacher pseudo-labeling under a cascaded Conformer transducer framework."
}