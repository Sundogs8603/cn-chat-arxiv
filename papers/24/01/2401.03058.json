{
    "title": "Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate. (arXiv:2401.03058v1 [math.OC])",
    "abstract": "Second-order optimization methods, such as cubic regularized Newton methods, are known for their rapid convergence rates; nevertheless, they become impractical in high-dimensional problems due to their substantial memory requirements and computational costs. One promising approach is to execute second-order updates within a lower-dimensional subspace, giving rise to subspace second-order methods. However, the majority of existing subspace second-order methods randomly select subspaces, consequently resulting in slower convergence rates depending on the problem's dimension $d$. In this paper, we introduce a novel subspace cubic regularized Newton method that achieves a dimension-independent global convergence rate of ${O}\\left(\\frac{1}{mk}+\\frac{1}{k^2}\\right)$ for solving convex optimization problems. Here, $m$ represents the subspace dimension, which can be significantly smaller than $d$. Instead of adopting a random subspace, our primary innovation involves performing the cubic regul",
    "link": "http://arxiv.org/abs/2401.03058",
    "context": "Title: Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate. (arXiv:2401.03058v1 [math.OC])\nAbstract: Second-order optimization methods, such as cubic regularized Newton methods, are known for their rapid convergence rates; nevertheless, they become impractical in high-dimensional problems due to their substantial memory requirements and computational costs. One promising approach is to execute second-order updates within a lower-dimensional subspace, giving rise to subspace second-order methods. However, the majority of existing subspace second-order methods randomly select subspaces, consequently resulting in slower convergence rates depending on the problem's dimension $d$. In this paper, we introduce a novel subspace cubic regularized Newton method that achieves a dimension-independent global convergence rate of ${O}\\left(\\frac{1}{mk}+\\frac{1}{k^2}\\right)$ for solving convex optimization problems. Here, $m$ represents the subspace dimension, which can be significantly smaller than $d$. Instead of adopting a random subspace, our primary innovation involves performing the cubic regul",
    "path": "papers/24/01/2401.03058.json",
    "total_tokens": 913,
    "translated_title": "Krylov立方正则化牛顿法：具有无维收敛速度的子空间二阶方法",
    "translated_abstract": "二阶优化算法，如立方正则化牛顿法，以其快速收敛速度而闻名；然而，在高维问题中，它们变得不实用，因为需要大量的内存和计算成本。一个有前景的方法是在低维子空间中执行二阶更新，从而产生子空间二阶方法。然而，现有的大多数子空间二阶方法随机选择子空间，因此收敛速度较慢，这取决于问题的维度d。在本文中，我们引入了一种新颖的子空间立方正则化牛顿方法，用于解决凸优化问题，其达到了一个维度无关的全局收敛速度，为${O}\\left(\\frac{1}{mk}+\\frac{1}{k^2}\\right)$。这里，m表示子空间维度，可以显著小于d。我们的主要创新不是采用随机子空间，而是进行立方正则化...",
    "tldr": "本论文提出了一种新的子空间立方正则化牛顿方法，可以在解决凸优化问题时实现无维度相关的全局收敛速度，通过在低维子空间上进行二阶更新，克服了高维问题中内存需求和计算成本大的问题。",
    "en_tdlr": "This paper introduces a novel subspace cubic regularized Newton method that achieves a dimension-independent global convergence rate for solving convex optimization problems, overcoming the high memory requirements and computational costs in high-dimensional problems by performing second-order updates within a lower-dimensional subspace."
}