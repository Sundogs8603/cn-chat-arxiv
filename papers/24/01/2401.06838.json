{
    "title": "MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization",
    "abstract": "arXiv:2401.06838v2 Announce Type: replace  Abstract: Though reasoning abilities are considered language-agnostic, existing LLMs exhibit inconsistent reasoning abilities across different languages, e.g., reasoning in the dominant language like English is superior to other languages due to the imbalance of multilingual training data. To enhance reasoning abilities in non-dominant languages, we propose a Multilingual-Alignment-as-Preference Optimization framework (MAPO), aiming to align the reasoning processes in other languages with the dominant language. Specifically, we harness an off-the-shelf translation model for the consistency between answers in non-dominant and dominant languages, which we adopt as the preference for optimization, e.g., Direct Preference Optimization (DPO) or Proximal Policy Optimization (PPO). Experiments show that MAPO stably achieves significant improvements in the multilingual reasoning of various models on all three benchmarks (MSVAMP +16.2%, MGSM +6.1%, and",
    "link": "https://arxiv.org/abs/2401.06838",
    "context": "Title: MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization\nAbstract: arXiv:2401.06838v2 Announce Type: replace  Abstract: Though reasoning abilities are considered language-agnostic, existing LLMs exhibit inconsistent reasoning abilities across different languages, e.g., reasoning in the dominant language like English is superior to other languages due to the imbalance of multilingual training data. To enhance reasoning abilities in non-dominant languages, we propose a Multilingual-Alignment-as-Preference Optimization framework (MAPO), aiming to align the reasoning processes in other languages with the dominant language. Specifically, we harness an off-the-shelf translation model for the consistency between answers in non-dominant and dominant languages, which we adopt as the preference for optimization, e.g., Direct Preference Optimization (DPO) or Proximal Policy Optimization (PPO). Experiments show that MAPO stably achieves significant improvements in the multilingual reasoning of various models on all three benchmarks (MSVAMP +16.2%, MGSM +6.1%, and",
    "path": "papers/24/01/2401.06838.json",
    "total_tokens": 857,
    "translated_title": "MAPO：通过多语言对齐作为偏好优化推进多语言推理",
    "translated_abstract": "尽管推理能力被认为与语言无关，但现有的多语言语言模型在不同语言中表现出的推理能力不一致，例如，对主导语言（如英语）的推理能力优于其他语言，这是由于多语言训练数据的不平衡造成的。为了增强非主导语言的推理能力，我们提出了一个名为多语言对齐作为偏好优化（MAPO）的框架，旨在将其他语言中的推理过程与主导语言对齐。具体来说，我们利用现成的翻译模型来保证非主导语言和主导语言之间答案的一致性，这被采纳为优化的偏好，例如，直接偏好优化（DPO）或临近策略优化（PPO）。实验表明，MAPO在所有三个基准测试中（MSVAMP +16.2％，MGSM +6.1％）稳定地实现了各种模型的多语言推理显著改进。",
    "tldr": "MAPO提出了一个多语言对齐作为偏好优化框架，可以显著提高各种模型在多语言推理中的表现。",
    "en_tdlr": "MAPO introduces a framework of multilingual alignment-as-preference optimization that significantly enhances the performance of various models in multilingual reasoning."
}