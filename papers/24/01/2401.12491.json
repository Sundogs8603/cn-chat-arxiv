{
    "title": "Assessing and Understanding Creativity in Large Language Models. (arXiv:2401.12491v1 [cs.CL])",
    "abstract": "In the field of natural language processing, the rapid development of large language model (LLM) has attracted more and more attention. LLMs have shown a high level of creativity in various tasks, but the methods for assessing such creativity are inadequate. The assessment of LLM creativity needs to consider differences from humans, requiring multi-dimensional measurement while balancing accuracy and efficiency. This paper aims to establish an efficient framework for assessing the level of creativity in LLMs. By adapting the modified Torrance Tests of Creative Thinking, the research evaluates the creative performance of various LLMs across 7 tasks, emphasizing 4 criteria including Fluency, Flexibility, Originality, and Elaboration. In this context, we develop a comprehensive dataset of 700 questions for testing and an LLM-based evaluation method. In addition, this study presents a novel analysis of LLMs' responses to diverse prompts and role-play situations. We found that the creativit",
    "link": "http://arxiv.org/abs/2401.12491",
    "context": "Title: Assessing and Understanding Creativity in Large Language Models. (arXiv:2401.12491v1 [cs.CL])\nAbstract: In the field of natural language processing, the rapid development of large language model (LLM) has attracted more and more attention. LLMs have shown a high level of creativity in various tasks, but the methods for assessing such creativity are inadequate. The assessment of LLM creativity needs to consider differences from humans, requiring multi-dimensional measurement while balancing accuracy and efficiency. This paper aims to establish an efficient framework for assessing the level of creativity in LLMs. By adapting the modified Torrance Tests of Creative Thinking, the research evaluates the creative performance of various LLMs across 7 tasks, emphasizing 4 criteria including Fluency, Flexibility, Originality, and Elaboration. In this context, we develop a comprehensive dataset of 700 questions for testing and an LLM-based evaluation method. In addition, this study presents a novel analysis of LLMs' responses to diverse prompts and role-play situations. We found that the creativit",
    "path": "papers/24/01/2401.12491.json",
    "total_tokens": 960,
    "translated_title": "评估和理解大型语言模型中的创造力",
    "translated_abstract": "在自然语言处理领域，大型语言模型（LLM）的快速发展引起了越来越多的关注。LLMs在各种任务中展现出了高水平的创造力，但评估这种创造力的方法尚不完善。评估LLM的创造力需要考虑与人类的差异，需要进行多维度的测量，同时平衡准确性和效率。本文旨在建立一个评估LLM创造力水平的高效框架。通过改进的托兰斯创造性思维测试的改编，本研究评估了各种LLM在7个任务中的创造性表现，强调了流畅度、灵活性、独创性和丰富性等4个标准。在这个背景下，我们开发了一个包含700个问题的全面数据集，用于测试和基于LLM的评估方法。此外，本研究还对LLM对各种提示和角色扮演情境的反应进行了新颖的分析。我们发现创造力的水平取决于任务的不同，同时也受到LLM的模型和参数的影响。",
    "tldr": "本文旨在建立一个评估大型语言模型（LLM）创造力水平的高效框架，并提出了评估方法和一个包含700个问题的全面数据集。研究发现创造力水平受到任务差异和LLM模型参数的影响。",
    "en_tdlr": "This paper aims to establish an efficient framework for assessing the level of creativity in large language models (LLMs), and presents an evaluation method and a comprehensive dataset of 700 questions. The study found that the level of creativity is influenced by task differences and LLM model parameters."
}