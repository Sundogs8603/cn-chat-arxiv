{
    "title": "Policy-regularized Offline Multi-objective Reinforcement Learning. (arXiv:2401.02244v1 [cs.LG])",
    "abstract": "In this paper, we aim to utilize only offline trajectory data to train a policy for multi-objective RL. We extend the offline policy-regularized method, a widely-adopted approach for single-objective offline RL problems, into the multi-objective setting in order to achieve the above goal. However, such methods face a new challenge in offline MORL settings, namely the preference-inconsistent demonstration problem. We propose two solutions to this problem: 1) filtering out preference-inconsistent demonstrations via approximating behavior preferences, and 2) adopting regularization techniques with high policy expressiveness. Moreover, we integrate the preference-conditioned scalarized update method into policy-regularized offline RL, in order to simultaneously learn a set of policies using a single policy network, thus reducing the computational cost induced by the training of a large number of individual policies for various preferences. Finally, we introduce Regularization Weight Adapta",
    "link": "http://arxiv.org/abs/2401.02244",
    "context": "Title: Policy-regularized Offline Multi-objective Reinforcement Learning. (arXiv:2401.02244v1 [cs.LG])\nAbstract: In this paper, we aim to utilize only offline trajectory data to train a policy for multi-objective RL. We extend the offline policy-regularized method, a widely-adopted approach for single-objective offline RL problems, into the multi-objective setting in order to achieve the above goal. However, such methods face a new challenge in offline MORL settings, namely the preference-inconsistent demonstration problem. We propose two solutions to this problem: 1) filtering out preference-inconsistent demonstrations via approximating behavior preferences, and 2) adopting regularization techniques with high policy expressiveness. Moreover, we integrate the preference-conditioned scalarized update method into policy-regularized offline RL, in order to simultaneously learn a set of policies using a single policy network, thus reducing the computational cost induced by the training of a large number of individual policies for various preferences. Finally, we introduce Regularization Weight Adapta",
    "path": "papers/24/01/2401.02244.json",
    "total_tokens": 974,
    "translated_title": "政策规范化的离线多目标强化学习",
    "translated_abstract": "本文旨在利用仅使用离线轨迹数据来训练多目标强化学习的政策。我们将广泛采用的用于单目标离线强化学习问题的离线规范化方法扩展到多目标设置，以实现上述目标。然而，在离线多目标强化学习中，这样的方法面临新的挑战，即偏好不一致的演示问题。我们提出了两种解决这个问题的方法：1）通过近似行为偏好来过滤出偏好不一致的演示，和2）采用具有高策略表达能力的正则化技术。此外，我们将偏好条件化标量化更新方法融入到政策规范化的离线强化学习中，以使用单个策略网络同时学习一组策略，从而减少为各种偏好训练大量个体策略所产生的计算成本。最后，我们引入了正则化权重调整方法...",
    "tldr": "本文将离线规范化方法扩展到多目标强化学习中，以利用离线轨迹数据训练多目标政策。在面对偏好不一致的演示问题时，提出了过滤方法和正则化技术。通过将偏好条件化标量化更新与政策规范化相结合，可以同时学习一组策略，从而降低计算成本。"
}