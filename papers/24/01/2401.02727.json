{
    "title": "Enhancing targeted transferability via feature space fine-tuning. (arXiv:2401.02727v1 [cs.CV])",
    "abstract": "Adversarial examples (AEs) have been extensively studied due to their potential for privacy protection and inspiring robust neural networks. However, making a targeted AE transferable across unknown models remains challenging. In this paper, to alleviate the overfitting dilemma common in an AE crafted by existing simple iterative attacks, we propose fine-tuning it in the feature space. Specifically, starting with an AE generated by a baseline attack, we encourage the features that contribute to the target class and discourage the features that contribute to the original class in a middle layer of the source model. Extensive experiments demonstrate that only a few iterations of fine-tuning can boost existing attacks in terms of targeted transferability nontrivially and universally. Our results also verify that the simple iterative attacks can yield comparable or even better transferability than the resource-intensive methods, which rely on training target-specific classifiers or generat",
    "link": "http://arxiv.org/abs/2401.02727",
    "context": "Title: Enhancing targeted transferability via feature space fine-tuning. (arXiv:2401.02727v1 [cs.CV])\nAbstract: Adversarial examples (AEs) have been extensively studied due to their potential for privacy protection and inspiring robust neural networks. However, making a targeted AE transferable across unknown models remains challenging. In this paper, to alleviate the overfitting dilemma common in an AE crafted by existing simple iterative attacks, we propose fine-tuning it in the feature space. Specifically, starting with an AE generated by a baseline attack, we encourage the features that contribute to the target class and discourage the features that contribute to the original class in a middle layer of the source model. Extensive experiments demonstrate that only a few iterations of fine-tuning can boost existing attacks in terms of targeted transferability nontrivially and universally. Our results also verify that the simple iterative attacks can yield comparable or even better transferability than the resource-intensive methods, which rely on training target-specific classifiers or generat",
    "path": "papers/24/01/2401.02727.json",
    "total_tokens": 930,
    "translated_title": "通过特征空间微调提高目标可传递性",
    "translated_abstract": "由于其对隐私保护的潜力和激发鲁棒性神经网络的能力，对抗性示例（AEs）已经被广泛研究。然而，使目标AE在未知模型之间可传递仍然具有挑战性。在本文中，为了减轻现有简单迭代攻击所生成的AE中常见的过拟合困境，我们提出在特征空间中对其进行微调。具体而言，我们从基线攻击生成的AE开始，在源模型的中间层中鼓励有助于目标类别的特征，阻碍有助于原始类别的特征。大量实验表明，仅使用几次微调即可显著和普遍地提高现有攻击的目标传递性。我们的结果还验证了简单的迭代攻击可以产生与资源密集型方法相当甚至更好的传递性，后者依赖于训练特定目标分类器或生成特定的AE方法。",
    "tldr": "本文提出了一种通过特征空间微调AE，显著提高现有攻击的目标传递性的方法。实验结果表明，只需少量的微调即可普遍地增强攻击的传递能力，并显示出简单的迭代攻击可以与资源密集型方法相媲美甚至更好。"
}