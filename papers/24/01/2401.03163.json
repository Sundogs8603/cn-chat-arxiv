{
    "title": "An Empirical Investigation of Value-Based Multi-objective Reinforcement Learning for Stochastic Environments. (arXiv:2401.03163v1 [cs.LG])",
    "abstract": "One common approach to solve multi-objective reinforcement learning (MORL) problems is to extend conventional Q-learning by using vector Q-values in combination with a utility function. However issues can arise with this approach in the context of stochastic environments, particularly when optimising for the Scalarised Expected Reward (SER) criterion. This paper extends prior research, providing a detailed examination of the factors influencing the frequency with which value-based MORL Q-learning algorithms learn the SER-optimal policy for an environment with stochastic state transitions. We empirically examine several variations of the core multi-objective Q-learning algorithm as well as reward engineering approaches, and demonstrate the limitations of these methods. In particular, we highlight the critical impact of the noisy Q-value estimates issue on the stability and convergence of these algorithms.",
    "link": "http://arxiv.org/abs/2401.03163",
    "context": "Title: An Empirical Investigation of Value-Based Multi-objective Reinforcement Learning for Stochastic Environments. (arXiv:2401.03163v1 [cs.LG])\nAbstract: One common approach to solve multi-objective reinforcement learning (MORL) problems is to extend conventional Q-learning by using vector Q-values in combination with a utility function. However issues can arise with this approach in the context of stochastic environments, particularly when optimising for the Scalarised Expected Reward (SER) criterion. This paper extends prior research, providing a detailed examination of the factors influencing the frequency with which value-based MORL Q-learning algorithms learn the SER-optimal policy for an environment with stochastic state transitions. We empirically examine several variations of the core multi-objective Q-learning algorithm as well as reward engineering approaches, and demonstrate the limitations of these methods. In particular, we highlight the critical impact of the noisy Q-value estimates issue on the stability and convergence of these algorithms.",
    "path": "papers/24/01/2401.03163.json",
    "total_tokens": 864,
    "translated_title": "基于价值的多目标强化学习在随机环境中的实证研究",
    "translated_abstract": "解决多目标强化学习问题的一种常见方法是通过使用矢量Q值与效用函数相结合来扩展传统的Q学习。然而，在随机环境下，尤其是在优化标量化预期回报（SER）准则时，这种方法可能会出现问题。本文扩展了之前的研究，对影响基于价值的多目标强化学习Q学习算法学习SER最优策略频率的因素进行了详细考察。我们经验性地研究了几种核心多目标Q学习算法的变体以及奖励工程方法，并展示了这些方法的局限性。特别是，我们强调了噪声Q值估计问题对这些算法的稳定性和收敛性的重要影响。",
    "tldr": "本文针对利用Q值与效用函数扩展传统Q学习解决多目标强化学习问题的方法，在随机环境中存在问题。通过实证研究核心算法变体和奖励工程方法，发现噪声Q值估计问题对算法的稳定性和收敛性有关键影响。"
}