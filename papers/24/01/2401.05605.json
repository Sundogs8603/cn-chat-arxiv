{
    "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models. (arXiv:2401.05605v1 [cs.CL])",
    "abstract": "We study and quantify the problem of forgetting when fine-tuning pre-trained large language models (LLMs) on a downstream task. We find that parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters (LoRA), still suffer from catastrophic forgetting. In particular, we identify a strong inverse linear relationship between the fine-tuning performance and the amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise scaling laws that show forgetting increases as a shifted power law in the number of parameters fine-tuned and the number of update steps. We also examine the impact of forgetting on knowledge, reasoning, and the safety guardrails trained into Llama 2 7B chat. Our study suggests that forgetting cannot be avoided through early stopping or by varying the number of parameters fine-tuned. We believe this opens up an important safety-critical direction for future research to evaluate and develop fine-tuning schemes which mitigate forgetting",
    "link": "http://arxiv.org/abs/2401.05605",
    "context": "Title: Scaling Laws for Forgetting When Fine-Tuning Large Language Models. (arXiv:2401.05605v1 [cs.CL])\nAbstract: We study and quantify the problem of forgetting when fine-tuning pre-trained large language models (LLMs) on a downstream task. We find that parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters (LoRA), still suffer from catastrophic forgetting. In particular, we identify a strong inverse linear relationship between the fine-tuning performance and the amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise scaling laws that show forgetting increases as a shifted power law in the number of parameters fine-tuned and the number of update steps. We also examine the impact of forgetting on knowledge, reasoning, and the safety guardrails trained into Llama 2 7B chat. Our study suggests that forgetting cannot be avoided through early stopping or by varying the number of parameters fine-tuned. We believe this opens up an important safety-critical direction for future research to evaluate and develop fine-tuning schemes which mitigate forgetting",
    "path": "papers/24/01/2401.05605.json",
    "total_tokens": 1042,
    "translated_title": "缩放大型语言模型微调时的遗忘规律",
    "translated_abstract": "我们研究并量化了在下游任务中对预训练大型语言模型（LLMs）进行微调时遗忘的问题。我们发现，参数高效的微调策略（如Low-Rank Adapters）仍然存在灾难性遗忘的问题。特别是，我们发现了在使用Low-Rank Adapters进行LLMs微调时，微调性能与遗忘程度之间存在强烈的反比线性关系。我们进一步得到了精确的缩放规律，表明遗忘程度随着微调参数的数量和更新步骤的数量呈现出一种平移的幂律增长。我们还研究了遗忘对Llama 2 7B聊天模型中的知识、推理和安全防护的影响。我们的研究表明，无法通过提前停止微调或改变微调参数的数量来避免遗忘。我们相信这为未来评估和开发能够减轻遗忘的微调方案开辟了重要的安全关键方向。",
    "tldr": "本研究探讨了在微调大型语言模型时的遗忘问题，并得出了微调性能与遗忘程度之间存在反比线性关系的结论，提出了遗忘程度随微调参数数量和更新步骤数量呈幂律增长的缩放规律。研究结果还显示，提前停止微调或改变微调参数数量都无法避免遗忘，这为未来减轻遗忘的微调方案的研究提供了重要的安全关键方向。",
    "en_tdlr": "This study investigates the issue of forgetting when fine-tuning large language models and provides insights into the inverse linear relationship between fine-tuning performance and forgetting. Precise scaling laws are derived, showing that forgetting increases with the number of fine-tuned parameters and update steps. The study also highlights the limitations of early stopping and parameter adjustment in mitigating forgetting, opening up a critical direction for future research on fine-tuning schemes to address this issue."
}