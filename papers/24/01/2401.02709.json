{
    "title": "German Text Embedding Clustering Benchmark. (arXiv:2401.02709v1 [cs.CL])",
    "abstract": "This work introduces a benchmark assessing the performance of clustering German text embeddings in different domains. This benchmark is driven by the increasing use of clustering neural text embeddings in tasks that require the grouping of texts (such as topic modeling) and the need for German resources in existing benchmarks. We provide an initial analysis for a range of pre-trained mono- and multilingual models evaluated on the outcome of different clustering algorithms. Results include strong performing mono- and multilingual models. Reducing the dimensions of embeddings can further improve clustering. Additionally, we conduct experiments with continued pre-training for German BERT models to estimate the benefits of this additional training. Our experiments suggest that significant performance improvements are possible for short text. All code and datasets are publicly available.",
    "link": "http://arxiv.org/abs/2401.02709",
    "context": "Title: German Text Embedding Clustering Benchmark. (arXiv:2401.02709v1 [cs.CL])\nAbstract: This work introduces a benchmark assessing the performance of clustering German text embeddings in different domains. This benchmark is driven by the increasing use of clustering neural text embeddings in tasks that require the grouping of texts (such as topic modeling) and the need for German resources in existing benchmarks. We provide an initial analysis for a range of pre-trained mono- and multilingual models evaluated on the outcome of different clustering algorithms. Results include strong performing mono- and multilingual models. Reducing the dimensions of embeddings can further improve clustering. Additionally, we conduct experiments with continued pre-training for German BERT models to estimate the benefits of this additional training. Our experiments suggest that significant performance improvements are possible for short text. All code and datasets are publicly available.",
    "path": "papers/24/01/2401.02709.json",
    "total_tokens": 921,
    "translated_title": "德语文本嵌入聚类基准研究",
    "translated_abstract": "本研究介绍了一个基准测试，用于评估在不同领域中聚类德语文本嵌入的性能。这个基准测试是由对聚类神经文本嵌入在需要对文本进行分组（如主题建模）的任务中的增加使用所驱动的，并且在现有基准测试中需要德语资源。我们对一系列预训练的单语和多语模型在不同聚类算法的结果上进行了初步分析。结果包括表现强劲的单语和多语模型。缩减嵌入的维度可以进一步提高聚类效果。此外，我们还进行了与德语BERT模型的持续预训练的实验，以评估这种额外训练的好处。我们的实验表明，对于短文本可能存在显著的性能改进。所有代码和数据集均可公开获取。",
    "tldr": "本研究提出了一个评估不同领域中聚类德语文本嵌入性能的基准测试，并对预训练模型和聚类算法进行了初步分析。结果显示单语和多语模型表现强劲，缩减嵌入维度可以提高聚类效果。此外，持续预训练德语BERT模型可能对短文本性能有显著改善。所有代码和数据集均公开可用。",
    "en_tdlr": "This research presents a benchmark for evaluating the performance of clustering German text embeddings in different domains. The analysis includes pre-trained mono- and multilingual models and various clustering algorithms, with strong performing models identified and the potential for improved clustering through dimension reduction. The study also explores the benefits of continued pre-training for German BERT models and its potential impact on short text performance. All code and datasets are publicly available."
}