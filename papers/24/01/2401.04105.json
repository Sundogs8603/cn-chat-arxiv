{
    "title": "Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning",
    "abstract": "arXiv:2401.04105v2 Announce Type: replace-cross  Abstract: Large pretrained models are increasingly crucial in modern computer vision tasks. These models are typically used in downstream tasks by end-to-end finetuning, which is highly memory-intensive for tasks with high-resolution data, e.g., video understanding, small object detection, and point cloud analysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks, or Dr$^2$Net, a novel family of network architectures that acts as a surrogate network to finetune a pretrained model with substantially reduced memory consumption. Dr$^2$Net contains two types of residual connections, one maintaining the residual structure in the pretrained models, and the other making the network reversible. Due to its reversibility, intermediate activations, which can be reconstructed from output, are cleared from memory during training. We use two coefficients on either type of residual connections respectively, and introduce a dynamic tr",
    "link": "https://arxiv.org/abs/2401.04105",
    "context": "Title: Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning\nAbstract: arXiv:2401.04105v2 Announce Type: replace-cross  Abstract: Large pretrained models are increasingly crucial in modern computer vision tasks. These models are typically used in downstream tasks by end-to-end finetuning, which is highly memory-intensive for tasks with high-resolution data, e.g., video understanding, small object detection, and point cloud analysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks, or Dr$^2$Net, a novel family of network architectures that acts as a surrogate network to finetune a pretrained model with substantially reduced memory consumption. Dr$^2$Net contains two types of residual connections, one maintaining the residual structure in the pretrained models, and the other making the network reversible. Due to its reversibility, intermediate activations, which can be reconstructed from output, are cleared from memory during training. We use two coefficients on either type of residual connections respectively, and introduce a dynamic tr",
    "path": "papers/24/01/2401.04105.json",
    "total_tokens": 888,
    "translated_title": "Dr$^2$Net：用于内存高效微调的动态可逆双残差网络",
    "translated_abstract": "大型预训练模型在现代计算机视觉任务中变得越来越重要。这些模型通常通过端对端微调在下游任务中使用，对于具有高分辨率数据的任务（例如视频理解、小目标检测和点云分析），这种微调对内存要求很高。本文提出了一种名为Dr$^2$Net（Dynamic Reversible Dual-Residual Networks）的新型网络结构系列，它充当预训练模型的替代网络，大大降低了内存消耗。Dr$^2$Net包含两种类型的残差连接，一种保持预训练模型中的残差结构，另一种使网络可逆。由于其可逆性，可以从输出中重建中间激活，并在训练过程中将其清除内存。我们分别在两种残差连接类型上引入两个系数，并引入动态tr",
    "tldr": "提出了一种名为Dr$^2$Net的新型网络结构，可以以大大降低内存消耗的方式作为预训练模型的替代网络，通过具有两种类型残差连接的设计实现了可逆性，实现了在训练过程中清除中间激活并减少内存占用",
    "en_tdlr": "Introduced Dr$^2$Net, a novel network architecture that serves as a surrogate network for pretrained models with significantly reduced memory consumption, achieving reversibility in training and clearing intermediate activations to minimize memory usage."
}