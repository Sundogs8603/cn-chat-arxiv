{
    "title": "Keeping Deep Learning Models in Check: A History-Based Approach to Mitigate Overfitting. (arXiv:2401.10359v1 [cs.SE])",
    "abstract": "In software engineering, deep learning models are increasingly deployed for critical tasks such as bug detection and code review. However, overfitting remains a challenge that affects the quality, reliability, and trustworthiness of software systems that utilize deep learning models. Overfitting can be (1) prevented (e.g., using dropout or early stopping) or (2) detected in a trained model (e.g., using correlation-based approaches). Both overfitting detection and prevention approaches that are currently used have constraints (e.g., requiring modification of the model structure, and high computing resources). In this paper, we propose a simple, yet powerful approach that can both detect and prevent overfitting based on the training history (i.e., validation losses). Our approach first trains a time series classifier on training histories of overfit models. This classifier is then used to detect if a trained model is overfit. In addition, our trained classifier can be used to prevent ove",
    "link": "http://arxiv.org/abs/2401.10359",
    "context": "Title: Keeping Deep Learning Models in Check: A History-Based Approach to Mitigate Overfitting. (arXiv:2401.10359v1 [cs.SE])\nAbstract: In software engineering, deep learning models are increasingly deployed for critical tasks such as bug detection and code review. However, overfitting remains a challenge that affects the quality, reliability, and trustworthiness of software systems that utilize deep learning models. Overfitting can be (1) prevented (e.g., using dropout or early stopping) or (2) detected in a trained model (e.g., using correlation-based approaches). Both overfitting detection and prevention approaches that are currently used have constraints (e.g., requiring modification of the model structure, and high computing resources). In this paper, we propose a simple, yet powerful approach that can both detect and prevent overfitting based on the training history (i.e., validation losses). Our approach first trains a time series classifier on training histories of overfit models. This classifier is then used to detect if a trained model is overfit. In addition, our trained classifier can be used to prevent ove",
    "path": "papers/24/01/2401.10359.json",
    "total_tokens": 874,
    "translated_title": "保持深度学习模型可控性: 基于历史的方法来缓解过拟合",
    "translated_abstract": "在软件工程中，深度学习模型越来越多地被用于关键任务，如漏洞检测和代码审查。然而，过拟合仍然是一个挑战，影响着利用深度学习模型的软件系统的质量、可靠性和可信度。当前使用的过拟合检测和防止方法都有一些限制，如需要修改模型结构和高计算资源消耗。在本文中，我们提出了一种简单而强大的方法，可以基于训练历史（即验证损失）同时检测和防止过拟合。我们的方法首先在过拟合模型的训练历史上训练一个时间序列分类器。然后，这个分类器被用来检测一个训练好的模型是否过拟合。此外，我们训练好的分类器也可以用来防止过拟合。",
    "tldr": "这篇论文提出了一种简单而强大的基于历史的方法来检测和防止深度学习模型的过拟合问题，为软件工程中深度学习模型的使用提供了质量和可靠性的保证。",
    "en_tdlr": "This paper proposes a simple yet powerful history-based approach to detect and prevent overfitting in deep learning models, providing quality and reliability assurance for the use of these models in software engineering."
}