{
    "title": "Adjustable Molecular Representation for Unified Pre-training Strategy. (arXiv:2401.06166v1 [q-bio.BM])",
    "abstract": "We propose a new large-scale molecular model, named AdaMR, which stands for Adjustable Molecular Representation for Unified Pre-training Strategy. Unlike recent large-scale molecular models that use a single molecular encoding, AdaMR employs a granularity-adjustable molecular encoder, learning molecular representations at both the atomic and substructure levels. For the pre-training process, we designed a task for molecular canonicalization, which involves transforming ltiple generic molecular representations into canonical representations. By adjusting the granularity of molecular encoding, the trained model can improve the effects on multiple downstream tasks, such as model attribute prediction and molecule generation. Substructure-level molecular representation retains information of specific atom groups or arrangements that determine chemical properties and have similar functions, which is beneficial for tasks like property prediction. Meanwhile, atomic-level representation, combin",
    "link": "http://arxiv.org/abs/2401.06166",
    "context": "Title: Adjustable Molecular Representation for Unified Pre-training Strategy. (arXiv:2401.06166v1 [q-bio.BM])\nAbstract: We propose a new large-scale molecular model, named AdaMR, which stands for Adjustable Molecular Representation for Unified Pre-training Strategy. Unlike recent large-scale molecular models that use a single molecular encoding, AdaMR employs a granularity-adjustable molecular encoder, learning molecular representations at both the atomic and substructure levels. For the pre-training process, we designed a task for molecular canonicalization, which involves transforming ltiple generic molecular representations into canonical representations. By adjusting the granularity of molecular encoding, the trained model can improve the effects on multiple downstream tasks, such as model attribute prediction and molecule generation. Substructure-level molecular representation retains information of specific atom groups or arrangements that determine chemical properties and have similar functions, which is beneficial for tasks like property prediction. Meanwhile, atomic-level representation, combin",
    "path": "papers/24/01/2401.06166.json",
    "total_tokens": 863,
    "translated_title": "可调整的分子表示方法用于统一的预训练策略",
    "translated_abstract": "我们提出了一种新的大规模分子模型，名为AdaMR，它代表可调整的分子表示方法用于统一的预训练策略。与最近使用单一分子编码的大规模分子模型不同，AdaMR采用了可调整粒度的分子编码器，在原子和亚结构水平上学习分子表示。对于预训练过程，我们设计了一个分子规范化的任务，涉及将多个通用分子表示转化为规范表示。通过调整分子编码的粒度，训练得到的模型可以提高对多个下游任务的效果，如模型属性预测和分子生成。亚结构水平的分子表示保留了决定化学性质和具有类似功能的特定原子组或排列的信息，对于性质预测等任务是有益的。同时，原子级表示将原子的特异信息纳入考虑，使模型能够更好地捕捉原子间的相互作用。",
    "tldr": "AdaMR是一种可调整粒度的分子模型，它在原子和亚结构水平上学习分子表示。通过预训练和分子规范化任务，AdaMR可以改善对多个下游任务的效果，包括模型属性预测和分子生成。",
    "en_tdlr": "AdaMR is an adjustable molecular model that learns molecular representations at both the atomic and substructure levels. By pre-training and involving a molecular canonicalization task, AdaMR improves the performance on multiple downstream tasks, including model attribute prediction and molecule generation."
}