{
    "title": "Local and Global Contexts for Conversation",
    "abstract": "The context in conversation is the dialog history crucial for multi-turn dialogue. Learning from the relevant contexts in dialog history for grounded conversation is a challenging problem. Local context is the most neighbor and more sensitive to the subsequent response, and global context is relevant to a whole conversation far beyond neighboring utterances. Currently, pretrained transformer models for conversation challenge capturing the correlation and connection between local and global contexts. We introduce a local and global conversation model (LGCM) for general-purpose conversation in open domain. It is a local-global hierarchical transformer model that excels at accurately discerning and assimilating the relevant contexts necessary for generating responses. It employs a local encoder to grasp the local context at the level of individual utterances and a global encoder to understand the broader context at the dialogue level. The seamless fusion of these locally and globally cont",
    "link": "https://arxiv.org/abs/2401.17588",
    "context": "Title: Local and Global Contexts for Conversation\nAbstract: The context in conversation is the dialog history crucial for multi-turn dialogue. Learning from the relevant contexts in dialog history for grounded conversation is a challenging problem. Local context is the most neighbor and more sensitive to the subsequent response, and global context is relevant to a whole conversation far beyond neighboring utterances. Currently, pretrained transformer models for conversation challenge capturing the correlation and connection between local and global contexts. We introduce a local and global conversation model (LGCM) for general-purpose conversation in open domain. It is a local-global hierarchical transformer model that excels at accurately discerning and assimilating the relevant contexts necessary for generating responses. It employs a local encoder to grasp the local context at the level of individual utterances and a global encoder to understand the broader context at the dialogue level. The seamless fusion of these locally and globally cont",
    "path": "papers/24/01/2401.17588.json",
    "total_tokens": 869,
    "translated_title": "对话的本地和全局环境",
    "translated_abstract": "对话中的上下文是多轮对话中至关重要的对话历史。学习通过对话历史中的相关上下文进行对话是一个具有挑战性的问题。本地上下文是与后续回应更接近且更敏感的最近邻上下文，而全局上下文与整个对话相关，远超出邻近的话语。目前，用于对话的预训练转换器模型无法准确捕捉本地和全局上下文之间的相关性和连接。我们引入了一个用于开放领域中的通用对话的本地和全局对话模型（LGCM）。它是一个本地-全局层次转换器模型，能够准确区分和融合生成回应所需的相关上下文。它使用本地编码器来获取个别话语层面的本地上下文，使用全局编码器来理解对话层面的更广泛上下文。通过无缝融合这些本地和全局上下文，模型能够更好地进行对话生成。",
    "tldr": "本文引入了一个本地和全局对话模型（LGCM），它是一个本地-全局层次转换器模型，能够准确区分和融合生成回应所需的相关上下文。通过无缝融合本地和全局上下文，模型能够更好地进行对话生成。",
    "en_tdlr": "This paper introduces a local and global conversation model (LGCM), which is a local-global hierarchical transformer model that accurately discerns and assimilates the relevant contexts necessary for generating responses. By seamlessly fusing the local and global contexts, the model excels at conversation generation."
}