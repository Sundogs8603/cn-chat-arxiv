{
    "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. (arXiv:2401.02731v1 [cs.AI])",
    "abstract": "Large Language Models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across a wide range of tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC), which transitions dense models to sparse models using a Mixture of Experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal increase ",
    "link": "http://arxiv.org/abs/2401.02731",
    "context": "Title: Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. (arXiv:2401.02731v1 [cs.AI])\nAbstract: Large Language Models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across a wide range of tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC), which transitions dense models to sparse models using a Mixture of Experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal increase ",
    "path": "papers/24/01/2401.02731.json",
    "total_tokens": 912,
    "translated_title": "参数高效稀疏制作：从密集型到专家混合式用于通用任务的指令调整",
    "translated_abstract": "大型语言模型(LLMs)在通用自然语言处理(NLP)任务中展示出相当的熟练程度。指令调整作为一种成功的范例，增强了LLMs遵循自然语言指令并在各种任务中展现出强大的泛化能力。然而，由于模型容量限制，这些模型在多个任务中经常遇到性能限制。在指令调整阶段扩展模型容量面临着巨大的挑战。为了解决这个问题，我们引入了一种新颖的方法，参数高效稀疏制作(PESC)，它使用专家混合式(MoE)架构将密集模型转换为稀疏模型。PESC将适配器集成到稀疏模型的MoE层中，区分不同的专家而不改变这些层中的个体权重。这种方法显著降低了计算成本和GPU内存需求，通过最小的增加实现了模型容量的扩展。",
    "tldr": "本文提出了一种参数高效稀疏制作的方法，它使用专家混合式架构将密集模型转换为稀疏模型，以实现在模型容量有限的情况下进行指令调整和泛化能力增强。",
    "en_tdlr": "This paper introduces a novel approach, Parameter-Efficient Sparsity Crafting (PESC), which uses a Mixture of Experts (MoE) architecture to transition dense models to sparse models, aiming to address performance limitations and expand model capacity for instruction tuning and enhanced generalization."
}