{
    "title": "MaLLaM -- Malaysia Large Language Model. (arXiv:2401.14680v1 [cs.CL])",
    "abstract": "Addressing the gap in Large Language Model pretrained from scratch with Malaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion parameters on a substantial 349GB dataset, equivalent to 90 billion tokens based on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch. MaLLaM contributes to enhanced natural language understanding and generation tasks in the Malay language. Although trained on a smaller dataset of 90 billion tokens, our instruction-tuned MaLLaM models perform competitively. When compared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models demonstrate notable proficiency, underscoring the effectiveness of our approach in capturing and understanding the nuances of the Malaysian language. MaLLaM models mark a significant contribution to the field, providing comprehensive language representations grounded in Malaysian context. This endeavor aims to pave the way for enhanced natural language understanding and generation ",
    "link": "http://arxiv.org/abs/2401.14680",
    "context": "Title: MaLLaM -- Malaysia Large Language Model. (arXiv:2401.14680v1 [cs.CL])\nAbstract: Addressing the gap in Large Language Model pretrained from scratch with Malaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion parameters on a substantial 349GB dataset, equivalent to 90 billion tokens based on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch. MaLLaM contributes to enhanced natural language understanding and generation tasks in the Malay language. Although trained on a smaller dataset of 90 billion tokens, our instruction-tuned MaLLaM models perform competitively. When compared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models demonstrate notable proficiency, underscoring the effectiveness of our approach in capturing and understanding the nuances of the Malaysian language. MaLLaM models mark a significant contribution to the field, providing comprehensive language representations grounded in Malaysian context. This endeavor aims to pave the way for enhanced natural language understanding and generation ",
    "path": "papers/24/01/2401.14680.json",
    "total_tokens": 985,
    "translated_title": "MaLLaM -- 马来西亚大型语言模型",
    "translated_abstract": "针对马来西亚语境下从头预训练的大型语言模型的不足，我们使用349GB的大型数据集（相当于90亿个标记），使用我们预训练的字节对编码（BPE）分词器，在1.1亿、30亿和50亿参数上训练了模型。MaLLaM在马来语的自然语言理解和生成任务中做出了贡献。尽管只是使用了90亿个标记的较小数据集进行了训练，我们经过指令调整的MaLLaM模型表现出了竞争力。与ChatGPT3.5和Malaysian Mistral相比，MaLLaM的指令调整模型展示了显著的熟练度，突出了我们方法在捕捉和理解马来西亚语言细微差异方面的有效性。MaLLaM模型在该领域中起到了重要作用，提供了与马来西亚语境紧密联系的全面语言表示。这项努力旨在为提升自然语言理解和生成铺平道路。",
    "tldr": "MaLLaM是马来西亚大型语言模型，通过使用大型数据集和预训练的BPE分词器，在马来语的自然语言理解和生成任务中取得了竞争力，并展示了在捕捉和理解马来西亚语言细微差异方面的有效性，为提升自然语言理解和生成打下了基础。",
    "en_tdlr": "MaLLaM is a large language model trained with a substantial dataset and a pretrained BPE tokenizer. It demonstrates competitive performance in natural language understanding and generation tasks in the Malay language, highlighting its effectiveness in capturing and understanding the nuances of the Malaysian language. MaLLaM models contribute significantly to the field, providing comprehensive language representations grounded in the Malaysian context and paving the way for improved natural language understanding and generation."
}