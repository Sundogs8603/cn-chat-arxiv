{
    "title": "Enabling On-device Continual Learning with Binary Neural Networks. (arXiv:2401.09916v1 [cs.LG])",
    "abstract": "On-device learning remains a formidable challenge, especially when dealing with resource-constrained devices that have limited computational capabilities. This challenge is primarily rooted in two key issues: first, the memory available on embedded devices is typically insufficient to accommodate the memory-intensive back-propagation algorithm, which often relies on floating-point precision. Second, the development of learning algorithms on models with extreme quantization levels, such as Binary Neural Networks (BNNs), is critical due to the drastic reduction in bit representation. In this study, we propose a solution that combines recent advancements in the field of Continual Learning (CL) and Binary Neural Networks to enable on-device training while maintaining competitive performance. Specifically, our approach leverages binary latent replay (LR) activations and a novel quantization scheme that significantly reduces the number of bits required for gradient computation. The experimen",
    "link": "http://arxiv.org/abs/2401.09916",
    "context": "Title: Enabling On-device Continual Learning with Binary Neural Networks. (arXiv:2401.09916v1 [cs.LG])\nAbstract: On-device learning remains a formidable challenge, especially when dealing with resource-constrained devices that have limited computational capabilities. This challenge is primarily rooted in two key issues: first, the memory available on embedded devices is typically insufficient to accommodate the memory-intensive back-propagation algorithm, which often relies on floating-point precision. Second, the development of learning algorithms on models with extreme quantization levels, such as Binary Neural Networks (BNNs), is critical due to the drastic reduction in bit representation. In this study, we propose a solution that combines recent advancements in the field of Continual Learning (CL) and Binary Neural Networks to enable on-device training while maintaining competitive performance. Specifically, our approach leverages binary latent replay (LR) activations and a novel quantization scheme that significantly reduces the number of bits required for gradient computation. The experimen",
    "path": "papers/24/01/2401.09916.json",
    "total_tokens": 869,
    "translated_title": "使用二值神经网络实现设备上的持续学习",
    "translated_abstract": "设备上的学习仍然是一个巨大的挑战，特别是当处理资源受限的设备时，这些设备的计算能力有限。这个挑战主要源于两个关键问题：首先，嵌入式设备上可用的内存通常不足以容纳消耗内存的反向传播算法，该算法通常依赖于浮点精度。其次，对于具有极端量化级别的模型（如二值神经网络），开发学习算法至关重要，因为它们对位表示进行了剧烈减少。在本研究中，我们提出了一种解决方案，结合了持续学习（CL）和二值神经网络领域的最新进展，以在设备上进行训练同时保持竞争性能。具体而言，我们的方法利用了二值激活的重放（LR）和一种新颖的量化方案，可显着减少用于梯度计算的位数。",
    "tldr": "本研究提出了一种结合了持续学习和二值神经网络的解决方案，以在资源受限的设备上进行训练，并保持竞争性能。该方法利用二值激活的重放和一种新颖的量化方案。",
    "en_tdlr": "This study proposes a solution that combines continual learning and binary neural networks to enable on-device training while maintaining competitive performance. The approach leverages binary latent replay and a novel quantization scheme."
}