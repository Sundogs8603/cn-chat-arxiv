{
    "title": "InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes. (arXiv:2401.05335v1 [cs.CV])",
    "abstract": "We introduce InseRF, a novel method for generative object insertion in the NeRF reconstructions of 3D scenes. Based on a user-provided textual description and a 2D bounding box in a reference viewpoint, InseRF generates new objects in 3D scenes. Recently, methods for 3D scene editing have been profoundly transformed, owing to the use of strong priors of text-to-image diffusion models in 3D generative modeling. Existing methods are mostly effective in editing 3D scenes via style and appearance changes or removing existing objects. Generating new objects, however, remains a challenge for such methods, which we address in this study. Specifically, we propose grounding the 3D object insertion to a 2D object insertion in a reference view of the scene. The 2D edit is then lifted to 3D using a single-view object reconstruction method. The reconstructed object is then inserted into the scene, guided by the priors of monocular depth estimation methods. We evaluate our method on various 3D scene",
    "link": "http://arxiv.org/abs/2401.05335",
    "context": "Title: InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes. (arXiv:2401.05335v1 [cs.CV])\nAbstract: We introduce InseRF, a novel method for generative object insertion in the NeRF reconstructions of 3D scenes. Based on a user-provided textual description and a 2D bounding box in a reference viewpoint, InseRF generates new objects in 3D scenes. Recently, methods for 3D scene editing have been profoundly transformed, owing to the use of strong priors of text-to-image diffusion models in 3D generative modeling. Existing methods are mostly effective in editing 3D scenes via style and appearance changes or removing existing objects. Generating new objects, however, remains a challenge for such methods, which we address in this study. Specifically, we propose grounding the 3D object insertion to a 2D object insertion in a reference view of the scene. The 2D edit is then lifted to 3D using a single-view object reconstruction method. The reconstructed object is then inserted into the scene, guided by the priors of monocular depth estimation methods. We evaluate our method on various 3D scene",
    "path": "papers/24/01/2401.05335.json",
    "total_tokens": 941,
    "translated_title": "InseRF: 基于文本驱动的神经3D场景中的生成物体插入方法",
    "translated_abstract": "我们引入了InseRF，一种在3D场景的NeRF重建中进行生成物体插入的新方法。基于用户提供的文本描述和参考视点中的2D边界框，InseRF在3D场景中生成新的物体。最近，基于文本到图像扩散模型在3D生成建模中使用强大的先验知识，使得3D场景编辑的方法发生了深刻的转变。现有的方法主要通过风格和外观的改变或者移除现有物体来有效编辑3D场景。然而，对于这种方法，生成新的物体仍然是一个挑战，我们在本研究中解决了这个问题。具体而言，我们提出了将3D物体插入与参考视图中的2D物体插入进行对接的方法。然后，使用单视图物体重建方法将2D编辑提升为3D。然后，根据单目深度估计方法的先验知识将重建的物体插入到场景中。我们在不同的3D场景上对我们的方法进行了评估。",
    "tldr": "InseRF是一种在NeRF重建的3D场景中进行生成物体插入的新方法，通过用户提供的文本描述和2D边界框，实现对于现有方法难以实现的生成新物体的需求。",
    "en_tdlr": "InseRF is a novel method for generative object insertion in NeRF reconstructions of 3D scenes, addressing the challenge of generating new objects that existing methods struggle with, using user-provided textual description and 2D bounding box."
}