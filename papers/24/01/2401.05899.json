{
    "title": "Optimistic Model Rollouts for Pessimistic Offline Policy Optimization. (arXiv:2401.05899v1 [cs.LG])",
    "abstract": "Model-based offline reinforcement learning (RL) has made remarkable progress, offering a promising avenue for improving generalization with synthetic model rollouts. Existing works primarily focus on incorporating pessimism for policy optimization, usually via constructing a Pessimistic Markov Decision Process (P-MDP). However, the P-MDP discourages the policies from learning in out-of-distribution (OOD) regions beyond the support of offline datasets, which can under-utilize the generalization ability of dynamics models. In contrast, we propose constructing an Optimistic MDP (O-MDP). We initially observed the potential benefits of optimism brought by encouraging more OOD rollouts. Motivated by this observation, we present ORPO, a simple yet effective model-based offline RL framework. ORPO generates Optimistic model Rollouts for Pessimistic offline policy Optimization. Specifically, we train an optimistic rollout policy in the O-MDP to sample more OOD model rollouts. Then we relabel the",
    "link": "http://arxiv.org/abs/2401.05899",
    "context": "Title: Optimistic Model Rollouts for Pessimistic Offline Policy Optimization. (arXiv:2401.05899v1 [cs.LG])\nAbstract: Model-based offline reinforcement learning (RL) has made remarkable progress, offering a promising avenue for improving generalization with synthetic model rollouts. Existing works primarily focus on incorporating pessimism for policy optimization, usually via constructing a Pessimistic Markov Decision Process (P-MDP). However, the P-MDP discourages the policies from learning in out-of-distribution (OOD) regions beyond the support of offline datasets, which can under-utilize the generalization ability of dynamics models. In contrast, we propose constructing an Optimistic MDP (O-MDP). We initially observed the potential benefits of optimism brought by encouraging more OOD rollouts. Motivated by this observation, we present ORPO, a simple yet effective model-based offline RL framework. ORPO generates Optimistic model Rollouts for Pessimistic offline policy Optimization. Specifically, we train an optimistic rollout policy in the O-MDP to sample more OOD model rollouts. Then we relabel the",
    "path": "papers/24/01/2401.05899.json",
    "total_tokens": 959,
    "translated_title": "乐观模型预测用于悲观离线策略优化",
    "translated_abstract": "基于模型的离线强化学习（RL）取得了显著的进展，为通过合成模型预测来改善泛化能力提供了一种有前途的途径。现有的研究主要集中在通过构建悲观马尔科夫决策过程（P-MDP）来将悲观主义纳入策略优化中。然而，P-MDP限制策略在超出离线数据集支持的分布区域学习，从而可能没有充分利用动力模型的泛化能力。相反，我们提出了构建乐观MDP（O-MDP）的方法。我们最初观察到通过鼓励更多超出离线数据集的模型预测可以带来乐观主义的潜在好处。受此观察的启发，我们提出了ORPO，一个简单而有效的基于模型的离线RL框架。ORPO通过在O-MDP中训练乐观的模型预测策略来采样更多超出离线数据集的模型预测。然后我们重新为这些预测打上标签，并通过悲观主义的离线策略优化来提升性能。",
    "tldr": "这篇论文提出了一种基于模型的离线强化学习框架ORPO，通过构建乐观MDP来鼓励更多超出离线数据集的模型预测，从而提升泛化能力。",
    "en_tdlr": "This paper proposes an optimistic model-based offline RL framework called ORPO, which encourages more out-of-distribution model rollouts by constructing an optimistic MDP, aiming to improve generalization."
}