{
    "title": "A Good Score Does not Lead to A Good Generative Model. (arXiv:2401.04856v1 [cs.LG])",
    "abstract": "Score-based Generative Models (SGMs) is one leading method in generative modeling, renowned for their ability to generate high-quality samples from complex, high-dimensional data distributions. The method enjoys empirical success and is supported by rigorous theoretical convergence properties. In particular, it has been shown that SGMs can generate samples from a distribution that is close to the ground-truth if the underlying score function is learned well, suggesting the success of SGM as a generative model. We provide a counter-example in this paper. Through the sample complexity argument, we provide one specific setting where the score function is learned well. Yet, SGMs in this setting can only output samples that are Gaussian blurring of training data points, mimicking the effects of kernel density estimation. The finding resonates a series of recent finding that reveal that SGMs can demonstrate strong memorization effect and fail to generate.",
    "link": "http://arxiv.org/abs/2401.04856",
    "context": "Title: A Good Score Does not Lead to A Good Generative Model. (arXiv:2401.04856v1 [cs.LG])\nAbstract: Score-based Generative Models (SGMs) is one leading method in generative modeling, renowned for their ability to generate high-quality samples from complex, high-dimensional data distributions. The method enjoys empirical success and is supported by rigorous theoretical convergence properties. In particular, it has been shown that SGMs can generate samples from a distribution that is close to the ground-truth if the underlying score function is learned well, suggesting the success of SGM as a generative model. We provide a counter-example in this paper. Through the sample complexity argument, we provide one specific setting where the score function is learned well. Yet, SGMs in this setting can only output samples that are Gaussian blurring of training data points, mimicking the effects of kernel density estimation. The finding resonates a series of recent finding that reveal that SGMs can demonstrate strong memorization effect and fail to generate.",
    "path": "papers/24/01/2401.04856.json",
    "total_tokens": 882,
    "translated_title": "一个好的评分并不会导致一个好的生成模型",
    "translated_abstract": "基于评分的生成模型（SGMs）是生成建模中的一种主要方法，以其能够从复杂的高维数据分布中生成高质量样本而闻名。该方法在经验上取得了成功，并且有着严格的理论收敛性质的支持。特别是已经证明，如果学习到的底层评分函数良好，SGMs能够生成接近真实数据分布的样本，这表明了SGM作为生成模型的成功之处。本文提供了一个反例。通过样本复杂度的分析，我们提供了一个特定的设置，其中评分函数学习得很好。然而，在这个设置中，SGMs只能输出训练数据点的高斯模糊样本，模拟核密度估计的效果。这一发现与最近的一系列发现相一致，揭示了SGMs可能表现出强大的记忆效应并且无法生成样本的问题。",
    "tldr": "本文通过反例证明，在某些情况下，即使评分函数学习良好，基于评分的生成模型（SGMs）仍然无法生成接近真实数据分布的样本，并且只能产生训练数据点的高斯模糊样本。",
    "en_tdlr": "This paper provides a counter-example to show that, in some cases, even with a well-learned score function, score-based generative models (SGMs) still fail to generate samples that are close to the ground-truth data distribution, and can only produce Gaussian-blurred samples of the training data points."
}