{
    "title": "Neighboring Perturbations of Knowledge Editing on Large Language Models",
    "abstract": "Despite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play ",
    "link": "https://arxiv.org/abs/2401.17623",
    "context": "Title: Neighboring Perturbations of Knowledge Editing on Large Language Models\nAbstract: Despite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play ",
    "path": "papers/24/01/2401.17623.json",
    "total_tokens": 890,
    "translated_title": "大型语言模型上邻近扰动的知识编辑",
    "translated_abstract": "尽管大型语言模型（LLMs）具有卓越的能力，但由于错误或过时的知识，它们容易生成意外的文本。考虑到重新训练LLMs的资源密集型性质，知识编辑的发展呈现出显著增长。然而，目前的方法和评估很少探索编辑对相邻知识的扰动。本文研究了将新知识更新到LLMs中是否扰乱了其中包含的相邻知识。具体而言，我们探讨了将新的答案附加到事实性问题的答案列表中是否会导致原始正确答案的丧失，以及不经意地包含了错误答案。引入了一个加性指标，并构建了一个被称为知识附加扰动评估（PEAK）的基准来评估附加新知识时邻近知识的扰动程度。此外，引入了一个即插即用的机制，用于处理新增知识的效果和扰动。",
    "tldr": "本文研究大型语言模型上知识编辑对邻近知识的扰动，提出了 additivity 指标以及 Perturbation Evaluation of Appending Knowledge (PEAK) 基准，用于评估附加新知识时邻近知识的扰动程度。",
    "en_tdlr": "This paper investigates the perturbation of knowledge editing on neighboring knowledge in large language models (LLMs). It introduces the additivity metric and constructs a benchmark called Perturbation Evaluation of Appending Knowledge (PEAK) to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge."
}