{
    "title": "Adaptive Discounting of Training Time Attacks. (arXiv:2401.02652v1 [cs.LG])",
    "abstract": "Among the most insidious attacks on Reinforcement Learning (RL) solutions are training-time attacks (TTAs) that create loopholes and backdoors in the learned behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are now available, where the attacker forces a specific, target behaviour upon a training RL agent (victim). However, even state-of-the-art C-TTAs focus on target behaviours that could be naturally adopted by the victim if not for a particular feature of the environment dynamics, which C-TTAs exploit. In this work, we show that a C-TTA is possible even when the target behaviour is un-adoptable due to both environment dynamics as well as non-optimality with respect to the victim objective(s). To find efficient attacks in this context, we develop a specialised flavour of the DDPG algorithm, which we term gammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically alters the attack policy planning horizon based on the victim's current behaviour",
    "link": "http://arxiv.org/abs/2401.02652",
    "context": "Title: Adaptive Discounting of Training Time Attacks. (arXiv:2401.02652v1 [cs.LG])\nAbstract: Among the most insidious attacks on Reinforcement Learning (RL) solutions are training-time attacks (TTAs) that create loopholes and backdoors in the learned behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are now available, where the attacker forces a specific, target behaviour upon a training RL agent (victim). However, even state-of-the-art C-TTAs focus on target behaviours that could be naturally adopted by the victim if not for a particular feature of the environment dynamics, which C-TTAs exploit. In this work, we show that a C-TTA is possible even when the target behaviour is un-adoptable due to both environment dynamics as well as non-optimality with respect to the victim objective(s). To find efficient attacks in this context, we develop a specialised flavour of the DDPG algorithm, which we term gammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically alters the attack policy planning horizon based on the victim's current behaviour",
    "path": "papers/24/01/2401.02652.json",
    "total_tokens": 1001,
    "translated_title": "自适应折扣训练时间攻击",
    "translated_abstract": "在强化学习解决方案中，训练时间攻击（TTAs）是最阴险的攻击之一，可以在学习到的行为中制造漏洞和后门。现在已经有了不仅仅是简单破坏的建设性TTAs（C-TTAs），攻击者可以强制训练RL agent（受害者）表现出特定的目标行为。然而，即使是最先进的C-TTAs也只针对那些如果不因环境动态的一个特定特征被利用，受害者本可以自然地采纳的目标行为。在这项工作中，我们展示了即使目标行为由于环境动态和相对于受害者目标的非最优性而无法采纳，C-TTA也是可能的。为了在这种情况下找到高效的攻击方法，我们开发了一种特定的DDPG算法，称为gammaDDPG，用于学习这种更强版本的C-TTA。gammaDDPG根据受害者当前的行为动态改变攻击策略规划时间。",
    "tldr": "本研究展示了在存在环境动态和相对于受害者目标的非最优性时，即使目标行为无法被采纳，仍然可能进行C-TTA。我们开发了一种gammaDDPG算法来学习这种更强版本的C-TTA，并根据受害者当前的行为动态改变攻击策略规划时间。"
}