{
    "title": "Prevalidated ridge regression is a highly-efficient drop-in replacement for logistic regression for high-dimensional data. (arXiv:2401.15610v1 [cs.LG])",
    "abstract": "Logistic regression is a ubiquitous method for probabilistic classification. However, the effectiveness of logistic regression depends upon careful and relatively computationally expensive tuning, especially for the regularisation hyperparameter, and especially in the context of high-dimensional data. We present a prevalidated ridge regression model that closely matches logistic regression in terms of classification error and log-loss, particularly for high-dimensional data, while being significantly more computationally efficient and having effectively no hyperparameters beyond regularisation. We scale the coefficients of the model so as to minimise log-loss for a set of prevalidated predictions derived from the estimated leave-one-out cross-validation error. This exploits quantities already computed in the course of fitting the ridge regression model in order to find the scaling parameter with nominal additional computational expense.",
    "link": "http://arxiv.org/abs/2401.15610",
    "context": "Title: Prevalidated ridge regression is a highly-efficient drop-in replacement for logistic regression for high-dimensional data. (arXiv:2401.15610v1 [cs.LG])\nAbstract: Logistic regression is a ubiquitous method for probabilistic classification. However, the effectiveness of logistic regression depends upon careful and relatively computationally expensive tuning, especially for the regularisation hyperparameter, and especially in the context of high-dimensional data. We present a prevalidated ridge regression model that closely matches logistic regression in terms of classification error and log-loss, particularly for high-dimensional data, while being significantly more computationally efficient and having effectively no hyperparameters beyond regularisation. We scale the coefficients of the model so as to minimise log-loss for a set of prevalidated predictions derived from the estimated leave-one-out cross-validation error. This exploits quantities already computed in the course of fitting the ridge regression model in order to find the scaling parameter with nominal additional computational expense.",
    "path": "papers/24/01/2401.15610.json",
    "total_tokens": 857,
    "translated_title": "预验证的岭回归是高维数据中逻辑回归的高效替代方法",
    "translated_abstract": "逻辑回归是一种常见的概率分类方法。然而，逻辑回归的有效性取决于仔细且相对计算密集的调优，尤其是对于正则化超参数，并且尤其在高维数据的背景下。我们提出了一种预验证的岭回归模型，该模型在分类错误和对数损失方面与逻辑回归非常接近，特别适用于高维数据，同时在计算效率上明显更高，并且除了正则化之外没有超参数。我们通过缩放模型的系数来最小化由估计的留一交叉验证误差推导出的一组预验证预测的对数损失。这利用了在拟合岭回归模型过程中已经计算的数量，以找到具有名义附加计算开销的缩放参数。",
    "tldr": "本论文提出了一种预验证的岭回归模型，该模型在高维数据中与逻辑回归非常接近，但具有更高的计算效率和几乎没有超参数。它通过利用在拟合过程中计算得到的数量来缩放模型系数，并最小化一组预验证预测的对数损失。"
}