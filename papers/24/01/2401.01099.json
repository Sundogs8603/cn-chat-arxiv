{
    "title": "Efficient Parallel Audio Generation using Group Masked Language Modeling. (arXiv:2401.01099v1 [eess.AS])",
    "abstract": "We present a fast and high-quality codec language model for parallel audio generation. While SoundStorm, a state-of-the-art parallel audio generation model, accelerates inference speed compared to autoregressive models, it still suffers from slow inference due to iterative sampling. To resolve this problem, we propose Group-Masked Language Modeling~(G-MLM) and Group Iterative Parallel Decoding~(G-IPD) for efficient parallel audio generation. Both the training and sampling schemes enable the model to synthesize high-quality audio with a small number of iterations by effectively modeling the group-wise conditional dependencies. In addition, our model employs a cross-attention-based architecture to capture the speaker style of the prompt voice and improves computational efficiency. Experimental results demonstrate that our proposed model outperforms the baselines in prompt-based audio generation.",
    "link": "http://arxiv.org/abs/2401.01099",
    "context": "Title: Efficient Parallel Audio Generation using Group Masked Language Modeling. (arXiv:2401.01099v1 [eess.AS])\nAbstract: We present a fast and high-quality codec language model for parallel audio generation. While SoundStorm, a state-of-the-art parallel audio generation model, accelerates inference speed compared to autoregressive models, it still suffers from slow inference due to iterative sampling. To resolve this problem, we propose Group-Masked Language Modeling~(G-MLM) and Group Iterative Parallel Decoding~(G-IPD) for efficient parallel audio generation. Both the training and sampling schemes enable the model to synthesize high-quality audio with a small number of iterations by effectively modeling the group-wise conditional dependencies. In addition, our model employs a cross-attention-based architecture to capture the speaker style of the prompt voice and improves computational efficiency. Experimental results demonstrate that our proposed model outperforms the baselines in prompt-based audio generation.",
    "path": "papers/24/01/2401.01099.json",
    "total_tokens": 836,
    "translated_title": "有效的并行语音生成方法：使用组掩码语言模型",
    "translated_abstract": "我们提出了一种快速且高质量的编解码语言模型，用于并行语音生成。虽然SoundStorm是一种先进的并行语音生成模型，相比自回归模型加速了推理速度，但由于迭代采样，它仍然受到推理速度慢的影响。为了解决这个问题，我们提出了组掩码语言模型（G-MLM）和组迭代并行解码（G-IPD），用于有效的并行语音生成。训练和采样方案都能够通过有效建模组间条件依赖来在少数迭代次数内合成高质量的音频。此外，我们的模型采用了基于交叉注意力的架构，以捕捉提示语音的说话人风格并提高计算效率。实验结果表明，我们提出的模型在基于提示的语音生成方面优于基线模型。",
    "tldr": "我们提出了一种有效的并行语音生成方法，通过使用组掩码语言模型和组迭代并行解码，能够快速生成高质量的音频，并成功捕捉提示语音的说话人风格，提高了计算效率。"
}