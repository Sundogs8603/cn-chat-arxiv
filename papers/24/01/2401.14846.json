{
    "title": "Understanding Domain Generalization: A Noise Robustness Perspective. (arXiv:2401.14846v1 [cs.LG])",
    "abstract": "Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We co",
    "link": "http://arxiv.org/abs/2401.14846",
    "context": "Title: Understanding Domain Generalization: A Noise Robustness Perspective. (arXiv:2401.14846v1 [cs.LG])\nAbstract: Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We co",
    "path": "papers/24/01/2401.14846.json",
    "total_tokens": 999,
    "translated_title": "理解域泛化：从噪声鲁棒性的角度",
    "translated_abstract": "尽管快速发展了用于域泛化的机器学习算法，但没有明确的经验证据表明现有的域泛化算法在标准基准测试中优于经典的经验风险最小化（ERM）算法。为了更好地理解这一现象，我们通过标签噪声的视角研究了域泛化算法相对于ERM的优势。具体而言，我们的有限样本分析揭示了标签噪声加剧了ERM中假冗余相关性的影响，削弱了泛化能力。相反，我们证明了在有限样本训练中，域泛化算法在存在假冗余相关时具有隐性的标签噪声鲁棒性。这种有利性有助于减轻假冗余相关性并改善合成实验中的泛化能力。然而，对真实世界基准数据集进行的额外综合实验表明，标签噪声鲁棒性并不一定意味着相比ERM具有更好的性能。",
    "tldr": "本文通过研究域泛化算法和经验风险最小化算法在标签噪声下的表现，发现域泛化算法在有限样本训练中显示出隐性的标签噪声鲁棒性，有助于减轻假冗余相关和提高泛化能力，但在真实世界基准数据集上的实验证明，标签噪声鲁棒性并不一定意味着相比ERM具有更好的性能。",
    "en_tdlr": "This paper investigates the benefits of domain generalization algorithms over empirical risk minimization (ERM) under label noise. The study reveals that domain generalization algorithms show implicit label-noise robustness during finite-sample training, helping mitigate spurious correlations and improve generalization. However, experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily result in better performance compared to ERM."
}