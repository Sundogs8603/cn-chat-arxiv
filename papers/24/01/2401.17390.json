{
    "title": "Customizing Language Model Responses with Contrastive In-Context Learning",
    "abstract": "Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real",
    "link": "https://arxiv.org/abs/2401.17390",
    "context": "Title: Customizing Language Model Responses with Contrastive In-Context Learning\nAbstract: Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real",
    "path": "papers/24/01/2401.17390.json",
    "total_tokens": 856,
    "translated_title": "使用对比式上下文学习定制语言模型的回复",
    "translated_abstract": "大型语言模型 (LLMs) 对于机器学习应用变得越来越重要。然而，将LLMs与我们的意图对齐可能会具有挑战性，特别是当我们希望生成优于其他内容的内容，或者当我们希望LLMs以一种难以描述的风格或语气进行回应时。为了解决这个问题，我们提出了一种使用对比示例来更好地描述我们的意图的方法。这涉及提供正面示例来说明真实的意图，以及负面示例来展示我们希望LLMs避免的特征。负面示例可以从标记数据中检索，由人工编写，或由LLMs自动生成。在生成答案之前，我们要求模型分析这些示例，以教会自己避免什么。这个推理步骤为模型提供了与用户需求相关的适当表达，并引导其生成更好的答案。我们在合成和真实数据上测试了我们的方法。",
    "tldr": "本论文提出了一种使用对比示例来定制语言模型回复的方法，通过提供正面示例和负面示例，使模型学会如何回避负面特征，从而更好地满足用户需求。"
}