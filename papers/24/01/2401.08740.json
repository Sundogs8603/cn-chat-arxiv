{
    "title": "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers. (arXiv:2401.08740v1 [cs.CV])",
    "abstract": "We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: using discrete vs. continuous time learning, deciding the objective for the model to learn, choosing the interpolant connecting the distributions, and deploying a deterministic or stochastic sampler. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark using the exact same backbone, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06.",
    "link": "http://arxiv.org/abs/2401.08740",
    "context": "Title: SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers. (arXiv:2401.08740v1 [cs.CV])\nAbstract: We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: using discrete vs. continuous time learning, deciding the objective for the model to learn, choosing the interpolant connecting the distributions, and deploying a deterministic or stochastic sampler. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 benchmark using the exact same backbone, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06.",
    "path": "papers/24/01/2401.08740.json",
    "total_tokens": 893,
    "translated_title": "SiT:使用可扩展的插值仿射变换探索基于流动和扩散的生成模型",
    "translated_abstract": "我们提出了一种基于扩散变换器（DiT）骨干的可扩展插值仿射变换器（SiT），这是一种生成模型的系列。插值框架允许以比标准扩散模型更灵活的方式连接两个分布，使得可以对建立在动态传输上的生成模型的各种设计选择进行模块化研究：使用离散时间学习还是连续时间学习，决定模型学习的目标，选择连接分布的插值器，以及部署确定性还是随机采样器。通过精心引入上述元素，SiT在具有相同骨干、参数数量和GFLOPs的条件ImageNet 256x256基准测试中，在模型大小上全面超过了DiT。通过探索可以与学习分开调整的各种扩散系数，SiT在FID-50K评分上达到了2.06。",
    "tldr": "SiT是一种基于Diffusion Transformers骨干的生成模型，通过插值框架和各种设计选择的模块化研究，实现了在模型大小上超过DiT，在条件ImageNet基准测试中获得了较低的FID-50K评分。",
    "en_tdlr": "SiT is a generative model built on the backbone of Diffusion Transformers, which surpasses DiT in model size and achieves a lower FID-50K score on the conditional ImageNet benchmark through modular research with an interpolant framework and various design choices."
}