{
    "title": "M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining",
    "abstract": "Vision-language foundation models like CLIP have revolutionized the field of artificial intelligence. Nevertheless, VLM models supporting multi-language, e.g., in both Chinese and English, have lagged due to the relative scarcity of large-scale pretraining datasets. Toward this end, we introduce a comprehensive bilingual (Chinese-English) dataset BM-6B with over 6 billion image-text pairs, aimed at enhancing multimodal foundation models to well understand images in both languages. To handle such a scale of dataset, we propose a novel grouped aggregation approach for image-text contrastive loss computation, which reduces the communication overhead and GPU memory demands significantly, facilitating a 60% increase in training speed. We pretrain a series of bilingual image-text foundation models with an enhanced fine-grained understanding ability on BM-6B, the resulting models, dubbed as $M^2$-Encoders (pronounced \"M-Square\"), set new benchmarks in both languages for multimodal retrieval a",
    "link": "https://arxiv.org/abs/2401.15896",
    "context": "Title: M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining\nAbstract: Vision-language foundation models like CLIP have revolutionized the field of artificial intelligence. Nevertheless, VLM models supporting multi-language, e.g., in both Chinese and English, have lagged due to the relative scarcity of large-scale pretraining datasets. Toward this end, we introduce a comprehensive bilingual (Chinese-English) dataset BM-6B with over 6 billion image-text pairs, aimed at enhancing multimodal foundation models to well understand images in both languages. To handle such a scale of dataset, we propose a novel grouped aggregation approach for image-text contrastive loss computation, which reduces the communication overhead and GPU memory demands significantly, facilitating a 60% increase in training speed. We pretrain a series of bilingual image-text foundation models with an enhanced fine-grained understanding ability on BM-6B, the resulting models, dubbed as $M^2$-Encoders (pronounced \"M-Square\"), set new benchmarks in both languages for multimodal retrieval a",
    "path": "papers/24/01/2401.15896.json",
    "total_tokens": 1010,
    "translated_title": "M2-编码器：通过大规模高效预训练推进双语图像-文本理解",
    "translated_abstract": "像CLIP这样的视觉-语言基础模型已经在人工智能领域取得了突破。然而，支持多语言的VLM模型，例如中英文双语，由于大规模预训练数据集的相对稀缺而滞后。为此，我们引入了一个全面的中英双语（中英文）数据集BM-6B，包含超过60亿个图像-文本对，旨在增强多模态基础模型对两种语言中的图像进行深入理解。为了处理这样规模的数据集，我们提出了一种新颖的集合聚合方法来计算图像-文本对比损失，显著减少了通信开销和GPU内存需求，大大提高了训练速度。我们在BM-6B上预训练了一系列增强细粒度理解能力的中英双语图像-文本基础模型，得到的模型被称为M²-编码器（发音为“M-Square”），在两种语言的多模态检索中刷新了新的基准。",
    "tldr": "本论文介绍了M2-编码器，通过大规模高效预训练，推动了双语图像-文本理解的发展。我们引入了一个全面的中英双语数据集BM-6B，使用新的聚合方法减少了损失计算的通信开销和GPU内存需求，提高了训练速度。在BM-6B上预训练的M²-编码器在多语言的多模态检索中创造了新的基准。",
    "en_tdlr": "This paper proposes M2-Encoder, which advances bilingual image-text understanding through large-scale efficient pretraining. The authors introduce the comprehensive bilingual dataset BM-6B and use a novel aggregation approach to reduce loss computation overhead and accelerate training. The M²-Encoders pretrained on BM-6B set new benchmarks in multimodal retrieval in both languages."
}