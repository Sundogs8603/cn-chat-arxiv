{
    "title": "Budgeted Online Model Selection and Fine-Tuning via Federated Learning. (arXiv:2401.10478v1 [cs.LG])",
    "abstract": "Online model selection involves selecting a model from a set of candidate models 'on the fly' to perform prediction on a stream of data. The choice of candidate models henceforth has a crucial impact on the performance. Although employing a larger set of candidate models naturally leads to more flexibility in model selection, this may be infeasible in cases where prediction tasks are performed on edge devices with limited memory. Faced with this challenge, the present paper proposes an online federated model selection framework where a group of learners (clients) interacts with a server with sufficient memory such that the server stores all candidate models. However, each client only chooses to store a subset of models that can be fit into its memory and performs its own prediction task using one of the stored models. Furthermore, employing the proposed algorithm, clients and the server collaborate to fine-tune models to adapt them to a non-stationary environment. Theoretical analysis ",
    "link": "http://arxiv.org/abs/2401.10478",
    "context": "Title: Budgeted Online Model Selection and Fine-Tuning via Federated Learning. (arXiv:2401.10478v1 [cs.LG])\nAbstract: Online model selection involves selecting a model from a set of candidate models 'on the fly' to perform prediction on a stream of data. The choice of candidate models henceforth has a crucial impact on the performance. Although employing a larger set of candidate models naturally leads to more flexibility in model selection, this may be infeasible in cases where prediction tasks are performed on edge devices with limited memory. Faced with this challenge, the present paper proposes an online federated model selection framework where a group of learners (clients) interacts with a server with sufficient memory such that the server stores all candidate models. However, each client only chooses to store a subset of models that can be fit into its memory and performs its own prediction task using one of the stored models. Furthermore, employing the proposed algorithm, clients and the server collaborate to fine-tune models to adapt them to a non-stationary environment. Theoretical analysis ",
    "path": "papers/24/01/2401.10478.json",
    "total_tokens": 868,
    "translated_title": "基于预算的在线模型选择和调优通过联邦学习",
    "translated_abstract": "在线模型选择涉及从候选模型集合中选择一个模型“即时”对数据流进行预测。候选模型的选择对性能有重要影响。虽然使用更大的候选模型集合自然在模型选择中具有更灵活性，但在预测任务在内存有限的边缘设备上执行时，这可能是不可行的。面临这一挑战，本文提出了一种在线联邦模型选择框架，其中一组学习者（客户端）与具有足够内存的服务器进行交互，使得服务器存储所有候选模型。然而，每个客户端只选择存储可以适配其内存的模型子集，并使用其中一个存储模型执行自己的预测任务。此外，使用所提出的算法，客户端和服务器合作对模型进行微调以调整到非平稳环境。",
    "tldr": "本文提出了一种基于预算的在线模型选择和调优框架，通过联邦学习实现。该框架允许在内存受限的边缘设备上进行模型选择，并实现了客户端和服务器的合作来对模型进行适应非平稳环境的调优。",
    "en_tdlr": "This paper proposes a budgeted online model selection and fine-tuning framework using federated learning. The framework allows for model selection on edge devices with limited memory and enables collaboration between clients and servers to adapt models to non-stationary environments."
}