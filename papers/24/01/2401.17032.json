{
    "title": "M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation. (arXiv:2401.17032v1 [cs.RO])",
    "abstract": "One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities. Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms. However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives. To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms. Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm. We evaluate M2CURL on the Tactile Gym 2 simulator ",
    "link": "http://arxiv.org/abs/2401.17032",
    "context": "Title: M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation. (arXiv:2401.17032v1 [cs.RO])\nAbstract: One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities. Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms. However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives. To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms. Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm. We evaluate M2CURL on the Tactile Gym 2 simulator ",
    "path": "papers/24/01/2401.17032.json",
    "total_tokens": 862,
    "translated_title": "M2CURL: 通过自监督表示学习实现样本高效的多模态强化学习，用于机器人操纵",
    "translated_abstract": "多模态强化学习中最重要的方面之一是有效地整合不同的观测模态。从这些模态中得到稳健准确的表示对于提升强化学习算法的鲁棒性和样本效率至关重要。然而，在视触觉数据的强化学习环境中学习表示面临着重要挑战，特别是由于数据的高维度和将视触觉输入与动态环境和任务目标进行相关性分析的复杂性。为了解决这些挑战，我们提出了多模态对比无监督强化学习（M2CURL）。我们的方法采用了一种新颖的多模态自监督学习技术，学习出高效的表示并加速了强化学习算法的收敛。我们的方法与强化学习算法无关，因此可以与任何可用的强化学习算法进行整合。我们在Tactile Gym 2模拟器上评估了M2CURL。",
    "tldr": "M2CURL是一种样本高效的多模态强化学习方法，通过自监督表示学习从视触觉数据中学习出高效的表示，并加速强化学习算法的收敛。"
}