{
    "title": "Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies. (arXiv:2401.06703v1 [cs.IR])",
    "abstract": "We explore leveraging corpus-specific vocabularies that improve both efficiency and effectiveness of learned sparse retrieval systems. We find that pre-training the underlying BERT model on the target corpus, specifically targeting different vocabulary sizes incorporated into the document expansion process, improves retrieval quality by up to 12% while in some scenarios decreasing latency by up to 50%. Our experiments show that adopting corpus-specific vocabulary and increasing vocabulary size decreases average postings list length which in turn reduces latency. Ablation studies show interesting interactions between custom vocabularies, document expansion techniques, and sparsification objectives of sparse models. Both effectiveness and efficiency improvements transfer to different retrieval approaches such as uniCOIL and SPLADE and offer a simple yet effective approach to providing new efficiency-effectiveness trade-offs for learned sparse retrieval systems.",
    "link": "http://arxiv.org/abs/2401.06703",
    "context": "Title: Improved Learned Sparse Retrieval with Corpus-Specific Vocabularies. (arXiv:2401.06703v1 [cs.IR])\nAbstract: We explore leveraging corpus-specific vocabularies that improve both efficiency and effectiveness of learned sparse retrieval systems. We find that pre-training the underlying BERT model on the target corpus, specifically targeting different vocabulary sizes incorporated into the document expansion process, improves retrieval quality by up to 12% while in some scenarios decreasing latency by up to 50%. Our experiments show that adopting corpus-specific vocabulary and increasing vocabulary size decreases average postings list length which in turn reduces latency. Ablation studies show interesting interactions between custom vocabularies, document expansion techniques, and sparsification objectives of sparse models. Both effectiveness and efficiency improvements transfer to different retrieval approaches such as uniCOIL and SPLADE and offer a simple yet effective approach to providing new efficiency-effectiveness trade-offs for learned sparse retrieval systems.",
    "path": "papers/24/01/2401.06703.json",
    "total_tokens": 1011,
    "translated_title": "借助特定于语料库的词汇库改进了学习稀疏检索",
    "translated_abstract": "我们探索了借助特定于语料库的词汇库来改善学习稀疏检索系统的效率和效果。我们发现，将底层BERT模型预训练于目标语料库，并针对文档扩展过程中不同的词汇库大小进行特定调整，可以将检索质量提高最多12％，同时在某些情况下将延迟降低最多50％。我们的实验证明，采用特定于语料库的词汇库并增加词汇库大小可以减少平均倒排列表长度，从而降低延迟。消融研究显示了自定义词汇库、文档扩展技术和稀疏模型的稀疏化目标之间的有趣互动。效果和效率的改进可以迁移到不同的检索方法，如uniCOIL和SPLADE，并提供了一种简单而有效的方法来提供学习稀疏检索系统的新的效率-效果权衡。",
    "tldr": "本论文研究了利用特定于语料库的词汇库改进了学习稀疏检索系统的效率和效果。通过将底层BERT模型针对不同词汇库大小进行预训练，可以显著提高检索质量并减少延迟。该研究还探讨了自定义词汇库、文档扩展技术和稀疏模型的相互作用。这种方法在不同的检索方法中都能取得效果和效率上的改进。"
}