{
    "title": "Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing Approach For Uncovering Edge Cases with Minimal Distribution Distortion",
    "abstract": "Adversarial attacks against language models(LMs) are a significant concern. In particular, adversarial samples exploit the model's sensitivity to small input changes. While these changes appear insignificant on the semantics of the input sample, they result in significant decay in model performance. In this paper, we propose Targeted Paraphrasing via RL (TPRL), an approach to automatically learn a policy to generate challenging samples that most likely improve the model's performance. TPRL leverages FLAN T5, a language model, as a generator and employs a self learned policy using a proximal policy gradient to generate the adversarial examples automatically. TPRL's reward is based on the confusion induced in the classifier, preserving the original text meaning through a Mutual Implication score. We demonstrate and evaluate TPRL's effectiveness in discovering natural adversarial attacks and improving model performance through extensive experiments on four diverse NLP classification tasks",
    "link": "https://arxiv.org/abs/2401.11373",
    "context": "Title: Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing Approach For Uncovering Edge Cases with Minimal Distribution Distortion\nAbstract: Adversarial attacks against language models(LMs) are a significant concern. In particular, adversarial samples exploit the model's sensitivity to small input changes. While these changes appear insignificant on the semantics of the input sample, they result in significant decay in model performance. In this paper, we propose Targeted Paraphrasing via RL (TPRL), an approach to automatically learn a policy to generate challenging samples that most likely improve the model's performance. TPRL leverages FLAN T5, a language model, as a generator and employs a self learned policy using a proximal policy gradient to generate the adversarial examples automatically. TPRL's reward is based on the confusion induced in the classifier, preserving the original text meaning through a Mutual Implication score. We demonstrate and evaluate TPRL's effectiveness in discovering natural adversarial attacks and improving model performance through extensive experiments on four diverse NLP classification tasks",
    "path": "papers/24/01/2401.11373.json",
    "total_tokens": 915,
    "translated_title": "在对抗的草堆中找到针：一种针对最小分布失真的边缘情况的目标改写方法",
    "translated_abstract": "对语言模型(LMs)进行对抗攻击是一个重要的问题。特别是，对抗样本利用模型对输入的微小变化的敏感性。虽然这些变化对输入样本的语义来说似乎微不足道，但却导致模型性能的显著下降。本文提出了一种称为Targeted Paraphrasing via RL (TPRL)的方法，通过自动学习生成具有挑战性的样本的策略，从而最可能提高模型的性能。TPRL利用FLAN T5作为生成器，并使用近端策略梯度来自动生成对抗性样本。TPRL的奖励基于分类器中引发的困惑程度，通过Mutual Implication分数保留原始文本的含义。我们通过对四个不同的自然语言处理分类任务进行大量实验，证明了TPRL发现自然对抗攻击和提高模型性能的有效性。",
    "tldr": "本文提出了一种名为TPRL的方法，通过生成具有挑战性的样本来改善自然语言处理模型的性能，以对抗攻击中的分布失真问题。该方法利用近端策略梯度自动生成对抗性样本，并通过Mutual Implication分数保持原始文本的语义含义。",
    "en_tdlr": "This paper proposes a method called TPRL to improve the performance of natural language processing models against adversarial attacks by generating challenging samples to tackle the problem of distribution distortion. The method automatically generates adversarial examples using proximal policy gradient and preserves the semantic meaning of the original text through Mutual Implication scores."
}