{
    "title": "Curiosity & Entropy Driven Unsupervised RL in Multiple Environments. (arXiv:2401.04198v1 [cs.LG])",
    "abstract": "The authors of 'Unsupervised Reinforcement Learning in Multiple environments' propose a method, alpha-MEPOL, to tackle unsupervised RL across multiple environments. They pre-train a task-agnostic exploration policy using interactions from an entire environment class and then fine-tune this policy for various tasks using supervision. We expanded upon this work, with the goal of improving performance. We primarily propose and experiment with five new modifications to the original work: sampling trajectories using an entropy-based probability distribution, dynamic alpha, higher KL Divergence threshold, curiosity-driven exploration, and alpha-percentile sampling on curiosity. Dynamic alpha and higher KL-Divergence threshold both provided a significant improvement over the baseline from the earlier work. PDF-sampling failed to provide any improvement due to it being approximately equivalent to the baseline method when the sample space is small. In high-dimensional environments, the addition",
    "link": "http://arxiv.org/abs/2401.04198",
    "context": "Title: Curiosity & Entropy Driven Unsupervised RL in Multiple Environments. (arXiv:2401.04198v1 [cs.LG])\nAbstract: The authors of 'Unsupervised Reinforcement Learning in Multiple environments' propose a method, alpha-MEPOL, to tackle unsupervised RL across multiple environments. They pre-train a task-agnostic exploration policy using interactions from an entire environment class and then fine-tune this policy for various tasks using supervision. We expanded upon this work, with the goal of improving performance. We primarily propose and experiment with five new modifications to the original work: sampling trajectories using an entropy-based probability distribution, dynamic alpha, higher KL Divergence threshold, curiosity-driven exploration, and alpha-percentile sampling on curiosity. Dynamic alpha and higher KL-Divergence threshold both provided a significant improvement over the baseline from the earlier work. PDF-sampling failed to provide any improvement due to it being approximately equivalent to the baseline method when the sample space is small. In high-dimensional environments, the addition",
    "path": "papers/24/01/2401.04198.json",
    "total_tokens": 829,
    "translated_title": "无指导下多环境中的好奇心与熵驱动强化学习",
    "translated_abstract": "本论文的作者们提出了一种名为alpha-MEPOL的方法来解决多环境下的无指导强化学习问题。他们通过使用整个环境类别的交互来预训练一个与任务无关的探索策略，然后利用监督来对各种任务进行微调。我们在原始工作的基础上进行了扩展，旨在提高性能。我们主要提出并实验了五个新的修改方法：使用基于熵的概率分布来采样轨迹，动态alpha，更高的KL散度阈值，以好奇心驱动的探索，以及基于好奇心的alpha分位数采样。动态alpha和更高的KL散度阈值都相对于早期工作的基线方法有了显著的改进。当样本空间较小时，PDF采样没有提供任何改进，因为它与基线方法近似等价。在高维环境中，这些修改方法的添加提高了性能。",
    "tldr": "本论文提出了一种无指导的强化学习方法，在多个环境下通过好奇心和熵驱动实现了性能的改进。"
}