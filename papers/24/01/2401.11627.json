{
    "title": "Tight Verification of Probabilistic Robustness in Bayesian Neural Networks",
    "abstract": "arXiv:2401.11627v2 Announce Type: replace-cross  Abstract: We introduce two algorithms for computing tight guarantees on the probabilistic robustness of Bayesian Neural Networks (BNNs). Computing robustness guarantees for BNNs is a significantly more challenging task than verifying the robustness of standard Neural Networks (NNs) because it requires searching the parameters' space for safe weights. Moreover, tight and complete approaches for the verification of standard NNs, such as those based on Mixed-Integer Linear Programming (MILP), cannot be directly used for the verification of BNNs because of the polynomial terms resulting from the consecutive multiplication of variables encoding the weights. Our algorithms efficiently and effectively search the parameters' space for safe weights by using iterative expansion and the network's gradient and can be used with any verification algorithm of choice for BNNs. In addition to proving that our algorithms compute tighter bounds than the So",
    "link": "https://arxiv.org/abs/2401.11627",
    "context": "Title: Tight Verification of Probabilistic Robustness in Bayesian Neural Networks\nAbstract: arXiv:2401.11627v2 Announce Type: replace-cross  Abstract: We introduce two algorithms for computing tight guarantees on the probabilistic robustness of Bayesian Neural Networks (BNNs). Computing robustness guarantees for BNNs is a significantly more challenging task than verifying the robustness of standard Neural Networks (NNs) because it requires searching the parameters' space for safe weights. Moreover, tight and complete approaches for the verification of standard NNs, such as those based on Mixed-Integer Linear Programming (MILP), cannot be directly used for the verification of BNNs because of the polynomial terms resulting from the consecutive multiplication of variables encoding the weights. Our algorithms efficiently and effectively search the parameters' space for safe weights by using iterative expansion and the network's gradient and can be used with any verification algorithm of choice for BNNs. In addition to proving that our algorithms compute tighter bounds than the So",
    "path": "papers/24/01/2401.11627.json",
    "total_tokens": 792,
    "translated_title": "Bayesian神经网络中概率鲁棒性的严格验证",
    "translated_abstract": "我们介绍了两种用于计算Bayesian神经网络（BNNs）的概率鲁棒性上严格保证的算法。计算BNNs的鲁棒性保证要比验证标准神经网络（NNs）的鲁棒性困难得多，因为它需要在参数空间中搜索安全权重。此外，标准NNs验证的紧密和完整方法，例如基于混合整数线性规划（MILP）的方法，不能直接用于BNNs的验证，因为由于编码权重的变量连续相乘而产生的多项式项。我们的算法通过使用迭代扩展和网络的梯度有效地搜索参数空间以寻找安全权重，并且可以与BNNs的任何验证算法一起使用。",
    "tldr": "通过引入两种算法，实现了对Bayesian神经网络概率鲁棒性的严格验证，相比标准神经网络，这些算法更加高效且能够搜索参数空间以找到安全权重。",
    "en_tdlr": "Tight guarantees on the probabilistic robustness of Bayesian Neural Networks are efficiently computed through the introduction of two algorithms, showcasing superior efficiency compared to standard Neural Networks by effectively searching the parameter space for safe weights."
}