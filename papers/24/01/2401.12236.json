{
    "title": "The Surprising Harmfulness of Benign Overfitting for Adversarial Robustness. (arXiv:2401.12236v1 [cs.LG])",
    "abstract": "Recent empirical and theoretical studies have established the generalization capabilities of large machine learning models that are trained to (approximately or exactly) fit noisy data. In this work, we prove a surprising result that even if the ground truth itself is robust to adversarial examples, and the benignly overfitted model is benign in terms of the ``standard'' out-of-sample risk objective, this benign overfitting process can be harmful when out-of-sample data are subject to adversarial manipulation. More specifically, our main results contain two parts: (i) the min-norm estimator in overparameterized linear model always leads to adversarial vulnerability in the ``benign overfitting'' setting; (ii) we verify an asymptotic trade-off result between the standard risk and the ``adversarial'' risk of every ridge regression estimator, implying that under suitable conditions these two items cannot both be small at the same time by any single choice of the ridge regularization parame",
    "link": "http://arxiv.org/abs/2401.12236",
    "context": "Title: The Surprising Harmfulness of Benign Overfitting for Adversarial Robustness. (arXiv:2401.12236v1 [cs.LG])\nAbstract: Recent empirical and theoretical studies have established the generalization capabilities of large machine learning models that are trained to (approximately or exactly) fit noisy data. In this work, we prove a surprising result that even if the ground truth itself is robust to adversarial examples, and the benignly overfitted model is benign in terms of the ``standard'' out-of-sample risk objective, this benign overfitting process can be harmful when out-of-sample data are subject to adversarial manipulation. More specifically, our main results contain two parts: (i) the min-norm estimator in overparameterized linear model always leads to adversarial vulnerability in the ``benign overfitting'' setting; (ii) we verify an asymptotic trade-off result between the standard risk and the ``adversarial'' risk of every ridge regression estimator, implying that under suitable conditions these two items cannot both be small at the same time by any single choice of the ridge regularization parame",
    "path": "papers/24/01/2401.12236.json",
    "total_tokens": 935,
    "translated_title": "无害过度拟合对敌对鲁棒性的意外危害",
    "translated_abstract": "最近的实证和理论研究已经证明了大规模机器学习模型对训练噪声数据的泛化能力。在本文中，我们证明了一个令人惊讶的结果：即使真正的数据本身对敌对示例具有鲁棒性，而且过度拟合的模型在“标准”的样本外风险目标上是无害的，但在样本外数据受到敌对操纵时，这种无害的过度拟合过程可能是有害的。具体而言，我们的主要结果包含两个部分：（i）在过度参数化线性模型中，最小范数估计总是在“无害过度拟合”设置中导致敌对易受攻击；（ii）我们验证了每个岭回归估计器的标准风险和“敌对”风险之间的渐进权衡结果，这意味着在适当的条件下，这两个项目不能同时通过任何单个岭正则化参数的选择来保持很小。",
    "tldr": "这项研究证明了即使机器学习模型在训练过程中对噪声数据拟合得很好，对敌对示例具有鲁棒性，但当面临敌对操纵的数据时，过度拟合的模型可能会给系统带来意外的危害。",
    "en_tdlr": "This study demonstrates that even if machine learning models fit noisy data well and exhibit robustness against adversarial examples during training, they may still pose surprising harm when facing adversarial manipulation of out-of-sample data."
}