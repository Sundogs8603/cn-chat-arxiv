{
    "title": "Misconfidence-based Demonstration Selection for LLM In-Context Learning. (arXiv:2401.06301v1 [cs.CL])",
    "abstract": "In-context learning with large language models (LLMs) excels at adapting to various tasks rapidly. However, its success hinges on carefully selecting demonstrations, which remains an obstacle in practice. Current approaches to this problem either rely on hard-to-acquire external supervision or require frequent interactions with LLMs, resulting in high costs. We propose a new method called In-Context Reflection (ICR) to overcome these challenges. ICR strategically selects demonstrations to reduce the discrepancy between the LLM's outputs and the actual input-output mappings. Specifically, ICR starts with a random set of initial demonstrations, then iteratively refines it. In each step, it analyzes a pool of candidate examples and identifies the ones most likely to challenge the LLM's current understanding, measured by a new metric called misconfidence. These most confusing examples are then selected to replace the less informative demonstrations in the current set. Our comprehensive eva",
    "link": "http://arxiv.org/abs/2401.06301",
    "context": "Title: Misconfidence-based Demonstration Selection for LLM In-Context Learning. (arXiv:2401.06301v1 [cs.CL])\nAbstract: In-context learning with large language models (LLMs) excels at adapting to various tasks rapidly. However, its success hinges on carefully selecting demonstrations, which remains an obstacle in practice. Current approaches to this problem either rely on hard-to-acquire external supervision or require frequent interactions with LLMs, resulting in high costs. We propose a new method called In-Context Reflection (ICR) to overcome these challenges. ICR strategically selects demonstrations to reduce the discrepancy between the LLM's outputs and the actual input-output mappings. Specifically, ICR starts with a random set of initial demonstrations, then iteratively refines it. In each step, it analyzes a pool of candidate examples and identifies the ones most likely to challenge the LLM's current understanding, measured by a new metric called misconfidence. These most confusing examples are then selected to replace the less informative demonstrations in the current set. Our comprehensive eva",
    "path": "papers/24/01/2401.06301.json",
    "total_tokens": 910,
    "translated_title": "基于自信度的LLM上下文学习中的演示选择",
    "translated_abstract": "借助大型语言模型（LLMs）进行上下文学习在快速适应各种任务方面表现出色。然而，其成功关键在于仔细选择演示，这在实践中仍然是一个障碍。目前解决这个问题的方法要么依赖于难以获取的外部监督，要么需要频繁与LLMs进行交互，导致成本高昂。我们提出了一种新的方法，称为上下文反思（ICR），以解决这些挑战。ICR通过策略性地选择演示来减少LLM输出与实际输入输出映射之间的差异。具体而言，ICR从随机的初始演示集开始，并逐步进行优化。在每个步骤中，它分析候选示例池，并通过一种称为\"自信度\"的新指标来确定最有可能挑战LLM当前理解的示例。然后选择这些最困惑的示例以替换当前集合中信息较少的演示。我们进行了全面的评估",
    "tldr": "这项研究提出了一种名为ICR的方法，通过基于自信度的策略性演示选择，以降低LLM输出与实际输入输出之间的差异，从而克服了当前LLM上下文学习中演示选择的难题。",
    "en_tdlr": "This study proposes a method called ICR, which strategically selects demonstrations based on misconfidence to reduce the discrepancy between LLM outputs and actual input-output mappings, overcoming the challenge of demonstration selection in current LLM in-context learning."
}