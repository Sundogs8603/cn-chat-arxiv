{
    "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. (arXiv:2401.01313v1 [cs.CL])",
    "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applica",
    "link": "http://arxiv.org/abs/2401.01313",
    "context": "Title: A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. (arXiv:2401.01313v1 [cs.CL])\nAbstract: As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applica",
    "path": "papers/24/01/2401.01313.json",
    "total_tokens": 876,
    "translated_title": "大型语言模型中幻觉缓解技术的综述",
    "translated_abstract": "随着大型语言模型（LLMs）在生成人类化文本方面的能力不断提高，一个关键挑战是它们倾向于产生虚构的内容，看似真实但没有依据。幻觉问题可以说是安全地将这些强大的LLMs部署到影响人们生活的现实生产系统中最大的障碍。在实际环境中广泛采用LLMs的过程严重依赖于解决和减轻幻觉。与传统的专注于有限任务的人工智能系统不同，LLMs在训练过程中可以接触到大量的在线文本数据。这使它们能够展示出令人印象深刻的语言流利性，但也意味着它们能够从训练数据的偏见中推断信息，错误解释含糊不清的提示，或者修改信息以表面上与输入一致。当我们依赖语言生成能力来完成敏感应用时，这变得非常令人担忧。",
    "tldr": "大型语言模型在生成文本时容易出现幻觉，这是安全部署这些模型的最大障碍。解决幻觉问题对于在实际环境中广泛使用这些模型至关重要。",
    "en_tdlr": "The biggest challenge in deploying large language models in real-world systems is their tendency to generate hallucinations. Addressing this issue is crucial for their widespread adoption."
}