{
    "title": "Hallucinations in Neural Automatic Speech Recognition: Identifying Errors and Hallucinatory Models. (arXiv:2401.01572v1 [cs.CL])",
    "abstract": "Hallucinations are a type of output error produced by deep neural networks. While this has been studied in natural language processing, they have not been researched previously in automatic speech recognition. Here, we define hallucinations in ASR as transcriptions generated by a model that are semantically unrelated to the source utterance, yet still fluent and coherent. The similarity of hallucinations to probable natural language outputs of the model creates a danger of deception and impacts the credibility of the system. We show that commonly used metrics, such as word error rates, cannot differentiate between hallucinatory and non-hallucinatory models. To address this, we propose a perturbation-based method for assessing the susceptibility of an automatic speech recognition (ASR) model to hallucination at test time, which does not require access to the training dataset. We demonstrate that this method helps to distinguish between hallucinatory and non-hallucinatory models that hav",
    "link": "http://arxiv.org/abs/2401.01572",
    "context": "Title: Hallucinations in Neural Automatic Speech Recognition: Identifying Errors and Hallucinatory Models. (arXiv:2401.01572v1 [cs.CL])\nAbstract: Hallucinations are a type of output error produced by deep neural networks. While this has been studied in natural language processing, they have not been researched previously in automatic speech recognition. Here, we define hallucinations in ASR as transcriptions generated by a model that are semantically unrelated to the source utterance, yet still fluent and coherent. The similarity of hallucinations to probable natural language outputs of the model creates a danger of deception and impacts the credibility of the system. We show that commonly used metrics, such as word error rates, cannot differentiate between hallucinatory and non-hallucinatory models. To address this, we propose a perturbation-based method for assessing the susceptibility of an automatic speech recognition (ASR) model to hallucination at test time, which does not require access to the training dataset. We demonstrate that this method helps to distinguish between hallucinatory and non-hallucinatory models that hav",
    "path": "papers/24/01/2401.01572.json",
    "total_tokens": 882,
    "translated_title": "神经自动语音识别中的幻听：识别错误和幻觉模型",
    "translated_abstract": "幻听是由深度神经网络产生的一种输出错误。虽然这在自然语言处理中已经被研究过，但在自动语音识别中以前还没有进行过研究。在这里，我们将ASR中的幻听定义为模型生成的转录与源话语不相关，但仍流畅和连贯的语义。幻听与模型可能的自然语言输出的相似性，会产生欺骗的危险，影响系统的可信度。我们证明了常用的指标，如词错误率，无法区分幻觉和非幻觉模型。为了解决这个问题，我们提出了一种基于扰动的方法，用于在测试时评估自动语音识别（ASR）系统对幻听的敏感性，该方法不需要访问训练数据集。我们证明了这种方法有助于区分幻觉和非幻觉模型。",
    "tldr": "该论文研究了神经自动语音识别中的幻听问题，并提出了一种用于在测试时评估幻听敏感性的基于扰动的方法，该方法能够帮助区分幻觉和非幻觉模型。"
}