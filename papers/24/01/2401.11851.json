{
    "title": "BETA: Binarized Energy-Efficient Transformer Accelerator at the Edge. (arXiv:2401.11851v2 [cs.AR] UPDATED)",
    "abstract": "Existing binary Transformers are promising in edge deployment due to their compact model size, low computational complexity, and considerable inference accuracy. However, deploying binary Transformers faces challenges on prior processors due to inefficient execution of quantized matrix multiplication (QMM) and the energy consumption overhead caused by multi-precision activations. To tackle the challenges above, we first develop a computation flow abstraction method for binary Transformers to improve QMM execution efficiency by optimizing the computation order. Furthermore, a binarized energy-efficient Transformer accelerator, namely BETA, is proposed to boost the efficient deployment at the edge. Notably, BETA features a configurable QMM engine, accommodating diverse activation precisions of binary Transformers and offering high-parallelism and high-speed for QMMs with impressive energy efficiency. Experimental results evaluated on ZCU102 FPGA show BETA achieves an average energy effic",
    "link": "http://arxiv.org/abs/2401.11851",
    "context": "Title: BETA: Binarized Energy-Efficient Transformer Accelerator at the Edge. (arXiv:2401.11851v2 [cs.AR] UPDATED)\nAbstract: Existing binary Transformers are promising in edge deployment due to their compact model size, low computational complexity, and considerable inference accuracy. However, deploying binary Transformers faces challenges on prior processors due to inefficient execution of quantized matrix multiplication (QMM) and the energy consumption overhead caused by multi-precision activations. To tackle the challenges above, we first develop a computation flow abstraction method for binary Transformers to improve QMM execution efficiency by optimizing the computation order. Furthermore, a binarized energy-efficient Transformer accelerator, namely BETA, is proposed to boost the efficient deployment at the edge. Notably, BETA features a configurable QMM engine, accommodating diverse activation precisions of binary Transformers and offering high-parallelism and high-speed for QMMs with impressive energy efficiency. Experimental results evaluated on ZCU102 FPGA show BETA achieves an average energy effic",
    "path": "papers/24/01/2401.11851.json",
    "total_tokens": 871,
    "translated_title": "BETA：边缘二值化节能Transformer加速器",
    "translated_abstract": "现有的二值化Transformer由于其紧凑的模型大小、低计算复杂度和可观的推理准确性，在边缘部署中具有潜力。然而，部署二值化Transformer面临着先前处理器的挑战，由于量化矩阵乘法(QMM)的低效执行和多精度激活带来的能耗开销。为了解决以上挑战，我们首先为二值化Transformer开发了一种计算流抽象方法，通过优化计算顺序来改善QMM的执行效率。此外，还提出了一种名为BETA的二值化节能Transformer加速器，以提升在边缘的高效部署。值得注意的是，BETA具有可配置的QMM引擎，适应二值化Transformer的不同激活精度，并为具有出色能效的QMM提供高并行性和高速度。在ZCU102 FPGA上进行的实验结果表明，BETA实现了平均能效。",
    "tldr": "二值化Transformer在边缘部署中有潜力，但面临QMM执行效率低和能耗开销高的挑战。本研究提出了一种计算流抽象方法优化QMM执行效率，并开发了一种二值化节能Transformer加速器BETA，具有可配置的高并行性和高能效的QMM引擎。实验结果表明BETA在ZCU102 FPGA上实现了平均能效。"
}