{
    "title": "Bridging the Preference Gap between Retrievers and LLMs",
    "abstract": "arXiv:2401.06954v2 Announce Type: replace  Abstract: Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-\"friendly\" information and assembling a LLM-\"friendly\" context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and per",
    "link": "https://arxiv.org/abs/2401.06954",
    "context": "Title: Bridging the Preference Gap between Retrievers and LLMs\nAbstract: arXiv:2401.06954v2 Announce Type: replace  Abstract: Large Language Models (LLMs) have demonstrated superior results across a wide range of tasks, and Retrieval-augmented Generation (RAG) is an effective way to enhance the performance by locating relevant information and placing it into the context window of the LLM. However, the relationship between retrievers and LLMs in a RAG is still under-investigated. Most existing work treats the retriever and the LLM as independent components and leaves a gap between retrieving human-\"friendly\" information and assembling a LLM-\"friendly\" context. In this work, we examine a novel bridge mechanism. We validate the ranking and selection assumptions of retrievers in the context of RAG and propose a framework that chains together supervised and reinforcement learning to train a bridge model that optimizes the connection between the retriever and the LLM. Empirical results demonstrate the effectiveness of our method in both question-answering and per",
    "path": "papers/24/01/2401.06954.json",
    "total_tokens": 854,
    "translated_title": "消除检索器和大型语言模型之间的偏好差距",
    "translated_abstract": "大型语言模型（LLMs）已在各种任务中展示出卓越的结果，而检索增强生成（RAG）是通过定位相关信息并将其放入LLM的上下文窗口来提高性能的有效方式。然而，在RAG中，检索器和LLMs之间的关系仍未得到充分调查。大多数现有工作将检索器和LLM视为独立组件，并在检索人性化信息和组装LLM友好上下文之间留下了差距。在这项工作中，我们研究了一种新颖的桥梁机制。我们验证了检索器在RAG环境中的排名和选择假设，并提出了一个框架，通过将监督学习和强化学习链接在一起来训练一个桥梁模型，优化检索器和LLM之间的连接。实证结果证明了我们的方法在问答和...",
    "tldr": "本研究提出了一种新颖的桥梁机制，通过训练一个桥梁模型来优化检索器和LLM之间的连接，消除了在RAG中检索器和LLMs之间的偏好差距。"
}