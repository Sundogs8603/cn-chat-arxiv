{
    "title": "Verifying Relational Explanations: A Probabilistic Approach. (arXiv:2401.02703v1 [cs.AI])",
    "abstract": "Explanations on relational data are hard to verify since the explanation structures are more complex (e.g. graphs). To verify interpretable explanations (e.g. explanations of predictions made in images, text, etc.), typically human subjects are used since it does not necessarily require a lot of expertise. However, to verify the quality of a relational explanation requires expertise and is hard to scale-up. GNNExplainer is arguably one of the most popular explanation methods for Graph Neural Networks. In this paper, we develop an approach where we assess the uncertainty in explanations generated by GNNExplainer. Specifically, we ask the explainer to generate explanations for several counterfactual examples. We generate these examples as symmetric approximations of the relational structure in the original data. From these explanations, we learn a factor graph model to quantify uncertainty in an explanation. Our results on several datasets show that our approach can help verify explanati",
    "link": "http://arxiv.org/abs/2401.02703",
    "context": "Title: Verifying Relational Explanations: A Probabilistic Approach. (arXiv:2401.02703v1 [cs.AI])\nAbstract: Explanations on relational data are hard to verify since the explanation structures are more complex (e.g. graphs). To verify interpretable explanations (e.g. explanations of predictions made in images, text, etc.), typically human subjects are used since it does not necessarily require a lot of expertise. However, to verify the quality of a relational explanation requires expertise and is hard to scale-up. GNNExplainer is arguably one of the most popular explanation methods for Graph Neural Networks. In this paper, we develop an approach where we assess the uncertainty in explanations generated by GNNExplainer. Specifically, we ask the explainer to generate explanations for several counterfactual examples. We generate these examples as symmetric approximations of the relational structure in the original data. From these explanations, we learn a factor graph model to quantify uncertainty in an explanation. Our results on several datasets show that our approach can help verify explanati",
    "path": "papers/24/01/2401.02703.json",
    "total_tokens": 875,
    "translated_title": "验证关联解释:一种概率方法",
    "translated_abstract": "在关联数据上解释是很难验证的，因为解释结构更加复杂（例如图形）。为了验证可解释的解释（例如在图像、文本等中做出的预测的解释），通常使用人类主体，因为这不一定需要很多专业知识。然而，验证关联解释的质量需要专业知识，并且很难规模化。GNNExplainer可以说是图神经网络解释方法中最受欢迎的方法之一。在本文中，我们开发了一种方法，用于评估GNNExplainer生成的解释中的不确定性。具体而言，我们要求解释器为几个反事实示例生成解释。我们将这些示例生成为原始数据中关联结构的对称近似。从这些解释中，我们学习了一个因子图模型来量化解释中的不确定性。我们在几个数据集上的结果表明，我们的方法可以帮助验证解释的质量。",
    "tldr": "本文提出了一种概率方法，用于验证关联数据上的解释。通过在原始数据上生成对称近似的反事实示例，学习了一个因子图模型来量化解释的不确定性。实验证明该方法可以帮助验证解释的质量。",
    "en_tdlr": "This paper presents a probabilistic approach to verify explanations on relational data. By generating counterfactual examples as symmetric approximations of the original data, a factor graph model is learned to quantify the uncertainty in explanations. Experimental results show that this approach can help verify the quality of explanations."
}