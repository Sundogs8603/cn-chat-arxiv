{
    "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer. (arXiv:2401.04577v1 [cs.SD])",
    "abstract": "We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensi",
    "link": "http://arxiv.org/abs/2401.04577",
    "context": "Title: Masked Audio Generation using a Single Non-Autoregressive Transformer. (arXiv:2401.04577v1 [cs.SD])\nAbstract: We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensi",
    "path": "papers/24/01/2401.04577.json",
    "total_tokens": 990,
    "translated_title": "使用单一非自回归Transformer生成遮蔽音频",
    "translated_abstract": "我们介绍了一种名为MAGNeT的遮蔽生成序列建模方法，它直接操作多个音频令牌流。与以往的方法不同，MAGNeT由单阶段非自回归Transformer组成。在训练过程中，我们根据遮蔽计划器预测遮蔽令牌的范围，而在推断过程中，我们逐步构建输出序列使用多个解码步骤。为了进一步提高生成音频的质量，我们引入了一种新颖的重新评分方法，其中我们利用外部预训练模型来重新评分和排名MAGNeT的预测结果，这些结果将被用于后续的解码步骤。最后，我们探索了MAGNeT的混合版本，其中我们在自回归模式下生成前几秒钟，而其余的序列则以并行方式进行解码。我们展示了MAGNeT在文本到音乐和文本到音频生成任务中的效率，并进行了广泛的实验验证。",
    "tldr": "MAGNeT是一种遮蔽生成序列建模方法，使用单一非自回归Transformer生成具有高质量的音频，并引入了一种新颖的重新评分方法来提高生成音频的质量。同时，MAGNeT还探索了混合版本，可在自回归模式和非自回归模式下生成序列。在实验中证明MAGNeT在文本到音乐和文本到音频生成任务中具有高效性。",
    "en_tdlr": "MAGNeT is a masked generative sequence modeling method that uses a single non-autoregressive transformer for high-quality audio generation. It introduces a novel rescoring method to enhance the generated audio quality and explores a hybrid version for both autoregressive and non-autoregressive sequence generation. The efficiency of MAGNeT is demonstrated in text-to-music and text-to-audio generation tasks."
}