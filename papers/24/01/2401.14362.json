{
    "title": "The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support. (arXiv:2401.14362v1 [cs.HC])",
    "abstract": "People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools. Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly. In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support. We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots. We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts. Our study offers recommendations for how designers can approach the ethica",
    "link": "http://arxiv.org/abs/2401.14362",
    "context": "Title: The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support. (arXiv:2401.14362v1 [cs.HC])\nAbstract: People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools. Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly. In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support. We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots. We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts. Our study offers recommendations for how designers can approach the ethica",
    "path": "papers/24/01/2401.14362.json",
    "total_tokens": 949,
    "translated_title": "打字疗法：大型语言模型聊天机器人在心理健康支持中的应用经验",
    "translated_abstract": "越来越多的人使用大型语言模型（LLM）聊天机器人作为心理健康支持工具，但有证据表明通用型LLM聊天机器人也存在一定风险，如果设计不负责任可能会危及用户的福祉。本研究调查了使用LLM聊天机器人进行心理健康支持的人的真实经历。我们通过对来自不同国家背景的21个个人进行访谈，分析了用户如何为他们的聊天机器人创建独特的支持角色，填补日常护理的空白，并在寻求来自聊天机器人的支持时如何导航相关的文化限制。我们将分析基于心理治疗文献中有效支持的概念，并引入了AI与心理健康背景下的治疗价值观对其进行匹配的概念。我们的研究提供了设计师如何处理伦理问题的建议。",
    "tldr": "本研究通过调查使用大型语言模型聊天机器人进行心理健康支持的人的经历，分析了用户如何为聊天机器人创建独特的支持角色，并介绍了在心理健康背景下将人工智能与治疗价值观相匹配的概念。研究提供了设计师处理伦理问题的建议。",
    "en_tdlr": "This study investigates the experiences of using large language model chatbots for mental health support, analyzes how users create unique support roles for chatbots, and introduces the concept of aligning AI with therapeutic values in the context of mental health. The study provides recommendations for designers to address ethical concerns."
}