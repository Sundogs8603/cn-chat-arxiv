{
    "title": "SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models",
    "abstract": "arXiv:2401.08295v2 Announce Type: replace  Abstract: The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two modules to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning \\& Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model si",
    "link": "https://arxiv.org/abs/2401.08295",
    "context": "Title: SAPT: A Shared Attention Framework for Parameter-Efficient Continual Learning of Large Language Models\nAbstract: arXiv:2401.08295v2 Announce Type: replace  Abstract: The continual learning (CL) ability is vital for deploying large language models (LLMs) in the dynamic world. Existing methods devise the learning module to acquire task-specific knowledge with parameter-efficient tuning (PET) block and the selection module to pick out the corresponding one for the testing input, aiming at handling the challenges of catastrophic forgetting and knowledge transfer in CL. However, these methods tend to address only one of the challenges, ignoring the potential of aligning the two modules to effectively address catastrophic forgetting and knowledge transfer simultaneously. To this end, we propose a novel Shared Attention Framework (SAPT), to align the PET learning and selection via the Shared Attentive Learning \\& Selection module. Extensive Experiments on two CL benchmarks demonstrate the superiority of SAPT. Moreover, SAPT consistently demonstrates its superiority when we scale it to different model si",
    "path": "papers/24/01/2401.08295.json",
    "total_tokens": 866,
    "translated_title": "SAPT：一种共享注意力框架，用于大型语言模型的参数高效持续学习",
    "translated_abstract": "持续学习（CL）能力对于在动态世界部署大型语言模型（LLM）至关重要。现有方法设计学习模块，通过参数高效调整（PET）块获取特定任务的知识，并通过选择模块选择出相应的输入，旨在应对CL中的灾难式遗忘和知识转移挑战。然而，这些方法往往只解决其中一个挑战，忽视了通过将两个模块对齐来有效同时解决灾难式遗忘和知识转移的潜力。为此，我们提出了一种新颖的共享注意力框架（SAPT），通过共享注意力学习与选择模块来对齐PET学习和选择。在两个CL基准测试上进行的广泛实验表明SAPT的优越性。此外，当我们将其扩展到不同模型大小时，SAPT一直展现出其优越性。",
    "tldr": "提出了一种共享注意力框架（SAPT），通过共享注意力学习与选择模块对齐PET学习和选择，以同时解决大型语言模型中的灾难性遗忘和知识转移挑战。",
    "en_tdlr": "Introduced a Shared Attention Framework (SAPT) aligning PET learning and selection via the Shared Attentive Learning & Selection module to simultaneously address the challenges of catastrophic forgetting and knowledge transfer in large language models."
}