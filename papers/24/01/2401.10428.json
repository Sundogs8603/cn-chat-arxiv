{
    "title": "Understanding Learning through the Lens of Dynamical Invariants. (arXiv:2401.10428v1 [cs.AI])",
    "abstract": "This paper proposes a novel perspective on learning, positing it as the pursuit of dynamical invariants -- data combinations that remain constant or exhibit minimal change over time as a system evolves. This concept is underpinned by both informational and physical principles, rooted in the inherent properties of these invariants. Firstly, their stability makes them ideal for memorization and integration into associative networks, forming the basis of our knowledge structures. Secondly, the predictability of these stable invariants makes them valuable sources of usable energy, quantifiable as kTln2 per bit of accurately predicted information. This energy can be harnessed to explore new transformations, rendering learning systems energetically autonomous and increasingly effective. Such systems are driven to continuously seek new data invariants as energy sources. The paper further explores several meta-architectures of autonomous, self-propelled learning agents that utilize predictable",
    "link": "http://arxiv.org/abs/2401.10428",
    "context": "Title: Understanding Learning through the Lens of Dynamical Invariants. (arXiv:2401.10428v1 [cs.AI])\nAbstract: This paper proposes a novel perspective on learning, positing it as the pursuit of dynamical invariants -- data combinations that remain constant or exhibit minimal change over time as a system evolves. This concept is underpinned by both informational and physical principles, rooted in the inherent properties of these invariants. Firstly, their stability makes them ideal for memorization and integration into associative networks, forming the basis of our knowledge structures. Secondly, the predictability of these stable invariants makes them valuable sources of usable energy, quantifiable as kTln2 per bit of accurately predicted information. This energy can be harnessed to explore new transformations, rendering learning systems energetically autonomous and increasingly effective. Such systems are driven to continuously seek new data invariants as energy sources. The paper further explores several meta-architectures of autonomous, self-propelled learning agents that utilize predictable",
    "path": "papers/24/01/2401.10428.json",
    "total_tokens": 885,
    "translated_title": "通过动力学不变量理解学习",
    "translated_abstract": "本论文提出了一个新的学习视角，将其作为动力学不变量的追求，即随着系统演化，保持恒定或呈现最小变化的数据组合。这个概念基于信息和物理原理，根植于这些不变量的固有特性。首先，它们的稳定性使其成为理想的记忆和整合到联想网络中的材料，构成我们知识结构的基础。其次，这些稳定不变量的可预测性使其成为有价值的可用能源，每比特的准确预测信息可量化为kTln2的能量。这种能量可以用来探索新的变换，并使学习系统在能量上自主且日益有效。这样的系统被驱使不断寻求新的数据不变量作为能源。本文进一步探讨了几种利用可预测性的自主、自驱动学习智能体的元建筑。",
    "tldr": "通过追求动力学不变量，这篇论文提出了一个新的学习视角，将其作为学习的目标。这些动力学不变量稳定性使其适合于记忆和整合，并且可以作为有价值的能源驱使学习系统更加高效地学习。",
    "en_tdlr": "This paper proposes a new perspective on learning by pursuing dynamical invariants as the goal of learning. The stability of these dynamical invariants makes them suitable for memory and integration, and they can serve as valuable sources of energy driving learning systems to be more efficient."
}