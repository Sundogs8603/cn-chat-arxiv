{
    "title": "DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models. (arXiv:2401.02132v1 [cs.CL])",
    "abstract": "Evaluating the quality and variability of text generated by Large Language Models (LLMs) poses a significant, yet unresolved research challenge. Traditional evaluation methods, such as ROUGE and BERTScore, which measure token similarity, often fail to capture the holistic semantic equivalence. This results in a low correlation with human judgments and intuition, which is especially problematic in high-stakes applications like healthcare and finance where reliability, safety, and robust decision-making are highly critical. This work proposes DCR, an automated framework for evaluating and improving the consistency of LLM-generated texts using a divide-conquer-reasoning approach. Unlike existing LLM-based evaluators that operate at the paragraph level, our method employs a divide-and-conquer evaluator (DCE) that breaks down the paragraph-to-paragraph comparison between two generated responses into individual sentence-to-paragraph comparisons, each evaluated based on predefined criteria. T",
    "link": "http://arxiv.org/abs/2401.02132",
    "context": "Title: DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models. (arXiv:2401.02132v1 [cs.CL])\nAbstract: Evaluating the quality and variability of text generated by Large Language Models (LLMs) poses a significant, yet unresolved research challenge. Traditional evaluation methods, such as ROUGE and BERTScore, which measure token similarity, often fail to capture the holistic semantic equivalence. This results in a low correlation with human judgments and intuition, which is especially problematic in high-stakes applications like healthcare and finance where reliability, safety, and robust decision-making are highly critical. This work proposes DCR, an automated framework for evaluating and improving the consistency of LLM-generated texts using a divide-conquer-reasoning approach. Unlike existing LLM-based evaluators that operate at the paragraph level, our method employs a divide-and-conquer evaluator (DCE) that breaks down the paragraph-to-paragraph comparison between two generated responses into individual sentence-to-paragraph comparisons, each evaluated based on predefined criteria. T",
    "path": "papers/24/01/2401.02132.json",
    "total_tokens": 936,
    "translated_title": "DCR-Consistency: 大型语言模型一致性评估和改进的划分-征服-推理方法",
    "translated_abstract": "评估大型语言模型（LLMs）生成的文本的质量和变异性是一个重要而尚未解决的研究难题。传统的评估方法，如ROUGE和BERTScore，通常无法捕捉到整体语义的等价性。这导致与人类判断和直觉的相关性较低，尤其在医疗和金融等高风险应用中，可靠性、安全性和强大的决策能力尤为重要。本研究提出了DCR框架，一种使用划分-征服-推理方法评估和改进LLM生成文本一致性的自动化框架。与现有的基于LLM的评估器不同，本方法采用了划分和征服评估器（DCE），将两个生成的回答之间的段落对段落比较分解为根据预定义标准评估的每个句子对段落的比较。",
    "tldr": "DCR-Consistency提出了一个基于划分-征服-推理方法的自动化框架，用于评估和改进大型语言模型生成文本的一致性。与传统的评估方法不同，该方法通过将段落对段落比较划分为句子对段落的比较，并根据预定义标准进行评估。"
}