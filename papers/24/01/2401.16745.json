{
    "title": "MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models. (arXiv:2401.16745v1 [cs.CL])",
    "abstract": "Large language models (LLMs) are increasingly relied upon for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks predominantly focus on single-turn evaluations, overlooking the models' capabilities in multi-turn interactions. To address this gap, we introduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn conversational abilities. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or by creating new examples with GPT-4 to avoid data leakage. To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 11 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in s",
    "link": "http://arxiv.org/abs/2401.16745",
    "context": "Title: MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models. (arXiv:2401.16745v1 [cs.CL])\nAbstract: Large language models (LLMs) are increasingly relied upon for complex multi-turn conversations across diverse real-world applications. However, existing benchmarks predominantly focus on single-turn evaluations, overlooking the models' capabilities in multi-turn interactions. To address this gap, we introduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn conversational abilities. By analyzing human-LLM conversations, we categorize interaction patterns into four types: recollection, expansion, refinement, and follow-up. We construct multi-turn queries for each category either by augmenting existing datasets or by creating new examples with GPT-4 to avoid data leakage. To study the factors impacting multi-turn abilities, we create single-turn versions of the 1170 multi-turn queries and compare performance. Our evaluation of 11 well-known LLMs shows that while closed-source models generally surpass open-source ones, certain open-source models exceed GPT-3.5-Turbo in s",
    "path": "papers/24/01/2401.16745.json",
    "total_tokens": 895,
    "translated_title": "MT-Eval：面向大型语言模型的多轮能力评估基准",
    "translated_abstract": "大型语言模型(LLMs)在各种真实场景下越来越多地用于复杂的多轮对话。然而，现有的评估基准主要集中在单轮评估上，忽视了模型在多轮交互中的能力。为了填补这一空白，我们引入了MT-Eval，一个综合性的基准用于评估多轮对话能力。通过分析人类-LLM对话，我们将交互模式分为四类：回忆、扩展、修改和跟进。我们通过增加现有数据集或使用GPT-4创建新的示例构建了每个类别的多轮查询，以避免数据泄漏。为了研究影响多轮能力的因素，我们创建了1170个多轮查询的单轮版本，并进行了性能比较。我们对11个知名LLM进行评估表明，尽管闭源模型通常优于开源模型，但某些开源模型超过了GPT-3.5-Turbo。",
    "tldr": "MT-Eval是一个用于评估大型语言模型多轮对话能力的综合性基准，通过分析人类-LLM对话，将交互模式分为四类，并创建了多轮查询来评估模型的性能。",
    "en_tdlr": "MT-Eval is a comprehensive benchmark for evaluating multi-turn conversational abilities of large language models. By categorizing interaction patterns and creating multi-turn queries based on human-LLM conversations, the benchmark provides insights into the performance of different models in multi-turn interactions."
}