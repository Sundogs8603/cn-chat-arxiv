{
    "title": "GRATH: Gradual Self-Truthifying for Large Language Models. (arXiv:2401.12292v1 [cs.CL])",
    "abstract": "Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful answers and content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate corresponding answers and adaptively optimizes the model via direct preference optimization (DPO). Note that during this process, GRATH learns truthfulness in a self-supervised manner without requiring annotated answers. In particular, GRATH first generates pairwise truthfulness training data by prompting the LLM itself, with each pair containing a question and its correct and incorrect answers. The model is then fine-tuned using DPO to learn from the difference between answer pairs. Subsequently, GRATH iteratively refines the ",
    "link": "http://arxiv.org/abs/2401.12292",
    "context": "Title: GRATH: Gradual Self-Truthifying for Large Language Models. (arXiv:2401.12292v1 [cs.CL])\nAbstract: Truthfulness is paramount for large language models (LLMs) as they are increasingly deployed in real-world applications. However, existing LLMs still struggle with generating truthful answers and content, as evidenced by their modest performance on benchmarks like TruthfulQA. To address this issue, we propose GRAdual self-truTHifying (GRATH), a novel post-processing method to enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to generate corresponding answers and adaptively optimizes the model via direct preference optimization (DPO). Note that during this process, GRATH learns truthfulness in a self-supervised manner without requiring annotated answers. In particular, GRATH first generates pairwise truthfulness training data by prompting the LLM itself, with each pair containing a question and its correct and incorrect answers. The model is then fine-tuned using DPO to learn from the difference between answer pairs. Subsequently, GRATH iteratively refines the ",
    "path": "papers/24/01/2401.12292.json",
    "total_tokens": 964,
    "translated_title": "GRATH: 大型语言模型的逐渐自我真实化方法",
    "translated_abstract": "随着大型语言模型（LLMs）在真实世界应用中的部署越来越多，真实性对它们来说至关重要。然而，现有的LLMs在生成真实答案和内容方面仍然存在困难，如在TruthfulQA等基准上的表现不佳。为了解决这个问题，我们提出了GRAdual self-truTHifying (GRATH)，一种通过后处理方法提高LLMs真实性的新方法。GRATH利用领域外的问题提示生成相应的答案，并通过直接偏好优化进行自适应模型优化。在这个过程中，GRATH以无需标注答案的自我监督方式学习真实性。具体而言，GRATH首先通过提示LLM自身生成成对真实性训练数据，每对包含一个问题及其正确和错误答案。然后，使用直接偏好优化来微调模型，从答案对的差异中学习。随后，GRATH迭代地优化模型以逐渐提高真实性。",
    "tldr": "GRATH是一种逐步自我真实化的方法，用于提高大型语言模型的真实性。它通过使用领域外问题提示生成答案，并通过直接偏好优化进行自适应模型优化。GRATH在没有标注答案的情况下以自我监督的方式学习真实性，并通过迭代优化来逐步提升模型真实性。",
    "en_tdlr": "GRATH is a gradual self-truthifying method to enhance the truthfulness of large language models (LLMs). It uses out-of-domain question prompts to generate answers and adaptsively optimizes the model using direct preference optimization. GRATH learns truthfulness in a self-supervised manner without requiring annotated answers and iteratively improves the model's truthfulness."
}