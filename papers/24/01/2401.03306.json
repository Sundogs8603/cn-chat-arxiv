{
    "title": "MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning. (arXiv:2401.03306v1 [cs.LG])",
    "abstract": "We study the problem of offline pre-training and online fine-tuning for reinforcement learning from high-dimensional observations in the context of realistic robot tasks. Recent offline model-free approaches successfully use online fine-tuning to either improve the performance of the agent over the data collection policy or adapt to novel tasks. At the same time, model-based RL algorithms have achieved significant progress in sample efficiency and the complexity of the tasks they can solve, yet remain under-utilized in the fine-tuning setting. In this work, we argue that existing model-based offline RL methods are not suitable for offline-to-online fine-tuning in high-dimensional domains due to issues with distribution shifts, off-dynamics data, and non-stationary rewards. We propose an on-policy model-based method that can efficiently reuse prior data through model-based value expansion and policy regularization, while preventing model exploitation by controlling epistemic uncertainty",
    "link": "http://arxiv.org/abs/2401.03306",
    "context": "Title: MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning. (arXiv:2401.03306v1 [cs.LG])\nAbstract: We study the problem of offline pre-training and online fine-tuning for reinforcement learning from high-dimensional observations in the context of realistic robot tasks. Recent offline model-free approaches successfully use online fine-tuning to either improve the performance of the agent over the data collection policy or adapt to novel tasks. At the same time, model-based RL algorithms have achieved significant progress in sample efficiency and the complexity of the tasks they can solve, yet remain under-utilized in the fine-tuning setting. In this work, we argue that existing model-based offline RL methods are not suitable for offline-to-online fine-tuning in high-dimensional domains due to issues with distribution shifts, off-dynamics data, and non-stationary rewards. We propose an on-policy model-based method that can efficiently reuse prior data through model-based value expansion and policy regularization, while preventing model exploitation by controlling epistemic uncertainty",
    "path": "papers/24/01/2401.03306.json",
    "total_tokens": 923,
    "translated_title": "MOTO: 离线预训练与在线微调用于基于模型的机器人学习",
    "translated_abstract": "我们研究了在现实机器人任务中，从高维观测中进行离线预训练和在线微调的强化学习问题。最近的离线无模型方法成功地使用在线微调来提高智能体在数据收集策略上的性能，或者适应新的任务。与此同时，基于模型的强化学习算法在样本效率和解决的任务复杂性方面取得了显著进展，但在微调方面仍然被低估了。在这项工作中，我们认为现有的基于模型的离线强化学习方法在高维领域中不适用于离线到在线微调，原因是分布偏移、离散动力学数据和非稳态奖励问题。我们提出了一种基于模型的方法，通过模型值扩展和策略正则化，可以高效地重用先前的数据，同时通过控制认知不确定性来防止模型的利用。",
    "tldr": "本研究提出了一种基于模型的方法，通过模型值扩展和策略正则化，可以在高维领域中有效地进行离线预训练和在线微调的强化学习，解决了离线到在线微调中的分布偏移、离散动力学数据和非稳态奖励问题。",
    "en_tdlr": "This study proposes a model-based method that efficiently performs offline pre-training and online fine-tuning for reinforcement learning in high-dimensional domains, addressing issues of distribution shifts, off-dynamics data, and non-stationary rewards during the offline-to-online fine-tuning process."
}