{
    "title": "Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation. (arXiv:2401.14257v1 [cs.CV])",
    "abstract": "Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents w",
    "link": "http://arxiv.org/abs/2401.14257",
    "context": "Title: Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation. (arXiv:2401.14257v1 [cs.CV])\nAbstract: Recently, text-to-3D approaches have achieved high-fidelity 3D content generation using text description. However, the generated objects are stochastic and lack fine-grained control. Sketches provide a cheap approach to introduce such fine-grained control. Nevertheless, it is challenging to achieve flexible control from these sketches due to their abstraction and ambiguity. In this paper, we present a multi-view sketch-guided text-to-3D generation framework (namely, Sketch2NeRF) to add sketch control to 3D generation. Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable Diffusion and ControlNet) to supervise the optimization of a 3D scene represented by a neural radiance field (NeRF). We propose a novel synchronized generation and reconstruction method to effectively optimize the NeRF. In the experiments, we collected two kinds of multi-view sketch datasets to evaluate the proposed method. We demonstrate that our method can synthesize 3D consistent contents w",
    "path": "papers/24/01/2401.14257.json",
    "total_tokens": 988,
    "translated_title": "Sketch2NeRF: 多视角草图引导的文本到3D生成",
    "translated_abstract": "最近，文本到3D的方法通过文本描述实现了高保真度的3D内容生成。然而，生成的对象是随机的，并且缺乏细粒度的控制。草图提供了一种廉价的方法来引入这种细粒度的控制。然而，由于草图的抽象和模糊性，实现从这些草图中获得灵活的控制是具有挑战性的。在本文中，我们提出了一个多视角草图引导的文本到3D生成框架（即Sketch2NeRF），以增加对3D生成的草图控制。具体而言，我们的方法利用预训练的2D扩散模型（例如Stable Diffusion和ControlNet）来监督由神经辐射场（NeRF）表示的3D场景的优化。我们提出了一种新颖的同步生成和重建方法，以有效优化NeRF。在实验中，我们收集了两种多视角草图数据集来评估所提出的方法。我们证明我们的方法可以合成一致的3D内容。",
    "tldr": "本文提出了一个多视角草图引导的文本到3D生成框架(Sketch2NeRF)，以增加对3D生成的草图控制。通过利用预训练的2D扩散模型和神经辐射场来优化3D场景实现细粒度控制。实验证明，该方法能够合成一致的3D内容。",
    "en_tdlr": "This paper presents a multi-view sketch-guided text-to-3D generation framework (Sketch2NeRF) to add sketch control to 3D generation. It leverages pretrained 2D diffusion models and a neural radiance field to optimize 3D scenes and achieve fine-grained control. Experimental results demonstrate the ability of this method to synthesize consistent 3D content."
}