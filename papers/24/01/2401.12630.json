{
    "title": "Full-Stack Optimization for CAM-Only DNN Inference. (arXiv:2401.12630v1 [cs.AR])",
    "abstract": "The accuracy of neural networks has greatly improved across various domains over the past years. Their ever-increasing complexity, however, leads to prohibitively high energy demands and latency in von Neumann systems. Several computing-in-memory (CIM) systems have recently been proposed to overcome this, but trade-offs involving accuracy, hardware reliability, and scalability for large models remain a challenge. Additionally, for some CIM designs, the activation movement still requires considerable time and energy. This paper explores the combination of algorithmic optimizations for ternary weight neural networks and associative processors (APs) implemented using racetrack memory (RTM). We propose a novel compilation flow to optimize convolutions on APs by reducing their arithmetic intensity. By leveraging the benefits of RTM-based APs, this approach substantially reduces data transfers within the memory while addressing accuracy, energy efficiency, and reliability concerns. Concretel",
    "link": "http://arxiv.org/abs/2401.12630",
    "context": "Title: Full-Stack Optimization for CAM-Only DNN Inference. (arXiv:2401.12630v1 [cs.AR])\nAbstract: The accuracy of neural networks has greatly improved across various domains over the past years. Their ever-increasing complexity, however, leads to prohibitively high energy demands and latency in von Neumann systems. Several computing-in-memory (CIM) systems have recently been proposed to overcome this, but trade-offs involving accuracy, hardware reliability, and scalability for large models remain a challenge. Additionally, for some CIM designs, the activation movement still requires considerable time and energy. This paper explores the combination of algorithmic optimizations for ternary weight neural networks and associative processors (APs) implemented using racetrack memory (RTM). We propose a novel compilation flow to optimize convolutions on APs by reducing their arithmetic intensity. By leveraging the benefits of RTM-based APs, this approach substantially reduces data transfers within the memory while addressing accuracy, energy efficiency, and reliability concerns. Concretel",
    "path": "papers/24/01/2401.12630.json",
    "total_tokens": 922,
    "translated_title": "CAM只有深度学习神经网络推理的全栈优化",
    "translated_abstract": "在过去的几年里，神经网络的精度在各个领域都有了显著提高。然而，它们日益增长的复杂性导致冯·诺伊曼系统在能源需求和延迟方面变得难以承受。最近有几个计算内存（CIM）系统被提出来克服这个问题，但在准确性、硬件可靠性和大型模型的可扩展性之间仍然存在着权衡问题。此外，对于一些CIM设计，激活的移动仍然需要相当大的时间和能量。本文研究了三元权重神经网络和关联处理器（AP）的算法优化相结合的方法，这些关联处理器使用了赛车磁记忆（RTM）来实现。我们提出了一种新的编译流程，通过减少AP中的算术强度来优化卷积。通过利用基于RTM的AP的优势，这种方法在处理准确性、能源效率和可靠性问题的同时，大大降低了内存中的数据传输。",
    "tldr": "本文研究了CAM只有深度学习神经网络推理的全栈优化，通过算法优化和关联处理器的设计，以及使用赛车磁记忆来实现，成功降低了能量消耗和延迟，并提高了神经网络的精度和可靠性。",
    "en_tdlr": "This paper explores full-stack optimization for CAM-only deep neural network inference, successfully reducing energy demands and latency while improving accuracy and reliability by leveraging algorithmic optimizations and associative processors designed using racetrack memory."
}