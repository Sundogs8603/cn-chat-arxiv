{
    "title": "DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever. (arXiv:2401.01076v1 [cs.CL])",
    "abstract": "Recently, substantial advancements in pre-trained vision-language models have greatly enhanced the capabilities of multi-modal dialog systems. These models have demonstrated significant improvements by fine-tuning on downstream tasks. However, the existing pre-trained models primarily focus on effectively capturing the alignment between vision and language modalities, often ignoring the intricate nature of dialog context. In this paper, we propose a parameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog retrieval. Specifically, our approach introduces a multi-modal context prompt generator to learn context features which are subsequently distilled into prompts within the pre-trained vision-language model CLIP. Besides, we introduce domain prompt to mitigate the disc repancy from the downstream dialog data. To facilitate various types of retrieval, we also design multiple experts to learn mappings from CLIP outputs to multi-modal representation space, with each e",
    "link": "http://arxiv.org/abs/2401.01076",
    "context": "Title: DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever. (arXiv:2401.01076v1 [cs.CL])\nAbstract: Recently, substantial advancements in pre-trained vision-language models have greatly enhanced the capabilities of multi-modal dialog systems. These models have demonstrated significant improvements by fine-tuning on downstream tasks. However, the existing pre-trained models primarily focus on effectively capturing the alignment between vision and language modalities, often ignoring the intricate nature of dialog context. In this paper, we propose a parameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog retrieval. Specifically, our approach introduces a multi-modal context prompt generator to learn context features which are subsequently distilled into prompts within the pre-trained vision-language model CLIP. Besides, we introduce domain prompt to mitigate the disc repancy from the downstream dialog data. To facilitate various types of retrieval, we also design multiple experts to learn mappings from CLIP outputs to multi-modal representation space, with each e",
    "path": "papers/24/01/2401.01076.json",
    "total_tokens": 864,
    "translated_title": "DialCLIP: 将CLIP扩展为多模态对话检索器",
    "translated_abstract": "最近，在预训练的视觉语言模型方面取得了重大进展，极大地提升了多模态对话系统的能力。这些模型通过在下游任务上进行微调，已经取得了显著的改进。然而，现有的预训练模型主要集中在有效地捕捉视觉和语言模态之间的对齐，往往忽视了对话环境的复杂性。本文提出了一种名为DialCLIP的参数高效的提示微调方法，用于多模态对话检索。具体而言，我们的方法引入了一个多模态上下文提示生成器，用于学习上下文特征，并在预训练视觉语言模型CLIP中将其提炼为提示。此外，我们引入了领域提示，以减轻下游对话数据引起的差异。为了方便各种类型的检索，我们还设计了多个专家，从CLIP的输出学习到多模态表示空间的映射，每个专家都有自己的模型。",
    "tldr": "DialCLIP是一种参数高效的多模态对话检索方法，通过在预训练的视觉语言模型CLIP中引入多模态上下文提示生成器和领域提示来提升对话检索的能力。"
}