{
    "title": "Decoupled Prototype Learning for Reliable Test-Time Adaptation. (arXiv:2401.08703v1 [cs.LG])",
    "abstract": "Test-time adaptation (TTA) is a task that continually adapts a pre-trained source model to the target domain during inference. One popular approach involves fine-tuning model with cross-entropy loss according to estimated pseudo-labels. However, its performance is significantly affected by noisy pseudo-labels. This study reveals that minimizing the classification error of each sample causes the cross-entropy loss's vulnerability to label noise. To address this issue, we propose a novel Decoupled Prototype Learning (DPL) method that features prototype-centric loss computation. First, we decouple the optimization of class prototypes. For each class prototype, we reduce its distance with positive samples and enlarge its distance with negative samples in a contrastive manner. This strategy prevents the model from overfitting to noisy pseudo-labels. Second, we propose a memory-based strategy to enhance DPL's robustness for the small batch sizes often encountered in TTA. We update each class",
    "link": "http://arxiv.org/abs/2401.08703",
    "context": "Title: Decoupled Prototype Learning for Reliable Test-Time Adaptation. (arXiv:2401.08703v1 [cs.LG])\nAbstract: Test-time adaptation (TTA) is a task that continually adapts a pre-trained source model to the target domain during inference. One popular approach involves fine-tuning model with cross-entropy loss according to estimated pseudo-labels. However, its performance is significantly affected by noisy pseudo-labels. This study reveals that minimizing the classification error of each sample causes the cross-entropy loss's vulnerability to label noise. To address this issue, we propose a novel Decoupled Prototype Learning (DPL) method that features prototype-centric loss computation. First, we decouple the optimization of class prototypes. For each class prototype, we reduce its distance with positive samples and enlarge its distance with negative samples in a contrastive manner. This strategy prevents the model from overfitting to noisy pseudo-labels. Second, we propose a memory-based strategy to enhance DPL's robustness for the small batch sizes often encountered in TTA. We update each class",
    "path": "papers/24/01/2401.08703.json",
    "total_tokens": 941,
    "translated_title": "可靠的测试时自适应的解耦样本学习",
    "translated_abstract": "测试时自适应（TTA）是在推理过程中持续将预训练的源模型适应到目标域的任务。一种常见的方法是使用估计的伪标签使用交叉熵损失微调模型。然而，该方法的性能很容易受到噪声伪标签的影响。本研究发现，将每个样本的分类错误最小化会使交叉熵损失对标签噪声非常敏感。为解决这个问题，我们提出了一种新颖的解耦样本学习（DPL）方法，采用样本原型为中心的损失计算。首先，我们解耦了类原型的优化。对于每个类原型，我们采用对比方式减小其与正样本的距离，并增加其与负样本的距离。这种策略可以防止模型对噪声伪标签过拟合。其次，我们提出了一种基于记忆的策略，以增强DPL在TTA中经常遇到的小批量情况下的鲁棒性。我们更新每个类别的原型时使用记忆策略。",
    "tldr": "提出了一种名为Decoupled Prototype Learning (DPL)的解耦样本学习方法，通过使用样本原型为中心的损失计算，来解决测试时自适应中噪声伪标签的问题。"
}