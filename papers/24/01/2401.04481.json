{
    "title": "Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset. (arXiv:2401.04481v1 [cs.CL])",
    "abstract": "The recent success in language generation capabilities of large language models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns about their possible misuse in inducing mass agitation and communal hatred via generating fake news and spreading misinformation. Traditional means of developing a misinformation ground-truth dataset does not scale well because of the extensive manual effort required to annotate the data. In this paper, we propose an LLM-based approach of creating silver-standard ground-truth datasets for identifying misinformation. Specifically speaking, given a trusted news article, our proposed approach involves prompting LLMs to automatically generate a summarised version of the original article. The prompts in our proposed approach act as a controlling mechanism to generate specific types of factual incorrectness in the generated summaries, e.g., incorrect quantities, false attributions etc. To investigate the usefulness of this dataset, we conduct",
    "link": "http://arxiv.org/abs/2401.04481",
    "context": "Title: Fighting Fire with Fire: Adversarial Prompting to Generate a Misinformation Detection Dataset. (arXiv:2401.04481v1 [cs.CL])\nAbstract: The recent success in language generation capabilities of large language models (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns about their possible misuse in inducing mass agitation and communal hatred via generating fake news and spreading misinformation. Traditional means of developing a misinformation ground-truth dataset does not scale well because of the extensive manual effort required to annotate the data. In this paper, we propose an LLM-based approach of creating silver-standard ground-truth datasets for identifying misinformation. Specifically speaking, given a trusted news article, our proposed approach involves prompting LLMs to automatically generate a summarised version of the original article. The prompts in our proposed approach act as a controlling mechanism to generate specific types of factual incorrectness in the generated summaries, e.g., incorrect quantities, false attributions etc. To investigate the usefulness of this dataset, we conduct",
    "path": "papers/24/01/2401.04481.json",
    "total_tokens": 832,
    "translated_title": "以火攻火：对抗启发式生成一个辨别虚假信息的数据集",
    "translated_abstract": "大型语言模型（LLM），如GPT、Bard和Llama等，在语言生成能力方面取得了显著的成功。然而，这种成功可能引发对其被滥用的担忧，比如通过生成假新闻和传播错误信息引发大规模激动和仇恨。传统的虚假信息数据集开发方法在标注数据时需要大量人工努力，无法很好地扩展。本文提出了一种基于LLM的方法来创建用于识别虚假信息的银标准数据集。具体而言，给定一个可信新闻文章，我们的方法通过引导LLM自动生成原始文章的摘要版本。我们的方法中的引导作为一种控制机制，用于在生成的摘要中产生特定类型的事实错误，例如错误的数量、错误的归属地等。为了研究这个数据集的实用性，我们进行了实证实验。",
    "tldr": "本文提出了一种基于大型语言模型的方法，通过对其进行引导，自动生成虚假信息的辨别数据集，以解决传统方法标注数据所需的大量人工努力的问题。"
}