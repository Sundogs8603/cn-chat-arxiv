{
    "title": "Episodic Reinforcement Learning with Expanded State-reward Space. (arXiv:2401.10516v1 [cs.LG])",
    "abstract": "Empowered by deep neural networks, deep reinforcement learning (DRL) has demonstrated tremendous empirical successes in various domains, including games, health care, and autonomous driving. Despite these advancements, DRL is still identified as data-inefficient as effective policies demand vast numbers of environmental samples. Recently, episodic control (EC)-based model-free DRL methods enable sample efficiency by recalling past experiences from episodic memory. However, existing EC-based methods suffer from the limitation of potential misalignment between the state and reward spaces for neglecting the utilization of (past) retrieval states with extensive information, which probably causes inaccurate value estimation and degraded policy performance. To tackle this issue, we introduce an efficient EC-based DRL framework with expanded state-reward space, where the expanded states used as the input and the expanded rewards used in the training both contain historical and current informa",
    "link": "http://arxiv.org/abs/2401.10516",
    "context": "Title: Episodic Reinforcement Learning with Expanded State-reward Space. (arXiv:2401.10516v1 [cs.LG])\nAbstract: Empowered by deep neural networks, deep reinforcement learning (DRL) has demonstrated tremendous empirical successes in various domains, including games, health care, and autonomous driving. Despite these advancements, DRL is still identified as data-inefficient as effective policies demand vast numbers of environmental samples. Recently, episodic control (EC)-based model-free DRL methods enable sample efficiency by recalling past experiences from episodic memory. However, existing EC-based methods suffer from the limitation of potential misalignment between the state and reward spaces for neglecting the utilization of (past) retrieval states with extensive information, which probably causes inaccurate value estimation and degraded policy performance. To tackle this issue, we introduce an efficient EC-based DRL framework with expanded state-reward space, where the expanded states used as the input and the expanded rewards used in the training both contain historical and current informa",
    "path": "papers/24/01/2401.10516.json",
    "total_tokens": 906,
    "translated_title": "扩展状态-奖励空间的情节式强化学习",
    "translated_abstract": "借助深度神经网络的力量，深度强化学习（DRL）在游戏、医疗保健和自动驾驶等各个领域取得了巨大的经验成功。尽管取得了这些进展，但是由于有效策略需要大量的环境样本，DRL仍被认为是数据效率低下的。最近，基于情节控制（EC）的无模型DRL方法通过从情节记忆中回顾过去的经验实现了样本效率。然而，现有的基于EC的方法由于忽略了利用（过去的）检索状态的广泛信息，存在状态和奖励空间之间的潜在不对齐的限制，这可能导致值估计不准确和策略性能下降。为了解决这个问题，我们引入了一种具有扩展状态-奖励空间的高效EC型DRL框架，其中用作输入的扩展状态和用于训练的扩展奖励都包含历史和当前信息。",
    "tldr": "通过引入扩展的状态-奖励空间，我们提出了一种高效的情节式强化学习（DRL）框架，可以改善DRL中状态与奖励空间之间的不对齐问题，从而提高值的估计准确性和策略性能。",
    "en_tdlr": "We propose an efficient episodic reinforcement learning (DRL) framework with expanded state-reward space to address the misalignment between state and reward spaces, which improves accuracy of value estimation and policy performance in DRL."
}