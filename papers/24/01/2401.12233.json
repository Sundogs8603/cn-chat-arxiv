{
    "title": "Memorization in Self-Supervised Learning Improves Downstream Generalization. (arXiv:2401.12233v1 [cs.LG])",
    "abstract": "Self-supervised learning (SSL) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data-often scraped from the internet. This data can still be sensitive and empirical evidence suggests that SSL encoders memorize private information of their training data and can disclose them at inference time. Since existing theoretical definitions of memorization from supervised learning rely on labels, they do not transfer to SSL. To address this gap, we propose SSLMem, a framework for defining memorization within SSL. Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not. Through comprehensive empirical analysis on diverse encoder architectures and datasets we highlight that even though SSL relies on large datasets and strong augmentations-both known in supervised learning as regulari",
    "link": "http://arxiv.org/abs/2401.12233",
    "context": "Title: Memorization in Self-Supervised Learning Improves Downstream Generalization. (arXiv:2401.12233v1 [cs.LG])\nAbstract: Self-supervised learning (SSL) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data-often scraped from the internet. This data can still be sensitive and empirical evidence suggests that SSL encoders memorize private information of their training data and can disclose them at inference time. Since existing theoretical definitions of memorization from supervised learning rely on labels, they do not transfer to SSL. To address this gap, we propose SSLMem, a framework for defining memorization within SSL. Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not. Through comprehensive empirical analysis on diverse encoder architectures and datasets we highlight that even though SSL relies on large datasets and strong augmentations-both known in supervised learning as regulari",
    "path": "papers/24/01/2401.12233.json",
    "total_tokens": 908,
    "translated_title": "自监督学习中的记忆化提高了下游概括能力",
    "translated_abstract": "自监督学习（SSL）最近因其在无标签数据上训练高性能编码器的能力而受到重视，这些数据通常来源于互联网的抓取。然而，经验证据表明，SSL编码器会记忆其训练数据的私人信息，并在推理时泄露这些信息。现有的监督学习记忆化的理论定义依赖于标签，因此无法适用于SSL。为了填补这一空白，我们提出了SSLMem，一个在SSL内定义记忆化的框架。我们的定义通过比较训练在这些数据点上的编码器和未被训练在这些数据点上的编码器返回的数据点和他们的增强视图的表示的对齐差异。通过对不同编码器架构和数据集的综合实证分析，我们强调了即使SSL依赖于大型数据集和强大的数据增强，这都是监督学习中作为正则化手段的已知技术，记忆化仍然存在。",
    "tldr": "自监督学习中的记忆化问题一直是一个挑战，本文提出了SSLMem框架，用于定义自监督学习中的记忆化，并通过实证分析证明了在大规模数据集和强数据增强的情况下，记忆化仍然存在。"
}