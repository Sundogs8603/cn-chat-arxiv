{
    "title": "MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech Recognition",
    "abstract": "arXiv:2401.03424v2 Announce Type: replace-cross  Abstract: While automatic speech recognition (ASR) systems degrade significantly in noisy environments, audio-visual speech recognition (AVSR) systems aim to complement the audio stream with noise-invariant visual cues and improve the system's robustness. However, current studies mainly focus on fusing the well-learned modality features, like the output of modality-specific encoders, without considering the contextual relationship during the modality feature learning. In this study, we propose a multi-layer cross-attention fusion based AVSR (MLCA-AVSR) approach that promotes representation learning of each modality by fusing them at different levels of audio/visual encoders. Experimental results on the MISP2022-AVSR Challenge dataset show the efficacy of our proposed system, achieving a concatenated minimum permutation character error rate (cpCER) of 30.57% on the Eval set and yielding up to 3.17% relative improvement compared with our p",
    "link": "https://arxiv.org/abs/2401.03424",
    "context": "Title: MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech Recognition\nAbstract: arXiv:2401.03424v2 Announce Type: replace-cross  Abstract: While automatic speech recognition (ASR) systems degrade significantly in noisy environments, audio-visual speech recognition (AVSR) systems aim to complement the audio stream with noise-invariant visual cues and improve the system's robustness. However, current studies mainly focus on fusing the well-learned modality features, like the output of modality-specific encoders, without considering the contextual relationship during the modality feature learning. In this study, we propose a multi-layer cross-attention fusion based AVSR (MLCA-AVSR) approach that promotes representation learning of each modality by fusing them at different levels of audio/visual encoders. Experimental results on the MISP2022-AVSR Challenge dataset show the efficacy of our proposed system, achieving a concatenated minimum permutation character error rate (cpCER) of 30.57% on the Eval set and yielding up to 3.17% relative improvement compared with our p",
    "path": "papers/24/01/2401.03424.json",
    "total_tokens": 924,
    "translated_title": "基于多层交叉注意力融合的音频-视觉语音识别（MLCA-AVSR）",
    "translated_abstract": "自动语音识别（ASR）系统在嘈杂环境中明显退化，而音频-视觉语音识别（AVSR）系统旨在用抗噪音的视觉线索补充音频流，并提高系统的稳健性。然而，当前研究主要集中在融合好学习的模态特征，如模态特定编码器的输出，而没有考虑模态特征学习期间的上下文关系。在本研究中，我们提出了一种基于多层交叉注意力融合的AVSR（MLCA-AVSR）方法，通过在不同级别的音频/视觉编码器上融合它们来促进每个模态的表示学习。对MISP2022-AVSR挑战数据集上的实验结果显示了我们提出的系统的有效性，在Eval集上实现了30.57%的拼接最小置换字符误差率（cpCER），相对于我们的p取得了高达3.17%的相对改善。",
    "tldr": "提出了基于多层交叉注意力融合的音频-视觉语音识别（MLCA-AVSR）方法，通过在不同级别的音频/视觉编码器上融合模态特征，有效提高了系统的稳健性。",
    "en_tdlr": "Introduced a Multi-Layer Cross Attention Fusion based Audio-Visual Speech Recognition (MLCA-AVSR) approach that promotes representation learning of each modality by fusing them at different levels of audio/visual encoders, effectively enhancing the system's robustness."
}