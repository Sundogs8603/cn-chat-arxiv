{
    "title": "Majority or Minority: Data Imbalance Learning Method for Named Entity Recognition",
    "abstract": "arXiv:2401.11431v2 Announce Type: replace  Abstract: Data imbalance presents a significant challenge in various machine learning (ML) tasks, particularly named entity recognition (NER) within natural language processing (NLP). NER exhibits a data imbalance with a long-tail distribution, featuring numerous minority classes (i.e., entity classes) and a single majority class (i.e., O-class). This imbalance leads to misclassifications of the entity classes as the O-class. To tackle this issue, we propose a simple and effective learning method named majority or minority (MoM) learning. MoM learning incorporates the loss computed only for samples whose ground truth is the majority class into the loss of the conventional ML model. Evaluation experiments on four NER datasets (Japanese and English) showed that MoM learning improves prediction performance of the minority classes without sacrificing the performance of the majority class and is more effective than widely known and state-of-the-art",
    "link": "https://arxiv.org/abs/2401.11431",
    "context": "Title: Majority or Minority: Data Imbalance Learning Method for Named Entity Recognition\nAbstract: arXiv:2401.11431v2 Announce Type: replace  Abstract: Data imbalance presents a significant challenge in various machine learning (ML) tasks, particularly named entity recognition (NER) within natural language processing (NLP). NER exhibits a data imbalance with a long-tail distribution, featuring numerous minority classes (i.e., entity classes) and a single majority class (i.e., O-class). This imbalance leads to misclassifications of the entity classes as the O-class. To tackle this issue, we propose a simple and effective learning method named majority or minority (MoM) learning. MoM learning incorporates the loss computed only for samples whose ground truth is the majority class into the loss of the conventional ML model. Evaluation experiments on four NER datasets (Japanese and English) showed that MoM learning improves prediction performance of the minority classes without sacrificing the performance of the majority class and is more effective than widely known and state-of-the-art",
    "path": "papers/24/01/2401.11431.json",
    "total_tokens": 885,
    "translated_title": "多数还是少数：用于命名实体识别的数据不平衡学习方法",
    "translated_abstract": "数据不平衡在各种机器学习（ML）任务中是一个重要挑战，特别是在自然语言处理（NLP）中的命名实体识别（NER）任务。NER表现出一种长尾分布的数据不平衡，其中有许多少数类别（即实体类别）和一个单一的多数类别（即O类别）。这种不平衡导致将实体类别误分类为O类别。为了解决这个问题，我们提出了一种简单且有效的学习方法，命名为多数还是少数（MoM）学习。MoM学习将只有地面事实为多数类别的样本所计算的损失融入到传统ML模型的损失中。对四个NER数据集（日语和英语）的评估实验表明，MoM学习提高了少数类别的预测性能，同时不牺牲多数类别的性能，并且比广为人知的最新技术更有效。",
    "tldr": "提出了一种名为多数还是少数（MoM）学习的数据不平衡学习方法，针对命名实体识别任务中的多数类别和少数类别之间的挑战，能够提高少数类别的预测性能，而不影响多数类别的性能。",
    "en_tdlr": "Proposed a data imbalance learning method called Majority or Minority (MoM) learning to address the challenge between majority and minority classes in named entity recognition task, which improves prediction performance of minority classes without sacrificing the performance of the majority class."
}