{
    "title": "SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval. (arXiv:2401.02369v1 [cs.CL])",
    "abstract": "Clinician must write a lengthy summary each time a patient is discharged from the hospital. This task is time-consuming due to the sheer number of unique clinical concepts covered in the admission. Identifying and covering salient entities is vital for the summary to be clinically useful. We fine-tune open-source LLMs (Mistral-7B-Instruct and Zephyr-7B-\\b{eta}) on the task and find that they generate incomplete and unfaithful summaries. To increase entity coverage, we train a smaller, encoder-only model to predict salient entities, which are treated as content-plans to guide the LLM. To encourage the LLM to focus on specific mentions in the source notes, we propose SPEER: Sentence-level Planning via Embedded Entity Retrieval. Specifically, we mark each salient entity span with special \"{{ }}\" boundary tags and instruct the LLM to retrieve marked spans before generating each sentence. Sentence-level planning acts as a form of state tracking in that the model is explicitly recording the ",
    "link": "http://arxiv.org/abs/2401.02369",
    "context": "Title: SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval. (arXiv:2401.02369v1 [cs.CL])\nAbstract: Clinician must write a lengthy summary each time a patient is discharged from the hospital. This task is time-consuming due to the sheer number of unique clinical concepts covered in the admission. Identifying and covering salient entities is vital for the summary to be clinically useful. We fine-tune open-source LLMs (Mistral-7B-Instruct and Zephyr-7B-\\b{eta}) on the task and find that they generate incomplete and unfaithful summaries. To increase entity coverage, we train a smaller, encoder-only model to predict salient entities, which are treated as content-plans to guide the LLM. To encourage the LLM to focus on specific mentions in the source notes, we propose SPEER: Sentence-level Planning via Embedded Entity Retrieval. Specifically, we mark each salient entity span with special \"{{ }}\" boundary tags and instruct the LLM to retrieve marked spans before generating each sentence. Sentence-level planning acts as a form of state tracking in that the model is explicitly recording the ",
    "path": "papers/24/01/2401.02369.json",
    "total_tokens": 918,
    "translated_title": "SPEER: Embedded Entity Retrieval下的长临床摘要句子级规划",
    "translated_abstract": "临床医生在每次病人出院时必须写一份冗长的摘要。由于涵盖的临床概念数量庞大，这项任务非常耗时。识别和涵盖显著实体对于摘要的临床实用性至关重要。我们在该任务上微调了开源的LLM模型（Mistral-7B-Instruct和Zephyr-7B-η），发现它们生成的摘要不完整且不准确。为了增加实体覆盖范围，我们训练了一个较小的仅编码器模型来预测显著实体，并将其作为内容计划来指导LLM。为了鼓励LLM关注源笔记中的特定提及，我们提出了SPEER：Embedded Entity Retrieval下的句子级规划。具体而言，我们使用特殊的\"{{ }}\"边界标签标记每个显著实体跨度，并要求LLM在生成每个句子之前检索标记的跨度。句子级规划相当于一种状态追踪，模型明确记录下每个句子的信息。",
    "tldr": "本研究提出了一种在临床摘要中使用句子级规划并通过嵌入式实体检索的方法，以提高摘要的准确性和实用性。",
    "en_tdlr": "This paper introduces a method that uses sentence-level planning and embedded entity retrieval in clinical summaries to enhance accuracy and usefulness."
}