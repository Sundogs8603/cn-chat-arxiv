{
    "title": "CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering. (arXiv:2401.13170v1 [cs.CL])",
    "abstract": "Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current evaluation metrics to determine answer equivalence (AE) often do not align with human judgments, particularly more verbose, free-form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big: LLM-based scorers can correlate better with human judges, but this task has only been tested on limited QA datasets, and even when available, update of the model is limited because LLMs are large and often expensive. We rectify both of these issues by providing clear and consistent guidelines for evaluating AE in machine QA adopted from professional human QA contests. We also introduce a combination of standard evaluation and a more efficient, robust, and lightweight discriminate AE classifier-based matching method (CFMatch, smaller than 1 MB), trained and validated to more accurately evalu",
    "link": "http://arxiv.org/abs/2401.13170",
    "context": "Title: CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert Judgments For Open-Domain Question Answering. (arXiv:2401.13170v1 [cs.CL])\nAbstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current evaluation metrics to determine answer equivalence (AE) often do not align with human judgments, particularly more verbose, free-form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big: LLM-based scorers can correlate better with human judges, but this task has only been tested on limited QA datasets, and even when available, update of the model is limited because LLMs are large and often expensive. We rectify both of these issues by providing clear and consistent guidelines for evaluating AE in machine QA adopted from professional human QA contests. We also introduce a combination of standard evaluation and a more efficient, robust, and lightweight discriminate AE classifier-based matching method (CFMatch, smaller than 1 MB), trained and validated to more accurately evalu",
    "path": "papers/24/01/2401.13170.json",
    "total_tokens": 997,
    "translated_title": "CFMatch: 将自动答案等价评估与人工专家判断在开放域问答中对齐",
    "translated_abstract": "问答系统只有在我们知道答案是否正确的情况下才能取得进展，但对于许多最具挑战和有趣的问答示例，当前用于确定答案等价性的评估指标通常与人类判断不一致，尤其是来自大型语言模型（LLM）的更冗长、自由形式的答案。存在两个挑战：缺乏数据和模型过大：基于LLM的评分器可以更好地与人工评判员相关联，但这个任务只在有限的问答数据集上进行了测试，即使可用，对模型的更新也有限，因为LLM过大且往往昂贵。我们通过提供明确一致的指南来解决这两个问题，这些指南用于从专业人工问答比赛中采纳机器问答在答案等价性评估方面的标准。我们还引入了一种标准评估和一种更高效、稳健且轻量级的判别式AE分类器匹配方法（CFMatch，大小小于1MB），经过训练和验证以更准确地评估答案等价性。",
    "tldr": "CFMatch提出了一个在开放域问答中将自动答案等价评估与人工专家判断对齐的方法，通过提供明确一致的评估指南并引入高效、稳健且轻量级的判别式AE分类器匹配方法来解决当前评估指标与人类判断不一致的问题。"
}