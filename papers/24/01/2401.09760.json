{
    "title": "A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation. (arXiv:2401.09760v1 [cs.CL])",
    "abstract": "Whether Large Language Models (LLMs) can outperform crowdsourcing on the data annotation task is attracting interest recently. Some works verified this issue with the average performance of individual crowd workers and LLM workers on some specific NLP tasks by collecting new datasets. However, on the one hand, existing datasets for the studies of annotation quality in crowdsourcing are not yet utilized in such evaluations, which potentially provide reliable evaluations from a different viewpoint. On the other hand, the quality of these aggregated labels is crucial because, when utilizing crowdsourcing, the estimated labels aggregated from multiple crowd labels to the same instances are the eventually collected labels. Therefore, in this paper, we first investigate which existing crowdsourcing datasets can be used for a comparative study and create a benchmark. We then compare the quality between individual crowd labels and LLM labels and make the evaluations on the aggregated labels. I",
    "link": "http://arxiv.org/abs/2401.09760",
    "context": "Title: A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation. (arXiv:2401.09760v1 [cs.CL])\nAbstract: Whether Large Language Models (LLMs) can outperform crowdsourcing on the data annotation task is attracting interest recently. Some works verified this issue with the average performance of individual crowd workers and LLM workers on some specific NLP tasks by collecting new datasets. However, on the one hand, existing datasets for the studies of annotation quality in crowdsourcing are not yet utilized in such evaluations, which potentially provide reliable evaluations from a different viewpoint. On the other hand, the quality of these aggregated labels is crucial because, when utilizing crowdsourcing, the estimated labels aggregated from multiple crowd labels to the same instances are the eventually collected labels. Therefore, in this paper, we first investigate which existing crowdsourcing datasets can be used for a comparative study and create a benchmark. We then compare the quality between individual crowd labels and LLM labels and make the evaluations on the aggregated labels. I",
    "path": "papers/24/01/2401.09760.json",
    "total_tokens": 941,
    "translated_title": "通过标签聚合，众包和LLM的标注质量的比较研究",
    "translated_abstract": "近来，大型语言模型(LLM)是否能在数据标注任务上胜过众包引起了人们的兴趣。一些研究通过采集新的数据集，通过对个体众包工作者和LLM工作者在一些特定的自然语言处理任务上的平均表现来验证这个问题。然而，一方面，现有的用于研究众包标注质量的数据集尚未在这样的评估中得到利用，这可能从不同的角度提供可靠的评估。另一方面，聚合标签的质量至关重要，因为在使用众包时，从多个众包标签到相同实例的估计标签是最终收集到的标签。因此，在本文中，我们首先研究了哪些现有的众包数据集可以用于比较研究并创建了一个基准。然后我们比较了个体众包标签和LLM标签的质量，并对聚合标签进行了评估。",
    "tldr": "本文通过对众包标签和LLM标签的质量比较研究，探讨了大型语言模型是否能在数据标注任务上胜过众包。本研究对现有众包数据集进行了利用并创建了一个基准来评估标注质量。评估结果显示聚合标签的质量对于众包任务尤为重要。",
    "en_tdlr": "This paper investigates whether Large Language Models (LLMs) can outperform crowdsourcing in data annotation tasks. By comparing the quality of individual crowd labels and LLM labels, the study explores the potential of LLMs in improving annotation quality. The findings highlight the importance of label aggregation in crowdsourcing tasks."
}