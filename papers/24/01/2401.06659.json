{
    "title": "WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge. (arXiv:2401.06659v1 [cs.CL])",
    "abstract": "Sentiment analysis is rapidly advancing by utilizing various data modalities (e.g., text, image). However, most previous works relied on superficial information, neglecting the incorporation of contextual world knowledge (e.g., background information derived from but beyond the given image and text pairs) and thereby restricting their ability to achieve better multimodal sentiment analysis. In this paper, we proposed a plug-in framework named WisdoM, designed to leverage contextual world knowledge induced from the large vision-language models (LVLMs) for enhanced multimodal sentiment analysis. WisdoM utilizes a LVLM to comprehensively analyze both images and corresponding sentences, simultaneously generating pertinent context. To reduce the noise in the context, we also introduce a training-free Contextual Fusion mechanism. Experimental results across diverse granularities of multimodal sentiment analysis tasks consistently demonstrate that our approach has substantial improvements (br",
    "link": "http://arxiv.org/abs/2401.06659",
    "context": "Title: WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge. (arXiv:2401.06659v1 [cs.CL])\nAbstract: Sentiment analysis is rapidly advancing by utilizing various data modalities (e.g., text, image). However, most previous works relied on superficial information, neglecting the incorporation of contextual world knowledge (e.g., background information derived from but beyond the given image and text pairs) and thereby restricting their ability to achieve better multimodal sentiment analysis. In this paper, we proposed a plug-in framework named WisdoM, designed to leverage contextual world knowledge induced from the large vision-language models (LVLMs) for enhanced multimodal sentiment analysis. WisdoM utilizes a LVLM to comprehensively analyze both images and corresponding sentences, simultaneously generating pertinent context. To reduce the noise in the context, we also introduce a training-free Contextual Fusion mechanism. Experimental results across diverse granularities of multimodal sentiment analysis tasks consistently demonstrate that our approach has substantial improvements (br",
    "path": "papers/24/01/2401.06659.json",
    "total_tokens": 878,
    "translated_title": "改善多模态情感分析的智慧M：融合背景知识的上下文世界知识",
    "translated_abstract": "情感分析通过利用各种数据模态（例如文本、图像）迅速发展。然而，大多数先前的研究都依赖于表面信息，忽视了上下文世界知识（例如从给定图像和文本对之外获取的背景信息），从而限制了实现更好的多模态情感分析的能力。本文提出了一个名为智慧M的插件框架，旨在利用从大型视觉语言模型（LVLMs）中产生的上下文世界知识来改进多模态情感分析。智慧M利用LVLM来全面分析图像和相应的句子，同时生成相关的上下文。为了减少上下文中的噪声，我们还引入了一种无需训练的上下文融合机制。在多样的多模态情感分析任务的实验结果一致表明，我们的方法有着显著的改进。",
    "tldr": "本文提出了一个名为智慧M的插件框架，利用从大型视觉语言模型中产生的上下文世界知识来改进多模态情感分析，实验证明该方法在不同任务上有着显著的改进。",
    "en_tdlr": "The paper proposes a plug-in framework called WisdoM that utilizes contextual world knowledge induced from large vision-language models to enhance multimodal sentiment analysis. Experimental results show substantial improvements across diverse tasks."
}