{
    "title": "Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding. (arXiv:2401.04575v1 [cs.CV])",
    "abstract": "Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgro",
    "link": "http://arxiv.org/abs/2401.04575",
    "context": "Title: Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding. (arXiv:2401.04575v1 [cs.CV])\nAbstract: Vision and vision-language applications of neural networks, such as image classification and captioning, rely on large-scale annotated datasets that require non-trivial data-collecting processes. This time-consuming endeavor hinders the emergence of large-scale datasets, limiting researchers and practitioners to a small number of choices. Therefore, we seek more efficient ways to collect and annotate images. Previous initiatives have gathered captions from HTML alt-texts and crawled social media postings, but these data sources suffer from noise, sparsity, or subjectivity. For this reason, we turn to commercial shopping websites whose data meet three criteria: cleanliness, informativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset, a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites. When compared with existing general-domain datasets, the LGS images focus on the foreground object and have less complex backgro",
    "path": "papers/24/01/2401.04575.json",
    "total_tokens": 830,
    "translated_title": "Let's Go Shopping (LGS) -- 用于视觉概念理解的大规模图像文本数据集",
    "translated_abstract": "神经网络的视觉和视觉-语言应用，如图像分类和字幕，依赖于需要非平凡的数据收集过程的大规模注释数据集。这种耗时的努力限制了大规模数据集的出现，使研究人员和实践者只能选择少数几种选择。因此，我们寻求更有效的方法来收集和注释图像。以往的倡议已经从HTML alt文本和爬取的社交媒体帖子中收集了字幕，但这些数据源存在噪声、稀疏或主观性的问题。因此，我们转向商业购物网站，其数据符合三个标准：干净、信息丰富和流畅。我们介绍了Let's Go Shopping（LGS）数据集，这是一个来自公开可用的电子商务网站的1500万个图像-字幕对的大规模公共数据集。与现有的通用领域数据集相比，LGS图像侧重于前景对象，背景复杂度较低。",
    "tldr": "Let's Go Shopping (LGS) dataset is a large-scale public dataset with 15 million image-caption pairs from publicly available e-commerce websites, providing a more efficient way to collect and annotate images for vision and vision-language applications."
}