{
    "title": "SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization",
    "abstract": "Multi-turn dialogues are characterized by their extended length and the presence of turn-taking conversations. Traditional language models often overlook the distinct features of these dialogues by treating them as regular text. In this paper, we propose a speaker-enhanced pre-training method for long dialogue summarization, which leverages the inherent structure of multiple-turn dialogues. To support our study, we curate a diverse dataset that includes transcripts from real-world scenarios, movie or TV show transcripts, and dialogues generated by a Large Language Model. We then perform a pre-training, which encompasses the detection of speaker changes, and masked utterance generation. Experimental results of fine-tuned models demonstrate that our model achieves state-of-the-art performance on downstream benchmarks with long context, surpassing baseline models and highlighting the effectiveness of our approach. Our findings highlight the importance of curating pre-training datasets tha",
    "link": "https://arxiv.org/abs/2401.17597",
    "context": "Title: SPECTRUM: Speaker-Enhanced Pre-Training for Long Dialogue Summarization\nAbstract: Multi-turn dialogues are characterized by their extended length and the presence of turn-taking conversations. Traditional language models often overlook the distinct features of these dialogues by treating them as regular text. In this paper, we propose a speaker-enhanced pre-training method for long dialogue summarization, which leverages the inherent structure of multiple-turn dialogues. To support our study, we curate a diverse dataset that includes transcripts from real-world scenarios, movie or TV show transcripts, and dialogues generated by a Large Language Model. We then perform a pre-training, which encompasses the detection of speaker changes, and masked utterance generation. Experimental results of fine-tuned models demonstrate that our model achieves state-of-the-art performance on downstream benchmarks with long context, surpassing baseline models and highlighting the effectiveness of our approach. Our findings highlight the importance of curating pre-training datasets tha",
    "path": "papers/24/01/2401.17597.json",
    "total_tokens": 873,
    "translated_title": "SPECTRUM: 增强说话者先训练用于长对话摘要",
    "translated_abstract": "多轮对话以其扩展长度和交替发言的特点而闻名。传统的语言模型通常将这些对话视为普通文本，忽视了其独特的特点。本研究提出了一种用于长对话摘要的增强说话者先训练方法，利用多轮对话的内在结构。为了支持我们的研究，我们收集了一个多样化的数据集，包括真实场景的对话记录、电影或电视剧剧本以及大型语言模型生成的对话。然后进行了预训练，其中包括发言者变更的检测和掩码话语生成。经过微调的模型的实验结果表明，我们的模型在带有长上下文的下游基准测试上实现了最先进的性能，超过了基线模型，并突显了我们方法的有效性。我们的研究结果凸显了策划预训练数据集的重要性。",
    "tldr": "本文提出了一种用于长对话摘要的增强说话者先训练方法，通过利用多轮对话的内在结构，帮助语言模型更好地理解和处理长对话摘要任务，实现了最先进的性能。",
    "en_tdlr": "This paper proposes a speaker-enhanced pre-training method for long dialogue summarization, leveraging the inherent structure of multi-turn dialogues. The model achieves state-of-the-art performance on downstream benchmarks with long context, demonstrating the effectiveness of the approach in better understanding and handling long dialogue summarization tasks."
}