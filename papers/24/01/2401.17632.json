{
    "title": "What Do Self-Supervised Speech and Speaker Models Learn? New Findings From a Cross Model Layer-Wise Analysis",
    "abstract": "Self-supervised learning (SSL) has attracted increased attention for learning meaningful speech representations. Speech SSL models, such as WavLM, employ masked prediction training to encode general-purpose representations. In contrast, speaker SSL models, exemplified by DINO-based models, adopt utterance-level training objectives primarily for speaker representation. Understanding how these models represent information is essential for refining model efficiency and effectiveness. Unlike the various analyses of speech SSL, there has been limited investigation into what information speaker SSL captures and how its representation differs from speech SSL or other fully-supervised speaker models. This paper addresses these fundamental questions. We explore the capacity to capture various speech properties by applying SUPERB evaluation probing tasks to speech and speaker SSL models. We also examine which layers are predominantly utilized for each task to identify differences in how speech i",
    "link": "https://arxiv.org/abs/2401.17632",
    "context": "Title: What Do Self-Supervised Speech and Speaker Models Learn? New Findings From a Cross Model Layer-Wise Analysis\nAbstract: Self-supervised learning (SSL) has attracted increased attention for learning meaningful speech representations. Speech SSL models, such as WavLM, employ masked prediction training to encode general-purpose representations. In contrast, speaker SSL models, exemplified by DINO-based models, adopt utterance-level training objectives primarily for speaker representation. Understanding how these models represent information is essential for refining model efficiency and effectiveness. Unlike the various analyses of speech SSL, there has been limited investigation into what information speaker SSL captures and how its representation differs from speech SSL or other fully-supervised speaker models. This paper addresses these fundamental questions. We explore the capacity to capture various speech properties by applying SUPERB evaluation probing tasks to speech and speaker SSL models. We also examine which layers are predominantly utilized for each task to identify differences in how speech i",
    "path": "papers/24/01/2401.17632.json",
    "total_tokens": 880,
    "translated_title": "自监督语音和说话者模型学习了什么？来自跨模型层面分析的新发现",
    "translated_abstract": "自监督学习在学习有意义的语音表示方面引起了越来越多的关注。语音自监督学习模型，如WavLM，采用掩码预测训练来编码通用表示。与之相反，说话者自监督学习模型，例如基于DINO的模型，采用基于语句的训练目标主要用于说话者表示。了解这些模型如何表示信息对于改进模型的效率和效果至关重要。与语音自监督学习的各种分析不同，对于说话者自监督学习模型捕捉了哪些信息以及其表示与语音自监督学习或其他完全监督的说话者模型有何不同的研究有限。本文解决了这些基本问题。我们通过将SUPERB评估探测任务应用于语音和说话者自监督学习模型来探索捕捉各种语音属性的能力。我们还研究了每个任务中主要使用的层，以确定语音和说话者模型在表示上的差异。",
    "tldr": "本研究通过应用SUPERB评估探测任务，研究了自监督学习模型中捕捉语音特性的能力，并比较了语音和说话者模型之间的差异。",
    "en_tdlr": "This study investigates the capability of self-supervised learning models in capturing speech properties and compares the differences between speech and speaker models using the SUPERB evaluation probing tasks."
}