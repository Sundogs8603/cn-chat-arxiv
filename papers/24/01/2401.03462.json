{
    "title": "Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon",
    "abstract": "The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small trai",
    "link": "https://rss.arxiv.org/abs/2401.03462",
    "context": "Title: Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon\nAbstract: The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small trai",
    "path": "papers/24/01/2401.03462.json",
    "total_tokens": 975,
    "translated_title": "从4K到400K的飞跃：利用激活标志扩展LLM的上下文",
    "translated_abstract": "长上下文的利用对于LLM来说是一个巨大的挑战，因为它们有限的上下文窗口大小。尽管通过微调可以扩展上下文窗口，但这会导致训练和推理时间的显著成本，并对LLM的原始能力产生不利影响。在这项工作中，我们提出了一种名为激活标志的新方法，它将LLM的原始激活压缩成紧凑的形式，使LLM能够以有限的上下文窗口感知更长的上下文。激活标志被引入为插件模块，完全保留了LLM在短上下文中的原始能力。它与滑动窗口一起实时处理长的上下文，从而在训练和推理中实现了竞争力的内存和时间效率。激活标志是通过多样化压缩比的短序列数据进行训练的。得益于这种处理，它可以有效地学习支持不同上下文长度，实现小规模的训练。",
    "tldr": "本论文提出了一种称为激活标志的新方法，它通过压缩LLM的激活状态，使其能够以有限的上下文窗口感知更长的上下文，同时保留了LLM在短上下文中的原始能力。这种方法具有竞争力的内存和时间效率，并通过多样化训练有效地支持不同上下文长度。"
}