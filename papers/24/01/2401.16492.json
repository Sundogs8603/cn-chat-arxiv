{
    "title": "GPU Cluster Scheduling for Network-Sensitive Deep Learning. (arXiv:2401.16492v1 [cs.PF])",
    "abstract": "We propose a novel GPU-cluster scheduler for distributed DL (DDL) workloads that enables proximity based consolidation of GPU resources based on the DDL jobs' sensitivities to the anticipated communication-network delays. Our scheduler consists of three major components: (i) a classical delay scheduling algorithm to facilitate job placement and consolidation; (ii) a network-sensitive job preemption strategy; and (iii) an \"auto-tuner\" mechanism to optimize delay timers for effective delay scheduling. Additionally, to enable a cost-effective methodology for large-scale experiments, we develop a data-driven DDL cluster simulation platform. Employing the simulation platform we compare against several state-of-the-art alternatives on real-world workload traces to demonstrate the benefits of our design. Our scheduler can provide improvement of up to 69% in end-to-end Makespan for training all jobs compared to the prevailing consolidation-based scheduling methods, while reducing the average j",
    "link": "http://arxiv.org/abs/2401.16492",
    "context": "Title: GPU Cluster Scheduling for Network-Sensitive Deep Learning. (arXiv:2401.16492v1 [cs.PF])\nAbstract: We propose a novel GPU-cluster scheduler for distributed DL (DDL) workloads that enables proximity based consolidation of GPU resources based on the DDL jobs' sensitivities to the anticipated communication-network delays. Our scheduler consists of three major components: (i) a classical delay scheduling algorithm to facilitate job placement and consolidation; (ii) a network-sensitive job preemption strategy; and (iii) an \"auto-tuner\" mechanism to optimize delay timers for effective delay scheduling. Additionally, to enable a cost-effective methodology for large-scale experiments, we develop a data-driven DDL cluster simulation platform. Employing the simulation platform we compare against several state-of-the-art alternatives on real-world workload traces to demonstrate the benefits of our design. Our scheduler can provide improvement of up to 69% in end-to-end Makespan for training all jobs compared to the prevailing consolidation-based scheduling methods, while reducing the average j",
    "path": "papers/24/01/2401.16492.json",
    "total_tokens": 925,
    "translated_title": "GPU集群调度对网络敏感的深度学习",
    "translated_abstract": "我们提出了一种新颖的GPU集群调度器，用于分布式DL（DDL）工作负载，以基于DDL作业对预期通信网络延迟的敏感性进行GPU资源的邻近基础一致性。我们的调度器由三个主要组成部分组成：（i）一个经典的延迟调度算法，用于促进作业放置和一致性；（ii）一个对网络敏感的作业抢占策略；和（iii）一种“自动调整器”机制，用于优化延迟计时器以实现有效的延迟调度。另外，为了实现大规模实验的成本效益方法，我们开发了一个数据驱动的DDL集群仿真平台。通过使用仿真平台，我们在实际工作负载跟踪中与几种最先进的替代方法进行了比较，以展示我们设计的优势。与传统的基于一致性调度方法相比，我们的调度器可以提供高达69％的端到端Makespan提升，同时减少了平均j",
    "tldr": "我们提出了一种GPU集群调度器，用于分布式深度学习任务，根据任务对通信网络延迟的敏感性进行GPU资源的邻近基础一致性。相比传统的调度方法，我们的调度器可以提供高达69％的端到端Makespan提升。",
    "en_tdlr": "We propose a GPU cluster scheduler for distributed deep learning tasks that enables proximity-based consolidation of GPU resources based on the tasks' sensitivities to communication network delays. Our scheduler can provide up to 69% improvement in end-to-end Makespan compared to traditional scheduling methods."
}