{
    "title": "Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model. (arXiv:2401.06400v1 [cs.CL])",
    "abstract": "Visual question answering (VQA) is a task where an image is given, and a series of questions are asked about the image. To build an efficient VQA algorithm, a large amount of QA data is required which is very expensive. Generating synthetic QA pairs based on templates is a practical way to obtain data. However, VQA models trained on those data do not perform well on complex, human-written questions. To address this issue, we propose a new method called {\\it chain of QA for human-written questions} (CoQAH). CoQAH utilizes a sequence of QA interactions between a large language model and a VQA model trained on synthetic data to reason and derive logical answers for human-written questions. We tested the effectiveness of CoQAH on two types of human-written VQA datasets for 3D-rendered and chest X-ray images and found that it achieved state-of-the-art accuracy in both types of data. Notably, CoQAH outperformed general vision-language models, VQA models, and medical foundation models with no",
    "link": "http://arxiv.org/abs/2401.06400",
    "context": "Title: Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model. (arXiv:2401.06400v1 [cs.CL])\nAbstract: Visual question answering (VQA) is a task where an image is given, and a series of questions are asked about the image. To build an efficient VQA algorithm, a large amount of QA data is required which is very expensive. Generating synthetic QA pairs based on templates is a practical way to obtain data. However, VQA models trained on those data do not perform well on complex, human-written questions. To address this issue, we propose a new method called {\\it chain of QA for human-written questions} (CoQAH). CoQAH utilizes a sequence of QA interactions between a large language model and a VQA model trained on synthetic data to reason and derive logical answers for human-written questions. We tested the effectiveness of CoQAH on two types of human-written VQA datasets for 3D-rendered and chest X-ray images and found that it achieved state-of-the-art accuracy in both types of data. Notably, CoQAH outperformed general vision-language models, VQA models, and medical foundation models with no",
    "path": "papers/24/01/2401.06400.json",
    "total_tokens": 1035,
    "translated_title": "通过将大型语言模型与合成数据训练的VQA模型连接起来，将可视问答从合成问题泛化到人工问题",
    "translated_abstract": "可视问答（VQA）是一个给定图像并就该图像提出一系列问题的任务。构建一个高效的VQA算法需要大量的QA数据，而且非常昂贵。根据模板生成合成的QA对是获得数据的一种实用方法。然而，训练于这些数据上的VQA模型在复杂的人工问题上表现不佳。为了解决这个问题，我们提出了一种新的方法，称为“人工问题连锁问答”（CoQAH）。CoQAH利用一个大型语言模型与一个训练于合成数据上的VQA模型之间的QA交互序列来推理和推导人工问题的逻辑答案。我们在两种类型的人工问题VQA数据集上测试了CoQAH的效果，包括3D渲染图像和胸部X线图像，并发现它在两种类型的数据上都达到了最先进的准确率。值得注意的是，CoQAH在通用视觉语言模型、VQA模型和医学基础模型方面的表现也超过了。",
    "tldr": "本文提出了一种新方法CoQAH，通过连接大型语言模型和训练于合成数据上的VQA模型的QA交互序列，实现了将可视问答从合成问题泛化到人工问题，并在两种类型的人工问题数据集上取得了最先进的准确率，超过了通用视觉语言模型、VQA模型和医学基础模型。",
    "en_tdlr": "This paper proposes a new method called CoQAH, which utilizes a sequence of QA interactions between a large language model and a VQA model trained on synthetic data to generalize visual question answering from synthetic to human-written questions. CoQAH achieves state-of-the-art accuracy on two types of human-written VQA datasets and outperforms general vision-language models, VQA models, and medical foundation models."
}