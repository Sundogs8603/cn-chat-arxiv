{
    "title": "No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data Artifacts. (arXiv:2401.13907v1 [cs.CL])",
    "abstract": "Researchers recently found out that sometimes language models achieve high accuracy on benchmark data set, but they can not generalize very well with even little changes to the original data set. This is sometimes due to data artifacts, model is learning the spurious correlation between tokens and labels, instead of the semantics and logic. In this work, we analyzed SNLI data and visualized such spurious correlations. We proposed an adaptive up-sampling algorithm to correct the data artifacts, which is simple and effective, and does not need human edits or annotation. We did an experiment applying the algorithm to fix the data artifacts in SNLI data and the model trained with corrected data performed significantly better than the model trained with raw SNLI data, overall, as well as on the subset we corrected.",
    "link": "http://arxiv.org/abs/2401.13907",
    "context": "Title: No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data Artifacts. (arXiv:2401.13907v1 [cs.CL])\nAbstract: Researchers recently found out that sometimes language models achieve high accuracy on benchmark data set, but they can not generalize very well with even little changes to the original data set. This is sometimes due to data artifacts, model is learning the spurious correlation between tokens and labels, instead of the semantics and logic. In this work, we analyzed SNLI data and visualized such spurious correlations. We proposed an adaptive up-sampling algorithm to correct the data artifacts, which is simple and effective, and does not need human edits or annotation. We did an experiment applying the algorithm to fix the data artifacts in SNLI data and the model trained with corrected data performed significantly better than the model trained with raw SNLI data, overall, as well as on the subset we corrected.",
    "path": "papers/24/01/2401.13907.json",
    "total_tokens": 820,
    "translated_title": "没有更多干扰：一种自适应上采样算法以减少数据伪像",
    "translated_abstract": "研究人员最近发现，有时语言模型在基准数据集上达到了很高的准确性，但是对于原始数据集的微小变化无法很好地进行泛化。这有时是由于数据伪像，模型学习了标记和标签之间的虚假关联，而不是语义和逻辑。在这项工作中，我们分析了SNLI数据并可视化了这种虚假关联。我们提出了一种自适应上采样算法来纠正数据伪像，该算法简单而有效，并且不需要人工编辑或注释。我们在SNLI数据中应用该算法进行实验证明，经过纠正后训练的模型在整体和我们纠正的子集上表现明显优于在原始SNLI数据上训练的模型。",
    "tldr": "这项工作提出了一种自适应上采样算法，用于纠正语言模型训练中的数据伪像。经过使用该算法训练的模型在整体和修订的子集上表现显著优于使用原始数据训练的模型。",
    "en_tdlr": "This work proposes an adaptive up-sampling algorithm to correct data artifacts in language model training. The model trained with this algorithm performs significantly better overall and on the subset where corrections were made, compared to the model trained with raw data."
}