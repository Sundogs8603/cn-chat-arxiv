{
    "title": "The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models",
    "abstract": "While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments expose the difficulty of solving such problems while showcasing the immense gap between open-source and closed-source models. We also reveal critical shortcomings with individual visual and textual modules, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experi",
    "link": "https://arxiv.org/abs/2401.12117",
    "context": "Title: The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models\nAbstract: While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments expose the difficulty of solving such problems while showcasing the immense gap between open-source and closed-source models. We also reveal critical shortcomings with individual visual and textual modules, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experi",
    "path": "papers/24/01/2401.12117.json",
    "total_tokens": 882,
    "translated_title": "多模态大型语言模型中非语言抽象推理的奇特案例",
    "translated_abstract": "尽管大型语言模型（LLMs）仍在逐渐应用于新领域并在新应用中被利用，但我们正在经历新一代基础模型的涌现，即多模态大型语言模型（MLLMs）。这些模型将语言和视觉信息进行整合，为展示出更复杂的推理能力在两种模态的交集处提供了新的可能性。然而，尽管MLLMs的前景具有革命性的前景，我们对它们的推理能力的理解仍然有限。在本研究中，我们使用Raven's Progressive Matrices的变式评估了开源和闭源MLLMs的非语言抽象推理能力。我们的实验揭示了解决这类问题的困难，同时展示了开源和闭源模型之间巨大的差距。我们还揭示了个体视觉和文本模块的关键缺陷，导致模型的性能受到低谷的限制。最后，为了提高MLLMs的性能，我们进行了一系列实验。",
    "tldr": "本研究评估了多模态大型语言模型（MLLMs）在非语言抽象推理方面的能力，并发现开源和闭源模型之间存在巨大差距和个体模块的关键缺陷。",
    "en_tdlr": "This study assesses the nonverbal abstract reasoning abilities of multi-modal large language models (MLLMs) and reveals a significant gap between open-source and closed-source models as well as critical shortcomings in individual modules."
}