{
    "title": "Particle-MALA and Particle-mGRAD: Gradient-based MCMC methods for high-dimensional state-space models. (arXiv:2401.14868v1 [stat.CO])",
    "abstract": "State-of-the-art methods for Bayesian inference in state-space models are (a) conditional sequential Monte Carlo (CSMC) algorithms; (b) sophisticated 'classical' MCMC algorithms like MALA, or mGRAD from Titsias and Papaspiliopoulos (2018, arXiv:1610.09641v3 [stat.ML]). The former propose $N$ particles at each time step to exploit the model's 'decorrelation-over-time' property and thus scale favourably with the time horizon, $T$ , but break down if the dimension of the latent states, $D$, is large. The latter leverage gradient-/prior-informed local proposals to scale favourably with $D$ but exhibit sub-optimal scalability with $T$ due to a lack of model-structure exploitation. We introduce methods which combine the strengths of both approaches. The first, Particle-MALA, spreads $N$ particles locally around the current state using gradient information, thus extending MALA to $T > 1$ time steps and $N > 1$ proposals. The second, Particle-mGRAD, additionally incorporates (conditionally) Ga",
    "link": "http://arxiv.org/abs/2401.14868",
    "context": "Title: Particle-MALA and Particle-mGRAD: Gradient-based MCMC methods for high-dimensional state-space models. (arXiv:2401.14868v1 [stat.CO])\nAbstract: State-of-the-art methods for Bayesian inference in state-space models are (a) conditional sequential Monte Carlo (CSMC) algorithms; (b) sophisticated 'classical' MCMC algorithms like MALA, or mGRAD from Titsias and Papaspiliopoulos (2018, arXiv:1610.09641v3 [stat.ML]). The former propose $N$ particles at each time step to exploit the model's 'decorrelation-over-time' property and thus scale favourably with the time horizon, $T$ , but break down if the dimension of the latent states, $D$, is large. The latter leverage gradient-/prior-informed local proposals to scale favourably with $D$ but exhibit sub-optimal scalability with $T$ due to a lack of model-structure exploitation. We introduce methods which combine the strengths of both approaches. The first, Particle-MALA, spreads $N$ particles locally around the current state using gradient information, thus extending MALA to $T > 1$ time steps and $N > 1$ proposals. The second, Particle-mGRAD, additionally incorporates (conditionally) Ga",
    "path": "papers/24/01/2401.14868.json",
    "total_tokens": 969,
    "translated_title": "粒子-MALA和粒子-mGRAD: 面向高维状态空间模型的基于梯度的MCMC方法",
    "translated_abstract": "在状态空间模型中，现有的贝叶斯推断方法包括条件顺序Monte Carlo (CSMC)算法和先进的“经典”MCMC算法，如MALA或来自Titsias和Papaspiliopoulos (2018)的mGRAD。前者在每个时间步骤中提出N个粒子来利用模型的“随时间相关性”属性，并且随着时间范围T的增加而可扩展，但如果潜在状态的维度D较大，则会失败。后者利用梯度/先验信息自适应调整局部提议，以在维度D方面有良好的可扩展性，但由于缺乏模型结构的利用而在时间范围T方面显示出亚优的可扩展性。我们介绍了结合了两种方法优点的方法。第一种是粒子-MALA，使用梯度信息在当前状态周围局部扩散N个粒子，从而将MALA扩展到T>1和N>1的提议。第二种是粒子-mGRAD，此外还引入了（条件）Ga",
    "tldr": "粒子-MALA和粒子-mGRAD是基于梯度的MCMC方法，综合了条件顺序Monte Carlo (CSMC)算法的时间可扩展性和MALA或mGRAD的维度可扩展性。",
    "en_tdlr": "Particle-MALA and Particle-mGRAD are gradient-based MCMC methods that combine the time scalability of conditional sequential Monte Carlo (CSMC) algorithms with the dimension scalability of MALA or mGRAD."
}