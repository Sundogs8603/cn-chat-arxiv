{
    "title": "Learning to be Homo Economicus: Can an LLM Learn Preferences from Choice. (arXiv:2401.07345v1 [econ.GN])",
    "abstract": "This paper explores the use of Large Language Models (LLMs) as decision aids, with a focus on their ability to learn preferences and provide personalized recommendations. To establish a baseline, we replicate standard economic experiments on choice under risk (Choi et al., 2007) with GPT, one of the most prominent LLMs, prompted to respond as (i) a human decision maker or (ii) a recommendation system for customers. With these baselines established, GPT is provided with a sample set of choices and prompted to make recommendations based on the provided data. From the data generated by GPT, we identify its (revealed) preferences and explore its ability to learn from data. Our analysis yields three results. First, GPT's choices are consistent with (expected) utility maximization theory. Second, GPT can align its recommendations with people's risk aversion, by recommending less risky portfolios to more risk-averse decision makers, highlighting GPT's potential as a personalized decision aid.",
    "link": "http://arxiv.org/abs/2401.07345",
    "context": "Title: Learning to be Homo Economicus: Can an LLM Learn Preferences from Choice. (arXiv:2401.07345v1 [econ.GN])\nAbstract: This paper explores the use of Large Language Models (LLMs) as decision aids, with a focus on their ability to learn preferences and provide personalized recommendations. To establish a baseline, we replicate standard economic experiments on choice under risk (Choi et al., 2007) with GPT, one of the most prominent LLMs, prompted to respond as (i) a human decision maker or (ii) a recommendation system for customers. With these baselines established, GPT is provided with a sample set of choices and prompted to make recommendations based on the provided data. From the data generated by GPT, we identify its (revealed) preferences and explore its ability to learn from data. Our analysis yields three results. First, GPT's choices are consistent with (expected) utility maximization theory. Second, GPT can align its recommendations with people's risk aversion, by recommending less risky portfolios to more risk-averse decision makers, highlighting GPT's potential as a personalized decision aid.",
    "path": "papers/24/01/2401.07345.json",
    "total_tokens": 1023,
    "translated_title": "学习成为经济人：LLM能否从选择中学习偏好",
    "translated_abstract": "本文探讨了大型语言模型（LLM）作为决策辅助工具的应用，重点关注其学习偏好和提供个性化推荐的能力。为了建立一个基准，我们使用了最知名的LLM之一GPT，在选择风险下进行了标准经济实验的复制（Choi et al., 2007），并要求GPT作为（i）人类决策者或者（ii）顾客推荐系统进行回应。在确定了这些基准之后，我们给予GPT一组选择样本，并要求基于提供的数据进行推荐。通过GPT生成的数据，我们确定了它的（揭示的）偏好，并探讨了它从数据中学习的能力。我们的分析得出了三个结果。首先，GPT的选择与（期望的）效用最大化理论一致。其次，GPT能够根据人们的风险厌恶程度来调整推荐，为更风险厌恶的决策者推荐较低风险的投资组合，突显了GPT作为个性化决策辅助工具的潜力。",
    "tldr": "本文研究了使用大型语言模型（LLM）作为决策辅助工具的能力，重点关注其学习偏好和提供个性化推荐的能力。通过实验和数据分析，研究发现LLM的选择与效用最大化理论一致，并能够根据个体的风险厌恶程度进行个性化的推荐。",
    "en_tdlr": "This paper explores the ability of Large Language Models (LLMs) to serve as decision aids, focusing on their capability to learn preferences and provide personalized recommendations. Through experiments and data analysis, it is found that LLM's choices align with utility maximization theory and can offer personalized recommendations based on individuals' risk aversion."
}