{
    "title": "Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning. (arXiv:2401.13986v1 [cs.CL])",
    "abstract": "Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may generate the explanation \"all birds can fly\" when answering the question \"Can sparrows fly?\" but meanwhile answer \"no\" to the related question \"Can penguins fly?\". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out",
    "link": "http://arxiv.org/abs/2401.13986",
    "context": "Title: Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning. (arXiv:2401.13986v1 [cs.CL])\nAbstract: Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may generate the explanation \"all birds can fly\" when answering the question \"Can sparrows fly?\" but meanwhile answer \"no\" to the related question \"Can penguins fly?\". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out",
    "path": "papers/24/01/2401.13986.json",
    "total_tokens": 961,
    "translated_title": "通过解释一致性微调实现一致的自然语言解释",
    "translated_abstract": "大型语言模型（LLMs）通常能够生成令人信服、流畅的解释。然而，与人类不同，它们在不同输入上生成的解释常常不一致。例如，LLM在回答问题“麻雀能飞吗？”时可能生成解释“所有鸟都能飞”，但同时在回答与之相关的问题“企鹅能飞吗？”时回答“不行”。解释应该在相关示例中保持一致，以便让人类能够模拟LLM在多个示例上的决策过程。我们提出了解释一致性微调（EC-finetuning）方法，该方法通过适应LLM在相关示例上生成更一致的自然语言解释。EC-finetuning包括在经过精心构建的包含一致解释的合成数据上微调LLM。在各种不同领域的问答数据集上，EC-finetuning在四个微调数据集上相对提高了10.0%的解释一致性，并且在七个外部数据集上具有泛化性能。",
    "tldr": "本文提出了一种通过解释一致性微调方法，使得大型语言模型（LLMs）在相关示例上生成更一致的自然语言解释。实验证明，该方法在不同领域的问答数据集上相对提高了10.0%的解释一致性，并且能够泛化到其他数据集。",
    "en_tdlr": "This paper proposes an explanation-consistency finetuning method to generate more consistent natural-language explanations on related examples for large language models (LLMs). Experimental results show a 10.0% relative improvement in explanation consistency across different question-answering datasets in various domains, and the method generalizes well to other datasets."
}