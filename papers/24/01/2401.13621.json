{
    "title": "DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning. (arXiv:2401.13621v1 [cs.CL])",
    "abstract": "Contrastive-learning-based methods have dominated sentence representation learning. These methods regularize the representation space by pulling similar sentence representations closer and pushing away the dissimilar ones and have been proven effective in various NLP tasks, e.g., semantic textual similarity (STS) tasks. However, it is challenging for these methods to learn fine-grained semantics as they only learn from the inter-sentence perspective, i.e., their supervision signal comes from the relationship between data samples. In this work, we propose a novel denoising objective that inherits from another perspective, i.e., the intra-sentence perspective. By introducing both discrete and continuous noise, we generate noisy sentences and then train our model to restore them to their original form. Our empirical evaluations demonstrate that this approach delivers competitive results on both semantic textual similarity (STS) and a wide range of transfer tasks, standing up well in compa",
    "link": "http://arxiv.org/abs/2401.13621",
    "context": "Title: DenoSent: A Denoising Objective for Self-Supervised Sentence Representation Learning. (arXiv:2401.13621v1 [cs.CL])\nAbstract: Contrastive-learning-based methods have dominated sentence representation learning. These methods regularize the representation space by pulling similar sentence representations closer and pushing away the dissimilar ones and have been proven effective in various NLP tasks, e.g., semantic textual similarity (STS) tasks. However, it is challenging for these methods to learn fine-grained semantics as they only learn from the inter-sentence perspective, i.e., their supervision signal comes from the relationship between data samples. In this work, we propose a novel denoising objective that inherits from another perspective, i.e., the intra-sentence perspective. By introducing both discrete and continuous noise, we generate noisy sentences and then train our model to restore them to their original form. Our empirical evaluations demonstrate that this approach delivers competitive results on both semantic textual similarity (STS) and a wide range of transfer tasks, standing up well in compa",
    "path": "papers/24/01/2401.13621.json",
    "total_tokens": 902,
    "translated_title": "DenoSent：用于自我监督句子表示学习的去噪目标",
    "translated_abstract": "对比学习方法主导着句子表示学习。这些方法通过拉近相似的句子表示并将不相似的句子表示推开来规范表示空间，在各种自然语言处理任务中已被证明有效，例如，语义文本相似度（STS）任务。然而，对于这些方法来说，学习细粒度语义是具有挑战性的，因为它们只从句间的角度学习，即，它们的监督信号来自数据样本之间的关系。在这项工作中，我们提出了一种新颖的去噪目标，它继承了另一个视角，即句内的视角。通过引入离散和连续的噪声，我们生成带有噪声的句子，然后训练我们的模型将其恢复为原始形式。我们的实证评估证明，这种方法在语义文本相似度（STS）和广泛的迁移任务中表现出色，与其他方法相媲美。",
    "tldr": "本论文提出了一种新颖的对比学习方法，通过引入离散和连续的噪声进行自我监督句子表示学习。实验证明，这种方法在语义文本相似度和迁移任务中表现出色，并且与其他方法相媲美。",
    "en_tdlr": "This paper proposes a novel contrastive-learning-based method for self-supervised sentence representation learning by introducing discrete and continuous noise. Experimental results demonstrate that this method achieves competitive performance in semantic textual similarity and transfer tasks, comparable to other methods."
}