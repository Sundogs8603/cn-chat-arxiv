{
    "title": "SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity. (arXiv:2401.17072v1 [cs.CL])",
    "abstract": "Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable advancements in their ability to generate fitting responses to natural language instructions. However, many current works rely on manual evaluation to judge the quality of generated responses. Since such manual evaluation is time-consuming, it does not easily scale to the evaluation of multiple models and model variants. In this short paper, we propose a straightforward but remarkably effective evaluation metric called SemScore, in which we directly compare model outputs to gold target responses using semantic textual similarity (STS). We conduct a comparative evaluation of the model outputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation metrics for text generation. We find that our proposed SemScore metric outperforms all other, in many cases more complex, evaluation metrics in terms of correlation to human evaluation. These findings indicate the utility of our proposed metric for ",
    "link": "http://arxiv.org/abs/2401.17072",
    "context": "Title: SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity. (arXiv:2401.17072v1 [cs.CL])\nAbstract: Instruction-tuned Large Language Models (LLMs) have recently showcased remarkable advancements in their ability to generate fitting responses to natural language instructions. However, many current works rely on manual evaluation to judge the quality of generated responses. Since such manual evaluation is time-consuming, it does not easily scale to the evaluation of multiple models and model variants. In this short paper, we propose a straightforward but remarkably effective evaluation metric called SemScore, in which we directly compare model outputs to gold target responses using semantic textual similarity (STS). We conduct a comparative evaluation of the model outputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation metrics for text generation. We find that our proposed SemScore metric outperforms all other, in many cases more complex, evaluation metrics in terms of correlation to human evaluation. These findings indicate the utility of our proposed metric for ",
    "path": "papers/24/01/2401.17072.json",
    "total_tokens": 929,
    "translated_title": "SemScore: 基于语义文本相似度的指令调校大型语言模型的自动评估",
    "translated_abstract": "最近，指令调校的大型语言模型（LLMs）在生成适合自然语言指令的回应方面展示出了令人瞩目的进展。然而，许多当前的研究依赖于手动评估来判断生成回应的质量。由于这种手动评估耗时，不容易扩展到对多个模型和模型变体的评估。在本短文中，我们提出了一种简单但非常有效的评估指标SemScore，通过语义文本相似度（STS）直接将模型输出与黄金目标回应进行比较。我们对12个知名的指令调校LLMs的模型输出进行了基于8个广泛使用的文本生成评估指标的比较评估。我们发现我们提出的SemScore指标在与人工评估的相关性方面优于所有其他、在许多情况下更复杂的评估指标。这些发现表明我们提出的指标对于评估指令调校LLMs的实用性。",
    "tldr": "这项研究提出了一个名为SemScore的评估指标，通过语义文本相似度直接比较模型输出和黄金目标回应，用于评估指令调校大型语言模型。实验证明，SemScore指标在与人工评估的相关性方面表现优于其他评估指标。"
}