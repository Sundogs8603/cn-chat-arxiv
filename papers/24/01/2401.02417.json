{
    "title": "Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition. (arXiv:2401.02417v1 [eess.AS])",
    "abstract": "While word error rates of automatic speech recognition (ASR) systems have consistently fallen, natural language understanding (NLU) applications built on top of ASR systems still attribute significant numbers of failures to low-quality speech recognition results. Existing assistant systems collect large numbers of these unsuccessful interactions, but these systems usually fail to learn from these interactions, even in an offline fashion. In this work, we introduce CLC: Contrastive Learning for Conversations, a family of methods for contrastive fine-tuning of models in a self-supervised fashion, making use of easily detectable artifacts in unsuccessful conversations with assistants. We demonstrate that our CLC family of approaches can improve the performance of ASR models on OD3, a new public large-scale semi-synthetic meta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains transfer to real-world systems as well, where we show that CLC can help to improve performance ",
    "link": "http://arxiv.org/abs/2401.02417",
    "context": "Title: Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic Speech Recognition. (arXiv:2401.02417v1 [eess.AS])\nAbstract: While word error rates of automatic speech recognition (ASR) systems have consistently fallen, natural language understanding (NLU) applications built on top of ASR systems still attribute significant numbers of failures to low-quality speech recognition results. Existing assistant systems collect large numbers of these unsuccessful interactions, but these systems usually fail to learn from these interactions, even in an offline fashion. In this work, we introduce CLC: Contrastive Learning for Conversations, a family of methods for contrastive fine-tuning of models in a self-supervised fashion, making use of easily detectable artifacts in unsuccessful conversations with assistants. We demonstrate that our CLC family of approaches can improve the performance of ASR models on OD3, a new public large-scale semi-synthetic meta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains transfer to real-world systems as well, where we show that CLC can help to improve performance ",
    "path": "papers/24/01/2401.02417.json",
    "total_tokens": 879,
    "translated_title": "任务导向对话作为自监督自动语音识别的催化剂",
    "translated_abstract": "虽然自动语音识别（ASR）系统的词错误率不断下降，但基于ASR系统构建的自然语言理解（NLU）应用仍然归因于低质量的语音识别结果导致的大量失败。现有的助手系统收集了大量这些失败的交互，但是这些系统通常无法从这些交互中学习，即使是离线学习也是如此。在这项工作中，我们引入了CLC：对话对比学习，这是一系列自监督方法，用于对模型进行对比微调，利用失败对话中容易检测到的人工痕迹。我们证明了我们的CLC系列方法可以提高ASR模型在OD3上的性能，这是一个新的公开的大规模半合成元数据集，对话是以任务为导向的，提升幅度高达19.2%。这些增益也可以转移到现实世界的系统中，我们展示了CLC可以帮助提高性能。",
    "tldr": "本研究引入了一种对话对比学习方法，用于对自动语音识别模型进行自监督微调，以改善低质量语音识别结果导致的自然语言理解应用的失败。实验证明，这种方法在任务导向的对话数据集上可以将ASR模型的性能提高达到19.2%。"
}