{
    "title": "EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model. (arXiv:2401.10278v1 [eess.SP])",
    "abstract": "Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance ",
    "link": "http://arxiv.org/abs/2401.10278",
    "context": "Title: EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model. (arXiv:2401.10278v1 [eess.SP])\nAbstract: Self-supervised learning has emerged as a highly effective approach in the fields of natural language processing and computer vision. It is also applicable to brain signals such as electroencephalography (EEG) data, given the abundance of available unlabeled data that exist in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on end-to-end model learning which is not easy for humans to understand. In this paper, we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained model cannot only learn universal representations on EEG signals with adaptable performance ",
    "path": "papers/24/01/2401.10278.json",
    "total_tokens": 911,
    "translated_title": "EEGFormer: 实现可迁移和可解释的大规模脑电基础模型",
    "translated_abstract": "自我监督学习在自然语言处理和计算机视觉领域中被证明是一种高效的方法。在脑电图(EEG)数据等大量无标签数据存在的情况下，它也适用于脑信号，这些数据在从癫痫检测到波形分析等各种真实医学应用中存在。现有的利用自我监督学习进行EEG建模的工作主要集中在对应于单个下游任务的每个独立数据集的预训练上，这无法充分利用丰富的数据，而且可能会导致缺乏泛化性的次优解决方案。此外，这些方法依赖于难以理解的端到端模型学习。本文介绍了一种新颖的EEG基础模型，名为EEGFormer，它在大规模复合EEG数据上进行预训练。预训练模型不仅可以学习适应性能的EEG信号的通用表示，还可以提供可解释性。",
    "tldr": "本论文提出了一种名为EEGFormer的脑电基础模型，通过在大规模的复合EEG数据上进行预训练，该模型能够学习到可迁移和可解释的通用表示，为脑电信号的分析提供了有力的工具。",
    "en_tdlr": "This paper proposes a novel EEG foundation model called EEGFormer, which is pretrained on large-scale compound EEG data. The model can learn transferable and interpretable universal representations for EEG signals, providing a powerful tool for EEG analysis."
}