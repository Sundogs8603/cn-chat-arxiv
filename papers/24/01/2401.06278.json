{
    "title": "A Study on Self-Supervised Pretraining for Vision Problems in Gastrointestinal Endoscopy. (arXiv:2401.06278v1 [cs.CV])",
    "abstract": "Solutions to vision tasks in gastrointestinal endoscopy (GIE) conventionally use image encoders pretrained in a supervised manner with ImageNet-1k as backbones. However, the use of modern self-supervised pretraining algorithms and a recent dataset of 100k unlabelled GIE images (Hyperkvasir-unlabelled) may allow for improvements. In this work, we study the fine-tuned performance of models with ResNet50 and ViT-B backbones pretrained in self-supervised and supervised manners with ImageNet-1k and Hyperkvasir-unlabelled (self-supervised only) in a range of GIE vision tasks. In addition to identifying the most suitable pretraining pipeline and backbone architecture for each task, out of those considered, our results suggest: that self-supervised pretraining generally produces more suitable backbones for GIE vision tasks than supervised pretraining; that self-supervised pretraining with ImageNet-1k is typically more suitable than pretraining with Hyperkvasir-unlabelled, with the notable exce",
    "link": "http://arxiv.org/abs/2401.06278",
    "context": "Title: A Study on Self-Supervised Pretraining for Vision Problems in Gastrointestinal Endoscopy. (arXiv:2401.06278v1 [cs.CV])\nAbstract: Solutions to vision tasks in gastrointestinal endoscopy (GIE) conventionally use image encoders pretrained in a supervised manner with ImageNet-1k as backbones. However, the use of modern self-supervised pretraining algorithms and a recent dataset of 100k unlabelled GIE images (Hyperkvasir-unlabelled) may allow for improvements. In this work, we study the fine-tuned performance of models with ResNet50 and ViT-B backbones pretrained in self-supervised and supervised manners with ImageNet-1k and Hyperkvasir-unlabelled (self-supervised only) in a range of GIE vision tasks. In addition to identifying the most suitable pretraining pipeline and backbone architecture for each task, out of those considered, our results suggest: that self-supervised pretraining generally produces more suitable backbones for GIE vision tasks than supervised pretraining; that self-supervised pretraining with ImageNet-1k is typically more suitable than pretraining with Hyperkvasir-unlabelled, with the notable exce",
    "path": "papers/24/01/2401.06278.json",
    "total_tokens": 1004,
    "translated_title": "自我监督预训练在胃肠内镜视觉问题中的研究",
    "translated_abstract": "胃肠内镜（GIE）中的视觉任务通常使用在ImageNet-1k上以有监督方式预训练的图像编码器作为骨干网络。然而，现代自我监督预训练算法和一个最近的包含10万张未标记GIE图像的数据集（Hyperkvasir-unlabelled）的使用可能会带来改进。在这项工作中，我们研究了在一系列GIE视觉任务中，使用ResNet50和ViT-B骨干网络以自我监督和有监督方式预训练的模型的微调性能。除了确定每个任务最适合的预训练流程和骨干网络架构外，我们的结果表明：相对于有监督预训练，自我监督预训练通常能够产生更适合GIE视觉任务的骨干网络；自我监督预训练使用ImageNet-1k通常比使用Hyperkvasir-unlabelled预训练更合适，但有一个明显的例外情况。",
    "tldr": "本研究研究了自我监督预训练在胃肠内镜视觉问题中的应用，结果发现相对于有监督预训练，自我监督预训练通常能够产生更适合的骨干网络，并且使用ImageNet-1k进行自我监督预训练通常比使用Hyperkvasir-unlabelled更合适。"
}