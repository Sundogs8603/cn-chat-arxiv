{
    "title": "Understanding the Generalization Benefits of Late Learning Rate Decay. (arXiv:2401.11600v1 [cs.LG])",
    "abstract": "Why do neural networks trained with large learning rates for a longer time often lead to better generalization? In this paper, we delve into this question by examining the relation between training and testing loss in neural networks. Through visualization of these losses, we note that the training trajectory with a large learning rate navigates through the minima manifold of the training loss, finally nearing the neighborhood of the testing loss minimum. Motivated by these findings, we introduce a nonlinear model whose loss landscapes mirror those observed for real neural networks. Upon investigating the training process using SGD on our model, we demonstrate that an extended phase with a large learning rate steers our model towards the minimum norm solution of the training loss, which may achieve near-optimal generalization, thereby affirming the empirically observed benefits of late learning rate decay.",
    "link": "http://arxiv.org/abs/2401.11600",
    "context": "Title: Understanding the Generalization Benefits of Late Learning Rate Decay. (arXiv:2401.11600v1 [cs.LG])\nAbstract: Why do neural networks trained with large learning rates for a longer time often lead to better generalization? In this paper, we delve into this question by examining the relation between training and testing loss in neural networks. Through visualization of these losses, we note that the training trajectory with a large learning rate navigates through the minima manifold of the training loss, finally nearing the neighborhood of the testing loss minimum. Motivated by these findings, we introduce a nonlinear model whose loss landscapes mirror those observed for real neural networks. Upon investigating the training process using SGD on our model, we demonstrate that an extended phase with a large learning rate steers our model towards the minimum norm solution of the training loss, which may achieve near-optimal generalization, thereby affirming the empirically observed benefits of late learning rate decay.",
    "path": "papers/24/01/2401.11600.json",
    "total_tokens": 1001,
    "translated_title": "理解后期学习率衰减的泛化优势",
    "translated_abstract": "为什么用大学习率长时间训练的神经网络通常能够实现更好的泛化呢？本文通过研究神经网络中训练损失和测试损失之间的关系来探讨这个问题。通过可视化这些损失情况，我们观察到在大学习率下的训练轨迹会通过训练损失的极小值流形，最终接近于测试损失的极小值附近。在这个发现的基础上，我们引入了一个非线性模型，其损失景观与真实神经网络观察到的相似。通过在我们的模型上使用SGD研究训练过程，我们证明了在大学习率下的延长阶段可以将我们的模型引导向训练损失的最小范数解，这可能实现接近最优的泛化，从而证实了后期学习率衰减的经验优势。",
    "tldr": "本文研究了为什么使用大学习率长时间训练神经网络可以实现更好的泛化。通过观察训练和测试损失之间的关系，我们发现大学习率下的训练轨迹能够接近测试损失的最小值附近。基于这个发现，我们提出了一个与真实神经网络类似的非线性模型，并证明了使用大学习率进行延长阶段的训练可以实现更接近最优泛化的效果。",
    "en_tdlr": "This paper investigates the reason why training neural networks with large learning rates for a longer time leads to better generalization. By examining the relationship between training and testing loss, the authors find that the training trajectory with a large learning rate can approach the minimum value of the testing loss. Based on this finding, they propose a nonlinear model that closely resembles real neural networks, and demonstrate that an extended phase of training with a large learning rate can achieve near-optimal generalization. This confirms the empirically observed benefits of late learning rate decay."
}