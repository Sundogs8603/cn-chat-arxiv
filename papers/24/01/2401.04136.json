{
    "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline. (arXiv:2401.04136v1 [cs.CR])",
    "abstract": "The commercialization of diffusion models, renowned for their ability to generate high-quality images that are often indistinguishable from real ones, brings forth potential copyright concerns. Although attempts have been made to impede unauthorized access to copyrighted material during training and to subsequently prevent DMs from generating copyrighted images, the effectiveness of these solutions remains unverified. This study explores the vulnerabilities associated with copyright protection in DMs by introducing a backdoor data poisoning attack (SilentBadDiffusion) against text-to-image diffusion models. Our attack method operates without requiring access to or control over the diffusion model's training or fine-tuning processes; it merely involves the insertion of poisoning data into the clean training dataset. This data, comprising poisoning images equipped with prompts, is generated by leveraging the powerful capabilities of multimodal large language models and text-guided image ",
    "link": "http://arxiv.org/abs/2401.04136",
    "context": "Title: The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline. (arXiv:2401.04136v1 [cs.CR])\nAbstract: The commercialization of diffusion models, renowned for their ability to generate high-quality images that are often indistinguishable from real ones, brings forth potential copyright concerns. Although attempts have been made to impede unauthorized access to copyrighted material during training and to subsequently prevent DMs from generating copyrighted images, the effectiveness of these solutions remains unverified. This study explores the vulnerabilities associated with copyright protection in DMs by introducing a backdoor data poisoning attack (SilentBadDiffusion) against text-to-image diffusion models. Our attack method operates without requiring access to or control over the diffusion model's training or fine-tuning processes; it merely involves the insertion of poisoning data into the clean training dataset. This data, comprising poisoning images equipped with prompts, is generated by leveraging the powerful capabilities of multimodal large language models and text-guided image ",
    "path": "papers/24/01/2401.04136.json",
    "total_tokens": 786,
    "translated_title": "扩散模型强大，后门更容易: 数据污染引发版权侵犯而无需调整微调流程",
    "translated_abstract": "扩散模型的商业化引发了潜在的版权问题。本研究通过引入后门数据污染攻击（SilentBadDiffusion）来探讨扩散模型中与版权保护相关的漏洞。攻击方法无需访问或控制扩散模型的训练或微调过程，仅涉及将污染数据插入干净的训练数据集中。这些数据是通过利用多模态大型语言模型和文本引导图像生成的。",
    "tldr": "本研究探讨了扩散模型中存在的版权保护漏洞，并提出了一种后门数据污染攻击方法。这种攻击方法无需操作扩散模型的训练或微调过程，仅通过向训练数据集插入污染数据来实施攻击。"
}