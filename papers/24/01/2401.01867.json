{
    "title": "Dataset Difficulty and the Role of Inductive Bias. (arXiv:2401.01867v1 [cs.LG])",
    "abstract": "Motivated by the goals of dataset pruning and defect identification, a growing body of methods have been developed to score individual examples within a dataset. These methods, which we call \"example difficulty scores\", are typically used to rank or categorize examples, but the consistency of rankings between different training runs, scoring methods, and model architectures is generally unknown. To determine how example rankings vary due to these random and controlled effects, we systematically compare different formulations of scores over a range of runs and model architectures. We find that scores largely share the following traits: they are noisy over individual runs of a model, strongly correlated with a single notion of difficulty, and reveal examples that range from being highly sensitive to insensitive to the inductive biases of certain model architectures. Drawing from statistical genetics, we develop a simple method for fingerprinting model architectures using a few sensitive ",
    "link": "http://arxiv.org/abs/2401.01867",
    "context": "Title: Dataset Difficulty and the Role of Inductive Bias. (arXiv:2401.01867v1 [cs.LG])\nAbstract: Motivated by the goals of dataset pruning and defect identification, a growing body of methods have been developed to score individual examples within a dataset. These methods, which we call \"example difficulty scores\", are typically used to rank or categorize examples, but the consistency of rankings between different training runs, scoring methods, and model architectures is generally unknown. To determine how example rankings vary due to these random and controlled effects, we systematically compare different formulations of scores over a range of runs and model architectures. We find that scores largely share the following traits: they are noisy over individual runs of a model, strongly correlated with a single notion of difficulty, and reveal examples that range from being highly sensitive to insensitive to the inductive biases of certain model architectures. Drawing from statistical genetics, we develop a simple method for fingerprinting model architectures using a few sensitive ",
    "path": "papers/24/01/2401.01867.json",
    "total_tokens": 885,
    "translated_title": "数据集难度和归纳偏差的作用",
    "translated_abstract": "受数据集修剪和缺陷识别目标的启发，已经开发出一系列方法来评分数据集中的个别示例。这些方法，我们称之为“示例难度评分”，通常用于对示例进行排名或分类，但是不同训练运行、评分方法和模型架构之间的排名一致性通常是未知的。为了确定由于这些随机和受控效应而导致的示例排名的变化情况，我们系统地比较了不同得分公式在多个运行和模型架构的范围内。我们发现得分主要具有以下特点：它们在模型的单次运行中存在噪音，并且与一种难度概念强相关，并揭示了一些示例在某些模型架构的归纳偏差中从高度敏感到不敏感的变化。借鉴统计遗传学的方法，我们开发了一种简单的方法来使用一些敏感性指纹模型架构。",
    "tldr": "该论文研究了数据集中个别示例的难度评分，并发现不同训练运行、评分方法和模型架构之间的排名一致性通常是未知的。他们还提出了一种简单的方法来使用一些敏感性指纹模型架构。"
}