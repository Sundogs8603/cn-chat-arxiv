{
    "title": "Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection. (arXiv:2401.15123v1 [cs.LG])",
    "abstract": "Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose \\textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network's feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) W",
    "link": "http://arxiv.org/abs/2401.15123",
    "context": "Title: Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection. (arXiv:2401.15123v1 [cs.LG])\nAbstract: Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose \\textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network's feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) W",
    "path": "papers/24/01/2401.15123.json",
    "total_tokens": 937,
    "translated_title": "大型语言模型引导的知识蒸馏用于时间序列异常检测",
    "translated_abstract": "自监督方法在时间序列异常检测中变得越来越重要，因为可用标注数据的稀缺性。然而，它们通常需要大量的训练数据来获得可泛化的表示映射，这与只有少量样本的情况冲突，从而限制了它们的性能。为了克服这个限制，我们提出了一种基于知识蒸馏的时间序列异常检测方法AnomalyLLM，在这种方法中，学生网络被训练成模仿基于大规模数据集预训练的大型语言模型(LLM)的特征。在测试阶段，当学生网络与教师网络的特征差异很大时，就检测到异常。为了避免学生网络学习到教师网络对异常样本的特征，我们设计了两个关键策略。1) 将典型信号融入学生网络，以巩固正常特征提取。2) 加权教师网络和学生网络的特征，以减少异常样本的影响。",
    "tldr": "提出了一种基于大型语言模型引导的知识蒸馏的时间序列异常检测方法AnomalyLLM，通过训练学生网络模仿预训练的大型语言模型的特征，在测试阶段通过比较学生网络和教师网络的特征差异来检测异常。",
    "en_tdlr": "A knowledge distillation-based time series anomaly detection method, AnomalyLLM, is proposed, where the student network is trained to mimic the features of a pre-trained large language model. Anomalies are detected by comparing the features of the student network and the teacher network."
}