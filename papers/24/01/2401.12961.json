{
    "title": "Chatterbox: Robust Transport for LLM Token Streaming under Unstable Network. (arXiv:2401.12961v1 [cs.NI])",
    "abstract": "To render each generated token in real time, the LLM server generates response tokens one by one and streams each generated token (or group of a few tokens) through the network to the user right after it is generated, which we refer to as LLM token streaming. However, under unstable network conditions, the LLM token streaming experience could suffer greatly from stalls since one packet loss could block the rendering of tokens contained in subsequent packets even if they arrive on time. With a real-world measurement study, we show that current applications including ChatGPT, Claude, and Bard all suffer from increased stall under unstable network.  For this emerging token streaming problem in LLM Chatbots, we propose a novel transport layer scheme, called Chatterbox, which puts new generated tokens as well as currently unacknowledged tokens in the next outgoing packet. This ensures that each packet contains some new tokens and can be independently rendered when received, thus avoiding af",
    "link": "http://arxiv.org/abs/2401.12961",
    "context": "Title: Chatterbox: Robust Transport for LLM Token Streaming under Unstable Network. (arXiv:2401.12961v1 [cs.NI])\nAbstract: To render each generated token in real time, the LLM server generates response tokens one by one and streams each generated token (or group of a few tokens) through the network to the user right after it is generated, which we refer to as LLM token streaming. However, under unstable network conditions, the LLM token streaming experience could suffer greatly from stalls since one packet loss could block the rendering of tokens contained in subsequent packets even if they arrive on time. With a real-world measurement study, we show that current applications including ChatGPT, Claude, and Bard all suffer from increased stall under unstable network.  For this emerging token streaming problem in LLM Chatbots, we propose a novel transport layer scheme, called Chatterbox, which puts new generated tokens as well as currently unacknowledged tokens in the next outgoing packet. This ensures that each packet contains some new tokens and can be independently rendered when received, thus avoiding af",
    "path": "papers/24/01/2401.12961.json",
    "total_tokens": 989,
    "translated_title": "聊天宝盒：不稳定网络下LLM Token Streaming的稳健传输",
    "translated_abstract": "为了实时渲染生成的令牌，LLM服务器逐个生成响应令牌，并通过网络将每个生成的令牌（或少量令牌组）流式传输到用户，我们称之为LLM令牌流。然而，在不稳定的网络条件下，LLM令牌传输体验可能会受到极大的停顿影响，因为一次数据包丢失可能会阻塞后续数据包中包含的令牌的渲染，即使它们按时到达。通过真实世界的测量研究，我们发现当前的应用程序（包括ChatGPT，Claude和Bard）在不稳定网络条件下都会遭受停顿问题的增加。针对LLM Chatbots中出现的这个新兴的令牌流问题，我们提出了一种新颖的传输层方案，称为聊天宝盒（Chatterbox），它将新生成的令牌和当前未确认的令牌放入下一个发送的数据包中。这样，每个数据包都包含一些新的令牌，并且在接收到时可以独立进行渲染，从而避免了停顿问题。",
    "tldr": "聊天宝盒（Chatterbox）是针对LLM Chatbots中的令牌流问题提出的一种新颖的传输层方案，通过将新生成的令牌和当前未确认的令牌放入下一个发送的数据包中，实现了稳定的传输和渲染，避免了在不稳定网络环境下的停顿现象。",
    "en_tdlr": "Chatterbox is a novel transport layer scheme proposed for the token streaming problem in LLM Chatbots, which ensures stable transmission and rendering by putting new generated tokens and currently unacknowledged tokens in the next outgoing packet, avoiding stalls in unstable network conditions."
}