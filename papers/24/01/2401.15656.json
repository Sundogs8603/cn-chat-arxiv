{
    "title": "LLsM: Generative Linguistic Steganography with Large Language Model",
    "abstract": "Linguistic Steganography (LS) tasks aim to generate steganographic text (stego) based on secret information. Only authorized recipients can perceive the existence of secrets in the texts and extract them, thereby preserving privacy. However, the controllability of the stego generated by existing schemes is poor, and the stego is difficult to contain specific discourse characteristics such as style. As a result, the stego is easily detectable, compromising covert communication. To address these problems, this paper proposes LLsM, the first LS with the Large Language Model (LLM). We fine-tuned the LLaMA2 with a large-scale constructed dataset encompassing rich discourse characteristics, which enables the fine-tuned LLM to generate texts with specific discourse in a controllable manner. Then the discourse is used as guiding information and inputted into the fine-tuned LLM in the form of the Prompt together with secret. On this basis, the constructed candidate pool will be range encoded an",
    "link": "https://arxiv.org/abs/2401.15656",
    "context": "Title: LLsM: Generative Linguistic Steganography with Large Language Model\nAbstract: Linguistic Steganography (LS) tasks aim to generate steganographic text (stego) based on secret information. Only authorized recipients can perceive the existence of secrets in the texts and extract them, thereby preserving privacy. However, the controllability of the stego generated by existing schemes is poor, and the stego is difficult to contain specific discourse characteristics such as style. As a result, the stego is easily detectable, compromising covert communication. To address these problems, this paper proposes LLsM, the first LS with the Large Language Model (LLM). We fine-tuned the LLaMA2 with a large-scale constructed dataset encompassing rich discourse characteristics, which enables the fine-tuned LLM to generate texts with specific discourse in a controllable manner. Then the discourse is used as guiding information and inputted into the fine-tuned LLM in the form of the Prompt together with secret. On this basis, the constructed candidate pool will be range encoded an",
    "path": "papers/24/01/2401.15656.json",
    "total_tokens": 907,
    "translated_title": "LLsM: 基于大型语言模型的生成式语言隐写术",
    "translated_abstract": "语言隐写术（LS）旨在根据秘密信息生成隐写文本（stego）。只有授权接收者才能察觉文本中秘密的存在并提取出来，从而保护隐私。然而，现有方案生成的隐写文本可控性较差，很难包含特定的话语特征，如风格。结果，隐写文本容易被检测出来，危及隐蔽通信。为解决这些问题，本文提出了LLsM，第一个基于大型语言模型（LLM）的LS方法。我们使用一个包含丰富话语特征的大规模构建数据集对LLaMA2进行微调，使得微调后的LLM能够以可控的方式生成具有特定话语特征的文本。然后将话语作为引导信息和秘密一起输入给微调后的LLM，形式为“Prompt”。在此基础上，构建的候选池将进行范围编码。",
    "tldr": "本研究提出了LLsM，一种基于大型语言模型的生成式语言隐写术。通过对大规模数据集进行微调，LLM能够以可控的方式生成具有特定话语特征的隐写文本，提高了隐蔽通信的效果。",
    "en_tdlr": "This paper introduces LLsM, a generative linguistic steganography method based on a large language model. By fine-tuning the model with a large-scale dataset, LLsM can generate steganographic text with specific discourse characteristics in a controllable manner, enhancing covert communication."
}