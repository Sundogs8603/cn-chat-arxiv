{
    "title": "Analytically-Driven Resource Management for Cloud-Native Microservices. (arXiv:2401.02920v1 [cs.DC])",
    "abstract": "Resource management for cloud-native microservices has attracted a lot of recent attention. Previous work has shown that machine learning (ML)-driven approaches outperform traditional techniques, such as autoscaling, in terms of both SLA maintenance and resource efficiency. However, ML-driven approaches also face challenges including lengthy data collection processes and limited scalability. We present Ursa, a lightweight resource management system for cloud-native microservices that addresses these challenges. Ursa uses an analytical model that decomposes the end-to-end SLA into per-service SLA, and maps per-service SLA to individual resource allocations per microservice tier. To speed up the exploration process and avoid prolonged SLA violations, Ursa explores each microservice individually, and swiftly stops exploration if latency exceeds its SLA.  We evaluate Ursa on a set of representative and end-to-end microservice topologies, including a social network, media service and video ",
    "link": "http://arxiv.org/abs/2401.02920",
    "context": "Title: Analytically-Driven Resource Management for Cloud-Native Microservices. (arXiv:2401.02920v1 [cs.DC])\nAbstract: Resource management for cloud-native microservices has attracted a lot of recent attention. Previous work has shown that machine learning (ML)-driven approaches outperform traditional techniques, such as autoscaling, in terms of both SLA maintenance and resource efficiency. However, ML-driven approaches also face challenges including lengthy data collection processes and limited scalability. We present Ursa, a lightweight resource management system for cloud-native microservices that addresses these challenges. Ursa uses an analytical model that decomposes the end-to-end SLA into per-service SLA, and maps per-service SLA to individual resource allocations per microservice tier. To speed up the exploration process and avoid prolonged SLA violations, Ursa explores each microservice individually, and swiftly stops exploration if latency exceeds its SLA.  We evaluate Ursa on a set of representative and end-to-end microservice topologies, including a social network, media service and video ",
    "path": "papers/24/01/2401.02920.json",
    "total_tokens": 924,
    "translated_title": "基于分析的云原生微服务资源管理",
    "translated_abstract": "云原生微服务的资源管理近年来受到了很多关注。先前的研究表明，机器学习驱动的方法在SLA维护和资源效率方面优于传统的技术，如自动伸缩。然而，机器学习驱动的方法也面临着数据收集过程冗长和可扩展性有限等挑战。本文介绍了Ursa，一个轻量级的云原生微服务资源管理系统，旨在解决这些挑战。Ursa使用一个分析模型将端到端SLA分解为每个服务的SLA，并将每个服务的SLA映射到每个微服务层的资源分配。为了加快探索过程并避免长时间的SLA违规，Ursa逐个微服务进行探索，并在延迟超过其SLA时快速停止探索。我们在一组代表性的端到端微服务拓扑上评估了Ursa，包括社交网络、媒体服务和视频等。",
    "tldr": "本文介绍了一种基于分析模型的轻量级云原生微服务资源管理系统Ursa，通过将端到端SLA分解为每个服务的SLA，并在每个微服务层进行资源分配，以解决机器学习驱动方法中数据收集过程冗长和可扩展性有限的挑战。",
    "en_tdlr": "This paper introduces Ursa, a lightweight resource management system for cloud-native microservices, which uses an analytical model to decompose the end-to-end SLA into per-service SLA and maps per-service SLA to individual resource allocations per microservice tier. It addresses challenges faced by ML-driven approaches such as lengthy data collection processes and limited scalability."
}