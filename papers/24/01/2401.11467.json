{
    "title": "Over-Reasoning and Redundant Calculation of Large Language Models",
    "abstract": "arXiv:2401.11467v2 Announce Type: replace  Abstract: Large language models (LLMs) can solve problems step-by-step. While this chain-of-thought (CoT) reasoning boosts LLMs' performance, it is unclear if LLMs \\textit{know} when to use CoT and whether those CoT are always necessary to answer the question. This paper shows that LLMs tend to generate redundant calculations and reasoning on a manually constructed math QA dataset, GSM8K-Zero. GSM8K-Zero is constructed such that the questions can be answered without any calculations, but LLMs, including Llama-2 models and Claude-2, tend to generate lengthy and unnecessary calculations to answer the questions. We also conduct experiments to explain why LLMs generate redundant calculations and reasonings. GSM8K-Zero is publicly available at https://github.com/d223302/Over-Reasoning-of-LLMs and https://huggingface.co/datasets/dcml0714/GSM8K-Zero.",
    "link": "https://arxiv.org/abs/2401.11467",
    "context": "Title: Over-Reasoning and Redundant Calculation of Large Language Models\nAbstract: arXiv:2401.11467v2 Announce Type: replace  Abstract: Large language models (LLMs) can solve problems step-by-step. While this chain-of-thought (CoT) reasoning boosts LLMs' performance, it is unclear if LLMs \\textit{know} when to use CoT and whether those CoT are always necessary to answer the question. This paper shows that LLMs tend to generate redundant calculations and reasoning on a manually constructed math QA dataset, GSM8K-Zero. GSM8K-Zero is constructed such that the questions can be answered without any calculations, but LLMs, including Llama-2 models and Claude-2, tend to generate lengthy and unnecessary calculations to answer the questions. We also conduct experiments to explain why LLMs generate redundant calculations and reasonings. GSM8K-Zero is publicly available at https://github.com/d223302/Over-Reasoning-of-LLMs and https://huggingface.co/datasets/dcml0714/GSM8K-Zero.",
    "path": "papers/24/01/2401.11467.json",
    "total_tokens": 830,
    "translated_title": "大型语言模型的过度推理和冗余计算",
    "translated_abstract": "大型语言模型（LLMs）可以逐步解决问题。尽管这种思维链（CoT）推理提升了LLMs的性能，但目前尚不清楚LLMs何时会使用CoT，以及这些CoT是否总是必要的来回答问题。本文表明，LLMs倾向于在一个手动构建的数学问答数据集GSM8K-Zero上生成冗余的计算和推理。GSM8K-Zero被构建为这样的问答可以在不做任何计算的情况下回答，但LLMs，包括Llama-2模型和Claude-2，倾向于生成冗长且不必要的计算来回答问题。我们还进行了实验解释为什么LLMs会生成冗余的计算和推理。GSM8K-Zero可以在https://github.com/d223302/Over-Reasoning-of-LLMs 和https://huggingface.co/datasets/dcml0714/GSM8K-Zero 公开获取。",
    "tldr": "本文研究表明，大型语言模型在回答问题时倾向于生成冗余的计算和推理，即使这些计算并不必要。",
    "en_tdlr": "This paper demonstrates that large language models tend to generate redundant calculations and reasoning when answering questions, even when these calculations are unnecessary."
}