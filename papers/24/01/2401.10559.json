{
    "title": "OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy. (arXiv:2401.10559v1 [cs.LG])",
    "abstract": "We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel multi-adapter method, OrchMoE, which capitalizes on modular skill architecture for enhanced forward transfer in neural networks. Unlike prior models that depend on explicit task identification inputs, OrchMoE automatically discerns task categories, streamlining the learning process. This is achieved through an integrated mechanism comprising an Automatic Task Classification module and a Task-Skill Allocation module, which collectively deduce task-specific classifications and tailor skill allocation matrices. Our extensive evaluations on the 'Super Natural Instructions' dataset, featuring 1,600 diverse instructional tasks, indicate that OrchMoE substantially outperforms comparable multi-adapter baselines in terms of both performance and sample utilization efficiency, all while operating within the same parameter constraints. These findings suggest that OrchMoE offers a significant leap forward in multi-task le",
    "link": "http://arxiv.org/abs/2401.10559",
    "context": "Title: OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy. (arXiv:2401.10559v1 [cs.LG])\nAbstract: We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel multi-adapter method, OrchMoE, which capitalizes on modular skill architecture for enhanced forward transfer in neural networks. Unlike prior models that depend on explicit task identification inputs, OrchMoE automatically discerns task categories, streamlining the learning process. This is achieved through an integrated mechanism comprising an Automatic Task Classification module and a Task-Skill Allocation module, which collectively deduce task-specific classifications and tailor skill allocation matrices. Our extensive evaluations on the 'Super Natural Instructions' dataset, featuring 1,600 diverse instructional tasks, indicate that OrchMoE substantially outperforms comparable multi-adapter baselines in terms of both performance and sample utilization efficiency, all while operating within the same parameter constraints. These findings suggest that OrchMoE offers a significant leap forward in multi-task le",
    "path": "papers/24/01/2401.10559.json",
    "total_tokens": 823,
    "translated_title": "OrchMoE：具有任务-技能协同效应的高效多适配器学习",
    "translated_abstract": "我们通过创新的多适配器方法OrchMoE推进了参数效率微调（PEFT）领域，利用模块化技能架构增强神经网络的前向传递。与依赖显式任务识别输入的先前模型不同，OrchMoE自动识别任务类别，简化学习过程。这是通过一个整合机制实现的，包括自动任务分类模块和任务-技能分配模块，共同推断任务特定的分类并调整技能分配矩阵。我们在“超自然指令”数据集上进行了广泛的评估，该数据集包含1,600个多样的指令任务，结果表明OrchMoE在性能和样本利用效率方面明显优于可比的多适配器基线，并且在相同参数限制下运行。这些发现表明，OrchMoE在多任务学习方面取得了重大进展。",
    "tldr": "OrchMoE通过利用模块化技能架构和自动任务识别，提升了参数效率微调领域的性能，实现了对多任务学习的重大进展。",
    "en_tdlr": "OrchMoE achieves significant progress in the field of parameter-efficient fine-tuning by leveraging modular skill architecture and automatic task identification, enhancing performance in multi-task learning."
}