{
    "title": "Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning. (arXiv:2401.05787v1 [cs.CL])",
    "abstract": "While chain-of-thought (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR)) suffer from limitations like slowness, limited context grounding, hallucination and inconsistent outputs. To overcome these challenges, we introduce Evidence to Generate (E2G), a novel single-agent, two-step prompting framework. Instead of unverified reasoning claims, this innovative approach leverages the power of \"evidence for decision making\" by first focusing exclusively on the thought sequences (the series of intermediate steps) explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM's output generation process with greater precision and efficiency. This simple yet powerful approach unlocks the true potential of chain-of-thought like prompting, paving the way for faster, more reliable, and more contextually aware reasoning in LLM",
    "link": "http://arxiv.org/abs/2401.05787",
    "context": "Title: Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning. (arXiv:2401.05787v1 [cs.CL])\nAbstract: While chain-of-thought (CoT) prompting has revolutionized how LLMs perform reasoning tasks, its current methods and variations (e.g, Self-consistency, ReACT, Reflexion, Tree-of-Thoughts (ToT), Cumulative Reasoning (CR)) suffer from limitations like slowness, limited context grounding, hallucination and inconsistent outputs. To overcome these challenges, we introduce Evidence to Generate (E2G), a novel single-agent, two-step prompting framework. Instead of unverified reasoning claims, this innovative approach leverages the power of \"evidence for decision making\" by first focusing exclusively on the thought sequences (the series of intermediate steps) explicitly mentioned in the context which then serve as extracted evidence, guiding the LLM's output generation process with greater precision and efficiency. This simple yet powerful approach unlocks the true potential of chain-of-thought like prompting, paving the way for faster, more reliable, and more contextually aware reasoning in LLM",
    "path": "papers/24/01/2401.05787.json",
    "total_tokens": 990,
    "translated_title": "生成证据（E2G）：一种单代理的两步提示用于上下文辅助和检索增强推理",
    "translated_abstract": "虽然思维链（CoT）提示革新了LLMs执行推理任务的方式，但其当前的方法和变体（例如，自一致性，反应，反射，思维树（ToT），累积推理（CR））存在缓慢、有限的上下文接地、幻象和不一致的输出等限制。为了克服这些挑战，我们引入了Evidence to Generate（E2G）这一新颖的单代理、两步提示框架。这种创新的方法利用“决策的证据”的力量，而不是未经验证的推理主张，首先专注于在上下文中明确提及的思维序列（中间步骤的系列），然后将其作为提取的证据，以更高的精确度和效率引导LLM的输出生成过程。这种简单而强大的方法解锁了像链式思维提示这样的潜力，为LLM中更快、更可靠和更具上下文意识的推理铺平了道路。",
    "tldr": "本研究提出了Evidence to Generate（E2G）框架，采用单代理、两步提示的方法来解决目前链式思维提示存在的限制，通过利用上下文中明确提及的思维序列作为证据，以更高的精确度和效率引导LLM的输出生成过程，实现更快、更可靠和更具上下文意识的推理。",
    "en_tdlr": "This study introduces the Evidence to Generate (E2G) framework, which addresses the limitations of current chain-of-thought prompting methods by leveraging thought sequences mentioned in the context as evidence to guide the output generation process of LLMs with higher precision and efficiency, enabling faster, more reliable, and more contextually aware reasoning."
}