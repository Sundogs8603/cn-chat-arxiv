{
    "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference. (arXiv:2401.02038v1 [cs.CL])",
    "abstract": "The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.",
    "link": "http://arxiv.org/abs/2401.02038",
    "context": "Title: Understanding LLMs: A Comprehensive Overview from Training to Inference. (arXiv:2401.02038v1 [cs.CL])\nAbstract: The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.",
    "path": "papers/24/01/2401.02038.json",
    "total_tokens": 910,
    "translated_title": "理解LLMs：从训练到推理的全面概述",
    "translated_abstract": "ChatGPT的引入导致了大规模语言模型（LLMs）在解决下游任务中的大量使用。在这个背景下，对于成本效率的关注越来越多。低成本的LLMs训练和部署代表了未来的发展趋势。本文回顾了与这一新兴趋势相一致的大规模语言模型训练技术和推理部署技术的演变。训练的讨论包括数据预处理、训练架构、预训练任务、并行训练以及与模型微调相关的内容。在推理方面，本文涵盖了模型压缩、并行计算、内存调度和结构优化等主题。它还探讨了LLMs的利用并提供了对其未来发展的见解。",
    "tldr": "本文提供了一份综合概述，介绍了大规模语言模型（LLMs）从训练到推理的演变过程，并探讨了这一新兴趋势中与成本效率相关的训练和部署方法。同时，还讨论了推理阶段的模型压缩、并行计算、内存调度和结构优化等关键主题，为LLMs的利用和未来发展提供了见解。",
    "en_tdlr": "This paper provides a comprehensive overview of the evolution of Large Language Models (LLMs) from training to inference, with a focus on cost-efficient training and deployment. It discusses various aspects of training, including data preprocessing, architecture, pre-training tasks, parallel training, and model fine-tuning. On the inference side, it covers topics like model compression, parallel computation, memory scheduling, and structural optimization. The paper offers insights into the utilization and future development of LLMs."
}