{
    "title": "Data-Dependent Stability Analysis of Adversarial Training. (arXiv:2401.03156v1 [cs.LG])",
    "abstract": "Stability analysis is an essential aspect of studying the generalization ability of deep learning, as it involves deriving generalization bounds for stochastic gradient descent-based training algorithms. Adversarial training is the most widely used defense against adversarial example attacks. However, previous generalization bounds for adversarial training have not included information regarding the data distribution. In this paper, we fill this gap by providing generalization bounds for stochastic gradient descent-based adversarial training that incorporate data distribution information. We utilize the concepts of on-average stability and high-order approximate Lipschitz conditions to examine how changes in data distribution and adversarial budget can affect robust generalization gaps. Our derived generalization bounds for both convex and non-convex losses are at least as good as the uniform stability-based counterparts which do not include data distribution information. Furthermore, ",
    "link": "http://arxiv.org/abs/2401.03156",
    "context": "Title: Data-Dependent Stability Analysis of Adversarial Training. (arXiv:2401.03156v1 [cs.LG])\nAbstract: Stability analysis is an essential aspect of studying the generalization ability of deep learning, as it involves deriving generalization bounds for stochastic gradient descent-based training algorithms. Adversarial training is the most widely used defense against adversarial example attacks. However, previous generalization bounds for adversarial training have not included information regarding the data distribution. In this paper, we fill this gap by providing generalization bounds for stochastic gradient descent-based adversarial training that incorporate data distribution information. We utilize the concepts of on-average stability and high-order approximate Lipschitz conditions to examine how changes in data distribution and adversarial budget can affect robust generalization gaps. Our derived generalization bounds for both convex and non-convex losses are at least as good as the uniform stability-based counterparts which do not include data distribution information. Furthermore, ",
    "path": "papers/24/01/2401.03156.json",
    "total_tokens": 873,
    "translated_title": "对抗训练的数据相关稳定性分析",
    "translated_abstract": "稳定性分析是研究深度学习泛化能力的重要方面，它涉及到基于随机梯度下降的训练算法的推导泛化界限。对抗训练是最常用的对抗性示例攻击防御方法。然而，先前的对抗训练泛化界限并未包含数据分布的信息。本文填补了这一空白，提供了基于随机梯度下降的对抗训练的泛化界限，并纳入了数据分布信息。我们利用平均稳定性和高阶近似Lipschitz条件的概念，研究数据分布和对抗预算的变化如何影响鲁棒泛化差距。我们得到的凸损失和非凸损失的泛化界限至少与不包含数据分布信息的均匀稳定性相当。",
    "tldr": "本文提出了基于随机梯度下降的对抗训练的泛化界限，包含了数据分布的信息，通过研究数据分布和对抗预算的变化对鲁棒泛化差距的影响，得到了与均匀稳定性相当的泛化界限。",
    "en_tdlr": "This paper presents generalization bounds for stochastic gradient descent-based adversarial training that incorporate data distribution information, and examines the impact of changes in data distribution and adversarial budget on robust generalization gaps. The derived bounds are comparable to uniform stability-based counterparts that do not include data distribution information."
}