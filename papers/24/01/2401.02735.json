{
    "title": "Shared active subspace for multivariate vector-valued functions. (arXiv:2401.02735v1 [stat.ME])",
    "abstract": "This paper proposes several approaches as baselines to compute a shared active subspace for multivariate vector-valued functions. The goal is to minimize the deviation between the function evaluations on the original space and those on the reconstructed one. This is done either by manipulating the gradients or the symmetric positive (semi-)definite (SPD) matrices computed from the gradients of each component function so as to get a single structure common to all component functions. These approaches can be applied to any data irrespective of the underlying distribution unlike the existing vector-valued approach that is constrained to a normal distribution. We test the effectiveness of these methods on five optimization problems. The experiments show that, in general, the SPD-level methods are superior to the gradient-level ones, and are close to the vector-valued approach in the case of a normal distribution. Interestingly, in most cases it suffices to take the sum of the SPD matrices ",
    "link": "http://arxiv.org/abs/2401.02735",
    "context": "Title: Shared active subspace for multivariate vector-valued functions. (arXiv:2401.02735v1 [stat.ME])\nAbstract: This paper proposes several approaches as baselines to compute a shared active subspace for multivariate vector-valued functions. The goal is to minimize the deviation between the function evaluations on the original space and those on the reconstructed one. This is done either by manipulating the gradients or the symmetric positive (semi-)definite (SPD) matrices computed from the gradients of each component function so as to get a single structure common to all component functions. These approaches can be applied to any data irrespective of the underlying distribution unlike the existing vector-valued approach that is constrained to a normal distribution. We test the effectiveness of these methods on five optimization problems. The experiments show that, in general, the SPD-level methods are superior to the gradient-level ones, and are close to the vector-valued approach in the case of a normal distribution. Interestingly, in most cases it suffices to take the sum of the SPD matrices ",
    "path": "papers/24/01/2401.02735.json",
    "total_tokens": 871,
    "translated_title": "共享主动子空间用于多元向量值函数",
    "translated_abstract": "本文提出了几种方法作为基线来计算多元向量值函数的共享主动子空间。目标是最小化原始空间上的函数评估与重构空间上的函数评估之间的偏差。这些方法通过操纵梯度或从每个分量函数的梯度计算的对称正定（半正定）矩阵来获得所有分量函数的共同结构。与现有的只适用于正态分布的向量值方法不同，这些方法可以应用于任何数据无论其潜在分布。我们在五个优化问题上测试了这些方法的有效性。实验结果表明，总体而言， SPD级别的方法优于梯度级别的方法，并且在正态分布情况下接近向量值方法。有趣的是，在大多数情况下，只需取SPD矩阵之和即可。",
    "tldr": "本文提出了一种共享主动子空间的方法来处理多元向量值函数，可以通过操纵梯度或计算对称正定矩阵实现。实验结果表明，SPD级别的方法比梯度级别的方法更好，并且在正态分布情况下表现接近向量值方法。",
    "en_tdlr": "This paper proposes a method of shared active subspace for multivariate vector-valued functions, which can be achieved by manipulating gradients or calculating symmetric positive definite matrices. The experimental results show that SPD-level methods are superior to gradient-level methods and perform similarly to vector-valued methods in the case of a normal distribution."
}