{
    "title": "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. (arXiv:2401.14112v1 [cs.LG])",
    "abstract": "Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a sin",
    "link": "http://arxiv.org/abs/2401.14112",
    "context": "Title: FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. (arXiv:2401.14112v1 [cs.LG])\nAbstract: Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a sin",
    "path": "papers/24/01/2401.14112.json",
    "total_tokens": 915,
    "translated_title": "FP6-LLM: 通过FP6中心算法-系统协同设计高效提供大型语言模型",
    "translated_abstract": "六位量化（FP6）可以有效地减小大型语言模型（LLM）的大小，并在不同应用中保持模型质量的一致性。然而，现有系统不提供FP6量化的张量核心支持，并且在LLM推断过程中很难实现实际性能改进。由于（1）模型权重具有不规则位宽的不友好内存访问和（2）权重去量化的高运行时开销，支持在GPU上进行FP6量化是具有挑战性的。为了解决这些问题，我们提出了TC-FPx，这是第一个具有统一张量核心支持的浮点权重的完整GPU内核设计方案，适用于各种量化位宽。我们将TC-FPx内核集成到现有推断系统中，提供了新的端到端支持（称为FP6-LLM）用于量化LLM推断，从而实现了推断成本和模型质量之间更好的平衡。实验证明，FP6-LLM仅使用一部分存储空间就可以进行LLaMA-70b的推断。",
    "tldr": "FP6-LLM提出了一种支持六位量化的GPU算法-系统协同设计方案，实现了在大型语言模型中推断成本和模型质量之间的平衡。"
}