{
    "title": "Decentralized Gossip Mutual Learning (GML) for brain tumor segmentation on multi-parametric MRI. (arXiv:2401.15434v1 [eess.IV])",
    "abstract": "Federated Learning (FL) enables collaborative model training among medical centers without sharing private data. However, traditional FL risks on server failures and suboptimal performance on local data due to the nature of centralized model aggregation. To address these issues, we present Gossip Mutual Learning (GML), a decentralized framework that uses Gossip Protocol for direct peer-to-peer communication. In addition, GML encourages each site to optimize its local model through mutual learning to account for data variations among different sites. For the task of tumor segmentation using 146 cases from four clinical sites in BraTS 2021 dataset, we demonstrated GML outperformed local models and achieved similar performance as FedAvg with only 25% communication overhead.",
    "link": "http://arxiv.org/abs/2401.15434",
    "context": "Title: Decentralized Gossip Mutual Learning (GML) for brain tumor segmentation on multi-parametric MRI. (arXiv:2401.15434v1 [eess.IV])\nAbstract: Federated Learning (FL) enables collaborative model training among medical centers without sharing private data. However, traditional FL risks on server failures and suboptimal performance on local data due to the nature of centralized model aggregation. To address these issues, we present Gossip Mutual Learning (GML), a decentralized framework that uses Gossip Protocol for direct peer-to-peer communication. In addition, GML encourages each site to optimize its local model through mutual learning to account for data variations among different sites. For the task of tumor segmentation using 146 cases from four clinical sites in BraTS 2021 dataset, we demonstrated GML outperformed local models and achieved similar performance as FedAvg with only 25% communication overhead.",
    "path": "papers/24/01/2401.15434.json",
    "total_tokens": 867,
    "translated_title": "基于去中心化流言传播共学习的多参数MRI脑肿瘤分割",
    "translated_abstract": "联邦学习使医疗中心之间能够在不共享私密数据的情况下进行协作模型训练。然而，传统的联邦学习在服务器故障和局部数据下性能不佳的风险方面存在问题，这是由于集中式模型聚合的特性导致的。为了解决这些问题，我们提出了去中心化的流言传播共学习（GML）框架，该框架使用流言协议进行点对点通信。此外，GML通过共学习鼓励每个站点优化其本地模型以应对不同站点之间的数据变化。在使用来自BraTS 2021数据集的四个临床站点的146个案例进行肿瘤分割的任务中，我们证明了GML胜过本地模型，并在只有25％通信开销的情况下实现了与FedAvg相似的性能。",
    "tldr": "本论文提出了一种基于去中心化流言传播共学习的多参数MRI脑肿瘤分割框架，该框架实现了在不共享私密数据的情况下医疗中心之间的协作模型训练，并通过共学习来优化不同站点之间的数据变化。",
    "en_tdlr": "This paper introduces a decentralized gossip mutual learning framework for multi-parametric MRI brain tumor segmentation, which enables collaborative model training among medical centers without sharing private data, and optimizes data variations among different sites through mutual learning."
}