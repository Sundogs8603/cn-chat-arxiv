{
    "title": "Multi-Task Learning for Front-End Text Processing in TTS. (arXiv:2401.06321v1 [cs.CL])",
    "abstract": "We propose a multi-task learning (MTL) model for jointly performing three tasks that are commonly solved in a text-to-speech (TTS) front-end: text normalization (TN), part-of-speech (POS) tagging, and homograph disambiguation (HD). Our framework utilizes a tree-like structure with a trunk that learns shared representations, followed by separate task-specific heads. We further incorporate a pre-trained language model to utilize its built-in lexical and contextual knowledge, and study how to best use its embeddings so as to most effectively benefit our multi-task model. Through task-wise ablations, we show that our full model trained on all three tasks achieves the strongest overall performance compared to models trained on individual or sub-combinations of tasks, confirming the advantages of our MTL framework. Finally, we introduce a new HD dataset containing a balanced number of sentences in diverse contexts for a variety of homographs and their pronunciations. We demonstrate that inco",
    "link": "http://arxiv.org/abs/2401.06321",
    "context": "Title: Multi-Task Learning for Front-End Text Processing in TTS. (arXiv:2401.06321v1 [cs.CL])\nAbstract: We propose a multi-task learning (MTL) model for jointly performing three tasks that are commonly solved in a text-to-speech (TTS) front-end: text normalization (TN), part-of-speech (POS) tagging, and homograph disambiguation (HD). Our framework utilizes a tree-like structure with a trunk that learns shared representations, followed by separate task-specific heads. We further incorporate a pre-trained language model to utilize its built-in lexical and contextual knowledge, and study how to best use its embeddings so as to most effectively benefit our multi-task model. Through task-wise ablations, we show that our full model trained on all three tasks achieves the strongest overall performance compared to models trained on individual or sub-combinations of tasks, confirming the advantages of our MTL framework. Finally, we introduce a new HD dataset containing a balanced number of sentences in diverse contexts for a variety of homographs and their pronunciations. We demonstrate that inco",
    "path": "papers/24/01/2401.06321.json",
    "total_tokens": 1041,
    "translated_title": "TTS中前端文本处理的多任务学习",
    "translated_abstract": "我们提出了一个多任务学习模型，用于在文本到语音（TTS）前端中同时执行三个常见任务：文本归一化（TN），词性标注（POS）和同形异义词消歧（HD）。我们的框架利用一个类似树形的结构，其中一棵树学习共享表示，然后是单独的任务特定头部。我们进一步加入了一个预训练的语言模型，利用其内置的词汇和上下文知识，并研究如何最好地使用其嵌入以最有效地使我们的多任务模型受益。通过逐任务的消融实验证明，我们在所有三个任务上训练的完整模型相比于单独或部分任务组合训练的模型表现出最强的综合性能，验证了我们的MTL框架的优势。最后，我们介绍了一个新的HD数据集，其中包含各种上下文中平衡数量的句子，用于不同同形异义词及其发音。我们证明，在不同上下文中训练的模型在同形异义词消歧任务上取得了更好的性能。",
    "tldr": "我们提出了一个多任务学习模型，用于在TTS前端处理文本。我们的模型同时解决了文本归一化、词性标注和同形异义词消歧这三个任务，并结合了预训练的语言模型以提高性能。通过逐任务消融实验证明，我们的模型在所有三个任务上表现最好。我们还提供了一个包含各种上下文的新HD数据集，用于同形异义词消歧任务的评估。",
    "en_tdlr": "We propose a multi-task learning model for front-end text processing in TTS, which jointly performs text normalization, part-of-speech tagging, and homograph disambiguation. We also incorporate a pre-trained language model and show that our model achieves the best performance across all three tasks. Additionally, we introduce a new HD dataset that includes diverse contexts for evaluating homograph disambiguation."
}