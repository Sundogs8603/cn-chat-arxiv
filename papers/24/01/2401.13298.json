{
    "title": "Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models. (arXiv:2401.13298v1 [cs.CL])",
    "abstract": "The age of social media is flooded with Internet memes, necessitating a clear grasp and effective identification of harmful ones. This task presents a significant challenge due to the implicit meaning embedded in memes, which is not explicitly conveyed through the surface text and image. However, existing harmful meme detection methods do not present readable explanations that unveil such implicit meaning to support their detection decisions. In this paper, we propose an explainable approach to detect harmful memes, achieved through reasoning over conflicting rationales from both harmless and harmful positions. Specifically, inspired by the powerful capacity of Large Language Models (LLMs) on text generation and reasoning, we first elicit multimodal debate between LLMs to generate the explanations derived from the contradictory arguments. Then we propose to fine-tune a small language model as the debate judge for harmfulness inference, to facilitate multimodal fusion between the harmfu",
    "link": "http://arxiv.org/abs/2401.13298",
    "context": "Title: Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models. (arXiv:2401.13298v1 [cs.CL])\nAbstract: The age of social media is flooded with Internet memes, necessitating a clear grasp and effective identification of harmful ones. This task presents a significant challenge due to the implicit meaning embedded in memes, which is not explicitly conveyed through the surface text and image. However, existing harmful meme detection methods do not present readable explanations that unveil such implicit meaning to support their detection decisions. In this paper, we propose an explainable approach to detect harmful memes, achieved through reasoning over conflicting rationales from both harmless and harmful positions. Specifically, inspired by the powerful capacity of Large Language Models (LLMs) on text generation and reasoning, we first elicit multimodal debate between LLMs to generate the explanations derived from the contradictory arguments. Then we propose to fine-tune a small language model as the debate judge for harmfulness inference, to facilitate multimodal fusion between the harmfu",
    "path": "papers/24/01/2401.13298.json",
    "total_tokens": 908,
    "translated_title": "通过大型语言模型之间的多模态辩论实现可解释的有害模因检测",
    "translated_abstract": "社交媒体时代充斥着互联网模因，需要明确掌握和有效识别有害模因。由于模因中蕴含的隐含含义不能通过表面文本和图像明确传达，这一任务带来了重大挑战。然而，现有的有害模因检测方法未提供可读的解释以揭示这种隐含含义以支持其检测决策。在本文中，我们提出了一种可解释的有害模因检测方法，通过推理有害和无害立场之间的相互矛盾理由来实现。具体而言，受大型语言模型在文本生成和推理方面的强大能力启发，我们首先引发大型语言模型之间的多模态辩论，生成基于矛盾论据的解释。然后，我们提出使用精调的小型语言模型作为辩论裁判来推断有害性，以促进有害和无害信息的多模态融合。",
    "tldr": "本文提出了一种通过大型语言模型之间的多模态辩论实现可解释的有害模因检测方法。通过推理有害和无害立场之间的相互矛盾理由，生成可读的解释，提升有害模因检测的效果。"
}