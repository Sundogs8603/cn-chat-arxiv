{
    "title": "PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \\textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \\textit{meta-questions} spanning various domains. Each meta-question contains corresponding \\textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \\textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \\textit{proxy-questions}. We examine multiple LLMs, emphasizing \\t",
    "link": "http://arxiv.org/abs/2401.15042",
    "context": "Title: PROXYQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models. (arXiv:2401.15042v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have exhibited remarkable success in long-form context comprehension tasks. However, their capacity to generate long contents, such as reports and articles, remains insufficiently explored. Current benchmarks do not adequately assess LLMs' ability to produce informative and comprehensive content, necessitating a more rigorous evaluation approach. In this study, we introduce \\textsc{ProxyQA}, a framework for evaluating long-form text generation, comprising in-depth human-curated \\textit{meta-questions} spanning various domains. Each meta-question contains corresponding \\textit{proxy-questions} with annotated answers. LLMs are prompted to generate extensive content in response to these meta-questions. Utilizing an evaluator and incorporating generated content as background context, \\textsc{ProxyQA} evaluates the quality of generated content based on the evaluator's performance in answering the \\textit{proxy-questions}. We examine multiple LLMs, emphasizing \\t",
    "path": "papers/24/01/2401.15042.json",
    "total_tokens": 916,
    "translated_title": "PROXYQA：一种用于评估大型语言模型长篇文本生成的替代框架",
    "translated_abstract": "大型语言模型（LLM）在长篇文本理解任务中取得了显著的成功。然而，它们生成长篇内容（如报告和文章）的能力尚未得到充分探索。当前的基准不足以充分评估LLMs生成信息丰富且全面的内容，因此需要一种更严格的评估方法。在本研究中，我们介绍了一种名为\\textsc{ProxyQA}的框架，用于评估长篇文本生成，包括深入人工策划的涵盖多个领域的“元问题”。每个元问题都包含相应的带注释答案的“代理问题”。LLMs被要求根据这些元问题生成详尽的内容。利用评估器并将生成的内容作为背景环境，\\textsc{ProxyQA}根据评估器回答“代理问题”的表现评估生成内容的质量。我们检验了多个LLMs，重点关注了...",
    "tldr": "PROXYQA是一个用于评估大型语言模型长篇文本生成的替代框架，通过生成详尽的内容，并利用评估器和生成内容作为背景环境，根据评估器回答代理问题的表现来评估生成内容的质量。",
    "en_tdlr": "PROXYQA is an alternative framework for evaluating long-form text generation with large language models. It generates extensive content and evaluates the quality of the generated content based on an evaluator's performance in answering proxy questions, utilizing the generated content as background context."
}