{
    "title": "Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation. (arXiv:2401.15449v1 [cs.CL])",
    "abstract": "We evaluate the ability of Large Language Models (LLMs) to discern and express their internal knowledge state, a key factor in countering factual hallucination and ensuring reliable application of LLMs. We observe a robust self-awareness of internal knowledge state in LLMs, evidenced by over 85% accuracy in knowledge probing. However, LLMs often fail to express their internal knowledge during generation, leading to factual hallucinations. We develop an automated hallucination annotation tool, Dreamcatcher, which merges knowledge probing and consistency checking methods to rank factual preference data. Using knowledge preference as reward, We propose a Reinforcement Learning from Knowledge Feedback (RLKF) training framework, leveraging reinforcement learning to enhance the factuality and honesty of LLMs. Our experiments across multiple models show that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of",
    "link": "http://arxiv.org/abs/2401.15449",
    "context": "Title: Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation. (arXiv:2401.15449v1 [cs.CL])\nAbstract: We evaluate the ability of Large Language Models (LLMs) to discern and express their internal knowledge state, a key factor in countering factual hallucination and ensuring reliable application of LLMs. We observe a robust self-awareness of internal knowledge state in LLMs, evidenced by over 85% accuracy in knowledge probing. However, LLMs often fail to express their internal knowledge during generation, leading to factual hallucinations. We develop an automated hallucination annotation tool, Dreamcatcher, which merges knowledge probing and consistency checking methods to rank factual preference data. Using knowledge preference as reward, We propose a Reinforcement Learning from Knowledge Feedback (RLKF) training framework, leveraging reinforcement learning to enhance the factuality and honesty of LLMs. Our experiments across multiple models show that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of",
    "path": "papers/24/01/2401.15449.json",
    "total_tokens": 959,
    "translated_title": "学会相信你的感觉：利用自我意识在对抗LMM中减轻幻觉",
    "translated_abstract": "我们评估了大型语言模型（LLMs）辨别和表达其内部知识状态的能力，这是对抗事实性幻觉和确保LLMs可靠应用的关键因素。我们观察到LLMs具有稳健的内部知识状态的自我意识，通过超过85%的知识探索准确度来证明。然而，LLMs在生成过程中常常无法表达其内部知识，导致了事实幻觉的产生。我们开发了一个自动化幻觉标注工具\"Dreamcatcher\"，它将知识探测和一致性检查方法结合起来，对事实偏好数据进行排名。通过将知识偏好作为奖励，我们提出了一种从知识反馈中进行强化学习（RLKF）训练框架，利用强化学习增强LLMs的事实性和诚实性。我们在多个模型上进行的实验表明，RLKF训练有效地增强了模型利用其内部知识状态的能力，在多个方面提升了性能。",
    "tldr": "该论文评估了大型语言模型（LLMs）辨别和表达其内部知识状态的能力。研究发现LLMs具有自我意识的内部知识状态，并提出了一个强化学习训练框架来增强模型的事实性和诚实性。",
    "en_tdlr": "This paper evaluates the ability of Large Language Models (LLMs) to discern and express their internal knowledge state. It finds that LLMs exhibit self-awareness of their internal knowledge state and proposes a reinforcement learning training framework to enhance the factuality and honesty of the models."
}