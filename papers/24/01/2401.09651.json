{
    "title": "Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning. (arXiv:2401.09651v1 [cs.LG])",
    "abstract": "We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.",
    "link": "http://arxiv.org/abs/2401.09651",
    "context": "Title: Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning. (arXiv:2401.09651v1 [cs.LG])\nAbstract: We address a key challenge for neuro-symbolic (NeSy) systems by leveraging convex and bilevel optimization techniques to develop a general gradient-based framework for end-to-end neural and symbolic parameter learning. The applicability of our framework is demonstrated with NeuPSL, a state-of-the-art NeSy architecture. To achieve this, we propose a smooth primal and dual formulation of NeuPSL inference and show learning gradients are functions of the optimal dual variables. Additionally, we develop a dual block coordinate descent algorithm for the new formulation that naturally exploits warm-starts. This leads to over 100x learning runtime improvements over the current best NeuPSL inference method. Finally, we provide extensive empirical evaluations across $8$ datasets covering a range of tasks and demonstrate our learning framework achieves up to a 16% point prediction performance improvement over alternative learning methods.",
    "path": "papers/24/01/2401.09651.json",
    "total_tokens": 866,
    "translated_title": "神经符号推理和学习的凸二级优化研究",
    "translated_abstract": "通过利用凸二级优化技术，我们解决了神经符号系统的一个关键挑战，开发了一个通用的基于梯度的端到端神经和符号参数学习框架。我们利用最先进的神经符号体系结构NeuPSL来证明我们的框架的适用性。为了实现这一目标，我们提出了NeuPSL推理的平滑原始和对偶形式，并显示学习梯度是最优对偶变量的函数。此外，我们为新的形式开发了一种对偶块坐标下降算法，自然地利用了热启动。这使得我们相比当前最好的NeuPSL推理方法的学习时间改进了100倍以上。最后，我们对涵盖各种任务的8个数据集进行了广泛的实证评估，并证明我们的学习框架相比替代学习方法能够提升高达16%的预测性能。",
    "tldr": "本研究通过凸二级优化技术，开发了一个通用的基于梯度的神经和符号参数学习框架，具有100倍以上的学习时间改进和高达16%的预测性能提升。",
    "en_tdlr": "This research develops a general gradient-based framework for end-to-end neural and symbolic parameter learning using convex and bilevel optimization techniques, achieving over 100x learning runtime improvements and up to a 16% point prediction performance improvement."
}