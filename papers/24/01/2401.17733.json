{
    "title": "Towards Physical Plausibility in Neuroevolution Systems",
    "abstract": "The increasing usage of Artificial Intelligence (AI) models, especially Deep Neural Networks (DNNs), is increasing the power consumption during training and inference, posing environmental concerns and driving the need for more energy-efficient algorithms and hardware solutions. This work addresses the growing energy consumption problem in Machine Learning (ML), particularly during the inference phase. Even a slight reduction in power usage can lead to significant energy savings, benefiting users, companies, and the environment. Our approach focuses on maximizing the accuracy of Artificial Neural Network (ANN) models using a neuroevolutionary framework whilst minimizing their power consumption. To do so, power consumption is considered in the fitness function. We introduce a new mutation strategy that stochastically reintroduces modules of layers, with power-efficient modules having a higher chance of being chosen. We introduce a novel technique that allows training two separate models",
    "link": "https://arxiv.org/abs/2401.17733",
    "context": "Title: Towards Physical Plausibility in Neuroevolution Systems\nAbstract: The increasing usage of Artificial Intelligence (AI) models, especially Deep Neural Networks (DNNs), is increasing the power consumption during training and inference, posing environmental concerns and driving the need for more energy-efficient algorithms and hardware solutions. This work addresses the growing energy consumption problem in Machine Learning (ML), particularly during the inference phase. Even a slight reduction in power usage can lead to significant energy savings, benefiting users, companies, and the environment. Our approach focuses on maximizing the accuracy of Artificial Neural Network (ANN) models using a neuroevolutionary framework whilst minimizing their power consumption. To do so, power consumption is considered in the fitness function. We introduce a new mutation strategy that stochastically reintroduces modules of layers, with power-efficient modules having a higher chance of being chosen. We introduce a novel technique that allows training two separate models",
    "path": "papers/24/01/2401.17733.json",
    "total_tokens": 856,
    "translated_title": "在神经进化系统中迈向物理合理性",
    "translated_abstract": "使用人工智能模型，特别是深度神经网络，越来越多地增加了训练和推理过程中的功耗，引发了对更加节能算法和硬件解决方案的需求，并引起了环境担忧。本研究致力于解决机器学习中推理阶段日益增长的能源消耗问题。即使稍微减少电力使用也可能导致显著的能源节约，使用户、公司和环境都受益。我们的方法着重于在神经进化框架中最大化人工神经网络模型的准确性，同时最小化其功耗。为此，我们在适应度函数中考虑了功耗。我们引入一种新的突变策略，以随机方式重新引入层模块，具有节能模块的选择机会更高。我们还引入了一种训练两个独立模型的新技术。",
    "tldr": "本研究关注神经进化系统中物理合理性的问题，提出了一种旨在最大化人工神经网络模型准确性同时最小化功耗的方法，通过引入新的突变策略和训练技术来优化模型表现和节能效果。",
    "en_tdlr": "This study focuses on the physical plausibility in neuroevolution systems, proposing a method to maximize the accuracy of artificial neural network models while minimizing power consumption. It introduces a new mutation strategy and training technique to optimize the model performance and energy efficiency."
}