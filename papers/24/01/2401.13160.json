{
    "title": "SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection. (arXiv:2401.13160v1 [cs.LG])",
    "abstract": "Pre-training large language models is known to be extremely resource intensive and often times inefficient, under-utilizing the information encapsulated in the training text sequences. In this paper, we present SpacTor, a new training procedure consisting of (1) a hybrid objective combining span corruption (SC) and token replacement detection (RTD), and (2) a two-stage curriculum that optimizes the hybrid objective over the initial $\\tau$ iterations, then transitions to standard SC loss. We show empirically that the effectiveness of the hybrid objective is tied to the two-stage pre-training schedule, and provide extensive analysis on why this is the case. In our experiments with encoder-decoder architectures (T5) on a variety of NLP tasks, SpacTor-T5 yields the same downstream performance as standard SC pre-training, while enabling a 50% reduction in pre-training iterations and 40% reduction in total FLOPs. Alternatively, given the same amount of computing budget, we find that SpacTor ",
    "link": "http://arxiv.org/abs/2401.13160",
    "context": "Title: SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection. (arXiv:2401.13160v1 [cs.LG])\nAbstract: Pre-training large language models is known to be extremely resource intensive and often times inefficient, under-utilizing the information encapsulated in the training text sequences. In this paper, we present SpacTor, a new training procedure consisting of (1) a hybrid objective combining span corruption (SC) and token replacement detection (RTD), and (2) a two-stage curriculum that optimizes the hybrid objective over the initial $\\tau$ iterations, then transitions to standard SC loss. We show empirically that the effectiveness of the hybrid objective is tied to the two-stage pre-training schedule, and provide extensive analysis on why this is the case. In our experiments with encoder-decoder architectures (T5) on a variety of NLP tasks, SpacTor-T5 yields the same downstream performance as standard SC pre-training, while enabling a 50% reduction in pre-training iterations and 40% reduction in total FLOPs. Alternatively, given the same amount of computing budget, we find that SpacTor ",
    "path": "papers/24/01/2401.13160.json",
    "total_tokens": 1047,
    "translated_title": "SpacTor-T5：使用跨度破坏和替换词汇检测进行T5模型的预训练",
    "translated_abstract": "预训练大型语言模型被认为是极其耗费资源且经常低效，未能充分利用训练文本序列中所蕴含的信息。本文提出了SpacTor，一种新的训练方法，包括(1)将跨度破坏(SC)和替换词汇检测(RTD)结合的混合目标函数，和(2)一个两阶段课程表，首先在初始的$\\tau$迭代中优化混合目标函数，然后过渡到标准的SC损失。我们通过实验证明，混合目标函数的有效性与两阶段预训练进度表密切相关，并对此进行了详细分析。在我们对各种NLP任务进行的编码器-解码器架构(T5)的实验中，SpacTor-T5与标准的SC预训练具有相同的下游性能，同时使预训练迭代减少50%，总FLOP减少40%。另外，在给定相同的计算预算的情况下，我们发现SpacTor具有比标准SC预训练更好的性能。",
    "tldr": "本文提出了一种新的训练方法SpacTor-T5，结合了跨度破坏和替换词汇检测的混合目标函数，并采用两阶段课程表进行预训练。在实验中，SpacTor-T5在各种NLP任务中取得了与标准SC预训练相同的下游性能，同时大大减少了预训练迭代次数和总FLOP。",
    "en_tdlr": "This paper presents a new training method, SpacTor-T5, which combines span corruption and replaced token detection in a hybrid objective function and uses a two-stage curriculum for pre-training. Experimental results show that SpacTor-T5 achieves the same downstream performance as standard span corruption pre-training, while significantly reducing the number of pre-training iterations and total FLOPs."
}