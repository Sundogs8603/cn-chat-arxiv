{
    "title": "Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA",
    "abstract": "arXiv:2401.15847v2 Announce Type: replace-cross  Abstract: Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99\\% accurac",
    "link": "https://arxiv.org/abs/2401.15847",
    "context": "Title: Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA\nAbstract: arXiv:2401.15847v2 Announce Type: replace-cross  Abstract: Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99\\% accurac",
    "path": "papers/24/01/2401.15847.json",
    "total_tokens": 882,
    "translated_title": "松饼还是吉娃娃？用多面板VQA挑战大型视觉语言模型",
    "translated_abstract": "多面板图像，通常在网页截图、海报等中看到，充斥着我们的日常生活。这些图像以多个子图以不同布局组成，有效地向人们传达信息。为了构建高级的多模态人工智能应用，如能理解复杂场景并在网页中导航的代理程序，多面板视觉推理的技能是至关重要的，对模型在这方面进行全面评估是很重要的。因此，我们引入了多面板视觉问答（MultipanelVQA），这是一个新颖的基准，包括6,600个问题、答案和多面板图像三元组，专门挑战模型理解多面板图像。我们的评估表明，MultipanelVQA基准中的问题对测试的最先进的大型视觉语言模型（LVLMs）提出了重大挑战，即使人类可以获得约99%的准确率。",
    "tldr": "引入了多面板视觉问答（MultipanelVQA）基准挑战大型视觉语言模型（LVLMs）对理解多面板图像的能力，并发现LVLMs在这方面仍然存在显著挑战。",
    "en_tdlr": "Introduced the Multipanel Visual Question Answering (MultipanelVQA) benchmark to challenge Large Vision Language Models (LVLMs) in comprehending multipanel images, highlighting significant challenges that LVLMs still face in this area."
}