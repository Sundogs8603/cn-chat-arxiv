{
    "title": "MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing. (arXiv:2401.03190v1 [cs.CL])",
    "abstract": "Large language models are known for encoding a vast amount of factual knowledge, but they often becomes outdated due to the ever-changing nature of external information. A promising solution to this challenge is the utilization of model editing methods to update the knowledge in an efficient manner. However, the majority of existing model editing techniques are limited to monolingual frameworks, thus failing to address the crucial issue of cross-lingual knowledge synchronization for multilingual models. To tackle this problem, we propose a simple yet effective method that trains multilingual patch neuron to store cross-lingual knowledge. It can be easily adapted to existing approaches to enhance their cross-lingual editing capabilities. To evaluate our method, we conduct experiments using both the XNLI dataset and a self-constructed XFEVER dataset. Experimental results demonstrate that our proposed method achieves improved performance in cross-lingual editing tasks without requiring ex",
    "link": "http://arxiv.org/abs/2401.03190",
    "context": "Title: MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model Editing. (arXiv:2401.03190v1 [cs.CL])\nAbstract: Large language models are known for encoding a vast amount of factual knowledge, but they often becomes outdated due to the ever-changing nature of external information. A promising solution to this challenge is the utilization of model editing methods to update the knowledge in an efficient manner. However, the majority of existing model editing techniques are limited to monolingual frameworks, thus failing to address the crucial issue of cross-lingual knowledge synchronization for multilingual models. To tackle this problem, we propose a simple yet effective method that trains multilingual patch neuron to store cross-lingual knowledge. It can be easily adapted to existing approaches to enhance their cross-lingual editing capabilities. To evaluate our method, we conduct experiments using both the XNLI dataset and a self-constructed XFEVER dataset. Experimental results demonstrate that our proposed method achieves improved performance in cross-lingual editing tasks without requiring ex",
    "path": "papers/24/01/2401.03190.json",
    "total_tokens": 796,
    "translated_title": "MPN: 利用多语言补丁神经元进行跨语言模型编辑",
    "translated_abstract": "大型语言模型通常编码了大量的事实知识，但由于外部信息的不断变化，它们经常过时。解决这个挑战的一个有前途的方法是利用模型编辑方法以高效的方式更新知识。然而，大部分现有的模型编辑技术局限于单语框架，因此无法解决多语言模型的跨语言知识同步的关键问题。为了解决这个问题，我们提出了一种简单而有效的方法，即训练多语言补丁神经元来存储跨语言知识。它可以轻松适应现有方法，增强它们的跨语言编辑能力。为了评估我们的方法，我们使用XNLI数据集和自建的XFEVER数据集进行了实验。实验结果表明，我们提出的方法在跨语言编辑任务中实现了改进的性能，而不需要额外的工作。",
    "tldr": "本论文提出了一种简单而有效的方法，利用多语言补丁神经元来解决多语言模型的跨语言知识同步问题，并取得了良好的实验结果。"
}