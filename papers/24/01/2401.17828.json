{
    "title": "Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation",
    "abstract": "In recent years, weakly supervised semantic segmentation using image-level labels as supervision has received significant attention in the field of computer vision. Most existing methods have addressed the challenges arising from the lack of spatial information in these labels by focusing on facilitating supervised learning through the generation of pseudo-labels from class activation maps (CAMs). Due to the localized pattern detection of Convolutional Neural Networks (CNNs), CAMs often emphasize only the most discriminative parts of an object, making it challenging to accurately distinguish foreground objects from each other and the background. Recent studies have shown that Vision Transformer (ViT) features, due to their global view, are more effective in capturing the scene layout than CNNs. However, the use of hierarchical ViTs has not been extensively explored in this field. This work explores the use of Swin Transformer by proposing \"SWTformer\" to enhance the accuracy of the init",
    "link": "https://arxiv.org/abs/2401.17828",
    "context": "Title: Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation\nAbstract: In recent years, weakly supervised semantic segmentation using image-level labels as supervision has received significant attention in the field of computer vision. Most existing methods have addressed the challenges arising from the lack of spatial information in these labels by focusing on facilitating supervised learning through the generation of pseudo-labels from class activation maps (CAMs). Due to the localized pattern detection of Convolutional Neural Networks (CNNs), CAMs often emphasize only the most discriminative parts of an object, making it challenging to accurately distinguish foreground objects from each other and the background. Recent studies have shown that Vision Transformer (ViT) features, due to their global view, are more effective in capturing the scene layout than CNNs. However, the use of hierarchical ViTs has not been extensively explored in this field. This work explores the use of Swin Transformer by proposing \"SWTformer\" to enhance the accuracy of the init",
    "path": "papers/24/01/2401.17828.json",
    "total_tokens": 802,
    "translated_title": "利用Swin Transformer进行从局部到全局的弱监督语义分割",
    "translated_abstract": "最近几年，利用图像级标签作为监督的弱监督语义分割在计算机视觉领域引起了极大关注。大多数现有方法通过从类激活图(CAM)中生成伪标签来解决这些标签中缺乏空间信息的挑战。由于卷积神经网络(CNNs)的局部模式检测，CAMs通常只强调对象的最具区分度的部分，使得准确区分前景对象与背景以及彼此之间变得困难。最近的研究表明，由于其全局视角，Vision Transformer (ViT)特征在捕捉场景布局方面比CNNs更有效。然而，层次化的ViTs在该领域中的应用尚未得到广泛探索。本文通过提出“SWTformer”来探索使用Swin Transformer来提高初始准确性。",
    "tldr": "本研究利用Swin Transformer提出了\"SWTformer\"，通过从局部到全局的视角来增强弱监督语义分割的准确性。",
    "en_tdlr": "This study leverages Swin Transformer to propose \"SWTformer\" for enhancing the accuracy of local-to-global weakly supervised semantic segmentation."
}