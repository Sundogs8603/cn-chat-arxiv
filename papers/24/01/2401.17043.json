{
    "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models. (arXiv:2401.17043v1 [cs.CL])",
    "abstract": "Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \"hallucinated\" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the ran",
    "link": "http://arxiv.org/abs/2401.17043",
    "context": "Title: CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models. (arXiv:2401.17043v1 [cs.CL])\nAbstract: Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \"hallucinated\" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the ran",
    "path": "papers/24/01/2401.17043.json",
    "total_tokens": 814,
    "translated_title": "CRUD-RAG: 用于检索增强生成的大型语言模型的全面中文基准",
    "translated_abstract": "检索增强生成（RAG）是一种通过整合外部知识源来增强大型语言模型（LLM）能力的技术。该方法解决了LLM的常见限制，包括过时的信息和产生不准确的“虚构”内容的倾向。然而，评估RAG系统具有挑战性，因为现有的基准测试在范围和多样性上存在限制。大多数当前的基准测试主要评估问答应用，忽视了RAG可能有优势的更广泛的场景。此外，它们只评估RAG流程中LLM组件的性能，并忽视检索组件和外部知识数据库的影响。为了解决这些问题，本文构建了一个大规模且更全面的基准测试，并在各种RAG应用场景中评估了RAG系统的所有组件。",
    "tldr": "这篇论文构建了一个大规模且更全面的中文基准测试，评估了检索增强生成系统的所有组件在各种应用场景中的性能。"
}