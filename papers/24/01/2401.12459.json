{
    "title": "Towards Socially and Morally Aware RL agent: Reward Design With LLM. (arXiv:2401.12459v1 [cs.AI])",
    "abstract": "When we design and deploy an Reinforcement Learning (RL) agent, reward functions motivates agents to achieve an objective. An incorrect or incomplete specification of the objective can result in behavior that does not align with human values - failing to adhere with social and moral norms that are ambiguous and context dependent, and cause undesired outcomes such as negative side effects and exploration that is unsafe. Previous work have manually defined reward functions to avoid negative side effects, use human oversight for safe exploration, or use foundation models as planning tools. This work studies the ability of leveraging Large Language Models (LLM)' understanding of morality and social norms on safe exploration augmented RL methods. This work evaluates language model's result against human feedbacks and demonstrates language model's capability as direct reward signals.",
    "link": "http://arxiv.org/abs/2401.12459",
    "context": "Title: Towards Socially and Morally Aware RL agent: Reward Design With LLM. (arXiv:2401.12459v1 [cs.AI])\nAbstract: When we design and deploy an Reinforcement Learning (RL) agent, reward functions motivates agents to achieve an objective. An incorrect or incomplete specification of the objective can result in behavior that does not align with human values - failing to adhere with social and moral norms that are ambiguous and context dependent, and cause undesired outcomes such as negative side effects and exploration that is unsafe. Previous work have manually defined reward functions to avoid negative side effects, use human oversight for safe exploration, or use foundation models as planning tools. This work studies the ability of leveraging Large Language Models (LLM)' understanding of morality and social norms on safe exploration augmented RL methods. This work evaluates language model's result against human feedbacks and demonstrates language model's capability as direct reward signals.",
    "path": "papers/24/01/2401.12459.json",
    "total_tokens": 801,
    "translated_title": "朝着具有社会和道德意识的强化学习智能体：使用LLM进行奖励设计",
    "translated_abstract": "在设计和部署强化学习（RL）智能体时，奖励函数激励智能体实现一个目标。目标的不正确或不完整的规范可能导致与人类价值观不一致的行为，不遵守模糊和依赖上下文的社会和道德规范，并导致负面副作用和不安全的探索等不希望的结果。先前的工作通过手动定义奖励函数来避免负面副作用，使用人类监督进行安全探索，或使用基础模型作为规划工具。本研究研究了利用大型语言模型（LLM）对安全探索增强强化学习方法中的道德和社会规范的理解能力。本研究通过与人类反馈评估语言模型的结果，并展示语言模型作为直接奖励信号的能力。",
    "tldr": "本文研究了利用大型语言模型的理解能力，以遵循社会和道德规范，在强化学习中进行安全探索，并将语言模型的结果作为直接奖励信号。"
}