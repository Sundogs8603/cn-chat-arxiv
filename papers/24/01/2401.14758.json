{
    "title": "Off-Policy Primal-Dual Safe Reinforcement Learning. (arXiv:2401.14758v1 [cs.LG])",
    "abstract": "Primal-dual safe RL methods commonly perform iterations between the primal update of the policy and the dual update of the Lagrange Multiplier. Such a training paradigm is highly susceptible to the error in cumulative cost estimation since this estimation serves as the key bond connecting the primal and dual update processes. We show that this problem causes significant underestimation of cost when using off-policy methods, leading to the failure to satisfy the safety constraint. To address this issue, we propose \\textit{conservative policy optimization}, which learns a policy in a constraint-satisfying area by considering the uncertainty in cost estimation. This improves constraint satisfaction but also potentially hinders reward maximization. We then introduce \\textit{local policy convexification} to help eliminate such suboptimality by gradually reducing the estimation uncertainty. We provide theoretical interpretations of the joint coupling effect of these two ingredients and furth",
    "link": "http://arxiv.org/abs/2401.14758",
    "context": "Title: Off-Policy Primal-Dual Safe Reinforcement Learning. (arXiv:2401.14758v1 [cs.LG])\nAbstract: Primal-dual safe RL methods commonly perform iterations between the primal update of the policy and the dual update of the Lagrange Multiplier. Such a training paradigm is highly susceptible to the error in cumulative cost estimation since this estimation serves as the key bond connecting the primal and dual update processes. We show that this problem causes significant underestimation of cost when using off-policy methods, leading to the failure to satisfy the safety constraint. To address this issue, we propose \\textit{conservative policy optimization}, which learns a policy in a constraint-satisfying area by considering the uncertainty in cost estimation. This improves constraint satisfaction but also potentially hinders reward maximization. We then introduce \\textit{local policy convexification} to help eliminate such suboptimality by gradually reducing the estimation uncertainty. We provide theoretical interpretations of the joint coupling effect of these two ingredients and furth",
    "path": "papers/24/01/2401.14758.json",
    "total_tokens": 912,
    "translated_title": "Off-Policy Primal-Dual Safe Reinforcement Learning. (arXiv:2401.14758v1 [cs.LG]) 论文的题目是：离策略原双安全强化学习",
    "translated_abstract": "原双安全强化学习方法通常在策略的原始更新和拉格朗日乘子的对偶更新之间进行迭代。由于累积成本估计作为连接原始和对偶更新过程的关键联系，这种训练范式极易受到累积成本估计误差的影响。我们表明，这个问题导致离策略方法使用时成本被严重低估，无法满足安全约束。为了解决这个问题，我们提出了一种“保守策略优化”的方法，通过考虑成本估计的不确定性，在约束满足的区域学习策略。这提高了约束的满足性，但也可能阻碍了奖励最大化。然后，我们引入了“局部策略凸化”来助于消除这种次优性，逐渐减小估计的不确定性。我们对这两个成分的联合作用进行了理论解释。",
    "tldr": "该论文提出了离策略原双安全强化学习方法，通过引入保守策略优化和局部策略凸化来解决累积成本估计误差导致的安全约束不满足问题。"
}