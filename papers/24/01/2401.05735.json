{
    "title": "Object-Centric Diffusion for Efficient Video Editing. (arXiv:2401.05735v1 [cs.CV])",
    "abstract": "Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as OCD, to further reduce latency by allocating computations more towards foreground edited regions that are arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient regions or background, allocating most of the model capacity to the former, and ii) Object-Centric 3D Token Merging, whi",
    "link": "http://arxiv.org/abs/2401.05735",
    "context": "Title: Object-Centric Diffusion for Efficient Video Editing. (arXiv:2401.05735v1 [cs.CV])\nAbstract: Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as OCD, to further reduce latency by allocating computations more towards foreground edited regions that are arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient regions or background, allocating most of the model capacity to the former, and ii) Object-Centric 3D Token Merging, whi",
    "path": "papers/24/01/2401.05735.json",
    "total_tokens": 892,
    "translated_title": "面向对象的扩散技术实现高效视频编辑",
    "translated_abstract": "基于扩散的视频编辑已经达到了令人印象深刻的质量，并且可以根据编辑提示来转换视频的全局风格、局部结构和属性。然而，这些解决方案通常需要使用大量的内存和计算资源来生成具有时序一致性的帧，可能涉及扩散反演和/或跨帧注意力。在本文中，我们对这种低效性进行了分析，并提出了简单而有效的修改，可以显著提高速度同时保持质量。此外，我们引入了面向对象的扩散技术（OCD），通过将计算资源更多地分配给对感知质量更重要的前景编辑区域，进一步降低延迟。我们通过两个新的提案来实现这一点：i）面向对象的采样，将用于显著区域或背景的扩散步骤与用于前景的扩散步骤分离开来，将大部分模型容量分配给前者；ii）面向对象的3D令牌合并，用于改善前景和背景之间的混合。",
    "tldr": "本论文提出了一种面向对象的扩散技术，通过分配更多的计算资源给前景编辑区域来实现视频编辑的高效率，从而大大提高了速度，同时保持了质量。"
}