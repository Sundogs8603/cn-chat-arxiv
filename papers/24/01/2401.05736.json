{
    "title": "Cross-modal Retrieval for Knowledge-based Visual Question Answering. (arXiv:2401.05736v1 [cs.CL])",
    "abstract": "Knowledge-based Visual Question Answering about Named Entities is a challenging task that requires retrieving information from a multimodal Knowledge Base. Named entities have diverse visual representations and are therefore difficult to recognize. We argue that cross-modal retrieval may help bridge the semantic gap between an entity and its depictions, and is foremost complementary with mono-modal retrieval. We provide empirical evidence through experiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE, InfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different strategies to fine-tune such a model: mono-modal, cross-modal, or joint training. Our method, which combines mono-and cross-modal retrieval, is competitive with billion-parameter models on the three datasets, while being conceptually simpler and computationally cheaper.",
    "link": "http://arxiv.org/abs/2401.05736",
    "context": "Title: Cross-modal Retrieval for Knowledge-based Visual Question Answering. (arXiv:2401.05736v1 [cs.CL])\nAbstract: Knowledge-based Visual Question Answering about Named Entities is a challenging task that requires retrieving information from a multimodal Knowledge Base. Named entities have diverse visual representations and are therefore difficult to recognize. We argue that cross-modal retrieval may help bridge the semantic gap between an entity and its depictions, and is foremost complementary with mono-modal retrieval. We provide empirical evidence through experiments with a multimodal dual encoder, namely CLIP, on the recent ViQuAE, InfoSeek, and Encyclopedic-VQA datasets. Additionally, we study three different strategies to fine-tune such a model: mono-modal, cross-modal, or joint training. Our method, which combines mono-and cross-modal retrieval, is competitive with billion-parameter models on the three datasets, while being conceptually simpler and computationally cheaper.",
    "path": "papers/24/01/2401.05736.json",
    "total_tokens": 868,
    "translated_title": "基于多模态检索的知识驱动视觉问答",
    "translated_abstract": "针对基于命名实体的知识驱动视觉问答这一具有挑战性的任务，需要从多模态知识库中检索信息。命名实体具有多样化的视觉表现，因此很难识别。我们认为，跨模态检索可以帮助弥合实体与其表现之间的语义鸿沟，并且与单模态检索相辅相成。通过对最近的ViQuAE、InfoSeek和Encyclopedic-VQA数据集进行多模态双编码器（即CLIP）实验，我们提供了经验证据。此外，我们研究了三种不同的模型微调策略：单模态、跨模态或联合训练。我们的方法将单模态和跨模态检索相结合，在三个数据集上与亿级参数模型竞争，而且在概念上更简单、计算成本更低。",
    "tldr": "该论文提出了一种基于多模态检索的知识驱动视觉问答方法，通过跨模态检索来弥合实体与其视觉表现之间的语义鸿沟。实验证明，该方法与亿级参数模型在多个数据集上具有竞争力，同时在概念上更简单、计算成本更低。"
}