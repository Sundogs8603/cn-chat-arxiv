{
    "title": "Hebbian Learning from First Principles. (arXiv:2401.07110v1 [cond-mat.dis-nn])",
    "abstract": "Recently, the original storage prescription for the Hopfield model of neural networks -- as well as for its dense generalizations -- has been turned into a genuine Hebbian learning rule by postulating the expression of its Hamiltonian for both the supervised and unsupervised protocols. In these notes, first, we obtain these explicit expressions by relying upon maximum entropy extremization \\`a la Jaynes. Beyond providing a formal derivation of these recipes for Hebbian learning, this construction also highlights how Lagrangian constraints within entropy extremization force network's outcomes on neural correlations: these try to mimic the empirical counterparts hidden in the datasets provided to the network for its training and, the denser the network, the longer the correlations that it is able to capture. Next, we prove that, in the big data limit, whatever the presence of a teacher (or its lacking), not only these Hebbian learning rules converge to the original storage prescription o",
    "link": "http://arxiv.org/abs/2401.07110",
    "context": "Title: Hebbian Learning from First Principles. (arXiv:2401.07110v1 [cond-mat.dis-nn])\nAbstract: Recently, the original storage prescription for the Hopfield model of neural networks -- as well as for its dense generalizations -- has been turned into a genuine Hebbian learning rule by postulating the expression of its Hamiltonian for both the supervised and unsupervised protocols. In these notes, first, we obtain these explicit expressions by relying upon maximum entropy extremization \\`a la Jaynes. Beyond providing a formal derivation of these recipes for Hebbian learning, this construction also highlights how Lagrangian constraints within entropy extremization force network's outcomes on neural correlations: these try to mimic the empirical counterparts hidden in the datasets provided to the network for its training and, the denser the network, the longer the correlations that it is able to capture. Next, we prove that, in the big data limit, whatever the presence of a teacher (or its lacking), not only these Hebbian learning rules converge to the original storage prescription o",
    "path": "papers/24/01/2401.07110.json",
    "total_tokens": 818,
    "translated_title": "从第一原理中的赫布学习",
    "translated_abstract": "最近，针对神经网络的Hopfield模型及其密集概化形式的原始存储方案已通过假设其哈密顿量的表达式为监督和无监督协议，成为真正的赫布学习规则。在本文中，我们首先依靠Jaynes的最大熵极值法得到了这些明确的表达式。除了形式上推导出这些赫布学习的规则，这个构建还突显了熵极值中的朗格朗日约束如何强制网络结果上的神经相关性：这些尝试模仿提供给网络进行训练的数据集中隐藏的经验支持，而且网络越密集，能够捕捉到的相关性时间越长。接下来，我们证明在大数据极限下，无论是否存在教师，这些赫布学习规则都会收敛到原始的存储方案。",
    "tldr": "本文从第一原理中得到了赫布学习的明确表达式，并证明了这些学习规则在大数据极限下收敛到原始的存储方案。"
}