{
    "title": "Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach for the Model-free LQR. (arXiv:2401.14534v1 [math.OC])",
    "abstract": "We investigate the problem of learning Linear Quadratic Regulators (LQR) in a multi-task, heterogeneous, and model-free setting. We characterize the stability and personalization guarantees of a Policy Gradient-based (PG) Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) approach for the LQR problem under different task-heterogeneity settings. We show that the MAML-LQR approach produces a stabilizing controller close to each task-specific optimal controller up to a task-heterogeneity bias for both model-based and model-free settings. Moreover, in the model-based setting, we show that this controller is achieved with a linear convergence rate, which improves upon sub-linear rates presented in existing MAML-LQR work. In contrast to existing MAML-LQR results, our theoretical guarantees demonstrate that the learned controller can efficiently adapt to unseen LQR tasks.",
    "link": "http://arxiv.org/abs/2401.14534",
    "context": "Title: Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML Approach for the Model-free LQR. (arXiv:2401.14534v1 [math.OC])\nAbstract: We investigate the problem of learning Linear Quadratic Regulators (LQR) in a multi-task, heterogeneous, and model-free setting. We characterize the stability and personalization guarantees of a Policy Gradient-based (PG) Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) approach for the LQR problem under different task-heterogeneity settings. We show that the MAML-LQR approach produces a stabilizing controller close to each task-specific optimal controller up to a task-heterogeneity bias for both model-based and model-free settings. Moreover, in the model-based setting, we show that this controller is achieved with a linear convergence rate, which improves upon sub-linear rates presented in existing MAML-LQR work. In contrast to existing MAML-LQR results, our theoretical guarantees demonstrate that the learned controller can efficiently adapt to unseen LQR tasks.",
    "path": "papers/24/01/2401.14534.json",
    "total_tokens": 935,
    "translated_title": "元学习线性二次调节器: 一种针对无模型LQR的策略梯度MAML方法",
    "translated_abstract": "我们研究在多任务、异构和无模型环境中学习线性二次调节器（LQR）的问题。我们对一种基于策略梯度的模型不可知元学习（MAML）方法（Finn等人，2017）在不同任务异质性设置下的LQR问题的稳定性和个性化保证进行了刻画。我们证明在模型基础和无模型设置下，MAML-LQR方法产生的控制器与每个任务特定的最优控制器接近，除了任务异质性偏差。此外，我们还展示了在模型基础设置下，这种控制器以线性收敛速率实现，这改进了现有MAML-LQR工作中的次线性速率。与现有的MAML-LQR结果相比，我们的理论保证表明学习到的控制器可以高效地适应未知的LQR任务。",
    "tldr": "本论文研究了在多任务、异构和无模型环境下学习线性二次调节器（LQR）的问题，提出了一种基于策略梯度元学习（MAML）方法的解决方案。该方法能够产生与每个任务特定的最优控制器接近的控制器，并在模型基础设置下以线性收敛速率实现。",
    "en_tdlr": "This paper investigates the problem of learning Linear Quadratic Regulators (LQR) in a multi-task, heterogeneous, and model-free setting. It proposes a Policy Gradient-based Model-Agnostic Meta-Learning (MAML) approach that produces controllers close to each task-specific optimal controller and achieves linear convergence rate in the model-based setting."
}