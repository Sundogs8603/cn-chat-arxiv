{
    "title": "Enhancing Context Through Contrast. (arXiv:2401.03314v1 [cs.CL])",
    "abstract": "Neural machine translation benefits from semantically rich representations. Considerable progress in learning such representations has been achieved by language modelling and mutual information maximization objectives using contrastive learning. The language-dependent nature of language modelling introduces a trade-off between the universality of the learned representations and the model's performance on the language modelling tasks. Although contrastive learning improves performance, its success cannot be attributed to mutual information alone. We propose a novel Context Enhancement step to improve performance on neural machine translation by maximizing mutual information using the Barlow Twins loss. Unlike other approaches, we do not explicitly augment the data but view languages as implicit augmentations, eradicating the risk of disrupting semantic information. Further, our method does not learn embeddings from scratch and can be generalised to any set of pre-trained embeddings. Fin",
    "link": "http://arxiv.org/abs/2401.03314",
    "context": "Title: Enhancing Context Through Contrast. (arXiv:2401.03314v1 [cs.CL])\nAbstract: Neural machine translation benefits from semantically rich representations. Considerable progress in learning such representations has been achieved by language modelling and mutual information maximization objectives using contrastive learning. The language-dependent nature of language modelling introduces a trade-off between the universality of the learned representations and the model's performance on the language modelling tasks. Although contrastive learning improves performance, its success cannot be attributed to mutual information alone. We propose a novel Context Enhancement step to improve performance on neural machine translation by maximizing mutual information using the Barlow Twins loss. Unlike other approaches, we do not explicitly augment the data but view languages as implicit augmentations, eradicating the risk of disrupting semantic information. Further, our method does not learn embeddings from scratch and can be generalised to any set of pre-trained embeddings. Fin",
    "path": "papers/24/01/2401.03314.json",
    "total_tokens": 860,
    "translated_title": "提升对比度的上下文增强",
    "translated_abstract": "神经机器翻译受益于语义丰富的表示。通过语言建模和对比学习使用互信息最大化目标，已经实现了对这种表示的大幅进展。语言建模的依赖性使得在学习表示的通用性和模型在语言建模任务上的性能之间存在权衡。虽然对比学习改进了性能，但其成功不能仅归因于互信息。我们提出了一种新的上下文增强步骤，通过使用巴洛双胞胎损失最大化互信息来提高神经机器翻译的性能。与其他方法不同的是，我们不是显式地增加数据，而是将语言视为隐含的增强，消除了破坏语义信息的风险。此外，我们的方法不会从头开始学习嵌入，并且可以推广到任何一组预训练的嵌入。",
    "tldr": "本研究提出了一种通过对比学习来提高神经机器翻译性能的新方法，利用巴洛双胞胎损失最大化互信息。与其他方法不同的是，该方法通过上下文增强来提升性能，而不需要明确地增加数据或从头开始学习嵌入。",
    "en_tdlr": "This research proposes a novel method to enhance neural machine translation performance by maximizing mutual information using the Barlow Twins loss through contrastive learning. Unlike other methods, this approach improves performance through context enhancement without the need for explicit data augmentation or learning embeddings from scratch."
}