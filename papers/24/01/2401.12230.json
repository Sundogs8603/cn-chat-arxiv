{
    "title": "Computing in the Era of Large Generative Models: From Cloud-Native to AI-Native. (arXiv:2401.12230v1 [cs.DC])",
    "abstract": "In this paper, we investigate the intersection of large generative AI models and cloud-native computing architectures. Recent large models such as ChatGPT, while revolutionary in their capabilities, face challenges like escalating costs and demand for high-end GPUs. Drawing analogies between large-model-as-a-service (LMaaS) and cloud database-as-a-service (DBaaS), we describe an AI-native computing paradigm that harnesses the power of both cloud-native technologies (e.g., multi-tenancy and serverless computing) and advanced machine learning runtime (e.g., batched LoRA inference). These joint efforts aim to optimize costs-of-goods-sold (COGS) and improve resource accessibility. The journey of merging these two domains is just at the beginning and we hope to stimulate future research and development in this area.",
    "link": "http://arxiv.org/abs/2401.12230",
    "context": "Title: Computing in the Era of Large Generative Models: From Cloud-Native to AI-Native. (arXiv:2401.12230v1 [cs.DC])\nAbstract: In this paper, we investigate the intersection of large generative AI models and cloud-native computing architectures. Recent large models such as ChatGPT, while revolutionary in their capabilities, face challenges like escalating costs and demand for high-end GPUs. Drawing analogies between large-model-as-a-service (LMaaS) and cloud database-as-a-service (DBaaS), we describe an AI-native computing paradigm that harnesses the power of both cloud-native technologies (e.g., multi-tenancy and serverless computing) and advanced machine learning runtime (e.g., batched LoRA inference). These joint efforts aim to optimize costs-of-goods-sold (COGS) and improve resource accessibility. The journey of merging these two domains is just at the beginning and we hope to stimulate future research and development in this area.",
    "path": "papers/24/01/2401.12230.json",
    "total_tokens": 853,
    "translated_title": "在大型生成模型时代的计算：从云原生到AI原生",
    "translated_abstract": "本文研究了大型生成AI模型和云原生计算架构的交集。最近的大型模型（如ChatGPT）在功能上具有革命性，但面临着成本上涨和对高端GPU需求增加的挑战。通过将大型模型作为服务（LMaaS）和云数据库作为服务（DBaaS）进行类比，我们描述了一种AI原生计算范 Paradigm, 其利用了云原生技术（如多租户和无服务器计算）和高级机器学习运行时（如批量LoRA推理）的能力。这些共同的努力旨在优化成本和提高资源可访问性。将这两个领域融合的旅程才刚刚开始，我们希望能够激发未来在这一领域的研究和发展。",
    "tldr": "本文探讨了大型生成AI模型和云原生计算架构的交集，提出了一种利用云原生技术和高级机器学习运行时的AI原生计算范式，旨在优化成本并提高资源可访问性，未来的研究和发展具有潜力。",
    "en_tdlr": "This paper investigates the intersection of large generative AI models and cloud-native computing architectures, proposing an AI-native computing paradigm that utilizes cloud-native technologies and advanced machine learning runtime to optimize costs and improve resource accessibility. This has the potential to stimulate future research and development in this area."
}