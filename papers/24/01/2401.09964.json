{
    "title": "When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference. (arXiv:2401.09964v1 [cs.SE])",
    "abstract": "Leveraging recent advancements in large language models, modern neural code completion models have demonstrated the capability to generate highly accurate code suggestions. However, their massive size poses challenges in terms of computational costs and environmental impact, hindering their widespread adoption in practical scenarios. Dynamic inference emerges as a promising solution, as it allocates minimal computation during inference while maintaining the model's performance. In this research, we explore dynamic inference within the context of code completion. Initially, we conducted an empirical investigation on GPT-2, focusing on the inference capabilities of intermediate layers for code completion. We found that 54.4% of tokens can be accurately generated using just the first layer, signifying significant computational savings potential. Moreover, despite using all layers, the model still fails to predict 14.5% of tokens correctly, and the subsequent completions continued from the",
    "link": "http://arxiv.org/abs/2401.09964",
    "context": "Title: When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference. (arXiv:2401.09964v1 [cs.SE])\nAbstract: Leveraging recent advancements in large language models, modern neural code completion models have demonstrated the capability to generate highly accurate code suggestions. However, their massive size poses challenges in terms of computational costs and environmental impact, hindering their widespread adoption in practical scenarios. Dynamic inference emerges as a promising solution, as it allocates minimal computation during inference while maintaining the model's performance. In this research, we explore dynamic inference within the context of code completion. Initially, we conducted an empirical investigation on GPT-2, focusing on the inference capabilities of intermediate layers for code completion. We found that 54.4% of tokens can be accurately generated using just the first layer, signifying significant computational savings potential. Moreover, despite using all layers, the model still fails to predict 14.5% of tokens correctly, and the subsequent completions continued from the",
    "path": "papers/24/01/2401.09964.json",
    "total_tokens": 947,
    "translated_title": "当神经代码补全模型调整情况时：通过动态模型推断实现更便宜更快的补全",
    "translated_abstract": "利用大型语言模型的最新进展，现代神经代码补全模型展示了生成高度准确代码建议的能力。然而，它们巨大的规模在计算成本和环境影响方面带来了挑战，阻碍了它们在实际场景中的广泛应用。动态推断成为一种有前景的解决方案，它在推断过程中分配最少的计算资源同时保持模型的性能。在这项研究中，我们在代码补全的背景下探索了动态推断。最初，我们对GPT-2进行了实证研究，重点关注中间层在代码补全中的推断能力。我们发现，仅使用第一层就可以准确生成54.4%的标记，表明存在显著的计算节省潜力。此外，即使使用了所有层，模型仍然无法正确预测14.5%的标记，并且从后续补全中也没有得到正确的预测。",
    "tldr": "本研究探索了代码补全领域中动态推断的应用，在对GPT-2进行实证研究的基础上发现，仅使用第一层即可准确生成超过一半的标记，从而节省了大量计算资源。使用所有层仍然无法完全正确预测一部分标记。",
    "en_tdlr": "This research explores the application of dynamic inference in code completion, and it is found that using only the first layer of GPT-2 can accurately generate more than half of the tokens, leading to significant computational savings. Despite using all layers, the model still fails to correctly predict a portion of the tokens."
}