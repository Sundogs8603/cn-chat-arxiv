{
    "title": "State of What Art? A Call for Multi-Prompt LLM Evaluation. (arXiv:2401.00595v2 [cs.CL] UPDATED)",
    "abstract": "Recent advances in large language models (LLMs) have led to the development of various evaluation benchmarks. These benchmarks typically rely on a single instruction template for evaluating all LLMs on a specific task. In this paper, we comprehensively analyze the brittleness of results obtained via single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. To improve robustness of the analysis, we propose to evaluate LLMs with a set of diverse prompts instead. We discuss tailored evaluation metrics for specific use cases (e.g., LLM developers vs. developers interested in a specific downstream task), ensuring a more reliable and meaningful assessment of LLM capabilities. We then implement these criteria and conduct evaluations of multiple models, providing insights into the true strengths and limitations of current LLMs.",
    "link": "http://arxiv.org/abs/2401.00595",
    "context": "Title: State of What Art? A Call for Multi-Prompt LLM Evaluation. (arXiv:2401.00595v2 [cs.CL] UPDATED)\nAbstract: Recent advances in large language models (LLMs) have led to the development of various evaluation benchmarks. These benchmarks typically rely on a single instruction template for evaluating all LLMs on a specific task. In this paper, we comprehensively analyze the brittleness of results obtained via single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. To improve robustness of the analysis, we propose to evaluate LLMs with a set of diverse prompts instead. We discuss tailored evaluation metrics for specific use cases (e.g., LLM developers vs. developers interested in a specific downstream task), ensuring a more reliable and meaningful assessment of LLM capabilities. We then implement these criteria and conduct evaluations of multiple models, providing insights into the true strengths and limitations of current LLMs.",
    "path": "papers/24/01/2401.00595.json",
    "total_tokens": 835,
    "translated_title": "状态是什么艺术？多提示LLM评估的呼吁。",
    "translated_abstract": "大语言模型（LLMs）的最新进展导致了各种评估基准的创建。这些基准通常依赖于单个指令模板来评估特定任务上的所有LLMs。在本文中，我们全面分析了通过单提示评估获得的结果的脆弱性，纳入了6.5M个实例，涉及20种不同的LLMs和来自3个基准的39个任务。为了改进分析的鲁棒性，我们提议使用一组多样化的提示来评估LLMs。我们讨论了特定用例（例如LLM开发人员与对特定下游任务感兴趣的开发人员）的定制评估指标，确保更可靠和有意义的LLM能力评估。然后，我们实施这些标准，并对多个模型进行评估，提供了关于当前LLMs真正优势和局限性的见解。",
    "tldr": "本研究呼吁使用多个提示来评估大语言模型（LLMs），以解决单提示评估的脆弱性，并提供了关于当前LLMs真正优势和局限性的见解。"
}