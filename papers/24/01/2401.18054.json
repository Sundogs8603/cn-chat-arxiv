{
    "title": "Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition",
    "abstract": "Continual learning (CL) is the research field that aims to build machine learning models that can accumulate knowledge continuously over different tasks without retraining from scratch. Previous studies have shown that pre-training graph neural networks (GNN) may lead to negative transfer (Hu et al., 2020) after fine-tuning, a setting which is closely related to CL. Thus, we focus on studying GNN in the continual graph learning (CGL) setting. We propose the first continual graph learning benchmark for spatio-temporal graphs and use it to benchmark well-known CGL methods in this novel setting. The benchmark is based on the N-UCLA and NTU-RGB+D datasets for skeleton-based action recognition. Beyond benchmarking for standard performance metrics, we study the class and task-order sensitivity of CGL methods, i.e., the impact of learning order on each class/task's performance, and the architectural sensitivity of CGL methods with backbone GNN at various widths and depths. We reveal that task",
    "link": "https://arxiv.org/abs/2401.18054",
    "context": "Title: Benchmarking Sensitivity of Continual Graph Learning for Skeleton-Based Action Recognition\nAbstract: Continual learning (CL) is the research field that aims to build machine learning models that can accumulate knowledge continuously over different tasks without retraining from scratch. Previous studies have shown that pre-training graph neural networks (GNN) may lead to negative transfer (Hu et al., 2020) after fine-tuning, a setting which is closely related to CL. Thus, we focus on studying GNN in the continual graph learning (CGL) setting. We propose the first continual graph learning benchmark for spatio-temporal graphs and use it to benchmark well-known CGL methods in this novel setting. The benchmark is based on the N-UCLA and NTU-RGB+D datasets for skeleton-based action recognition. Beyond benchmarking for standard performance metrics, we study the class and task-order sensitivity of CGL methods, i.e., the impact of learning order on each class/task's performance, and the architectural sensitivity of CGL methods with backbone GNN at various widths and depths. We reveal that task",
    "path": "papers/24/01/2401.18054.json",
    "total_tokens": 941,
    "translated_title": "为基于骨骼的动作识别的连续图学习评估敏感度",
    "translated_abstract": "连续学习（CL）是一个旨在构建能够在不从头开始重新训练的情况下，连续地积累不同任务知识的机器学习模型的研究领域。先前的研究表明，预训练图神经网络（GNN）在微调后可能会导致负迁移（Hu等，2020），这与CL密切相关。因此，我们关注研究CGL设置下的GNN。我们提出了第一个空时图连续学习基准，并在这种新颖的设置中使用它来评估知名的CGL方法。该基准基于N-UCLA和NTU-RGB+D数据集，用于骨骼动作识别。除了对标准性能指标进行基准测试外，我们还研究了CGL方法的类别和任务顺序敏感性，即学习顺序对每个类/任务性能的影响，以及在不同宽度和深度下使用骨干GNN的CGL方法的架构敏感性。我们揭示了任务",
    "tldr": "本研究关注连续图学习中的GNN，并提出了基于骨骼动作识别的连续图学习基准。我们研究了CGL方法在类别和任务顺序上的敏感性以及在不同架构下的敏感性。",
    "en_tdlr": "This study focuses on GNN in continual graph learning and proposes a benchmark for continual graph learning based on skeleton-based action recognition. We investigate the sensitivity of CGL methods in terms of class and task order, as well as the architectural sensitivity with different backbone GNN configurations."
}