{
    "title": "A Preliminary Study on Using Large Language Models in Software Pentesting",
    "abstract": "Large language models (LLM) are perceived to offer promising potentials for automating security tasks, such as those found in security operation centers (SOCs). As a first step towards evaluating this perceived potential, we investigate the use of LLMs in software pentesting, where the main task is to automatically identify software security vulnerabilities in source code. We hypothesize that an LLM-based AI agent can be improved over time for a specific security task as human operators interact with it. Such improvement can be made, as a first step, by engineering prompts fed to the LLM based on the responses produced, to include relevant contexts and structures so that the model provides more accurate results. Such engineering efforts become sustainable if the prompts that are engineered to produce better results on current tasks, also produce better results on future unknown tasks. To examine this hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains 2,740 hand-craft",
    "link": "https://arxiv.org/abs/2401.17459",
    "context": "Title: A Preliminary Study on Using Large Language Models in Software Pentesting\nAbstract: Large language models (LLM) are perceived to offer promising potentials for automating security tasks, such as those found in security operation centers (SOCs). As a first step towards evaluating this perceived potential, we investigate the use of LLMs in software pentesting, where the main task is to automatically identify software security vulnerabilities in source code. We hypothesize that an LLM-based AI agent can be improved over time for a specific security task as human operators interact with it. Such improvement can be made, as a first step, by engineering prompts fed to the LLM based on the responses produced, to include relevant contexts and structures so that the model provides more accurate results. Such engineering efforts become sustainable if the prompts that are engineered to produce better results on current tasks, also produce better results on future unknown tasks. To examine this hypothesis, we utilize the OWASP Benchmark Project 1.2 which contains 2,740 hand-craft",
    "path": "papers/24/01/2401.17459.json",
    "total_tokens": 833,
    "translated_title": "使用大型语言模型在软件渗透测试中的初步研究",
    "translated_abstract": "大型语言模型（LLM）被认为具有自动化安全任务的潜在潜力，例如安全操作中心（SOC）中的任务。作为评估这种潜力的第一步，我们研究了在软件渗透测试中使用LLMs，其中主要任务是自动识别源代码中的软件安全漏洞。我们假设基于LLM的人工智能代理可以随着人员的互动而不断改进特定安全任务。作为第一步，通过根据生成的响应，将相关上下文和结构包含在LLM中的提示之中，来改进AI代理。如果这样的工程努力可以在当前任务上产生更好的结果，并且在未来的未知任务上也能产生更好的结果，则这样的工程努力将具有可持续性。为了检验这个假设，我们利用包含2,740个手工制作的测试用例的OWASP基准项目1.2进行研究。",
    "tldr": "本研究探索了在软件渗透测试中使用大型语言模型（LLM）的潜力，并证明了通过改进提示来提高模型准确性的可行性。",
    "en_tdlr": "This study explores the potential of using large language models (LLM) in software pentesting and demonstrates the feasibility of improving model accuracy through prompt engineering."
}