{
    "title": "DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models",
    "abstract": "arXiv:2401.08392v2 Announce Type: replace-cross  Abstract: Recent LLM-driven visual agents mainly focus on solving image-based tasks, which limits their ability to understand dynamic scenes, making it far from real-life applications like guiding students in laboratory experiments and identifying their mistakes. Considering the video modality better reflects the ever-changing nature of real-world scenarios, we devise DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks. Given a video with a question/task, DoraemonGPT begins by converting the input video into a symbolic memory that stores task-related attributes. This structured representation allows for spatial-temporal querying and reasoning by well-designed sub-task tools, resulting in concise intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorpor",
    "link": "https://arxiv.org/abs/2401.08392",
    "context": "Title: DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models\nAbstract: arXiv:2401.08392v2 Announce Type: replace-cross  Abstract: Recent LLM-driven visual agents mainly focus on solving image-based tasks, which limits their ability to understand dynamic scenes, making it far from real-life applications like guiding students in laboratory experiments and identifying their mistakes. Considering the video modality better reflects the ever-changing nature of real-world scenarios, we devise DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks. Given a video with a question/task, DoraemonGPT begins by converting the input video into a symbolic memory that stores task-related attributes. This structured representation allows for spatial-temporal querying and reasoning by well-designed sub-task tools, resulting in concise intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorpor",
    "path": "papers/24/01/2401.08392.json",
    "total_tokens": 833,
    "translated_title": "DoraemonGPT：朝向理解具有大语言模型的动态场景迈进",
    "translated_abstract": "最近由LLM驱动的视觉代理主要集中于解决基于图像的任务，这限制了它们理解动态场景的能力，使其远离像引导学生进行实验室实验和识别错误这样的真实应用。考虑到视频模态更好地反映了真实世界场景的不断变化性质，我们设计了DoraemonGPT，这是一个由LLM驱动的综合概念简洁系统，用于处理动态视频任务。给定一个带有问题/任务的视频，DoraemonGPT首先将输入视频转换为存储与任务相关属性的符号存储器。这种结构化表示允许通过精心设计的子任务工具进行空间-时间查询和推理，从而产生简洁的中间结果。鉴于LLM在涉及专业领域（例如分析实验中潜在的科学原理）时具有有限的内部知识，我们引入了",
    "tldr": "DoraemonGPT是一个由LLMs驱动的系统，旨在处理动态视频任务，通过将视频转换为符号记忆来进行空间-时间查询和推理，并取得简洁的中间结果。",
    "en_tdlr": "DoraemonGPT is a system driven by LLMs designed to handle dynamic video tasks by converting videos into symbolic memory for spatial-temporal querying and reasoning, leading to concise intermediate results."
}