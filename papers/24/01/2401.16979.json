{
    "title": "Re3val: Reinforced and Reranked Generative Retrieval. (arXiv:2401.16979v1 [cs.IR])",
    "abstract": "Generative retrieval models encode pointers to information in a corpus as an index within the model's parameters. These models serve as part of a larger pipeline, where retrieved information conditions generation for knowledge-intensive NLP tasks. However, we identify two limitations: the generative retrieval does not account for contextual information. Secondly, the retrieval can't be tuned for the downstream readers as decoding the page title is a non-differentiable operation. This paper introduces Re3val, trained with generative reranking and reinforcement learning using limited data. Re3val leverages context acquired via Dense Passage Retrieval to rerank the retrieved page titles and utilizes REINFORCE to maximize rewards generated by constrained decoding. Additionally, we generate questions from our pre-training dataset to mitigate epistemic uncertainty and bridge the domain gap between the pre-training and fine-tuning datasets. Subsequently, we extract and rerank contexts from th",
    "link": "http://arxiv.org/abs/2401.16979",
    "context": "Title: Re3val: Reinforced and Reranked Generative Retrieval. (arXiv:2401.16979v1 [cs.IR])\nAbstract: Generative retrieval models encode pointers to information in a corpus as an index within the model's parameters. These models serve as part of a larger pipeline, where retrieved information conditions generation for knowledge-intensive NLP tasks. However, we identify two limitations: the generative retrieval does not account for contextual information. Secondly, the retrieval can't be tuned for the downstream readers as decoding the page title is a non-differentiable operation. This paper introduces Re3val, trained with generative reranking and reinforcement learning using limited data. Re3val leverages context acquired via Dense Passage Retrieval to rerank the retrieved page titles and utilizes REINFORCE to maximize rewards generated by constrained decoding. Additionally, we generate questions from our pre-training dataset to mitigate epistemic uncertainty and bridge the domain gap between the pre-training and fine-tuning datasets. Subsequently, we extract and rerank contexts from th",
    "path": "papers/24/01/2401.16979.json",
    "total_tokens": 909,
    "translated_title": "Re3val: 强化和重新排名的生成检索",
    "translated_abstract": "生成检索模型将文档中的信息指针编码为模型参数中的索引。这些模型作为更大的流程的一部分，通过检索的信息来为知识密集型自然语言处理任务生成条件。然而，我们发现有两个限制：生成检索没有考虑上下文信息。其次，检索无法为下游读者进行调整，因为解码页面标题是一个非可微分的操作。本文介绍了经过有限数据训练的生成重新排名和强化学习的 Re3val。Re3val利用通过密集通道检索获得的上下文对已检索页面标题进行重新排名，并利用REINFORCE算法最大化受限解码生成的奖励。此外，我们从预训练数据集中生成问题，以减小认识不确定性，并弥合预训练和微调数据集之间的领域差距。随后，我们从中提取和重新排名上下文信息。",
    "tldr": "Re3val是一个使用强化学习和重新排名技术进行训练的生成检索模型，它通过利用上下文信息来重新排名检索得到的页面标题，以最大化通过受限解码生成的奖励。同时，该模型通过生成问题来减小认识不确定性，并弥合预训练和微调数据集之间的领域差距。",
    "en_tdlr": "Re3val is a generative retrieval model trained with reinforcement learning and reranking techniques, which reranks the retrieved page titles using contextual information to maximize rewards generated by constrained decoding. Furthermore, the model mitigates epistemic uncertainty by generating questions and bridges the domain gap between the pre-training and fine-tuning datasets."
}