{
    "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])",
    "abstract": "We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model v",
    "link": "http://arxiv.org/abs/2401.14405",
    "context": "Title: Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])\nAbstract: We propose to improve transformers of a specific modality with irrelevant data from other modalities, e.g., improve an ImageNet model with audio or point cloud datasets. We would like to highlight that the data samples of the target modality are irrelevant to the other modalities, which distinguishes our method from other works utilizing paired (e.g., CLIP) or interleaved data of different modalities. We propose a methodology named Multimodal Pathway - given a target modality and a transformer designed for it, we use an auxiliary transformer trained with data of another modality and construct pathways to connect components of the two models so that data of the target modality can be processed by both models. In this way, we utilize the universal sequence-to-sequence modeling abilities of transformers obtained from two modalities. As a concrete implementation, we use a modality-specific tokenizer and task-specific head as usual but utilize the transformer blocks of the auxiliary model v",
    "path": "papers/24/01/2401.14405.json",
    "total_tokens": 821,
    "translated_title": "多模态路径：通过其他模态的无关数据来改进Transformer",
    "translated_abstract": "我们提出使用来自其他模态的无关数据来改进特定模态的Transformer，例如，使用音频或点云数据集来改进ImageNet模型。我们强调目标模态的数据样本与其他模态无关，这与利用不同模态的配对数据（如CLIP）或交错数据的其他方法不同。我们提出了一种名为多模态路径的方法-给定目标模态和设计用于该模态的Transformer，我们使用使用另一个模态的数据训练的辅助Transformer，并构建路径来连接两个模型的组件，以便目标模态的数据可以被两个模型处理。通过这种方式，我们利用了从两个模态获得的Transformer的通用序列建模能力。作为具体实现，我们通常使用特定模态的tokenizer和任务特定的head，但是利用辅助模型的Transformer block。",
    "tldr": "本文提出了一种名为多模态路径的方法，通过利用其他模态的无关数据来改进特定模态的Transformer，实现了两个模型之间的组件连接，从而提高了模型的序列建模能力。",
    "en_tdlr": "This paper proposes a method called Multimodal Pathway to improve Transformers of a specific modality by utilizing irrelevant data from other modalities, connecting the components of two models to enhance the sequence modeling capabilities."
}