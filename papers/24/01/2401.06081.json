{
    "title": "Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint. (arXiv:2401.06081v1 [cs.CL])",
    "abstract": "Reinforcement learning (RL) has been widely used in training large language models~(LLMs) for preventing unexpected outputs, \\eg reducing harmfulness and errors. However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness. To address it, we propose a new RL method named \\textbf{RLMEC} that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training. Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. The experiment results on mathematical tasks and ",
    "link": "http://arxiv.org/abs/2401.06081",
    "context": "Title: Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint. (arXiv:2401.06081v1 [cs.CL])\nAbstract: Reinforcement learning (RL) has been widely used in training large language models~(LLMs) for preventing unexpected outputs, \\eg reducing harmfulness and errors. However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness. To address it, we propose a new RL method named \\textbf{RLMEC} that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training. Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. The experiment results on mathematical tasks and ",
    "path": "papers/24/01/2401.06081.json",
    "total_tokens": 883,
    "translated_title": "通过最小编辑约束的细粒度强化学习改进大型语言模型",
    "translated_abstract": "强化学习（RL）被广泛应用于训练大型语言模型，以避免意外输出，如减少有害和错误。然而，现有的RL方法大多采用实例级奖励，无法为复杂推理任务提供细粒度的监督，并且不能专注于导致不正确的少数关键标记。为了解决这个问题，我们提出了一种名为RLMEC的新的RL方法，它将生成模型作为奖励模型，并通过最小编辑约束下的错误解重写任务进行训练，可以为RL训练生成标记级别的奖励。基于生成的奖励模型，我们设计了标记级别的RL目标进行训练，以及基于模仿的正则化来稳定RL进程。这两个目标都着重于错误解的关键标记的学习，减少其他不重要标记的影响。",
    "tldr": "通过最小编辑约束下的细粒度强化学习，我们提出了一种名为RLMEC的新的RL方法，它使用生成模型作为奖励模型，可以为复杂推理任务提供标记级别的细粒度监督，专注于关键标记的学习。",
    "en_tdlr": "We propose a new RL method called RLMEC, which uses a generative model as the reward model to provide token-level fine-grained supervision for complex reasoning tasks and focuses on learning the key tokens."
}