{
    "title": "Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion. (arXiv:2401.12947v1 [cs.CL])",
    "abstract": "This paper investigates the ability of transformer-based models to learn structural recursion from examples. Recursion is a universal concept in both natural and formal languages. Structural recursion is central to the programming language and formal mathematics tasks where symbolic tools currently excel beyond neural models, such as inferring semantic relations between datatypes and emulating program behavior. We introduce a general framework that nicely connects the abstract concepts of structural recursion in the programming language domain to concrete sequence modeling problems and learned models' behavior. The framework includes a representation that captures the general \\textit{syntax} of structural recursion, coupled with two different frameworks for understanding their \\textit{semantics} -- one that is more natural from a programming languages perspective and one that helps bridge that perspective with a mechanistic understanding of the underlying transformer architecture.  Wit",
    "link": "http://arxiv.org/abs/2401.12947",
    "context": "Title: Transformer-Based Models Are Not Yet Perfect At Learning to Emulate Structural Recursion. (arXiv:2401.12947v1 [cs.CL])\nAbstract: This paper investigates the ability of transformer-based models to learn structural recursion from examples. Recursion is a universal concept in both natural and formal languages. Structural recursion is central to the programming language and formal mathematics tasks where symbolic tools currently excel beyond neural models, such as inferring semantic relations between datatypes and emulating program behavior. We introduce a general framework that nicely connects the abstract concepts of structural recursion in the programming language domain to concrete sequence modeling problems and learned models' behavior. The framework includes a representation that captures the general \\textit{syntax} of structural recursion, coupled with two different frameworks for understanding their \\textit{semantics} -- one that is more natural from a programming languages perspective and one that helps bridge that perspective with a mechanistic understanding of the underlying transformer architecture.  Wit",
    "path": "papers/24/01/2401.12947.json",
    "total_tokens": 842,
    "translated_title": "基于Transformer模型在学习模拟结构递归方面尚未完美",
    "translated_abstract": "本论文研究了基于Transformer模型学习结构递归的能力。递归是自然语言和形式语言中的一种通用概念。结构递归在编程语言和形式数学任务中是至关重要的，而在这些任务中，符号工具目前在神经模型之上有优势，比如推断数据类型之间的语义关系和模拟程序行为。我们引入了一个通用框架，将编程语言领域中的抽象概念与具体的序列建模问题和学习模型行为相连接。该框架包括一个捕捉结构递归一般\"语法\"的表示，以及两种不同的框架来理解它们的\"语义\"——一种更符合编程语言视角的自然方式，以及一种有助于将该视角与底层Transformer模型的机械理解相结合的方式。",
    "tldr": "本论文研究了基于Transformer模型学习结构递归的能力，介绍了一个通用框架，将编程语言领域中的抽象概念与具体的序列建模问题和学习模型行为相连接。",
    "en_tdlr": "This paper investigates the ability of transformer-based models to learn structural recursion and introduces a general framework that connects abstract concepts from programming language domain to concrete sequence modeling problems and learned models' behavior."
}