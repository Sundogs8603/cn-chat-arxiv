{
    "title": "BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation. (arXiv:2401.17053v1 [cs.CV])",
    "abstract": "We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation. To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature ",
    "link": "http://arxiv.org/abs/2401.17053",
    "context": "Title: BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation. (arXiv:2401.17053v1 [cs.CV])\nAbstract: We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation. To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature ",
    "path": "papers/24/01/2401.17053.json",
    "total_tokens": 919,
    "translated_title": "BlockFusion: 使用潜在三平面外推扩展的可扩展三维场景生成模型",
    "translated_abstract": "我们提出了BlockFusion，一种基于扩散的模型，以单位块形式生成三维场景，并无缝地添加新的块以扩展场景。BlockFusion使用从完整的三维场景中随机裁剪的3D块数据集进行训练。通过块拟合，将所有训练块转换为混合神经场：它包含几何特征的三平面，以及用于解码有符号距离值的多层感知机(MLP)。采用变分自动编码器将三平面压缩到潜在三平面空间，并在其上执行去噪扩散过程。对潜在表示应用扩散，可以实现高质量和多样化的三维场景生成。在生成过程中扩展场景时，只需将空块添加到与当前场景重叠，并外推现有的潜在三平面以填充新块。外推过程通过使用特征对生成过程进行约束来完成。",
    "tldr": "BlockFusion是一种使用扩散和外推技术生成三维场景的模型，能无缝地添加新的块以扩展场景。采用混合神经场和潜在三平面空间来保证高质量和多样化的生成结果。",
    "en_tdlr": "BlockFusion is a model that generates 3D scenes using diffusion and extrapolation techniques, allowing for seamless addition of new blocks to expand the scene. It ensures high-quality and diverse generation results through the use of hybrid neural fields and latent tri-plane space."
}