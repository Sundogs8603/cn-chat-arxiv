{
    "title": "Domain Adaptation for Time series Transformers using One-step fine-tuning. (arXiv:2401.06524v1 [cs.LG])",
    "abstract": "The recent breakthrough of Transformers in deep learning has drawn significant attention of the time series community due to their ability to capture long-range dependencies. However, like other deep learning models, Transformers face limitations in time series prediction, including insufficient temporal understanding, generalization challenges, and data shift issues for the domains with limited data. Additionally, addressing the issue of catastrophic forgetting, where models forget previously learned information when exposed to new data, is another critical aspect that requires attention in enhancing the robustness of Transformers for time series tasks. To address these limitations, in this paper, we pre-train the time series Transformer model on a source domain with sufficient data and fine-tune it on the target domain with limited data. We introduce the \\emph{One-step fine-tuning} approach, adding some percentage of source domain data to the target domains, providing the model with ",
    "link": "http://arxiv.org/abs/2401.06524",
    "context": "Title: Domain Adaptation for Time series Transformers using One-step fine-tuning. (arXiv:2401.06524v1 [cs.LG])\nAbstract: The recent breakthrough of Transformers in deep learning has drawn significant attention of the time series community due to their ability to capture long-range dependencies. However, like other deep learning models, Transformers face limitations in time series prediction, including insufficient temporal understanding, generalization challenges, and data shift issues for the domains with limited data. Additionally, addressing the issue of catastrophic forgetting, where models forget previously learned information when exposed to new data, is another critical aspect that requires attention in enhancing the robustness of Transformers for time series tasks. To address these limitations, in this paper, we pre-train the time series Transformer model on a source domain with sufficient data and fine-tune it on the target domain with limited data. We introduce the \\emph{One-step fine-tuning} approach, adding some percentage of source domain data to the target domains, providing the model with ",
    "path": "papers/24/01/2401.06524.json",
    "total_tokens": 953,
    "translated_title": "使用一步微调的时间序列Transformer的领域适应",
    "translated_abstract": "随着深度学习的Transformer在捕捉长程依赖性方面的能力受到广泛关注，它们在时间序列预测中面临着一些限制，包括对时间理解的不足、泛化挑战以及面对数据有限的领域中的数据偏移问题。此外，解决遗忘灾难的问题，即模型在接触到新数据时忘记了先前学到的信息，也是提高时间序列任务中Transformer鲁棒性需要关注的关键方面。为了解决这些限制，在本文中，我们在具有充足数据的源领域上对时间序列Transformer模型进行预训练，并在具有有限数据的目标域上进行微调。我们引入了“一步微调”方法，将一定百分比的源领域数据添加到目标域中，为模型提供了一些先验知识，并增强了其在目标域上的性能。",
    "tldr": "本文介绍了一种在时间序列Transformer模型中使用一步微调的领域适应方法。通过在源领域上预训练模型，并在目标领域上进行微调，我们解决了时间理解不足、泛化挑战和数据偏移问题。此外，通过添加源领域数据到目标领域，我们提高了模型的鲁棒性，解决了遗忘灾难的问题。"
}