{
    "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation. (arXiv:2401.08417v2 [cs.CL] UPDATED)",
    "abstract": "Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winn",
    "link": "http://arxiv.org/abs/2401.08417",
    "context": "Title: Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation. (arXiv:2401.08417v2 [cs.CL] UPDATED)\nAbstract: Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winn",
    "path": "papers/24/01/2401.08417.json",
    "total_tokens": 967,
    "translated_title": "对比性偏好优化：推动机器翻译中LLM性能的边界",
    "translated_abstract": "中等规模的大型语言模型（LLM）——7B或者13B参数的模型在机器翻译（MT）任务中表现出有希望的性能。然而，即使是表现最好的13B LLM翻译模型，如ALMA，也无法达到现有的最先进的传统编码器-解码器翻译模型或者更大规模的LLM（如GPT-4）的性能水平。本研究旨在弥合这一性能差距。首先，我们评估了监督微调在MT任务中针对LLM的不足之处，强调了尽管是人工生成的参考数据，但其中存在质量问题。然后，与模仿参考翻译的SFT相反，我们引入了对比性偏好优化（CPO），一种新的方法，训练模型避免生成仅仅合乎要求但不完美的翻译。将CPO应用于仅有22K对句子和12M参数的ALMA模型中，可以显著提高性能。得到的模型称为ALMA-R，其性能可以达到或超过WMT比赛的获胜水平。",
    "tldr": "本研究通过引入对比性偏好优化（CPO）的方法，弥合了大型语言模型（LLM）在机器翻译中性能与传统编码器-解码器模型之间的差距，实现了更好的翻译效果。"
}