{
    "title": "Adaptive Text Watermark for Large Language Models. (arXiv:2401.13927v1 [cs.CL])",
    "abstract": "The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive watermarking strategy to address this problem. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured using an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits in proportion based on the semantic embedding of prev",
    "link": "http://arxiv.org/abs/2401.13927",
    "context": "Title: Adaptive Text Watermark for Large Language Models. (arXiv:2401.13927v1 [cs.CL])\nAbstract: The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive watermarking strategy to address this problem. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured using an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits in proportion based on the semantic embedding of prev",
    "path": "papers/24/01/2401.13927.json",
    "total_tokens": 911,
    "translated_title": "大型语言模型自适应文字水印",
    "translated_abstract": "大型语言模型的发展引发了人们对人工智能生成文本滥用的担忧，而基于大型语言模型生成的文字水印成为潜在解决方案。然而，在保持水印强度、稳健性和无需预先知道提示或模型的情况下检测水印的同时，生成高质量的带水印文本是具有挑战性的。本文提出了一种自适应的水印策略来解决这个问题。为了提高文本质量和保持稳健性，我们根据辅助模型测量的高熵令牌分布自适应地添加水印，而保持低熵令牌分布不变。为了保证安全性并进一步减小水印对文本质量的影响，我们不再使用从随机秘钥生成的固定红/绿名单，而是根据前一个语义嵌入将输出对数比例适应性缩放。",
    "tldr": "这个论文提出了一种自适应的大型语言模型文字水印策略，通过辅助模型测量高熵令牌分布，将水印自适应地添加到具有高熵的令牌分布中，同时保持低熵令牌分布不变，以提高文本质量和水印的稳健性。"
}