{
    "title": "Improving ASR Contextual Biasing with Guided Attention. (arXiv:2401.08835v1 [cs.CL])",
    "abstract": "In this paper, we propose a Guided Attention (GA) auxiliary training loss, which improves the effectiveness and robustness of automatic speech recognition (ASR) contextual biasing without introducing additional parameters. A common challenge in previous literature is that the word error rate (WER) reduction brought by contextual biasing diminishes as the number of bias phrases increases. To address this challenge, we employ a GA loss as an additional training objective besides the Transducer loss. The proposed GA loss aims to teach the cross attention how to align bias phrases with text tokens or audio frames. Compared to studies with similar motivations, the proposed loss operates directly on the cross attention weights and is easier to implement. Through extensive experiments based on Conformer Transducer with Contextual Adapter, we demonstrate that the proposed method not only leads to a lower WER but also retains its effectiveness as the number of bias phrases increases. Specifical",
    "link": "http://arxiv.org/abs/2401.08835",
    "context": "Title: Improving ASR Contextual Biasing with Guided Attention. (arXiv:2401.08835v1 [cs.CL])\nAbstract: In this paper, we propose a Guided Attention (GA) auxiliary training loss, which improves the effectiveness and robustness of automatic speech recognition (ASR) contextual biasing without introducing additional parameters. A common challenge in previous literature is that the word error rate (WER) reduction brought by contextual biasing diminishes as the number of bias phrases increases. To address this challenge, we employ a GA loss as an additional training objective besides the Transducer loss. The proposed GA loss aims to teach the cross attention how to align bias phrases with text tokens or audio frames. Compared to studies with similar motivations, the proposed loss operates directly on the cross attention weights and is easier to implement. Through extensive experiments based on Conformer Transducer with Contextual Adapter, we demonstrate that the proposed method not only leads to a lower WER but also retains its effectiveness as the number of bias phrases increases. Specifical",
    "path": "papers/24/01/2401.08835.json",
    "total_tokens": 938,
    "translated_title": "使用引导注意力改进ASR的上下文偏差",
    "translated_abstract": "本文提出了一种引导注意力（GA）的辅助训练损失，可以提高自动语音识别（ASR）上下文偏差的效果和鲁棒性，而不引入额外的参数。在之前的文献中，一个常见的挑战是随着偏差短语数量的增加，上下文偏差带来的词错误率（WER）降低效果会减弱。为了解决这个挑战，我们除了使用转录损失外，还采用了GA损失作为额外的训练目标。所提出的GA损失旨在教会交叉注意力如何将偏差短语与文本标记或音频帧对齐。与具有类似动机的其他研究相比，所提出的损失直接作用于交叉注意力权重，更容易实现。通过基于带上下文适配器的Conformer Transducer的大量实验证明，所提出的方法不仅可以导致更低的WER，而且在偏差短语数量增加时仍然保持有效性。",
    "tldr": "本文提出了一种引导注意力的辅助训练损失方法，可以改进自动语音识别中的上下文偏差问题，不引入额外参数，并且通过实验证明在偏差短语数量增加时仍然保持有效性。",
    "en_tdlr": "This paper proposes a Guided Attention auxiliary training loss to improve contextual biasing in automatic speech recognition (ASR), without introducing additional parameters. The proposed method effectively addresses the challenge of diminishing word error rate reduction with increasing bias phrases, and it has been demonstrated to retain its effectiveness even with a higher number of bias phrases."
}