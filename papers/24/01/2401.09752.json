{
    "title": "Improving Speaker-independent Speech Emotion Recognition Using Dynamic Joint Distribution Adaptation. (arXiv:2401.09752v1 [cs.SD])",
    "abstract": "In speaker-independent speech emotion recognition, the training and testing samples are collected from diverse speakers, leading to a multi-domain shift challenge across the feature distributions of data from different speakers. Consequently, when the trained model is confronted with data from new speakers, its performance tends to degrade. To address the issue, we propose a Dynamic Joint Distribution Adaptation (DJDA) method under the framework of multi-source domain adaptation. DJDA firstly utilizes joint distribution adaptation (JDA), involving marginal distribution adaptation (MDA) and conditional distribution adaptation (CDA), to more precisely measure the multi-domain distribution shifts caused by different speakers. This helps eliminate speaker bias in emotion features, allowing for learning discriminative and speaker-invariant speech emotion features from coarse-level to fine-level. Furthermore, we quantify the adaptation contributions of MDA and CDA within JDA by using a dynam",
    "link": "http://arxiv.org/abs/2401.09752",
    "context": "Title: Improving Speaker-independent Speech Emotion Recognition Using Dynamic Joint Distribution Adaptation. (arXiv:2401.09752v1 [cs.SD])\nAbstract: In speaker-independent speech emotion recognition, the training and testing samples are collected from diverse speakers, leading to a multi-domain shift challenge across the feature distributions of data from different speakers. Consequently, when the trained model is confronted with data from new speakers, its performance tends to degrade. To address the issue, we propose a Dynamic Joint Distribution Adaptation (DJDA) method under the framework of multi-source domain adaptation. DJDA firstly utilizes joint distribution adaptation (JDA), involving marginal distribution adaptation (MDA) and conditional distribution adaptation (CDA), to more precisely measure the multi-domain distribution shifts caused by different speakers. This helps eliminate speaker bias in emotion features, allowing for learning discriminative and speaker-invariant speech emotion features from coarse-level to fine-level. Furthermore, we quantify the adaptation contributions of MDA and CDA within JDA by using a dynam",
    "path": "papers/24/01/2401.09752.json",
    "total_tokens": 889,
    "translated_title": "使用动态联合分布适应提高说话人无关的语音情感识别",
    "translated_abstract": "在说话人无关的语音情感识别中，训练和测试样本来自不同的说话人，导致了不同说话人数据的特征分布之间存在多域转移挑战。因此，当训练模型面对来自新说话人的数据时，其性能往往会下降。为了解决这个问题，我们提出了一种动态联合分布适应（DJDA）方法，在多源域适应框架下使用。DJDA首先利用联合分布适应（JDA），包括边缘分布适应（MDA）和条件分布适应（CDA），更精确地量化不同说话人引起的多域分布差异。这有助于消除情感特征中的说话人偏差，从粗粒度到细粒度学习具有辨别性和说话人无关性的语音情感特征。此外，我们通过使用动态权衡方法量化JDA中MDA和CDA的适应贡献来对其进行评估。",
    "tldr": "本研究提出了一种动态联合分布适应（DJDA）方法，通过联合分布适应来消除多说话人引起的多域转移挑战，并学习具有辨别性和说话人无关性的语音情感特征。"
}