{
    "title": "Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks. (arXiv:2401.03177v1 [cs.CV])",
    "abstract": "Text-video retrieval is a challenging task that aims to identify relevant videos given textual queries. Compared to conventional textual retrieval, the main obstacle for text-video retrieval is the semantic gap between the textual nature of queries and the visual richness of video content. Previous works primarily focus on aligning the query and the video by finely aggregating word-frame matching signals. Inspired by the human cognitive process of modularly judging the relevance between text and video, the judgment needs high-order matching signal due to the consecutive and complex nature of video contents. In this paper, we propose chunk-level text-video matching, where the query chunks are extracted to describe a specific retrieval unit, and the video chunks are segmented into distinct clips from videos. We formulate the chunk-level matching as n-ary correlations modeling between words of the query and frames of the video and introduce a multi-modal hypergraph for n-ary correlation m",
    "link": "http://arxiv.org/abs/2401.03177",
    "context": "Title: Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks. (arXiv:2401.03177v1 [cs.CV])\nAbstract: Text-video retrieval is a challenging task that aims to identify relevant videos given textual queries. Compared to conventional textual retrieval, the main obstacle for text-video retrieval is the semantic gap between the textual nature of queries and the visual richness of video content. Previous works primarily focus on aligning the query and the video by finely aggregating word-frame matching signals. Inspired by the human cognitive process of modularly judging the relevance between text and video, the judgment needs high-order matching signal due to the consecutive and complex nature of video contents. In this paper, we propose chunk-level text-video matching, where the query chunks are extracted to describe a specific retrieval unit, and the video chunks are segmented into distinct clips from videos. We formulate the chunk-level matching as n-ary correlations modeling between words of the query and frames of the video and introduce a multi-modal hypergraph for n-ary correlation m",
    "path": "papers/24/01/2401.03177.json",
    "total_tokens": 775,
    "translated_title": "文本-视频检索通过变分多模态超图网络",
    "translated_abstract": "文本-视频检索是一个具有挑战性的任务，旨在根据文本查询识别相关视频。与传统的文本检索相比，文本-视频检索的主要障碍是查询的文本性质和视频内容的视觉丰富性之间的语义差距。以人类认知过程中模块化判断文本和视频相关性为灵感，由于视频内容的连续和复杂性，判断需要高阶匹配信号。在本文中，我们提出了块级文本-视频匹配，其中提取查询块以描述特定的检索单元，而视频块则从视频中分割成不同的片段。我们将块级匹配形式化为查询词和视频帧之间的n元相关建模，并引入多模态超图进行n元相关建模。",
    "tldr": "本文提出了一种文本-视频检索的方法，通过引入多模态超图进行n元相关建模，从而解决了文本性质和视频内容的语义差距问题。"
}