{
    "title": "Tuning Language Models by Proxy",
    "abstract": "arXiv:2401.08565v2 Announce Type: replace  Abstract: Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the same end as direct tuning, but by accessing only its predictions over the output vocabulary, not its parameters. Our method tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the larger untuned model in the direction of tuning, while retaining the benefits of larger-scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when ",
    "link": "https://arxiv.org/abs/2401.08565",
    "context": "Title: Tuning Language Models by Proxy\nAbstract: arXiv:2401.08565v2 Announce Type: replace  Abstract: Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the same end as direct tuning, but by accessing only its predictions over the output vocabulary, not its parameters. Our method tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the larger untuned model in the direction of tuning, while retaining the benefits of larger-scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when ",
    "path": "papers/24/01/2401.08565.json",
    "total_tokens": 921,
    "translated_title": "通过代理调整语言模型",
    "translated_abstract": "尽管大型预训练语言模型具有一般的能力，但它们始终受益于进一步调整以更好地实现所需的行为。然而，调整这些模型变得越来越消耗资源，或者在模型权重是私有的情况下是不可能的。我们引入了代理调整，这是一种轻量级的解码时算法，它在黑盒语言模型的基础上运行，以实现与直接调整相同的目的，但只访问其在输出词汇上的预测，而不是其参数。我们的方法调整了一个较小的语言模型，然后将经过调整和未经调整的小模型的预测之间的差异应用于将更大的未调整模型的原始预测转移到调整方向，同时保留较大规模预训练的好处。在实验中，当我们使用仅为7B大小的代理对Llama2-70B应用代理调整时，我们可以关闭88% Llama2-70B 与其真正调整过的聊天版本之间的差距，",
    "tldr": "介绍了一种代理调整的轻量级解码时算法，可以通过对小型调整后的LM的预测与未调整LM的预测之间的差异来调整大型预训练LM的预测，从而实现资源节约和保留更大规模预训练的好处。",
    "en_tdlr": "Introduced a lightweight decoding-time algorithm called proxy-tuning to adjust predictions of large pretrained language models by leveraging the difference between predictions of a small tuned LM and the original predictions of the larger untuned model, leading to resource-saving and retention of benefits from larger-scale pretraining."
}