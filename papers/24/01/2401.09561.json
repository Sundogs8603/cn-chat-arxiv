{
    "title": "Sharing Knowledge in Multi-Task Deep Reinforcement Learning. (arXiv:2401.09561v1 [cs.LG])",
    "abstract": "We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in",
    "link": "http://arxiv.org/abs/2401.09561",
    "context": "Title: Sharing Knowledge in Multi-Task Deep Reinforcement Learning. (arXiv:2401.09561v1 [cs.LG])\nAbstract: We study the benefit of sharing representations among tasks to enable the effective use of deep neural networks in Multi-Task Reinforcement Learning. We leverage the assumption that learning from different tasks, sharing common properties, is helpful to generalize the knowledge of them resulting in a more effective feature extraction compared to learning a single task. Intuitively, the resulting set of features offers performance benefits when used by Reinforcement Learning algorithms. We prove this by providing theoretical guarantees that highlight the conditions for which is convenient to share representations among tasks, extending the well-known finite-time bounds of Approximate Value-Iteration to the multi-task setting. In addition, we complement our analysis by proposing multi-task extensions of three Reinforcement Learning algorithms that we empirically evaluate on widely used Reinforcement Learning benchmarks showing significant improvements over the single-task counterparts in",
    "path": "papers/24/01/2401.09561.json",
    "total_tokens": 797,
    "translated_title": "在多任务深度强化学习中分享知识",
    "translated_abstract": "我们研究了在多任务强化学习中分享表示以实现深度神经网络的有效应用的益处。我们利用了学习不同任务，并分享共同属性的假设，有助于将它们的知识推广，从而获得比单个任务学习更有效的特征提取。直观地说，所得到的特征集在强化学习算法使用时可以带来性能方面的好处。我们通过提供理论保证来证明这一点，这些理论保证强调了在何种条件下分享任务之间的表示是方便的，并将近似值迭代的已知有限时间界扩展到多任务设置中。此外，我们通过提出三种强化学习算法的多任务扩展来补充我们的分析，并在广泛使用的强化学习基准上进行经验评估，表明与单任务对比相比有显著改进。",
    "tldr": "本研究探讨了在多任务强化学习中分享表示的益处，并提供理论保证和实验评估结果，表明在多任务设置中分享表示可以改进性能。"
}