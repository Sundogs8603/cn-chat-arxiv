{
    "title": "DocGraphLM: Documental Graph Language Model for Information Extraction. (arXiv:2401.02823v1 [cs.CL])",
    "abstract": "Advances in Visually Rich Document Understanding (VrDU) have enabled information extraction and question answering over documents with complex layouts. Two tropes of architectures have emerged -- transformer-based models inspired by LLMs, and Graph Neural Networks. In this paper, we introduce DocGraphLM, a novel framework that combines pre-trained language models with graph semantics. To achieve this, we propose 1) a joint encoder architecture to represent documents, and 2) a novel link prediction approach to reconstruct document graphs. DocGraphLM predicts both directions and distances between nodes using a convergent joint loss function that prioritizes neighborhood restoration and downweighs distant node detection. Our experiments on three SotA datasets show consistent improvement on IE and QA tasks with the adoption of graph features. Moreover, we report that adopting the graph features accelerates convergence in the learning process during training, despite being solely constructe",
    "link": "http://arxiv.org/abs/2401.02823",
    "context": "Title: DocGraphLM: Documental Graph Language Model for Information Extraction. (arXiv:2401.02823v1 [cs.CL])\nAbstract: Advances in Visually Rich Document Understanding (VrDU) have enabled information extraction and question answering over documents with complex layouts. Two tropes of architectures have emerged -- transformer-based models inspired by LLMs, and Graph Neural Networks. In this paper, we introduce DocGraphLM, a novel framework that combines pre-trained language models with graph semantics. To achieve this, we propose 1) a joint encoder architecture to represent documents, and 2) a novel link prediction approach to reconstruct document graphs. DocGraphLM predicts both directions and distances between nodes using a convergent joint loss function that prioritizes neighborhood restoration and downweighs distant node detection. Our experiments on three SotA datasets show consistent improvement on IE and QA tasks with the adoption of graph features. Moreover, we report that adopting the graph features accelerates convergence in the learning process during training, despite being solely constructe",
    "path": "papers/24/01/2401.02823.json",
    "total_tokens": 988,
    "translated_title": "DocGraphLM：信息抽取的文档图语言模型",
    "translated_abstract": "在视觉丰富的文档理解(VrDU)方面取得的进展使得可以对具有复杂布局的文档进行信息抽取和问题回答成为可能。出现了两种架构的模式-受LLM启发的基于transformer的模型和图神经网络。在本文中，我们介绍了一种新颖的框架DocGraphLM，它将预训练的语言模型与图语义相结合。为了实现这一目标，我们提出了1)一种联合编码器架构来表示文档，以及2)一种新颖的链接预测方法来重构文档图。DocGraphLM使用一个收敛的联合损失函数来预测节点之间的方向和距离，该损失函数优先考虑邻域恢复并减轻远程节点检测。我们在三个最先进的数据集上的实验证明，采用图特征可以在信息抽取和问题回答任务上保持一致的改进。此外，我们报告说，尽管仅由构建而来，在训练过程中采用图特征可以加快收敛过程。",
    "tldr": "本文介绍了一个名为DocGraphLM的框架，它将预训练的语言模型与图语义相结合，通过联合编码器架构表示文档，并使用一种新颖的链接预测方法重构文档图。实验证明，采用图特征可以在信息抽取和问题回答任务上取得一致的改进，并且在训练过程中加速了收敛过程。",
    "en_tdlr": "This paper introduces a framework named DocGraphLM that combines pre-trained language models with graph semantics. It represents documents using a joint encoder architecture and reconstructs document graphs using a novel link prediction approach. The experiments demonstrate consistent improvements on information extraction and question answering tasks with the adoption of graph features, and the use of graph features accelerates the convergence process during training."
}