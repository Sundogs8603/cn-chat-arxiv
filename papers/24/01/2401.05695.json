{
    "title": "Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback. (arXiv:2401.05695v1 [cs.CL])",
    "abstract": "The use of large language models in medical dialogue generation has garnered significant attention, with a focus on improving response quality and fluency. While previous studies have made progress in optimizing model performance for single-round medical Q&A tasks, there is a need to enhance the model's capability for multi-round conversations to avoid logical inconsistencies. To address this, we propose an approach called preference learning from process feedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF involves rule modeling, preference data generation, and preference alignment to train the model to adhere to the diagnostic process. Experimental results using Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.6%, outperforming traditional reinforcement learning from human feedback. Additionally, PLPF demonstrates effectiveness in both multi-round and single-round dialogue task",
    "link": "http://arxiv.org/abs/2401.05695",
    "context": "Title: Integrating Physician Diagnostic Logic into Large Language Models: Preference Learning from Process Feedback. (arXiv:2401.05695v1 [cs.CL])\nAbstract: The use of large language models in medical dialogue generation has garnered significant attention, with a focus on improving response quality and fluency. While previous studies have made progress in optimizing model performance for single-round medical Q&A tasks, there is a need to enhance the model's capability for multi-round conversations to avoid logical inconsistencies. To address this, we propose an approach called preference learning from process feedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF involves rule modeling, preference data generation, and preference alignment to train the model to adhere to the diagnostic process. Experimental results using Standardized Patient Testing show that PLPF enhances the diagnostic accuracy of the baseline model in medical conversations by 17.6%, outperforming traditional reinforcement learning from human feedback. Additionally, PLPF demonstrates effectiveness in both multi-round and single-round dialogue task",
    "path": "papers/24/01/2401.05695.json",
    "total_tokens": 892,
    "translated_title": "将医生的诊断逻辑整合到大型语言模型中：从过程反馈进行偏好学习",
    "translated_abstract": "大型语言模型在医疗对话生成中的应用引起了重视，致力于改善响应质量和流畅性。虽然先前的研究在单轮医疗问答任务的模型性能优化方面取得了进展，但有必要增强模型在多轮对话中避免逻辑不一致的能力。为了解决这个问题，我们提出了一种称为从过程反馈进行偏好学习的方法（PLPF），将医生的诊断逻辑整合到LLM中。PLPF包括规则建模、偏好数据生成和偏好对齐，以训练模型遵循诊断过程。使用标准化患者测试的实验结果表明，PLPF将医疗对话中基准模型的诊断准确性提高了17.6％，优于传统的人类反馈强化学习。此外，PLPF在多轮和单轮对话任务中均表现出有效性。",
    "tldr": "这项研究整合了医生的诊断逻辑到大型语言模型中，提出了一种称为偏好学习从过程反馈（PLPF）的方法，并通过实验结果证明了其在医疗对话中的有效性和优越性。",
    "en_tdlr": "This research integrates physician diagnostic logic into large language models, proposing an approach called preference learning from process feedback (PLPF), and demonstrates its effectiveness and superiority in medical dialogue through experimental results."
}