{
    "title": "Benchmarking LLMs via Uncertainty Quantification. (arXiv:2401.12794v1 [cs.CL])",
    "abstract": "The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves eight LLMs (LLM series) spanning five representative natural language processing tasks. Additionally, we introduce an uncertainty-aware evaluation metric, UAcc, which takes into account both prediction accuracy and prediction uncertainty. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. By taking uncertainty",
    "link": "http://arxiv.org/abs/2401.12794",
    "context": "Title: Benchmarking LLMs via Uncertainty Quantification. (arXiv:2401.12794v1 [cs.CL])\nAbstract: The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves eight LLMs (LLM series) spanning five representative natural language processing tasks. Additionally, we introduce an uncertainty-aware evaluation metric, UAcc, which takes into account both prediction accuracy and prediction uncertainty. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. By taking uncertainty",
    "path": "papers/24/01/2401.12794.json",
    "total_tokens": 981,
    "translated_title": "通过不确定性量化对LLMs进行基准测试",
    "translated_abstract": "随着各个机构开源大型语言模型（LLMs）的增多，彰显了对综合评估方法的迫切需求。然而，当前的评估平台，如广为人知的HuggingFace开放的LLM排行榜，忽视了一个关键方面--不确定性，而不确定性对于全面评估LLMs至关重要。为了弥补这一差距，我们引入了一种新的LLMs基准测试方法，将不确定性量化集成进去。我们的研究涵盖了五个典型的自然语言处理任务中的八个LLMs（LLM系列）。此外，我们引入了一种考虑了预测准确性和预测不确定性的不确定性感知评估指标UAcc。我们的研究结果显示：I）准确度越高的LLMs可能会表现出较低的确定性；II）规模较大的LLMs可能与较小的LLMs相比显示出更大的不确定性；III）指令微调倾向于增加LLMs的不确定性。通过考虑不确定性，我们的方法可以更全面地评估LLMs的性能。",
    "tldr": "这项研究提出了一种通过不确定性量化来对LLMs进行基准测试的方法，并引入了一种不确定性感知评估指标UAcc。研究结果显示，高准确度的LLMs可能具有较低的确定性，而大规模LLMs可能比较小规模LLMs更不确定。指令微调倾向于增加LLMs的不确定性。",
    "en_tdlr": "This study proposes a benchmarking approach for Large Language Models (LLMs) by integrating uncertainty quantification and introduces an uncertainty-aware evaluation metric, UAcc. The findings reveal that LLMs with higher accuracy may exhibit lower certainty, while larger-scale LLMs may display greater uncertainty compared to smaller counterparts. Instruction-finetuning tends to increase the uncertainty of LLMs."
}