{
    "title": "Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain Dialogue Systems. (arXiv:2401.02256v1 [cs.CL])",
    "abstract": "Open-domain dialogue systems have started to engage in continuous conversations with humans. Those dialogue systems are required to be adjusted to the human interlocutor and evaluated in terms of their perspective. However, it is questionable whether the current automatic evaluation methods can approximate the interlocutor's judgments. In this study, we analyzed and examined what features are needed in an automatic response evaluator from the interlocutor's perspective. The first experiment on the Hazumi dataset revealed that interlocutor awareness plays a critical role in making automatic response evaluation correlate with the interlocutor's judgments. The second experiment using massive conversations on X (formerly Twitter) confirmed that dialogue continuity prediction can train an interlocutor-aware response evaluator without human feedback while revealing the difficulty in evaluating generated responses compared to human responses.",
    "link": "http://arxiv.org/abs/2401.02256",
    "context": "Title: Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain Dialogue Systems. (arXiv:2401.02256v1 [cs.CL])\nAbstract: Open-domain dialogue systems have started to engage in continuous conversations with humans. Those dialogue systems are required to be adjusted to the human interlocutor and evaluated in terms of their perspective. However, it is questionable whether the current automatic evaluation methods can approximate the interlocutor's judgments. In this study, we analyzed and examined what features are needed in an automatic response evaluator from the interlocutor's perspective. The first experiment on the Hazumi dataset revealed that interlocutor awareness plays a critical role in making automatic response evaluation correlate with the interlocutor's judgments. The second experiment using massive conversations on X (formerly Twitter) confirmed that dialogue continuity prediction can train an interlocutor-aware response evaluator without human feedback while revealing the difficulty in evaluating generated responses compared to human responses.",
    "path": "papers/24/01/2401.02256.json",
    "total_tokens": 810,
    "translated_title": "重新考虑从对话参与者的角度对开放域对话系统进行响应评估",
    "translated_abstract": "开放域对话系统已经开始与人类进行连续对话。这些对话系统需要根据人类对话者进行调整，并以其角度进行评估。然而，当前的自动评估方法是否能够近似对话参与者的判断是存疑的。在本研究中，我们分析和研究了从对话参与者角度需要哪些特征的自动响应评估器。第一次在Hazumi数据集上的实验表明，对话参与者意识在使自动响应评估与对话参与者判断相关方面起着关键作用。第二个实验使用大规模对话（前称Twitter）确认了对话连贯性预测可以训练出一个具有对话参与者意识的响应评估器，而相比人类响应，评估生成的响应的难度更大。",
    "tldr": "本研究重新考虑了从对话参与者的角度对开放域对话系统进行响应评估的问题，并发现了对话参与者意识和对话连贯性在自动评估中的关键作用。",
    "en_tdlr": "This study rethinks the evaluation of open-domain dialogue systems from the perspective of interlocutors and identifies the crucial role of interlocutor awareness and dialogue continuity in automatic evaluation."
}