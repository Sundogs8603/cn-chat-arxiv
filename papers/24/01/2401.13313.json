{
    "title": "InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions. (arXiv:2401.13313v1 [cs.CV])",
    "abstract": "We study the problem of completing various visual document understanding (VDU) tasks, e.g., question answering and information extraction, on real-world documents through human-written instructions. To this end, we propose InstructDoc, the first large-scale collection of 30 publicly available VDU datasets, each with diverse instructions in a unified format, which covers a wide range of 12 tasks and includes open document types/formats. Furthermore, to enhance the generalization performance on VDU tasks, we design a new instruction-based document reading and understanding model, InstructDr, that connects document images, image encoders, and large language models (LLMs) through a trainable bridging module. Experiments demonstrate that InstructDr can effectively adapt to new VDU datasets, tasks, and domains via given instructions and outperforms existing multimodal LLMs and ChatGPT without specific training.",
    "link": "http://arxiv.org/abs/2401.13313",
    "context": "Title: InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document Understanding with Instructions. (arXiv:2401.13313v1 [cs.CV])\nAbstract: We study the problem of completing various visual document understanding (VDU) tasks, e.g., question answering and information extraction, on real-world documents through human-written instructions. To this end, we propose InstructDoc, the first large-scale collection of 30 publicly available VDU datasets, each with diverse instructions in a unified format, which covers a wide range of 12 tasks and includes open document types/formats. Furthermore, to enhance the generalization performance on VDU tasks, we design a new instruction-based document reading and understanding model, InstructDr, that connects document images, image encoders, and large language models (LLMs) through a trainable bridging module. Experiments demonstrate that InstructDr can effectively adapt to new VDU datasets, tasks, and domains via given instructions and outperforms existing multimodal LLMs and ChatGPT without specific training.",
    "path": "papers/24/01/2401.13313.json",
    "total_tokens": 985,
    "translated_title": "InstructDoc：一个用于通过指令实现对视觉文档理解的零样本泛化的数据集",
    "translated_abstract": "本研究探讨了通过人工编写的指令在真实世界文档上完成各种视觉文档理解（VDU）任务（例如问答和信息提取）的问题。为此，我们提出了InstructDoc，这是第一个包含30个公开可用的VDU数据集的大规模收集，每个数据集都具有统一格式的多样指令，涵盖了12个不同任务和包括开放文档类型/格式。此外，为了提高在VDU任务上的泛化性能，我们设计了一种新的基于指令的文档阅读与理解模型InstructDr，它通过可训练的桥接模块连接文档图像、图像编码器和大型语言模型（LLMs）。实验证明，InstructDr能够通过给定的指令有效适应新的VDU数据集、任务和领域，并且在没有具体训练的情况下胜过现有的多模态LLMs和ChatGPT。",
    "tldr": "本论文提出了InstructDoc数据集，该数据集包含了30个具有统一指令的VDU数据集，并通过新的基于指令的文档阅读和理解模型InstructDr来提高VDU任务的泛化性能。实验证明，InstructDr可以通过给定的指令有效适应新的VDU数据集、任务和领域，并且在没有具体训练的情况下胜过现有的多模态LLMs和ChatGPT。",
    "en_tdlr": "This paper proposes the InstructDoc dataset, which includes 30 VDU datasets with unified instructions, and improves the generalization performance of VDU tasks through a new instruction-based document reading and understanding model called InstructDr. Experiments show that InstructDr can effectively adapt to new VDU datasets, tasks, and domains via given instructions and outperforms existing multimodal LLMs and ChatGPT without specific training."
}