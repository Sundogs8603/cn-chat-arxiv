{
    "title": "Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning. (arXiv:2401.01013v1 [cs.LG])",
    "abstract": "Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU) has revealed that traditional machine learning methods, such as semi-supervised label propagation and K-nearest neighbors, outperform Transformer-based models in artifact detection from PPG signals, mainly when data is limited. This study addresses the underutilization of abundant unlabeled data by employing self-supervised learning (SSL) to extract latent features from these data, followed by fine-tuning on labeled data. Our experiments demonstrate that SSL significantly enhances the Transformer model's ability to learn representations, improving its robustness in artifact classification tasks. Among various SSL techniques, including masking, contrastive learning, and DINO (self-distillation with no labels)-contrastive learning exhibited the most stable and superior performance in small PPG datasets. Further, we delve into optimizing contrastive loss functions, which are crucial for contrastive SSL. Inspired b",
    "link": "http://arxiv.org/abs/2401.01013",
    "context": "Title: Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning. (arXiv:2401.01013v1 [cs.LG])\nAbstract: Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU) has revealed that traditional machine learning methods, such as semi-supervised label propagation and K-nearest neighbors, outperform Transformer-based models in artifact detection from PPG signals, mainly when data is limited. This study addresses the underutilization of abundant unlabeled data by employing self-supervised learning (SSL) to extract latent features from these data, followed by fine-tuning on labeled data. Our experiments demonstrate that SSL significantly enhances the Transformer model's ability to learn representations, improving its robustness in artifact classification tasks. Among various SSL techniques, including masking, contrastive learning, and DINO (self-distillation with no labels)-contrastive learning exhibited the most stable and superior performance in small PPG datasets. Further, we delve into optimizing contrastive loss functions, which are crucial for contrastive SSL. Inspired b",
    "path": "papers/24/01/2401.01013.json",
    "total_tokens": 953,
    "translated_title": "基于自监督学习的PPG信号伪迹检测中提高Transformer的鲁棒性和效力",
    "translated_abstract": "最近在圣杰斯坦医学院的儿科重症监护室的研究表明，传统的机器学习方法，在有限数据情况下，如半监督标签传播和K最近邻，在从PPG信号中检测到伪迹方面比基于Transformer的模型表现更好。本研究通过采用自监督学习（SSL）从大量无标签数据中提取潜在特征，然后对有标签数据进行微调，以解决对丰富无标签数据的低效利用问题。我们的实验证明，SSL能够显著增强Transformer模型学习表示的能力，提高其在伪迹分类任务中的鲁棒性。在各种SSL技术中，包括掩码、对比学习和无标签自蒸馏（DINO）-对比学习在小型PPG数据集上表现最稳定、性能最优。此外，我们还探讨了优化对比损失函数的方法，对于对比SSL来说至关重要。受到...的启发",
    "tldr": "本研究通过自监督学习提高了Transformer在PPG信号伪迹检测中的鲁棒性和效力，并发现对比学习是最稳定且表现最优的SSL技术。进一步优化对比损失函数对于对比SSL至关重要。"
}