{
    "title": "Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example. (arXiv:2401.04362v1 [cs.CV])",
    "abstract": "We introduce DiffSketch, a method for generating a variety of stylized sketches from images. Our approach focuses on selecting representative features from the rich semantics of deep features within a pretrained diffusion model. This novel sketch generation method can be trained with one manual drawing. Furthermore, efficient sketch extraction is ensured by distilling a trained generator into a streamlined extractor. We select denoising diffusion features through analysis and integrate these selected features with VAE features to produce sketches. Additionally, we propose a sampling scheme for training models using a conditional generative approach. Through a series of comparisons, we verify that distilled DiffSketch not only outperforms existing state-of-the-art sketch extraction methods but also surpasses diffusion-based stylization methods in the task of extracting sketches.",
    "link": "http://arxiv.org/abs/2401.04362",
    "context": "Title: Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example. (arXiv:2401.04362v1 [cs.CV])\nAbstract: We introduce DiffSketch, a method for generating a variety of stylized sketches from images. Our approach focuses on selecting representative features from the rich semantics of deep features within a pretrained diffusion model. This novel sketch generation method can be trained with one manual drawing. Furthermore, efficient sketch extraction is ensured by distilling a trained generator into a streamlined extractor. We select denoising diffusion features through analysis and integrate these selected features with VAE features to produce sketches. Additionally, we propose a sampling scheme for training models using a conditional generative approach. Through a series of comparisons, we verify that distilled DiffSketch not only outperforms existing state-of-the-art sketch extraction methods but also surpasses diffusion-based stylization methods in the task of extracting sketches.",
    "path": "papers/24/01/2401.04362.json",
    "total_tokens": 833,
    "translated_title": "在扩散过程中提取代表性特征，用于从一个样本中提取草图",
    "translated_abstract": "我们介绍了DiffSketch，一种从图像中生成各种风格化草图的方法。我们的方法主要是从预训练的扩散模型中选择代表性特征，以提取草图。这种新颖的草图生成方法只需要一个手工绘制的草图作为训练样本。此外，通过将训练好的生成器提取出一种简化的提取器，确保了高效的草图提取。我们通过分析选择去噪扩散特征，并将这些特征与VAE特征结合，生成草图。此外，我们提出了一种使用条件生成方法进行模型训练的采样方案。通过一系列的比较，我们验证了精简的DiffSketch不仅优于现有的最先进的草图提取方法，而且在提取草图的任务上也超越了基于扩散的风格化方法。",
    "tldr": "本论文介绍了一种名为DiffSketch的草图生成方法，通过选择代表性特征并结合生成模型进行提取，实现从图像中生成多样化的草图。与现有方法相比，该方法在提取草图的性能上具有显著的优势。",
    "en_tdlr": "This paper introduces DiffSketch, a method for generating a variety of stylized sketches from images. By selecting representative features and integrating them with a generator, it outperforms existing methods in the task of sketch extraction."
}