{
    "title": "Long-term Safe Reinforcement Learning with Binary Feedback. (arXiv:2401.03786v2 [cs.LG] UPDATED)",
    "abstract": "Safety is an indispensable requirement for applying reinforcement learning (RL) to real problems. Although there has been a surge of safe RL algorithms proposed in recent years, most existing work typically 1) relies on receiving numeric safety feedback; 2) does not guarantee safety during the learning process; 3) limits the problem to a priori known, deterministic transition dynamics; and/or 4) assume the existence of a known safe policy for any states. Addressing the issues mentioned above, we thus propose Long-term Binaryfeedback Safe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision processes (CMDPs) with binary safety feedback and an unknown, stochastic state transition function. LoBiSaRL optimizes a policy to maximize rewards while guaranteeing a long-term safety that an agent executes only safe state-action pairs throughout each episode with high probability. Specifically, LoBiSaRL models the binary safety function via a generalized linear model (GLM) and conser",
    "link": "http://arxiv.org/abs/2401.03786",
    "context": "Title: Long-term Safe Reinforcement Learning with Binary Feedback. (arXiv:2401.03786v2 [cs.LG] UPDATED)\nAbstract: Safety is an indispensable requirement for applying reinforcement learning (RL) to real problems. Although there has been a surge of safe RL algorithms proposed in recent years, most existing work typically 1) relies on receiving numeric safety feedback; 2) does not guarantee safety during the learning process; 3) limits the problem to a priori known, deterministic transition dynamics; and/or 4) assume the existence of a known safe policy for any states. Addressing the issues mentioned above, we thus propose Long-term Binaryfeedback Safe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision processes (CMDPs) with binary safety feedback and an unknown, stochastic state transition function. LoBiSaRL optimizes a policy to maximize rewards while guaranteeing a long-term safety that an agent executes only safe state-action pairs throughout each episode with high probability. Specifically, LoBiSaRL models the binary safety function via a generalized linear model (GLM) and conser",
    "path": "papers/24/01/2401.03786.json",
    "total_tokens": 986,
    "translated_title": "长期安全的强化学习与二进制反馈",
    "translated_abstract": "安全是将强化学习应用于实际问题的不可或缺的要求。尽管近年来提出了大量的安全强化学习算法，但大多数现有工作通常1）依赖于接收数值安全反馈；2）在学习过程中无法保证安全性；3）将问题限制在先验已知的确定性转换动态；以及/或者4）假设存在一个已知的安全策略以处理任何状态。针对上述问题，我们提出了长期二进制反馈安全强化学习（LoBiSaRL），这是一种针对具有二进制安全反馈和未知随机状态转换函数的约束马尔可夫决策过程（CMDPs）的安全强化学习算法。LoBiSaRL优化一个策略以使奖励最大化，同时保证在每个回合中代理仅以高概率执行安全的状态-动作对，从而确保长期安全性。具体而言，LoBiSaRL通过广义线性模型（GLM）来建模二进制安全函数，并保证长期安全性。",
    "tldr": "本论文提出了一种长期安全的强化学习算法LoBiSaRL，该算法针对具有二进制安全反馈和未知随机状态转换函数的约束马尔可夫决策过程（CMDPs），通过最大化奖励的方式优化策略，同时以高概率确保每个回合中代理只执行安全的状态-动作对。",
    "en_tdlr": "This paper proposes a long-term safe reinforcement learning algorithm, LoBiSaRL, that addresses the issues of receiving numeric safety feedback, guaranteeing safety during the learning process, and handling unknown stochastic state transition functions. LoBiSaRL optimizes policies to maximize rewards while ensuring the execution of safe state-action pairs throughout each episode."
}