{
    "title": "Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss. (arXiv:2401.02677v1 [cs.CV])",
    "abstract": "Stable Diffusion XL (SDXL) has become the best open source text-to-image model (T2I) for its versatility and top-notch image quality. Efficiently addressing the computational demands of SDXL models is crucial for wider reach and applicability. In this work, we introduce two scaled-down variants, Segmind Stable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter UNets, respectively, achieved through progressive removal using layer-level losses focusing on reducing the model size while preserving generative quality. We release these models weights at https://hf.co/Segmind. Our methodology involves the elimination of residual networks and transformer blocks from the U-Net structure of SDXL, resulting in significant reductions in parameters, and latency. Our compact models effectively emulate the original SDXL by capitalizing on transferred knowledge, achieving competitive results against larger multi-billion parameter SDXL. Our work underscores the efficacy of knowledge dis",
    "link": "http://arxiv.org/abs/2401.02677",
    "context": "Title: Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss. (arXiv:2401.02677v1 [cs.CV])\nAbstract: Stable Diffusion XL (SDXL) has become the best open source text-to-image model (T2I) for its versatility and top-notch image quality. Efficiently addressing the computational demands of SDXL models is crucial for wider reach and applicability. In this work, we introduce two scaled-down variants, Segmind Stable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter UNets, respectively, achieved through progressive removal using layer-level losses focusing on reducing the model size while preserving generative quality. We release these models weights at https://hf.co/Segmind. Our methodology involves the elimination of residual networks and transformer blocks from the U-Net structure of SDXL, resulting in significant reductions in parameters, and latency. Our compact models effectively emulate the original SDXL by capitalizing on transferred knowledge, achieving competitive results against larger multi-billion parameter SDXL. Our work underscores the efficacy of knowledge dis",
    "path": "papers/24/01/2401.02677.json",
    "total_tokens": 1023,
    "translated_title": "逐层损失稳定扩散XL的渐进式知识蒸馏",
    "translated_abstract": "稳定扩散XL（SDXL）已成为非常多功能且质量卓越的开源文本到图像模型（T2I）。高效地解决SDXL模型的计算需求对于扩大其应用范围至关重要。在本研究中，我们通过逐层损失的渐进式削减引入了两个精简变体，即Segmind稳定扩散（SSD-1B）和Segmind-Vega，分别使用13亿和0.74亿参数的UNet结构实现，并注重减小模型大小同时保持生成质量。我们在https://hf.co/Segmind上发布了这些模型的权重。我们的方法包括从SDXL的U-Net结构中消除残差网络和Transformer块，从而显著减少参数和延迟。我们的精简模型通过利用转移学习能够有效地模拟原始的SDXL，并在与更大的千亿参数级SDXL相比的竞争中取得了有竞争力的结果。我们的工作强调了知识蒸馏的有效性。",
    "tldr": "本研究通过逐层损失的渐进式削减，引入了两个精简变体Segmind稳定扩散（SSD-1B）和Segmind-Vega，分别使用1.3B和0.74B参数的UNet结构实现。这些精简模型通过转移学习有效地模拟了原始的SDXL，并在与更大的SDXL模型相比的竞争中取得了有竞争力的结果。",
    "en_tdlr": "In this work, two scaled-down variants of Stable Diffusion XL (SDXL), Segmind Stable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter UNets respectively, were introduced using progressive removal with layer-level losses. These compact models effectively emulate the original SDXL through transferred knowledge, achieving competitive results against larger SDXL models."
}