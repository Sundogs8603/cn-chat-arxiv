{
    "title": "Setting the Record Straight on Transformer Oversmoothing. (arXiv:2401.04301v1 [cs.LG])",
    "abstract": "Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that gradually oversmooth the inputs, reducing the expressivity of their representations. A natural question is: How can Transformers achieve these successes given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Based on this analysis, we derive a simple way to parameterize the weights of the Transformer update equations that allows for control over its spectrum, ensuring that oversmoothing does not occur. Compared to a ",
    "link": "http://arxiv.org/abs/2401.04301",
    "context": "Title: Setting the Record Straight on Transformer Oversmoothing. (arXiv:2401.04301v1 [cs.LG])\nAbstract: Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that gradually oversmooth the inputs, reducing the expressivity of their representations. A natural question is: How can Transformers achieve these successes given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Based on this analysis, we derive a simple way to parameterize the weights of the Transformer update equations that allows for control over its spectrum, ensuring that oversmoothing does not occur. Compared to a ",
    "path": "papers/24/01/2401.04301.json",
    "total_tokens": 875,
    "translated_title": "《关于Transformer过度平滑的真相》",
    "translated_abstract": "最近，基于Transformer的模型在各个领域取得了巨大成功。与此同时，最新的研究表明，Transformer本质上是一种低通滤波器，会逐渐过度平滑输入数据，降低其表示能力。一个自然的问题是：在存在这个缺陷的情况下，Transformer是如何取得这些成功的？在这项研究中，我们展示了事实上Transformer并不本质上是一种低通滤波器。相反，Transformer是否过度平滑取决于其更新方程的特征谱。我们的分析扩展了之前关于过度平滑和相关现象的研究。我们表明，许多成功的Transformer模型具有满足避免过度平滑条件的注意力和权重。基于这个分析，我们提出了一种简单的方法，对Transformer更新方程的权重进行参数化，使其可以控制其谱特性，确保不会发生过度平滑。与传统的方法相比，我们的方法可以更好地控制过度平滑问题。",
    "tldr": "Transformers are not inherently low-pass filters as previously thought and whether they oversmooth or not depends on the eigenspectrum of their update equations. Based on the analysis, a simple way to parameterize the weights of the Transformer update equations is derived, allowing for control over its spectrum and preventing oversmoothing."
}