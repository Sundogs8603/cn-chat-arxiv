{
    "title": "Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning. (arXiv:2401.04361v1 [cs.CL])",
    "abstract": "Knowledge-grounded dialogue (KGD) learns to generate an informative response based on a given dialogue context and external knowledge (\\emph{e.g.}, knowledge graphs; KGs). Recently, the emergence of large language models (LLMs) and pre-training techniques has brought great success to knowledge-grounded dialogue. However, when building KGD systems in real applications, there are various real-world noises that are inevitable to face. For example, the dialogue context might involve perturbations such as misspellings and abbreviations. In addition, KGs typically suffer from incompletion and also might contain erroneous and outdated facts. Such real-world noises pose a challenge to the robustness of KGD systems and hinder their applications in the real world. In this paper, we propose an entity-based contrastive learning framework for improving the robustness of KGD. Specifically, we make use of the entity information in a KGD sample to create both its positive and negative samples which in",
    "link": "http://arxiv.org/abs/2401.04361",
    "context": "Title: Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive Learning. (arXiv:2401.04361v1 [cs.CL])\nAbstract: Knowledge-grounded dialogue (KGD) learns to generate an informative response based on a given dialogue context and external knowledge (\\emph{e.g.}, knowledge graphs; KGs). Recently, the emergence of large language models (LLMs) and pre-training techniques has brought great success to knowledge-grounded dialogue. However, when building KGD systems in real applications, there are various real-world noises that are inevitable to face. For example, the dialogue context might involve perturbations such as misspellings and abbreviations. In addition, KGs typically suffer from incompletion and also might contain erroneous and outdated facts. Such real-world noises pose a challenge to the robustness of KGD systems and hinder their applications in the real world. In this paper, we propose an entity-based contrastive learning framework for improving the robustness of KGD. Specifically, we make use of the entity information in a KGD sample to create both its positive and negative samples which in",
    "path": "papers/24/01/2401.04361.json",
    "total_tokens": 898,
    "translated_title": "通过对比学习提高知识驱动对话的稳健性",
    "translated_abstract": "知识驱动的对话（KGD）通过给定的对话环境和外部知识（如知识图谱）来生成信息丰富的回应。最近，大型语言模型（LLMs）和预训练技术的出现为知识驱动的对话带来了巨大的成功。然而，在实际应用中构建KGD系统时，难免会遇到各种实际的噪音。例如，对话环境可能涉及错别字和缩写等扰动。此外，知识图谱通常存在不完整性，也可能包含错误和过时的事实。这些实际的噪音给KGD系统的稳健性带来了挑战，并阻碍了它们在真实世界中的应用。本文提出了一种基于实体对比学习框架来提高KGD稳健性的方法。具体而言，我们利用KGD样本中的实体信息创建其正样本和负样本。",
    "tldr": "本文提出了一种基于实体对比学习框架来提高知识驱动对话（KGD）系统的稳健性，通过创建正负样本以应对实际噪音，如错别字和不完整的知识图谱。",
    "en_tdlr": "This paper proposes an entity-based contrastive learning framework to improve the robustness of knowledge-grounded dialogue (KGD) systems, by creating positive and negative samples to address real-world noises such as misspellings and incomplete knowledge graphs."
}