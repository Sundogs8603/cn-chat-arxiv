{
    "title": "T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives. (arXiv:2401.16677v1 [cs.AR])",
    "abstract": "Large Language Models increasingly rely on distributed techniques for their training and inference. These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases. While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution. One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner. However, this fine-grained interleaving of communication and computation in software can be difficult. Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy.  To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently ",
    "link": "http://arxiv.org/abs/2401.16677",
    "context": "Title: T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives. (arXiv:2401.16677v1 [cs.AR])\nAbstract: Large Language Models increasingly rely on distributed techniques for their training and inference. These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases. While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution. One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner. However, this fine-grained interleaving of communication and computation in software can be difficult. Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy.  To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently ",
    "path": "papers/24/01/2401.16677.json",
    "total_tokens": 846,
    "translated_title": "T3：面向计算和集合细粒度重叠的透明跟踪和触发",
    "translated_abstract": "大型语言模型越来越多地依赖于分布式技术进行训练和推理。这些技术需要设备之间的通信，随着设备数量的增加，通信会降低扩展效率。虽然一些分布式技术可以重叠并隐藏这种通信与独立计算，但张量并行技术(TP)本质上将通信与模型执行串行化。隐藏这种串行通信的一种方法是以细粒度的方式将其与生产操作(通信数据)交替进行。然而，软件中的这种细粒度通信和计算交错可能很困难。此外，与任何并发执行一样，它需要计算和内存资源在计算和通信之间共享，导致资源争用减少重叠效果。为了克服这些挑战，我们提出了T3，它应用硬件-软件协同设计来实现透明的跟踪和触发。",
    "tldr": "T3提出了一种透明跟踪和触发的硬件-软件协同设计方法，用于解决大型语言模型在分布式训练和推理中的通信效率问题。"
}