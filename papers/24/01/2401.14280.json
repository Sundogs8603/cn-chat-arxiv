{
    "title": "RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization. (arXiv:2401.14280v1 [cs.CL])",
    "abstract": "This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks.",
    "link": "http://arxiv.org/abs/2401.14280",
    "context": "Title: RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization. (arXiv:2401.14280v1 [cs.CL])\nAbstract: This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks.",
    "path": "papers/24/01/2401.14280.json",
    "total_tokens": 924,
    "translated_title": "RomanSetu: 通过罗马化有效地利用大语言模型的多语言能力",
    "translated_abstract": "本研究解决了将大型语言模型扩展到非英语语言（特别是使用非拉丁字母表的语言）的挑战。我们提出了一种创新的方法，利用罗马化形式的文本作为大语言模型的接口，假设频繁的非正式使用和与英语共享的标记有助于跨语言对齐。我们以印地语为重点，通过印地语到英语的翻译和情感分析任务，证明罗马化文本不仅由于其较低的生产力而显著改善了推理效率，还在有限的预训练中实现了有竞争力的性能。此外，我们的新颖的多脚本提示方法结合了罗马化和原生文本，在进一步提高任务性能方面显示出潜力。这些发现表明罗马化在弥合大语言模型应用中的语言障碍方面具有潜力，未来的工作将致力于将此方法扩展到更多的语言和任务。",
    "tldr": "本研究提出了一种创新的方法，通过使用罗马化形式的文本作为接口，有效地利用大语言模型的多语言能力。通过在印地语上的实验证明，罗马化文本不仅提高了推理效率，还在有限的预训练下实现了有竞争力的性能。这些发现表明罗马化有潜力弥合大语言模型应用中的语言障碍。"
}