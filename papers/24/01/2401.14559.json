{
    "title": "Language Modelling Approaches to Adaptive Machine Translation. (arXiv:2401.14559v1 [cs.CL])",
    "abstract": "Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, in-domain data scarcity is common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. While real-time adaptation can make use of smaller amounts of in-domain data to improve the translation on the fly, it remains challenging due to supported context limitations and efficiency constraints. Large language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-outpu",
    "link": "http://arxiv.org/abs/2401.14559",
    "context": "Title: Language Modelling Approaches to Adaptive Machine Translation. (arXiv:2401.14559v1 [cs.CL])\nAbstract: Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, in-domain data scarcity is common in translation settings, due to the lack of specialised datasets and terminology, or inconsistency and inaccuracy of available in-domain translations. In such scenarios where there is insufficient in-domain data to fine-tune MT models, producing translations that are consistent with the relevant context is challenging. While real-time adaptation can make use of smaller amounts of in-domain data to improve the translation on the fly, it remains challenging due to supported context limitations and efficiency constraints. Large language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-outpu",
    "path": "papers/24/01/2401.14559.json",
    "total_tokens": 904,
    "translated_title": "自适应机器翻译的语言建模方法",
    "translated_abstract": "一致性是高质量翻译的关键要求。在特定领域项目中，遵循预定义的术语和适应更正后的翻译尤为重要。机器翻译在域适应方面取得了显著进展。然而，在翻译环境中，由于缺乏专门的数据集和术语，或者可用的域内翻译不一致且不准确，导致域内数据稀缺现象普遍存在。在没有足够域内数据用于微调机器翻译模型的情况下，生成与相关上下文一致的翻译是具有挑战性的。虽然实时适应可以利用较少量的域内数据实时改进翻译质量，但由于受到上下文限制和效率约束，仍然具有挑战性。最近，大型语言模型（LLMs）展示了在上下文中学习的有趣能力，通过学习复制特定的输入输出来改进翻译效果。",
    "tldr": "自适应机器翻译中的语言建模方法，通过利用大型语言模型(LLMs)来在特定上下文中学习、改进翻译质量，解决了在域适应中因缺乏域内数据而导致翻译不一致的问题。",
    "en_tdlr": "Language modelling approaches in adaptive machine translation address the issue of inconsistent translations in domain adaptation by utilizing large language models (LLMs) to learn and improve translation quality in specific contexts where there is a scarcity of in-domain data."
}