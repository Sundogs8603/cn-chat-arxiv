{
    "title": "Enhancing Image-Text Matching with Adaptive Feature Aggregation. (arXiv:2401.09725v1 [cs.IR])",
    "abstract": "Image-text matching aims to find matched cross-modal pairs accurately. While current methods often rely on projecting cross-modal features into a common embedding space, they frequently suffer from imbalanced feature representations across different modalities, leading to unreliable retrieval results. To address these limitations, we introduce a novel Feature Enhancement Module that adaptively aggregates single-modal features for more balanced and robust image-text retrieval. Additionally, we propose a new loss function that overcomes the shortcomings of original triplet ranking loss, thereby significantly improving retrieval performance. The proposed model has been evaluated on two public datasets and achieves competitive retrieval performance when compared with several state-of-the-art models. Implementation codes can be found here.",
    "link": "http://arxiv.org/abs/2401.09725",
    "context": "Title: Enhancing Image-Text Matching with Adaptive Feature Aggregation. (arXiv:2401.09725v1 [cs.IR])\nAbstract: Image-text matching aims to find matched cross-modal pairs accurately. While current methods often rely on projecting cross-modal features into a common embedding space, they frequently suffer from imbalanced feature representations across different modalities, leading to unreliable retrieval results. To address these limitations, we introduce a novel Feature Enhancement Module that adaptively aggregates single-modal features for more balanced and robust image-text retrieval. Additionally, we propose a new loss function that overcomes the shortcomings of original triplet ranking loss, thereby significantly improving retrieval performance. The proposed model has been evaluated on two public datasets and achieves competitive retrieval performance when compared with several state-of-the-art models. Implementation codes can be found here.",
    "path": "papers/24/01/2401.09725.json",
    "total_tokens": 767,
    "translated_title": "改进图像-文本匹配的自适应特征聚合",
    "translated_abstract": "图像-文本匹配旨在准确找到匹配的跨模态对。当前的方法通常依赖于将跨模态特征映射到一个共同的嵌入空间，但往往存在不平衡的特征表示，造成检索结果不可靠。为了解决这些问题，我们引入了一种新的特征增强模块，通过自适应聚合单模态特征，实现更平衡和稳健的图像-文本检索。此外，我们提出了一种新的损失函数，克服了原始三元排名损失的不足，从而显著提高了检索性能。我们在两个公共数据集上对模型进行了评估，与几种最先进的模型相比，在检索性能上取得了有竞争力的结果。实现代码可以在这里找到。",
    "tldr": "本研究提出了一种改进图像-文本匹配的方法，通过引入特征增强模块和新的损失函数，实现了更平衡和稳健的图像-文本检索。"
}