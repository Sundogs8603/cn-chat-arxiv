{
    "title": "ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change. (arXiv:2401.09646v1 [cs.LG])",
    "abstract": "This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to ",
    "link": "http://arxiv.org/abs/2401.09646",
    "context": "Title: ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change. (arXiv:2401.09646v1 [cs.LG])\nAbstract: This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to ",
    "path": "papers/24/01/2401.09646.json",
    "total_tokens": 912,
    "translated_title": "ClimateGPT: 实现对气候变化领域的跨学科研究进行合成的AI模型",
    "translated_abstract": "本文介绍了ClimateGPT，一种特定领域的大型语言模型系列，用于合成气候变化的跨学科研究。我们从头开始训练了两个7B模型，训练数据集包含300B个科学导向的令牌。第一个模型在预训练期间包含了4.2B个特定领域的令牌，第二个模型在预训练后针对气候领域进行了调整。此外，我们还对ClimateGPT-7B，13B和70B进行了连续预训练，训练数据集包含4.2B个特定领域的令牌，并与气候科学家紧密合作创建。为了减少虚构生成的数量，我们为模型进行了检索增强优化，并提出了一种分层检索策略。为了提高我们模型对非英语使用者的可访问性，我们建议利用级联机器翻译，并证明这种方法可以与翻译的性能相媲美。",
    "tldr": "ClimateGPT是一个针对气候变化领域的跨学科研究合成的AI模型，通过优化检索增强和使用级联机器翻译方法，提高了模型的性能和可访问性。"
}