{
    "title": "Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model. (arXiv:2401.10700v1 [cs.LG])",
    "abstract": "Safe offline RL is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the f",
    "link": "http://arxiv.org/abs/2401.10700",
    "context": "Title: Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model. (arXiv:2401.10700v1 [cs.LG])\nAbstract: Safe offline RL is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the f",
    "path": "papers/24/01/2401.10700.json",
    "total_tokens": 914,
    "translated_title": "安全离线强化学习与可行性导向扩散模型",
    "translated_abstract": "安全离线强化学习是一种有效避免风险的在线交互以实现安全策略学习的方法。大多数现有的方法只强制软约束，即将期望的安全违规约束在预定的阈值以下。然而，这可能导致潜在的不安全结果，在安全关键场景中是不可接受的。另一种选择是强制零违规的硬约束。然而，在离线设置中，这可能具有挑战性，因为需要在安全约束满足、奖励最大化和离线数据集所施加的行为正则化之间取得适当的平衡。有趣的是，我们发现通过安全控制理论的可达性分析，硬安全约束可以等效地转化为识别给定离线数据集的最大可行区域。这无缝地将原始的三重问题转化为依赖于可行性的目标，即在可行性范围内最大化奖励值。",
    "tldr": "本论文提出了一种安全离线强化学习方法，通过可行性导向的扩散模型来平衡安全约束满足、奖励最大化和离线数据集的行为规范化。通过可达性分析，将硬安全约束转化为在离线数据集中识别最大可行区域。",
    "en_tdlr": "This paper presents a safe offline reinforcement learning approach that balances safety constraint satisfaction, reward maximization, and behavior regularization of offline datasets using a feasibility-guided diffusion model. The hard safety constraint is translated into identifying the largest feasible region within the offline dataset through reachability analysis."
}