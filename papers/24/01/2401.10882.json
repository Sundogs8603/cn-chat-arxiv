{
    "title": "Reinforcement learning for question answering in programming domain using public community scoring as a human feedback. (arXiv:2401.10882v1 [cs.CL])",
    "abstract": "In this study, we investigate the enhancement of the GPT Neo 125M performance in Community Question Answering (CQA) with a focus on programming, through the integration of Reinforcement Learning from Human Feedback (RLHF) and the utilization of scores from Stack Overflow. Two distinct reward model training strategies are employed for fine-tuning with Proximal Policy Optimization (PPO). Notably, the improvements in performance achieved through this method are comparable to those of GPT Neo 2.7B parameter variant. Additionally, an auxiliary scoring mechanism is introduced, which demonstrates the limitations of conventional linguistic metrics in evaluating responses in the programming domain. Through accurate analysis, this paper looks at the divergence between traditional linguistic metrics and our human-preferences-based reward model, underscoring the imperative for domain-specific evaluation methods. By elucidating the complexities involved in applying RLHF to programming CQA and accen",
    "link": "http://arxiv.org/abs/2401.10882",
    "context": "Title: Reinforcement learning for question answering in programming domain using public community scoring as a human feedback. (arXiv:2401.10882v1 [cs.CL])\nAbstract: In this study, we investigate the enhancement of the GPT Neo 125M performance in Community Question Answering (CQA) with a focus on programming, through the integration of Reinforcement Learning from Human Feedback (RLHF) and the utilization of scores from Stack Overflow. Two distinct reward model training strategies are employed for fine-tuning with Proximal Policy Optimization (PPO). Notably, the improvements in performance achieved through this method are comparable to those of GPT Neo 2.7B parameter variant. Additionally, an auxiliary scoring mechanism is introduced, which demonstrates the limitations of conventional linguistic metrics in evaluating responses in the programming domain. Through accurate analysis, this paper looks at the divergence between traditional linguistic metrics and our human-preferences-based reward model, underscoring the imperative for domain-specific evaluation methods. By elucidating the complexities involved in applying RLHF to programming CQA and accen",
    "path": "papers/24/01/2401.10882.json",
    "total_tokens": 819,
    "translated_title": "使用公共社区评分作为人类反馈，在编程领域中使用强化学习进行问答的研究",
    "translated_abstract": "本研究通过整合人类反馈的强化学习和来自Stack Overflow的评分，探讨了在编程领域中提高GPT Neo 125M在社区问答中的性能的方法。采用了两种不同的奖励模型训练策略进行微调，通过Proximal Policy Optimization (PPO)实现。这种方法在性能改进方面与GPT Neo 2.7B参数变体相当。此外，还引入了辅助评分机制，显示了传统语言度量在编程领域中评估响应的局限性。通过准确的分析，本文研究了将强化学习从人类反馈应用于编程社区问答的复杂性，并强调了需要领域特定的评估方法的必要性。",
    "tldr": "本研究通过整合强化学习和Stack Overflow评分，提高了GPT Neo在编程问答中的性能，同时指出传统语言度量方法在编程领域的局限性。"
}