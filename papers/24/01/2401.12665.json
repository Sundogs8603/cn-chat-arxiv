{
    "title": "ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation. (arXiv:2401.12665v1 [cs.CV])",
    "abstract": "Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple sc",
    "link": "http://arxiv.org/abs/2401.12665",
    "context": "Title: ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation. (arXiv:2401.12665v1 [cs.CV])\nAbstract: Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple sc",
    "path": "papers/24/01/2401.12665.json",
    "total_tokens": 920,
    "translated_title": "ClipSAM：CLIP和SAM的合作用于零样本异常分割",
    "translated_abstract": "最近，基于CLIP和SAM等基础模型在零样本异常分割（ZSAS）任务中表现出了很有希望的性能。然而，无论是基于CLIP还是SAM的ZSAS方法仍然存在一些不可忽视的关键缺点：1）CLIP主要关注不同输入之间的全局特征对齐，导致对局部异常部分的分割不准确；2）SAM倾向于生成大量没有适当提示约束的冗余掩码，导致复杂的后处理要求。在这项工作中，我们创新性地提出了一种名为ClipSAM的CLIP和SAM协作框架，用于ZSAS。ClipSAM的思路是利用CLIP的语义理解能力进行异常定位和粗糙分割，进一步将其用作提供提示约束以改进SAM的异常分割结果。具体地，我们引入了一个关键的统一多尺度跨模态交互（UMCI）模块，用于在多个尺度上相互作用语言和视觉特征。",
    "tldr": "这项工作提出了一种名为ClipSAM的CLIP和SAM协作框架，用于零样本异常分割。ClipSAM利用CLIP的语义理解能力进行异常定位和粗糙分割，然后将其用作SAM的提示约束，进一步改进异常分割结果。",
    "en_tdlr": "This work proposes a collaboration framework called ClipSAM for zero-shot anomaly segmentation, which utilizes CLIP's semantic understanding to localize and roughly segment anomalies, and further serves as prompt constraints for SAM to improve the segmentation results."
}