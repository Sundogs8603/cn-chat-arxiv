{
    "title": "Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis. (arXiv:2401.09587v1 [cs.LG])",
    "abstract": "Bilevel optimization is an important formulation for many machine learning problems. Current bilevel optimization algorithms assume that the gradient of the upper-level function is Lipschitz. However, recent studies reveal that certain neural networks such as recurrent neural networks (RNNs) and long-short-term memory networks (LSTMs) exhibit potential unbounded smoothness, rendering conventional bilevel optimization algorithms unsuitable. In this paper, we design a new bilevel optimization algorithm, namely BO-REP, to address this challenge. This algorithm updates the upper-level variable using normalized momentum and incorporates two novel techniques for updating the lower-level variable: \\textit{initialization refinement} and \\textit{periodic updates}. Specifically, once the upper-level variable is initialized, a subroutine is invoked to obtain a refined estimate of the corresponding optimal lower-level variable, and the lower-level variable is updated only after every specific peri",
    "link": "http://arxiv.org/abs/2401.09587",
    "context": "Title: Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis. (arXiv:2401.09587v1 [cs.LG])\nAbstract: Bilevel optimization is an important formulation for many machine learning problems. Current bilevel optimization algorithms assume that the gradient of the upper-level function is Lipschitz. However, recent studies reveal that certain neural networks such as recurrent neural networks (RNNs) and long-short-term memory networks (LSTMs) exhibit potential unbounded smoothness, rendering conventional bilevel optimization algorithms unsuitable. In this paper, we design a new bilevel optimization algorithm, namely BO-REP, to address this challenge. This algorithm updates the upper-level variable using normalized momentum and incorporates two novel techniques for updating the lower-level variable: \\textit{initialization refinement} and \\textit{periodic updates}. Specifically, once the upper-level variable is initialized, a subroutine is invoked to obtain a refined estimate of the corresponding optimal lower-level variable, and the lower-level variable is updated only after every specific peri",
    "path": "papers/24/01/2401.09587.json",
    "total_tokens": 824,
    "translated_title": "无界平滑约束下的双层优化：一种新算法和收敛分析",
    "translated_abstract": "双层优化是许多机器学习问题中的重要形式。当前的双层优化算法假设上层函数的梯度是Lipschitz的。然而，最近的研究发现，某些神经网络如循环神经网络（RNN）和长短期记忆网络（LSTM）展现出潜在的无界平滑性，使得传统的双层优化算法无法适用。在本文中，我们设计了一种新的双层优化算法，称为BO-REP，以应对这一挑战。该算法使用归一化动量更新上层变量，并采用两种新技术来更新下层变量：初始化细化和周期性更新。具体而言，一旦上层变量被初始化，将调用一个子程序来获取相应优化的下层变量的精细估计，并且只在特定周期后更新下层变量。",
    "tldr": "该论文提出了一种新的双层优化算法BO-REP，用于解决神经网络中存在的无界平滑性问题，通过使用归一化动量更新上层变量并引入初始化细化和周期性更新两种新技术来更新下层变量。"
}