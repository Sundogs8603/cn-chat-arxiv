{
    "title": "Linear Recursive Feature Machines provably recover low-rank matrices. (arXiv:2401.04553v1 [stat.ML])",
    "abstract": "A fundamental problem in machine learning is to understand how neural networks make accurate predictions, while seemingly bypassing the curse of dimensionality. A possible explanation is that common training algorithms for neural networks implicitly perform dimensionality reduction - a process called feature learning. Recent work posited that the effects of feature learning can be elicited from a classical statistical estimator called the average gradient outer product (AGOP). The authors proposed Recursive Feature Machines (RFMs) as an algorithm that explicitly performs feature learning by alternating between (1) reweighting the feature vectors by the AGOP and (2) learning the prediction function in the transformed space. In this work, we develop the first theoretical guarantees for how RFM performs dimensionality reduction by focusing on the class of overparametrized problems arising in sparse linear regression and low-rank matrix recovery. Specifically, we show that RFM restricted t",
    "link": "http://arxiv.org/abs/2401.04553",
    "context": "Title: Linear Recursive Feature Machines provably recover low-rank matrices. (arXiv:2401.04553v1 [stat.ML])\nAbstract: A fundamental problem in machine learning is to understand how neural networks make accurate predictions, while seemingly bypassing the curse of dimensionality. A possible explanation is that common training algorithms for neural networks implicitly perform dimensionality reduction - a process called feature learning. Recent work posited that the effects of feature learning can be elicited from a classical statistical estimator called the average gradient outer product (AGOP). The authors proposed Recursive Feature Machines (RFMs) as an algorithm that explicitly performs feature learning by alternating between (1) reweighting the feature vectors by the AGOP and (2) learning the prediction function in the transformed space. In this work, we develop the first theoretical guarantees for how RFM performs dimensionality reduction by focusing on the class of overparametrized problems arising in sparse linear regression and low-rank matrix recovery. Specifically, we show that RFM restricted t",
    "path": "papers/24/01/2401.04553.json",
    "total_tokens": 944,
    "translated_title": "线性递归特征机器可以可靠地恢复低秩矩阵",
    "translated_abstract": "机器学习中一个基本的问题是要理解神经网络如何准确预测，同时似乎避免了维数诅咒。一个可能的解释是神经网络的常见训练算法隐含地进行维数减少 - 一个被称为特征学习的过程。最近的工作假设特征学习的效果可以从一个称为平均梯度外积（AGOP）的经典统计估计器中推测出来。作者提出了递归特征机器（RFMs）作为一种算法，通过在转换空间中交替进行（1）通过AGOP对特征向量重新加权和（2）学习预测函数，显式地执行特征学习。在这项工作中，我们通过关注稀疏线性回归和低秩矩阵恢复中出现的过参数化问题的类别，开发了关于RFM如何进行维数减少的第一个理论保证。具体地，我们展示了RFM在限制条件下的性能。",
    "tldr": "该论文介绍了递归特征机器（RFMs）算法，它通过交替重新加权特征向量和在转换空间中学习预测函数来执行显式的特征学习。研究分析了RFM在稀疏线性回归和低秩矩阵恢复问题中的维数减少性能。"
}