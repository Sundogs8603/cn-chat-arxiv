{
    "title": "Expressivity-aware Music Performance Retrieval using Mid-level Perceptual Features and Emotion Word Embeddings. (arXiv:2401.14826v1 [cs.SD])",
    "abstract": "This paper explores a specific sub-task of cross-modal music retrieval. We consider the delicate task of retrieving a performance or rendition of a musical piece based on a description of its style, expressive character, or emotion from a set of different performances of the same piece. We observe that a general purpose cross-modal system trained to learn a common text-audio embedding space does not yield optimal results for this task. By introducing two changes -- one each to the text encoder and the audio encoder -- we demonstrate improved performance on a dataset of piano performances and associated free-text descriptions. On the text side, we use emotion-enriched word embeddings (EWE) and on the audio side, we extract mid-level perceptual features instead of generic audio embeddings. Our results highlight the effectiveness of mid-level perceptual features learnt from music and emotion enriched word embeddings learnt from emotion-labelled text in capturing musical expression in a cr",
    "link": "http://arxiv.org/abs/2401.14826",
    "context": "Title: Expressivity-aware Music Performance Retrieval using Mid-level Perceptual Features and Emotion Word Embeddings. (arXiv:2401.14826v1 [cs.SD])\nAbstract: This paper explores a specific sub-task of cross-modal music retrieval. We consider the delicate task of retrieving a performance or rendition of a musical piece based on a description of its style, expressive character, or emotion from a set of different performances of the same piece. We observe that a general purpose cross-modal system trained to learn a common text-audio embedding space does not yield optimal results for this task. By introducing two changes -- one each to the text encoder and the audio encoder -- we demonstrate improved performance on a dataset of piano performances and associated free-text descriptions. On the text side, we use emotion-enriched word embeddings (EWE) and on the audio side, we extract mid-level perceptual features instead of generic audio embeddings. Our results highlight the effectiveness of mid-level perceptual features learnt from music and emotion enriched word embeddings learnt from emotion-labelled text in capturing musical expression in a cr",
    "path": "papers/24/01/2401.14826.json",
    "total_tokens": 926,
    "translated_title": "利用中层感知特征和情感词嵌入的表达力感知音乐演奏检索",
    "translated_abstract": "本文探讨了跨模态音乐检索的一个特定子任务。我们考虑根据音乐作品的风格、表现力或情感描述来从不同演奏中检索该作品的演奏或演绎的艰巨任务。我们观察到，训练了一个通用的跨模态系统来学习共同的文本-音频嵌入空间，并不能为这个任务提供最优结果。通过引入两个改变——一个是对文本编码器的改变，一个是对音频编码器的改变——我们证明了在一个由钢琴演奏和相关自由文本描述组成的数据集上获得了改进的性能。在文本方面，我们使用富含情感的词嵌入（EWE）；在音频方面，我们提取中层感知特征而不是一般的音频嵌入。我们的结果强调了从音乐中学习的中层感知特征和从带有情感标签的文本中学习的情感富集词嵌入在捕捉音乐表达方面的有效性。",
    "tldr": "本文提出了一种利用中层感知特征和情感词嵌入的新方法，用于从不同音乐演奏中根据风格、表现力或情感描述检索特定曲目的演奏或演绎。",
    "en_tdlr": "This paper proposes a new approach using mid-level perceptual features and emotion word embeddings to retrieve a performance or rendition of a specific musical piece based on style, expressive character, or emotion descriptions from different musical performances."
}