{
    "title": "Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning. (arXiv:2401.04151v1 [cs.LG])",
    "abstract": "Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks.  We introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as",
    "link": "http://arxiv.org/abs/2401.04151",
    "context": "Title: Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning. (arXiv:2401.04151v1 [cs.LG])\nAbstract: Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks.  We introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as",
    "path": "papers/24/01/2401.04151.json",
    "total_tokens": 927,
    "translated_title": "LoRA链：通过残差学习实现语言模型的高效微调",
    "translated_abstract": "微调是将预训练的大型语言模型定制为特定任务的主要方法。随着模型规模和任务多样性的扩大，参数高效的微调方法变得至关重要。其中最常用的方法之一是低秩适应（LoRA）及其变体。LoRA将权重更新编码为两个低秩矩阵的乘积。尽管具有一定优势，但在某些任务的泛化错误方面，LoRA无法完全参数化微调。我们引入了一种称为LoRA链（COLA）的迭代优化框架，受Frank-Wolfe算法的启发，以弥合LoRA和全参数微调之间的差距，而不会增加额外的计算成本或内存开销。COLA采用残差学习过程，在预训练语言模型参数中合并学到的LoRA模块，并重新初始化新生的LoRA模块的优化过程。我们提供了理论上的收敛保证以及...",
    "tldr": "本论文提出了一种名为LoRA链（COLA）的迭代优化框架，通过残差学习过程将LoRA模块与预训练的语言模型参数合并，并重新初始化新的LoRA模块的优化过程，从而实现LoRA和全参数微调之间的平衡。",
    "en_tdlr": "This paper proposes an iterative optimization framework called Chain of LoRA (COLA), which merges LoRA modules with pre-trained language model parameters using a residual learning procedure and re-initializes the optimization process for new LoRA modules, thus achieving a balance between LoRA and full parameter fine-tuning."
}