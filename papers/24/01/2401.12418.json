{
    "title": "Towards Improved Variational Inference for Deep Bayesian Models. (arXiv:2401.12418v1 [cs.LG])",
    "abstract": "Deep learning has revolutionized the last decade, being at the forefront of extraordinary advances in a wide range of tasks including computer vision, natural language processing, and reinforcement learning, to name but a few. However, it is well-known that deep models trained via maximum likelihood estimation tend to be overconfident and give poorly-calibrated predictions. Bayesian deep learning attempts to address this by placing priors on the model parameters, which are then combined with a likelihood to perform posterior inference. Unfortunately, for deep models, the true posterior is intractable, forcing the user to resort to approximations. In this thesis, we explore the use of variational inference (VI) as an approximation, as it is unique in simultaneously approximating the posterior and providing a lower bound to the marginal likelihood. If tight enough, this lower bound can be used to optimize hyperparameters and to facilitate model selection. However, this capacity has rarel",
    "link": "http://arxiv.org/abs/2401.12418",
    "context": "Title: Towards Improved Variational Inference for Deep Bayesian Models. (arXiv:2401.12418v1 [cs.LG])\nAbstract: Deep learning has revolutionized the last decade, being at the forefront of extraordinary advances in a wide range of tasks including computer vision, natural language processing, and reinforcement learning, to name but a few. However, it is well-known that deep models trained via maximum likelihood estimation tend to be overconfident and give poorly-calibrated predictions. Bayesian deep learning attempts to address this by placing priors on the model parameters, which are then combined with a likelihood to perform posterior inference. Unfortunately, for deep models, the true posterior is intractable, forcing the user to resort to approximations. In this thesis, we explore the use of variational inference (VI) as an approximation, as it is unique in simultaneously approximating the posterior and providing a lower bound to the marginal likelihood. If tight enough, this lower bound can be used to optimize hyperparameters and to facilitate model selection. However, this capacity has rarel",
    "path": "papers/24/01/2401.12418.json",
    "total_tokens": 904,
    "translated_title": "改进深度贝叶斯模型的变分推断方法",
    "translated_abstract": "在过去的十年中，深度学习在计算机视觉、自然语言处理和强化学习等多个领域取得了重大突破。然而，众所周知，通过最大似然估计训练的深度模型往往过于自信，并且给出的预测不准确。贝叶斯深度学习试图通过给模型参数设置先验来解决这个问题，然后将先验与似然函数结合进行后验推断。然而，对于深度模型来说，真实的后验是无法计算的，因此需要使用近似方法。在本论文中，我们探讨了使用变分推断作为近似的方法，因为它既可以近似后验分布又可以提供边缘似然的下界。如果下界足够紧致，这个下界可以用来优化超参数和进行模型选择。然而，这种能力很少受到重视。",
    "tldr": "本论文探讨了改进深度贝叶斯模型的变分推断方法，旨在解决深度模型训练过程中的过度自信和不准确预测问题。通过使用变分推断提供的后验近似和边缘似然下界，可以优化超参数并实现模型选择。"
}