{
    "title": "Optimization Over Trained Neural Networks: Taking a Relaxing Walk. (arXiv:2401.03451v2 [math.OC] UPDATED)",
    "abstract": "Besides training, mathematical optimization is also used in deep learning to model and solve formulations over trained neural networks for purposes such as verification, compression, and optimization with learned constraints. However, solving these formulations soon becomes difficult as the network size grows due to the weak linear relaxation and dense constraint matrix. We have seen improvements in recent years with cutting plane algorithms, reformulations, and an heuristic based on Mixed-Integer Linear Programming (MILP). In this work, we propose a more scalable heuristic based on exploring global and local linear relaxations of the neural network model. Our heuristic is competitive with a state-of-the-art MILP solver and the prior heuristic while producing better solutions with increases in input, depth, and number of neurons.",
    "link": "http://arxiv.org/abs/2401.03451",
    "context": "Title: Optimization Over Trained Neural Networks: Taking a Relaxing Walk. (arXiv:2401.03451v2 [math.OC] UPDATED)\nAbstract: Besides training, mathematical optimization is also used in deep learning to model and solve formulations over trained neural networks for purposes such as verification, compression, and optimization with learned constraints. However, solving these formulations soon becomes difficult as the network size grows due to the weak linear relaxation and dense constraint matrix. We have seen improvements in recent years with cutting plane algorithms, reformulations, and an heuristic based on Mixed-Integer Linear Programming (MILP). In this work, we propose a more scalable heuristic based on exploring global and local linear relaxations of the neural network model. Our heuristic is competitive with a state-of-the-art MILP solver and the prior heuristic while producing better solutions with increases in input, depth, and number of neurons.",
    "path": "papers/24/01/2401.03451.json",
    "total_tokens": 790,
    "translated_title": "在训练过的神经网络上优化：进行一次轻松的漫步",
    "translated_abstract": "除了训练之外，数学优化也被用于深度学习中，用于建模和解决训练过的神经网络的问题，如验证、压缩和带学习约束的优化。然而，随着网络规模的增长，由于弱线性松弛和密集约束矩阵，求解这些问题很快变得困难。近年来，我们通过割平面算法、重述和基于混合整数线性规划（MILP）的启发式方法取得了改进。在这项工作中，我们提出了一种基于探索全局和局部线性松弛的神经网络模型的更可扩展的启发式方法。我们的启发式方法在输入、深度和神经元数量增加时，与最先进的MILP求解器和先前的启发式方法相竞争，同时产生更好的解决方案。",
    "tldr": "本文提出了一种更可扩展的启发式方法来优化训练过的神经网络，通过探索全局和局部线性松弛来产生更好的解决方案。"
}