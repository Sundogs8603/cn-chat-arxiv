{
    "title": "Semantics of Multiword Expressions in Transformer-Based Models: A Survey. (arXiv:2401.15393v1 [cs.CL])",
    "abstract": "Multiword expressions (MWEs) are composed of multiple words and exhibit variable degrees of compositionality. As such, their meanings are notoriously difficult to model, and it is unclear to what extent this issue affects transformer architectures. Addressing this gap, we provide the first in-depth survey of MWE processing with transformer models. We overall find that they capture MWE semantics inconsistently, as shown by reliance on surface patterns and memorized information. MWE meaning is also strongly localized, predominantly in early layers of the architecture. Representations benefit from specific linguistic properties, such as lower semantic idiosyncrasy and ambiguity of target expressions. Our findings overall question the ability of transformer models to robustly capture fine-grained semantics. Furthermore, we highlight the need for more directly comparable evaluation setups.",
    "link": "http://arxiv.org/abs/2401.15393",
    "context": "Title: Semantics of Multiword Expressions in Transformer-Based Models: A Survey. (arXiv:2401.15393v1 [cs.CL])\nAbstract: Multiword expressions (MWEs) are composed of multiple words and exhibit variable degrees of compositionality. As such, their meanings are notoriously difficult to model, and it is unclear to what extent this issue affects transformer architectures. Addressing this gap, we provide the first in-depth survey of MWE processing with transformer models. We overall find that they capture MWE semantics inconsistently, as shown by reliance on surface patterns and memorized information. MWE meaning is also strongly localized, predominantly in early layers of the architecture. Representations benefit from specific linguistic properties, such as lower semantic idiosyncrasy and ambiguity of target expressions. Our findings overall question the ability of transformer models to robustly capture fine-grained semantics. Furthermore, we highlight the need for more directly comparable evaluation setups.",
    "path": "papers/24/01/2401.15393.json",
    "total_tokens": 862,
    "translated_title": "基于Transformer模型的多词表达式语义：一项调查",
    "translated_abstract": "多词表达式（MWEs）由多个词组成，具有可变的组合程度。因此，它们的含义难以建模，而且不清楚这个问题在多头注意力模型中的影响程度。针对这一空白，我们首次对使用Transformer模型处理MWE进行了深入调查。我们发现它们在捕捉MWE语义方面存在不一致性，以表面模式和记忆信息为依赖。MWE的含义也在体系结构的早期层中局部化。表示收益于特定的语言属性，例如目标表达式的较低语义异质性和歧义性。我们的调查结果总体上质疑Transformer模型对细粒度语义的可靠捕捉能力。此外，我们强调需要更直接可比的评估设置。",
    "tldr": "该研究调查了使用Transformer模型处理多词表达式的语义问题。结果显示，这些模型在捕捉MWE语义方面存在一致性问题，并且MWE的含义主要集中在体系结构的早期层中。这个发现对于Transformer模型是否能够可靠捕捉细粒度语义提出了质疑。",
    "en_tdlr": "This survey investigates the semantics of Multiword Expressions (MWEs) in Transformer models, revealing inconsistencies in capturing MWE semantics and the localization of MWE meaning in early layers of the architecture. These findings cast doubt on the ability of Transformer models to robustly capture fine-grained semantics."
}