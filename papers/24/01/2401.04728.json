{
    "title": "Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation. (arXiv:2401.04728v1 [cs.CV])",
    "abstract": "Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multiview-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; exte",
    "link": "http://arxiv.org/abs/2401.04728",
    "context": "Title: Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation. (arXiv:2401.04728v1 [cs.CV])\nAbstract: Recent advances in generative diffusion models have enabled the previously unfeasible capability of generating 3D assets from a single input image or a text prompt. In this work, we aim to enhance the quality and functionality of these models for the task of creating controllable, photorealistic human avatars. We achieve this by integrating a 3D morphable model into the state-of-the-art multiview-consistent diffusion approach. We demonstrate that accurate conditioning of a generative pipeline on the articulated 3D model enhances the baseline model performance on the task of novel view synthesis from a single image. More importantly, this integration facilitates a seamless and accurate incorporation of facial expression and body pose control into the generation process. To the best of our knowledge, our proposed framework is the first diffusion model to enable the creation of fully 3D-consistent, animatable, and photorealistic human avatars from a single image of an unseen subject; exte",
    "path": "papers/24/01/2401.04728.json",
    "total_tokens": 908,
    "translated_title": "可塑性扩散：单图像化身创建的三维一致扩散",
    "translated_abstract": "最近的生成扩散模型的进展使得从单个输入图像或文本提示生成三维资产成为可能。在这项工作中，我们旨在增强这些模型在创建可控、逼真的人物化身任务中的质量和功能。我们通过将三维可塑模型整合到最先进的多视点一致扩散方法中来实现这一目标。我们证明了精确地将生成流程与关节三维模型的条件相结合，提高了基准模型在单个图像的新视图合成任务上的性能。更重要的是，这种整合使面部表情和身体姿势控制在生成过程中无缝准确地融入其中。据我们所知，我们提出的框架是第一个能够从未见过的主题的单个图像创建完全三维一致、可动画和逼真的人物化身的扩散模型；扩展了当前研究的能力。",
    "tldr": "本文通过在多视点一致扩散方法中整合三维可塑模型，提高了从单个图像生成三维一致、可动画和逼真的人物化身的质量和功能。"
}