{
    "title": "FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs. (arXiv:2401.03868v2 [cs.AR] UPDATED)",
    "abstract": "Transformer-based Large Language Models (LLMs) have made a significant impact on various domains. However, LLMs' efficiency suffers from both heavy computation and memory overheads. Compression techniques like sparsification and quantization are commonly used to mitigate the gap between LLM's computation/memory overheads and hardware capacity. However, existing GPU and transformer-based accelerators cannot efficiently process compressed LLMs, due to the following unresolved challenges: low computational efficiency, underutilized memory bandwidth, and large compilation overheads.  This paper proposes FlightLLM, enabling efficient LLMs inference with a complete mapping flow on FPGAs. In FlightLLM, we highlight an innovative solution that the computation and memory overhead of LLMs can be solved by utilizing FPGA-specific resources (e.g., DSP48 and heterogeneous memory hierarchy). We propose a configurable sparse DSP chain to support different sparsity patterns with high computation effic",
    "link": "http://arxiv.org/abs/2401.03868",
    "context": "Title: FlightLLM: Efficient Large Language Model Inference with a Complete Mapping Flow on FPGAs. (arXiv:2401.03868v2 [cs.AR] UPDATED)\nAbstract: Transformer-based Large Language Models (LLMs) have made a significant impact on various domains. However, LLMs' efficiency suffers from both heavy computation and memory overheads. Compression techniques like sparsification and quantization are commonly used to mitigate the gap between LLM's computation/memory overheads and hardware capacity. However, existing GPU and transformer-based accelerators cannot efficiently process compressed LLMs, due to the following unresolved challenges: low computational efficiency, underutilized memory bandwidth, and large compilation overheads.  This paper proposes FlightLLM, enabling efficient LLMs inference with a complete mapping flow on FPGAs. In FlightLLM, we highlight an innovative solution that the computation and memory overhead of LLMs can be solved by utilizing FPGA-specific resources (e.g., DSP48 and heterogeneous memory hierarchy). We propose a configurable sparse DSP chain to support different sparsity patterns with high computation effic",
    "path": "papers/24/01/2401.03868.json",
    "total_tokens": 939,
    "translated_title": "FlightLLM: 高效的FPGA上大型语言模型推断与完整映射流程",
    "translated_abstract": "基于Transformer的大型语言模型(LLMs)对各个领域产生了重大影响。然而，LLMs的效率受到了大量计算和内存开销的影响。压缩技术如稀疏化和量化常被用于缓解LLMs的计算/内存开销与硬件容量之间的差距。然而，现有的GPU和基于Transformer的加速器无法高效处理压缩的LLMs，因为存在以下未解决的挑战：低计算效率、低利用率的内存带宽和巨大的编译开销。该论文提出了FlightLLM，通过在FPGA上实现完整的映射流程，实现了高效的LLMs推断。在FlightLLM中，我们提出了创新性的解决方案，通过利用FPGA特定的资源（如DSP48和异构内存层次），可以解决LLMs的计算和内存开销问题。我们提出了一种可配置的稀疏DSP链，以高计算效率支持不同稀疏模式。",
    "tldr": "该论文提出了FlightLLM，一种在FPGA上实现完整映射流程的高效大型语言模型推断方法。通过利用FPGA特定资源，解决了LLMs的计算和内存开销问题，并提出了可配置的稀疏DSP链以支持不同稀疏模式。",
    "en_tdlr": "This paper proposes FlightLLM, an efficient large language model inference method with a complete mapping flow on FPGAs. By utilizing FPGA-specific resources, it solves the computation and memory overheads of LLMs and introduces a configurable sparse DSP chain that supports different sparsity patterns."
}