{
    "title": "DsDm: Model-Aware Dataset Selection with Datamodels. (arXiv:2401.12926v1 [cs.LG])",
    "abstract": "When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with \"high quality\" data sources may not increase (and can even hurt) performance compared to randomly selecting data.  To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously",
    "link": "http://arxiv.org/abs/2401.12926",
    "context": "Title: DsDm: Model-Aware Dataset Selection with Datamodels. (arXiv:2401.12926v1 [cs.LG])\nAbstract: When selecting data for training large-scale models, standard practice is to filter for examples that match human notions of data quality. Such filtering yields qualitatively clean datapoints that intuitively should improve model behavior. However, in practice the opposite can often happen: we find that selecting according to similarity with \"high quality\" data sources may not increase (and can even hurt) performance compared to randomly selecting data.  To develop better methods for selecting data, we start by framing dataset selection as an optimization problem that we can directly solve for: given target tasks, a learning algorithm, and candidate data, select the subset that maximizes model performance. This framework thus avoids handpicked notions of data quality, and instead models explicitly how the learning process uses train datapoints to predict on the target tasks. Our resulting method greatly improves language model (LM) performance on both pre-specified tasks and previously",
    "path": "papers/24/01/2401.12926.json",
    "total_tokens": 863,
    "translated_title": "DsDm：具有数据模型的模型感知数据集选择",
    "translated_abstract": "在选择用于训练大规模模型的数据时，标准做法是根据人类对数据质量的认知进行筛选。这种筛选可以得到直观上能提高模型行为的数据点。然而，在实践中通常相反的情况可能发生：我们发现根据与“高质量”数据源的相似性进行选择可能不会增加（甚至可能削弱）与随机选择数据相比的性能。为了开发更好的数据选择方法，我们首先将数据集选择作为一个优化问题来解决：给定目标任务、学习算法和候选数据，选择最大化模型性能的子集。这个框架避免了手动选择数据质量的概念，并明确地建模了学习过程如何使用训练数据点来预测目标任务。我们的方法显著提高了语言模型（LM）在预先指定的任务和以前不包括的任务上的性能。",
    "tldr": "该论文提出了一种模型感知的数据集选择方法，通过将数据集选择视为一个优化问题来解决，并明确地建模了学习过程如何使用训练数据点来预测目标任务。该方法在提高语言模型性能方面表现出色。",
    "en_tdlr": "This paper proposes a model-aware dataset selection method by treating dataset selection as an optimization problem and explicitly modeling how the learning process uses training data points to predict target tasks. The approach significantly improves language model performance."
}