{
    "title": "Document Structure in Long Document Transformers",
    "abstract": "Long documents often exhibit structure with hierarchically organized elements of different functions, such as section headers and paragraphs. Despite the omnipresence of document structure, its role in natural language processing (NLP) remains opaque. Do long-document Transformer models acquire an internal representation of document structure during pre-training? How can structural information be communicated to a model after pre-training, and how does it influence downstream performance? To answer these questions, we develop a novel suite of probing tasks to assess structure-awareness of long-document Transformers, propose general-purpose structure infusion methods, and evaluate the effects of structure infusion on QASPER and Evidence Inference, two challenging long-document NLP tasks. Results on LED and LongT5 suggest that they acquire implicit understanding of document structure during pre-training, which can be further enhanced by structure infusion, leading to improved end-task pe",
    "link": "https://arxiv.org/abs/2401.17658",
    "context": "Title: Document Structure in Long Document Transformers\nAbstract: Long documents often exhibit structure with hierarchically organized elements of different functions, such as section headers and paragraphs. Despite the omnipresence of document structure, its role in natural language processing (NLP) remains opaque. Do long-document Transformer models acquire an internal representation of document structure during pre-training? How can structural information be communicated to a model after pre-training, and how does it influence downstream performance? To answer these questions, we develop a novel suite of probing tasks to assess structure-awareness of long-document Transformers, propose general-purpose structure infusion methods, and evaluate the effects of structure infusion on QASPER and Evidence Inference, two challenging long-document NLP tasks. Results on LED and LongT5 suggest that they acquire implicit understanding of document structure during pre-training, which can be further enhanced by structure infusion, leading to improved end-task pe",
    "path": "papers/24/01/2401.17658.json",
    "total_tokens": 813,
    "translated_title": "长文档转换器中的文档结构",
    "translated_abstract": "长文档通常呈现出具有不同功能的层次化组织元素，例如章节标题和段落。尽管文档结构普遍存在，但其在自然语言处理（NLP）中的作用仍然不清楚。长文档Transformer模型在预训练期间是否会掌握文档结构的内部表示？在预训练后，如何向模型传达结构信息，并且它如何影响下游性能？为了回答这些问题，我们开发了一套新的探测任务来评估长文档Transformer的结构感知能力，提出了通用的结构注入方法，并评估了结构注入对于QASPER和Evidence Inference这两个具有挑战性的长文档NLP任务的影响。LED和LongT5上的结果表明，它们在预训练期间获得了对文档结构的隐性理解，而结构注入可以进一步增强这种理解，从而提高了最终的任务性能。",
    "tldr": "长文档Transformer模型在预训练期间获得了对文档结构的隐性理解，并且结构注入可以进一步增强这种理解，提高下游任务的性能。",
    "en_tdlr": "Long-document Transformer models acquire implicit understanding of document structure during pre-training, and structure infusion further enhances this understanding, improving downstream task performance."
}