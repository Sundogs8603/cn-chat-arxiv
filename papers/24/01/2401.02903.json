{
    "title": "Deep Reinforcement Learning for Local Path Following of an Autonomous Formula SAE Vehicle. (arXiv:2401.02903v1 [cs.RO])",
    "abstract": "With the continued introduction of driverless events to Formula:Society of Automotive Engineers (F:SAE) competitions around the world, teams are investigating all aspects of the autonomous vehicle stack. This paper presents the use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning (IRL) to map locally-observed cone positions to a desired steering angle for race track following. Two state-of-the-art algorithms not previously tested in this context: soft actor critic (SAC) and adversarial inverse reinforcement learning (AIRL), are used to train models in a representative simulation. Three novel reward functions for use by RL algorithms in an autonomous racing context are also discussed. Tests performed in simulation and the real world suggest that both algorithms can successfully train models for local path following. Suggestions for future work are presented to allow these models to scale to a full F:SAE vehicle.",
    "link": "http://arxiv.org/abs/2401.02903",
    "context": "Title: Deep Reinforcement Learning for Local Path Following of an Autonomous Formula SAE Vehicle. (arXiv:2401.02903v1 [cs.RO])\nAbstract: With the continued introduction of driverless events to Formula:Society of Automotive Engineers (F:SAE) competitions around the world, teams are investigating all aspects of the autonomous vehicle stack. This paper presents the use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning (IRL) to map locally-observed cone positions to a desired steering angle for race track following. Two state-of-the-art algorithms not previously tested in this context: soft actor critic (SAC) and adversarial inverse reinforcement learning (AIRL), are used to train models in a representative simulation. Three novel reward functions for use by RL algorithms in an autonomous racing context are also discussed. Tests performed in simulation and the real world suggest that both algorithms can successfully train models for local path following. Suggestions for future work are presented to allow these models to scale to a full F:SAE vehicle.",
    "path": "papers/24/01/2401.02903.json",
    "total_tokens": 1019,
    "translated_title": "使用深度强化学习实现自主Formula SAE车辆的局部路径跟随",
    "translated_abstract": "随着无人驾驶比赛在全球Formula:Society of Automotive Engineers (F:SAE)竞赛中的持续推出，车队正在研究自主驾驶车辆系统的各个方面。本文介绍了使用深度强化学习（Deep Reinforcement Learning，DRL）和逆向增强学习（Inverse Reinforcement Learning，IRL）来将局部观察到的锥形位置映射到期望的转向角度以实现赛道跟随。该论文使用了两种在此背景下尚未测试过的最先进算法：软actor评判（soft actor critic，SAC）和对抗性逆向增强学习（adversarial inverse reinforcement learning，AIRL），并在代表性仿真中进行模型训练。还讨论了三种在自主赛车背景下由强化学习算法使用的新颖奖励函数。在仿真和真实环境中进行的测试表明，这两种算法都可以成功训练出用于局部路径跟随的模型。还提出了未来工作的建议，以使这些模型能够扩展到完整的F:SAE车辆。",
    "tldr": "本文介绍了使用深度强化学习和逆向增强学习来实现自主Formula SAE车辆的局部路径跟随。使用软actor评判和对抗性逆向增强学习算法进行模型训练，并提出了三种新颖奖励函数。仿真和真实环境测试表明，这两种算法都可以成功训练模型进行局部路径跟随。"
}