{
    "title": "Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics. (arXiv:2401.05146v1 [cs.LG])",
    "abstract": "Federated Learning (FL) enables collaborative training of a Machine Learning (ML) model across multiple parties, facilitating the preservation of users' and institutions' privacy by keeping data stored locally. Instead of centralizing raw data, FL exchanges locally refined model parameters to build a global model incrementally. While FL is more compliant with emerging regulations such as the European General Data Protection Regulation (GDPR), ensuring the right to be forgotten in this context - allowing FL participants to remove their data contributions from the learned model - remains unclear. In addition, it is recognized that malicious clients may inject backdoors into the global model through updates, e.g. to generate mispredictions on specially crafted data examples. Consequently, there is the need for mechanisms that can guarantee individuals the possibility to remove their data and erase malicious contributions even after aggregation, without compromising the already acquired \"g",
    "link": "http://arxiv.org/abs/2401.05146",
    "context": "Title: Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics. (arXiv:2401.05146v1 [cs.LG])\nAbstract: Federated Learning (FL) enables collaborative training of a Machine Learning (ML) model across multiple parties, facilitating the preservation of users' and institutions' privacy by keeping data stored locally. Instead of centralizing raw data, FL exchanges locally refined model parameters to build a global model incrementally. While FL is more compliant with emerging regulations such as the European General Data Protection Regulation (GDPR), ensuring the right to be forgotten in this context - allowing FL participants to remove their data contributions from the learned model - remains unclear. In addition, it is recognized that malicious clients may inject backdoors into the global model through updates, e.g. to generate mispredictions on specially crafted data examples. Consequently, there is the need for mechanisms that can guarantee individuals the possibility to remove their data and erase malicious contributions even after aggregation, without compromising the already acquired \"g",
    "path": "papers/24/01/2401.05146.json",
    "total_tokens": 868,
    "translated_title": "联邦遗忘：方法、设计准则和评估指标的综述",
    "translated_abstract": "联邦学习使得多个参与方能够协同训练一个机器学习模型，通过保留数据在本地存储，从而维护了用户和机构的隐私。与集中化原始数据不同，联邦学习通过交换本地优化的模型参数来逐步构建全局模型。尽管联邦学习更加符合新兴规定，如欧洲通用数据保护条例（GDPR），但在此背景下确保遗忘权——允许联邦学习参与方从学习的模型中删除他们的数据贡献仍然不明确。此外，人们认识到恶意客户端可能通过更新将后门注入全局模型，例如对特制数据示例进行错误预测。因此，需要机制来确保个人有可能在聚合后移除他们的数据并清除恶意贡献，而不损害已获得的\"全",
    "tldr": "这篇综述论文介绍了联邦遗忘的概念和挑战，以及解决这些问题的方法和设计准则，旨在为联邦学习中保护用户隐私和防止恶意攻击提供解决方案。"
}