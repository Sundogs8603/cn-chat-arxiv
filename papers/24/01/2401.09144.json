{
    "title": "Monitoring Machine Learning Forecasts for Platform Data Streams. (arXiv:2401.09144v1 [stat.AP])",
    "abstract": "Data stream forecasts are essential inputs for decision making at digital platforms. Machine learning algorithms are appealing candidates to produce such forecasts. Yet, digital platforms require a large-scale forecast framework that can flexibly respond to sudden performance drops. Re-training ML algorithms at the same speed as new data batches enter is usually computationally too costly. On the other hand, infrequent re-training requires specifying the re-training frequency and typically comes with a severe cost of forecast deterioration. To ensure accurate and stable forecasts, we propose a simple data-driven monitoring procedure to answer the question when the ML algorithm should be re-trained. Instead of investigating instability of the data streams, we test if the incoming streaming forecast loss batch differs from a well-defined reference batch. Using a novel dataset constituting 15-min frequency data streams from an on-demand logistics platform operating in London, we apply the",
    "link": "http://arxiv.org/abs/2401.09144",
    "context": "Title: Monitoring Machine Learning Forecasts for Platform Data Streams. (arXiv:2401.09144v1 [stat.AP])\nAbstract: Data stream forecasts are essential inputs for decision making at digital platforms. Machine learning algorithms are appealing candidates to produce such forecasts. Yet, digital platforms require a large-scale forecast framework that can flexibly respond to sudden performance drops. Re-training ML algorithms at the same speed as new data batches enter is usually computationally too costly. On the other hand, infrequent re-training requires specifying the re-training frequency and typically comes with a severe cost of forecast deterioration. To ensure accurate and stable forecasts, we propose a simple data-driven monitoring procedure to answer the question when the ML algorithm should be re-trained. Instead of investigating instability of the data streams, we test if the incoming streaming forecast loss batch differs from a well-defined reference batch. Using a novel dataset constituting 15-min frequency data streams from an on-demand logistics platform operating in London, we apply the",
    "path": "papers/24/01/2401.09144.json",
    "total_tokens": 868,
    "translated_title": "监控机器学习对平台数据流的预测",
    "translated_abstract": "数据流预测对数字平台的决策至关重要。机器学习算法是生成这种预测的有吸引力的候选方案。然而，数字平台需要一个大规模的预测框架，可以灵活地应对突然的性能下降。通常情况下，以与新数据批次输入的速度相同的速度重新训练机器学习算法的计算成本太高。另一方面，不频繁的重新训练需要指定重新训练的频率，并且通常会导致预测效果的严重下降。为了确保准确稳定的预测，我们提出了一种简单的数据驱动的监控流程，来回答机器学习算法何时应该重新训练的问题。我们不是研究数据流的不稳定性，而是测试传入的流式预测损失批次是否与一个明确定义的参考批次不同。 使用一个由15分钟频率的数据流组成的新数据集，该数据集来自于在伦敦运营的按需物流平台，我们应用了这个监控流程，",
    "tldr": "这篇论文提出了一种简单的数据驱动的监控流程，用于确定机器学习算法何时需要重新训练，以确保准确稳定的预测结果。"
}