{
    "title": "Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data. (arXiv:2401.12295v1 [cs.CL])",
    "abstract": "The field of machine learning has recently made significant progress in reducing the requirements for labelled training data when building new models. These `cheaper' learning techniques hold significant potential for the social sciences, where development of large labelled training datasets is often a significant practical impediment to the use of machine learning for analytical tasks. In this article we review three `cheap' techniques that have developed in recent years: weak supervision, transfer learning and prompt engineering. For the latter, we also review the particular case of zero-shot prompting of large language models. For each technique we provide a guide of how it works and demonstrate its application across six different realistic social science applications (two different tasks paired with three different dataset makeups). We show good performance for all techniques, and in particular we demonstrate how prompting of large language models can achieve high accuracy at very",
    "link": "http://arxiv.org/abs/2401.12295",
    "context": "Title: Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data. (arXiv:2401.12295v1 [cs.CL])\nAbstract: The field of machine learning has recently made significant progress in reducing the requirements for labelled training data when building new models. These `cheaper' learning techniques hold significant potential for the social sciences, where development of large labelled training datasets is often a significant practical impediment to the use of machine learning for analytical tasks. In this article we review three `cheap' techniques that have developed in recent years: weak supervision, transfer learning and prompt engineering. For the latter, we also review the particular case of zero-shot prompting of large language models. For each technique we provide a guide of how it works and demonstrate its application across six different realistic social science applications (two different tasks paired with three different dataset makeups). We show good performance for all techniques, and in particular we demonstrate how prompting of large language models can achieve high accuracy at very",
    "path": "papers/24/01/2401.12295.json",
    "total_tokens": 878,
    "translated_title": "廉价学习：最大化社会数据科学中语言模型的性能，使用最少的数据。",
    "translated_abstract": "机器学习领域在构建新模型时，最近取得了降低标注训练数据要求的重要进展。这些“廉价”学习技术在社会科学领域具有巨大潜力，因为开发大型标注训练数据集通常是机器学习用于分析任务的实际障碍。在本文中，我们回顾了最近发展的三种“廉价”技术：弱监督、迁移学习和提示工程。对于后者，我们还回顾了大规模语言模型的零样本提示的特殊情况。针对每种技术，我们提供了工作原理的指南，并展示了它们在六个不同的实际社会科学应用程序中的应用情况（两个不同任务与三种不同数据集的组合）。我们展示了所有技术的良好性能，特别是我们演示了如何通过大规模语言模型的提示可以实现很高的准确性。",
    "tldr": "本文回顾了“廉价”学习技术在社会科学中的应用，包括弱监督、迁移学习和提示工程。特别地，通过提示大规模语言模型，可以实现高准确性的性能。",
    "en_tdlr": "This article reviews the application of \"cheap\" learning techniques in social sciences, including weak supervision, transfer learning, and prompt engineering. Specifically, high performance with high accuracy can be achieved through prompting of large language models."
}