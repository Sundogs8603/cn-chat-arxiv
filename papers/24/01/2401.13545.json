{
    "title": "Fine-grained Contract NER using instruction based model. (arXiv:2401.13545v1 [cs.IR])",
    "abstract": "Lately, instruction-based techniques have made significant strides in improving performance in few-shot learning scenarios. They achieve this by bridging the gap between pre-trained language models and fine-tuning for specific downstream tasks. Despite these advancements, the performance of Large Language Models (LLMs) in information extraction tasks like Named Entity Recognition (NER), using prompts or instructions, still falls short of supervised baselines. The reason for this performance gap can be attributed to the fundamental disparity between NER and LLMs. NER is inherently a sequence labeling task, where the model must assign entity-type labels to individual tokens within a sentence. In contrast, LLMs are designed as a text generation task. This distinction between semantic labeling and text generation leads to subpar performance. In this paper, we transform the NER task into a text-generation task that can be readily adapted by LLMs. This involves enhancing source sentences wit",
    "link": "http://arxiv.org/abs/2401.13545",
    "context": "Title: Fine-grained Contract NER using instruction based model. (arXiv:2401.13545v1 [cs.IR])\nAbstract: Lately, instruction-based techniques have made significant strides in improving performance in few-shot learning scenarios. They achieve this by bridging the gap between pre-trained language models and fine-tuning for specific downstream tasks. Despite these advancements, the performance of Large Language Models (LLMs) in information extraction tasks like Named Entity Recognition (NER), using prompts or instructions, still falls short of supervised baselines. The reason for this performance gap can be attributed to the fundamental disparity between NER and LLMs. NER is inherently a sequence labeling task, where the model must assign entity-type labels to individual tokens within a sentence. In contrast, LLMs are designed as a text generation task. This distinction between semantic labeling and text generation leads to subpar performance. In this paper, we transform the NER task into a text-generation task that can be readily adapted by LLMs. This involves enhancing source sentences wit",
    "path": "papers/24/01/2401.13545.json",
    "total_tokens": 890,
    "translated_title": "使用基于指令的模型进行细粒度的合同命名实体识别",
    "translated_abstract": "最近，基于指令的技术在改善少样本学习场景下的性能方面取得了显著进展。它们通过弥合预训练语言模型和针对特定下游任务的微调之间的差距来实现这一点。尽管取得了这些进展，但大规模语言模型（LLMs）在命名实体识别（NER）等信息提取任务中，使用提示或指令仍然不及受监督的基线。造成这种性能差距的原因可以归因于NER和LLMs之间的基本差异。NER本质上是一个序列标注任务，模型必须为句子中的各个标记分配实体类型标签。相比之下，LLMs被设计为一个文本生成任务。语义标注和文本生成之间的区别导致性能不佳。在本文中，我们将NER任务转化为一个可以被LLMs轻松适应的文本生成任务。这涉及增强源句子的特征。",
    "tldr": "该论文介绍了一种使用基于指令的模型进行细粒度的合同命名实体识别的方法。通过将NER任务转化为文本生成任务，该方法能够提高大规模语言模型在NER任务中的性能。",
    "en_tdlr": "This paper presents a method for fine-grained contract Named Entity Recognition (NER) using an instruction-based model. By transforming the NER task into a text generation task, this method improves the performance of Large Language Models (LLMs) in NER tasks."
}