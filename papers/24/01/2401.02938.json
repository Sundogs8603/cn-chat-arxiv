{
    "title": "Fast and Optimal Weight Update for Pruned Large Language Models. (arXiv:2401.02938v1 [cs.CL])",
    "abstract": "Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations.  In our paper, we propose a fast and optimal weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). Coupled with a simple iterative pruning mask selection, our algorithm achieves state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.",
    "link": "http://arxiv.org/abs/2401.02938",
    "context": "Title: Fast and Optimal Weight Update for Pruned Large Language Models. (arXiv:2401.02938v1 [cs.CL])\nAbstract: Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations.  In our paper, we propose a fast and optimal weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). Coupled with a simple iterative pruning mask selection, our algorithm achieves state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning.",
    "path": "papers/24/01/2401.02938.json",
    "total_tokens": 899,
    "translated_title": "快速且最优的修剪大型语言模型的权重更新。",
    "translated_abstract": "修剪大型语言模型(LLMs)是一个具有挑战性的任务，因为它们的规模庞大。主要困难在于修剪后的模型微调，这是为了恢复因删除权重而导致的性能损失。最近的方法要么完全忽略了微调，专注于高效的修剪标准，要么尝试逐层权重更新，保持每个层的行为。然而，即使是逐层权重更新对LLMs来说也可能代价高昂，之前的工作不得不采用各种近似方法。在我们的论文中，我们提出了一种基于交替方向乘积算法(ADMM)的快速且最优的修剪层权重更新算法。结合简单的迭代修剪掩码选择，我们的算法在广泛的LLMs范围内实现了最先进的修剪性能。代码可以在https://github.com/fmfi-compbio/admm-pruning获得。",
    "tldr": "本研究提出了一种基于交替方向乘积算法(ADMM)的快速且最优的修剪层权重更新算法，结合简单的迭代修剪掩码选择，在广泛的大型语言模型范围内实现了最先进的修剪性能。",
    "en_tdlr": "This paper presents a fast and optimal weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM), coupled with a simple iterative pruning mask selection, achieving state-of-the-art pruning performance across a wide range of large language models (LLMs)."
}