{
    "title": "A Comprehensive Survey of Compression Algorithms for Language Models. (arXiv:2401.15347v1 [cs.CL])",
    "abstract": "How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of com",
    "link": "http://arxiv.org/abs/2401.15347",
    "context": "Title: A Comprehensive Survey of Compression Algorithms for Language Models. (arXiv:2401.15347v1 [cs.CL])\nAbstract: How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of com",
    "path": "papers/24/01/2401.15347.json",
    "total_tokens": 866,
    "translated_title": "关于语言模型压缩算法的综合调查",
    "translated_abstract": "如何在不损失准确性的情况下压缩语言模型？语言模型的压缩算法数量正在快速增长，以从最近语言模型的显著进展中受益，而不会产生庞大语言模型的副作用，比如增加的碳排放和昂贵的维护费用。虽然许多压缩算法在压缩语言模型方面表现出色，但由于过多的算法，部分的难题在于捕捉新兴趋势并识别其基本概念。在本文中，我们对包括修剪、量化、知识蒸馏、低秩逼近、参数共享和高效架构设计在内的多种压缩算法进行了调查和总结。我们不仅总结了各种压缩算法的整体趋势，还选择了代表性算法，并对其进行了深入分析。我们讨论了每个类别的压缩算法的价值。",
    "tldr": "这篇论文是关于语言模型压缩算法的综合调查，讨论了如何在不损失准确性的情况下压缩语言模型。通过对多种压缩算法的调查和分析，总结了各个算法的整体趋势和价值。",
    "en_tdlr": "This paper is a comprehensive survey of compression algorithms for language models, discussing how to compress language models without sacrificing accuracy. Through investigating and analyzing various compression algorithms, the paper summarizes the overall trends and values of each algorithm."
}