{
    "title": "Leveraging Biases in Large Language Models: \"bias-kNN'' for Effective Few-Shot Learning. (arXiv:2401.09783v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have shown significant promise in various applications, including zero-shot and few-shot learning. However, their performance can be hampered by inherent biases. Instead of traditionally sought methods that aim to minimize or correct these biases, this study introduces a novel methodology named ``bias-kNN''. This approach capitalizes on the biased outputs, harnessing them as primary features for kNN and supplementing with gold labels. Our comprehensive evaluations, spanning diverse domain text classification datasets and different GPT-2 model sizes, indicate the adaptability and efficacy of the ``bias-kNN'' method. Remarkably, this approach not only outperforms conventional in-context learning in few-shot scenarios but also demonstrates robustness across a spectrum of samples, templates and verbalizers. This study, therefore, presents a unique perspective on harnessing biases, transforming them into assets for enhanced model performance.",
    "link": "http://arxiv.org/abs/2401.09783",
    "context": "Title: Leveraging Biases in Large Language Models: \"bias-kNN'' for Effective Few-Shot Learning. (arXiv:2401.09783v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have shown significant promise in various applications, including zero-shot and few-shot learning. However, their performance can be hampered by inherent biases. Instead of traditionally sought methods that aim to minimize or correct these biases, this study introduces a novel methodology named ``bias-kNN''. This approach capitalizes on the biased outputs, harnessing them as primary features for kNN and supplementing with gold labels. Our comprehensive evaluations, spanning diverse domain text classification datasets and different GPT-2 model sizes, indicate the adaptability and efficacy of the ``bias-kNN'' method. Remarkably, this approach not only outperforms conventional in-context learning in few-shot scenarios but also demonstrates robustness across a spectrum of samples, templates and verbalizers. This study, therefore, presents a unique perspective on harnessing biases, transforming them into assets for enhanced model performance.",
    "path": "papers/24/01/2401.09783.json",
    "total_tokens": 945,
    "translated_title": "利用大型语言模型中的偏见：有效的少样本学习中的“bias-kNN”",
    "translated_abstract": "大型语言模型（LLMs）在零样本和少样本学习等各种应用中显示出了显著的潜力。然而，它们的性能可能受到内在偏见的限制。本研究介绍了一种名为“bias-kNN”的新方法，而不是传统上致力于最小化或修正这些偏见的方法。该方法利用有偏见的输出，将其作为kNN的主要特征，并与金标签相结合。我们对多样化领域文本分类数据集和不同GPT-2模型尺寸的全面评估表明了“bias-kNN”方法的适应性和效果。值得注意的是，这种方法不仅在少样本场景中优于传统的上下文学习，而且在各种样本、模板和言语器上表现出鲁棒性。因此，本研究提出了一种将偏见转化为增强模型性能的资产的独特观点。",
    "tldr": "本研究介绍了一种名为“bias-kNN”的新方法，它利用大型语言模型中的偏见，在少样本学习中表现出比传统方法更好的效果，并且在不同样本和模型情境下表现出鲁棒性。这一方法提供了一种独特的视角，将偏见转化为提升模型性能的资产。",
    "en_tdlr": "This study introduces a novel methodology named \"bias-kNN\" that leverages biases in large language models, outperforming traditional methods in few-shot learning and demonstrating robustness across different samples and model scenarios. It provides a unique perspective on harnessing biases and transforming them into assets for enhanced model performance."
}