{
    "title": "MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning. (arXiv:2401.11380v1 [cs.LG])",
    "abstract": "Model-based offline reinforcement learning methods (RL) have achieved state-of-the-art performance in many decision-making problems thanks to their sample efficiency and generalizability. Despite these advancements, existing model-based offline RL approaches either focus on theoretical studies without developing practical algorithms or rely on a restricted parametric policy space, thus not fully leveraging the advantages of an unrestricted policy space inherent to model-based methods. To address this limitation, we develop MoMA, a model-based mirror ascent algorithm with general function approximations under partial coverage of offline data. MoMA distinguishes itself from existing literature by employing an unrestricted policy class. In each iteration, MoMA conservatively estimates the value function by a minimization procedure within a confidence set of transition models in the policy evaluation step, then updates the policy with general function approximations instead of commonly-use",
    "link": "http://arxiv.org/abs/2401.11380",
    "context": "Title: MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning. (arXiv:2401.11380v1 [cs.LG])\nAbstract: Model-based offline reinforcement learning methods (RL) have achieved state-of-the-art performance in many decision-making problems thanks to their sample efficiency and generalizability. Despite these advancements, existing model-based offline RL approaches either focus on theoretical studies without developing practical algorithms or rely on a restricted parametric policy space, thus not fully leveraging the advantages of an unrestricted policy space inherent to model-based methods. To address this limitation, we develop MoMA, a model-based mirror ascent algorithm with general function approximations under partial coverage of offline data. MoMA distinguishes itself from existing literature by employing an unrestricted policy class. In each iteration, MoMA conservatively estimates the value function by a minimization procedure within a confidence set of transition models in the policy evaluation step, then updates the policy with general function approximations instead of commonly-use",
    "path": "papers/24/01/2401.11380.json",
    "total_tokens": 890,
    "translated_title": "MoMA: 模型为基础的镜像上升算法的离线强化学习",
    "translated_abstract": "模型为基础的离线强化学习方法在许多决策问题中取得了最先进的性能，得益于它们的样本效率和通用性。尽管取得了这些进展，但现有的模型为基础的离线强化学习方法要么侧重于理论研究而没有开发实际算法，要么依赖于受限的参数化策略空间，因此没有充分利用模型为基础方法固有的无限制策略空间的优势。为了解决这个限制，我们开发了MoMA，一个在离线数据部分覆盖下使用一般函数逼近的模型为基础的镜像上升算法。MoMA通过采用无限制的策略类别，区别于现有文献。在每个迭代中，MoMA在策略评估步骤中通过在过渡模型的置信区间内进行最小化过程来保守地估计值函数，然后使用一般函数逼近来更新策略，而不是常用的方法。",
    "tldr": "MoMA提出了一种模型为基础的镜像上升算法，通过使用无限制的策略类别和一般函数逼近来实现离线强化学习，充分利用了模型为基础方法的优势。",
    "en_tdlr": "MoMA proposes a model-based mirror ascent algorithm that achieves offline reinforcement learning by leveraging an unrestricted policy class and general function approximations, taking full advantage of the benefits offered by model-based approaches."
}