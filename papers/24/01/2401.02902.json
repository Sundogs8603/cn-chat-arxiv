{
    "title": "State Derivative Normalization for Continuous-Time Deep Neural Networks. (arXiv:2401.02902v1 [eess.SY])",
    "abstract": "The importance of proper data normalization for deep neural networks is well known. However, in continuous-time state-space model estimation, it has been observed that improper normalization of either the hidden state or hidden state derivative of the model estimate, or even of the time interval can lead to numerical and optimization challenges with deep learning based methods. This results in a reduced model quality. In this contribution, we show that these three normalization tasks are inherently coupled. Due to the existence of this coupling, we propose a solution to all three normalization challenges by introducing a normalization constant at the state derivative level. We show that the appropriate choice of the normalization constant is related to the dynamics of the to-be-identified system and we derive multiple methods of obtaining an effective normalization constant. We compare and discuss all the normalization strategies on a benchmark problem based on experimental data from a",
    "link": "http://arxiv.org/abs/2401.02902",
    "context": "Title: State Derivative Normalization for Continuous-Time Deep Neural Networks. (arXiv:2401.02902v1 [eess.SY])\nAbstract: The importance of proper data normalization for deep neural networks is well known. However, in continuous-time state-space model estimation, it has been observed that improper normalization of either the hidden state or hidden state derivative of the model estimate, or even of the time interval can lead to numerical and optimization challenges with deep learning based methods. This results in a reduced model quality. In this contribution, we show that these three normalization tasks are inherently coupled. Due to the existence of this coupling, we propose a solution to all three normalization challenges by introducing a normalization constant at the state derivative level. We show that the appropriate choice of the normalization constant is related to the dynamics of the to-be-identified system and we derive multiple methods of obtaining an effective normalization constant. We compare and discuss all the normalization strategies on a benchmark problem based on experimental data from a",
    "path": "papers/24/01/2401.02902.json",
    "total_tokens": 918,
    "translated_title": "连续时间深度神经网络的状态导数标准化",
    "translated_abstract": "深度神经网络的适当数据标准化的重要性是众所周知的。然而，在连续时间状态空间模型估计中，观察到模型估计的隐藏状态或隐藏状态导数，甚至时间间隔的不适当标准化可能会导致使用基于深度学习的方法时的数值和优化挑战。这导致模型质量降低。在本文中，我们展示了这三个标准化任务的内在耦合。由于存在这种耦合，我们提出了一种在状态导数水平引入标准化常数的解决方案。我们展示了适当选择标准化常数与待识别系统的动力学相关，并推导了多种获得有效标准化常数的方法。我们在基于实验数据的基准问题上比较和讨论了所有标准化策略。",
    "tldr": "本文研究了在连续时间状态空间模型估计中，深度神经网络的数据标准化问题。通过引入状态导数级别的标准化常数，解决了隐藏状态、隐藏状态导数以及时间间隔的标准化挑战。选择适当的标准化常数与待识别系统的动力学相关，并提出了多种获得有效标准化常数的方法。",
    "en_tdlr": "This paper addresses the issue of data normalization in deep neural networks for continuous-time state-space model estimation. By introducing a normalization constant at the state derivative level, the paper proposes a solution to the challenges of normalizing the hidden state, hidden state derivative, and time interval. The appropriate choice of the normalization constant is shown to be related to the dynamics of the system being identified, and multiple methods for obtaining an effective constant are derived and compared."
}