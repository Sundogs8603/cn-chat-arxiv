{
    "title": "Interpreting Adaptive Gradient Methods by Parameter Scaling for Learning-Rate-Free Optimization. (arXiv:2401.03240v1 [cs.LG])",
    "abstract": "We address the challenge of estimating the learning rate for adaptive gradient methods used in training deep neural networks. While several learning-rate-free approaches have been proposed, they are typically tailored for steepest descent. However, although steepest descent methods offer an intuitive approach to finding minima, many deep learning applications require adaptive gradient methods to achieve faster convergence. In this paper, we interpret adaptive gradient methods as steepest descent applied on parameter-scaled networks, proposing learning-rate-free adaptive gradient methods. Experimental results verify the effectiveness of this approach, demonstrating comparable performance to hand-tuned learning rates across various scenarios. This work extends the applicability of learning-rate-free methods, enhancing training with adaptive gradient methods.",
    "link": "http://arxiv.org/abs/2401.03240",
    "context": "Title: Interpreting Adaptive Gradient Methods by Parameter Scaling for Learning-Rate-Free Optimization. (arXiv:2401.03240v1 [cs.LG])\nAbstract: We address the challenge of estimating the learning rate for adaptive gradient methods used in training deep neural networks. While several learning-rate-free approaches have been proposed, they are typically tailored for steepest descent. However, although steepest descent methods offer an intuitive approach to finding minima, many deep learning applications require adaptive gradient methods to achieve faster convergence. In this paper, we interpret adaptive gradient methods as steepest descent applied on parameter-scaled networks, proposing learning-rate-free adaptive gradient methods. Experimental results verify the effectiveness of this approach, demonstrating comparable performance to hand-tuned learning rates across various scenarios. This work extends the applicability of learning-rate-free methods, enhancing training with adaptive gradient methods.",
    "path": "papers/24/01/2401.03240.json",
    "total_tokens": 827,
    "translated_title": "通过参数缩放解释自适应梯度方法以实现零学习率优化",
    "translated_abstract": "本文解决了在训练深度神经网络中估计自适应梯度方法学习率的挑战。尽管已经提出了几种无学习率方法，但它们通常是针对最陡下降法设计的。然而，尽管最陡下降法对于寻找极小值提供了直观的方法，但许多深度学习应用需要自适应梯度方法以实现更快的收敛。本文将自适应梯度方法解释为应用于参数缩放网络上的最陡下降法，并提出了无学习率的自适应梯度方法。实验结果验证了这种方法的有效性，在各种情况下展示出与手动调整学习率相当的性能。这项工作扩展了无学习率方法的适用性，增强了使用自适应梯度方法进行训练的效果。",
    "tldr": "本文提出了一种无学习率的自适应梯度方法，通过参数缩放解释自适应梯度方法，扩展了无学习率方法的适用性并增强了使用自适应梯度方法进行训练的效果。",
    "en_tdlr": "This paper proposes a learning-rate-free adaptive gradient method that interprets adaptive gradient methods by parameter scaling, extends the applicability of learning-rate-free methods, and enhances training with adaptive gradient methods."
}