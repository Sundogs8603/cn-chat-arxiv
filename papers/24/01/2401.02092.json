{
    "title": "k-Winners-Take-All Ensemble Neural Network. (arXiv:2401.02092v1 [cs.NE])",
    "abstract": "Ensembling is one approach that improves the performance of a neural network by combining a number of independent neural networks, usually by either averaging or summing up their individual outputs. We modify this ensembling approach by training the sub-networks concurrently instead of independently. This concurrent training of sub-networks leads them to cooperate with each other, and we refer to them as \"cooperative ensemble\". Meanwhile, the mixture-of-experts approach improves a neural network performance by dividing up a given dataset to its sub-networks. It then uses a gating network that assigns a specialization to each of its sub-networks called \"experts\". We improve on these aforementioned ways for combining a group of neural networks by using a k-Winners-Take-All (kWTA) activation function, that acts as the combination method for the outputs of each sub-network in the ensemble. We refer to this proposed model as \"kWTA ensemble neural networks\" (kWTA-ENN). With the kWTA activati",
    "link": "http://arxiv.org/abs/2401.02092",
    "context": "Title: k-Winners-Take-All Ensemble Neural Network. (arXiv:2401.02092v1 [cs.NE])\nAbstract: Ensembling is one approach that improves the performance of a neural network by combining a number of independent neural networks, usually by either averaging or summing up their individual outputs. We modify this ensembling approach by training the sub-networks concurrently instead of independently. This concurrent training of sub-networks leads them to cooperate with each other, and we refer to them as \"cooperative ensemble\". Meanwhile, the mixture-of-experts approach improves a neural network performance by dividing up a given dataset to its sub-networks. It then uses a gating network that assigns a specialization to each of its sub-networks called \"experts\". We improve on these aforementioned ways for combining a group of neural networks by using a k-Winners-Take-All (kWTA) activation function, that acts as the combination method for the outputs of each sub-network in the ensemble. We refer to this proposed model as \"kWTA ensemble neural networks\" (kWTA-ENN). With the kWTA activati",
    "path": "papers/24/01/2401.02092.json",
    "total_tokens": 873,
    "translated_title": "k-Winners-Take-All集合神经网络",
    "translated_abstract": "集合是通过组合多个独立的神经网络来提高神经网络性能的一种方法，通常通过对它们的单独输出进行平均或求和来实现。我们改进了这种集合方法，通过同时训练子网络而不是独立训练它们。这种并发训练使子网络相互合作，我们称之为“合作集合”。同时，专家组合方法通过将给定数据集划分为子网络来改善神经网络性能。然后使用一个门控网络，给它的子网络分配一个称为“专家”的专业化角色。我们改进了上述结合神经网络的方法，使用了一种称为k-Winners-Take-All（kWTA）的激活函数，它作为集合中每个子网络输出的组合方法。我们将这个提出的模型称为“kWTA集合神经网络”（kWTA-ENN）。",
    "tldr": "本论文提出了k-Winners-Take-All集合神经网络（kWTA-ENN），通过同时训练子网络来改进以往的集合方法，使用kWTA激活函数作为输出的组合方法，从而实现神经网络性能的提升。",
    "en_tdlr": "This paper proposes a k-Winners-Take-All Ensemble Neural Network (kWTA-ENN) that improves previous ensemble methods by concurrently training sub-networks and using kWTA activation function as the combination method for outputs, leading to improved neural network performance."
}