{
    "title": "DiarizationLM: Speaker Diarization Post-Processing with Large Language Models. (arXiv:2401.03506v2 [eess.AS] UPDATED)",
    "abstract": "In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. Various goals can be achieved with the proposed framework, such as improving the readability of the diarized transcript, or reducing the word diarization error rate (WDER). In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset.",
    "link": "http://arxiv.org/abs/2401.03506",
    "context": "Title: DiarizationLM: Speaker Diarization Post-Processing with Large Language Models. (arXiv:2401.03506v2 [eess.AS] UPDATED)\nAbstract: In this paper, we introduce DiarizationLM, a framework to leverage large language models (LLM) to post-process the outputs from a speaker diarization system. Various goals can be achieved with the proposed framework, such as improving the readability of the diarized transcript, or reducing the word diarization error rate (WDER). In this framework, the outputs of the automatic speech recognition (ASR) and speaker diarization systems are represented as a compact textual format, which is included in the prompt to an optionally finetuned LLM. The outputs of the LLM can be used as the refined diarization results with the desired enhancement. As a post-processing step, this framework can be easily applied to any off-the-shelf ASR and speaker diarization systems without retraining existing components. Our experiments show that a finetuned PaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone conversation dataset, and rel. 44.9% on the Callhome English dataset.",
    "path": "papers/24/01/2401.03506.json",
    "total_tokens": 927,
    "translated_title": "DiarizationLM: 基于大语言模型的说话人分离后处理",
    "translated_abstract": "本文介绍了DiarizationLM，一个利用大语言模型（LLM）对说话人分离系统输出进行后处理的框架。这个框架可以实现多种目标，如改善分离对话转录的可读性，或减少词级分离错误率（WDER）。在这个框架中，自动语音识别（ASR）和说话人分离系统的输出被表示为一种紧凑的文本格式，其包含在一个可选择调整的LLM的提示中。LLM的输出可以作为所需改进的精细化分离结果。作为后处理步骤，该框架可以轻松应用于任何现有的ASR和说话人分离系统，无需重新训练现有的组件。我们的实验证明，finetuned的PaLM 2-S模型可以在Fisher电话对话数据集上将WDER降低55.5%，在Callhome英语数据集上降低44.9%。",
    "tldr": "本文介绍了DiarizationLM框架，利用大语言模型对说话人分离系统的输出进行后处理。实验证明，使用finetuned的PaLM 2-S模型可以显著减少分离错误率，对多种目标都有优化效果。",
    "en_tdlr": "This paper introduces DiarizationLM, a framework that utilizes large language models (LLM) to post-process the outputs of a speaker diarization system. Experimental results show that using a finetuned PaLM 2-S model can substantially reduce the word diarization error rate and optimize various goals."
}