{
    "title": "Distilling Event Sequence Knowledge From Large Language Models",
    "abstract": "Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex st",
    "link": "https://arxiv.org/abs/2401.07237",
    "context": "Title: Distilling Event Sequence Knowledge From Large Language Models\nAbstract: Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex st",
    "path": "papers/24/01/2401.07237.json",
    "total_tokens": 874,
    "translated_title": "从大型语言模型中提取事件序列知识",
    "translated_abstract": "事件序列模型在事件的分析和预测中被发现是非常有效的。建立这样的模型需要丰富的高质量事件序列数据。然而，在某些应用中，干净的结构化事件序列不可用，自动化序列提取导致的数据太嘈杂和不完整。在这项工作中，我们探索了使用大型语言模型（LLMs）生成可以有效用于概率事件模型构建的事件序列的方法。这可以看作是从LLMs中提取事件序列知识的一种机制。我们的方法依赖于一个具有部分因果关系的事件概念的知识图（KG）来指导生成语言模型进行因果事件序列的生成。我们展示了我们的方法可以生成高质量的事件序列，填补了输入KG中的知识空白。此外，我们还探索了如何利用生成的序列来发现更有用和更复杂的内容。",
    "tldr": "本论文研究了通过大型语言模型从中提取事件序列知识的方法。采用了基于知识图的指导生成语言模型的方式，实现对具有部分因果关系的事件概念的事件序列的生成。实验证明了该方法可以生成高质量的事件序列，并且在填补知识空白方面具有潜在的价值。"
}