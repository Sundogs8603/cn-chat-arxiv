{
    "title": "The Distributional Reward Critic Architecture for Perturbed-Reward Reinforcement Learning. (arXiv:2401.05710v1 [cs.LG])",
    "abstract": "We study reinforcement learning in the presence of an unknown reward perturbation. Existing methodologies for this problem make strong assumptions including reward smoothness, known perturbations, and/or perturbations that do not modify the optimal policy. We study the case of unknown arbitrary perturbations that discretize and shuffle reward space, but have the property that the true reward belongs to the most frequently observed class after perturbation. This class of perturbations generalizes existing classes (and, in the limit, all continuous bounded perturbations) and defeats existing methods. We introduce an adaptive distributional reward critic and show theoretically that it can recover the true rewards under technical conditions. Under the targeted perturbation in discrete and continuous control tasks, we win/tie the highest return in 40/57 settings (compared to 16/57 for the best baseline). Even under the untargeted perturbation, we still win an edge over the baseline designed",
    "link": "http://arxiv.org/abs/2401.05710",
    "context": "Title: The Distributional Reward Critic Architecture for Perturbed-Reward Reinforcement Learning. (arXiv:2401.05710v1 [cs.LG])\nAbstract: We study reinforcement learning in the presence of an unknown reward perturbation. Existing methodologies for this problem make strong assumptions including reward smoothness, known perturbations, and/or perturbations that do not modify the optimal policy. We study the case of unknown arbitrary perturbations that discretize and shuffle reward space, but have the property that the true reward belongs to the most frequently observed class after perturbation. This class of perturbations generalizes existing classes (and, in the limit, all continuous bounded perturbations) and defeats existing methods. We introduce an adaptive distributional reward critic and show theoretically that it can recover the true rewards under technical conditions. Under the targeted perturbation in discrete and continuous control tasks, we win/tie the highest return in 40/57 settings (compared to 16/57 for the best baseline). Even under the untargeted perturbation, we still win an edge over the baseline designed",
    "path": "papers/24/01/2401.05710.json",
    "total_tokens": 929,
    "translated_title": "对扰动奖励强化学习的分布式奖励评论家架构的研究",
    "translated_abstract": "我们研究了在未知奖励扰动的情况下的强化学习。现有的方法对这个问题做出了强大的假设，包括奖励平滑性、已知扰动和/或不会改变最优策略的扰动。我们研究了未知任意扰动的情况，这些扰动对奖励空间进行了离散化和洗牌，但在扰动后，真实奖励属于最频繁观察到的类别。这类扰动泛化了现有的类别（并在极限情况下泛化了所有连续有界扰动），并战胜了现有的方法。我们引入了一种自适应的分布式奖励评论家，并在理论上证明了在技术条件下它可以恢复真实奖励。在离散和连续控制任务中的目标扰动下，我们在40/57个环境中赢利/平局（相对于最佳基线的16/57）。即使在非目标扰动下，我们仍然胜过设计为带有目标扰动的基线。",
    "tldr": "这项研究展示了一种适应性分布式奖励评论家架构，能够在未知扰动的情况下恢复真实奖励，并在多个控制任务中取得较高的回报。",
    "en_tdlr": "This study presents an adaptive distributional reward critic architecture that can recover the true rewards under unknown perturbations and achieves higher returns in multiple control tasks."
}