{
    "title": "Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View. (arXiv:2401.11237v1 [cs.LG])",
    "abstract": "Some reinforcement learning (RL) algorithms can stitch pieces of experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which RL methods based on dynamic-programming differ from RL methods based on supervised-learning (SL). Yet, certain RL methods based on off-the-shelf SL algorithms achieve excellent results without an explicit mechanism for stitching; it remains unclear whether those methods forgo this important stitching property. This paper studies this question for the problems of achieving a target goal state and achieving a target return value. Our main result is to show that the stitching property corresponds to a form of combinatorial generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data. Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching ",
    "link": "http://arxiv.org/abs/2401.11237",
    "context": "Title: Closing the Gap between TD Learning and Supervised Learning -- A Generalisation Point of View. (arXiv:2401.11237v1 [cs.LG])\nAbstract: Some reinforcement learning (RL) algorithms can stitch pieces of experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which RL methods based on dynamic-programming differ from RL methods based on supervised-learning (SL). Yet, certain RL methods based on off-the-shelf SL algorithms achieve excellent results without an explicit mechanism for stitching; it remains unclear whether those methods forgo this important stitching property. This paper studies this question for the problems of achieving a target goal state and achieving a target return value. Our main result is to show that the stitching property corresponds to a form of combinatorial generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data. Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching ",
    "path": "papers/24/01/2401.11237.json",
    "total_tokens": 901,
    "translated_title": "缩小TD学习和监督学习之间的差距--从一般化的角度来看",
    "translated_abstract": "一些强化学习算法可以将经验片段组合起来解决训练过程中从未见过的任务。这种经常被追求的特性是基于动态规划的强化学习方法与基于监督学习的强化学习方法之间的几种区别之一。然而，某些基于现成监督学习算法的强化学习方法在没有明确的组合机制的情况下，也能取得出色的结果；目前尚不清楚这些方法是否放弃了这种重要的组合特性。本文研究了即将达到目标状态和达到目标回报值的问题，我们的主要结果是展示了组合特性对应了一种组合泛化：在(state, goal)对的分布上进行训练后，希望在训练数据中没有同时出现的(state, goal)对上进行评估。我们的分析表明，这种组合泛化与i.i.d.泛化是不同的。这种连接将组合特性与泛化联系在一起。",
    "tldr": "这篇论文研究了强化学习方法在训练过程中将不同经验片段组合起来解决未见过的任务的属性，并通过分析发现这种组合属性与组合泛化有关。"
}