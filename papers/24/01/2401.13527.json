{
    "title": "SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation. (arXiv:2401.13527v1 [cs.CL])",
    "abstract": "Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate th",
    "link": "http://arxiv.org/abs/2401.13527",
    "context": "Title: SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation. (arXiv:2401.13527v1 [cs.CL])\nAbstract: Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate th",
    "path": "papers/24/01/2401.13527.json",
    "total_tokens": 800,
    "translated_title": "SpeechGPT-Gen: 缩放信息链语音生成",
    "translated_abstract": "凭借有效的语音建模，当前的语音大型语言模型（SLLMs）在上下文语音生成和对未见过的说话人的高效泛化方面展示出了出色的能力。然而，现有的信息建模过程受到一定冗余的限制，导致语音生成效率低下。我们提出了信息链生成（CoIG）的方法，用于解耦大规模语音生成中的语义和感知信息。在此基础上，我们开发了SpeechGPT-Gen，一个8亿参数的SLLM，能够高效地进行语义和感知信息建模。它包括一个基于LLM的自回归模型用于语义信息建模，以及一个使用流匹配进行感知信息建模的非自回归模型。此外，我们引入了将语义信息注入先验分布以增强流匹配效率的新方法。广泛的实验结果表明…",
    "tldr": "SpeechGPT-Gen是一个8亿参数的语音大型语言模型，通过Chain-of-Information Generation方法来解耦语义和感知信息，在语音生成方面提高了效率。",
    "en_tdlr": "SpeechGPT-Gen is an 8-billion-parameter speech large language model that improves efficiency in speech generation by decoupling semantic and perceptual information using the Chain-of-Information Generation method."
}