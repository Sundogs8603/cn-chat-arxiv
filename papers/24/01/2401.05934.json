{
    "title": "Combining Normalizing Flows and Quasi-Monte Carlo. (arXiv:2401.05934v1 [stat.CO])",
    "abstract": "Recent advances in machine learning have led to the development of new methods for enhancing Monte Carlo methods such as Markov chain Monte Carlo (MCMC) and importance sampling (IS). One such method is normalizing flows, which use a neural network to approximate a distribution by evaluating it pointwise. Normalizing flows have been shown to improve the performance of MCMC and IS. On the other side, (randomized) quasi-Monte Carlo methods are used to perform numerical integration. They replace the random sampling of Monte Carlo by a sequence which cover the hypercube more uniformly, resulting in better convergence rates for the error that plain Monte Carlo. In this work, we combine these two methods by using quasi-Monte Carlo to sample the initial distribution that is transported by the flow. We demonstrate through numerical experiments that this combination can lead to an estimator with significantly lower variance than if the flow was sampled with a classic Monte Carlo.",
    "link": "http://arxiv.org/abs/2401.05934",
    "context": "Title: Combining Normalizing Flows and Quasi-Monte Carlo. (arXiv:2401.05934v1 [stat.CO])\nAbstract: Recent advances in machine learning have led to the development of new methods for enhancing Monte Carlo methods such as Markov chain Monte Carlo (MCMC) and importance sampling (IS). One such method is normalizing flows, which use a neural network to approximate a distribution by evaluating it pointwise. Normalizing flows have been shown to improve the performance of MCMC and IS. On the other side, (randomized) quasi-Monte Carlo methods are used to perform numerical integration. They replace the random sampling of Monte Carlo by a sequence which cover the hypercube more uniformly, resulting in better convergence rates for the error that plain Monte Carlo. In this work, we combine these two methods by using quasi-Monte Carlo to sample the initial distribution that is transported by the flow. We demonstrate through numerical experiments that this combination can lead to an estimator with significantly lower variance than if the flow was sampled with a classic Monte Carlo.",
    "path": "papers/24/01/2401.05934.json",
    "total_tokens": 888,
    "translated_title": "结合正则化流和准蒙特卡洛方法",
    "translated_abstract": "机器学习的最新进展促使了改进蒙特卡洛方法（如马尔可夫链蒙特卡洛和重要性采样）的新方法的发展。其中一种方法是正则化流，它使用神经网络通过逐点评估来近似分布。正则化流已被证明可以提高马尔可夫链蒙特卡洛和重要性采样的性能。另一方面，（随机）准蒙特卡洛方法用于进行数值积分，它们用更均匀地覆盖超立方体的序列取代了蒙特卡洛的随机采样，从而使误差的收敛速度更快。在本研究中，我们通过使用准蒙特卡洛方法对流进行初始采样的方式结合了这两种方法。通过数值实验证明，与使用经典蒙特卡洛采样的流相比，这种组合可以导致具有显著更低方差的估计。",
    "tldr": "本研究将正则化流和准蒙特卡洛方法相结合，通过使用准蒙特卡洛方法采样流的初始分布，实现了具有显著更低方差的估计。",
    "en_tdlr": "The paper combines normalizing flows and quasi-Monte Carlo methods, utilizing quasi-Monte Carlo to sample the initial distribution transported by the flow, resulting in significantly lower variance in the estimator compared to classic Monte Carlo sampling."
}