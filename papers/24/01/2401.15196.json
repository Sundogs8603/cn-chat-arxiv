{
    "title": "Regularized Q-Learning with Linear Function Approximation. (arXiv:2401.15196v1 [cs.AI])",
    "abstract": "Several successful reinforcement learning algorithms make use of regularization to promote multi-modal policies that exhibit enhanced exploration and robustness. With functional approximation, the convergence properties of some of these algorithms (e.g. soft Q-learning) are not well understood. In this paper, we consider a single-loop algorithm for minimizing the projected Bellman error with finite time convergence guarantees in the case of linear function approximation. The algorithm operates on two scales: a slower scale for updating the target network of the state-action values, and a faster scale for approximating the Bellman backups in the subspace of the span of basis vectors. We show that, under certain assumptions, the proposed algorithm converges to a stationary point in the presence of Markovian noise. In addition, we provide a performance guarantee for the policies derived from the proposed algorithm.",
    "link": "http://arxiv.org/abs/2401.15196",
    "context": "Title: Regularized Q-Learning with Linear Function Approximation. (arXiv:2401.15196v1 [cs.AI])\nAbstract: Several successful reinforcement learning algorithms make use of regularization to promote multi-modal policies that exhibit enhanced exploration and robustness. With functional approximation, the convergence properties of some of these algorithms (e.g. soft Q-learning) are not well understood. In this paper, we consider a single-loop algorithm for minimizing the projected Bellman error with finite time convergence guarantees in the case of linear function approximation. The algorithm operates on two scales: a slower scale for updating the target network of the state-action values, and a faster scale for approximating the Bellman backups in the subspace of the span of basis vectors. We show that, under certain assumptions, the proposed algorithm converges to a stationary point in the presence of Markovian noise. In addition, we provide a performance guarantee for the policies derived from the proposed algorithm.",
    "path": "papers/24/01/2401.15196.json",
    "total_tokens": 808,
    "translated_title": "带有线性函数逼近的正则化Q学习",
    "translated_abstract": "一些成功的强化学习算法利用正则化来促进多模态策略，从而提高探索能力和鲁棒性。在使用函数逼近时，这些算法（如软Q学习）的收敛性质并不被很好地理解。本文考虑了一种单环路算法，在线性函数逼近的情况下具有有限时间收敛保证，用于最小化投影贝尔曼误差。该算法在两个尺度上运行：一个较慢的尺度用于更新状态动作值的目标网络，一个较快的尺度用于在基向量空间中逼近贝尔曼备份。在某些假设下，我们证明了在马尔可夫噪声存在下，该算法收敛于一个稳定点。此外，我们还提供了该算法衍生策略的性能保证。",
    "tldr": "本文提出了一种带有线性函数逼近的正则化Q学习算法，通过在不同尺度上操作，实现了有限时间内的收敛，并在马尔可夫噪声下具有性能保证。"
}