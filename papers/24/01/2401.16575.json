{
    "title": "Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking. (arXiv:2401.16575v1 [cs.CL])",
    "abstract": "The dominant probing approaches rely on the zero-shot performance of image-text matching tasks to gain a finer-grained understanding of the representations learned by recent multimodal image-language transformer models. The evaluation is carried out on carefully curated datasets focusing on counting, relations, attributes, and others. This work introduces an alternative probing strategy called guided masking. The proposed approach ablates different modalities using masking and assesses the model's ability to predict the masked word with high accuracy. We focus on studying multimodal models that consider regions of interest (ROI) features obtained by object detectors as input tokens. We probe the understanding of verbs using guided masking on ViLBERT, LXMERT, UNITER, and VisualBERT and show that these models can predict the correct verb with high accuracy. This contrasts with previous conclusions drawn from image-text matching probing techniques that frequently fail in situations requir",
    "link": "http://arxiv.org/abs/2401.16575",
    "context": "Title: Beyond Image-Text Matching: Verb Understanding in Multimodal Transformers Using Guided Masking. (arXiv:2401.16575v1 [cs.CL])\nAbstract: The dominant probing approaches rely on the zero-shot performance of image-text matching tasks to gain a finer-grained understanding of the representations learned by recent multimodal image-language transformer models. The evaluation is carried out on carefully curated datasets focusing on counting, relations, attributes, and others. This work introduces an alternative probing strategy called guided masking. The proposed approach ablates different modalities using masking and assesses the model's ability to predict the masked word with high accuracy. We focus on studying multimodal models that consider regions of interest (ROI) features obtained by object detectors as input tokens. We probe the understanding of verbs using guided masking on ViLBERT, LXMERT, UNITER, and VisualBERT and show that these models can predict the correct verb with high accuracy. This contrasts with previous conclusions drawn from image-text matching probing techniques that frequently fail in situations requir",
    "path": "papers/24/01/2401.16575.json",
    "total_tokens": 953,
    "translated_title": "超越图像-文本匹配：使用引导遮罩的多模态Transformer中的动词理解。",
    "translated_abstract": "主要的探测方法依赖于图像-文本匹配任务的零-shot性能，以更细粒度地了解近期多模态图像-语言Transformer模型学习到的表示。评估是在精心策划的数据集上进行的，重点关注计数、关系、属性等。本研究介绍了一种名为引导遮罩的替代探测策略。所提出的方法使用遮罩来消除不同的模态，并评估模型准确预测被遮罩的单词的能力。我们着重研究考虑ROI特征的多模态模型，这些特征是通过物体检测器获得的输入标记。我们使用引导遮罩在ViLBERT、LXMERT、UNITER和VisualBERT上研究动词的理解，并展示了这些模型能够准确预测正确的动词。这与先前从图像-文本匹配探测技术中得出的经常无法成功的情况形成对比。",
    "tldr": "本研究提出了一种新的探测策略，名为引导遮罩，在多模态Transformer模型中通过消除不同的模态来评估其对动词的理解能力。研究结果表明，这些模型能够准确预测正确的动词，与先前从图像-文本匹配探测技术中得出的结论形成对比。"
}