{
    "title": "Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?. (arXiv:2401.04218v1 [cs.CL])",
    "abstract": "We present a benchmark for assessing the capability of Large Language Models (LLMs) to discern intercardinal directions between geographic locations and apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar to humans, where judgments about individual locations' spatial relationships are influenced by the perceived relationships of the larger groups that contain them. To investigate this, we formulated 14 questions focusing on well-known American cities. Seven questions were designed to challenge the LLMs with scenarios potentially influenced by the orientation of larger geographical units, such as states or countries, while the remaining seven targeted locations less susceptible to such hierarchical categorization. Among the tested models, GPT-4 exhibited superior performance with 55.3% accuracy, followed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed significantly redu",
    "link": "http://arxiv.org/abs/2401.04218",
    "context": "Title: Distortions in Judged Spatial Relations in Large Language Models: The Dawn of Natural Language Geographic Data?. (arXiv:2401.04218v1 [cs.CL])\nAbstract: We present a benchmark for assessing the capability of Large Language Models (LLMs) to discern intercardinal directions between geographic locations and apply it to three prominent LLMs: GPT-3.5, GPT-4, and Llama-2. This benchmark specifically evaluates whether LLMs exhibit a hierarchical spatial bias similar to humans, where judgments about individual locations' spatial relationships are influenced by the perceived relationships of the larger groups that contain them. To investigate this, we formulated 14 questions focusing on well-known American cities. Seven questions were designed to challenge the LLMs with scenarios potentially influenced by the orientation of larger geographical units, such as states or countries, while the remaining seven targeted locations less susceptible to such hierarchical categorization. Among the tested models, GPT-4 exhibited superior performance with 55.3% accuracy, followed by GPT-3.5 at 47.3%, and Llama-2 at 44.7%. The models showed significantly redu",
    "path": "papers/24/01/2401.04218.json",
    "total_tokens": 903,
    "translated_title": "评估大型语言模型中的判断空间关系失真：自然语言地理数据的黎明？",
    "translated_abstract": "我们提出了一个用于评估大型语言模型(LLMs)在判断地理位置之间的方向上的能力的基准，并将其应用于三个知名的LLMs：GPT-3.5，GPT-4和Llama-2。这个基准特别评估了LLMs是否表现出类似人类的分层空间偏差，即对于包含它们的更大群体的感知关系会影响对个别位置空间关系的判断。为了调查这个问题，我们制定了14个关于美国知名城市的问题。其中七个问题旨在挑战LLMs，这些问题可能受到了更大地理单位（如州或国家）方向的影响，而另外七个问题则针对不容易受到这种层次化分类的位置。在经过测试的模型中，GPT-4的准确率最高，为55.3％，其次是GPT-3.5的47.3％和Llama-2的44.7％。",
    "tldr": "该研究评估了大型语言模型在判断地理位置方向上的能力，并发现这些模型可能存在分层空间偏差。其中，GPT-4表现最佳，准确率为55.3％。",
    "en_tdlr": "This study evaluates the ability of large language models to discern spatial directions between geographic locations and discovers potential hierarchical spatial bias. Among the tested models, GPT-4 achieves the highest accuracy of 55.3%."
}