{
    "title": "Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v1 [cs.AI])",
    "abstract": "Sequential decision-making under uncertainty is present in many important problems. Two popular approaches for tackling such problems are reinforcement learning and online search (e.g., Monte Carlo tree search). While the former learns a policy by interacting with the environment (typically done before execution), the latter uses a generative model of the environment to sample promising action trajectories at decision time. Decision-making is particularly challenging in non-stationary environments, where the environment in which an agent operates can change over time. Both approaches have shortcomings in such settings -- on the one hand, policies learned before execution become stale when the environment changes and relearning takes both time and computational effort. Online search, on the other hand, can return sub-optimal actions when there are limitations on allowed runtime. In this paper, we introduce \\textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines actio",
    "link": "http://arxiv.org/abs/2401.03197",
    "context": "Title: Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v1 [cs.AI])\nAbstract: Sequential decision-making under uncertainty is present in many important problems. Two popular approaches for tackling such problems are reinforcement learning and online search (e.g., Monte Carlo tree search). While the former learns a policy by interacting with the environment (typically done before execution), the latter uses a generative model of the environment to sample promising action trajectories at decision time. Decision-making is particularly challenging in non-stationary environments, where the environment in which an agent operates can change over time. Both approaches have shortcomings in such settings -- on the one hand, policies learned before execution become stale when the environment changes and relearning takes both time and computational effort. Online search, on the other hand, can return sub-optimal actions when there are limitations on allowed runtime. In this paper, we introduce \\textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines actio",
    "path": "papers/24/01/2401.03197.json",
    "total_tokens": 890,
    "translated_title": "在非稳定环境下的决策制定与策略增强搜索",
    "translated_abstract": "在许多重要问题中，存在着不确定性下的连续决策制定。针对这类问题，传统的方法包括强化学习和在线搜索（如蒙特卡洛树搜索）。前者通过与环境的交互来学习策略（通常在执行之前完成），而后者在决策时使用环境的生成模型来采样有前景的行动轨迹。在非稳定环境下的决策制定尤为具有挑战性，因为代理操作的环境可能随时间变化。两种方法在这种情况下都存在缺陷--一方面，执行之前学习的策略在环境改变时变得陈旧，重新学习需要时间和计算资源。另一方面，在线搜索在允许的运行时间有限时可能会返回次优行动。本文介绍了一种新的算法--策略增强蒙特卡洛树搜索（PA-MCTS），它将在线搜索与策略学习相结合。",
    "tldr": "在非稳定环境下的决策制定是一个具有挑战性的问题，本文介绍了一种新的算法--策略增强蒙特卡洛树搜索（PA-MCTS），它将在线搜索与策略学习相结合。",
    "en_tdlr": "Decision-making in non-stationary environments is challenging, and this paper introduces a new algorithm called Policy-Augmented Monte Carlo tree search (PA-MCTS), which combines online search with policy learning."
}