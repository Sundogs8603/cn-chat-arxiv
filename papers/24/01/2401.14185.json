{
    "title": "TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down Fusion. (arXiv:2401.14185v1 [cs.SD])",
    "abstract": "Audio-visual speech separation has gained significant traction in recent years due to its potential applications in various fields such as speech recognition, diarization, scene analysis and assistive technologies. Designing a lightweight audio-visual speech separation network is important for low-latency applications, but existing methods often require higher computational costs and more parameters to achieve better separation performance. In this paper, we present an audio-visual speech separation model called Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for audio-visual speech separation, which builds upon the architecture of TDANet, an audio-only speech separation method. TDANet serves as the architectural foundation for the auditory and visual networks within TDFNet, offering an efficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet achieves a performance increase of up to 10\\% across all performance metrics compared with the previous SOTA method ",
    "link": "http://arxiv.org/abs/2401.14185",
    "context": "Title: TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down Fusion. (arXiv:2401.14185v1 [cs.SD])\nAbstract: Audio-visual speech separation has gained significant traction in recent years due to its potential applications in various fields such as speech recognition, diarization, scene analysis and assistive technologies. Designing a lightweight audio-visual speech separation network is important for low-latency applications, but existing methods often require higher computational costs and more parameters to achieve better separation performance. In this paper, we present an audio-visual speech separation model called Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for audio-visual speech separation, which builds upon the architecture of TDANet, an audio-only speech separation method. TDANet serves as the architectural foundation for the auditory and visual networks within TDFNet, offering an efficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet achieves a performance increase of up to 10\\% across all performance metrics compared with the previous SOTA method ",
    "path": "papers/24/01/2401.14185.json",
    "total_tokens": 874,
    "translated_title": "TDFNet: 一种高效的音频视觉语音分离模型，具有自顶向下融合",
    "translated_abstract": "音频视觉语音分离近年来受到广泛关注，由于其在语音识别、日程分析、场景分析和辅助技术等领域的潜在应用。设计一个轻量级的音频视觉语音分离网络对于低延迟应用非常重要，但是现有方法通常需要更高的计算成本和更多的参数才能实现更好的分离性能。在本文中，我们提出了一种名为TDFNet的音频视觉语音分离模型，该模型是一种基于TDANet架构的最新技术（SOTA）模型，TDANet是一种仅用于音频的语音分离方法。TDANet在TDFNet内部构建了听觉和视觉网络的架构基础，提供了一个具有较少参数的高效模型。在LRS2-2Mix数据集上，与前一最佳方法相比，TDFNet在所有性能指标上的表现提高了10%。",
    "tldr": "TDFNet是一种高效的音频视觉语音分离模型，基于TDANet架构，具有更少的参数，并在性能上相比现有方法提高了10%。",
    "en_tdlr": "TDFNet is an efficient audio-visual speech separation model based on the TDANet architecture, with fewer parameters, achieving a 10% performance improvement compared to existing methods."
}