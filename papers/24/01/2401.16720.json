{
    "title": "SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing. (arXiv:2401.16720v1 [cs.LG])",
    "abstract": "There has been a proliferation of artificial intelligence applications, where model training is key to promising high-quality services for these applications. However, the model training process is both time-intensive and energy-intensive, inevitably affecting the user's demand for application efficiency. Layer freezing, an efficient model training technique, has been proposed to improve training efficiency. Although existing layer freezing methods demonstrate the great potential to reduce model training costs, they still remain shortcomings such as lacking generalizability and compromised accuracy. For instance, existing layer freezing methods either require the freeze configurations to be manually defined before training, which does not apply to different networks, or use heuristic freezing criteria that is hard to guarantee decent accuracy in different scenarios. Therefore, there lacks a generic and smart layer freezing method that can automatically perform ``in-situation'' layer fr",
    "link": "http://arxiv.org/abs/2401.16720",
    "context": "Title: SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing. (arXiv:2401.16720v1 [cs.LG])\nAbstract: There has been a proliferation of artificial intelligence applications, where model training is key to promising high-quality services for these applications. However, the model training process is both time-intensive and energy-intensive, inevitably affecting the user's demand for application efficiency. Layer freezing, an efficient model training technique, has been proposed to improve training efficiency. Although existing layer freezing methods demonstrate the great potential to reduce model training costs, they still remain shortcomings such as lacking generalizability and compromised accuracy. For instance, existing layer freezing methods either require the freeze configurations to be manually defined before training, which does not apply to different networks, or use heuristic freezing criteria that is hard to guarantee decent accuracy in different scenarios. Therefore, there lacks a generic and smart layer freezing method that can automatically perform ``in-situation'' layer fr",
    "path": "papers/24/01/2401.16720.json",
    "total_tokens": 828,
    "translated_title": "SmartFRZ:一种使用基于注意力的层冻结的高效训练框架",
    "translated_abstract": "人工智能应用正不断增加，模型训练对于提供高质量服务至关重要。然而，模型训练过程既耗时又耗能，不可避免地影响到用户对应用效率的需求。层冻结作为一种高效的模型训练技术被提出来提高训练效率。虽然现有的层冻结方法展示了减少模型训练成本的巨大潜力，但它们仍然存在缺点，如缺乏通用性和降低准确性。例如，现有的层冻结方法要么需要在训练之前手动定义冻结配置，这不适用于不同的网络，要么使用启发式的冻结标准，在不同场景下很难保证准确性。因此，缺乏一种能够自动执行“现场”层冻结的通用而智能的方法。",
    "tldr": "SmartFRZ是一种通用而智能的层冻结方法，能够自动执行“现场”层冻结，在提高训练效率的同时兼顾通用性和准确性。",
    "en_tdlr": "SmartFRZ is a general and intelligent layer freezing method that can automatically perform \"in-situ\" layer freezing, while balancing between training efficiency, generalizability, and accuracy."
}