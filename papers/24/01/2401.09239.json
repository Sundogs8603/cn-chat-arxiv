{
    "title": "DaFoEs: Mixing Datasets towards the generalization of vision-state deep-learning Force Estimation in Minimally Invasive Robotic Surgery. (arXiv:2401.09239v1 [cs.CV])",
    "abstract": "Precisely determining the contact force during safe interaction in Minimally Invasive Robotic Surgery (MIRS) is still an open research challenge. Inspired by post-operative qualitative analysis from surgical videos, the use of cross-modality data driven deep neural network models has been one of the newest approaches to predict sensorless force trends. However, these methods required for large and variable datasets which are not currently available. In this paper, we present a new vision-haptic dataset (DaFoEs) with variable soft environments for the training of deep neural models. In order to reduce the bias from a single dataset, we present a pipeline to generalize different vision and state data inputs for mixed dataset training, using a previously validated dataset with different setup. Finally, we present a variable encoder-decoder architecture to predict the forces done by the laparoscopic tool using single input or sequence of inputs. For input sequence, we use a recurrent decod",
    "link": "http://arxiv.org/abs/2401.09239",
    "context": "Title: DaFoEs: Mixing Datasets towards the generalization of vision-state deep-learning Force Estimation in Minimally Invasive Robotic Surgery. (arXiv:2401.09239v1 [cs.CV])\nAbstract: Precisely determining the contact force during safe interaction in Minimally Invasive Robotic Surgery (MIRS) is still an open research challenge. Inspired by post-operative qualitative analysis from surgical videos, the use of cross-modality data driven deep neural network models has been one of the newest approaches to predict sensorless force trends. However, these methods required for large and variable datasets which are not currently available. In this paper, we present a new vision-haptic dataset (DaFoEs) with variable soft environments for the training of deep neural models. In order to reduce the bias from a single dataset, we present a pipeline to generalize different vision and state data inputs for mixed dataset training, using a previously validated dataset with different setup. Finally, we present a variable encoder-decoder architecture to predict the forces done by the laparoscopic tool using single input or sequence of inputs. For input sequence, we use a recurrent decod",
    "path": "papers/24/01/2401.09239.json",
    "total_tokens": 1005,
    "translated_title": "DaFoEs：混合数据集以实现在微创机器人手术中视觉状态深度学习力量估计的泛化",
    "translated_abstract": "在微创机器人手术（MIRS）中准确确定安全交互过程中的接触力仍然是一个开放的研究难题。受手术视频的术后定性分析的启发，使用跨模态数据驱动的深度神经网络模型已经成为预测无感知力量趋势的最新方法之一。然而，这些方法需要大量且可变的数据集，目前还不可用。在本文中，我们提出了一个新的视觉-触觉数据集（DaFoEs），其中包含可以用于深度神经模型训练的可变软环境。为了减少单一数据集的偏见，我们提供了一个流程将不同的视觉和状态数据输入泛化到混合数据集训练中，使用先前验证过的具有不同设置的数据集。最后，我们提出了一个可变的编码器-解码器架构，以预测腹腔镜工具施加的力量，可以使用单个输入或输入序列。对于输入序列，我们使用了一个循环解码器。",
    "tldr": "本文介绍了一个新的视觉-触觉数据集（DaFoEs），用于训练深度神经网络模型以预测微创机器人手术中腹腔镜工具施加的力量。通过混合不同的数据集训练，以减少偏见，并提出了可变的编码器-解码器架构来处理单个输入或输入序列。"
}