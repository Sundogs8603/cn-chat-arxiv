{
    "title": "Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study",
    "abstract": "Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a not",
    "link": "https://arxiv.org/abs/2401.17981",
    "context": "Title: Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study\nAbstract: Despite the impressive capabilities of Multimodal Large Language Models (MLLMs) in integrating text and image modalities, challenges remain in accurately interpreting detailed visual elements. This paper presents an empirical study on enhancing MLLMs with state-of-the-art (SOTA) object detection and Optical Character Recognition models to improve fine-grained image understanding and reduce hallucination in responses. Our research investigates the embedding-based infusion of detection information, the impact of such infusion on the MLLMs' original abilities, and the interchangeability of detection models. We conduct systematic experiments with models such as LLaVA-1.5, DINO, and PaddleOCRv2, revealing that our approach not only refines MLLMs' performance in specific visual tasks but also maintains their original strengths. The resulting enhanced MLLMs outperform SOTA models on 9 out of 10 benchmarks, achieving an improvement of up to 12.99% on the normalized average score, marking a not",
    "path": "papers/24/01/2401.17981.json",
    "total_tokens": 937,
    "translated_title": "通过视觉检测模型增强多模态大型语言模型：一项实证研究",
    "translated_abstract": "尽管多模态大型语言模型（MLLMs）在集成文本和图像模态方面具有令人印象深刻的能力，但在准确解释细节视觉元素方面仍存在挑战。本文通过将最先进的目标检测和光学字符识别模型与MLLMs结合，进行实证研究，旨在提高对细粒度图像理解，并减少回答中的错误插入。我们的研究探讨了基于嵌入的检测信息的融合，这种融合对MLLMs的原始能力的影响，以及检测模型的可互换性。我们对LLaVA-1.5、DINO和PaddleOCRv2等模型进行了系统实验，发现我们的方法不仅改善了MLLMs在特定视觉任务中的性能，而且保持了它们的原始优势。通过在10个基准测试中，增强的MLLMs在9个测试中超越了最先进模型，标准化平均得分提升高达12.99%，取得了显著的改进。",
    "tldr": "本文通过实证研究，将最先进的目标检测和光学字符识别模型与多模态大型语言模型相结合，在提高图像理解和减少回答错误插入方面取得了显著改进。"
}