{
    "title": "Activations and Gradients Compression for Model-Parallel Training",
    "abstract": "arXiv:2401.07788v2 Announce Type: replace  Abstract: Large neural networks require enormous computational clusters of machines. Model-parallel training, when the model architecture is partitioned sequentially between workers, is a popular approach for training modern models. Information compression can be applied to decrease workers communication time, as it is often a bottleneck in such systems. This work explores how simultaneous compression of activations and gradients in model-parallel distributed training setup affects convergence. We analyze compression methods such as quantization and TopK compression, and also experiment with error compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error feedback approach. We conduct experiments on image classification and language model fine-tuning tasks. Our findings demonstrate that gradients require milder compression rates than activations. We observe that $K=10\\%$ is the lowest TopK compression level, which does not h",
    "link": "https://arxiv.org/abs/2401.07788",
    "context": "Title: Activations and Gradients Compression for Model-Parallel Training\nAbstract: arXiv:2401.07788v2 Announce Type: replace  Abstract: Large neural networks require enormous computational clusters of machines. Model-parallel training, when the model architecture is partitioned sequentially between workers, is a popular approach for training modern models. Information compression can be applied to decrease workers communication time, as it is often a bottleneck in such systems. This work explores how simultaneous compression of activations and gradients in model-parallel distributed training setup affects convergence. We analyze compression methods such as quantization and TopK compression, and also experiment with error compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error feedback approach. We conduct experiments on image classification and language model fine-tuning tasks. Our findings demonstrate that gradients require milder compression rates than activations. We observe that $K=10\\%$ is the lowest TopK compression level, which does not h",
    "path": "papers/24/01/2401.07788.json",
    "total_tokens": 860,
    "translated_title": "模型并行训练中的激活和梯度压缩",
    "translated_abstract": "大规模神经网络需要庞大的计算机集群。在模型并行训练中，当模型架构在工作者之间被顺序分割时，成为训练现代模型的一种流行方法。信息压缩可以应用于减少工作者的通信时间，在这种系统中通常是一个瓶颈。本文探讨了在模型并行分布式训练设置中同时压缩激活和梯度对收敛的影响。我们分析了诸如量化和TopK压缩等压缩方法，并尝试了误差补偿技术。此外，我们采用了TopK与AQ-SGD每批次误差反馈方法。我们在图像分类和语言模型微调任务上进行了实验。我们的研究结果表明，梯度需要比激活更轻微的压缩率。我们观察到，$K=10\\%$是最低的TopK压缩级别，这不导致收敛速度下降。",
    "tldr": "本研究探讨了在模型-并行分布式训练设置中对激活和梯度同时进行压缩对收敛的影响，发现梯度需要比激活更轻微的压缩率。",
    "en_tdlr": "This study investigates the impact of compressing activations and gradients simultaneously in a model-parallel distributed training setup, revealing that gradients require milder compression rates than activations."
}