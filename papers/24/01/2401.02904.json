{
    "title": "Class-wise Generalization Error: an Information-Theoretic Analysis. (arXiv:2401.02904v1 [cs.LG])",
    "abstract": "Existing generalization theories of supervised learning typically take a holistic approach and provide bounds for the expected generalization over the whole data distribution, which implicitly assumes that the model generalizes similarly for all the classes. In practice, however, there are significant variations in generalization performance among different classes, which cannot be captured by the existing generalization bounds. In this work, we tackle this problem by theoretically studying the class-generalization error, which quantifies the generalization performance of each individual class. We derive a novel information-theoretic bound for class-generalization error using the KL divergence, and we further obtain several tighter bounds using the conditional mutual information (CMI), which are significantly easier to estimate in practice. We empirically validate our proposed bounds in different neural networks and show that they accurately capture the complex class-generalization err",
    "link": "http://arxiv.org/abs/2401.02904",
    "context": "Title: Class-wise Generalization Error: an Information-Theoretic Analysis. (arXiv:2401.02904v1 [cs.LG])\nAbstract: Existing generalization theories of supervised learning typically take a holistic approach and provide bounds for the expected generalization over the whole data distribution, which implicitly assumes that the model generalizes similarly for all the classes. In practice, however, there are significant variations in generalization performance among different classes, which cannot be captured by the existing generalization bounds. In this work, we tackle this problem by theoretically studying the class-generalization error, which quantifies the generalization performance of each individual class. We derive a novel information-theoretic bound for class-generalization error using the KL divergence, and we further obtain several tighter bounds using the conditional mutual information (CMI), which are significantly easier to estimate in practice. We empirically validate our proposed bounds in different neural networks and show that they accurately capture the complex class-generalization err",
    "path": "papers/24/01/2401.02904.json",
    "total_tokens": 851,
    "translated_title": "类别普适误差：信息论分析",
    "translated_abstract": "现有的监督学习普适性理论通常采用整体方法，并提供了整个数据分布上预期普适化的界限，这隐含地假定模型对所有类别都具有类似的普适性。然而在实践中，不同类别的普适性性能存在显著差异，这无法被现有的普适性界限所捕捉。本文通过理论上研究类别普适误差来解决这个问题，该误差量化了每个个体类别的普适性能力。我们利用KL散度推导了一种新的信息论界限来衡量类别普适误差，并进一步使用条件互信息(CMI)得到了几个更紧的界限，在实践中更容易估计。我们在不同的神经网络上进行了实验证实，验证了我们提出的界限准确地捕捉了复杂的类别普适误差。",
    "tldr": "本文通过信息论分析研究了类别普适误差问题，并提出了使用KL散度的信息论界限和使用条件互信息(CMI)的更紧界限，能够准确捕捉复杂的类别普适误差。",
    "en_tdlr": "This paper addresses the problem of class-wise generalization error and proposes an information-theoretic analysis with novel bounds using KL divergence and conditional mutual information (CMI), which accurately capture the complex class-wise generalization error."
}