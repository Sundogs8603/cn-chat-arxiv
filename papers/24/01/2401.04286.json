{
    "title": "Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes. (arXiv:2401.04286v1 [stat.ML])",
    "abstract": "In this paper, we first extend the result of FL93 and prove universal consistency for a classification rule based on wide and deep ReLU neural networks trained on the logistic loss. Unlike the approach in FL93 that decomposes the estimation and empirical error, we directly analyze the classification risk based on the observation that a realization of a neural network that is wide enough is capable of interpolating an arbitrary number of points. Secondly, we give sufficient conditions for a class of probability measures under which classifiers based on neural networks achieve minimax optimal rates of convergence. Our result is motivated from the practitioner's observation that neural networks are often trained to achieve 0 training error, which is the case for our proposed neural network classifiers. Our proofs hinge on recent developments in empirical risk minimization and on approximation rates of deep ReLU neural networks for various function classes of interest. Applications to clas",
    "link": "http://arxiv.org/abs/2401.04286",
    "context": "Title: Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes. (arXiv:2401.04286v1 [stat.ML])\nAbstract: In this paper, we first extend the result of FL93 and prove universal consistency for a classification rule based on wide and deep ReLU neural networks trained on the logistic loss. Unlike the approach in FL93 that decomposes the estimation and empirical error, we directly analyze the classification risk based on the observation that a realization of a neural network that is wide enough is capable of interpolating an arbitrary number of points. Secondly, we give sufficient conditions for a class of probability measures under which classifiers based on neural networks achieve minimax optimal rates of convergence. Our result is motivated from the practitioner's observation that neural networks are often trained to achieve 0 training error, which is the case for our proposed neural network classifiers. Our proofs hinge on recent developments in empirical risk minimization and on approximation rates of deep ReLU neural networks for various function classes of interest. Applications to clas",
    "path": "papers/24/01/2401.04286.json",
    "total_tokens": 933,
    "translated_title": "宽而深的ReLU神经网络的普适一致性以及Kolmogorov-Donoho最优函数类的极小极限收敛速率",
    "translated_abstract": "本文首先扩展了FL93的结果，并证明了基于宽而深的ReLU神经网络和逻辑损失训练的分类规则的普适一致性。与FL93中分解估计和经验误差的方法不同，我们根据一个广泛的神经网络能够插值任意数量的点的观察，直接分析分类风险。其次，我们给出了一类概率测度的充分条件，在这些条件下，基于神经网络的分类器实现了极小极限收敛速率。我们的结果源于实践者观察到神经网络通常被训练成达到0训练误差的事实，这也是我们提出的神经网络分类器的情况。我们的证明依赖于最近在经验风险最小化和深ReLU神经网络的逼近速率方面的发展，适用于不同的感兴趣函数类的应用。",
    "tldr": "本文扩展了之前的结果，证明了基于宽而深的ReLU神经网络和逻辑损失训练的分类规则具有普适一致性，并给出了一类概率测度条件下基于神经网络的分类器实现极小极限收敛速率的充分条件。",
    "en_tdlr": "This paper extends previous results, proving the universal consistency of a classification rule based on wide and deep ReLU neural networks trained on logistic loss. It also provides sufficient conditions for a class of probability measures under which classifiers based on neural networks achieve minimax optimal convergence rates."
}