{
    "title": "F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods. (arXiv:2401.14869v1 [cs.CL])",
    "abstract": "Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced L",
    "link": "http://arxiv.org/abs/2401.14869",
    "context": "Title: F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods. (arXiv:2401.14869v1 [cs.CL])\nAbstract: Large language models (LLMs) garner significant attention for their unprecedented performance, leading to an increasing number of researches evaluating LLMs. However, these evaluation benchmarks are limited to assessing the instruction-following capabilities, overlooking the fundamental abilities that emerge during the pre-training stage. Previous subjective evaluation methods mainly reply on scoring by API models. However, in the absence of references, large models have shown limited ability to discern subtle differences. To bridge the gap, we propose F-Eval, a bilingual evaluation benchmark to evaluate the fundamental abilities, including expression, commonsense and logic. The tasks in F-Eval include multi-choice objective tasks, open-ended objective tasks, reference-based subjective tasks and reference-free subjective tasks. For reference-free subjective tasks, we devise new evaluation methods, serving as alternatives to scoring by API models. We conduct evaluations on 13 advanced L",
    "path": "papers/24/01/2401.14869.json",
    "total_tokens": 919,
    "translated_title": "F-Eval:使用优化的评估方法评估基本能力",
    "translated_abstract": "大型语言模型（LLMs）因其前所未有的性能而受到广泛关注，导致越来越多的研究评估LLMs。然而，这些评估基准仅限于评估指令遵循能力，忽视了在预训练阶段出现的基本能力。先前的主观评估方法主要依赖于由API模型评分。然而，在没有参考文献的情况下，大模型显示出有限的能力来区分细微差异。为了弥合这一差距，我们提出了F-Eval，一个双语评估基准，用于评估基本能力，包括表达、常识和逻辑。F-Eval中的任务包括多项选择客观任务、开放式客观任务、基于参考的主观任务和无参考的主观任务。对于无参考的主观任务，我们设计了新的评估方法，作为替代API模型评分的方法。我们对13个先进的LLMs进行了评估。",
    "tldr": "F-Eval是一个双语评估基准，用于评估大型语言模型的基本能力，包括表达、常识和逻辑。它采用多种任务形式进行评估，包括客观任务和主观任务，并提出了新的评估方法来解决无参考的主观任务评估问题。",
    "en_tdlr": "F-Eval is a bilingual evaluation benchmark for assessing the fundamental abilities of large language models, including expression, commonsense, and logic. It employs various task formats for evaluation, including objective tasks and subjective tasks, and introduces new evaluation methods to address the issue of reference-free subjective task evaluation."
}