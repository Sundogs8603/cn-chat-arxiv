{
    "title": "OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models. (arXiv:2401.06628v1 [cs.CL])",
    "abstract": "Advancing automated programming necessitates robust and comprehensive code generation benchmarks, yet current evaluation frameworks largely neglect object-oriented programming (OOP) in favor of functional programming (FP), e.g., HumanEval and MBPP. To address this, our study introduces a pioneering OOP-focused benchmark, featuring 431 Python programs that encompass essential OOP concepts and features like classes and encapsulation methods. We propose a novel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k measures. Our evaluation of 23 leading large language models (LLMs), including both general and code-specialized models, reveals three key insights: 1) pass@o offers a more relevant and comprehensive assessment for OOP code generation; 2) Despite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP compared to models like ChatGPT; 3) The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in thi",
    "link": "http://arxiv.org/abs/2401.06628",
    "context": "Title: OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models. (arXiv:2401.06628v1 [cs.CL])\nAbstract: Advancing automated programming necessitates robust and comprehensive code generation benchmarks, yet current evaluation frameworks largely neglect object-oriented programming (OOP) in favor of functional programming (FP), e.g., HumanEval and MBPP. To address this, our study introduces a pioneering OOP-focused benchmark, featuring 431 Python programs that encompass essential OOP concepts and features like classes and encapsulation methods. We propose a novel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k measures. Our evaluation of 23 leading large language models (LLMs), including both general and code-specialized models, reveals three key insights: 1) pass@o offers a more relevant and comprehensive assessment for OOP code generation; 2) Despite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP compared to models like ChatGPT; 3) The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in thi",
    "path": "papers/24/01/2401.06628.json",
    "total_tokens": 920,
    "translated_title": "OOP：针对大型语言模型的面向对象编程评估基准",
    "translated_abstract": "推进自动化编程需要健壮且全面的代码生成评估基准，然而当前的评估框架在功能式编程方面（例如HumanEval和MBPP）很大程度上忽视了面向对象编程（OOP）。为了解决这个问题，我们的研究引入了一项创新的面向对象编程重点基准，包括431个Python程序，涵盖了类和封装方法等基本的OOP概念和特性。我们提出了一种新颖的评估度量指标pass@o，针对OOP进行了改进，增强传统的pass@k度量。我们评估了23个领先的大型语言模型（LLMs），包括通用模型和专门用于代码的模型，得出了三个关键发现：1）pass@o提供了更相关和全面的OOP代码生成评估；2）尽管在FP方面表现出色，像WizardCoder这样的代码专用LLMs在OOP方面落后于像ChatGPT这样的模型；3）所有先进的LLMs在我们的OOP基准上表现不佳，突显了改进此领域的迫切需求。",
    "tldr": "本研究提出了一种面向对象编程的新型评估基准，包括431个Python程序，采用pass@o度量指标来提供更全面和相关的OOP代码生成评估。评估结果显示代码专用LLMs在OOP方面表现较差，需进一步改进此领域。"
}