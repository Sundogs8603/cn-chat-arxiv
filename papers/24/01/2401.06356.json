{
    "title": "An Empirical Investigation into the Effect of Parameter Choices in Knowledge Distillation. (arXiv:2401.06356v1 [cs.LG])",
    "abstract": "We present a large-scale empirical study of how choices of configuration parameters affect performance in knowledge distillation (KD). An example of such a KD parameter is the measure of distance between the predictions of the teacher and the student, common choices for which include the mean squared error (MSE) and the KL-divergence. Although scattered efforts have been made to understand the differences between such options, the KD literature still lacks a systematic study on their general effect on student performance. We take an empirical approach to this question in this paper, seeking to find out the extent to which such choices influence student performance across 13 datasets from 4 NLP tasks and 3 student sizes. We quantify the cost of making sub-optimal choices and identify a single configuration that performs well across the board.",
    "link": "http://arxiv.org/abs/2401.06356",
    "context": "Title: An Empirical Investigation into the Effect of Parameter Choices in Knowledge Distillation. (arXiv:2401.06356v1 [cs.LG])\nAbstract: We present a large-scale empirical study of how choices of configuration parameters affect performance in knowledge distillation (KD). An example of such a KD parameter is the measure of distance between the predictions of the teacher and the student, common choices for which include the mean squared error (MSE) and the KL-divergence. Although scattered efforts have been made to understand the differences between such options, the KD literature still lacks a systematic study on their general effect on student performance. We take an empirical approach to this question in this paper, seeking to find out the extent to which such choices influence student performance across 13 datasets from 4 NLP tasks and 3 student sizes. We quantify the cost of making sub-optimal choices and identify a single configuration that performs well across the board.",
    "path": "papers/24/01/2401.06356.json",
    "total_tokens": 817,
    "translated_title": "对知识蒸馏中参数选择的实证研究",
    "translated_abstract": "我们展示了一项关于参数配置选择对知识蒸馏性能影响的大规模实证研究。其中一个示例是教师和学生预测之间距离的度量，在此方面常见的选择包括均方误差（MSE）和KL散度。尽管已经进行了一些散乱的努力来理解这些选项之间的差异，但是知识蒸馏领域仍然缺乏一个对它们对学生性能的整体影响进行系统研究的工作。我们在这篇论文中采用实证方法来探索这个问题，试图找出这些选择在包括4个NLP任务和3种学生规模的13个数据集上对学生性能的影响程度。我们衡量了做出次优选择的代价，并确定了一个在各方面表现良好的单一配置。",
    "tldr": "本文对知识蒸馏中参数选择的影响进行了大规模实证研究，揭示了不同选项对学生性能的整体影响，并找到了在各方面表现良好的单一配置。",
    "en_tdlr": "This paper presents a large-scale empirical study on the effect of parameter choices in knowledge distillation, revealing the overall impact of different options on student performance and identifying a single configuration that performs well across various aspects."
}