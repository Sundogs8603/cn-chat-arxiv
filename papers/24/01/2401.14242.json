{
    "title": "Improving Natural Language Capability of Code Large Language Model. (arXiv:2401.14242v1 [cs.CL])",
    "abstract": "Code large language models (Code LLMs) have demonstrated remarkable performance in code generation. Nonetheless, most existing works focus on boosting code LLMs from the perspective of programming capabilities, while their natural language capabilities receive less attention. To fill this gap, we thus propose a novel framework, comprising two modules: AttentionExtractor, which is responsible for extracting key phrases from the user's natural language requirements, and AttentionCoder, which leverages these extracted phrases to generate target code to solve the requirement. This framework pioneers an innovative idea by seamlessly integrating code LLMs with traditional natural language processing tools. To validate the effectiveness of the framework, we craft a new code generation benchmark, called MultiNL-H, covering five natural languages. Extensive experimental results demonstrate the effectiveness of our proposed framework.",
    "link": "http://arxiv.org/abs/2401.14242",
    "context": "Title: Improving Natural Language Capability of Code Large Language Model. (arXiv:2401.14242v1 [cs.CL])\nAbstract: Code large language models (Code LLMs) have demonstrated remarkable performance in code generation. Nonetheless, most existing works focus on boosting code LLMs from the perspective of programming capabilities, while their natural language capabilities receive less attention. To fill this gap, we thus propose a novel framework, comprising two modules: AttentionExtractor, which is responsible for extracting key phrases from the user's natural language requirements, and AttentionCoder, which leverages these extracted phrases to generate target code to solve the requirement. This framework pioneers an innovative idea by seamlessly integrating code LLMs with traditional natural language processing tools. To validate the effectiveness of the framework, we craft a new code generation benchmark, called MultiNL-H, covering five natural languages. Extensive experimental results demonstrate the effectiveness of our proposed framework.",
    "path": "papers/24/01/2401.14242.json",
    "total_tokens": 874,
    "translated_title": "提升代码大型语言模型的自然语言能力",
    "translated_abstract": "代码大型语言模型（Code LLMs）表现出色，在代码生成方面取得了显著成绩。然而，现有的大部分工作都集中在提升代码LLMs的编程能力方面，而对其自然语言能力关注较少。为了填补这一空白，我们提出了一个新颖的框架，包括两个模块：AttentionExtractor，负责从用户的自然语言需求中提取关键短语，和AttentionCoder，利用这些提取出的短语生成目标代码来解决需求。该框架通过无缝融合传统自然语言处理工具与代码LLMs开创了一种创新的思路。为了验证该框架的有效性，我们创建了一个新的代码生成基准测试，名为MultiNL-H，涵盖了五种自然语言。广泛的实验结果证明了我们提出的框架的有效性。",
    "tldr": "该论文提出了一个新颖框架，通过融合传统自然语言处理工具和代码大型语言模型，提升了代码大型语言模型的自然语言能力。研究者提出了两个模块，AttentionExtractor用于提取关键短语，AttentionCoder利用这些短语生成目标代码。实验结果表明该框架的有效性。"
}