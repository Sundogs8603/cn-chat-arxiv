{
    "title": "Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation",
    "abstract": "Building open-ended learning agents involves challenges in pre-trained language model (LLM) and reinforcement learning (RL) approaches. LLMs struggle with context-specific real-time interactions, while RL methods face efficiency issues for exploration. To this end, we propose OpenContra, a co-training framework that cooperates LLMs and GRL to construct an open-ended agent capable of comprehending arbitrary human instructions. The implementation comprises two stages: (1) fine-tuning an LLM to translate human instructions into structured goals, and curriculum training a goal-conditioned RL policy to execute arbitrary goals; (2) collaborative training to make the LLM and RL policy learn to adapt each, achieving open-endedness on instruction space. We conduct experiments on Contra, a battle royale FPS game with a complex and vast goal space. The results show that an agent trained with OpenContra comprehends arbitrary human instructions and completes goals with a high completion ratio, whic",
    "link": "https://arxiv.org/abs/2401.00006",
    "context": "Title: Building Open-Ended Embodied Agent via Language-Policy Bidirectional Adaptation\nAbstract: Building open-ended learning agents involves challenges in pre-trained language model (LLM) and reinforcement learning (RL) approaches. LLMs struggle with context-specific real-time interactions, while RL methods face efficiency issues for exploration. To this end, we propose OpenContra, a co-training framework that cooperates LLMs and GRL to construct an open-ended agent capable of comprehending arbitrary human instructions. The implementation comprises two stages: (1) fine-tuning an LLM to translate human instructions into structured goals, and curriculum training a goal-conditioned RL policy to execute arbitrary goals; (2) collaborative training to make the LLM and RL policy learn to adapt each, achieving open-endedness on instruction space. We conduct experiments on Contra, a battle royale FPS game with a complex and vast goal space. The results show that an agent trained with OpenContra comprehends arbitrary human instructions and completes goals with a high completion ratio, whic",
    "path": "papers/24/01/2401.00006.json",
    "total_tokens": 910,
    "translated_title": "通过语言策略双向适应构建开放式具身代理",
    "translated_abstract": "构建开放式学习代理涉及预训练的语言模型（LLM）和强化学习（RL）方法面临的挑战。LLM在上下文特定的实时交互中遇到困难，而RL方法则面临探索效率问题。为此，我们提出了OpenContra，一种协同训练框架，它协同LLM和Goal-Conditioned强化学习（GRL），构建一个能够理解任意人类指令的开放式代理。实现包括两个阶段：（1）用LLM微调翻译人类指令为结构化目标，以及课程训练目标条件的RL策略，执行任意目标；（2）协同训练LLM和RL策略相互适应，实现指令空间的开放性。我们在Contra上进行实验，该游戏是一个复杂而广阔的目标空间的大逃杀FPS游戏。结果表明，使用OpenContra训练的代理能够理解任意人类指令，并以高完成率完成目标。",
    "tldr": "通过OpenContra框架，我们提出了一种协同训练方法，结合语言模型和强化学习，构建开放式具身代理，能够理解任意人类指令，并以较高的完成率完成目标。",
    "en_tdlr": "By using the OpenContra framework, we propose a co-training approach that combines language models and reinforcement learning to build an open-ended embodied agent. This agent is capable of comprehending arbitrary human instructions and achieving a high completion ratio for goals."
}