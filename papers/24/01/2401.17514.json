{
    "title": "FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation",
    "abstract": "A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled data from both source and target domains to learn domain-invariant representations for adaptation. However, these methods showcase certain limitations, encouraging the use of self-supervised learning through continued pre-training. The necessity of continued pre-training or learning domain-invariant representations is still unclear in the prompt-based classification framework, where an input example is modified by a template and then fed into a language model (LM) to generate a label string. To examine this new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and labeled examples using two different instruction-tuning tasks. Specifically, the first task trains the LM on unlabeled texts from both domains via masked language modeling (MLM), and the other uses supervised instruction-tuning on source-labeled data for c",
    "link": "https://arxiv.org/abs/2401.17514",
    "context": "Title: FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation\nAbstract: A major thread of unsupervised domain adaptation (UDA) methods uses unlabeled data from both source and target domains to learn domain-invariant representations for adaptation. However, these methods showcase certain limitations, encouraging the use of self-supervised learning through continued pre-training. The necessity of continued pre-training or learning domain-invariant representations is still unclear in the prompt-based classification framework, where an input example is modified by a template and then fed into a language model (LM) to generate a label string. To examine this new paradigm of UDA in the prompt-based setup, we propose a frustratingly easy UDA method (FEUDA) that trains an autoregressive LM on both unlabeled and labeled examples using two different instruction-tuning tasks. Specifically, the first task trains the LM on unlabeled texts from both domains via masked language modeling (MLM), and the other uses supervised instruction-tuning on source-labeled data for c",
    "path": "papers/24/01/2401.17514.json",
    "total_tokens": 928,
    "translated_title": "FEUDA：令人沮丧地简单的基于提示的无监督领域自适应",
    "translated_abstract": "无监督领域自适应方法的一个主要分支利用来自源领域和目标领域的未标记数据，学习适应的领域不变表示。然而，这些方法存在一定的局限性，鼓励通过持续的预训练使用自监督学习。在基于提示的分类框架中，持续的预训练或学习领域不变表示的必要性仍不清楚，其中一个输入示例由模板修改后，再输入到语言模型（LM）中生成一个标签字符串。为了研究基于提示的无监督领域自适应中的这种新范例，我们提出了一种令人沮丧地简单的无监督领域自适应方法（FEUDA），该方法使用两种不同的指令调整任务，在未标记和标记的示例上训练自回归LM。具体而言，第一个任务通过掩蔽语言建模（MLM）在两个领域的未标记文本上训练LM，第二个任务使用源标记数据进行监督指令调整。",
    "tldr": "FEUDA是一种令人沮丧地简单的无监督领域自适应方法，通过在未标记和标记的示例上训练自回归语言模型，在提示基础的分类框架中探索无监督领域自适应的新范例。",
    "en_tdlr": "FEUDA is a frustratingly easy unsupervised domain adaptation method that explores a new paradigm of unsupervised domain adaptation in the prompt-based classification framework by training an autoregressive language model on both unlabeled and labeled examples."
}