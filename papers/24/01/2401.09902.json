{
    "title": "Interplay between depth and width for interpolation in neural ODEs. (arXiv:2401.09902v1 [math.OC])",
    "abstract": "Neural ordinary differential equations (neural ODEs) have emerged as a natural tool for supervised learning from a control perspective, yet a complete understanding of their optimal architecture remains elusive. In this work, we examine the interplay between their width $p$ and number of layer transitions $L$ (effectively the depth $L+1$). Specifically, we assess the model expressivity in terms of its capacity to interpolate either a finite dataset $D$ comprising $N$ pairs of points or two probability measures in $\\mathbb{R}^d$ within a Wasserstein error margin $\\varepsilon>0$. Our findings reveal a balancing trade-off between $p$ and $L$, with $L$ scaling as $O(1+N/p)$ for dataset interpolation, and $L=O\\left(1+(p\\varepsilon^d)^{-1}\\right)$ for measure interpolation.  In the autonomous case, where $L=0$, a separate study is required, which we undertake focusing on dataset interpolation. We address the relaxed problem of $\\varepsilon$-approximate controllability and establish an error ",
    "link": "http://arxiv.org/abs/2401.09902",
    "context": "Title: Interplay between depth and width for interpolation in neural ODEs. (arXiv:2401.09902v1 [math.OC])\nAbstract: Neural ordinary differential equations (neural ODEs) have emerged as a natural tool for supervised learning from a control perspective, yet a complete understanding of their optimal architecture remains elusive. In this work, we examine the interplay between their width $p$ and number of layer transitions $L$ (effectively the depth $L+1$). Specifically, we assess the model expressivity in terms of its capacity to interpolate either a finite dataset $D$ comprising $N$ pairs of points or two probability measures in $\\mathbb{R}^d$ within a Wasserstein error margin $\\varepsilon>0$. Our findings reveal a balancing trade-off between $p$ and $L$, with $L$ scaling as $O(1+N/p)$ for dataset interpolation, and $L=O\\left(1+(p\\varepsilon^d)^{-1}\\right)$ for measure interpolation.  In the autonomous case, where $L=0$, a separate study is required, which we undertake focusing on dataset interpolation. We address the relaxed problem of $\\varepsilon$-approximate controllability and establish an error ",
    "path": "papers/24/01/2401.09902.json",
    "total_tokens": 1022,
    "translated_title": "深度和宽度在神经ODE插值中的相互作用",
    "translated_abstract": "神经常微分方程(neural ODEs)已经成为从控制角度进行监督学习的自然工具，然而对其最佳结构的完全理解仍然难以捉摸。在这项工作中，我们研究了宽度$p$和层之间的过渡次数$L$（实际上是深度$L+1$）之间的相互作用。具体来说，我们评估了模型的表达能力，以其能够在Wasserstein误差边界$\\varepsilon>0$内插值一个包含$N$对点的有限数据集$D$或两个概率测度在$\\mathbb{R}^d$中。我们的发现揭示了$p$和$L$之间的平衡折衷关系，在数据集插值中，$L$随着$O(1+N/p)$的比例增长，而在测度插值中，$L=O\\left(1+(p\\varepsilon^d)^{-1}\\right)$。在自主情况下，$L=0$，需要进行单独的研究，我们将重点关注数据集插值。我们解决了$\\varepsilon$-近似控制性的放松问题，并建立了误差",
    "tldr": "本文研究了神经ODE插值中深度和宽度之间的相互作用，并发现在数据集插值中存在着$p$和$L$之间的平衡折衷关系，而在测度插值中，$L$的增长与$p$和$\\varepsilon$的关系有关。",
    "en_tdlr": "This paper examines the interplay between depth and width in neural ODE interpolation, and finds a balancing trade-off between $p$ and $L$ in dataset interpolation, while the growth of $L$ in measure interpolation is related to $p$ and $\\varepsilon$."
}