{
    "title": "Elastic Multi-Gradient Descent for Parallel Continual Learning. (arXiv:2401.01054v1 [cs.LG])",
    "abstract": "The goal of Continual Learning (CL) is to continuously learn from new data streams and accomplish the corresponding tasks. Previously studied CL assumes that data are given in sequence nose-to-tail for different tasks, thus indeed belonging to Serial Continual Learning (SCL). This paper studies the novel paradigm of Parallel Continual Learning (PCL) in dynamic multi-task scenarios, where a diverse set of tasks is encountered at different time points. PCL presents challenges due to the training of an unspecified number of tasks with varying learning progress, leading to the difficulty of guaranteeing effective model updates for all encountered tasks. In our previous conference work, we focused on measuring and reducing the discrepancy among gradients in a multi-objective optimization problem, which, however, may still contain negative transfers in every model update. To address this issue, in the dynamic multi-objective optimization problem, we introduce task-specific elastic factors to",
    "link": "http://arxiv.org/abs/2401.01054",
    "context": "Title: Elastic Multi-Gradient Descent for Parallel Continual Learning. (arXiv:2401.01054v1 [cs.LG])\nAbstract: The goal of Continual Learning (CL) is to continuously learn from new data streams and accomplish the corresponding tasks. Previously studied CL assumes that data are given in sequence nose-to-tail for different tasks, thus indeed belonging to Serial Continual Learning (SCL). This paper studies the novel paradigm of Parallel Continual Learning (PCL) in dynamic multi-task scenarios, where a diverse set of tasks is encountered at different time points. PCL presents challenges due to the training of an unspecified number of tasks with varying learning progress, leading to the difficulty of guaranteeing effective model updates for all encountered tasks. In our previous conference work, we focused on measuring and reducing the discrepancy among gradients in a multi-objective optimization problem, which, however, may still contain negative transfers in every model update. To address this issue, in the dynamic multi-objective optimization problem, we introduce task-specific elastic factors to",
    "path": "papers/24/01/2401.01054.json",
    "total_tokens": 848,
    "translated_title": "弹性多梯度下降用于并行连续学习",
    "translated_abstract": "连续学习（CL）的目标是从新的数据流中持续学习并完成相应的任务。过去研究的CL假设数据按任务的顺序给出，因此属于串行连续学习（SCL）。本文研究了动态多任务场景下的新兴范式——并行连续学习（PCL），其中在不同时间点遇到了多样的任务。PCL面临的挑战是训练数量不确定且学习进度不同的任务，导致很难保证所有遇到的任务都能得到有效的模型更新。在我们之前的会议论文中，我们主要研究了在多目标优化问题中测量和减小梯度之间的差异，然而，每次模型更新仍然可能存在负迁移。为了解决这个问题，在动态多目标优化问题中，我们引入了任务特定的弹性因子。",
    "tldr": "这是一篇关于并行连续学习的论文，介绍了在动态多任务场景下的挑战和解决方法。通过使用任务特定的弹性因子，可以解决梯度差异和负迁移的问题。",
    "en_tdlr": "This paper discusses the challenges and solutions of parallel continual learning in dynamic multi-task scenarios. By introducing task-specific elastic factors, the issues of gradient discrepancy and negative transfer can be addressed."
}