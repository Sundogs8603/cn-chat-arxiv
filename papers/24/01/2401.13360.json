{
    "title": "Debiased Sample Selection for Combating Noisy Labels. (arXiv:2401.13360v1 [cs.LG])",
    "abstract": "Learning with noisy labels aims to ensure model generalization given a label-corrupted training set. The sample selection strategy achieves promising performance by selecting a label-reliable subset for model training. In this paper, we empirically reveal that existing sample selection methods suffer from both data and training bias that are represented as imbalanced selected sets and accumulation errors in practice, respectively. However, only the training bias was handled in previous studies. To address this limitation, we propose a noIse-Tolerant Expert Model (ITEM) for debiased learning in sample selection. Specifically, to mitigate the training bias, we design a robust network architecture that integrates with multiple experts. Compared with the prevailing double-branch network, our network exhibits better performance of selection and prediction by ensembling these experts while training with fewer parameters. Meanwhile, to mitigate the data bias, we propose a mixed sampling strat",
    "link": "http://arxiv.org/abs/2401.13360",
    "context": "Title: Debiased Sample Selection for Combating Noisy Labels. (arXiv:2401.13360v1 [cs.LG])\nAbstract: Learning with noisy labels aims to ensure model generalization given a label-corrupted training set. The sample selection strategy achieves promising performance by selecting a label-reliable subset for model training. In this paper, we empirically reveal that existing sample selection methods suffer from both data and training bias that are represented as imbalanced selected sets and accumulation errors in practice, respectively. However, only the training bias was handled in previous studies. To address this limitation, we propose a noIse-Tolerant Expert Model (ITEM) for debiased learning in sample selection. Specifically, to mitigate the training bias, we design a robust network architecture that integrates with multiple experts. Compared with the prevailing double-branch network, our network exhibits better performance of selection and prediction by ensembling these experts while training with fewer parameters. Meanwhile, to mitigate the data bias, we propose a mixed sampling strat",
    "path": "papers/24/01/2401.13360.json",
    "total_tokens": 912,
    "translated_title": "对抗噪声标签的无偏样本选择",
    "translated_abstract": "学习使用噪声标签旨在确保模型在标签错误的训练集上具有泛化能力。样本选择策略通过选择可靠的标签子集来实现有希望的性能。本文实证表明，现有的样本选择方法在实践中存在数据和训练偏差，分别表示为选择集不平衡和累积错误。然而，先前的研究只处理了训练偏差。为了解决这个局限性，我们提出了一个适用于样本选择的无噪声专家模型（ITEM）。具体来说，为了减轻训练偏差，我们设计了一个鲁棒的网络架构，与多个专家集成。与目前的双分支网络相比，我们的网络在训练更少参数的情况下，通过集成这些专家来实现更好的选择和预测性能。同时，为了减轻数据偏差，我们提出了一种混合采样策略。",
    "tldr": "本文提出了一个无噪声专家模型（ITEM）来解决样本选择中的训练偏差和数据偏差问题。通过设计一个鲁棒的网络架构来集成多个专家，可以减少选择集不平衡和累积错误，并在使用更少参数的情况下实现更好的选择和预测性能。",
    "en_tdlr": "This paper proposes a noise-tolerant expert model (ITEM) to address the training bias and data bias issues in sample selection. By designing a robust network architecture to integrate multiple experts, it can reduce the imbalance of selected sets and accumulation errors and achieve better selection and prediction performance with fewer parameters."
}