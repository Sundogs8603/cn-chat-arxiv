{
    "title": "Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training. (arXiv:2401.14948v1 [cs.LG])",
    "abstract": "Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also ",
    "link": "http://arxiv.org/abs/2401.14948",
    "context": "Title: Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training. (arXiv:2401.14948v1 [cs.LG])\nAbstract: Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our empirical findings demonstrate that selectively updating specific layers while preserving others can substantially enhance the network's learning capacity. We therefore propose CURE, a novel training framework that leverages a gradient prominence criterion to perform selective conservation, updating, and revision of weights. Importantly, CURE is designed to be dataset- and architecture-agnostic, ensuring its applicability across various scenarios. It effectively tackles both memorization and overfitting issues, thus enhancing the trade-off between robustness and generalization and additionally, this training approach also ",
    "path": "papers/24/01/2401.14948.json",
    "total_tokens": 1010,
    "translated_title": "保留-更新-修订以解决对抗训练中的泛化性和鲁棒性的权衡",
    "translated_abstract": "对抗训练可以提高神经网络对抗攻击的鲁棒性，但会导致标准泛化和鲁棒泛化之间的权衡。为了揭示驱动这一现象的潜在因素，我们研究了神经网络在从标准设置到对抗设置过渡时的逐层学习能力。经验结果表明，选择性地更新特定层而保留其他层可以大幅增强网络的学习能力。因此，我们提出了一种新的训练框架CURE，利用梯度显著性准则对权重进行选择性保留、更新和修订。重要的是，CURE的设计是数据集和架构不可知的，确保其适用于各种情况。它有效解决了记忆化和过度拟合问题，从而增强了鲁棒性和泛化性之间的权衡，并且这种训练方法还可以",
    "tldr": "对抗训练在提高神经网络鲁棒性方面取得了进展，但是牺牲了标准和鲁棒泛化之间的权衡。研究发现，选择性地更新特定层可以显著提高网络学习能力。因此，提出了CURE框架，通过选择性保留、更新和修订权重来解决这一问题。这一方法可以有效解决记忆化和过度拟合问题，并提高鲁棒性和泛化性之间的权衡。"
}