{
    "title": "Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods. (arXiv:2401.14228v1 [cs.CL])",
    "abstract": "As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning. In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module. We find that the ported modules far outperform the two alternatives tested, but t",
    "link": "http://arxiv.org/abs/2401.14228",
    "context": "Title: Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods. (arXiv:2401.14228v1 [cs.CL])\nAbstract: As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning. In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module. We find that the ported modules far outperform the two alternatives tested, but t",
    "path": "papers/24/01/2401.14228.json",
    "total_tokens": 958,
    "translated_title": "评估通过参数高效微调方法训练的参数矩阵的可移植性",
    "translated_abstract": "随着训练规模越来越大的语言模型的成本增加，对重复利用先前学到的知识的兴趣也在增加。迁移学习方法表明，重复利用非任务特定知识可以帮助后续特定任务学习。本文研究了相反的情况：将从一个模型移植编码任务特定知识的完整功能模块到另一个模型。我们设计了一项包括1,440个训练/测试运行的研究，以测试通过参数高效微调(PEFT)技术训练的模块的可移植性，以情感分析为示例任务。我们在各种场景中测试了可移植性，涉及不同的PEFT技术和不同的预训练主机模型，等等。我们将移植的模块的性能与(i)从头开始训练的相等模块的性能和(ii)从与移植的模块相同分布的参数中采样训练的模块进行比较。我们发现，移植的模块的性能远远超过所测试的两种替代方案的性能，但需注意一些局限性。",
    "tldr": "本文研究了通过参数高效微调方法训练的模块的可移植性，发现这些移植的模块在各种情景下表现出优异的性能，可以有效地重复利用任务特定知识。",
    "en_tdlr": "This paper investigates the portability of modules trained by parameter-efficient finetuning methods and finds that these ported modules demonstrate excellent performance in various scenarios, making them effective for reusing task-specific knowledge."
}