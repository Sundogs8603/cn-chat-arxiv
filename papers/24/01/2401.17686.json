{
    "title": "Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning",
    "abstract": "Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves",
    "link": "https://arxiv.org/abs/2401.17686",
    "context": "Title: Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought Reasoning\nAbstract: Recent advancements have significantly augmented the reasoning capabilities of Large Language Models (LLMs) through various methodologies, especially chain-of-thought (CoT) reasoning. However, previous methods fail to address reasoning errors in intermediate steps, leading to accumulative errors.In this paper, we propose Deductive Beam Search (DBS), which seamlessly integrates CoT and deductive reasoning with step-wise beam search for LLMs. Our approach deploys a verifier, verifying the deducibility of a reasoning step and its premises, thus alleviating the error accumulation. Furthermore, we introduce a scalable and labor-free data construction method to amplify our model's verification capabilities. Extensive experiments demonstrate that our approach significantly enhances the base performance of LLMs of various scales (7B, 13B, 70B, and ChatGPT) across 8 reasoning datasets from 3 diverse reasoning genres, including arithmetic, commonsense, and symbolic. Moreover, our analysis proves",
    "path": "papers/24/01/2401.17686.json",
    "total_tokens": 945,
    "translated_title": "推理束搜索：为链式思维推断寻找可推导的理由",
    "translated_abstract": "最近的研究通过各种方法，尤其是链式思维推理，极大增强了大型语言模型（LLMs）的推理能力。然而，以往的方法未能解决中间步骤的推理错误问题，导致错误的累积。本文提出了一种称为推理束搜索（DBS）的方法，它将链式思维和演绎推理与逐步束搜索无缝集成到LLMs中。我们的方法部署了一个验证器，用于验证推理步骤及其前提的可推导性，从而减少错误的累积。此外，我们引入了一种可扩展且无需人工劳动的数据构建方法，来增强我们模型的验证能力。广泛的实验证明，我们的方法显著提升了各种规模的LLMs（7B、13B、70B和ChatGPT）的基础性能，在3种不同的推理场景（算术、常识和符号）的8个推理数据集中都表现出色。此外，我们的分析证明了",
    "tldr": "本文提出了一种称为推理束搜索（DBS）的方法，将链式思维和演绎推理与逐步束搜索无缝集成到大型语言模型（LLMs）中，通过引入验证器来减少错误的累积，并通过可扩展和无需人工劳动的数据构建方法提升模型的验证能力。"
}