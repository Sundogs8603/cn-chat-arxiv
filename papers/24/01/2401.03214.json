{
    "title": "Understanding Representation Learnability of Nonlinear Self-Supervised Learning. (arXiv:2401.03214v1 [cs.LG])",
    "abstract": "Self-supervised learning (SSL) has empirically shown its data representation learnability in many downstream tasks. There are only a few theoretical works on data representation learnability, and many of those focus on final data representation, treating the nonlinear neural network as a ``black box\". However, the accurate learning results of neural networks are crucial for describing the data distribution features learned by SSL models. Our paper is the first to analyze the learning results of the nonlinear SSL model accurately. We consider a toy data distribution that contains two features: the label-related feature and the hidden feature. Unlike previous linear setting work that depends on closed-form solutions, we use the gradient descent algorithm to train a 1-layer nonlinear SSL model with a certain initialization region and prove that the model converges to a local minimum. Furthermore, different from the complex iterative analysis, we propose a new analysis process which uses t",
    "link": "http://arxiv.org/abs/2401.03214",
    "context": "Title: Understanding Representation Learnability of Nonlinear Self-Supervised Learning. (arXiv:2401.03214v1 [cs.LG])\nAbstract: Self-supervised learning (SSL) has empirically shown its data representation learnability in many downstream tasks. There are only a few theoretical works on data representation learnability, and many of those focus on final data representation, treating the nonlinear neural network as a ``black box\". However, the accurate learning results of neural networks are crucial for describing the data distribution features learned by SSL models. Our paper is the first to analyze the learning results of the nonlinear SSL model accurately. We consider a toy data distribution that contains two features: the label-related feature and the hidden feature. Unlike previous linear setting work that depends on closed-form solutions, we use the gradient descent algorithm to train a 1-layer nonlinear SSL model with a certain initialization region and prove that the model converges to a local minimum. Furthermore, different from the complex iterative analysis, we propose a new analysis process which uses t",
    "path": "papers/24/01/2401.03214.json",
    "total_tokens": 868,
    "translated_title": "理解非线性自监督学习的表示可学习性",
    "translated_abstract": "自监督学习（SSL）在许多下游任务中经验证明其数据表示可学习性。关于数据表示可学习性的理论研究很少，其中许多将非线性神经网络视为“黑箱”，仅关注最终的数据表示。然而，神经网络的准确学习结果对于描述SSL模型学到的数据分布特征至关重要。我们的论文是首次准确分析非线性SSL模型的学习结果。我们考虑了一个包含两个特征的玩具数据分布：与标签相关的特征和隐藏特征。与以往的依赖于闭式解的线性设置工作不同，我们使用梯度下降算法在特定的初始化区域下训练一个1层非线性SSL模型，并证明模型收敛到一个局部最小值。此外，与复杂的迭代分析不同，我们提出了一个新的分析过程，使用t",
    "tldr": "这篇论文是第一次准确分析非线性自监督学习模型的学习结果，通过使用梯度下降算法训练模型并证明其收敛到局部最小值，在理解数据表示可学习性方面做出了重要贡献。"
}