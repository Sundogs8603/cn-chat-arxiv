{
    "title": "Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])",
    "abstract": "The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\\textbf{a}res lessons for ex\\textbf{p}anding \\textbf{o}perations by \\textbf{l}earning high-\\textbf{l}ayer functi\\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even",
    "link": "http://arxiv.org/abs/2401.09192",
    "context": "Title: Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])\nAbstract: The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\\textbf{a}res lessons for ex\\textbf{p}anding \\textbf{o}perations by \\textbf{l}earning high-\\textbf{l}ayer functi\\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even",
    "path": "papers/24/01/2401.09192.json",
    "total_tokens": 873,
    "translated_title": "为渐进式训练语言模型准备课程的方法",
    "translated_abstract": "Transformers在人工智能领域的迅速发展带来了资源消耗和温室气体排放的增加，这是由于模型规模的增长。先前的研究表明使用预训练的小模型可以提高训练效率，但这种方法对于新的模型结构可能不适用。另一方面，从头开始训练可能很慢，并且渐进堆叠层往往无法实现显著的加速。为了解决这些挑战，我们提出了一种名为Apollo的新方法，它通过在低层训练期间学习高层功能来准备膨胀操作的课程。我们的方法涉及低值优先采样(LVPS)来训练不同深度，并引入权重共享以促进高效扩展。我们还介绍了一种插值方法来实现稳定的模型深度扩展。实验证明，Apollo实现了最先进的加速比率，甚至……",
    "tldr": "提出了一种名为Apollo的方法，通过在低层训练期间学习高层功能，为渐进式训练语言模型设计了课程，实现了最先进的加速比率。",
    "en_tdlr": "A method called Apollo is proposed, which prepares lessons for progressive training on language models by learning high-layer functionality during training of low layers, achieving state-of-the-art acceleration ratios."
}