{
    "title": "Aligning Large Language Models with Counterfactual DPO. (arXiv:2401.09566v1 [cs.CL])",
    "abstract": "Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and",
    "link": "http://arxiv.org/abs/2401.09566",
    "context": "Title: Aligning Large Language Models with Counterfactual DPO. (arXiv:2401.09566v1 [cs.CL])\nAbstract: Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and",
    "path": "papers/24/01/2401.09566.json",
    "total_tokens": 863,
    "translated_title": "使用反事实对抗优化实现大型语言模型的对齐",
    "translated_abstract": "大型语言模型(LLMs)的进步在各种应用中展示了卓越的能力。这些模型在生成上下文连贯且涵盖广泛主题的文本补全方面表现出色。然而，它们训练所需的大量数据使得在预训练和指令调整阶段对齐响应风格变得具有挑战性。因此，通常会采用额外的对齐阶段，进一步使用人类偏好数据对模型进行训练，以更好地将其输出与人类期望对齐。虽然这个过程本身并没有引入新的能力，但它突出了模型固有的生成风格。本文研究了在直接偏好优化(DPO)框架内利用反事实提示来对齐模型的风格，而不依赖人类干预。我们证明了这种方法有效地培养了可取的行为，减轻了不可取的行为。",
    "tldr": "本文研究了在大型语言模型中使用反事实对抗优化框架，以实现风格对齐，避免人类干预，并成功培养出可取行为和减轻不可取行为。",
    "en_tdlr": "This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization to align the style of large language models without relying on human intervention, effectively instilling desirable behavior and mitigating undesirable ones."
}