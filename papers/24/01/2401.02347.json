{
    "title": "Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training. (arXiv:2401.02347v1 [cs.CV])",
    "abstract": "Image captioning aims at generating descriptive and meaningful textual descriptions of images, enabling a broad range of vision-language applications. Prior works have demonstrated that harnessing the power of Contrastive Image Language Pre-training (CLIP) offers a promising approach to achieving zero-shot captioning, eliminating the need for expensive caption annotations. However, the widely observed modality gap in the latent space of CLIP harms the performance of zero-shot captioning by breaking the alignment between paired image-text features. To address this issue, we conduct an analysis on the CLIP latent space which leads to two findings. Firstly, we observe that the CLIP's visual feature of image subregions can achieve closer proximity to the paired caption due to the inherent information loss in text descriptions. In addition, we show that the modality gap between a paired image-text can be empirically modeled as a zero-mean Gaussian distribution. Motivated by the findings, we",
    "link": "http://arxiv.org/abs/2401.02347",
    "context": "Title: Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training. (arXiv:2401.02347v1 [cs.CV])\nAbstract: Image captioning aims at generating descriptive and meaningful textual descriptions of images, enabling a broad range of vision-language applications. Prior works have demonstrated that harnessing the power of Contrastive Image Language Pre-training (CLIP) offers a promising approach to achieving zero-shot captioning, eliminating the need for expensive caption annotations. However, the widely observed modality gap in the latent space of CLIP harms the performance of zero-shot captioning by breaking the alignment between paired image-text features. To address this issue, we conduct an analysis on the CLIP latent space which leads to two findings. Firstly, we observe that the CLIP's visual feature of image subregions can achieve closer proximity to the paired caption due to the inherent information loss in text descriptions. In addition, we show that the modality gap between a paired image-text can be empirically modeled as a zero-mean Gaussian distribution. Motivated by the findings, we",
    "path": "papers/24/01/2401.02347.json",
    "total_tokens": 917,
    "translated_title": "通过纯文本训练挖掘细粒度图像-文本对齐进行零样本字幕生成",
    "translated_abstract": "图像字幕的目标是生成对图像进行描述的有意义的文本描述，从而实现广泛的视觉-语言应用。先前的研究表明，利用对比图像语言预训练（CLIP）的力量可以有望实现零样本字幕生成，消除了昂贵的字幕注释的需求。然而，CLIP潜在空间中普遍存在的模态差距破坏了图像-文本特征之间的对齐，从而影响了零样本字幕生成的性能。为了解决这个问题，我们对CLIP潜在空间进行了分析，得出了两个发现。首先，我们观察到，由于文本描述中固有的信息损失，CLIP图像子区域的视觉特征可以更接近于配对的字幕。另外，我们展示了配对的图像-文本之间的模态差距可以经验性地建模为一个零均值的高斯分布。受到这些发现的启发，我们...",
    "tldr": "通过分析CLIP潜在空间，我们发现CLIP的视觉特征可以更接近于配对的字幕，而图像-文本之间的模态差距可以经验性地建模为一个零均值的高斯分布。",
    "en_tdlr": "By analyzing the CLIP latent space, we find that CLIP's visual features can be closer to paired captions, and the modality gap between image-text pairs can be empirically modeled as a zero-mean Gaussian distribution."
}