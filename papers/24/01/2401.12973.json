{
    "title": "In-Context Language Learning: Architectures and Algorithms. (arXiv:2401.12973v2 [cs.CL] UPDATED)",
    "abstract": "Large-scale neural language models exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input. Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall. There remains a significant gap between these model problems and the \"real\" ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. We focus on in-context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models (including seve",
    "link": "http://arxiv.org/abs/2401.12973",
    "context": "Title: In-Context Language Learning: Architectures and Algorithms. (arXiv:2401.12973v2 [cs.CL] UPDATED)\nAbstract: Large-scale neural language models exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input. Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall. There remains a significant gap between these model problems and the \"real\" ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. We focus on in-context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models (including seve",
    "path": "papers/24/01/2401.12973.json",
    "total_tokens": 997,
    "translated_title": "上下文语言学习：架构与算法",
    "translated_abstract": "大规模神经语言模型展现了在上下文学习中令人惊叹的能力：它们能够从输入的数据集中推断出新的函数。目前，我们对于上下文学习何时以及如何发生的了解主要来自于在极其简单的学习问题上训练的语言模型，例如线性回归和关联记忆。然而，这些模型问题与在大型文本语料库上训练的语言模型展现的“真正”上下文学习之间存在显著差距，后者不仅涉及检索和函数近似，还包括了自由生成语言和其他结构化输出。本文通过研究一个被称为上下文语言学习（ICLL）的新型问题家族，来探讨上下文学习。在ICLL中，语言模型被呈现一组来自形式语言的字符串，并需要生成与该语言相同的其他字符串。我们重点研究了通过随机有限自动机生成的正则语言的上下文学习。我们评估了多种神经序列模型（包括几种常用的模型）。",
    "tldr": "本文通过研究一个新的问题家族——上下文语言学习（ICLL），探讨了大规模神经语言模型在上下文学习中的能力。在ICLL中，模型通过生成与给定形式语言相同的字符串来进行上下文学习。研究结果对于理解真实场景中的上下文学习以及神经语言模型的发展具有重要意义。",
    "en_tdlr": "This paper explores the ability of large-scale neural language models in in-context learning through studying a new family of problems called in-context language learning (ICLL). In ICLL, models learn to generate strings from a given formal language, which has implications for understanding real-world in-context learning and the development of neural language models."
}