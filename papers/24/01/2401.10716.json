{
    "title": "Structured Code Representations Enable Data-Efficient Adaptation of Code Language Models. (arXiv:2401.10716v1 [cs.CL])",
    "abstract": "Current language models tailored for code tasks often adopt the pre-training-then-fine-tuning paradigm from natural language processing, modeling source code as plain text. This approach, however, overlooks the unambiguous structures inherent in programming languages. In this work, we explore data-efficient adaptation of pre-trained code models by further pre-training and fine-tuning them with program structures. Specifically, we represent programs as parse trees -- also known as concrete syntax trees (CSTs) -- and adapt pre-trained models on serialized CSTs. Although the models that we adapt have been pre-trained only on the surface form of programs, we find that a small amount of continual pre-training and fine-tuning on CSTs without changing the model architecture yields improvements over the baseline approach across various code tasks. The improvements are found to be particularly significant when there are limited training examples, demonstrating the effectiveness of integrating p",
    "link": "http://arxiv.org/abs/2401.10716",
    "context": "Title: Structured Code Representations Enable Data-Efficient Adaptation of Code Language Models. (arXiv:2401.10716v1 [cs.CL])\nAbstract: Current language models tailored for code tasks often adopt the pre-training-then-fine-tuning paradigm from natural language processing, modeling source code as plain text. This approach, however, overlooks the unambiguous structures inherent in programming languages. In this work, we explore data-efficient adaptation of pre-trained code models by further pre-training and fine-tuning them with program structures. Specifically, we represent programs as parse trees -- also known as concrete syntax trees (CSTs) -- and adapt pre-trained models on serialized CSTs. Although the models that we adapt have been pre-trained only on the surface form of programs, we find that a small amount of continual pre-training and fine-tuning on CSTs without changing the model architecture yields improvements over the baseline approach across various code tasks. The improvements are found to be particularly significant when there are limited training examples, demonstrating the effectiveness of integrating p",
    "path": "papers/24/01/2401.10716.json",
    "total_tokens": 852,
    "translated_title": "结构化代码表示使得代码语言模型的数据效率适应变得可能",
    "translated_abstract": "目前为代码任务定制的语言模型常常采用自然语言处理中的预训练-微调范式，将源代码建模为纯文本。然而，这种方法忽视了编程语言中固有的明确结构。在这项工作中，我们通过进一步预训练和微调，探索了对预训练的代码模型进行数据效率适应的方法，将程序表示为解析树（也称为具体语法树），并在序列化的解析树上进行预训练和微调。尽管我们适应的模型仅在程序的表面形式上进行了预训练，但我们发现在不改变模型架构的情况下，在解析树上进行少量的连续预训练和微调可以在各种代码任务上改进基线方法。当训练样本有限时，这种改进尤为显著，证明了整合程序结构的有效性。",
    "tldr": "本研究探索了使用结构化的代码表示来提高代码语言模型的数据效率适应性，通过使用解析树进行预训练和微调，即使只对表面形式进行预训练，也能在各种代码任务上取得显著改进。"
}