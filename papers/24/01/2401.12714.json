{
    "title": "Evaluation of large language models for assessing code maintainability. (arXiv:2401.12714v1 [cs.SE])",
    "abstract": "Increased availability of open-source software repositories and recent advances in code analysis using large language models (LLMs) has triggered a wave of new work to automate software engineering tasks that were previously very difficult to automate. In this paper, we investigate a recent line of work that hypothesises that comparing the probability of code generated by LLMs with the probability the current code would have had can indicate potential quality problems. We investigate the association between the cross-entropy of code generated by ten different models (based on GPT2 and Llama2) and the following quality aspects: readability, understandability, complexity, modularisation, and overall maintainability assessed by experts and available in an benchmark dataset. Our results show that, controlling for the number of logical lines of codes (LLOC), cross-entropy computed by LLMs is indeed a predictor of maintainability on a class level (the higher the cross-entropy the lower the m",
    "link": "http://arxiv.org/abs/2401.12714",
    "context": "Title: Evaluation of large language models for assessing code maintainability. (arXiv:2401.12714v1 [cs.SE])\nAbstract: Increased availability of open-source software repositories and recent advances in code analysis using large language models (LLMs) has triggered a wave of new work to automate software engineering tasks that were previously very difficult to automate. In this paper, we investigate a recent line of work that hypothesises that comparing the probability of code generated by LLMs with the probability the current code would have had can indicate potential quality problems. We investigate the association between the cross-entropy of code generated by ten different models (based on GPT2 and Llama2) and the following quality aspects: readability, understandability, complexity, modularisation, and overall maintainability assessed by experts and available in an benchmark dataset. Our results show that, controlling for the number of logical lines of codes (LLOC), cross-entropy computed by LLMs is indeed a predictor of maintainability on a class level (the higher the cross-entropy the lower the m",
    "path": "papers/24/01/2401.12714.json",
    "total_tokens": 883,
    "translated_title": "评估大规模语言模型用于评估代码可维护性",
    "translated_abstract": "开源软件仓库的可用性增加以及利用大规模语言模型（LLMs）进行代码分析的最新进展已经引发了一系列尝试自动化之前非常难以自动化的软件工程任务的新工作。本文研究了最近的一项工作，假设通过比较LLMs生成的代码的概率与当前代码可能具有的概率可以指示潜在的质量问题。我们调查了十种不同模型（基于GPT2和Llama2）生成的代码的交叉熵与可读性、可理解性、复杂性、模块化以及由专家评估并在基准数据集中可用的总体可维护性之间的关联。我们的结果表明，在控制逻辑代码行数（LLOC）的情况下，由LLMs计算得到的交叉熵确实是类级别上可维护性的预测因子（交叉熵越高，可维护性越低）。",
    "tldr": "本文评估大规模语言模型用于评估代码可维护性，发现通过比较生成的代码的概率与当前代码可能具有的概率可以指示潜在的质量问题，且交叉熵是可维护性的预测因子。",
    "en_tdlr": "This paper evaluates large language models for assessing code maintainability and finds that comparing the probability of generated code with the probability of current code can indicate potential quality problems, and cross-entropy is a predictor of maintainability."
}