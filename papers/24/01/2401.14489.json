{
    "title": "The Case for Co-Designing Model Architectures with Hardware. (arXiv:2401.14489v1 [cs.DC])",
    "abstract": "While GPUs are responsible for training the vast majority of state-of-the-art deep learning models, the implications of their architecture are often overlooked when designing new deep learning (DL) models. As a consequence, modifying a DL model to be more amenable to the target hardware can significantly improve the runtime performance of DL training and inference. In this paper, we provide a set of guidelines for users to maximize the runtime performance of their transformer models. These guidelines have been created by carefully considering the impact of various model hyperparameters controlling model shape on the efficiency of the underlying computation kernels executed on the GPU. We find the throughput of models with efficient model shapes is up to 39\\% higher while preserving accuracy compared to models with a similar number of parameters but with unoptimized shapes.",
    "link": "http://arxiv.org/abs/2401.14489",
    "context": "Title: The Case for Co-Designing Model Architectures with Hardware. (arXiv:2401.14489v1 [cs.DC])\nAbstract: While GPUs are responsible for training the vast majority of state-of-the-art deep learning models, the implications of their architecture are often overlooked when designing new deep learning (DL) models. As a consequence, modifying a DL model to be more amenable to the target hardware can significantly improve the runtime performance of DL training and inference. In this paper, we provide a set of guidelines for users to maximize the runtime performance of their transformer models. These guidelines have been created by carefully considering the impact of various model hyperparameters controlling model shape on the efficiency of the underlying computation kernels executed on the GPU. We find the throughput of models with efficient model shapes is up to 39\\% higher while preserving accuracy compared to models with a similar number of parameters but with unoptimized shapes.",
    "path": "papers/24/01/2401.14489.json",
    "total_tokens": 850,
    "translated_title": "与硬件共同设计模型架构的理由",
    "translated_abstract": "虽然GPU负责训练大部分最先进的深度学习模型，但在设计新的深度学习模型时往往忽视了其架构的影响。因此，将深度学习模型修改为更适合目标硬件可以显著提高深度学习训练和推理的运行时性能。本文提供了一组指南，用于使用户最大程度地提高他们的变换器模型的运行时性能。这些指南是通过仔细考虑控制模型形状的各种模型超参数对GPU上执行的底层计算内核的效率的影响而创建的。我们发现具有高效模型形状的模型的吞吐量比具有相似参数数量但形状未经优化的模型高出39％，同时保持准确性。",
    "tldr": "本文提供了一组指南，通过考虑模型超参数对GPU上执行的计算内核的效率的影响，来最大化变换器模型的运行时性能。相比于具有相似参数数量但形状未经优化的模型，使用高效模型形状的模型可以提高39%的吞吐量且保持准确性。",
    "en_tdlr": "This paper provides a set of guidelines to maximize the runtime performance of transformer models by considering the impact of model hyperparameters on the efficiency of computation kernels executed on the GPU. Using efficient model shapes can increase throughput by 39% while maintaining accuracy compared to models with similar parameter counts but unoptimized shapes."
}