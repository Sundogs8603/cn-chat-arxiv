{
    "title": "Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models. (arXiv:2401.05861v1 [cs.CL])",
    "abstract": "The training paradigm for machine translation has gradually shifted, from learning neural machine translation (NMT) models with extensive parallel corpora to instruction finetuning on pretrained multilingual large language models (LLMs) with high-quality translation pairs. In this paper, we focus on boosting the many-to-many multilingual translation performance of LLMs with an emphasis on zero-shot translation directions. We demonstrate that prompt strategies adopted during instruction finetuning are crucial to zero-shot translation performance and introduce a cross-lingual consistency regularization, XConST, to bridge the representation gap among different languages and improve zero-shot translation performance. XConST is not a new method, but a version of CrossConST (Gao et al., 2023a) adapted for multilingual finetuning on LLMs with translation instructions. Experimental results on ALMA (Xu et al., 2023) and LLaMA-2 (Touvron et al., 2023) show that our approach consistently improves",
    "link": "http://arxiv.org/abs/2401.05861",
    "context": "Title: Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models. (arXiv:2401.05861v1 [cs.CL])\nAbstract: The training paradigm for machine translation has gradually shifted, from learning neural machine translation (NMT) models with extensive parallel corpora to instruction finetuning on pretrained multilingual large language models (LLMs) with high-quality translation pairs. In this paper, we focus on boosting the many-to-many multilingual translation performance of LLMs with an emphasis on zero-shot translation directions. We demonstrate that prompt strategies adopted during instruction finetuning are crucial to zero-shot translation performance and introduce a cross-lingual consistency regularization, XConST, to bridge the representation gap among different languages and improve zero-shot translation performance. XConST is not a new method, but a version of CrossConST (Gao et al., 2023a) adapted for multilingual finetuning on LLMs with translation instructions. Experimental results on ALMA (Xu et al., 2023) and LLaMA-2 (Touvron et al., 2023) show that our approach consistently improves",
    "path": "papers/24/01/2401.05861.json",
    "total_tokens": 994,
    "translated_title": "以大型语言模型为基础，提升多对多多语言机器翻译能力的研究",
    "translated_abstract": "机器翻译的训练范式逐渐从使用大量平行语料库训练神经机器翻译（NMT）模型，转变为在预训练的多语言大型语言模型（LLM）上进行指令微调，并利用高质量翻译对。本文着重于提升LLMs在多对多多语言翻译性能上的表现，尤其是零翻译方向。我们证明了在指令微调期间采用的提示策略对于零翻译性能至关重要，并引入了跨语言一致性正则化XConST来弥合不同语言之间的表示差距，从而改善零翻译性能。XConST并不是一种新方法，而是CrossConST（Gao et al., 2023a）在LLMs上适配翻译指令多语言微调的版本。在ALMA（Xu et al., 2023）和LLaMA-2（Touvron et al., 2023）上的实验结果表明，我们的方法不断改进了性能。",
    "tldr": "本文旨在提升基于大型语言模型的多对多多语言机器翻译能力，尤其是零翻译方向。通过引入跨语言一致性正则化XConST，并采用适当的提示策略，我们改善了零翻译性能，并在实验中得到了一致的改进。",
    "en_tdlr": "This paper aims to enhance the many-to-many multilingual machine translation performance based on large language models, with a focus on zero-shot translation directions. By introducing cross-lingual consistency regularization XConST and employing suitable prompt strategies, we improve zero-shot translation performance and achieve consistent improvements in experiments."
}