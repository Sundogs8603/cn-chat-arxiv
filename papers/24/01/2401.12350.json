{
    "title": "Scaling Up Quantization-Aware Neural Architecture Search for Efficient Deep Learning on the Edge. (arXiv:2401.12350v1 [cs.CV])",
    "abstract": "Neural Architecture Search (NAS) has become the de-facto approach for designing accurate and efficient networks for edge devices. Since models are typically quantized for edge deployment, recent work has investigated quantization-aware NAS (QA-NAS) to search for highly accurate and efficient quantized models. However, existing QA-NAS approaches, particularly few-bit mixed-precision (FB-MP) methods, do not scale to larger tasks. Consequently, QA-NAS has mostly been limited to low-scale tasks and tiny networks. In this work, we present an approach to enable QA-NAS (INT8 and FB-MP) on large-scale tasks by leveraging the block-wise formulation introduced by block-wise NAS. We demonstrate strong results for the semantic segmentation task on the Cityscapes dataset, finding FB-MP models 33% smaller and INT8 models 17.6% faster than DeepLabV3 (INT8) without compromising task performance.",
    "link": "http://arxiv.org/abs/2401.12350",
    "context": "Title: Scaling Up Quantization-Aware Neural Architecture Search for Efficient Deep Learning on the Edge. (arXiv:2401.12350v1 [cs.CV])\nAbstract: Neural Architecture Search (NAS) has become the de-facto approach for designing accurate and efficient networks for edge devices. Since models are typically quantized for edge deployment, recent work has investigated quantization-aware NAS (QA-NAS) to search for highly accurate and efficient quantized models. However, existing QA-NAS approaches, particularly few-bit mixed-precision (FB-MP) methods, do not scale to larger tasks. Consequently, QA-NAS has mostly been limited to low-scale tasks and tiny networks. In this work, we present an approach to enable QA-NAS (INT8 and FB-MP) on large-scale tasks by leveraging the block-wise formulation introduced by block-wise NAS. We demonstrate strong results for the semantic segmentation task on the Cityscapes dataset, finding FB-MP models 33% smaller and INT8 models 17.6% faster than DeepLabV3 (INT8) without compromising task performance.",
    "path": "papers/24/01/2401.12350.json",
    "total_tokens": 955,
    "translated_title": "在边缘计算中扩展量化感知神经架构搜索以进行高效的深度学习",
    "translated_abstract": "神经架构搜索（NAS）已成为边缘设备设计准确高效网络的事实标准方法。由于边缘部署通常需要对模型进行量化，因此最近的研究探索了量化感知神经架构搜索（QA-NAS）以搜索高精度高效的量化模型。但是，现有的QA-NAS方法，特别是少位混合精度（FB-MP）方法，在较大任务上无法扩展。因此，QA-NAS大多受限于低规模任务和小型网络。在这项工作中，我们提出了一种方法，通过利用block-wise NAS引入的块状形式，实现了在大规模任务上启用QA-NAS（INT8和FB-MP）。我们在Cityscapes数据集的语义分割任务中展示了强大的结果，找到了比DeepLabV3（INT8）尺寸小33%的FB-MP模型和速度快17.6%的INT8模型，而不会牺牲任务性能。",
    "tldr": "本文提出了一种在较大规模任务上实现量化感知神经架构搜索（QA-NAS）的方法，通过利用块状形式实现了在边缘计算中进行高效的深度学习。实验结果在语义分割任务上展示了相对于DeepLabV3模型更小的FB-MP模型和更快的INT8模型，而不会损害任务性能。",
    "en_tdlr": "This paper proposes a method for quantization-aware neural architecture search (QA-NAS) on larger-scale tasks, enabling efficient deep learning on the edge by leveraging a block-wise formulation. Experimental results demonstrate smaller FB-MP models and faster INT8 models compared to DeepLabV3 without compromising task performance."
}