{
    "title": "Future Impact Decomposition in Request-level Recommendations",
    "abstract": "In recommender systems, reinforcement learning solutions have shown promising results in optimizing the interaction sequence between users and the system over the long-term performance. For practical reasons, the policy's actions are typically designed as recommending a list of items to handle users' frequent and continuous browsing requests more efficiently. In this list-wise recommendation scenario, the user state is updated upon every request in the corresponding MDP formulation. However, this request-level formulation is essentially inconsistent with the user's item-level behavior. In this study, we demonstrate that an item-level optimization approach can better utilize item characteristics and optimize the policy's performance even under the request-level MDP. We support this claim by comparing the performance of standard request-level methods with the proposed item-level actor-critic framework in both simulation and online experiments. Furthermore, we show that a reward-based fut",
    "link": "https://arxiv.org/abs/2401.16108",
    "context": "Title: Future Impact Decomposition in Request-level Recommendations\nAbstract: In recommender systems, reinforcement learning solutions have shown promising results in optimizing the interaction sequence between users and the system over the long-term performance. For practical reasons, the policy's actions are typically designed as recommending a list of items to handle users' frequent and continuous browsing requests more efficiently. In this list-wise recommendation scenario, the user state is updated upon every request in the corresponding MDP formulation. However, this request-level formulation is essentially inconsistent with the user's item-level behavior. In this study, we demonstrate that an item-level optimization approach can better utilize item characteristics and optimize the policy's performance even under the request-level MDP. We support this claim by comparing the performance of standard request-level methods with the proposed item-level actor-critic framework in both simulation and online experiments. Furthermore, we show that a reward-based fut",
    "path": "papers/24/01/2401.16108.json",
    "total_tokens": 843,
    "translated_title": "请求级别推荐中的未来影响分解",
    "translated_abstract": "在推荐系统中，强化学习解决方案在优化用户和系统之间的交互序列以提高长期性能方面显示出有希望的结果。出于实际原因，策略的动作通常被设计为推荐一组物品以更高效地处理用户的频繁和连续的浏览请求。在这种列表式推荐场景中，用户状态在相应的MDP（马尔可夫决策过程）表述中的每个请求上都会更新。然而，这种请求级别的表述与用户的物品级别行为实质上是不一致的。在这项研究中，我们证明了在请求级别MDP下，基于物品级别的优化方法可以更好地利用物品特性并优化策略的性能。我们通过比较标准请求级别方法和提出的基于物品级别的演员-评论家框架在模拟和在线实验中的性能来支持这一观点。",
    "tldr": "在请求级别的推荐系统中，我们通过比较标准方法和基于物品级别的演员-评论家框架在模拟和在线实验中的性能，证明了基于物品级别的优化方法可以更好地利用物品特性并优化策略的性能。",
    "en_tdlr": "In request-level recommender systems, we show that an item-level optimization approach can better utilize item characteristics and optimize the policy's performance, as demonstrated by comparing the performance of standard methods with the proposed item-level actor-critic framework in simulation and online experiments."
}