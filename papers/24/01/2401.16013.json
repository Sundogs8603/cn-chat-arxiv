{
    "title": "SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning",
    "abstract": "In recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling methods that handle complex image observations, train in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that the particular implementation details of these algorithms are often just as important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge to widespread adoption of robotic RL, as well as further development of robotic RL methods, is the comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely-adopted robot, and a number of challenging example task",
    "link": "https://arxiv.org/abs/2401.16013",
    "context": "Title: SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nAbstract: In recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling methods that handle complex image observations, train in the real world, and incorporate auxiliary data, such as demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is acknowledged among practitioners that the particular implementation details of these algorithms are often just as important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge to widespread adoption of robotic RL, as well as further development of robotic RL methods, is the comparative inaccessibility of such methods. To address this challenge, we developed a carefully implemented library containing a sample efficient off-policy deep RL method, together with methods for computing rewards and resetting the environment, a high-quality controller for a widely-adopted robot, and a number of challenging example task",
    "path": "papers/24/01/2401.16013.json",
    "total_tokens": 965,
    "translated_title": "SERL: 用于样本高效的机器人强化学习的软件套件",
    "translated_abstract": "近年来，在机器人强化学习领域取得了显著进展，使得可以处理复杂的图像观察，实际训练，并结合辅助数据（如示范和先前经验）。然而，尽管取得了这些进展，机器人强化学习仍然难以使用。从实践者中认识到，这些算法的具体实现细节对性能的影响常常与算法选择同样重要（如果不是更重要）。我们认为，机器人强化学习被广泛采用以及进一步发展机器人强化学习方法的一个重要挑战是这些方法的相对难以获取性。为了解决这个挑战，我们开发了一个精心实现的库，其中包含了一种高效样本离线深度强化学习方法，以及计算奖励和重置环境的方法，针对广泛采用的机器人的高质量控制器，以及一些具有挑战性的示例任务。",
    "tldr": "这个论文介绍了SERL软件套件，它是一个用于样本高效的机器人强化学习的库。该库包含了一个离线深度强化学习方法、计算奖励和重置环境的方法，高质量的机器人控制器，以及一些具有挑战性的示例任务。这个软件套件的目标是解决机器人强化学习的难以使用和获取性的挑战。",
    "en_tdlr": "This paper introduces SERL, a software suite for sample-efficient robotic reinforcement learning. The suite includes an offline deep RL method, reward computation and environment resetting methods, a high-quality robot controller, and challenging example tasks. The goal of this software suite is to address the challenges of usability and accessibility in robotic RL."
}