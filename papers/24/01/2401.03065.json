{
    "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. (arXiv:2401.03065v1 [cs.SE])",
    "abstract": "We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to ",
    "link": "http://arxiv.org/abs/2401.03065",
    "context": "Title: CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. (arXiv:2401.03065v1 [cs.SE])\nAbstract: We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to ",
    "path": "papers/24/01/2401.03065.json",
    "total_tokens": 903,
    "translated_title": "CRUXEval: 一个代码推理、理解和执行的基准测试",
    "translated_abstract": "我们提出了CRUXEval（代码推理、理解和执行评估），一个包含800个Python函数（3-13行）的基准测试。每个函数都有一个输入输出对，可以进行两个自然任务：输入预测和输出预测。首先，我们提出了一个通用的生成执行基准测试的方法，可以用来创建未来变种的基准测试。其次，我们在我们的基准测试中评估了二十个代码模型，并发现许多最近在HumanEval上得分高的模型在我们的基准测试上没有取得相同的改进。第三，我们展示了简单的CoT和微调方案可以改善在我们的基准测试上的性能，但仍然远未解决问题。最佳设置，使用CoT的GPT-4，在输入和输出预测上的pass@1分别达到了75%和81%。相比之下，Code Llama 34B在输入和输出预测上的pass@1分别为50%和46%，突显了开源模型和闭源模型之间的差距。",
    "tldr": "CRUXEval是一个包含800个Python函数的代码推理、理解和执行的基准测试。通过评估二十个代码模型，发现许多在HumanEval上得分高的模型在该基准测试上没有相同的改进。使用CoT的GPT-4展现了最佳性能，但仍未解决问题。与开源模型相比，闭源模型的性能差距更大。"
}