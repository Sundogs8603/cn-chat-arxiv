{
    "title": "Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender. (arXiv:2401.06561v1 [cs.CL])",
    "abstract": "Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreaks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis Prompting (IAPrompt). The principle behind is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IAPrompt could consistently and significantly reduce the harmfulness in response (averagely -46.5% attack success rate) and maintain the general helpfulness. Further analyses present some insights into how our method works. To facilitate reproducibility, We release our code and scripts at: https://github.com/alph",
    "link": "http://arxiv.org/abs/2401.06561",
    "context": "Title: Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender. (arXiv:2401.06561v1 [cs.CL])\nAbstract: Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreaks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis Prompting (IAPrompt). The principle behind is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IAPrompt could consistently and significantly reduce the harmfulness in response (averagely -46.5% attack success rate) and maintain the general helpfulness. Further analyses present some insights into how our method works. To facilitate reproducibility, We release our code and scripts at: https://github.com/alph",
    "path": "papers/24/01/2401.06561.json",
    "total_tokens": 998,
    "translated_title": "Intention Analysis Prompting使得大型语言模型成为良好的越狱防御者",
    "translated_abstract": "在面对隐蔽和复杂的越狱攻击时，将大型语言模型(LLMs)与人类价值观保持一致是一项极具挑战性的任务。在本研究中，我们提出了一种简单但非常有效的防御策略，即Intention Analysis Prompting（IAPrompt）。其原理是通过两个阶段的过程触发LLMs的内在自我纠正和改进能力：1）基本意图分析，2）与政策一致的响应。值得注意的是，IAPrompt是一种仅推断的方法，因此可以提高LLMs的安全性而不损害其有用性。在Vicuna、ChatGLM、MPT、DeepSeek和GPT-3.5上进行的广泛实验表明，IAPrompt能够持续且显著地减少响应中的有害行为（平均攻击成功率下降46.5%），同时保持整体有用性。进一步的分析揭示了我们方法的一些见解。为了保证可重复性，我们在https://github.com/alph上发布了我们的代码和脚本。",
    "tldr": "本研究提出了一种名为Intention Analysis Prompting (IAPrompt)的方法，通过触发大型语言模型（LLMs）的自我纠正和改进能力来防御越狱攻击。实验证明，该方法能够显著减少响应中的有害行为并保持整体有用性。"
}