{
    "title": "Learning Universal Predictors. (arXiv:2401.14953v1 [cs.LG])",
    "abstract": "Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data. Broad exposure to different tasks leads to versatile representations enabling general problem solving. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal predicti",
    "link": "http://arxiv.org/abs/2401.14953",
    "context": "Title: Learning Universal Predictors. (arXiv:2401.14953v1 [cs.LG])\nAbstract: Meta-learning has emerged as a powerful approach to train neural networks to learn new tasks quickly from limited data. Broad exposure to different tasks leads to versatile representations enabling general problem solving. But, what are the limits of meta-learning? In this work, we explore the potential of amortizing the most powerful universal predictor, namely Solomonoff Induction (SI), into neural networks via leveraging meta-learning to its limits. We use Universal Turing Machines (UTMs) to generate training data used to expose networks to a broad range of patterns. We provide theoretical analysis of the UTM data generation processes and meta-training protocols. We conduct comprehensive experiments with neural architectures (e.g. LSTMs, Transformers) and algorithmic data generators of varying complexity and universality. Our results suggest that UTM data is a valuable resource for meta-learning, and that it can be used to train neural networks capable of learning universal predicti",
    "path": "papers/24/01/2401.14953.json",
    "total_tokens": 890,
    "translated_title": "学习通用预测器",
    "translated_abstract": "元学习已经成为一个强大的方法，训练神经网络快速从有限的数据中学习新任务。对不同任务的广泛暴露导致了多功能表示，从而实现了通用问题解决能力。但是，元学习的限制是什么？在这项工作中，我们通过将最强大的通用预测器Solomonoff Induction（SI）通过元学习的极限进行分担，探索其潜力。我们使用万能图灵机（UTMs）生成训练数据，用于将网络暴露于广泛的模式。我们提供了关于UTM数据生成过程和元训练协议的理论分析。我们使用不同复杂性和普适性的算法数据生成器对神经网络架构（如LSTMs、Transformers）进行了全面的实验。我们的结果表明，UTM数据是元学习的宝贵资源，可以用来训练能够学习通用预测的神经网络。",
    "tldr": "本论文探索了将Solomonoff Induction（SI）引入神经网络中的潜力，并使用万能图灵机（UTMs）生成的数据来进行元学习，研究结果表明UTM数据是元学习的有价值资源，可以用来训练能够学习通用预测的神经网络。"
}