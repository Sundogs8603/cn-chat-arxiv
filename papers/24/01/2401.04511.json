{
    "title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement. (arXiv:2401.04511v1 [eess.AS])",
    "abstract": "The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our",
    "link": "http://arxiv.org/abs/2401.04511",
    "context": "Title: Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement. (arXiv:2401.04511v1 [eess.AS])\nAbstract: The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our",
    "path": "papers/24/01/2401.04511.json",
    "total_tokens": 930,
    "translated_title": "零样本音频到音频情感转移与说话人分离",
    "translated_abstract": "音频到音频的风格转移问题涉及将源音频的风格特征替换为目标音频的风格特征，同时保留源音频的与内容相关的属性。本文提出了一种高效的方法，称为零样本情感风格转移（ZEST），它允许将给定源音频中的情感内容与嵌入在目标音频中的情感内容进行转移，同时保留源音频的说话人和语音内容。所提出的系统基于将语音分解为语义标记、说话人表示和情感嵌入。利用这些因素，我们提出了一个框架来重建给定语音信号的音高轮廓，并训练一个解码器来重建语音信号。该模型使用基于自监督的重构损失进行训练。在转换过程中，情感嵌入仅从目标音频中得出，而其他因素均来自源音频。",
    "tldr": "本文提出了一种名为ZEST的零样本情感风格转移方法，允许将给定源音频中的情感内容与目标音频中的情感内容进行转移，同时保留源音频的说话人和语音内容。通过分解语音为语义标记、说话人表示和情感嵌入，并利用重构模型进行训练，实现了高效的转换过程。"
}