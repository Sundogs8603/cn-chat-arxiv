{
    "title": "Do deep neural networks utilize the weight space efficiently?. (arXiv:2401.16438v1 [cs.LG])",
    "abstract": "Deep learning models like Transformers and Convolutional Neural Networks (CNNs) have revolutionized various domains, but their parameter-intensive nature hampers deployment in resource-constrained settings. In this paper, we introduce a novel concept utilizes column space and row space of weight matrices, which allows for a substantial reduction in model parameters without compromising performance. Leveraging this paradigm, we achieve parameter-efficient deep learning models.. Our approach applies to both Bottleneck and Attention layers, effectively halving the parameters while incurring only minor performance degradation. Extensive experiments conducted on the ImageNet dataset with ViT and ResNet50 demonstrate the effectiveness of our method, showcasing competitive performance when compared to traditional models. This approach not only addresses the pressing demand for parameter efficient deep learning solutions but also holds great promise for practical deployment in real-world scena",
    "link": "http://arxiv.org/abs/2401.16438",
    "context": "Title: Do deep neural networks utilize the weight space efficiently?. (arXiv:2401.16438v1 [cs.LG])\nAbstract: Deep learning models like Transformers and Convolutional Neural Networks (CNNs) have revolutionized various domains, but their parameter-intensive nature hampers deployment in resource-constrained settings. In this paper, we introduce a novel concept utilizes column space and row space of weight matrices, which allows for a substantial reduction in model parameters without compromising performance. Leveraging this paradigm, we achieve parameter-efficient deep learning models.. Our approach applies to both Bottleneck and Attention layers, effectively halving the parameters while incurring only minor performance degradation. Extensive experiments conducted on the ImageNet dataset with ViT and ResNet50 demonstrate the effectiveness of our method, showcasing competitive performance when compared to traditional models. This approach not only addresses the pressing demand for parameter efficient deep learning solutions but also holds great promise for practical deployment in real-world scena",
    "path": "papers/24/01/2401.16438.json",
    "total_tokens": 907,
    "translated_title": "深度神经网络是否高效利用了权重空间？",
    "translated_abstract": "深度学习模型如Transformer和卷积神经网络（CNN）已经在各个领域引起了革命，但是它们参数密集的特性限制了在资源有限的情况下的应用。在本文中，我们引入一种新的概念，利用权重矩阵的列空间和行空间，可以大幅减少模型参数而不影响性能。利用这种范式，我们实现了参数高效的深度学习模型。我们的方法适用于瓶颈层和注意力层，可以将参数减半，仅带来轻微的性能降低。我们在ImageNet数据集上使用ViT和ResNet50进行了大量实验，证明了我们方法的有效性，在与传统模型的比较中展示了竞争性的性能。这种方法不仅解决了对参数高效的深度学习解决方案的紧迫需求，而且在真实场景的实际部署中具有巨大的潜力。",
    "tldr": "该论文介绍了一种利用权重矩阵的列空间和行空间的新概念，可以大幅减少深度学习模型的参数而不影响性能。实验证明该方法能够在资源有限的情况下实现参数高效的深度学习模型，并在ImageNet数据集上展现了竞争性的性能。",
    "en_tdlr": "This paper introduces a novel concept that utilizes the column space and row space of weight matrices to substantially reduce the parameters of deep learning models without compromising performance. The experiments demonstrate the effectiveness of this method in achieving parameter-efficient deep learning models and showcase competitive performance on the ImageNet dataset."
}