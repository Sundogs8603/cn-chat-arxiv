{
    "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. (arXiv:2401.06066v1 [cs.CL])",
    "abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly",
    "link": "http://arxiv.org/abs/2401.06066",
    "context": "Title: DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. (arXiv:2401.06066v1 [cs.CL])\nAbstract: In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly",
    "path": "papers/24/01/2401.06066.json",
    "total_tokens": 928,
    "translated_title": "DeepSeekMoE: 迈向混合专家语言模型的终极专家特化",
    "translated_abstract": "在大语言模型时代，混合专家（MoE）是一种在扩大模型参数时管理计算成本的有前途的架构。然而，传统的MoE架构如GShard在确保专家专业化方面面临挑战，即每个专家获取非重叠和专注的知识。为此，我们提出了DeepSeekMoE架构，以实现终极的专家特化。其包含了两个主要策略：（1）将专家细分为$mN$个，并从中激活$mK$个，以实现激活专家的更灵活组合；（2）将$K_s$个专家独立出来作为共享专家，旨在捕捉共同知识并减少路由专家中的冗余。从2B参数的起步规模开始，我们证明DeepSeekMoE 2B的性能与拥有1.5倍专家参数和计算的GShard 2.9B相当。此外，DeepSeekMoE 2B几乎...",
    "tldr": "DeepSeekMoE架构是一种面向混合专家语言模型的架构，通过细分专家和激活机制的改进来实现专家的特化与组合，并能够在较小规模的参数下取得与传统架构相当的性能。",
    "en_tdlr": "DeepSeekMoE is an architecture for mixture-of-experts language models that achieves expert specialization and combination through improved fine-grained segmentation and activation mechanism, and achieves comparable performance with conventional architectures at a smaller scale of parameters."
}