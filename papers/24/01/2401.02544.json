{
    "title": "Hyperparameter Estimation for Sparse Bayesian Learning Models. (arXiv:2401.02544v1 [cs.LG])",
    "abstract": "Sparse Bayesian Learning (SBL) models are extensively used in signal processing and machine learning for promoting sparsity through hierarchical priors. The hyperparameters in SBL models are crucial for the model's performance, but they are often difficult to estimate due to the non-convexity and the high-dimensionality of the associated objective function. This paper presents a comprehensive framework for hyperparameter estimation in SBL models, encompassing well-known algorithms such as the expectation-maximization (EM), MacKay, and convex bounding (CB) algorithms. These algorithms are cohesively interpreted within an alternating minimization and linearization (AML) paradigm, distinguished by their unique linearized surrogate functions. Additionally, a novel algorithm within the AML framework is introduced, showing enhanced efficiency, especially under low signal noise ratios. This is further improved by a new alternating minimization and quadratic approximation (AMQ) paradigm, which",
    "link": "http://arxiv.org/abs/2401.02544",
    "context": "Title: Hyperparameter Estimation for Sparse Bayesian Learning Models. (arXiv:2401.02544v1 [cs.LG])\nAbstract: Sparse Bayesian Learning (SBL) models are extensively used in signal processing and machine learning for promoting sparsity through hierarchical priors. The hyperparameters in SBL models are crucial for the model's performance, but they are often difficult to estimate due to the non-convexity and the high-dimensionality of the associated objective function. This paper presents a comprehensive framework for hyperparameter estimation in SBL models, encompassing well-known algorithms such as the expectation-maximization (EM), MacKay, and convex bounding (CB) algorithms. These algorithms are cohesively interpreted within an alternating minimization and linearization (AML) paradigm, distinguished by their unique linearized surrogate functions. Additionally, a novel algorithm within the AML framework is introduced, showing enhanced efficiency, especially under low signal noise ratios. This is further improved by a new alternating minimization and quadratic approximation (AMQ) paradigm, which",
    "path": "papers/24/01/2401.02544.json",
    "total_tokens": 887,
    "translated_title": "SBL模型的超参数估计",
    "translated_abstract": "稀疏贝叶斯学习（SBL）模型广泛应用于信号处理和机器学习，通过层次先验来促进稀疏性。SBL模型中的超参数对模型的性能至关重要，但由于相关目标函数的非凸性和高维度，往往难以估计。该论文提出了一个全面的SBL模型超参数估计框架，包括期望最大化（EM）、MacKay和凸边界（CB）算法等众所周知的算法。这些算法在交替最小化和线性化（AML）范式下得到了一致的解释，其特点是独特的线性化代理函数。此外，还引入了一种新的AML框架中的新算法，显示出增强的效率，尤其是在低信噪比情况下。这一效果通过新的交替最小化和二次逼近（AMQ）范式进一步改进，",
    "tldr": "该论文介绍了一个综合的SBL模型超参数估计框架，其中包括了期望最大化、MacKay和凸边界等算法。此外，还引入了一种新的算法，通过交替最小化和二次逼近范式，实现了更高的效率。"
}