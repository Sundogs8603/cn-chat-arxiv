{
    "title": "Navigating the OverKill in Large Language Models",
    "abstract": "Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to an over-attention of harmful words like 'kill' and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such over-attention by amplifying the difference in the model's output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the over-attention from the model via contrastive decoding. Empirical results indicate that our method has",
    "link": "https://arxiv.org/abs/2401.17633",
    "context": "Title: Navigating the OverKill in Large Language Models\nAbstract: Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to an over-attention of harmful words like 'kill' and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such over-attention by amplifying the difference in the model's output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the over-attention from the model via contrastive decoding. Empirical results indicate that our method has",
    "path": "papers/24/01/2401.17633.json",
    "total_tokens": 931,
    "translated_title": "在大型语言模型中解决过度杀伤问题的导航",
    "translated_abstract": "大型语言模型被精心调整，以既有助益又无害。然而，最近的研究指出存在潜在的过度杀伤问题，这意味着模型可能会拒绝回答无害查询。本文通过探索模型如何处理和确定查询的安全性，来研究过度杀伤的因素。我们的发现揭示了模型内部存在捷径，导致对“杀伤”等有害词语过度关注，而强调安全性的提示将加剧过度杀伤。基于这些发现，我们提出了一种无需训练且适用于各种模型的策略，名为自对比解码（Self-Contrastive Decoding，Self-CD），来缓解这一现象。首先，我们通过放大模型在回应系统提示时输出分布的差异，提取这种过度关注。然后，通过对比解码来减弱模型对这种过度关注的影响，以确定最终的下一个标记预测。实证结果表明，我们的方法具有",
    "tldr": "本研究调查了大型语言模型中过度杀伤的因素，并发现了其中存在的捷径和对有害词语的过度关注。我们提出了自对比解码（Self-CD）策略来缓解过度杀伤现象，该策略无需训练且适用于各种模型。"
}