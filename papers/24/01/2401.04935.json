{
    "title": "Learning Audio Concepts from Counterfactual Natural Language. (arXiv:2401.04935v1 [cs.MM])",
    "abstract": "Conventional audio classification relied on predefined classes, lacking the ability to learn from free-form text. Recent methods unlock learning joint audio-text embeddings from raw audio-text pairs describing audio in natural language. Despite recent advancements, there is little exploration of systematic methods to train models for recognizing sound events and sources in alternative scenarios, such as distinguishing fireworks from gunshots at outdoor events in similar situations. This study introduces causal reasoning and counterfactual analysis in the audio domain. We use counterfactual instances and include them in our model across different aspects. Our model considers acoustic characteristics and sound source information from human-annotated reference texts. To validate the effectiveness of our model, we conducted pre-training utilizing multiple audio captioning datasets. We then evaluate with several common downstream tasks, demonstrating the merits of the proposed method as one",
    "link": "http://arxiv.org/abs/2401.04935",
    "context": "Title: Learning Audio Concepts from Counterfactual Natural Language. (arXiv:2401.04935v1 [cs.MM])\nAbstract: Conventional audio classification relied on predefined classes, lacking the ability to learn from free-form text. Recent methods unlock learning joint audio-text embeddings from raw audio-text pairs describing audio in natural language. Despite recent advancements, there is little exploration of systematic methods to train models for recognizing sound events and sources in alternative scenarios, such as distinguishing fireworks from gunshots at outdoor events in similar situations. This study introduces causal reasoning and counterfactual analysis in the audio domain. We use counterfactual instances and include them in our model across different aspects. Our model considers acoustic characteristics and sound source information from human-annotated reference texts. To validate the effectiveness of our model, we conducted pre-training utilizing multiple audio captioning datasets. We then evaluate with several common downstream tasks, demonstrating the merits of the proposed method as one",
    "path": "papers/24/01/2401.04935.json",
    "total_tokens": 878,
    "translated_title": "从对抗性自然语言中学习音频概念",
    "translated_abstract": "传统的音频分类依赖于预定义的类别，缺乏从自由文本中学习的能力。最近的方法可以从描述音频的原始音频文本对中解锁学习联合音频-文本嵌入。尽管最近取得了一些进展，但对于在不同情景下训练模型以识别声音事件和来源的系统方法的探索很少，例如在类似的情况下区分室外活动中的烟火和枪声。本研究引入了音频领域的因果推理和反事实分析。我们使用反事实实例，并将它们包含在我们的模型中的不同方面。我们的模型考虑了来自人工注释参考文本的声学特征和声音来源信息。为了验证我们模型的有效性，我们进行了使用多个音频字幕数据集进行的预训练。然后我们使用几个常见的下游任务进行评估，展示了该方法的优点。",
    "tldr": "本研究通过引入因果推理和反事实分析，提出了一种能够从自由文本中学习音频概念的方法。通过使用反事实实例，并综合考虑声学特征和声音来源信息，该方法在不同情景下能够有效识别声音事件和来源。"
}