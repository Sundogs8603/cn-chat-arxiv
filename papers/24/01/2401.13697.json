{
    "title": "Toward Robust Multimodal Learning using Multimodal Foundational Models. (arXiv:2401.13697v1 [cs.CV])",
    "abstract": "Existing multimodal sentiment analysis tasks are highly rely on the assumption that the training and test sets are complete multimodal data, while this assumption can be difficult to hold: the multimodal data are often incomplete in real-world scenarios. Therefore, a robust multimodal model in scenarios with randomly missing modalities is highly preferred. Recently, CLIP-based multimodal foundational models have demonstrated impressive performance on numerous multimodal tasks by learning the aligned cross-modal semantics of image and text pairs, but the multimodal foundational models are also unable to directly address scenarios involving modality absence. To alleviate this issue, we propose a simple and effective framework, namely TRML, Toward Robust Multimodal Learning using Multimodal Foundational Models. TRML employs generated virtual modalities to replace missing modalities, and aligns the semantic spaces between the generated and missing modalities. Concretely, we design a missin",
    "link": "http://arxiv.org/abs/2401.13697",
    "context": "Title: Toward Robust Multimodal Learning using Multimodal Foundational Models. (arXiv:2401.13697v1 [cs.CV])\nAbstract: Existing multimodal sentiment analysis tasks are highly rely on the assumption that the training and test sets are complete multimodal data, while this assumption can be difficult to hold: the multimodal data are often incomplete in real-world scenarios. Therefore, a robust multimodal model in scenarios with randomly missing modalities is highly preferred. Recently, CLIP-based multimodal foundational models have demonstrated impressive performance on numerous multimodal tasks by learning the aligned cross-modal semantics of image and text pairs, but the multimodal foundational models are also unable to directly address scenarios involving modality absence. To alleviate this issue, we propose a simple and effective framework, namely TRML, Toward Robust Multimodal Learning using Multimodal Foundational Models. TRML employs generated virtual modalities to replace missing modalities, and aligns the semantic spaces between the generated and missing modalities. Concretely, we design a missin",
    "path": "papers/24/01/2401.13697.json",
    "total_tokens": 979,
    "translated_title": "迈向使用多模态基础模型的稳健多模态学习",
    "translated_abstract": "现有的多模态情感分析任务高度依赖于训练集和测试集是完整的多模态数据的假设，然而这个假设在现实场景中往往很难成立：多模态数据往往在现实世界中是不完整的。因此，在存在随机缺失模态的场景中，一种稳健的多模态模型将会更受欢迎。最近，基于CLIP的多模态基础模型通过学习图像和文本对的跨模态语义对齐，在众多多模态任务中展示了令人印象深刻的性能，但是这些多模态基础模型也无法直接解决涉及模态缺失的场景。为了缓解这个问题，我们提出了一个简单而有效的框架，即TRML（Toward Robust Multimodal Learning using Multimodal Foundational Models）。TRML利用生成的虚拟模态替换缺失的模态，并且对生成的模态和缺失的模态之间的语义空间进行对齐。具体来说，我们设计了一个缺失模态生成模块，可以生成缺失的模态，然后通过特征对齐模块来学习模态之间的对齐关系。",
    "tldr": "该论文提出了一种名为TRML的框架，旨在实现在随机缺失模态的情况下的稳健多模态学习。TRML利用生成的虚拟模态替换缺失的模态，并且通过对齐语义空间来解决模态缺失的问题。"
}