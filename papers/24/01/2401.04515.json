{
    "title": "Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models. (arXiv:2401.04515v1 [cs.CL])",
    "abstract": "This article investigates a zero-shot approach to hypernymy prediction using large language models (LLMs). The study employs a method based on text probability calculation, applying it to various generated prompts. The experiments demonstrate a strong correlation between the effectiveness of language model prompts and classic patterns, indicating that preliminary prompt selection can be carried out using smaller models before moving to larger ones. We also explore prompts for predicting co-hyponyms and improving hypernymy predictions by augmenting prompts with additional information through automatically identified co-hyponyms. An iterative approach is developed for predicting higher-level concepts, which further improves the quality on the BLESS dataset (MAP = 0.8).",
    "link": "http://arxiv.org/abs/2401.04515",
    "context": "Title: Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with Large Language Models. (arXiv:2401.04515v1 [cs.CL])\nAbstract: This article investigates a zero-shot approach to hypernymy prediction using large language models (LLMs). The study employs a method based on text probability calculation, applying it to various generated prompts. The experiments demonstrate a strong correlation between the effectiveness of language model prompts and classic patterns, indicating that preliminary prompt selection can be carried out using smaller models before moving to larger ones. We also explore prompts for predicting co-hyponyms and improving hypernymy predictions by augmenting prompts with additional information through automatically identified co-hyponyms. An iterative approach is developed for predicting higher-level concepts, which further improves the quality on the BLESS dataset (MAP = 0.8).",
    "path": "papers/24/01/2401.04515.json",
    "total_tokens": 935,
    "translated_title": "使用大型语言模型探索基于提示的零样本上位词预测方法",
    "translated_abstract": "本文研究了使用大型语言模型(LLMs)进行零样本上位词预测的方法。研究采用了基于文本概率计算的方法，并将其应用于各种生成的提示中。实验证明语言模型提示的有效性与经典模式之间存在着较强的相关性，这表明可以通过较小的模型进行初步的提示选择，然后再转向更大的模型。我们还探讨了用于预测共同下位词和通过自动识别的共同下位词将额外信息加入提示以改善上位词预测的提示。我们还开发了一种迭代方法来预测更高层次的概念，进一步提高了在BLESS数据集上的质量（MAP = 0.8）。",
    "tldr": "本文研究了使用大型语言模型进行零样本上位词预测的方法。实验结果表明，语言模型提示的有效性与经典模式之间存在较强的相关性，并且可以通过使用较小的模型进行初步的提示选择。此外，通过使用自动识别的共同下位词将额外信息加入提示，可以改善上位词预测的效果。作者还开发了一种迭代方法来预测更高层次的概念，并在BLESS数据集上取得了显著的质量提升（MAP = 0.8）。"
}