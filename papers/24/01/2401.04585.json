{
    "title": "Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models. (arXiv:2401.04585v1 [cs.CV])",
    "abstract": "Diffusion models have achieved great success in image generation tasks through iterative noise estimation. However, the heavy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising in accelerating the denoising process. Unfortunately, we find that due to the highly dynamic distribution of activations in different denoising steps, existing PTQ methods for diffusion models suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory, especially in low-bit cases. In this paper, we propose Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models (EDA-DM) to address the above issues. Specifically, at the calibration sample level, we select calibration samples based on the ",
    "link": "http://arxiv.org/abs/2401.04585",
    "context": "Title: Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models. (arXiv:2401.04585v1 [cs.CV])\nAbstract: Diffusion models have achieved great success in image generation tasks through iterative noise estimation. However, the heavy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising in accelerating the denoising process. Unfortunately, we find that due to the highly dynamic distribution of activations in different denoising steps, existing PTQ methods for diffusion models suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory, especially in low-bit cases. In this paper, we propose Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models (EDA-DM) to address the above issues. Specifically, at the calibration sample level, we select calibration samples based on the ",
    "path": "papers/24/01/2401.04585.json",
    "total_tokens": 898,
    "translated_title": "扩展分布对齐来实现弥散模型的后训练量化",
    "translated_abstract": "通过迭代噪声估计，扩散模型在图像生成任务中取得了巨大成功。然而，繁重的去噪过程和复杂的神经网络阻碍了它们在实际场景中的低延迟应用。量化可以有效降低模型复杂度，而后训练量化(PTQ)在加速去噪过程方面具有很高的潜力，并且不需要微调。不幸的是，我们发现由于不同去噪步骤中激活的高度动态分布，现有的扩散模型的PTQ方法在校准样本和重构输出两个层面上都存在分布不匹配的问题，导致性能远低于令人满意的水平，特别是在低位情况下。在本文中，我们提出了增强的分布对齐用于弥散模型的后训练量化(EDA-DM)来解决上述问题。具体来说，在校准样本层面，我们基于...[缺省]",
    "tldr": "本文提出了一种扩展分布对齐方法以解决后训练量化对于弥散模型的分布不匹配问题，该方法在低延迟应用中具有较高的潜力，并且能有效提升性能。",
    "en_tdlr": "This paper proposes an Enhanced Distribution Alignment method to address the distribution mismatch issue in post-training quantization for diffusion models, which has high potential in low-latency applications and can effectively improve performance."
}