{
    "title": "Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation",
    "abstract": "arXiv:2401.07382v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful crit",
    "link": "https://arxiv.org/abs/2401.07382",
    "context": "Title: Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation\nAbstract: arXiv:2401.07382v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful crit",
    "path": "papers/24/01/2401.07382.json",
    "total_tokens": 866,
    "translated_title": "超越稀疏奖励：在文本生成中通过语言模型评论增强强化学习",
    "translated_abstract": "强化学习可以将语言模型与非可微分奖励信号（如人类偏好）进行对齐。然而，其中一个主要挑战在于这些奖励信号的稀疏性 - 通常，整个输出只有一个奖励。这种奖励的稀疏性可能导致学习效率低下且不稳定。为了解决这一挑战，本文介绍了一种新颖的框架，利用大型语言模型（LLMs）的评论能力在强化学习训练过程中产生中间步骤奖励。我们的方法涉及将策略模型与评论语言模型相结合，评论语言模型负责为输出的每个部分提供细致的反馈。然后将这些反馈转化为可用于指导强化学习训练过程的标记或跨度级别奖励。我们在两种不同的设置下研究了这种方法：一种是策略模型较小且与更强大的评论者配对。",
    "tldr": "本文提出了一种新的框架，利用大型语言模型的评论能力在强化学习训练中产生中间步骤奖励，以应对稀疏奖励信号所带来的挑战。"
}