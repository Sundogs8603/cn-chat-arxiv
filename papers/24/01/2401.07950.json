{
    "title": "SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning",
    "abstract": "arXiv:2401.07950v2 Announce Type: replace  Abstract: Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciGLM, a suite of scientific language models able to conduct college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated SciInstruct, a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We fine-tuned the ChatGLM family of language models with SciInstruct, enhancing",
    "link": "https://arxiv.org/abs/2401.07950",
    "context": "Title: SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning\nAbstract: arXiv:2401.07950v2 Announce Type: replace  Abstract: Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciGLM, a suite of scientific language models able to conduct college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated SciInstruct, a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We fine-tuned the ChatGLM family of language models with SciInstruct, enhancing",
    "path": "papers/24/01/2401.07950.json",
    "total_tokens": 894,
    "translated_title": "SciGLM: 用自我反思指导注释和调整训练科学语言模型",
    "translated_abstract": "大型语言模型(LLMs)已显示出在协助科学发现方面的潜力。然而，目前LLMs在理解复杂科学概念、推导符号方程式和解决高级数值计算方面存在局限。为了弥补这些差距，我们引入了SciGLM，一套能够进行大学水平科学推理的科学语言模型。我们方法的核心是一种新颖的自我反思指导注释框架，以解决科学领域中数据稀缺挑战。该框架利用现有LLMs为未标记的科学问题生成逐步推理，随后经过自我反思的批评和修改过程。应用这一框架，我们整理了SciInstruct，这是一个涵盖物理、化学、数学和形式证明的多样化、高质量的数据集。我们利用SciInstruct对ChatGLM系列语言模型进行了微调，增强了",
    "tldr": "SciGLM引入了自我反思指导注释框架，用于弥补大型语言模型在理解复杂科学概念、推导符号方程式和解决高级数值计算方面的不足，以训练能够进行大学水平科学推理的科学语言模型。"
}