{
    "title": "Taming \"data-hungry\" reinforcement learning? Stability in continuous state-action spaces. (arXiv:2401.05233v1 [cs.LG])",
    "abstract": "We introduce a novel framework for analyzing reinforcement learning (RL) in continuous state-action spaces, and use it to prove fast rates of convergence in both off-line and on-line settings. Our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect the Bellman operator and occupation measures. We argue that these properties are satisfied in many continuous state-action Markov decision processes, and demonstrate how they arise naturally when using linear function approximation methods. Our analysis offers fresh perspectives on the roles of pessimism and optimism in off-line and on-line RL, and highlights the connection between off-line RL and transfer learning.",
    "link": "http://arxiv.org/abs/2401.05233",
    "context": "Title: Taming \"data-hungry\" reinforcement learning? Stability in continuous state-action spaces. (arXiv:2401.05233v1 [cs.LG])\nAbstract: We introduce a novel framework for analyzing reinforcement learning (RL) in continuous state-action spaces, and use it to prove fast rates of convergence in both off-line and on-line settings. Our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect the Bellman operator and occupation measures. We argue that these properties are satisfied in many continuous state-action Markov decision processes, and demonstrate how they arise naturally when using linear function approximation methods. Our analysis offers fresh perspectives on the roles of pessimism and optimism in off-line and on-line RL, and highlights the connection between off-line RL and transfer learning.",
    "path": "papers/24/01/2401.05233.json",
    "total_tokens": 957,
    "translated_title": "驯服“数据饥渴”的强化学习？在连续状态-动作空间中的稳定性",
    "translated_abstract": "我们引入了一个在连续状态-动作空间中分析强化学习的新框架，并将其用于证明离线和在线设置中的快速收敛速度。我们的分析突出了两个关键的稳定性属性，涉及值函数和/或策略变化如何影响贝尔曼算子和占据度测度。我们认为这些属性在许多连续状态-动作马尔可夫决策过程中成立，并演示了在使用线性函数逼近方法时如何自然地产生这些属性。我们的分析提供了关于离线和在线强化学习中悲观主义和乐观主义的新视角，并强调了离线强化学习和迁移学习之间的联系。",
    "tldr": "本文介绍了一个在连续状态-动作空间中分析强化学习的新框架，并证明了在离线和在线设置中具有快速收敛速度。分析发现了两个稳定性属性，与值函数和/或策略变化如何影响贝尔曼算子和占据度测度相关。这些属性在许多连续状态-动作马尔可夫决策过程中成立，并且在线性函数逼近方法下自然产生。分析还揭示了离线和在线强化学习中悲观主义和乐观主义的角色，以及离线强化学习和迁移学习之间的联系。",
    "en_tdlr": "This paper introduces a new framework for analyzing reinforcement learning in continuous state-action spaces and proves fast convergence rates in both offline and online settings. The analysis identifies two key stability properties related to changes in value functions and/or policies, and highlights their connection to the Bellman operator and occupation measures. The paper also offers insights into the roles of pessimism and optimism in offline and online reinforcement learning, and explores the link between offline RL and transfer learning."
}