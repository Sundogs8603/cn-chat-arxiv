{
    "title": "Anatomy of Neural Language Models",
    "abstract": "arXiv:2401.03797v2 Announce Type: replace  Abstract: The fields of generative AI and transfer learning have experienced remarkable advancements in recent years especially in the domain of Natural Language Processing (NLP). Transformers have been at the heart of these advancements where the cutting-edge transformer-based Language Models (LMs) have led to new state-of-the-art results in a wide spectrum of applications. While the number of research works involving neural LMs is exponentially increasing, their vast majority are high-level and far from self-contained. Consequently, a deep understanding of the literature in this area is a tough task especially in the absence of a unified mathematical framework explaining the main types of neural LMs. We address the aforementioned problem in this tutorial where the objective is to explain neural LMs in a detailed, simplified and unambiguous mathematical framework accompanied by clear graphical illustrations. Concrete examples on widely used m",
    "link": "https://arxiv.org/abs/2401.03797",
    "context": "Title: Anatomy of Neural Language Models\nAbstract: arXiv:2401.03797v2 Announce Type: replace  Abstract: The fields of generative AI and transfer learning have experienced remarkable advancements in recent years especially in the domain of Natural Language Processing (NLP). Transformers have been at the heart of these advancements where the cutting-edge transformer-based Language Models (LMs) have led to new state-of-the-art results in a wide spectrum of applications. While the number of research works involving neural LMs is exponentially increasing, their vast majority are high-level and far from self-contained. Consequently, a deep understanding of the literature in this area is a tough task especially in the absence of a unified mathematical framework explaining the main types of neural LMs. We address the aforementioned problem in this tutorial where the objective is to explain neural LMs in a detailed, simplified and unambiguous mathematical framework accompanied by clear graphical illustrations. Concrete examples on widely used m",
    "path": "papers/24/01/2401.03797.json",
    "total_tokens": 796,
    "translated_title": "神经语言模型解剖学",
    "translated_abstract": "生成式人工智能和迁移学习领域在最近几年取得了显著进展，特别是在自然语言处理（NLP）领域。变压器已经成为这些进展的核心，基于最前沿的变压器的语言模型（LMs）在广泛的应用中导致了新的最先进的结果。尽管涉及神经LMs的研究作品数量呈指数增长，但其中绝大多数是高级的，离实际操作颇远。因此，在缺乏解释主要类型神经LMs的统一数学框架的前提下，对这一领域的文献深入了解是一项艰巨的任务。我们在本教程中解决了上述问题，旨在在详细、简化和清晰的数学框架中解释神经LMs，并附有清晰的图形说明。",
    "tldr": "本教程详细、简化和清晰地解释了神经语言模型，并提供清晰的图形说明，填补了缺乏统一数学框架的现存问题。",
    "en_tdlr": "This tutorial provides a detailed, simplified, and clear explanation of neural language models with clear graphical illustrations, addressing the existing issue of lacking a unified mathematical framework."
}