{
    "title": "Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation. (arXiv:2401.02258v1 [cs.LG])",
    "abstract": "Missingness is ubiquitous in multivariate time series and poses an obstacle to reliable downstream analysis. Although recurrent network imputation achieved the SOTA, existing models do not scale to deep architectures that can potentially alleviate issues arising in complex data. Moreover, imputation carries the risk of biased estimations of the ground truth. Yet, confidence in the imputed values is always unmeasured or computed post hoc from model output. We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates missing values and their associated uncertainty in heterogeneous multivariate time series. By jointly representing feature-wise correlations and temporal dynamics, we adopt a self attention mechanism, along with an effective residual component, to achieve a deep recurrent neural network with good imputation performance and stable convergence. We also leverage self-supervised metric learning to boost performance by optimizing sample similarity. Finally, we ",
    "link": "http://arxiv.org/abs/2401.02258",
    "context": "Title: Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation. (arXiv:2401.02258v1 [cs.LG])\nAbstract: Missingness is ubiquitous in multivariate time series and poses an obstacle to reliable downstream analysis. Although recurrent network imputation achieved the SOTA, existing models do not scale to deep architectures that can potentially alleviate issues arising in complex data. Moreover, imputation carries the risk of biased estimations of the ground truth. Yet, confidence in the imputed values is always unmeasured or computed post hoc from model output. We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates missing values and their associated uncertainty in heterogeneous multivariate time series. By jointly representing feature-wise correlations and temporal dynamics, we adopt a self attention mechanism, along with an effective residual component, to achieve a deep recurrent neural network with good imputation performance and stable convergence. We also leverage self-supervised metric learning to boost performance by optimizing sample similarity. Finally, we ",
    "path": "papers/24/01/2401.02258.json",
    "total_tokens": 870,
    "translated_title": "不确定性感知的深度关注循环神经网络用于异质时间序列插补",
    "translated_abstract": "多变量时间序列中普遍存在缺失，给可靠的下游分析带来了障碍。尽管递归网络插补达到了最先进的水平，但现有模型不能扩展到可以缓解复杂数据中出现的问题的深度结构。此外，插补还存在估计地面真值偏差的风险。然而，对插补值的置信度始终是未被测量的或从模型输出后计算的。我们提出了DEep Attention Recurrent Imputation (DEARI)，它在异质多变量时间序列中同时估计缺失值及其相关的不确定性。通过联合表示特征相关性和时序动态，我们采用了自注意机制和有效的残差组件，实现了一个具有良好插补性能和稳定收敛性的深度循环神经网络。我们还利用自监督度量学习来通过优化样本相似性来提高性能。",
    "tldr": "该论文提出了鉴于复杂数据中出现的问题，能够同时估计异质多变量时间序列中缺失值及其相关不确定性的深度关注循环神经网络插补方法。"
}