{
    "title": "Learning Long Sequences in Spiking Neural Networks. (arXiv:2401.00955v1 [cs.NE])",
    "abstract": "Spiking neural networks (SNNs) take inspiration from the brain to enable energy-efficient computations. Since the advent of Transformers, SNNs have struggled to compete with artificial networks on modern sequential tasks, as they inherit limitations from recurrent neural networks (RNNs), with the added challenge of training with non-differentiable binary spiking activations. However, a recent renewed interest in efficient alternatives to Transformers has given rise to state-of-the-art recurrent architectures named state space models (SSMs). This work systematically investigates, for the first time, the intersection of state-of-the-art SSMs with SNNs for long-range sequence modelling. Results suggest that SSM-based SNNs can outperform the Transformer on all tasks of a well-established long-range sequence modelling benchmark. It is also shown that SSM-based SNNs can outperform current state-of-the-art SNNs with fewer parameters on sequential image classification. Finally, a novel feature",
    "link": "http://arxiv.org/abs/2401.00955",
    "context": "Title: Learning Long Sequences in Spiking Neural Networks. (arXiv:2401.00955v1 [cs.NE])\nAbstract: Spiking neural networks (SNNs) take inspiration from the brain to enable energy-efficient computations. Since the advent of Transformers, SNNs have struggled to compete with artificial networks on modern sequential tasks, as they inherit limitations from recurrent neural networks (RNNs), with the added challenge of training with non-differentiable binary spiking activations. However, a recent renewed interest in efficient alternatives to Transformers has given rise to state-of-the-art recurrent architectures named state space models (SSMs). This work systematically investigates, for the first time, the intersection of state-of-the-art SSMs with SNNs for long-range sequence modelling. Results suggest that SSM-based SNNs can outperform the Transformer on all tasks of a well-established long-range sequence modelling benchmark. It is also shown that SSM-based SNNs can outperform current state-of-the-art SNNs with fewer parameters on sequential image classification. Finally, a novel feature",
    "path": "papers/24/01/2401.00955.json",
    "total_tokens": 952,
    "translated_title": "在脉冲神经网络中学习长序列",
    "translated_abstract": "脉冲神经网络（SNN）受到大脑的启发，能够实现高效的计算。然而，由于遗留的循环神经网络（RNN）的限制以及训练时使用非可微的二值脉冲激活函数，SNN在现代序列任务中与人工神经网络的竞争一直存在困难。最近，对于Transformers的高效替代方案的兴趣的复苏，使得一种名为状态空间模型（SSM）的最新递归架构得以出现。本研究首次系统地研究了最新的SSM与SNN相结合在长序列建模方面的交叉点。结果表明，基于SSM的SNN在一个经典的长序列建模基准任务中能够超越Transformers的性能。同时，还展示了基于SSM的SNN在顺序图像分类上可以以更少的参数超越当前最先进的SNN。最后，提出了一种新的特征",
    "tldr": "这项研究首次系统地探索了最新的状态空间模型（SSM）与脉冲神经网络（SNN）在长序列建模方面的结合。结果表明，在经典的长序列建模任务中，基于SSM的SNN能够超越Transformers并且在顺序图像分类中以更少的参数超越当前最先进的SNN。"
}