{
    "title": "Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model",
    "abstract": "The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic s",
    "link": "https://arxiv.org/abs/2401.17868",
    "context": "Title: Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model\nAbstract: The Segment Anything Model (SAM) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, this paper introduces Conv-LoRA, a simple yet effective parameter-efficient fine-tuning approach. By integrating ultra-lightweight convolutional parameters into Low-Rank Adaptation (LoRA), Conv-LoRA can inject image-related inductive biases into the plain ViT encoder, further reinforcing SAM's local prior assumption. Notably, Conv-LoRA not only preserves SAM's extensive segmentation knowledge but also revives its capacity of learning high-level image semantics, which is constrained by SAM's foreground-background segmentation pretraining. Comprehensive experimentation across diverse benchmarks spanning multiple domains underscores Conv-LoRA's superiority in adapting SAM to real-world semantic s",
    "path": "papers/24/01/2401.17868.json",
    "total_tokens": 927,
    "translated_title": "卷积与LoRA相遇: 用于Segment Anything模型的参数高效微调",
    "translated_abstract": "Segment Anything模型（SAM）是图像分割的基础框架。在典型场景中，它展现出了非常出色的零样本泛化能力，但在应用于医学图像和遥感等专业领域时，其优势减弱。为了解决这个局限性，本文引入了Conv-LoRA，一种简单而有效的参数高效微调方法。通过将超轻量级卷积参数与低秩调整（LoRA）相结合，Conv-LoRA能够将与图像相关的归纳偏见注入普通的ViT编码器中，进一步强化SAM的局部先验假设。值得注意的是，Conv-LoRA不仅保留了SAM的广泛分割知识，还恢复了其学习高级图像语义能力，这一能力受到SAM的前景-背景分割预训练的限制。在跨越多个领域的各种基准测试中进行的全面实验强调了Conv-LoRA在将SAM适应到实际语义分割任务中的优越性。",
    "tldr": "本文介绍了一种名为Conv-LoRA的参数高效微调方法，它通过在SAM的基础上整合轻量级卷积参数和低秩调整，将图像相关的归纳偏见注入ViT编码器，以提高SAM在特定领域的分割能力和高级图像语义学习能力。",
    "en_tdlr": "This paper introduces Conv-LoRA, a parameter-efficient fine-tuning approach that integrates lightweight convolutional parameters and low-rank adaptation to inject image-related biases into the ViT encoder, enhancing SAM's segmentation capability and high-level image semantic learning in specialized domains."
}