{
    "title": "Knowledge Fusion of Large Language Models. (arXiv:2401.10491v1 [cs.CL])",
    "abstract": "While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of",
    "link": "http://arxiv.org/abs/2401.10491",
    "context": "Title: Knowledge Fusion of Large Language Models. (arXiv:2401.10491v1 [cs.CL])\nAbstract: While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the fusion of",
    "path": "papers/24/01/2401.10491.json",
    "total_tokens": 899,
    "translated_title": "大型语言模型的知识融合",
    "translated_abstract": "尽管从头开始训练大型语言模型（LLMs）可以生成具有独特功能和优势的模型，但这将带来巨大的成本，并可能导致冗余的能力。相反，一种具有成本效益和强大功能的方法是将现有的预训练LLMs合并为一个更强大的模型。然而，由于这些LLMs的不同架构，直接混合它们的权重是不切实际的。在本文中，我们引入了LLMs的知识融合的概念，旨在将现有LLMs的能力结合起来并转移到单个LLM中。通过利用源LLMs的生成分布，我们外部化它们的集体知识和独特优势，从而可能提高目标模型的能力超过任何单个源LLM的能力。我们使用具有不同架构的三个流行LLMs —— Llama-2、MPT和OpenLLaMA在各种基准和任务中验证了我们的方法。我们的研究结果证实了融合这三个LLMs的能力。",
    "tldr": "本文介绍了一种大型语言模型知识融合的方法，通过将现有预训练的语言模型合并为一个更强大的模型，从而提高目标模型的能力，验证实验结果证实了该方法的有效性。",
    "en_tdlr": "This paper presents a method for knowledge fusion of large language models (LLMs), merging existing pre-trained LLMs into a more potent model to enhance its capabilities. Experimental results validate the effectiveness of this approach."
}