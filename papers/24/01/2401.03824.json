{
    "title": "A topological description of loss surfaces based on Betti Numbers. (arXiv:2401.03824v1 [cs.LG])",
    "abstract": "In the context of deep learning models, attention has recently been paid to studying the surface of the loss function in order to better understand training with methods based on gradient descent. This search for an appropriate description, both analytical and topological, has led to numerous efforts to identify spurious minima and characterize gradient dynamics. Our work aims to contribute to this field by providing a topological measure to evaluate loss complexity in the case of multilayer neural networks. We compare deep and shallow architectures with common sigmoidal activation functions by deriving upper and lower bounds on the complexity of their loss function and revealing how that complexity is influenced by the number of hidden units, training models, and the activation function used. Additionally, we found that certain variations in the loss function or model architecture, such as adding an $\\ell_2$ regularization term or implementing skip connections in a feedforward network",
    "link": "http://arxiv.org/abs/2401.03824",
    "context": "Title: A topological description of loss surfaces based on Betti Numbers. (arXiv:2401.03824v1 [cs.LG])\nAbstract: In the context of deep learning models, attention has recently been paid to studying the surface of the loss function in order to better understand training with methods based on gradient descent. This search for an appropriate description, both analytical and topological, has led to numerous efforts to identify spurious minima and characterize gradient dynamics. Our work aims to contribute to this field by providing a topological measure to evaluate loss complexity in the case of multilayer neural networks. We compare deep and shallow architectures with common sigmoidal activation functions by deriving upper and lower bounds on the complexity of their loss function and revealing how that complexity is influenced by the number of hidden units, training models, and the activation function used. Additionally, we found that certain variations in the loss function or model architecture, such as adding an $\\ell_2$ regularization term or implementing skip connections in a feedforward network",
    "path": "papers/24/01/2401.03824.json",
    "total_tokens": 907,
    "translated_title": "基于Betti数的损失曲面的拓扑描述",
    "translated_abstract": "在深度学习模型的背景下，研究损失函数的曲面已经引起了人们的关注，以更好地理解基于梯度下降的训练方法。这种对一个合适的描述的寻求，既包括分析又包括拓扑，已经导致了大量努力来识别虚假最小值和表征梯度动态。我们的工作旨在为这一领域做出贡献，通过提供一个拓扑度量来评估多层神经网络中损失的复杂性。我们通过推导深层和浅层架构的损失函数复杂性的上下界以及揭示这种复杂性如何受隐藏单元数量、训练模型和激活函数影响，比较了使用常见的Sigmoid激活函数的深层和浅层架构。此外，我们发现了损失函数或模型架构的某些变化，比如在前馈网络中添加$\\ell_2$正则化项或实施跳跃连接",
    "tldr": "本文提出了一种基于Betti数的拓扑度量，用于评估多层神经网络中损失的复杂性，并发现复杂性受到隐藏单元数量、训练模型和激活函数的影响。",
    "en_tdlr": "In this work, we propose a topological measure based on Betti numbers to evaluate the complexity of loss in multilayer neural networks, and we discover that the complexity is influenced by the number of hidden units, training models, and the activation function used."
}