{
    "title": "Power in Numbers: Robust reading comprehension by finetuning with four adversarial sentences per example. (arXiv:2401.10091v1 [cs.CL])",
    "abstract": "Recent models have achieved human level performance on the Stanford Question Answering Dataset when using F1 scores to evaluate the reading comprehension task. Yet, teaching machines to comprehend text has not been solved in the general case. By appending one adversarial sentence to the context paragraph, past research has shown that the F1 scores from reading comprehension models drop almost in half. In this paper, I replicate past adversarial research with a new model, ELECTRA-Small, and demonstrate that the new model's F1 score drops from 83.9% to 29.2%. To improve ELECTRA-Small's resistance to this attack, I finetune the model on SQuAD v1.1 training examples with one to five adversarial sentences appended to the context paragraph. Like past research, I find that the finetuned model on one adversarial sentence does not generalize well across evaluation datasets. However, when finetuned on four or five adversarial sentences the model attains an F1 score of more than 70% on most evalu",
    "link": "http://arxiv.org/abs/2401.10091",
    "context": "Title: Power in Numbers: Robust reading comprehension by finetuning with four adversarial sentences per example. (arXiv:2401.10091v1 [cs.CL])\nAbstract: Recent models have achieved human level performance on the Stanford Question Answering Dataset when using F1 scores to evaluate the reading comprehension task. Yet, teaching machines to comprehend text has not been solved in the general case. By appending one adversarial sentence to the context paragraph, past research has shown that the F1 scores from reading comprehension models drop almost in half. In this paper, I replicate past adversarial research with a new model, ELECTRA-Small, and demonstrate that the new model's F1 score drops from 83.9% to 29.2%. To improve ELECTRA-Small's resistance to this attack, I finetune the model on SQuAD v1.1 training examples with one to five adversarial sentences appended to the context paragraph. Like past research, I find that the finetuned model on one adversarial sentence does not generalize well across evaluation datasets. However, when finetuned on four or five adversarial sentences the model attains an F1 score of more than 70% on most evalu",
    "path": "papers/24/01/2401.10091.json",
    "total_tokens": 949,
    "translated_title": "数字的力量：通过每个示例细调与四个对抗句子来实现鲁棒的阅读理解",
    "translated_abstract": "在评估阅读理解任务时，最近的模型已经达到了人类水平的性能，使用F1分数进行评估。然而，在一般情况下，教机器理解文本还没有解决。通过将一个对抗性句子添加到上下文段落中，过去的研究表明，阅读理解模型的F1分数几乎减半。在本文中，我使用一个新的模型ELECTRA-Small复制了过去的对抗性研究，并证明了新模型的F1分数从83.9％下降到29.2％。为了提高ELECTRA-Small对这种攻击的抵抗力，我对SQuAD v1.1训练示例进行了细调，将一到五个对抗句子附加到上下文段落中。与过去的研究一样，我发现细调后的模型对一个对抗句子的泛化能力不佳。然而，当细调用于四个或五个对抗句子时，该模型在大多数评估数据集上获得超过70％的F1分数。",
    "tldr": "本文研究通过每个示例细调与四个对抗句子来实现鲁棒的阅读理解，并发现这种细调能提高模型在评估数据集上的F1分数。",
    "en_tdlr": "This paper investigates robust reading comprehension by finetuning with four adversarial sentences per example, and demonstrates that this finetuning improves the F1 scores of the model on evaluation datasets."
}