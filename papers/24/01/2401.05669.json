{
    "title": "ConcEPT: Concept-Enhanced Pre-Training for Language Models. (arXiv:2401.05669v1 [cs.CL])",
    "abstract": "Pre-trained language models (PLMs) have been prevailing in state-of-the-art methods for natural language processing, and knowledge-enhanced PLMs are further proposed to promote model performance in knowledge-intensive tasks. However, conceptual knowledge, one essential kind of knowledge for human cognition, still remains understudied in this line of research. This limits PLMs' performance in scenarios requiring human-like cognition, such as understanding long-tail entities with concepts. In this paper, we propose ConcEPT, which stands for Concept-Enhanced Pre-Training for language models, to infuse conceptual knowledge into PLMs. ConcEPT exploits external taxonomies with entity concept prediction, a novel pre-training objective to predict the concepts of entities mentioned in the pre-training contexts. Unlike previous concept-enhanced methods, ConcEPT can be readily adapted to various downstream applications without entity linking or concept mapping. Results of extensive experiments sh",
    "link": "http://arxiv.org/abs/2401.05669",
    "context": "Title: ConcEPT: Concept-Enhanced Pre-Training for Language Models. (arXiv:2401.05669v1 [cs.CL])\nAbstract: Pre-trained language models (PLMs) have been prevailing in state-of-the-art methods for natural language processing, and knowledge-enhanced PLMs are further proposed to promote model performance in knowledge-intensive tasks. However, conceptual knowledge, one essential kind of knowledge for human cognition, still remains understudied in this line of research. This limits PLMs' performance in scenarios requiring human-like cognition, such as understanding long-tail entities with concepts. In this paper, we propose ConcEPT, which stands for Concept-Enhanced Pre-Training for language models, to infuse conceptual knowledge into PLMs. ConcEPT exploits external taxonomies with entity concept prediction, a novel pre-training objective to predict the concepts of entities mentioned in the pre-training contexts. Unlike previous concept-enhanced methods, ConcEPT can be readily adapted to various downstream applications without entity linking or concept mapping. Results of extensive experiments sh",
    "path": "papers/24/01/2401.05669.json",
    "total_tokens": 930,
    "translated_title": "ConcEPT: 语言模型的概念增强预训练方法",
    "translated_abstract": "预训练语言模型（PLMs）已经在自然语言处理的最新方法中盛行，而知识增强的PLMs进一步提出以提高知识密集型任务中的模型性能。然而，概念知识作为人类认知的一种基本知识，在这一研究领域仍然缺乏研究。这限制了PLMs在需要人类样的认知能力的场景中的性能，例如理解具有概念的尾部实体。在本文中，我们提出ConcEPT，即概念增强预训练语言模型，将概念知识融入PLMs中。 ConcEPT利用外部分类法和实体概念预测，一种新的预训练目标来预测预训练上下文中提到的实体的概念。与以前的概念增强方法不同，ConcEPT可以在没有实体链接或概念映射的情况下轻松适应各种下游应用。广泛的实验证明了ConcEPT的有效性。",
    "tldr": "本文提出了一种名为ConcEPT的语言模型预训练方法，通过将概念知识融入到模型中来提高性能。与以往的方法不同的是，ConcEPT可以适用于各种应用，无需进行实体链接或概念映射。",
    "en_tdlr": "This paper proposes ConcEPT, a language model pre-training method that enhances performance by incorporating conceptual knowledge into the model. Unlike previous methods, ConcEPT can be applied to various applications without the need for entity linking or concept mapping."
}