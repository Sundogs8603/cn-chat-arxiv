{
    "title": "Task structure and nonlinearity jointly determine learned representational geometry. (arXiv:2401.13558v1 [cs.LG])",
    "abstract": "The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks. This geometry depends on the structure of the inputs, the structure of the target outputs, and the architecture of the network. By studying the learning dynamics of networks with one hidden layer, we discovered that the network's activation function has an unexpectedly strong impact on the representational geometry: Tanh networks tend to learn representations that reflect the structure of the target outputs, while ReLU networks retain more information about the structure of the raw inputs. This difference is consistently observed across a broad class of parameterized tasks in which we modulated the degree of alignment between the geometry of the task inputs and that of the task labels. We analyzed the learning dynamics in weight space and show how the differences between the networks with Tanh and ReLU nonlinearities arise from the asymmetric asymptotic behavior of ",
    "link": "http://arxiv.org/abs/2401.13558",
    "context": "Title: Task structure and nonlinearity jointly determine learned representational geometry. (arXiv:2401.13558v1 [cs.LG])\nAbstract: The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks. This geometry depends on the structure of the inputs, the structure of the target outputs, and the architecture of the network. By studying the learning dynamics of networks with one hidden layer, we discovered that the network's activation function has an unexpectedly strong impact on the representational geometry: Tanh networks tend to learn representations that reflect the structure of the target outputs, while ReLU networks retain more information about the structure of the raw inputs. This difference is consistently observed across a broad class of parameterized tasks in which we modulated the degree of alignment between the geometry of the task inputs and that of the task labels. We analyzed the learning dynamics in weight space and show how the differences between the networks with Tanh and ReLU nonlinearities arise from the asymmetric asymptotic behavior of ",
    "path": "papers/24/01/2401.13558.json",
    "total_tokens": 861,
    "translated_title": "任务结构和非线性共同决定了学习的表示几何关系",
    "translated_abstract": "一个学习出的神经表示的效用取决于它的几何结构对下游任务性能的支持程度。这个几何结构取决于输入的结构、目标输出的结构以及网络的结构。通过研究具有一个隐藏层的网络的学习动力学，我们发现网络的激活函数对表示几何有一个意外的强烈影响：Tanh网络倾向于学习反映目标输出结构的表示，而ReLU网络保留了更多关于原始输入结构的信息。在一个参数化任务类中，我们调整任务输入的几何结构和任务标签的几何结构之间的对齐程度，发现这种差异在所有任务上都是一致的。我们分析了权重空间中的学习动力学，并展示了Tanh和ReLU非线性网络之间的差异是由于它们的渐进行为的不对称性引起的。",
    "tldr": "学习得到的神经表示的几何结构对下游任务性能很重要。研究发现激活函数对几何结构有重要影响：Tanh网络学到的表示反映了目标输出的结构，而ReLU网络保留了原始输入结构的信息。",
    "en_tdlr": "The geometry of a learned neural representation is crucial for downstream task performance. The activation function plays a significant role: Tanh networks capture the structure of target outputs, while ReLU networks retain information about the structure of raw inputs."
}