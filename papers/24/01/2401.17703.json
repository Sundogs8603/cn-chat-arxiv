{
    "title": "WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts",
    "abstract": "The Winograd Schema Challenge (WSC) serves as a prominent benchmark for evaluating machine understanding. While Large Language Models (LLMs) excel at answering WSC questions, their ability to generate such questions remains less explored. In this work, we propose Tree-of-Experts (ToE), a novel prompting method which enhances the generation of WSC instances (50% valid cases vs. 10% in recent methods). Using this approach, we introduce WSC+, a novel dataset comprising 3,026 LLM-generated sentences. Notably, we extend the WSC framework by incorporating new 'ambiguous' and 'offensive' categories, providing a deeper insight into model overconfidence and bias. Our analysis reveals nuances in generation-evaluation consistency, suggesting that LLMs may not always outperform in evaluating their own generated questions when compared to those crafted by other models. On WSC+, GPT-4, the top-performing LLM, achieves an accuracy of 68.7%, significantly below the human benchmark of 95.1%.",
    "link": "https://arxiv.org/abs/2401.17703",
    "context": "Title: WSC+: Enhancing The Winograd Schema Challenge Using Tree-of-Experts\nAbstract: The Winograd Schema Challenge (WSC) serves as a prominent benchmark for evaluating machine understanding. While Large Language Models (LLMs) excel at answering WSC questions, their ability to generate such questions remains less explored. In this work, we propose Tree-of-Experts (ToE), a novel prompting method which enhances the generation of WSC instances (50% valid cases vs. 10% in recent methods). Using this approach, we introduce WSC+, a novel dataset comprising 3,026 LLM-generated sentences. Notably, we extend the WSC framework by incorporating new 'ambiguous' and 'offensive' categories, providing a deeper insight into model overconfidence and bias. Our analysis reveals nuances in generation-evaluation consistency, suggesting that LLMs may not always outperform in evaluating their own generated questions when compared to those crafted by other models. On WSC+, GPT-4, the top-performing LLM, achieves an accuracy of 68.7%, significantly below the human benchmark of 95.1%.",
    "path": "papers/24/01/2401.17703.json",
    "total_tokens": 1029,
    "translated_title": "WSC+: 使用专家树增强Winograd Schema挑战",
    "translated_abstract": "Winograd Schema Challenge (WSC)是评估机器理解能力的重要基准。虽然大型语言模型在回答WSC问题方面表现出色，但它们生成这类问题的能力还未得到充分探索。在这项工作中，我们提出了一种新的提示方法-专家树（Tree-of-Experts，ToE），它增强了WSC实例的生成能力（50%有效案例，相比最新方法的10%）。利用这种方法，我们引入了WSC+，一个包含3026个大型语言模型生成句子的新数据集。值得注意的是，我们扩展了WSC框架，加入了新的“模棱两可”（ambiguous）和“冒犯性”（offensive）的类别，从而更深入地了解模型过度自信和偏见。我们的分析揭示了生成-评估一致性方面的细微差别，表明在评估自己生成的问题时，大型语言模型可能并不总是优于其他模型所创建的问题。在WSC+上，GPT-4，即表现最好的大型语言模型，准确率为68.7%，明显低于人类准确率95.1%的基准。",
    "tldr": "WSC+提出了一种新的提示方法-专家树，增强了Winograd Schema挑战中问题的生成能力。通过引入新的数据集和扩展框架，WSC+揭示了模型的过度自信和偏见，并发现大型语言模型在评估自己生成的问题时并不总是优于其他模型。在WSC+上，当前最好的大型语言模型GPT-4的准确率为68.7%。"
}