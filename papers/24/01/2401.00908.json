{
    "title": "DocLLM: A layout-aware generative language model for multimodal document understanding. (arXiv:2401.00908v1 [cs.CL])",
    "abstract": "Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layo",
    "link": "http://arxiv.org/abs/2401.00908",
    "context": "Title: DocLLM: A layout-aware generative language model for multimodal document understanding. (arXiv:2401.00908v1 [cs.CL])\nAbstract: Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layo",
    "path": "papers/24/01/2401.00908.json",
    "total_tokens": 908,
    "translated_title": "DocLLM: 一种面向布局感知的多模态文档理解生成语言模型",
    "translated_abstract": "企业文档，如表单、发票、收据、报告、合同等记录，通常在文本和空间模态的交汇处具有丰富的语义。它们复杂的布局所提供的视觉线索在有效理解这些文档中起着至关重要的作用。在本文中，我们提出了DocLLM，一个针对视觉文档推理的轻量级扩展传统大型语言模型（LLMs），它考虑了文本语义和空间布局。我们的模型与现有的多模态LLMs不同，避免了昂贵的图像编码器，并且专注于利用边界框信息来融入空间布局结构。具体而言，我们通过将经典Transformer中的注意机制分解为一组解耦矩阵，来捕捉文本和空间模态之间的交叉对齐。此外，我们设计了一种预训练目标，用于学习填充文本片段。这种方法使我们能够处理不规则的布局情况。",
    "tldr": "本文提出了一种名为DocLLM的面向布局感知的多模态文档理解生成语言模型，通过结合文本语义和空间布局，避免了使用昂贵的图像编码器，提供了一种有效理解企业文档的方法。",
    "en_tdlr": "This paper presents DocLLM, a layout-aware generative language model for multimodal document understanding. It integrates textual semantics and spatial layout by avoiding expensive image encoders, and effectively comprehends enterprise documents such as forms, invoices, and reports."
}