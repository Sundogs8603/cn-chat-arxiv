{
    "title": "Accelerating Fractional PINNs using Operational Matrices of Derivative. (arXiv:2401.14081v1 [cs.LG])",
    "abstract": "This paper presents a novel operational matrix method to accelerate the training of fractional Physics-Informed Neural Networks (fPINNs). Our approach involves a non-uniform discretization of the fractional Caputo operator, facilitating swift computation of fractional derivatives within Caputo-type fractional differential problems with $0<\\alpha<1$. In this methodology, the operational matrix is precomputed, and during the training phase, automatic differentiation is replaced with a matrix-vector product. While our methodology is compatible with any network, we particularly highlight its successful implementation in PINNs, emphasizing the enhanced accuracy achieved when utilizing the Legendre Neural Block (LNB) architecture. LNB incorporates Legendre polynomials into the PINN structure, providing a significant boost in accuracy. The effectiveness of our proposed method is validated across diverse differential equations, including Delay Differential Equations (DDEs) and Systems of Diffe",
    "link": "http://arxiv.org/abs/2401.14081",
    "context": "Title: Accelerating Fractional PINNs using Operational Matrices of Derivative. (arXiv:2401.14081v1 [cs.LG])\nAbstract: This paper presents a novel operational matrix method to accelerate the training of fractional Physics-Informed Neural Networks (fPINNs). Our approach involves a non-uniform discretization of the fractional Caputo operator, facilitating swift computation of fractional derivatives within Caputo-type fractional differential problems with $0<\\alpha<1$. In this methodology, the operational matrix is precomputed, and during the training phase, automatic differentiation is replaced with a matrix-vector product. While our methodology is compatible with any network, we particularly highlight its successful implementation in PINNs, emphasizing the enhanced accuracy achieved when utilizing the Legendre Neural Block (LNB) architecture. LNB incorporates Legendre polynomials into the PINN structure, providing a significant boost in accuracy. The effectiveness of our proposed method is validated across diverse differential equations, including Delay Differential Equations (DDEs) and Systems of Diffe",
    "path": "papers/24/01/2401.14081.json",
    "total_tokens": 869,
    "translated_title": "使用导数的操作矩阵加速分数PINNs",
    "translated_abstract": "本文提出了一种新颖的操作矩阵方法，用于加速分数物理信息神经网络（fPINNs）的训练。我们的方法涉及对分数Caputo算子进行非均匀离散化，便于在Caputo型分数微分问题中计算分数导数。在这种方法中，操作矩阵是预先计算的，在训练阶段，自动微分被一个矩阵-向量乘积替代。虽然我们的方法适用于任何网络，但我们特别强调其在PINNs中成功实现的情况，强调在使用Legendre神经块（LNB）架构时所实现的增强准确性。LNB将Legendre多项式融入到PINN结构中，极大地提高了准确性。我们提出的方法的有效性在包括时滞微分方程和微分方程系统在内的多种微分方程中得到验证。",
    "tldr": "本文提出了一种操作矩阵方法来加速分数PINNs的训练，通过非均匀离散化分数Caputo算子，实现了对分数导数的快速计算。使用Legendre神经块（LNB）架构提高了PINNs的准确性。",
    "en_tdlr": "This paper introduces an operational matrix method to accelerate the training of fractional PINNs, achieving fast computation of fractional derivatives through non-uniform discretization of the Caputo operator. The accuracy of PINNs is enhanced by incorporating the Legendre Neural Block (LNB) architecture."
}