{
    "title": "Enhancing Reliability of Neural Networks at the Edge: Inverted Normalization with Stochastic Affine Transformations. (arXiv:2401.12416v1 [cs.LG])",
    "abstract": "Bayesian Neural Networks (BayNNs) naturally provide uncertainty in their predictions, making them a suitable choice in safety-critical applications. Additionally, their realization using memristor-based in-memory computing (IMC) architectures enables them for resource-constrained edge applications. In addition to predictive uncertainty, however, the ability to be inherently robust to noise in computation is also essential to ensure functional safety. In particular, memristor-based IMCs are susceptible to various sources of non-idealities such as manufacturing and runtime variations, drift, and failure, which can significantly reduce inference accuracy. In this paper, we propose a method to inherently enhance the robustness and inference accuracy of BayNNs deployed in IMC architectures. To achieve this, we introduce a novel normalization layer combined with stochastic affine transformations. Empirical results in various benchmark datasets show a graceful degradation in inference accurac",
    "link": "http://arxiv.org/abs/2401.12416",
    "context": "Title: Enhancing Reliability of Neural Networks at the Edge: Inverted Normalization with Stochastic Affine Transformations. (arXiv:2401.12416v1 [cs.LG])\nAbstract: Bayesian Neural Networks (BayNNs) naturally provide uncertainty in their predictions, making them a suitable choice in safety-critical applications. Additionally, their realization using memristor-based in-memory computing (IMC) architectures enables them for resource-constrained edge applications. In addition to predictive uncertainty, however, the ability to be inherently robust to noise in computation is also essential to ensure functional safety. In particular, memristor-based IMCs are susceptible to various sources of non-idealities such as manufacturing and runtime variations, drift, and failure, which can significantly reduce inference accuracy. In this paper, we propose a method to inherently enhance the robustness and inference accuracy of BayNNs deployed in IMC architectures. To achieve this, we introduce a novel normalization layer combined with stochastic affine transformations. Empirical results in various benchmark datasets show a graceful degradation in inference accurac",
    "path": "papers/24/01/2401.12416.json",
    "total_tokens": 905,
    "translated_title": "在边缘上增强神经网络的可靠性：倒置归一化与随机仿射变换",
    "translated_abstract": "贝叶斯神经网络（BayNNs）在预测中自然地提供了不确定性，使其成为安全关键应用的合适选择。此外，利用基于忆阻器的内存计算（IMC）架构实现贝叶斯神经网络使其适用于资源有限的边缘应用。除了预测不确定性外，对计算中的噪音具有固有的鲁棒性也是确保功能安全的关键。特别是，基于忆阻器的IMC对制造和运行时的变化、漂移和故障等各种非理想因素敏感，这可能会显著降低推理精度。在本文中，我们提出了一种方法，以在部署在IMC架构中的BayNNs上增强鲁棒性和推理精度。为了实现这一目标，我们引入了一种新颖的归一化层与随机仿射变换相结合。在各种基准数据集上的实证结果显示出推理精度的逐渐降低",
    "tldr": "本论文介绍了一种通过倒置归一化和随机仿射变换来提高内存计算架构中贝叶斯神经网络鲁棒性和推理精度的方法",
    "en_tdlr": "This paper presents a method to enhance the robustness and inference accuracy of Bayesian Neural Networks deployed in memristor-based in-memory computing architectures by utilizing inverted normalization and stochastic affine transformations."
}