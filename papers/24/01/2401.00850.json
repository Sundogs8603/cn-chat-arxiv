{
    "title": "Refining Pre-Trained Motion Models",
    "abstract": "arXiv:2401.00850v2 Announce Type: replace-cross  Abstract: Given the difficulty of manually annotating motion in video, the current best motion estimation methods are trained with synthetic data, and therefore struggle somewhat due to a train/test gap. Self-supervised methods hold the promise of training directly on real video, but typically perform worse. These include methods trained with warp error (i.e., color constancy) combined with smoothness terms, and methods that encourage cycle-consistency in the estimates (i.e., tracking backwards should yield the opposite trajectory as tracking forwards). In this work, we take on the challenge of improving state-of-the-art supervised models with self-supervised training. We find that when the initialization is supervised weights, most existing self-supervision techniques actually make performance worse instead of better, which suggests that the benefit of seeing the new data is overshadowed by the noise in the training signal. Focusing on ",
    "link": "https://arxiv.org/abs/2401.00850",
    "context": "Title: Refining Pre-Trained Motion Models\nAbstract: arXiv:2401.00850v2 Announce Type: replace-cross  Abstract: Given the difficulty of manually annotating motion in video, the current best motion estimation methods are trained with synthetic data, and therefore struggle somewhat due to a train/test gap. Self-supervised methods hold the promise of training directly on real video, but typically perform worse. These include methods trained with warp error (i.e., color constancy) combined with smoothness terms, and methods that encourage cycle-consistency in the estimates (i.e., tracking backwards should yield the opposite trajectory as tracking forwards). In this work, we take on the challenge of improving state-of-the-art supervised models with self-supervised training. We find that when the initialization is supervised weights, most existing self-supervision techniques actually make performance worse instead of better, which suggests that the benefit of seeing the new data is overshadowed by the noise in the training signal. Focusing on ",
    "path": "papers/24/01/2401.00850.json",
    "total_tokens": 781,
    "translated_title": "优化预训练的运动模型",
    "translated_abstract": "鉴于在视频中手动注释运动的困难，目前最好的运动估计方法是使用合成数据进行训练，因此在训练/测试之间存在一定困难。自监督方法承诺直接在真实视频上进行训练，但通常表现较差。这些方法包括使用翘曲误差（即颜色恒定）与平滑项相结合进行训练的方法，以及鼓励估计的循环一致性的方法（即向后跟踪应产生与向前跟踪相反的轨迹）。在这项工作中，我们接受了通过自监督训练改进最先进监督模型的挑战。我们发现，当初始化是监督权重时，大多数现有的自监督技术实际上会使性能变差而不是更好，这表明看到新数据的好处被训练信号中的噪声所掩盖。专注于...",
    "tldr": "通过自监督训练改进了现有监督模型，发现大多数自监督技术使性能变差，而不是更好。"
}