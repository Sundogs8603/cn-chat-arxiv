{
    "title": "Reward-Relevance-Filtered Linear Offline Reinforcement Learning. (arXiv:2401.12934v1 [stat.ML])",
    "abstract": "This paper studies offline reinforcement learning with linear function approximation in a setting with decision-theoretic, but not estimation sparsity. The structural restrictions of the data-generating process presume that the transitions factor into a sparse component that affects the reward and could affect additional exogenous dynamics that do not affect the reward. Although the minimally sufficient adjustment set for estimation of full-state transition properties depends on the whole state, the optimal policy and therefore state-action value function depends only on the sparse component: we call this causal/decision-theoretic sparsity. We develop a method for reward-filtering the estimation of the state-action value function to the sparse component by a modification of thresholded lasso in least-squares policy evaluation. We provide theoretical guarantees for our reward-filtered linear fitted-Q-iteration, with sample complexity depending only on the size of the sparse component.",
    "link": "http://arxiv.org/abs/2401.12934",
    "context": "Title: Reward-Relevance-Filtered Linear Offline Reinforcement Learning. (arXiv:2401.12934v1 [stat.ML])\nAbstract: This paper studies offline reinforcement learning with linear function approximation in a setting with decision-theoretic, but not estimation sparsity. The structural restrictions of the data-generating process presume that the transitions factor into a sparse component that affects the reward and could affect additional exogenous dynamics that do not affect the reward. Although the minimally sufficient adjustment set for estimation of full-state transition properties depends on the whole state, the optimal policy and therefore state-action value function depends only on the sparse component: we call this causal/decision-theoretic sparsity. We develop a method for reward-filtering the estimation of the state-action value function to the sparse component by a modification of thresholded lasso in least-squares policy evaluation. We provide theoretical guarantees for our reward-filtered linear fitted-Q-iteration, with sample complexity depending only on the size of the sparse component.",
    "path": "papers/24/01/2401.12934.json",
    "total_tokens": 950,
    "translated_title": "基于奖励相关性过滤的线性离线强化学习",
    "translated_abstract": "本文研究了在线决策理论环境中线性函数逼近的离线强化学习，其中假设数据生成过程具有决策理论稀疏性而不是估计稀疏性。数据生成过程的结构性限制预设了转移可以分解为一个影响奖励的稀疏组件，并且可能影响不影响奖励的其他外生动力学。虽然用于估计全状态过渡属性的最小可调整集合取决于整个状态，但最优策略，因此状态-动作值函数只依赖于稀疏组件：我们将其称为因果/决策论稀疏性。我们通过修改阈值岭回归在最小二乘策略评估中的应用提出了一种过滤奖励的方法，将状态-动作值函数的估计限制在稀疏组件上。我们为奖励过滤的线性拟合Q-迭代提供了理论保证，样本复杂度仅取决于稀疏组件的大小。",
    "tldr": "本文研究了在线决策理论环境中线性函数逼近的离线强化学习，提出了一种基于奖励相关性过滤的方法，将状态-动作值函数的估计限制在稀疏组件上，具有理论保证，并且样本复杂度仅取决于稀疏组件的大小。",
    "en_tdlr": "This paper studies offline reinforcement learning with linear function approximation in a decision-theoretic environment and proposes a reward-relevance-filtering method to restrict the estimation of the state-action value function to the sparse component, with theoretical guarantees and sample complexity depending only on the size of the sparse component."
}