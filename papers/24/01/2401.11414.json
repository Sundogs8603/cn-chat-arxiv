{
    "title": "S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Driving. (arXiv:2401.11414v2 [cs.CV] UPDATED)",
    "abstract": "Semantic segmentation and stereo matching are two essential components of 3D environmental perception systems for autonomous driving. Nevertheless, conventional approaches often address these two problems independently, employing separate models for each task. This approach poses practical limitations in real-world scenarios, particularly when computational resources are scarce or real-time performance is imperative. Hence, in this article, we introduce S$^3$M-Net, a novel joint learning framework developed to perform semantic segmentation and stereo matching simultaneously. Specifically, S$^3$M-Net shares the features extracted from RGB images between both tasks, resulting in an improved overall scene understanding capability. This feature sharing process is realized using a feature fusion adaption (FFA) module, which effectively transforms the shared features into semantic space and subsequently fuses them with the encoded disparity features. The entire joint learning framework is tr",
    "link": "http://arxiv.org/abs/2401.11414",
    "context": "Title: S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching for Autonomous Driving. (arXiv:2401.11414v2 [cs.CV] UPDATED)\nAbstract: Semantic segmentation and stereo matching are two essential components of 3D environmental perception systems for autonomous driving. Nevertheless, conventional approaches often address these two problems independently, employing separate models for each task. This approach poses practical limitations in real-world scenarios, particularly when computational resources are scarce or real-time performance is imperative. Hence, in this article, we introduce S$^3$M-Net, a novel joint learning framework developed to perform semantic segmentation and stereo matching simultaneously. Specifically, S$^3$M-Net shares the features extracted from RGB images between both tasks, resulting in an improved overall scene understanding capability. This feature sharing process is realized using a feature fusion adaption (FFA) module, which effectively transforms the shared features into semantic space and subsequently fuses them with the encoded disparity features. The entire joint learning framework is tr",
    "path": "papers/24/01/2401.11414.json",
    "total_tokens": 915,
    "translated_title": "S$^3$M-Net:用于自动驾驶的语义分割和立体匹配的联合学习",
    "translated_abstract": "语义分割和立体匹配是自动驾驶的三维环境感知系统的两个重要组成部分。然而，传统方法通常独立处理这两个问题，为每个任务使用单独的模型。这种方法在现实世界的场景中存在实际限制，特别是在计算资源稀缺或实时性能至关重要的情况下。因此，在本文中，我们介绍了S$^3$M-Net，这是一种新颖的联合学习框架，用于同时进行语义分割和立体匹配。具体而言，S$^3$M-Net共享来自RGB图像的特征，从而提高整体场景理解能力。这个特征共享过程通过一个特征融合适应（FFA）模块实现，有效地将共享特征转换为语义空间，并随后将它们与编码的视差特征融合。整个联合学习框架是通过联合训练实现的。",
    "tldr": "S$^3$M-Net是一种用于自动驾驶的联合学习框架，同时进行语义分割和立体匹配。通过共享特征和特征融合适应模块的使用，S$^3$M-Net能够提高整体场景理解能力。"
}