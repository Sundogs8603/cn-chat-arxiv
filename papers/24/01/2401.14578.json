{
    "title": "GOAt: Explaining Graph Neural Networks via Graph Output Attribution. (arXiv:2401.14578v1 [cs.LG])",
    "abstract": "Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-ofthe-art GNN explainers in terms of the comm",
    "link": "http://arxiv.org/abs/2401.14578",
    "context": "Title: GOAt: Explaining Graph Neural Networks via Graph Output Attribution. (arXiv:2401.14578v1 [cs.LG])\nAbstract: Understanding the decision-making process of Graph Neural Networks (GNNs) is crucial to their interpretability. Most existing methods for explaining GNNs typically rely on training auxiliary models, resulting in the explanations remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a novel method to attribute graph outputs to input graph features, creating GNN explanations that are faithful, discriminative, as well as stable across similar samples. By expanding the GNN as a sum of scalar products involving node features, edge features and activation patterns, we propose an efficient analytical method to compute contribution of each node or edge feature to each scalar product and aggregate the contributions from all scalar products in the expansion form to derive the importance of each node and edge. Through extensive experiments on synthetic and real-world data, we show that our method not only outperforms various state-ofthe-art GNN explainers in terms of the comm",
    "path": "papers/24/01/2401.14578.json",
    "total_tokens": 970,
    "translated_title": "GOAt: 通过图输出属性解释图神经网络",
    "translated_abstract": "理解图神经网络（GNNs）的决策过程对于其可解释性至关重要。现有的大多数解释GNNs的方法通常依赖于训练辅助模型，导致解释结果仍然是黑盒的。本文介绍了一种名为Graph Output Attribution（GOAt）的新方法，用于将图输出归因于输入图特征，从而创建既忠实、有区别，又在相似样本上稳定的GNN解释。通过将GNN扩展为涉及节点特征、边特征和激活模式的标量积之和，我们提出了一种高效的分析方法，用于计算每个节点或边特征对每个标量积的贡献，并将扩展形式中所有标量积的贡献聚合起来，以推导出每个节点和边的重要性。通过对合成数据和现实世界数据的广泛实验证明，我们的方法不仅在评估GNN解释器的性能方面优于各种最新方法，",
    "tldr": "本论文引入了一种名为Graph Output Attribution（GOAt）的新方法，通过将GNN扩展为涉及节点特征、边特征和激活模式的标量积之和，计算每个节点或边特征对每个标量积的贡献，并将贡献聚合起来，从而实现将图输出归因于输入图特征的GNN解释。",
    "en_tdlr": "This paper introduces a novel method called Graph Output Attribution (GOAt), which expands the Graph Neural Network (GNN) as a sum of scalar products involving node features, edge features, and activation patterns. It computes the contribution of each node or edge feature to each scalar product and aggregates the contributions to attribute the graph outputs to the input graph features, providing faithful, discriminative, and stable explanations for GNNs."
}