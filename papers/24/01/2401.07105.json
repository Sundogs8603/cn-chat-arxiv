{
    "title": "Graph Language Models",
    "abstract": "arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical ",
    "link": "https://arxiv.org/abs/2401.07105",
    "context": "Title: Graph Language Models\nAbstract: arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical ",
    "path": "papers/24/01/2401.07105.json",
    "total_tokens": 845,
    "translated_title": "图语言模型",
    "translated_abstract": "虽然语言模型（LMs）是自然语言处理的主力军，它们与结构化知识图谱（KGs）的相互作用仍在积极研究中。当前用于编码这些图形的方法通常要么（i）将它们线性化以供LM嵌入--这样会低效利用结构信息，要么（ii）使用图神经网络（GNNs）来保留图结构--但GNNs无法像预训练的LM一样很好地表示文本特征。在我们的工作中，我们引入了一种新型LM类型，即图语言模型（GLM），它整合了两种方法的优点并减轻了它们的弱点。GLM参数从预训练的LM中初始化，以增强对个别图概念和三元组的理解。同时，我们设计GLM的架构以整合图偏差，从而促进图内的知识分布。这使GLM能够处理图形、文本以及两者的交织输入。实证",
    "tldr": "引入了一种新型的图语言模型（GLM），结合线性化和图神经网络的优点，解决了传统方法在处理结构化知识图谱时的弱点。",
    "en_tdlr": "Introduced a novel Graph Language Model (GLM) that integrates the strengths of linearization and Graph Neural Networks, addressing the weaknesses of traditional methods in handling structured knowledge graphs."
}