{
    "title": "Optimizing Convolutional Neural Network Architecture. (arXiv:2401.01361v1 [cs.CV])",
    "abstract": "Convolutional Neural Networks (CNN) are widely used to face challenging tasks like speech recognition, natural language processing or computer vision. As CNN architectures get larger and more complex, their computational requirements increase, incurring significant energetic costs and challenging their deployment on resource-restricted devices. In this paper, we propose Optimizing Convolutional Neural Network Architecture (OCNNA), a novel CNN optimization and construction method based on pruning and knowledge distillation designed to establish the importance of convolutional layers. The proposal has been evaluated though a thorough empirical study including the best known datasets (CIFAR-10, CIFAR-100 and Imagenet) and CNN architectures (VGG-16, ResNet-50, DenseNet-40 and MobileNet), setting Accuracy Drop and Remaining Parameters Ratio as objective metrics to compare the performance of OCNNA against the other state-of-art approaches. Our method has been compared with more than 20 convo",
    "link": "http://arxiv.org/abs/2401.01361",
    "context": "Title: Optimizing Convolutional Neural Network Architecture. (arXiv:2401.01361v1 [cs.CV])\nAbstract: Convolutional Neural Networks (CNN) are widely used to face challenging tasks like speech recognition, natural language processing or computer vision. As CNN architectures get larger and more complex, their computational requirements increase, incurring significant energetic costs and challenging their deployment on resource-restricted devices. In this paper, we propose Optimizing Convolutional Neural Network Architecture (OCNNA), a novel CNN optimization and construction method based on pruning and knowledge distillation designed to establish the importance of convolutional layers. The proposal has been evaluated though a thorough empirical study including the best known datasets (CIFAR-10, CIFAR-100 and Imagenet) and CNN architectures (VGG-16, ResNet-50, DenseNet-40 and MobileNet), setting Accuracy Drop and Remaining Parameters Ratio as objective metrics to compare the performance of OCNNA against the other state-of-art approaches. Our method has been compared with more than 20 convo",
    "path": "papers/24/01/2401.01361.json",
    "total_tokens": 843,
    "translated_title": "优化卷积神经网络架构",
    "translated_abstract": "卷积神经网络（CNN）被广泛应用于应对诸如语音识别、自然语言处理或计算机视觉等具有挑战性的任务。随着CNN架构变得越来越大和复杂，其计算需求增加，增加了能源成本，并在资源受限设备上部署变得困难。在本文中，我们提出了一种基于剪枝和知识蒸馏的优化卷积神经网络架构（OCNNA），旨在建立卷积层的重要性。我们通过全面的实证研究评估了该提议，包括最好的已知数据集（CIFAR-10，CIFAR-100和Imagenet）和CNN架构（VGG-16，ResNet-50，DenseNet-40和MobileNet），通过精度下降和剩余参数比率作为客观指标来比较OCNNA与其他先进方法的性能。我们的方法与20多个卷积深度网络架构相比较。",
    "tldr": "本文提出了一种优化卷积神经网络架构的方法，通过剪枝和知识蒸馏来建立卷积层的重要性。通过实证研究，我们证明了该方法在各种数据集和CNN架构上的性能优于其他先进方法。"
}