{
    "title": "Question Translation Training for Better Multilingual Reasoning",
    "abstract": "arXiv:2401.07817v2 Announce Type: replace  Abstract: Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of mathematical chain-of-thought. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English parallel question data. In this way we perform targeted, in-domain language alignment which makes best use of English instruction data to unlock the LLMs' multilingual reasoning abilities. Experimental results on LLaMA2-13",
    "link": "https://arxiv.org/abs/2401.07817",
    "context": "Title: Question Translation Training for Better Multilingual Reasoning\nAbstract: arXiv:2401.07817v2 Announce Type: replace  Abstract: Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of mathematical chain-of-thought. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English parallel question data. In this way we perform targeted, in-domain language alignment which makes best use of English instruction data to unlock the LLMs' multilingual reasoning abilities. Experimental results on LLaMA2-13",
    "path": "papers/24/01/2401.07817.json",
    "total_tokens": 799,
    "translated_title": "为了更好的多语言推理能力而进行的问题翻译训练",
    "translated_abstract": "大型语言模型在推理任务上表现出色，但在非英语语言上的表现往往较差。传统解决方案是将指导数据翻译成所有感兴趣的语言，然后在生成的多语言数据上进行训练，这被称为翻译训练。本文探讨了问题对齐的好处，通过在X-英语平行问题数据上微调，训练模型将推理问题翻译成英语。这种方法通过有针对性的、领域内的语言对齐，最大限度地利用英语指导数据，释放了LLMs的多语言推理能力。实验结果表明在LLaMA2-13上",
    "tldr": "本文探讨了通过问题对齐训练模型将推理问题翻译成英语的方法，以实现有针对性的、领域内的语言对齐，最大限度地利用英语指导数据，释放了LLMs的多语言推理能力。",
    "en_tdlr": "This paper explores the approach of translating reasoning questions into English through question alignment training to achieve targeted, in-domain language alignment, making the best use of English instruction data to unleash the multilingual reasoning abilities of LLMs."
}