{
    "title": "Mitigating the Impact of False Negatives in Dense Retrieval with Contrastive Confidence Regularization. (arXiv:2401.00165v2 [cs.CL] UPDATED)",
    "abstract": "In open-domain Question Answering (QA), dense retrieval is crucial for finding relevant passages for answer generation. Typically, contrastive learning is used to train a retrieval model that maps passages and queries to the same semantic space. The objective is to make similar ones closer and dissimilar ones further apart. However, training such a system is challenging due to the false negative issue, where relevant passages may be missed during data annotation. Hard negative sampling, which is commonly used to improve contrastive learning, can introduce more noise in training. This is because hard negatives are those closer to a given query, and thus more likely to be false negatives. To address this issue, we propose a novel contrastive confidence regularizer for Noise Contrastive Estimation (NCE) loss, a commonly used loss for dense retrieval. Our analysis shows that the regularizer helps dense retrieval models be more robust against false negatives with a theoretical guarantee. Ad",
    "link": "http://arxiv.org/abs/2401.00165",
    "context": "Title: Mitigating the Impact of False Negatives in Dense Retrieval with Contrastive Confidence Regularization. (arXiv:2401.00165v2 [cs.CL] UPDATED)\nAbstract: In open-domain Question Answering (QA), dense retrieval is crucial for finding relevant passages for answer generation. Typically, contrastive learning is used to train a retrieval model that maps passages and queries to the same semantic space. The objective is to make similar ones closer and dissimilar ones further apart. However, training such a system is challenging due to the false negative issue, where relevant passages may be missed during data annotation. Hard negative sampling, which is commonly used to improve contrastive learning, can introduce more noise in training. This is because hard negatives are those closer to a given query, and thus more likely to be false negatives. To address this issue, we propose a novel contrastive confidence regularizer for Noise Contrastive Estimation (NCE) loss, a commonly used loss for dense retrieval. Our analysis shows that the regularizer helps dense retrieval models be more robust against false negatives with a theoretical guarantee. Ad",
    "path": "papers/24/01/2401.00165.json",
    "total_tokens": 874,
    "translated_title": "通过对比置信度正则化来减轻密集检索中的假阴性影响",
    "translated_abstract": "在开放域问答中，密集检索对于找到相关段落以生成答案至关重要。通常，对比学习用于训练一个将段落和查询映射到相同语义空间的检索模型。其目标是使相似的更加接近，不相似的更加远离。然而，由于虚假阴性问题，训练这样的系统是具有挑战性的，即在数据注释过程中可能会错过相关段落。通常用于改进对比学习的硬负例采样可能会在训练中引入更多噪声。这是因为硬负例是那些靠近给定查询的样本，因此更有可能是虚假阴性。为了解决这个问题，我们提出了一种新颖的对比置信度正则化器，用于密集检索常用的噪声对比估计（NCE）损失。我们的分析表明，该正则化器能够帮助密集检索模型更加稳健地应对虚假阴性，并具有理论保证。",
    "tldr": "该论文提出了一种用于减轻密集检索中假阴性影响的对比置信度正则化方法，通过改进对比学习的硬负例采样问题，使得密集检索模型更加稳健。"
}