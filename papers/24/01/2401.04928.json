{
    "title": "Relaxed Contrastive Learning for Federated Learning. (arXiv:2401.04928v1 [cs.LG])",
    "abstract": "We propose a novel contrastive learning framework to effectively address the challenges of data heterogeneity in federated learning. We first analyze the inconsistency of gradient updates across clients during local training and establish its dependence on the distribution of feature representations, leading to the derivation of the supervised contrastive learning (SCL) objective to mitigate local deviations. In addition, we show that a na\\\"ive adoption of SCL in federated learning leads to representation collapse, resulting in slow convergence and limited performance gains. To address this issue, we introduce a relaxed contrastive learning loss that imposes a divergence penalty on excessively similar sample pairs within each class. This strategy prevents collapsed representations and enhances feature transferability, facilitating collaborative training and leading to significant performance improvements. Our framework outperforms all existing federated learning approaches by huge marg",
    "link": "http://arxiv.org/abs/2401.04928",
    "context": "Title: Relaxed Contrastive Learning for Federated Learning. (arXiv:2401.04928v1 [cs.LG])\nAbstract: We propose a novel contrastive learning framework to effectively address the challenges of data heterogeneity in federated learning. We first analyze the inconsistency of gradient updates across clients during local training and establish its dependence on the distribution of feature representations, leading to the derivation of the supervised contrastive learning (SCL) objective to mitigate local deviations. In addition, we show that a na\\\"ive adoption of SCL in federated learning leads to representation collapse, resulting in slow convergence and limited performance gains. To address this issue, we introduce a relaxed contrastive learning loss that imposes a divergence penalty on excessively similar sample pairs within each class. This strategy prevents collapsed representations and enhances feature transferability, facilitating collaborative training and leading to significant performance improvements. Our framework outperforms all existing federated learning approaches by huge marg",
    "path": "papers/24/01/2401.04928.json",
    "total_tokens": 851,
    "translated_title": "放松的对比学习用于联邦学习",
    "translated_abstract": "我们提出了一种新颖的对比学习框架，以有效解决联邦学习中的数据异构性挑战。我们首先分析了本地训练中客户端之间梯度更新的不一致性，并建立其与特征表示分布的依赖关系，从而导出了监督对比学习（SCL）目标来减轻局部偏差。此外，我们还展示了在联邦学习中对SCL的朴素应用会导致表示坍缩，导致收敛缓慢和性能提升有限。为了解决这个问题，我们引入了一种放松的对比学习损失，对每个类别内过于相似的样本对施加发散惩罚。这种策略可以防止表示坍缩，增强特征的可传递性，促进协作训练，并导致显著的性能提升。我们的框架在所有现有的联邦学习方法中表现出巨大的优势。",
    "tldr": "我们提出了一种放松的对比学习框架，用于解决联邦学习中的数据异构性挑战。我们的方法通过引入放松的对比学习损失，防止表示坍缩，增强特征的可传递性，从而实现了显著的性能提升。"
}