{
    "title": "KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning. (arXiv:2401.12863v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in natural language processing tasks by leveraging chain of thought (CoT) that enables step-by-step thinking. Extending LLMs with multimodal capabilities is the recent interest, but incurs computational cost and requires substantial hardware resources. To address these challenges, we propose KAM-CoT a framework that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a two-stage training process with KG grounding to generate effective rationales and answers. By incorporating external knowledge from KGs during reasoning, the model gains a deeper contextual understanding reducing hallucinations and enhancing the quality of answers. This knowledge-augmented CoT reasoning empowers the model to handle questions requiring external context, providing more informed answers. Experimental findings show KAM-CoT outperforms the state-of-t",
    "link": "http://arxiv.org/abs/2401.12863",
    "context": "Title: KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning. (arXiv:2401.12863v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have demonstrated impressive performance in natural language processing tasks by leveraging chain of thought (CoT) that enables step-by-step thinking. Extending LLMs with multimodal capabilities is the recent interest, but incurs computational cost and requires substantial hardware resources. To address these challenges, we propose KAM-CoT a framework that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a two-stage training process with KG grounding to generate effective rationales and answers. By incorporating external knowledge from KGs during reasoning, the model gains a deeper contextual understanding reducing hallucinations and enhancing the quality of answers. This knowledge-augmented CoT reasoning empowers the model to handle questions requiring external context, providing more informed answers. Experimental findings show KAM-CoT outperforms the state-of-t",
    "path": "papers/24/01/2401.12863.json",
    "total_tokens": 887,
    "translated_title": "KAM-CoT: 知识增强的多模式思维链推理",
    "translated_abstract": "大型语言模型（LLM）通过利用思维链（CoT）实现了在自然语言处理任务中令人印象深刻的性能，在最近的研究中，将LLMs扩展为多模式能力是一个有吸引力的方向，但会增加计算成本并需要大量硬件资源。为了应对这些挑战，我们提出了KAM-CoT框架，该框架融合了CoT推理、知识图谱（KGs）和多种模式，以全面理解多模式任务。KAM-CoT采用两阶段训练过程，通过KG基础生成有效的理由和答案。通过在推理过程中引入来自知识图谱的外部知识，模型获得了更深入的语境理解，减少了虚构和改善了答案的质量。这种知识增强的CoT推理使模型能够处理需要外部上下文的问题，并提供更有根据的答案。实验结果显示，KAM-CoT在性能上优于现有的方法。",
    "tldr": "KAM-CoT是一个知识增强的多模式思维链推理框架，可以通过引入外部知识图谱来提高模型对多模式任务的理解能力，并生成更准确的答案。"
}