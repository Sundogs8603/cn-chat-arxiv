{
    "title": "Cross-Domain Few-Shot Learning via Adaptive Transformer Networks. (arXiv:2401.13987v1 [cs.LG])",
    "abstract": "Most few-shot learning works rely on the same domain assumption between the base and the target tasks, hindering their practical applications. This paper proposes an adaptive transformer network (ADAPTER), a simple but effective solution for cross-domain few-shot learning where there exist large domain shifts between the base task and the target task. ADAPTER is built upon the idea of bidirectional cross-attention to learn transferable features between the two domains. The proposed architecture is trained with DINO to produce diverse, and less biased features to avoid the supervision collapse problem. Furthermore, the label smoothing approach is proposed to improve the consistency and reliability of the predictions by also considering the predicted labels of the close samples in the embedding space. The performance of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it outperforms prior arts with significant margins.",
    "link": "http://arxiv.org/abs/2401.13987",
    "context": "Title: Cross-Domain Few-Shot Learning via Adaptive Transformer Networks. (arXiv:2401.13987v1 [cs.LG])\nAbstract: Most few-shot learning works rely on the same domain assumption between the base and the target tasks, hindering their practical applications. This paper proposes an adaptive transformer network (ADAPTER), a simple but effective solution for cross-domain few-shot learning where there exist large domain shifts between the base task and the target task. ADAPTER is built upon the idea of bidirectional cross-attention to learn transferable features between the two domains. The proposed architecture is trained with DINO to produce diverse, and less biased features to avoid the supervision collapse problem. Furthermore, the label smoothing approach is proposed to improve the consistency and reliability of the predictions by also considering the predicted labels of the close samples in the embedding space. The performance of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it outperforms prior arts with significant margins.",
    "path": "papers/24/01/2401.13987.json",
    "total_tokens": 955,
    "translated_title": "通过自适应变压器网络进行跨领域的少样本学习",
    "translated_abstract": "大多数少样本学习方法依赖于基任务和目标任务之间的相同领域的假设，限制了它们的实际应用。本文提出了自适应变压器网络（ADAPTER），这是一种简单但有效的解决方案，用于处理基任务和目标任务之间存在大领域转移的跨领域少样本学习。ADAPTER基于双向交叉注意力的思想构建，以学习两个领域之间的可转移特征。所提出的架构使用DINO进行训练，以产生多样的、较少偏见的特征，以避免监督崩溃问题。此外，还提出了标签平滑方法，通过在嵌入空间中同时考虑接近样本的预测标签，提高预测的一致性和可靠性。ADAPTER的性能在BSCD-FSL基准测试中进行了严格评估，它以显著的优势超越了之前的方法。",
    "tldr": "本文提出了一种名为ADAPTER的自适应变压器网络，用于跨领域的少样本学习。借助双向交叉注意力和标签平滑方法，ADAPTER能够在存在领域转移的情况下学习可转移的特征，并避免监督崩溃问题。该方法在BSCD-FSL基准测试中表现优秀，超过了之前的方法。",
    "en_tdlr": "This paper proposes an adaptive transformer network (ADAPTER) for cross-domain few-shot learning. By utilizing bidirectional cross-attention and label smoothing, ADAPTER is able to learn transferable features in the presence of domain shifts and avoid the problem of supervision collapse. The method outperforms prior arts in the BSCD-FSL benchmarks."
}