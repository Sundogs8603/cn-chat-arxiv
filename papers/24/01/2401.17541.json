{
    "title": "Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration",
    "abstract": "Machine learning models traditionally assume that training and test data are independently and identically distributed. However, in real-world applications, the test distribution often differs from training. This problem, known as out-of-distribution generalization, challenges conventional models. Invariant Risk Minimization (IRM) emerges as a solution, aiming to identify features invariant across different environments to enhance out-of-distribution robustness. However, IRM's complexity, particularly its bi-level optimization, has led to the development of various approximate methods. Our study investigates these approximate IRM techniques, employing the Expected Calibration Error (ECE) as a key metric. ECE, which measures the reliability of model prediction, serves as an indicator of whether models effectively capture environment-invariant features. Through a comparative analysis of datasets with distributional shifts, we observe that Information Bottleneck-based IRM, which condenses",
    "link": "https://arxiv.org/abs/2401.17541",
    "context": "Title: Towards Understanding Variants of Invariant Risk Minimization through the Lens of Calibration\nAbstract: Machine learning models traditionally assume that training and test data are independently and identically distributed. However, in real-world applications, the test distribution often differs from training. This problem, known as out-of-distribution generalization, challenges conventional models. Invariant Risk Minimization (IRM) emerges as a solution, aiming to identify features invariant across different environments to enhance out-of-distribution robustness. However, IRM's complexity, particularly its bi-level optimization, has led to the development of various approximate methods. Our study investigates these approximate IRM techniques, employing the Expected Calibration Error (ECE) as a key metric. ECE, which measures the reliability of model prediction, serves as an indicator of whether models effectively capture environment-invariant features. Through a comparative analysis of datasets with distributional shifts, we observe that Information Bottleneck-based IRM, which condenses",
    "path": "papers/24/01/2401.17541.json",
    "total_tokens": 915,
    "translated_title": "透过校准的视角理解不变风险最小化的变体",
    "translated_abstract": "传统的机器学习模型假设训练和测试数据是独立且同分布的。然而，在现实世界的应用中，测试分布往往与训练不同。这个问题被称为越域泛化，在常规模型面临挑战。不变风险最小化（IRM）作为一个解决方案出现，旨在识别在不同环境中保持不变的特征，以增强越域鲁棒性。然而，IRM的复杂性，特别是其双层优化，导致了各种近似方法的开发。我们的研究调查了这些近似IRM技术，使用期望校准误差（ECE）作为关键指标。ECE可以衡量模型预测的可靠性，它是衡量模型是否有效捕捉到环境不变特征的指标。通过对具有分布变化的数据集进行比较分析，我们观察到基于信息瓶颈的IRM在压缩了...（接下部分摘要超过200字，提取前200字）",
    "tldr": "本研究通过比较分析使用不同近似方法的不变风险最小化（IRM）技术，以期望校准误差（ECE）作为关键指标，观察到基于信息瓶颈的IRM在压缩关键特征上表现最佳。"
}