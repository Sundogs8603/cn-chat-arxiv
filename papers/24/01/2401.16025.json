{
    "title": "Simple Policy Optimization",
    "abstract": "arXiv:2401.16025v2 Announce Type: replace  Abstract: PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose Simple Policy Optimization (SPO) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. Extensive experimental results in Atari 2600 environments indicate that, compared to the mainstream variants of PPO, SPO achieves better sample efficiency, extremely low KL divergence, and higher policy entropy, and is robust to the increase in network depth or complexity. More importantly, SPO maintains the simplicity of an unconstrained first-order algorithm. Code is available at https://github.co",
    "link": "https://arxiv.org/abs/2401.16025",
    "context": "Title: Simple Policy Optimization\nAbstract: arXiv:2401.16025v2 Announce Type: replace  Abstract: PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose Simple Policy Optimization (SPO) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. Extensive experimental results in Atari 2600 environments indicate that, compared to the mainstream variants of PPO, SPO achieves better sample efficiency, extremely low KL divergence, and higher policy entropy, and is robust to the increase in network depth or complexity. More importantly, SPO maintains the simplicity of an unconstrained first-order algorithm. Code is available at https://github.co",
    "path": "papers/24/01/2401.16025.json",
    "total_tokens": 839,
    "translated_title": "简单策略优化",
    "translated_abstract": "PPO（Proximal Policy Optimization）算法在许多领域表现出色，被认为是TRPO（Trust Region Policy Optimization）算法的简化版本。然而，PPO中的比率剪切操作并不总是有效地强制执行信任区域约束，这可能会影响算法的稳定性。本文提出了一种新颖的剪切方法，即Simple Policy Optimization（SPO）算法，用于旧策略和当前策略之间的KL散度。在Atari 2600环境中进行的大量实验结果表明，与PPO的主流变体相比，SPO实现了更好的样本效率，极低的KL散度和更高的策略熵，并且对网络深度或复杂度的增加具有鲁棒性。更重要的是，SPO保持了无约束一阶算法的简单性。",
    "tldr": "SPO算法引入了新的KL散度剪切方法，相较于PPO的主流变体，在Atari 2600环境中表现出更好的样本效率、极低的KL散度和更高的策略熵，且对网络深度或复杂度的增加具有鲁棒性。",
    "en_tdlr": "SPO algorithm introduces a novel KL divergence clipping method, showing better sample efficiency, extremely low KL divergence, and higher policy entropy compared to mainstream variants of PPO in Atari 2600 environments, and it is robust to the increase in network depth or complexity."
}