{
    "title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models. (arXiv:2401.09861v1 [cs.CV])",
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content, bringing together diverse modalities such as text, images, and videos. However, a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently, we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation, conducted using the Charades-STA dataset, demonstrate",
    "link": "http://arxiv.org/abs/2401.09861",
    "context": "Title: Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models. (arXiv:2401.09861v1 [cs.CV])\nAbstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content, bringing together diverse modalities such as text, images, and videos. However, a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently, we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation, conducted using the Charades-STA dataset, demonstrate",
    "path": "papers/24/01/2401.09861.json",
    "total_tokens": 853,
    "translated_title": "时间洞察增强：减轻多模态大型语言模型中的时间幻觉",
    "translated_abstract": "近年来，多模态大型语言模型(MLLMs)的发展显著增强了对多媒体内容的理解能力，将文本、图像和视频等多种模态集合在一起。然而，这些模型面临的一个关键挑战，尤其是在处理视频输入时，是产生幻觉 - 错误的感知或解释，特别是在事件层面上。本研究引入了一种创新的方法，以解决MLLMs中的事件级幻觉问题，重点关注视频内容的时间理解。我们的方法利用了一种新颖的框架，从事件查询和提供的视频中提取并利用事件特定信息来优化MLLMs的响应。我们提出了一种独特的机制，将按需的事件查询分解为代表性的行为。随后，我们使用类似CLIP和BLIP2的模型来预测事件发生的具体时间戳。我们使用Charades-STA数据集进行了评估，结果表明...",
    "tldr": "本研究提出了一种创新的方法，通过从视频内容中提取时间特定的信息，来解决多模态大型语言模型中的事件级幻觉问题。",
    "en_tdlr": "This study introduces an innovative method to address event-level hallucinations in Multimodal Large Language Models (MLLMs) by extracting temporal-specific information from video content."
}