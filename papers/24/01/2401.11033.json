{
    "title": "FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?. (arXiv:2401.11033v1 [cs.CL])",
    "abstract": "Advancements in Large Language Models (LLMs) highlight the need for ethical practices and data integrity. We introduce a framework that embeds FAIR (Findable, Accessible, Interoperable, Reusable) data principles into LLM training. This approach marks a shift towards practices compliant with FAIR standards. Our framework presents guidelines for integrating FAIR data principles into LLM training. This initiative includes a checklist for researchers and developers. We also demonstrate its practical application through a case study focused on bias identification and mitigation in our FAIR-compliant dataset. This work is a significant contribution to AI ethics and data science, advocating for balanced and ethical training methods in LLMs.",
    "link": "http://arxiv.org/abs/2401.11033",
    "context": "Title: FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?. (arXiv:2401.11033v1 [cs.CL])\nAbstract: Advancements in Large Language Models (LLMs) highlight the need for ethical practices and data integrity. We introduce a framework that embeds FAIR (Findable, Accessible, Interoperable, Reusable) data principles into LLM training. This approach marks a shift towards practices compliant with FAIR standards. Our framework presents guidelines for integrating FAIR data principles into LLM training. This initiative includes a checklist for researchers and developers. We also demonstrate its practical application through a case study focused on bias identification and mitigation in our FAIR-compliant dataset. This work is a significant contribution to AI ethics and data science, advocating for balanced and ethical training methods in LLMs.",
    "path": "papers/24/01/2401.11033.json",
    "total_tokens": 856,
    "translated_title": "FAIR到位：我们如何为大型语言模型的训练开发和评估符合FAIR标准的数据集？",
    "translated_abstract": "大型语言模型的进展凸显了道德实践和数据完整性的需求。我们引入了一个将FAIR（可发现、可访问、可互操作、可重用）数据原则嵌入到LLM训练中的框架。这一方法标志着朝着符合FAIR标准的实践的转变。我们的框架提供了将FAIR数据原则整合到LLM训练中的指南。该举措包括研究人员和开发人员的检查清单。我们还通过一个案例研究展示了它的实际应用，重点是在我们符合FAIR标准的数据集中识别和减轻偏见。这项工作对于人工智能伦理和数据科学是重要的贡献，倡导在LLMs中使用平衡和道德的训练方法。",
    "tldr": "这项工作介绍了一个将FAIR数据原则嵌入到大型语言模型训练中的框架，并提供了整合指南和检查清单。通过一个案例研究，展示了在符合FAIR标准的数据集中识别和减轻偏见的实际应用。"
}