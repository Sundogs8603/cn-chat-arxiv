{
    "title": "BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models. (arXiv:2401.13974v1 [cs.CV])",
    "abstract": "Recent text-to-image generation models have demonstrated incredible success in generating images that faithfully follow input prompts. However, the requirement of using words to describe a desired concept provides limited control over the appearance of the generated concepts. In this work, we address this shortcoming by proposing an approach to enable personalization capabilities in existing text-to-image diffusion models. We propose a novel architecture (BootPIG) that allows a user to provide reference images of an object in order to guide the appearance of a concept in the generated images.  The proposed BootPIG architecture makes minimal modifications to a pretrained text-to-image diffusion model and utilizes a separate UNet model to steer the generations toward the desired appearance. We introduce a training procedure that allows us to bootstrap personalization capabilities in the BootPIG architecture using data generated from pretrained text-to-image models, LLM chat agents, and i",
    "link": "http://arxiv.org/abs/2401.13974",
    "context": "Title: BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models. (arXiv:2401.13974v1 [cs.CV])\nAbstract: Recent text-to-image generation models have demonstrated incredible success in generating images that faithfully follow input prompts. However, the requirement of using words to describe a desired concept provides limited control over the appearance of the generated concepts. In this work, we address this shortcoming by proposing an approach to enable personalization capabilities in existing text-to-image diffusion models. We propose a novel architecture (BootPIG) that allows a user to provide reference images of an object in order to guide the appearance of a concept in the generated images.  The proposed BootPIG architecture makes minimal modifications to a pretrained text-to-image diffusion model and utilizes a separate UNet model to steer the generations toward the desired appearance. We introduce a training procedure that allows us to bootstrap personalization capabilities in the BootPIG architecture using data generated from pretrained text-to-image models, LLM chat agents, and i",
    "path": "papers/24/01/2401.13974.json",
    "total_tokens": 918,
    "translated_title": "BootPIG: 在预训练扩散模型中引导零样本个性化图像生成能力的引导模型",
    "translated_abstract": "最近的文本到图像生成模型在生成遵循输入提示的图像方面取得了令人难以置信的成功。然而，使用词语来描述所需的概念限制了对生成图像外观的控制能力。在本文中，我们提出了一种方法，通过在现有的文本到图像扩散模型中引入个性化能力来解决这个问题。我们提出了一种新的架构（BootPIG），允许用户提供目标对象的参考图像，以指导生成图像中概念的外观。提出的BootPIG架构对预训练的文本到图像扩散模型进行最小的修改，并利用一个单独的UNet模型来引导生成图像的外观。我们介绍了一种训练过程，能够使用从预训练的文本到图像模型、LLM聊天代理和i（候选图像）生成的数据来引导BootPIG架构的个性化能力。",
    "tldr": "提出了BootPIG架构，在预训练扩散模型中引导个性化图像生成能力。该架构使用用户提供的参考图像来指导生成图像中概念的外观。通过利用预训练的模型生成的数据进行训练，实现了对BootPIG架构的个性化能力的引导。",
    "en_tdlr": "BootPIG is proposed to bootstrap personalized image generation capabilities in pretrained diffusion models. The architecture guides the appearance of concepts in generated images using user-provided reference images. By utilizing data generated from pretrained models, BootPIG enables personalized image generation."
}