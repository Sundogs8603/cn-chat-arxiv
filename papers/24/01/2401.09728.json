{
    "title": "Offline Imitation Learning by Controlling the Effective Planning Horizon. (arXiv:2401.09728v1 [cs.LG])",
    "abstract": "In offline imitation learning (IL), we generally assume only a handful of expert trajectories and a supplementary offline dataset from suboptimal behaviors to learn the expert policy. While it is now common to minimize the divergence between state-action visitation distributions so that the agent also considers the future consequences of an action, a sampling error in an offline dataset may lead to erroneous estimates of state-action visitations in the offline case. In this paper, we investigate the effect of controlling the effective planning horizon (i.e., reducing the discount factor) as opposed to imposing an explicit regularizer, as previously studied. Unfortunately, it turns out that the existing algorithms suffer from magnified approximation errors when the effective planning horizon is shortened, which results in a significant degradation in performance. We analyze the main cause of the problem and provide the right remedies to correct the algorithm. We show that the corrected ",
    "link": "http://arxiv.org/abs/2401.09728",
    "context": "Title: Offline Imitation Learning by Controlling the Effective Planning Horizon. (arXiv:2401.09728v1 [cs.LG])\nAbstract: In offline imitation learning (IL), we generally assume only a handful of expert trajectories and a supplementary offline dataset from suboptimal behaviors to learn the expert policy. While it is now common to minimize the divergence between state-action visitation distributions so that the agent also considers the future consequences of an action, a sampling error in an offline dataset may lead to erroneous estimates of state-action visitations in the offline case. In this paper, we investigate the effect of controlling the effective planning horizon (i.e., reducing the discount factor) as opposed to imposing an explicit regularizer, as previously studied. Unfortunately, it turns out that the existing algorithms suffer from magnified approximation errors when the effective planning horizon is shortened, which results in a significant degradation in performance. We analyze the main cause of the problem and provide the right remedies to correct the algorithm. We show that the corrected ",
    "path": "papers/24/01/2401.09728.json",
    "total_tokens": 839,
    "translated_title": "离线模仿学习通过控制有效规划范围",
    "translated_abstract": "在离线模仿学习中，我们通常假设只有少量的专家路径和来自次优行为的离线数据集来学习专家策略。虽然现在常常通过最小化状态-动作访问分布的差异来使代理考虑到动作的未来后果，但是离线数据集中的抽样误差可能导致对离线情况下状态-动作访问估计的错误。在本文中，我们研究了通过控制有效规划范围（即减小折扣因子）来对抗显式正则化之前所研究的影响。不幸的是，现有算法在有效规划范围缩短时会出现放大的逼近误差，从而导致性能的显著下降。我们分析了问题的主要原因，并提供了正确的修正算法方法。我们展示了修正后算法的效果，并验证了它的有效性。",
    "tldr": "本文研究了离线模仿学习中通过控制有效规划范围来提高性能的问题，并提出了修正算法方法，解决了存在的逼近误差问题。"
}