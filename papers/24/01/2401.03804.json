{
    "title": "TeleChat Technical Report",
    "abstract": "arXiv:2401.03804v2 Announce Type: replace-cross  Abstract: In this technical report, we present TeleChat, a collection of large language models (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, including trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves comparable performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing ",
    "link": "https://arxiv.org/abs/2401.03804",
    "context": "Title: TeleChat Technical Report\nAbstract: arXiv:2401.03804v2 Announce Type: replace-cross  Abstract: In this technical report, we present TeleChat, a collection of large language models (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It includes pretrained language models as well as fine-tuned chat models that is aligned with human preferences. TeleChat is initially pretrained on an extensive corpus containing a diverse collection of texts from both English and Chinese languages, including trillions of tokens. Subsequently, the model undergoes fine-tuning to align with human preferences, following a detailed methodology that we describe. We evaluate the performance of TeleChat on various tasks, including language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. Our findings indicate that TeleChat achieves comparable performance to other open-source models of similar size across a wide range of public benchmarks. To support future research and applications utilizing ",
    "path": "papers/24/01/2401.03804.json",
    "total_tokens": 851,
    "translated_title": "TeleChat技术报告",
    "translated_abstract": "在这份技术报告中，我们介绍了TeleChat，这是由30亿、70亿和120亿参数的大型语言模型（LLMs）组成的合集。它包括预训练的语言模型以及与人类喜好相一致的微调聊天模型。TeleChat最初在包含英语和中文语言的多样文本集合中预训练，包括数万亿的标记。随后，该模型经过微调以与人类喜好相一致，遵循我们描述的详细方法论。我们评估了TeleChat在各种任务上的表现，包括语言理解、数学、推理、代码生成和基于知识的问答。我们的发现表明，TeleChat在各种公开基准测试中表现出与其他开源模型相似规模的可比性能。为支持未来利用该模型进行研究和应用，",
    "tldr": "TeleChat是一个包含30亿、70亿和120亿参数的大型语言模型合集，旨在提供预训练语言模型和与人类喜好相一致的微调聊天模型。研究表明，TeleChat在各种任务上表现出与其他类似规模的开源模型相当的性能。",
    "en_tdlr": "TeleChat is a collection of large language models with 3 billion, 7 billion, and 12 billion parameters, aiming to provide pretrained language models and fine-tuned chat models aligned with human preferences. The research indicates that TeleChat performs comparably to other open-source models of similar scale across various tasks."
}