{
    "title": "Large Language Models Relearn Removed Concepts. (arXiv:2401.01814v1 [cs.AI])",
    "abstract": "Advances in model editing through neuron pruning hold promise for removing undesirable concepts from large language models. However, it remains unclear whether models have the capacity to reacquire pruned concepts after editing. To investigate this, we evaluate concept relearning in models by tracking concept saliency and similarity in pruned neurons during retraining. Our findings reveal that models can quickly regain performance post-pruning by relocating advanced concepts to earlier layers and reallocating pruned concepts to primed neurons with similar semantics. This demonstrates that models exhibit polysemantic capacities and can blend old and new concepts in individual neurons. While neuron pruning provides interpretability into model concepts, our results highlight the challenges of permanent concept removal for improved model \\textit{safety}. Monitoring concept reemergence and developing techniques to mitigate relearning of unsafe concepts will be important directions for more ",
    "link": "http://arxiv.org/abs/2401.01814",
    "context": "Title: Large Language Models Relearn Removed Concepts. (arXiv:2401.01814v1 [cs.AI])\nAbstract: Advances in model editing through neuron pruning hold promise for removing undesirable concepts from large language models. However, it remains unclear whether models have the capacity to reacquire pruned concepts after editing. To investigate this, we evaluate concept relearning in models by tracking concept saliency and similarity in pruned neurons during retraining. Our findings reveal that models can quickly regain performance post-pruning by relocating advanced concepts to earlier layers and reallocating pruned concepts to primed neurons with similar semantics. This demonstrates that models exhibit polysemantic capacities and can blend old and new concepts in individual neurons. While neuron pruning provides interpretability into model concepts, our results highlight the challenges of permanent concept removal for improved model \\textit{safety}. Monitoring concept reemergence and developing techniques to mitigate relearning of unsafe concepts will be important directions for more ",
    "path": "papers/24/01/2401.01814.json",
    "total_tokens": 914,
    "translated_title": "大型语言模型可以重新学习已删除的概念",
    "translated_abstract": "通过神经元修剪来改进模型编辑有望从大型语言模型中去除不理想的概念。然而，模型是否具有重新学习修剪掉的概念的能力仍不清楚。为了调查这个问题，我们通过跟踪修剪神经元中的概念显著性和相似性，在模型中评估了概念的重新学习。我们的研究发现，模型可以通过将高级概念重新分配给较早的层，并将修剪掉的概念重新分配给具有相似语义的神经元来迅速恢复修剪后的性能。这表明模型具有多义能力，可以在单个神经元中融合旧的和新的概念。尽管神经元修剪提供了对模型概念的可解释性，但我们的结果突显了永久删除概念以改善模型安全性的挑战。监控概念重新出现并开发减少重新学习不安全概念的技术将是更好的模型安全方向的重要研究方向。",
    "tldr": "大型语言模型可以通过重新分配概念到不同层级和修剪前的神经元实现对已删除概念的重新学习，这表明模型具有多义能力，但同时也带来了在改善模型安全性方面的挑战。"
}