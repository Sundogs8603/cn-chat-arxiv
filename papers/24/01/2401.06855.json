{
    "title": "Fine-grained Hallucination Detection and Editing for Language Models. (arXiv:2401.06855v2 [cs.CL] UPDATED)",
    "abstract": "Large language models (LMs) are prone to generate diverse factually incorrect statements, which are widely called hallucinations. Current approaches predominantly focus on coarse-grained automatic hallucination detection or editing, overlooking nuanced error levels. In this paper, we propose a novel task -- automatic fine-grained hallucination detection -- and present a comprehensive taxonomy encompassing six hierarchically defined types of hallucination. To facilitate evaluation, we introduce a new benchmark that includes fine-grained human judgments on two LM outputs across various domains. Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in 60% and 75% of their outputs, respectively, and a majority of these hallucinations fall into categories that have been underexplored. As an initial step to address this, we train FAVA, a retrieval-augmented LM by carefully designing synthetic data generations to detect and correct fine-grained hallucinations. On our bench",
    "link": "http://arxiv.org/abs/2401.06855",
    "context": "Title: Fine-grained Hallucination Detection and Editing for Language Models. (arXiv:2401.06855v2 [cs.CL] UPDATED)\nAbstract: Large language models (LMs) are prone to generate diverse factually incorrect statements, which are widely called hallucinations. Current approaches predominantly focus on coarse-grained automatic hallucination detection or editing, overlooking nuanced error levels. In this paper, we propose a novel task -- automatic fine-grained hallucination detection -- and present a comprehensive taxonomy encompassing six hierarchically defined types of hallucination. To facilitate evaluation, we introduce a new benchmark that includes fine-grained human judgments on two LM outputs across various domains. Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in 60% and 75% of their outputs, respectively, and a majority of these hallucinations fall into categories that have been underexplored. As an initial step to address this, we train FAVA, a retrieval-augmented LM by carefully designing synthetic data generations to detect and correct fine-grained hallucinations. On our bench",
    "path": "papers/24/01/2401.06855.json",
    "total_tokens": 958,
    "translated_title": "语言模型的细粒度幻觉检测与编辑",
    "translated_abstract": "大型语言模型往往会生成多样的事实不正确的陈述，被广泛称为幻觉。目前的方法主要集中在粗粒度的自动幻觉检测或编辑上，忽视了细微的错误级别。本文提出了一项新任务——自动细粒度幻觉检测，并提出了一个包含六个层次分明的幻觉类型的综合分类法。为了便于评估，我们引入了一个新的基准，其中包括对两个语言模型输出在各个领域上进行细粒度人工判断的数据。我们的分析发现，ChatGPT和Llama 2-Chat的输出中有60%和75%的幻觉，其中多数幻觉属于未被充分探索的类别。作为解决这一问题的初始步骤，我们训练了FAVA，一个经过精心设计合成数据生成来检测和纠正细粒度幻觉的检索增强语言模型。",
    "tldr": "这项研究提出了一个新任务，即自动细粒度幻觉检测，并介绍了一个综合分类方法。通过对两个语言模型的输出进行分析，发现大部分幻觉属于少有的类别。为了解决这个问题，研究者通过训练一个检索增强语言模型，使用合成数据来检测和纠正幻觉。",
    "en_tdlr": "This research proposes a new task of automatic fine-grained hallucination detection and presents a comprehensive taxonomy. Analysis of two language models reveals that most hallucinations fall into underexplored categories. To address this issue, the researchers train a retrieval-augmented language model using synthetic data to detect and correct hallucinations."
}