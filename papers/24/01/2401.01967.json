{
    "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity. (arXiv:2401.01967v1 [cs.CL])",
    "abstract": "While alignment algorithms are now commonly used to tune pre-trained language models towards a user's preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in a pre-trained language model, GPT2-medium. We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting model averts toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the model, reverting it back to its toxic behavior.",
    "link": "http://arxiv.org/abs/2401.01967",
    "context": "Title: A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity. (arXiv:2401.01967v1 [cs.CL])\nAbstract: While alignment algorithms are now commonly used to tune pre-trained language models towards a user's preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in a pre-trained language model, GPT2-medium. We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting model averts toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the model, reverting it back to its toxic behavior.",
    "path": "papers/24/01/2401.01967.json",
    "total_tokens": 848,
    "translated_title": "对齐算法的机制理解：基于DPO和毒性的案例研究",
    "translated_abstract": "虽然对齐算法现在常用于调整预训练语言模型以适应用户喜好，但我们缺乏解释模型如何“对齐”的基本机制，因此难以解释诸如越狱等现象。本研究中，我们研究了一种常见的算法——直接偏好优化（DPO），以及它如何降低毒性的机制。具体而言，我们首先研究了毒性在预训练语言模型GPT2-medium中的表示和唤起方式。然后，我们使用精心设计的成对数据集应用DPO来降低毒性。我们检查了生成模型是如何避免输出有害结果的，并发现预训练学到的能力并没有被移除，而是被绕过。我们利用这一观察结果展示了一种简单的方法来取消模型的对齐，将其恢复为有害行为。",
    "tldr": "通过研究对齐算法和预训练语言模型，本论文揭示了对齐模型的机制，并提出了一种简单的方法来取消模型的对齐，从而使其恢复有害行为。",
    "en_tdlr": "This paper investigates the mechanisms of alignment algorithms and pre-trained language models, revealing the mechanisms behind alignment and proposing a simple method to un-align the model and restore its toxic behavior."
}