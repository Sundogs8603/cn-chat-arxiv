{
    "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding",
    "abstract": "arXiv:2401.07851v2 Announce Type: replace  Abstract: To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding,",
    "link": "https://arxiv.org/abs/2401.07851",
    "context": "Title: Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding\nAbstract: arXiv:2401.07851v2 Announce Type: replace  Abstract: To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding,",
    "path": "papers/24/01/2401.07851.json",
    "total_tokens": 807,
    "translated_title": "大型语言模型推理效率的提升：投机解码的全面调查",
    "translated_abstract": "为了减少大型语言模型（LLMs）中自回归解码导致的推理延迟，投机解码已经成为LLMs推理的一种新颖解码范式。该方法在每个解码步骤中首先高效地起草几个未来标记，然后并行验证这些标记。与自回归解码不同，投机解码促进了每个步骤同时解码多个标记，从而加速了推理。本文提供了这一有前景的解码范式的全面概述和分析。我们首先提供了对投机解码的正式定义和公式化。然后，我们就其关键方面进行了深入讨论，如起草者选择和验证策略。此外，我们在第三方测试环境下对主要方法进行了比较分析。我们希望这项工作能够成为推动进一步研究投机解码的催化剂。",
    "tldr": "投机解码作为一种新颖的解码范式，能够加速大型语言模型推理过程，提供了全面的概述和分析。"
}