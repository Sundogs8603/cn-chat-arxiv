{
    "title": "Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs. (arXiv:2401.16638v1 [cs.CL])",
    "abstract": "Fine-tuning large pre-trained language models (LLMs) on particular datasets is a commonly employed strategy in Natural Language Processing (NLP) classification tasks. However, this approach usually results in a loss of models generalizability. In this paper, we present a framework that allows for maintaining generalizability, and enhances the performance on the downstream task by utilizing task-specific context attribution. We show that a linear transformation of the text representation from any transformer model using the task-specific concept operator results in a projection onto the latent concept space, referred to as context attribution in this paper. The specific concept operator is optimized during the supervised learning stage via novel loss functions. The proposed framework demonstrates that context attribution of the text representation for each task objective can improve the capacity of the discriminator function and thus achieve better performance for the classification tas",
    "link": "http://arxiv.org/abs/2401.16638",
    "context": "Title: Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs. (arXiv:2401.16638v1 [cs.CL])\nAbstract: Fine-tuning large pre-trained language models (LLMs) on particular datasets is a commonly employed strategy in Natural Language Processing (NLP) classification tasks. However, this approach usually results in a loss of models generalizability. In this paper, we present a framework that allows for maintaining generalizability, and enhances the performance on the downstream task by utilizing task-specific context attribution. We show that a linear transformation of the text representation from any transformer model using the task-specific concept operator results in a projection onto the latent concept space, referred to as context attribution in this paper. The specific concept operator is optimized during the supervised learning stage via novel loss functions. The proposed framework demonstrates that context attribution of the text representation for each task objective can improve the capacity of the discriminator function and thus achieve better performance for the classification tas",
    "path": "papers/24/01/2401.16638.json",
    "total_tokens": 875,
    "translated_title": "打破Transformer模型的束缚：任务特定的上下文归因提供了在不微调预训练LLMs的情况下提高泛化性能的承诺",
    "translated_abstract": "在自然语言处理（NLP）分类任务中，对特定数据集进行大型预训练语言模型（LLMs）的微调是一种常用策略。然而，这种方法通常会导致模型的泛化性能下降。在本文中，我们提出了一个框架，通过利用任务特定的上下文归因，实现了泛化性能的保持，并提升了下游任务的性能。我们展示了任何Transformer模型的文本表示的线性变换，使用任务特定的概念运算符，会得到一个投影到潜在概念空间上的结果，本文中称之为上下文归因。特定的概念运算符在监督学习阶段通过新颖的损失函数进行优化。所提出的框架表明，对于每个任务目标的文本表示进行上下文归因可以改善鉴别器函数的能力，从而实现更好的分类性能。",
    "tldr": "本文提出了一种框架，通过任务特定的上下文归因来提高模型在下游任务中的性能，而不需要对预训练的语言模型进行微调，从而保持了模型的泛化性能。"
}