{
    "title": "A Smoothing Algorithm for l1 Support Vector Machines. (arXiv:2401.09431v1 [math.OC])",
    "abstract": "A smoothing algorithm is presented for solving the soft-margin Support Vector Machine (SVM) optimization problem with an $\\ell^{1}$ penalty. This algorithm is designed to require a modest number of passes over the data, which is an important measure of its cost for very large datasets. The algorithm uses smoothing for the hinge-loss function, and an active set approach for the $\\ell^{1}$ penalty. The smoothing parameter $\\alpha$ is initially large, but typically halved when the smoothed problem is solved to sufficient accuracy. Convergence theory is presented that shows $\\mathcal{O}(1+\\log(1+\\log_+(1/\\alpha)))$ guarded Newton steps for each value of $\\alpha$ except for asymptotic bands $\\alpha=\\Theta(1)$ and $\\alpha=\\Theta(1/N)$, with only one Newton step provided $\\eta\\alpha\\gg1/N$, where $N$ is the number of data points and the stopping criterion that the predicted reduction is less than $\\eta\\alpha$. The experimental results show that our algorithm is capable of strong test accuracy",
    "link": "http://arxiv.org/abs/2401.09431",
    "context": "Title: A Smoothing Algorithm for l1 Support Vector Machines. (arXiv:2401.09431v1 [math.OC])\nAbstract: A smoothing algorithm is presented for solving the soft-margin Support Vector Machine (SVM) optimization problem with an $\\ell^{1}$ penalty. This algorithm is designed to require a modest number of passes over the data, which is an important measure of its cost for very large datasets. The algorithm uses smoothing for the hinge-loss function, and an active set approach for the $\\ell^{1}$ penalty. The smoothing parameter $\\alpha$ is initially large, but typically halved when the smoothed problem is solved to sufficient accuracy. Convergence theory is presented that shows $\\mathcal{O}(1+\\log(1+\\log_+(1/\\alpha)))$ guarded Newton steps for each value of $\\alpha$ except for asymptotic bands $\\alpha=\\Theta(1)$ and $\\alpha=\\Theta(1/N)$, with only one Newton step provided $\\eta\\alpha\\gg1/N$, where $N$ is the number of data points and the stopping criterion that the predicted reduction is less than $\\eta\\alpha$. The experimental results show that our algorithm is capable of strong test accuracy",
    "path": "papers/24/01/2401.09431.json",
    "total_tokens": 896,
    "translated_title": "一个用于l1支持向量机的平滑算法",
    "translated_abstract": "本文提出了一个用于解决具有$\\ell^{1}$惩罚的软间隔支持向量机（SVM）优化问题的平滑算法。该算法旨在对数据进行适度的遍历，这是衡量其对于非常大的数据集的成本的重要指标。该算法使用了对铰链损失函数的平滑和对$\\ell^{1}$惩罚的主动集方法。平滑参数$\\alpha$最初较大，但通常在求解到足够精度的平滑问题后减半。收敛理论表明，在除渐近带$\\alpha=\\Theta(1)$和$\\alpha=\\Theta(1/N)$之外的每个$\\alpha$值上，每个$\\alpha$值需要$\\mathcal{O}(1+\\log(1+\\log_+(1/\\alpha)))$个守卫牛顿步骤，仅当$\\eta\\alpha\\gg1/N$时，提供一个牛顿步骤，其中$N$是数据点的数量，停止准则是预测的减少小于$\\eta\\alpha$。实验结果表明，我们的算法能够获得很高的测试准确性。",
    "tldr": "本文提出了一个用于解决具有$\\ell^{1}$惩罚的软间隔支持向量机（SVM）优化问题的平滑算法，可以在遍历数据时保持较低的成本，并经过实验证明算法具有很高的测试准确性。"
}