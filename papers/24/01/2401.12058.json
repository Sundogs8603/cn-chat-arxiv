{
    "title": "The Dimension Strikes Back with Gradients: Generalization of Gradient Methods in Stochastic Convex Optimization. (arXiv:2401.12058v1 [cs.LG])",
    "abstract": "We study the generalization performance of gradient methods in the fundamental stochastic convex optimization setting, focusing on its dimension dependence. First, for full-batch gradient descent (GD) we give a construction of a learning problem in dimension $d=O(n^2)$, where the canonical version of GD (tuned for optimal performance of the empirical risk) trained with $n$ training examples converges, with constant probability, to an approximate empirical risk minimizer with $\\Omega(1)$ population excess risk. Our bound translates to a lower bound of $\\Omega (\\sqrt{d})$ on the number of training examples required for standard GD to reach a non-trivial test error, answering an open question raised by Feldman (2016) and Amir, Koren, and Livni (2021b) and showing that a non-trivial dimension dependence is unavoidable. Furthermore, for standard one-pass stochastic gradient descent (SGD), we show that an application of the same construction technique provides a similar $\\Omega(\\sqrt{d})$ lo",
    "link": "http://arxiv.org/abs/2401.12058",
    "context": "Title: The Dimension Strikes Back with Gradients: Generalization of Gradient Methods in Stochastic Convex Optimization. (arXiv:2401.12058v1 [cs.LG])\nAbstract: We study the generalization performance of gradient methods in the fundamental stochastic convex optimization setting, focusing on its dimension dependence. First, for full-batch gradient descent (GD) we give a construction of a learning problem in dimension $d=O(n^2)$, where the canonical version of GD (tuned for optimal performance of the empirical risk) trained with $n$ training examples converges, with constant probability, to an approximate empirical risk minimizer with $\\Omega(1)$ population excess risk. Our bound translates to a lower bound of $\\Omega (\\sqrt{d})$ on the number of training examples required for standard GD to reach a non-trivial test error, answering an open question raised by Feldman (2016) and Amir, Koren, and Livni (2021b) and showing that a non-trivial dimension dependence is unavoidable. Furthermore, for standard one-pass stochastic gradient descent (SGD), we show that an application of the same construction technique provides a similar $\\Omega(\\sqrt{d})$ lo",
    "path": "papers/24/01/2401.12058.json",
    "total_tokens": 1059,
    "translated_title": "用梯度来反击维度：随机凸优化中梯度方法的泛化研究",
    "translated_abstract": "我们研究了梯度方法在基础随机凸优化设置中的泛化性能，并着重关注其维度依赖性。首先，针对全批量梯度下降（GD），我们构造了一个在维度$d=O(n^2)$下的学习问题，其中经过调整以达到经验风险的最佳性能的标准GD（使用$n$个训练样本进行训练）以常数概率收敛到一个近似的经验风险最小化器，其人口过剩风险为$\\Omega(1)$。我们的界限可以转化为标准GD需要$\\Omega(\\sqrt{d})$个训练样本才能达到非平凡的测试误差，从而回答了Feldman（2016）和Amir、Koren、Livni（2021b）提出的一个开放问题，并表明非平凡的维度依赖性是不可避免的。此外，对于标准的一遍随机梯度下降（SGD），我们发现同样的构造技术可以提供类似的$\\Omega(\\sqrt{d})$的下界。",
    "tldr": "本研究探讨了随机凸优化中梯度方法的泛化性能以及维度依赖性。我们发现标准的全批量梯度下降方法在维度$d = O(n^2)$时需要至少$\\Omega(\\sqrt{d})$个训练样本才能达到非平凡的测试误差。而对于标准的一遍随机梯度下降方法，同样的维度下界也适用。",
    "en_tdlr": "This study investigates the generalization performance and dimension dependence of gradient methods in stochastic convex optimization. It is found that standard full-batch gradient descent requires at least Ω(√d) training examples to achieve non-trivial test error in dimension d = O(n^2). The same lower bound also applies to standard one-pass stochastic gradient descent."
}