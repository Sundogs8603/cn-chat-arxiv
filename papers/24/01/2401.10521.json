{
    "title": "Cross-lingual Editing in Multilingual Language Models. (arXiv:2401.10521v1 [cs.CL])",
    "abstract": "The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (\\textbf{XME}) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: \\textit{Latin} (English, French, and Spanish) and \\textit{Indic} (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distin",
    "link": "http://arxiv.org/abs/2401.10521",
    "context": "Title: Cross-lingual Editing in Multilingual Language Models. (arXiv:2401.10521v1 [cs.CL])\nAbstract: The training of large language models (LLMs) necessitates substantial data and computational resources, and updating outdated LLMs entails significant efforts and resources. While numerous model editing techniques (METs) have emerged to efficiently update model outputs without retraining, their effectiveness in multilingual LLMs, where knowledge is stored in diverse languages, remains an underexplored research area. This research paper introduces the cross-lingual model editing (\\textbf{XME}) paradigm, wherein a fact is edited in one language, and the subsequent update propagation is observed across other languages. To investigate the XME paradigm, we conducted experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts: \\textit{Latin} (English, French, and Spanish) and \\textit{Indic} (Hindi, Gujarati, and Bengali). The results reveal notable performance limitations of state-of-the-art METs under the XME setting, mainly when the languages involved belong to two distin",
    "path": "papers/24/01/2401.10521.json",
    "total_tokens": 929,
    "translated_title": "多语言语言模型中的跨语言编辑",
    "translated_abstract": "大规模语言模型（LLM）的训练需要大量的数据和计算资源，而更新过时的LLM需要大量的工作和资源。虽然出现了许多模型编辑技术（MET）以便在不重新训练的情况下高效更新模型输出，但在多语言LLM中，其中的知识以多种语言存储，这仍然是一个未深入研究的领域。本研究介绍了跨语言模型编辑（XME）范式，在该范式中，一个事实在一种语言中被编辑，观察其在其他语言中的更新传播。为了研究XME范式，我们使用BLOOM、mBERT和XLM-RoBERTa进行了实验，使用了两种写作脚本，即拉丁语（英语、法语和西班牙语）和印地语（印地语、古吉拉特语和孟加拉语）。结果显示，在XME设置下，当前最先进的MET存在明显的性能限制，特别是当涉及的语言属于两个不同的语族时。",
    "tldr": "本论文介绍了跨语言模型编辑（XME）范式，通过在一种语言中编辑事实并观察其对其他语言的更新传播，研究了多语言语言模型中的模型编辑技术（MET）的性能限制。",
    "en_tdlr": "This paper introduces the cross-lingual model editing (XME) paradigm and investigates the performance limitations of model editing techniques (METs) in multilingual language models by editing a fact in one language and observing its update propagation to other languages."
}