{
    "title": "Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do. (arXiv:2401.14523v1 [cs.CY])",
    "abstract": "Advances in the performance of large language models (LLMs) have led some researchers to propose the emergence of theory of mind (ToM) in artificial intelligence (AI). LLMs can attribute beliefs, desires, intentions, and emotions, and they will improve in their accuracy. Rather than employing the characteristically human method of empathy, they learn to attribute mental states by recognizing linguistic patterns in a dataset that typically do not include that individual. We ask whether LLMs' inability to empathize precludes them from honoring an individual's right to be an exception, that is, from making assessments of character and predictions of behavior that reflect appropriate sensitivity to a person's individuality. Can LLMs seriously consider an individual's claim that their case is different based on internal mental states like beliefs, desires, and intentions, or are they limited to judging that case based on its similarities to others? We propose that the method of empathy has ",
    "link": "http://arxiv.org/abs/2401.14523",
    "context": "Title: Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do. (arXiv:2401.14523v1 [cs.CY])\nAbstract: Advances in the performance of large language models (LLMs) have led some researchers to propose the emergence of theory of mind (ToM) in artificial intelligence (AI). LLMs can attribute beliefs, desires, intentions, and emotions, and they will improve in their accuracy. Rather than employing the characteristically human method of empathy, they learn to attribute mental states by recognizing linguistic patterns in a dataset that typically do not include that individual. We ask whether LLMs' inability to empathize precludes them from honoring an individual's right to be an exception, that is, from making assessments of character and predictions of behavior that reflect appropriate sensitivity to a person's individuality. Can LLMs seriously consider an individual's claim that their case is different based on internal mental states like beliefs, desires, and intentions, or are they limited to judging that case based on its similarities to others? We propose that the method of empathy has ",
    "path": "papers/24/01/2401.14523.json",
    "total_tokens": 946,
    "translated_title": "共情与成为一个例外的权利：LLMs可以做什么，不能做什么",
    "translated_abstract": "大语言模型（LLMs）性能的进步导致一些研究者提出了人工智能（AI）中理论心智（ToM）的出现。LLMs能够归因于信念、欲望、意图和情感，并且它们在准确性方面会有所提高。与人类特有的共情方法不同，它们通过识别数据集中通常不包括的语言模式来学习归因心理状态。我们问LLMs的无法共情是否会妨碍它们尊重个体成为一个例外的权利，即是否会妨碍它们基于对个体的个性敏感性进行性格评估和行为预测。LLMs能否认真考虑个体的主张，即他们的情况是基于信念、欲望和意图等内部心理状态而不同，还是仅限于基于其与他人的相似之处来判断该案件？我们提出共情的方法",
    "tldr": "LLMs既有能力归因于信念、欲望、意图和情感，也能在准确性方面不断提高，但他们无法通过共情方法来尊重个体成为例外的权利。他们仅通过识别语言模式判断案件相似性，而无法考虑个体的内部心理状态。我们提出了共情的方法。",
    "en_tdlr": "LLMs have the ability to attribute beliefs, desires, intentions, and emotions with improving accuracy, but they lack the ability to empathize and honor an individual's right to be an exception. They can only judge cases based on similarities identified through linguistic patterns, without considering the individual's internal mental states. We propose the method of empathy."
}