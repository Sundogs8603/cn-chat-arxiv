{
    "title": "Federated Class-Incremental Learning with New-Class Augmented Self-Distillation. (arXiv:2401.00622v2 [cs.LG] UPDATED)",
    "abstract": "Federated Learning (FL) enables collaborative model training among participants while guaranteeing the privacy of raw data. Mainstream FL methodologies overlook the dynamic nature of real-world data, particularly its tendency to grow in volume and diversify in classes over time. This oversight results in FL methods suffering from catastrophic forgetting, where the trained models inadvertently discard previously learned information upon assimilating new data. In response to this challenge, we propose a novel Federated Class-Incremental Learning (FCIL) method, named \\underline{Fed}erated \\underline{C}lass-Incremental \\underline{L}earning with New-Class \\underline{A}ugmented \\underline{S}elf-Di\\underline{S}tillation (FedCLASS). The core of FedCLASS is to enrich the class scores of historical models with new class scores predicted by current models and utilize the combined knowledge for self-distillation, enabling a more sufficient and precise knowledge transfer from historical models to c",
    "link": "http://arxiv.org/abs/2401.00622",
    "context": "Title: Federated Class-Incremental Learning with New-Class Augmented Self-Distillation. (arXiv:2401.00622v2 [cs.LG] UPDATED)\nAbstract: Federated Learning (FL) enables collaborative model training among participants while guaranteeing the privacy of raw data. Mainstream FL methodologies overlook the dynamic nature of real-world data, particularly its tendency to grow in volume and diversify in classes over time. This oversight results in FL methods suffering from catastrophic forgetting, where the trained models inadvertently discard previously learned information upon assimilating new data. In response to this challenge, we propose a novel Federated Class-Incremental Learning (FCIL) method, named \\underline{Fed}erated \\underline{C}lass-Incremental \\underline{L}earning with New-Class \\underline{A}ugmented \\underline{S}elf-Di\\underline{S}tillation (FedCLASS). The core of FedCLASS is to enrich the class scores of historical models with new class scores predicted by current models and utilize the combined knowledge for self-distillation, enabling a more sufficient and precise knowledge transfer from historical models to c",
    "path": "papers/24/01/2401.00622.json",
    "total_tokens": 877,
    "translated_title": "具有增量学习和自蒸馏的联邦化分类学习",
    "translated_abstract": "联邦学习（FL）使参与者能够在保护原始数据隐私的同时进行协作模型训练。主流的FL方法忽视了现实世界数据的动态特性，尤其是数据在时间上的增长和类别上的多样化。这一忽视导致FL方法在吸收新数据时不可避免地遗忘之前学习到的信息。为了应对这一挑战，我们提出了一种新颖的联邦化增量学习方法，称为联邦化增量学习与增强自蒸馏（FedCLASS）。FedCLASS的核心是通过当前模型预测的新类别分数丰富历史模型的类别分数，并利用结合的知识进行自蒸馏，从而实现了更充分精确的知识传递，从历史模型到当前模型。",
    "tldr": "该论文提出了一种具有增量学习和自蒸馏的联邦化分类学习方法，通过丰富历史模型的类别分数并利用结合的知识进行自蒸馏，实现了更充分精确的知识传递。"
}