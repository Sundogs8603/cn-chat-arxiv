{
    "title": "Speeding up and reducing memory usage for scientific machine learning via mixed precision. (arXiv:2401.16645v1 [cs.LG])",
    "abstract": "Scientific machine learning (SciML) has emerged as a versatile approach to address complex computational science and engineering problems. Within this field, physics-informed neural networks (PINNs) and deep operator networks (DeepONets) stand out as the leading techniques for solving partial differential equations by incorporating both physical equations and experimental data. However, training PINNs and DeepONets requires significant computational resources, including long computational times and large amounts of memory. In search of computational efficiency, training neural networks using half precision (float16) rather than the conventional single (float32) or double (float64) precision has gained substantial interest, given the inherent benefits of reduced computational time and memory consumed. However, we find that float16 cannot be applied to SciML methods, because of gradient divergence at the start of training, weight updates going to zero, and the inability to converge to a ",
    "link": "http://arxiv.org/abs/2401.16645",
    "context": "Title: Speeding up and reducing memory usage for scientific machine learning via mixed precision. (arXiv:2401.16645v1 [cs.LG])\nAbstract: Scientific machine learning (SciML) has emerged as a versatile approach to address complex computational science and engineering problems. Within this field, physics-informed neural networks (PINNs) and deep operator networks (DeepONets) stand out as the leading techniques for solving partial differential equations by incorporating both physical equations and experimental data. However, training PINNs and DeepONets requires significant computational resources, including long computational times and large amounts of memory. In search of computational efficiency, training neural networks using half precision (float16) rather than the conventional single (float32) or double (float64) precision has gained substantial interest, given the inherent benefits of reduced computational time and memory consumed. However, we find that float16 cannot be applied to SciML methods, because of gradient divergence at the start of training, weight updates going to zero, and the inability to converge to a ",
    "path": "papers/24/01/2401.16645.json",
    "total_tokens": 860,
    "translated_title": "通过混合精度加速科学机器学习并减少内存使用",
    "translated_abstract": "科学机器学习（SciML）已成为解决复杂的计算科学和工程问题的多功能方法。在这个领域中，基于物理信息的神经网络（PINNs）和深度算子网络（DeepONets）以其结合物理方程和实验数据的能力而脱颖而出，成为解决偏微分方程的主要技术。然而，训练PINNs和DeepONets需要大量的计算资源，包括长时间的计算和大量的内存。为了提高计算效率，使用半精度（float16）而不是传统的单精度（float32）或双精度（float64）来训练神经网络已经引起了广泛关注，因为它具有减少计算时间和内存消耗的潜在优势。然而，我们发现float16不能应用于SciML方法，因为训练开始时梯度发散，权重更新变为零，无法收敛。",
    "tldr": "通过使用混合精度训练神经网络，可以加速科学机器学习并减少内存使用，但对于PINNs和DeepONets等方法，半精度不适用。",
    "en_tdlr": "By training neural networks using mixed precision, scientific machine learning can be sped up and memory usage reduced. However, for methods like PINNs and DeepONets, half precision is not applicable due to gradient divergence, weight updates going to zero, and the inability to converge."
}