{
    "title": "ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers. (arXiv:2401.02072v1 [cs.CL])",
    "abstract": "The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA encounter limitations in domain-specific tasks, with these models often lacking depth and accuracy in specialized areas, and exhibiting a decrease in general capabilities when fine-tuned, particularly analysis ability in small sized models. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement Learning from Human Feedback (RLHF) grounded in Proximal Policy Optimization (PPO), demonstrating remarkable ability in in-domain scenarios without compromising general task performance. Our exploration of ICE-GRT highlights its understanding and reasoning ability to not only generate robust answers but also to provide detailed analyses of the reasons behind the answer. This capability marks a significant progression beyond the scope of Supervised Fine-Tuning models. The success of ICE-GRT is dependent on several crucial factors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage Normalizati",
    "link": "http://arxiv.org/abs/2401.02072",
    "context": "Title: ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers. (arXiv:2401.02072v1 [cs.CL])\nAbstract: The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA encounter limitations in domain-specific tasks, with these models often lacking depth and accuracy in specialized areas, and exhibiting a decrease in general capabilities when fine-tuned, particularly analysis ability in small sized models. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement Learning from Human Feedback (RLHF) grounded in Proximal Policy Optimization (PPO), demonstrating remarkable ability in in-domain scenarios without compromising general task performance. Our exploration of ICE-GRT highlights its understanding and reasoning ability to not only generate robust answers but also to provide detailed analyses of the reasons behind the answer. This capability marks a significant progression beyond the scope of Supervised Fine-Tuning models. The success of ICE-GRT is dependent on several crucial factors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage Normalizati",
    "path": "papers/24/01/2401.02072.json",
    "total_tokens": 944,
    "translated_title": "ICE-GRT: 基于生成增强的转换的指令上下文增强",
    "translated_abstract": "大型语言模型（LLMs）如ChatGPT和LLaMA在特定领域的任务中存在局限性，这些模型在专业领域缺乏深度和准确性，在精细调整时的整体能力下降，特别是小型模型的分析能力。为了解决这些问题，我们引入了ICE-GRT，利用基于近端策略优化（PPO）的人类反馈强化学习（RLHF），在领域内场景中展示出卓越的能力，同时不损失通用任务性能。我们对ICE-GRT的探索突出了其理解和推理能力，不仅能够生成稳健的答案，还能提供对答案背后原因的详细分析。这种能力标志着对指导性微调模型的显著进展。ICE-GRT的成功取决于几个关键因素，包括适当的数据、奖励规模缩放、KL控制、优势归一化等。",
    "tldr": "ICE-GRT是一种利用生成强化转换的指令上下文增强方法，能够在特定领域的任务中取得卓越的结果，同时不影响通用任务性能。它不仅能生成稳健的答案，还能提供对答案背后原因的详细分析，是对现有指导性微调模型的显著进展。",
    "en_tdlr": "ICE-GRT is an instruction context enhancement method that utilizes generative reinforcement based transformers, achieving remarkable results in domain-specific tasks without compromising general task performance. It not only generates robust answers but also provides detailed analysis of the reasons behind the answers, representing a significant progression beyond existing supervised fine-tuning models."
}