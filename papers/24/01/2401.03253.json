{
    "title": "Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])",
    "abstract": "Recent advances achieved by deep learning models rely on the independent and identically distributed assumption, hindering their applications in real-world scenarios with domain shifts. To address the above issues, cross-domain learning aims at extracting domain-invariant knowledge to reduce the domain shift between training and testing data. However, in visual cross-domain learning, traditional methods concentrate solely on the image modality, neglecting the use of the text modality to alleviate the domain shift. In this work, we propose Large Language models as Visual cross-dOmain learners (LLaVO). LLaVO uses vision-language models to convert images into detailed textual descriptions. A large language model is then finetuned on textual descriptions of the source/target domain generated by a designed instruction template. Extensive experimental results on various cross-domain tasks under the domain generalization and unsupervised domain adaptation settings have demonstrated the effect",
    "link": "http://arxiv.org/abs/2401.03253",
    "context": "Title: Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])\nAbstract: Recent advances achieved by deep learning models rely on the independent and identically distributed assumption, hindering their applications in real-world scenarios with domain shifts. To address the above issues, cross-domain learning aims at extracting domain-invariant knowledge to reduce the domain shift between training and testing data. However, in visual cross-domain learning, traditional methods concentrate solely on the image modality, neglecting the use of the text modality to alleviate the domain shift. In this work, we propose Large Language models as Visual cross-dOmain learners (LLaVO). LLaVO uses vision-language models to convert images into detailed textual descriptions. A large language model is then finetuned on textual descriptions of the source/target domain generated by a designed instruction template. Extensive experimental results on various cross-domain tasks under the domain generalization and unsupervised domain adaptation settings have demonstrated the effect",
    "path": "papers/24/01/2401.03253.json",
    "total_tokens": 909,
    "translated_title": "大型语言模型作为视觉跨领域学习器",
    "translated_abstract": "深度学习模型取得的最新进展依赖于独立同分布的假设，这限制了它们在真实世界中面对领域转移时的应用。为了解决以上问题，跨领域学习旨在提取领域不变的知识，减少训练与测试数据之间的领域转移。然而，在视觉跨领域学习中，传统方法仅关注图像模态，忽视了使用文本模态来缓解领域转移的作用。在本研究中，我们提出了大型语言模型作为视觉跨领域学习器（LLaVO）。LLaVO使用视觉-语言模型将图像转换为详细的文本描述。然后，在经过设计的指导模板生成的源域/目标域的文本描述上，对大型语言模型进行微调。在领域泛化和无监督领域自适应设置下进行的各种跨领域任务的广泛实验结果表明了该方法的效果。",
    "tldr": "本研究提出了大型语言模型作为视觉跨领域学习器（LLaVO），通过将图像转换为文本描述，使用大型语言模型进行训练和微调，实现了在跨领域任务中减少领域转移的效果。",
    "en_tdlr": "This research proposes Large Language models as Visual cross-Domain learners (LLaVO) that convert images into textual descriptions and utilize a large language model for training and fine-tuning, achieving the reduction of domain shift in cross-domain tasks."
}