{
    "title": "Evaluation of Out-of-Distribution Detection Performance on Autonomous Driving Datasets. (arXiv:2401.17013v1 [cs.LG])",
    "abstract": "Safety measures need to be systemically investigated to what extent they evaluate the intended performance of Deep Neural Networks (DNNs) for critical applications. Due to a lack of verification methods for high-dimensional DNNs, a trade-off is needed between accepted performance and handling of out-of-distribution (OOD) samples.  This work evaluates rejecting outputs from semantic segmentation DNNs by applying a Mahalanobis distance (MD) based on the most probable class-conditional Gaussian distribution for the predicted class as an OOD score. The evaluation follows three DNNs trained on the Cityscapes dataset and tested on four automotive datasets and finds that classification risk can drastically be reduced at the cost of pixel coverage, even when applied on unseen datasets. The applicability of our findings will support legitimizing safety measures and motivate their usage when arguing for safe usage of DNNs in automotive perception.",
    "link": "http://arxiv.org/abs/2401.17013",
    "context": "Title: Evaluation of Out-of-Distribution Detection Performance on Autonomous Driving Datasets. (arXiv:2401.17013v1 [cs.LG])\nAbstract: Safety measures need to be systemically investigated to what extent they evaluate the intended performance of Deep Neural Networks (DNNs) for critical applications. Due to a lack of verification methods for high-dimensional DNNs, a trade-off is needed between accepted performance and handling of out-of-distribution (OOD) samples.  This work evaluates rejecting outputs from semantic segmentation DNNs by applying a Mahalanobis distance (MD) based on the most probable class-conditional Gaussian distribution for the predicted class as an OOD score. The evaluation follows three DNNs trained on the Cityscapes dataset and tested on four automotive datasets and finds that classification risk can drastically be reduced at the cost of pixel coverage, even when applied on unseen datasets. The applicability of our findings will support legitimizing safety measures and motivate their usage when arguing for safe usage of DNNs in automotive perception.",
    "path": "papers/24/01/2401.17013.json",
    "total_tokens": 837,
    "translated_title": "在自动驾驶数据集上评估对于模型的越界检测性能",
    "translated_abstract": "需要系统地研究安全措施，以评估深度神经网络（DNN）在关键应用中的预期性能程度。由于缺乏用于高维DNN的验证方法，需要在接受的性能和处理越界样本之间进行权衡。本文通过应用基于最可能的类条件高斯分布的Mahalanobis距离（MD）作为越界评分，评估拒绝语义分割DNN的输出。评估过程涉及三个在Cityscapes数据集上训练并在四个汽车数据集上测试的DNN，并发现即使应用在未见过的数据集上，通过减少像素覆盖率可以大幅降低分类风险。我们的研究结果的适用性将支持合法化安全措施并推动其在汽车感知中的安全使用。",
    "tldr": "本研究评估了在自动驾驶数据集上的越界检测性能，发现通过使用Mahalanobis距离来拒绝输出可以显著降低分类风险，即使应用在未见过的数据集上。"
}