{
    "title": "Diffusion Model Conditioning on Gaussian Mixture Model and Negative Gaussian Mixture Gradient. (arXiv:2401.11261v1 [cs.LG])",
    "abstract": "Diffusion models (DMs) are a type of generative model that has a huge impact on image synthesis and beyond. They achieve state-of-the-art generation results in various generative tasks. A great diversity of conditioning inputs, such as text or bounding boxes, are accessible to control the generation. In this work, we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as feature conditioning to guide the denoising process. Based on set theory, we provide a comprehensive theoretical analysis that shows that conditional latent distribution based on features and classes is significantly different, so that conditional latent distribution on features produces fewer defect generations than conditioning on classes. Two diffusion models conditioned on the Gaussian mixture model are trained separately for comparison. Experiments support our findings. A novel gradient function called the negative Gaussian mixture gradient (NGMG) is proposed and applied in diffusion model tr",
    "link": "http://arxiv.org/abs/2401.11261",
    "context": "Title: Diffusion Model Conditioning on Gaussian Mixture Model and Negative Gaussian Mixture Gradient. (arXiv:2401.11261v1 [cs.LG])\nAbstract: Diffusion models (DMs) are a type of generative model that has a huge impact on image synthesis and beyond. They achieve state-of-the-art generation results in various generative tasks. A great diversity of conditioning inputs, such as text or bounding boxes, are accessible to control the generation. In this work, we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as feature conditioning to guide the denoising process. Based on set theory, we provide a comprehensive theoretical analysis that shows that conditional latent distribution based on features and classes is significantly different, so that conditional latent distribution on features produces fewer defect generations than conditioning on classes. Two diffusion models conditioned on the Gaussian mixture model are trained separately for comparison. Experiments support our findings. A novel gradient function called the negative Gaussian mixture gradient (NGMG) is proposed and applied in diffusion model tr",
    "path": "papers/24/01/2401.11261.json",
    "total_tokens": 900,
    "translated_title": "基于高斯混合模型和负高斯混合梯度的扩散模型条件设计",
    "translated_abstract": "扩散模型（DMs）是一种对图像合成和其他领域有巨大影响的生成模型。它们在各种生成任务中取得了最先进的生成结果。可以使用各种多样的条件输入，如文本或边界框，来控制生成过程。本研究中，我们提出了一种使用高斯混合模型（GMM）作为特征条件来引导去噪过程的条件机制。基于集合论，我们提供了一种全面的理论分析，表明基于特征和类别的条件潜在分布显著不同，因此基于特征的条件潜在分布比基于类别的条件潜在分布产生更少的缺陷生成。分别训练了两个基于高斯混合模型的扩散模型进行比较。实验证实了我们的发现。我们提出了一种新的梯度函数，称为负高斯混合梯度（NGMG），并应用于扩散模型训练中。",
    "tldr": "本研究提出了一种利用高斯混合模型作为特征条件来引导扩散模型的去噪过程的条件机制，并通过实验证实了基于特征的条件潜在分布能够产生更少的缺陷生成。"
}