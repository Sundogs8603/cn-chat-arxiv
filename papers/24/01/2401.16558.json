{
    "title": "Diverse, but Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation. (arXiv:2401.16558v1 [cs.CY])",
    "abstract": "The pervasive spread of misinformation and disinformation poses a significant threat to society. Professional fact-checkers play a key role in addressing this threat, but the vast scale of the problem forces them to prioritize their limited resources. This prioritization may consider a range of factors, such as varying risks of harm posed to specific groups of people. In this work, we investigate potential implications of using a large language model (LLM) to facilitate such prioritization. Because fact-checking impacts a wide range of diverse segments of society, it is important that diverse views are represented in the claim prioritization process. This paper examines whether a LLM can reflect the views of various groups when assessing the harms of misinformation, focusing on gender as a primary variable. We pose two central questions: (1) To what extent do prompts with explicit gender references reflect gender differences in opinion in the United States on topics of social relevance",
    "link": "http://arxiv.org/abs/2401.16558",
    "context": "Title: Diverse, but Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation. (arXiv:2401.16558v1 [cs.CY])\nAbstract: The pervasive spread of misinformation and disinformation poses a significant threat to society. Professional fact-checkers play a key role in addressing this threat, but the vast scale of the problem forces them to prioritize their limited resources. This prioritization may consider a range of factors, such as varying risks of harm posed to specific groups of people. In this work, we investigate potential implications of using a large language model (LLM) to facilitate such prioritization. Because fact-checking impacts a wide range of diverse segments of society, it is important that diverse views are represented in the claim prioritization process. This paper examines whether a LLM can reflect the views of various groups when assessing the harms of misinformation, focusing on gender as a primary variable. We pose two central questions: (1) To what extent do prompts with explicit gender references reflect gender differences in opinion in the United States on topics of social relevance",
    "path": "papers/24/01/2401.16558.json",
    "total_tokens": 969,
    "translated_title": "多样但有分歧：LLM可能夸大与错误信息有关的伤害问题上的性别差异",
    "translated_abstract": "误导性和虚假信息的普遍传播对社会构成了重大威胁。专业的事实核查人员在应对此威胁时发挥着关键作用，但问题规模巨大迫使他们必须对有限的资源进行优先排序。这种优先排序可能考虑到一系列因素，如对特定人群造成的伤害风险不同。本文研究了使用大型语言模型（LLM）来促进此类优先排序可能带来的潜在影响。因为事实核查影响着社会各个多样化部分，所以重要的是在对索赔的优先排序过程中反映多样化的观点。本文旨在调查LLM在评估错误信息的危害性时，能否反映不同群体的观点，重点关注性别作为主要变量。我们提出了两个核心问题：（1）带有明确性别指称的提示在美国社会相关主题上是否反映了性别差异的程度",
    "tldr": "本论文探讨了使用大型语言模型（LLM）用于优先排序误导性信息伤害时可能出现的问题。作者将性别作为主要变量，研究LLM在评估错误信息的危害性时是否能够反映不同性别群体的观点，结果发现LLM可能夸大了性别差异。"
}