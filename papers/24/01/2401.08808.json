{
    "title": "Sample Relationship from Learning Dynamics Matters for Generalisation. (arXiv:2401.08808v1 [cs.LG])",
    "abstract": "Although much research has been done on proposing new models or loss functions to improve the generalisation of artificial neural networks (ANNs), less attention has been directed to the impact of the training data on generalisation. In this work, we start from approximating the interaction between samples, i.e. how learning one sample would modify the model's prediction on other samples. Through analysing the terms involved in weight updates in supervised learning, we find that labels influence the interaction between samples. Therefore, we propose the labelled pseudo Neural Tangent Kernel (lpNTK) which takes label information into consideration when measuring the interactions between samples. We first prove that lpNTK asymptotically converges to the empirical neural tangent kernel in terms of the Frobenius norm under certain assumptions. Secondly, we illustrate how lpNTK helps to understand learning phenomena identified in previous work, specifically the learning difficulty of sample",
    "link": "http://arxiv.org/abs/2401.08808",
    "context": "Title: Sample Relationship from Learning Dynamics Matters for Generalisation. (arXiv:2401.08808v1 [cs.LG])\nAbstract: Although much research has been done on proposing new models or loss functions to improve the generalisation of artificial neural networks (ANNs), less attention has been directed to the impact of the training data on generalisation. In this work, we start from approximating the interaction between samples, i.e. how learning one sample would modify the model's prediction on other samples. Through analysing the terms involved in weight updates in supervised learning, we find that labels influence the interaction between samples. Therefore, we propose the labelled pseudo Neural Tangent Kernel (lpNTK) which takes label information into consideration when measuring the interactions between samples. We first prove that lpNTK asymptotically converges to the empirical neural tangent kernel in terms of the Frobenius norm under certain assumptions. Secondly, we illustrate how lpNTK helps to understand learning phenomena identified in previous work, specifically the learning difficulty of sample",
    "path": "papers/24/01/2401.08808.json",
    "total_tokens": 903,
    "translated_title": "学习动力学对泛化的影响：从样本关系开始",
    "translated_abstract": "尽管在改进人工神经网络（ANN）的泛化性能方面，已经有很多关于提出新模型或损失函数的研究，但对训练数据对泛化的影响却关注较少。在这项工作中，我们从近似样本之间的相互作用开始，即学习一个样本如何影响模型对其他样本的预测。通过分析监督学习中涉及的权重更新项，我们发现标签会影响样本之间的相互作用。因此，我们提出了带标签的伪神经切向核（lpNTK），在测量样本之间的相互作用时考虑标签信息。我们首先证明，在某些假设下，lpNTK在Frobenius范数下渐近收敛于经验神经切向核。其次，我们说明了lpNTK如何帮助理解先前工作中发现的学习现象，特别是样本学习困难的现象。",
    "tldr": "这项研究从近似样本之间的相互作用开始，揭示了标签对样本之间的影响，提出了带标签的伪神经切向核 (lpNTK)，并探讨了lpNTK如何帮助理解学习现象。"
}