{
    "title": "Language Models Understand Numbers, at Least Partially",
    "abstract": "Large language models (LLMs) have exhibited impressive competence in various tasks, but their opaque internal mechanisms hinder their use in mathematical problems. In this paper, we study a fundamental question: whether language models understand numbers, a basic element in math. Based on an assumption that LLMs should be capable of compressing numbers in their hidden states to solve mathematical problems, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of compressed numbers in LLMs. However, it is difficult to precisely reconstruct the original numbers, indicating that the compression process may not be lossless. Further experiments show that LLMs can utilize encoded numbers to perform arithmetic computations, and the computational ability scales up with the model size. Our preliminary research suggests that LLMs exhibit a partial understanding of number",
    "link": "https://arxiv.org/abs/2401.03735",
    "context": "Title: Language Models Understand Numbers, at Least Partially\nAbstract: Large language models (LLMs) have exhibited impressive competence in various tasks, but their opaque internal mechanisms hinder their use in mathematical problems. In this paper, we study a fundamental question: whether language models understand numbers, a basic element in math. Based on an assumption that LLMs should be capable of compressing numbers in their hidden states to solve mathematical problems, we construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states. Experimental results support the existence of compressed numbers in LLMs. However, it is difficult to precisely reconstruct the original numbers, indicating that the compression process may not be lossless. Further experiments show that LLMs can utilize encoded numbers to perform arithmetic computations, and the computational ability scales up with the model size. Our preliminary research suggests that LLMs exhibit a partial understanding of number",
    "path": "papers/24/01/2401.03735.json",
    "total_tokens": 777,
    "translated_title": "语言模型在某种程度上理解数字",
    "translated_abstract": "大型语言模型(LLMs)在各种任务中展现出令人印象深刻的能力，但其不透明的内部机制限制了它们在数学问题中的应用。在本文中，我们研究了一个基本问题：语言模型是否理解数字，数学中的基本元素。基于一个假设，即LLMs应该能够在其隐藏状态中压缩数字以解决数学问题，我们构建了一个合成数据集，包括加法问题，并利用线性探测器从隐藏状态中读取输入数字。实验结果支持LLMs中存在压缩的数字。然而，精确重建原始数字是困难的，表明压缩过程可能不是无损的。进一步的实验证明，LLMs可以利用编码的数字来执行算术计算，并且计算能力随模型大小的增加而扩展。我们的初步研究表明，LLMs在数字上展现出部分理解。",
    "tldr": "本研究表明，大型语言模型在某种程度上理解数字，可以通过压缩和编码的方式执行算术计算。"
}