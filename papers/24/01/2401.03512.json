{
    "title": "Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format. (arXiv:2401.03512v2 [cs.CL] UPDATED)",
    "abstract": "Finetuned large language models (such as ChatGPT and Qwen-chat) can generate Chinese classical poetry following human's instructions. LLMs perform well in content, but are usually lacking in format, with occasionally excess or insufficient number of characters in each line. Since most SOTA LLMs are token-based, we assume that the format inaccuracy is due to the difficulty of the \"token planning\" task, which means that the LLM need to know exactly how much characters are contained in each token and do length-control planning based on that knowledge. In this paper, we first confirm our assumption by showing that existing token-based large language models has limited knowledge on token-character relationship. We use a spelling bee probing procedure, and find that Qwen-chat failed in nearly 15% Chinese spelling test. We then show that a token-based model can be easily tailored into a token-free model (in terms of Chinese), which can largely solve the format accuracy problem. Our tailoring ",
    "link": "http://arxiv.org/abs/2401.03512",
    "context": "Title: Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format. (arXiv:2401.03512v2 [cs.CL] UPDATED)\nAbstract: Finetuned large language models (such as ChatGPT and Qwen-chat) can generate Chinese classical poetry following human's instructions. LLMs perform well in content, but are usually lacking in format, with occasionally excess or insufficient number of characters in each line. Since most SOTA LLMs are token-based, we assume that the format inaccuracy is due to the difficulty of the \"token planning\" task, which means that the LLM need to know exactly how much characters are contained in each token and do length-control planning based on that knowledge. In this paper, we first confirm our assumption by showing that existing token-based large language models has limited knowledge on token-character relationship. We use a spelling bee probing procedure, and find that Qwen-chat failed in nearly 15% Chinese spelling test. We then show that a token-based model can be easily tailored into a token-free model (in terms of Chinese), which can largely solve the format accuracy problem. Our tailoring ",
    "path": "papers/24/01/2401.03512.json",
    "total_tokens": 967,
    "translated_title": "无需分词的大型语言模型能够以更准确的格式生成中国古典诗词",
    "translated_abstract": "经过微调的大型语言模型（如ChatGPT和Qwen-chat）能够根据人类的指令生成中国古典诗词。虽然语言模型在内容方面表现良好，但通常在格式上存在问题，每行字符的数量有时过多或不足。由于大多数最新的语言模型是基于分词的，我们认为格式不准确是由于\"分词规划\"任务的难度，即语言模型需要准确知道每个分词中包含多少个字符，并基于这个知识进行长度控制规划。本文首先通过展示现有的基于分词的大型语言模型在分词和字符之间的关系方面知识有限来验证我们的假设。我们使用了拼写比赛探测程序，并发现Qwen-chat在近15%的中文拼写测试中失败。然后，我们展示了一个基于分词的模型可以轻松定制成无需分词的模型（对于中文来说），从而能够很大程度上解决格式准确性问题。",
    "tldr": "本文提出了一种无需分词的大型语言模型（LLMs）来生成中国古典诗词，并解决了格式不准确性的问题。验证了现有基于分词的模型在字符和分词之间的关系方面的知识有限，并展示了如何通过定制模型解决这一问题。",
    "en_tdlr": "This paper proposes a token-free large language model (LLM) for generating Chinese classical poetry and addresses the issue of format inaccuracy. It validates the limited knowledge of existing token-based models in terms of the relationship between characters and tokens, and demonstrates how customizing models can solve this problem."
}