{
    "title": "Explainable Benchmarking for Iterative Optimization Heuristics",
    "abstract": "Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This paper presents a novel approach we call explainable benchmarking. Introducing the IOH-Xplainer software framework, for analyzing and understanding the performance of various optimization algorithms and the impact of their different components and hyper-parameters. We showcase the framework in the context of two modular optimization frameworks. Through this framework, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterativ",
    "link": "https://arxiv.org/abs/2401.17842",
    "context": "Title: Explainable Benchmarking for Iterative Optimization Heuristics\nAbstract: Benchmarking heuristic algorithms is vital to understand under which conditions and on what kind of problems certain algorithms perform well. In most current research into heuristic optimization algorithms, only a very limited number of scenarios, algorithm configurations and hyper-parameter settings are explored, leading to incomplete and often biased insights and results. This paper presents a novel approach we call explainable benchmarking. Introducing the IOH-Xplainer software framework, for analyzing and understanding the performance of various optimization algorithms and the impact of their different components and hyper-parameters. We showcase the framework in the context of two modular optimization frameworks. Through this framework, we examine the impact of different algorithmic components and configurations, offering insights into their performance across diverse scenarios. We provide a systematic method for evaluating and interpreting the behaviour and efficiency of iterativ",
    "path": "papers/24/01/2401.17842.json",
    "total_tokens": 854,
    "translated_title": "迭代优化启发式方法可解释性基准测试",
    "translated_abstract": "启发式算法的基准测试对于理解在什么条件下以及在何种问题上某些算法表现良好至关重要。目前大部分对启发式优化算法的研究只探索了非常有限的场景、算法配置和超参数设置，导致了不完整且常常有偏见的见解和结果。本文提出了一种新的方法，称之为可解释基准测试。介绍了IOH-Xplainer软件框架，用于分析和理解各种优化算法的性能以及它们不同组件和超参数的影响。我们在两个模块化优化框架的背景下展示了该框架。通过该框架，我们研究不同算法组件和配置的影响，提供了它们在不同场景下的性能见解。我们提供了一种系统的方法来评估和解释迭代优化启发式方法的行为和效率。",
    "tldr": "本文介绍了一种称为可解释基准测试的新方法，并提出了IOH-Xplainer软件框架，用于分析和理解优化算法的性能和影响。通过该框架，研究人员可以评估和解释迭代优化启发式方法在不同场景下的行为和效率。",
    "en_tdlr": "This paper introduces a new method called explainable benchmarking and presents the IOH-Xplainer software framework for analyzing and understanding the performance and impact of optimization algorithms. Through this framework, researchers can evaluate and interpret the behavior and efficiency of iterative optimization heuristics in different scenarios."
}