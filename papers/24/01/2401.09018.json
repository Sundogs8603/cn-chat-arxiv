{
    "title": "Residual Alignment: Uncovering the Mechanisms of Residual Networks. (arXiv:2401.09018v1 [cs.LG])",
    "abstract": "The ResNet architecture has been widely adopted in deep learning due to its significant boost to performance through the use of simple skip connections, yet the underlying mechanisms leading to its success remain largely unknown. In this paper, we conduct a thorough empirical study of the ResNet architecture in classification tasks by linearizing its constituent residual blocks using Residual Jacobians and measuring their singular value decompositions. Our measurements reveal a process called Residual Alignment (RA) characterized by four properties:  (RA1) intermediate representations of a given input are equispaced on a line, embedded in high dimensional space, as observed by Gai and Zhang [2021];  (RA2) top left and right singular vectors of Residual Jacobians align with each other and across different depths;  (RA3) Residual Jacobians are at most rank C for fully-connected ResNets, where C is the number of classes; and  (RA4) top singular values of Residual Jacobians scale inversely",
    "link": "http://arxiv.org/abs/2401.09018",
    "context": "Title: Residual Alignment: Uncovering the Mechanisms of Residual Networks. (arXiv:2401.09018v1 [cs.LG])\nAbstract: The ResNet architecture has been widely adopted in deep learning due to its significant boost to performance through the use of simple skip connections, yet the underlying mechanisms leading to its success remain largely unknown. In this paper, we conduct a thorough empirical study of the ResNet architecture in classification tasks by linearizing its constituent residual blocks using Residual Jacobians and measuring their singular value decompositions. Our measurements reveal a process called Residual Alignment (RA) characterized by four properties:  (RA1) intermediate representations of a given input are equispaced on a line, embedded in high dimensional space, as observed by Gai and Zhang [2021];  (RA2) top left and right singular vectors of Residual Jacobians align with each other and across different depths;  (RA3) Residual Jacobians are at most rank C for fully-connected ResNets, where C is the number of classes; and  (RA4) top singular values of Residual Jacobians scale inversely",
    "path": "papers/24/01/2401.09018.json",
    "total_tokens": 977,
    "translated_title": "残差对齐：揭示残差网络的机制",
    "translated_abstract": "由于通过简单的跳跃连接显著提高了性能，ResNet架构在深度学习中被广泛采用，然而导致其成功的基本机制仍然大部分未知。在本文中，我们通过使用残差雅可比和测量其奇异值分解，对ResNet架构在分类任务中进行了深入的实证研究，线性化其组成的残差模块。我们的测量结果揭示了一个名为残差对齐的过程，其具有以下四个特性：（RA1）给定输入的中间表示在嵌入高维空间的线上等间隔，如Gai和Zhang [2021]观察到的一样；（RA2）残差雅可比的左上和右上奇异向量相互对齐，并且在不同深度上也相互对齐；（RA3）对于全连接的ResNet来说，残差雅可比至多是秩C，其中C是类别数；（RA4）残差雅可比的前N个奇异值与N相关联的数据集规模成反比例缩放。",
    "tldr": "本研究通过线性化残差模块并测量其奇异值分解，揭示了ResNet架构的成功机制，包括中间表示的等间隔分布、残差雅可比的对齐以及与类别数相关的残差雅可比秩的限制。",
    "en_tdlr": "This paper conducts an empirical study on the ResNet architecture in classification tasks by linearizing its constituent residual blocks and measuring their singular value decompositions, revealing the mechanisms of ResNet's success, including equispaced intermediate representations, alignment of Residual Jacobians, and restrictions on the rank of Residual Jacobians related to the number of classes."
}