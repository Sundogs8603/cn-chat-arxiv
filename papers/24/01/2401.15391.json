{
    "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries. (arXiv:2401.15391v1 [cs.CL])",
    "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models fo",
    "link": "http://arxiv.org/abs/2401.15391",
    "context": "Title: MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries. (arXiv:2401.15391v1 [cs.CL])\nAbstract: Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models fo",
    "path": "papers/24/01/2401.15391.json",
    "total_tokens": 964,
    "translated_title": "MultiHop-RAG: 用于多跳查询的检索增强生成的基准测试",
    "translated_abstract": "检索增强生成（RAG）通过检索相关知识来增强大型语言模型（LLM），在缓解LLM的幻觉和提高响应质量方面显示出有希望的潜力，从而促进了LLM在实践中的广泛应用。然而，我们发现现有的RAG系统在回答多跳查询方面不足，这需要检索和推理多个支持证据。此外，据我们所知，目前没有任何现有的RAG基准测试数据集专注于多跳查询。在本文中，我们开发了一个新颖的数据集MultiHop-RAG，它包括一个知识库、一个大型的多跳查询集合、它们的真实答案和相关的支持证据。我们详细介绍了构建数据集的过程，利用英语新闻文章数据集作为基本的RAG知识库。我们通过两个实验展示了MultiHop-RAG的基准测试实用性。第一个实验比较了不同的嵌入模型，用于生成答案展示",
    "tldr": "本论文提出了一个用于多跳查询的检索增强生成（RAG）的基准测试，通过在大型语言模型中检索相关知识来提高响应质量。通过构建MultiHop-RAG数据集，该数据集包含知识库、多个多跳查询、真实答案和支持证据，展示了该基准测试的实用性。"
}