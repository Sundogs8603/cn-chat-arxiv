{
    "title": "Causal State Distillation for Explainable Reinforcement Learning",
    "abstract": "arXiv:2401.00104v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a powerful technique for training intelligent agents, but understanding why these agents make specific decisions can be quite challenging. This lack of transparency in RL models has been a long-standing problem, making it difficult for users to grasp the reasons behind an agent's behaviour. Various approaches have been explored to address this problem, with one promising avenue being reward decomposition (RD). RD is appealing as it sidesteps some of the concerns associated with other methods that attempt to rationalize an agent's behaviour in a post-hoc manner. RD works by exposing various facets of the rewards that contribute to the agent's objectives during training. However, RD alone has limitations as it primarily offers insights based on sub-rewards and does not delve into the intricate cause-and-effect relationships that occur within an RL agent's neural model. In this paper, we present an e",
    "link": "https://arxiv.org/abs/2401.00104",
    "context": "Title: Causal State Distillation for Explainable Reinforcement Learning\nAbstract: arXiv:2401.00104v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a powerful technique for training intelligent agents, but understanding why these agents make specific decisions can be quite challenging. This lack of transparency in RL models has been a long-standing problem, making it difficult for users to grasp the reasons behind an agent's behaviour. Various approaches have been explored to address this problem, with one promising avenue being reward decomposition (RD). RD is appealing as it sidesteps some of the concerns associated with other methods that attempt to rationalize an agent's behaviour in a post-hoc manner. RD works by exposing various facets of the rewards that contribute to the agent's objectives during training. However, RD alone has limitations as it primarily offers insights based on sub-rewards and does not delve into the intricate cause-and-effect relationships that occur within an RL agent's neural model. In this paper, we present an e",
    "path": "papers/24/01/2401.00104.json",
    "total_tokens": 747,
    "translated_title": "因果状态精炼用于可解释强化学习",
    "translated_abstract": "强化学习（RL）是训练智能体的强大技术，但理解这些智能体为何做出特定决策可能非常具有挑战性。RL模型的不透明性一直是一个长期存在的问题，使用户难以理解智能体行为背后的原因。本文提出了一种基于因果状态精炼（Causal State Distillation）的方法，旨在克服奖励分解等其他方法所带来的问题，该方法在训练过程中揭示出对智能体目标产生影响的奖励的各个方面。",
    "tldr": "本文提出了一种因果状态精炼的方法，通过揭示在训练过程中影响智能体目标的奖励的各个方面，解决了强化学习模型不透明性的问题。",
    "en_tdlr": "This paper introduces a method of causal state distillation to address the lack of transparency in reinforcement learning models by exposing various facets of rewards that influence the agent's objectives during training."
}