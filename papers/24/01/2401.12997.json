{
    "title": "Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion. (arXiv:2401.12997v1 [cs.CL])",
    "abstract": "In recent years, knowledge graph completion (KGC) models based on pre-trained language model (PLM) have shown promising results. However, the large number of parameters and high computational cost of PLM models pose challenges for their application in downstream tasks. This paper proposes a progressive distillation method based on masked generation features for KGC task, aiming to significantly reduce the complexity of pre-trained models. Specifically, we perform pre-distillation on PLM to obtain high-quality teacher models, and compress the PLM network to obtain multi-grade student models. However, traditional feature distillation suffers from the limitation of having a single representation of information in teacher models. To solve this problem, we propose masked generation of teacher-student features, which contain richer representation information. Furthermore, there is a significant gap in representation ability between teacher and student. Therefore, we design a progressive dist",
    "link": "http://arxiv.org/abs/2401.12997",
    "context": "Title: Progressive Distillation Based on Masked Generation Feature Method for Knowledge Graph Completion. (arXiv:2401.12997v1 [cs.CL])\nAbstract: In recent years, knowledge graph completion (KGC) models based on pre-trained language model (PLM) have shown promising results. However, the large number of parameters and high computational cost of PLM models pose challenges for their application in downstream tasks. This paper proposes a progressive distillation method based on masked generation features for KGC task, aiming to significantly reduce the complexity of pre-trained models. Specifically, we perform pre-distillation on PLM to obtain high-quality teacher models, and compress the PLM network to obtain multi-grade student models. However, traditional feature distillation suffers from the limitation of having a single representation of information in teacher models. To solve this problem, we propose masked generation of teacher-student features, which contain richer representation information. Furthermore, there is a significant gap in representation ability between teacher and student. Therefore, we design a progressive dist",
    "path": "papers/24/01/2401.12997.json",
    "total_tokens": 975,
    "translated_title": "基于遮蔽生成特征方法的渐进蒸馏用于知识图谱补全",
    "translated_abstract": "最近几年，基于预训练语言模型 (PLM) 的知识图谱补全 (KGC) 模型展示了有希望的结果。然而，PLM 模型的大量参数和高计算成本对其在下游任务中的应用提出了挑战。本文提出了一种基于遮蔽生成特征的渐进蒸馏方法，用于 KGC 任务，旨在显著降低预训练模型的复杂性。具体而言，我们对 PLM 进行预蒸馏，得到高质量的教师模型，并压缩 PLM 网络得到多等级的学生模型。然而，传统的特征蒸馏在教师模型中只有单一信息表示的限制。为了解决这个问题，我们提出了教师-学生特征的遮蔽生成，其中包含更丰富的表示信息。此外，教师和学生之间存在显著的表示能力差距。因此，我们设计了一种渐进式的蒸馏方法。",
    "tldr": "本文提出了一种基于遮蔽生成特征的渐进蒸馏方法，用于知识图谱补全任务，通过预蒸馏和压缩预训练模型，以及引入遮蔽生成的教师-学生特征，显著降低了模型的复杂性，并解决了特征表示能力差距的问题。"
}