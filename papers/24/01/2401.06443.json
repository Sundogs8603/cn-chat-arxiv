{
    "title": "BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via Graph Representation Pretraining. (arXiv:2401.06443v1 [cs.CL])",
    "abstract": "The current research direction in generative models, such as the recently developed GPT4, aims to find relevant knowledge information for multimodal and multilingual inputs to provide answers. Under these research circumstances, the demand for multilingual evaluation of visual question answering (VQA) tasks, a representative task of multimodal systems, has increased. Accordingly, we propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that can be extended to multilingualism. The proposed data include 17K images, 17K question-answer pairs for both Korean and English and 280K instances of knowledge information related to question-answer content. We also present a framework that can effectively inject knowledge information into a VQA system by pretraining the knowledge information of BOK-VQA data in the form of graph embeddings. Finally, through in-depth analysis, we demonstrated the actual effect of the knowledge information contained in the constructed training data",
    "link": "http://arxiv.org/abs/2401.06443",
    "context": "Title: BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via Graph Representation Pretraining. (arXiv:2401.06443v1 [cs.CL])\nAbstract: The current research direction in generative models, such as the recently developed GPT4, aims to find relevant knowledge information for multimodal and multilingual inputs to provide answers. Under these research circumstances, the demand for multilingual evaluation of visual question answering (VQA) tasks, a representative task of multimodal systems, has increased. Accordingly, we propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that can be extended to multilingualism. The proposed data include 17K images, 17K question-answer pairs for both Korean and English and 280K instances of knowledge information related to question-answer content. We also present a framework that can effectively inject knowledge information into a VQA system by pretraining the knowledge information of BOK-VQA data in the form of graph embeddings. Finally, through in-depth analysis, we demonstrated the actual effect of the knowledge information contained in the constructed training data",
    "path": "papers/24/01/2401.06443.json",
    "total_tokens": 872,
    "translated_title": "BOK-VQA: 基于外部知识的双语视觉问答系统通过图表示预训练",
    "translated_abstract": "目前的生成模型研究方向，如最近开发的GPT4，旨在为多模态和多语言输入寻找相关的知识信息以提供答案。根据这些研究情况，对多语言评估视觉问答（VQA）任务的需求，作为多模态系统的代表任务，逐渐增加。因此，我们在本研究中提出了一种能够扩展到多语言的双语外部知识VQA（BOK-VQA）数据集。所提出的数据包括17K张图片，17K个韩语和英语的问题-回答对以及与问题-回答内容相关的28K个知识信息实例。我们还提出了一个框架，通过以图嵌入的形式预训练BOK-VQA数据的知识信息，可以有效地将知识信息注入VQA系统中。最后，通过深入分析，我们展示了构建训练数据中包含的知识信息的实际效果。",
    "tldr": "本研究提出了BOK-VQA数据集，包含多语言的视觉问答数据以及与问题-回答内容相关的知识信息。通过以图嵌入的形式预训练数据的知识信息，可以有效地将外部知识注入VQA系统中，实现更好的问答效果。"
}