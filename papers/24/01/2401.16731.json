{
    "title": "Towards Generating Informative Textual Description for Neurons in Language Models. (arXiv:2401.16731v1 [cs.CL])",
    "abstract": "Recent developments in transformer-based language models have allowed them to capture a wide variety of world knowledge that can be adapted to downstream tasks with limited resources. However, what pieces of information are understood in these models is unclear, and neuron-level contributions in identifying them are largely unknown. Conventional approaches in neuron explainability either depend on a finite set of pre-defined descriptors or require manual annotations for training a secondary model that can then explain the neurons of the primary model. In this paper, we take BERT as an example and we try to remove these constraints and propose a novel and scalable framework that ties textual descriptions to neurons. We leverage the potential of generative language models to discover human-interpretable descriptors present in a dataset and use an unsupervised approach to explain neurons with these descriptors. Through various qualitative and quantitative analyses, we demonstrate the effe",
    "link": "http://arxiv.org/abs/2401.16731",
    "context": "Title: Towards Generating Informative Textual Description for Neurons in Language Models. (arXiv:2401.16731v1 [cs.CL])\nAbstract: Recent developments in transformer-based language models have allowed them to capture a wide variety of world knowledge that can be adapted to downstream tasks with limited resources. However, what pieces of information are understood in these models is unclear, and neuron-level contributions in identifying them are largely unknown. Conventional approaches in neuron explainability either depend on a finite set of pre-defined descriptors or require manual annotations for training a secondary model that can then explain the neurons of the primary model. In this paper, we take BERT as an example and we try to remove these constraints and propose a novel and scalable framework that ties textual descriptions to neurons. We leverage the potential of generative language models to discover human-interpretable descriptors present in a dataset and use an unsupervised approach to explain neurons with these descriptors. Through various qualitative and quantitative analyses, we demonstrate the effe",
    "path": "papers/24/01/2401.16731.json",
    "total_tokens": 900,
    "translated_title": "为语言模型中的神经元生成信息性文本描述的研究",
    "translated_abstract": "近期基于Transformer的语言模型的发展使其能够捕捉到各种世界知识并适应具有有限资源的下游任务。然而，这些模型理解哪些信息尚不清楚，而在识别它们方面的神经元级贡献基本上是未知的。神经元解释的传统方法要么依赖于有限的预定义描述符，要么需要手动注释以训练一个能够解释主模型神经元的次要模型。本文以BERT为例，尝试摆脱这些限制，提出了一种新颖而可伸缩的框架，将文本描述与神经元联系起来。我们利用生成语言模型的潜力，在数据集中发现人可解释的描述符，并使用无监督方法解释带有这些描述符的神经元。通过各种定性和定量分析，我们展示了这种方法的有效性。",
    "tldr": "本文提出了一种新颖而可伸缩的框架，将文本描述与语言模型中的神经元联系起来，从而解释模型中理解的信息。通过使用生成语言模型发现人可解释的描述符，并使用无监督方法解释神经元，通过定性和定量分析证明了该方法的有效性。"
}