{
    "title": "Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models. (arXiv:2401.08683v1 [cs.AR])",
    "abstract": "The design and optimization of hardware have traditionally been resource-intensive, demanding considerable expertise and dependence on established design automation tools. This paper discusses the possibility of exploiting large language models to streamline the code generation process in hardware design. In contrast to earlier studies, this paper aims to use large language models that accepts high-level design specifications through a single prompt to generate corresponding Register-Transfer Level (RTL) code. The ability to use large language models on RTL code generation not only expedites design iteration cycles but also facilitates the exploration of design spaces that have computational challenges for conventional techniques. Through our evaluation, we demonstrate the shortcoming of existing attention mechanisms, and present the abilities of language models to produce functional, optimized, and industry-standard compliant RTL code when a novel attention mechanism is used. These fi",
    "link": "http://arxiv.org/abs/2401.08683",
    "context": "Title: Zero-Shot RTL Code Generation with Attention Sink Augmented Large Language Models. (arXiv:2401.08683v1 [cs.AR])\nAbstract: The design and optimization of hardware have traditionally been resource-intensive, demanding considerable expertise and dependence on established design automation tools. This paper discusses the possibility of exploiting large language models to streamline the code generation process in hardware design. In contrast to earlier studies, this paper aims to use large language models that accepts high-level design specifications through a single prompt to generate corresponding Register-Transfer Level (RTL) code. The ability to use large language models on RTL code generation not only expedites design iteration cycles but also facilitates the exploration of design spaces that have computational challenges for conventional techniques. Through our evaluation, we demonstrate the shortcoming of existing attention mechanisms, and present the abilities of language models to produce functional, optimized, and industry-standard compliant RTL code when a novel attention mechanism is used. These fi",
    "path": "papers/24/01/2401.08683.json",
    "total_tokens": 790,
    "translated_title": "使用增强型大型语言模型实现零样本RTL代码生成",
    "translated_abstract": "传统上，硬件设计和优化需耗费大量资源，需要相当专业知识，并依赖于已建立的设计自动化工具。本文讨论了利用大型语言模型来简化硬件设计中的代码生成过程的可能性。与之前的研究不同，本文旨在使用大型语言模型，通过一个单一提示接受高层设计规范，生成相应的寄存器传输级（RTL）代码。能够在RTL代码生成中使用大型语言模型不仅加快了设计迭代周期，还便于探索传统技术难以处理的设计空间的计算挑战。通过我们的评估，我们展示了现有注意力机制的不足，并介绍了语言模型在使用一种新的注意力机制时产生功能、优化且符合行业标准的RTL代码的能力。",
    "tldr": "本文探讨了使用大型语言模型在硬件设计中快速生成RTL代码的可能性，并展示了新的注意力机制如何提供功能、优化和符合行业标准的RTL代码。"
}