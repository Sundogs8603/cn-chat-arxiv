{
    "title": "Deep Learning with Tabular Data: A Self-supervised Approach. (arXiv:2401.15238v1 [cs.LG])",
    "abstract": "We have described a novel approach for training tabular data using the TabTransformer model with self-supervised learning. Traditional machine learning models for tabular data, such as GBDT are being widely used though our paper examines the effectiveness of the TabTransformer which is a Transformer based model optimised specifically for tabular data. The TabTransformer captures intricate relationships and dependencies among features in tabular data by leveraging the self-attention mechanism of Transformers. We have used a self-supervised learning approach in this study, where the TabTransformer learns from unlabelled data by creating surrogate supervised tasks, eliminating the need for the labelled data. The aim is to find the most effective TabTransformer model representation of categorical and numerical features. To address the challenges faced during the construction of various input settings into the Transformers. Furthermore, a comparative analysis is also been conducted to exami",
    "link": "http://arxiv.org/abs/2401.15238",
    "context": "Title: Deep Learning with Tabular Data: A Self-supervised Approach. (arXiv:2401.15238v1 [cs.LG])\nAbstract: We have described a novel approach for training tabular data using the TabTransformer model with self-supervised learning. Traditional machine learning models for tabular data, such as GBDT are being widely used though our paper examines the effectiveness of the TabTransformer which is a Transformer based model optimised specifically for tabular data. The TabTransformer captures intricate relationships and dependencies among features in tabular data by leveraging the self-attention mechanism of Transformers. We have used a self-supervised learning approach in this study, where the TabTransformer learns from unlabelled data by creating surrogate supervised tasks, eliminating the need for the labelled data. The aim is to find the most effective TabTransformer model representation of categorical and numerical features. To address the challenges faced during the construction of various input settings into the Transformers. Furthermore, a comparative analysis is also been conducted to exami",
    "path": "papers/24/01/2401.15238.json",
    "total_tokens": 836,
    "translated_title": "基于自监督学习的表格数据深度学习：一种自监督学习的方法",
    "translated_abstract": "我们描述了一种利用TabTransformer模型和自监督学习来训练表格数据的新方法。传统的表格数据机器学习模型，如GBDT，虽然被广泛使用，但我们的论文研究了专为表格数据优化的TabTransformer的有效性。TabTransformer通过利用Transformer的自注意机制捕捉表格数据中特征之间的复杂关系和依赖关系。我们在这项研究中使用了自监督学习方法，TabTransformer通过创建代理监督任务从无标签数据中学习，消除了对标记数据的需求。目标是找到最有效的TabTransformer模型来表示分类和数值特征。为了解决在Transformer中构建不同输入设置时所面临的挑战，我们还进行了比较分析。",
    "tldr": "我们提出了一种使用自监督学习和TabTransformer模型进行表格数据训练的新方法，该方法能够捕捉表格数据中的复杂关系和依赖关系。相比传统的机器学习模型，我们的方法能够消除对标记数据的需求。"
}