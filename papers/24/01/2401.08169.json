{
    "title": "Statistical Test for Attention Map in Vision Transformer. (arXiv:2401.08169v1 [stat.ML])",
    "abstract": "The Vision Transformer (ViT) demonstrates exceptional performance in various computer vision tasks. Attention is crucial for ViT to capture complex wide-ranging relationships among image patches, allowing the model to weigh the importance of image patches and aiding our understanding of the decision-making process. However, when utilizing the attention of ViT as evidence in high-stakes decision-making tasks such as medical diagnostics, a challenge arises due to the potential of attention mechanisms erroneously focusing on irrelevant regions. In this study, we propose a statistical test for ViT's attentions, enabling us to use the attentions as reliable quantitative evidence indicators for ViT's decision-making with a rigorously controlled error rate. Using the framework called selective inference, we quantify the statistical significance of attentions in the form of p-values, which enables the theoretically grounded quantification of the false positive detection probability of attentio",
    "link": "http://arxiv.org/abs/2401.08169",
    "context": "Title: Statistical Test for Attention Map in Vision Transformer. (arXiv:2401.08169v1 [stat.ML])\nAbstract: The Vision Transformer (ViT) demonstrates exceptional performance in various computer vision tasks. Attention is crucial for ViT to capture complex wide-ranging relationships among image patches, allowing the model to weigh the importance of image patches and aiding our understanding of the decision-making process. However, when utilizing the attention of ViT as evidence in high-stakes decision-making tasks such as medical diagnostics, a challenge arises due to the potential of attention mechanisms erroneously focusing on irrelevant regions. In this study, we propose a statistical test for ViT's attentions, enabling us to use the attentions as reliable quantitative evidence indicators for ViT's decision-making with a rigorously controlled error rate. Using the framework called selective inference, we quantify the statistical significance of attentions in the form of p-values, which enables the theoretically grounded quantification of the false positive detection probability of attentio",
    "path": "papers/24/01/2401.08169.json",
    "total_tokens": 814,
    "translated_title": "Vision Transformer中的注意力图统计检验",
    "translated_abstract": "Vision Transformer（ViT）在各种计算机视觉任务中展示出了出色的性能。注意力对于ViT捕捉图像补丁之间复杂广泛的关系非常重要，使得模型可以权衡图像补丁的重要性，并帮助我们理解决策过程。然而，当将ViT的注意力用作高风险决策任务（如医学诊断）中的证据时，面临一个挑战，即注意机制可能错误地关注无关的区域。在本研究中，我们提出了一种ViT注意力的统计检验，使我们能够将注意力作为可靠的定量证据指标用于ViT的决策，并严格控制误差率。使用选择性推理框架，我们以p值的形式量化注意力的统计显著性，从而能够理论上基于假阳性检测概率量化注意力。",
    "tldr": "本研究提出了一种Vision Transformer中注意力图的统计检验方法，可以将注意力作为可靠的定量证据指标用于决策，并通过p值进行统计显著性量化。",
    "en_tdlr": "This study proposes a statistical test for attention maps in Vision Transformer, allowing them to be used as reliable quantitative evidence indicators for decision-making and quantified through p-values."
}