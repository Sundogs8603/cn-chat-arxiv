{
    "title": "Trainable Fixed-Point Quantization for Deep Learning Acceleration on FPGAs",
    "abstract": "Quantization is a crucial technique for deploying deep learning models on resource-constrained devices, such as embedded FPGAs. Prior efforts mostly focus on quantizing matrix multiplications, leaving other layers like BatchNorm or shortcuts in floating-point form, even though fixed-point arithmetic is more efficient on FPGAs. A common practice is to fine-tune a pre-trained model to fixed-point for FPGA deployment, but potentially degrading accuracy.   This work presents QFX, a novel trainable fixed-point quantization approach that automatically learns the binary-point position during model training. Additionally, we introduce a multiplier-free quantization strategy within QFX to minimize DSP usage. QFX is implemented as a PyTorch-based library that efficiently emulates fixed-point arithmetic, supported by FPGA HLS, in a differentiable manner during backpropagation. With minimal effort, models trained with QFX can readily be deployed through HLS, producing the same numerical results as",
    "link": "https://arxiv.org/abs/2401.17544",
    "context": "Title: Trainable Fixed-Point Quantization for Deep Learning Acceleration on FPGAs\nAbstract: Quantization is a crucial technique for deploying deep learning models on resource-constrained devices, such as embedded FPGAs. Prior efforts mostly focus on quantizing matrix multiplications, leaving other layers like BatchNorm or shortcuts in floating-point form, even though fixed-point arithmetic is more efficient on FPGAs. A common practice is to fine-tune a pre-trained model to fixed-point for FPGA deployment, but potentially degrading accuracy.   This work presents QFX, a novel trainable fixed-point quantization approach that automatically learns the binary-point position during model training. Additionally, we introduce a multiplier-free quantization strategy within QFX to minimize DSP usage. QFX is implemented as a PyTorch-based library that efficiently emulates fixed-point arithmetic, supported by FPGA HLS, in a differentiable manner during backpropagation. With minimal effort, models trained with QFX can readily be deployed through HLS, producing the same numerical results as",
    "path": "papers/24/01/2401.17544.json",
    "total_tokens": 846,
    "translated_title": "可训练的固定点量化加速深度学习在FPGA上的实施",
    "translated_abstract": "量化是在资源受限设备（如嵌入式FPGA）上部署深度学习模型的重要技术。先前的工作主要关注于对矩阵乘法进行量化，而将BatchNorm或shortcut等其他层保留为浮点形式，尽管在FPGA上固定点算术更高效。一种常见的做法是对预训练模型进行微调以进行FPGA部署，但可能会降低准确性。本文提出了一种新颖的可训练固定点量化方法QFX，该方法在模型训练过程中自动学习二进制点位置。此外，我们引入了一种无乘法的量化策略，以最小化DSP的使用。QFX是基于PyTorch的库，可以在反向传播过程中以可微分的方式有效地模拟固定点算术，并得到FPGA HLS支持。通过使用QFX训练的模型，可以轻松地通过HLS部署，并产生与浮点模型相同的数值结果。",
    "tldr": "本论文提出了一种可训练的固定点量化方法QFX，实现了基于FPGA的深度学习加速。QFX可以自动学习二进制点位置，并引入了无乘法的量化策略以最小化硬件资源的使用。"
}