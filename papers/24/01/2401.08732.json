{
    "title": "Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. (arXiv:2401.08732v1 [cs.LG])",
    "abstract": "It is believed that in knowledge distillation (KD), the role of the teacher is to provide an estimate for the unknown Bayes conditional probability distribution (BCPD) to be used in the student training process. Conventionally, this estimate is obtained by training the teacher using maximum log-likelihood (MLL) method. To improve this estimate for KD, in this paper we introduce the concept of conditional mutual information (CMI) into the estimation of BCPD and propose a novel estimator called the maximum CMI (MCMI) method. Specifically, in MCMI estimation, both the log-likelihood and CMI of the teacher are simultaneously maximized when the teacher is trained. Through Eigen-CAM, it is further shown that maximizing the teacher's CMI value allows the teacher to capture more contextual information in an image cluster. Via conducting a thorough set of experiments, we show that by employing a teacher trained via MCMI estimation rather than one trained via MLL estimation in various state-of-t",
    "link": "http://arxiv.org/abs/2401.08732",
    "context": "Title: Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information. (arXiv:2401.08732v1 [cs.LG])\nAbstract: It is believed that in knowledge distillation (KD), the role of the teacher is to provide an estimate for the unknown Bayes conditional probability distribution (BCPD) to be used in the student training process. Conventionally, this estimate is obtained by training the teacher using maximum log-likelihood (MLL) method. To improve this estimate for KD, in this paper we introduce the concept of conditional mutual information (CMI) into the estimation of BCPD and propose a novel estimator called the maximum CMI (MCMI) method. Specifically, in MCMI estimation, both the log-likelihood and CMI of the teacher are simultaneously maximized when the teacher is trained. Through Eigen-CAM, it is further shown that maximizing the teacher's CMI value allows the teacher to capture more contextual information in an image cluster. Via conducting a thorough set of experiments, we show that by employing a teacher trained via MCMI estimation rather than one trained via MLL estimation in various state-of-t",
    "path": "papers/24/01/2401.08732.json",
    "total_tokens": 955,
    "translated_title": "基于条件互信息的贝叶斯条件分布估计用于知识蒸馏",
    "translated_abstract": "在知识蒸馏中，通常认为教师的角色是提供用于学生训练过程中的未知贝叶斯条件概率分布（BCPD）的估计。传统上，通过使用最大对数似然（MLL）方法训练教师来获得此估计。为了改进知识蒸馏中的估计，本文将条件互信息（CMI）的概念引入到BCPD的估计中，提出了一种新的估计方法，称为最大CMI（MCMI）方法。具体而言，在MCMI估计中，教师在训练时同时最大化对数似然和条件互信息。通过Eigen-CAM，进一步展示了最大化教师的CMI值可以使教师在图像聚类中捕捉更多的上下文信息。通过进行一系列彻底的实验，我们展示了通过使用通过MCMI估计训练的教师而不是通过MLL估计训练的教师，在各种状态下的性能提升。",
    "tldr": "本文介绍了一种基于条件互信息的贝叶斯条件分布估计方法，通过最大化教师的对数似然和条件互信息来改进知识蒸馏中的估计。实验证明，使用该方法训练的教师能够更好地捕捉图像聚类中的上下文信息。",
    "en_tdlr": "This paper proposes a novel estimator for Bayesian conditional distribution estimation in knowledge distillation using maximum conditional mutual information. The experiments demonstrate that training the teacher using this method allows for better capture of contextual information in image clustering."
}