{
    "title": "Explaining Predictive Uncertainty by Exposing Second-Order Effects",
    "abstract": "Explainable AI has brought transparency into complex ML blackboxes, enabling, in particular, to identify which features these models use for their predictions. So far, the question of explaining predictive uncertainty, i.e. why a model 'doubts', has been scarcely studied. Our investigation reveals that predictive uncertainty is dominated by second-order effects, involving single features or product interactions between them. We contribute a new method for explaining predictive uncertainty based on these second-order effects. Computationally, our method reduces to a simple covariance computation over a collection of first-order explanations. Our method is generally applicable, allowing for turning common attribution techniques (LRP, Gradient x Input, etc.) into powerful second-order uncertainty explainers, which we call CovLRP, CovGI, etc. The accuracy of the explanations our method produces is demonstrated through systematic quantitative evaluations, and the overall usefulness of our m",
    "link": "https://arxiv.org/abs/2401.17441",
    "context": "Title: Explaining Predictive Uncertainty by Exposing Second-Order Effects\nAbstract: Explainable AI has brought transparency into complex ML blackboxes, enabling, in particular, to identify which features these models use for their predictions. So far, the question of explaining predictive uncertainty, i.e. why a model 'doubts', has been scarcely studied. Our investigation reveals that predictive uncertainty is dominated by second-order effects, involving single features or product interactions between them. We contribute a new method for explaining predictive uncertainty based on these second-order effects. Computationally, our method reduces to a simple covariance computation over a collection of first-order explanations. Our method is generally applicable, allowing for turning common attribution techniques (LRP, Gradient x Input, etc.) into powerful second-order uncertainty explainers, which we call CovLRP, CovGI, etc. The accuracy of the explanations our method produces is demonstrated through systematic quantitative evaluations, and the overall usefulness of our m",
    "path": "papers/24/01/2401.17441.json",
    "total_tokens": 969,
    "translated_title": "揭示二阶影响来解释预测不确定性",
    "translated_abstract": "可解释的人工智能（Explainable AI）使复杂的机器学习黑箱变得透明，特别是可以确定模型用来进行预测的特征。然而，关于解释预测不确定性，即为什么模型“不确定”，目前研究较少。我们的研究发现，预测不确定性主要由涉及单个特征或特征之间的乘积相互作用的二阶影响所主导。我们提出了一种基于这些二阶影响来解释预测不确定性的新方法。在计算上，我们的方法简化成对一组一阶解释进行简单协方差计算。我们的方法具有普遍适用性，可以将常见的归因技术（LRP，Gradient x Input等）转化为强大的二阶不确定性解释器，称为CovLRP，CovGI等。我们通过系统量化评估验证了我们方法产生解释的准确性，展示了我们方法的整体实用性。",
    "tldr": "该论文研究发现，预测不确定性主要受到单个特征或特征之间乘积相互作用的二阶影响的影响。作者提出了一种基于这些二阶影响来解释预测不确定性的新方法。该方法通过简单的协方差计算对一阶解释进行处理，可以将常见的归因技术转化为强大的二阶不确定性解释器。作者通过量化评估验证了该方法解释的准确性，并展示了整体实用性。",
    "en_tdlr": "This paper reveals that predictive uncertainty is mostly affected by second-order effects involving single features or product interactions between them. The authors propose a new method that explains predictive uncertainty based on these second-order effects. The method simplifies to a covariance computation over a collection of first-order explanations, making it applicable to various attribution techniques and demonstrating accuracy and overall usefulness."
}