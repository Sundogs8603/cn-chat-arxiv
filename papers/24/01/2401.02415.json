{
    "title": "LLaMA Pro: Progressive LLaMA with Block Expansion. (arXiv:2401.02415v1 [cs.CL])",
    "abstract": "Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a",
    "link": "http://arxiv.org/abs/2401.02415",
    "context": "Title: LLaMA Pro: Progressive LLaMA with Block Expansion. (arXiv:2401.02415v1 [cs.CL])\nAbstract: Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a",
    "path": "papers/24/01/2401.02415.json",
    "total_tokens": 971,
    "translated_title": "LLaMA Pro: 带有模块扩展的渐进LLaMA方法",
    "translated_abstract": "人类通常在不牺牲旧技能的情况下学习新技能；然而，对于大型语言模型（LLMs），例如从LLaMA到CodeLLaMA则相反。为此，我们提出了一种具有Transformer模块扩展的LLMs的新的预训练后处理方法。我们仅使用新的语料库调整扩展的模块，以有效地提高模型的知识而不会发生灾难性遗忘。本文在代码和数学语料库上进行实验，得到了从LLaMA2-7B初始化的通用基础模型LLaMA Pro-8.3B，在常规任务、编程和数学方面表现出色。LLaMA Pro及其按照指令执行的对应模型（LLaMA Pro-Instruct）在各种基准测试中都取得了先进的性能，展示了在LLaMA系列和其他开放模型中的优越性以及作为智能体推理和解决多样化任务的巨大潜力。我们的发现对于整合自然语言和编程语言提供了有价值的见解，为发展推理和解决多样化任务的智能体奠定了基础。",
    "tldr": "本论文提出了一种新型的LLMs后处理方法，通过扩展Transformer模块并使用新的语料库进行调整，有效地提高了模型知识，而不会发生灾难性遗忘。在代码和数学领域的实验结果表明，LLaMA Pro是一种功能强大且适用于通用任务、编程和数学的基础模型，同时在各种基准测试中取得了先进的性能，展示了智能体推理和解决多样化任务的潜力。"
}