{
    "title": "Multilinear Operator Networks",
    "abstract": "Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area and has yet to be eliminated. On the other hand, Polynomial Networks is a class of models that does not require activation functions, but have yet to perform on par with modern architectures. In this work, we aim close this gap and propose MONet, which relies solely on multilinear operators. The core layer of MONet, called Mu-Layer, captures multiplicative interactions of the elements of the input token. MONet captures high-degree interactions of the input elements and we demonstrate the efficacy of our approach on a series of image recognition and scientific computing benchmarks. The proposed model outperforms prior polynomial networks and performs on par with modern architectures. We believe that MONet can inspire further research on models that use entirely multilinear operations.",
    "link": "https://arxiv.org/abs/2401.17992",
    "context": "Title: Multilinear Operator Networks\nAbstract: Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area and has yet to be eliminated. On the other hand, Polynomial Networks is a class of models that does not require activation functions, but have yet to perform on par with modern architectures. In this work, we aim close this gap and propose MONet, which relies solely on multilinear operators. The core layer of MONet, called Mu-Layer, captures multiplicative interactions of the elements of the input token. MONet captures high-degree interactions of the input elements and we demonstrate the efficacy of our approach on a series of image recognition and scientific computing benchmarks. The proposed model outperforms prior polynomial networks and performs on par with modern architectures. We believe that MONet can inspire further research on models that use entirely multilinear operations.",
    "path": "papers/24/01/2401.17992.json",
    "total_tokens": 863,
    "translated_title": "多线性算子网络",
    "translated_abstract": "尽管深度神经网络在图像识别方面具有显著的能力，但对激活函数的依赖仍然是一个基本未探索的领域，并且有待消除。另一方面，多项式网络是一类不需要激活函数的模型，但其性能仍未与现代架构相媲美。在本文中，我们旨在弥合这一差距，并提出了仅依赖多线性算子的MONet模型。MONet的核心层称为Mu-Layer，捕捉了输入元素的乘法交互。MONet捕捉了输入元素的高次交互，并通过一系列图像识别和科学计算基准测试展示了我们方法的有效性。所提出的模型优于以前的多项式网络，并与现代架构性能相近。我们相信MONet可以激发对完全使用多线性操作的模型进一步的研究。",
    "tldr": "该论文提出了一种名为MONet的模型，该模型仅依赖多线性算子来进行图像识别，通过捕捉输入元素的高次交互，优于以前的多项式网络，并与现代架构性能相近。这一研究可以激发对完全使用多线性操作的模型的进一步研究。",
    "en_tdlr": "The paper introduces a model called MONet, which relies solely on multilinear operators for image recognition. By capturing high-degree interactions of the input elements, the proposed model outperforms prior polynomial networks and performs on par with modern architectures. This research can inspire further exploration of models that use entirely multilinear operations."
}