{
    "title": "Maximum Causal Entropy Inverse Reinforcement Learning for Mean-Field Games. (arXiv:2401.06566v1 [eess.SY])",
    "abstract": "In this paper, we introduce the maximum casual entropy Inverse Reinforcement Learning (IRL) problem for discrete-time mean-field games (MFGs) under an infinite-horizon discounted-reward optimality criterion. The state space of a typical agent is finite. Our approach begins with a comprehensive review of the maximum entropy IRL problem concerning deterministic and stochastic Markov decision processes (MDPs) in both finite and infinite-horizon scenarios. Subsequently, we formulate the maximum casual entropy IRL problem for MFGs - a non-convex optimization problem with respect to policies. Leveraging the linear programming formulation of MDPs, we restructure this IRL problem into a convex optimization problem and establish a gradient descent algorithm to compute the optimal solution with a rate of convergence. Finally, we present a new algorithm by formulating the MFG problem as a generalized Nash equilibrium problem (GNEP), which is capable of computing the mean-field equilibrium (MFE) f",
    "link": "http://arxiv.org/abs/2401.06566",
    "context": "Title: Maximum Causal Entropy Inverse Reinforcement Learning for Mean-Field Games. (arXiv:2401.06566v1 [eess.SY])\nAbstract: In this paper, we introduce the maximum casual entropy Inverse Reinforcement Learning (IRL) problem for discrete-time mean-field games (MFGs) under an infinite-horizon discounted-reward optimality criterion. The state space of a typical agent is finite. Our approach begins with a comprehensive review of the maximum entropy IRL problem concerning deterministic and stochastic Markov decision processes (MDPs) in both finite and infinite-horizon scenarios. Subsequently, we formulate the maximum casual entropy IRL problem for MFGs - a non-convex optimization problem with respect to policies. Leveraging the linear programming formulation of MDPs, we restructure this IRL problem into a convex optimization problem and establish a gradient descent algorithm to compute the optimal solution with a rate of convergence. Finally, we present a new algorithm by formulating the MFG problem as a generalized Nash equilibrium problem (GNEP), which is capable of computing the mean-field equilibrium (MFE) f",
    "path": "papers/24/01/2401.06566.json",
    "total_tokens": 899,
    "translated_title": "最大因果熵逆强化学习用于均场博弈问题",
    "translated_abstract": "本文介绍了在无限时间间隔折扣回报最优性准则下，针对离散时间均场博弈（MFG）的最大因果熵逆强化学习（IRL）问题。典型智能体的状态空间是有限的。我们的方法首先全面回顾了关于确定性和随机马尔科夫决策过程（MDPs）在有限和无限时间间隔情",
    "tldr": "本文介绍了最大因果熵逆强化学习（IRL）方法用于均场博弈（MFG）问题，提出了将MFG问题转化为广义纳什均衡问题（GNEP）的新算法。",
    "en_tdlr": "This paper introduces the maximum casual entropy Inverse Reinforcement Learning (IRL) method for mean-field games (MFG). It also proposes a new algorithm by formulating the MFG problem as a generalized Nash equilibrium problem (GNEP)."
}