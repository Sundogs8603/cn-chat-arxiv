{
    "title": "AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data. (arXiv:2401.10220v1 [cs.CV])",
    "abstract": "Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different lo",
    "link": "http://arxiv.org/abs/2401.10220",
    "context": "Title: AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data. (arXiv:2401.10220v1 [cs.CV])\nAbstract: Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different lo",
    "path": "papers/24/01/2401.10220.json",
    "total_tokens": 804,
    "translated_title": "AutoFT: 通过优化OOD数据上的超参数实现鲁棒的微调",
    "translated_abstract": "基础模型可以通过在特定任务的数据上进行微调来适应所需任务，但是在一种特定数据分布上微调模型往往会损害模型在其他分布上的原始性能。目前的鲁棒微调方法利用手工制定的正则化技术来约束微调过程，以保留基础模型的特征。然而，很难准确指定在微调过程中应保留哪些基础模型的特征，因为这取决于预训练、微调和评估数据分布之间的关系。我们提出了AutoFT，一种数据驱动的方法来引导基础模型的微调。AutoFT通过优化微调超参数来在小的ODD验证集上最大化性能。为了以细粒度的方式指导微调，AutoFT搜索一个高度表达的超参数空间，其中包括许多不同的权重系数。",
    "tldr": "AutoFT是一种通过优化超参数在OOD数据上进行基础模型微调的方法，以实现鲁棒性能。"
}