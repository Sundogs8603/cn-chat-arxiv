{
    "title": "Supervised Fine-tuning in turn Improves Visual Foundation Models. (arXiv:2401.10222v1 [cs.CV])",
    "abstract": "Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and visi",
    "link": "http://arxiv.org/abs/2401.10222",
    "context": "Title: Supervised Fine-tuning in turn Improves Visual Foundation Models. (arXiv:2401.10222v1 [cs.CV])\nAbstract: Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and visi",
    "path": "papers/24/01/2401.10222.json",
    "total_tokens": 928,
    "translated_title": "监督微调进一步改进了视觉基础模型",
    "translated_abstract": "近年来，像CLIP这样的图像-文本训练方法已经主导了视觉基础模型的预训练。随后，为了将区域级别的视觉学习引入CLIP的预训练中，在大规模区域级别数据集的缺乏下面临可扩展性挑战。受到自然语言处理中监督微调（SFT）的启发，比如指令微调，我们探索了在视觉基础模型的预训练之后，微粒SFT能够提升其生成能力的潜力。因此，我们提出了一个两阶段的方法ViSFT（Vision SFT），来释放视觉基础模型的细粒度知识。在ViSFT中，通过在一些领域内任务上进行视觉联合学习来增强视觉基础模型，然后在领域外基准测试中进行测试。通过使用ViSFT在少于2天内在8个V100 GPU上进行更新，一个具有超过4.4B参数的视觉Transformer在各种领域外基准测试中显示出改进。",
    "tldr": "本文提出了一个叫做ViSFT（Vision SFT）的两阶段方法，通过在领域内任务上进行视觉联合学习来提升视觉基础模型的生成能力，在各种领域外基准测试中取得了改进。",
    "en_tdlr": "This paper proposes a two-stage method called ViSFT (Vision SFT) that enhances the generation capability of vision foundation models by performing visual joint learning on in-domain tasks, achieving improvements in various out-of-domain benchmarks."
}