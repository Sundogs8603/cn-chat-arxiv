{
    "title": "Transfer Learning for Text Diffusion Models",
    "abstract": "In this report, we explore the potential for text diffusion to replace autoregressive (AR) decoding for the training and deployment of large language models (LLMs). We are particularly interested to see whether pretrained AR models can be transformed into text diffusion models through a lightweight adaptation procedure we call ``AR2Diff''. We begin by establishing a strong baseline setup for training text diffusion models. Comparing across multiple architectures and pretraining objectives, we find that training a decoder-only model with a prefix LM objective is best or near-best across several tasks. Building on this finding, we test various transfer learning setups for text diffusion models. On machine translation, we find that text diffusion underperforms the standard AR approach. However, on code synthesis and extractive QA, we find diffusion models trained from scratch outperform AR models in many cases. We also observe quality gains from AR2Diff -- adapting AR models to use diffus",
    "link": "https://arxiv.org/abs/2401.17181",
    "context": "Title: Transfer Learning for Text Diffusion Models\nAbstract: In this report, we explore the potential for text diffusion to replace autoregressive (AR) decoding for the training and deployment of large language models (LLMs). We are particularly interested to see whether pretrained AR models can be transformed into text diffusion models through a lightweight adaptation procedure we call ``AR2Diff''. We begin by establishing a strong baseline setup for training text diffusion models. Comparing across multiple architectures and pretraining objectives, we find that training a decoder-only model with a prefix LM objective is best or near-best across several tasks. Building on this finding, we test various transfer learning setups for text diffusion models. On machine translation, we find that text diffusion underperforms the standard AR approach. However, on code synthesis and extractive QA, we find diffusion models trained from scratch outperform AR models in many cases. We also observe quality gains from AR2Diff -- adapting AR models to use diffus",
    "path": "papers/24/01/2401.17181.json",
    "total_tokens": 1132,
    "translated_title": "文本扩散模型的迁移学习",
    "translated_abstract": "在这份报告中，我们探索了使用文本扩散替代自回归（AR）解码进行大型语言模型（LLM）的训练和部署的潜力。我们特别关注于观察预训练的AR模型能否通过我们称之为“AR2Diff”的轻量级适应程序转化为文本扩散模型。我们首先建立了一个强大的基线设置来训练文本扩散模型。通过比较多种架构和预训练目标，我们发现使用仅解码器的模型并以前缀语言模型目标进行训练是在多个任务中最好或接近最好的方法。在此基础上，我们测试了各种文本扩散模型的迁移学习设置。在机器翻译任务中，我们发现文本扩散模型表现不如标准的AR方法。然而，在代码合成和抽取型问答任务上，我们发现从零开始训练的扩散模型在许多情况下优于AR模型。我们还观察到AR2Diff带来的质量提升——将AR模型适应为使用扩散的模型。",
    "tldr": "本文研究了文本扩散模型在大型语言模型训练和部署中取代自回归解码的潜力，并通过AR2Diff轻量级适应程序将预训练的AR模型转化为文本扩散模型。与多种架构和预训练目标进行比较后发现，使用仅解码器的模型并以前缀语言模型目标进行训练是最佳方法。在机器翻译中，文本扩散模型不如标准的AR方法，但在代码合成和抽取型问答上，从零开始训练的扩散模型在很多情况下优于AR模型。适应AR模型使用扩散的方法（AR2Diff）也带来质量提升。",
    "en_tdlr": "This paper explores the potential of text diffusion models in replacing autoregressive decoding for large language models. It introduces a lightweight adaptation procedure, AR2Diff, to transform pretrained autoregressive models into text diffusion models. The research finds that training a decoder-only model with a prefix language model objective yields the best results across multiple tasks. While text diffusion underperforms in machine translation, it outperforms autoregressive models in code synthesis and extractive QA. Adapting autoregressive models using diffusion, through AR2Diff, also improves model quality."
}