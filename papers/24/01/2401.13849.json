{
    "title": "TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance. (arXiv:2401.13849v1 [cs.CL])",
    "abstract": "Large Language Models (LLMs) have recently showcased remarkable reasoning abilities. However, larger models often surpass their smaller counterparts in reasoning tasks, posing the challenge of effectively transferring these capabilities from larger models. Existing approaches heavily rely on extensive fine-tuning data or continuous interactions with a superior teacher LLM during inference. We introduce a principle-based teacher-student framework called ``Teaching via Principle Discovery'' (TPD) to address these limitations. Inspired by human learning mechanisms, TPD mimics the interaction between a teacher and a student using a principle-based approach. The teacher LLM generates problem-solving instructions and corrective principles based on the student LLM's errors. These principles guide the refinement of instructions and the selection of instructive examples from a validation set. This enables the student model to learn from both the teacher's guidance and its own mistakes. Once the",
    "link": "http://arxiv.org/abs/2401.13849",
    "context": "Title: TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance. (arXiv:2401.13849v1 [cs.CL])\nAbstract: Large Language Models (LLMs) have recently showcased remarkable reasoning abilities. However, larger models often surpass their smaller counterparts in reasoning tasks, posing the challenge of effectively transferring these capabilities from larger models. Existing approaches heavily rely on extensive fine-tuning data or continuous interactions with a superior teacher LLM during inference. We introduce a principle-based teacher-student framework called ``Teaching via Principle Discovery'' (TPD) to address these limitations. Inspired by human learning mechanisms, TPD mimics the interaction between a teacher and a student using a principle-based approach. The teacher LLM generates problem-solving instructions and corrective principles based on the student LLM's errors. These principles guide the refinement of instructions and the selection of instructive examples from a validation set. This enables the student model to learn from both the teacher's guidance and its own mistakes. Once the",
    "path": "papers/24/01/2401.13849.json",
    "total_tokens": 899,
    "translated_title": "TPD: 通过原则发现和指导提升学生语言模型的推理能力",
    "translated_abstract": "大型语言模型(LLM)最近展示出了令人惊叹的推理能力。然而，更大的模型往往在推理任务上超过了较小的模型，因此有效地将这些能力从较大的模型转移到较小的模型是一项挑战。现有的方法往往依赖于大量的微调数据或在推理过程中与优秀的教师LLM进行持续的交互。我们引入了一种基于原则的师生框架，称为“通过原则发现教学”(TPD)，以解决这些限制。受人类学习机制启发，TPD模仿了教师和学生之间的互动，采用基于原则的方法。教师LLM生成问题解决指令和纠正原则，基于学生LLM的错误。这些原则指导指令的完善和从验证集中选择有教育意义的示例。这使得学生模型能够从教师的指导和自己的错误中学习。一旦",
    "tldr": "通过原则发现和指导提升学生语言模型的推理能力的TPD框架模拟了教师和学生之间的互动，通过生成问题解决指令和纠正原则，从而引导学生模型从教师的指导和自身的错误中进行学习。",
    "en_tdlr": "TPD framework enhances student language model reasoning by simulating the interaction between a teacher and a student, guiding the student model's learning through generating problem-solving instructions and corrective principles."
}