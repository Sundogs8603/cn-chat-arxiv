{
    "title": "Scavenging Hyena: Distilling Transformers into Long Convolution Models",
    "abstract": "The rapid evolution of Large Language Models (LLMs), epitomized by architectures like GPT-4, has reshaped the landscape of natural language processing. This paper introduces a pioneering approach to address the efficiency concerns associated with LLM pre-training, proposing the use of knowledge distillation for cross-architecture transfer. Leveraging insights from the efficient Hyena mechanism, our method replaces attention heads in transformer models by Hyena, offering a cost-effective alternative to traditional pre-training while confronting the challenge of processing long contextual information, inherent in quadratic attention mechanisms. Unlike conventional compression-focused methods, our technique not only enhances inference speed but also surpasses pre-training in terms of both accuracy and efficiency. In the era of evolving LLMs, our work contributes to the pursuit of sustainable AI solutions, striking a balance between computational power and environmental impact.",
    "link": "https://arxiv.org/abs/2401.17574",
    "context": "Title: Scavenging Hyena: Distilling Transformers into Long Convolution Models\nAbstract: The rapid evolution of Large Language Models (LLMs), epitomized by architectures like GPT-4, has reshaped the landscape of natural language processing. This paper introduces a pioneering approach to address the efficiency concerns associated with LLM pre-training, proposing the use of knowledge distillation for cross-architecture transfer. Leveraging insights from the efficient Hyena mechanism, our method replaces attention heads in transformer models by Hyena, offering a cost-effective alternative to traditional pre-training while confronting the challenge of processing long contextual information, inherent in quadratic attention mechanisms. Unlike conventional compression-focused methods, our technique not only enhances inference speed but also surpasses pre-training in terms of both accuracy and efficiency. In the era of evolving LLMs, our work contributes to the pursuit of sustainable AI solutions, striking a balance between computational power and environmental impact.",
    "path": "papers/24/01/2401.17574.json",
    "total_tokens": 938,
    "translated_title": "Scavenging Hyena: 将Transformer模型精炼为长卷积模型",
    "translated_abstract": "大型语言模型（LLMs）的快速发展，以GPT-4等架构为典范，重塑了自然语言处理的领域。本文介绍了一种开创性方法，用于解决LLM预训练中的效率问题，提出了使用知识蒸馏进行跨架构迁移的方法。借鉴高效的Hyena机制的见解，我们的方法通过使用Hyena来替换Transformer模型中的注意力头，提供了一种经济有效的替代传统预训练的方法，同时要面对处理长上下文信息的挑战，这是二次注意力机制固有的。与传统的压缩方法不同，我们的技术不仅提高了推理速度，还在准确性和效率方面超越了预训练。在LLM不断发展的时代，我们的工作为追求可持续的人工智能解决方案作出了贡献，实现了计算能力和环境影响之间的平衡。",
    "tldr": "本文介绍了一种通过使用知识蒸馏将Transformer模型中的注意力头替换为Hyena，从而提高效率并处理长上下文信息的方法，超越了传统预训练方法，在准确性和效率方面取得了优秀的结果。这一技术为追求可持续的AI解决方案做出了贡献，实现了计算能力和环境影响的平衡。",
    "en_tdlr": "This paper introduces a method that replaces attention heads in Transformer models with Hyena through knowledge distillation, improving efficiency and handling long contextual information, and surpasses traditional pre-training methods in both accuracy and efficiency. This technique contributes to the pursuit of sustainable AI solutions by striking a balance between computational power and environmental impact."
}