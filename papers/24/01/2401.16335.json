{
    "title": "Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF. (arXiv:2401.16335v1 [cs.LG])",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.",
    "link": "http://arxiv.org/abs/2401.16335",
    "context": "Title: Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF. (arXiv:2401.16335v1 [cs.LG])\nAbstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.",
    "path": "papers/24/01/2401.16335.json",
    "total_tokens": 900,
    "translated_title": "迭代数据平滑：减轻强化学习从人类反馈中过拟合和过优化问题",
    "translated_abstract": "强化学习从人类反馈中（RLHF）是一种使语言模型与人类中心价值紧密对齐的关键技术。RLHF的初始阶段涉及使用排名数据的奖励模型来学习人类价值观。观察到在一轮训练后，奖励模型的性能会下降，并且过多地优化学习到的奖励模型最终会阻碍真正的目标。本文深入研究了这些问题，并利用理论洞察力设计了改进的奖励学习算法，称为“迭代数据平滑”（IDS）。核心思想是在每个训练轮次中，我们不仅用数据更新模型，还用模型更新数据，用软标签替换硬标签。我们的实证发现突出了这种方法相对于传统方法的优越性能。",
    "tldr": "这篇论文提出了一种称为“迭代数据平滑”的改进奖励学习算法，用于减轻强化学习从人类反馈中的过拟合和过优化问题。通过在每个训练轮次中更新模型和数据，并用软标签替换硬标签，该方法表现出优越性能。",
    "en_tdlr": "This paper proposes an improved reward learning algorithm called 'Iterative Data Smoothing' (IDS) to mitigate reward overfitting and overoptimization issues in reinforcement learning from human feedback. By updating both the model and the data during each training epoch and replacing hard labels with soft labels, this approach demonstrates superior performance compared to traditional methods."
}