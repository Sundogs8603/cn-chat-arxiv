{
    "title": "Enhancing Automatic Modulation Recognition through Robust Global Feature Extraction. (arXiv:2401.01056v1 [eess.SP])",
    "abstract": "Automatic Modulation Recognition (AMR) plays a crucial role in wireless communication systems. Deep learning AMR strategies have achieved tremendous success in recent years. Modulated signals exhibit long temporal dependencies, and extracting global features is crucial in identifying modulation schemes. Traditionally, human experts analyze patterns in constellation diagrams to classify modulation schemes. Classical convolutional-based networks, due to their limited receptive fields, excel at extracting local features but struggle to capture global relationships. To address this limitation, we introduce a novel hybrid deep framework named TLDNN, which incorporates the architectures of the transformer and long short-term memory (LSTM). We utilize the self-attention mechanism of the transformer to model the global correlations in signal sequences while employing LSTM to enhance the capture of temporal dependencies. To mitigate the impact like RF fingerprint features and channel characteri",
    "link": "http://arxiv.org/abs/2401.01056",
    "context": "Title: Enhancing Automatic Modulation Recognition through Robust Global Feature Extraction. (arXiv:2401.01056v1 [eess.SP])\nAbstract: Automatic Modulation Recognition (AMR) plays a crucial role in wireless communication systems. Deep learning AMR strategies have achieved tremendous success in recent years. Modulated signals exhibit long temporal dependencies, and extracting global features is crucial in identifying modulation schemes. Traditionally, human experts analyze patterns in constellation diagrams to classify modulation schemes. Classical convolutional-based networks, due to their limited receptive fields, excel at extracting local features but struggle to capture global relationships. To address this limitation, we introduce a novel hybrid deep framework named TLDNN, which incorporates the architectures of the transformer and long short-term memory (LSTM). We utilize the self-attention mechanism of the transformer to model the global correlations in signal sequences while employing LSTM to enhance the capture of temporal dependencies. To mitigate the impact like RF fingerprint features and channel characteri",
    "path": "papers/24/01/2401.01056.json",
    "total_tokens": 860,
    "translated_title": "通过鲁棒的全局特征提取增强自动调制识别",
    "translated_abstract": "自动调制识别在无线通信系统中起着至关重要的作用。近年来，深度学习自动调制识别策略取得了巨大的成功。调制信号展示了长期的时域依赖性，提取全局特征对于识别调制方案至关重要。传统上，人工专家分析星座图中的模式来分类调制方案。由于有限的感受野，传统的卷积网络擅长提取局部特征，但难以捕捉全局关系。为了解决这个问题，我们引入了一种名为TLDNN的新型混合深度框架，它将Transformer和长短期记忆（LSTM）的结构结合在一起。我们利用Transformer的自注意机制模拟信号序列中的全局关联，同时利用LSTM增强时域依赖性的捕捉。为了减轻射频指纹特征和信道特性等因素的影响。",
    "tldr": "该论文提出了一种名为TLDNN的混合深度框架，将Transformer和LSTM的结构结合，通过全局特征提取和捕捉时域依赖性的增强，为自动调制识别带来了改进。",
    "en_tdlr": "The paper introduces a hybrid deep framework called TLDNN, which combines the architectures of Transformer and LSTM to enhance automatic modulation recognition through robust global feature extraction and capturing temporal dependencies."
}