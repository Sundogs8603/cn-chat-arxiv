{
    "title": "Orion-14B: Open-source Multilingual Large Language Models. (arXiv:2401.12246v1 [cs.CL])",
    "abstract": "In this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion-14B model family and its associated code publicly accessible https://github.com/OrionStarAI/Orion, aiming to inspire future research and practical applications in the field.",
    "link": "http://arxiv.org/abs/2401.12246",
    "context": "Title: Orion-14B: Open-source Multilingual Large Language Models. (arXiv:2401.12246v1 [cs.CL])\nAbstract: In this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion-14B model family and its associated code publicly accessible https://github.com/OrionStarAI/Orion, aiming to inspire future research and practical applications in the field.",
    "path": "papers/24/01/2401.12246.json",
    "total_tokens": 928,
    "translated_title": "Orion-14B: 开源多语言大型语言模型",
    "translated_abstract": "在这项研究中，我们介绍了Orion-14B，一个具有140亿参数的多语言大型语言模型集合。我们采用数据调度方法，在包括英语、中文、日语、韩语和其他语言的文本中，对一个基础模型进行了训练，使用了来自2.5万亿个标记的多样化语料库。此外，我们还针对对话应用和其他特定用例进行了一系列模型的微调。我们的评估结果表明，Orion-14B在广泛的任务领域中实现了领先的性能。我们将Orion-14B模型系列及其相关代码公开可访问，旨在激发未来在该领域的研究和实际应用。",
    "tldr": "Orion-14B是一个具有140亿参数的开源多语言大型语言模型。在该研究中，我们采用数据调度方法对一个基础模型进行训练，使用了来自多种语言的2.5万亿个标记的多样化语料库。我们还对对话应用和其他特定用例进行了微调。评估结果显示，Orion-14B在广泛的任务中取得了领先的性能。我们将Orion-14B模型系列及其相关代码公开，以鼓励未来在这一领域的研究和实际应用。",
    "en_tdlr": "Orion-14B is an open-source multilingual large language model with 14 billion parameters. In this study, we trained a foundational model using a data scheduling approach on a diverse corpus of 2.5 trillion tokens from multiple languages. We also fine-tuned models for conversational applications and other specific use cases. Evaluation results show that Orion-14B achieves state-of-the-art performance across a broad range of tasks. We make the Orion-14B model family and its associated code publicly accessible to inspire future research and practical applications in the field."
}