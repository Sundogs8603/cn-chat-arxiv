{
    "title": "The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts. (arXiv:2401.13136v1 [cs.CL])",
    "abstract": "As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages, we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, traini",
    "link": "http://arxiv.org/abs/2401.13136",
    "context": "Title: The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts. (arXiv:2401.13136v1 [cs.CL])\nAbstract: As the influence of large language models (LLMs) spans across global communities, their safety challenges in multilingual settings become paramount for alignment research. This paper examines the variations in safety challenges faced by LLMs across different languages and discusses approaches to alleviating such concerns. By comparing how state-of-the-art LLMs respond to the same set of malicious prompts written in higher- vs. lower-resource languages, we observe that (1) LLMs tend to generate unsafe responses much more often when a malicious prompt is written in a lower-resource language, and (2) LLMs tend to generate more irrelevant responses to malicious prompts in lower-resource languages. To understand where the discrepancy can be attributed, we study the effect of instruction tuning with reinforcement learning from human feedback (RLHF) or supervised finetuning (SFT) on the HH-RLHF dataset. Surprisingly, while training with high-resource languages improves model alignment, traini",
    "path": "papers/24/01/2401.13136.json",
    "total_tokens": 1034,
    "translated_title": "语言障碍：剖析LLMs在多语言环境中的安全挑战",
    "translated_abstract": "随着大型语言模型（LLMs）对全球社区的影响不断扩大，它们在多语言环境中的安全挑战对齐研究变得至关重要。本文研究了LLMs在不同语言中面临的安全挑战的变化，并讨论了缓解这些问题的方法。通过比较当前最先进的LLMs在高资源语言和低资源语言中对同一组恶意提示的响应情况，我们观察到（1）当恶意提示用低资源语言编写时，LLMs往往更容易生成不安全的响应，（2）LLMs在低资源语言下更容易生成与恶意提示无关的响应。为了理解造成这种差异的原因，我们研究了使用与人类反馈的强化学习（RLHF）或有监督微调（SFT）进行指导调节对HH-RLHF数据集的影响。令人惊讶的是，虽然使用高资源语言进行训练可以改善模型的对齐性，但通过训练仍会产生不一致的结果。",
    "tldr": "本文研究了在多语言环境中，LLMs面临的安全挑战以及缓解这些挑战的方法。在不同语言中，LLMs对恶意提示的响应存在差异，低资源语言下的响应更容易产生不安全、不相关的结果。对于这种差异的原因，本文还研究了使用强化学习和有监督微调等方法对数据进行调节的影响。"
}