{
    "title": "MLPs Compass: What is learned when MLPs are combined with PLMs?. (arXiv:2401.01667v1 [cs.CL])",
    "abstract": "While Transformer-based pre-trained language models and their variants exhibit strong semantic representation capabilities, the question of comprehending the information gain derived from the additional components of PLMs remains an open question in this field. Motivated by recent efforts that prove Multilayer-Perceptrons (MLPs) modules achieving robust structural capture capabilities, even outperforming Graph Neural Networks (GNNs), this paper aims to quantify whether simple MLPs can further enhance the already potent ability of PLMs to capture linguistic information. Specifically, we design a simple yet effective probing framework containing MLPs components based on BERT structure and conduct extensive experiments encompassing 10 probing tasks spanning three distinct linguistic levels. The experimental results demonstrate that MLPs can indeed enhance the comprehension of linguistic structure by PLMs. Our research provides interpretable and valuable insights into crafting variations o",
    "link": "http://arxiv.org/abs/2401.01667",
    "context": "Title: MLPs Compass: What is learned when MLPs are combined with PLMs?. (arXiv:2401.01667v1 [cs.CL])\nAbstract: While Transformer-based pre-trained language models and their variants exhibit strong semantic representation capabilities, the question of comprehending the information gain derived from the additional components of PLMs remains an open question in this field. Motivated by recent efforts that prove Multilayer-Perceptrons (MLPs) modules achieving robust structural capture capabilities, even outperforming Graph Neural Networks (GNNs), this paper aims to quantify whether simple MLPs can further enhance the already potent ability of PLMs to capture linguistic information. Specifically, we design a simple yet effective probing framework containing MLPs components based on BERT structure and conduct extensive experiments encompassing 10 probing tasks spanning three distinct linguistic levels. The experimental results demonstrate that MLPs can indeed enhance the comprehension of linguistic structure by PLMs. Our research provides interpretable and valuable insights into crafting variations o",
    "path": "papers/24/01/2401.01667.json",
    "total_tokens": 879,
    "translated_title": "MLPs Compass: MLP与PLM相结合时学到了什么？",
    "translated_abstract": "尽管基于Transformer的预训练语言模型及其变体具有很强的语义表示能力，但关于PLM的额外组件所带来的信息增益的理解仍然是一个未解开的问题。受到最近的工作的激励，其证明了多层感知器（MLP）模块实现了强大的结构捕捉能力，甚至 超过了图神经网络（GNN），本文旨在量化简单的MLP是否能进一步增强PLM捕捉语言信息的能力。具体而言，我们设计了一个简单但有效的探测框架，包含了基于BERT结构的MLP组件，并进行了广泛的实验，涵盖了三个不同语言层次的10个探测任务。实验结果表明，MLP确实可以通过PLM提高对语言结构的理解能力。我们的研究为设计各种变体提供了可解释且有价值的见解。",
    "tldr": "本文研究探究了Multilayer-Perceptrons (MLPs)模块是否能增强预训练语言模型（PLMs）对语言信息的捕捉能力。实验结果表明，MLPs确实能够提高PLMs对语言结构的理解能力。",
    "en_tdlr": "This paper investigates whether Multilayer-Perceptrons (MLPs) modules can enhance the ability of pre-trained language models (PLMs) to capture linguistic information. The experimental results demonstrate that MLPs can indeed improve the comprehension of linguistic structure by PLMs."
}