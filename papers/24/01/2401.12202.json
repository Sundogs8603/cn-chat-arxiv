{
    "title": "OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics",
    "abstract": "arXiv:2401.12202v2 Announce Type: replace-cross  Abstract: Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environme",
    "link": "https://arxiv.org/abs/2401.12202",
    "context": "Title: OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics\nAbstract: arXiv:2401.12202v2 Announce Type: replace-cross  Abstract: Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environme",
    "path": "papers/24/01/2401.12202.json",
    "total_tokens": 893,
    "translated_title": "OK-Robot: 整合开放知识模型在机器人领域的重要性",
    "translated_abstract": "近年来在视觉、语言和机器人领域取得了显著进展。我们现在拥有能够根据语言查询识别物体的视觉模型，能有效控制移动系统的导航系统，以及能够处理各种物体的抓取模型。尽管取得了这些进步，但机器人的通用应用仍然落后，尽管它们依赖于识别、导航和抓取等基本能力。在本文中，我们采用了系统优先的方法，开发了一个名为OK-Robot的新型基于开放知识的机器人框架。通过将用于对象检测的视觉-语言模型（VLMs）、用于移动的导航原语和用于物体操作的抓取原语结合在一起，OK-Robot为Pick-and-Drop操作提供了一个集成解决方案，而无需任何训练。为了评估其性能，我们在10个真实家庭环境中运行了OK-Robot。",
    "tldr": "该研究提出了一种新型基于开放知识的机器人框架OK-Robot，通过整合视觉-语言模型、导航原语和抓取原语，为Pick-and-Drop操作提供了一个集成解决方案，无需任何训练。",
    "en_tdlr": "This paper introduces a new Open Knowledge-based robotics framework called OK-Robot, which integrates Vision-Language Models, navigation primitives, and grasping primitives to provide an integrated solution for Pick-and-Drop operations without requiring any training."
}