{
    "title": "An Empirical Study of In-context Learning in LLMs for Machine Translation",
    "abstract": "arXiv:2401.12097v2 Announce Type: replace  Abstract: Recent interest has surged in employing Large Language Models (LLMs) for machine translation (MT) via in-context learning (ICL) (Vilar et al., 2023). Most prior studies primarily focus on optimizing translation quality, with limited attention to understanding the specific aspects of ICL that influence the said quality. To this end, we perform the first of its kind, exhaustive study of in-context learning for machine translation. We first establish that ICL is primarily example-driven and not instruction-driven. Following this, we conduct an extensive exploration of various aspects of the examples to understand their influence on downstream performance. Our analysis includes factors such as quality and quantity of demonstrations, spatial proximity, and source versus target originality. Further, we also investigate challenging scenarios involving indirectness and misalignment of examples to understand the limits of ICL. While we establ",
    "link": "https://arxiv.org/abs/2401.12097",
    "context": "Title: An Empirical Study of In-context Learning in LLMs for Machine Translation\nAbstract: arXiv:2401.12097v2 Announce Type: replace  Abstract: Recent interest has surged in employing Large Language Models (LLMs) for machine translation (MT) via in-context learning (ICL) (Vilar et al., 2023). Most prior studies primarily focus on optimizing translation quality, with limited attention to understanding the specific aspects of ICL that influence the said quality. To this end, we perform the first of its kind, exhaustive study of in-context learning for machine translation. We first establish that ICL is primarily example-driven and not instruction-driven. Following this, we conduct an extensive exploration of various aspects of the examples to understand their influence on downstream performance. Our analysis includes factors such as quality and quantity of demonstrations, spatial proximity, and source versus target originality. Further, we also investigate challenging scenarios involving indirectness and misalignment of examples to understand the limits of ICL. While we establ",
    "path": "papers/24/01/2401.12097.json",
    "total_tokens": 867,
    "translated_title": "机器翻译中LLMs的上下文学习的实证研究",
    "translated_abstract": "最近，使用大型语言模型（LLMs）进行机器翻译（MT）的上下文学习（ICL）引起了人们的浓厚兴趣。大多数先前的研究主要集中在优化翻译质量上，对影响所述质量的ICL的特定方面关注有限。为此，我们进行了首次全面研究上下文学习用于机器翻译。我们首先确定ICL主要是由示例驱动而不是指令驱动。随后，我们对示例的各个方面进行了广泛探索，以了解它们对下游性能的影响。我们的分析包括示范的质量和数量、空间接近性以及源语言与目标语言的原创性等因素。此外，我们还研究了涉及间接性和示例不匹配的具有挑战性的场景，以了解ICL的局限性。",
    "tldr": "该研究对机器翻译中LLMs的上下文学习进行了全面深入的研究，探讨了示例驱动的特点以及示例对下游性能的影响，同时也探讨了ICL的局限性。",
    "en_tdlr": "This study provides an exhaustive exploration of in-context learning in LLMs for machine translation, discussing the example-driven nature, the influence of examples on downstream performance, and the limitations of ICL."
}