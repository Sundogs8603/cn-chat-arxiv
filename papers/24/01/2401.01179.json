{
    "title": "Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training. (arXiv:2401.01179v1 [cs.CV])",
    "abstract": "Modern healthcare often utilises radiographic images alongside textual reports for diagnostics, encouraging the use of Vision-Language Self-Supervised Learning (VL-SSL) with large pre-trained models to learn versatile medical vision representations. However, most existing VL-SSL frameworks are trained end-to-end, which is computation-heavy and can lose vital prior information embedded in pre-trained encoders. To address both issues, we introduce the backbone-agnostic Adaptor framework, which preserves medical knowledge in pre-trained image and text encoders by keeping them frozen, and employs a lightweight Adaptor module for cross-modal learning. Experiments on medical image classification and segmentation tasks across three datasets reveal that our framework delivers competitive performance while cutting trainable parameters by over 90% compared to current pre-training approaches. Notably, when fine-tuned with just 1% of data, Adaptor outperforms several Transformer-based methods trai",
    "link": "http://arxiv.org/abs/2401.01179",
    "context": "Title: Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training. (arXiv:2401.01179v1 [cs.CV])\nAbstract: Modern healthcare often utilises radiographic images alongside textual reports for diagnostics, encouraging the use of Vision-Language Self-Supervised Learning (VL-SSL) with large pre-trained models to learn versatile medical vision representations. However, most existing VL-SSL frameworks are trained end-to-end, which is computation-heavy and can lose vital prior information embedded in pre-trained encoders. To address both issues, we introduce the backbone-agnostic Adaptor framework, which preserves medical knowledge in pre-trained image and text encoders by keeping them frozen, and employs a lightweight Adaptor module for cross-modal learning. Experiments on medical image classification and segmentation tasks across three datasets reveal that our framework delivers competitive performance while cutting trainable parameters by over 90% compared to current pre-training approaches. Notably, when fine-tuned with just 1% of data, Adaptor outperforms several Transformer-based methods trai",
    "path": "papers/24/01/2401.01179.json",
    "total_tokens": 958,
    "translated_title": "冻结主干：一种参数高效的抗干扰医学视觉语言预训练对比方法",
    "translated_abstract": "现代医疗常常在诊断中同时使用放射图像和文字报告，鼓励使用大型预训练模型进行视觉-语言自监督学习(VL-SSL)以学习多功能的医学视觉表征。然而，现有的大多数VL-SSL框架是端到端训练的，这是计算密集型的，并且可能丢失嵌入在预训练编码器中的重要先验信息。为了解决这两个问题，我们引入了无主干适配器框架，通过保持预训练图像和文本编码器的冻结状态，保留医学知识，并使用轻量级适配器模块进行跨模态学习。在三个数据集上进行的医学图像分类和分割任务的实验证明，与当前的预训练方法相比，我们的框架在保留信息的同时，可将可训练参数减少超过90%。值得注意的是，当只使用1%的数据进行微调时，适配器的性能优于几种基于Transformer的方法。",
    "tldr": "这项研究提出了一种冻结主干的适配器框架，可以实现参数高效的抗干扰医学视觉语言预训练。实验证明，该框架在保留信息的同时大大减少了可训练参数，并在医学图像分类和分割任务上取得了竞争性的性能。"
}