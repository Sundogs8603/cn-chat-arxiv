{
    "title": "Endowing Protein Language Models with Structural Knowledge. (arXiv:2401.14819v1 [q-bio.QM])",
    "abstract": "Understanding the relationships between protein sequence, structure and function is a long-standing biological challenge with manifold implications from drug design to our understanding of evolution. Recently, protein language models have emerged as the preferred method for this challenge, thanks to their ability to harness large sequence databases. Yet, their reliance on expansive sequence data and parameter sets limits their flexibility and practicality in real-world scenarios. Concurrently, the recent surge in computationally predicted protein structures unlocks new opportunities in protein representation learning. While promising, the computational burden carried by such complex data still hinders widely-adopted practical applications. To address these limitations, we introduce a novel framework that enhances protein language models by integrating protein structural data. Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretr",
    "link": "http://arxiv.org/abs/2401.14819",
    "context": "Title: Endowing Protein Language Models with Structural Knowledge. (arXiv:2401.14819v1 [q-bio.QM])\nAbstract: Understanding the relationships between protein sequence, structure and function is a long-standing biological challenge with manifold implications from drug design to our understanding of evolution. Recently, protein language models have emerged as the preferred method for this challenge, thanks to their ability to harness large sequence databases. Yet, their reliance on expansive sequence data and parameter sets limits their flexibility and practicality in real-world scenarios. Concurrently, the recent surge in computationally predicted protein structures unlocks new opportunities in protein representation learning. While promising, the computational burden carried by such complex data still hinders widely-adopted practical applications. To address these limitations, we introduce a novel framework that enhances protein language models by integrating protein structural data. Drawing from recent advances in graph transformers, our approach refines the self-attention mechanisms of pretr",
    "path": "papers/24/01/2401.14819.json",
    "total_tokens": 900,
    "translated_title": "赋予蛋白质语言模型结构知识",
    "translated_abstract": "理解蛋白质序列、结构和功能之间的关系是一个长期存在的生物学挑战，对于药物设计和我们对进化的理解具有多方面的影响。最近，蛋白质语言模型已经成为这一挑战的首选方法，因为它们能够利用大规模的序列数据库。然而，它们对于广泛应用于实际场景的灵活性和实用性的依赖于大规模的序列数据和参数集的限制。与此同时，近年来计算预测蛋白质结构的增长为蛋白质表示学习提供了新的机会。虽然有很大的潜力，但是这样复杂数据所带来的计算负担仍然阻碍了广泛应用于实际应用。为了解决这些限制，我们提出了一个新的框架，通过整合蛋白质结构数据来增强蛋白质语言模型。从图变换的最新进展中汲取灵感，我们的方法改进了预训练模型的自注意机制。",
    "tldr": "本研究提出了一种新的框架，通过整合蛋白质结构数据来增强蛋白质语言模型，以解决在实际应用场景中由于依赖大量序列数据和参数集而限制其灵活性和实用性的问题。"
}