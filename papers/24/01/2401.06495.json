{
    "title": "An investigation of structures responsible for gender bias in BERT and DistilBERT. (arXiv:2401.06495v1 [cs.CL])",
    "abstract": "In recent years, large Transformer-based Pre-trained Language Models (PLM) have changed the Natural Language Processing (NLP) landscape, by pushing the performance boundaries of the state-of-the-art on a wide variety of tasks. However, this performance gain goes along with an increase in complexity, and as a result, the size of such models (up to billions of parameters) represents a constraint for their deployment on embedded devices or short-inference time tasks. To cope with this situation, compressed models emerged (e.g. DistilBERT), democratizing their usage in a growing number of applications that impact our daily lives. A crucial issue is the fairness of the predictions made by both PLMs and their distilled counterparts. In this paper, we propose an empirical exploration of this problem by formalizing two questions: (1) Can we identify the neural mechanism(s) responsible for gender bias in BERT (and by extension DistilBERT)? (2) Does distillation tend to accentuate or mitigate ge",
    "link": "http://arxiv.org/abs/2401.06495",
    "context": "Title: An investigation of structures responsible for gender bias in BERT and DistilBERT. (arXiv:2401.06495v1 [cs.CL])\nAbstract: In recent years, large Transformer-based Pre-trained Language Models (PLM) have changed the Natural Language Processing (NLP) landscape, by pushing the performance boundaries of the state-of-the-art on a wide variety of tasks. However, this performance gain goes along with an increase in complexity, and as a result, the size of such models (up to billions of parameters) represents a constraint for their deployment on embedded devices or short-inference time tasks. To cope with this situation, compressed models emerged (e.g. DistilBERT), democratizing their usage in a growing number of applications that impact our daily lives. A crucial issue is the fairness of the predictions made by both PLMs and their distilled counterparts. In this paper, we propose an empirical exploration of this problem by formalizing two questions: (1) Can we identify the neural mechanism(s) responsible for gender bias in BERT (and by extension DistilBERT)? (2) Does distillation tend to accentuate or mitigate ge",
    "path": "papers/24/01/2401.06495.json",
    "total_tokens": 852,
    "translated_title": "BERT和DistilBERT中负责性别偏见的结构的研究",
    "translated_abstract": "近年来，基于Transformer的大规模预训练语言模型（PLM）通过推动最先进技术在各种任务上的性能边界，改变了自然语言处理（NLP）领域。然而，这种性能提升伴随着复杂性的增加，因此这些模型的大小（可高达数十亿参数）在嵌入式设备或短推理时间任务上的部署受到限制。为了应对这种情况，出现了压缩模型（如DistilBERT），使得它们在越来越多影响我们日常生活的应用中可以被广泛使用。一个关键问题是PLM和其精简版本的预测公正性。在本文中，我们通过明确两个问题来对这个问题进行实证研究：（1）我们能否确定BERT（以及DistilBERT）中负责性别偏见的神经机制？（2）蒸馏是否倾向于加重或减轻这种偏见？",
    "tldr": "本文研究了BERT和DistilBERT中负责性别偏见的结构，讨论了它们在预测中的公正性问题。",
    "en_tdlr": "This paper investigates the structures responsible for gender bias in BERT and DistilBERT, and discusses the fairness issue in their predictions."
}