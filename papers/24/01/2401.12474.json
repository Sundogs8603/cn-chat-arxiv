{
    "title": "Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment. (arXiv:2401.12474v1 [cs.CL])",
    "abstract": "Considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, in this study, we introduce Ditto, a self-alignment method for role-play. Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension. This method creates a role-play training set comprising 4,000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Subsequently, we fine-tune the LLM using this self-generated dataset to augment its role-playing capabilities. Upon evaluating our meticulously constructed and reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in var",
    "link": "http://arxiv.org/abs/2401.12474",
    "context": "Title: Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment. (arXiv:2401.12474v1 [cs.CL])\nAbstract: Considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, in this study, we introduce Ditto, a self-alignment method for role-play. Ditto capitalizes on character knowledge, encouraging an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension. This method creates a role-play training set comprising 4,000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Subsequently, we fine-tune the LLM using this self-generated dataset to augment its role-playing capabilities. Upon evaluating our meticulously constructed and reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in var",
    "path": "papers/24/01/2401.12474.json",
    "total_tokens": 990,
    "translated_title": "大型语言模型是所有字符的叠加：通过自我对齐实现任意角色扮演",
    "translated_abstract": "大量的工作已经投入到通过模拟专有对手来增强开源大型语言模型（LLMs）的角色扮演能力。然而，我们认为LLMs本质上具有角色扮演能力，因为它们在广泛的训练语料库中蕴含了对角色和潜在对话的广泛了解。因此，在这项研究中，我们引入了Ditto，一种用于角色扮演的自我对齐方法。Ditto利用角色知识，鼓励LLM按照指令模拟角色扮演对话，作为阅读理解的一种变体。该方法创建了一个包含4000个字符的角色扮演训练集，相对于目前可用数据集的角色数量增加了十倍。随后，我们使用这个自动生成的数据集对LLM进行微调，以增强其角色扮演能力。通过评估我们精心构建且可复制的角色扮演基准和MT-Bench的角色扮演子集，Ditto在各项指标上表现出色。",
    "tldr": "本文介绍了一种名为Ditto的角色扮演自我对齐方法，通过对角色知识的利用，使大型语言模型能够模拟角色扮演对话，从而增强其角色扮演能力。实验证明，Ditto在角色扮演基准和MT-Bench的评估中取得了出色的结果。"
}