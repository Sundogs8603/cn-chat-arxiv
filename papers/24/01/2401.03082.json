{
    "title": "UMIE: Unified Multimodal Information Extraction with Instruction Tuning. (arXiv:2401.03082v1 [cs.AI])",
    "abstract": "Multimodal information extraction (MIE) gains significant attention as the popularity of multimedia content increases. However, current MIE methods often resort to using task-specific model structures, which results in limited generalizability across tasks and underutilizes shared knowledge across MIE tasks. To address these issues, we propose UMIE, a unified multimodal information extractor to unify three MIE tasks as a generation problem using instruction tuning, being able to effectively extract both textual and visual mentions. Extensive experiments show that our single UMIE outperforms various state-of-the-art (SoTA) methods across six MIE datasets on three tasks. Furthermore, in-depth analysis demonstrates UMIE's strong generalization in the zero-shot setting, robustness to instruction variants, and interpretability. Our research serves as an initial step towards a unified MIE model and initiates the exploration into both instruction tuning and large language models within the MI",
    "link": "http://arxiv.org/abs/2401.03082",
    "context": "Title: UMIE: Unified Multimodal Information Extraction with Instruction Tuning. (arXiv:2401.03082v1 [cs.AI])\nAbstract: Multimodal information extraction (MIE) gains significant attention as the popularity of multimedia content increases. However, current MIE methods often resort to using task-specific model structures, which results in limited generalizability across tasks and underutilizes shared knowledge across MIE tasks. To address these issues, we propose UMIE, a unified multimodal information extractor to unify three MIE tasks as a generation problem using instruction tuning, being able to effectively extract both textual and visual mentions. Extensive experiments show that our single UMIE outperforms various state-of-the-art (SoTA) methods across six MIE datasets on three tasks. Furthermore, in-depth analysis demonstrates UMIE's strong generalization in the zero-shot setting, robustness to instruction variants, and interpretability. Our research serves as an initial step towards a unified MIE model and initiates the exploration into both instruction tuning and large language models within the MI",
    "path": "papers/24/01/2401.03082.json",
    "total_tokens": 845,
    "translated_title": "UMIE：带有指导调节的统一多模态信息提取",
    "translated_abstract": "随着多媒体内容的普及，多模态信息提取（MIE）越来越受关注。然而，当前的MIE方法通常采用特定任务的模型结构，导致跨任务的泛化能力有限，并且未充分利用共享的MIE任务知识。为了解决这些问题，我们提出了UMIE，一种统一的多模态信息提取器，将三个MIE任务统一为一个生成问题，使用指导调节能够有效提取文本和视觉提及。大量实验证明，我们的单一UMIE在三个任务的六个MIE数据集上优于各种最先进（SoTA）方法。此外，深入分析证明UMIE在零-shot设置下具有很强的泛化能力，对指导变体具有鲁棒性，并能提供可解释性。我们的研究是朝着统一的MIE模型迈出的第一步，同时也开始探索指导调节和大型语言模型在MI领域的应用。",
    "tldr": "UMIE是一种统一的多模态信息提取器，使用指导调节的方法能够跨任务进行信息提取，并且具有强大的泛化能力和可解释性。",
    "en_tdlr": "UMIE is a unified multimodal information extractor that utilizes instruction tuning to perform information extraction across tasks, demonstrating strong generalization and interpretability."
}