{
    "title": "Scaling Sparse Fine-Tuning to Large Language Models",
    "abstract": "Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse fine-tuning method which, for a desired density level, maintains an array of parameter indices and the deltas of these parameters relative to their pretrained values. It iterates over: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LL",
    "link": "https://rss.arxiv.org/abs/2401.16405",
    "context": "Title: Scaling Sparse Fine-Tuning to Large Language Models\nAbstract: Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse fine-tuning method which, for a desired density level, maintains an array of parameter indices and the deltas of these parameters relative to their pretrained values. It iterates over: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LL",
    "path": "papers/24/01/2401.16405.json",
    "total_tokens": 905,
    "translated_title": "将稀疏微调扩展到大型语言模型",
    "translated_abstract": "大型语言模型（LLM）由于其参数的庞大数量，很难完全进行微调（例如使用指令或人工反馈）。一系列参数高效的稀疏微调方法在性能方面表现出了很大的潜力，但它们的存储需求与LLM的大小成正比增加。在这项工作中，我们将稀疏微调扩展到最先进的LLM，如LLaMA 2 7B和13B。我们提出了SpIEL，一种新颖的稀疏微调方法，它针对所需的稀疏度水平，维护一个参数索引数组和这些参数相对于预训练值的增量。它遍历以下步骤：（a）更新活跃增量，（b）修剪索引（基于其增量的变化大小），以及（c）重新生长索引。对于重新生长，我们探索了基于少量候选参数的累积梯度或使用高效的SM3优化器估计的近似动差的两个准则。我们尝试使用指令微调LLM。",
    "tldr": "本研究将稀疏微调方法扩展到大型语言模型，提出了一种新的稀疏微调算法SpIEL，并对LLM进行了指令微调，以解决其参数庞大的问题。",
    "en_tdlr": "This research scales sparse fine-tuning methods to large language models, proposes a novel method called SpIEL, and explores instruction-tuning of LLMs to address the issue of their sheer number of parameters."
}