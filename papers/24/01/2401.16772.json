{
    "title": "Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator. (arXiv:2401.16772v1 [cs.LG])",
    "abstract": "Imitation learning is often used in addition to reinforcement learning in environments where reward design is difficult or where the reward is sparse, but it is difficult to be able to imitate well in unknown states from a small amount of expert data and sampling data. Supervised learning methods such as Behavioral Cloning do not require sampling data, but usually suffer from distribution shift. The methods based on reinforcement learning, such as inverse reinforcement learning and Generative Adversarial imitation learning (GAIL), can learn from only a few expert data. However, they often need to interact with the environment. Soft Q imitation learning (SQIL) addressed the problems, and it was shown that it could learn efficiently by combining Behavioral Cloning and soft Q-learning with constant rewards. In order to make this algorithm more robust to distribution shift, we propose more efficient and robust algorithm by adding to this method a reward function based on adversarial invers",
    "link": "http://arxiv.org/abs/2401.16772",
    "context": "Title: Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator. (arXiv:2401.16772v1 [cs.LG])\nAbstract: Imitation learning is often used in addition to reinforcement learning in environments where reward design is difficult or where the reward is sparse, but it is difficult to be able to imitate well in unknown states from a small amount of expert data and sampling data. Supervised learning methods such as Behavioral Cloning do not require sampling data, but usually suffer from distribution shift. The methods based on reinforcement learning, such as inverse reinforcement learning and Generative Adversarial imitation learning (GAIL), can learn from only a few expert data. However, they often need to interact with the environment. Soft Q imitation learning (SQIL) addressed the problems, and it was shown that it could learn efficiently by combining Behavioral Cloning and soft Q-learning with constant rewards. In order to make this algorithm more robust to distribution shift, we propose more efficient and robust algorithm by adding to this method a reward function based on adversarial invers",
    "path": "papers/24/01/2401.16772.json",
    "total_tokens": 914,
    "translated_title": "基于外部奖励的鉴别器的软Q模仿学习",
    "translated_abstract": "在难以设计奖励或奖励稀疏的环境中，模仿学习常常与强化学习结合使用，但在少量专家数据和采样数据中很难在未知状态中良好地进行模仿。行为克隆等监督学习方法不需要采样数据，但通常会受到分布偏移的困扰。基于强化学习的方法，如逆向强化学习和生成对抗模仿学习（GAIL），可以从少量专家数据中进行学习，但通常需要与环境进行交互。软Q模仿学习（SQIL）解决了这些问题，并通过将行为克隆和常数奖励的软Q学习相结合，表明能够高效学习。为了使该算法对分布偏移更加稳健，我们提出了一种更高效和更稳健的算法，通过在该方法中添加基于对抗的奖励函数。",
    "tldr": "本论文提出了一种基于外部奖励的鉴别器的软Q模仿学习方法，旨在解决在少量专家数据和采样数据中进行模仿学习时遇到的困难，同时通过添加基于对抗的奖励函数，使算法更加稳健和高效。",
    "en_tdlr": "This paper proposes an extrinsically rewarded soft Q imitation learning method with a discriminator, aiming to address the challenges of imitating well in unknown states from a small amount of expert data and sampling data. By adding an adversarial reward function, the algorithm becomes more robust and efficient."
}