{
    "title": "LEGOBench: Leaderboard Generation Benchmark for Scientific Models. (arXiv:2401.06233v1 [cs.CL])",
    "abstract": "The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce LEGOBench, a benchmark for evaluating systems that generate leaderboards. LEGOBench is curated from 22 years of preprint submission data in arXiv and more than 11,000 machine learning leaderboards in the PapersWithCode portal. We evaluate the performance of four traditional graph-based ranking variants and three recently proposed large language models. Our preliminary results show significant performance gaps in automatic leaderboard generation. The code is available on https://github.com/lingo-iitgn/LEGOBench and the dataset is hosted on https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c .",
    "link": "http://arxiv.org/abs/2401.06233",
    "context": "Title: LEGOBench: Leaderboard Generation Benchmark for Scientific Models. (arXiv:2401.06233v1 [cs.CL])\nAbstract: The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce LEGOBench, a benchmark for evaluating systems that generate leaderboards. LEGOBench is curated from 22 years of preprint submission data in arXiv and more than 11,000 machine learning leaderboards in the PapersWithCode portal. We evaluate the performance of four traditional graph-based ranking variants and three recently proposed large language models. Our preliminary results show significant performance gaps in automatic leaderboard generation. The code is available on https://github.com/lingo-iitgn/LEGOBench and the dataset is hosted on https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c .",
    "path": "papers/24/01/2401.06233.json",
    "total_tokens": 836,
    "translated_title": "LEGOBench：科学模型排行榜生成基准测试",
    "translated_abstract": "随着论文提交数量的不断增加，难以及时了解最新的最先进研究成果成为了一个难题。为了解决这个挑战，我们引入了LEGOBench，这是一个评估生成排行榜系统的基准测试。LEGOBench由22年来在arXiv上提交的预印本数据和PapersWithCode门户上的11,000多个机器学习排行榜组成。我们评估了四种传统的基于图的排名变体和三种最近提出的大型语言模型的性能。我们的初步结果显示自动排行榜生成存在显著的性能差距。代码可在https://github.com/lingo-iitgn/LEGOBench获取，数据集托管在https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c。",
    "tldr": "LEGOBench是一个评估生成科学模型排行榜系统的基准测试，使用22年来的论文预印本数据和PapersWithCode门户上的机器学习排行榜的数据，初步结果显示自动排行榜生成存在显著性能差距。"
}