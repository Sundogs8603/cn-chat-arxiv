{
    "title": "Neural Sinkhorn Gradient Flow. (arXiv:2401.14069v1 [cs.LG])",
    "abstract": "Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. Recently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution. We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. To further enhance model efficiency on high-dimensional tasks, a two-phase NS",
    "link": "http://arxiv.org/abs/2401.14069",
    "context": "Title: Neural Sinkhorn Gradient Flow. (arXiv:2401.14069v1 [cs.LG])\nAbstract: Wasserstein Gradient Flows (WGF) with respect to specific functionals have been widely used in the machine learning literature. Recently, neural networks have been adopted to approximate certain intractable parts of the underlying Wasserstein gradient flow and result in efficient inference procedures. In this paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which parametrizes the time-varying velocity field of the Wasserstein gradient flow w.r.t. the Sinkhorn divergence to the target distribution starting a given source distribution. We utilize the velocity field matching training scheme in NSGF, which only requires samples from the source and target distribution to compute an empirical velocity field approximation. Our theoretical analyses show that as the sample size increases to infinity, the mean-field limit of the empirical approximation converges to the true underlying velocity field. To further enhance model efficiency on high-dimensional tasks, a two-phase NS",
    "path": "papers/24/01/2401.14069.json",
    "total_tokens": 902,
    "translated_title": "神经Sinkhorn梯度流",
    "translated_abstract": "针对特定泛函的Wasserstein梯度流在机器学习领域得到了广泛应用。最近，人们采用神经网络来近似处理Wasserstein梯度流中的一些难以计算的部分，并得到了高效的推理过程。本文介绍了神经Sinkhorn梯度流（NSGF）模型，该模型将Wasserstein梯度流的时间变化速度场参数化为Sinkhorn距离到目标分布与给定源分布的关系。我们在NSGF中利用速度场匹配训练方案，该方案只需要来自源分布和目标分布的样本来计算经验速度场近似值。我们的理论分析表明，随着样本大小趋近于无穷大，经验近似值的均场极限会收敛到真实的底层速度场。为了进一步提高模型在高维任务上的效率，引入了一个两阶段的NSGF",
    "tldr": "神经Sinkhorn梯度流模型通过参数化Wasserstein梯度流中的速度场，并利用速度场匹配训练方案实现高效的推理过程。其理论分析表明，随着样本大小趋近于无穷大，经验近似值的均场极限会收敛到真实的底层速度场。",
    "en_tdlr": "The Neural Sinkhorn Gradient Flow (NSGF) model parametrizes the velocity field of the Wasserstein gradient flow using the Sinkhorn divergence and achieves efficient inference through velocity field matching. The theoretical analysis shows that as the sample size increases, the empirical approximation converges to the true underlying velocity field."
}