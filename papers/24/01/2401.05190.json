{
    "title": "Divide and Conquer for Large Language Models Reasoning. (arXiv:2401.05190v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs). However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones. To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning. First, we divide questions into different subsets based on the statistical confidence score ($\\mathcal{CS}$), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants. Our experiments demonstrate that this proposed",
    "link": "http://arxiv.org/abs/2401.05190",
    "context": "Title: Divide and Conquer for Large Language Models Reasoning. (arXiv:2401.05190v1 [cs.CL])\nAbstract: Large language models (LLMs) have shown impressive performance in various reasoning benchmarks with the emergence of Chain-of-Thought (CoT) and its derivative methods, particularly in tasks involving multi-choice questions (MCQs). However, current works all process data uniformly without considering the problem-solving difficulty, which means an excessive focus on simple questions while insufficient to intricate ones. To address this challenge, we inspired by humans using heuristic strategies to categorize tasks and handle them individually, propose to apply the Divide and Conquer to LLMs reasoning. First, we divide questions into different subsets based on the statistical confidence score ($\\mathcal{CS}$), then fix nearly resolved sets and conquer demanding nuanced process ones with elaborately designed methods, including Prior Knowledge based Reasoning (PKR) and Filter Choices based Reasoning (FCR), as well as their integration variants. Our experiments demonstrate that this proposed",
    "path": "papers/24/01/2401.05190.json",
    "total_tokens": 931,
    "translated_title": "大型语言模型的分治求解方法在推理中的应用",
    "translated_abstract": "随着Chain-of-Thought（CoT）及其衍生方法的出现，大型语言模型（LLMs）在各种推理基准测试中表现出令人印象深刻的性能，特别是在涉及多项选择题（MCQs）的任务中。然而，当前的工作都是统一处理数据，没有考虑到问题解决的难度，这意味着过分关注简单问题，而对复杂问题不够重视。为了应对这一挑战，我们受到人类使用启发式策略对任务进行分类并单独处理的启发，提议将分治方法应用于LLMs推理中。首先，我们根据统计置信度分数（$\\mathcal{CS}$）将问题划分为不同的子集，然后固定解决的子集，用精心设计的方法解决复杂的纷繁问题，包括基于先验知识的推理（PKR）和基于筛选选项的推理（FCR），以及它们的集成变体。我们的实验表明，这种提出的分治求解方法可以显著提高推理性能，并在不同难度的问题上取得了优异结果。",
    "tldr": "分治求解方法应用于大型语言模型的推理中，通过根据统计置信度分数将问题划分为不同的子集，并采用基于先验知识和筛选选项的推理方法，提高了推理性能，取得了优异结果。",
    "en_tdlr": "Divide and conquer approach is applied to reasoning in large language models, where questions are divided into subsets based on statistical confidence score and tackled using reasoning methods based on prior knowledge and filtered choices, resulting in improved performance and excellent results across different difficulty levels."
}