{
    "title": "ELRT: Efficient Low-Rank Training for Compact Convolutional Neural Networks. (arXiv:2401.10341v1 [cs.CV])",
    "abstract": "Low-rank compression, a popular model compression technique that produces compact convolutional neural networks (CNNs) with low rankness, has been well-studied in the literature. On the other hand, low-rank training, as an alternative way to train low-rank CNNs from scratch, has been exploited little yet. Unlike low-rank compression, low-rank training does not need pre-trained full-rank models, and the entire training phase is always performed on the low-rank structure, bringing attractive benefits for practical applications. However, the existing low-rank training solutions still face several challenges, such as a considerable accuracy drop and/or still needing to update full-size models during the training. In this paper, we perform a systematic investigation on low-rank CNN training. By identifying the proper low-rank format and performance-improving strategy, we propose ELRT, an efficient low-rank training solution for high-accuracy, high-compactness, low-rank CNN models. Our exten",
    "link": "http://arxiv.org/abs/2401.10341",
    "context": "Title: ELRT: Efficient Low-Rank Training for Compact Convolutional Neural Networks. (arXiv:2401.10341v1 [cs.CV])\nAbstract: Low-rank compression, a popular model compression technique that produces compact convolutional neural networks (CNNs) with low rankness, has been well-studied in the literature. On the other hand, low-rank training, as an alternative way to train low-rank CNNs from scratch, has been exploited little yet. Unlike low-rank compression, low-rank training does not need pre-trained full-rank models, and the entire training phase is always performed on the low-rank structure, bringing attractive benefits for practical applications. However, the existing low-rank training solutions still face several challenges, such as a considerable accuracy drop and/or still needing to update full-size models during the training. In this paper, we perform a systematic investigation on low-rank CNN training. By identifying the proper low-rank format and performance-improving strategy, we propose ELRT, an efficient low-rank training solution for high-accuracy, high-compactness, low-rank CNN models. Our exten",
    "path": "papers/24/01/2401.10341.json",
    "total_tokens": 990,
    "translated_title": "ELRT：高效低秩训练紧凑卷积神经网络",
    "translated_abstract": "低秩压缩是一种流行的模型压缩技术，在文献中已经得到了广泛研究。另一方面，低秩训练作为一种从头开始训练低秩卷积神经网络的替代方法，尚未得到充分利用。不同于低秩压缩，低秩训练不需要预训练的全秩模型，整个训练阶段始终在低秩结构上进行，为实际应用带来了吸引人的好处。然而，现有的低秩训练解决方案仍面临一些挑战，如显著的准确性下降和/或在训练过程中仍需更新全尺寸模型。在本文中，我们对低秩卷积神经网络训练进行了系统研究。通过确定适当的低秩格式和性能改进策略，我们提出了ELRT，一种用于高准确性、高紧凑性、低秩卷积神经网络模型的高效低秩训练解决方案。",
    "tldr": "本文提出了ELRT，一种高效低秩训练解决方案，可用于高准确性、高紧凑性的低秩卷积神经网络模型。与现有解决方案相比，ELRT具有较少的准确性下降和不需要更新全尺寸模型的优势。"
}