{
    "title": "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding. (arXiv:2401.01970v1 [cs.CV])",
    "abstract": "Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present \\algfull{} (\\algname{}), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, faci",
    "link": "http://arxiv.org/abs/2401.01970",
    "context": "Title: FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding. (arXiv:2401.01970v1 [cs.CV])\nAbstract: Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present \\algfull{} (\\algname{}), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, faci",
    "path": "papers/24/01/2401.01970.json",
    "total_tokens": 877,
    "translated_title": "FMGS：基于嵌入式3D高斯分割的全面3D场景理解",
    "translated_abstract": "准确地感知现实世界3D物体的几何和语义特性对于增强现实和机器人应用的持续进化至关重要。为此，我们提出了FMGS（Foundation Model Embedded 3D Gaussian Splatting），将视觉语言嵌入基础模型到3D高斯分割中。本工作的主要贡献是一种高效的方法，用于重建和表示3D视觉语言模型。这是通过将基于图像的基础模型生成的特征图融合到我们的3D模型中渲染实现的。为了确保高质量的渲染和快速训练，我们引入了一种新颖的场景表示方法，将GS和多分辨率哈希编码（MHE）的优势结合起来。我们的有效训练过程还引入了像素对齐损失，使相同语义实体的渲染特征距离接近，遵循像素级语义边界。我们的结果展示了显著的多视图语义一致性。",
    "tldr": "提出了一种新颖的方法，将视觉语言嵌入基础模型到3D高斯分割中，实现了高质量的3D场景理解。",
    "en_tdlr": "A novel method, FMGS, is proposed to incorporate vision-language embeddings of foundation models into 3D Gaussian Splatting, achieving high-quality 3D scene understanding."
}