{
    "title": "Stochastic Subnetwork Annealing: A Regularization Technique for Fine Tuning Pruned Subnetworks. (arXiv:2401.08830v1 [cs.LG])",
    "abstract": "Pruning methods have recently grown in popularity as an effective way to reduce the size and computational complexity of deep neural networks. Large numbers of parameters can be removed from trained models with little discernible loss in accuracy after a small number of continued training epochs. However, pruning too many parameters at once often causes an initial steep drop in accuracy which can undermine convergence quality. Iterative pruning approaches mitigate this by gradually removing a small number of parameters over multiple epochs. However, this can still lead to subnetworks that overfit local regions of the loss landscape. We introduce a novel and effective approach to tuning subnetworks through a regularization technique we call Stochastic Subnetwork Annealing. Instead of removing parameters in a discrete manner, we instead represent subnetworks with stochastic masks where each parameter has a probabilistic chance of being included or excluded on any given forward pass. We a",
    "link": "http://arxiv.org/abs/2401.08830",
    "context": "Title: Stochastic Subnetwork Annealing: A Regularization Technique for Fine Tuning Pruned Subnetworks. (arXiv:2401.08830v1 [cs.LG])\nAbstract: Pruning methods have recently grown in popularity as an effective way to reduce the size and computational complexity of deep neural networks. Large numbers of parameters can be removed from trained models with little discernible loss in accuracy after a small number of continued training epochs. However, pruning too many parameters at once often causes an initial steep drop in accuracy which can undermine convergence quality. Iterative pruning approaches mitigate this by gradually removing a small number of parameters over multiple epochs. However, this can still lead to subnetworks that overfit local regions of the loss landscape. We introduce a novel and effective approach to tuning subnetworks through a regularization technique we call Stochastic Subnetwork Annealing. Instead of removing parameters in a discrete manner, we instead represent subnetworks with stochastic masks where each parameter has a probabilistic chance of being included or excluded on any given forward pass. We a",
    "path": "papers/24/01/2401.08830.json",
    "total_tokens": 880,
    "translated_title": "随机子网络退火：一种用于微调修剪子网络的正则化技术",
    "translated_abstract": "最近，修剪方法因其降低深度神经网络的大小和计算复杂度的有效性而变得流行起来。经过少数训练轮次后，大量参数可以从训练模型中移除，而准确度几乎没有明显损失。然而，一次性修剪太多参数通常会导致准确度的陡然下降，这可能破坏收敛质量。迭代修剪方法通过在多个轮次中逐渐移除少量参数来缓解这个问题。然而，这仍可能导致子网络过拟合损失函数的局部区域。我们引入了一种新颖而有效的方法，通过一种名为随机子网络退火的正则化技术来调整子网络。我们使用随机掩码来表示子网络，其中每个参数在任何前向传播中都有被包含或排除的概率。",
    "tldr": "随机子网络退火是一种用于微调修剪子网络的正则化技术，通过在每次前向传播时使用随机掩码表示参数的包含或排除概率，以避免修剪过多参数导致的准确度下降和子网络过拟合的问题。",
    "en_tdlr": "Stochastic Subnetwork Annealing is a regularization technique for fine-tuning pruned subnetworks, which avoids the issue of accuracy drop and subnetwork overfitting caused by pruning too many parameters by representing parameter inclusion or exclusion probabilities using stochastic masks during each forward pass."
}