{
    "title": "One Step Learning, One Step Review. (arXiv:2401.10962v1 [cs.CV])",
    "abstract": "Visual fine-tuning has garnered significant attention with the rise of pre-trained vision models. The current prevailing method, full fine-tuning, suffers from the issue of knowledge forgetting as it focuses solely on fitting the downstream training set. In this paper, we propose a novel weight rollback-based fine-tuning method called OLOR (One step Learning, One step Review). OLOR combines fine-tuning with optimizers, incorporating a weight rollback term into the weight update term at each step. This ensures consistency in the weight range of upstream and downstream models, effectively mitigating knowledge forgetting and enhancing fine-tuning performance. In addition, a layer-wise penalty is presented to employ penalty decay and the diversified decay rate to adjust the weight rollback levels of layers for adapting varying downstream tasks. Through extensive experiments on various tasks such as image classification, object detection, semantic segmentation, and instance segmentation, we",
    "link": "http://arxiv.org/abs/2401.10962",
    "context": "Title: One Step Learning, One Step Review. (arXiv:2401.10962v1 [cs.CV])\nAbstract: Visual fine-tuning has garnered significant attention with the rise of pre-trained vision models. The current prevailing method, full fine-tuning, suffers from the issue of knowledge forgetting as it focuses solely on fitting the downstream training set. In this paper, we propose a novel weight rollback-based fine-tuning method called OLOR (One step Learning, One step Review). OLOR combines fine-tuning with optimizers, incorporating a weight rollback term into the weight update term at each step. This ensures consistency in the weight range of upstream and downstream models, effectively mitigating knowledge forgetting and enhancing fine-tuning performance. In addition, a layer-wise penalty is presented to employ penalty decay and the diversified decay rate to adjust the weight rollback levels of layers for adapting varying downstream tasks. Through extensive experiments on various tasks such as image classification, object detection, semantic segmentation, and instance segmentation, we",
    "path": "papers/24/01/2401.10962.json",
    "total_tokens": 864,
    "translated_title": "一步学习，一步评审",
    "translated_abstract": "随着预训练视觉模型的兴起，视觉微调已经引起了广泛关注。当前主流的方法——完全微调，存在知识遗忘的问题，因为它只专注于拟合下游训练集。在本文中，我们提出了一种新颖的基于权重回滚的微调方法，称为OLOR（一步学习，一步评审）。OLOR将微调与优化器相结合，将权重回滚项加入到每个步骤的权重更新项中。这确保了上游和下游模型的权重范围的一致性，有效地减轻了知识遗忘问题，并增强了微调性能。此外，我们提出了一种逐层惩罚方法，通过 penalty decay 和不同的衰减率来调整层的权重回滚程度，以适应不同的下游任务。通过在图像分类、目标检测、语义分割和实例分割等各种任务上进行大量实验证明，我们的方法提高了微调的性能。",
    "tldr": "本文提出了一种基于权重回滚的微调方法OLOR，通过结合优化器和权重回滚项，解决了完全微调方法中的知识遗忘问题，并在各种任务上提高了微调性能。",
    "en_tdlr": "This paper proposes a weight rollback-based fine-tuning method called OLOR, which solves the issue of knowledge forgetting in full fine-tuning by combining optimizers and weight rollback terms, and improves fine-tuning performance on various tasks."
}