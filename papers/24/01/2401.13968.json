{
    "title": "Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks. (arXiv:2401.13968v1 [cs.LG])",
    "abstract": "A reliable long-term time-series forecaster is highly demanded in practice but comes across many challenges such as low computational and memory footprints as well as robustness against dynamic learning environments. This paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic long-term time-series forecasting tasks. MANTRA relies on the concept of fast and slow learners where a collection of fast learners learns different aspects of data distributions while adapting quickly to changes. A slow learner tailors suitable representations to fast learners. Fast adaptations to dynamic environments are achieved using the universal representation transformer layers producing task-adapted representations with a small number of parameters. Our experiments using four datasets with different prediction lengths demonstrate the advantage of our approach with at least $3\\%$ improvements over the baseline algorithms for both multivariate and univariate settings. Source codes of MANT",
    "link": "http://arxiv.org/abs/2401.13968",
    "context": "Title: Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks. (arXiv:2401.13968v1 [cs.LG])\nAbstract: A reliable long-term time-series forecaster is highly demanded in practice but comes across many challenges such as low computational and memory footprints as well as robustness against dynamic learning environments. This paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic long-term time-series forecasting tasks. MANTRA relies on the concept of fast and slow learners where a collection of fast learners learns different aspects of data distributions while adapting quickly to changes. A slow learner tailors suitable representations to fast learners. Fast adaptations to dynamic environments are achieved using the universal representation transformer layers producing task-adapted representations with a small number of parameters. Our experiments using four datasets with different prediction lengths demonstrate the advantage of our approach with at least $3\\%$ improvements over the baseline algorithms for both multivariate and univariate settings. Source codes of MANT",
    "path": "papers/24/01/2401.13968.json",
    "total_tokens": 890,
    "translated_title": "动态长期时间序列预测：基于元转换网络的研究",
    "translated_abstract": "在实践中，可靠的长期时间序列预测模型面临诸多挑战，如低计算和内存占用以及对动态学习环境的鲁棒性。本文提出了一种名为Meta-Transformer Networks（MANTRA）的模型，用于处理动态的长期时间序列预测任务。MANTRA依赖于快速和慢速学习器的概念，其中一组快速学习器学习数据分布的不同方面，同时快速适应变化。慢速学习器为快速学习器定制适当的表示。通过使用通用表示转换层产生任务适应性表示，并具有少量参数，实现对动态环境的快速适应。我们在四个具有不同预测长度的数据集上进行实验，结果表明我们的方法在多变量和单变量设置下至少比基准算法改进了3％。MANTRA的源代码可在链接中找到。",
    "tldr": "本文提出了一种名为Meta-Transformer Networks（MANTRA）的模型，用于动态长期时间序列预测。该模型通过快速和慢速学习器的结合，以及使用通用表示转换层，实现对动态环境的快速适应，并在实验中表现出优于基准算法的性能提升。",
    "en_tdlr": "This paper proposes a model called Meta-Transformer Networks (MANTRA) for dynamic long-term time-series forecasting. The model combines fast and slow learners and uses universal representation transformer layers to achieve fast adaptation to dynamic environments, resulting in improved performance compared to baseline algorithms."
}