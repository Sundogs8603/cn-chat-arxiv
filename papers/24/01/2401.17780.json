{
    "title": "A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees",
    "abstract": "We study a primal-dual reinforcement learning (RL) algorithm for the online constrained Markov decision processes (CMDP) problem, wherein the agent explores an optimal policy that maximizes return while satisfying constraints. Despite its widespread practical use, the existing theoretical literature on primal-dual RL algorithms for this problem only provides sublinear regret guarantees and fails to ensure convergence to optimal policies. In this paper, we introduce a novel policy gradient primal-dual algorithm with uniform probably approximate correctness (Uniform-PAC) guarantees, simultaneously ensuring convergence to optimal policies, sublinear regret, and polynomial sample complexity for any target accuracy. Notably, this represents the first Uniform-PAC algorithm for the online CMDP problem. In addition to the theoretical guarantees, we empirically demonstrate in a simple CMDP that our algorithm converges to optimal policies, while an existing algorithm exhibits oscillatory perform",
    "link": "https://arxiv.org/abs/2401.17780",
    "context": "Title: A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees\nAbstract: We study a primal-dual reinforcement learning (RL) algorithm for the online constrained Markov decision processes (CMDP) problem, wherein the agent explores an optimal policy that maximizes return while satisfying constraints. Despite its widespread practical use, the existing theoretical literature on primal-dual RL algorithms for this problem only provides sublinear regret guarantees and fails to ensure convergence to optimal policies. In this paper, we introduce a novel policy gradient primal-dual algorithm with uniform probably approximate correctness (Uniform-PAC) guarantees, simultaneously ensuring convergence to optimal policies, sublinear regret, and polynomial sample complexity for any target accuracy. Notably, this represents the first Uniform-PAC algorithm for the online CMDP problem. In addition to the theoretical guarantees, we empirically demonstrate in a simple CMDP that our algorithm converges to optimal policies, while an existing algorithm exhibits oscillatory perform",
    "path": "papers/24/01/2401.17780.json",
    "total_tokens": 944,
    "translated_title": "一种带有均匀PAC保证的约束MDPs的策略梯度原始对偶算法",
    "translated_abstract": "我们研究了一种基于原始对偶强化学习算法的在线约束马尔可夫决策过程（CMDP）问题，其中代理探索的最优策略在满足约束的同时最大化回报。尽管在实践中被广泛使用，但现有的理论文献仅提供次线性遗憾保证，并未能确保收敛到最优策略。在本文中，我们引入了一种新颖的带有均匀可能近似正确性（Uniform-PAC）保证的策略梯度原始对偶算法，同时确保收敛到最优策略、次线性遗憾和多项式样本复杂度以实现任何目标精度。值得注意的是，这是在线CMDP问题的第一个Uniform-PAC算法。除了理论保证外，我们还在一个简单的CMDP中进行了实证研究，证明了我们的算法收敛到最优策略，而现有算法表现出振荡性能表现。",
    "tldr": "本文介绍了一种带有均匀PAC保证的策略梯度原始对偶算法，用于在线约束马尔可夫决策过程（CMDP）问题。该算法同时保证了收敛到最优策略、次线性遗憾和多项式样本复杂度，并在实证研究中验证了其优越性能。",
    "en_tdlr": "This paper introduces a policy gradient primal-dual algorithm with uniform PAC guarantees for online constrained Markov decision processes (CMDPs). The algorithm ensures convergence to optimal policies, sublinear regret, and polynomial sample complexity, and is empirically validated to outperform existing algorithms in a simple CMDP scenario."
}