{
    "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
    "abstract": "Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved",
    "link": "https://arxiv.org/abs/2401.09417",
    "context": "Title: Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\nAbstract: Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved",
    "path": "papers/24/01/2401.09417.json",
    "total_tokens": 906,
    "translated_title": "Vision Mamba: 使用双向状态空间模型高效学习视觉表示",
    "translated_abstract": "最近，具有高效硬件感知设计的状态空间模型（SSMs），即Mamba深度学习模型，在长序列建模方面展现出巨大潜力。与此同时，在SSMs上构建高效且通用的视觉骨干模型也是一种有吸引力的方向。然而，由于视觉数据的位置敏感性和对全局上下文的需求，对于SSMs来说，表示视觉数据是具有挑战性的。在本文中，我们展示了对于视觉表示学习来说，依赖自注意力并不是必要的，并提出了一种新的通用视觉骨干模型，即带有双向Mamba块（Vim），它使用位置嵌入标记图像序列，并使用双向状态空间模型压缩视觉表示。在ImageNet分类、COCO目标检测和ADE20k语义分割任务中，Vim相比于DeiT等经过良好验证的视觉转换器，实现了更高的性能，并且显示出了显著的改进。",
    "tldr": "本文提出了一种新的通用视觉骨干模型Vim，通过使用双向状态空间模型和位置嵌入来高效表示视觉数据，相比于传统的视觉转换器如DeiT，在各种视觉任务上取得了更好的性能，并且实现了显著提升。",
    "en_tdlr": "This paper proposes a new generic vision backbone, called Vim, which efficiently represents visual data using bidirectional state space models and position embeddings. Compared to traditional vision transformers like DeiT, Vim achieves better performance in various visual tasks and demonstrates significant improvements."
}