{
    "title": "Towards Efficient Communication and Secure Federated Recommendation System via Low-rank Training",
    "abstract": "arXiv:2401.03748v2 Announce Type: replace  Abstract: Federated Recommendation (FedRec) systems have emerged as a solution to safeguard users' data in response to growing regulatory concerns. However, one of the major challenges in these systems lies in the communication costs that arise from the need to transmit neural network models between user devices and a central server. Prior approaches to these challenges often lead to issues such as computational overheads, model specificity constraints, and compatibility issues with secure aggregation protocols. In response, we propose a novel framework, called Correlated Low-rank Structure (CoLR), which leverages the concept of adjusting lightweight trainable parameters while keeping most parameters frozen. Our approach substantially reduces communication overheads without introducing additional computational burdens. Critically, our framework remains fully compatible with secure aggregation protocols, including the robust use of Homomorphic ",
    "link": "https://arxiv.org/abs/2401.03748",
    "context": "Title: Towards Efficient Communication and Secure Federated Recommendation System via Low-rank Training\nAbstract: arXiv:2401.03748v2 Announce Type: replace  Abstract: Federated Recommendation (FedRec) systems have emerged as a solution to safeguard users' data in response to growing regulatory concerns. However, one of the major challenges in these systems lies in the communication costs that arise from the need to transmit neural network models between user devices and a central server. Prior approaches to these challenges often lead to issues such as computational overheads, model specificity constraints, and compatibility issues with secure aggregation protocols. In response, we propose a novel framework, called Correlated Low-rank Structure (CoLR), which leverages the concept of adjusting lightweight trainable parameters while keeping most parameters frozen. Our approach substantially reduces communication overheads without introducing additional computational burdens. Critically, our framework remains fully compatible with secure aggregation protocols, including the robust use of Homomorphic ",
    "path": "papers/24/01/2401.03748.json",
    "total_tokens": 872,
    "translated_title": "通过低秩训练实现高效通信和安全的联邦推荐系统",
    "translated_abstract": "arXiv:2401.03748v2 公告类型: 替换 摘要: 针对不断增长的监管关注引发的需求，联邦推荐（FedRec）系统已经成为保护用户数据的一种解决方案。然而，这类系统面临的主要挑战之一在于传输神经网络模型所带来的通信成本，即在用户设备和中央服务器之间传输。先前针对这些挑战的方法通常会导致诸如计算开销、模型特性限制以及与安全聚合协议兼容性的问题。 作为响应，我们提出了一种新颖的框架，称为相关低秩结构（CoLR），它利用了调整轻量级可训练参数的概念，同时保持大部分参数冻结。我们的方法显著减少了通信开销而不引入额外的计算负担。至关重要的是，我们的框架与安全聚合协议完全兼容，包括鲁棒地使用同态",
    "tldr": "通过相关低秩结构（CoLR）框架，实现了联邦推荐系统中高效的通信和安全性，显著降低了通信开销同时保持与安全聚合协议的兼容性。",
    "en_tdlr": "Efficient communication and secure federated recommendation system are achieved through the Correlated Low-rank Structure (CoLR) framework, significantly reducing communication overheads while maintaining compatibility with secure aggregation protocols."
}