{
    "title": "FedGT: Federated Node Classification with Scalable Graph Transformer. (arXiv:2401.15203v1 [cs.LG])",
    "abstract": "Graphs are widely used to model relational data. As graphs are getting larger and larger in real-world scenarios, there is a trend to store and compute subgraphs in multiple local systems. For example, recently proposed \\emph{subgraph federated learning} methods train Graph Neural Networks (GNNs) distributively on local subgraphs and aggregate GNN parameters with a central server. However, existing methods have the following limitations: (1) The links between local subgraphs are missing in subgraph federated learning. This could severely damage the performance of GNNs that follow message-passing paradigms to update node/edge features. (2) Most existing methods overlook the subgraph heterogeneity issue, brought by subgraphs being from different parts of the whole graph. To address the aforementioned challenges, we propose a scalable \\textbf{Fed}erated \\textbf{G}raph \\textbf{T}ransformer (\\textbf{FedGT}) in the paper. Firstly, we design a hybrid attention scheme to reduce the complexity ",
    "link": "http://arxiv.org/abs/2401.15203",
    "context": "Title: FedGT: Federated Node Classification with Scalable Graph Transformer. (arXiv:2401.15203v1 [cs.LG])\nAbstract: Graphs are widely used to model relational data. As graphs are getting larger and larger in real-world scenarios, there is a trend to store and compute subgraphs in multiple local systems. For example, recently proposed \\emph{subgraph federated learning} methods train Graph Neural Networks (GNNs) distributively on local subgraphs and aggregate GNN parameters with a central server. However, existing methods have the following limitations: (1) The links between local subgraphs are missing in subgraph federated learning. This could severely damage the performance of GNNs that follow message-passing paradigms to update node/edge features. (2) Most existing methods overlook the subgraph heterogeneity issue, brought by subgraphs being from different parts of the whole graph. To address the aforementioned challenges, we propose a scalable \\textbf{Fed}erated \\textbf{G}raph \\textbf{T}ransformer (\\textbf{FedGT}) in the paper. Firstly, we design a hybrid attention scheme to reduce the complexity ",
    "path": "papers/24/01/2401.15203.json",
    "total_tokens": 867,
    "translated_title": "FedGT: 可扩展图形变压器的联邦节点分类",
    "translated_abstract": "图表被广泛用于建模关系数据。随着图表在现实场景中变得越来越大，存储和计算多个本地系统中的子图表成为一种趋势。最近提出的子图联邦学习方法例如通过在本地子图上进行图神经网络（GNN）的分布式训练，并通过集中服务器聚合GNN参数。然而，现有的方法存在以下限制：（1）子图联邦学习中缺少本地子图之间的链接。这可能严重影响遵循消息传递范式更新节点/边特征的GNN的性能。 （2）大多数现有方法忽视了子图异构性的问题，因为子图来自整个图的不同部分。为解决上述挑战，我们在本文中提出了一种可扩展的联邦图变压器（FedGT）。首先，我们设计了一种混合注意力方案来降低复杂性。",
    "tldr": "本论文提出了一种可扩展的联邦图形变压器（FedGT），用于解决子图联邦学习中缺少链接和子图异构性的挑战。",
    "en_tdlr": "This paper proposes a scalable FedGT for federated node classification, which tackles the challenges of missing links and subgraph heterogeneity in subgraph federated learning."
}