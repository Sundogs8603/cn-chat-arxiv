{
    "title": "LCV2: An Efficient Pretraining-Free Framework for Grounded Visual Question Answering",
    "abstract": "arXiv:2401.15842v2 Announce Type: replace-cross  Abstract: In this paper, the LCV2 modular method is proposed for the Grounded Visual Question Answering task in the vision-language multimodal domain. This approach relies on a frozen large language model (LLM) as intermediate mediator between the off-the-shelf VQA model and the off-the-shelf visual grounding (VG) model, where the LLM transforms and conveys textual information between the two modules based on a designed prompt. LCV2 establish an integrated plug-and-play framework without the need for any pre-training process. This framework can be deployed for VQA Grounding tasks under low computational resources. The modularized model within the framework allows application with various state-of-the-art pre-trained models, exhibiting significant potential to be advance with the times. Experimental implementations were conducted under constrained computational and memory resources, evaluating the proposed method's performance on benchmar",
    "link": "https://arxiv.org/abs/2401.15842",
    "context": "Title: LCV2: An Efficient Pretraining-Free Framework for Grounded Visual Question Answering\nAbstract: arXiv:2401.15842v2 Announce Type: replace-cross  Abstract: In this paper, the LCV2 modular method is proposed for the Grounded Visual Question Answering task in the vision-language multimodal domain. This approach relies on a frozen large language model (LLM) as intermediate mediator between the off-the-shelf VQA model and the off-the-shelf visual grounding (VG) model, where the LLM transforms and conveys textual information between the two modules based on a designed prompt. LCV2 establish an integrated plug-and-play framework without the need for any pre-training process. This framework can be deployed for VQA Grounding tasks under low computational resources. The modularized model within the framework allows application with various state-of-the-art pre-trained models, exhibiting significant potential to be advance with the times. Experimental implementations were conducted under constrained computational and memory resources, evaluating the proposed method's performance on benchmar",
    "path": "papers/24/01/2401.15842.json",
    "total_tokens": 806,
    "translated_title": "LCV2：一种高效的免预训练框架用于基于视觉的问答任务",
    "translated_abstract": "本文提出了LCV2模块化方法，用于在视觉-语言多模域中的基于视觉的问答任务。该方法依赖于一个冻结的大型语言模型（LLM），作为现成VQA模型和现成视觉定位（VG）模型之间的中间介质，LLM基于设计的提示转换和传递文本信息。LCV2建立了一个集成的即插即用框架，无需任何预训练过程，可以在低计算资源下用于VQA Grounding任务。该框架允许与各种最先进的预训练模型一起使用，具有与时俱进的潜力。在受限制的计算资源和内存资源下进行了实验实现，评估了所提方法在基准上的性",
    "tldr": "LCV2提出了一种高效的兼容各种预训练模型的基于视觉问答的框架，无需预训练过程，适用于低计算资源下的任务",
    "en_tdlr": "LCV2 proposes an efficient framework for grounded visual question answering that is compatible with various pre-trained models, without the need for pre-training, suitable for tasks with low computational resources."
}