{
    "title": "Even-if Explanations: Formal Foundations, Priorities and Complexity. (arXiv:2401.10938v1 [cs.AI])",
    "abstract": "EXplainable AI has received significant attention in recent years. Machine learning models often operate as black boxes, lacking explainability and transparency while supporting decision-making processes. Local post-hoc explainability queries attempt to answer why individual inputs are classified in a certain way by a given model. While there has been important work on counterfactual explanations, less attention has been devoted to semifactual ones. In this paper, we focus on local post-hoc explainability queries within the semifactual `even-if' thinking and their computational complexity among different classes of models, and show that both linear and tree-based models are strictly more interpretable than neural networks. After this, we introduce a preference-based framework that enables users to personalize explanations based on their preferences, both in the case of semifactuals and counterfactuals, enhancing interpretability and user-centricity. Finally, we explore the complexity o",
    "link": "http://arxiv.org/abs/2401.10938",
    "context": "Title: Even-if Explanations: Formal Foundations, Priorities and Complexity. (arXiv:2401.10938v1 [cs.AI])\nAbstract: EXplainable AI has received significant attention in recent years. Machine learning models often operate as black boxes, lacking explainability and transparency while supporting decision-making processes. Local post-hoc explainability queries attempt to answer why individual inputs are classified in a certain way by a given model. While there has been important work on counterfactual explanations, less attention has been devoted to semifactual ones. In this paper, we focus on local post-hoc explainability queries within the semifactual `even-if' thinking and their computational complexity among different classes of models, and show that both linear and tree-based models are strictly more interpretable than neural networks. After this, we introduce a preference-based framework that enables users to personalize explanations based on their preferences, both in the case of semifactuals and counterfactuals, enhancing interpretability and user-centricity. Finally, we explore the complexity o",
    "path": "papers/24/01/2401.10938.json",
    "total_tokens": 919,
    "translated_title": "即使解释：形式基础，优先级和复杂性",
    "translated_abstract": "解释性人工智能近年来受到了重要关注。机器学习模型通常作为黑盒子运行，缺乏解释和透明性，而又支持决策过程。局部事后解释性查询试图回答为什么给定模型如何对个体输入进行分类的问题。虽然关于反事实解释已经进行了重要工作，但对半事实解释的关注较少。本文关注于半事实的局部事后解释性查询以及不同模型类别中其计算复杂性，并表明线性和基于树的模型比神经网络更易于解释。接着，我们介绍了一种基于偏好的框架，使用户能够根据自己的偏好个性化解释，无论是在半事实还是反事实的情况下，提高解释能力和用户中心性。最后，我们探讨了模型复杂度。",
    "tldr": "本论文研究了解释性人工智能中的局部事后解释性查询，特别关注半事实的解释，并对线性模型和基于树的模型与神经网络的解释能力进行了比较。此外，提出了一种基于偏好的框架，允许用户根据自己的首选项个性化解释。最后，探讨了模型复杂度的问题。"
}