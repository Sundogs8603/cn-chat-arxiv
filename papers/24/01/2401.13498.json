{
    "title": "Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting. (arXiv:2401.13498v1 [cs.SD])",
    "abstract": "Synthesizing performing guitar sound is a highly challenging task due to the polyphony and high variability in expression. Recently, deep generative models have shown promising results in synthesizing expressive polyphonic instrument sounds from music scores, often using a generic MIDI input. In this work, we propose an expressive acoustic guitar sound synthesis model with a customized input representation to the instrument, which we call guitarroll. We implement the proposed approach using diffusion-based outpainting which can generate audio with long-term consistency. To overcome the lack of MIDI/audio-paired datasets, we used not only an existing guitar dataset but also collected data from a high quality sample-based guitar synthesizer. Through quantitative and qualitative evaluations, we show that our proposed model has higher audio quality than the baseline model and generates more realistic timbre sounds than the previous leading work.",
    "link": "http://arxiv.org/abs/2401.13498",
    "context": "Title: Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific Input Representation and Diffusion Outpainting. (arXiv:2401.13498v1 [cs.SD])\nAbstract: Synthesizing performing guitar sound is a highly challenging task due to the polyphony and high variability in expression. Recently, deep generative models have shown promising results in synthesizing expressive polyphonic instrument sounds from music scores, often using a generic MIDI input. In this work, we propose an expressive acoustic guitar sound synthesis model with a customized input representation to the instrument, which we call guitarroll. We implement the proposed approach using diffusion-based outpainting which can generate audio with long-term consistency. To overcome the lack of MIDI/audio-paired datasets, we used not only an existing guitar dataset but also collected data from a high quality sample-based guitar synthesizer. Through quantitative and qualitative evaluations, we show that our proposed model has higher audio quality than the baseline model and generates more realistic timbre sounds than the previous leading work.",
    "path": "papers/24/01/2401.13498.json",
    "total_tokens": 932,
    "translated_title": "采用乐器特定输入表示和扩散外扩技术的表现力丰富的声学吉他声音合成",
    "translated_abstract": "由于复音和表现的高度变异，合成演奏吉他声音是一项极具挑战性的任务。最近，深度生成模型在从音乐乐谱中合成表现力丰富的多音乐器声音方面显示出了有希望的结果，通常使用通用的MIDI输入。在这项工作中，我们提出了一种具有自定义乐器输入表示的表现力丰富的声学吉他声音合成模型，我们称之为guitarroll。我们使用基于扩散的外扩技术实现了所提出的方法，该技术可以生成具有长期一致性的音频。为了克服缺乏MIDI/音频配对数据集的问题，我们不仅使用了现有的吉他数据集，还从高质量的基于样本的吉他合成器中收集了数据。通过定量和定性评估，我们展示了我们提出的模型比基准模型具有更高的音频质量，并且比先前的领先工作生成更加逼真的音色声音。",
    "tldr": "本文提出了一种采用乐器特定输入表示和扩散外扩技术的表现力丰富的声学吉他声音合成模型，通过定量和定性评估，展示了该模型比其他模型具有更高的音频质量和更加逼真的音色声音。"
}