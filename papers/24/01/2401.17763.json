{
    "title": "Convergence of Expectation-Maximization Algorithm with Mixed-Integer Optimization",
    "abstract": "The convergence of expectation-maximization (EM)-based algorithms typically requires continuity of the likelihood function with respect to all the unknown parameters (optimization variables). The requirement is not met when parameters comprise both discrete and continuous variables, making the convergence analysis nontrivial. This paper introduces a set of conditions that ensure the convergence of a specific class of EM algorithms that estimate a mixture of discrete and continuous parameters. Our results offer a new analysis technique for iterative algorithms that solve mixed-integer non-linear optimization problems. As a concrete example, we prove the convergence of the EM-based sparse Bayesian learning algorithm in [1] that estimates the state of a linear dynamical system with jointly sparse inputs and bursty missing observations. Our results establish that the algorithm in [1] converges to the set of stationary points of the maximum likelihood cost with respect to the continuous opt",
    "link": "https://arxiv.org/abs/2401.17763",
    "context": "Title: Convergence of Expectation-Maximization Algorithm with Mixed-Integer Optimization\nAbstract: The convergence of expectation-maximization (EM)-based algorithms typically requires continuity of the likelihood function with respect to all the unknown parameters (optimization variables). The requirement is not met when parameters comprise both discrete and continuous variables, making the convergence analysis nontrivial. This paper introduces a set of conditions that ensure the convergence of a specific class of EM algorithms that estimate a mixture of discrete and continuous parameters. Our results offer a new analysis technique for iterative algorithms that solve mixed-integer non-linear optimization problems. As a concrete example, we prove the convergence of the EM-based sparse Bayesian learning algorithm in [1] that estimates the state of a linear dynamical system with jointly sparse inputs and bursty missing observations. Our results establish that the algorithm in [1] converges to the set of stationary points of the maximum likelihood cost with respect to the continuous opt",
    "path": "papers/24/01/2401.17763.json",
    "total_tokens": 836,
    "translated_title": "基于混合整数优化的期望最大化算法的收敛性",
    "translated_abstract": "期望最大化（EM）算法的收敛通常需要似然函数对所有未知参数（优化变量）连续。当参数包括离散和连续变量时，这一要求无法满足，导致收敛分析非常困难。本文引入了一组条件，保证了一类估计离散和连续参数混合的特定EM算法的收敛性。我们的结果为解决混合整数非线性优化问题的迭代算法提供了一种新的分析技术。作为一个具体的例子，我们证明了基于EM的稀疏贝叶斯学习算法在估计具有联合稀疏输入和断续缺失观测的线性动态系统的状态时的收敛性。我们的结果证明了[1]中的算法收敛到最大似然代价关于连续优化变量的稳定点集。",
    "tldr": "本文引入了一组条件，保证了一类估计离散和连续参数混合的特定EM算法的收敛性，并为解决混合整数非线性优化问题的迭代算法提供了一种新的分析技术。",
    "en_tdlr": "This paper introduces a set of conditions that ensure the convergence of a specific class of EM algorithms that estimate a mixture of discrete and continuous parameters, offering a new analysis technique for iterative algorithms that solve mixed-integer non-linear optimization problems."
}