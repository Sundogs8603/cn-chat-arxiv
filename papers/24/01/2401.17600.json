{
    "title": "Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data",
    "abstract": "Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting",
    "link": "https://arxiv.org/abs/2401.17600",
    "context": "Title: Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data\nAbstract: Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks. Motivated by real-world applications, our benchmark includes scenarios like urban monitoring, disaster relief, land use, and conservation. We discover that, although state-of-the-art VLMs like GPT-4V possess extensive world knowledge that leads to strong performance on open-ended tasks like location understanding and image captioning, their poor spatial reasoning limits usefulness on object localization and counting",
    "path": "papers/24/01/2401.17600.json",
    "total_tokens": 975,
    "translated_title": "在地球观测数据上对GPT-4V进行标注任务评估：对语言视觉模型在视觉任务上的一项基准测试",
    "translated_abstract": "大型语言视觉模型（VLMs）在涉及自然语言指令和视觉输入的复杂任务上展示出令人印象深刻的性能。然而，目前尚不清楚这些模型在以卫星和航空图像为主的地球观测（EO）数据上的能力，这类数据在VLMs的训练数据中较为罕见。在这项研究中，我们提出了一个全面的基准测试，通过评估VLMs在场景理解、定位和计数以及变化检测任务上的能力，以衡量它们在EO数据上作为有效工具的进展。受现实世界应用的启发，我们的基准测试包括了城市监测、灾害救援、土地利用和保护等场景。我们发现，尽管像GPT-4V这样的最新VLMs具有丰富的世界知识，导致在位置理解和图像标注等开放式任务上表现强劲，但是它们的空间推理能力不足，限制了它们在目标定位和计数方面的实用性。",
    "tldr": "在这项研究中，我们对GPT-4V模型在地球观测数据上的性能进行了评估。结果显示，尽管GPT-4V在开放式任务如位置理解和图像标注方面表现良好，但其空间推理能力不足，限制了其在目标定位和计数方面的实用性。",
    "en_tdlr": "In this study, the performance of the GPT-4V model on Earth observation data was evaluated. The results showed that while GPT-4V performs well on open-ended tasks such as location understanding and image captioning, its spatial reasoning abilities are limited, restricting its usefulness in object localization and counting."
}