{
    "title": "On Time-Indexing as Inductive Bias in Deep RL for Sequential Manipulation Tasks. (arXiv:2401.01993v1 [cs.RO])",
    "abstract": "While solving complex manipulation tasks, manipulation policies often need to learn a set of diverse skills to accomplish these tasks. The set of skills is often quite multimodal - each one may have a quite distinct distribution of actions and states. Standard deep policy-learning algorithms often model policies as deep neural networks with a single output head (deterministic or stochastic). This structure requires the network to learn to switch between modes internally, which can lead to lower sample efficiency and poor performance. In this paper we explore a simple structure which is conducive to skill learning required for so many of the manipulation tasks. Specifically, we propose a policy architecture that sequentially executes different action heads for fixed durations, enabling the learning of primitive skills such as reaching and grasping. Our empirical evaluation on the Metaworld tasks reveals that this simple structure outperforms standard policy learning methods, highlightin",
    "link": "http://arxiv.org/abs/2401.01993",
    "context": "Title: On Time-Indexing as Inductive Bias in Deep RL for Sequential Manipulation Tasks. (arXiv:2401.01993v1 [cs.RO])\nAbstract: While solving complex manipulation tasks, manipulation policies often need to learn a set of diverse skills to accomplish these tasks. The set of skills is often quite multimodal - each one may have a quite distinct distribution of actions and states. Standard deep policy-learning algorithms often model policies as deep neural networks with a single output head (deterministic or stochastic). This structure requires the network to learn to switch between modes internally, which can lead to lower sample efficiency and poor performance. In this paper we explore a simple structure which is conducive to skill learning required for so many of the manipulation tasks. Specifically, we propose a policy architecture that sequentially executes different action heads for fixed durations, enabling the learning of primitive skills such as reaching and grasping. Our empirical evaluation on the Metaworld tasks reveals that this simple structure outperforms standard policy learning methods, highlightin",
    "path": "papers/24/01/2401.01993.json",
    "total_tokens": 933,
    "translated_title": "关于用时间索引作为深度强化学习中的归纳偏好处理顺序操控任务",
    "translated_abstract": "在解决复杂的操控任务时，操控策略通常需要学习一组多样化的技能来完成这些任务。这组技能通常是多模态的，每个技能可能具有相当不同的行动和状态分布。标准的深度策略学习算法通常将策略建模为具有单个输出头的深度神经网络（确定性或随机性）。这种结构要求网络在内部学习在不同模式之间切换，这可能导致样本利用效率低和性能差。在本文中，我们探索了一种简单的结构，有助于学习许多操控任务所需的技能。具体而言，我们提出了一种策略架构，其按固定持续时间依次执行不同的行动头，从而实现了诸如到达和抓取等基本技能的学习。我们对Metaworld任务进行了实证评估，结果表明这种简单的结构优于标准的策略学习方法，凸显出其创新之处。",
    "tldr": "本论文提出一种简单的策略架构，通过按固定持续时间依次执行不同的行动头，使得深度强化学习在解决顺序操控任务时能够学习到更多的基本技能，实证评估结果显示这种结构优于标准方法。",
    "en_tdlr": "This paper proposes a simple policy architecture that sequentially executes different action heads for fixed durations, allowing deep reinforcement learning to learn more primitive skills for solving sequential manipulation tasks. Empirical evaluation results demonstrate the superiority of this structure over standard methods."
}