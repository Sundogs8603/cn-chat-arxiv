{
    "title": "Neural Networks for Singular Perturbations. (arXiv:2401.06656v1 [math.NA])",
    "abstract": "We prove deep neural network (DNN for short) expressivity rate bounds for solution sets of a model class of singularly perturbed, elliptic two-point boundary value problems, in Sobolev norms, on the bounded interval $(-1,1)$. We assume that the given source term and reaction coefficient are analytic in $[-1,1]$.  We establish expression rate bounds in Sobolev norms in terms of the NN size which are uniform with respect to the singular perturbation parameter for several classes of DNN architectures. In particular, ReLU NNs, spiking NNs, and $\\tanh$- and sigmoid-activated NNs. The latter activations can represent ``exponential boundary layer solution features'' explicitly, in the last hidden layer of the DNN, i.e. in a shallow subnetwork, and afford improved robust expression rate bounds in terms of the NN size.  We prove that all DNN architectures allow robust exponential solution expression in so-called `energy' as well as in `balanced' Sobolev norms, for analytic input data.",
    "link": "http://arxiv.org/abs/2401.06656",
    "context": "Title: Neural Networks for Singular Perturbations. (arXiv:2401.06656v1 [math.NA])\nAbstract: We prove deep neural network (DNN for short) expressivity rate bounds for solution sets of a model class of singularly perturbed, elliptic two-point boundary value problems, in Sobolev norms, on the bounded interval $(-1,1)$. We assume that the given source term and reaction coefficient are analytic in $[-1,1]$.  We establish expression rate bounds in Sobolev norms in terms of the NN size which are uniform with respect to the singular perturbation parameter for several classes of DNN architectures. In particular, ReLU NNs, spiking NNs, and $\\tanh$- and sigmoid-activated NNs. The latter activations can represent ``exponential boundary layer solution features'' explicitly, in the last hidden layer of the DNN, i.e. in a shallow subnetwork, and afford improved robust expression rate bounds in terms of the NN size.  We prove that all DNN architectures allow robust exponential solution expression in so-called `energy' as well as in `balanced' Sobolev norms, for analytic input data.",
    "path": "papers/24/01/2401.06656.json",
    "total_tokens": 991,
    "translated_title": "神经网络用于奇异摄动",
    "translated_abstract": "我们证明了在有界区间$(-1, 1)$上，关于特定类别的奇异摄动椭圆型两点边界值问题的解集的深度神经网络（DNN）的表达率界限，用Sobolev范数表示。我们假设给定的源项和反应系数在$[-1, 1]$中是解析的。针对几种DNN架构，我们建立了与奇异摄动参数无关的Sobolev范数下的表达率界限，特别是对于ReLU NNs，spiking NNs，$\\tanh$和sigmoid激活的NNs。后两种激活函数可以明确地表示“指数边界层解特性”，即在DNN的最后一个隐藏层中，即在一个浅层子网络中，并以NN大小量化得到改进的鲁棒表达率界限。我们证明了所有的DNN架构都允许在所谓的“能量”和“平衡”Sobolev范数中以解析输入数据的方式获得鲁棒的指数解表达。",
    "tldr": "本论文探讨了用于奇异摄动的神经网络模型的表达率极限，并在不同类型的神经网络架构中得到了统一的表达率界限。研究表明，基于ReLU、spiking、$\\tanh$和sigmoid激活函数的神经网络可以明确地表示“指数边界层解特性”，并能得到改进的鲁棒表达率界限。"
}