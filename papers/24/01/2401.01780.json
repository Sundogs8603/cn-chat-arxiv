{
    "title": "Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering. (arXiv:2401.01780v1 [cs.CL])",
    "abstract": "While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers. Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information. However, searching in a large collection of documents introduces additional computational/time costs. An optimal behavior would be to query external resources only when the LLM is not confident about answers. In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool. We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task. In addition, we propose to leverage parameter-efficient fine-tuning t",
    "link": "http://arxiv.org/abs/2401.01780",
    "context": "Title: Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering. (arXiv:2401.01780v1 [cs.CL])\nAbstract: While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers. Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information. However, searching in a large collection of documents introduces additional computational/time costs. An optimal behavior would be to query external resources only when the LLM is not confident about answers. In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool. We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task. In addition, we propose to leverage parameter-efficient fine-tuning t",
    "path": "papers/24/01/2401.01780.json",
    "total_tokens": 899,
    "translated_title": "在封闭式问答中优化API依赖以减少幻觉的不确定性导航",
    "translated_abstract": "尽管大型语言模型(LLM)能够积累和恢复知识，但它们仍然容易产生幻觉。特别是在面对事实性问题时，LLM不能仅仅依靠参数中存储的知识来保证真实和正确的答案。将这些模型与搜索外部信息源(如网络)的能力相结合，是一种将知识基于检索信息的有希望的方法。然而，在大量文档中进行搜索会带来额外的计算/时间成本。最佳策略是只有在LLM对答案不确定时才查询外部资源。在本文中，我们提出了一种新的LLM，能够自我评估是否能够直接回答问题或者需要请求外部工具。我们通过在闭书问答任务中引入幻觉屏蔽机制来进行监督学习。此外，我们还提出利用参数高效微调的方法。",
    "tldr": "本文提出了一种新的大型语言模型，通过让模型自我评估是否需要查询外部资源，来优化封闭式问答中的幻觉问题。研究者通过引入幻觉屏蔽机制以及参数高效微调的方法，实现了该模型。",
    "en_tdlr": "This paper proposes a new approach to optimize hallucination reduction in closed-book question answering by allowing the model to self-estimate the need for external resources. The researchers introduce a hallucination masking mechanism and leverage parameter-efficient fine-tuning to achieve this."
}