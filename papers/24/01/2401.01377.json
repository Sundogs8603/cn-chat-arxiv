{
    "title": "Does Few-shot Learning Suffer from Backdoor Attacks?. (arXiv:2401.01377v1 [cs.CR])",
    "abstract": "The field of few-shot learning (FSL) has shown promising results in scenarios where training data is limited, but its vulnerability to backdoor attacks remains largely unexplored. We first explore this topic by first evaluating the performance of the existing backdoor attack methods on few-shot learning scenarios. Unlike in standard supervised learning, existing backdoor attack methods failed to perform an effective attack in FSL due to two main issues. Firstly, the model tends to overfit to either benign features or trigger features, causing a tough trade-off between attack success rate and benign accuracy. Secondly, due to the small number of training samples, the dirty label or visible trigger in the support set can be easily detected by victims, which reduces the stealthiness of attacks. It seemed that FSL could survive from backdoor attacks. However, in this paper, we propose the Few-shot Learning Backdoor Attack (FLBA) to show that FSL can still be vulnerable to backdoor attacks.",
    "link": "http://arxiv.org/abs/2401.01377",
    "context": "Title: Does Few-shot Learning Suffer from Backdoor Attacks?. (arXiv:2401.01377v1 [cs.CR])\nAbstract: The field of few-shot learning (FSL) has shown promising results in scenarios where training data is limited, but its vulnerability to backdoor attacks remains largely unexplored. We first explore this topic by first evaluating the performance of the existing backdoor attack methods on few-shot learning scenarios. Unlike in standard supervised learning, existing backdoor attack methods failed to perform an effective attack in FSL due to two main issues. Firstly, the model tends to overfit to either benign features or trigger features, causing a tough trade-off between attack success rate and benign accuracy. Secondly, due to the small number of training samples, the dirty label or visible trigger in the support set can be easily detected by victims, which reduces the stealthiness of attacks. It seemed that FSL could survive from backdoor attacks. However, in this paper, we propose the Few-shot Learning Backdoor Attack (FLBA) to show that FSL can still be vulnerable to backdoor attacks.",
    "path": "papers/24/01/2401.01377.json",
    "total_tokens": 894,
    "translated_title": "少样本学习是否容易受到后门攻击？",
    "translated_abstract": "少样本学习（FSL）在训练数据有限的情况下表现出了很好的结果，但它对后门攻击的脆弱性仍然未被充分探索。我们首先通过评估现有后门攻击方法在少样本学习场景中的表现来探索这个问题。与标准的监督学习不同，现有的后门攻击方法在少样本学习中无法进行有效的攻击，主要存在两个问题。首先，模型往往过拟合于良性特征或触发特征，导致攻击成功率和良性准确率之间存在很难平衡的问题。其次，由于训练样本数量较少，支持集中的脏标签或可见触发器很容易被受攻击者检测到，从而降低了攻击的隐蔽性。似乎少样本学习可以抵御后门攻击。然而，在本文中，我们提出了少样本学习后门攻击（FLBA）来展示少样本学习仍然容易受到后门攻击的情况。",
    "tldr": "本研究探索了少样本学习对后门攻击的脆弱性，并提出了少样本学习后门攻击方法（FLBA）。",
    "en_tdlr": "This study explores the vulnerability of few-shot learning to backdoor attacks and proposes a few-shot learning backdoor attack method (FLBA)."
}