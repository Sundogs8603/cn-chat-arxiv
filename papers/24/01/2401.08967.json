{
    "title": "ReFT: Reasoning with Reinforced Fine-Tuning. (arXiv:2401.08967v1 [cs.CL])",
    "abstract": "One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question",
    "link": "http://arxiv.org/abs/2401.08967",
    "context": "Title: ReFT: Reasoning with Reinforced Fine-Tuning. (arXiv:2401.08967v1 [cs.CL])\nAbstract: One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question",
    "path": "papers/24/01/2401.08967.json",
    "total_tokens": 855,
    "translated_title": "ReFT: 加强强化微调的推理能力",
    "translated_abstract": "增强大型语言模型（LLMs）的推理能力的一种方法是使用链式思考（CoT）注释进行监督微调（SFT）。然而，这种方法在泛化能力上并不十分强大，因为训练仅依赖于给定的CoT数据。例如，在数学问题解决中，训练数据中通常只有一个注释的推理路径用于每个问题。直观来说，让算法从给定的问题中学习多个注释的推理路径会更好。为了解决这个问题，我们提出了一种简单而有效的方法，称为加强强化微调（ReFT），以增强学习LLMs进行推理的泛化能力，以数学问题解决为例。ReFT首先使用SFT对模型进行热身，然后采用在线强化学习（具体来说，在本文中是使用PPO算法）进一步微调模型，其中根据问题自动采样了大量的推理路径。",
    "tldr": "ReFT是一种加强推理能力的强化微调方法，通过利用更多的推理路径进行微调，提高了大型语言模型在数学问题解决中的泛化能力。",
    "en_tdlr": "ReFT is a reinforced fine-tuning method that enhances reasoning capability by fine-tuning large language models with multiple reasoning paths, improving their generalizability in math problem-solving."
}