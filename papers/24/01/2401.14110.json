{
    "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators. (arXiv:2401.14110v1 [cs.LG])",
    "abstract": "The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy.",
    "link": "http://arxiv.org/abs/2401.14110",
    "context": "Title: Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators. (arXiv:2401.14110v1 [cs.LG])\nAbstract: The majority of the research on the quantization of Deep Neural Networks (DNNs) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits accumulators, with no significant degradation in accuracy. Lastly, we show that as we decrease the accumulation precision further, using fine-grained gradient approximations can improve the DNN accuracy.",
    "path": "papers/24/01/2401.14110.json",
    "total_tokens": 896,
    "translated_title": "以较低比特宽度的累加器降低深度网络推理成本",
    "translated_abstract": "目前大多数关于深度神经网络（DNN）量化的研究都着重于降低高级框架可见的张量精度（例如权重、激活和梯度）。然而，当前的硬件仍然依赖于高精度的核心操作，最重要的是累加乘积的运算。这种高精度累加运算逐渐成为主要的计算瓶颈。这是因为到目前为止，低精度累加器的使用导致了性能的显著降低。在这项工作中，我们提出了一种简单的方法来训练和微调高端DNN，首次实现使用更便宜的12位累加器，而不会导致显著的精度降低。最后，我们还展示出随着累加精度进一步降低，使用细粒度梯度近似可以提高DNN的精度。",
    "tldr": "本论文提出了一种简单的方法，可以训练和微调高端DNN，实现使用更便宜的12位累加器，而不会导致精度降低，并展示了使用细粒度梯度近似可以提高DNN的精度。",
    "en_tdlr": "This paper presents a simple method to train and fine-tune high-end DNNs, allowing the utilization of cheaper 12-bit accumulators without significant degradation in accuracy. It also demonstrates that using fine-grained gradient approximations can improve DNN accuracy as accumulation precision decreases."
}