{
    "title": "S$^{2}$-DMs:Skip-Step Diffusion Models. (arXiv:2401.01520v1 [cs.CV])",
    "abstract": "Diffusion models have emerged as powerful generative tools, rivaling GANs in sample quality and mirroring the likelihood scores of autoregressive models. A subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry: they are trained over $T$ steps but only sample from a subset of $T$ during generation. This selective sampling approach, though optimized for speed, inadvertently misses out on vital information from the unsampled steps, leading to potential compromises in sample quality. To address this issue, we present the S$^{2}$-DMs, which is a new training method by using an innovative $L_{skip}$, meticulously designed to reintegrate the information omitted during the selective sampling phase. The benefits of this approach are manifold: it notably enhances sample quality, is exceptionally simple to implement, requires minimal code modifications, and is flexible enough to be compatible with various sampling algorithms. On the CIFAR10 dataset, models trained using our ",
    "link": "http://arxiv.org/abs/2401.01520",
    "context": "Title: S$^{2}$-DMs:Skip-Step Diffusion Models. (arXiv:2401.01520v1 [cs.CV])\nAbstract: Diffusion models have emerged as powerful generative tools, rivaling GANs in sample quality and mirroring the likelihood scores of autoregressive models. A subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry: they are trained over $T$ steps but only sample from a subset of $T$ during generation. This selective sampling approach, though optimized for speed, inadvertently misses out on vital information from the unsampled steps, leading to potential compromises in sample quality. To address this issue, we present the S$^{2}$-DMs, which is a new training method by using an innovative $L_{skip}$, meticulously designed to reintegrate the information omitted during the selective sampling phase. The benefits of this approach are manifold: it notably enhances sample quality, is exceptionally simple to implement, requires minimal code modifications, and is flexible enough to be compatible with various sampling algorithms. On the CIFAR10 dataset, models trained using our ",
    "path": "papers/24/01/2401.01520.json",
    "total_tokens": 960,
    "translated_title": "S$^{2}$-DMs：跳过步骤的扩散模型",
    "translated_abstract": "扩散模型已经成为强大的生成工具，样本质量与生成对抗网络（GANs）相当，并且反映了自回归模型的似然分数。其中一部分模型，如DDIMs，展示了固有的不对称性：它们在训练过程中使用$T$个步骤，但在生成过程中只从其中的子集进行采样。这种选择性采样方法虽然优化了速度，但无意中错过了未采样步骤中的重要信息，导致样本质量可能出现问题。为了解决这个问题，我们提出了S$^{2}$-DMs，一种新的训练方法，使用创新的$L_{skip}$，精心设计以重新整合在选择性采样阶段中省略的信息。这种方法的好处很多：它显著提高了样本质量，实现起来非常简单，需要最少的代码修改，并且足够灵活，可以与各种采样算法兼容。在CIFAR10数据集上，使用我们的模型训练的结果...",
    "tldr": "本论文提出了一种新的训练方法S$^{2}$-DMs，通过创新的$L_{skip}$重新整合选择性采样阶段中省略的信息，显著提高了样本质量，并且实现简单，对代码修改要求少，与各种采样算法兼容。",
    "en_tdlr": "This paper proposes a new training method called S$^{2}$-DMs, which utilizes an innovative $L_{skip}$ to reintegrate the information omitted during the selective sampling phase, significantly improving sample quality. It is simple to implement, requires minimal code modifications, and is compatible with various sampling algorithms."
}