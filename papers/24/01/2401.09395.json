{
    "title": "Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions",
    "abstract": "arXiv:2401.09395v2 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust probl",
    "link": "https://arxiv.org/abs/2401.09395",
    "context": "Title: Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions\nAbstract: arXiv:2401.09395v2 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust probl",
    "path": "papers/24/01/2401.09395.json",
    "total_tokens": 928,
    "translated_title": "被理性的流沙所困，远离AGI峰会：通过本体引导干预评估LLMs的数学和编码能力",
    "translated_abstract": "最近大型语言模型（LLMs）的先进发展展示了在现有逻辑推理基准测试中取得了引人注目的成果，其中一些模型甚至超过了人类表现。然而，它们在推理任务中的实际能力和稳健性仍然是一个未解之谜。因此，本文关注两个流行的推理任务：算术推理和代码生成。特别是，我们引入了：（i）数学和编码问题的通用扰动本体，（ii）一种半自动方法来应用这些扰动，以及（iii）两个数据集MORE和CORE，分别用于扰动数学和编码问题，以探究LLM在数字推理和编码任务中的能力极限。通过对封闭源和开源LLMs的全面评估，我们展示了所有模型对扰动问题的显著性能下降，表明当前的LLMs缺乏稳健性。",
    "tldr": "通过引入数学和编码问题的扰动本体以及两个数据集，作者评估了LLMs在数字推理和编码任务中的能力，在全面评估中发现所有模型在扰动问题上表现显著下降，表明当前的LLMs缺乏稳健性。"
}