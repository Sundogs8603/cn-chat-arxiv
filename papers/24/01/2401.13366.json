{
    "title": "Mitigating System Bias in Resource Constrained Asynchronous Federated Learning Systems. (arXiv:2401.13366v1 [cs.LG])",
    "abstract": "Federated learning (FL) systems face performance challenges in dealing with heterogeneous devices and non-identically distributed data across clients. We propose a dynamic global model aggregation method within Asynchronous Federated Learning (AFL) deployments to address these issues. Our aggregation method scores and adjusts the weighting of client model updates based on their upload frequency to accommodate differences in device capabilities. Additionally, we also immediately provide an updated global model to clients after they upload their local models to reduce idle time and improve training efficiency. We evaluate our approach within an AFL deployment consisting of 10 simulated clients with heterogeneous compute constraints and non-IID data. The simulation results, using the FashionMNIST dataset, demonstrate over 10% and 19% improvement in global model accuracy compared to state-of-the-art methods PAPAYA and FedAsync, respectively. Our dynamic aggregation method allows reliable g",
    "link": "http://arxiv.org/abs/2401.13366",
    "context": "Title: Mitigating System Bias in Resource Constrained Asynchronous Federated Learning Systems. (arXiv:2401.13366v1 [cs.LG])\nAbstract: Federated learning (FL) systems face performance challenges in dealing with heterogeneous devices and non-identically distributed data across clients. We propose a dynamic global model aggregation method within Asynchronous Federated Learning (AFL) deployments to address these issues. Our aggregation method scores and adjusts the weighting of client model updates based on their upload frequency to accommodate differences in device capabilities. Additionally, we also immediately provide an updated global model to clients after they upload their local models to reduce idle time and improve training efficiency. We evaluate our approach within an AFL deployment consisting of 10 simulated clients with heterogeneous compute constraints and non-IID data. The simulation results, using the FashionMNIST dataset, demonstrate over 10% and 19% improvement in global model accuracy compared to state-of-the-art methods PAPAYA and FedAsync, respectively. Our dynamic aggregation method allows reliable g",
    "path": "papers/24/01/2401.13366.json",
    "total_tokens": 936,
    "translated_title": "在资源受限的异步联邦学习系统中减轻系统偏差",
    "translated_abstract": "联邦学习系统在处理异构设备和客户端非同分布数据时面临性能挑战。我们提出一种动态全局模型聚合方法来解决这些问题。我们的聚合方法根据客户端的上传频率对其模型更新进行评分和调整权重，以适应设备能力的差异。此外，我们还在客户端上传本地模型后立即向其提供更新的全局模型，以减少空闲时间并提高训练效率。我们在一个包含10个模拟客户端的异步联邦学习部署中评估了我们的方法，这些客户端具有异构计算约束和非IID数据。使用FashionMNIST数据集的仿真结果表明，与最先进的方法PAPAYA和FedAsync相比，全局模型的准确性改善了10%和19%。我们的动态聚合方法可以实现可靠的全局模型更新。",
    "tldr": "该论文提出了一种在资源受限的异步联邦学习系统中减轻系统偏差的动态全局模型聚合方法，通过根据客户端的上传频率评分和调整模型更新的权重，以适应异构设备和非同分布数据的挑战。实验结果表明，在仿真环境中，与最先进的方法相比，该方法在全局模型准确性上有显著的改善。"
}