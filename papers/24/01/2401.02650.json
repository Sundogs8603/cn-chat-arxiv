{
    "title": "Improving sample efficiency of high dimensional Bayesian optimization with MCMC. (arXiv:2401.02650v1 [cs.LG])",
    "abstract": "Sequential optimization methods are often confronted with the curse of dimensionality in high-dimensional spaces. Current approaches under the Gaussian process framework are still burdened by the computational complexity of tracking Gaussian process posteriors and need to partition the optimization problem into small regions to ensure exploration or assume an underlying low-dimensional structure. With the idea of transiting the candidate points towards more promising positions, we propose a new method based on Markov Chain Monte Carlo to efficiently sample from an approximated posterior. We provide theoretical guarantees of its convergence in the Gaussian process Thompson sampling setting. We also show experimentally that both the Metropolis-Hastings and the Langevin Dynamics version of our algorithm outperform state-of-the-art methods in high-dimensional sequential optimization and reinforcement learning benchmarks.",
    "link": "http://arxiv.org/abs/2401.02650",
    "context": "Title: Improving sample efficiency of high dimensional Bayesian optimization with MCMC. (arXiv:2401.02650v1 [cs.LG])\nAbstract: Sequential optimization methods are often confronted with the curse of dimensionality in high-dimensional spaces. Current approaches under the Gaussian process framework are still burdened by the computational complexity of tracking Gaussian process posteriors and need to partition the optimization problem into small regions to ensure exploration or assume an underlying low-dimensional structure. With the idea of transiting the candidate points towards more promising positions, we propose a new method based on Markov Chain Monte Carlo to efficiently sample from an approximated posterior. We provide theoretical guarantees of its convergence in the Gaussian process Thompson sampling setting. We also show experimentally that both the Metropolis-Hastings and the Langevin Dynamics version of our algorithm outperform state-of-the-art methods in high-dimensional sequential optimization and reinforcement learning benchmarks.",
    "path": "papers/24/01/2401.02650.json",
    "total_tokens": 811,
    "translated_title": "通过MCMC改进高维贝叶斯优化的样本效率",
    "translated_abstract": "高维空间中的顺序优化方法经常面临维度诅咒。目前的高斯过程方法在跟踪高斯过程后验的计算复杂度上仍存在负担，并且需要将优化问题划分为小区域以确保探索或假设一个潜在的低维结构。我们提出了一种基于马尔科夫链蒙特卡罗的新方法，通过将候选点转移到更有希望的位置来有效地从近似后验中采样。我们在高斯过程汤普森采样设置下提供了其收敛的理论保证。我们还通过实验表明，我们算法的Metropolis-Hastings版本和Langevin Dynamics版本在高维顺序优化和强化学习基准测试中均优于现有方法。",
    "tldr": "本文提出了一种基于马尔科夫链蒙特卡罗的方法，用于改进高维贝叶斯优化的样本效率。实验结果表明，该方法在高维顺序优化和强化学习任务中表现优于现有方法。",
    "en_tdlr": "This paper proposes a new method based on Markov Chain Monte Carlo to improve the sample efficiency of high dimensional Bayesian optimization. Experimental results show that the proposed method outperforms state-of-the-art methods in high-dimensional sequential optimization and reinforcement learning tasks."
}