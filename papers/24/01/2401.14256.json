{
    "title": "Producing Plankton Classifiers that are Robust to Dataset Shift. (arXiv:2401.14256v1 [cs.CV])",
    "abstract": "Modern plankton high-throughput monitoring relies on deep learning classifiers for species recognition in water ecosystems. Despite satisfactory nominal performances, a significant challenge arises from Dataset Shift, which causes performances to drop during deployment. In our study, we integrate the ZooLake dataset with manually-annotated images from 10 independent days of deployment, serving as test cells to benchmark Out-Of-Dataset (OOD) performances. Our analysis reveals instances where classifiers, initially performing well in In-Dataset conditions, encounter notable failures in practical scenarios. For example, a MobileNet with a 92% nominal test accuracy shows a 77% OOD accuracy. We systematically investigate conditions leading to OOD performance drops and propose a preemptive assessment method to identify potential pitfalls when classifying new data, and pinpoint features in OOD images that adversely impact classification. We present a three-step pipeline: (i) identifying OOD d",
    "link": "http://arxiv.org/abs/2401.14256",
    "context": "Title: Producing Plankton Classifiers that are Robust to Dataset Shift. (arXiv:2401.14256v1 [cs.CV])\nAbstract: Modern plankton high-throughput monitoring relies on deep learning classifiers for species recognition in water ecosystems. Despite satisfactory nominal performances, a significant challenge arises from Dataset Shift, which causes performances to drop during deployment. In our study, we integrate the ZooLake dataset with manually-annotated images from 10 independent days of deployment, serving as test cells to benchmark Out-Of-Dataset (OOD) performances. Our analysis reveals instances where classifiers, initially performing well in In-Dataset conditions, encounter notable failures in practical scenarios. For example, a MobileNet with a 92% nominal test accuracy shows a 77% OOD accuracy. We systematically investigate conditions leading to OOD performance drops and propose a preemptive assessment method to identify potential pitfalls when classifying new data, and pinpoint features in OOD images that adversely impact classification. We present a three-step pipeline: (i) identifying OOD d",
    "path": "papers/24/01/2401.14256.json",
    "total_tokens": 1037,
    "translated_title": "产生对数据集变化具有鲁棒性的浮游生物分类器",
    "translated_abstract": "现代浮游生物高通量监测依赖于深度学习分类器对水生态系统中的物种进行识别。尽管在理论性能方面令人满意，但数据集变化给部署过程中的性能带来了显著挑战。在我们的研究中，我们将ZooLake数据集与10个独立部署日的手动注释图像集成，作为测试单元，以评估超出数据集的性能。我们的分析揭示了分类器在理想情况下表现良好的情况下，在实际情景中遇到显著失败的实例。例如，一个具有92％标称测试准确度的MobileNet模型显示了77％的超出数据集准确度。我们系统地研究了导致超出数据集性能下降的条件，并提出了一种预防性评估方法，以识别对新数据进行分类时可能出现的问题，并确定影响分类的超出数据集图像中的特征。我们提出了一个三步流程：(i)识别超出数据集的实例，(ii)评估超出数据集性能的影响因素，(iii)预测新数据的分类准确性。",
    "tldr": "本研究针对浮游生物分类器在数据集变化下性能下降的挑战，通过整合不同部署日的数据集并评估超出数据集的性能，揭示了分类器在实际情景中遇到的失败情况。基于对导致性能下降的条件的研究，我们提出了一种预防性评估方法，用于识别新数据分类中可能出现的问题，并确定影响分类的特征。"
}