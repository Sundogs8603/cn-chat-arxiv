{
    "title": "BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving. (arXiv:2401.01065v1 [cs.CV])",
    "abstract": "The demand for the retrieval of complex scene data in autonomous driving is increasing, especially as passenger vehicles have been equipped with the ability to navigate urban settings, with the imperative to address long-tail scenarios. Meanwhile, under the pre-existing two dimensional image retrieval method, some problems may arise with scene retrieval, such as lack of global feature representation and subpar text retrieval ability. To address these issues, we have proposed \\textbf{BEV-CLIP}, the first multimodal Bird's-Eye View(BEV) retrieval methodology that utilizes descriptive text as an input to retrieve corresponding scenes. This methodology applies the semantic feature extraction abilities of a large language model (LLM) to facilitate zero-shot retrieval of extensive text descriptions, and incorporates semi-structured information from a knowledge graph to improve the semantic richness and variety of the language embedding. Our experiments result in 87.66% accuracy on NuScenes d",
    "link": "http://arxiv.org/abs/2401.01065",
    "context": "Title: BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving. (arXiv:2401.01065v1 [cs.CV])\nAbstract: The demand for the retrieval of complex scene data in autonomous driving is increasing, especially as passenger vehicles have been equipped with the ability to navigate urban settings, with the imperative to address long-tail scenarios. Meanwhile, under the pre-existing two dimensional image retrieval method, some problems may arise with scene retrieval, such as lack of global feature representation and subpar text retrieval ability. To address these issues, we have proposed \\textbf{BEV-CLIP}, the first multimodal Bird's-Eye View(BEV) retrieval methodology that utilizes descriptive text as an input to retrieve corresponding scenes. This methodology applies the semantic feature extraction abilities of a large language model (LLM) to facilitate zero-shot retrieval of extensive text descriptions, and incorporates semi-structured information from a knowledge graph to improve the semantic richness and variety of the language embedding. Our experiments result in 87.66% accuracy on NuScenes d",
    "path": "papers/24/01/2401.01065.json",
    "total_tokens": 882,
    "translated_title": "BEV-CLIP：用于自动驾驶中复杂场景的多模态BEV检索方法论",
    "translated_abstract": "随着乘用车辆具备在城市环境中导航的能力，自动驾驶中对复杂场景数据的检索需求不断增加，尤其是在处理长尾情景时。然而，在现有的二维图像检索方法下，存在一些场景检索的问题，例如全局特征表征不足和文本检索能力不佳。为了解决这些问题，我们提出了BEV-CLIP，这是第一个利用描述性文本作为输入来检索相应场景的多模态鸟瞰图检索方法。该方法利用大型语言模型 (LLM) 的语义特征提取能力，实现了对广泛文本描述的零样本检索，并结合知识图谱的半结构化信息，提高了语言嵌入的语义丰富性和多样性。实验结果表明，在NuScenes数据集上取得了87.66%的准确率。",
    "tldr": "BEV-CLIP是一种用于自动驾驶中复杂场景的多模态BEV检索方法，通过使用描述性文本进行检索，利用大型语言模型的 semantic feature extraction 和知识图谱的半结构化信息来提高检索准确性。"
}