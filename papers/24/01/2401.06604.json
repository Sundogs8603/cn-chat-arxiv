{
    "title": "Identifying Policy Gradient Subspaces. (arXiv:2401.06604v1 [cs.LG])",
    "abstract": "Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various simulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization.",
    "link": "http://arxiv.org/abs/2401.06604",
    "context": "Title: Identifying Policy Gradient Subspaces. (arXiv:2401.06604v1 [cs.LG])\nAbstract: Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various simulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization.",
    "path": "papers/24/01/2401.06604.json",
    "total_tokens": 799,
    "translated_title": "识别策略梯度子空间",
    "translated_abstract": "策略梯度方法在解决复杂的连续控制任务方面具有巨大的潜力。然而，通过利用优化问题内部的结构，可以提高其训练效率。最近的研究表明，通过利用梯度位于低维且缓慢变化子空间中的事实，可以加速监督学习。在本文中，我们对两种流行的深度策略梯度方法在各种模拟基准任务上进行了全面的评估。我们的结果表明，尽管强化学习固有的数据分布不断变化，但存在这样的梯度子空间。这些发现为未来更高效的强化学习工作，例如改进参数空间探索或实现二阶优化，提供了有希望的方向。",
    "tldr": "本文研究了两种深度策略梯度方法在不同模拟基准任务上的评估结果，发现尽管数据分布不断变化，但存在低维且缓慢变化的梯度子空间，这有助于未来更高效的强化学习工作。",
    "en_tdlr": "This paper evaluates two popular deep policy gradient methods on various simulated benchmark tasks, revealing the existence of low-dimensional and slowly-changing gradient subspaces, despite the continuously changing data distribution inherent to reinforcement learning, which provides promising directions for more efficient future work on reinforcement learning."
}