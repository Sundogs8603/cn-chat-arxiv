{
    "title": "Reinforcement Learning and Deep Stochastic Optimal Control for Final Quadratic Hedging. (arXiv:2401.08600v1 [q-fin.CP])",
    "abstract": "We consider two data driven approaches, Reinforcement Learning (RL) and Deep Trajectory-based Stochastic Optimal Control (DTSOC) for hedging a European call option without and with transaction cost according to a quadratic hedging P&L objective at maturity (\"variance-optimal hedging\" or \"final quadratic hedging\"). We study the performance of the two approaches under various market environments (modeled via the Black-Scholes and/or the log-normal SABR model) to understand their advantages and limitations. Without transaction costs and in the Black-Scholes model, both approaches match the performance of the variance-optimal Delta hedge. In the log-normal SABR model without transaction costs, they match the performance of the variance-optimal Barlett's Delta hedge. Agents trained on Black-Scholes trajectories with matching initial volatility but used on SABR trajectories match the performance of Bartlett's Delta hedge in average cost, but show substantially wider variance. To apply RL app",
    "link": "http://arxiv.org/abs/2401.08600",
    "context": "Title: Reinforcement Learning and Deep Stochastic Optimal Control for Final Quadratic Hedging. (arXiv:2401.08600v1 [q-fin.CP])\nAbstract: We consider two data driven approaches, Reinforcement Learning (RL) and Deep Trajectory-based Stochastic Optimal Control (DTSOC) for hedging a European call option without and with transaction cost according to a quadratic hedging P&L objective at maturity (\"variance-optimal hedging\" or \"final quadratic hedging\"). We study the performance of the two approaches under various market environments (modeled via the Black-Scholes and/or the log-normal SABR model) to understand their advantages and limitations. Without transaction costs and in the Black-Scholes model, both approaches match the performance of the variance-optimal Delta hedge. In the log-normal SABR model without transaction costs, they match the performance of the variance-optimal Barlett's Delta hedge. Agents trained on Black-Scholes trajectories with matching initial volatility but used on SABR trajectories match the performance of Bartlett's Delta hedge in average cost, but show substantially wider variance. To apply RL app",
    "path": "papers/24/01/2401.08600.json",
    "total_tokens": 978,
    "translated_title": "强化学习和深度随机最优控制在最终二次对冲中的应用",
    "translated_abstract": "我们考虑了两种数据驱动方法，强化学习（RL）和基于轨迹的深度随机最优控制（DTSOC），用于在没有和有交易成本的情况下对冲欧式看涨期权，根据二次对冲的利润和损失目标来衡量（“方差最优对冲”或“最终二次对冲”）。我们研究了这两种方法在不同的市场环境下（通过Black-Scholes模型和/或对数正态SABR模型建模）的表现，以了解它们的优势和局限性。在没有交易成本和Black-Scholes模型中，这两种方法与方差最优Delta对冲的表现相匹配。在没有交易成本和对数正态SABR模型中，它们与方差最优Bartlett's Delta对冲的表现相匹配。在具有匹配初始波动率的Black-Scholes轨迹上训练的代理模型在SABR轨迹上使用可以在平均成本上与Bartlett's Delta对冲的表现相匹配，但方差更大。为了应用强化学习方法，我们还引入了状态变量。",
    "tldr": "本文研究应用强化学习和深度随机最优控制方法在最终二次对冲中，通过实验发现这两种方法在不同市场环境下的表现优势和局限性，并提出了状态变量的解决方案。",
    "en_tdlr": "This paper investigates the application of reinforcement learning and deep stochastic optimal control methods for final quadratic hedging. The study examines the advantages and limitations of these approaches under various market environments and proposes a solution using state variables."
}