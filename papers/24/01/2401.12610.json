{
    "title": "The twin peaks of learning neural networks. (arXiv:2401.12610v1 [cs.LG])",
    "abstract": "Recent works demonstrated the existence of a double-descent phenomenon for the generalization error of neural networks, where highly overparameterized models escape overfitting and achieve good test performance, at odds with the standard bias-variance trade-off described by statistical learning theory. In the present work, we explore a link between this phenomenon and the increase of complexity and sensitivity of the function represented by neural networks. In particular, we study the Boolean mean dimension (BMD), a metric developed in the context of Boolean function analysis. Focusing on a simple teacher-student setting for the random feature model, we derive a theoretical analysis based on the replica method that yields an interpretable expression for the BMD, in the high dimensional regime where the number of data points, the number of features, and the input size grow to infinity. We find that, as the degree of overparameterization of the network is increased, the BMD reaches an ev",
    "link": "http://arxiv.org/abs/2401.12610",
    "context": "Title: The twin peaks of learning neural networks. (arXiv:2401.12610v1 [cs.LG])\nAbstract: Recent works demonstrated the existence of a double-descent phenomenon for the generalization error of neural networks, where highly overparameterized models escape overfitting and achieve good test performance, at odds with the standard bias-variance trade-off described by statistical learning theory. In the present work, we explore a link between this phenomenon and the increase of complexity and sensitivity of the function represented by neural networks. In particular, we study the Boolean mean dimension (BMD), a metric developed in the context of Boolean function analysis. Focusing on a simple teacher-student setting for the random feature model, we derive a theoretical analysis based on the replica method that yields an interpretable expression for the BMD, in the high dimensional regime where the number of data points, the number of features, and the input size grow to infinity. We find that, as the degree of overparameterization of the network is increased, the BMD reaches an ev",
    "path": "papers/24/01/2401.12610.json",
    "total_tokens": 1003,
    "translated_title": "学习神经网络的双峰现象",
    "translated_abstract": "最近的研究表明，在神经网络的泛化误差方面存在双峰现象，即高度过参数化的模型可以避免过拟合并实现良好的测试性能，与统计学习理论描述的标准偏差-方差折衷法则不符。在本研究中，我们探讨了这一现象与神经网络所表示的函数的复杂性和敏感性增加之间的联系。具体而言，我们研究了布尔均值维度（BMD），这是在布尔函数分析背景下发展起来的一种度量。针对随机特征模型的简单教师-学生设置，我们基于副本方法进行理论分析，得到了一个可解释的BMD表达式，其中数据点的数量、特征的数量和输入大小在高维度范围内不断增长。我们发现，随着网络过参数化程度的增加，BMD达到一个极值点。",
    "tldr": "该论文研究了神经网络的双峰现象，发现高度过参数化的模型可以避免过拟合并实现良好的测试性能，与传统的偏差-方差折衷法则不同。研究分析了布尔均值维度（BMD）与网络复杂性和敏感性之间的关系，得到了在高维度范围内BMD的可解释表达式，发现BMD在网络过参数化程度增加时达到极值点。",
    "en_tdlr": "This paper investigates the double-descent phenomenon of neural networks, revealing that highly overparameterized models can avoid overfitting and achieve good test performance, contrary to the traditional bias-variance trade-off. The study analyzes the relationship between the Boolean mean dimension (BMD) and the complexity and sensitivity of the networks, deriving an interpretable expression for BMD in the high-dimensional regime. Results show that BMD reaches an extremum as the degree of overparameterization of the network increases."
}