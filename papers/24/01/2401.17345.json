{
    "title": "Reproducibility, energy efficiency and performance of pseudorandom number generators in machine learning: a comparative study of python, numpy, tensorflow, and pytorch implementations",
    "abstract": "Pseudo-Random Number Generators (PRNGs) have become ubiquitous in machine learning technologies because they are interesting for numerous methods. The field of machine learning holds the potential for substantial advancements across various domains, as exemplified by recent breakthroughs in Large Language Models (LLMs). However, despite the growing interest, persistent concerns include issues related to reproducibility and energy consumption. Reproducibility is crucial for robust scientific inquiry and explainability, while energy efficiency underscores the imperative to conserve finite global resources. This study delves into the investigation of whether the leading Pseudo-Random Number Generators (PRNGs) employed in machine learning languages, libraries, and frameworks uphold statistical quality and numerical reproducibility when compared to the original C implementation of the respective PRNG algorithms. Additionally, we aim to evaluate the time efficiency and energy consumption of ",
    "link": "https://arxiv.org/abs/2401.17345",
    "context": "Title: Reproducibility, energy efficiency and performance of pseudorandom number generators in machine learning: a comparative study of python, numpy, tensorflow, and pytorch implementations\nAbstract: Pseudo-Random Number Generators (PRNGs) have become ubiquitous in machine learning technologies because they are interesting for numerous methods. The field of machine learning holds the potential for substantial advancements across various domains, as exemplified by recent breakthroughs in Large Language Models (LLMs). However, despite the growing interest, persistent concerns include issues related to reproducibility and energy consumption. Reproducibility is crucial for robust scientific inquiry and explainability, while energy efficiency underscores the imperative to conserve finite global resources. This study delves into the investigation of whether the leading Pseudo-Random Number Generators (PRNGs) employed in machine learning languages, libraries, and frameworks uphold statistical quality and numerical reproducibility when compared to the original C implementation of the respective PRNG algorithms. Additionally, we aim to evaluate the time efficiency and energy consumption of ",
    "path": "papers/24/01/2401.17345.json",
    "total_tokens": 896,
    "translated_title": "机器学习中伪随机数生成器的可重复性、能效和性能：对Python、NumPy、TensorFlow和PyTorch实现的比较研究",
    "translated_abstract": "伪随机数生成器(PRNGs)在机器学习技术中已经无处不在，因为它们在众多方法中都非常有意思。机器学习领域有着巨大的潜力，在各个领域中取得突破性进展，比如最近在大型语言模型(LLMs)中的突破。然而，尽管越来越受关注，但仍然存在一些持续关注的问题，包括与可重复性和能源消耗相关的问题。可重复性对于强大的科学研究和可解释性至关重要，而能效则强调了保护有限全球资源的必要性。本研究深入探讨了机器学习语言、库和框架中最主要的伪随机数生成器(PRNGs)与各自算法原始C实现相比，在统计质量和数值可重复性方面的表现。此外，我们还旨在评估时间效率和能源消耗。",
    "tldr": "本研究比较了机器学习中常用的伪随机数生成器在统计质量、数值可重复性、时间效率和能源消耗等方面与原始C实现的差异。"
}