{
    "title": "Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable and Transferable Bandwidth Allocation. (arXiv:2401.10253v1 [cs.NI])",
    "abstract": "In this paper, we develop a deep learning-based bandwidth allocation policy that is: 1) scalable with the number of users and 2) transferable to different communication scenarios, such as non-stationary wireless channels, different quality-of-service (QoS) requirements, and dynamically available resources. To support scalability, the bandwidth allocation policy is represented by a graph neural network (GNN), with which the number of training parameters does not change with the number of users. To enable the generalization of the GNN, we develop a hybrid-task meta-learning (HML) algorithm that trains the initial parameters of the GNN with different communication scenarios during meta-training. Next, during meta-testing, a few samples are used to fine-tune the GNN with unseen communication scenarios. Simulation results demonstrate that our HML approach can improve the initial performance by $8.79\\%$, and sampling efficiency by $73\\%$, compared with existing benchmarks. After fine-tuning,",
    "link": "http://arxiv.org/abs/2401.10253",
    "context": "Title: Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable and Transferable Bandwidth Allocation. (arXiv:2401.10253v1 [cs.NI])\nAbstract: In this paper, we develop a deep learning-based bandwidth allocation policy that is: 1) scalable with the number of users and 2) transferable to different communication scenarios, such as non-stationary wireless channels, different quality-of-service (QoS) requirements, and dynamically available resources. To support scalability, the bandwidth allocation policy is represented by a graph neural network (GNN), with which the number of training parameters does not change with the number of users. To enable the generalization of the GNN, we develop a hybrid-task meta-learning (HML) algorithm that trains the initial parameters of the GNN with different communication scenarios during meta-training. Next, during meta-testing, a few samples are used to fine-tune the GNN with unseen communication scenarios. Simulation results demonstrate that our HML approach can improve the initial performance by $8.79\\%$, and sampling efficiency by $73\\%$, compared with existing benchmarks. After fine-tuning,",
    "path": "papers/24/01/2401.10253.json",
    "total_tokens": 933,
    "translated_title": "混合任务元学习：一种用于可扩展和可转移带宽分配的图神经网络方法",
    "translated_abstract": "本文提出了一种基于深度学习的带宽分配策略，该策略具有以下特点：1）随着用户数量的增加具有可扩展性；2）能够在不同的通信场景下进行转移，例如非平稳的无线信道、不同的服务质量要求和动态可用资源。为了支持可扩展性，带宽分配策略采用了图神经网络（GNN）进行表示，训练参数的数量随用户数量的增加而不变。为了实现GNN的泛化能力，我们开发了一种混合任务元学习（HML）算法，在元训练过程中使用不同的通信场景来训练GNN的初始参数。然后，在元测试过程中，使用少量样本对GNN进行微调以适应未见过的通信场景。仿真结果表明，与现有基准相比，我们的HML方法可以将初始性能提高8.79％，并提高采样效率73％。在微调后，",
    "tldr": "本文提出了一种基于图神经网络的混合任务元学习算法，用于可扩展和可转移的带宽分配。通过引入GNN和HML算法，该方法在不同的通信场景下具有较好的性能和采样效率。"
}