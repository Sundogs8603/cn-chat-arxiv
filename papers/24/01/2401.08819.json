{
    "title": "Learning from Sparse Offline Datasets via Conservative Density Estimation. (arXiv:2401.08819v1 [cs.LG])",
    "abstract": "Offline reinforcement learning (RL) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution (OOD) extrapolation errors, especially in sparse reward or scarce data settings. In this paper, we propose a novel training algorithm called Conservative Density Estimation (CDE), which addresses this challenge by explicitly imposing constraints on the state-action occupancy stationary distribution. CDE overcomes the limitations of existing approaches, such as the stationary distribution correction method, by addressing the support mismatch issue in marginal importance sampling. Our method achieves state-of-the-art performance on the D4RL benchmark. Notably, CDE consistently outperforms baselines in challenging tasks with sparse rewards or insufficient data, demonstrating the advantages of our approach in addressing the extrapolation error problem i",
    "link": "http://arxiv.org/abs/2401.08819",
    "context": "Title: Learning from Sparse Offline Datasets via Conservative Density Estimation. (arXiv:2401.08819v1 [cs.LG])\nAbstract: Offline reinforcement learning (RL) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution (OOD) extrapolation errors, especially in sparse reward or scarce data settings. In this paper, we propose a novel training algorithm called Conservative Density Estimation (CDE), which addresses this challenge by explicitly imposing constraints on the state-action occupancy stationary distribution. CDE overcomes the limitations of existing approaches, such as the stationary distribution correction method, by addressing the support mismatch issue in marginal importance sampling. Our method achieves state-of-the-art performance on the D4RL benchmark. Notably, CDE consistently outperforms baselines in challenging tasks with sparse rewards or insufficient data, demonstrating the advantages of our approach in addressing the extrapolation error problem i",
    "path": "papers/24/01/2401.08819.json",
    "total_tokens": 919,
    "translated_title": "通过保守密度估计从稀疏离线数据集中学习",
    "translated_abstract": "离线强化学习（RL）为从预先收集的数据集中学习策略提供了一种有前景的方向，而无需与环境进一步交互。然而，现有的方法在处理分布外（OOD）外推错误方面存在困难，特别是在稀疏奖励或数据稀缺的情况下。在本文中，我们提出了一种名为保守密度估计（CDE）的新的训练算法，通过明确约束状态-行为占据稳态分布来解决这个挑战。CDE通过解决边际重要性抽样中的支持不匹配问题，克服了现有方法的局限性，如稳态分布校正方法。我们的方法在D4RL基准测试中实现了最先进的性能。值得注意的是，CDE在具有稀疏奖励或不足数据的挑战性任务中持续优于基准方法，证明了我们的方法在解决外推错误问题上的优势。",
    "tldr": "本文提出了一种名为保守密度估计（CDE）的训练算法，通过明确约束状态-行为占据稳态分布来解决离线强化学习中的外推错误问题。在稀疏奖励或不足数据的任务中，CDE显示出明显优于基准方法的性能。",
    "en_tdlr": "This paper proposes a training algorithm called Conservative Density Estimation (CDE), which explicitly imposes constraints on the state-action occupancy stationary distribution to address the extrapolation error problem in offline reinforcement learning. CDE consistently outperforms baselines in tasks with sparse rewards or insufficient data, demonstrating its superiority."
}