{
    "title": "Recovering Mental Representations from Large Language Models with Markov Chain Monte Carlo. (arXiv:2401.16657v1 [cs.AI])",
    "abstract": "Simulating sampling algorithms with people has proven a useful method for efficiently probing and understanding their mental representations. We propose that the same methods can be used to study the representations of Large Language Models (LLMs). While one can always directly prompt either humans or LLMs to disclose their mental representations introspectively, we show that increased efficiency can be achieved by using LLMs as elements of a sampling algorithm. We explore the extent to which we recover human-like representations when LLMs are interrogated with Direct Sampling and Markov chain Monte Carlo (MCMC). We found a significant increase in efficiency and performance using adaptive sampling algorithms based on MCMC. We also highlight the potential of our method to yield a more general method of conducting Bayesian inference \\textit{with} LLMs.",
    "link": "http://arxiv.org/abs/2401.16657",
    "context": "Title: Recovering Mental Representations from Large Language Models with Markov Chain Monte Carlo. (arXiv:2401.16657v1 [cs.AI])\nAbstract: Simulating sampling algorithms with people has proven a useful method for efficiently probing and understanding their mental representations. We propose that the same methods can be used to study the representations of Large Language Models (LLMs). While one can always directly prompt either humans or LLMs to disclose their mental representations introspectively, we show that increased efficiency can be achieved by using LLMs as elements of a sampling algorithm. We explore the extent to which we recover human-like representations when LLMs are interrogated with Direct Sampling and Markov chain Monte Carlo (MCMC). We found a significant increase in efficiency and performance using adaptive sampling algorithms based on MCMC. We also highlight the potential of our method to yield a more general method of conducting Bayesian inference \\textit{with} LLMs.",
    "path": "papers/24/01/2401.16657.json",
    "total_tokens": 860,
    "translated_title": "从大型语言模型中通过马尔可夫链蒙特卡洛恢复心智表示",
    "translated_abstract": "通过模拟采样算法进行人类研究已被证明是一种有效的方法，可以高效地探索和理解其心智表示。我们提出相同的方法可以用来研究大型语言模型（LLMs）的表示。虽然人类或LLMs都可以通过内省的方式直接揭示其心智表示，但我们表明，使用LLMs作为一种采样算法的元素可以提高效率。我们探索了在使用直接采样和马尔可夫链蒙特卡洛（MCMC）进行LLMs插问时恢复人类化表示程度的程度。我们发现使用基于MCMC的自适应采样算法可以显著提高效率和性能。我们还强调了我们的方法潜力，可以产生一种更通用的使用LLMs进行贝叶斯推理的方法。",
    "tldr": "本文研究了使用马尔可夫链蒙特卡洛（MCMC）方法从大型语言模型中恢复心智表示的方法，并且发现使用基于MCMC的自适应采样算法可以显著提高效率和性能，这对于进行贝叶斯推理具有潜在意义。",
    "en_tdlr": "This paper investigates the method of recovering mental representations from large language models using Markov chain Monte Carlo (MCMC) and finds that adaptive sampling algorithms based on MCMC significantly improve efficiency and performance, which has potential implications for Bayesian inference."
}