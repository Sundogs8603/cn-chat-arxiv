{
    "title": "Step-size Optimization for Continual Learning",
    "abstract": "In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitat",
    "link": "https://arxiv.org/abs/2401.17401",
    "context": "Title: Step-size Optimization for Continual Learning\nAbstract: In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitat",
    "path": "papers/24/01/2401.17401.json",
    "total_tokens": 868,
    "translated_title": "连续学习的步长优化",
    "translated_abstract": "在连续学习中，学习者需要在整个生命周期内不断学习数据。一个关键问题是决定要保留什么知识和放弃什么知识。在神经网络中，可以通过使用步长向量来缩放梯度样本对网络权重的改变程度来实现。常见的算法，如RMSProp和Adam，使用启发式方法，特别是标准化，来适应这个步长向量。在本文中，我们展示了这些启发式方法忽视了它们对整体目标函数的适应效果，例如将步长向量远离更好的步长向量。另一方面，像IDBD（Sutton，1992）这样的随机元梯度下降算法，明确地针对整体目标函数优化步长向量。在简单问题上，我们展示了IDBD能够持续改善步长向量，而RMSProp和Adam则不行。我们解释了这两种方法以及它们各自的局限性之间的差异。",
    "tldr": "本文研究了连续学习中的步长优化问题，指出传统算法忽视了对整体目标函数的影响，而随机元梯度下降算法能够明确优化步长向量，在简单问题中表现更优。"
}