{
    "title": "VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE. (arXiv:2401.11110v1 [cs.CV])",
    "abstract": "Unsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present VONet, an innovative approach that is inspired by MONet. While utilizing a U-Net architecture, VONet employs an efficient and effective parallel attention inference process, generating attention masks for all slots simultaneously. Additionally, to enhance the temporal consistency of each mask across consecutive video frames, VONet develops an object-wise sequential VAE framework. The integration of these innovative encoder-side techniques, in conjunction with an expressive transformer-based decoder, establishes VONet as the leading unsupervised method for object learning across five MOVI datasets, encompassing videos of diverse complexities. Code is available at https://github.com/hnyu/vonet.",
    "link": "http://arxiv.org/abs/2401.11110",
    "context": "Title: VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE. (arXiv:2401.11110v1 [cs.CV])\nAbstract: Unsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present VONet, an innovative approach that is inspired by MONet. While utilizing a U-Net architecture, VONet employs an efficient and effective parallel attention inference process, generating attention masks for all slots simultaneously. Additionally, to enhance the temporal consistency of each mask across consecutive video frames, VONet develops an object-wise sequential VAE framework. The integration of these innovative encoder-side techniques, in conjunction with an expressive transformer-based decoder, establishes VONet as the leading unsupervised method for object learning across five MOVI datasets, encompassing videos of diverse complexities. Code is available at https://github.com/hnyu/vonet.",
    "path": "papers/24/01/2401.11110.json",
    "total_tokens": 838,
    "translated_title": "VONet: 无监督视频对象学习通过并行U-Net注意力和逐个对象的序列VAE",
    "translated_abstract": "无监督的视频对象学习旨在将视频场景分解为结构化的对象表示，无需深度、光流或分割的监督。我们提出了VONet，这是一种受MONet启发的创新方法。VONet利用U-Net架构，采用高效且有效的并行注意力推理过程，同时为所有槽生成注意力掩码。此外，为了增强每个掩码在连续视频帧中的时间一致性，VONet开发了一种逐个对象的序列VAE框架。这些创新的编码器端技术与表达性强的基于Transformer的解码器的整合，使VONet成为涵盖了各种复杂性视频的五个MOVI数据集上领先的无监督对象学习方法。代码可在https://github.com/hnyu/vonet找到。",
    "tldr": "VONet是一种无监督视频对象学习方法，通过并行U-Net注意力和逐个对象的序列VAE实现，具有高效且有效的推理过程和提升时间一致性的特点，成为五个MOVI数据集上领先的方法。",
    "en_tdlr": "VONet is an unsupervised video object learning method that utilizes parallel U-Net attention and object-wise sequential VAE, with efficient and effective inference process and enhanced temporal consistency. It is the leading method across five MOVI datasets."
}