{
    "title": "Red Teaming Visual Language Models. (arXiv:2401.12915v1 [cs.AI])",
    "abstract": "VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RT",
    "link": "http://arxiv.org/abs/2401.12915",
    "context": "Title: Red Teaming Visual Language Models. (arXiv:2401.12915v1 [cs.AI])\nAbstract: VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language Models) to accept multimodal inputs. Since it has been verified that LLMs can be induced to generate harmful or inaccurate content through specific test cases (termed as Red Teaming), how VLMs perform in similar scenarios, especially with their combination of textual and visual inputs, remains a question. To explore this problem, we present a novel red teaming dataset RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness, privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to benchmark current VLMs in terms of these 4 different aspects. Detailed analysis shows that 10 prominent open-sourced VLMs struggle with the red teaming in different degrees and have up to 31% performance gap with GPT-4V. Additionally, we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning (SFT) using RT",
    "path": "papers/24/01/2401.12915.json",
    "total_tokens": 994,
    "translated_title": "红队针对视觉语言模型的研究",
    "translated_abstract": "视觉语言模型(VLMs)扩展了大型语言模型(LLMs)接受多模态输入的能力。由于已经验证LLMs可以通过特定测试用例产生有害或不准确的内容(称为红队行动)，VLMs在类似场景中的表现如何，特别是在文本和视觉输入的组合中，仍然是一个问题。为了探索这个问题，我们提供了一个新颖的红队数据集RTVLM，包含4个主要方面(忠实度、隐私、安全、公平)下的10个子任务(如图像误导、多模态越狱、脸部公平等)。我们的RTVLM是第一个从这四个不同方面评估当前VLMs的红队数据集。详细分析表明，10个知名的开放源代码VLMs在红队行动中遇到不同程度的困难，并且与GPT-4V相比，性能差距高达31%。此外，我们还使用RT简单地将红队行动对齐到LLaVA-v1.5上，使用监督微调(SFT)。",
    "tldr": "本论文通过提供一个新颖的红队数据集RTVLM，评估了当前视觉语言模型VLMs在忠实度、隐私、安全和公平性方面的性能，并与GPT-4V进行了比较。结果显示，10个知名的VLMs在红队行动中存在不同程度的困难。",
    "en_tdlr": "This paper evaluates the performance of current Vision-Language Models (VLMs) in terms of faithfulness, privacy, safety, and fairness, by providing a novel red teaming dataset RTVLM and comparing them to GPT-4V. The results show that 10 prominent VLMs struggle to perform well in red teaming scenarios."
}