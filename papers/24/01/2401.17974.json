{
    "title": "GUMsley: Evaluating Entity Salience in Summarization for 12 English Genres",
    "abstract": "As NLP models become increasingly capable of understanding documents in terms of coherent entities rather than strings, obtaining the most salient entities for each document is not only an important end task in itself but also vital for Information Retrieval (IR) and other downstream applications such as controllable summarization. In this paper, we present and evaluate GUMsley, the first entity salience dataset covering all named and non-named salient entities for 12 genres of English text, aligned with entity types, Wikification links and full coreference resolution annotations. We promote a strict definition of salience using human summaries and demonstrate high inter-annotator agreement for salience based on whether a source entity is mentioned in the summary. Our evaluation shows poor performance by pre-trained SOTA summarization models and zero-shot LLM prompting in capturing salient entities in generated summaries. We also show that predicting or providing salient entities to se",
    "link": "https://arxiv.org/abs/2401.17974",
    "context": "Title: GUMsley: Evaluating Entity Salience in Summarization for 12 English Genres\nAbstract: As NLP models become increasingly capable of understanding documents in terms of coherent entities rather than strings, obtaining the most salient entities for each document is not only an important end task in itself but also vital for Information Retrieval (IR) and other downstream applications such as controllable summarization. In this paper, we present and evaluate GUMsley, the first entity salience dataset covering all named and non-named salient entities for 12 genres of English text, aligned with entity types, Wikification links and full coreference resolution annotations. We promote a strict definition of salience using human summaries and demonstrate high inter-annotator agreement for salience based on whether a source entity is mentioned in the summary. Our evaluation shows poor performance by pre-trained SOTA summarization models and zero-shot LLM prompting in capturing salient entities in generated summaries. We also show that predicting or providing salient entities to se",
    "path": "papers/24/01/2401.17974.json",
    "total_tokens": 946,
    "translated_title": "GUMsley：评估英语12种流派中的摘要中的实体显著性",
    "translated_abstract": "随着自然语言处理模型在理解文档方面变得越来越能够以连贯的实体而不是字符串的形式，获取每个文档中最显著的实体不仅是一个重要的最终任务，而且对于信息检索（IR）和其他下游应用，如可控制的摘要，也至关重要。本文提出并评估了GUMsley，这是第一个涵盖英语文本12种流派的所有命名和非命名显著实体的实体显著性数据集，与实体类型、维基化链接和完整的核心引用解析对齐。我们使用人工摘要提倡了对显著性的严格定义，并展示了基于摘要中是否提到源实体的显著性的高度标注者间一致性。我们的评估结果显示，预训练的最先进摘要模型和零-shot语言模型在生成摘要时捕捉到显著实体的表现差。我们还表明，在生成摘要时预测或提供显著实体对提高实体显著性有益。",
    "tldr": "GUMsley是第一个覆盖12种英文文本流派中所有命名和非命名显著实体的实体显著性数据集，并证明了当前最先进的摘要模型在捕捉生成摘要中的显著实体方面表现不佳。",
    "en_tdlr": "GUMsley is the first entity salience dataset covering all named and non-named salient entities for 12 genres of English text, and it demonstrates poor performance by current state-of-the-art summarization models in capturing salient entities in generated summaries."
}