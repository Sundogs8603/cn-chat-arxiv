{
    "title": "Gradient-Based Language Model Red Teaming. (arXiv:2401.16656v1 [cs.CL])",
    "abstract": "Red teaming is a common strategy for identifying weaknesses in generative language models (LMs), where adversarial prompts are produced that trigger an LM to generate unsafe responses. Red teaming is instrumental for both model alignment and evaluation, but is labor-intensive and difficult to scale when done by humans. In this paper, we present Gradient-Based Red Teaming (GBRT), a red teaming method for automatically generating diverse prompts that are likely to cause an LM to output unsafe responses. GBRT is a form of prompt learning, trained by scoring an LM response with a safety classifier and then backpropagating through the frozen safety classifier and LM to update the prompt. To improve the coherence of input prompts, we introduce two variants that add a realism loss and fine-tune a pretrained model to generate the prompts instead of learning the prompts directly. Our experiments show that GBRT is more effective at finding prompts that trigger an LM to generate unsafe responses ",
    "link": "http://arxiv.org/abs/2401.16656",
    "context": "Title: Gradient-Based Language Model Red Teaming. (arXiv:2401.16656v1 [cs.CL])\nAbstract: Red teaming is a common strategy for identifying weaknesses in generative language models (LMs), where adversarial prompts are produced that trigger an LM to generate unsafe responses. Red teaming is instrumental for both model alignment and evaluation, but is labor-intensive and difficult to scale when done by humans. In this paper, we present Gradient-Based Red Teaming (GBRT), a red teaming method for automatically generating diverse prompts that are likely to cause an LM to output unsafe responses. GBRT is a form of prompt learning, trained by scoring an LM response with a safety classifier and then backpropagating through the frozen safety classifier and LM to update the prompt. To improve the coherence of input prompts, we introduce two variants that add a realism loss and fine-tune a pretrained model to generate the prompts instead of learning the prompts directly. Our experiments show that GBRT is more effective at finding prompts that trigger an LM to generate unsafe responses ",
    "path": "papers/24/01/2401.16656.json",
    "total_tokens": 904,
    "translated_title": "基于梯度的语言模型红队测试",
    "translated_abstract": "红队测试是一种常见的策略，用于识别生成式语言模型中的弱点，通过产生对抗性提示来触发语言模型生成不安全的回应。红队测试对于模型对齐和评估都非常重要，但由于需要人工操作，劳动强度大且难以扩展。本文提出了一种基于梯度的红队测试（GBRT）方法，用于自动生成多样化的提示，能够引起语言模型输出不安全的回应。GBRT是一种提示学习方法，通过使用安全分类器对语言模型的响应进行评分，并通过冻结的安全分类器和语言模型进行反向传播来更新提示。为了提高输入提示的连贯性，我们引入了两个变种，通过添加真实性损失和微调预训练模型来生成提示，而不是直接学习提示。实验证明，GBRT能够更有效地找到能够触发语言模型生成不安全回应的提示。",
    "tldr": "本文介绍了一种基于梯度的语言模型红队测试（GBRT）方法，通过自动生成的多样化提示来触发语言模型生成不安全回应。通过使用安全分类器评分和反向传播来更新提示，GBRT在发现触发语言模型生成不安全回应的提示方面更加有效。",
    "en_tdlr": "This paper presents Gradient-Based Red Teaming (GBRT), a method for automatically generating diverse prompts to trigger unsafe responses from language models. By scoring and updating prompts using a safety classifier, GBRT is more effective in finding prompts that generate unsafe responses from language models."
}