{
    "title": "Optimal Data Splitting in Distributed Optimization for Machine Learning",
    "abstract": "arXiv:2401.07809v2 Announce Type: replace-cross  Abstract: The distributed optimization problem has become increasingly relevant recently. It has a lot of advantages such as processing a large amount of data in less time compared to non-distributed methods. However, most distributed approaches suffer from a significant bottleneck - the cost of communications. Therefore, a large amount of research has recently been directed at solving this problem. One such approach uses local data similarity. In particular, there exists an algorithm provably optimally exploiting the similarity property. But this result, as well as results from other works solve the communication bottleneck by focusing only on the fact that communication is significantly more expensive than local computing and does not take into account the various capacities of network devices and the different relationship between communication time and local computing expenses. We consider this setup and the objective of this study i",
    "link": "https://arxiv.org/abs/2401.07809",
    "context": "Title: Optimal Data Splitting in Distributed Optimization for Machine Learning\nAbstract: arXiv:2401.07809v2 Announce Type: replace-cross  Abstract: The distributed optimization problem has become increasingly relevant recently. It has a lot of advantages such as processing a large amount of data in less time compared to non-distributed methods. However, most distributed approaches suffer from a significant bottleneck - the cost of communications. Therefore, a large amount of research has recently been directed at solving this problem. One such approach uses local data similarity. In particular, there exists an algorithm provably optimally exploiting the similarity property. But this result, as well as results from other works solve the communication bottleneck by focusing only on the fact that communication is significantly more expensive than local computing and does not take into account the various capacities of network devices and the different relationship between communication time and local computing expenses. We consider this setup and the objective of this study i",
    "path": "papers/24/01/2401.07809.json",
    "total_tokens": 756,
    "translated_title": "机器学习中的分布式优化中的最佳数据分裂",
    "translated_abstract": "分布式优化问题近来变得越来越重要。与非分布式方法相比，它具有处理大量数据所需时间较短等许多优势。然而，大多数分布式方法存在一个显著瓶颈 - 通信成本。因此，最近大量研究致力于解决这个问题。一种方法利用本地数据相似性。特别是有一种算法可以证明地利用相似性属性。但是，这一结果以及其他作品的结果都是通过专注于通信明显比本地计算更昂贵这一事实而解决了通信瓶颈，并未考虑到网络设备的各种容量以及通信时间和本地计算费用之间的不同关系。我们考虑了这一设置，本研究的目标是探讨这个问题的一个新视角。",
    "tldr": "分布式优化问题中，提出了一种利用本地数据相似性的算法，可有效解决通信成本瓶颈。",
    "en_tdlr": "A novel algorithm utilizing local data similarity is proposed in distributed optimization to effectively address the bottleneck of communication costs."
}