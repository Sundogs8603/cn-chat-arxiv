{
    "title": "On the Stochastic (Variance-Reduced) Proximal Gradient Method for Regularized Expected Reward Optimization. (arXiv:2401.12508v1 [cs.LG])",
    "abstract": "We consider a regularized expected reward optimization problem in the non-oblivious setting that covers many existing problems in reinforcement learning (RL). In order to solve such an optimization problem, we apply and analyze the classical stochastic proximal gradient method. In particular, the method has shown to admit an $O(\\epsilon^{-4})$ sample complexity to an $\\epsilon$-stationary point, under standard conditions. Since the variance of the classical stochastic gradient estimator is typically large which slows down the convergence, we also apply an efficient stochastic variance-reduce proximal gradient method with an importance sampling based ProbAbilistic Gradient Estimator (PAGE). To the best of our knowledge, the application of this method represents a novel approach in addressing the general regularized reward optimization problem. Our analysis shows that the sample complexity can be improved from $O(\\epsilon^{-4})$ to $O(\\epsilon^{-3})$ under additional conditions. Our resu",
    "link": "http://arxiv.org/abs/2401.12508",
    "context": "Title: On the Stochastic (Variance-Reduced) Proximal Gradient Method for Regularized Expected Reward Optimization. (arXiv:2401.12508v1 [cs.LG])\nAbstract: We consider a regularized expected reward optimization problem in the non-oblivious setting that covers many existing problems in reinforcement learning (RL). In order to solve such an optimization problem, we apply and analyze the classical stochastic proximal gradient method. In particular, the method has shown to admit an $O(\\epsilon^{-4})$ sample complexity to an $\\epsilon$-stationary point, under standard conditions. Since the variance of the classical stochastic gradient estimator is typically large which slows down the convergence, we also apply an efficient stochastic variance-reduce proximal gradient method with an importance sampling based ProbAbilistic Gradient Estimator (PAGE). To the best of our knowledge, the application of this method represents a novel approach in addressing the general regularized reward optimization problem. Our analysis shows that the sample complexity can be improved from $O(\\epsilon^{-4})$ to $O(\\epsilon^{-3})$ under additional conditions. Our resu",
    "path": "papers/24/01/2401.12508.json",
    "total_tokens": 953,
    "translated_title": "关于正则化预期奖励优化的随机（方差减小）近端梯度法",
    "translated_abstract": "我们考虑在非明显设置中的正则化预期奖励优化问题，该问题涵盖了强化学习中的许多现有问题。为了解决这样的优化问题，我们应用和分析了经典的随机近端梯度法。具体而言，该方法已经证明在标准条件下，可以达到$O(\\epsilon^{-4})$的样本复杂度来寻找$\\epsilon$-稳定点。由于经典随机梯度估计器的方差通常较大，导致收敛速度较慢，因此我们还应用了一种高效的随机方差减小近端梯度法，其中采用了基于重要性抽样的概率梯度估计器 (PAGE)。据我们所知，这种方法的应用代表了在解决一般的正则化奖励优化问题上的一种新方法。我们的分析表明，在额外条件下，样本复杂度可以从$O(\\epsilon^{-4})$提高到$O(\\epsilon^{-3})$。",
    "tldr": "本文介绍了一种用于正则化预期奖励优化问题的随机近端梯度法，该方法通过引入方差减小的技术以提高收敛速度。实验结果表明，在满足一定条件的情况下，该方法的样本复杂度可以达到$O(\\epsilon^{-3})$。",
    "en_tdlr": "This paper presents a stochastic proximal gradient method for regularized expected reward optimization problems, which employs variance reduction techniques to improve convergence speed. Experimental results demonstrate that, under certain conditions, the sample complexity of the method can achieve $O(\\epsilon^{-3})$."
}