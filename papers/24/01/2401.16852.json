{
    "title": "Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess. (arXiv:2401.16852v1 [cs.LG])",
    "abstract": "This paper presents a new approach that integrates deep learning with computational chess, using both the Mixture of Experts (MoE) method and Monte-Carlo Tree Search (MCTS). Our methodology employs a suite of specialized models, each designed to respond to specific changes in the game's input data. This results in a framework with sparsely activated models, which provides significant computational benefits. Our framework combines the MoE method with MCTS, in order to align it with the strategic phases of chess, thus departing from the conventional ``one-for-all'' model. Instead, we utilize distinct game phase definitions to effectively distribute computational tasks across multiple expert neural networks. Our empirical research shows a substantial improvement in playing strength, surpassing the traditional single-model framework. This validates the efficacy of our integrated approach and highlights the potential of incorporating expert knowledge and strategic principles into neural net",
    "link": "http://arxiv.org/abs/2401.16852",
    "context": "Title: Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess. (arXiv:2401.16852v1 [cs.LG])\nAbstract: This paper presents a new approach that integrates deep learning with computational chess, using both the Mixture of Experts (MoE) method and Monte-Carlo Tree Search (MCTS). Our methodology employs a suite of specialized models, each designed to respond to specific changes in the game's input data. This results in a framework with sparsely activated models, which provides significant computational benefits. Our framework combines the MoE method with MCTS, in order to align it with the strategic phases of chess, thus departing from the conventional ``one-for-all'' model. Instead, we utilize distinct game phase definitions to effectively distribute computational tasks across multiple expert neural networks. Our empirical research shows a substantial improvement in playing strength, surpassing the traditional single-model framework. This validates the efficacy of our integrated approach and highlights the potential of incorporating expert knowledge and strategic principles into neural net",
    "path": "papers/24/01/2401.16852.json",
    "total_tokens": 941,
    "translated_title": "通过多种方式，将混合专家与MCTS相结合以提高国际象棋中的校验",
    "translated_abstract": "本文提出了一种新的方法，将深度学习与计算机棋盘相结合，同时使用混合专家方法和蒙特卡罗树搜索方法。我们的方法采用一套专门设计的模型，每个模型都针对游戏输入数据的特定变化做出响应。这导致了一个稀疏激活模型的框架，提供了显著的计算优势。我们的框架将混合专家方法与蒙特卡罗树搜索方法结合起来，以使其与国际象棋的战略阶段相一致，从而摆脱传统的“一刀切”的模型。相反，我们利用不同的游戏阶段定义，将计算任务有效地分配给多个专家神经网络。我们的实证研究显示，在游戏实力方面有了显著改进，超过了传统的单模型框架。这证实了我们集成方法的功效，并凸显了将专家知识和战略原则纳入神经网络中的潜力。",
    "tldr": "通过将混合专家方法和MCTS相结合，本研究在国际象棋中显著提升了下棋水平，验证了集成方法的有效性并展示了融入专家知识和战略原则到神经网络中的潜力。",
    "en_tdlr": "By combining the Mixture of Experts (MoE) method with Monte-Carlo Tree Search (MCTS), this study successfully improves chess playing strength. It validates the efficacy of the integrated approach and demonstrates the potential of incorporating expert knowledge and strategic principles into neural networks."
}