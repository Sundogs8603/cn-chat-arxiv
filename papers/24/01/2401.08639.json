{
    "title": "One-Step Diffusion Distillation via Deep Equilibrium Models. (arXiv:2401.08639v1 [cs.CV])",
    "abstract": "Diffusion models excel at producing high-quality samples but naively require hundreds of iterations, prompting multiple attempts to distill the generation process into a faster network. However, many existing approaches suffer from a variety of challenges: the process for distillation training can be complex, often requiring multiple training stages, and the resulting models perform poorly when utilized in single-step generative applications. In this paper, we introduce a simple yet effective means of distilling diffusion models directly from initial noise to the resulting image. Of particular importance to our approach is to leverage a new Deep Equilibrium (DEQ) model as the distilled architecture: the Generative Equilibrium Transformer (GET). Our method enables fully offline training with just noise/image pairs from the diffusion model while achieving superior performance compared to existing one-step methods on comparable training budgets. We demonstrate that the DEQ architecture is",
    "link": "http://arxiv.org/abs/2401.08639",
    "context": "Title: One-Step Diffusion Distillation via Deep Equilibrium Models. (arXiv:2401.08639v1 [cs.CV])\nAbstract: Diffusion models excel at producing high-quality samples but naively require hundreds of iterations, prompting multiple attempts to distill the generation process into a faster network. However, many existing approaches suffer from a variety of challenges: the process for distillation training can be complex, often requiring multiple training stages, and the resulting models perform poorly when utilized in single-step generative applications. In this paper, we introduce a simple yet effective means of distilling diffusion models directly from initial noise to the resulting image. Of particular importance to our approach is to leverage a new Deep Equilibrium (DEQ) model as the distilled architecture: the Generative Equilibrium Transformer (GET). Our method enables fully offline training with just noise/image pairs from the diffusion model while achieving superior performance compared to existing one-step methods on comparable training budgets. We demonstrate that the DEQ architecture is",
    "path": "papers/24/01/2401.08639.json",
    "total_tokens": 818,
    "translated_title": "一步扩散蒸馏：通过深度平衡模型",
    "translated_abstract": "扩散模型在生成高质量样本方面表现出色，但需要大量迭代，因此需要尝试将生成过程蒸馏到更快的网络中。然而，许多现有方法存在各种挑战：蒸馏训练过程复杂，通常需要多个训练阶段，而生成应用中使用这些模型的性能较差。本文介绍了一种简单而有效的方法，将扩散模型直接从初始噪声蒸馏到生成的图像。我们的方法中一个重要的方面是利用一种新的深度平衡模型作为蒸馏结构：生成型平衡变压器（GET）。我们的方法可以完全离线训练，只需使用扩散模型的噪声/图像对，而且在相当的训练预算下，实现了比现有一步方法更好的性能。我们证明了DEQ架构是有效的。",
    "tldr": "本文通过深度平衡模型实现了一步扩散蒸馏，将生成过程蒸馏到更快的网络中，取得了比现有一步方法更好的性能。",
    "en_tdlr": "This paper presents a one-step diffusion distillation method using deep equilibrium models, achieving better performance compared to existing one-step methods."
}