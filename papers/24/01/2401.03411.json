{
    "title": "GRAM: Global Reasoning for Multi-Page VQA",
    "abstract": "arXiv:2401.03411v2 Announce Type: replace  Abstract: The increasing use of transformer-based large language models brings forward the challenge of processing long sequences. In document visual question answering (DocVQA), leading methods focus on the single-page setting, while documents can span hundreds of pages. We present GRAM, a method that seamlessly extends pre-trained single-page models to the multi-page setting, without requiring computationally-heavy pretraining. To do so, we leverage a single-page encoder for local page-level understanding, and enhance it with document-level designated layers and learnable tokens, facilitating the flow of information across pages for global reasoning. To enforce our model to utilize the newly introduced document tokens, we propose a tailored bias adaptation method. For additional computational savings during decoding, we introduce an optional compression stage using our compression-transformer (C-Former),reducing the encoded sequence length, ",
    "link": "https://arxiv.org/abs/2401.03411",
    "context": "Title: GRAM: Global Reasoning for Multi-Page VQA\nAbstract: arXiv:2401.03411v2 Announce Type: replace  Abstract: The increasing use of transformer-based large language models brings forward the challenge of processing long sequences. In document visual question answering (DocVQA), leading methods focus on the single-page setting, while documents can span hundreds of pages. We present GRAM, a method that seamlessly extends pre-trained single-page models to the multi-page setting, without requiring computationally-heavy pretraining. To do so, we leverage a single-page encoder for local page-level understanding, and enhance it with document-level designated layers and learnable tokens, facilitating the flow of information across pages for global reasoning. To enforce our model to utilize the newly introduced document tokens, we propose a tailored bias adaptation method. For additional computational savings during decoding, we introduce an optional compression stage using our compression-transformer (C-Former),reducing the encoded sequence length, ",
    "path": "papers/24/01/2401.03411.json",
    "total_tokens": 806,
    "translated_title": "GRAM:全局推理用于多页面视觉问答",
    "translated_abstract": "随着基于Transformer的大型语言模型的不断增加，处理长序列的挑战越发突出。在文档视觉问答（DocVQA）中，主流方法专注于单页设置，而文档可能跨越数百页。我们提出了GRAM，一种能够无缝将经过预训练的单页模型扩展到多页面设置的方法，而无需进行计算密集的预训练。为此，我们利用单页编码器进行局部页面级理解，并利用文档级指定层和可学习标记来增强它，促进信息在页面间的全局推理传递。为了确保我们的模型使用新引入的文档标记，我们提出了一种定制的偏置适应方法。在解码过程中为额外的计算节省，我们引入了一个可选的压缩阶段，使用我们的压缩Transformer（C-Former），减少了编码序列的长度。",
    "tldr": "GRAM方法通过引入文档级的指定层和可学习标记，实现了将单页模型扩展到多页面设置，促进信息跨页面的全局推理。",
    "en_tdlr": "GRAM extends single-page models to multi-page setting by introducing document-level designated layers and learnable tokens, facilitating global reasoning across pages."
}