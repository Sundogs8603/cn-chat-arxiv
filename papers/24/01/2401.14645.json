{
    "title": "Omnipredictors for Regression and the Approximate Rank of Convex Functions. (arXiv:2401.14645v1 [cs.LG])",
    "abstract": "Consider the supervised learning setting where the goal is to learn to predict labels $\\mathbf y$ given points $\\mathbf x$ from a distribution. An \\textit{omnipredictor} for a class $\\mathcal L$ of loss functions and a class $\\mathcal C$ of hypotheses is a predictor whose predictions incur less expected loss than the best hypothesis in $\\mathcal C$ for every loss in $\\mathcal L$. Since the work of [GKR+21] that introduced the notion, there has been a large body of work in the setting of binary labels where $\\mathbf y \\in \\{0, 1\\}$, but much less is known about the regression setting where $\\mathbf y \\in [0,1]$ can be continuous. Our main conceptual contribution is the notion of \\textit{sufficient statistics} for loss minimization over a family of loss functions: these are a set of statistics about a distribution such that knowing them allows one to take actions that minimize the expected loss for any loss in the family. The notion of sufficient statistics relates directly to the approx",
    "link": "http://arxiv.org/abs/2401.14645",
    "context": "Title: Omnipredictors for Regression and the Approximate Rank of Convex Functions. (arXiv:2401.14645v1 [cs.LG])\nAbstract: Consider the supervised learning setting where the goal is to learn to predict labels $\\mathbf y$ given points $\\mathbf x$ from a distribution. An \\textit{omnipredictor} for a class $\\mathcal L$ of loss functions and a class $\\mathcal C$ of hypotheses is a predictor whose predictions incur less expected loss than the best hypothesis in $\\mathcal C$ for every loss in $\\mathcal L$. Since the work of [GKR+21] that introduced the notion, there has been a large body of work in the setting of binary labels where $\\mathbf y \\in \\{0, 1\\}$, but much less is known about the regression setting where $\\mathbf y \\in [0,1]$ can be continuous. Our main conceptual contribution is the notion of \\textit{sufficient statistics} for loss minimization over a family of loss functions: these are a set of statistics about a distribution such that knowing them allows one to take actions that minimize the expected loss for any loss in the family. The notion of sufficient statistics relates directly to the approx",
    "path": "papers/24/01/2401.14645.json",
    "total_tokens": 888,
    "translated_title": "回归问题的全能预测器与凸函数的近似秩",
    "translated_abstract": "考虑在监督学习环境中，目标是学习通过给定分布中的点𝐱来预测标签𝐲。对于损失函数类𝒀和假设类𝒞，全能预测器是一种预测器，其预测的损失小于𝒀中任意损失下的最佳假设。自从引入这个概念的[GKR+21]的工作以来，在二分类标签的设置下，即𝐲∈{0,1}，已经有大量的工作，但是对于连续标签𝐲∈[0,1]的回归设置却知之甚少。我们的主要概念贡献是关于一类损失函数的损失最小化的充分统计量的概念：这是一组关于分布的统计量，通过了解它们可以采取最小化任何损失的期望损失的行动。充分统计量的概念直接相关到凸函数近似的概念上。",
    "tldr": "全能预测器是一种预测器，其预测的损失小于任意损失下的最佳假设。我们提出了关于损失函数的充分统计量的概念，可以用来最小化任何损失的期望损失。"
}