{
    "title": "RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series Tasks. (arXiv:2401.09093v1 [cs.LG])",
    "abstract": "Traditional Recurrent Neural Network (RNN) architectures, such as LSTM and GRU, have historically held prominence in time series tasks. However, they have recently seen a decline in their dominant position across various time series tasks. As a result, recent advancements in time series forecasting have seen a notable shift away from RNNs towards alternative architectures such as Transformers, MLPs, and CNNs. To go beyond the limitations of traditional RNNs, we design an efficient RNN-based model for time series tasks, named RWKV-TS, with three distinctive features: (i) A novel RNN architecture characterized by $O(L)$ time complexity and memory usage. (ii) An enhanced ability to capture long-term sequence information compared to traditional RNNs. (iii) High computational efficiency coupled with the capacity to scale up effectively. Through extensive experimentation, our proposed RWKV-TS model demonstrates competitive performance when compared to state-of-the-art Transformer-based or CN",
    "link": "http://arxiv.org/abs/2401.09093",
    "context": "Title: RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series Tasks. (arXiv:2401.09093v1 [cs.LG])\nAbstract: Traditional Recurrent Neural Network (RNN) architectures, such as LSTM and GRU, have historically held prominence in time series tasks. However, they have recently seen a decline in their dominant position across various time series tasks. As a result, recent advancements in time series forecasting have seen a notable shift away from RNNs towards alternative architectures such as Transformers, MLPs, and CNNs. To go beyond the limitations of traditional RNNs, we design an efficient RNN-based model for time series tasks, named RWKV-TS, with three distinctive features: (i) A novel RNN architecture characterized by $O(L)$ time complexity and memory usage. (ii) An enhanced ability to capture long-term sequence information compared to traditional RNNs. (iii) High computational efficiency coupled with the capacity to scale up effectively. Through extensive experimentation, our proposed RWKV-TS model demonstrates competitive performance when compared to state-of-the-art Transformer-based or CN",
    "path": "papers/24/01/2401.09093.json",
    "total_tokens": 960,
    "translated_title": "RWKV-TS：超越传统循环神经网络在时间序列任务中的应用",
    "translated_abstract": "传统的循环神经网络（RNN）模型如LSTM和GRU在时间序列任务中一直占据主导地位。然而，近年来它们在各种时间序列任务中的地位有所下降。因此，最近的时间序列预测研究已经发生了明显的转变，从RNN向Transformers、MLPs和CNNs等其他架构转变。为了超越传统RNN的局限性，我们设计了一种名为RWKV-TS的高效RNN模型，具有以下三个独特的特点：（i）具有$O(L)$的时间复杂度和内存使用的新颖RNN架构；（ii）相较于传统RNN，更能够捕捉长期的序列信息；（iii）高计算效率，能够有效地扩展。通过广泛的实验，我们提出的RWKV-TS模型在与基于Transformer类型或CNN类型的最先进模型比较时表现出竞争力。",
    "tldr": "RWKV-TS是一个高效的RNN模型，它通过具有低时间复杂度和内存使用的新颖架构、增强的捕捉长期序列信息能力以及高计算效率的特点，超越了传统循环神经网络在时间序列任务中的应用，并在与最先进的Transformer或CNN模型的比较中展现出竞争力。",
    "en_tdlr": "RWKV-TS is an efficient RNN model that goes beyond the traditional recurrent neural network in time series tasks. It achieves this by introducing a novel architecture with low time complexity and memory usage, enhanced ability to capture long-term sequence information compared to traditional RNNs, and high computational efficiency. Through extensive experimentation, RWKV-TS demonstrates competitive performance when compared to state-of-the-art Transformer-based or CNN models."
}