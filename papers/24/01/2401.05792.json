{
    "title": "Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations. (arXiv:2401.05792v1 [cs.CL])",
    "abstract": "Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong language identity information which hinders the expression of linguistic factors shared across languages. For semantic tasks like cross-lingual sentence retrieval, it is desired to remove such language identity signals to fully leverage semantic information. In this work, we provide a novel view of projecting away language-specific factors from a multilingual embedding space. Specifically, we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information). To identify this subspace, we present a simple but effective unsupervised method based on singular value decomposition with multiple monolingual corpora as input. Once the s",
    "link": "http://arxiv.org/abs/2401.05792",
    "context": "Title: Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations. (arXiv:2401.05792v1 [cs.CL])\nAbstract: Large pretrained multilingual language models (ML-LMs) have shown remarkable capabilities of zero-shot cross-lingual transfer, without direct cross-lingual supervision. While these results are promising, follow-up works found that, within the multilingual embedding spaces, there exists strong language identity information which hinders the expression of linguistic factors shared across languages. For semantic tasks like cross-lingual sentence retrieval, it is desired to remove such language identity signals to fully leverage semantic information. In this work, we provide a novel view of projecting away language-specific factors from a multilingual embedding space. Specifically, we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information). To identify this subspace, we present a simple but effective unsupervised method based on singular value decomposition with multiple monolingual corpora as input. Once the s",
    "path": "papers/24/01/2401.05792.json",
    "total_tokens": 904,
    "translated_title": "发现用于跨语言不可知多语言表示的低秩子空间",
    "translated_abstract": "大型预训练的多语言语言模型（ML-LMs）展示出了在无直接跨语言监督的情况下具有卓越的零样本跨语言转换能力。然而，随后的研究发现，在多语言嵌入空间中存在强烈的语言身份信息，这阻碍了语言间共享的语言因素的表达。对于跨语言句子检索等语义任务，希望消除这种语言身份信号，充分利用语义信息。在这项工作中，我们提供了从多语言嵌入空间中投影语言特定因素的新视角。具体来说，我们发现存在一个低秩子空间，该子空间主要编码与语义无关的信息（例如，句法信息）。为了识别该子空间，我们提出了一个简单但有效的无监督方法，该方法基于奇异值分解，将多语言语料库作为输入。一旦找到了该子空间，我们可以通过投影将信息向该子空间的零空间投影，从而获得消除了语言特定因素的语义信息。",
    "tldr": "本论文提出了一种从多语言嵌入空间中投影语言特定因素的新视角，并通过发现一个低秩子空间来消除与语义无关的信息，从而充分利用语义信息。"
}