{
    "title": "Data-efficient Fine-tuning for LLM-based Recommendation",
    "abstract": "Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data.   To tackle these issues, we introduce two objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and",
    "link": "https://arxiv.org/abs/2401.17197",
    "context": "Title: Data-efficient Fine-tuning for LLM-based Recommendation\nAbstract: Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data.   To tackle these issues, we introduce two objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and",
    "path": "papers/24/01/2401.17197.json",
    "total_tokens": 785,
    "translated_title": "基于LLM的推荐的数据高效微调",
    "translated_abstract": "近年来，利用大型语言模型（LLM）进行推荐引起了广泛关注，其中微调在LLM的适应中起着关键作用。然而，快速扩展的推荐数据上微调LLM的成本限制了它们的实际应用。为了解决这一挑战，少样本微调提供了一种快速适应LLM到新的推荐数据的方法。我们提出了为高效的LLM推荐任务剪枝数据的任务，旨在找到适合LLM的少样本微调的代表样本。虽然核心集选择与所提出的任务密切相关，但现有的核心集选择方法往往依赖于次优启发式指标或需要在大规模推荐数据上进行昂贵的优化。",
    "tldr": "本论文中介绍了基于LLM的推荐中的数据高效微调的方法，通过剪枝数据来减少LLM的微调成本，并提出了两种目标来实现高准确性的数据剪枝。",
    "en_tdlr": "This paper presents an approach for data-efficient fine-tuning in LLM-based recommendation, which reduces the cost of fine-tuning LLMs by pruning data and introduces two objectives for accurate data pruning."
}