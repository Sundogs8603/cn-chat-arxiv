{
    "title": "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation. (arXiv:2401.06591v1 [cs.CL])",
    "abstract": "Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at https://github.com/kaistAI/prometheus-vision",
    "link": "http://arxiv.org/abs/2401.06591",
    "context": "Title: Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation. (arXiv:2401.06591v1 [cs.CL])\nAbstract: Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at https://github.com/kaistAI/prometheus-vision",
    "path": "papers/24/01/2401.06591.json",
    "total_tokens": 900,
    "translated_title": "Prometheus-Vision: 视觉语言模型作为细粒度评估的裁判",
    "translated_abstract": "评估由视觉语言模型（VLMs）生成的长篇回答是具有挑战性的。这不仅需要检查VLM是否遵循给定的指令，还需要验证文本输出是否正确地与给定的图像相联系。受到使用LMs评估LMs的新方法的启发，本文提出使用VLMs来评估VLMs。为此，我们提出了一个名为Perception Collection的新的反馈数据集，包含15K个用户可能在评估过程中关注的定制评分标准。使用Perception Collection，我们训练了Prometheus-Vision，这是第一个能够在评估过程中理解用户定义的评分标准的开源VLM评估模型。Prometheus-Vision与人类评估员和GPT-4V之间显示出最高的Pearson相关性，显示了其用于透明和可访问的VLM评估的有效性。我们将我们的代码、数据集和模型开源在https://github.com/kaistAI/prometheus-vision。",
    "tldr": "Prometheus-Vision是一个开源的视觉语言模型评估器，使用了一个名为Perception Collection的反馈数据集，训练出的模型能够理解用户定义的评分标准，与人类评估员和GPT-4V之间显示出最高的相关性。",
    "en_tdlr": "Prometheus-Vision is an open-source evaluator for vision-language models. It utilizes the Perception Collection dataset to train a model that can understand user-defined score criteria. It achieves the highest correlation with human evaluators and GPT-4V."
}