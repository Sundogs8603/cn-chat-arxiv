{
    "title": "Exploring Gender Biases in Language Patterns of Human-Conversational Agent Conversations. (arXiv:2401.03030v1 [cs.HC])",
    "abstract": "With the rise of human-machine communication, machines are increasingly designed with humanlike characteristics, such as gender, which can inadvertently trigger cognitive biases. Many conversational agents (CAs), such as voice assistants and chatbots, default to female personas, leading to concerns about perpetuating gender stereotypes and inequality. Critiques have emerged regarding the potential objectification of females and reinforcement of gender stereotypes by these technologies. This research, situated in conversational AI design, aims to delve deeper into the impacts of gender biases in human-CA interactions. From a behavioral and communication research standpoint, this program focuses not only on perceptions but also the linguistic styles of users when interacting with CAs, as previous research has rarely explored. It aims to understand how pre-existing gender biases might be triggered by CAs' gender designs. It further investigates how CAs' gender designs may reinforce gender",
    "link": "http://arxiv.org/abs/2401.03030",
    "context": "Title: Exploring Gender Biases in Language Patterns of Human-Conversational Agent Conversations. (arXiv:2401.03030v1 [cs.HC])\nAbstract: With the rise of human-machine communication, machines are increasingly designed with humanlike characteristics, such as gender, which can inadvertently trigger cognitive biases. Many conversational agents (CAs), such as voice assistants and chatbots, default to female personas, leading to concerns about perpetuating gender stereotypes and inequality. Critiques have emerged regarding the potential objectification of females and reinforcement of gender stereotypes by these technologies. This research, situated in conversational AI design, aims to delve deeper into the impacts of gender biases in human-CA interactions. From a behavioral and communication research standpoint, this program focuses not only on perceptions but also the linguistic styles of users when interacting with CAs, as previous research has rarely explored. It aims to understand how pre-existing gender biases might be triggered by CAs' gender designs. It further investigates how CAs' gender designs may reinforce gender",
    "path": "papers/24/01/2401.03030.json",
    "total_tokens": 907,
    "translated_title": "探索人与对话型智能体交互中语言模式中的性别偏见",
    "translated_abstract": "随着人机交互的兴起，机器越来越多地设计成类似人类的特征，如性别，这可能无意中引发认知偏见。许多对话型智能体（CAs），如语音助手和聊天机器人，默认为女性角色，引发了有关持续性性别刻板印象和不平等的担忧。有人批评这些技术可能把女性物化并强化性别刻板印象。这项研究旨在从对话型人工智能设计的角度深入探讨性别偏见在人与对话型智能体交互中的影响。从行为和沟通研究的角度来看，该项目不仅关注用户与对话型智能体互动时的认知，还关注其语言风格，之前的研究很少涉及这方面。研究目的是了解对话型智能体的性别设计如何触发既有性别偏见，并进一步研究对话型智能体的性别设计如何强化性别刻板印象。",
    "tldr": "本研究旨在探索人与对话型智能体交互中的性别偏见，并研究对话型智能体的性别设计如何触发既有性别偏见并强化性别刻板印象。"
}