{
    "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling. (arXiv:2401.06080v1 [cs.AI])",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent. (2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training.  In this report, we attempt to address these two issues. (1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models. Experimental result",
    "link": "http://arxiv.org/abs/2401.06080",
    "context": "Title: Secrets of RLHF in Large Language Models Part II: Reward Modeling. (arXiv:2401.06080v1 [cs.AI])\nAbstract: Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent. (2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training.  In this report, we attempt to address these two issues. (1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models. Experimental result",
    "path": "papers/24/01/2401.06080.json",
    "total_tokens": 876,
    "translated_title": "RLHF在大型语言模型中的秘密 Part II: 奖励建模",
    "translated_abstract": "从人类反馈中进行强化学习（RLHF）已成为将语言模型与人类价值和意图对齐的关键技术，使模型能够产生更有帮助且无害的回应。奖励模型被训练为人类偏好的代理，以驱动强化学习优化。然而，在实际应用中，奖励模型面临以下挑战：（1）数据集中不正确和模糊的偏好对可能妨碍奖励模型准确捕捉人类意图。（2）在特定分布的数据上训练的奖励模型往往难以推广到分布之外的示例，并且不适用于迭代RLHF训练。本研究尝试解决这两个问题。从数据角度出发，我们提出一种方法来测量数据中偏好的强度，基于多个奖励模型的投票机制。实验结果...",
    "tldr": "本报告探讨了在 RLHF 中解决奖励建模的两个挑战，通过使用多个奖励模型的投票机制来测量数据中偏好的强度，并解决在特定分布数据上训练奖励模型难以推广的问题。"
}