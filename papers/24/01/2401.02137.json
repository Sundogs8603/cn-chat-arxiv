{
    "title": "SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment. (arXiv:2401.02137v1 [cs.CV])",
    "abstract": "Multimodal alignment between language and vision is the fundamental topic in current vision-language model research. Contrastive Captioners (CoCa), as a representative method, integrates Contrastive Language-Image Pretraining (CLIP) and Image Caption (IC) into a unified framework, resulting in impressive results. CLIP imposes a bidirectional constraints on global representation of entire images and sentences. Although IC conducts an unidirectional image-to-text generation on local representation, it lacks any constraint on local text-to-image reconstruction, which limits the ability to understand images at a fine-grained level when aligned with texts. To achieve multimodal alignment from both global and local perspectives, this paper proposes Symmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional interactions on images and texts across the global and local representation levels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM) head based on ITC",
    "link": "http://arxiv.org/abs/2401.02137",
    "context": "Title: SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment. (arXiv:2401.02137v1 [cs.CV])\nAbstract: Multimodal alignment between language and vision is the fundamental topic in current vision-language model research. Contrastive Captioners (CoCa), as a representative method, integrates Contrastive Language-Image Pretraining (CLIP) and Image Caption (IC) into a unified framework, resulting in impressive results. CLIP imposes a bidirectional constraints on global representation of entire images and sentences. Although IC conducts an unidirectional image-to-text generation on local representation, it lacks any constraint on local text-to-image reconstruction, which limits the ability to understand images at a fine-grained level when aligned with texts. To achieve multimodal alignment from both global and local perspectives, this paper proposes Symmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional interactions on images and texts across the global and local representation levels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM) head based on ITC",
    "path": "papers/24/01/2401.02137.json",
    "total_tokens": 901,
    "translated_title": "SyCoCa: 用关注掩码对多模态对齐进行对称化的对比式字幕生成器",
    "translated_abstract": "语言和视觉之间的多模态对齐是当前视觉-语言模型研究中的基本主题。对比式字幕生成器（CoCa）作为一种代表性方法，将对比式语言-图像预训练（CLIP）和图像字幕（IC）整合到统一的框架中，取得了令人瞩目的结果。虽然IC在本地表示上进行了单向的图像到文本生成，但它缺乏对本地文本到图像重构的任何约束，在与文本对齐时限制了对图像的细粒度理解能力。为了从全局和本地两个角度实现多模态对齐，本文提出了对称化对比式字幕生成器（SyCoCa），它在全局和本地表示层面上引入了图像和文本的双向交互。具体而言，我们在基于ITC的文本引导掩码图像建模（TG-MIM）头上进行了扩展。",
    "tldr": "本文提出了SyCoCa方法，通过引入全局和本地表示层面上图像和文本的双向交互，实现了多模态对齐。这种方法可以更好地理解图像和文本之间的细粒度关联。"
}