{
    "title": "SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning. (arXiv:2401.09949v1 [cs.LG])",
    "abstract": "Contrary to the use of genetic programming, the neural network approach to symbolic regression can scale well with high input dimension and leverage gradient methods for faster equation searching. Common ways of constraining expression complexity have relied on multistage pruning methods with fine-tuning, but these often lead to significant performance loss. In this work, we propose SymbolNet, a neural network approach to symbolic regression in a novel framework that enables dynamic pruning of model weights, input features, and mathematical operators in a single training, where both training loss and expression complexity are optimized simultaneously. We introduce a sparsity regularization term per pruning type, which can adaptively adjust its own strength and lead to convergence to a target sparsity level. In contrast to most existing symbolic regression methods that cannot efficiently handle datasets with more than $O$(10) inputs, we demonstrate the effectiveness of our model on the ",
    "link": "http://arxiv.org/abs/2401.09949",
    "context": "Title: SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning. (arXiv:2401.09949v1 [cs.LG])\nAbstract: Contrary to the use of genetic programming, the neural network approach to symbolic regression can scale well with high input dimension and leverage gradient methods for faster equation searching. Common ways of constraining expression complexity have relied on multistage pruning methods with fine-tuning, but these often lead to significant performance loss. In this work, we propose SymbolNet, a neural network approach to symbolic regression in a novel framework that enables dynamic pruning of model weights, input features, and mathematical operators in a single training, where both training loss and expression complexity are optimized simultaneously. We introduce a sparsity regularization term per pruning type, which can adaptively adjust its own strength and lead to convergence to a target sparsity level. In contrast to most existing symbolic regression methods that cannot efficiently handle datasets with more than $O$(10) inputs, we demonstrate the effectiveness of our model on the ",
    "path": "papers/24/01/2401.09949.json",
    "total_tokens": 949,
    "translated_title": "SymbolNet: 自适应动态修剪的神经符号回归",
    "translated_abstract": "与遗传编程的使用相反，神经网络方法可在高输入维度下有效扩展，并利用梯度方法加速方程搜索。常见的表达式复杂性约束方法依赖于多阶段修剪方法进行微调，但这往往会导致显著的性能损失。在本文中，我们提出了一种神经网络方法，即SymbolNet，以一种新颖的框架实现符号回归，该框架可以在单个训练中动态修剪模型权重、输入特征和数学运算符，同时优化训练损失和表达式复杂性。我们引入了每个修剪类型的稀疏正则化项，该项可以自适应调整自身的强度，并导致收敛到目标稀疏度水平。与大多数现有的符号回归方法无法高效处理具有超过10个输入的数据集不同，我们证明了我们的模型的有效性。",
    "tldr": "SymbolNet是一种神经网络方法，通过动态修剪模型权重、输入特征和数学运算符，同时优化训练损失和表达式复杂性，实现了符号回归。通过引入稀疏正则化项，我们的模型可以自适应调整自身的强度，并收敛到目标稀疏度水平。与现有方法相比，SymbolNet能高效处理具有超过10个输入的数据集。",
    "en_tdlr": "SymbolNet is a neural network approach to symbolic regression that achieves dynamic pruning of model weights, input features, and mathematical operators, optimizing both training loss and expression complexity. The model introduces a sparsity regularization term that adaptively adjusts its strength to converge to a target sparsity level. SymbolNet outperforms existing methods in efficiently handling datasets with more than 10 inputs."
}