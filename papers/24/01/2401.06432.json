{
    "title": "Heterogeneous Low-Rank Approximation for Federated Fine-tuning of On-Device Foundation Models. (arXiv:2401.06432v1 [cs.LG])",
    "abstract": "Large foundation models (FMs) adapt surprisingly well to specific domains or tasks with fine-tuning. Federated learning (FL) further enables private FM fine-tuning using the local data on devices. However, the standard FMs' large size poses challenges for resource-constrained and heterogeneous devices. To address this, we consider FMs with reduced parameter sizes, referred to as on-device FMs (ODFMs). While ODFMs allow on-device inference, computational constraints still hinder efficient federated fine-tuning. We propose a parameter-efficient federated fine-tuning method for ODFMs using heterogeneous low-rank approximations (LoRAs) that addresses system and data heterogeneity. We show that homogeneous LoRA ranks face a trade-off between overfitting and slow convergence, and propose HetLoRA, which employs heterogeneous ranks across clients and eliminates the shortcomings of homogeneous HetLoRA. By applying rank self-pruning locally and sparsity-weighted aggregation at the server, we com",
    "link": "http://arxiv.org/abs/2401.06432",
    "context": "Title: Heterogeneous Low-Rank Approximation for Federated Fine-tuning of On-Device Foundation Models. (arXiv:2401.06432v1 [cs.LG])\nAbstract: Large foundation models (FMs) adapt surprisingly well to specific domains or tasks with fine-tuning. Federated learning (FL) further enables private FM fine-tuning using the local data on devices. However, the standard FMs' large size poses challenges for resource-constrained and heterogeneous devices. To address this, we consider FMs with reduced parameter sizes, referred to as on-device FMs (ODFMs). While ODFMs allow on-device inference, computational constraints still hinder efficient federated fine-tuning. We propose a parameter-efficient federated fine-tuning method for ODFMs using heterogeneous low-rank approximations (LoRAs) that addresses system and data heterogeneity. We show that homogeneous LoRA ranks face a trade-off between overfitting and slow convergence, and propose HetLoRA, which employs heterogeneous ranks across clients and eliminates the shortcomings of homogeneous HetLoRA. By applying rank self-pruning locally and sparsity-weighted aggregation at the server, we com",
    "path": "papers/24/01/2401.06432.json",
    "total_tokens": 938,
    "translated_title": "异构低秩近似用于设备本地基础模型联邦微调",
    "translated_abstract": "大型基础模型（FMs）通过微调适应特定领域或任务。联邦学习（FL）进一步利用设备上的本地数据实现了私有化的FM微调。然而，标准FMs的大尺寸对于资源受限和异构设备带来了挑战。为了解决这个问题，我们考虑了参数尺寸较小的FM，称为设备本地FM（ODFMs）。虽然ODFMs允许设备上的推断，但计算限制仍然阻碍了高效的联邦微调。我们提出了一种参数高效的ODFM联邦微调方法，使用了异构低秩近似（LoRA），解决了系统和数据异质性的问题。我们发现，同质LoRA秩面临着过拟合和收敛缓慢之间的折衷，提出了HetLoRA，它在客户端之间使用异质的秩并消除了同质HetLoRA的缺点。通过在本地应用秩自剪枝，并在服务器上应用稀疏加权聚合，我们完成了摘要中的内容。",
    "tldr": "本文提出了一种用于异构设备的设备本地基础模型联邦微调的参数高效方法，使用了异构低秩近似（LoRA），解决了资源受限和异构设备的挑战。",
    "en_tdlr": "This paper proposes a parameter-efficient method for federated fine-tuning of on-device foundation models on heterogeneous devices, using heterogeneous low-rank approximations (LoRAs), addressing resource-constrained and heterogeneous device challenges."
}