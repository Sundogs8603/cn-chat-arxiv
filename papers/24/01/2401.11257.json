{
    "title": "Measuring Policy Distance for Multi-Agent Reinforcement Learning. (arXiv:2401.11257v1 [cs.MA])",
    "abstract": "Diversity plays a crucial role in improving the performance of multi-agent reinforcement learning (MARL). Currently, many diversity-based methods have been developed to overcome the drawbacks of excessive parameter sharing in traditional MARL. However, there remains a lack of a general metric to quantify policy differences among agents. Such a metric would not only facilitate the evaluation of the diversity evolution in multi-agent systems, but also provide guidance for the design of diversity-based MARL algorithms. In this paper, we propose the multi-agent policy distance (MAPD), a general tool for measuring policy differences in MARL. By learning the conditional representations of agents' decisions, MAPD can computes the policy distance between any pair of agents. Furthermore, we extend MAPD to a customizable version, which can quantify differences among agent policies on specified aspects. Based on the online deployment of MAPD, we design a multi-agent dynamic parameter sharing (MAD",
    "link": "http://arxiv.org/abs/2401.11257",
    "context": "Title: Measuring Policy Distance for Multi-Agent Reinforcement Learning. (arXiv:2401.11257v1 [cs.MA])\nAbstract: Diversity plays a crucial role in improving the performance of multi-agent reinforcement learning (MARL). Currently, many diversity-based methods have been developed to overcome the drawbacks of excessive parameter sharing in traditional MARL. However, there remains a lack of a general metric to quantify policy differences among agents. Such a metric would not only facilitate the evaluation of the diversity evolution in multi-agent systems, but also provide guidance for the design of diversity-based MARL algorithms. In this paper, we propose the multi-agent policy distance (MAPD), a general tool for measuring policy differences in MARL. By learning the conditional representations of agents' decisions, MAPD can computes the policy distance between any pair of agents. Furthermore, we extend MAPD to a customizable version, which can quantify differences among agent policies on specified aspects. Based on the online deployment of MAPD, we design a multi-agent dynamic parameter sharing (MAD",
    "path": "papers/24/01/2401.11257.json",
    "total_tokens": 1052,
    "translated_title": "多智能体强化学习中的政策距离测量",
    "translated_abstract": "多样性在改善多智能体强化学习（MARL）的性能中起着关键作用。目前，已经开发出许多基于多样性的方法，以克服传统MARL中过多参数共享的缺点。然而，目前缺乏一种通用的度量标准来量化智能体之间的政策差异。这样的度量标准不仅可以方便地评估多智能体系统中多样性的演变，还可以为基于多样性的MARL算法的设计提供指导。本文中，我们提出了多智能体政策距离（MAPD），这是一个用于测量MARL中政策差异的通用工具。通过学习智能体决策的条件表示，MAPD可以计算任意一对智能体之间的政策距离。此外，我们还将MAPD扩展为可定制的版本，可以量化在指定方面的智能体政策之间的差异。基于MAPD的在线部署，我们设计了一个多智能体动态参数共享（MAD）模型。",
    "tldr": "本文提出了一种用于测量多智能体强化学习中政策差异的通用工具，即多智能体政策距离（MAPD）。通过学习智能体的决策条件表示，MAPD可以计算任意一对智能体之间的政策距离，并且可以扩展到定制化版本以量化智能体政策在特定方面的差异。这个工具不仅有助于评估多智能体系统中多样性的演变，还为基于多样性的MARL算法的设计提供指导。",
    "en_tdlr": "This paper proposes a general tool for measuring policy differences in multi-agent reinforcement learning (MARL), called Multi-Agent Policy Distance (MAPD). By learning the conditional representations of agents' decisions, MAPD can compute the policy distance between any pair of agents and can be extended to quantify differences among agent policies on specified aspects. This tool not only facilitates the evaluation of diversity evolution in multi-agent systems, but also provides guidance for the design of diversity-based MARL algorithms."
}