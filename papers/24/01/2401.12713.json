{
    "title": "Generating Unsupervised Abstractive Explanations for Rumour Verification. (arXiv:2401.12713v1 [cs.CL])",
    "abstract": "The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it. While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric, free-text explanations of a rumour's veracity. We follow an unsupervised approach by first utilising post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanatory summaries by employing template-guided summarisation. To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM). Our experiments show that LLMs can have similar agreement to humans in evaluating summaries. Importantly, we show that explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread.",
    "link": "http://arxiv.org/abs/2401.12713",
    "context": "Title: Generating Unsupervised Abstractive Explanations for Rumour Verification. (arXiv:2401.12713v1 [cs.CL])\nAbstract: The task of rumour verification in social media concerns assessing the veracity of a claim on the basis of conversation threads that result from it. While previous work has focused on predicting a veracity label, here we reformulate the task to generate model-centric, free-text explanations of a rumour's veracity. We follow an unsupervised approach by first utilising post-hoc explainability methods to score the most important posts within a thread and then we use these posts to generate informative explanatory summaries by employing template-guided summarisation. To evaluate the informativeness of the explanatory summaries, we exploit the few-shot learning capabilities of a large language model (LLM). Our experiments show that LLMs can have similar agreement to humans in evaluating summaries. Importantly, we show that explanatory abstractive summaries are more informative and better reflect the predicted rumour veracity than just using the highest ranking posts in the thread.",
    "path": "papers/24/01/2401.12713.json",
    "total_tokens": 861,
    "translated_title": "生成无监督的言辞解释用于谣言验证",
    "translated_abstract": "在社交媒体上进行谣言验证的任务涉及根据由该谣言引起的对话线程评估其真实性的问题。尽管之前的工作已经专注于预测真实性标签，但我们在这里重新制定了任务，以生成与模型相关的自由文本解释谣言的真实性。我们采用无监督的方法，首先利用事后可解释性方法对线程中最重要的帖子进行评分，然后使用这些帖子通过使用模板引导总结生成信息丰富的解释性摘要。为了评估解释性摘要的信息量，我们利用了大型语言模型的少样本学习能力。我们的实验表明，语言模型在评估摘要时可以与人类达到类似的一致性。重要的是，我们证明了解释性的概括摘要比仅使用线程中排名最高的帖子更具信息量，并更好地反映了预测的谣言真实性。",
    "tldr": "该论文利用无监督的方法，通过对言辞进行评分并生成解释性摘要，来增加谣言验证的信息丰富度和准确性。"
}