{
    "title": "Safe reinforcement learning in uncertain contexts. (arXiv:2401.05876v1 [cs.LG])",
    "abstract": "When deploying machine learning algorithms in the real world, guaranteeing safety is an essential asset. Existing safe learning approaches typically consider continuous variables, i.e., regression tasks. However, in practice, robotic systems are also subject to discrete, external environmental changes, e.g., having to carry objects of certain weights or operating on frozen, wet, or dry surfaces. Such influences can be modeled as discrete context variables. In the existing literature, such contexts are, if considered, mostly assumed to be known. In this work, we drop this assumption and show how we can perform safe learning when we cannot directly measure the context variables. To achieve this, we derive frequentist guarantees for multi-class classification, allowing us to estimate the current context from measurements. Further, we propose an approach for identifying contexts through experiments. We discuss under which conditions we can retain theoretical guarantees and demonstrate the ",
    "link": "http://arxiv.org/abs/2401.05876",
    "context": "Title: Safe reinforcement learning in uncertain contexts. (arXiv:2401.05876v1 [cs.LG])\nAbstract: When deploying machine learning algorithms in the real world, guaranteeing safety is an essential asset. Existing safe learning approaches typically consider continuous variables, i.e., regression tasks. However, in practice, robotic systems are also subject to discrete, external environmental changes, e.g., having to carry objects of certain weights or operating on frozen, wet, or dry surfaces. Such influences can be modeled as discrete context variables. In the existing literature, such contexts are, if considered, mostly assumed to be known. In this work, we drop this assumption and show how we can perform safe learning when we cannot directly measure the context variables. To achieve this, we derive frequentist guarantees for multi-class classification, allowing us to estimate the current context from measurements. Further, we propose an approach for identifying contexts through experiments. We discuss under which conditions we can retain theoretical guarantees and demonstrate the ",
    "path": "papers/24/01/2401.05876.json",
    "total_tokens": 845,
    "translated_title": "在不确定环境中的安全强化学习",
    "translated_abstract": "在实际应用中，保证安全是机器学习算法的重要方面。现有的安全学习方法通常考虑连续变量，即回归任务。然而，在实践中，机器人系统也会受到离散的外部环境变化的影响，例如必须携带特定重量的物体或在冰冻、湿润或干燥的表面上操作。这些影响可以建模为离散的上下文变量。在现有文献中，如果考虑这些上下文，大多数情况下假设它们是已知的。在这项工作中，我们放弃了这个假设，展示了在无法直接测量上下文变量的情况下如何进行安全学习。为了实现这一点，我们针对多类分类推导了频率保证，从测量值中估计当前上下文。此外，我们提出了一种通过实验来识别上下文的方法。我们讨论了在哪些条件下可以保留理论保证，并加以证明。",
    "tldr": "本文提出了一种在不确定环境中进行安全强化学习的方法，通过推导频率保证来估计当前上下文，并通过实验来识别上下文变量。",
    "en_tdlr": "This paper proposes a method for safe reinforcement learning in uncertain contexts, estimating the current context by deriving frequentist guarantees and identifying context variables through experiments."
}