{
    "title": "$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy. (arXiv:2401.01268v1 [cs.LG])",
    "abstract": "In deep learning, classification tasks are formalized as optimization problems solved via the minimization of the cross-entropy. However, recent advancements in the design of objective functions allow the $f$-divergence measure to generalize the formulation of the optimization problem for classification. With this goal in mind, we adopt a Bayesian perspective and formulate the classification task as a maximum a posteriori probability problem. We propose a class of objective functions based on the variational representation of the $f$-divergence, from which we extract a list of five posterior probability estimators leveraging well-known $f$-divergences. In addition, driven by the challenge of improving the state-of-the-art approach, we propose a bottom-up method that leads us to the formulation of a new objective function (and posterior probability estimator) corresponding to a novel $f$-divergence referred to as shifted log (SL). First, we theoretically prove the convergence property o",
    "link": "http://arxiv.org/abs/2401.01268",
    "context": "Title: $f$-Divergence Based Classification: Beyond the Use of Cross-Entropy. (arXiv:2401.01268v1 [cs.LG])\nAbstract: In deep learning, classification tasks are formalized as optimization problems solved via the minimization of the cross-entropy. However, recent advancements in the design of objective functions allow the $f$-divergence measure to generalize the formulation of the optimization problem for classification. With this goal in mind, we adopt a Bayesian perspective and formulate the classification task as a maximum a posteriori probability problem. We propose a class of objective functions based on the variational representation of the $f$-divergence, from which we extract a list of five posterior probability estimators leveraging well-known $f$-divergences. In addition, driven by the challenge of improving the state-of-the-art approach, we propose a bottom-up method that leads us to the formulation of a new objective function (and posterior probability estimator) corresponding to a novel $f$-divergence referred to as shifted log (SL). First, we theoretically prove the convergence property o",
    "path": "papers/24/01/2401.01268.json",
    "total_tokens": 1048,
    "translated_title": "基于$f$散度的分类：超越交叉熵的应用",
    "translated_abstract": "在深度学习中，分类任务被形式化为通过最小化交叉熵来解决的优化问题。然而，目前在目标函数设计方面的最新进展使得$f$散度度量可以推广分类问题的优化问题。为了实现这个目标，我们采用贝叶斯视角，并将分类任务形式化为一个最大后验概率问题。我们提出了一类基于$f$散度变分表示的目标函数，从中提取了一系列五个后验概率估计器，利用了众所周知的$f$散度。此外，受到改进最先进方法的挑战的驱动力，我们提出了一种自底向上的方法，使我们能够提出一个对应于一种新的被称为平移对数 (SL) 的$f$散度的新目标函数（和后验概率估计器）的公式。首先，我们在理论上证明了收敛性的属性。",
    "tldr": "这项研究提出了一种基于$f$散度的分类方法，超越了传统的使用交叉熵的方法。通过提取基于变分表示的$f$散度目标函数，该方法采用贝叶斯视角将分类任务视为最大后验概率问题，并提出了五个采用不同$f$散度的后验概率估计器。此外，通过一种自底向上的方法，还提出了一种新的基于平移对数的$f$散度的目标函数。这项研究在理论上证明了收敛性的属性。",
    "en_tdlr": "This research proposes a classification method based on $f$-divergence, going beyond the traditional use of cross-entropy. By extracting the objective functions based on variational representation of $f$-divergence, this method formulates the classification task as a maximum a posteriori probability problem from a Bayesian perspective and presents five posterior probability estimators utilizing different $f$-divergences. Additionally, a new objective function based on shifted log ($SL$) $f$-divergence is introduced through a bottom-up approach. The convergence property of the proposed method is theoretically proven."
}