{
    "title": "LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models. (arXiv:2401.00907v1 [cs.LG])",
    "abstract": "Fine-tuning Large Language Models (LLMs) adapts a trained model to specific downstream tasks, significantly improving task-specific performance. Supervised Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce desired answers. However, LLMs trained with SFT sometimes make simple mistakes and result in hallucinations on reasoning tasks such as question-answering. Without external feedback, it is difficult for SFT to learn a good mapping between the question and the desired answer, especially with a small dataset. This paper introduces an alternative to SFT called Natural Language Feedback for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they will receive from an annotator. We find that requiring such reflection can significantly improve the accuracy in in-domain question-answering tasks, providing a promising direction for the application of natural language feedback in the realm of SFT LLMs. Additional ablation studies show that the portion",
    "link": "http://arxiv.org/abs/2401.00907",
    "context": "Title: LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models. (arXiv:2401.00907v1 [cs.LG])\nAbstract: Fine-tuning Large Language Models (LLMs) adapts a trained model to specific downstream tasks, significantly improving task-specific performance. Supervised Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce desired answers. However, LLMs trained with SFT sometimes make simple mistakes and result in hallucinations on reasoning tasks such as question-answering. Without external feedback, it is difficult for SFT to learn a good mapping between the question and the desired answer, especially with a small dataset. This paper introduces an alternative to SFT called Natural Language Feedback for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they will receive from an annotator. We find that requiring such reflection can significantly improve the accuracy in in-domain question-answering tasks, providing a promising direction for the application of natural language feedback in the realm of SFT LLMs. Additional ablation studies show that the portion",
    "path": "papers/24/01/2401.00907.json",
    "total_tokens": 961,
    "translated_title": "LaFFi: 利用混合自然语言反馈来优化语言模型的微调",
    "translated_abstract": "大型语言模型（LLM）的微调可以将训练好的模型适应特定的下游任务，并显著提高任务特定性能。监督微调（SFT）是一种常见的方法，其中LLM被训练成产生期望的答案。然而，使用SFT训练的LLM在推理任务（如问答）中有时会出现简单错误和幻觉。在没有外部反馈的情况下，SFT很难学习到问题和期望答案之间的良好映射，特别是在数据集较小的情况下。本文介绍了一种名为自然语言反馈微调LLM（LaFFi）的替代方法。LaFFi要求LLM直接预测标注者将会给出的反馈。我们发现，这样的反思要求可以显著提高在领域内问答任务中的准确性，为在SFT LLM领域中应用自然语言反馈提供了一个有前途的方向。额外的消融研究表明这种方法的一部分可以被替代。",
    "tldr": "LaFFi是一种用于微调语言模型的替代方法，通过要求模型预测标注者将会给出的反馈，显著提高了在问答任务中的准确性，为应用自然语言反馈提供了一个有前途的方向。",
    "en_tdlr": "LaFFi is an alternative approach for fine-tuning language models. By requiring the model to predict the feedback it will receive from annotators, it significantly improves accuracy in question-answering tasks and provides a promising direction for the application of natural language feedback."
}