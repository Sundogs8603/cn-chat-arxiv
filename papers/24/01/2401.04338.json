{
    "title": "G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale Recommender Systems. (arXiv:2401.04338v1 [cs.LG])",
    "abstract": "Recently, a new paradigm, meta learning, has been widely applied to Deep Learning Recommendation Models (DLRM) and significantly improves statistical performance, especially in cold-start scenarios. However, the existing systems are not tailored for meta learning based DLRM models and have critical problems regarding efficiency in distributed training in the GPU cluster. It is because the conventional deep learning pipeline is not optimized for two task-specific datasets and two update loops in meta learning. This paper provides a high-performance framework for large-scale training for Optimization-based Meta DLRM models over the \\textbf{G}PU cluster, namely \\textbf{G}-Meta. Firstly, G-Meta utilizes both data parallelism and model parallelism with careful orchestration regarding computation and communication efficiency, to enable high-speed distributed training. Secondly, it proposes a Meta-IO pipeline for efficient data ingestion to alleviate the I/O bottleneck. Various experimental r",
    "link": "http://arxiv.org/abs/2401.04338",
    "context": "Title: G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale Recommender Systems. (arXiv:2401.04338v1 [cs.LG])\nAbstract: Recently, a new paradigm, meta learning, has been widely applied to Deep Learning Recommendation Models (DLRM) and significantly improves statistical performance, especially in cold-start scenarios. However, the existing systems are not tailored for meta learning based DLRM models and have critical problems regarding efficiency in distributed training in the GPU cluster. It is because the conventional deep learning pipeline is not optimized for two task-specific datasets and two update loops in meta learning. This paper provides a high-performance framework for large-scale training for Optimization-based Meta DLRM models over the \\textbf{G}PU cluster, namely \\textbf{G}-Meta. Firstly, G-Meta utilizes both data parallelism and model parallelism with careful orchestration regarding computation and communication efficiency, to enable high-speed distributed training. Secondly, it proposes a Meta-IO pipeline for efficient data ingestion to alleviate the I/O bottleneck. Various experimental r",
    "path": "papers/24/01/2401.04338.json",
    "total_tokens": 911,
    "translated_title": "G-Meta: 大规模推荐系统中的GPU集群分布式元学习",
    "translated_abstract": "最近，一个名为元学习的新范式被广泛应用于深度学习推荐模型(DLRM)，并在统计性能方面取得了显著的改进，特别是在冷启动场景中。然而，现有的系统并没有为基于元学习的DLRM模型量身定制，并且在GPU集群的分布式训练中存在关于效率的重要问题。这是因为传统的深度学习流水线对于元学习中的两个任务特定数据集和两个更新循环并没有进行优化。本文提出了一个高性能框架，用于在GPU集群上进行基于优化的元DLRM模型的大规模训练，即G-Meta。首先，G-Meta利用数据并行性和模型并行性，并对计算和通信效率进行精心协调，实现高速分布式训练。其次，它提出了一个用于高效数据摄入的元-IO流水线，以缓解输入/输出瓶颈。进行了各种实验",
    "tldr": "本文提出了一个用于大规模推荐系统中的GPU集群分布式元学习的高性能框架G-Meta，通过利用数据并行性和模型并行性以及设计高效的元-IO流水线，实现了高速分布式训练。",
    "en_tdlr": "This paper proposes a high-performance framework called G-Meta for distributed meta learning in GPU clusters for large-scale recommender systems. It utilizes data parallelism and model parallelism, along with an efficient Meta-IO pipeline, to achieve high-speed distributed training."
}