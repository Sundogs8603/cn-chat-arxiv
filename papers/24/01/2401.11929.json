{
    "title": "The Bigger the Better? Rethinking the Effective Model Scale in Long-term Time Series Forecasting",
    "abstract": "Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, distinguished by its focus on extensive input sequences, in contrast to the constrained lengths typical of traditional approaches. While longer sequences inherently convey richer information, potentially enhancing predictive precision, prevailing techniques often respond by escalating model complexity. These intricate models can inflate into millions of parameters, incorporating parameter-intensive elements like positional encodings, feed-forward networks and self-attention mechanisms. This complexity, however, leads to prohibitive model scale, particularly given the time series data's semantic simplicity. Motivated by the pursuit of parsimony, our research employs conditional correlation and auto-correlation as investigative tools, revealing significant redundancies within the input data. Leveraging these insights, we introduce the HDformer, a lightweight Transformer variant enhanced with ",
    "link": "https://arxiv.org/abs/2401.11929",
    "context": "Title: The Bigger the Better? Rethinking the Effective Model Scale in Long-term Time Series Forecasting\nAbstract: Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, distinguished by its focus on extensive input sequences, in contrast to the constrained lengths typical of traditional approaches. While longer sequences inherently convey richer information, potentially enhancing predictive precision, prevailing techniques often respond by escalating model complexity. These intricate models can inflate into millions of parameters, incorporating parameter-intensive elements like positional encodings, feed-forward networks and self-attention mechanisms. This complexity, however, leads to prohibitive model scale, particularly given the time series data's semantic simplicity. Motivated by the pursuit of parsimony, our research employs conditional correlation and auto-correlation as investigative tools, revealing significant redundancies within the input data. Leveraging these insights, we introduce the HDformer, a lightweight Transformer variant enhanced with ",
    "path": "papers/24/01/2401.11929.json",
    "total_tokens": 882,
    "translated_title": "“越大越好？”重新思考长期时间序列预测中的有效模型规模",
    "translated_abstract": "长期时间序列预测（LTSF）是时间序列分析中的一个重要前沿，其特点是关注于大量输入序列，与传统方法中的有限长度相比有所不同。尽管更长的序列本质上传达了更丰富的信息，可能提高了预测的精度，但目前的技术往往通过提高模型复杂性来应对。这些复杂的模型可以膨胀为数百万个参数，包括位置编码、前馈网络和自注意机制等参数密集型元素。然而，这种复杂性导致了禁止性的模型规模，特别是考虑到时间序列数据的语义简单性。出于追求简洁性的动机，我们的研究利用条件相关性和自相关性作为调查工具，揭示了输入数据中的显著冗余。借助这些见解，我们引入了HDformer，这是一种轻量级的Transformer变体，经过增强，使用蒸馏技术和快速网络连接层来降低模型复杂性。",
    "tldr": "本研究通过调查条件相关性和自相关性，揭示了输入数据中的冗余性，并提出了HDformer，这是一种轻量级的Transformer变种，利用蒸馏技术和快速网络连接层来降低模型复杂性。"
}