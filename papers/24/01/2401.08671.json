{
    "title": "DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference. (arXiv:2401.08671v1 [cs.PF])",
    "abstract": "The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking ",
    "link": "http://arxiv.org/abs/2401.08671",
    "context": "Title: DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference. (arXiv:2401.08671v1 [cs.PF])\nAbstract: The deployment and scaling of large language models (LLMs) have become critical as they permeate various applications, demanding high-throughput and low-latency serving systems. Existing frameworks struggle to balance these requirements, especially for workloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced implementation supports a range of models and offers both non-persistent and persistent deployment options, catering to diverse user scenarios from interactive sessions to long-running applications. We present a detailed benchmarking ",
    "path": "papers/24/01/2401.08671.json",
    "total_tokens": 1024,
    "translated_title": "DeepSpeed-FastGen: 通过MII和DeepSpeed-Inference实现高吞吐量的LLMs文本生成",
    "translated_abstract": "部署和扩展大型语言模型（LLMs）已经变得至关重要，因为它们在各种应用中渗透，并要求高吞吐量和低延迟的服务系统。现有框架对于具有长提示的工作负载很难平衡这些要求。本文介绍了DeepSpeed-FastGen，这是一个采用动态SplitFuse的系统，它采用了一种新颖的提示和生成组合策略，相比于vLLM等最先进的系统，实现了高达2.3倍的有效吞吐量提升，平均降低了2倍的延迟，并降低了高达3.7倍（令牌级）的尾延迟。我们利用DeepSpeed-MII和DeepSpeed-Inference的协同组合，为LLMs提供了高效易用的服务系统。DeepSpeed-FastGen的先进实现支持一系列模型，并提供非持久和持久部署选项，适用于从交互式会话到长时间运行应用的各种用户场景。我们提供了详细的基准测试。",
    "tldr": "本文介绍了DeepSpeed-FastGen，该系统采用了动态SplitFuse策略，通过MII和DeepSpeed-Inference实现了高吞吐量和低延迟的LLMs文本生成，相比于最先进的系统，实现了高达2.3倍的有效吞吐量提升，平均降低了2倍的延迟，并降低了高达3.7倍的尾延迟。",
    "en_tdlr": "This paper introduces DeepSpeed-FastGen, a system that uses Dynamic SplitFuse strategy and MII and DeepSpeed-Inference to achieve high throughput and low latency text generation for LLMs. It achieves up to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower tail latency compared to state-of-the-art systems."
}