{
    "title": "Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study. (arXiv:2401.12789v1 [cs.CL])",
    "abstract": "In the era of large models, the autoregressive nature of decoding often results in latency serving as a significant bottleneck. We propose a non-autoregressive LM-fused ASR system that effectively leverages the parallelization capabilities of accelerator hardware. Our approach combines the Universal Speech Model (USM) and the PaLM 2 language model in per-segment scoring mode, achieving an average relative WER improvement across all languages of 10.8% on FLEURS and 3.6% on YouTube captioning. Furthermore, our comprehensive ablation study analyzes key parameters such as LLM size, context length, vocabulary size, fusion methodology. For instance, we explore the impact of LLM size ranging from 128M to 340B parameters on ASR performance. This study provides valuable insights into the factors influencing the effectiveness of practical large-scale LM-fused speech recognition systems.",
    "link": "http://arxiv.org/abs/2401.12789",
    "context": "Title: Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study. (arXiv:2401.12789v1 [cs.CL])\nAbstract: In the era of large models, the autoregressive nature of decoding often results in latency serving as a significant bottleneck. We propose a non-autoregressive LM-fused ASR system that effectively leverages the parallelization capabilities of accelerator hardware. Our approach combines the Universal Speech Model (USM) and the PaLM 2 language model in per-segment scoring mode, achieving an average relative WER improvement across all languages of 10.8% on FLEURS and 3.6% on YouTube captioning. Furthermore, our comprehensive ablation study analyzes key parameters such as LLM size, context length, vocabulary size, fusion methodology. For instance, we explore the impact of LLM size ranging from 128M to 340B parameters on ASR performance. This study provides valuable insights into the factors influencing the effectiveness of practical large-scale LM-fused speech recognition systems.",
    "path": "papers/24/01/2401.12789.json",
    "total_tokens": 921,
    "translated_title": "多语种非自回归ASR与大型语言模型融合：一项全面研究",
    "translated_abstract": "在大型模型时代，解码的自回归特性经常导致延迟成为一个重要的瓶颈。我们提出了一种非自回归的语言模型融合ASR系统，有效地利用了加速器硬件的并行能力。我们的方法将通用语音模型（USM）和PaLM 2语言模型结合在每个片段的评分模式下，实现了在所有语言上的平均相对WER改善10.8%的FLEURS，并在YouTube字幕上实现了3.6%的改善。此外，我们的全面消融研究分析了关键参数，如LLM大小、上下文长度、词汇表大小和融合方法。例如，我们研究了LLM大小从128M到340B参数对ASR性能的影响。这项研究提供了对影响实际大规模语言模型融合语音识别系统效果的因素的有价值的见解。",
    "tldr": "本研究提出了一种多语种非自回归ASR系统，通过融合大型语言模型，实现了在不同语言上的平均相对WER改善，并通过全面的消融研究分析了影响实际大规模语言模型融合语音识别系统效果的因素。",
    "en_tdlr": "This study proposes a multilingual and fully non-autoregressive ASR system that improves average relative WER by leveraging large language model fusion, and provides valuable insights into the factors influencing the effectiveness of practical large-scale LM-fused speech recognition systems through a comprehensive ablation study."
}