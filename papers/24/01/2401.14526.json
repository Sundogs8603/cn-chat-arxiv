{
    "title": "MEDs for PETs: Multilingual Euphemism Disambiguation for Potentially Euphemistic Terms. (arXiv:2401.14526v1 [cs.CL])",
    "abstract": "This study investigates the computational processing of euphemisms, a universal linguistic phenomenon, across multiple languages. We train a multilingual transformer model (XLM-RoBERTa) to disambiguate potentially euphemistic terms (PETs) in multilingual and cross-lingual settings. In line with current trends, we demonstrate that zero-shot learning across languages takes place. We also show cases where multilingual models perform better on the task compared to monolingual models by a statistically significant margin, indicating that multilingual data presents additional opportunities for models to learn about cross-lingual, computational properties of euphemisms. In a follow-up analysis, we focus on universal euphemistic \"categories\" such as death and bodily functions among others. We test to see whether cross-lingual data of the same domain is more important than within-language data of other domains to further understand the nature of the cross-lingual transfer.",
    "link": "http://arxiv.org/abs/2401.14526",
    "context": "Title: MEDs for PETs: Multilingual Euphemism Disambiguation for Potentially Euphemistic Terms. (arXiv:2401.14526v1 [cs.CL])\nAbstract: This study investigates the computational processing of euphemisms, a universal linguistic phenomenon, across multiple languages. We train a multilingual transformer model (XLM-RoBERTa) to disambiguate potentially euphemistic terms (PETs) in multilingual and cross-lingual settings. In line with current trends, we demonstrate that zero-shot learning across languages takes place. We also show cases where multilingual models perform better on the task compared to monolingual models by a statistically significant margin, indicating that multilingual data presents additional opportunities for models to learn about cross-lingual, computational properties of euphemisms. In a follow-up analysis, we focus on universal euphemistic \"categories\" such as death and bodily functions among others. We test to see whether cross-lingual data of the same domain is more important than within-language data of other domains to further understand the nature of the cross-lingual transfer.",
    "path": "papers/24/01/2401.14526.json",
    "total_tokens": 971,
    "translated_title": "MEDs for PETs: 多语言委婉语消歧为潜在委婉术语",
    "translated_abstract": "本研究调查了跨多语言的委婉语计算处理，委婉语是一种普遍的语言现象。我们使用一种多语言转换器模型（XLM-RoBERTa）来消除多语言和跨语言环境中的潜在委婉术语（PETs）的歧义。符合当前趋势，我们证明跨语言的零样本学习是可能的。我们还展示了在统计上存在显著优势的情况下，多语言模型在该任务上的表现比单语言模型更好，这表明多语言数据为模型学习关于跨语言委婉语计算性质提供了额外的机会。在后续分析中，我们着重考察了包括死亡和身体功能在内的通用委婉术语类别。我们测试了跨语言数据是否比同一领域的语言内数据更重要，以进一步了解跨语言转移的性质。",
    "tldr": "本研究讨论了跨多语言的委婉语处理，并通过多语言转换器模型成功消歧潜在委婉术语。结果显示多语言模型在该任务上的表现优于单语言模型，从而证明多语言数据对于跨语言委婉语的计算特性具有额外的潜力。"
}