{
    "title": "Regularizing Discrimination in Optimal Policy Learning with Distributional Targets",
    "abstract": "A decision maker typically (i) incorporates training data to learn about the relative effectiveness of the treatments, and (ii) chooses an implementation mechanism that implies an \"optimal\" predicted outcome distribution according to some target functional. Nevertheless, a discrimination-aware decision maker may not be satisfied achieving said optimality at the cost of heavily discriminating against subgroups of the population, in the sense that the outcome distribution in a subgroup deviates strongly from the overall optimal outcome distribution. We study a framework that allows the decision maker to penalize for such deviations, while allowing for a wide range of target functionals and discrimination measures to be employed. We establish regret and consistency guarantees for empirical success policies with data-driven tuning parameters, and provide numerical results. Furthermore, we briefly illustrate the methods in two empirical settings.",
    "link": "https://arxiv.org/abs/2401.17909",
    "context": "Title: Regularizing Discrimination in Optimal Policy Learning with Distributional Targets\nAbstract: A decision maker typically (i) incorporates training data to learn about the relative effectiveness of the treatments, and (ii) chooses an implementation mechanism that implies an \"optimal\" predicted outcome distribution according to some target functional. Nevertheless, a discrimination-aware decision maker may not be satisfied achieving said optimality at the cost of heavily discriminating against subgroups of the population, in the sense that the outcome distribution in a subgroup deviates strongly from the overall optimal outcome distribution. We study a framework that allows the decision maker to penalize for such deviations, while allowing for a wide range of target functionals and discrimination measures to be employed. We establish regret and consistency guarantees for empirical success policies with data-driven tuning parameters, and provide numerical results. Furthermore, we briefly illustrate the methods in two empirical settings.",
    "path": "papers/24/01/2401.17909.json",
    "total_tokens": 847,
    "translated_title": "优化策略学习中正则化歧视问题的研究",
    "translated_abstract": "决策者通常通过训练数据学习治疗的相对效果，并选择一个实施机制，该机制根据某个目标函数预测了“最优”结果分布。然而，一个意识到歧视问题的决策者可能不满意以严重歧视人群子组的代价来实现该优化，即在子组中的结果分布明显偏离整体最优结果分布。我们研究了一个框架，允许决策者惩罚这种偏差，并可以使用各种目标函数和歧视度量。我们对具有数据驱动调参的经验成功策略建立了遗憾和一致性保证，并提供了数值结果。此外，我们还对两个实证场景进行了简要说明。",
    "tldr": "为了解决优化策略学习中的歧视问题，研究者提出了一个框架，允许决策者通过惩罚来防止在特定人群中的不公平结果分布，该框架对目标函数和歧视度量具有很大的灵活性，通过数据驱动的参数调整，可以在实践中具备遗憾和一致性保证。",
    "en_tdlr": "In order to address discrimination in optimal policy learning, researchers propose a framework that allows decision makers to penalize deviation from optimal outcome distribution in specific subgroups. This framework provides flexibility in terms of target functionals and discrimination measures, and offers regret and consistency guarantees through data-driven tuning parameters."
}