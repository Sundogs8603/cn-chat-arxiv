{
    "title": "SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond the Memory Budget. (arXiv:2401.16757v1 [cs.LG])",
    "abstract": "Executing deep neural networks (DNNs) on edge artificial intelligence (AI) devices enables various autonomous mobile computing applications. However, the memory budget of edge AI devices restricts the number and complexity of DNNs allowed in such applications. Existing solutions, such as model compression or cloud offloading, reduce the memory footprint of DNN inference at the cost of decreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN into blocks and swap them in and out in order, such that large DNNs can execute within a small memory budget. Nevertheless, naive swapping on edge AI devices induces significant delays due to the redundant memory operations in the DNN development ecosystem for edge AI devices. To this end, we develop SwapNet, an efficient DNN block swapping middleware for edge AI devices. We systematically eliminate the unnecessary memory operations during block swapping while retaining compatible with the deep learning frameworks, GPU backends,",
    "link": "http://arxiv.org/abs/2401.16757",
    "context": "Title: SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond the Memory Budget. (arXiv:2401.16757v1 [cs.LG])\nAbstract: Executing deep neural networks (DNNs) on edge artificial intelligence (AI) devices enables various autonomous mobile computing applications. However, the memory budget of edge AI devices restricts the number and complexity of DNNs allowed in such applications. Existing solutions, such as model compression or cloud offloading, reduce the memory footprint of DNN inference at the cost of decreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN into blocks and swap them in and out in order, such that large DNNs can execute within a small memory budget. Nevertheless, naive swapping on edge AI devices induces significant delays due to the redundant memory operations in the DNN development ecosystem for edge AI devices. To this end, we develop SwapNet, an efficient DNN block swapping middleware for edge AI devices. We systematically eliminate the unnecessary memory operations during block swapping while retaining compatible with the deep learning frameworks, GPU backends,",
    "path": "papers/24/01/2401.16757.json",
    "total_tokens": 913,
    "translated_title": "SwapNet: 超出内存预算的边缘AI设备上进行DNN推理的高效交换技术",
    "translated_abstract": "在边缘人工智能（AI）设备上执行深度神经网络（DNN）可以实现各种自主移动计算应用。然而，边缘AI设备的内存预算限制了这些应用中允许的DNN数量和复杂性。现有的解决方案，如模型压缩或云卸载，减少了DNN推理的内存占用，但同时也降低了模型准确度或自主性。为了避免这些缺点，我们将DNN分解成块，并按顺序互相交换，以便在较小的内存预算下执行大型DNN。然而，在边缘AI设备上进行简单交换会引起显著的延迟，因为在边缘AI设备的DNN开发生态系统中存在冗余的内存操作。为此，我们开发了SwapNet，一种高效的边缘AI设备DNN块交换中间件。我们系统地消除了块交换过程中不必要的内存操作，同时保持与深度学习框架和GPU后端的兼容性。",
    "tldr": "SwapNet是一种高效的边缘AI设备DNN块交换中间件，在超出内存预算的情况下，通过分解DNN为块并进行交换，实现了大型DNN的高效执行。"
}