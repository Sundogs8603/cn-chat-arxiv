{
    "title": "Topology-aware Embedding Memory for Learning on Expanding Graphs. (arXiv:2401.13200v1 [cs.LG])",
    "abstract": "Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding graphs, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from $\\mathcal{O}(nd^L)$ to $\\mathcal{O}(n)$~\\footnote{$n$: memory budget, $d$: average node degree, $L$: the radius of the GNN receptive field}, but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subgraph via \\textit{Topology-aware Embeddings} ",
    "link": "http://arxiv.org/abs/2401.13200",
    "context": "Title: Topology-aware Embedding Memory for Learning on Expanding Graphs. (arXiv:2401.13200v1 [cs.LG])\nAbstract: Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding graphs, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from $\\mathcal{O}(nd^L)$ to $\\mathcal{O}(n)$~\\footnote{$n$: memory budget, $d$: average node degree, $L$: the radius of the GNN receptive field}, but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subgraph via \\textit{Topology-aware Embeddings} ",
    "path": "papers/24/01/2401.13200.json",
    "total_tokens": 798,
    "translated_title": "基于拓扑感知嵌入记忆的学习扩展图",
    "translated_abstract": "基于记忆回放的技术在连续学习中应用广泛，但是直接应用于不断扩展的图会导致潜在的内存爆炸问题。为了解决这个问题，我们系统分析了内存爆炸问题的关键挑战，并提出了一个通用框架，即Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM)，来解决这个问题。该框架不仅将内存空间复杂度从$\\mathcal{O}(nd^L)$降低到$\\mathcal{O}(n)$，还充分利用了拓扑信息进行记忆回放。",
    "tldr": "这篇论文提出了一个基于拓扑感知嵌入记忆的学习扩展图的框架，该框架可以解决在不断扩展的图上应用记忆回放技术导致的内存爆炸问题。",
    "en_tdlr": "This paper proposes a framework for learning on expanding graphs using topology-aware embedding memory, which addresses the memory explosion problem caused by applying memory replay techniques on continually expanding graphs."
}