{
    "title": "Towards an Adaptable and Generalizable Optimization Engine in Decision and Control: A Meta Reinforcement Learning Approach. (arXiv:2401.02508v1 [cs.LG])",
    "abstract": "Sampling-based model predictive control (MPC) has found significant success in optimal control problems with non-smooth system dynamics and cost function. Many machine learning-based works proposed to improve MPC by a) learning or fine-tuning the dynamics/ cost function, or b) learning to optimize for the update of the MPC controllers. For the latter, imitation learning-based optimizers are trained to update the MPC controller by mimicking the expert demonstrations, which, however, are expensive or even unavailable. More significantly, many sequential decision-making problems are in non-stationary environments, requiring that an optimizer should be adaptable and generalizable to update the MPC controller for solving different tasks. To address those issues, we propose to learn an optimizer based on meta-reinforcement learning (RL) to update the controllers. This optimizer does not need expert demonstration and can enable fast adaptation (e.g., few-shots) when it is deployed in unseen c",
    "link": "http://arxiv.org/abs/2401.02508",
    "context": "Title: Towards an Adaptable and Generalizable Optimization Engine in Decision and Control: A Meta Reinforcement Learning Approach. (arXiv:2401.02508v1 [cs.LG])\nAbstract: Sampling-based model predictive control (MPC) has found significant success in optimal control problems with non-smooth system dynamics and cost function. Many machine learning-based works proposed to improve MPC by a) learning or fine-tuning the dynamics/ cost function, or b) learning to optimize for the update of the MPC controllers. For the latter, imitation learning-based optimizers are trained to update the MPC controller by mimicking the expert demonstrations, which, however, are expensive or even unavailable. More significantly, many sequential decision-making problems are in non-stationary environments, requiring that an optimizer should be adaptable and generalizable to update the MPC controller for solving different tasks. To address those issues, we propose to learn an optimizer based on meta-reinforcement learning (RL) to update the controllers. This optimizer does not need expert demonstration and can enable fast adaptation (e.g., few-shots) when it is deployed in unseen c",
    "path": "papers/24/01/2401.02508.json",
    "total_tokens": 773,
    "translated_title": "在决策与控制中实现一种适应性和可推广的优化引擎：一种元强化学习方法",
    "translated_abstract": "基于采样的模型预测控制在具有非平滑系统动力学和成本函数的最优控制问题中取得了显著的成功。许多基于机器学习的工作提出通过学习或微调动力学/成本函数或通过学习优化更新模型预测控制器来改进模型预测控制。为了解决这些问题，我们提出了一种基于元强化学习的优化器来更新控制器。这种优化器不需要专家示范，并且可以在面对未知环境时实现快速适应（例如，少量示范）。",
    "tldr": "本文提出了一种基于元强化学习的优化器，可以不需要专家示范并在非稳态环境下实现快速适应，用于更新模型预测控制器。",
    "en_tdlr": "This paper proposes a meta reinforcement learning optimizer that can update model predictive controllers without expert demonstrations and achieve fast adaptation in non-stationary environments."
}