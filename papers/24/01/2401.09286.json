{
    "title": "Deployable Reinforcement Learning with Variable Control Rate",
    "abstract": "arXiv:2401.09286v2 Announce Type: replace-cross  Abstract: Deploying controllers trained with Reinforcement Learning (RL) on real robots can be challenging: RL relies on agents' policies being modeled as Markov Decision Processes (MDPs), which assume an inherently discrete passage of time. The use of MDPs results in that nearly all RL-based control systems employ a fixed-rate control strategy with a period (or time step) typically chosen based on the developer's experience or specific characteristics of the application environment. Unfortunately, the system should be controlled at the highest, worst-case frequency to ensure stability, which can demand significant computational and energy resources and hinder the deployability of the controller on onboard hardware. Adhering to the principles of reactive programming, we surmise that applying control actions only when necessary enables the use of simpler hardware and helps reduce energy consumption. We challenge the fixed frequency assump",
    "link": "https://arxiv.org/abs/2401.09286",
    "context": "Title: Deployable Reinforcement Learning with Variable Control Rate\nAbstract: arXiv:2401.09286v2 Announce Type: replace-cross  Abstract: Deploying controllers trained with Reinforcement Learning (RL) on real robots can be challenging: RL relies on agents' policies being modeled as Markov Decision Processes (MDPs), which assume an inherently discrete passage of time. The use of MDPs results in that nearly all RL-based control systems employ a fixed-rate control strategy with a period (or time step) typically chosen based on the developer's experience or specific characteristics of the application environment. Unfortunately, the system should be controlled at the highest, worst-case frequency to ensure stability, which can demand significant computational and energy resources and hinder the deployability of the controller on onboard hardware. Adhering to the principles of reactive programming, we surmise that applying control actions only when necessary enables the use of simpler hardware and helps reduce energy consumption. We challenge the fixed frequency assump",
    "path": "papers/24/01/2401.09286.json",
    "total_tokens": 827,
    "translated_title": "具有可变控制频率的可部署强化学习系统",
    "translated_abstract": "部署经过强化学习（RL）训练的控制器到真实机器人上可能具有挑战性：RL依赖于被建模为马尔可夫决策过程（MDPs）的代理策略，这些策略假设时间离散。使用MDPs导致几乎所有基于RL的控制系统都采用固定速率控制策略，周期（或时间步长）通常基于开发者经验或应用环境的特定特征选择。不幸的是，为了确保稳定性，系统应以最高、最坏情况频率进行控制，这可能需要大量的计算和能源资源，并且阻碍控制器在板载硬件上的可部署性。遵循反应式编程原则，我们认为仅在必要时应用控制动作可以使用更简单的硬件，并有助于减少能源消耗。我们挑战了固定频率的假设。",
    "tldr": "应用可变控制频率的强化学习系统，以简化硬件使用、降低能源消耗，并挑战固定频率控制的假设",
    "en_tdlr": "Deploying a reinforcement learning system with variable control rates can simplify hardware usage, reduce energy consumption, and challenge the assumption of fixed frequency control."
}