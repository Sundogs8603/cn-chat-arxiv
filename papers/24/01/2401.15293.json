{
    "title": "SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection. (arXiv:2401.15293v1 [cs.CV])",
    "abstract": "Vision transformers are known to be more computationally and data-intensive than CNN models. These transformer models such as ViT, require all the input image tokens to learn the relationship among them. However, many of these tokens are not informative and may contain irrelevant information such as unrelated background or unimportant scenery. These tokens are overlooked by the multi-head self-attention (MHSA), resulting in many redundant and unnecessary computations in MHSA and the feed-forward network (FFN). In this work, we propose a method to optimize the amount of unnecessary interactions between unimportant tokens by separating and sending them through a different low-cost computational path. Our method does not add any parameters to the ViT model and aims to find the best trade-off between training throughput and achieving a 0% loss in the Top-1 accuracy of the final model. Our experimental results on training ViT-small from scratch show that SkipViT is capable of effectively dr",
    "link": "http://arxiv.org/abs/2401.15293",
    "context": "Title: SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection. (arXiv:2401.15293v1 [cs.CV])\nAbstract: Vision transformers are known to be more computationally and data-intensive than CNN models. These transformer models such as ViT, require all the input image tokens to learn the relationship among them. However, many of these tokens are not informative and may contain irrelevant information such as unrelated background or unimportant scenery. These tokens are overlooked by the multi-head self-attention (MHSA), resulting in many redundant and unnecessary computations in MHSA and the feed-forward network (FFN). In this work, we propose a method to optimize the amount of unnecessary interactions between unimportant tokens by separating and sending them through a different low-cost computational path. Our method does not add any parameters to the ViT model and aims to find the best trade-off between training throughput and achieving a 0% loss in the Top-1 accuracy of the final model. Our experimental results on training ViT-small from scratch show that SkipViT is capable of effectively dr",
    "path": "papers/24/01/2401.15293.json",
    "total_tokens": 852,
    "translated_title": "SkipViT: 使用令牌级跳跃连接加速Vision Transformers",
    "translated_abstract": "Vision transformers被认为比CNN模型更具计算和数据密集性。这些Transformer模型，如ViT，需要所有输入图像令牌来学习它们之间的关系。然而，许多这些令牌并不信息丰富，可能包含无关的背景或不重要的场景等无关信息。这些令牌被多头自注意力（MHSA）忽略，导致MHSA和前馈网络（FFN）中存在许多冗余和不必要的计算。在这项工作中，我们提出了一种方法，通过将这些不重要的令牌分离并通过不同的低成本计算路径发送，来优化不必要的交互量。我们的方法不会给ViT模型添加任何参数，并旨在在训练吞吐量和最终模型的Top-1准确率损失为0%之间找到最佳平衡。我们对从头开始训练ViT-small的实验结果表明，SkipViT能够有效地提高训练速度。",
    "tldr": "SkipViT通过令牌级跳跃连接将不重要的图像令牌分离，以提高Vision Transformers的训练速度，而不影响最终模型的准确率。",
    "en_tdlr": "SkipViT separates unimportant image tokens using token-level skip connection to improve training speed of Vision Transformers without affecting the final model's accuracy."
}