{
    "title": "Improving Domain Adaptation through Extended-Text Reading Comprehension. (arXiv:2401.07284v2 [cs.CL] UPDATED)",
    "abstract": "To enhance the domain-specific capabilities of large language models, continued pre-training on a domain-specific corpus is a prevalent method. Recent work demonstrates that adapting models using reading comprehension data formatted by regex-based patterns can significantly improve performance on domain-specific tasks. However, regex-based patterns are incapable of parsing raw corpora using domain-specific knowledge. Furthermore, the question and answer pairs are extracted directly from the corpus in predefined formats offers limited context. To address this limitation, we improve reading comprehension via LLM and clustering. LLM focuses on leveraging domain knowledge within the corpus to refine comprehension stage, while clustering supplies relevant knowledge by extending the context to enrich reading stage. Additionally, our method incorporates parameter-efficient fine-tuning to improve the efficiency of domain adaptation. In comparison to AdaptLLM, our method achieves an improvement",
    "link": "http://arxiv.org/abs/2401.07284",
    "context": "Title: Improving Domain Adaptation through Extended-Text Reading Comprehension. (arXiv:2401.07284v2 [cs.CL] UPDATED)\nAbstract: To enhance the domain-specific capabilities of large language models, continued pre-training on a domain-specific corpus is a prevalent method. Recent work demonstrates that adapting models using reading comprehension data formatted by regex-based patterns can significantly improve performance on domain-specific tasks. However, regex-based patterns are incapable of parsing raw corpora using domain-specific knowledge. Furthermore, the question and answer pairs are extracted directly from the corpus in predefined formats offers limited context. To address this limitation, we improve reading comprehension via LLM and clustering. LLM focuses on leveraging domain knowledge within the corpus to refine comprehension stage, while clustering supplies relevant knowledge by extending the context to enrich reading stage. Additionally, our method incorporates parameter-efficient fine-tuning to improve the efficiency of domain adaptation. In comparison to AdaptLLM, our method achieves an improvement",
    "path": "papers/24/01/2401.07284.json",
    "total_tokens": 830,
    "translated_title": "通过扩展文本阅读理解提高领域适应性",
    "translated_abstract": "为了增强大型语言模型的领域特定能力，对领域特定语料库进行持续预训练是一种流行的方法。最近的研究表明，使用基于正则表达式模式格式化的阅读理解数据来调整模型可以显著提高领域特定任务的性能。然而，基于正则表达式模式无法使用领域特定知识解析原始语料库。此外，问题和答案对是直接从语料库中以预定义的格式提取的，提供了有限的上下文。为解决这一限制，我们通过LLM和聚类改进了阅读理解。LLM专注于利用语料库中的领域知识来优化理解阶段，而聚类通过扩展上下文来丰富阅读阶段提供相关知识。此外，我们的方法还结合了高效参数微调来提高领域适应的效率。与AdaptLLM相比，我们的方法取得了改进",
    "tldr": "通过扩展文本阅读理解，结合领域知识和聚类，以及参数微调的方法，可以显著提高领域适应性。",
    "en_tdlr": "Improving domain adaptation can be achieved by extending text reading comprehension, leveraging domain knowledge through LLM, clustering to enrich context, and incorporating parameter-efficient fine-tuning."
}