{
    "title": "Can generative AI and ChatGPT outperform humans on cognitive-demanding problem-solving tasks in science?. (arXiv:2401.15081v1 [cs.AI])",
    "abstract": "This study aimed to examine an assumption that generative artificial intelligence (GAI) tools can overcome the cognitive intensity that humans suffer when solving problems. We compared the performance of ChatGPT and GPT-4 on 2019 NAEP science assessments with students by cognitive demands of the items. Fifty-four tasks were coded by experts using a two-dimensional cognitive load framework, including task cognitive complexity and dimensionality. ChatGPT and GPT-4 responses were scored using the scoring keys of NAEP. The analysis of the available data was based on the average student ability scores for students who answered each item correctly and the percentage of students who responded to individual items. Results showed that both ChatGPT and GPT-4 consistently outperformed most students who answered the NAEP science assessments. As the cognitive demand for NAEP tasks increases, statistically higher average student ability scores are required to correctly address the questions. This pa",
    "link": "http://arxiv.org/abs/2401.15081",
    "context": "Title: Can generative AI and ChatGPT outperform humans on cognitive-demanding problem-solving tasks in science?. (arXiv:2401.15081v1 [cs.AI])\nAbstract: This study aimed to examine an assumption that generative artificial intelligence (GAI) tools can overcome the cognitive intensity that humans suffer when solving problems. We compared the performance of ChatGPT and GPT-4 on 2019 NAEP science assessments with students by cognitive demands of the items. Fifty-four tasks were coded by experts using a two-dimensional cognitive load framework, including task cognitive complexity and dimensionality. ChatGPT and GPT-4 responses were scored using the scoring keys of NAEP. The analysis of the available data was based on the average student ability scores for students who answered each item correctly and the percentage of students who responded to individual items. Results showed that both ChatGPT and GPT-4 consistently outperformed most students who answered the NAEP science assessments. As the cognitive demand for NAEP tasks increases, statistically higher average student ability scores are required to correctly address the questions. This pa",
    "path": "papers/24/01/2401.15081.json",
    "total_tokens": 869,
    "translated_title": "《生成式人工智能和ChatGPT能否在科学领域的认知需求问题解决任务上胜过人类？》",
    "translated_abstract": "本研究旨在考察生成式人工智能（GAI）工具能否克服人类在解决问题时所遭受的认知强度。我们将ChatGPT和GPT-4在2019年NAEP科学评估中与学生的表现进行了比较，根据任务的认知需求对五十四个任务进行了编码，其中包括任务的认知复杂度和维度。使用NAEP的评分标准对ChatGPT和GPT-4的回答进行了评分。对可用数据的分析基于每个问题正确回答的学生的平均能力分数和回答每个问题的学生的百分比。结果显示，ChatGPT和GPT-4在大多数回答NAEP科学评估的学生中表现出色。随着NAEP任务的认知需求增加，需要具备统计上更高的平均学生能力分数才能正确回答问题。此项研究提供了关于GAI工具在科学问题解决任务中的性能表现的有益信息。",
    "tldr": "该研究探讨了生成式人工智能工具在解决认知强度的科学问题时是否能超越人类，并通过与学生的对比实验发现，ChatGPT和GPT-4在大多数情况下表现出色。"
}