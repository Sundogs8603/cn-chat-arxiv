{
    "title": "What's my role? Modelling responsibility for AI-based safety-critical systems. (arXiv:2401.09459v1 [cs.CY])",
    "abstract": "AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in the real world. These can pose a risk of harm to people and the environment. Reducing that risk is an overarching priority during development and operation. As more AI-SCS become autonomous, a layer of risk management via human intervention has been removed. Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Many authors have commented on the \"responsibility gap\" where it is difficult for developers and manufacturers to be held responsible for harmful behaviour of an AI-SCS. This is due to the complex development cycle for AI, uncertainty in AI performance, and dynamic operating environment. A human operator can become a \"liability sink\" absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating, and may not have understanding of.  This cross-dis",
    "link": "http://arxiv.org/abs/2401.09459",
    "context": "Title: What's my role? Modelling responsibility for AI-based safety-critical systems. (arXiv:2401.09459v1 [cs.CY])\nAbstract: AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in the real world. These can pose a risk of harm to people and the environment. Reducing that risk is an overarching priority during development and operation. As more AI-SCS become autonomous, a layer of risk management via human intervention has been removed. Following an accident it will be important to identify causal contributions and the different responsible actors behind those to learn from mistakes and prevent similar future events. Many authors have commented on the \"responsibility gap\" where it is difficult for developers and manufacturers to be held responsible for harmful behaviour of an AI-SCS. This is due to the complex development cycle for AI, uncertainty in AI performance, and dynamic operating environment. A human operator can become a \"liability sink\" absorbing blame for the consequences of AI-SCS outputs they weren't responsible for creating, and may not have understanding of.  This cross-dis",
    "path": "papers/24/01/2401.09459.json",
    "total_tokens": 1014,
    "translated_title": "我的角色是什么？对基于AI的安全关键系统的责任建模",
    "translated_abstract": "基于AI的安全关键系统（AI-SCS）正在越来越多地在现实世界中部署。这可能对人类和环境造成危害。在开发和运营过程中，降低风险是一项首要任务。随着越来越多的AI-SCS变得自主化，通过人类干预管理风险的一层已被移除。在事故发生后，重要的是要确定因果关系贡献和背后的不同责任主体，以从错误中汲取教训，防止类似的未来事件。许多作者已经评论了“责任差距”，即开发人员和制造商很难对AI-SCS的有害行为负责。这是由于AI开发周期复杂、AI性能不确定和动态操作环境所致。人类操作员可能成为“责任替罪羊”，因AI-SCS输出的后果承担责任，这些后果他们没有参与创建，也可能不理解。",
    "tldr": "许多作者已经评论了AI-SCS的“责任差距”，即开发人员和制造商难以对AI的有害行为负责，这是由于AI的复杂开发周期、性能不确定性和动态操作环境所致。在AI-SCS变得自主化后，人类操作员可能成为承担责任的替罪羊，他们可能承担由AI-SCS输出的后果，而这些后果他们没有参与创建，也可能不理解。"
}