{
    "title": "Accelerating Sinkhorn Algorithm with Sparse Newton Iterations. (arXiv:2401.12253v1 [math.OC])",
    "abstract": "Computing the optimal transport distance between statistical distributions is a fundamental task in machine learning. One remarkable recent advancement is entropic regularization and the Sinkhorn algorithm, which utilizes only matrix scaling and guarantees an approximated solution with near-linear runtime. Despite the success of the Sinkhorn algorithm, its runtime may still be slow due to the potentially large number of iterations needed for convergence. To achieve possibly super-exponential convergence, we present Sinkhorn-Newton-Sparse (SNS), an extension to the Sinkhorn algorithm, by introducing early stopping for the matrix scaling steps and a second stage featuring a Newton-type subroutine. Adopting the variational viewpoint that the Sinkhorn algorithm maximizes a concave Lyapunov potential, we offer the insight that the Hessian matrix of the potential function is approximately sparse. Sparsification of the Hessian results in a fast $O(n^2)$ per-iteration complexity, the same as t",
    "link": "http://arxiv.org/abs/2401.12253",
    "context": "Title: Accelerating Sinkhorn Algorithm with Sparse Newton Iterations. (arXiv:2401.12253v1 [math.OC])\nAbstract: Computing the optimal transport distance between statistical distributions is a fundamental task in machine learning. One remarkable recent advancement is entropic regularization and the Sinkhorn algorithm, which utilizes only matrix scaling and guarantees an approximated solution with near-linear runtime. Despite the success of the Sinkhorn algorithm, its runtime may still be slow due to the potentially large number of iterations needed for convergence. To achieve possibly super-exponential convergence, we present Sinkhorn-Newton-Sparse (SNS), an extension to the Sinkhorn algorithm, by introducing early stopping for the matrix scaling steps and a second stage featuring a Newton-type subroutine. Adopting the variational viewpoint that the Sinkhorn algorithm maximizes a concave Lyapunov potential, we offer the insight that the Hessian matrix of the potential function is approximately sparse. Sparsification of the Hessian results in a fast $O(n^2)$ per-iteration complexity, the same as t",
    "path": "papers/24/01/2401.12253.json",
    "total_tokens": 988,
    "translated_title": "使用稀疏牛顿迭代加速Sinkhorn算法",
    "translated_abstract": "在机器学习中，计算统计分布之间的最优传输距离是一项基本任务。最近的一项突破性进展是熵正则化和Sinkhorn算法，它只使用矩阵缩放并保证近似解的线性运行时间。尽管Sinkhorn算法取得了成功，但由于可能需要大量迭代来达到收敛，它的运行时间仍可能较慢。为了实现可能的超指数收敛，我们提出了Sinkhorn-Newton-Sparse（SNS），这是Sinkhorn算法的一个扩展，通过引入矩阵缩放步骤的提前停止和一个特征牛顿子程序的第二阶段来实现。采用Sinkhorn算法最大化凹性李雅普诺夫势的变分视角，我们得出结论，势函数的Hessian矩阵近似稀疏。稀疏化Hessian矩阵导致每次迭代的复杂性为快速的$O(n^2)$，与传统Sinkhorn算法相同。",
    "tldr": "该论文提出了一种扩展的Sinkhorn算法，通过引入提前停止和牛顿迭代子程序，实现了可能的超指数收敛。他们利用了Sinkhorn算法最大化凹性李雅普诺夫势的特性，发现了势函数的Hessian矩阵近似稀疏，从而将每次迭代的复杂性降低到了$O(n^2)$。"
}