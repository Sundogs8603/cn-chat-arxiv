{
    "title": "Synthesizing Moving People with 3D Control. (arXiv:2401.10889v1 [cs.CV])",
    "abstract": "In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to tha",
    "link": "http://arxiv.org/abs/2401.10889",
    "context": "Title: Synthesizing Moving People with 3D Control. (arXiv:2401.10889v1 [cs.CV])\nAbstract: In this paper, we present a diffusion model-based framework for animating people from a single image for a given target 3D motion sequence. Our approach has two core components: a) learning priors about invisible parts of the human body and clothing, and b) rendering novel body poses with proper clothing and texture. For the first part, we learn an in-filling diffusion model to hallucinate unseen parts of a person given a single image. We train this model on texture map space, which makes it more sample-efficient since it is invariant to pose and viewpoint. Second, we develop a diffusion-based rendering pipeline, which is controlled by 3D human poses. This produces realistic renderings of novel poses of the person, including clothing, hair, and plausible in-filling of unseen regions. This disentangled approach allows our method to generate a sequence of images that are faithful to the target motion in the 3D pose and, to the input image in terms of visual similarity. In addition to tha",
    "path": "papers/24/01/2401.10889.json",
    "total_tokens": 902,
    "translated_title": "使用3D控制合成移动人物",
    "translated_abstract": "本文提出了一种基于扩散模型的框架，用于从单张图像中为给定的目标3D运动序列生成人物动画。我们的方法包含两个核心组成部分：a) 学习关于人体和服装不可见部分的先验知识，b) 以适当的服装和纹理渲染新的人体姿势。对于第一部分，我们学习了一种填充扩散模型，以给定单张图像生成人物的不可见部分。我们在纹理映射空间上训练这个模型，使其对姿势和视角不变，从而提高了样本效率。其次，我们开发了一个基于扩散的渲染流水线，由3D人体姿势控制。这可以产生逼真的人物新姿势的渲染图像，包括服装、头发和未知区域的合理补充。这种分解的方法使我们的方法能够生成一系列图像，既符合3D姿势中的目标运动，也符合视觉上与输入图像的相似性。",
    "tldr": "本文提出了一种基于扩散模型的框架，用于从单张图像中生成具有逼真移动的人物动画，并成功处理了人体不可见部分的合成问题。",
    "en_tdlr": "This paper presents a diffusion model-based framework for synthesizing realistic animations of moving people from a single image, successfully addressing the challenge of synthesizing invisible parts of the human body."
}