{
    "title": "SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning. (arXiv:2401.03137v1 [cs.LG])",
    "abstract": "Alleviating overestimation bias is a critical challenge for deep reinforcement learning to achieve successful performance on more complex tasks or offline datasets containing out-of-distribution data. In order to overcome overestimation bias, ensemble methods for Q-learning have been investigated to exploit the diversity of multiple Q-functions. Since network initialization has been the predominant approach to promote diversity in Q-functions, heuristically designed diversity injection methods have been studied in the literature. However, previous studies have not attempted to approach guaranteed independence over an ensemble from a theoretical perspective. By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning. Specifically, we modify the intractable hypothesis testing criterion for the Q-ensemble independence into a tractable KL divergence ",
    "link": "http://arxiv.org/abs/2401.03137",
    "context": "Title: SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning. (arXiv:2401.03137v1 [cs.LG])\nAbstract: Alleviating overestimation bias is a critical challenge for deep reinforcement learning to achieve successful performance on more complex tasks or offline datasets containing out-of-distribution data. In order to overcome overestimation bias, ensemble methods for Q-learning have been investigated to exploit the diversity of multiple Q-functions. Since network initialization has been the predominant approach to promote diversity in Q-functions, heuristically designed diversity injection methods have been studied in the literature. However, previous studies have not attempted to approach guaranteed independence over an ensemble from a theoretical perspective. By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning. Specifically, we modify the intractable hypothesis testing criterion for the Q-ensemble independence into a tractable KL divergence ",
    "path": "papers/24/01/2401.03137.json",
    "total_tokens": 796,
    "translated_title": "SPQR:使用尖峰随机模型控制Q-集合的独立性，用于强化学习",
    "translated_abstract": "缓解过高估计偏差是深度强化学习在更复杂任务或包含超出分布数据的离线数据集上获得成功表现的关键挑战。为了克服过高估计偏差，研究了Q-learning的集成方法来利用多个Q函数的多样性。由于网络初始化一直是促进Q函数多样性的主要方法，因此在文献中研究了启发式设计的多样性注入方法。然而，先前的研究并未尝试从理论角度保证集成的独立性。通过引入基于随机矩阵理论的Q-集合独立性的新型正则化损失，我们提出了一种用于强化学习的尖峰Wishart Q-集合独立性正则化方法（SPQR）。",
    "tldr": "SPQR论文介绍了一种使用尖峰随机模型来控制强化学习中Q-集合的独立性的方法，通过引入基于随机矩阵理论的正则化损失来克服过高估计偏差。"
}