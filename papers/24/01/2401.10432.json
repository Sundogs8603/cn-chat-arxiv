{
    "title": "A2Q+: Improving Accumulator-Aware Weight Quantization. (arXiv:2401.10432v1 [cs.LG])",
    "abstract": "Quantization techniques commonly reduce the inference costs of neural networks by restricting the precision of weights and activations. Recent studies show that also reducing the precision of the accumulator can further improve hardware efficiency at the risk of numerical overflow, which introduces arithmetic errors that can degrade model accuracy. To avoid numerical overflow while maintaining accuracy, recent work proposed accumulator-aware quantization (A2Q), a quantization-aware training method that constrains model weights during training to safely use a target accumulator bit width during inference. Although this shows promise, we demonstrate that A2Q relies on an overly restrictive constraint and a sub-optimal weight initialization strategy that each introduce superfluous quantization error. To address these shortcomings, we introduce: (1) an improved bound that alleviates accumulator constraints without compromising overflow avoidance; and (2) a new strategy for initializing qua",
    "link": "http://arxiv.org/abs/2401.10432",
    "context": "Title: A2Q+: Improving Accumulator-Aware Weight Quantization. (arXiv:2401.10432v1 [cs.LG])\nAbstract: Quantization techniques commonly reduce the inference costs of neural networks by restricting the precision of weights and activations. Recent studies show that also reducing the precision of the accumulator can further improve hardware efficiency at the risk of numerical overflow, which introduces arithmetic errors that can degrade model accuracy. To avoid numerical overflow while maintaining accuracy, recent work proposed accumulator-aware quantization (A2Q), a quantization-aware training method that constrains model weights during training to safely use a target accumulator bit width during inference. Although this shows promise, we demonstrate that A2Q relies on an overly restrictive constraint and a sub-optimal weight initialization strategy that each introduce superfluous quantization error. To address these shortcomings, we introduce: (1) an improved bound that alleviates accumulator constraints without compromising overflow avoidance; and (2) a new strategy for initializing qua",
    "path": "papers/24/01/2401.10432.json",
    "total_tokens": 860,
    "translated_title": "A2Q+: 改进的累加器感知权重量化方法",
    "translated_abstract": "量化技术通常通过限制权重和激活函数的精度来减少神经网络的推理成本。最近的研究表明，减少累加器的精度还可以进一步提高硬件效率，但会增加数值溢出的风险，从而降低模型的准确性。为了在保持准确性的同时避免数值溢出，最近的研究提出了累加器感知量化（A2Q），这是一种量化感知训练方法，通过在训练期间约束模型权重，以在推理期间安全地使用目标累加器位宽。尽管这显示出很大的潜力，但我们证明了A2Q依赖于过于限制性的约束和次优的权重初始化策略，这些都引入了不必要的量化误差。为了解决这些缺点，我们引入了以下改进：（1）一种改进的约束方式，缓解累加器约束而不损害溢出避免；（2）一种新的权重初始化策略。",
    "tldr": "A2Q+是一种改进的累加器感知权重量化方法，通过改进约束和初始化策略，实现了在保持模型准确性的同时提高硬件效率。",
    "en_tdlr": "A2Q+ is an improved accumulator-aware weight quantization method that enhances hardware efficiency while maintaining model accuracy by improving constraints and initialization strategies."
}