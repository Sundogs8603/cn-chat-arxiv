{
    "title": "PIXAR: Auto-Regressive Language Modeling in Pixel Space. (arXiv:2401.03321v1 [cs.CL])",
    "abstract": "Recent works showed the possibility of building open-vocabulary large language models (LLMs) that directly operate on pixel representations and are implemented as encoder-decoder models that reconstruct masked image patches of rendered text. However, these pixel-based LLMs are limited to autoencoding tasks and cannot generate new text as images. As such, they cannot be used for open-answer or generative language tasks. In this work, we overcome this limitation and introduce PIXAR, the first pixel-based autoregressive LLM that does not rely on a pre-defined vocabulary for both input and output text. Consisting of only a decoder, PIXAR can answer free-form generative tasks while keeping the text representation learning performance on par with previous encoder-decoder models. Furthermore, we highlight the challenges to autoregressively generate non-blurred text as images and link this to the usual maximum likelihood objective. We propose a simple adversarial pretraining that significantly",
    "link": "http://arxiv.org/abs/2401.03321",
    "context": "Title: PIXAR: Auto-Regressive Language Modeling in Pixel Space. (arXiv:2401.03321v1 [cs.CL])\nAbstract: Recent works showed the possibility of building open-vocabulary large language models (LLMs) that directly operate on pixel representations and are implemented as encoder-decoder models that reconstruct masked image patches of rendered text. However, these pixel-based LLMs are limited to autoencoding tasks and cannot generate new text as images. As such, they cannot be used for open-answer or generative language tasks. In this work, we overcome this limitation and introduce PIXAR, the first pixel-based autoregressive LLM that does not rely on a pre-defined vocabulary for both input and output text. Consisting of only a decoder, PIXAR can answer free-form generative tasks while keeping the text representation learning performance on par with previous encoder-decoder models. Furthermore, we highlight the challenges to autoregressively generate non-blurred text as images and link this to the usual maximum likelihood objective. We propose a simple adversarial pretraining that significantly",
    "path": "papers/24/01/2401.03321.json",
    "total_tokens": 928,
    "translated_title": "PIXAR：像素空间中的自回归语言建模",
    "translated_abstract": "最近的研究显示可以构建基于像素表示的开放词汇量大语言模型（LLMs），这些模型以编码器-解码器模型的形式，重构遮蔽的图像文本补丁。然而，这些基于像素的LLMs仅限于自编码任务，无法生成新的文本作为图像。因此，它们不能用于开放式回答或生成式语言任务。在这项工作中，我们克服了这一限制，并引入了PIXAR，这是第一个不需要预定义词汇表的像素自回归LLM，用于输入和输出文本。PIXAR只有一个解码器，可以回答自由形式的生成任务，并且在文本表示学习性能上与以前的编码器-解码器模型持平。此外，我们强调了以自回归方式生成非模糊文本作为图像的挑战，并将其与通常的最大似然目标联系起来。我们提出了一个简单的对抗性预训练方法，该方法在很大程度上解决了这个问题。",
    "tldr": "本论文介绍了PIXAR，这是第一个像素自回归的语言模型，可以用于生成自由形式的文本作为图像，而不依赖于预定义的词汇表。同时，论文还提出了一个简单的对抗性预训练方法来解决生成非模糊文本的挑战。",
    "en_tdlr": "This paper introduces PIXAR, the first pixel-based autoregressive language model that can generate free-form text as images without relying on a pre-defined vocabulary. It also proposes a simple adversarial pretraining approach to address the challenge of generating non-blurred text as images."
}