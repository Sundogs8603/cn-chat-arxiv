{
    "title": "Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers. (arXiv:2401.04695v1 [cs.CL])",
    "abstract": "Factual questions typically can be answered correctly at different levels of granularity. For example, both ``August 4, 1961'' and ``1961'' are correct answers to the question ``When was Barack Obama born?''. Standard question answering (QA) evaluation protocols, however, do not explicitly take this into account and compare a predicted answer against answers of a single granularity level. In this work, we propose GRANOLA QA, a novel evaluation setting where a predicted answer is evaluated in terms of accuracy and informativeness against a set of multi-granularity answers. We present a simple methodology for enriching existing datasets with multi-granularity answers, and create GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm, called Decoding with Response Aggregation (DRAG), that is geared towards aligning the response granularity with the model's uncertainty. Our experiments show th",
    "link": "http://arxiv.org/abs/2401.04695",
    "context": "Title: Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers. (arXiv:2401.04695v1 [cs.CL])\nAbstract: Factual questions typically can be answered correctly at different levels of granularity. For example, both ``August 4, 1961'' and ``1961'' are correct answers to the question ``When was Barack Obama born?''. Standard question answering (QA) evaluation protocols, however, do not explicitly take this into account and compare a predicted answer against answers of a single granularity level. In this work, we propose GRANOLA QA, a novel evaluation setting where a predicted answer is evaluated in terms of accuracy and informativeness against a set of multi-granularity answers. We present a simple methodology for enriching existing datasets with multi-granularity answers, and create GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm, called Decoding with Response Aggregation (DRAG), that is geared towards aligning the response granularity with the model's uncertainty. Our experiments show th",
    "path": "papers/24/01/2401.04695.json",
    "total_tokens": 973,
    "translated_title": "缩小知识评估差距：多层次答案的开放领域问答",
    "translated_abstract": "事实性问题通常可以以不同层次的粒度正确回答。例如，“1961年8月4日”和“1961年”都是对“巴拉克·奥巴马是在什么时候出生的？”这个问题的正确答案。然而，标准的问答评估协议并没有明确考虑这一点，而是将预测的答案与单一粒度层次的答案进行比较。在这项工作中，我们提出了GRANOLA QA，一种新颖的评估设置，其中预测的答案根据准确性和信息量与一组多粒度答案进行评估。我们提出了一种简单的方法来丰富现有数据集的多粒度答案，并创建了GRANOLA-EQ，一个多粒度版本的EntityQuestions数据集。我们在GRANOLA-EQ上评估了一系列解码方法，包括一种新的算法，称为Decoding with Response Aggregation (DRAG)，该算法旨在将响应的粒度与模型的不确定性对齐。我们的实验显示...",
    "tldr": "本研究提出了一种名为GRANOLA QA的评估设置，在开放领域问答中使用多粒度答案来评估预测的答案的准确性和信息量。作者提出了一种简单的方法来丰富现有数据集，并创建了一个多粒度版本的数据集。实验结果表明...",
    "en_tdlr": "This work proposes GRANOLA QA, an evaluation setting that uses multi-granularity answers to assess the accuracy and informativeness of predicted answers in open-domain question answering. The authors present a simple methodology for enriching existing datasets and create a multi-granularity version of a dataset. Experimental results show..."
}