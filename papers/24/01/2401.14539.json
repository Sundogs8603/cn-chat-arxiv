{
    "title": "Understanding Disparities in Post Hoc Machine Learning Explanation. (arXiv:2401.14539v1 [cs.LG])",
    "abstract": "Previous work has highlighted that existing post-hoc explanation methods exhibit disparities in explanation fidelity (across 'race' and 'gender' as sensitive attributes), and while a large body of work focuses on mitigating these issues at the explanation metric level, the role of the data generating process and black box model in relation to explanation disparities remains largely unexplored. Accordingly, through both simulations as well as experiments on a real-world dataset, we specifically assess challenges to explanation disparities that originate from properties of the data: limited sample size, covariate shift, concept shift, omitted variable bias, and challenges based on model properties: inclusion of the sensitive attribute and appropriate functional form. Through controlled simulation analyses, our study demonstrates that increased covariate shift, concept shift, and omission of covariates increase explanation disparities, with the effect pronounced higher for neural network ",
    "link": "http://arxiv.org/abs/2401.14539",
    "context": "Title: Understanding Disparities in Post Hoc Machine Learning Explanation. (arXiv:2401.14539v1 [cs.LG])\nAbstract: Previous work has highlighted that existing post-hoc explanation methods exhibit disparities in explanation fidelity (across 'race' and 'gender' as sensitive attributes), and while a large body of work focuses on mitigating these issues at the explanation metric level, the role of the data generating process and black box model in relation to explanation disparities remains largely unexplored. Accordingly, through both simulations as well as experiments on a real-world dataset, we specifically assess challenges to explanation disparities that originate from properties of the data: limited sample size, covariate shift, concept shift, omitted variable bias, and challenges based on model properties: inclusion of the sensitive attribute and appropriate functional form. Through controlled simulation analyses, our study demonstrates that increased covariate shift, concept shift, and omission of covariates increase explanation disparities, with the effect pronounced higher for neural network ",
    "path": "papers/24/01/2401.14539.json",
    "total_tokens": 865,
    "translated_title": "理解事后机器学习解释中的差异",
    "translated_abstract": "先前的研究已经指出，现有的事后解释方法在解释准确性上存在差异（涉及“种族”和“性别”等敏感属性），虽然已有大量研究致力于在解释度量水平上减少这些问题，但数据生成过程和黑盒模型与解释差异之间的关系仍然未被广泛探讨。因此，通过模拟和在真实数据集上的实验，我们特别评估了解释差异面临的挑战：数据性质引起的局限样本量、协变量偏移、概念转变、被省略的变量偏差，以及模型性质引起的挑战：敏感属性的包含和适当的函数形式。通过受控模拟分析，我们的研究证明增加协变量偏移、概念转变和省略协变量会增加解释差异，对于神经网络而言，这种效应更加显著。",
    "tldr": "该研究通过模拟和实验评估了事后机器学习解释中的差异，并发现协变量偏移、概念转变和省略协变量会增加解释差异，对神经网络影响更大。",
    "en_tdlr": "This study examines disparities in post hoc machine learning explanations and finds that covariate shift, concept shift, and omitted variables increase explanation disparities, with a greater impact on neural networks."
}