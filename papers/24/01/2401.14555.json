{
    "title": "Revisiting Active Learning in the Era of Vision Foundation Models. (arXiv:2401.14555v1 [cs.CV])",
    "abstract": "Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zeroor few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We exten",
    "link": "http://arxiv.org/abs/2401.14555",
    "context": "Title: Revisiting Active Learning in the Era of Vision Foundation Models. (arXiv:2401.14555v1 [cs.CV])\nAbstract: Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zeroor few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We exten",
    "path": "papers/24/01/2401.14555.json",
    "total_tokens": 919,
    "translated_title": "在视觉基础模型时代重新审视主动学习",
    "translated_abstract": "基础视觉或视觉-语言模型是在大规模无标签或噪声数据上训练的，并学习到可以在各种任务上实现令人印象深刻的零标注或少标注性能的鲁棒表示。鉴于这些特性，它们是主动学习（AL）的自然选择，旨在实现标记效率的最大化，但在低预算条件下，基础模型的全部潜力在AL环境中尚未得到探索。在这项工作中，我们评估了基础模型对有效AL的三个关键组成部分的影响，即1）初始标记样本池的选择，2）确保多样性抽样，以及3）代表性和不确定性抽样之间的权衡。我们系统地研究了基础模型（DINOv2、OpenCLIP）的鲁棒表示如何挑战已有的主动学习结果。我们的观察结果为一个新的简单优雅的AL策略的有原则构建提供了指导，该策略通过使用dropout估计不确定性和样本多样性之间的平衡。",
    "tldr": "本文评估了基础视觉模型对有效主动学习的三个关键组成部分的影响，并提出了一个新的简单优雅的主动学习策略，该策略通过平衡不确定性估计和样本多样性来实现。",
    "en_tdlr": "This paper evaluates the impact of foundation vision models on three critical components of effective active learning and proposes a new simple and elegant active learning strategy that balances uncertainty estimation and sample diversity."
}