{
    "title": "Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning",
    "abstract": "Contrastive representation learning is crucial in time series analysis as it alleviates the issue of data noise and incompleteness as well as sparsity of supervision signal. However, existing constrastive learning frameworks usually focus on intral-temporal features, which fails to fully exploit the intricate nature of time series data. To address this issue, we propose DE-TSMCL, an innovative distillation enhanced framework for long sequence time series forecasting. Specifically, we design a learnable data augmentation mechanism which adaptively learns whether to mask a timestamp to obtain optimized sub-sequences. Then, we propose a contrastive learning task with momentum update to explore inter-sample and intra-temporal correlations of time series to learn the underlying structure feature on the unlabeled time series. Meanwhile, we design a supervised task to learn more robust representations and facilitate the contrastive learning process. Finally, we jointly optimize the above two ",
    "link": "https://arxiv.org/abs/2401.17802",
    "context": "Title: Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning\nAbstract: Contrastive representation learning is crucial in time series analysis as it alleviates the issue of data noise and incompleteness as well as sparsity of supervision signal. However, existing constrastive learning frameworks usually focus on intral-temporal features, which fails to fully exploit the intricate nature of time series data. To address this issue, we propose DE-TSMCL, an innovative distillation enhanced framework for long sequence time series forecasting. Specifically, we design a learnable data augmentation mechanism which adaptively learns whether to mask a timestamp to obtain optimized sub-sequences. Then, we propose a contrastive learning task with momentum update to explore inter-sample and intra-temporal correlations of time series to learn the underlying structure feature on the unlabeled time series. Meanwhile, we design a supervised task to learn more robust representations and facilitate the contrastive learning process. Finally, we jointly optimize the above two ",
    "path": "papers/24/01/2401.17802.json",
    "total_tokens": 902,
    "translated_title": "带有动量对比学习的蒸馏增强时间序列预测网络",
    "translated_abstract": "对比表示学习在时间序列分析中非常重要，可以缓解数据噪声、不完整性以及监督信号稀疏性等问题。然而，现有的对比学习框架通常聚焦于时间内部特征，未能充分利用时间序列数据的复杂性。为解决这个问题，我们提出了DE-TSMCL，一种创新的用于长序列时间序列预测的蒸馏增强框架。具体来说，我们设计了一种可学习的数据增强机制，用于自适应地学习是否屏蔽时间戳以获得优化的子序列。然后，我们提出了一种带有动量更新的对比学习任务，探索时间序列的样本间和时间内部的相关性，从而学习未标记时间序列的潜在结构特征。同时，我们设计了一个监督任务，以学习更鲁棒的表示并促进对比学习过程。最后，我们联合优化上述两个任务。",
    "tldr": "本论文提出了一种创新的蒸馏增强框架，用于长序列时间序列预测。通过设计可学习的数据增强机制和带有动量更新的对比学习任务，能够充分利用时间序列数据的复杂性，并获得更鲁棒的表示。"
}