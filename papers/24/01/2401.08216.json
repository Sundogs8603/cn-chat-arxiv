{
    "title": "Towards Efficient and Certified Recovery from Poisoning Attacks in Federated Learning. (arXiv:2401.08216v2 [cs.CR] UPDATED)",
    "abstract": "Federated learning (FL) is vulnerable to poisoning attacks, where malicious clients manipulate their updates to affect the global model. Although various methods exist for detecting those clients in FL, identifying malicious clients requires sufficient model updates, and hence by the time malicious clients are detected, FL models have been already poisoned. Thus, a method is needed to recover an accurate global model after malicious clients are identified. Current recovery methods rely on (i) all historical information from participating FL clients and (ii) the initial model unaffected by the malicious clients, leading to a high demand for storage and computational resources. In this paper, we show that highly effective recovery can still be achieved based on (i) selective historical information rather than all historical information and (ii) a historical model that has not been significantly affected by malicious clients rather than the initial model. In this scenario, while maintaini",
    "link": "http://arxiv.org/abs/2401.08216",
    "context": "Title: Towards Efficient and Certified Recovery from Poisoning Attacks in Federated Learning. (arXiv:2401.08216v2 [cs.CR] UPDATED)\nAbstract: Federated learning (FL) is vulnerable to poisoning attacks, where malicious clients manipulate their updates to affect the global model. Although various methods exist for detecting those clients in FL, identifying malicious clients requires sufficient model updates, and hence by the time malicious clients are detected, FL models have been already poisoned. Thus, a method is needed to recover an accurate global model after malicious clients are identified. Current recovery methods rely on (i) all historical information from participating FL clients and (ii) the initial model unaffected by the malicious clients, leading to a high demand for storage and computational resources. In this paper, we show that highly effective recovery can still be achieved based on (i) selective historical information rather than all historical information and (ii) a historical model that has not been significantly affected by malicious clients rather than the initial model. In this scenario, while maintaini",
    "path": "papers/24/01/2401.08216.json",
    "total_tokens": 908,
    "translated_title": "高效且可验证地从毒化攻击中恢复联邦学习中的模型",
    "translated_abstract": "联邦学习面临着毒化攻击的威胁，恶意客户端通过操纵更新来影响全局模型。目前存在各种方法用于检测这些恶意客户端，但识别恶意客户端需要足够的模型更新，因此一旦检测到恶意客户端，联邦学习模型已经被污染。因此，需要一种在识别恶意客户端后恢复准确全局模型的方法。当前的恢复方法依赖于（i）参与联邦学习的所有历史信息和（ii）未受到恶意客户端影响的初始模型，导致对存储和计算资源的需求很高。本文中，我们展示了在选择性历史信息而不是所有历史信息以及基于未被恶意客户端显著影响的历史模型而不是初始模型的情况下，仍然可以实现高效的恢复。",
    "tldr": "本文提出了一种高效且可验证地从恶意客户端的毒化攻击中恢复联邦学习模型的方法，通过选择性利用历史信息以及未受恶意客户端影响的历史模型，能够在恶意客户端被识别后实现准确的全局模型恢复。",
    "en_tdlr": "This paper proposes an efficient and certified method to recover federated learning models from poisoning attacks by selectively utilizing historical information and using historical models unaffected by malicious clients, enabling accurate global model recovery after malicious clients are identified."
}