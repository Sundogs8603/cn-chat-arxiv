{
    "title": "Federated Learning for distribution skewed data using sample weights. (arXiv:2401.02586v1 [cs.LG])",
    "abstract": "One of the most challenging issues in federated learning is that the data is often not independent and identically distributed (nonIID). Clients are expected to contribute the same type of data and drawn from one global distribution. However, data are often collected in different ways from different resources. Thus, the data distributions among clients might be different from the underlying global distribution. This creates a weight divergence issue and reduces federated learning performance. This work focuses on improving federated learning performance for skewed data distribution across clients. The main idea is to adjust the client distribution closer to the global distribution using sample weights. Thus, the machine learning model converges faster with higher accuracy. We start from the fundamental concept of empirical risk minimization and theoretically derive a solution for adjusting the distribution skewness using sample weights. To determine sample weights, we implicitly exchan",
    "link": "http://arxiv.org/abs/2401.02586",
    "context": "Title: Federated Learning for distribution skewed data using sample weights. (arXiv:2401.02586v1 [cs.LG])\nAbstract: One of the most challenging issues in federated learning is that the data is often not independent and identically distributed (nonIID). Clients are expected to contribute the same type of data and drawn from one global distribution. However, data are often collected in different ways from different resources. Thus, the data distributions among clients might be different from the underlying global distribution. This creates a weight divergence issue and reduces federated learning performance. This work focuses on improving federated learning performance for skewed data distribution across clients. The main idea is to adjust the client distribution closer to the global distribution using sample weights. Thus, the machine learning model converges faster with higher accuracy. We start from the fundamental concept of empirical risk minimization and theoretically derive a solution for adjusting the distribution skewness using sample weights. To determine sample weights, we implicitly exchan",
    "path": "papers/24/01/2401.02586.json",
    "total_tokens": 877,
    "translated_title": "使用样本权重进行分布偏斜数据的联邦学习",
    "translated_abstract": "联邦学习中最具挑战性的问题之一是数据通常不是独立同分布的（nonIID）。客户端被期望贡献相同类型的数据并从一个全局分布中抽取数据。然而，数据往往以不同的方式从不同资源收集。因此，客户端之间的数据分布可能与底层全局分布不同。这就产生了权重发散问题，并降低了联邦学习的性能。该工作侧重于改善客户端之间分布偏斜的联邦学习性能。主要思想是使用样本权重将客户端分布调整到全局分布更接近，从而使机器学习模型收敛更快且精度更高。我们从经验风险最小化的基本概念开始，从理论上推导出使用样本权重调整分布偏斜的解决方案。为了确定样本权重，我们隐含地交换...",
    "tldr": "本论文研究了在分布偏斜数据情况下如何通过使用样本权重来改进联邦学习性能。主要思路是通过调整客户端分布使其更接近全局分布，从而实现机器学习模型更快地收敛和更高的准确性。"
}