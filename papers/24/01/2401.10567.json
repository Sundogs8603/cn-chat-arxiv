{
    "title": "Self-training from Self-memory in Data-to-text Generation. (arXiv:2401.10567v1 [cs.CL])",
    "abstract": "This paper introduces a novel training model, self-training from self-memory (STSM) in data-to-text generation (DTG), allowing the model to self-train on subsets, including self-memory as outputs inferred directly from the trained models and/or the new data. The quality of self-memory is validated by two models, data-to-text (D2T) and text-to-data (T2D), by two pre-defined conditions: (1) the appearance of all source values in the outputs of the D2T model and (2) the ability to convert back to source data in the outputs in the T2D model. We utilize a greedy algorithm to generate shorter D2T outputs if they contain all source values. Subsequently, we use the T2D model to confirm that these outputs can capture input relationships by demonstrating their capacity to convert text back into data. With 30% of the dataset, we can train the D2T model with a competitive performance compared to full training in the same setup. We experiment with our model on two datasets, E2E NLG and DART. STSM o",
    "link": "http://arxiv.org/abs/2401.10567",
    "context": "Title: Self-training from Self-memory in Data-to-text Generation. (arXiv:2401.10567v1 [cs.CL])\nAbstract: This paper introduces a novel training model, self-training from self-memory (STSM) in data-to-text generation (DTG), allowing the model to self-train on subsets, including self-memory as outputs inferred directly from the trained models and/or the new data. The quality of self-memory is validated by two models, data-to-text (D2T) and text-to-data (T2D), by two pre-defined conditions: (1) the appearance of all source values in the outputs of the D2T model and (2) the ability to convert back to source data in the outputs in the T2D model. We utilize a greedy algorithm to generate shorter D2T outputs if they contain all source values. Subsequently, we use the T2D model to confirm that these outputs can capture input relationships by demonstrating their capacity to convert text back into data. With 30% of the dataset, we can train the D2T model with a competitive performance compared to full training in the same setup. We experiment with our model on two datasets, E2E NLG and DART. STSM o",
    "path": "papers/24/01/2401.10567.json",
    "total_tokens": 939,
    "translated_title": "数据到文本生成中的自我记忆自我训练模型",
    "translated_abstract": "本文介绍了一种新颖的自我记忆自我训练模型（STSM）在数据到文本生成中的应用，使得模型可以在子集上进行自我训练，其中包括从训练模型和/或新数据中直接推断出的自我记忆。通过两个预定义条件来验证自我记忆的质量：（1）D2T模型的输出中包含所有源数值，（2）T2D模型的输出能够转换回源数据。如果D2T的输出包含所有源数值，我们使用贪婪算法生成较短的输出。随后，我们使用T2D模型来确认这些输出能够捕捉输入关系，通过演示它们将文本转换回数据的能力。在30%的数据集上，我们的D2T模型可以在相同的设置中以与完全训练相竞争的性能进行训练。我们在两个数据集（E2E NLG和DART）上对我们的模型进行了实验。",
    "tldr": "本文提出了一种在数据到文本生成中的自我记忆自我训练（STSM）模型，通过利用自我记忆作为训练子集，并使用预定义条件验证其质量。实验证明，这种方法可以以较高的性能进行训练，并在两个数据集上进行了实验。"
}