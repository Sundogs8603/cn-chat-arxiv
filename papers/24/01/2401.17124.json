{
    "title": "Spectral Co-Distillation for Personalized Federated Learning. (arXiv:2401.17124v1 [cs.LG])",
    "abstract": "Personalized federated learning (PFL) has been widely investigated to address the challenge of data heterogeneity, especially when a single generic model is inadequate in satisfying the diverse performance requirements of local clients simultaneously. Existing PFL methods are inherently based on the idea that the relations between the generic global and personalized local models are captured by the similarity of model weights. Such a similarity is primarily based on either partitioning the model architecture into generic versus personalized components, or modeling client relationships via model weights. To better capture similar (yet distinct) generic versus personalized model representations, we propose \\textit{spectral distillation}, a novel distillation method based on model spectrum information. Building upon spectral distillation, we also introduce a co-distillation framework that establishes a two-way bridge between generic and personalized model training. Moreover, to utilize th",
    "link": "http://arxiv.org/abs/2401.17124",
    "context": "Title: Spectral Co-Distillation for Personalized Federated Learning. (arXiv:2401.17124v1 [cs.LG])\nAbstract: Personalized federated learning (PFL) has been widely investigated to address the challenge of data heterogeneity, especially when a single generic model is inadequate in satisfying the diverse performance requirements of local clients simultaneously. Existing PFL methods are inherently based on the idea that the relations between the generic global and personalized local models are captured by the similarity of model weights. Such a similarity is primarily based on either partitioning the model architecture into generic versus personalized components, or modeling client relationships via model weights. To better capture similar (yet distinct) generic versus personalized model representations, we propose \\textit{spectral distillation}, a novel distillation method based on model spectrum information. Building upon spectral distillation, we also introduce a co-distillation framework that establishes a two-way bridge between generic and personalized model training. Moreover, to utilize th",
    "path": "papers/24/01/2401.17124.json",
    "total_tokens": 861,
    "translated_title": "个性化联邦学习中的光谱共蒸馏",
    "translated_abstract": "个性化联邦学习（PFL）被广泛研究以解决数据异构性的挑战，尤其是当单个通用模型无法满足本地客户端的不同性能要求时。现有的PFL方法本质上基于通用全局模型和个性化本地模型之间的关系由模型权重的相似性捕获的思想。这种相似性主要基于将模型架构划分为通用与个性化组件，或通过模型权重建模客户关系。为了更好地捕获相似（但不同的）通用与个性化模型表示，我们提出了一种基于模型谱信息的新型蒸馏方法：光谱共蒸馏。在光谱共蒸馏的基础上，我们还引入了一个共蒸馏框架，建立了通用和个性化模型训练之间的双向桥梁。",
    "tldr": "本论文提出了一种基于模型谱信息的光谱共蒸馏方法，用于个性化联邦学习。通过建立一个共蒸馏框架，建立了通用和个性化模型训练之间的双向桥梁。",
    "en_tdlr": "This paper proposes a novel spectral co-distillation method based on model spectrum information for personalized federated learning. By establishing a co-distillation framework, it bridges the training of generic and personalized models in a two-way manner."
}