{
    "title": "Object-oriented backdoor attack against image captioning. (arXiv:2401.02600v1 [cs.CV])",
    "abstract": "Backdoor attack against image classification task has been widely studied and proven to be successful, while there exist little research on the backdoor attack against vision-language models. In this paper, we explore backdoor attack towards image captioning models by poisoning training data. Assuming the attacker has total access to the training dataset, and cannot intervene in model construction or training process. Specifically, a portion of benign training samples is randomly selected to be poisoned. Afterwards, considering that the captions are usually unfolded around objects in an image, we design an object-oriented method to craft poisons, which aims to modify pixel values by a slight range with the modification number proportional to the scale of the current detected object region. After training with the poisoned data, the attacked model behaves normally on benign images, but for poisoned images, the model will generate some sentences irrelevant to the given image. The attack ",
    "link": "http://arxiv.org/abs/2401.02600",
    "context": "Title: Object-oriented backdoor attack against image captioning. (arXiv:2401.02600v1 [cs.CV])\nAbstract: Backdoor attack against image classification task has been widely studied and proven to be successful, while there exist little research on the backdoor attack against vision-language models. In this paper, we explore backdoor attack towards image captioning models by poisoning training data. Assuming the attacker has total access to the training dataset, and cannot intervene in model construction or training process. Specifically, a portion of benign training samples is randomly selected to be poisoned. Afterwards, considering that the captions are usually unfolded around objects in an image, we design an object-oriented method to craft poisons, which aims to modify pixel values by a slight range with the modification number proportional to the scale of the current detected object region. After training with the poisoned data, the attacked model behaves normally on benign images, but for poisoned images, the model will generate some sentences irrelevant to the given image. The attack ",
    "path": "papers/24/01/2401.02600.json",
    "total_tokens": 870,
    "translated_title": "面向图像描述的面向对象后门攻击",
    "translated_abstract": "在对图像分类任务进行后门攻击已经被广泛研究并被证明成功的同时，对视觉语言模型进行后门攻击的研究还很少。本文通过污染训练数据，探索了对图像描述模型的后门攻击。假设攻击者完全访问训练数据集，但不能干预模型的构建或训练过程。具体而言，我们随机选择部分良性训练样本进行污染。然后，考虑到描述通常围绕图像中的对象展开，我们设计了一种面向对象的方法来制作污染样本，该方法旨在通过微小的像素值修改来修改当前检测到的对象区域的规模相应的修改数量。在使用经过污染数据训练的模型上，对于良性图像，攻击模型的行为正常，但对于污染图像，模型将生成与给定图像无关的句子。",
    "tldr": "本文研究了图像描述模型的面向对象后门攻击，通过污染训练数据，并设计了一种面向对象的方法来制作污染样本。攻击后，模型在良性图像上表现正常，但对于污染图像，模型将生成与图像无关的句子。"
}