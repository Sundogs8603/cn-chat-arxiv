{
    "title": "RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems. (arXiv:2401.01369v1 [cs.IR])",
    "abstract": "Recommender systems aim to recommend the most suitable items to users from a large number of candidates. Their computation cost grows as the number of user requests and the complexity of services (or models) increases. Under the limitation of computation resources (CRs), how to make a trade-off between computation cost and business revenue becomes an essential question. The existing studies focus on dynamically allocating CRs in queue truncation scenarios (i.e., allocating the size of candidates), and formulate the CR allocation problem as an optimization problem with constraints. Some of them focus on single-phase CR allocation, and others focus on multi-phase CR allocation but introduce some assumptions about queue truncation scenarios. However, these assumptions do not hold in other scenarios, such as retrieval channel selection and prediction model selection. Moreover, existing studies ignore the state transition process of requests between different phases, limiting the effectiven",
    "link": "http://arxiv.org/abs/2401.01369",
    "context": "Title: RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems. (arXiv:2401.01369v1 [cs.IR])\nAbstract: Recommender systems aim to recommend the most suitable items to users from a large number of candidates. Their computation cost grows as the number of user requests and the complexity of services (or models) increases. Under the limitation of computation resources (CRs), how to make a trade-off between computation cost and business revenue becomes an essential question. The existing studies focus on dynamically allocating CRs in queue truncation scenarios (i.e., allocating the size of candidates), and formulate the CR allocation problem as an optimization problem with constraints. Some of them focus on single-phase CR allocation, and others focus on multi-phase CR allocation but introduce some assumptions about queue truncation scenarios. However, these assumptions do not hold in other scenarios, such as retrieval channel selection and prediction model selection. Moreover, existing studies ignore the state transition process of requests between different phases, limiting the effectiven",
    "path": "papers/24/01/2401.01369.json",
    "total_tokens": 867,
    "translated_title": "RL-MPCA: 基于强化学习的多阶段计算分配方法用于推荐系统",
    "translated_abstract": "推荐系统旨在从大量候选项中向用户推荐最合适的物品。随着用户请求的增加和服务（或模型）的复杂性增加，其计算成本也在增加。在计算资源有限的情况下，如何在计算成本和业务收益之间做出权衡成为一个重要问题。现有的研究集中于在队列截断场景中动态分配计算资源（即分配候选项的大小），并将计算资源分配问题建模为带约束条件的优化问题。其中一些研究集中于单阶段的计算资源分配，而其他研究则集中于多阶段的计算资源分配，但引入了一些关于队列截断场景的假设。然而，这些假设在其他情景下（如检索通道选择和预测模型选择）是不成立的。此外，现有的研究忽略了请求在不同阶段之间的状态转移过程，限制了其有效性。",
    "tldr": "RL-MPCA是一种基于强化学习的多阶段计算分配方法，用于解决推荐系统中在计算资源有限情况下的计算成本和业务收益之间的权衡问题。"
}