{
    "title": "A Topology-aware Graph Coarsening Framework for Continual Graph Learning. (arXiv:2401.03077v1 [cs.LG])",
    "abstract": "Continual learning on graphs tackles the problem of training a graph neural network (GNN) where graph data arrive in a streaming fashion and the model tends to forget knowledge from previous tasks when updating with new data. Traditional continual learning strategies such as Experience Replay can be adapted to streaming graphs, however, these methods often face challenges such as inefficiency in preserving graph topology and incapability of capturing the correlation between old and new tasks. To address these challenges, we propose TA$\\mathbb{CO}$, a (t)opology-(a)ware graph (co)arsening and (co)ntinual learning framework that stores information from previous tasks as a reduced graph. At each time period, this reduced graph expands by combining with a new graph and aligning shared nodes, and then it undergoes a \"zoom out\" process by reduction to maintain a stable size. We design a graph coarsening algorithm based on node representation proximities to efficiently reduce a graph and pres",
    "link": "http://arxiv.org/abs/2401.03077",
    "context": "Title: A Topology-aware Graph Coarsening Framework for Continual Graph Learning. (arXiv:2401.03077v1 [cs.LG])\nAbstract: Continual learning on graphs tackles the problem of training a graph neural network (GNN) where graph data arrive in a streaming fashion and the model tends to forget knowledge from previous tasks when updating with new data. Traditional continual learning strategies such as Experience Replay can be adapted to streaming graphs, however, these methods often face challenges such as inefficiency in preserving graph topology and incapability of capturing the correlation between old and new tasks. To address these challenges, we propose TA$\\mathbb{CO}$, a (t)opology-(a)ware graph (co)arsening and (co)ntinual learning framework that stores information from previous tasks as a reduced graph. At each time period, this reduced graph expands by combining with a new graph and aligning shared nodes, and then it undergoes a \"zoom out\" process by reduction to maintain a stable size. We design a graph coarsening algorithm based on node representation proximities to efficiently reduce a graph and pres",
    "path": "papers/24/01/2401.03077.json",
    "total_tokens": 916,
    "translated_title": "一种面向拓扑感知的连续图学习的图减粗框架",
    "translated_abstract": "图上的连续学习解决了在数据以流式方式到达且模型在更新时容易忘记以前任务的知识的图神经网络（GNN）的训练问题。传统的连续学习策略，如经验回放，可以适应流式图，但是，这些方法通常面临保留图拓扑的低效率和捕捉旧任务与新任务之间关联性的无能等挑战。为了解决这些挑战，我们提出了TA$\\mathbb{CO}$，一个面向拓扑感知的图减粗和连续学习框架，将以前任务的信息存储为一个减少的图。在每个时间段，这个减少的图通过与新图合并并对齐共享节点来扩展，然后进行“缩小”过程以保持稳定的大小。我们设计了一种基于节点表示接近度的图减粗算法来高效地减少图的大小。",
    "tldr": "提出了一种面向拓扑感知的连续图学习框架TA$\\mathbb{CO}$，通过图减粗和连续学习的方法，解决了传统连续学习策略在处理流式图时的低效率和无法捕捉任务关联性的问题。",
    "en_tdlr": "Introducing a topology-aware graph coarsening framework TA$\\mathbb{CO}$ for continual graph learning, which addresses the challenges faced by traditional strategies in handling streaming graphs, such as inefficiency in preserving graph topology and incapability of capturing task correlation."
}