{
    "title": "Reinforcement Learning for Graph Coloring: Understanding the Power and Limits of Non-Label Invariant Representations. (arXiv:2401.12470v1 [cs.LG])",
    "abstract": "Register allocation is one of the most important problems for modern compilers. With a practically unlimited number of user variables and a small number of CPU registers, assigning variables to registers without conflicts is a complex task. This work demonstrates the use of casting the register allocation problem as a graph coloring problem. Using technologies such as PyTorch and OpenAI Gymnasium Environments we will show that a Proximal Policy Optimization model can learn to solve the graph coloring problem. We will also show that the labeling of a graph is critical to the performance of the model by taking the matrix representation of a graph and permuting it. We then test the model's effectiveness on each of these permutations and show that it is not effective when given a relabeling of the same graph. Our main contribution lies in showing the need for label reordering invariant representations of graphs for machine learning models to achieve consistent performance.",
    "link": "http://arxiv.org/abs/2401.12470",
    "context": "Title: Reinforcement Learning for Graph Coloring: Understanding the Power and Limits of Non-Label Invariant Representations. (arXiv:2401.12470v1 [cs.LG])\nAbstract: Register allocation is one of the most important problems for modern compilers. With a practically unlimited number of user variables and a small number of CPU registers, assigning variables to registers without conflicts is a complex task. This work demonstrates the use of casting the register allocation problem as a graph coloring problem. Using technologies such as PyTorch and OpenAI Gymnasium Environments we will show that a Proximal Policy Optimization model can learn to solve the graph coloring problem. We will also show that the labeling of a graph is critical to the performance of the model by taking the matrix representation of a graph and permuting it. We then test the model's effectiveness on each of these permutations and show that it is not effective when given a relabeling of the same graph. Our main contribution lies in showing the need for label reordering invariant representations of graphs for machine learning models to achieve consistent performance.",
    "path": "papers/24/01/2401.12470.json",
    "total_tokens": 825,
    "translated_title": "基于图着色的强化学习：理解非标签不变表示的能力和局限性",
    "translated_abstract": "寄存器分配是现代编译器中最重要的问题之一。在拥有几乎无限数量的用户变量和少量CPU寄存器的情况下，将变量分配给寄存器以避免冲突是一个复杂的任务。本研究将寄存器分配问题转化为图着色问题，并使用PyTorch和OpenAI Gymnasium Environments等技术展示了Proximal Policy Optimization模型能够学习解决图着色问题。同时，我们还展示了图的标记对模型性能的重要性，通过获取图的矩阵表示并对其进行排列来进行测试。然后测试模型在每个排列上的效果，并展示当给出同一图的重新标记时其效果不佳。我们的主要贡献在于表明了机器学习模型需要具有标签重新排序不变性的图表示以实现一致的性能。",
    "tldr": "本论文将寄存器分配问题转化为图着色问题，并展示了Proximal Policy Optimization模型通过学习解决图着色问题。同时，我们还发现图的标记对模型性能至关重要，并提出了机器学习模型需要具有标签重新排序不变性的图表示。"
}