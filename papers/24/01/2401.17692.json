{
    "title": "Mitigating the Problem of Strong Priors in LMs with Context Extrapolation",
    "abstract": "Language models (LMs) have become important tools in a variety of applications, from data processing to the creation of instruction-following assistants. But despite their advantages, LMs have certain idiosyncratic limitations such as the problem of `strong priors', where a model learns to output typical continuations in response to certain, usually local, portions of the input regardless of any earlier instructions. For example, prompt injection attacks can induce models to ignore explicit directives. In some cases, larger models have been shown to be more susceptible to these problems than similar smaller models, an example of the phenomenon of `inverse scaling'. We develop a new technique for mitigating the problem of strong priors: we take the original set of instructions, produce a weakened version of the original prompt that is even more susceptible to the strong priors problem, and then extrapolate the continuation away from the weakened prompt. This lets us infer how the model ",
    "link": "https://arxiv.org/abs/2401.17692",
    "context": "Title: Mitigating the Problem of Strong Priors in LMs with Context Extrapolation\nAbstract: Language models (LMs) have become important tools in a variety of applications, from data processing to the creation of instruction-following assistants. But despite their advantages, LMs have certain idiosyncratic limitations such as the problem of `strong priors', where a model learns to output typical continuations in response to certain, usually local, portions of the input regardless of any earlier instructions. For example, prompt injection attacks can induce models to ignore explicit directives. In some cases, larger models have been shown to be more susceptible to these problems than similar smaller models, an example of the phenomenon of `inverse scaling'. We develop a new technique for mitigating the problem of strong priors: we take the original set of instructions, produce a weakened version of the original prompt that is even more susceptible to the strong priors problem, and then extrapolate the continuation away from the weakened prompt. This lets us infer how the model ",
    "path": "papers/24/01/2401.17692.json",
    "total_tokens": 866,
    "translated_title": "用上下文外推缓解语言模型中强先验问题的方法",
    "translated_abstract": "语言模型（LMs）已成为各种应用程序中重要的工具，从数据处理到创建指令跟随助手。但是尽管它们有优势，LMs还有一些特殊的局限性，比如“强先验”问题，其中模型会在对某些局部输入的响应中学习输出典型的延续，而不考虑之前的指令。例如，prompt注入攻击可以诱使模型忽略显式指令。在某些情况下，大型模型被证明比类似的较小模型更容易受到这些问题的影响，这是“反向缩放”现象的一个例子。我们开发了一种缓解强先验问题的新技术：我们采用原始指令集，生成原始提示的削弱版本，使其更容易受到强先验问题的影响，然后将延续外推远离削弱的提示。这让我们可以推断模型如何对上下文进行理解并产生输出。",
    "tldr": "本论文提出了一种缓解语言模型中强先验问题的新技术，通过削弱原始提示并进行上下文外推，以减少模型受到强先验问题的影响。",
    "en_tdlr": "This paper presents a new technique to mitigate the problem of strong priors in language models by weakening the original prompt and extrapolating the context, reducing the model's susceptibility to strong prior biases."
}