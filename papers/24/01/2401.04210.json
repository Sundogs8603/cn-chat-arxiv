{
    "title": "FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild. (arXiv:2401.04210v1 [cs.CV])",
    "abstract": "Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as body language, dialogues and culture. In this paper, we propose FunnyNet-W, a model that relies on cross- and self-attention for visual, audio and text data to predict funny moments in videos. Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a Large Language Model. To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments. We provide experiments on five datasets: ",
    "link": "http://arxiv.org/abs/2401.04210",
    "context": "Title: FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild. (arXiv:2401.04210v1 [cs.CV])\nAbstract: Automatically understanding funny moments (i.e., the moments that make people laugh) when watching comedy is challenging, as they relate to various features, such as body language, dialogues and culture. In this paper, we propose FunnyNet-W, a model that relies on cross- and self-attention for visual, audio and text data to predict funny moments in videos. Unlike most methods that rely on ground truth data in the form of subtitles, in this work we exploit modalities that come naturally with videos: (a) video frames as they contain visual information indispensable for scene understanding, (b) audio as it contains higher-level cues associated with funny moments, such as intonation, pitch and pauses and (c) text automatically extracted with a speech-to-text model as it can provide rich information when processed by a Large Language Model. To acquire labels for training, we propose an unsupervised approach that spots and labels funny audio moments. We provide experiments on five datasets: ",
    "path": "papers/24/01/2401.04210.json",
    "total_tokens": 949,
    "translated_title": "FunnyNet-W:视频中野外有趣瞬间的多模态学习",
    "translated_abstract": "在观看喜剧时自动理解有趣的瞬间（即使让人发笑的瞬间）是具有挑战性的，因为它们涉及到各种特征，如肢体语言、对话和文化。在本文中，我们提出了FunnyNet-W，它是一个依靠视觉、音频和文本数据的交叉和自注意力模型，用于预测视频中的有趣瞬间。与大多数依赖于字幕形式的标注数据的方法不同，在这项工作中，我们利用自然与视频一起出现的多模态数据：（a）视频帧，因为它们包含了场景理解所必需的视觉信息，（b）音频，因为它包含与有趣瞬间相关的更高级别的线索，如语调、音高和停顿，以及（c）由语音转文本模型自动提取的文本，因为它可以在经过大型语言模型处理后提供丰富的信息。为了获得训练标签，我们提出了一种无监督的方法来发现和标记有趣的音频瞬间。我们在五个数据集上进行了实验。",
    "tldr": "提出了FunnyNet-W，一个多模态模型用于预测视频中的有趣瞬间。该模型利用视觉、音频和文本数据以及交叉和自注意力机制，并且采用无监督方法获得训练标签。",
    "en_tdlr": "FunnyNet-W is a multimodal model that predicts funny moments in videos by utilizing visual, audio, and text data with cross- and self-attention mechanisms, and employs an unsupervised approach for acquiring training labels."
}