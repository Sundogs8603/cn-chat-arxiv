{
    "title": "FedTGP: Trainable Global Prototypes with Adaptive-Margin-Enhanced Contrastive Learning for Data and Model Heterogeneity in Federated Learning. (arXiv:2401.03230v1 [cs.LG])",
    "abstract": "Recently, Heterogeneous Federated Learning (HtFL) has attracted attention due to its ability to support heterogeneous models and data. To reduce the high communication cost of transmitting model parameters, a major challenge in HtFL, prototype-based HtFL methods are proposed to solely share class representatives, a.k.a, prototypes, among heterogeneous clients while maintaining the privacy of clients' models. However, these prototypes are naively aggregated into global prototypes on the server using weighted averaging, resulting in suboptimal global knowledge which negatively impacts the performance of clients. To overcome this challenge, we introduce a novel HtFL approach called FedTGP, which leverages our Adaptive-margin-enhanced Contrastive Learning (ACL) to learn Trainable Global Prototypes (TGP) on the server. By incorporating ACL, our approach enhances prototype separability while preserving semantic meaning. Extensive experiments with twelve heterogeneous models demonstrate that ",
    "link": "http://arxiv.org/abs/2401.03230",
    "context": "Title: FedTGP: Trainable Global Prototypes with Adaptive-Margin-Enhanced Contrastive Learning for Data and Model Heterogeneity in Federated Learning. (arXiv:2401.03230v1 [cs.LG])\nAbstract: Recently, Heterogeneous Federated Learning (HtFL) has attracted attention due to its ability to support heterogeneous models and data. To reduce the high communication cost of transmitting model parameters, a major challenge in HtFL, prototype-based HtFL methods are proposed to solely share class representatives, a.k.a, prototypes, among heterogeneous clients while maintaining the privacy of clients' models. However, these prototypes are naively aggregated into global prototypes on the server using weighted averaging, resulting in suboptimal global knowledge which negatively impacts the performance of clients. To overcome this challenge, we introduce a novel HtFL approach called FedTGP, which leverages our Adaptive-margin-enhanced Contrastive Learning (ACL) to learn Trainable Global Prototypes (TGP) on the server. By incorporating ACL, our approach enhances prototype separability while preserving semantic meaning. Extensive experiments with twelve heterogeneous models demonstrate that ",
    "path": "papers/24/01/2401.03230.json",
    "total_tokens": 971,
    "translated_title": "FedTGP: 用自适应边缘增强对比学习训练可训练全局原型以解决联邦学习中的数据和模型异质性问题",
    "translated_abstract": "最近，由于其支持异构模型和数据的能力，异构联邦学习（HtFL）引起了人们的关注。为了减少在HtFL中传输模型参数的高通信成本，一个主要挑战是提出了基于原型的HtFL方法，将类别代表，即原型，仅在异构客户端之间共享，同时保护客户端模型的隐私。然而，这些原型在服务器上通过加权平均值简单地聚合成全局原型，从而导致子优化的全局知识，从而对客户端的性能产生负面影响。为了克服这个挑战，我们引入了一种新的HtFL方法，称为FedTGP，它利用了我们的自适应边缘增强对比学习（ACL）在服务器上学习可训练的全局原型（TGP）。通过结合ACL，我们的方法增强了原型的可分离性同时保留语义含义。对于十二个异构模型的大量实验证明",
    "tldr": "FedTGP 是一种用于解决联邦学习中数据和模型异质性问题的方法，通过引入自适应边缘增强对比学习 (ACL) 来学习可训练的全局原型 (TGP)，从而提高了原型的可分离性和语义含义。",
    "en_tdlr": "FedTGP is a novel approach for addressing data and model heterogeneity in Federated Learning. It leverages Adaptive-margin-enhanced Contrastive Learning (ACL) to learn Trainable Global Prototypes (TGP) on the server, resulting in improved prototype separability and preserved semantic meaning."
}