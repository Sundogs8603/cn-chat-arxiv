{
    "title": "APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning. (arXiv:2401.06827v2 [cs.CV] UPDATED)",
    "abstract": "Pre-trained Vision-Language (V-L) models set the benchmark for generalization to downstream tasks among the noteworthy contenders. Many characteristics of the V-L model have been explored in existing research including the challenge of the sensitivity to text input and the tuning process across multi-modal prompts. With the advanced utilization of the V-L model like CLIP, recent approaches deploy learnable prompts instead of hand-craft prompts to boost the generalization performance and address the aforementioned challenges. Inspired by layer-wise training, which is wildly used in image fusion, we note that using a sequential training process to adapt different modalities branches of CLIP efficiently facilitates the improvement of generalization. In the context of addressing the multi-modal prompting challenge, we propose Token-wise Adaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities prompts, vision and language, as tokens in a sequential manner. APLe addresses t",
    "link": "http://arxiv.org/abs/2401.06827",
    "context": "Title: APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning. (arXiv:2401.06827v2 [cs.CV] UPDATED)\nAbstract: Pre-trained Vision-Language (V-L) models set the benchmark for generalization to downstream tasks among the noteworthy contenders. Many characteristics of the V-L model have been explored in existing research including the challenge of the sensitivity to text input and the tuning process across multi-modal prompts. With the advanced utilization of the V-L model like CLIP, recent approaches deploy learnable prompts instead of hand-craft prompts to boost the generalization performance and address the aforementioned challenges. Inspired by layer-wise training, which is wildly used in image fusion, we note that using a sequential training process to adapt different modalities branches of CLIP efficiently facilitates the improvement of generalization. In the context of addressing the multi-modal prompting challenge, we propose Token-wise Adaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities prompts, vision and language, as tokens in a sequential manner. APLe addresses t",
    "path": "papers/24/01/2401.06827.json",
    "total_tokens": 811,
    "translated_title": "APLe: 多模式提示学习的逐令牌自适应方法",
    "translated_abstract": "预训练的视觉语言模型在下游任务的泛化能力方面为同类模型树立了一个基准。现有研究中已经探索了视觉语言模型的许多特征，包括对文本输入的敏感性和跨多模式提示的调节过程所面临的挑战。最近的方法中，借鉴了图像融合中广泛使用的逐层训练思想，以提高泛化性能并解决上述挑战，取代了手工设计的提示，使用可学习的提示来提升性能。在解决多模式提示挑战的背景下，我们提出了逐令牌自适应方法(APLe)，以顺序方式调节CLIP中的视觉和语言两种模式提示。APLe能够高效地改进泛化能力。",
    "tldr": "本研究提出了APLe方法，通过逐令牌自适应的方式调节CLIP模型中的视觉和语言模式提示，提高了模型的泛化性能。",
    "en_tdlr": "This paper proposes APLe, a method that adapts visual and language prompts in the CLIP model in a token-wise manner, leading to improved generalization performance."
}