{
    "title": "On Principled Local Optimization Methods for Federated Learning. (arXiv:2401.13216v1 [cs.LG])",
    "abstract": "Federated Learning (FL), a distributed learning paradigm that scales on-device learning collaboratively, has emerged as a promising approach for decentralized AI applications. Local optimization methods such as Federated Averaging (FedAvg) are the most prominent methods for FL applications. Despite their simplicity and popularity, the theoretical understanding of local optimization methods is far from clear. This dissertation aims to advance the theoretical foundation of local methods in the following three directions.  First, we establish sharp bounds for FedAvg, the most popular algorithm in Federated Learning. We demonstrate how FedAvg may suffer from a notion we call iterate bias, and how an additional third-order smoothness assumption may mitigate this effect and lead to better convergence rates. We explain this phenomenon from a Stochastic Differential Equation (SDE) perspective.  Second, we propose Federated Accelerated Stochastic Gradient Descent (FedAc), the first principled a",
    "link": "http://arxiv.org/abs/2401.13216",
    "context": "Title: On Principled Local Optimization Methods for Federated Learning. (arXiv:2401.13216v1 [cs.LG])\nAbstract: Federated Learning (FL), a distributed learning paradigm that scales on-device learning collaboratively, has emerged as a promising approach for decentralized AI applications. Local optimization methods such as Federated Averaging (FedAvg) are the most prominent methods for FL applications. Despite their simplicity and popularity, the theoretical understanding of local optimization methods is far from clear. This dissertation aims to advance the theoretical foundation of local methods in the following three directions.  First, we establish sharp bounds for FedAvg, the most popular algorithm in Federated Learning. We demonstrate how FedAvg may suffer from a notion we call iterate bias, and how an additional third-order smoothness assumption may mitigate this effect and lead to better convergence rates. We explain this phenomenon from a Stochastic Differential Equation (SDE) perspective.  Second, we propose Federated Accelerated Stochastic Gradient Descent (FedAc), the first principled a",
    "path": "papers/24/01/2401.13216.json",
    "total_tokens": 916,
    "translated_title": "关于联邦学习的原则性局部优化方法的研究",
    "translated_abstract": "联邦学习是一种分布式学习范式，通过在设备上协同进行学习，已经成为去中心化人工智能应用的一种有前景的方法。像联邦平均（FedAvg）这样的局部优化方法是联邦学习应用中最突出的方法。尽管这些方法简单且受欢迎，但对局部优化方法的理论理解仍然不够清晰。本论文旨在推进局部方法的理论基础，主要包括以下三个方面。首先，我们为FedAvg建立了严格的界限，这是联邦学习中最流行的算法。我们展示了FedAvg可能受到的一个我们称之为迭代偏见的概念，并且说明了额外的三阶平滑性假设如何减轻这种影响并导致更好的收敛速度。我们从随机微分方程的角度解释了这一现象。其次，我们提出了联邦加速随机梯度下降（FedAc），这是第一个有原则性且速度更快的联邦学习优化方法。",
    "tldr": "本论文提出了关于联邦学习中局部优化方法的研究，主要包括对FedAvg算法的界限探索以及提出了联邦加速随机梯度下降（FedAc）方法。"
}