{
    "title": "Transformers are Multi-State RNNs. (arXiv:2401.06104v1 [cs.CL])",
    "abstract": "Transformers are considered conceptually different compared to the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as infinite multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that pretrained transformers can be converted into $\\textit{finite}$ multi-state RNNs by fixing the size of their hidden state. We observe that several existing transformers cache compression techniques can be framed as such conversion policies, and introduce a novel policy, TOVA, which is simpler compared to these policies. Our experiments with several long range tasks indicate that TOVA outperforms all other baseline policies, while being nearly on par with the full (infinite) model, and using in some cases only $\\frac{1}{8}$ of the original cache size. Our results indicate that transformer decoder LLMs often behave in practice as RNNs. They also lay",
    "link": "http://arxiv.org/abs/2401.06104",
    "context": "Title: Transformers are Multi-State RNNs. (arXiv:2401.06104v1 [cs.CL])\nAbstract: Transformers are considered conceptually different compared to the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as infinite multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that pretrained transformers can be converted into $\\textit{finite}$ multi-state RNNs by fixing the size of their hidden state. We observe that several existing transformers cache compression techniques can be framed as such conversion policies, and introduce a novel policy, TOVA, which is simpler compared to these policies. Our experiments with several long range tasks indicate that TOVA outperforms all other baseline policies, while being nearly on par with the full (infinite) model, and using in some cases only $\\frac{1}{8}$ of the original cache size. Our results indicate that transformer decoder LLMs often behave in practice as RNNs. They also lay",
    "path": "papers/24/01/2401.06104.json",
    "total_tokens": 1000,
    "translated_title": "Transformers 是多状态的 RNNs",
    "translated_abstract": "在这项工作中，我们展示了只使用解码器的 Transformers 实际上可以被概念化为无限多状态的 RNNs，即具有无限隐藏状态尺寸的 RNNs 变种。我们进一步展示了预训练的 Transformers 可以通过固定其隐藏状态大小来转换为有限多状态的 RNNs。我们观察到几种现有的 Transformers 缓存压缩技术可以被看作是这种转换策略，并引入了一种新的策略 TOVA，相比于这些策略更简单。我们在几种长距离任务上的实验证明了 TOVA 优于所有其他基准策略，同时与完整（无限）模型几乎不相上下，并且在某些情况下仅使用原始缓存大小的 $\\frac{1}{8}$。我们的结果表明，Transformer 解码器 LLMs 在实践中通常表现为 RNNs。",
    "tldr": "本文研究发现，只使用解码器的 Transformers 可以被视为无限多状态的 RNNs，并且可以通过固定隐藏状态的大小来转换为有限多状态的 RNNs。我们提出了一种新的转换策略 TOVA，在多个长距离任务中表现优于其他基准策略，并且与完整模型几乎持平，在某些情况下仅使用原始缓存大小的 $\\frac{1}{8}$。这些结果表明，Transformer 解码器 LLMs 在实践中通常表现为 RNNs。"
}