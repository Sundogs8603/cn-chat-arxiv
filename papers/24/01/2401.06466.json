{
    "title": "PersianMind: A Cross-Lingual Persian-English Large Language Model. (arXiv:2401.06466v1 [cs.CL])",
    "abstract": "Large language models demonstrate remarkable proficiency in various linguistic tasks and have extensive knowledge across various domains. Although they perform best in English, their ability in other languages is notable too. In contrast, open-source models, such as LLaMa, are primarily trained on English datasets, resulting in poor performance in non-English languages. In this paper, we introduce PersianMind, an open-source bilingual large language model which demonstrates comparable performance to closed-source GPT-3.5-turbo in the Persian language. By expanding LLaMa2's vocabulary with 10,000 Persian tokens and training it on a dataset comprising nearly 2 billion Persian tokens, we show that our approach preserves the model's English knowledge and employs transfer learning to excel at transferring task knowledge from one language to another.",
    "link": "http://arxiv.org/abs/2401.06466",
    "context": "Title: PersianMind: A Cross-Lingual Persian-English Large Language Model. (arXiv:2401.06466v1 [cs.CL])\nAbstract: Large language models demonstrate remarkable proficiency in various linguistic tasks and have extensive knowledge across various domains. Although they perform best in English, their ability in other languages is notable too. In contrast, open-source models, such as LLaMa, are primarily trained on English datasets, resulting in poor performance in non-English languages. In this paper, we introduce PersianMind, an open-source bilingual large language model which demonstrates comparable performance to closed-source GPT-3.5-turbo in the Persian language. By expanding LLaMa2's vocabulary with 10,000 Persian tokens and training it on a dataset comprising nearly 2 billion Persian tokens, we show that our approach preserves the model's English knowledge and employs transfer learning to excel at transferring task knowledge from one language to another.",
    "path": "papers/24/01/2401.06466.json",
    "total_tokens": 869,
    "translated_title": "PersianMind: 一个跨语言的波斯语-英语大型语言模型",
    "translated_abstract": "大型语言模型在各种语言任务中展现出了卓越的能力，并具备广泛的多领域知识。虽然它们在英语方面表现最优，但它们在其他语言方面的能力也很显著。与此相反，如LLaMa这样的开源模型主要是在英文数据集上训练的，导致在非英文语言中表现不佳。在本文中，我们介绍了PersianMind，一个开源的双语大型语言模型，在波斯语中展示了与闭源的GPT-3.5-turbo相当的性能。通过将LLaMa2的词汇表扩展10,000个波斯语标记，并训练约20亿个波斯语标记的数据集，我们展示了我们的方法保留了模型的英语知识并利用迁移学习在不同语言间传递任务知识的优势。",
    "tldr": "PersianMind是一个开源的双语大型语言模型，通过在波斯语中展现与闭源的GPT-3.5-turbo相当的性能，并利用迁移学习在不同语言间传递任务知识的优势，解决了开源模型在非英文语言上性能不佳的问题。",
    "en_tdlr": "PersianMind is an open-source bilingual large language model that addresses the poor performance of open-source models in non-English languages by achieving comparable performance to closed-source GPT-3.5-turbo in Persian language and utilizing transfer learning to transfer task knowledge across different languages."
}