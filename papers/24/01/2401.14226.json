{
    "title": "Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks. (arXiv:2401.14226v1 [cs.LG])",
    "abstract": "Improving sample efficiency is central to Reinforcement Learning (RL), especially in environments where the rewards are sparse. Some recent approaches have proposed to specify reward functions as manually designed or learned reward structures whose integrations in the RL algorithms are claimed to significantly improve the learning efficiency. Manually designed reward structures can suffer from inaccuracy and existing automatically learning methods are often computationally intractable for complex tasks. The integration of inaccurate or partial reward structures in RL algorithms fail to learn optimal policies. In this work, we propose an RL algorithm that can automatically structure the reward function for sample efficiency, given a set of labels that signify subtasks. Given such minimal knowledge about the task, we train a high-level policy that selects optimal sub-tasks in each state together with a low-level policy that efficiently learns to complete each sub-task. We evaluate our al",
    "link": "http://arxiv.org/abs/2401.14226",
    "context": "Title: Sample Efficient Reinforcement Learning by Automatically Learning to Compose Subtasks. (arXiv:2401.14226v1 [cs.LG])\nAbstract: Improving sample efficiency is central to Reinforcement Learning (RL), especially in environments where the rewards are sparse. Some recent approaches have proposed to specify reward functions as manually designed or learned reward structures whose integrations in the RL algorithms are claimed to significantly improve the learning efficiency. Manually designed reward structures can suffer from inaccuracy and existing automatically learning methods are often computationally intractable for complex tasks. The integration of inaccurate or partial reward structures in RL algorithms fail to learn optimal policies. In this work, we propose an RL algorithm that can automatically structure the reward function for sample efficiency, given a set of labels that signify subtasks. Given such minimal knowledge about the task, we train a high-level policy that selects optimal sub-tasks in each state together with a low-level policy that efficiently learns to complete each sub-task. We evaluate our al",
    "path": "papers/24/01/2401.14226.json",
    "total_tokens": 842,
    "translated_title": "自动学习组合子任务的高样本效率强化学习",
    "translated_abstract": "提高样本效率在强化学习中是核心问题，尤其是在奖励稀疏的环境中。最近的一些方法提出将奖励函数作为手动设计或学习得到的奖励结构来改进学习效率。手动设计的奖励结构可能存在不准确性，而现有的自动学习方法对于复杂任务而言常常计算难以处理。不准确或局部的奖励结构的整合在强化学习算法中无法学到最优策略。在本文中，我们提出了一种能够自动构建奖励函数提高样本效率的强化学习算法，给定一组表示子任务的标签。在对任务的最小了解下，我们训练了一个高层策略，在每个状态下选择最优的子任务，同时还有一个低层策略高效学习完成每个子任务。我们评估了我们的算法在不同环境下的表现，并与其他方法进行了对比。",
    "tldr": "本工作提出了一种用于提高样本效率的强化学习算法，通过自动构建奖励函数来选择最优子任务，在复杂任务中取得了较好的性能。"
}