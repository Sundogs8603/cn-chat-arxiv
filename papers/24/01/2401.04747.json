{
    "title": "DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation. (arXiv:2401.04747v1 [cs.SD])",
    "abstract": "We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpainting-based sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms ",
    "link": "http://arxiv.org/abs/2401.04747",
    "context": "Title: DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation. (arXiv:2401.04747v1 [cs.SD])\nAbstract: We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D Expression and Gesture generation with arbitrary length. While previous works focused on co-speech gesture or expression generation individually, the joint generation of synchronized expressions and gestures remains barely explored. To address this, our diffusion-based co-speech motion generation transformer enables uni-directional information flow from expression to gesture, facilitating improved matching of joint expression-gesture distributions. Furthermore, we introduce an outpainting-based sampling strategy for arbitrary long sequence generation in diffusion models, offering flexibility and computational efficiency. Our method provides a practical solution that produces high-quality synchronized expression and gesture generation driven by speech. Evaluated on two public datasets, our approach achieves state-of-the-art performance both quantitatively and qualitatively. Additionally, a user study confirms ",
    "path": "papers/24/01/2401.04747.json",
    "total_tokens": 911,
    "translated_title": "DiffSHEG:一种基于扩散的实时语音驱动的整体三维表情和手势生成方法",
    "translated_abstract": "我们提出了DiffSHEG，一种基于扩散的方法，用于实时语音驱动的整体三维表情和手势生成，适用于任意长度的语音输入。与以往的研究不同，我们专注于联合生成同步表情和手势，而不是分别生成。为了解决这个问题，我们提出了基于扩散的联动生成变换器，使得从表情到手势的单向信息流更加顺畅，有利于匹配联合的表情和手势分布。此外，我们引入了一种基于扩散模型的任意长序列生成策略，提供了灵活性和计算效率。我们的方法能够实现由语音驱动的高质量同步表情和手势生成的实际解决方案。通过在两个公共数据集上进行评估，我们的方法在定量和定性方面均取得了最先进的性能。此外，用户研究证实了我们方法的有效性。",
    "tldr": "DiffSHEG是一种基于扩散的实时语音驱动的整体三维表情和手势生成方法，通过联合生成同步表情和手势，并引入基于扩散模型的任意长序列生成策略，实现了高质量的同步表情和手势生成。",
    "en_tdlr": "DiffSHEG is a diffusion-based approach for real-time speech-driven holistic 3D expression and gesture generation. It jointly generates synchronized expressions and gestures, introduces an outpainting-based sampling strategy for arbitrary long sequence generation, and achieves high-quality synchronized expression and gesture generation."
}