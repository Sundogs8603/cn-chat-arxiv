{
    "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models. (arXiv:2401.03205v1 [cs.CL])",
    "abstract": "In the era of large language models (LLMs), hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the LLM hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucination, focused on the the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet effective detection method for LLM hallucination. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucination. Finally, we implement and examine a series of widely used techniques to mitigate the halluci",
    "link": "http://arxiv.org/abs/2401.03205",
    "context": "Title: The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models. (arXiv:2401.03205v1 [cs.CL])\nAbstract: In the era of large language models (LLMs), hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the LLM hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucination, focused on the the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet effective detection method for LLM hallucination. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucination. Finally, we implement and examine a series of widely used techniques to mitigate the halluci",
    "path": "papers/24/01/2401.03205.json",
    "total_tokens": 1075,
    "translated_title": "黑暗之后的黎明：大型语言模型中关于事实性幻觉的实证研究",
    "translated_abstract": "在大型语言模型（LLM）的时代，幻觉（即生成错误事实内容的倾向）给在现实世界中信任和可靠地部署LLMs带来了巨大的挑战。为了解决LLM幻觉问题，需要深入研究三个关键问题：如何检测幻觉（检测），LLMs为什么会产生幻觉（来源），以及如何减轻LLM幻觉（缓解）。为了解决这些挑战，本研究提出了一项系统的LLM幻觉实证研究，专注于幻觉的检测、来源和缓解三个方面。具体地，我们构建了一个新的幻觉基准HaluluEval 2.0，并为LLM幻觉设计了一个简单而有效的检测方法。此外，我们对LLMs的不同训练或利用阶段进行了深入分析，广泛研究了导致LLM幻觉的潜在因素。最后，我们实施并检验了一系列广泛使用的技术来减轻幻觉。",
    "tldr": "本研究通过实证研究探讨了大型语言模型中事实性幻觉的检测、来源和缓解方法。研究构建了新的幻觉基准HaluluEval 2.0，并设计了简单有效的LLM幻觉检测方法。通过深入分析LLMs的训练和利用阶段，并研究导致幻觉的潜在因素，提出了一系列可行的减轻幻觉的技术。"
}