{
    "title": "Efficient Online Crowdsourcing with Complex Annotations. (arXiv:2401.15116v1 [cs.HC])",
    "abstract": "Crowdsourcing platforms use various truth discovery algorithms to aggregate annotations from multiple labelers. In an online setting, however, the main challenge is to decide whether to ask for more annotations for each item to efficiently trade off cost (i.e., the number of annotations) for quality of the aggregated annotations. In this paper, we propose a novel approach for general complex annotation (such as bounding boxes and taxonomy paths), that works in an online crowdsourcing setting. We prove that the expected average similarity of a labeler is linear in their accuracy \\emph{conditional on the reported label}. This enables us to infer reported label accuracy in a broad range of scenarios. We conduct extensive evaluations on real-world crowdsourcing data from Meta and show the effectiveness of our proposed online algorithms in improving the cost-quality trade-off.",
    "link": "http://arxiv.org/abs/2401.15116",
    "context": "Title: Efficient Online Crowdsourcing with Complex Annotations. (arXiv:2401.15116v1 [cs.HC])\nAbstract: Crowdsourcing platforms use various truth discovery algorithms to aggregate annotations from multiple labelers. In an online setting, however, the main challenge is to decide whether to ask for more annotations for each item to efficiently trade off cost (i.e., the number of annotations) for quality of the aggregated annotations. In this paper, we propose a novel approach for general complex annotation (such as bounding boxes and taxonomy paths), that works in an online crowdsourcing setting. We prove that the expected average similarity of a labeler is linear in their accuracy \\emph{conditional on the reported label}. This enables us to infer reported label accuracy in a broad range of scenarios. We conduct extensive evaluations on real-world crowdsourcing data from Meta and show the effectiveness of our proposed online algorithms in improving the cost-quality trade-off.",
    "path": "papers/24/01/2401.15116.json",
    "total_tokens": 816,
    "translated_title": "有效的在线众包与复杂注释",
    "translated_abstract": "众包平台使用各种真实性发现算法来聚合来自多个标注者的注释。然而，在在线环境中，主要挑战是决定是否为每个项目请求更多的注释，以高效地权衡成本（即注释数量）和聚合注释的质量。本文提出了一种适用于在线众包环境的通用复杂注释（如边界框和分类路径）的新方法。我们证明了标注者的预期平均相似度与他们的准确性在\"给定报告的标签\"条件下是线性关系。这使得我们能够推断在各种场景下的报告标签准确度。我们在Meta的真实众包数据上进行了大量评估，并展示了我们提出的在线算法在提升成本-质量权衡方面的有效性。",
    "tldr": "本文提出了一种有效的在线众包算法，能够在多个标注者中聚合复杂注释，通过推断标注者的准确性来改善成本-质量权衡。基于实验结果表明该算法在真实众包数据上取得了很好的效果。",
    "en_tdlr": "This paper proposes an efficient online crowdsourcing algorithm that aggregates complex annotations from multiple labelers and improves the cost-quality trade-off by inferring labelers' accuracy. Experimental results on real-world crowdsourcing data demonstrate the effectiveness of the proposed algorithm."
}