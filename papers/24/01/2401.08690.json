{
    "title": "Contrastive Learning with Negative Sampling Correction. (arXiv:2401.08690v1 [cs.LG])",
    "abstract": "As one of the most effective self-supervised representation learning methods, contrastive learning (CL) relies on multiple negative pairs to contrast against each positive pair. In the standard practice of contrastive learning, data augmentation methods are utilized to generate both positive and negative pairs. While existing works have been focusing on improving the positive sampling, the negative sampling process is often overlooked. In fact, the generated negative samples are often polluted by positive samples, which leads to a biased loss and performance degradation. To correct the negative sampling bias, we propose a novel contrastive learning method named Positive-Unlabeled Contrastive Learning (PUCL). PUCL treats the generated negative samples as unlabeled samples and uses information from positive samples to correct bias in contrastive loss. We prove that the corrected loss used in PUCL only incurs a negligible bias compared to the unbiased contrastive loss. PUCL can be applied",
    "link": "http://arxiv.org/abs/2401.08690",
    "context": "Title: Contrastive Learning with Negative Sampling Correction. (arXiv:2401.08690v1 [cs.LG])\nAbstract: As one of the most effective self-supervised representation learning methods, contrastive learning (CL) relies on multiple negative pairs to contrast against each positive pair. In the standard practice of contrastive learning, data augmentation methods are utilized to generate both positive and negative pairs. While existing works have been focusing on improving the positive sampling, the negative sampling process is often overlooked. In fact, the generated negative samples are often polluted by positive samples, which leads to a biased loss and performance degradation. To correct the negative sampling bias, we propose a novel contrastive learning method named Positive-Unlabeled Contrastive Learning (PUCL). PUCL treats the generated negative samples as unlabeled samples and uses information from positive samples to correct bias in contrastive loss. We prove that the corrected loss used in PUCL only incurs a negligible bias compared to the unbiased contrastive loss. PUCL can be applied",
    "path": "papers/24/01/2401.08690.json",
    "total_tokens": 854,
    "translated_title": "负采样矫正的对比学习",
    "translated_abstract": "对比学习是一种有效的自监督表示学习方法，它依赖于多个负样本与正样本进行对比。在标准的对比学习中，使用数据增强方法生成正负样本。然而，现有的研究一直致力于改进正样本采样，而负样本采样则常常被忽视。事实上，生成的负样本通常会受到正样本的污染，这导致了偏置损失和性能下降。为了矫正负样本采样偏置，我们提出了一种名为正负样本对比学习（PUCL）的新型对比学习方法。PUCL将生成的负样本视为未标记样本，并使用正样本的信息来纠正对比损失中的偏差。我们证明了在PUCL中使用的校正损失与无偏差的对比损失相比，只引入了可忽略的偏差。PUCL可以应用于... （原文省略）",
    "tldr": "对比学习是一种自监督表示学习方法，通过矫正负样本采样偏置来提高性能，我们提出了一种名为PUCL的新型对比学习方法。",
    "en_tdlr": "Contrastive learning is a self-supervised representation learning method that relies on contrasting positive and negative pairs. We propose PUCL, a novel contrastive learning method, to correct the bias in negative sample generation and improve performance."
}