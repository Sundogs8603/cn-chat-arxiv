{
    "title": "Optimal Rates of Kernel Ridge Regression under Source Condition in Large Dimensions. (arXiv:2401.01270v1 [cs.LG])",
    "abstract": "Motivated by the studies of neural networks (e.g.,the neural tangent kernel theory), we perform a study on the large-dimensional behavior of kernel ridge regression (KRR) where the sample size $n \\asymp d^{\\gamma}$ for some $\\gamma > 0$. Given an RKHS $\\mathcal{H}$ associated with an inner product kernel defined on the sphere $\\mathbb{S}^{d}$, we suppose that the true function $f_{\\rho}^{*} \\in [\\mathcal{H}]^{s}$, the interpolation space of $\\mathcal{H}$ with source condition $s>0$. We first determined the exact order (both upper and lower bound) of the generalization error of kernel ridge regression for the optimally chosen regularization parameter $\\lambda$. We then further showed that when $0<s\\le1$, KRR is minimax optimal; and when $s>1$, KRR is not minimax optimal (a.k.a. he saturation effect). Our results illustrate that the curves of rate varying along $\\gamma$ exhibit the periodic plateau behavior and the multiple descent behavior and show how the curves evolve with $s>0$. Inte",
    "link": "http://arxiv.org/abs/2401.01270",
    "context": "Title: Optimal Rates of Kernel Ridge Regression under Source Condition in Large Dimensions. (arXiv:2401.01270v1 [cs.LG])\nAbstract: Motivated by the studies of neural networks (e.g.,the neural tangent kernel theory), we perform a study on the large-dimensional behavior of kernel ridge regression (KRR) where the sample size $n \\asymp d^{\\gamma}$ for some $\\gamma > 0$. Given an RKHS $\\mathcal{H}$ associated with an inner product kernel defined on the sphere $\\mathbb{S}^{d}$, we suppose that the true function $f_{\\rho}^{*} \\in [\\mathcal{H}]^{s}$, the interpolation space of $\\mathcal{H}$ with source condition $s>0$. We first determined the exact order (both upper and lower bound) of the generalization error of kernel ridge regression for the optimally chosen regularization parameter $\\lambda$. We then further showed that when $0<s\\le1$, KRR is minimax optimal; and when $s>1$, KRR is not minimax optimal (a.k.a. he saturation effect). Our results illustrate that the curves of rate varying along $\\gamma$ exhibit the periodic plateau behavior and the multiple descent behavior and show how the curves evolve with $s>0$. Inte",
    "path": "papers/24/01/2401.01270.json",
    "total_tokens": 1061,
    "translated_title": "在大维度下的源条件下的核脊回归的最优率",
    "translated_abstract": "在神经网络研究（如神经切向核理论）的启发下，我们对核脊回归（KRR）在大维度下的行为进行了研究，其中样本量$n \\asymp d^{\\gamma}$，其中$\\gamma > 0$。给定与球$\\mathbb{S}^{d}$上定义的内积核相关的RKHS $\\mathcal{H}$，我们假设真实函数$f_{\\rho}^{*} \\in [\\mathcal{H}]^{s}$，即$\\mathcal{H}$的插值空间，其源条件$s>0$。我们首先确定了核脊回归在最优选择的正则化参数$\\lambda$下的泛化误差的精确阶数（上界和下界）。然后我们进一步展示了当$0<s\\le1$时，KRR是最小化风险的最优选择；当$s>1$时，KRR不是最小化风险的最优选择（也称饱和效应）。我们的结果说明了在$\\gamma$的变化下，速率曲线呈现周期性的台阶行为和多次下降行为，并展示了这些曲线如何随$s>0$而变化。",
    "tldr": "本研究研究了在大维度下的核脊回归的行为以及其最优选择的正则化参数。结果发现在满足源条件$s>0$时，KRR是最优的选择；而当$s>1$时，KRR不是最优的选择。此外，研究还展示了速率曲线在不同$\\gamma$和$s$下的周期性台阶行为和多次下降行为。",
    "en_tdlr": "This study investigates the behavior of kernel ridge regression in large dimensions and its optimal choice of regularization parameter. The results show that KRR is the optimal choice when the source condition is satisfied, and it is not optimal when the source condition is not satisfied. Additionally, the study demonstrates the periodic plateau behavior and multiple descent behavior of the rate curves under different values of gamma and s."
}