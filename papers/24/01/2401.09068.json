{
    "title": "DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning. (arXiv:2401.09068v1 [cs.LG])",
    "abstract": "DTMM is a library designed for efficient deployment and execution of machine learning models on weak IoT devices such as microcontroller units (MCUs). The motivation for designing DTMM comes from the emerging field of tiny machine learning (TinyML), which explores extending the reach of machine learning to many low-end IoT devices to achieve ubiquitous intelligence. Due to the weak capability of embedded devices, it is necessary to compress models by pruning enough weights before deploying. Although pruning has been studied extensively on many computing platforms, two key issues with pruning methods are exacerbated on MCUs: models need to be deeply compressed without significantly compromising accuracy, and they should perform efficiently after pruning. Current solutions only achieve one of these objectives, but not both. In this paper, we find that pruned models have great potential for efficient deployment and execution on MCUs. Therefore, we propose DTMM with pruning unit selection,",
    "link": "http://arxiv.org/abs/2401.09068",
    "context": "Title: DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning. (arXiv:2401.09068v1 [cs.LG])\nAbstract: DTMM is a library designed for efficient deployment and execution of machine learning models on weak IoT devices such as microcontroller units (MCUs). The motivation for designing DTMM comes from the emerging field of tiny machine learning (TinyML), which explores extending the reach of machine learning to many low-end IoT devices to achieve ubiquitous intelligence. Due to the weak capability of embedded devices, it is necessary to compress models by pruning enough weights before deploying. Although pruning has been studied extensively on many computing platforms, two key issues with pruning methods are exacerbated on MCUs: models need to be deeply compressed without significantly compromising accuracy, and they should perform efficiently after pruning. Current solutions only achieve one of these objectives, but not both. In this paper, we find that pruned models have great potential for efficient deployment and execution on MCUs. Therefore, we propose DTMM with pruning unit selection,",
    "path": "papers/24/01/2401.09068.json",
    "total_tokens": 944,
    "translated_title": "DTMM：利用修剪在极弱的物联网设备上部署TinyML模型",
    "translated_abstract": "DTMM是一个为弱物联网设备（如微控制器单元）上的机器学习模型的高效部署和执行而设计的库。设计DTMM的动机来自于新兴领域的小型机器学习（TinyML），它探索将机器学习扩展到许多低端物联网设备以实现普遍智能。由于嵌入式设备的能力较弱，需要在部署之前通过修剪足够的权重来压缩模型。尽管修剪已在许多计算平台上进行了广泛研究，但修剪方法在MCUs上面临两个关键问题的加剧：需要在不显著损害准确性的情况下深度压缩模型，并且修剪后的模型在执行效率上应具备高效性。目前的解决方案只能实现其中一个目标，而不能同时实现两者。在本文中，我们发现修剪后的模型在MCUs上具有高效部署和执行的巨大潜力。因此，我们提出了具有修剪单元选择的DTMM。",
    "tldr": "DTMM是一个库，旨在在弱物联网设备上高效部署和执行机器学习模型。之前的解决方案无法同时实现在不损害准确性的情况下深度压缩模型和高效执行的目标，而DTMM通过修剪单元选择解决了这个问题。"
}