{
    "title": "Communication-Efficient Personalized Federated Learning for Speech-to-Text Tasks. (arXiv:2401.10070v1 [cs.CL])",
    "abstract": "To protect privacy and meet legal regulations, federated learning (FL) has gained significant attention for training speech-to-text (S2T) systems, including automatic speech recognition (ASR) and speech translation (ST). However, the commonly used FL approach (i.e., \\textsc{FedAvg}) in S2T tasks typically suffers from extensive communication overhead due to multi-round interactions based on the whole model and performance degradation caused by data heterogeneity among clients.To address these issues, we propose a personalized federated S2T framework that introduces \\textsc{FedLoRA}, a lightweight LoRA module for client-side tuning and interaction with the server to minimize communication overhead, and \\textsc{FedMem}, a global model equipped with a $k$-nearest-neighbor ($k$NN) classifier that captures client-specific distributional shifts to achieve personalization and overcome data heterogeneity. Extensive experiments based on Conformer and Whisper backbone models on CoVoST and GigaSp",
    "link": "http://arxiv.org/abs/2401.10070",
    "context": "Title: Communication-Efficient Personalized Federated Learning for Speech-to-Text Tasks. (arXiv:2401.10070v1 [cs.CL])\nAbstract: To protect privacy and meet legal regulations, federated learning (FL) has gained significant attention for training speech-to-text (S2T) systems, including automatic speech recognition (ASR) and speech translation (ST). However, the commonly used FL approach (i.e., \\textsc{FedAvg}) in S2T tasks typically suffers from extensive communication overhead due to multi-round interactions based on the whole model and performance degradation caused by data heterogeneity among clients.To address these issues, we propose a personalized federated S2T framework that introduces \\textsc{FedLoRA}, a lightweight LoRA module for client-side tuning and interaction with the server to minimize communication overhead, and \\textsc{FedMem}, a global model equipped with a $k$-nearest-neighbor ($k$NN) classifier that captures client-specific distributional shifts to achieve personalization and overcome data heterogeneity. Extensive experiments based on Conformer and Whisper backbone models on CoVoST and GigaSp",
    "path": "papers/24/01/2401.10070.json",
    "total_tokens": 919,
    "translated_title": "通信高效的个性化联邦学习在语音转文本任务中的应用",
    "translated_abstract": "为了保护隐私并满足法规要求，联邦学习在训练语音转文本系统（包括自动语音识别和语音翻译）方面引起了广泛关注。然而，在语音转文本任务中常用的联邦学习方法（即FedAvg）通常面临着大量的通信开销和数据异构导致的性能下降问题。为了解决这些问题，我们提出了一种个性化的联邦语音转文本框架，引入了轻量级的LoRA模块（FedLoRA）用于客户端调整和与服务器进行交互以最小化通信开销，以及全局模型（FedMem）配备了K最近邻分类器，以捕捉客户特定的分布变化以实现个性化并克服数据异构。在CoVoST和GigaSp数据集上基于Conformer和Whisper主干模型进行了大量实验。",
    "tldr": "该论文提出了一种通信高效的个性化联邦学习框架，通过引入轻量级的LoRA模块进行客户端调整和与服务器的交互，以最小化通信开销，以及使用K最近邻分类器的全局模型来实现个性化并克服数据异构问题。",
    "en_tdlr": "This paper proposes a communication-efficient personalized federated learning framework for speech-to-text tasks, which minimizes communication overhead by introducing a lightweight LoRA module for client-side tuning and interaction with the server, and overcomes data heterogeneity through a global model equipped with a K-nearest-neighbor classifier for personalization."
}