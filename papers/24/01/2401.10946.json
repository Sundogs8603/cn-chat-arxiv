{
    "title": "Self context-aware emotion perception on human-robot interaction. (arXiv:2401.10946v1 [cs.HC])",
    "abstract": "Emotion recognition plays a crucial role in various domains of human-robot interaction. In long-term interactions with humans, robots need to respond continuously and accurately, however, the mainstream emotion recognition methods mostly focus on short-term emotion recognition, disregarding the context in which emotions are perceived. Humans consider that contextual information and different contexts can lead to completely different emotional expressions. In this paper, we introduce self context-aware model (SCAM) that employs a two-dimensional emotion coordinate system for anchoring and re-labeling distinct emotions. Simultaneously, it incorporates its distinctive information retention structure and contextual loss. This approach has yielded significant improvements across audio, video, and multimodal. In the auditory modality, there has been a notable enhancement in accuracy, rising from 63.10% to 72.46%. Similarly, the visual modality has demonstrated improved accuracy, increasing f",
    "link": "http://arxiv.org/abs/2401.10946",
    "context": "Title: Self context-aware emotion perception on human-robot interaction. (arXiv:2401.10946v1 [cs.HC])\nAbstract: Emotion recognition plays a crucial role in various domains of human-robot interaction. In long-term interactions with humans, robots need to respond continuously and accurately, however, the mainstream emotion recognition methods mostly focus on short-term emotion recognition, disregarding the context in which emotions are perceived. Humans consider that contextual information and different contexts can lead to completely different emotional expressions. In this paper, we introduce self context-aware model (SCAM) that employs a two-dimensional emotion coordinate system for anchoring and re-labeling distinct emotions. Simultaneously, it incorporates its distinctive information retention structure and contextual loss. This approach has yielded significant improvements across audio, video, and multimodal. In the auditory modality, there has been a notable enhancement in accuracy, rising from 63.10% to 72.46%. Similarly, the visual modality has demonstrated improved accuracy, increasing f",
    "path": "papers/24/01/2401.10946.json",
    "total_tokens": 858,
    "translated_title": "自我上下文感知下的人机交互情绪识别",
    "translated_abstract": "情绪识别在人机交互的各个领域中起着至关重要的作用。在与人类的长期交互中，机器人需要持续准确地做出反应，然而，主流的情绪识别方法大多关注短期情绪识别，忽视了情绪感知的上下文。人们认为上下文信息和不同的背景可以导致完全不同的情绪表达。本文介绍了一种自我上下文感知模型（SCAM），它采用二维情绪坐标系统来锚定和重新标记不同的情绪。同时，它还融合了独特的信息保留结构和上下文损失。该方法在音频、视频和多模态方面都取得了显著的改进。在听觉模态下，准确率明显提高，从63.10%提升到72.46%。同样，视觉模态也表现出了提高的准确率，增加了...",
    "tldr": "本论文提出了一种自我上下文感知模型（SCAM）用于人机交互中的情绪识别，在音频、视频和多模态方面取得了显著的改进，提高了准确率。"
}