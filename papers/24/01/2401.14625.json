{
    "title": "Toward Practical Automatic Speech Recognition and Post-Processing: a Call for Explainable Error Benchmark Guideline. (arXiv:2401.14625v1 [cs.CL])",
    "abstract": "Automatic speech recognition (ASR) outcomes serve as input for downstream tasks, substantially impacting the satisfaction level of end-users. Hence, the diagnosis and enhancement of the vulnerabilities present in the ASR model bear significant importance. However, traditional evaluation methodologies of ASR systems generate a singular, composite quantitative metric, which fails to provide comprehensive insight into specific vulnerabilities. This lack of detail extends to the post-processing stage, resulting in further obfuscation of potential weaknesses. Despite an ASR model's ability to recognize utterances accurately, subpar readability can negatively affect user satisfaction, giving rise to a trade-off between recognition accuracy and user-friendliness. To effectively address this, it is imperative to consider both the speech-level, crucial for recognition accuracy, and the text-level, critical for user-friendliness. Consequently, we propose the development of an Error Explainable B",
    "link": "http://arxiv.org/abs/2401.14625",
    "context": "Title: Toward Practical Automatic Speech Recognition and Post-Processing: a Call for Explainable Error Benchmark Guideline. (arXiv:2401.14625v1 [cs.CL])\nAbstract: Automatic speech recognition (ASR) outcomes serve as input for downstream tasks, substantially impacting the satisfaction level of end-users. Hence, the diagnosis and enhancement of the vulnerabilities present in the ASR model bear significant importance. However, traditional evaluation methodologies of ASR systems generate a singular, composite quantitative metric, which fails to provide comprehensive insight into specific vulnerabilities. This lack of detail extends to the post-processing stage, resulting in further obfuscation of potential weaknesses. Despite an ASR model's ability to recognize utterances accurately, subpar readability can negatively affect user satisfaction, giving rise to a trade-off between recognition accuracy and user-friendliness. To effectively address this, it is imperative to consider both the speech-level, crucial for recognition accuracy, and the text-level, critical for user-friendliness. Consequently, we propose the development of an Error Explainable B",
    "path": "papers/24/01/2401.14625.json",
    "total_tokens": 928,
    "translated_title": "迈向实用的自动语音识别和后处理：关于可解释错误基准指南的呼吁",
    "translated_abstract": "自动语音识别（ASR）的结果作为下游任务的输入，极大地影响着最终用户的满意度。因此，诊断和增强ASR模型中存在的弱点具有重要意义。然而，传统的ASR系统评估方法生成一个单一的、综合的定量指标，无法提供对特定弱点的全面洞察。这种缺乏细节也延伸到后处理阶段，进一步混淆了潜在的弱点。尽管ASR模型有能力准确识别话语，但读者感知度较差可能对用户满意度产生负面影响，从而在识别准确度和用户友好性之间产生一种权衡。为了有效解决这个问题，必须同时考虑到语音层面（对于识别准确性至关重要）和文本层面（对于用户友好性至关重要）。因此，我们提议开发一个错误可解释性基准指南。",
    "tldr": "传统ASR评估方法无法全面了解特定脆弱性，同时后处理阶段也缺乏详细信息，这对用户友好性产生负面影响。为了解决这个问题，需要综合考虑语音层面和文本层面，并提出了错误可解释性基准指南。",
    "en_tdlr": "Traditional ASR evaluation methods fail to comprehensively understand specific vulnerabilities, while the lack of detail in the post-processing stage negatively impacts user-friendliness. To address this, it is necessary to consider both the speech-level and text-level, leading to the proposal of an Error Explainable Benchmark Guideline."
}