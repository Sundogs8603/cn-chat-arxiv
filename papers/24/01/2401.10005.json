{
    "title": "Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation. (arXiv:2401.10005v1 [cs.CV])",
    "abstract": "The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of Large Multi-Modal Models (LMMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. Our method comprises the development of a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. We designed an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment. The model undergoes a three-stage training phase, starting with large-scale image-text alignment using a large-scale datasets, followed ",
    "link": "http://arxiv.org/abs/2401.10005",
    "context": "Title: Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation. (arXiv:2401.10005v1 [cs.CV])\nAbstract: The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of Large Multi-Modal Models (LMMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. Our method comprises the development of a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. We designed an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment. The model undergoes a three-stage training phase, starting with large-scale image-text alignment using a large-scale datasets, followed ",
    "path": "papers/24/01/2401.10005.json",
    "total_tokens": 881,
    "translated_title": "以显性推理链和视觉问题生成推进大型多模态模型",
    "translated_abstract": "随着对能够解释和推理视觉内容的智能系统需求越来越高，需要开发不仅准确而且具有显性推理能力的大型多模态模型（LMMs）。本文提出了一种新颖的方法，将显性推理能力赋予LMMs，基于视觉内容和文本指导进行显性推理。我们引入了一个系统，可以提问以获取必要的知识，从而增强推理过程的鲁棒性和可解释性。我们的方法包括通过一个大型语言模型（LLM）生成的新颖数据集的开发，旨在促进思维链推理与提问机制的结合。我们设计了一个高度具有区域意识的LMM，以解决图像-文本对齐的复杂需求。该模型经历了三个阶段的训练，首先是使用大规模数据集进行大规模图像-文本对齐，接下来是通过显式推理的问题生成阶段。",
    "tldr": "本文提出了一种新的方法，通过显性推理和问题生成，将大型多模态模型(LMM)赋予了显性推理能力，从而提高了推理过程的鲁棒性和可解释性。",
    "en_tdlr": "This paper presents a novel approach that enhances the robustness and explicability of Large Multi-Modal Models (LMMs) by incorporating explicit reasoning and question generation, enabling the models to conduct explicit reasoning based on visual content and textual instructions."
}