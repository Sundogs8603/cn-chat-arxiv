{
    "title": "Fine-tuning Strategies for Domain Specific Question Answering under Low Annotation Budget Constraints. (arXiv:2401.09168v1 [cs.CL])",
    "abstract": "The progress introduced by pre-trained language models and their fine-tuning has resulted in significant improvements in most downstream NLP tasks. The unsupervised training of a language model combined with further target task fine-tuning has become the standard QA fine-tuning procedure. In this work, we demonstrate that this strategy is sub-optimal for fine-tuning QA models, especially under a low QA annotation budget, which is a usual setting in practice due to the extractive QA labeling cost. We draw our conclusions by conducting an exhaustive analysis of the performance of the alternatives of the sequential fine-tuning strategy on different QA datasets. Based on the experiments performed, we observed that the best strategy to fine-tune the QA model in low-budget settings is taking a pre-trained language model (PLM) and then fine-tuning PLM with a dataset composed of the target dataset and SQuAD dataset. With zero extra annotation effort, the best strategy outperforms the standard ",
    "link": "http://arxiv.org/abs/2401.09168",
    "context": "Title: Fine-tuning Strategies for Domain Specific Question Answering under Low Annotation Budget Constraints. (arXiv:2401.09168v1 [cs.CL])\nAbstract: The progress introduced by pre-trained language models and their fine-tuning has resulted in significant improvements in most downstream NLP tasks. The unsupervised training of a language model combined with further target task fine-tuning has become the standard QA fine-tuning procedure. In this work, we demonstrate that this strategy is sub-optimal for fine-tuning QA models, especially under a low QA annotation budget, which is a usual setting in practice due to the extractive QA labeling cost. We draw our conclusions by conducting an exhaustive analysis of the performance of the alternatives of the sequential fine-tuning strategy on different QA datasets. Based on the experiments performed, we observed that the best strategy to fine-tune the QA model in low-budget settings is taking a pre-trained language model (PLM) and then fine-tuning PLM with a dataset composed of the target dataset and SQuAD dataset. With zero extra annotation effort, the best strategy outperforms the standard ",
    "path": "papers/24/01/2401.09168.json",
    "total_tokens": 820,
    "translated_title": "低标注预算约束下域特定问答Fine-tuning策略",
    "translated_abstract": "预训练语言模型及其Fine-tuning的进展在大多数下游NLP任务中取得了显著改进。然而，我们的研究发现在低问答标注预算的情况下，这种策略对于Fine-tuning QA模型而言并不是最优的。通过对不同QA数据集上顺序Fine-tuning策略的性能进行详尽分析，我们得出结论，对于低预算设置下Fine-tuning QA模型的最佳策略是使用预训练语言模型（PLM），并将PLM与目标数据集和SQuAD数据集进行Fine-tuning。在没有额外标注的情况下，这种策略优于标准的baseline。",
    "tldr": "该研究对低预算下Fine-tuning QA模型的策略进行了研究分析，发现基于预训练语言模型进行Fine-tuning，并结合目标数据集和SQuAD数据集能够在不增加额外标注的情况下取得更好的性能。",
    "en_tdlr": "This study investigates strategies for fine-tuning QA models under low annotation budget and finds that fine-tuning a pre-trained language model with target and SQuAD datasets without additional annotation effort outperforms the standard baseline."
}