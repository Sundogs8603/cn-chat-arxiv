{
    "title": "Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency. (arXiv:2401.10843v1 [cs.NE])",
    "abstract": "Spiking Neural Networks (SNNs) that operate in an event-driven manner and employ binary spike representation have recently emerged as promising candidates for energy-efficient computing. However, a cost bottleneck arises in obtaining high-performance SNNs: training a SNN model requires a large number of time steps in addition to the usual learning iterations, hence this limits their energy efficiency. This paper proposes a general training framework that enhances feature learning and activation efficiency within a limited time step, providing a new solution for more energy-efficient SNNs. Our framework allows SNN neurons to learn robust spike feature from different receptive fields and update neuron states by utilizing both current stimuli and recurrence information transmitted from other neurons. This setting continuously complements information within a single time step. Additionally, we propose a projection function to merge these two stimuli to smoothly optimize neuron weights (spi",
    "link": "http://arxiv.org/abs/2401.10843",
    "context": "Title: Training a General Spiking Neural Network with Improved Efficiency and Minimum Latency. (arXiv:2401.10843v1 [cs.NE])\nAbstract: Spiking Neural Networks (SNNs) that operate in an event-driven manner and employ binary spike representation have recently emerged as promising candidates for energy-efficient computing. However, a cost bottleneck arises in obtaining high-performance SNNs: training a SNN model requires a large number of time steps in addition to the usual learning iterations, hence this limits their energy efficiency. This paper proposes a general training framework that enhances feature learning and activation efficiency within a limited time step, providing a new solution for more energy-efficient SNNs. Our framework allows SNN neurons to learn robust spike feature from different receptive fields and update neuron states by utilizing both current stimuli and recurrence information transmitted from other neurons. This setting continuously complements information within a single time step. Additionally, we propose a projection function to merge these two stimuli to smoothly optimize neuron weights (spi",
    "path": "papers/24/01/2401.10843.json",
    "total_tokens": 874,
    "translated_title": "以改进效率和最小延迟训练通用脉冲神经网络",
    "translated_abstract": "最近，以事件驱动方式运行并采用二进制脉冲表示的脉冲神经网络(SNNs)被认为是节能计算的有希望候选者。然而，高性能SNNs的训练存在成本瓶颈：训练SNN模型需要大量的时间步数以及通常的学习迭代，这限制了它们的能效。本文提出了一个通用训练框架，在有限的时间步内增强特征学习和激活效率，为更加节能的SNNs提供了新的解决方案。我们的框架允许SNN中的神经元从不同的感受野学习强大的脉冲特征，并通过利用当前刺激和来自其他神经元的循环信息来更新神经元状态。该设置在单个时间步内连续补充信息。此外，我们提出了一个投影函数来合并这两个刺激，以平滑优化神经元权重。",
    "tldr": "本论文提出了一个通用训练框架，通过在有限的时间步内增强特征学习和激活效率，为更加节能的脉冲神经网络（SNNs）提供了解决方案。"
}