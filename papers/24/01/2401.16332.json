{
    "title": "Tradeoffs Between Alignment and Helpfulness in Language Models",
    "abstract": "Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d",
    "link": "https://arxiv.org/abs/2401.16332",
    "context": "Title: Tradeoffs Between Alignment and Helpfulness in Language Models\nAbstract: Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally d",
    "path": "papers/24/01/2401.16332.json",
    "total_tokens": 838,
    "translated_title": "对齐和有用性之间的权衡：语言模型的研究",
    "translated_abstract": "语言模型对齐已成为人工智能安全的重要组成部分，通过增强期望行为和抑制非期望行为，实现人类与语言模型之间的安全交互。通常通过调整模型或插入预设的对齐提示来实现。最近，通过改变训练后的表示来改变模型行为的表示工程方法在对齐语言模型方面表现出了有效性。表示工程在面对对抗攻击和降低社会偏见等对齐导向任务方面取得了增益，但也导致了模型执行基本任务能力的降低。本文研究了增加对齐度和减少模型有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。有趣的是，我们发现，尽管模型的有用性通常会减少",
    "tldr": "本文研究了在语言模型中增加对齐度和减少有用性之间的权衡。我们提出了一个理论框架来提供这两个数量的边界，并通过实验证明了它们的相关性。",
    "en_tdlr": "This paper studies the tradeoff between increasing alignment and decreasing helpfulness in language models. We propose a theoretical framework that provides bounds for these quantities and demonstrate their relevance empirically."
}