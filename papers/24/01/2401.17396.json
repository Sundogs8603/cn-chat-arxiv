{
    "title": "Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks",
    "abstract": "Deep learning-based and lately Transformer-based language models have been dominating the studies of natural language processing in the last years. Thanks to their accurate and fast fine-tuning characteristics, they have outperformed traditional machine learning-based approaches and achieved state-of-the-art results for many challenging natural language understanding (NLU) problems. Recent studies showed that the Transformer-based models such as BERT, which is Bidirectional Encoder Representations from Transformers, have reached impressive achievements on many tasks. Moreover, thanks to their transfer learning capacity, these architectures allow us to transfer pre-built models and fine-tune them to specific NLU tasks such as question answering. In this study, we provide a Transformer-based model and a baseline benchmark for the Turkish Language. We successfully fine-tuned a Turkish BERT model, namely BERTurk that is trained with base settings, to many downstream tasks and evaluated wit",
    "link": "https://arxiv.org/abs/2401.17396",
    "context": "Title: Fine-tuning Transformer-based Encoder for Turkish Language Understanding Tasks\nAbstract: Deep learning-based and lately Transformer-based language models have been dominating the studies of natural language processing in the last years. Thanks to their accurate and fast fine-tuning characteristics, they have outperformed traditional machine learning-based approaches and achieved state-of-the-art results for many challenging natural language understanding (NLU) problems. Recent studies showed that the Transformer-based models such as BERT, which is Bidirectional Encoder Representations from Transformers, have reached impressive achievements on many tasks. Moreover, thanks to their transfer learning capacity, these architectures allow us to transfer pre-built models and fine-tune them to specific NLU tasks such as question answering. In this study, we provide a Transformer-based model and a baseline benchmark for the Turkish Language. We successfully fine-tuned a Turkish BERT model, namely BERTurk that is trained with base settings, to many downstream tasks and evaluated wit",
    "path": "papers/24/01/2401.17396.json",
    "total_tokens": 859,
    "translated_title": "对土耳其语理解任务进行基于Transformer编码器的微调",
    "translated_abstract": "深度学习和最近的Transformer语言模型在自然语言处理领域的研究中占据主导地位。由于其准确和快速的微调特性，它们已经超越了传统的基于机器学习的方法，并在许多具有挑战性的自然语言理解（NLU）问题上取得了最先进的结果。最近的研究表明，基于Transformer的模型，如BERT（双向Transformer编码器表示），在许多任务上取得了令人瞩目的成果。此外，由于它们的迁移学习能力，这些架构允许我们将预先构建好的模型转移并针对特定的NLU任务进行微调，如问答。在这项研究中，我们为土耳其语提供了一个基于Transformer的模型和一个基准测试。我们成功地对一种名为BERTurk的土耳其BERT模型进行了微调，该模型是使用基本设置进行训练的，并用于许多下游任务进行了评估。",
    "tldr": "在本研究中，我们提供了一个基于Transformer的模型和一个土耳其语基准测试，成功地对名为BERTurk的土耳其BERT模型进行了微调，实现了许多下游任务的理解和评估。",
    "en_tdlr": "In this study, we present a Transformer-based model and a benchmark for the Turkish Language, by successfully fine-tuning a Turkish BERT model called BERTurk, achieving understanding and evaluation on multiple downstream tasks."
}