{
    "title": "AAT: Adapting Audio Transformer for Various Acoustics Recognition Tasks. (arXiv:2401.10544v1 [cs.SD])",
    "abstract": "Recently, Transformers have been introduced into the field of acoustics recognition. They are pre-trained on large-scale datasets using methods such as supervised learning and semi-supervised learning, demonstrating robust generality--It fine-tunes easily to downstream tasks and shows more robust performance. However, the predominant fine-tuning method currently used is still full fine-tuning, which involves updating all parameters during training. This not only incurs significant memory usage and time costs but also compromises the model's generality. Other fine-tuning methods either struggle to address this issue or fail to achieve matching performance. Therefore, we conducted a comprehensive analysis of existing fine-tuning methods and proposed an efficient fine-tuning approach based on Adapter tuning, namely AAT. The core idea is to freeze the audio Transformer model and insert extra learnable Adapters, efficiently acquiring downstream task knowledge without compromising the model'",
    "link": "http://arxiv.org/abs/2401.10544",
    "context": "Title: AAT: Adapting Audio Transformer for Various Acoustics Recognition Tasks. (arXiv:2401.10544v1 [cs.SD])\nAbstract: Recently, Transformers have been introduced into the field of acoustics recognition. They are pre-trained on large-scale datasets using methods such as supervised learning and semi-supervised learning, demonstrating robust generality--It fine-tunes easily to downstream tasks and shows more robust performance. However, the predominant fine-tuning method currently used is still full fine-tuning, which involves updating all parameters during training. This not only incurs significant memory usage and time costs but also compromises the model's generality. Other fine-tuning methods either struggle to address this issue or fail to achieve matching performance. Therefore, we conducted a comprehensive analysis of existing fine-tuning methods and proposed an efficient fine-tuning approach based on Adapter tuning, namely AAT. The core idea is to freeze the audio Transformer model and insert extra learnable Adapters, efficiently acquiring downstream task knowledge without compromising the model'",
    "path": "papers/24/01/2401.10544.json",
    "total_tokens": 811,
    "translated_title": "AAT: 适应不同声学识别任务的音频Transformer",
    "translated_abstract": "最近，Transformer被引入到声学识别领域。它们使用监督学习和半监督学习等方法在大规模数据集上进行预训练，展示了稳健的通用性 - 它容易对下游任务进行微调并显示出更强的性能。然而，目前主要使用的微调方法仍然是完全微调，即在训练过程中更新所有参数。这不仅导致显著的内存使用和时间成本，还损害了模型的通用性。其他微调方法要么无法解决这个问题，要么无法达到相匹配的性能。因此，我们对现有微调方法进行了全面的分析，并提出了一种基于Adapter微调的高效微调方法，即AAT。核心思想是冻结音频Transformer模型并插入额外的可学习Adapter，以高效获取下游任务的知识而不损害模型的通用性。",
    "tldr": "AAT提出了一种基于Adapter微调的高效微调方法，能够在不牺牲模型的通用性的情况下获得下游任务的知识。",
    "en_tdlr": "AAT proposes an efficient fine-tuning approach based on Adapter tuning, which can acquire downstream task knowledge without compromising the model's generality."
}