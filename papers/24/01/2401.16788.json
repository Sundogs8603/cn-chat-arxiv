{
    "title": "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate. (arXiv:2401.16788v1 [cs.CL])",
    "abstract": "Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capab",
    "link": "http://arxiv.org/abs/2401.16788",
    "context": "Title: Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate. (arXiv:2401.16788v1 [cs.CL])\nAbstract: Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capab",
    "path": "papers/24/01/2401.16788.json",
    "total_tokens": 919,
    "translated_title": "大型语言模型作为评估者是否可信？通过代理辩论进行可扩展的元评估来评估LLMs",
    "translated_abstract": "尽管大型语言模型（LLMs）在各种任务和场景中具有实用性，但要在不同的上下文中可靠地评估LLMs仍然具有挑战性。现代评估方法通常使用LLMs来评估LLMs生成的响应。然而，用于评估这些LLMs作为评估者的元评估通常受现有基准的覆盖范围限制，或者需要大量的人工标注。这凸显了迫切需要可扩展的元评估方法，能够有效、可靠、高效地评估LLMs在各种任务和场景中作为评估者的性能，特别是在潜在的新的、用户定义的场景中。为了填补这一空白，我们提出了ScaleEval，这是一个代理辩论辅助的元评估框架，利用多个交流型LLM代理的能力。这个框架支持多轮讨论，以帮助人工标注者判断最具能力的",
    "tldr": "本论文提出了ScaleEval，一个基于代理辩论的元评估框架，通过利用多个交流型LLM代理的能力来有效、可靠、高效地评估LLMs在不同任务和场景中作为评估者的性能。",
    "en_tdlr": "This paper proposes ScaleEval, an agent-debate-assisted meta-evaluation framework that effectively and reliably evaluates the performance of LLMs as evaluators in different tasks and scenarios by leveraging the capabilities of multiple communicative LLM agents."
}