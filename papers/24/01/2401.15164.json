{
    "title": "AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations. (arXiv:2401.15164v1 [cs.SD])",
    "abstract": "Analyzing individual emotions during group conversation is crucial in developing intelligent agents capable of natural human-machine interaction. While reliable emotion recognition techniques depend on different modalities (text, audio, video), the inherent heterogeneity between these modalities and the dynamic cross-modal interactions influenced by an individual's unique behavioral patterns make the task of emotion recognition very challenging. This difficulty is compounded in group settings, where the emotion and its temporal evolution are not only influenced by the individual but also by external contexts like audience reaction and context of the ongoing conversation. To meet this challenge, we propose a Multimodal Attention Network that captures cross-modal interactions at various levels of spatial abstraction by jointly learning its interactive bunch of mode-specific Peripheral and Central networks. The proposed MAN injects cross-modal attention via its Peripheral key-value pairs ",
    "link": "http://arxiv.org/abs/2401.15164",
    "context": "Title: AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations. (arXiv:2401.15164v1 [cs.SD])\nAbstract: Analyzing individual emotions during group conversation is crucial in developing intelligent agents capable of natural human-machine interaction. While reliable emotion recognition techniques depend on different modalities (text, audio, video), the inherent heterogeneity between these modalities and the dynamic cross-modal interactions influenced by an individual's unique behavioral patterns make the task of emotion recognition very challenging. This difficulty is compounded in group settings, where the emotion and its temporal evolution are not only influenced by the individual but also by external contexts like audience reaction and context of the ongoing conversation. To meet this challenge, we propose a Multimodal Attention Network that captures cross-modal interactions at various levels of spatial abstraction by jointly learning its interactive bunch of mode-specific Peripheral and Central networks. The proposed MAN injects cross-modal attention via its Peripheral key-value pairs ",
    "path": "papers/24/01/2401.15164.json",
    "total_tokens": 883,
    "translated_title": "AMuSE: 自适应多模态分析用于群体对话中的说话者情感识别",
    "translated_abstract": "在开发具有自然人机交互能力的智能代理时，分析群体对话中的个体情感是至关重要的。虽然可靠的情感识别技术依赖于不同的模态（文本、音频、视频），但这些模态之间的异质性以及受个体行为模式影响的动态跨模态交互使情感识别任务变得非常具有挑战性。在群体环境中，这种困难变得更加复杂，因为情感及其时间演变不仅受个体影响，还受到观众反应和进行中对话的背景等外部环境的影响。为了应对这个挑战，我们提出了一种多模态注意力网络，通过联合学习模态特定的外围网络和中央网络的交互集合来捕捉不同级别的空间抽象中的跨模态交互。所提出的网络通过其外围键值对注入跨模态注意力。",
    "tldr": "本文提出了一种AMuSE模型，通过多模态注意力网络在群体对话中捕捉并分析说话者的情感。模型通过联合学习多个模态的外围和中央网络实现了不同级别的跨模态交互。",
    "en_tdlr": "This paper proposes AMuSE, a model that utilizes a multimodal attention network to capture and analyze speaker emotions in group conversations. The model achieves cross-modal interactions at different levels by jointly learning peripheral and central networks for multiple modalities."
}