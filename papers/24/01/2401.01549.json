{
    "title": "Towards Modeling Uncertainties of Self-explaining Neural Networks via Conformal Prediction. (arXiv:2401.01549v1 [cs.LG])",
    "abstract": "Despite the recent progress in deep neural networks (DNNs), it remains challenging to explain the predictions made by DNNs. Existing explanation methods for DNNs mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations. The fact that post-hoc methods can fail to reveal the actual original reasoning process of DNNs raises the need to build DNNs with built-in interpretability. Motivated by this, many self-explaining neural networks have been proposed to generate not only accurate predictions but also clear and intuitive insights into why a particular decision was made. However, existing self-explaining networks are limited in providing distribution-free uncertainty quantification for the two simultaneously generated prediction outcomes (i.e., a sample's final prediction and its corresponding explanations for interpreting that prediction). Importantly, they also fail to establish a connection between the confidence values assigned to the ge",
    "link": "http://arxiv.org/abs/2401.01549",
    "context": "Title: Towards Modeling Uncertainties of Self-explaining Neural Networks via Conformal Prediction. (arXiv:2401.01549v1 [cs.LG])\nAbstract: Despite the recent progress in deep neural networks (DNNs), it remains challenging to explain the predictions made by DNNs. Existing explanation methods for DNNs mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations. The fact that post-hoc methods can fail to reveal the actual original reasoning process of DNNs raises the need to build DNNs with built-in interpretability. Motivated by this, many self-explaining neural networks have been proposed to generate not only accurate predictions but also clear and intuitive insights into why a particular decision was made. However, existing self-explaining networks are limited in providing distribution-free uncertainty quantification for the two simultaneously generated prediction outcomes (i.e., a sample's final prediction and its corresponding explanations for interpreting that prediction). Importantly, they also fail to establish a connection between the confidence values assigned to the ge",
    "path": "papers/24/01/2401.01549.json",
    "total_tokens": 876,
    "translated_title": "通过符合性预测来模拟自解释神经网络的不确定性建模",
    "translated_abstract": "尽管深度神经网络(DNN)取得了近年来的进展，但解释DNN的预测仍然具有挑战性。现有的DNN解释方法主要集中在事后解释，即使用另一个解释模型来提供解释。事后方法可能无法揭示DNN的实际原始推理过程，这引发了构建具有内置可解释性的DNN的需求。基于这一动机，提出了许多自解释神经网络，它们不仅能够生成准确的预测，还能够清晰直观地解释为什么做出特定决策。然而，现有的自解释网络在提供分布无关的不确定性量化方面存在局限性，同时生成的两个预测结果（即样本的最终预测和解释该预测的相应解释）之间也无法建立连接。",
    "tldr": "这篇论文介绍了一种通过符合性预测来模拟自解释神经网络的不确定性建模的方法。现有的自解释网络存在不足，无法提供关于同时生成的预测结果和相应解释的分布无关的不确定性量化，并且无法建立其之间的连接。"
}