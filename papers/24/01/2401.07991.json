{
    "title": "Robustness Against Adversarial Attacks via Learning Confined Adversarial Polytopes. (arXiv:2401.07991v2 [cs.LG] UPDATED)",
    "abstract": "Deep neural networks (DNNs) could be deceived by generating human-imperceptible perturbations of clean samples. Therefore, enhancing the robustness of DNNs against adversarial attacks is a crucial task. In this paper, we aim to train robust DNNs by limiting the set of outputs reachable via a norm-bounded perturbation added to a clean sample. We refer to this set as adversarial polytope, and each clean sample has a respective adversarial polytope. Indeed, if the respective polytopes for all the samples are compact such that they do not intersect the decision boundaries of the DNN, then the DNN is robust against adversarial samples. Hence, the inner-working of our algorithm is based on learning \\textbf{c}onfined \\textbf{a}dversarial \\textbf{p}olytopes (CAP). By conducting a thorough set of experiments, we demonstrate the effectiveness of CAP over existing adversarial robustness methods in improving the robustness of models against state-of-the-art attacks including AutoAttack.",
    "link": "http://arxiv.org/abs/2401.07991",
    "context": "Title: Robustness Against Adversarial Attacks via Learning Confined Adversarial Polytopes. (arXiv:2401.07991v2 [cs.LG] UPDATED)\nAbstract: Deep neural networks (DNNs) could be deceived by generating human-imperceptible perturbations of clean samples. Therefore, enhancing the robustness of DNNs against adversarial attacks is a crucial task. In this paper, we aim to train robust DNNs by limiting the set of outputs reachable via a norm-bounded perturbation added to a clean sample. We refer to this set as adversarial polytope, and each clean sample has a respective adversarial polytope. Indeed, if the respective polytopes for all the samples are compact such that they do not intersect the decision boundaries of the DNN, then the DNN is robust against adversarial samples. Hence, the inner-working of our algorithm is based on learning \\textbf{c}onfined \\textbf{a}dversarial \\textbf{p}olytopes (CAP). By conducting a thorough set of experiments, we demonstrate the effectiveness of CAP over existing adversarial robustness methods in improving the robustness of models against state-of-the-art attacks including AutoAttack.",
    "path": "papers/24/01/2401.07991.json",
    "total_tokens": 941,
    "translated_title": "通过学习受限对抗多面体提高对抗攻击的鲁棒性",
    "translated_abstract": "深度神经网络(DNNs)可能会被生成的对干净样本的人类无法察觉的扰动欺骗。因此，提高DNNs对抗攻击的鲁棒性是一项关键任务。本文旨在通过限制添加到干净样本的范数有界扰动的输出集来训练鲁棒的DNNs。我们将这个集合称为对抗多面体，每个干净样本都有相应的对抗多面体。实际上，如果所有样本的相应多面体都是紧凑的，即它们不与DNN的决策边界相交，那么DNN对抗样本具有鲁棒性。因此，我们的算法的内部工作是基于学习受限的对抗多面体(CAP)。通过进行一系列的实验，我们证明了CAP在改善模型对抗最先进攻击(包括AutoAttack)的鲁棒性方面相对于现有的对抗鲁棒性方法的有效性。",
    "tldr": "本文提出了一种通过学习受限对抗多面体来提高深度神经网络对抗攻击的鲁棒性的方法，并通过实验证明了该方法相对于现有对抗鲁棒性方法的有效性。",
    "en_tdlr": "This paper proposes a method to enhance the robustness of deep neural networks against adversarial attacks by learning confined adversarial polytopes, and demonstrates its effectiveness over existing adversarial robustness methods through experiments."
}