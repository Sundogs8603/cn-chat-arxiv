{
    "title": "Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models. (arXiv:2401.10647v1 [cs.CL])",
    "abstract": "In the rapidly advancing field of artificial intelligence, the concept of Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a crucial area of study. This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models. This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity. Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors. Additionally, we propose a benchmark dataset NicheHazardQA to investigate this unsafe behavior both within the same and cross topical domain. This aspect of our research sheds light on how the edits, impact the model's safety metrics and guardrails. Our find",
    "link": "http://arxiv.org/abs/2401.10647",
    "context": "Title: Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models. (arXiv:2401.10647v1 [cs.CL])\nAbstract: In the rapidly advancing field of artificial intelligence, the concept of Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a crucial area of study. This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models. This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity. Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors. Additionally, we propose a benchmark dataset NicheHazardQA to investigate this unsafe behavior both within the same and cross topical domain. This aspect of our research sheds light on how the edits, impact the model's safety metrics and guardrails. Our find",
    "path": "papers/24/01/2401.10647.json",
    "total_tokens": 905,
    "translated_title": "播风撩起风暴：编辑语言模型的影响",
    "translated_abstract": "在人工智能领域中，红队测试或越狱大型语言模型（LLM）的概念已成为一个重要的研究领域。通过对模型进行编辑，揭示了这种修改的复杂后果，发现了增强模型准确性与保持其道德完整性之间的复杂关系。我们的深入分析揭示了一个令人惊讶的悖论：虽然注入准确信息对于模型的可靠性至关重要，但它却可能破坏模型的基本框架，导致不可预测和潜在的不安全行为。此外，我们提出了一个基准数据集NicheHazardQA，用于研究模型在相同和跨领域中的不安全行为。这一方面的研究揭示了编辑如何影响模型的安全度量和保护机制。",
    "tldr": "本文研究了通过编辑语言模型的复杂后果，发现在增强模型准确性与保持道德完整性之间存在悖论。我们发现，尽管注入准确信息对模型的可靠性很重要，但它可能破坏模型的基本框架，导致不可预测和潜在的不安全行为。",
    "en_tdlr": "This paper investigates the complex consequences of editing language models, revealing a paradox between enhancing model accuracy and preserving ethical integrity. While injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors."
}