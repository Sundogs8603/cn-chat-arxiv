{
    "title": "Rademacher Complexity of Neural ODEs via Chen-Fliess Series. (arXiv:2401.16655v1 [stat.ML])",
    "abstract": "We show how continuous-depth neural ODE models can be framed as single-layer, infinite-width nets using the Chen--Fliess series expansion for nonlinear ODEs. In this net, the output ''weights'' are taken from the signature of the control input -- a tool used to represent infinite-dimensional paths as a sequence of tensors -- which comprises iterated integrals of the control input over a simplex. The ''features'' are taken to be iterated Lie derivatives of the output function with respect to the vector fields in the controlled ODE model. The main result of this work applies this framework to derive compact expressions for the Rademacher complexity of ODE models that map an initial condition to a scalar output at some terminal time. The result leverages the straightforward analysis afforded by single-layer architectures. We conclude with some examples instantiating the bound for some specific systems and discuss potential follow-up work.",
    "link": "http://arxiv.org/abs/2401.16655",
    "context": "Title: Rademacher Complexity of Neural ODEs via Chen-Fliess Series. (arXiv:2401.16655v1 [stat.ML])\nAbstract: We show how continuous-depth neural ODE models can be framed as single-layer, infinite-width nets using the Chen--Fliess series expansion for nonlinear ODEs. In this net, the output ''weights'' are taken from the signature of the control input -- a tool used to represent infinite-dimensional paths as a sequence of tensors -- which comprises iterated integrals of the control input over a simplex. The ''features'' are taken to be iterated Lie derivatives of the output function with respect to the vector fields in the controlled ODE model. The main result of this work applies this framework to derive compact expressions for the Rademacher complexity of ODE models that map an initial condition to a scalar output at some terminal time. The result leverages the straightforward analysis afforded by single-layer architectures. We conclude with some examples instantiating the bound for some specific systems and discuss potential follow-up work.",
    "path": "papers/24/01/2401.16655.json",
    "total_tokens": 845,
    "translated_title": "通过Chen-Fliess序列，我们展示了如何将连续深度神经ODE模型构建为单层、无限宽度的网络。",
    "translated_abstract": "本文将连续深度神经ODE模型使用Chen-Fliess序列展开为单层、无限宽度的网络。在这个网络中，输出的“权重”来自控制输入的特征序列，它由控制输入在单纯形上的迭代积分构成。而“特征”则基于受控ODE模型中输出函数相对于向量场的迭代李导数。本文的主要结果是，应用这个框架推导出了将初始条件映射到某个终端时间的ODE模型的Rademacher复杂度的紧凑表达式。这一结果利用了单层结构所带来的直接分析性质。最后，我们通过一些具体系统的例子实例化该界，并讨论了可能的后续工作。",
    "tldr": "本文通过Chen-Fliess序列展开将连续深度神经ODE模型转化为单层、无限宽度的网络，并利用此框架推导出了将初始条件映射到某个终端时间的ODE模型的Rademacher复杂度的紧凑表达式。",
    "en_tdlr": "This paper transforms continuous-depth neural ODE models into single-layer, infinite-width nets using the Chen-Fliess series expansion and derives compact expressions for the Rademacher complexity."
}