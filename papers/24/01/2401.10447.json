{
    "title": "Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition. (arXiv:2401.10447v1 [cs.CL])",
    "abstract": "The use of low-rank adaptation (LoRA) with frozen pretrained language models (PLMs) has become increasing popular as a mainstream, resource-efficient modeling approach for memory-constrained hardware. In this study, we first explore how to enhance model performance by introducing various LoRA training strategies, achieving relative word error rate reductions of 3.50\\% on the public Librispeech dataset and of 3.67\\% on an internal dataset in the messaging domain. To further characterize the stability of LoRA-based second-pass speech recognition models, we examine robustness against input perturbations. These perturbations are rooted in homophone replacements and a novel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both designed to measure the relative degradation in the performance of rescoring models. Our experimental results indicate that while advanced variants of LoRA, such as dynamic rank-allocated LoRA, lead to performance degradation in $1$-best perturbati",
    "link": "http://arxiv.org/abs/2401.10447",
    "context": "Title: Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition. (arXiv:2401.10447v1 [cs.CL])\nAbstract: The use of low-rank adaptation (LoRA) with frozen pretrained language models (PLMs) has become increasing popular as a mainstream, resource-efficient modeling approach for memory-constrained hardware. In this study, we first explore how to enhance model performance by introducing various LoRA training strategies, achieving relative word error rate reductions of 3.50\\% on the public Librispeech dataset and of 3.67\\% on an internal dataset in the messaging domain. To further characterize the stability of LoRA-based second-pass speech recognition models, we examine robustness against input perturbations. These perturbations are rooted in homophone replacements and a novel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both designed to measure the relative degradation in the performance of rescoring models. Our experimental results indicate that while advanced variants of LoRA, such as dynamic rank-allocated LoRA, lead to performance degradation in $1$-best perturbati",
    "path": "papers/24/01/2401.10447.json",
    "total_tokens": 1029,
    "translated_title": "语音识别中低秩适应的训练策略和模型鲁棒性研究",
    "translated_abstract": "随着资源有限的硬件设备的普及，使用低秩适应（LoRA）与冻结预训练语言模型（PLMs）已成为一种主流、资源高效的建模方法。本研究首先探讨了如何通过引入各种LoRA训练策略来提高模型性能，在公开的Librispeech数据集上实现了相对词错误率降低3.50％，在消息领域的内部数据集上实现了3.67％的降低。为了进一步评估基于LoRA的二次传递语音识别模型的稳定性，我们研究了对输入扰动的鲁棒性。这些扰动源于同音字替代和一种名为N-best Perturbation-based Rescoring Robustness（NPRR）的新度量标准，这两种方法都用于衡量重评分模型性能的相对降解程度。我们的实验结果表明，虽然LoRA的高级变体（例如动态秩分配的LoRA）导致了$1$-best扰动的性能下降。",
    "tldr": "本研究研究了语音识别中低秩适应的训练策略和模型鲁棒性。通过引入不同的LoRA训练策略，实现了相对词错误率的降低，并研究了模型对输入扰动的稳定性。实验结果表明，高级LoRA变体导致了某些扰动的性能下降。",
    "en_tdlr": "This study investigates training strategies and model robustness of low-rank adaptation for language modeling in speech recognition. By introducing various LoRA training strategies, the relative word error rate is reduced, and the stability of the model against input perturbations is examined. Experimental results indicate that advanced variants of LoRA lead to performance degradation for certain perturbations."
}