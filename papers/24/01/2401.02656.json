{
    "title": "GTA: Guided Transfer of Spatial Attention from Object-Centric Representations. (arXiv:2401.02656v1 [cs.CV])",
    "abstract": "Utilizing well-trained representations in transfer learning often results in superior performance and faster convergence compared to training from scratch. However, even if such good representations are transferred, a model can easily overfit the limited training dataset and lose the valuable properties of the transferred representations. This phenomenon is more severe in ViT due to its low inductive bias. Through experimental analysis using attention maps in ViT, we observe that the rich representations deteriorate when trained on a small dataset. Motivated by this finding, we propose a novel and simple regularization method for ViT called Guided Transfer of spatial Attention (GTA). Our proposed method regularizes the self-attention maps between the source and target models. A target model can fully exploit the knowledge related to object localization properties through this explicit regularization. Our experimental results show that the proposed GTA consistently improves the accuracy",
    "link": "http://arxiv.org/abs/2401.02656",
    "context": "Title: GTA: Guided Transfer of Spatial Attention from Object-Centric Representations. (arXiv:2401.02656v1 [cs.CV])\nAbstract: Utilizing well-trained representations in transfer learning often results in superior performance and faster convergence compared to training from scratch. However, even if such good representations are transferred, a model can easily overfit the limited training dataset and lose the valuable properties of the transferred representations. This phenomenon is more severe in ViT due to its low inductive bias. Through experimental analysis using attention maps in ViT, we observe that the rich representations deteriorate when trained on a small dataset. Motivated by this finding, we propose a novel and simple regularization method for ViT called Guided Transfer of spatial Attention (GTA). Our proposed method regularizes the self-attention maps between the source and target models. A target model can fully exploit the knowledge related to object localization properties through this explicit regularization. Our experimental results show that the proposed GTA consistently improves the accuracy",
    "path": "papers/24/01/2401.02656.json",
    "total_tokens": 927,
    "translated_title": "GTA: 从物体中心的表示中引导空间注意力的转移",
    "translated_abstract": "利用训练良好的表示在转移学习中通常会得到更好的性能和更快的收敛速度，与从头开始训练相比。然而，即使这样的好的表示被转移，模型仍然很容易过拟合有限的训练数据集，并且失去了转移表示的有价值的特性。这种现象在 ViT 中更为严重，因为它的归纳偏置较低。通过使用 ViT 中的注意力映射进行实验分析，我们观察到当在小数据集上训练时，丰富的表示会退化。受到这一发现的启发，我们提出了一种名为 Guided Transfer of spatial Attention (GTA) 的新颖而简单的 ViT 正则化方法。我们提出的方法通过显式的正则化在源模型和目标模型之间的自注意力映射。通过这种显式的正则化，目标模型可以充分利用与物体定位属性相关的知识。我们的实验结果表明，提出的 GTA 方法能够稳定提高准确性。",
    "tldr": "本论文提出了一种名为GTA的正则化方法，它通过在源模型和目标模型之间的自注意力映射中引导空间注意力的转移，可以解决在ViT中转移表示容易过拟合和失去有价值特性的问题，实验证明该方法能够稳定提高准确性。"
}