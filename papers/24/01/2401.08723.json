{
    "title": "HierSFL: Local Differential Privacy-aided Split Federated Learning in Mobile Edge Computing. (arXiv:2401.08723v1 [cs.CR])",
    "abstract": "Federated Learning is a promising approach for learning from user data while preserving data privacy. However, the high requirements of the model training process make it difficult for clients with limited memory or bandwidth to participate. To tackle this problem, Split Federated Learning is utilized, where clients upload their intermediate model training outcomes to a cloud server for collaborative server-client model training. This methodology facilitates resource-constrained clients' participation in model training but also increases the training time and communication overhead. To overcome these limitations, we propose a novel algorithm, called Hierarchical Split Federated Learning (HierSFL), that amalgamates models at the edge and cloud phases, presenting qualitative directives for determining the best aggregation timeframes to reduce computation and communication expenses. By implementing local differential privacy at the client and edge server levels, we enhance privacy during ",
    "link": "http://arxiv.org/abs/2401.08723",
    "context": "Title: HierSFL: Local Differential Privacy-aided Split Federated Learning in Mobile Edge Computing. (arXiv:2401.08723v1 [cs.CR])\nAbstract: Federated Learning is a promising approach for learning from user data while preserving data privacy. However, the high requirements of the model training process make it difficult for clients with limited memory or bandwidth to participate. To tackle this problem, Split Federated Learning is utilized, where clients upload their intermediate model training outcomes to a cloud server for collaborative server-client model training. This methodology facilitates resource-constrained clients' participation in model training but also increases the training time and communication overhead. To overcome these limitations, we propose a novel algorithm, called Hierarchical Split Federated Learning (HierSFL), that amalgamates models at the edge and cloud phases, presenting qualitative directives for determining the best aggregation timeframes to reduce computation and communication expenses. By implementing local differential privacy at the client and edge server levels, we enhance privacy during ",
    "path": "papers/24/01/2401.08723.json",
    "total_tokens": 865,
    "translated_title": "HierSFL：移动边缘计算中基于局部差分隐私的分割联合学习",
    "translated_abstract": "联邦学习是一种在保护数据隐私的同时利用用户数据进行学习的有前途的方法。然而，模型训练过程的高要求使得内存或带宽有限的客户端参与变得困难。为了解决这个问题，使用了分割联合学习，其中客户端将中间模型训练结果上传到云服务器进行协同的服务器-客户端模型训练。这种方法促进了资源受限客户端的参与，但也增加了训练时间和通信开销。为了克服这些限制，我们提出了一种新的算法，称为Hierarchical Split Federated Learning（HierSFL），它在边缘和云阶段合并模型，并提供了定性指导来确定最佳聚合时间框架，从而减少计算和通信开销。通过在客户端和边缘服务器级别实施局部差分隐私，我们提高了隐私保护能力。",
    "tldr": "HierSFL提出了一种基于局部差分隐私的分割联合学习算法，通过合并模型并定性指导最佳聚合时间框架，减少计算和通信开销，提高了隐私保护能力。"
}