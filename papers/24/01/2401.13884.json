{
    "title": "Constant Stepsize Q-learning: Distributional Convergence, Bias and Extrapolation. (arXiv:2401.13884v1 [stat.ML])",
    "abstract": "Stochastic Approximation (SA) is a widely used algorithmic approach in various fields, including optimization and reinforcement learning (RL). Among RL algorithms, Q-learning is particularly popular due to its empirical success. In this paper, we study asynchronous Q-learning with constant stepsize, which is commonly used in practice for its fast convergence. By connecting the constant stepsize Q-learning to a time-homogeneous Markov chain, we show the distributional convergence of the iterates in Wasserstein distance and establish its exponential convergence rate. We also establish a Central Limit Theory for Q-learning iterates, demonstrating the asymptotic normality of the averaged iterates. Moreover, we provide an explicit expansion of the asymptotic bias of the averaged iterate in stepsize. Specifically, the bias is proportional to the stepsize up to higher-order terms and we provide an explicit expression for the linear coefficient. This precise characterization of the bias allows",
    "link": "http://arxiv.org/abs/2401.13884",
    "context": "Title: Constant Stepsize Q-learning: Distributional Convergence, Bias and Extrapolation. (arXiv:2401.13884v1 [stat.ML])\nAbstract: Stochastic Approximation (SA) is a widely used algorithmic approach in various fields, including optimization and reinforcement learning (RL). Among RL algorithms, Q-learning is particularly popular due to its empirical success. In this paper, we study asynchronous Q-learning with constant stepsize, which is commonly used in practice for its fast convergence. By connecting the constant stepsize Q-learning to a time-homogeneous Markov chain, we show the distributional convergence of the iterates in Wasserstein distance and establish its exponential convergence rate. We also establish a Central Limit Theory for Q-learning iterates, demonstrating the asymptotic normality of the averaged iterates. Moreover, we provide an explicit expansion of the asymptotic bias of the averaged iterate in stepsize. Specifically, the bias is proportional to the stepsize up to higher-order terms and we provide an explicit expression for the linear coefficient. This precise characterization of the bias allows",
    "path": "papers/24/01/2401.13884.json",
    "total_tokens": 987,
    "translated_title": "Constant Stepsize Q-learning: 分布收敛性、偏差和外推",
    "translated_abstract": "随机逼近（SA）是一种广泛应用于各个领域的算法方法，包括优化和强化学习（RL）。在RL算法中，由于其经验成功，Q-learning特别受欢迎。在本文中，我们研究了常数步长异步Q-learning，这在实践中通常用于快速收敛。通过将常数步长Q-learning与时间均匀的马尔可夫链相关联，我们展示了迭代在Wasserstein距离下的分布收敛性，并建立了其指数收敛速度。我们还为Q-learning迭代建立了中心极限定理，证明了平均迭代的渐近正态性。此外，我们提供了平均迭代步骤的渐近偏差的明确展开。具体而言，偏差与步长成正比，直到高阶项，并为线性系数提供了明确的表达式。这种对偏差的精确描述可以使得我们可以探索出对于这种偏差的更深入理解。",
    "tldr": "本文研究了常数步长异步Q-learning算法，并通过分析其与马尔可夫链的关联，展示了其迭代在分布上的收敛性和指数收敛速度。同时，研究者还建立了该算法迭代的中心极限定理，并对其渐近偏差进行了精确的展开。通过这些分析，我们可以深入理解这种算法的优化效果和偏差特性。",
    "en_tdlr": "This paper investigates the constant stepsize Q-learning algorithm and shows its distributional convergence and exponential convergence rate by analyzing its connection with a Markov chain. The researchers also establish the central limit theorem for the algorithm's iterates and provide an accurate expansion of the asymptotic bias. These analyses deepen our understanding of the optimization performance and bias characteristics of this algorithm."
}