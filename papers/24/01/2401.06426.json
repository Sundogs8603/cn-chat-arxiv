{
    "title": "UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer. (arXiv:2401.06426v1 [cs.CV])",
    "abstract": "Traditional channel-wise pruning methods by reducing network channels struggle to effectively prune efficient CNN models with depth-wise convolutional layers and certain efficient modules, such as popular inverted residual blocks. Prior depth pruning methods by reducing network depths are not suitable for pruning some efficient models due to the existence of some normalization layers. Moreover, finetuning subnet by directly removing activation layers would corrupt the original model weights, hindering the pruned model from achieving high performance. To address these issues, we propose a novel depth pruning method for efficient models. Our approach proposes a novel block pruning strategy and progressive training method for the subnet. Additionally, we extend our pruning method to vision transformer models. Experimental results demonstrate that our method consistently outperforms existing depth pruning methods across various pruning configurations. We obtained three pruned ConvNeXtV1 mo",
    "link": "http://arxiv.org/abs/2401.06426",
    "context": "Title: UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer. (arXiv:2401.06426v1 [cs.CV])\nAbstract: Traditional channel-wise pruning methods by reducing network channels struggle to effectively prune efficient CNN models with depth-wise convolutional layers and certain efficient modules, such as popular inverted residual blocks. Prior depth pruning methods by reducing network depths are not suitable for pruning some efficient models due to the existence of some normalization layers. Moreover, finetuning subnet by directly removing activation layers would corrupt the original model weights, hindering the pruned model from achieving high performance. To address these issues, we propose a novel depth pruning method for efficient models. Our approach proposes a novel block pruning strategy and progressive training method for the subnet. Additionally, we extend our pruning method to vision transformer models. Experimental results demonstrate that our method consistently outperforms existing depth pruning methods across various pruning configurations. We obtained three pruned ConvNeXtV1 mo",
    "path": "papers/24/01/2401.06426.json",
    "total_tokens": 897,
    "translated_title": "UPDP: 一种适用于CNN和Vision Transformer的统一渐进深度修剪器",
    "translated_abstract": "传统的逐通道修剪方法通过减少网络通道来修剪CNN模型中的深度卷积层和某些高效模块，如流行的反向残差块。之前的深度修剪方法通过减少网络深度不能很好地修剪一些高效模型，因为存在某些归一化层。此外，通过直接删除激活层来微调子网络会破坏原始模型权重，阻碍修剪模型的高性能。为了解决这些问题，我们提出了一种新颖的高效模型深度修剪方法。我们的方法提出了一种新颖的块修剪策略和子网络的渐进训练方法。此外，我们将修剪方法扩展到了Vision Transformer模型。实验证明，我们的方法在各种修剪配置上持续优于现有的深度修剪方法。我们获得了三个修剪后的ConvNeXtV1模型。",
    "tldr": "UPDP是一种适用于CNN和Vision Transformer的统一渐进深度修剪器，通过引入新的块修剪策略和渐进训练方法，解决了传统的修剪方法在修剪高效模型时遇到的问题，且在各种修剪配置下表现优于现有的深度修剪方法。"
}