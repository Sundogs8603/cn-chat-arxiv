{
    "title": "Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance. (arXiv:2401.15328v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. We explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely depending on the LLM's inherent abilities. More concretely, using financial domain question-answering datasets, we apply supervised fine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and 'task solver'. The 'task router' dynamically directs a question to either be answered internally by the LLM or externally via the right tool from the tool set. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2% and 5.06% over the base model and SFT-only baselines, respectively, and is highly competitive with st",
    "link": "http://arxiv.org/abs/2401.15328",
    "context": "Title: Equipping Language Models with Tool Use Capability for Tabular Data Analysis in Finance. (arXiv:2401.15328v1 [cs.CL])\nAbstract: Large language models (LLMs) have exhibited an array of reasoning capabilities but face challenges like error propagation and hallucination, particularly in specialised areas like finance, where data is heterogeneous, and precision is paramount. We explore the potential of language model augmentation with external tools to mitigate these limitations and offload certain reasoning steps to external tools that are more suited for the task, instead of solely depending on the LLM's inherent abilities. More concretely, using financial domain question-answering datasets, we apply supervised fine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and 'task solver'. The 'task router' dynamically directs a question to either be answered internally by the LLM or externally via the right tool from the tool set. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2% and 5.06% over the base model and SFT-only baselines, respectively, and is highly competitive with st",
    "path": "papers/24/01/2401.15328.json",
    "total_tokens": 908,
    "translated_title": "在金融数据分析中，为语言模型添加工具使用能力",
    "translated_abstract": "大型语言模型(LLMs)展示了一系列的推理能力，但在金融等专业领域中遇到了错误传播和产生幻觉等挑战，其中数据异构性和精度至关重要。为了克服这些限制，我们探索了语言模型通过外部工具增强能力，将某些推理步骤转移到更适合该任务的外部工具上，而不仅仅依赖LLM的内在能力。具体而言，我们使用金融领域的问答数据集，在LLaMA-2 13B Chat模型上进行有监督的微调，使其既充当“任务路由器”又充当“任务解决器”。该“任务路由器”动态将问题定向到LLM内部回答或通过工具集中的正确工具外部回答。我们的工具增强型SFT模型Raven相比基础模型和仅有SFT的基准模型分别提高了35.2%和5.06%，在竞争力方面具有很高的水平。",
    "tldr": "在金融数据分析领域，通过为语言模型添加工具使用能力，我们成功解决了大型语言模型在处理异构金融数据和保证精度时所面临的挑战，并取得了显著的改进。",
    "en_tdlr": "We address the challenges faced by large language models in handling heterogeneous financial data and ensuring precision by equipping them with tool use capability, leading to significant improvements."
}