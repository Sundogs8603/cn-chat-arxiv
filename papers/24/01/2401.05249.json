{
    "title": "CASA: Causality-driven Argument Sufficiency Assessment. (arXiv:2401.05249v1 [cs.CL])",
    "abstract": "The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion. To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the probability of sufficiency (PS) definition in the causal literature, we propose CASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion, when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion, and revise them by injecting the premise event. Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance a",
    "link": "http://arxiv.org/abs/2401.05249",
    "context": "Title: CASA: Causality-driven Argument Sufficiency Assessment. (arXiv:2401.05249v1 [cs.CL])\nAbstract: The argument sufficiency assessment task aims to determine if the premises of a given argument support its conclusion. To tackle this task, existing works often train a classifier on data annotated by humans. However, annotating data is laborious, and annotations are often inconsistent due to subjective criteria. Motivated by the probability of sufficiency (PS) definition in the causal literature, we propose CASA, a zero-shot causality-driven argument sufficiency assessment framework. PS measures how likely introducing the premise event would lead to the conclusion, when both the premise and conclusion events are absent. To estimate this probability, we propose to use large language models (LLMs) to generate contexts that are inconsistent with the premise and conclusion, and revise them by injecting the premise event. Experiments on two logical fallacy detection datasets demonstrate that CASA accurately identifies insufficient arguments. We further deploy CASA in a writing assistance a",
    "path": "papers/24/01/2401.05249.json",
    "total_tokens": 876,
    "translated_title": "CASA: 因果驱动的论证充分性评估",
    "translated_abstract": "论证充分性评估任务旨在确定一个给定论证的前提是否支持其结论。为了解决这个任务，现有的方法通常会对人工注释的数据进行分类器训练。然而，标注数据是费力的，而且由于主观标准的不一致性，标注往往也不一致。受因果文献中的充分概率（PS）定义的启发，我们提出了CASA，一个零射因果驱动的论证充分性评估框架。PS衡量的是当前提事件和结论事件都不存在时，引入前提事件是否会导致结论的可能性。为了估计这个概率，我们提出使用大型语言模型（LLMs）生成与前提和结论不一致的上下文，并通过注入前提事件对它们进行修改。在两个逻辑谬误检测数据集上的实验证明，CASA能够准确识别不足的论证。我们进一步将CASA部署在写作辅助系统中。",
    "tldr": "CASA是一个因果驱动的论证充分性评估框架，利用大型语言模型生成与前提和结论不一致的上下文，并通过注入前提事件对其进行修改，能够准确识别不足的论证。"
}