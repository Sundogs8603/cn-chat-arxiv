{
    "title": "Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature and Bayesian Optimization. (arXiv:2401.05716v1 [cs.LG])",
    "abstract": "In this paper, we study the problem of estimating the normalizing constant $\\int e^{-\\lambda f(x)}dx$ through queries to the black-box function $f$, where $f$ belongs to a reproducing kernel Hilbert space (RKHS), and $\\lambda$ is a problem parameter. We show that to estimate the normalizing constant within a small relative error, the level of difficulty depends on the value of $\\lambda$: When $\\lambda$ approaches zero, the problem is similar to Bayesian quadrature (BQ), while when $\\lambda$ approaches infinity, the problem is similar to Bayesian optimization (BO). More generally, the problem varies between BQ and BO. We find that this pattern holds true even when the function evaluations are noisy, bringing new aspects to this topic. Our findings are supported by both algorithm-independent lower bounds and algorithmic upper bounds, as well as simulation studies conducted on a variety of benchmark functions.",
    "link": "http://arxiv.org/abs/2401.05716",
    "context": "Title: Kernelized Normalizing Constant Estimation: Bridging Bayesian Quadrature and Bayesian Optimization. (arXiv:2401.05716v1 [cs.LG])\nAbstract: In this paper, we study the problem of estimating the normalizing constant $\\int e^{-\\lambda f(x)}dx$ through queries to the black-box function $f$, where $f$ belongs to a reproducing kernel Hilbert space (RKHS), and $\\lambda$ is a problem parameter. We show that to estimate the normalizing constant within a small relative error, the level of difficulty depends on the value of $\\lambda$: When $\\lambda$ approaches zero, the problem is similar to Bayesian quadrature (BQ), while when $\\lambda$ approaches infinity, the problem is similar to Bayesian optimization (BO). More generally, the problem varies between BQ and BO. We find that this pattern holds true even when the function evaluations are noisy, bringing new aspects to this topic. Our findings are supported by both algorithm-independent lower bounds and algorithmic upper bounds, as well as simulation studies conducted on a variety of benchmark functions.",
    "path": "papers/24/01/2401.05716.json",
    "total_tokens": 1016,
    "translated_title": "基于核函数的归一化常数估计：连接贝叶斯积分和贝叶斯优化",
    "translated_abstract": "本文研究通过黑盒函数查询来估计归一化常数$\\int e^{-\\lambda f(x)}dx$的问题，其中$f$属于再生核希尔伯特空间(RKHS)，而$\\lambda$是一个问题参数。我们发现，为了在相对误差较小的情况下估计归一化常数，难度的级别取决于$\\lambda$的值：当$\\lambda$趋近于零时，问题类似于贝叶斯积分(BQ)，而当$\\lambda$趋近于无穷大时，问题类似于贝叶斯优化(BO)。更一般地，问题在BQ和BO之间变化。我们发现，即使在函数评估存在噪声的情况下，这种模式仍然适用，为该主题带来了新的方面。我们的发现得到了算法无关的下界和算法上界的支持，以及在各种基准函数上进行的模拟研究。",
    "tldr": "本文研究了通过查询黑盒函数来估计归一化常数的问题，发现问题的难度取决于问题参数$\\lambda$的大小，当$\\lambda$趋近于零时类似于贝叶斯积分(BQ)，当$\\lambda$趋近于无穷大时类似于贝叶斯优化(BO)，且这种模式适用于存在噪声的情况。结果得到了算法无关的下界和上界的支持，以及模拟研究的验证。",
    "en_tdlr": "This paper investigates the problem of estimating the normalizing constant through queries to a black-box function, and finds that the difficulty of the problem depends on the parameter $\\lambda$, with similarities to Bayesian quadrature when $\\lambda$ approaches zero and to Bayesian optimization when $\\lambda$ approaches infinity. This pattern holds even when the function evaluations are noisy. The findings are supported by both algorithm-independent lower bounds and algorithmic upper bounds, as well as simulation studies."
}