{
    "title": "A Cross-Language Investigation into Jailbreak Attacks in Large Language Models. (arXiv:2401.16765v1 [cs.CR])",
    "abstract": "Large Language Models (LLMs) have become increasingly popular for their advanced text generation capabilities across various domains. However, like any software, they face security challenges, including the risk of 'jailbreak' attacks that manipulate LLMs to produce prohibited content. A particularly underexplored area is the Multilingual Jailbreak attack, where malicious questions are translated into various languages to evade safety filters. Currently, there is a lack of comprehensive empirical studies addressing this specific threat.  To address this research gap, we conducted an extensive empirical study on Multilingual Jailbreak attacks. We developed a novel semantic-preserving algorithm to create a multilingual jailbreak dataset and conducted an exhaustive evaluation on both widely-used open-source and commercial LLMs, including GPT-4 and LLaMa. Additionally, we performed interpretability analysis to uncover patterns in Multilingual Jailbreak attacks and implemented a fine-tuning",
    "link": "http://arxiv.org/abs/2401.16765",
    "context": "Title: A Cross-Language Investigation into Jailbreak Attacks in Large Language Models. (arXiv:2401.16765v1 [cs.CR])\nAbstract: Large Language Models (LLMs) have become increasingly popular for their advanced text generation capabilities across various domains. However, like any software, they face security challenges, including the risk of 'jailbreak' attacks that manipulate LLMs to produce prohibited content. A particularly underexplored area is the Multilingual Jailbreak attack, where malicious questions are translated into various languages to evade safety filters. Currently, there is a lack of comprehensive empirical studies addressing this specific threat.  To address this research gap, we conducted an extensive empirical study on Multilingual Jailbreak attacks. We developed a novel semantic-preserving algorithm to create a multilingual jailbreak dataset and conducted an exhaustive evaluation on both widely-used open-source and commercial LLMs, including GPT-4 and LLaMa. Additionally, we performed interpretability analysis to uncover patterns in Multilingual Jailbreak attacks and implemented a fine-tuning",
    "path": "papers/24/01/2401.16765.json",
    "total_tokens": 1001,
    "translated_title": "大型语言模型中的越狱攻击的跨语言研究",
    "translated_abstract": "大型语言模型（LLMs）因其在各个领域中先进的文本生成能力而越来越受欢迎。然而，像任何软件一样，它们面临安全挑战，包括“越狱”攻击的风险，即操纵LLMs生成被禁内容。一个在研究中尚未得到充分探索的领域是多语言越狱攻击，即将恶意问题翻译成各种语言以逃避安全过滤器。目前，尚缺乏全面的经验证据的研究来解决这一特定威胁。为了填补这一研究空白，我们对多语言越狱攻击进行了广泛的经验研究。我们开发了一种新颖的语义保留算法来创建一个多语言越狱数据集，并对包括GPT-4和LLaMa在内的广泛使用的开源和商业LLMs进行了详尽的评估。此外，我们进行了可解释性分析，以揭示多语言越狱攻击中的模式，并实施了精调。",
    "tldr": "大型语言模型面临着越狱攻击的威胁，在跨语言的环境下，恶意问题可以逃避安全过滤器。本研究填补了这一研究空白，通过广泛的经验研究和语义保留算法的开发，揭示了多语言越狱攻击的模式和安全威胁。",
    "en_tdlr": "Large language models are susceptible to jailbreak attacks, especially in multilingual environments where malicious queries can evade safety filters. This study addresses this research gap by conducting extensive empirical research, developing a semantic-preserving algorithm, and uncovering patterns and security threats in multilingual jailbreak attacks."
}