{
    "title": "A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification. (arXiv:2401.13887v1 [cs.CL])",
    "abstract": "Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have demonstrated promising transfer learning capability. In this study, we explored whether recent LLMs can reduce the need for large-scale data annotations. We curated a manually-labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare zero-shot classification capability of the GPT-4 model and the GPT-3.5 model with supervised classification performance of three model architectures: random forests classifier, long short-term memory networks with attention (LSTM-Att), and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either significantly better than or as well as the best supervised model, the LSTM-Att model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance between labels, the di",
    "link": "http://arxiv.org/abs/2401.13887",
    "context": "Title: A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification. (arXiv:2401.13887v1 [cs.CL])\nAbstract: Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have demonstrated promising transfer learning capability. In this study, we explored whether recent LLMs can reduce the need for large-scale data annotations. We curated a manually-labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare zero-shot classification capability of the GPT-4 model and the GPT-3.5 model with supervised classification performance of three model architectures: random forests classifier, long short-term memory networks with attention (LSTM-Att), and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either significantly better than or as well as the best supervised model, the LSTM-Att model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance between labels, the di",
    "path": "papers/24/01/2401.13887.json",
    "total_tokens": 982,
    "translated_title": "通过大型语言模型和监督建模的零样本推断进行乳腺癌病理分类的比较研究",
    "translated_abstract": "尽管监督机器学习在从临床记录中提取信息方面十分流行，但创建大型注释数据集需要广泛的领域专业知识和耗费时间。与此同时，大型语言模型（LLMs）展示了很强的迁移学习能力。在本研究中，我们探讨了最近的LLMs是否可以减少对大规模数据注释的需求。我们整理了一个手动标注的769个乳腺癌病理报告的数据集（标注了13个类别），来比较GPT-4模型和GPT-3.5模型的零样本分类能力与三种模型架构的监督分类性能：随机森林分类器，具有注意力的长短期记忆网络（LSTM-Att）和UCSF-BERT模型。在所有13个任务中，GPT-4模型的性能要么明显优于最佳的监督模型LSTM-Att模型（平均宏F1得分为0.83 vs. 0.75），要么与其相当。在存在标签之间高度不平衡的任务中，di",
    "tldr": "本研究比较了大型语言模型与监督建模在乳腺癌病理分类上的零样本推断能力，发现GPT-4模型在所有任务中要么明显优于，要么与最佳的监督模型LSTM-Att模型相当。",
    "en_tdlr": "This study compares the zero-shot inference capability of large language models (LLMs) and supervised modeling in breast cancer pathology classification. The results show that the GPT-4 model outperforms or performs as well as the best supervised model, the LSTM-Att model, in all tasks."
}