{
    "title": "Pre-training and Diagnosing Knowledge Base Completion Models. (arXiv:2401.15439v1 [cs.CL])",
    "abstract": "In this work, we introduce and analyze an approach to knowledge transfer from one collection of facts to another without the need for entity or relation matching. The method works for both canonicalized knowledge bases and uncanonicalized or open knowledge bases, i.e., knowledge bases where more than one copy of a real-world entity or relation may exist. The main contribution is a method that can make use of large-scale pre-training on facts, which were collected from unstructured text, to improve predictions on structured data from a specific domain. The introduced method is most impactful on small datasets such as ReVerb20k, where a 6% absolute increase of mean reciprocal rank and 65% relative decrease of mean rank over the previously best method was achieved, despite not relying on large pre-trained models like Bert. To understand the obtained pre-trained models better, we then introduce a novel dataset for the analysis of pre-trained models for Open Knowledge Base Completion, calle",
    "link": "http://arxiv.org/abs/2401.15439",
    "context": "Title: Pre-training and Diagnosing Knowledge Base Completion Models. (arXiv:2401.15439v1 [cs.CL])\nAbstract: In this work, we introduce and analyze an approach to knowledge transfer from one collection of facts to another without the need for entity or relation matching. The method works for both canonicalized knowledge bases and uncanonicalized or open knowledge bases, i.e., knowledge bases where more than one copy of a real-world entity or relation may exist. The main contribution is a method that can make use of large-scale pre-training on facts, which were collected from unstructured text, to improve predictions on structured data from a specific domain. The introduced method is most impactful on small datasets such as ReVerb20k, where a 6% absolute increase of mean reciprocal rank and 65% relative decrease of mean rank over the previously best method was achieved, despite not relying on large pre-trained models like Bert. To understand the obtained pre-trained models better, we then introduce a novel dataset for the analysis of pre-trained models for Open Knowledge Base Completion, calle",
    "path": "papers/24/01/2401.15439.json",
    "total_tokens": 913,
    "translated_title": "Pre-training and Diagnosing Knowledge Base Completion Models. (arXiv:2401.15439v1 [cs.CL])",
    "translated_abstract": "在这项工作中，我们介绍并分析了一种无需实体或关系匹配的知识迁移方法，该方法可将一个事实集合的知识转移到另一个事实集合中。该方法适用于规范化知识库和非规范化或开放式知识库，即可能存在多个实体或关系的知识库。主要贡献是一种方法，可以利用从非结构化文本中收集的大规模预训练事实，改进对特定领域结构化数据的预测。引入的方法在小数据集（如ReVerb20k）上产生了重大影响，尽管没有依赖于大型预训练模型如Bert，但平均倒数排名增加了6个百分点，平均排名相对减少了65%。为了更好地理解获得的预训练模型，我们还介绍了一种用于分析开放式知识库完成预训练模型的新数据集，称为...",
    "tldr": "本论文介绍了一种无需实体或关系匹配的知识迁移方法，可以利用大规模预训练事实改进对特定领域结构化数据的预测。在小数据集上取得了显著的提高，尽管没有依赖于大型预训练模型。",
    "en_tdlr": "This paper introduces an approach to knowledge transfer without the need for entity or relation matching. It utilizes large-scale pre-training on facts to improve predictions on structured data in a specific domain, achieving significant improvements on small datasets without relying on large pre-trained models."
}