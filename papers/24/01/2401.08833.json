{
    "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective. (arXiv:2401.08833v1 [eess.AS])",
    "abstract": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without u",
    "link": "http://arxiv.org/abs/2401.08833",
    "context": "Title: Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective. (arXiv:2401.08833v1 [eess.AS])\nAbstract: Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without u",
    "path": "papers/24/01/2401.08833.json",
    "total_tokens": 871,
    "translated_title": "从互信息角度重新审视自监督学习的语音表征",
    "translated_abstract": "现有的自监督语音表征学习研究主要集中在开发新的训练方法并应用预训练模型于不同应用中。然而，这些模型的质量往往是通过不同下游任务的性能来衡量的。有关表征如何访问所需信息的研究较少。在本研究中，我们从信息论的角度对现有的自监督语音方法进行了更深入的研究。我们旨在使用互信息来开发度量标准，帮助解决模型设计和选择等实际问题。我们使用线性探测器估计目标信息与学习到的表征之间的互信息，展示了从语音表征中访问目标信息的另一种见解。此外，我们还探索了自监督方式评估表征的潜力，通过估计不同数据部分之间的互信息来实现。",
    "tldr": "本文从信息论的角度重新审视了自监督学习的语音表征方法，开发了使用互信息来度量表征质量的方法，并通过线性探测器评估了表征与目标信息之间的互信息，为模型设计和选择提供了新的见解。",
    "en_tdlr": "This paper re-evaluates self-supervised learning methods for speech representation from an information-theoretic perspective, developing metrics based on mutual information and using linear probes to estimate the accessibility of target information from speech representations, providing new insights for model design and selection."
}