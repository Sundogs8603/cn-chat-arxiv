{
    "title": "Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences. (arXiv:2401.01641v1 [cs.LG])",
    "abstract": "Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction. Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data. Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven't been adapted to multivariate time series of financial transactions. In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions. Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks. We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets. The embedding model significantly impro",
    "link": "http://arxiv.org/abs/2401.01641",
    "context": "Title: Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences. (arXiv:2401.01641v1 [cs.LG])\nAbstract: Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction. Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data. Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven't been adapted to multivariate time series of financial transactions. In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions. Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks. We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets. The embedding model significantly impro",
    "path": "papers/24/01/2401.01641.json",
    "total_tokens": 887,
    "translated_title": "朝着基于交易序列的基础采购模型: 预训练的生成自回归",
    "translated_abstract": "机器学习模型广泛应用于现代金融系统中，用于欺诈检测和留存预测等用例。大多数模型基于有标签数据的监督学习和手动设计的特征。目前，大规模自监督生成模型在自然语言处理和计算机视觉方面取得了巨大成功，但尚未将其应用于金融交易的多变量时间序列。本文提出了一种生成预训练方法，可用于获取金融交易的上下文嵌入。公共数据集上的基准测试表明，它在一系列下游任务中优于最先进的自监督方法。我们还使用包含51亿笔交易的180个发卡银行的语料库进行大规模预训练嵌入模型，并将其应用于保留数据集上的卡片欺诈检测问题。嵌入模型显著提高了性能。",
    "tldr": "本文提出了一种基于生成预训练方法的金融交易上下文嵌入模型，该模型在公共数据集上的测试中表现优于最先进的自监督方法，并在卡片欺诈检测问题上取得了显著的性能提升。",
    "en_tdlr": "This paper presents a generative pretraining method for financial transaction contextualized embeddings, which outperforms state-of-the-art self-supervised methods on public datasets and achieves significant performance improvement in card fraud detection problem."
}