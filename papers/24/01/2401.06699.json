{
    "title": "A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks. (arXiv:2401.06699v1 [cs.LG])",
    "abstract": "This work addresses weight optimization problem for fully-connected feed-forward neural networks. Unlike existing approaches that are based on back-propagation (BP) and chain rule gradient-based optimization (which implies iterative execution, potentially burdensome and time-consuming in some cases), the proposed approach offers the solution for weight optimization in closed-form by means of least squares (LS) methodology. In the case where the input-to-output mapping is injective, the new approach optimizes the weights in a back-propagating fashion in a single iteration by jointly optimizing a set of weights in each layer for each neuron. In the case where the input-to-output mapping is not injective (e.g., in classification problems), the proposed solution is easily adapted to obtain its final solution in a few iterations. An important advantage over the existing solutions is that these computations (for all neurons in a layer) are independent from each other; thus, they can be carri",
    "link": "http://arxiv.org/abs/2401.06699",
    "context": "Title: A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks. (arXiv:2401.06699v1 [cs.LG])\nAbstract: This work addresses weight optimization problem for fully-connected feed-forward neural networks. Unlike existing approaches that are based on back-propagation (BP) and chain rule gradient-based optimization (which implies iterative execution, potentially burdensome and time-consuming in some cases), the proposed approach offers the solution for weight optimization in closed-form by means of least squares (LS) methodology. In the case where the input-to-output mapping is injective, the new approach optimizes the weights in a back-propagating fashion in a single iteration by jointly optimizing a set of weights in each layer for each neuron. In the case where the input-to-output mapping is not injective (e.g., in classification problems), the proposed solution is easily adapted to obtain its final solution in a few iterations. An important advantage over the existing solutions is that these computations (for all neurons in a layer) are independent from each other; thus, they can be carri",
    "path": "papers/24/01/2401.06699.json",
    "total_tokens": 799,
    "translated_title": "全连接前馈神经网络权重优化的闭合解方案",
    "translated_abstract": "本文针对全连接前馈神经网络的权重优化问题进行了研究。与现有的基于反向传播和链式规则梯度优化的方法不同，该方法通过最小二乘法提供了闭合形式的权重优化解决方案。在输入到输出映射是可逆的情况下，新方法通过同时优化每个神经元层的一组权重，在单次迭代中以反向传播的方式优化权重。在输入到输出映射不可逆的情况下（例如分类问题），提出的解决方案可以轻松地在几次迭代中获得最终解。与现有解决方案相比，一个重要的优势是这些计算（对于每个神经元层的所有神经元）是独立的，因此它们可以同时进行。",
    "tldr": "本文提出了一种闭合解法，通过最小二乘法来优化全连接前馈神经网络的权重，具有非常高的效率和独立性。",
    "en_tdlr": "This paper proposes a closed-form solution using least squares methodology for weight optimization in fully-connected feed-forward neural networks, offering high efficiency and independence compared to existing approaches."
}