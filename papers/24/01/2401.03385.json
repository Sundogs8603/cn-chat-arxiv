{
    "title": "Grimoire is All You Need for Enhancing Large Language Models. (arXiv:2401.03385v2 [cs.CL] UPDATED)",
    "abstract": "In-context Learning (ICL) is one of the key methods for enhancing the performance of large language models on specific tasks by providing a set of few-shot examples. However, the ICL capability of different types of models shows significant variation due to factors such as model architecture, volume of learning data, and the size of parameters. Generally, the larger the model's parameter size and the more extensive the learning data, the stronger its ICL capability. In this paper, we propose a method SLEICL that involves learning from examples using strong language models and then summarizing and transferring these learned skills to weak language models for inference and application. This ensures the stability and effectiveness of ICL. Compared to directly enabling weak language models to learn from prompt examples, SLEICL reduces the difficulty of ICL for these models. Our experiments, conducted on up to eight datasets with five language models, demonstrate that weak language models a",
    "link": "http://arxiv.org/abs/2401.03385",
    "context": "Title: Grimoire is All You Need for Enhancing Large Language Models. (arXiv:2401.03385v2 [cs.CL] UPDATED)\nAbstract: In-context Learning (ICL) is one of the key methods for enhancing the performance of large language models on specific tasks by providing a set of few-shot examples. However, the ICL capability of different types of models shows significant variation due to factors such as model architecture, volume of learning data, and the size of parameters. Generally, the larger the model's parameter size and the more extensive the learning data, the stronger its ICL capability. In this paper, we propose a method SLEICL that involves learning from examples using strong language models and then summarizing and transferring these learned skills to weak language models for inference and application. This ensures the stability and effectiveness of ICL. Compared to directly enabling weak language models to learn from prompt examples, SLEICL reduces the difficulty of ICL for these models. Our experiments, conducted on up to eight datasets with five language models, demonstrate that weak language models a",
    "path": "papers/24/01/2401.03385.json",
    "total_tokens": 904,
    "translated_title": "Grimoire是增强大型语言模型所需的一切",
    "translated_abstract": "在上下文学习（ICL）是通过提供一组少样例来增强大型语言模型在特定任务上表现的关键方法之一。然而，不同类型的模型的ICL能力存在显著差异，由于模型架构、学习数据的量和参数的大小等因素。一般来说，模型的参数大小越大，学习数据越广泛，其ICL能力越强。在本文中，我们提出了一种方法SLEICL，它涉及使用强语言模型从示例中学习，然后将这些学到的技能总结和传递给弱语言模型进行推理和应用。这确保了ICL的稳定性和效果。与直接使弱语言模型从提示示例中学习相比，SLEICL降低了这些模型的ICL难度。我们在多达八个数据集上进行的实验证明，弱语言模型可以通过SLEICL方法获得与强语言模型相当的ICL能力。",
    "tldr": "Grimoire提出了一种名为SLEICL的方法，通过从示例中学习并将学到的技能传递给弱语言模型，增强了ICL能力。实验证明，这种方法可以使弱语言模型获得与强语言模型相当的ICL能力。"
}