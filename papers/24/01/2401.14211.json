{
    "title": "Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation. (arXiv:2401.14211v1 [cs.LG])",
    "abstract": "Federated Learning (FL) is a promising technique for the collaborative training of deep neural networks across multiple devices while preserving data privacy. Despite its potential benefits, FL is hindered by excessive communication costs due to repeated server-client communication during training. To address this challenge, model compression techniques, such as sparsification and weight clustering are applied, which often require modifying the underlying model aggregation schemes or involve cumbersome hyperparameter tuning, with the latter not only adjusts the model's compression rate but also limits model's potential for continuous improvement over growing data. In this paper, we propose FedCompress, a novel approach that combines dynamic weight clustering and server-side knowledge distillation to reduce communication costs while learning highly generalizable models. Through a comprehensive evaluation on diverse public datasets, we demonstrate the efficacy of our approach compared to",
    "link": "http://arxiv.org/abs/2401.14211",
    "context": "Title: Communication-Efficient Federated Learning through Adaptive Weight Clustering and Server-Side Distillation. (arXiv:2401.14211v1 [cs.LG])\nAbstract: Federated Learning (FL) is a promising technique for the collaborative training of deep neural networks across multiple devices while preserving data privacy. Despite its potential benefits, FL is hindered by excessive communication costs due to repeated server-client communication during training. To address this challenge, model compression techniques, such as sparsification and weight clustering are applied, which often require modifying the underlying model aggregation schemes or involve cumbersome hyperparameter tuning, with the latter not only adjusts the model's compression rate but also limits model's potential for continuous improvement over growing data. In this paper, we propose FedCompress, a novel approach that combines dynamic weight clustering and server-side knowledge distillation to reduce communication costs while learning highly generalizable models. Through a comprehensive evaluation on diverse public datasets, we demonstrate the efficacy of our approach compared to",
    "path": "papers/24/01/2401.14211.json",
    "total_tokens": 908,
    "translated_title": "通过自适应权重聚类和服务器端蒸馏实现高效通信的联邦学习",
    "translated_abstract": "联邦学习是一种有望在保护数据隐私的同时，通过多个设备共同训练深度神经网络的技术。然而，由于训练过程中重复的服务器-客户端通信导致了过多的通信成本，这给联邦学习带来了困难。为了解决这个挑战，我们应用了模型压缩技术，例如稀疏化和权重聚类，然而这些技术通常需要修改底层的模型聚合方案或者涉及繁琐的超参数调整，后者不仅调整了模型的压缩率，还限制了模型在不断增长的数据上的持续改进潜力。在本文中，我们提出了一种新颖的方法FedCompress，它结合了动态权重聚类和服务器端知识蒸馏，以降低通信成本同时学习高度可泛化的模型。通过对多个公共数据集进行全面评估，我们证明了我们的方法相比于其他方法的有效性。",
    "tldr": "本论文提出了一种名为FedCompress的新方法，通过动态权重聚类和服务器端知识蒸馏的结合，实现了高效通信的联邦学习。该方法在降低通信成本的同时，能够学习到高度可泛化的模型。",
    "en_tdlr": "This paper proposes a novel approach called FedCompress, which combines dynamic weight clustering and server-side knowledge distillation to achieve communication-efficient federated learning. The approach reduces communication costs and learns highly generalizable models."
}