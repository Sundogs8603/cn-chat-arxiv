{
    "title": "On Learning for Ambiguous Chance Constrained Problems",
    "abstract": "We study chance constrained optimization problems $\\min_x f(x)$ s.t. $P(\\left\\{ \\theta: g(x,\\theta)\\le 0 \\right\\})\\ge 1-\\epsilon$ where $\\epsilon\\in (0,1)$ is the violation probability, when the distribution $P$ is not known to the decision maker (DM). When the DM has access to a set of distributions $\\mathcal{U}$ such that $P$ is contained in $\\mathcal{U}$, then the problem is known as the ambiguous chance-constrained problem \\cite{erdougan2006ambiguous}. We study ambiguous chance-constrained problem for the case when $\\mathcal{U}$ is of the form $\\left\\{\\mu:\\frac{\\mu (y)}{\\nu(y)}\\leq C, \\forall y\\in\\Theta, \\mu(y)\\ge 0\\right\\}$, where $\\nu$ is a ``reference distribution.'' We show that in this case the original problem can be ``well-approximated'' by a sampled problem in which $N$ i.i.d. samples of $\\theta$ are drawn from $\\nu$, and the original constraint is replaced with $g(x,\\theta_i)\\le 0,~i=1,2,\\ldots,N$. We also derive the sample complexity associated with this approximation, i.",
    "link": "https://arxiv.org/abs/2401.00547",
    "context": "Title: On Learning for Ambiguous Chance Constrained Problems\nAbstract: We study chance constrained optimization problems $\\min_x f(x)$ s.t. $P(\\left\\{ \\theta: g(x,\\theta)\\le 0 \\right\\})\\ge 1-\\epsilon$ where $\\epsilon\\in (0,1)$ is the violation probability, when the distribution $P$ is not known to the decision maker (DM). When the DM has access to a set of distributions $\\mathcal{U}$ such that $P$ is contained in $\\mathcal{U}$, then the problem is known as the ambiguous chance-constrained problem \\cite{erdougan2006ambiguous}. We study ambiguous chance-constrained problem for the case when $\\mathcal{U}$ is of the form $\\left\\{\\mu:\\frac{\\mu (y)}{\\nu(y)}\\leq C, \\forall y\\in\\Theta, \\mu(y)\\ge 0\\right\\}$, where $\\nu$ is a ``reference distribution.'' We show that in this case the original problem can be ``well-approximated'' by a sampled problem in which $N$ i.i.d. samples of $\\theta$ are drawn from $\\nu$, and the original constraint is replaced with $g(x,\\theta_i)\\le 0,~i=1,2,\\ldots,N$. We also derive the sample complexity associated with this approximation, i.",
    "path": "papers/24/01/2401.00547.json",
    "total_tokens": 962,
    "translated_title": "关于学习模糊机会约束问题",
    "translated_abstract": "我们研究了当决策者(DM)不知道分布P时，满足机会约束的优化问题$\\min_x f(x)$ s.t. $P(\\left\\{ \\theta: g(x,\\theta)\\le 0 \\right\\})\\ge 1-\\epsilon$。当DM可以访问一组分布$\\mathcal{U}$，其中包含了分布P时，问题被称为模糊机会约束问题。我们研究了当$\\mathcal{U}$以$\\left\\{\\mu:\\frac{\\mu (y)}{\\nu(y)}\\leq C, \\forall y\\in\\Theta, \\mu(y)\\ge 0\\right\\}$的形式存在时的模糊机会约束问题，其中$\\nu$是参考分布。我们证明在这种情况下，原问题可以通过从$\\nu$中抽取N个独立同分布的样本$\\theta_i$，并将原始约束替换为$g(x,\\theta_i)\\le 0,~i=1,2,\\ldots,N$的采样问题来“很好地逼近”。我们还推导出与这种逼近相关的样本复杂度。",
    "tldr": "本文研究了模糊机会约束优化问题中当决策者不知道分布P时的情况，通过使用一组分布来近似原问题，并推导出了相应的样本复杂度。"
}