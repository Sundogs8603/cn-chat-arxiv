{
    "title": "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models. (arXiv:2401.12242v1 [cs.CR])",
    "abstract": "Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step int",
    "link": "http://arxiv.org/abs/2401.12242",
    "context": "Title: BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models. (arXiv:2401.12242v1 [cs.CR])\nAbstract: Large language models (LLMs) are shown to benefit from chain-of-thought (COT) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, COT prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial LLMs that typically operate via API access. In this paper, we propose BadChain, the first backdoor attack against LLMs employing COT prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. BadChain leverages the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning step int",
    "path": "papers/24/01/2401.12242.json",
    "total_tokens": 825,
    "translated_title": "BadChain: 大型语言模型中的后门思维链提示",
    "translated_abstract": "大型语言模型(LLM)在处理需要系统推理过程的任务时，表现出从思维链提示(COT)中受益。然而，COT提示也在推理中出现新的后门攻击形式，即在特定的后门触发条件下，模型将输出意外的恶意内容。传统的发动后门攻击的方法包括污染训练数据集或在部署过程中直接操纵模型参数。然而，这些方法对于通常通过API访问的商业LLM来说并不实用。本文提出了BadChain，它是针对采用COT提示的LLM的第一个后门攻击，不需要访问训练数据集或模型参数，并且计算开销较低。BadChain利用LLM的内在推理能力，在推理过程中插入一个后门推理步骤。",
    "tldr": "BadChain是对大型语言模型(LLM)采用从思维链提示(COT)的一种新的后门攻击方法。它不需要访问训练数据集或模型参数，并且具有较低的计算开销。",
    "en_tdlr": "BadChain is a novel backdoor attack method against Large Language Models (LLMs) using Chain-of-Thought (COT) prompting. It does not require access to the training dataset or model parameters and imposes low computational overhead."
}