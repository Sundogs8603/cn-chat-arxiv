{
    "title": "AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with Thirteen Modalities",
    "abstract": "arXiv:2401.00546v2 Announce Type: replace  Abstract: For a long time, due to the high heterogeneity in structure and semantics among various spatiotemporal modal data, the joint interpretation of multimodal spatiotemporal data has been an extremely challenging problem. The primary challenge resides in striking a trade-off between the cohesion and autonomy of diverse modalities, and this trade-off exhibits a progressively nonlinear nature as the number of modalities expands. We introduce the Language as Reference Framework (LaRF), a fundamental principle for constructing a multimodal unified model, aiming to strike a trade-off between the cohesion and autonomy among different modalities. We propose a multimodal spatiotemporal general artificial intelligence model, called AllSpark. Our model integrates thirteen different modalities into a unified framework, including 1D (text, code), 2D (RGB, infrared, SAR, multispectral, hyperspectral, tables, graphs, trajectory, oblique photography), a",
    "link": "https://arxiv.org/abs/2401.00546",
    "context": "Title: AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with Thirteen Modalities\nAbstract: arXiv:2401.00546v2 Announce Type: replace  Abstract: For a long time, due to the high heterogeneity in structure and semantics among various spatiotemporal modal data, the joint interpretation of multimodal spatiotemporal data has been an extremely challenging problem. The primary challenge resides in striking a trade-off between the cohesion and autonomy of diverse modalities, and this trade-off exhibits a progressively nonlinear nature as the number of modalities expands. We introduce the Language as Reference Framework (LaRF), a fundamental principle for constructing a multimodal unified model, aiming to strike a trade-off between the cohesion and autonomy among different modalities. We propose a multimodal spatiotemporal general artificial intelligence model, called AllSpark. Our model integrates thirteen different modalities into a unified framework, including 1D (text, code), 2D (RGB, infrared, SAR, multispectral, hyperspectral, tables, graphs, trajectory, oblique photography), a",
    "path": "papers/24/01/2401.00546.json",
    "total_tokens": 869,
    "translated_title": "AllSpark: 一个具有十三种模态的多模态时空智能模型",
    "translated_abstract": "长期以来，由于各种时空模态数据之间结构和语义的高度异质性，多模态时空数据的联合解释一直是一个极具挑战性的问题。主要挑战在于在不同模态之间的凝聚力和自治性之间取得平衡，而随着模态数量的增加，这种平衡表现出逐渐非线性的特性。我们引入了语言作为参考框架（LaRF），这是构建多模态统一模型的基本原则，旨在在不同模态之间取得凝聚力和自治性之间的平衡。我们提出了一个名为AllSpark的多模态时空智能通用人工智能模型。我们的模型将十三种不同的模态集成到一个统一框架中，包括1D（文本，代码），2D（RGB，红外线，SAR，多光谱，高光谱，表格，图表，轨迹，斜角摄影）。",
    "tldr": "提出了一个名为AllSpark的多模态时空智能通用人工智能模型，集成了十三种不同的模态，旨在解决多模态时空数据联合解释的挑战。",
    "en_tdlr": "Proposed a multimodal spatio-temporal general artificial intelligence model called AllSpark, integrating thirteen different modalities to address the challenge of joint interpretation of multimodal spatio-temporal data."
}