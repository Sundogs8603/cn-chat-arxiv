{
    "title": "Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models",
    "abstract": "We present Gyan AI Paramanu (\"atom\"), a family of novel language models for Indian languages. It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU. The models are very efficient, small, fast, and powerful. We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages. In order to avoid the \"curse of multi-linguality\" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script. We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics fo",
    "link": "https://arxiv.org/abs/2401.18034",
    "context": "Title: Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models\nAbstract: We present Gyan AI Paramanu (\"atom\"), a family of novel language models for Indian languages. It is a collection of auto-regressive monolingual, bilingual, and multilingual Indic language models pretrained from scratch on a single GPU for 10 Indian languages (Assamese, Bangla, Hindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts (Bangla, Devanagari, Odia, Tamil, Telugu) of varying sizes ranging from 13.29M to 367.5M.The models are pretrained with a context size of 1024 on a single GPU. The models are very efficient, small, fast, and powerful. We have also developed an efficient most advanced Indic tokenizer that can even tokenize unseen languages. In order to avoid the \"curse of multi-linguality\" in our multilingual mParamanu model, we pretrained on comparable corpora by typological grouping using the same script. We performed human evaluation of our pretrained models for open end text generation on grammar, coherence, creativity, and factuality metrics fo",
    "path": "papers/24/01/2401.18034.json",
    "total_tokens": 1017,
    "translated_title": "Paramanu: 一种高效的印度生成式基础语言模型系列",
    "translated_abstract": "我们介绍了Gyan AI Paramanu（“原子”），一种适用于印度语言的新型语言模型系列。它是一个在单个GPU上从头开始预训练的包含单语、双语和多语印度语言模型的集合，涵盖了10种印度语言（阿萨姆语、孟加拉语、印地语、康坎尼语、迈蒂利语、马拉地语、奥迪亚语、梵语、泰米尔语和泰卢固语）以及5种不同大小的字母表（孟加拉语、天城体、奥迪亚语、泰米尔语和泰卢固语）。这些模型以1024的上下文大小在单个GPU上预训练，非常高效、小巧、快速且强大。我们还开发了一种高效的先进的印度语分词器，甚至可以标记未知语言。为了避免我们的多语言mParamanu模型中的“多语言诅咒”，我们使用相同的字母表按语言类型进行了可比较语料库的预训练。我们对我们预训练模型进行了人工评估，评估指标包括语法、连贯性、创造性和事实准确性。",
    "tldr": "Paramanu是一种高效的印度生成式基础语言模型系列，包含多种印度语言模型，并且在单个GPU上进行了从头预训练。它还包括一个先进的印度分词器以及避免多语言诅咒的预训练方法。这些模型在人工评估中展现出良好的语法、连贯性、创造性和事实准确性。"
}