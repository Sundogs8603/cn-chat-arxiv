{
    "title": "Large Language Models are Efficient Learners of Noise-Robust Speech Recognition. (arXiv:2401.10446v1 [cs.CL])",
    "abstract": "Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with HyPoradise dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech, whic",
    "link": "http://arxiv.org/abs/2401.10446",
    "context": "Title: Large Language Models are Efficient Learners of Noise-Robust Speech Recognition. (arXiv:2401.10446v1 [cs.CL])\nAbstract: Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with HyPoradise dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech, whic",
    "path": "papers/24/01/2401.10446.json",
    "total_tokens": 955,
    "translated_title": "大型语言模型是噪声鲁棒语音识别的高效学习者",
    "translated_abstract": "最近对大型语言模型（LLMs）的进展促进了自动语音识别（ASR）的生成式错误纠正（GER），利用LLMs的丰富语言知识和强大的推理能力来改善识别结果。最新的研究提出了一个GER基准测试，并使用HyPoradise数据集通过高效的LLM微调从ASR N-best假设到地面真实转录的映射，这显示出极大的效果，但在噪声鲁棒ASR方面缺乏具体性。在这项工作中，我们将基准测试扩展到噪声条件下，并研究是否可以教会LLMs像噪声鲁棒ASR一样执行去噪。其中一个解决方案是将噪声信息作为条件器引入LLM中。然而，直接从音频编码器中引入噪声嵌入可能会对LLM微调造成损害，因为存在跨模态差距。因此，我们提出了从N-best列表中提取语言空间噪声嵌入来表示源语音的噪声条件的方法。",
    "tldr": "本文通过引入噪声信息作为条件器，并从N-best列表中提取语言空间噪声嵌入，教会了大型语言模型（LLMs）进行噪声去除，从而实现了噪声鲁棒语音识别的生成式错误纠正（GER）。",
    "en_tdlr": "This paper teaches large language models (LLMs) to perform noise removal for generative error correction (GER) in noise-robust speech recognition, by introducing noise information as a conditioner and extracting language-space noise embeddings from the N-best list."
}