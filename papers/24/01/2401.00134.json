{
    "title": "Unicron: Economizing Self-Healing LLM Training at Scale. (arXiv:2401.00134v1 [cs.DC] CROSS LISTED)",
    "abstract": "Training large-scale language models is increasingly critical in various domains, but it is hindered by frequent failures, leading to significant time and economic costs. Current failure recovery methods in cloud-based settings inadequately address the diverse and complex scenarios that arise, focusing narrowly on erasing downtime for individual tasks without considering the overall cost impact on a cluster. We introduce Unicron, a workload manager designed for efficient self-healing in large-scale language model training. Unicron optimizes the training process by minimizing failure-related costs across multiple concurrent tasks within a cluster. Its key features include in-band error detection for real-time error identification without extra overhead, a dynamic cost-aware plan generation mechanism for optimal reconfiguration, and an efficient transition strategy to reduce downtime during state changes. Deployed on a 128-GPU distributed cluster, Unicron demonstrates up to a 1.9x improv",
    "link": "http://arxiv.org/abs/2401.00134",
    "context": "Title: Unicron: Economizing Self-Healing LLM Training at Scale. (arXiv:2401.00134v1 [cs.DC] CROSS LISTED)\nAbstract: Training large-scale language models is increasingly critical in various domains, but it is hindered by frequent failures, leading to significant time and economic costs. Current failure recovery methods in cloud-based settings inadequately address the diverse and complex scenarios that arise, focusing narrowly on erasing downtime for individual tasks without considering the overall cost impact on a cluster. We introduce Unicron, a workload manager designed for efficient self-healing in large-scale language model training. Unicron optimizes the training process by minimizing failure-related costs across multiple concurrent tasks within a cluster. Its key features include in-band error detection for real-time error identification without extra overhead, a dynamic cost-aware plan generation mechanism for optimal reconfiguration, and an efficient transition strategy to reduce downtime during state changes. Deployed on a 128-GPU distributed cluster, Unicron demonstrates up to a 1.9x improv",
    "path": "papers/24/01/2401.00134.json",
    "total_tokens": 930,
    "translated_title": "Unicron: 在规模化自愈LLM训练中的经济节约",
    "translated_abstract": "在各个领域中，训练大规模语言模型变得越来越关键，但由于频繁的失败，导致了显著的时间和经济成本。云端环境中当前的故障恢复方法无法充分解决各种复杂场景的问题，仅仅局限于减少个别任务的停机时间而忽视了对整个集群成本影响的考虑。我们引入了 Unicron，这是一个专为大规模语言模型训练中的有效自愈而设计的工作负载管理器。Unicron通过在集群内的多个并发任务中最小化与故障相关的成本来优化训练过程。其主要特点包括带内错误检测，实时识别错误而无需额外开销；动态成本感知的计划生成机制，以实现最优的重新配置；以及高效的转换策略，以减少状态变化期间的停机时间。在一个128-GPU的分布式集群上部署，Unicron展现了多达1.9倍的改进。",
    "tldr": "Unicron是一个针对大规模语言模型训练中的自愈设计的工作负载管理器，通过最小化故障相关成本来优化训练过程，拥有带内错误检测、动态成本感知的计划生成机制和高效的转换策略。"
}