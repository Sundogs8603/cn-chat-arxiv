{
    "title": "UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models. (arXiv:2401.14379v1 [cs.CV])",
    "abstract": "In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes. These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction. This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design. Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions. Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation. This was evidenced by superior Inter",
    "link": "http://arxiv.org/abs/2401.14379",
    "context": "Title: UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models. (arXiv:2401.14379v1 [cs.CV])\nAbstract: In contemporary design practices, the integration of computer vision and generative artificial intelligence (genAI) represents a transformative shift towards more interactive and inclusive processes. These technologies offer new dimensions of image analysis and generation, which are particularly relevant in the context of urban landscape reconstruction. This paper presents a novel workflow encapsulated within a prototype application, designed to leverage the synergies between advanced image segmentation and diffusion models for a comprehensive approach to urban design. Our methodology encompasses the OneFormer model for detailed image segmentation and the Stable Diffusion XL (SDXL) diffusion model, implemented through ControlNet, for generating images from textual descriptions. Validation results indicated a high degree of performance by the prototype application, showcasing significant accuracy in both object detection and text-to-image generation. This was evidenced by superior Inter",
    "path": "papers/24/01/2401.14379.json",
    "total_tokens": 847,
    "translated_title": "UrbanGenAI: 使用全景分割和扩散模型重构城市景观",
    "translated_abstract": "在当代设计实践中，计算机视觉和生成式人工智能(genAI)的整合代表了一种转变，更加交互和包容的过程。这些技术提供了图像分析和生成的新维度，在城市景观重建的背景下尤其重要。本文提出了一种新的工作流程，以原型应用程序的形式展示，旨在利用先进的图像分割和扩散模型之间的协同作用，实现对城市设计的全面方法。我们的方法包括详细的图像分割模型OneFormer和通过ControlNet实现的稳定扩散XL（SDXL）扩散模型，用于从文本描述中生成图像。验证结果表明，原型应用程序具有很高的性能，展示了在物体检测和文本到图像生成方面的显著准确性。",
    "tldr": "本文介绍了一种使用全景分割和扩散模型重构城市景观的新方法，该方法通过整合先进的图像分割和扩散模型，实现了对城市设计的全面处理，并在物体检测和文本到图像生成方面取得了显著准确性。",
    "en_tdlr": "This paper presents a new approach to reconstruct urban landscapes using panoptic segmentation and diffusion models, which achieves a comprehensive treatment of urban design by integrating advanced image segmentation and diffusion models, and demonstrates significant accuracy in object detection and text-to-image generation."
}