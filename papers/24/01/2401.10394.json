{
    "title": "Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels. (arXiv:2401.10394v1 [cs.LG])",
    "abstract": "Few-shot node classification poses a significant challenge for Graph Neural Networks (GNNs) due to insufficient supervision and potential distribution shifts between labeled and unlabeled nodes. Self-training has emerged as a widely popular framework to leverage the abundance of unlabeled data, which expands the training set by assigning pseudo-labels to selected unlabeled nodes. Efforts have been made to develop various selection strategies based on confidence, information gain, etc. However, none of these methods takes into account the distribution shift between the training and testing node sets. The pseudo-labeling step may amplify this shift and even introduce new ones, hindering the effectiveness of self-training. Therefore, in this work, we explore the potential of explicitly bridging the distribution shift between the expanded training set and test set during self-training. To this end, we propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework to identif",
    "link": "http://arxiv.org/abs/2401.10394",
    "context": "Title: Distribution Consistency based Self-Training for Graph Neural Networks with Sparse Labels. (arXiv:2401.10394v1 [cs.LG])\nAbstract: Few-shot node classification poses a significant challenge for Graph Neural Networks (GNNs) due to insufficient supervision and potential distribution shifts between labeled and unlabeled nodes. Self-training has emerged as a widely popular framework to leverage the abundance of unlabeled data, which expands the training set by assigning pseudo-labels to selected unlabeled nodes. Efforts have been made to develop various selection strategies based on confidence, information gain, etc. However, none of these methods takes into account the distribution shift between the training and testing node sets. The pseudo-labeling step may amplify this shift and even introduce new ones, hindering the effectiveness of self-training. Therefore, in this work, we explore the potential of explicitly bridging the distribution shift between the expanded training set and test set during self-training. To this end, we propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework to identif",
    "path": "papers/24/01/2401.10394.json",
    "total_tokens": 899,
    "translated_title": "基于分布一致性的稀疏标签图神经网络自训练",
    "translated_abstract": "少样本节点分类对于图神经网络(GNNs)来说是一个重大挑战，因为标记和未标记节点之间的监督不足和潜在的分布转移问题。自训练是一种广泛流行的框架，利用大量未标记数据扩展训练集，通过给选定的未标记节点分配伪标签。已经开展了许多基于置信度、信息增益等选择策略的努力。然而，这些方法中没有一个考虑到训练和测试节点集之间的分布转移。伪标签步骤可能增加这种转移甚至引入新的转移，从而阻碍自训练的有效性。因此，在这项研究中，我们探索了在自训练过程中明确地消除扩展训练集和测试集之间的分布转移的潜力。为此，我们提出了一种新颖的分布一致图自训练(DC-GST)框架，用于辨别",
    "tldr": "本文提出了一种基于分布一致性的自训练方法，用于解决图神经网络中少样本节点分类的挑战，通过消除训练集和测试集之间的分布转移，提高自训练的有效性。",
    "en_tdlr": "This paper proposes a self-training method based on distribution consistency to address the challenge of few-shot node classification in Graph Neural Networks (GNNs), improving the effectiveness of self-training by eliminating distribution shifts between the training and testing sets."
}