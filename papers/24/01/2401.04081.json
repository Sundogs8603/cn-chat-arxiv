{
    "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts",
    "abstract": "arXiv:2401.04081v2 Announce Type: replace-cross  Abstract: State Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-based Large Language Models, including recent state-of-the-art open models. We propose that to unlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model that achieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba and baseline Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in $2.35\\times$ fewer training steps while preserving the inference performance gains of Mamba against Transformer.",
    "link": "https://arxiv.org/abs/2401.04081",
    "context": "Title: MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts\nAbstract: arXiv:2401.04081v2 Announce Type: replace-cross  Abstract: State Space Models (SSMs) have become serious contenders in the field of sequential modeling, challenging the dominance of Transformers. At the same time, Mixture of Experts (MoE) has significantly improved Transformer-based Large Language Models, including recent state-of-the-art open models. We propose that to unlock the potential of SSMs for scaling, they should be combined with MoE. We showcase this on Mamba, a recent SSM-based model that achieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba and baseline Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba in $2.35\\times$ fewer training steps while preserving the inference performance gains of Mamba against Transformer.",
    "path": "papers/24/01/2401.04081.json",
    "total_tokens": 796,
    "translated_title": "MoE-Mamba: 混合专家模型的高效选择性状态空间模型",
    "translated_abstract": "状态空间模型（SSMs）已经成为顺序建模领域的严肃竞争者，挑战了Transformer的主导地位。与此同时，混合专家（MoE）显著改进了基于Transformer的大型语言模型，包括最近的最先进开放模型。我们提出要发掘SSMs在扩展方面的潜力，它们应该与MoE相结合。我们在Mamba上展示了这一点，这是一个最近基于SSM的模型，取得了显著的性能。我们的模型MoE-Mamba在性能方面表现优异，优于Mamba和基准Transformer-MoE。特别地，MoE-Mamba在更少的训练步骤中达到与Mamba相同的性能，同时保持Mamba相对于Transformer的推理性能增益。",
    "tldr": "结合混合专家模型的MoE-Mamba在性能上优于Mamba和基准Transformer-MoE，达到了与Mamba相同性能的同时，训练步骤减少了2.35倍。"
}