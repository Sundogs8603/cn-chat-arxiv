{
    "title": "FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients. (arXiv:2401.02433v1 [cs.CV])",
    "abstract": "With the rapid development of imaging sensor technology in the field of remote sensing, multi-modal remote sensing data fusion has emerged as a crucial research direction for land cover classification tasks. While diffusion models have made great progress in generative models and image classification tasks, existing models primarily focus on single-modality and single-client control, that is, the diffusion process is driven by a single modal in a single computing node. To facilitate the secure fusion of heterogeneous data from clients, it is necessary to enable distributed multi-modal control, such as merging the hyperspectral data of organization A and the LiDAR data of organization B privately on each base station client. In this study, we propose a multi-modal collaborative diffusion federated learning framework called FedDiff. Our framework establishes a dual-branch diffusion model feature extraction setup, where the two modal data are inputted into separate branches of the encoder",
    "link": "http://arxiv.org/abs/2401.02433",
    "context": "Title: FedDiff: Diffusion Model Driven Federated Learning for Multi-Modal and Multi-Clients. (arXiv:2401.02433v1 [cs.CV])\nAbstract: With the rapid development of imaging sensor technology in the field of remote sensing, multi-modal remote sensing data fusion has emerged as a crucial research direction for land cover classification tasks. While diffusion models have made great progress in generative models and image classification tasks, existing models primarily focus on single-modality and single-client control, that is, the diffusion process is driven by a single modal in a single computing node. To facilitate the secure fusion of heterogeneous data from clients, it is necessary to enable distributed multi-modal control, such as merging the hyperspectral data of organization A and the LiDAR data of organization B privately on each base station client. In this study, we propose a multi-modal collaborative diffusion federated learning framework called FedDiff. Our framework establishes a dual-branch diffusion model feature extraction setup, where the two modal data are inputted into separate branches of the encoder",
    "path": "papers/24/01/2401.02433.json",
    "total_tokens": 862,
    "translated_title": "FedDiff: 基于扩散模型的多模态和多客户联邦学习",
    "translated_abstract": "随着遥感领域成像传感器技术的快速发展，多模态遥感数据融合已成为土地覆盖分类任务中的一个关键研究方向。然而，现有的扩散模型主要集中在单模态和单客户控制上，即扩散过程由单个模态在单个计算节点驱动。为了实现来自客户的异构数据的安全融合，需要实现分布式的多模态控制，例如在每个基站客户端上，将组织A的高光谱数据和组织B的激光雷达数据进行私密合并。本研究提出了一种名为FedDiff的多模态协作扩散联邦学习框架。我们的框架建立了一个双分支扩散模型特征提取设置，其中两种模态数据被输入到编码器的分支中。",
    "tldr": "FedDiff是一个多模态协作扩散联邦学习框架，旨在实现来自不同客户的异构数据的安全融合，通过建立双分支扩散模型特征提取设置来驱动联邦学习过程。",
    "en_tdlr": "FedDiff is a multi-modal collaborative diffusion federated learning framework that aims to securely fuse heterogeneous data from different clients by establishing a dual-branch diffusion model feature extraction setup to drive the federated learning process."
}