{
    "title": "Transformer Neural Autoregressive Flows. (arXiv:2401.01855v1 [cs.LG])",
    "abstract": "Density estimation, a central problem in machine learning, can be performed using Normalizing Flows (NFs). NFs comprise a sequence of invertible transformations, that turn a complex target distribution into a simple one, by exploiting the change of variables theorem. Neural Autoregressive Flows (NAFs) and Block Neural Autoregressive Flows (B-NAFs) are arguably the most perfomant members of the NF family. However, they suffer scalability issues and training instability due to the constraints imposed on the network structure. In this paper, we propose a novel solution to these challenges by exploiting transformers to define a new class of neural flows called Transformer Neural Autoregressive Flows (T-NAFs). T-NAFs treat each dimension of a random variable as a separate input token, using attention masking to enforce an autoregressive constraint. We take an amortization-inspired approach where the transformer outputs the parameters of an invertible transformation. The experimental results",
    "link": "http://arxiv.org/abs/2401.01855",
    "context": "Title: Transformer Neural Autoregressive Flows. (arXiv:2401.01855v1 [cs.LG])\nAbstract: Density estimation, a central problem in machine learning, can be performed using Normalizing Flows (NFs). NFs comprise a sequence of invertible transformations, that turn a complex target distribution into a simple one, by exploiting the change of variables theorem. Neural Autoregressive Flows (NAFs) and Block Neural Autoregressive Flows (B-NAFs) are arguably the most perfomant members of the NF family. However, they suffer scalability issues and training instability due to the constraints imposed on the network structure. In this paper, we propose a novel solution to these challenges by exploiting transformers to define a new class of neural flows called Transformer Neural Autoregressive Flows (T-NAFs). T-NAFs treat each dimension of a random variable as a separate input token, using attention masking to enforce an autoregressive constraint. We take an amortization-inspired approach where the transformer outputs the parameters of an invertible transformation. The experimental results",
    "path": "papers/24/01/2401.01855.json",
    "total_tokens": 877,
    "translated_title": "变压器神经自回归流",
    "translated_abstract": "密度估计是机器学习中的一个核心问题，可以使用归一化流（NFs）来执行。NFs由一系列可逆变换组成，利用变量变换定理将复杂的目标分布转化为简单的分布。神经自回归流（NAFs）和块神经自回归流（B-NAFs）可以说是NF家族中性能最好的成员。然而，由于对网络结构施加的约束，它们存在可扩展性问题和训练不稳定性。在本文中，我们提出了一种新的解决方案，通过利用变压器来定义一种新型的神经流，称为变压器神经自回归流（T-NAFs）。T-NAFs将随机变量的每个维度视为单独的输入令牌，利用注意力掩码来强制执行自回归约束。我们采用一种类似于分期偿还的方法，变压器输出可逆变换的参数。实验证明.",
    "tldr": "本文提出了一种称为变压器神经自回归流（T-NAFs）的新技术，通过利用变压器来解决神经自回归流的可扩展性和训练稳定性问题，并取得了实验验证。"
}