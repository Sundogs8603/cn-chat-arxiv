{
    "title": "Superiority of Multi-Head Attention in In-Context Linear Regression",
    "abstract": "We present a theoretical analysis of the performance of transformer with softmax attention in in-context learning with linear regression tasks. While the existing literature predominantly focuses on the convergence of transformers with single-/multi-head attention, our research centers on comparing their performance. We conduct an exact theoretical analysis to demonstrate that multi-head attention with a substantial embedding dimension performs better than single-head attention. When the number of in-context examples D increases, the prediction loss using single-/multi-head attention is in O(1/D), and the one for multi-head attention has a smaller multiplicative constant. In addition to the simplest data distribution setting, we consider more scenarios, e.g., noisy labels, local examples, correlated features, and prior knowledge. We observe that, in general, multi-head attention is preferred over single-head attention. Our results verify the effectiveness of the design of multi-head at",
    "link": "https://arxiv.org/abs/2401.17426",
    "context": "Title: Superiority of Multi-Head Attention in In-Context Linear Regression\nAbstract: We present a theoretical analysis of the performance of transformer with softmax attention in in-context learning with linear regression tasks. While the existing literature predominantly focuses on the convergence of transformers with single-/multi-head attention, our research centers on comparing their performance. We conduct an exact theoretical analysis to demonstrate that multi-head attention with a substantial embedding dimension performs better than single-head attention. When the number of in-context examples D increases, the prediction loss using single-/multi-head attention is in O(1/D), and the one for multi-head attention has a smaller multiplicative constant. In addition to the simplest data distribution setting, we consider more scenarios, e.g., noisy labels, local examples, correlated features, and prior knowledge. We observe that, in general, multi-head attention is preferred over single-head attention. Our results verify the effectiveness of the design of multi-head at",
    "path": "papers/24/01/2401.17426.json",
    "total_tokens": 816,
    "translated_title": "多头注意力在上下文线性回归中的优势",
    "translated_abstract": "我们通过理论分析在线性回归任务的上下文学习中，使用softmax注意力的transformer的性能。与现有文献主要关注单头/多头注意力的收敛性不同，我们的研究着重比较它们的性能。我们进行了精确的理论分析，证明了具有较大嵌入维度的多头注意力比单头注意力表现更好。当上下文示例数量D增加时，使用单头/多头注意力的预测损失为O(1/D)，而多头注意力的乘法常数较小。除了最简单的数据分布设置，我们考虑了更多情景，例如噪声标签，局部示例，相关特征和先验知识。我们观察到，总的来说，多头注意力优于单头注意力。我们的结果验证了多头注意力设计的有效性。",
    "tldr": "多头注意力在上下文线性回归任务中表现出优于单头注意力的性能，通过理论分析证明了多头注意力在大嵌入维度情况下有更小的预测损失，并且在各种数据分布设置下都显示出优势。",
    "en_tdlr": "Multi-head attention outperforms single-head attention in in-context linear regression tasks, as demonstrated by theoretical analysis showing smaller prediction loss in multi-head attention with larger embedding dimensions, and this superiority holds across various data distribution settings."
}