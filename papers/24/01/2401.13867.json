{
    "title": "Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation. (arXiv:2401.13867v1 [cs.CL])",
    "abstract": "Large language models like GPT-3.5-turbo and GPT-4 hold promise for healthcare professionals, but they may inadvertently inherit biases during their training, potentially affecting their utility in medical applications. Despite few attempts in the past, the precise impact and extent of these biases remain uncertain. Through both qualitative and quantitative analyses, we find that these models tend to project higher costs and longer hospitalizations for White populations and exhibit optimistic views in challenging medical scenarios with much higher survival rates. These biases, which mirror real-world healthcare disparities, are evident in the generation of patient backgrounds, the association of specific diseases with certain races, and disparities in treatment recommendations, etc. Our findings underscore the critical need for future research to address and mitigate biases in language models, especially in critical healthcare applications, to ensure fair and accurate outcomes for all ",
    "link": "http://arxiv.org/abs/2401.13867",
    "context": "Title: Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation. (arXiv:2401.13867v1 [cs.CL])\nAbstract: Large language models like GPT-3.5-turbo and GPT-4 hold promise for healthcare professionals, but they may inadvertently inherit biases during their training, potentially affecting their utility in medical applications. Despite few attempts in the past, the precise impact and extent of these biases remain uncertain. Through both qualitative and quantitative analyses, we find that these models tend to project higher costs and longer hospitalizations for White populations and exhibit optimistic views in challenging medical scenarios with much higher survival rates. These biases, which mirror real-world healthcare disparities, are evident in the generation of patient backgrounds, the association of specific diseases with certain races, and disparities in treatment recommendations, etc. Our findings underscore the critical need for future research to address and mitigate biases in language models, especially in critical healthcare applications, to ensure fair and accurate outcomes for all ",
    "path": "papers/24/01/2401.13867.json",
    "total_tokens": 1029,
    "translated_title": "揭示和量化大型语言模型在医学报告生成中的种族偏见",
    "translated_abstract": "GPT-3.5-turbo和GPT-4等大型语言模型在医疗专业领域具有潜力，但它们在训练过程中可能会无意中继承偏见，可能影响其在医学应用中的效用。尽管过去有一些尝试，但这些偏见的确切影响和程度仍不确定。通过定性和定量分析，我们发现这些模型倾向于为白人种族投射更高的医疗费用和较长的住院时间，并在面临挑战性医学情况时呈现乐观的生存率。这些偏见与现实中的医疗差异相一致，可在生成患者背景、将特定疾病与某些种族相关联以及治疗推荐的差异等方面体现出来。我们的发现强调了将来需要开展研究来解决和减轻语言模型中的偏见问题，特别是在关键的医疗应用中，以确保公正和准确的结果。",
    "tldr": "该论文揭示了大型语言模型在医学报告生成中存在的种族偏见，并通过定性和定量分析证明了这些偏见的影响。这些偏见主要表现为对白人种族的高护理成本和住院时间预测，以及在临床医学中面临挑战的情况下呈现过度乐观的生存率。意识到这些偏见的存在和影响，对于在关键的医疗应用中确保公平和准确的结果至关重要。"
}