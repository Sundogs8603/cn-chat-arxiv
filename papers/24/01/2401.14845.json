{
    "title": "Adaptive Point Transformer. (arXiv:2401.14845v1 [cs.CV])",
    "abstract": "The recent surge in 3D data acquisition has spurred the development of geometric deep learning models for point cloud processing, boosted by the remarkable success of transformers in natural language processing. While point cloud transformers (PTs) have achieved impressive results recently, their quadratic scaling with respect to the point cloud size poses a significant scalability challenge for real-world applications. To address this issue, we propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model augmented by an adaptive token selection mechanism. AdaPT dynamically reduces the number of tokens during inference, enabling efficient processing of large point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust the computational cost of the model at inference time without the need for retraining or fine-tuning separate models. Our extensive experimental evaluation on point cloud classification tasks demonstrates that AdaPT significantly reduces comp",
    "link": "http://arxiv.org/abs/2401.14845",
    "context": "Title: Adaptive Point Transformer. (arXiv:2401.14845v1 [cs.CV])\nAbstract: The recent surge in 3D data acquisition has spurred the development of geometric deep learning models for point cloud processing, boosted by the remarkable success of transformers in natural language processing. While point cloud transformers (PTs) have achieved impressive results recently, their quadratic scaling with respect to the point cloud size poses a significant scalability challenge for real-world applications. To address this issue, we propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model augmented by an adaptive token selection mechanism. AdaPT dynamically reduces the number of tokens during inference, enabling efficient processing of large point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust the computational cost of the model at inference time without the need for retraining or fine-tuning separate models. Our extensive experimental evaluation on point cloud classification tasks demonstrates that AdaPT significantly reduces comp",
    "path": "papers/24/01/2401.14845.json",
    "total_tokens": 823,
    "translated_title": "自适应点云 Transformer",
    "translated_abstract": "最近3D数据获取的激增推动了几何深度学习模型在点云处理中的发展，这一发展受到了在自然语言处理中 Transformer 的显著成功的推动。虽然最近的点云 Transformer 已经取得了令人印象深刻的结果，但是它们与点云大小呈二次比例关系，对实际应用来说存在可伸缩性挑战。为了解决这个问题，我们提出了自适应点云 Transformer (AdaPT)，这是一个标准的Transformer模型，通过自适应的token选择机制加以扩展。AdaPT在推断过程中动态减少了token的数量，实现了对大型点云的高效处理。此外，我们引入了一个预算机制，在推断时灵活调整模型的计算成本，而无需重新训练或微调分离的模型。我们在点云分类任务上进行了广泛的实验评估，证明了AdaPT显著降低了计算成本。",
    "tldr": "AdaPT是一个自适应的点云Transformer模型，通过动态减少token的数量来高效处理大型点云，并且可以灵活调整计算成本。Experimental evaluation demonstrates that AdaPT significantly reduces computational cost."
}