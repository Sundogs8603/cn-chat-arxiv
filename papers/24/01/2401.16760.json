{
    "title": "One-Step Forward and Backtrack: Overcoming Zig-Zagging in Loss-Aware Quantization Training. (arXiv:2401.16760v1 [cs.LG])",
    "abstract": "Weight quantization is an effective technique to compress deep neural networks for their deployment on edge devices with limited resources. Traditional loss-aware quantization methods commonly use the quantized gradient to replace the full-precision gradient. However, we discover that the gradient error will lead to an unexpected zig-zagging-like issue in the gradient descent learning procedures, where the gradient directions rapidly oscillate or zig-zag, and such issue seriously slows down the model convergence. Accordingly, this paper proposes a one-step forward and backtrack way for loss-aware quantization to get more accurate and stable gradient direction to defy this issue. During the gradient descent learning, a one-step forward search is designed to find the trial gradient of the next-step, which is adopted to adjust the gradient of current step towards the direction of fast convergence. After that, we backtrack the current step to update the full-precision and quantized weights",
    "link": "http://arxiv.org/abs/2401.16760",
    "context": "Title: One-Step Forward and Backtrack: Overcoming Zig-Zagging in Loss-Aware Quantization Training. (arXiv:2401.16760v1 [cs.LG])\nAbstract: Weight quantization is an effective technique to compress deep neural networks for their deployment on edge devices with limited resources. Traditional loss-aware quantization methods commonly use the quantized gradient to replace the full-precision gradient. However, we discover that the gradient error will lead to an unexpected zig-zagging-like issue in the gradient descent learning procedures, where the gradient directions rapidly oscillate or zig-zag, and such issue seriously slows down the model convergence. Accordingly, this paper proposes a one-step forward and backtrack way for loss-aware quantization to get more accurate and stable gradient direction to defy this issue. During the gradient descent learning, a one-step forward search is designed to find the trial gradient of the next-step, which is adopted to adjust the gradient of current step towards the direction of fast convergence. After that, we backtrack the current step to update the full-precision and quantized weights",
    "path": "papers/24/01/2401.16760.json",
    "total_tokens": 877,
    "translated_title": "一步向前和回溯：克服损失感知量化训练中的曲线行进问题",
    "translated_abstract": "权重量化是一种有效的方法，用于在资源有限的边缘设备上部署深度神经网络。传统的损失感知量化方法常常使用量化梯度来替代全精度梯度。然而，我们发现梯度误差会导致梯度下降学习过程中出现意想不到的曲线行进问题，其中梯度方向迅速振荡或曲线行进，这个问题严重减慢了模型的收敛速度。因此，本文提出了一种一步向前和回溯的损失感知量化方法，以获得更准确稳定的梯度方向来克服此问题。在梯度下降学习过程中，设计了一步向前搜索来寻找下一步的试验梯度，该梯度被用来调整当前步骤的梯度，朝着快速收敛的方向调整。之后，我们回溯当前步骤，更新全精度和量化权重。",
    "tldr": "本文提出了一种新的损失感知量化方法，通过一步向前搜索和回溯的方式解决了梯度下降中出现的曲线行进问题。",
    "en_tdlr": "This paper proposes a new loss-aware quantization method that overcomes the issue of zig-zagging in gradient descent by using a one-step forward search and backtrack technique."
}