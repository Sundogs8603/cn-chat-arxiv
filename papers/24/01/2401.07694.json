{
    "title": "Stochastic optimization with arbitrary recurrent data sampling. (arXiv:2401.07694v1 [math.OC])",
    "abstract": "For obtaining optimal first-order convergence guarantee for stochastic optimization, it is necessary to use a recurrent data sampling algorithm that samples every data point with sufficient frequency. Most commonly used data sampling algorithms (e.g., i.i.d., MCMC, random reshuffling) are indeed recurrent under mild assumptions. In this work, we show that for a particular class of stochastic optimization algorithms, we do not need any other property (e.g., independence, exponential mixing, and reshuffling) than recurrence in data sampling algorithms to guarantee the optimal rate of first-order convergence. Namely, using regularized versions of Minimization by Incremental Surrogate Optimization (MISO), we show that for non-convex and possibly non-smooth objective functions, the expected optimality gap converges at an optimal rate $O(n^{-1/2})$ under general recurrent sampling schemes. Furthermore, the implied constant depends explicitly on the `speed of recurrence', measured by the expe",
    "link": "http://arxiv.org/abs/2401.07694",
    "context": "Title: Stochastic optimization with arbitrary recurrent data sampling. (arXiv:2401.07694v1 [math.OC])\nAbstract: For obtaining optimal first-order convergence guarantee for stochastic optimization, it is necessary to use a recurrent data sampling algorithm that samples every data point with sufficient frequency. Most commonly used data sampling algorithms (e.g., i.i.d., MCMC, random reshuffling) are indeed recurrent under mild assumptions. In this work, we show that for a particular class of stochastic optimization algorithms, we do not need any other property (e.g., independence, exponential mixing, and reshuffling) than recurrence in data sampling algorithms to guarantee the optimal rate of first-order convergence. Namely, using regularized versions of Minimization by Incremental Surrogate Optimization (MISO), we show that for non-convex and possibly non-smooth objective functions, the expected optimality gap converges at an optimal rate $O(n^{-1/2})$ under general recurrent sampling schemes. Furthermore, the implied constant depends explicitly on the `speed of recurrence', measured by the expe",
    "path": "papers/24/01/2401.07694.json",
    "total_tokens": 918,
    "translated_title": "具有任意经常性数据抽样的随机优化",
    "translated_abstract": "为了获得随机优化的最佳一阶收敛保证，需要使用一个经常性数据抽样算法，该算法以足够的频率对每个数据点进行抽样。大多数常用的数据抽样算法（如i.i.d.，MCMC，随机重排）在温和的假设下确实是经常性的。在这项工作中，我们表明对于一类特殊的随机优化算法，我们无需除了数据抽样算法中的经常性之外的任何其他属性（如独立性，指数混合和重排）来保证最佳的一阶收敛速率。也就是说，使用Minimization by Incremental Surrogate Optimization (MISO)的正则化版本，我们证明了对于非凸的、可能不光滑的目标函数，期望的最优性差异在一般的经常性抽样方案下收敛于最佳速率$O(n^{-1/2})$。此外，暗示的常数明确取决于\"经常性的速度\"，由指数测量。",
    "tldr": "这篇论文研究了一种随机优化算法，证明了对于非凸、可能不光滑的目标函数，在一般的经常性抽样方案下，可以以最佳速率收敛；同时指出了收敛速度与\"经常性的速度\"之间的关系。",
    "en_tdlr": "This paper investigates a stochastic optimization algorithm and proves that, for non-convex and possibly non-smooth objective functions, it can achieve optimal convergence rate under general recurrent sampling schemes. The relationship between the convergence rate and the \"speed of recurrence\" is also highlighted."
}