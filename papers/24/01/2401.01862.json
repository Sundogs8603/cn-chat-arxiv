{
    "title": "A Vision Check-up for Language Models. (arXiv:2401.01862v1 [cs.CV])",
    "abstract": "What does learning to model relationships between strings teach large language models (LLMs) about the visual world? We systematically evaluate LLMs' abilities to generate and recognize an assortment of visual concepts of increasing complexity and then demonstrate how a preliminary visual representation learning system can be trained using models of text. As language models lack the ability to consume or output visual information as pixels, we use code to represent images in our study. Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world. Furthermore, experiments on self-supervised visual representation learning, utilizing images generated with text models, highlight the potential to train vision models capable of making semantic assessments of natural images using just LLMs.",
    "link": "http://arxiv.org/abs/2401.01862",
    "context": "Title: A Vision Check-up for Language Models. (arXiv:2401.01862v1 [cs.CV])\nAbstract: What does learning to model relationships between strings teach large language models (LLMs) about the visual world? We systematically evaluate LLMs' abilities to generate and recognize an assortment of visual concepts of increasing complexity and then demonstrate how a preliminary visual representation learning system can be trained using models of text. As language models lack the ability to consume or output visual information as pixels, we use code to represent images in our study. Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world. Furthermore, experiments on self-supervised visual representation learning, utilizing images generated with text models, highlight the potential to train vision models capable of making semantic assessments of natural images using just LLMs.",
    "path": "papers/24/01/2401.01862.json",
    "total_tokens": 952,
    "translated_title": "语言模型的视觉检查",
    "translated_abstract": "学习建模字符串之间关系是否能教会大型语言模型（LLMs）关于视觉世界的知识？我们系统地评估了LLMs生成和识别各种逐渐复杂的视觉概念的能力，然后演示了如何使用文本模型来训练初步的视觉表示学习系统。由于语言模型缺乏以像素形式获取或输出视觉信息的能力，我们在研究中使用代码来表示图像。虽然LLMs生成的图像看起来不像自然图像，但在图像生成和模型纠正这些生成图像的能力方面的结果表明，精确建模字符串可以教会语言模型关于视觉世界的许多方面。此外，利用与文本模型生成的图像进行自我监督的视觉表示学习实验证明了仅使用LLMs可以训练能够对自然图像进行语义评估的视觉模型的潜力。",
    "tldr": "本研究系统地评估了大型语言模型（LLMs）在生成和识别各种视觉概念方面的能力，并通过使用文本模型进行训练的视觉表示学习系统展示了LLMs对视觉世界的认知。实验结果表明，精确建模字符串可以教会LLMs关于视觉世界的多个方面，并且仅使用LLMs可以训练能够对自然图像进行语义评估的视觉模型。",
    "en_tdlr": "This study systematically evaluates the abilities of large language models (LLMs) to generate and recognize visual concepts, and demonstrates how a visual representation learning system trained using text models can teach LLMs about the visual world. The results indicate that precise modeling of strings can teach LLMs about various aspects of the visual world, and LLMs have the potential to train vision models capable of making semantic assessments of natural images."
}