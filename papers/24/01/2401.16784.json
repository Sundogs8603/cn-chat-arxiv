{
    "title": "Graph Fairness Learning under Distribution Shifts. (arXiv:2401.16784v1 [cs.LG])",
    "abstract": "Graph neural networks (GNNs) have achieved remarkable performance on graph-structured data. However, GNNs may inherit prejudice from the training data and make discriminatory predictions based on sensitive attributes, such as gender and race. Recently, there has been an increasing interest in ensuring fairness on GNNs, but all of them are under the assumption that the training and testing data are under the same distribution, i.e., training data and testing data are from the same graph. Will graph fairness performance decrease under distribution shifts? How does distribution shifts affect graph fairness learning? All these open questions are largely unexplored from a theoretical perspective. To answer these questions, we first theoretically identify the factors that determine bias on a graph. Subsequently, we explore the factors influencing fairness on testing graphs, with a noteworthy factor being the representation distances of certain groups between the training and testing graph. M",
    "link": "http://arxiv.org/abs/2401.16784",
    "context": "Title: Graph Fairness Learning under Distribution Shifts. (arXiv:2401.16784v1 [cs.LG])\nAbstract: Graph neural networks (GNNs) have achieved remarkable performance on graph-structured data. However, GNNs may inherit prejudice from the training data and make discriminatory predictions based on sensitive attributes, such as gender and race. Recently, there has been an increasing interest in ensuring fairness on GNNs, but all of them are under the assumption that the training and testing data are under the same distribution, i.e., training data and testing data are from the same graph. Will graph fairness performance decrease under distribution shifts? How does distribution shifts affect graph fairness learning? All these open questions are largely unexplored from a theoretical perspective. To answer these questions, we first theoretically identify the factors that determine bias on a graph. Subsequently, we explore the factors influencing fairness on testing graphs, with a noteworthy factor being the representation distances of certain groups between the training and testing graph. M",
    "path": "papers/24/01/2401.16784.json",
    "total_tokens": 877,
    "translated_title": "在分布变化下的图公平学习",
    "translated_abstract": "图神经网络（GNNs）在图结构数据上取得了显著的性能。然而，GNNs可能会从训练数据中继承偏见，并根据敏感属性（如性别和种族）做出歧视性预测。最近，越来越多的人关注在GNNs上确保公平性，但所有这些方法都基于训练数据和测试数据在相同分布下的假设，即训练数据和测试数据来自同一个图。在分布变化下，图的公平性性能是否会降低？分布变化如何影响图的公平学习？所有这些开放问题从理论上很大程度上未被探索。为了回答这些问题，我们首先在理论上确定了决定图中偏差的因素。随后，我们探索了影响测试图公平性的因素，其中一个值得注意的因素是训练图和测试图之间某些群体的表示距离。",
    "tldr": "论文主要研究了在分布变化下的图公平学习，通过理论分析和实证研究发现了决定图中偏差的因素，并探索了训练图和测试图之间表示距离的影响，对于在图结构数据上确保公平性具有重要意义。",
    "en_tdlr": "This paper investigates graph fairness learning under distribution shifts. The authors identify factors that determine bias and explore the influence of representation distances between training and testing graphs. The findings are significant for ensuring fairness on graph-structured data."
}