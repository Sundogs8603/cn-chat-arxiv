{
    "title": "Weak Correlations as the Underlying Principle for Linearization of Gradient-Based Learning Systems. (arXiv:2401.04013v1 [cs.LG])",
    "abstract": "Deep learning models, such as wide neural networks, can be conceptualized as nonlinear dynamical physical systems characterized by a multitude of interacting degrees of freedom. Such systems in the infinite limit, tend to exhibit simplified dynamics. This paper delves into gradient descent-based learning algorithms, that display a linear structure in their parameter dynamics, reminiscent of the neural tangent kernel. We establish this apparent linearity arises due to weak correlations between the first and higher-order derivatives of the hypothesis function, concerning the parameters, taken around their initial values. This insight suggests that these weak correlations could be the underlying reason for the observed linearization in such systems. As a case in point, we showcase this weak correlations structure within neural networks in the large width limit. Exploiting the relationship between linearity and weak correlations, we derive a bound on deviations from linearity observed duri",
    "link": "http://arxiv.org/abs/2401.04013",
    "context": "Title: Weak Correlations as the Underlying Principle for Linearization of Gradient-Based Learning Systems. (arXiv:2401.04013v1 [cs.LG])\nAbstract: Deep learning models, such as wide neural networks, can be conceptualized as nonlinear dynamical physical systems characterized by a multitude of interacting degrees of freedom. Such systems in the infinite limit, tend to exhibit simplified dynamics. This paper delves into gradient descent-based learning algorithms, that display a linear structure in their parameter dynamics, reminiscent of the neural tangent kernel. We establish this apparent linearity arises due to weak correlations between the first and higher-order derivatives of the hypothesis function, concerning the parameters, taken around their initial values. This insight suggests that these weak correlations could be the underlying reason for the observed linearization in such systems. As a case in point, we showcase this weak correlations structure within neural networks in the large width limit. Exploiting the relationship between linearity and weak correlations, we derive a bound on deviations from linearity observed duri",
    "path": "papers/24/01/2401.04013.json",
    "total_tokens": 900,
    "translated_title": "以弱相关性作为梯度下降学习系统线性化的基本原则",
    "translated_abstract": "深度学习模型，如宽神经网络，可以被概念化为非线性动力学物理系统，其具有多个相互作用的自由度。在无限极限下，这些系统趋向于表现出简化的动力学。本文深入研究了基于梯度下降的学习算法，在其参数动力学中展示出与神经切向核类似的线性结构。我们发现，这种明显的线性化是因为在初始值附近，假设函数的一阶和高阶导数之间的弱相关性。这一洞见表明，这些弱相关性可能是此类系统中观察到的线性化的潜在原因。作为一个例证，我们展示了在宽度很大的神经网络中存在的这种弱相关性结构。利用线性和弱相关性之间的关系，我们推导出线性度偏离的一个界限。",
    "tldr": "本文研究了梯度下降学习算法在参数动力学中的线性结构，发现这种线性化现象是由于初始值附近假设函数的一阶和高阶导数之间的弱相关性所致。这一发现为深度学习模型的线性化提供了新的认识。"
}