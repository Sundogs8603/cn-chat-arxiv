{
    "title": "Leveraging Print Debugging to Improve Code Generation in Large Language Models. (arXiv:2401.05319v1 [cs.CL])",
    "abstract": "Large language models (LLMs) have made significant progress in code generation tasks, but their performance in tackling programming problems with complex data structures and algorithms remains suboptimal. To address this issue, we propose an in-context learning approach that guides LLMs to debug by using a \"print debugging\" method, which involves inserting print statements to trace and analysing logs for fixing the bug. We collect a Leetcode problem dataset and evaluate our method using the Leetcode online judging system. Experiments with GPT-4 demonstrate the effectiveness of our approach, outperforming rubber duck debugging in easy and medium-level Leetcode problems by 1.5% and 17.9%.",
    "link": "http://arxiv.org/abs/2401.05319",
    "context": "Title: Leveraging Print Debugging to Improve Code Generation in Large Language Models. (arXiv:2401.05319v1 [cs.CL])\nAbstract: Large language models (LLMs) have made significant progress in code generation tasks, but their performance in tackling programming problems with complex data structures and algorithms remains suboptimal. To address this issue, we propose an in-context learning approach that guides LLMs to debug by using a \"print debugging\" method, which involves inserting print statements to trace and analysing logs for fixing the bug. We collect a Leetcode problem dataset and evaluate our method using the Leetcode online judging system. Experiments with GPT-4 demonstrate the effectiveness of our approach, outperforming rubber duck debugging in easy and medium-level Leetcode problems by 1.5% and 17.9%.",
    "path": "papers/24/01/2401.05319.json",
    "total_tokens": 788,
    "translated_title": "利用打印调试提高大型语言模型中的代码生成能力",
    "translated_abstract": "大型语言模型(LLMs)在代码生成任务中取得了显著进展，但在处理具有复杂数据结构和算法的编程问题时性能仍不理想。为了解决这个问题，我们提出了一种上下文学习方法，通过使用\"打印调试\"方法来引导LLMs进行调试，该方法涉及插入打印语句以跟踪和分析日志来修复错误。我们收集了Leetcode问题数据集，并使用Leetcode在线判题系统评估了我们的方法。使用GPT-4进行的实验证明了我们方法的有效性，在Leetcode的简单和中等级别问题上优于橡皮鸭调试分别达到1.5%和17.9%。",
    "tldr": "通过利用打印调试方法，我们提出了一种上下文学习方法来改进大型语言模型(LLMs)在复杂编程问题中的代码生成能力。实验证明我们的方法比橡皮鸭调试在Leetcode的简单和中等级别问题上分别提高了1.5%和17.9%。",
    "en_tdlr": "By leveraging print debugging, we propose an in-context learning approach to improve code generation in large language models (LLMs) for complex programming problems. Experimental results demonstrate that our method outperforms rubber duck debugging by 1.5% and 17.9% in easy and medium-level Leetcode problems, respectively."
}