{
    "title": "Cost-sensitive Feature Selection for Support Vector Machines. (arXiv:2401.07627v1 [stat.ML])",
    "abstract": "Feature Selection is a crucial procedure in Data Science tasks such as Classification, since it identifies the relevant variables, making thus the classification procedures more interpretable, cheaper in terms of measurement and more effective by reducing noise and data overfit. The relevance of features in a classification procedure is linked to the fact that misclassifications costs are frequently asymmetric, since false positive and false negative cases may have very different consequences. However, off-the-shelf Feature Selection procedures seldom take into account such cost-sensitivity of errors.  In this paper we propose a mathematical-optimization-based Feature Selection procedure embedded in one of the most popular classification procedures, namely, Support Vector Machines, accommodating asymmetric misclassification costs. The key idea is to replace the traditional margin maximization by minimizing the number of features selected, but imposing upper bounds on the false positive",
    "link": "http://arxiv.org/abs/2401.07627",
    "context": "Title: Cost-sensitive Feature Selection for Support Vector Machines. (arXiv:2401.07627v1 [stat.ML])\nAbstract: Feature Selection is a crucial procedure in Data Science tasks such as Classification, since it identifies the relevant variables, making thus the classification procedures more interpretable, cheaper in terms of measurement and more effective by reducing noise and data overfit. The relevance of features in a classification procedure is linked to the fact that misclassifications costs are frequently asymmetric, since false positive and false negative cases may have very different consequences. However, off-the-shelf Feature Selection procedures seldom take into account such cost-sensitivity of errors.  In this paper we propose a mathematical-optimization-based Feature Selection procedure embedded in one of the most popular classification procedures, namely, Support Vector Machines, accommodating asymmetric misclassification costs. The key idea is to replace the traditional margin maximization by minimizing the number of features selected, but imposing upper bounds on the false positive",
    "path": "papers/24/01/2401.07627.json",
    "total_tokens": 777,
    "translated_title": "针对支持向量机的成本敏感特征选择",
    "translated_abstract": "特征选择是数据科学任务（如分类）中的关键步骤，它可以识别相关变量，使分类过程更具解释性，更省成本（测量方面），通过减少噪声和数据过拟合而更有效。特征在分类过程中的相关性与误分类成本的不对称性有关，因为假阳性和假阴性情况可能具有非常不同的后果。然而，现有的特征选择方法很少考虑到这种误差的成本敏感性。本文提出了一种基于数学优化的特征选择方法，嵌入到其中一个最常用的分类方法——支持向量机中，以适应误分类成本的不对称性。其关键思想是通过最小化所选特征的数量来取代传统的边界最大化，并对假阳性设置上限。",
    "tldr": "该论文提出了一种针对支持向量机的成本敏感特征选择方法，通过最小化所选特征的数量来适应误分类成本的不对称性。",
    "en_tdlr": "This paper presents a cost-sensitive feature selection method for Support Vector Machines, which adapts to the asymmetry of misclassification costs by minimizing the number of selected features."
}