{
    "title": "Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning",
    "abstract": "While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities. To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability. In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF). We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment. Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences. It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to l",
    "link": "https://arxiv.org/abs/2401.17186",
    "context": "Title: Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning\nAbstract: While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities. To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability. In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF). We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment. Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences. It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to l",
    "path": "papers/24/01/2401.17186.json",
    "total_tokens": 969,
    "translated_title": "在CLIP中拥抱语言包容性和多样性：通过持续语言学习",
    "translated_abstract": "近年来，视觉-语言预训练模型(VL-PTMs)在多模态研究方面取得了进展，但由于只掌握了少数几种语言（比如英语），限制了其在更广泛的社区中的适用性。为了解决这个问题，人们越来越关注通过联合学习来开发多语言VL模型，然而，由于昂贵的成本和数据可用性，这种方法可能不切实际。在这项工作中，我们提出通过持续语言学习（CLL）来扩展VL-PTMs的语言能力，即模型需要增量地更新其语言知识，同时避免灾难性遗忘（CF）。我们首先介绍了一个名为CLL-CLIP的模型，它是在CLIP的基础上构建的，而CLIP是一种已经具备了图像-英语文本对齐能力的流行VL-PTM。具体而言，CLL-CLIP包含了一个可扩展的词嵌入层，用于处理语言差异。它仅训练词嵌入以提高内存稳定性，并在跨模态和跨语言目标下进行优化。",
    "tldr": "本论文提出了一种通过持续语言学习来扩展视觉-语言预训练模型（VL-PTMs）的语言能力的方法。该方法通过增量更新语言知识，避免灾难性遗忘，并在跨模态和跨语言目标下进行优化。"
}