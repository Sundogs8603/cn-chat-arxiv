{
    "title": "Multi-Candidate Speculative Decoding. (arXiv:2401.06706v1 [cs.CL])",
    "abstract": "Large language models have shown impressive capabilities across a variety of NLP tasks, yet their generating text autoregressively is time-consuming. One way to speed them up is speculative decoding, which generates candidate segments (a sequence of tokens) from a fast draft model that is then verified in parallel by the target model. However, the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup. This paper proposes sampling multiple candidates from a draft model and then organising them in batches for verification. We design algorithms for efficient multi-candidate verification while maintaining the distribution of the target model. Our approach shows significant improvements in acceptance rates on multiple datasets and models, consistently outperforming standard speculative decoding.",
    "link": "http://arxiv.org/abs/2401.06706",
    "context": "Title: Multi-Candidate Speculative Decoding. (arXiv:2401.06706v1 [cs.CL])\nAbstract: Large language models have shown impressive capabilities across a variety of NLP tasks, yet their generating text autoregressively is time-consuming. One way to speed them up is speculative decoding, which generates candidate segments (a sequence of tokens) from a fast draft model that is then verified in parallel by the target model. However, the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup. This paper proposes sampling multiple candidates from a draft model and then organising them in batches for verification. We design algorithms for efficient multi-candidate verification while maintaining the distribution of the target model. Our approach shows significant improvements in acceptance rates on multiple datasets and models, consistently outperforming standard speculative decoding.",
    "path": "papers/24/01/2401.06706.json",
    "total_tokens": 775,
    "translated_title": "多候选推测解码",
    "translated_abstract": "大型语言模型在各种自然语言处理任务中展现了令人瞩目的能力，但使用自回归生成文本的效率较低。其中一种提高效率的方法是推测解码，它从一个快速的草稿模型生成候选段落（一系列的标记），然后通过目标模型并行验证。然而，候选标记的接受率受到模型、数据集和解码设置等多个因素的限制。本文提出了从草稿模型中采样多个候选标记，并将它们组织成批次进行验证的方法。我们设计了高效的多候选验证算法，同时保持了目标模型的分布。我们的方法在多个数据集和模型上都显示出了显著的接受率提高，始终优于标准的推测解码方法。",
    "tldr": "本研究提出了一种多候选推测解码的方法，通过从草稿模型中采样多个候选标记并进行批次验证，显著提高了接受率并优于标准推测解码方法。",
    "en_tdlr": "This paper proposes a method called multi-candidate speculative decoding, which improves the acceptance rate by sampling multiple candidates from a draft model and verifying them in batches, outperforming standard speculative decoding."
}