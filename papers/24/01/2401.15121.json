{
    "title": "Expressive Power of ReLU and Step Networks under Floating-Point Operations. (arXiv:2401.15121v1 [cs.LG])",
    "abstract": "The study of the expressive power of neural networks has investigated the fundamental limits of neural networks. Most existing results assume real-valued inputs and parameters as well as exact operations during the evaluation of neural networks. However, neural networks are typically executed on computers that can only represent a tiny subset of the reals and apply inexact operations. In this work, we analyze the expressive power of neural networks under a more realistic setup: when we use floating-point numbers and operations. Our first set of results assumes floating-point operations where the significand of a float is represented by finite bits but its exponent can take any integer value. Under this setup, we show that neural networks using a binary threshold unit or ReLU can memorize any finite input/output pairs and can approximate any continuous function within a small error. We also show similar results on memorization and universal approximation when floating-point operations u",
    "link": "http://arxiv.org/abs/2401.15121",
    "context": "Title: Expressive Power of ReLU and Step Networks under Floating-Point Operations. (arXiv:2401.15121v1 [cs.LG])\nAbstract: The study of the expressive power of neural networks has investigated the fundamental limits of neural networks. Most existing results assume real-valued inputs and parameters as well as exact operations during the evaluation of neural networks. However, neural networks are typically executed on computers that can only represent a tiny subset of the reals and apply inexact operations. In this work, we analyze the expressive power of neural networks under a more realistic setup: when we use floating-point numbers and operations. Our first set of results assumes floating-point operations where the significand of a float is represented by finite bits but its exponent can take any integer value. Under this setup, we show that neural networks using a binary threshold unit or ReLU can memorize any finite input/output pairs and can approximate any continuous function within a small error. We also show similar results on memorization and universal approximation when floating-point operations u",
    "path": "papers/24/01/2401.15121.json",
    "total_tokens": 842,
    "translated_title": "ReLU和Step网络在浮点运算下的表达能力",
    "translated_abstract": "神经网络表达能力的研究调查了神经网络的基本限制。大多数现有的结果假设实数输入和参数以及在神经网络评估过程中进行精确运算。然而，神经网络通常在只能表示实数的计算机上执行，并且进行不精确的运算。在这项工作中，我们分析了在更实际的设置下神经网络的表达能力：使用浮点数和浮点运算。我们的第一组结果假设浮点运算中，浮点数的有效位数由有限位表示，但其指数可以取任何整数值。在这种设置下，我们展示了神经网络使用二进制阈值单元或ReLU可以记忆任何有限的输入/输出对，并可以在小误差内逼近任何连续函数。我们还展示了浮点运算下关于记忆和通用逼近的类似结果。",
    "tldr": "该论文研究了在浮点运算下神经网络的表达能力，证明了使用二进制阈值单元或ReLU的神经网络可以记忆任何实数输入/输出对，并且可以在小误差内逼近任何连续函数。",
    "en_tdlr": "This paper investigates the expressive power of neural networks under floating-point operations and demonstrates that neural networks using binary threshold units or ReLU can memorize any real-valued input/output pairs and approximate any continuous function within a small error."
}