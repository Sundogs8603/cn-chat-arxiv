{
    "title": "On the Audio Hallucinations in Large Audio-Video Language Models. (arXiv:2401.09774v1 [cs.MM])",
    "abstract": "Large audio-video language models can generate descriptions for both video and audio. However, they sometimes ignore audio content, producing audio descriptions solely reliant on visual information. This paper refers to this as audio hallucinations and analyzes them in large audio-video language models. We gather 1,000 sentences by inquiring about audio information and annotate them whether they contain hallucinations. If a sentence is hallucinated, we also categorize the type of hallucination. The results reveal that 332 sentences are hallucinated with distinct trends observed in nouns and verbs for each hallucination type. Based on this, we tackle a task of audio hallucination classification using pre-trained audio-text models in the zero-shot and fine-tuning settings. Our experimental results reveal that the zero-shot models achieve higher performance (52.2% in F1) than the random (40.3%) and the fine-tuning models achieve 87.9%, outperforming the zero-shot models.",
    "link": "http://arxiv.org/abs/2401.09774",
    "context": "Title: On the Audio Hallucinations in Large Audio-Video Language Models. (arXiv:2401.09774v1 [cs.MM])\nAbstract: Large audio-video language models can generate descriptions for both video and audio. However, they sometimes ignore audio content, producing audio descriptions solely reliant on visual information. This paper refers to this as audio hallucinations and analyzes them in large audio-video language models. We gather 1,000 sentences by inquiring about audio information and annotate them whether they contain hallucinations. If a sentence is hallucinated, we also categorize the type of hallucination. The results reveal that 332 sentences are hallucinated with distinct trends observed in nouns and verbs for each hallucination type. Based on this, we tackle a task of audio hallucination classification using pre-trained audio-text models in the zero-shot and fine-tuning settings. Our experimental results reveal that the zero-shot models achieve higher performance (52.2% in F1) than the random (40.3%) and the fine-tuning models achieve 87.9%, outperforming the zero-shot models.",
    "path": "papers/24/01/2401.09774.json",
    "total_tokens": 932,
    "translated_title": "关于大型音视频语言模型中的音频幻听问题",
    "translated_abstract": "大型音视频语言模型可以为视频和音频生成描述。然而，它们有时会忽略音频内容，仅根据视觉信息生成音频描述。本文将此称为音频幻听，并对大型音视频语言模型中的幻听进行了分析。我们收集了1000个句子，通过询问音频信息，并注释它们是否包含幻听。如果一个句子是幻听的，我们还对幻听类型进行了分类。结果表明，有332个句子是幻听的，并且在每种幻听类型的名词和动词中观察到了明显的趋势。基于这些结果，我们使用预训练的音频文本模型在零样本学习和微调设置下解决了音频幻听分类任务。我们的实验结果显示，零样本模型的性能较高（F1为52.2%），优于随机模型（40.3%），而微调模型的性能为87.9%，超过了零样本模型。",
    "tldr": "本文分析了大型音视频语言模型中的音频幻听问题。通过收集1,000个句子并进行分类，研究发现有332个句子出现了幻听现象，针对这个问题，使用预训练的音频文本模型进行了零样本学习和微调，结果显示微调模型具有更好的性能。"
}