{
    "title": "PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and Ensemble Techniques. (arXiv:2401.02122v1 [cs.CL])",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is increasingly recognized as an effective method in speech processing. However, the optimal approach and the placement of PEFT methods remain inconclusive. Our study conducts extensive experiments to compare different PEFT methods and their layer-wise placement adapting Differentiable Architecture Search (DARTS). We also explore the use of ensemble learning to leverage diverse PEFT strategies. The results reveal that DARTS does not outperform the baseline approach, which involves inserting the same PEFT method into all layers of a Self-Supervised Learning (SSL) model. In contrast, an ensemble learning approach, particularly one employing majority voting, demonstrates superior performance. Our statistical evidence indicates that different PEFT methods learn in varied ways. This variation might explain why the synergistic integration of various PEFT methods through ensemble learning can harness their unique learning capabilities more effectively co",
    "link": "http://arxiv.org/abs/2401.02122",
    "context": "Title: PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and Ensemble Techniques. (arXiv:2401.02122v1 [cs.CL])\nAbstract: Parameter-Efficient Fine-Tuning (PEFT) is increasingly recognized as an effective method in speech processing. However, the optimal approach and the placement of PEFT methods remain inconclusive. Our study conducts extensive experiments to compare different PEFT methods and their layer-wise placement adapting Differentiable Architecture Search (DARTS). We also explore the use of ensemble learning to leverage diverse PEFT strategies. The results reveal that DARTS does not outperform the baseline approach, which involves inserting the same PEFT method into all layers of a Self-Supervised Learning (SSL) model. In contrast, an ensemble learning approach, particularly one employing majority voting, demonstrates superior performance. Our statistical evidence indicates that different PEFT methods learn in varied ways. This variation might explain why the synergistic integration of various PEFT methods through ensemble learning can harness their unique learning capabilities more effectively co",
    "path": "papers/24/01/2401.02122.json",
    "total_tokens": 969,
    "translated_title": "用于语音的PEFT：揭示优化放置、合并策略和集成技术",
    "translated_abstract": "参数高效微调（PEFT）被越来越认为是语音处理中一种有效的方法。然而，PEFT方法的最佳方法和放置仍然没有定论。我们的研究通过扩展实验来比较不同的PEFT方法及其逐层放置，使用可微分架构搜索（DARTS）进行调整。我们还探索了使用集成学习来利用多样化的PEFT策略。结果显示，DARTS并不比基线方法表现更好，基线方法涉及将相同的PEFT方法插入到自监督学习（SSL）模型的所有层中。相反，采用多数投票的集成学习方法表现出更好的性能。我们的统计证据表明，不同的PEFT方法以不同的方式进行学习。这种变异可能解释了为什么通过集成学习有效地利用各种PEFT方法的独特学习能力。",
    "tldr": "本研究通过比较不同的PEFT方法和逐层放置方式，以及采用集成学习策略，揭示了用于语音处理的PEFT的最佳方法和放置策略。结果表明，集成学习方法通过多数投票可以实现优于其他方法的性能。这项研究还发现不同的PEFT方法以不同的方式进行学习，从而解释了为什么通过集成学习可以更有效地利用它们的学习能力。"
}