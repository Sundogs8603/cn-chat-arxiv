{
    "title": "An Information Theoretic Approach to Interaction-Grounded Learning. (arXiv:2401.05015v1 [cs.LG])",
    "abstract": "Reinforcement learning (RL) problems where the learner attempts to infer an unobserved reward from some feedback variables have been studied in several recent papers. The setting of Interaction-Grounded Learning (IGL) is an example of such feedback-based reinforcement learning tasks where the learner optimizes the return by inferring latent binary rewards from the interaction with the environment. In the IGL setting, a relevant assumption used in the RL literature is that the feedback variable $Y$ is conditionally independent of the context-action $(X,A)$ given the latent reward $R$. In this work, we propose Variational Information-based IGL (VI-IGL) as an information-theoretic method to enforce the conditional independence assumption in the IGL-based RL problem. The VI-IGL framework learns a reward decoder using an information-based objective based on the conditional mutual information (MI) between the context-action $(X,A)$ and the feedback variable $Y$ observed from the environment.",
    "link": "http://arxiv.org/abs/2401.05015",
    "context": "Title: An Information Theoretic Approach to Interaction-Grounded Learning. (arXiv:2401.05015v1 [cs.LG])\nAbstract: Reinforcement learning (RL) problems where the learner attempts to infer an unobserved reward from some feedback variables have been studied in several recent papers. The setting of Interaction-Grounded Learning (IGL) is an example of such feedback-based reinforcement learning tasks where the learner optimizes the return by inferring latent binary rewards from the interaction with the environment. In the IGL setting, a relevant assumption used in the RL literature is that the feedback variable $Y$ is conditionally independent of the context-action $(X,A)$ given the latent reward $R$. In this work, we propose Variational Information-based IGL (VI-IGL) as an information-theoretic method to enforce the conditional independence assumption in the IGL-based RL problem. The VI-IGL framework learns a reward decoder using an information-based objective based on the conditional mutual information (MI) between the context-action $(X,A)$ and the feedback variable $Y$ observed from the environment.",
    "path": "papers/24/01/2401.05015.json",
    "total_tokens": 886,
    "translated_title": "信息论方法在基于互动学习中的应用",
    "translated_abstract": "最近的几篇论文研究了强化学习（RL）中学习者试图从一些反馈变量中推断出未观测到的奖励的问题。互动基础学习（IGL）是这种基于反馈的强化学习任务的一个例子，学习者通过与环境的互动推断出潜在的二值奖励来优化返回。在IGL设置中，RL文献中使用的一个相关假设是，在给定潜在奖励R的情况下，反馈变量Y在上下文-动作（X，A）上是条件独立的。在本文中，我们提出了一种基于变分信息的IGL（VI-IGL）方法，以应用于IGL的RL问题中，以强制执行条件独立性假设。VI-IGL框架使用基于条件互信息（MI）的信息目标来学习奖励解码器，该信息目标衡量了从环境中观察到的上下文-动作（X，A）和反馈变量Y之间的条件互信息。",
    "tldr": "本文提出了一个基于变分信息的互动基础学习（VI-IGL）方法，用于在强化学习任务中强制执行条件独立性假设。该方法通过学习奖励解码器来最大化上下文-动作（X，A）和反馈变量Y之间的条件互信息。"
}