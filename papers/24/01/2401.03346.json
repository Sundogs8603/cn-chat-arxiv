{
    "title": "An Investigation of Large Language Models for Real-World Hate Speech Detection. (arXiv:2401.03346v1 [cs.CY])",
    "abstract": "Hate speech has emerged as a major problem plaguing our social spaces today. While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online. A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions. Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection. In this study, we conduct a large-scale study of hat",
    "link": "http://arxiv.org/abs/2401.03346",
    "context": "Title: An Investigation of Large Language Models for Real-World Hate Speech Detection. (arXiv:2401.03346v1 [cs.CY])\nAbstract: Hate speech has emerged as a major problem plaguing our social spaces today. While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online. A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions. Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection. In this study, we conduct a large-scale study of hat",
    "path": "papers/24/01/2401.03346.json",
    "total_tokens": 977,
    "translated_title": "大规模语言模型在现实世界中对恶意言论检测的调查",
    "translated_abstract": "恶意言论已经成为困扰我们社交空间的一个主要问题。虽然在解决这个问题上已经做出了一些显著的努力，但现有方法在有效检测在线恶意言论方面仍然存在显著限制。现有方法的一个主要限制是恶意言论检测是一个高度上下文相关的问题，而这些方法无法完全捕捉恶意言论的上下文以进行准确的预测。最近，大规模语言模型（LLMs）在几个自然语言任务中展示了最先进的性能。LLMs经过大量的自然语言数据进行了广泛的训练，使其能够掌握复杂的上下文细节。因此，它们可以用作上下文感知的恶意言论检测的知识库。然而，使用LLMs检测恶意言论的一个基本问题是没有关于有效提示LLMs进行上下文感知的恶意言论检测的研究。在本研究中，我们进行了一个大规模的研究，调查恶意言论检测方面的问题。",
    "tldr": "本研究调查了大规模语言模型在现实世界中对恶意言论检测的有效性，并发现现有方法在上下文感知方面存在显著限制。而大规模语言模型具有潜力作为上下文感知恶意言论检测的知识库，但目前缺乏有效提示这些模型的方法。",
    "en_tdlr": "This study investigates the effectiveness of large language models (LLMs) for real-world hate speech detection, and finds that existing methods are limited in context awareness. LLMs have the potential to serve as knowledge bases for context-aware hate speech detection, but there is currently a lack of effective methods to prompt these models."
}