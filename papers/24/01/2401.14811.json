{
    "title": "On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks. (arXiv:2401.14811v1 [cs.AI])",
    "abstract": "In this paper, we study the expressivity of scalar, Markovian reward functions in Reinforcement Learning (RL), and identify several limitations to what they can express. Specifically, we look at three classes of RL tasks; multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive necessary and sufficient conditions that describe when a problem in this class can be expressed using a scalar, Markovian reward. Moreover, we find that scalar, Markovian rewards are unable to express most of the instances in each of these three classes. We thereby contribute to a more complete understanding of what standard reward functions can and cannot express. In addition to this, we also call attention to modal problems as a new class of problems, since they have so far not been given any systematic treatment in the RL literature. We also briefly outline some approaches for solving some of the problems we discuss, by means of bespoke RL algorithms.",
    "link": "http://arxiv.org/abs/2401.14811",
    "context": "Title: On the Limitations of Markovian Rewards to Express Multi-Objective, Risk-Sensitive, and Modal Tasks. (arXiv:2401.14811v1 [cs.AI])\nAbstract: In this paper, we study the expressivity of scalar, Markovian reward functions in Reinforcement Learning (RL), and identify several limitations to what they can express. Specifically, we look at three classes of RL tasks; multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive necessary and sufficient conditions that describe when a problem in this class can be expressed using a scalar, Markovian reward. Moreover, we find that scalar, Markovian rewards are unable to express most of the instances in each of these three classes. We thereby contribute to a more complete understanding of what standard reward functions can and cannot express. In addition to this, we also call attention to modal problems as a new class of problems, since they have so far not been given any systematic treatment in the RL literature. We also briefly outline some approaches for solving some of the problems we discuss, by means of bespoke RL algorithms.",
    "path": "papers/24/01/2401.14811.json",
    "total_tokens": 938,
    "translated_title": "关于马尔可夫奖励在多目标、风险敏感和模态任务中表达的局限性",
    "translated_abstract": "本文研究了强化学习中标量、马尔可夫奖励函数的表达能力，并确定了它们的一些局限性。具体来说，我们研究了三类强化学习任务：多目标强化学习、风险敏感强化学习和模态强化学习。对于每一类任务，我们导出了描述什么样的问题可以使用标量、马尔可夫奖励函数来表达的必要和充分条件。此外，我们发现在这三个类别中，标量、马尔可夫奖励函数无法表达大部分实例。通过这项研究，我们为了解标准奖励函数可以和不能表达的内容做出了更完整的贡献。除此之外，我们还将模态问题作为一个新的问题类别引入，因为在强化学习文献中，目前尚未对其进行系统性研究。我们还简要概述了通过定制的强化学习算法解决所讨论问题的一些方法。",
    "tldr": "本文研究了强化学习中标量、马尔可夫奖励函数的表达能力，并确定了它们的一些局限性。我们发现这些奖励函数无法表达多目标、风险敏感和模态任务中的大部分实例。"
}