{
    "title": "Effect of Weight Quantization on Learning Models by Typical Case Analysis. (arXiv:2401.17269v1 [stat.ML])",
    "abstract": "This paper examines the quantization methods used in large-scale data analysis models and their hyperparameter choices. The recent surge in data analysis scale has significantly increased computational resource requirements. To address this, quantizing model weights has become a prevalent practice in data analysis applications such as deep learning. Quantization is particularly vital for deploying large models on devices with limited computational resources. However, the selection of quantization hyperparameters, like the number of bits and value range for weight quantization, remains an underexplored area. In this study, we employ the typical case analysis from statistical physics, specifically the replica method, to explore the impact of hyperparameters on the quantization of simple learning models. Our analysis yields three key findings: (i) an unstable hyperparameter phase, known as replica symmetry breaking, occurs with a small number of bits and a large quantization width; (ii) t",
    "link": "http://arxiv.org/abs/2401.17269",
    "context": "Title: Effect of Weight Quantization on Learning Models by Typical Case Analysis. (arXiv:2401.17269v1 [stat.ML])\nAbstract: This paper examines the quantization methods used in large-scale data analysis models and their hyperparameter choices. The recent surge in data analysis scale has significantly increased computational resource requirements. To address this, quantizing model weights has become a prevalent practice in data analysis applications such as deep learning. Quantization is particularly vital for deploying large models on devices with limited computational resources. However, the selection of quantization hyperparameters, like the number of bits and value range for weight quantization, remains an underexplored area. In this study, we employ the typical case analysis from statistical physics, specifically the replica method, to explore the impact of hyperparameters on the quantization of simple learning models. Our analysis yields three key findings: (i) an unstable hyperparameter phase, known as replica symmetry breaking, occurs with a small number of bits and a large quantization width; (ii) t",
    "path": "papers/24/01/2401.17269.json",
    "total_tokens": 863,
    "translated_title": "权重量化对典型案例分析中学习模型的影响",
    "translated_abstract": "本文研究了在大规模数据分析模型中使用的量化方法及其超参数选择。随着数据分析规模的增加，计算资源需求显著增加。为了解决这个问题，在数据分析应用（如深度学习）中，量化模型权重已经成为一种常见的做法。对于在计算资源有限的设备上部署大型模型，量化尤为重要。然而，量化超参数的选择，如位数和权重量化的值范围，仍然是一个未经充分研究的领域。在本研究中，我们采用了统计物理学中的典型案例分析方法，具体是重复方法，来探索超参数对简单学习模型量化的影响。我们的分析得出了三个关键发现：（i）小位数和大量化宽度会导致不稳定的超参数阶段，即重复对称性破缺；（ii）",
    "tldr": "本文研究了大规模数据分析模型中的权重量化方法及超参数选择。通过典型案例分析，我们发现低位数和大量化宽度会导致不稳定的超参数阶段。"
}