{
    "title": "Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection. (arXiv:2401.12326v1 [cs.CL])",
    "abstract": "SemEval-2024 Task 8 introduces the challenge of identifying machine-generated texts from diverse Large Language Models (LLMs) in various languages and domains. The task comprises three subtasks: binary classification in monolingual and multilingual (Subtask A), multi-class classification (Subtask B), and mixed text detection (Subtask C). This paper focuses on Subtask A & B. Each subtask is supported by three datasets for training, development, and testing. To tackle this task, two methods: 1) using traditional machine learning (ML) with natural language preprocessing (NLP) for feature extraction, and 2) fine-tuning LLMs for text classification. The results show that transformer models, particularly LoRA-RoBERTa, exceed traditional ML methods in effectiveness, with majority voting being particularly effective in multilingual contexts for identifying machine-generated texts.",
    "link": "http://arxiv.org/abs/2401.12326",
    "context": "Title: Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection. (arXiv:2401.12326v1 [cs.CL])\nAbstract: SemEval-2024 Task 8 introduces the challenge of identifying machine-generated texts from diverse Large Language Models (LLMs) in various languages and domains. The task comprises three subtasks: binary classification in monolingual and multilingual (Subtask A), multi-class classification (Subtask B), and mixed text detection (Subtask C). This paper focuses on Subtask A & B. Each subtask is supported by three datasets for training, development, and testing. To tackle this task, two methods: 1) using traditional machine learning (ML) with natural language preprocessing (NLP) for feature extraction, and 2) fine-tuning LLMs for text classification. The results show that transformer models, particularly LoRA-RoBERTa, exceed traditional ML methods in effectiveness, with majority voting being particularly effective in multilingual contexts for identifying machine-generated texts.",
    "path": "papers/24/01/2401.12326.json",
    "total_tokens": 917,
    "translated_title": "对多生成器、多领域和多语言机器生成文本检测的大型语言模型进行微调",
    "translated_abstract": "SemEval-2024任务8引入了从多种语言和领域的不同大型语言模型（LLMs）中识别机器生成文本的挑战。该任务由三个子任务组成：单语和多语言的二元分类（子任务A）、多类别分类（子任务B）以及混合文本检测（子任务C）。本文重点关注子任务A和B。每个子任务都有三个数据集用于训练、开发和测试。为了解决这个任务，采用了两种方法：1）使用传统的机器学习（ML）和自然语言预处理（NLP）进行特征提取，2）对文本分类进行大型语言模型的微调。结果表明，转换器模型，特别是LoRA-RoBERTa，在效果上超过了传统的机器学习方法，多数投票在多语言环境中识别机器生成文本方面尤为有效。",
    "tldr": "本文研究了对多生成器、多领域和多语言机器生成文本的大型语言模型进行微调的任务。实验结果显示，转换器模型特别是LoRA-RoBERTa在效果上超过了传统的机器学习方法，对于多语言环境下的机器生成文本识别，多数投票方法尤为有效。",
    "en_tdlr": "This paper focuses on the fine-tuning of large language models for the detection of machine-generated text in multigenerator, multidomain, and multilingual settings. The results show that transformer models, especially LoRA-RoBERTa, outperform traditional machine learning methods, with majority voting being particularly effective for identifying machine-generated text in multilingual contexts."
}