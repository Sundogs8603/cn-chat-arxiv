{
    "title": "Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective. (arXiv:2401.15248v1 [cs.LG])",
    "abstract": "Pre-training is known to generate universal representations for downstream tasks in large-scale deep learning such as large language models. Existing literature, e.g., \\cite{kim2020adversarial}, empirically observe that the downstream tasks can inherit the adversarial robustness of the pre-trained model. We provide theoretical justifications for this robustness inheritance phenomenon. Our theoretical results reveal that feature purification plays an important role in connecting the adversarial robustness of the pre-trained model and the downstream tasks in two-layer neural networks. Specifically, we show that (i) with adversarial training, each hidden node tends to pick only one (or a few) feature; (ii) without adversarial training, the hidden nodes can be vulnerable to attacks. This observation is valid for both supervised pre-training and contrastive learning. With purified nodes, it turns out that clean training is enough to achieve adversarial robustness in downstream tasks.",
    "link": "http://arxiv.org/abs/2401.15248",
    "context": "Title: Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective. (arXiv:2401.15248v1 [cs.LG])\nAbstract: Pre-training is known to generate universal representations for downstream tasks in large-scale deep learning such as large language models. Existing literature, e.g., \\cite{kim2020adversarial}, empirically observe that the downstream tasks can inherit the adversarial robustness of the pre-trained model. We provide theoretical justifications for this robustness inheritance phenomenon. Our theoretical results reveal that feature purification plays an important role in connecting the adversarial robustness of the pre-trained model and the downstream tasks in two-layer neural networks. Specifically, we show that (i) with adversarial training, each hidden node tends to pick only one (or a few) feature; (ii) without adversarial training, the hidden nodes can be vulnerable to attacks. This observation is valid for both supervised pre-training and contrastive learning. With purified nodes, it turns out that clean training is enough to achieve adversarial robustness in downstream tasks.",
    "path": "papers/24/01/2401.15248.json",
    "total_tokens": 918,
    "translated_title": "在预训练中通过对抗训练改进表示：从理论角度出发",
    "translated_abstract": "预训练被认为可以为大规模深度学习中的下游任务生成通用表示，例如大型语言模型。现有文献例如\\cite{kim2020adversarial}经验性地观察到下游任务可以继承预训练模型的对抗鲁棒性。我们提供了这种鲁棒性继承现象的理论证明。我们的理论结果揭示了在两层神经网络中，特征净化在连接预训练模型的对抗鲁棒性和下游任务中起重要作用。具体来说，我们展示了(i)通过对抗训练，每个隐藏节点倾向于选择只有一个（或几个）特征；(ii)在没有对抗训练的情况下，隐藏节点可能容易受到攻击。这个观察对于监督预训练和对比学习都是有效的。通过净化节点，事实证明在下游任务中，仅仅进行干净训练就足以实现对抗鲁棒性。",
    "tldr": "该论文从理论角度探讨了在预训练中通过对抗训练改进表示的思路，并证明了特征净化在预训练模型的对抗鲁棒性和下游任务之间起到重要作用。",
    "en_tdlr": "This paper provides a theoretical perspective on improving representations via adversarial training in pre-training. The authors demonstrate that feature purification plays a crucial role in connecting the adversarial robustness of the pre-trained model and the downstream tasks in a two-layer neural network."
}