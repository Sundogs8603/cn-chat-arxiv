{
    "title": "Calibration-then-Calculation: A Variance Reduced Metric Framework in Deep Click-Through Rate Prediction Models. (arXiv:2401.16692v1 [cs.LG])",
    "abstract": "Deep learning has been widely adopted across various fields, but there has been little focus on evaluating the performance of deep learning pipelines. With the increased use of large datasets and complex models, it has become common to run the training process only once and compare the result to previous benchmarks. However, this procedure can lead to imprecise comparisons due to the variance in neural network evaluation metrics. The metric variance comes from the randomness inherent in the training process of deep learning pipelines. Traditional solutions such as running the training process multiple times are usually not feasible in deep learning due to computational limitations. In this paper, we propose a new metric framework, Calibrated Loss Metric, that addresses this issue by reducing the variance in its vanilla counterpart. As a result, the new metric has a higher accuracy to detect effective modeling improvement. Our approach is supported by theoretical justifications and exte",
    "link": "http://arxiv.org/abs/2401.16692",
    "context": "Title: Calibration-then-Calculation: A Variance Reduced Metric Framework in Deep Click-Through Rate Prediction Models. (arXiv:2401.16692v1 [cs.LG])\nAbstract: Deep learning has been widely adopted across various fields, but there has been little focus on evaluating the performance of deep learning pipelines. With the increased use of large datasets and complex models, it has become common to run the training process only once and compare the result to previous benchmarks. However, this procedure can lead to imprecise comparisons due to the variance in neural network evaluation metrics. The metric variance comes from the randomness inherent in the training process of deep learning pipelines. Traditional solutions such as running the training process multiple times are usually not feasible in deep learning due to computational limitations. In this paper, we propose a new metric framework, Calibrated Loss Metric, that addresses this issue by reducing the variance in its vanilla counterpart. As a result, the new metric has a higher accuracy to detect effective modeling improvement. Our approach is supported by theoretical justifications and exte",
    "path": "papers/24/01/2401.16692.json",
    "total_tokens": 874,
    "translated_title": "预校准和计算：深度点击率预测模型中一种方差减少的度量框架",
    "translated_abstract": "深度学习已经在各个领域得到广泛应用，但对于深度学习流水线的性能评估关注较少。随着大型数据集和复杂模型的使用增加，通常只运行一次训练过程并与之前的基准进行比较。然而，由于神经网络评估指标的方差，这种过程可能导致不精确的比较。指标方差来自深度学习流水线训练过程中固有的随机性。传统解决方案如多次运行训练过程在深度学习中往往不可行，因为计算限制。在本文中，我们提出了一种新的度量框架，称为校准损失度量，通过减少其基准模型中的方差来解决这个问题。结果，这个新的度量方法具有更高的准确性来检测有效建模改进。我们的方法得到了理论上的证明和实验证明。",
    "tldr": "本文提出一种新的度量框架，通过减少方差来改进深度学习流水线的性能评估，具有更高的准确性来检测有效建模改进。",
    "en_tdlr": "This paper proposes a new metric framework to improve the performance evaluation of deep learning pipelines by reducing variance, resulting in higher accuracy in detecting effective modeling improvements."
}