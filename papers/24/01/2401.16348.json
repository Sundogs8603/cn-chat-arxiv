{
    "title": "Improving the TENOR of Labeling: Re-evaluating Topic Models for Content Analysis",
    "abstract": "arXiv:2401.16348v2 Announce Type: replace  Abstract: Topic models are a popular tool for understanding text collections, but their evaluation has been a point of contention. Automated evaluation metrics such as coherence are often used, however, their validity has been questioned for neural topic models (NTMs) and can overlook a models benefits in real world applications. To this end, we conduct the first evaluation of neural, supervised and classical topic models in an interactive task based setting. We combine topic models with a classifier and test their ability to help humans conduct content analysis and document annotation. From simulated, real user and expert pilot studies, the Contextual Neural Topic Model does the best on cluster evaluation metrics and human evaluations; however, LDA is competitive with two other NTMs under our simulated experiment and user study results, contrary to what coherence scores suggest. We show that current automated metrics do not provide a complete",
    "link": "https://arxiv.org/abs/2401.16348",
    "context": "Title: Improving the TENOR of Labeling: Re-evaluating Topic Models for Content Analysis\nAbstract: arXiv:2401.16348v2 Announce Type: replace  Abstract: Topic models are a popular tool for understanding text collections, but their evaluation has been a point of contention. Automated evaluation metrics such as coherence are often used, however, their validity has been questioned for neural topic models (NTMs) and can overlook a models benefits in real world applications. To this end, we conduct the first evaluation of neural, supervised and classical topic models in an interactive task based setting. We combine topic models with a classifier and test their ability to help humans conduct content analysis and document annotation. From simulated, real user and expert pilot studies, the Contextual Neural Topic Model does the best on cluster evaluation metrics and human evaluations; however, LDA is competitive with two other NTMs under our simulated experiment and user study results, contrary to what coherence scores suggest. We show that current automated metrics do not provide a complete",
    "path": "papers/24/01/2401.16348.json",
    "total_tokens": 886,
    "translated_title": "改进标签的TENOR：重新评估用于内容分析的主题模型",
    "translated_abstract": "主题模型是理解文本集合的一种流行工具，但它们的评估一直是一个争论点。自动化评估指标如连贯性经常被使用，然而对于神经主题模型（NTMs）他们的有效性受到质疑，可能忽略了模型在现实世界应用中的益处。为此，我们在基于交互式任务的环境中首次评估了神经、监督和经典主题模型。我们将主题模型与分类器结合，并测试它们帮助人类进行内容分析和文档注释的能力。从模拟、真实用户和专家的试点研究中，上下文神经主题模型在聚类评估指标和人类评估方面表现最佳；然而，LDA在我们的模拟实验和用户研究结果中与另外两种NTMs竞争激烈，与连贯性分数所暗示的情况相反。我们表明当前的自动化指标并不提供完整的评估。",
    "tldr": "本研究重新评估了神经、监督和经典主题模型在内容分析中的效果，结果显示上下文神经主题模型在聚类评估和人类评估方面表现最佳。"
}