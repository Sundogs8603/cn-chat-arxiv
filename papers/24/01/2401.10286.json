{
    "title": "Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v1 [cs.CL])",
    "abstract": "While the alignment between tasks and training corpora is a fundamental consensus in the application of language models, our series of experiments and the metrics we designed reveal that code-based Large Language Models (LLMs) significantly outperform models trained on data that is closely matched to the tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to Chinese hallucinations, models exhibiting fewer linguistic features of the Chinese language achieve better performance. Our experimental results can be easily replicated in Chinese data processing tasks, such as preparing data for Retrieval-Augmented Generation (RAG), by simply replacing the base model with a code-based model. Additionally, our research offers a distinct perspective for discussion on the philosophical \"Chinese Room\" thought experiment.",
    "link": "http://arxiv.org/abs/2401.10286",
    "context": "Title: Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v1 [cs.CL])\nAbstract: While the alignment between tasks and training corpora is a fundamental consensus in the application of language models, our series of experiments and the metrics we designed reveal that code-based Large Language Models (LLMs) significantly outperform models trained on data that is closely matched to the tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to Chinese hallucinations, models exhibiting fewer linguistic features of the Chinese language achieve better performance. Our experimental results can be easily replicated in Chinese data processing tasks, such as preparing data for Retrieval-Augmented Generation (RAG), by simply replacing the base model with a code-based model. Additionally, our research offers a distinct perspective for discussion on the philosophical \"Chinese Room\" thought experiment.",
    "path": "papers/24/01/2401.10286.json",
    "total_tokens": 774,
    "translated_title": "中文数据处理中的佼佼者：英文代码模型",
    "translated_abstract": "尽管在语言模型的应用中，任务与训练语料之间的对齐是一个基本的共识，但我们的一系列实验和我们设计的评估指标表明，基于代码的大型语言模型(LLMs)在非编程中文任务中的表现明显优于与任务紧密匹配的训练数据。此外，在对中文幻觉敏感程度较高的任务中，展示较少中文语言特征的模型表现更好。我们的实验结果可以通过简单地用代码模型替换基础模型，在中文数据处理任务中，如为检索增强生成(RAG)准备数据，很容易得到复制。此外，我们的研究为讨论“中文房间”思想实验提供了独特的视角。",
    "tldr": "在中文数据处理中，基于代码的语言模型在非编程中文任务中表现出色，尤其是在对中文幻觉敏感的任务中。此研究为讨论“中文房间”思想实验提供了独特的视角。",
    "en_tdlr": "Code-based language models perform exceptionally well in non-coding Chinese tasks, particularly those with high sensitivity to Chinese hallucinations. This research offers a unique perspective for discussing the \"Chinese Room\" thought experiment."
}