{
    "title": "Zero Bubble Pipeline Parallelism. (arXiv:2401.10241v1 [cs.DC])",
    "abstract": "Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles under synchronous training semantics. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the 1F1B schedule up to 23% in throughput under a simila",
    "link": "http://arxiv.org/abs/2401.10241",
    "context": "Title: Zero Bubble Pipeline Parallelism. (arXiv:2401.10241v1 [cs.DC])\nAbstract: Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles under synchronous training semantics. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the 1F1B schedule up to 23% in throughput under a simila",
    "path": "papers/24/01/2401.10241.json",
    "total_tokens": 975,
    "translated_title": "零泡沫管道并行性",
    "translated_abstract": "管道并行性是大规模分布式训练的关键组成部分之一，然而其效率受到被视为不可避免的管道泡沫的影响。在这项工作中，我们引入了一种调度策略，据我们所知，这是第一次在同步训练语义下成功实现零管道泡沫。这个改进背后的关键思想是将反向计算分为两部分，一部分计算输入的梯度，另一部分计算参数的梯度。基于这个思想，我们手工设计了新颖的管道调度方法，显著优于基线方法。我们还开发了一个根据特定的模型配置和内存限制自动找到最优调度的算法。此外，为了真正实现零泡沫，我们引入了一种新颖的技术，在优化器步骤中绕过同步操作。实验评估表明，我们的方法在吞吐量方面比1F1B调度高出多达23%。",
    "tldr": "本论文介绍了一种新的调度策略，在同步训练语义下成功实现了零管道泡沫，通过将反向计算分为两部分，设计了优于基线方法的新颖管道调度。此外，还提出了一种自动找到最优调度的算法，并引入新颖技术绕过同步操作实现零泡沫。实验证明，在相似条件下，本方法在吞吐量方面优于1F1B调度23%。",
    "en_tdlr": "This paper introduces a new scheduling strategy that achieves zero pipeline bubbles under synchronous training semantics, by splitting the backward computation into two parts and designing novel pipeline schedules that outperform baseline methods. It also presents an algorithm to automatically find optimal schedules and a technique to bypass synchronizations for achieving zero bubbles. Experimental evaluations show that the proposed method has a 23% higher throughput than the 1F1B schedule under similar conditions."
}