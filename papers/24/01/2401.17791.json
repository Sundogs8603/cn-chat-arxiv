{
    "title": "Graph Transformers without Positional Encodings",
    "abstract": "Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show",
    "link": "https://arxiv.org/abs/2401.17791",
    "context": "Title: Graph Transformers without Positional Encodings\nAbstract: Recently, Transformers for graph representation learning have become increasingly popular, achieving state-of-the-art performance on a wide-variety of datasets, either alone or in combination with message-passing graph neural networks (MP-GNNs). Infusing graph inductive-biases in the innately structure-agnostic transformer architecture in the form of structural or positional encodings (PEs) is key to achieving these impressive results. However, designing such encodings is tricky and disparate attempts have been made to engineer such encodings including Laplacian eigenvectors, relative random-walk probabilities (RRWP), spatial encodings, centrality encodings, edge encodings etc. In this work, we argue that such encodings may not be required at all, provided the attention mechanism itself incorporates information about the graph structure. We introduce Eigenformer, which uses a novel spectrum-aware attention mechanism cognizant of the Laplacian spectrum of the graph, and empirically show",
    "path": "papers/24/01/2401.17791.json",
    "total_tokens": 790,
    "translated_title": "不带位置编码的图变压器",
    "translated_abstract": "最近，用于图表示学习的变压器越来越受欢迎，在各种数据集上取得了最先进的性能，无论是单独使用还是与消息传递图神经网络（MP-GNN）结合。将图归纳偏见融入天然与结构无关的变压器架构中，以结构或位置编码（PEs）的形式，是实现这些令人印象深刻的结果的关键。然而，设计这样的编码是棘手的，人们已经提出了不同的尝试来设计这样的编码，包括拉普拉斯特征向量、相对随机行走概率（RRWP）、空间编码、中心度编码、边缘编码等。在这项工作中，我们认为这些编码可能根本不需要，只要注意机制本身包含有关图结构的信息。我们介绍了Eigenformer，它使用一种新颖的谱感知注意机制，了解图的拉普拉斯谱，并通过实验证明",
    "tldr": "本文介绍了一种不需要位置编码的图变压器模型，该模型通过注意机制本身包含图结构信息，并通过实验证明了其有效性。"
}