{
    "title": "Procedural Fairness in Machine Learning",
    "abstract": "arXiv:2404.01877v1 Announce Type: new  Abstract: Fairness in machine learning (ML) has received much attention. However, existing studies have mainly focused on the distributive fairness of ML models. The other dimension of fairness, i.e., procedural fairness, has been neglected. In this paper, we first define the procedural fairness of ML models, and then give formal definitions of individual and group procedural fairness. We propose a novel metric to evaluate the group procedural fairness of ML models, called $GPF_{FAE}$, which utilizes a widely used explainable artificial intelligence technique, namely feature attribution explanation (FAE), to capture the decision process of the ML models. We validate the effectiveness of $GPF_{FAE}$ on a synthetic dataset and eight real-world datasets. Our experiments reveal the relationship between procedural and distributive fairness of the ML model. Based on our analysis, we propose a method for identifying the features that lead to the procedur",
    "link": "https://arxiv.org/abs/2404.01877",
    "context": "Title: Procedural Fairness in Machine Learning\nAbstract: arXiv:2404.01877v1 Announce Type: new  Abstract: Fairness in machine learning (ML) has received much attention. However, existing studies have mainly focused on the distributive fairness of ML models. The other dimension of fairness, i.e., procedural fairness, has been neglected. In this paper, we first define the procedural fairness of ML models, and then give formal definitions of individual and group procedural fairness. We propose a novel metric to evaluate the group procedural fairness of ML models, called $GPF_{FAE}$, which utilizes a widely used explainable artificial intelligence technique, namely feature attribution explanation (FAE), to capture the decision process of the ML models. We validate the effectiveness of $GPF_{FAE}$ on a synthetic dataset and eight real-world datasets. Our experiments reveal the relationship between procedural and distributive fairness of the ML model. Based on our analysis, we propose a method for identifying the features that lead to the procedur",
    "path": "papers/24/04/2404.01877.json",
    "total_tokens": 926,
    "translated_title": "机器学习中的程序公平性",
    "translated_abstract": "机器学习中的公平性一直受到广泛关注，然而现有研究主要集中在模型的分配公平性上。另一个公平性维度，即程序公平性，却被忽视了。本文首先定义了机器学习模型的程序公平性，然后给出了个体和群体程序公平性的正式定义。我们提出了一个新的度量标准来评估机器学习模型的群体程序公平性，称为$GPF_{FAE}$，它利用了一个广泛使用的可解释人工智能技术，即特征归因解释（FAE），来捕捉机器学习模型的决策过程。我们在一个合成数据集和八个真实数据集上验证了$GPF_{FAE}$的有效性。我们的实验揭示了机器学习模型的程序和分配公平性之间的关系。基于我们的分析，我们提出了一种识别导致程序性不公平问题的特征的方法。",
    "tldr": "本文定义了机器学习模型的程序公平性，提出了评估群体程序公平性的新度量标准$GPF_{FAE}$，并使用特征归因解释来捕捉决策过程，实验证实了其有效性，同时揭示了程序和分配公平性之间的关系，并提出了一种方法来识别导致程序性不公平的特征。",
    "en_tdlr": "This paper defines procedural fairness in machine learning, proposes a new metric $GPF_{FAE}$ to evaluate group procedural fairness, validates its effectiveness using feature attribution explanation, reveals the relationship between procedural and distributive fairness, and introduces a method to identify features leading to procedural unfairness."
}