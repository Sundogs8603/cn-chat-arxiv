{
    "title": "Benchmark Transparency: Measuring the Impact of Data on Evaluation",
    "abstract": "arXiv:2404.00748v1 Announce Type: cross  Abstract: In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and perplexity.   We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric.   In a second set of experiments, we demonstrate that the impact of data on eval",
    "link": "https://arxiv.org/abs/2404.00748",
    "context": "Title: Benchmark Transparency: Measuring the Impact of Data on Evaluation\nAbstract: arXiv:2404.00748v1 Announce Type: cross  Abstract: In this paper we present an exploratory research on quantifying the impact that data distribution has on the performance and evaluation of NLP models. We propose an automated framework that measures the data point distribution across 6 different dimensions: ambiguity, difficulty, discriminability, length, noise, and perplexity.   We use disproportional stratified sampling to measure how much the data distribution affects absolute (Acc/F1) and relative (Rank) model performance. We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135 different models (125 on SQUAD and 10 on MNLI). We demonstrate that without explicit control of the data distribution, standard evaluation frameworks are inconsistent and unreliable. We find that the impact of the data is statistically significant and is often larger than the impact of changing the metric.   In a second set of experiments, we demonstrate that the impact of data on eval",
    "path": "papers/24/04/2404.00748.json",
    "total_tokens": 861,
    "translated_title": "基准透明度：衡量数据对评估的影响",
    "translated_abstract": "在本文中，我们介绍了一项关于量化数据分布对自然语言处理模型性能和评估影响的初步研究。我们提出了一个自动化框架，可以衡量数据点在6个不同维度上的分布：模糊度、困难度、可区分性、长度、噪声和困惑度。我们使用不均衡分层抽样来衡量数据分布对绝对（准确率/F1）和相对（排名）模型性能的影响。我们在两个不同的数据集（SQUAD和MNLI）上进行实验，测试了总共135种不同的模型（125种在SQUAD上，10种在MNLI上）。我们证明了在没有明确控制数据分布的情况下，标准评估框架是不一致和不可靠的。我们发现数据的影响在统计上是显著的，并且通常比更改度量的影响更大。",
    "tldr": "本研究提出了一种自动化框架，可以衡量数据分布对自然语言处理模型性能和评估的影响，揭示了数据分布的重要性和对评估框架的影响。",
    "en_tdlr": "This study introduces an automated framework to quantify the impact of data distribution on the performance and evaluation of NLP models, revealing the significance of data distribution and its effects on evaluation frameworks."
}