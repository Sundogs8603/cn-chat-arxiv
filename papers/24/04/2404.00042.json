{
    "title": "Stochastic Optimization with Constraints: A Non-asymptotic Instance-Dependent Analysis",
    "abstract": "arXiv:2404.00042v1 Announce Type: cross  Abstract: We consider the problem of stochastic convex optimization under convex constraints. We analyze the behavior of a natural variance reduced proximal gradient (VRPG) algorithm for this problem. Our main result is a non-asymptotic guarantee for VRPG algorithm. Contrary to minimax worst case guarantees, our result is instance-dependent in nature. This means that our guarantee captures the complexity of the loss function, the variability of the noise, and the geometry of the constraint set. We show that the non-asymptotic performance of the VRPG algorithm is governed by the scaled distance (scaled by $\\sqrt{N}$) between the solutions of the given problem and that of a certain small perturbation of the given problem -- both solved under the given convex constraints; here, $N$ denotes the number of samples. Leveraging a well-established connection between local minimax lower bounds and solutions to perturbed problems, we show that as $N \\right",
    "link": "https://arxiv.org/abs/2404.00042",
    "context": "Title: Stochastic Optimization with Constraints: A Non-asymptotic Instance-Dependent Analysis\nAbstract: arXiv:2404.00042v1 Announce Type: cross  Abstract: We consider the problem of stochastic convex optimization under convex constraints. We analyze the behavior of a natural variance reduced proximal gradient (VRPG) algorithm for this problem. Our main result is a non-asymptotic guarantee for VRPG algorithm. Contrary to minimax worst case guarantees, our result is instance-dependent in nature. This means that our guarantee captures the complexity of the loss function, the variability of the noise, and the geometry of the constraint set. We show that the non-asymptotic performance of the VRPG algorithm is governed by the scaled distance (scaled by $\\sqrt{N}$) between the solutions of the given problem and that of a certain small perturbation of the given problem -- both solved under the given convex constraints; here, $N$ denotes the number of samples. Leveraging a well-established connection between local minimax lower bounds and solutions to perturbed problems, we show that as $N \\right",
    "path": "papers/24/04/2404.00042.json",
    "total_tokens": 894,
    "translated_title": "具有约束的随机优化：非渐近实例相关分析",
    "translated_abstract": "我们考虑了随机凸优化在凸约束下的问题。我们分析了一种适用于这个问题的自然方差减少的近端梯度（VRPG）算法的行为。我们的主要结果是VRPG算法的非渐近保证。与极小值最坏情况保证相反，我们的结果是基于实例的。这意味着我们的保证捕捉了损失函数的复杂性，噪声的变异性和约束集的几何性。我们表明，VRPG算法的非渐近性能受给定问题的解和给定凸约束下解决的特定小扰动问题的解之间的缩放距离（由$\\sqrt{N}$缩放）的控制，这里，$N$表示样本数。利用局部极小值下界和扰动问题解之间的一种成熟联系，我们表明当$N \\rightarrow +\\infty$时，极小值存在并且受指定凸约束的约束。",
    "tldr": "该论文研究了具有凸约束的随机凸优化问题，提出了一种非渐近保证的VRPG算法，并展示了其性能受到解以及带凸约束解决的问题的缩放距离控制。",
    "en_tdlr": "The paper investigates stochastic convex optimization under convex constraints, proposes a non-asymptotic guarantee for the VRPG algorithm, and demonstrates that its performance is governed by the scaled distance between the solutions and perturbed problems under convex constraints."
}