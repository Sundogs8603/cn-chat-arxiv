{
    "title": "A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?",
    "abstract": "arXiv:2404.01775v1 Announce Type: cross  Abstract: The ability to detect unfamiliar or unexpected images is essential for safe deployment of computer vision systems. In the context of classification, the task of detecting images outside of a model's training domain is known as out-of-distribution (OOD) detection. While there has been a growing research interest in developing post-hoc OOD detection methods, there has been comparably little discussion around how these methods perform when the underlying classifier is not trained on a clean, carefully curated dataset. In this work, we take a closer look at 20 state-of-the-art OOD detection methods in the (more realistic) scenario where the labels used to train the underlying classifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive experiments across different datasets, noise types & levels, architectures and checkpointing strategies provide insights into the effect of class label noise on OOD detection, and show tha",
    "link": "https://arxiv.org/abs/2404.01775",
    "context": "Title: A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?\nAbstract: arXiv:2404.01775v1 Announce Type: cross  Abstract: The ability to detect unfamiliar or unexpected images is essential for safe deployment of computer vision systems. In the context of classification, the task of detecting images outside of a model's training domain is known as out-of-distribution (OOD) detection. While there has been a growing research interest in developing post-hoc OOD detection methods, there has been comparably little discussion around how these methods perform when the underlying classifier is not trained on a clean, carefully curated dataset. In this work, we take a closer look at 20 state-of-the-art OOD detection methods in the (more realistic) scenario where the labels used to train the underlying classifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive experiments across different datasets, noise types & levels, architectures and checkpointing strategies provide insights into the effect of class label noise on OOD detection, and show tha",
    "path": "papers/24/04/2404.01775.json",
    "total_tokens": 902,
    "translated_title": "房间里的一只吵闹的大象：您的离群检测器对标签噪音鲁棒吗？",
    "translated_abstract": "多数情况下，检测计算机视觉系统中的陌生或意外图像是确保安全部署的关键。在分类领域，检测模型训练领域外图像的任务被称为离群检测（OOD检测）。尽管人们越来越关注发展事后OOD检测方法，但对于在基础分类器未经过干净、精心筛选数据集训练时这些方法表现如何的讨论相对较少。本研究在20种最新的OOD检测方法中更为现实的情况下进行了深入探讨，在此情况下，用于训练基础分类器的标签不可靠（例如，众包或网络抓取标签）。通过在不同数据集、噪音类型和级别、架构和检查点策略上进行广泛实验，我们研究了类别标签噪音对OOD检测的影响，并表明...",
    "tldr": "本研究探讨了当下训练分类器的标签不可靠时，20种最新OOD检测方法在离群检测中的表现，为了解类别标签噪音对OOD检测的影响提供了深入见解。",
    "en_tdlr": "This study investigates the performance of 20 state-of-the-art OOD detection methods in the scenario where the labels used to train the underlying classifier are unreliable, providing insights into the effect of class label noise on OOD detection."
}