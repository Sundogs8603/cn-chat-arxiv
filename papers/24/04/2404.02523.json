{
    "title": "Text-driven Affordance Learning from Egocentric Vision",
    "abstract": "arXiv:2404.02523v1 Announce Type: cross  Abstract: Visual affordance learning is a key component for robots to understand how to interact with objects. Conventional approaches in this field rely on pre-defined objects and actions, falling short of capturing diverse interactions in realworld scenarios. The key idea of our approach is employing textual instruction, targeting various affordances for a wide range of objects. This approach covers both hand-object and tool-object interactions. We introduce text-driven affordance learning, aiming to learn contact points and manipulation trajectories from an egocentric view following textual instruction. In our task, contact points are represented as heatmaps, and the manipulation trajectory as sequences of coordinates that incorporate both linear and rotational movements for various manipulations. However, when we gather data for this task, manual annotations of these diverse interactions are costly. To this end, we propose a pseudo dataset c",
    "link": "https://arxiv.org/abs/2404.02523",
    "context": "Title: Text-driven Affordance Learning from Egocentric Vision\nAbstract: arXiv:2404.02523v1 Announce Type: cross  Abstract: Visual affordance learning is a key component for robots to understand how to interact with objects. Conventional approaches in this field rely on pre-defined objects and actions, falling short of capturing diverse interactions in realworld scenarios. The key idea of our approach is employing textual instruction, targeting various affordances for a wide range of objects. This approach covers both hand-object and tool-object interactions. We introduce text-driven affordance learning, aiming to learn contact points and manipulation trajectories from an egocentric view following textual instruction. In our task, contact points are represented as heatmaps, and the manipulation trajectory as sequences of coordinates that incorporate both linear and rotational movements for various manipulations. However, when we gather data for this task, manual annotations of these diverse interactions are costly. To this end, we propose a pseudo dataset c",
    "path": "papers/24/04/2404.02523.json",
    "total_tokens": 773,
    "translated_title": "从主体视角学习基于文本驱动的可操作性",
    "translated_abstract": "视觉可操作性学习是机器人理解如何与物体交互的关键组成部分。我们的方法的关键思想是利用文本指导，针对各种物体的各种可操作性。该方法涵盖手-物体和工具-物体交互。我们引入了文本驱动的可操作性学习，旨在从主体视角根据文本指导学习接触点和操作轨迹。在我们的任务中，接触点被表示为热图，操作轨迹被表示为包含各种操作的线性和旋转运动的坐标序列。然而，当我们为这个任务收集数据时，这些多样化交互的手动注释是昂贵的。因此，我们提出了一个伪数据集",
    "tldr": "该论文提出了一种通过文本指导从主体视角学习可操作性的方法，涵盖手-物体和工具-物体交互，旨在学习接触点和操作轨迹。",
    "en_tdlr": "This paper presents a method of learning affordances from egocentric vision guided by text, covering both hand-object and tool-object interactions, aiming to learn contact points and manipulation trajectories."
}