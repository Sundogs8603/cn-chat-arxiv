{
    "title": "MODNO: Multi Operator Learning With Distributed Neural Operators",
    "abstract": "arXiv:2404.02892v1 Announce Type: new  Abstract: The study of operator learning involves the utilization of neural networks to approximate operators. Traditionally, the focus has been on single-operator learning (SOL). However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL). In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs. Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON). The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input fu",
    "link": "https://arxiv.org/abs/2404.02892",
    "context": "Title: MODNO: Multi Operator Learning With Distributed Neural Operators\nAbstract: arXiv:2404.02892v1 Announce Type: new  Abstract: The study of operator learning involves the utilization of neural networks to approximate operators. Traditionally, the focus has been on single-operator learning (SOL). However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL). In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs. Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON). The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input fu",
    "path": "papers/24/04/2404.02892.json",
    "total_tokens": 797,
    "translated_title": "MODNO: 具有分布式神经算子的多算子学习",
    "translated_abstract": "运算符学习的研究涉及利用神经网络来逼近算子。传统上，重点放在单算子学习（SOL）上。然而，最近的进展迅速将其扩展到包含使用具有数百万或数十亿可训练参数的基础模型来逼近多算子，从而导致了多算子学习（MOL）的研究。在本文中，我们提出了一种新颖的分布式训练方法，旨在使单个神经算子能够有效地处理多算子学习挑战，而不会产生额外的平均成本。我们的方法适用于各种类似Chen-Chen型神经算子，如深算子神经网络（DON）。其核心思想是独立学习每个算子的输出基函数，使用其专用数据，同时集中学习输入fu。",
    "tldr": "本文提出了一种针对多算子学习挑战的新型分布式训练方法，有效地实现单个神经算子处理多算子学习问题，而不增加额外平均成本。",
    "en_tdlr": "This paper proposes a novel distributed training approach aimed at effectively enabling a single neural operator to tackle multi-operator learning challenges without incurring additional average costs."
}