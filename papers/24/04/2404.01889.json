{
    "title": "RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement",
    "abstract": "arXiv:2404.01889v1 Announce Type: cross  Abstract: In this paper we propose a novel modification of Contrastive Language-Image Pre-Training (CLIP) guidance for the task of unsupervised backlit image enhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which learns a prompt pair by constraining the text-image similarity between a prompt (negative/positive sample) and a corresponding image (backlit image/well-lit image) in the CLIP embedding space. Learned prompts then guide an image enhancement network. Based on the CLIP-LIT framework, we propose two novel methods for CLIP guidance. First, we show that instead of tuning prompts in the space of text embeddings, it is possible to directly tune their embeddings in the latent space without any loss in quality. This accelerates training and potentially enables the use of additional encoders that do not have a text encoder. Second, we propose a novel approach that does not require any prompt tuning. Instead, based on CLIP e",
    "link": "https://arxiv.org/abs/2404.01889",
    "context": "Title: RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image Enhancement\nAbstract: arXiv:2404.01889v1 Announce Type: cross  Abstract: In this paper we propose a novel modification of Contrastive Language-Image Pre-Training (CLIP) guidance for the task of unsupervised backlit image enhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which learns a prompt pair by constraining the text-image similarity between a prompt (negative/positive sample) and a corresponding image (backlit image/well-lit image) in the CLIP embedding space. Learned prompts then guide an image enhancement network. Based on the CLIP-LIT framework, we propose two novel methods for CLIP guidance. First, we show that instead of tuning prompts in the space of text embeddings, it is possible to directly tune their embeddings in the latent space without any loss in quality. This accelerates training and potentially enables the use of additional encoders that do not have a text encoder. Second, we propose a novel approach that does not require any prompt tuning. Instead, based on CLIP e",
    "path": "papers/24/04/2404.01889.json",
    "total_tokens": 889,
    "translated_title": "RAVE: CLIP引导的残差向量嵌入用于背光图像增强",
    "translated_abstract": "在本文中，我们提出了一种对反差异式语言-图像预训练（CLIP）指导进行了新颖修改的方法，用于无监督背光图像增强任务。我们的工作建立在最先进的CLIP-LIT方法基础之上，该方法通过约束在CLIP嵌入空间中一个提示对之间的文本-图像相似性来学习一个提示对（负/正样本）和相应图像（背光图像/光照良好的图像）。学习的提示然后指导图像增强网络。基于CLIP-LIT框架，我们提出了两种CLIP引导的新方法。首先，我们展示了在文本嵌入空间调整提示而不损失质量的可能性，从而可以直接在潜在空间中调整它们的嵌入，加快训练并潜在地实现使用没有文本编码器的其他编码器。其次，我们提出了一种不需要任何提示调整的新方法。",
    "tldr": "该论文提出了一种用于背光图像增强的CLIP引导方法RAVE，通过残差向量嵌入和提示调整的新颖方法，加快了训练并提高了质量。",
    "en_tdlr": "This paper presents RAVE, a CLIP-guided method for backlit image enhancement, which accelerates training and improves quality through residual vector embedding and novel prompt tuning."
}