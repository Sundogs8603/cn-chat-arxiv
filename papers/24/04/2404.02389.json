{
    "title": "On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL",
    "abstract": "arXiv:2404.02389v1 Announce Type: cross  Abstract: Structured data, prevalent in tables, databases, and knowledge graphs, poses a significant challenge in its representation. With the advent of large language models (LLMs), there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly model structure, often as a graph. Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear. This work investigates the linear handling of structured data in encoder-decoder language models, specifically T5. Our findings reveal the model's ability to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing. We also uncover insights into the model's internal mechanisms, including the ego-centric nature of structure nod",
    "link": "https://arxiv.org/abs/2404.02389",
    "context": "Title: On Linearizing Structured Data in Encoder-Decoder Language Models: Insights from Text-to-SQL\nAbstract: arXiv:2404.02389v1 Announce Type: cross  Abstract: Structured data, prevalent in tables, databases, and knowledge graphs, poses a significant challenge in its representation. With the advent of large language models (LLMs), there has been a shift towards linearization-based methods, which process structured data as sequential token streams, diverging from approaches that explicitly model structure, often as a graph. Crucially, there remains a gap in our understanding of how these linearization-based methods handle structured data, which is inherently non-linear. This work investigates the linear handling of structured data in encoder-decoder language models, specifically T5. Our findings reveal the model's ability to mimic human-designed processes such as schema linking and syntax prediction, indicating a deep, meaningful learning of structure beyond simple token sequencing. We also uncover insights into the model's internal mechanisms, including the ego-centric nature of structure nod",
    "path": "papers/24/04/2404.02389.json",
    "total_tokens": 830,
    "translated_title": "在编码器-解码器语言模型中线性化结构化数据：来自文本到SQL的启示",
    "translated_abstract": "结构化数据在表格、数据库和知识图中广泛存在，在其表示方面存在重大挑战。 随着大型语言模型（LLMs）的出现，人们开始转向基于线性化的方法，该方法将结构化数据处理为顺序标记流，而不是作为图形明确地建模的方法。 本文探讨了编码器-解码器语言模型（特别是T5）中对结构化数据进行线性处理的情况。 我们的研究发现模型能够模仿人类设计的流程，比如模式链接和语法预测，表明模型对结构的深刻、有意义的学习远远超过简单的标记排序。 我们还发现了模型内部机制的见解，包括结构节点的以自我为中心的特性。",
    "tldr": "本研究调查了编码器-解码器语言模型中线性处理结构化数据的方法，发现模型能够模仿人类设计的流程，学习结构的深刻含义，揭示了模型内部机制的一些见解。",
    "en_tdlr": "This study investigates the linear handling of structured data in encoder-decoder language models, revealing the model's ability to mimic human-designed processes, learn the deep meanings of structure, and uncover insights into the model's internal mechanisms."
}