{
    "title": "Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding",
    "abstract": "arXiv:2404.00862v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source LLMs are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of LLMs on low-resource languages by full-parameter fine-tuning with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual transfer on English-dominated open-source LLM. To effectively enhance the model's proficiency in Traditional Chines",
    "link": "https://arxiv.org/abs/2404.00862",
    "context": "Title: Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie Embedding\nAbstract: arXiv:2404.00862v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated exceptional performance in various NLP applications. However, the majority of existing open-source LLMs are pre-trained primarily on English data and little part of other languages. This deficiency in multilingual training data results in suboptimal performance when applied to languages with fewer available resources. Furthermore, enhancing the performance of LLMs on low-resource languages by full-parameter fine-tuning with additional data requires substantial computational resources, posing computational barriers for research organizations and individual researchers. Consequently, several techniques such as parameter-efficient tuning and advanced embedding initialization have been proposed to address these challenges. In this work, we combine them to facilitate cross-lingual transfer on English-dominated open-source LLM. To effectively enhance the model's proficiency in Traditional Chines",
    "path": "papers/24/04/2404.00862.json",
    "total_tokens": 779,
    "translated_title": "基于QLoRA和Zip-tie嵌入的双语迁移学习Bailong",
    "translated_abstract": "大型语言模型（LLMs）在各种自然语言处理应用中展现出卓越的性能。然而，现有大多数开源LLMs主要在英语数据上进行预训练，对其他语言的覆盖较少。多语言训练数据的不足导致在应用到资源较少的语言时性能亚优。此外，通过使用额外数据对低资源语言进行全参数微调以提高LLMs性能需要大量计算资源，这给研究机构和个人研究人员带来了计算障碍。因此，提出了一些技术，如参数高效微调和先进的嵌入初始化来解决这些挑战。本文将它们结合起来，以促进对以英语为主导的开源LLMs进行跨语言迁移。",
    "tldr": "通过结合参数高效微调和先进的嵌入初始化技术，本研究在以英语为主导的开源LLMs上实现了跨语言迁移学习。",
    "en_tdlr": "This study achieves cross-lingual transfer learning on English-dominated open-source LLMs by combining parameter-efficient tuning and advanced embedding initialization techniques."
}