{
    "title": "On the Scalability of Diffusion-based Text-to-Image Generation",
    "abstract": "arXiv:2404.02883v1 Announce Type: cross  Abstract: Scaling up model and data size has been quite successful for the evolution of LLMs. However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored. It is also unclear how to efficiently scale the model for better performance at reduced cost. The different training settings and expensive training cost make a fair model comparison extremely difficult. In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images. For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs. And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel nu",
    "link": "https://arxiv.org/abs/2404.02883",
    "context": "Title: On the Scalability of Diffusion-based Text-to-Image Generation\nAbstract: arXiv:2404.02883v1 Announce Type: cross  Abstract: Scaling up model and data size has been quite successful for the evolution of LLMs. However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored. It is also unclear how to efficiently scale the model for better performance at reduced cost. The different training settings and expensive training cost make a fair model comparison extremely difficult. In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images. For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs. And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel nu",
    "path": "papers/24/04/2404.02883.json",
    "total_tokens": 892,
    "translated_title": "关于基于扩散的文本到图像生成方法的可扩展性",
    "translated_abstract": "扩大模型和数据规模对于LLMs的发展取得了相当大的成功。然而，尚未充分探讨基于扩散的文本到图像（T2I）模型的扩展法则。如何有效地扩展模型以在降低成本的情况下提高性能也不太清楚。不同的训练设置和昂贵的训练成本使得进行公平的模型比较变得极为困难。在本研究中，我们通过对去噪骨干和训练集的大量而严格的消融实验，对扩散型T2I模型的扩展特性进行了实证研究，包括在数据集上达到600M图像的范围内训练参数从0.4B到4B的缩放UNet和Transformer变体。对于模型的扩展，我们发现跨关注的位置和数量区分了现有UNet设计的性能。增加transformer块对于提高文本-图像对齐比增加通道数更具参数效率。",
    "tldr": "本研究通过针对扩散型T2I模型进行消融实验，发现增加transformer块对于改善文本-图像对齐比增加通道数更具参数效率。",
    "en_tdlr": "This study empirically investigates the scaling properties of diffusion-based text-to-image (T2I) models, and finds that increasing transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel number for the existing UNet designs."
}