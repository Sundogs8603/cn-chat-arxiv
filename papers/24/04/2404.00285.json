{
    "title": "Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model",
    "abstract": "arXiv:2404.00285v1 Announce Type: cross  Abstract: Deploying deep models in real-world scenarios entails a number of challenges, including computational efficiency and real-world (e.g., long-tailed) data distributions. We address the combined challenge of learning long-tailed distributions using highly resource-efficient binary neural networks as backbones. Specifically, we propose a calibrate-and-distill framework that uses off-the-shelf pretrained full-precision models trained on balanced datasets to use as teachers for distillation when learning binary networks on long-tailed datasets. To better generalize to various datasets, we further propose a novel adversarial balancing among the terms in the objective function and an efficient multiresolution learning scheme. We conducted the largest empirical study in the literature using 15 datasets, including newly derived long-tailed datasets from existing balanced datasets, and show that our proposed method outperforms prior art by large ",
    "link": "https://arxiv.org/abs/2404.00285",
    "context": "Title: Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained Model\nAbstract: arXiv:2404.00285v1 Announce Type: cross  Abstract: Deploying deep models in real-world scenarios entails a number of challenges, including computational efficiency and real-world (e.g., long-tailed) data distributions. We address the combined challenge of learning long-tailed distributions using highly resource-efficient binary neural networks as backbones. Specifically, we propose a calibrate-and-distill framework that uses off-the-shelf pretrained full-precision models trained on balanced datasets to use as teachers for distillation when learning binary networks on long-tailed datasets. To better generalize to various datasets, we further propose a novel adversarial balancing among the terms in the objective function and an efficient multiresolution learning scheme. We conducted the largest empirical study in the literature using 15 datasets, including newly derived long-tailed datasets from existing balanced datasets, and show that our proposed method outperforms prior art by large ",
    "path": "papers/24/04/2404.00285.json",
    "total_tokens": 914,
    "translated_title": "通过校准预训练模型，在二值网络上进行长尾识别",
    "translated_abstract": "在真实世界的情景中部署深度模型面临多个挑战，包括计算效率和真实世界（如长尾）数据分布。本文解决了学习长尾分布，利用高度资源有效的二值神经网络作为骨干网的组合挑战。具体来说，我们提出了一个校准和蒸馏框架，使用预先训练的平衡数据集上的全精度模型作为教师，在学习长尾数据集上的二值网络时进行蒸馏。为了更好地泛化到各种数据集，我们进一步提出了一种新颖的目标函数中各项之间的敌对平衡和高效的多分辨率学习方案。我们进行了文献中规模最大的经验研究，使用了15个数据集，包括从现有平衡数据集中新导出的长尾数据集，并展示了我们提出的方法大大优于先前的技术",
    "tldr": "该论文提出了一个校准和蒸馏框架，利用预先训练的全精度模型作为教师，在学习长尾数据集上的二值网络时进行蒸馏，通过对目标函数中项的敌对平衡和多分辨率学习，显著优于以往技术",
    "en_tdlr": "The paper proposes a calibration and distillation framework that utilizes pretrained full-precision models as teachers for distillation when learning binary networks on long-tailed datasets, outperforming prior art significantly through adversarial balancing among the terms in the objective function and multiresolution learning."
}