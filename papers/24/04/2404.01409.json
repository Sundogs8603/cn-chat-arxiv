{
    "title": "OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation",
    "abstract": "arXiv:2404.01409v1 Announce Type: cross  Abstract: In the realm of food computing, segmenting ingredients from images poses substantial challenges due to the large intra-class variance among the same ingredients, the emergence of new ingredients, and the high annotation costs associated with large food segmentation datasets. Existing approaches primarily utilize a closed-vocabulary and static text embeddings setting. These methods often fall short in effectively handling the ingredients, particularly new and diverse ones. In response to these limitations, we introduce OVFoodSeg, a framework that adopts an open-vocabulary setting and enhances text embeddings with visual context. By integrating vision-language models (VLMs), our approach enriches text embedding with image-specific information through two innovative modules, eg, an image-to-text learner FoodLearner and an Image-Informed Text Encoder. The training process of OVFoodSeg is divided into two stages: the pre-training of FoodLea",
    "link": "https://arxiv.org/abs/2404.01409",
    "context": "Title: OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation\nAbstract: arXiv:2404.01409v1 Announce Type: cross  Abstract: In the realm of food computing, segmenting ingredients from images poses substantial challenges due to the large intra-class variance among the same ingredients, the emergence of new ingredients, and the high annotation costs associated with large food segmentation datasets. Existing approaches primarily utilize a closed-vocabulary and static text embeddings setting. These methods often fall short in effectively handling the ingredients, particularly new and diverse ones. In response to these limitations, we introduce OVFoodSeg, a framework that adopts an open-vocabulary setting and enhances text embeddings with visual context. By integrating vision-language models (VLMs), our approach enriches text embedding with image-specific information through two innovative modules, eg, an image-to-text learner FoodLearner and an Image-Informed Text Encoder. The training process of OVFoodSeg is divided into two stages: the pre-training of FoodLea",
    "path": "papers/24/04/2404.01409.json",
    "total_tokens": 855,
    "translated_title": "OVFoodSeg：通过基于图像信息的文本表示提升开放词汇食品图像分割",
    "translated_abstract": "在食品计算领域，从图像中分割出食材面临着巨大挑战，这主要是由于相同食材之间存在大量的内部类别差异、新食材的出现，以及与大型食品分割数据集相关的高昂注释成本。现有方法主要采用闭合词汇和静态文本嵌入设置。这些方法通常在有效处理食材方面存在不足，特别是新的和多样化的食材。为了应对这些局限性，我们引入了OVFoodSeg，这是一个采用开放词汇设置并通过视觉背景增强文本嵌入的框架。通过集成视觉-语言模型（VLMs），我们的方法通过两个创新模块（例如，图像到文本学习器FoodLearner和基于图像信息的文本编码器）将文本嵌入与图像特定信息相结合。OVFoodSeg的训练过程分为两个阶段：FoodLea的预训练",
    "tldr": "OVFoodSeg通过开放词汇设定和视觉-语言模型，结合图像信息，提升食品图像分割任务的效果。",
    "en_tdlr": "OVFoodSeg improves the task of food image segmentation by utilizing an open-vocabulary setting, incorporating visual-language models, and integrating image-specific information."
}