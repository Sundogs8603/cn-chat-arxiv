{
    "title": "On Inherent Adversarial Robustness of Active Vision Systems",
    "abstract": "arXiv:2404.00185v1 Announce Type: cross  Abstract: Current Deep Neural Networks are vulnerable to adversarial examples, which alter their predictions by adding carefully crafted noise. Since human eyes are robust to such inputs, it is possible that the vulnerability stems from the standard way of processing inputs in one shot by processing every pixel with the same importance. In contrast, neuroscience suggests that the human vision system can differentiate salient features by (1) switching between multiple fixation points (saccades) and (2) processing the surrounding with a non-uniform external resolution (foveation). In this work, we advocate that the integration of such active vision mechanisms into current deep learning systems can offer robustness benefits. Specifically, we empirically demonstrate the inherent robustness of two active vision methods - GFNet and FALcon - under a black box threat model. By learning and inferencing based on downsampled glimpses obtained from multiple",
    "link": "https://arxiv.org/abs/2404.00185",
    "context": "Title: On Inherent Adversarial Robustness of Active Vision Systems\nAbstract: arXiv:2404.00185v1 Announce Type: cross  Abstract: Current Deep Neural Networks are vulnerable to adversarial examples, which alter their predictions by adding carefully crafted noise. Since human eyes are robust to such inputs, it is possible that the vulnerability stems from the standard way of processing inputs in one shot by processing every pixel with the same importance. In contrast, neuroscience suggests that the human vision system can differentiate salient features by (1) switching between multiple fixation points (saccades) and (2) processing the surrounding with a non-uniform external resolution (foveation). In this work, we advocate that the integration of such active vision mechanisms into current deep learning systems can offer robustness benefits. Specifically, we empirically demonstrate the inherent robustness of two active vision methods - GFNet and FALcon - under a black box threat model. By learning and inferencing based on downsampled glimpses obtained from multiple",
    "path": "papers/24/04/2404.00185.json",
    "total_tokens": 847,
    "translated_title": "论主动视觉系统固有对抗鲁棒性",
    "translated_abstract": "当前的深度神经网络容易受到对抗样本的攻击，这种对抗样本通过添加精心设计的噪声来改变它们的预测。由于人类眼睛对这种输入具有鲁棒性，可能的弱点源于用相同重要性处理每个像素的标准方式。相比之下，神经科学表明人类视觉系统可以通过（1）切换多个注视点（扫视）和（2）以非均匀外部分辨率（视神经）处理周围信息来区分显著特征。本研究提倡将这种主动视觉机制融入当前的深度学习系统中，以提供鲁棒性优势。具体来说，我们在黑盒威胁模型下，经验性地展示了两种主动视觉方法GFNet和FALcon的固有鲁棒性。",
    "tldr": "主动视觉机制的集成可以提供当前深度学习系统的鲁棒性优势，并且通过实验证明了GFNet和FALcon这两种主动视觉方法在黑盒威胁模型下的固有鲁棒性。",
    "en_tdlr": "The integration of active vision mechanisms can offer robustness benefits to current deep learning systems, with empirical demonstrations of the inherent robustness of GFNet and FALcon under a black box threat model."
}