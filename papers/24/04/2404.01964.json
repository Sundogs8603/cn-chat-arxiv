{
    "title": "CAM-Based Methods Can See through Walls",
    "abstract": "arXiv:2404.01964v1 Announce Type: cross  Abstract: CAM-based methods are widely-used post-hoc interpretability method that produce a saliency map to explain the decision of an image classification model. The saliency map highlights the important areas of the image relevant to the prediction. In this paper, we show that most of these methods can incorrectly attribute an important score to parts of the image that the model cannot see. We show that this phenomenon occurs both theoretically and experimentally. On the theory side, we analyze the behavior of GradCAM on a simple masked CNN model at initialization. Experimentally, we train a VGG-like model constrained to not use the lower part of the image and nevertheless observe positive scores in the unseen part of the image. This behavior is evaluated quantitatively on two new datasets. We believe that this is problematic, potentially leading to mis-interpretation of the model's behavior.",
    "link": "https://arxiv.org/abs/2404.01964",
    "context": "Title: CAM-Based Methods Can See through Walls\nAbstract: arXiv:2404.01964v1 Announce Type: cross  Abstract: CAM-based methods are widely-used post-hoc interpretability method that produce a saliency map to explain the decision of an image classification model. The saliency map highlights the important areas of the image relevant to the prediction. In this paper, we show that most of these methods can incorrectly attribute an important score to parts of the image that the model cannot see. We show that this phenomenon occurs both theoretically and experimentally. On the theory side, we analyze the behavior of GradCAM on a simple masked CNN model at initialization. Experimentally, we train a VGG-like model constrained to not use the lower part of the image and nevertheless observe positive scores in the unseen part of the image. This behavior is evaluated quantitatively on two new datasets. We believe that this is problematic, potentially leading to mis-interpretation of the model's behavior.",
    "path": "papers/24/04/2404.01964.json",
    "total_tokens": 781,
    "translated_title": "基于CAM的方法可以穿墙而过",
    "translated_abstract": "CAM-based方法是一种广泛使用的事后解释性方法，生成显著性地图来解释图像分类模型的决策。显著性地图突出显示与预测相关的图像重要区域。本文展示了大多数这些方法可能错误地将图像的某些部分归因为模型无法看到的重要得分。我们表明这种现象在理论和实验中均存在。理论上，我们分析了GradCAM在一个简单的掩膜CNN模型初始化时的行为。实验上，我们训练了一个类似VGG的模型，限制其不使用图像的下半部分，仍然观察到未见部分的正分数。这种行为在两个新数据集上进行了定量评估。我们认为这是有问题的，可能会导致对模型行为的错误解释。",
    "tldr": "CAM-based方法解释图像分类模型的决策时，可能会错误地将模型无法看到的部分归因为重要，这可能导致对模型行为的误解释。",
    "en_tdlr": "CAM-based methods for explaining image classification models may incorrectly attribute importance to parts of the image that the model cannot see, potentially leading to a misinterpretation of the model's behavior."
}