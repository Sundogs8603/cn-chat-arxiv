{
    "title": "TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search",
    "abstract": "arXiv:2404.00271v1 Announce Type: cross  Abstract: Neural architecture search (NAS) is an effective method for discovering new convolutional neural network (CNN) architectures. However, existing approaches often require time-consuming training or intensive sampling and evaluations. Zero-shot NAS aims to create training-free proxies for architecture performance prediction. However, existing proxies have suboptimal performance, and are often outperformed by simple metrics such as model parameter counts or the number of floating-point operations. Besides, existing model-based proxies cannot be generalized to new search spaces with unseen new types of operators without golden accuracy truth. A universally optimal proxy remains elusive. We introduce TG-NAS, a novel model-based universal proxy that leverages a transformer-based operator embedding generator and a graph convolution network (GCN) to predict architecture performance. This approach guides neural architecture search across any giv",
    "link": "https://arxiv.org/abs/2404.00271",
    "context": "Title: TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search\nAbstract: arXiv:2404.00271v1 Announce Type: cross  Abstract: Neural architecture search (NAS) is an effective method for discovering new convolutional neural network (CNN) architectures. However, existing approaches often require time-consuming training or intensive sampling and evaluations. Zero-shot NAS aims to create training-free proxies for architecture performance prediction. However, existing proxies have suboptimal performance, and are often outperformed by simple metrics such as model parameter counts or the number of floating-point operations. Besides, existing model-based proxies cannot be generalized to new search spaces with unseen new types of operators without golden accuracy truth. A universally optimal proxy remains elusive. We introduce TG-NAS, a novel model-based universal proxy that leverages a transformer-based operator embedding generator and a graph convolution network (GCN) to predict architecture performance. This approach guides neural architecture search across any giv",
    "path": "papers/24/04/2404.00271.json",
    "total_tokens": 830,
    "translated_title": "TG-NAS：利用Transformer和图卷积网络与零成本代理进行高效神经结构搜索",
    "translated_abstract": "神经结构搜索(NAS)是一种发现新的卷积神经网络(CNN)架构的有效方法。然而，现有方法通常需要耗时的训练或密集的采样和评估。零成本NAS旨在为架构性能预测创建免训练代理。然而，现有代理性能亚优，并且常常被模型参数数量或浮点运算次数等简单指标所超越。此外，现有基于模型的代理无法将泛化到新的搜索空间，其中具有未见新类型运算符且不带有黄金准确度。一个普遍最优的代理仍然难以找到。我们引入了TG-NAS，一种利用基于Transformer的运算符嵌入生成器和图卷积网络(GCN)来预测架构性能的新型模型通用代理。这种方法指导着在任何给定搜索空间内进行神经结构搜索。",
    "tldr": "TG-NAS提出了一种新型模型通用代理，利用Transformer的运算符嵌入生成器和图卷积网络来预测架构性能，指导神经结构搜索。",
    "en_tdlr": "TG-NAS introduces a novel model-based universal proxy using a transformer-based operator embedding generator and a graph convolution network to predict architecture performance, guiding neural architecture search."
}