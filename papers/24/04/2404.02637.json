{
    "title": "Vocabulary Attack to Hijack Large Language Model Applications",
    "abstract": "arXiv:2404.02637v1 Announce Type: cross  Abstract: The fast advancements in Large Language Models (LLMs) are driving an increasing number of applications. Together with the growing number of users, we also see an increasing number of attackers who try to outsmart these systems. They want the model to reveal confidential information, specific false information, or offensive behavior. To this end, they manipulate their instructions for the LLM by inserting separators or rephrasing them systematically until they reach their goal. Our approach is different. It inserts words from the model vocabulary. We find these words using an optimization procedure and embeddings from another LLM (attacker LLM). We prove our approach by goal hijacking two popular open-source LLMs from the Llama2 and the Flan-T5 families, respectively. We present two main findings. First, our approach creates inconspicuous instructions and therefore it is hard to detect. For many attack cases, we find that even a single ",
    "link": "https://arxiv.org/abs/2404.02637",
    "context": "Title: Vocabulary Attack to Hijack Large Language Model Applications\nAbstract: arXiv:2404.02637v1 Announce Type: cross  Abstract: The fast advancements in Large Language Models (LLMs) are driving an increasing number of applications. Together with the growing number of users, we also see an increasing number of attackers who try to outsmart these systems. They want the model to reveal confidential information, specific false information, or offensive behavior. To this end, they manipulate their instructions for the LLM by inserting separators or rephrasing them systematically until they reach their goal. Our approach is different. It inserts words from the model vocabulary. We find these words using an optimization procedure and embeddings from another LLM (attacker LLM). We prove our approach by goal hijacking two popular open-source LLMs from the Llama2 and the Flan-T5 families, respectively. We present two main findings. First, our approach creates inconspicuous instructions and therefore it is hard to detect. For many attack cases, we find that even a single ",
    "path": "papers/24/04/2404.02637.json",
    "total_tokens": 843,
    "translated_title": "词汇攻击以劫持大型语言模型应用",
    "translated_abstract": "大型语言模型（LLMs）的快速发展推动了越来越多的应用程序。随着用户数量的增加，我们也看到越来越多的攻击者试图智胜这些系统。他们希望模型透露机密信息、特定错误信息或冒犯行为。为此，他们通过插入分隔符或系统性地改写指令，直到达到目标。我们的方法与众不同。它插入来自模型词汇的词汇。我们使用优化过程和来自另一个LLM（攻击者LLM）的嵌入来找到这些词汇。我们通过劫持两个热门开源LLM（分别来自Llama2和Flan-T5系列）证明了我们的方法。我们得出了两个主要发现。首先，我们的方法生成不引人注目的指令，因此很难检测。对于许多攻击案例，我们发现即使一个",
    "tldr": "通过插入来源模型词汇的方式，成功实施了对两个热门开源大型语言模型的目标劫持，创造出难以检测的不引人注目指令。",
    "en_tdlr": "Successfully hijacked two popular open-source large language models by inserting words from the model vocabulary, creating inconspicuous instructions that are hard to detect."
}