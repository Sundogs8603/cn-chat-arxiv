{
    "title": "The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks",
    "abstract": "arXiv:2404.00176v1 Announce Type: new  Abstract: Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task, which is usually operationalized based on two subsequently applied usage-level tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages. Then, these labels are represented in a graph on which Word Sense Induction (WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by comparing sense clusters over time. This modularity is reflected in most LSCD datasets and models. It also leads to a large heterogeneity in modeling options and task definitions, which is exacerbated by a variety of dataset versions, preprocessing options and evaluation metrics. This heterogeneity makes it difficult to evaluate models under comparable conditions, to choose optimal model combinations or to reproduce results. Hence, we provide a benchmark repository standardizing LSCD evaluation. Through transparent implementation results become easily reproducib",
    "link": "https://arxiv.org/abs/2404.00176",
    "context": "Title: The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks\nAbstract: arXiv:2404.00176v1 Announce Type: new  Abstract: Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task, which is usually operationalized based on two subsequently applied usage-level tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages. Then, these labels are represented in a graph on which Word Sense Induction (WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by comparing sense clusters over time. This modularity is reflected in most LSCD datasets and models. It also leads to a large heterogeneity in modeling options and task definitions, which is exacerbated by a variety of dataset versions, preprocessing options and evaluation metrics. This heterogeneity makes it difficult to evaluate models under comparable conditions, to choose optimal model combinations or to reproduce results. Hence, we provide a benchmark repository standardizing LSCD evaluation. Through transparent implementation results become easily reproducib",
    "path": "papers/24/04/2404.00176.json",
    "total_tokens": 823,
    "translated_title": "LSCD基准测试：历时词义任务的测试平台",
    "translated_abstract": "词汇语义变化检测（LSCD）是一项复杂的词元级任务，通常基于两个连续应用的使用级任务来操作：首先，为使用对得到Word-in-Context (WiC)标签。然后，在图上表示这些标签，对其应用Word Sense Induction (WSI)来推导出含义聚类。最后，通过比较随时间变化的含义聚类来推导出LSCD标签。这种模块化反映在大多数LSCD数据集和模型中。这也导致了建模选择和任务定义上的大量异质性，这一点又因各种数据集版本、预处理选项和评估指标而加剧。这种异质性使得在可比条件下评估模型、选择最佳模型组合或复现结果变得困难。因此，我们提供了一个基准测试库，以规范LSCD评估。通过透明的实现，结果变得易于复现。",
    "tldr": "LSCD基准测试提供了一个标准化的LSCD评估平台，解决了模型评估和结果复现中存在的异质性问题。",
    "en_tdlr": "The LSCD Benchmark provides a standardized evaluation platform for LSCD, addressing the heterogeneity in model evaluation and result reproducibility."
}