{
    "title": "An Interpretable Client Decision Tree Aggregation process for Federated Learning",
    "abstract": "arXiv:2404.02510v1 Announce Type: cross  Abstract: Trustworthy Artificial Intelligence solutions are essential in today's data-driven applications, prioritizing principles such as robustness, safety, transparency, explainability, and privacy among others. This has led to the emergence of Federated Learning as a solution for privacy and distributed machine learning. While decision trees, as self-explanatory models, are ideal for collaborative model training across multiple devices in resource-constrained environments such as federated learning environments for injecting interpretability in these models. Decision tree structure makes the aggregation in a federated learning environment not trivial. They require techniques that can merge their decision paths without introducing bias or overfitting while keeping the aggregated decision trees robust and generalizable. In this paper, we propose an Interpretable Client Decision Tree Aggregation process for Federated Learning scenarios that kee",
    "link": "https://arxiv.org/abs/2404.02510",
    "context": "Title: An Interpretable Client Decision Tree Aggregation process for Federated Learning\nAbstract: arXiv:2404.02510v1 Announce Type: cross  Abstract: Trustworthy Artificial Intelligence solutions are essential in today's data-driven applications, prioritizing principles such as robustness, safety, transparency, explainability, and privacy among others. This has led to the emergence of Federated Learning as a solution for privacy and distributed machine learning. While decision trees, as self-explanatory models, are ideal for collaborative model training across multiple devices in resource-constrained environments such as federated learning environments for injecting interpretability in these models. Decision tree structure makes the aggregation in a federated learning environment not trivial. They require techniques that can merge their decision paths without introducing bias or overfitting while keeping the aggregated decision trees robust and generalizable. In this paper, we propose an Interpretable Client Decision Tree Aggregation process for Federated Learning scenarios that kee",
    "path": "papers/24/04/2404.02510.json",
    "total_tokens": 837,
    "translated_title": "一种用于联邦学习的可解释客户端决策树聚合过程",
    "translated_abstract": "可信的人工智能解决方案在当今的数据驱动应用中至关重要，优先考虑诸如鲁棒性、安全性、透明性、可解释性和隐私性等原则。这导致联邦学习作为隐私和分布式机器学习的解决方案的出现。决策树作为自解释模型，在资源受限的环境中如联邦学习环境中进行协作模型训练是理想的，以在这些模型中注入可解释性。决策树结构使得在联邦学习环境中进行聚合并不是一件简单的事情。它们需要能够合并它们的决策路径而不引入偏差或过拟合的技术，同时保持聚合决策树的稳健性和泛化性。本文提出了一种适用于联邦学习场景的可解释客户端决策树聚合过程。",
    "tldr": "提出了一种用于联邦学习的可解释客户端决策树聚合过程，旨在解决在这些模型中注入可解释性的挑战。",
    "en_tdlr": "Introduces an interpretable client decision tree aggregation process for Federated Learning aimed at addressing the challenge of injecting interpretability in these models."
}