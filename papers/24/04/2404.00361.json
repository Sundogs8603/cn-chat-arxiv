{
    "title": "Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation",
    "abstract": "arXiv:2404.00361v1 Announce Type: new  Abstract: Data augmentation (DA) is crucial to mitigate model training instability and over-fitting problems in low-resource open-domain dialogue generation. However, traditional DA methods often neglect semantic data diversity, restricting the overall quality. Recently, large language models (LLM) have been used for DA to generate diversified dialogues. However, they have limited controllability and tend to generate dialogues with a distribution shift compared to the seed dialogues. To maximize the augmentation diversity and address the controllability problem, we propose \\textbf{S}ummary-based \\textbf{D}ialogue \\textbf{A}ugmentation with LLM (SDA). Our approach enhances the controllability of LLM by using dialogue summaries as a planning tool. Based on summaries, SDA can generate high-quality and diverse dialogue data even with a small seed dataset. To evaluate the efficacy of data augmentation methods for open-domain dialogue, we designed a clu",
    "link": "https://arxiv.org/abs/2404.00361",
    "context": "Title: Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation\nAbstract: arXiv:2404.00361v1 Announce Type: new  Abstract: Data augmentation (DA) is crucial to mitigate model training instability and over-fitting problems in low-resource open-domain dialogue generation. However, traditional DA methods often neglect semantic data diversity, restricting the overall quality. Recently, large language models (LLM) have been used for DA to generate diversified dialogues. However, they have limited controllability and tend to generate dialogues with a distribution shift compared to the seed dialogues. To maximize the augmentation diversity and address the controllability problem, we propose \\textbf{S}ummary-based \\textbf{D}ialogue \\textbf{A}ugmentation with LLM (SDA). Our approach enhances the controllability of LLM by using dialogue summaries as a planning tool. Based on summaries, SDA can generate high-quality and diverse dialogue data even with a small seed dataset. To evaluate the efficacy of data augmentation methods for open-domain dialogue, we designed a clu",
    "path": "papers/24/04/2404.00361.json",
    "total_tokens": 857,
    "translated_title": "低资源开放域对话生成的可控多样化数据增强与大型语言模型",
    "translated_abstract": "数据增强（DA）对于减轻低资源开放域对话生成中模型训练不稳定和过拟合问题至关重要。然而，传统的DA方法通常忽略了语义数据多样性，限制了整体质量。最近，大型语言模型（LLM）已被用于DA以生成多样化的对话。然而，它们受到限制的可控性，并且倾向于生成与种子对话相比具有分布偏移的对话。为了最大化增强多样性并解决可控性问题，我们提出了基于总结的LLM（SDA）的对话增强。我们的方法通过使用对话总结作为规划工具增强了LLM的可控性。基于总结，SDA可以生成高质量且多样化的对话数据，即使只有一个小的种子数据集。为了评估开放域对话的数据增强方法的有效性，我们设计了一个聚",
    "tldr": "提出了一种基于总结的对话增强方法SDA，通过使用对话总结增强了LLM的可控性，实现高质量和多样化的对话数据生成。"
}