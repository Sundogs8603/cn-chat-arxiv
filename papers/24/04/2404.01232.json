{
    "title": "Open-Vocabulary Federated Learning with Multimodal Prototyping",
    "abstract": "arXiv:2404.01232v1 Announce Type: new  Abstract: Existing federated learning (FL) studies usually assume the training label space and test label space are identical. However, in real-world applications, this assumption is too ideal to be true. A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems. Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL. That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes. To address this problem, we leverage the pre-trained vision-language models (VLMs). In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as Federated Multimodal Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel multimodal prototyping mechan",
    "link": "https://arxiv.org/abs/2404.01232",
    "context": "Title: Open-Vocabulary Federated Learning with Multimodal Prototyping\nAbstract: arXiv:2404.01232v1 Announce Type: new  Abstract: Existing federated learning (FL) studies usually assume the training label space and test label space are identical. However, in real-world applications, this assumption is too ideal to be true. A new user could come up with queries that involve data from unseen classes, and such open-vocabulary queries would directly defect such FL systems. Therefore, in this work, we explicitly focus on the under-explored open-vocabulary challenge in FL. That is, for a new user, the global server shall understand her/his query that involves arbitrary unknown classes. To address this problem, we leverage the pre-trained vision-language models (VLMs). In particular, we present a novel adaptation framework tailored for VLMs in the context of FL, named as Federated Multimodal Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights based on light-weight client residuals, and makes predictions based on a novel multimodal prototyping mechan",
    "path": "papers/24/04/2404.01232.json",
    "total_tokens": 932,
    "translated_title": "具有多模式原型的开放词汇联邦学习",
    "translated_abstract": "现有的联邦学习（FL）研究通常假设训练标签空间和测试标签空间是相同的。然而，在真实应用中，这个假设太理想化了。新用户可能提出涉及来自未见类别数据的查询，这些开放词汇查询将直接导致这种FL系统的缺陷。因此，在这项工作中，我们明确关注FL中尚未开发的开放词汇挑战。也就是说，对于一个新用户，全局服务器应该理解她/他的查询涉及任意未知类别。为了解决这个问题，我们利用了预训练的视觉语言模型（VLMs）。具体来说，我们提出了一个针对FL环境中VLMs的新型自适应框架，命名为联邦多模式原型（Fed-MP）。Fed-MP根据轻量级客户端残差自适应聚合本地模型权重，并根据一个新颖的多模式原型机制进行预测。",
    "tldr": "本研究针对联邦学习中的开放词汇挑战，提出了一种基于预训练视觉语言模型的新型自适应框架，命名为联邦多模式原型（Fed-MP），用于解决新用户提出的涉及任意未知类别的查询问题。",
    "en_tdlr": "This study focuses on the open-vocabulary challenge in federated learning and proposes a novel adaptation framework based on pre-trained vision-language models, named Federated Multimodal Prototyping (Fed-MP), to address queries involving arbitrary unknown classes from new users."
}