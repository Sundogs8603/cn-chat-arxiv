{
    "title": "METAL: Towards Multilingual Meta-Evaluation",
    "abstract": "arXiv:2404.01667v1 Announce Type: new  Abstract: With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that LLMs excel on many standard NLP benchmarks. However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics. However, past work has shown that LLM-based evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization. This dataset is created specifically to evalua",
    "link": "https://arxiv.org/abs/2404.01667",
    "context": "Title: METAL: Towards Multilingual Meta-Evaluation\nAbstract: arXiv:2404.01667v1 Announce Type: new  Abstract: With the rising human-like precision of Large Language Models (LLMs) in numerous tasks, their utilization in a variety of real-world applications is becoming more prevalent. Several studies have shown that LLMs excel on many standard NLP benchmarks. However, it is challenging to evaluate LLMs due to test dataset contamination and the limitations of traditional metrics. Since human evaluations are difficult to collect, there is a growing interest in the community to use LLMs themselves as reference-free evaluators for subjective metrics. However, past work has shown that LLM-based evaluators can exhibit bias and have poor alignment with human judgments. In this study, we propose a framework for an end-to-end assessment of LLMs as evaluators in multilingual scenarios. We create a carefully curated dataset, covering 10 languages containing native speaker judgments for the task of summarization. This dataset is created specifically to evalua",
    "path": "papers/24/04/2404.01667.json",
    "total_tokens": 669,
    "translated_title": "METAL: 迈向多语言元评估",
    "translated_abstract": "随着大型语言模型（LLMs）在许多任务中表现出越来越接近人类的精度，它们在各种实际应用中的利用变得更加普遍。本研究提出了一个用于多语言场景下LLMs评估的端到端框架，创建了一个精心策划的数据集，涵盖10种语言，包含了原生说话者对摘要任务的判断。",
    "tldr": "提出了一个用于多语言场景下LLMs评估的端到端框架，并创建了一个涵盖10种语言的精心策划数据集。",
    "en_tdlr": "A framework for end-to-end assessment of LLMs as evaluators in multilingual scenarios is proposed, along with a carefully curated dataset covering 10 languages."
}