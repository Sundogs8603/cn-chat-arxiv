{
    "title": "Rethinking Resource Management in Edge Learning: A Joint Pre-training and Fine-tuning Design Paradigm",
    "abstract": "arXiv:2404.00836v1 Announce Type: cross  Abstract: In some applications, edge learning is experiencing a shift in focusing from conventional learning from scratch to new two-stage learning unifying pre-training and task-specific fine-tuning. This paper considers the problem of joint communication and computation resource management in a two-stage edge learning system. In this system, model pre-training is first conducted at an edge server via centralized learning on local pre-stored general data, and then task-specific fine-tuning is performed at edge devices based on the pre-trained model via federated edge learning. For the two-stage learning model, we first analyze the convergence behavior (in terms of the average squared gradient norm bound), which characterizes the impacts of various system parameters such as the number of learning rounds and batch sizes in the two stages on the convergence rate. Based on our analytical results, we then propose a joint communication and computatio",
    "link": "https://arxiv.org/abs/2404.00836",
    "context": "Title: Rethinking Resource Management in Edge Learning: A Joint Pre-training and Fine-tuning Design Paradigm\nAbstract: arXiv:2404.00836v1 Announce Type: cross  Abstract: In some applications, edge learning is experiencing a shift in focusing from conventional learning from scratch to new two-stage learning unifying pre-training and task-specific fine-tuning. This paper considers the problem of joint communication and computation resource management in a two-stage edge learning system. In this system, model pre-training is first conducted at an edge server via centralized learning on local pre-stored general data, and then task-specific fine-tuning is performed at edge devices based on the pre-trained model via federated edge learning. For the two-stage learning model, we first analyze the convergence behavior (in terms of the average squared gradient norm bound), which characterizes the impacts of various system parameters such as the number of learning rounds and batch sizes in the two stages on the convergence rate. Based on our analytical results, we then propose a joint communication and computatio",
    "path": "papers/24/04/2404.00836.json",
    "total_tokens": 839,
    "translated_title": "在边缘学习中重新思考资源管理：一种联合预训练和微调设计范式",
    "translated_abstract": "在一些应用中，边缘学习正经历着从传统的从头开始学习转向统一预训练和特定任务微调的新两阶段学习。本文考虑了在两阶段边缘学习系统中进行联合通信和计算资源管理的问题。在这个系统中，模型预训练首先通过边缘服务器在本地预存的通用数据上进行集中式学习，然后基于预训练模型通过联邦边缘学习在边缘设备上执行特定任务微调。对于两阶段学习模型，我们首先分析了收敛行为（以平均平方梯度范数界形式），这表征了各种系统参数（如两阶段中学习轮数和批次大小）对收敛速率的影响。基于我们的分析结果，我们提出了一种联合通信和计算",
    "tldr": "该论文提出了一种边缘学习中联合预训练和微调设计范式，通过分析系统参数对收敛速率的影响，并提出了联合通信和计算资源管理方案。",
    "en_tdlr": "This paper presents a joint pre-training and fine-tuning design paradigm in edge learning, analyzing the impacts of system parameters on convergence rate and proposing a scheme for joint communication and computation resource management."
}