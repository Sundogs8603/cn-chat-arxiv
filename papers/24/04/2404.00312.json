{
    "title": "Bayesian Exploration of Pre-trained Models for Low-shot Image Classification",
    "abstract": "arXiv:2404.00312v1 Announce Type: cross  Abstract: Low-shot image classification is a fundamental task in computer vision, and the emergence of large-scale vision-language models such as CLIP has greatly advanced the forefront of research in this field. However, most existing CLIP-based methods lack the flexibility to effectively incorporate other pre-trained models that encompass knowledge distinct from CLIP. To bridge the gap, this work proposes a simple and effective probabilistic model ensemble framework based on Gaussian processes, which have previously demonstrated remarkable efficacy in processing small data. We achieve the integration of prior knowledge by specifying the mean function with CLIP and the kernel function with an ensemble of deep kernels built upon various pre-trained models. By regressing the classification label directly, our framework enables analytical inference, straightforward uncertainty quantification, and principled hyper-parameter tuning. Through extensiv",
    "link": "https://arxiv.org/abs/2404.00312",
    "context": "Title: Bayesian Exploration of Pre-trained Models for Low-shot Image Classification\nAbstract: arXiv:2404.00312v1 Announce Type: cross  Abstract: Low-shot image classification is a fundamental task in computer vision, and the emergence of large-scale vision-language models such as CLIP has greatly advanced the forefront of research in this field. However, most existing CLIP-based methods lack the flexibility to effectively incorporate other pre-trained models that encompass knowledge distinct from CLIP. To bridge the gap, this work proposes a simple and effective probabilistic model ensemble framework based on Gaussian processes, which have previously demonstrated remarkable efficacy in processing small data. We achieve the integration of prior knowledge by specifying the mean function with CLIP and the kernel function with an ensemble of deep kernels built upon various pre-trained models. By regressing the classification label directly, our framework enables analytical inference, straightforward uncertainty quantification, and principled hyper-parameter tuning. Through extensiv",
    "path": "papers/24/04/2404.00312.json",
    "total_tokens": 837,
    "translated_title": "贝叶斯探索预训练模型用于低样本图像分类",
    "translated_abstract": "低样本图像分类是计算机视觉领域的一项基础任务，大规模视觉-语言模型（如CLIP）的出现极大推进了该领域的研究前沿。然而，大多数现有基于CLIP的方法缺乏有效整合其他预训练模型的灵活性，这些模型包含不同于CLIP的知识。为弥补这一差距，本研究提出了一种基于高斯过程的简单高效的概率模型集成框架，高斯过程在处理少量数据时已经显示出显著的效力。我们通过将均值函数指定为CLIP，将核函数指定为基于各种预训练模型构建的深层核，实现了先验知识的整合。通过直接回归分类标签，我们的框架实现了分析推断、直观不确定性量化和合理的超参数调整。",
    "tldr": "提出了基于高斯过程的概率模型集成框架，能有效整合不同于CLIP的先验知识，实现了低样本图像分类中的分析推断、不确定性量化和超参数调整。",
    "en_tdlr": "Introduced a probabilistic model ensemble framework based on Gaussian processes to effectively incorporate prior knowledge distinct from CLIP, enabling analytical inference, uncertainty quantification, and principled hyper-parameter tuning in low-shot image classification."
}