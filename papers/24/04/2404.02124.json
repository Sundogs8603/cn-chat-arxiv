{
    "title": "Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models",
    "abstract": "arXiv:2404.02124v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconcept",
    "link": "https://arxiv.org/abs/2404.02124",
    "context": "Title: Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models\nAbstract: arXiv:2404.02124v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconcept",
    "path": "papers/24/04/2404.02124.json",
    "total_tokens": 879,
    "translated_title": "通过大型语言模型探索数学多项选择题的自动生成干扰项",
    "translated_abstract": "多项选择题在几乎所有教育层次中都是普遍存在的，因为它们易于管理、评分，并且是评估和实践中可靠的格式。其中最重要的方面之一是干扰项，即针对真实学生常见错误或误解而设计的不正确选项。目前，制作高质量干扰项的任务在很大程度上仍然是教师和学习内容设计者的劳动和耗时工作，这限制了可扩展性。在这项工作中，我们研究了在数学多项选择题领域中自动生成干扰项的任务，并探索了各种基于大型语言模型（LLM）的方法，从上下文学习到微调。我们使用真实数学多项选择题数据集进行了大量实验，发现虽然LLM可以生成一些数学上有效的干扰项，但它们在预测常见错误或误解方面表现不佳。",
    "tldr": "通过大型语言模型探索数学多项选择题的自动生成干扰项，发现虽然LLMs可以生成一些数学上有效的干扰项，但在预测常见错误或误解方面表现不佳",
    "en_tdlr": "Investigating automated distractor generation for math multiple-choice questions using large language models, where it was found that while LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconceptions."
}