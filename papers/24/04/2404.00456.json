{
    "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
    "abstract": "arXiv:2404.00456v1 Announce Type: new  Abstract: We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4-bits, without any channels identified for retention in higher precision. Our quantized LLaMa2-70B model has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the zero-shot performance. Code is available at: https://github.com/spcl/QuaRot.",
    "link": "https://arxiv.org/abs/2404.00456",
    "context": "Title: QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs\nAbstract: arXiv:2404.00456v1 Announce Type: new  Abstract: We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4-bits, without any channels identified for retention in higher precision. Our quantized LLaMa2-70B model has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the zero-shot performance. Code is available at: https://github.com/spcl/QuaRot.",
    "path": "papers/24/04/2404.00456.json",
    "total_tokens": 845,
    "translated_title": "QuaRot：旋转LLMs中无异常值的4位推断",
    "translated_abstract": "我们介绍了QuaRot，一种基于旋转的新量化方案，能够端对端地将LLMs中的所有权重、激活和KV缓存量化为4位。QuaRot以一种能够去除隐藏状态中异常值但不改变输出的方式对LLMs进行旋转，使得量化变得更简单。这种计算不变性被应用于LLM的隐藏状态（残差），以及前馈组件的激活、注意机制的部分内容和KV缓存。结果是一个量化模型，其中所有矩阵乘法都以4位进行，无需识别需要以更高精度保留的通道。我们的量化LLaMa2-70B模型在最坏情况下仅损失0.29的WikiText-2困惑度，并保留了99%的零-shot表现。代码可在此处获得：https://github.com/spcl/QuaRot。",
    "tldr": "QuaRot是一种基于旋转的新量化方案，能够在LLMs中进行无异常值的4位推断，实现了端到端的量化，并保持了99%的零-shot表现。",
    "en_tdlr": "QuaRot is a new quantization scheme based on rotations that enables outlier-free 4-bit inference in LLMs, achieving end-to-end quantization while retaining 99% of zero-shot performance."
}