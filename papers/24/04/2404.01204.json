{
    "title": "The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis",
    "abstract": "arXiv:2404.01204v1 Announce Type: new  Abstract: Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream",
    "link": "https://arxiv.org/abs/2404.01204",
    "context": "Title: The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis\nAbstract: arXiv:2404.01204v1 Announce Type: new  Abstract: Uncovering early-stage metrics that reflect final model performance is one core principle for large-scale pretraining. The existing scaling law demonstrates the power-law correlation between pretraining loss and training flops, which serves as an important indicator of the current training state for large language models. However, this principle only focuses on the model's compression properties on the training data, resulting in an inconsistency with the ability improvements on the downstream tasks. Some follow-up works attempted to extend the scaling-law to more complex metrics (such as hyperparameters), but still lacked a comprehensive analysis of the dynamic differences among various capabilities during pretraining. To address the aforementioned limitations, this paper undertakes a comprehensive comparison of model capabilities at various pretraining intermediate checkpoints. Through this analysis, we confirm that specific downstream",
    "path": "papers/24/04/2404.01204.json",
    "total_tokens": 790,
    "translated_title": "大型语言模型预训练与下游能力分析的微妙平衡",
    "translated_abstract": "揭示反映最终模型性能的早期指标是大规模预训练的一个核心原则。现有的标度定律展示了预训练损失与训练浮点数之间的幂律相关性，这对于大型语言模型当前训练状态的重要指标十分关键。然而，这一原则只关注模型在训练数据上的压缩特性，导致与下游任务能力的提升之间存在不一致性。一些后续研究试图将标度定律扩展到更复杂的指标（如超参数），但仍然缺乏对预训练过程中各种能力之间动态差异的全面分析。为解决上述限制，本文对各种预训练中间检查点下模型能力进行了全面比较。",
    "tldr": "本文对大型语言模型在预训练过程中不同能力的综合分析揭示了其动态差异，填补了现有标度定律中的缺失。"
}