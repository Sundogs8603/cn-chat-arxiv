{
    "title": "Adversarial Attacks and Dimensionality in Text Classifiers",
    "abstract": "arXiv:2404.02660v1 Announce Type: new  Abstract: Adversarial attacks on machine learning algorithms have been a key deterrent to the adoption of AI in many real-world use cases. They significantly undermine the ability of high-performance neural networks by forcing misclassifications. These attacks introduce minute and structured perturbations or alterations in the test samples, imperceptible to human annotators in general, but trained neural networks and other models are sensitive to it. Historically, adversarial attacks have been first identified and studied in the domain of image processing. In this paper, we study adversarial examples in the field of natural language processing, specifically text classification tasks. We investigate the reasons for adversarial vulnerability, particularly in relation to the inherent dimensionality of the model. Our key finding is that there is a very strong correlation between the embedding dimensionality of the adversarial samples and their effecti",
    "link": "https://arxiv.org/abs/2404.02660",
    "context": "Title: Adversarial Attacks and Dimensionality in Text Classifiers\nAbstract: arXiv:2404.02660v1 Announce Type: new  Abstract: Adversarial attacks on machine learning algorithms have been a key deterrent to the adoption of AI in many real-world use cases. They significantly undermine the ability of high-performance neural networks by forcing misclassifications. These attacks introduce minute and structured perturbations or alterations in the test samples, imperceptible to human annotators in general, but trained neural networks and other models are sensitive to it. Historically, adversarial attacks have been first identified and studied in the domain of image processing. In this paper, we study adversarial examples in the field of natural language processing, specifically text classification tasks. We investigate the reasons for adversarial vulnerability, particularly in relation to the inherent dimensionality of the model. Our key finding is that there is a very strong correlation between the embedding dimensionality of the adversarial samples and their effecti",
    "path": "papers/24/04/2404.02660.json",
    "total_tokens": 742,
    "translated_title": "对抗攻击与文本分类器中的维度性研究",
    "translated_abstract": "机器学习算法遭受对抗攻击已成为许多真实世界用例中人工智能采用的主要障碍。这些攻击通过在测试样本中引入微小且结构化的扰动或改变，从根本上削弱了高性能神经网络的能力，迫使其进行误分类。本文研究自然语言处理领域中的对抗样本，特别是文本分类任务，探讨了对抗性脆弱性的原因，特别是与模型固有维度性的关系。我们的关键发现是对抗样本的嵌入维度与其影响之间存在非常强烈的相关性。",
    "tldr": "在文本分类任务中研究对抗攻击，发现对抗样本的嵌入维度与其影响之间存在非常强烈的相关性。",
    "en_tdlr": "Studying adversarial attacks in text classifiers, a strong correlation is found between the embedding dimensionality of adversarial samples and their impact."
}