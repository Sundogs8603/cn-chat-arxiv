{
    "title": "Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game",
    "abstract": "arXiv:2404.02532v1 Announce Type: new  Abstract: With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers' attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disgu",
    "link": "https://arxiv.org/abs/2404.02532",
    "context": "Title: Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game\nAbstract: arXiv:2404.02532v1 Announce Type: new  Abstract: With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers' attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers' capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disgu",
    "path": "papers/24/04/2404.02532.json",
    "total_tokens": 876,
    "translated_title": "学会伪装：通过多智能体攻击者-伪装者博弈避免LLM的拒绝响应",
    "translated_abstract": "随着大型模型在自然语言处理任务上表现出的增强性能，大型模型可能引发潜在的道德和伦理问题。存在一些恶意攻击者，他们通过诸如提示工程等技术诱使大型模型越狱，并生成包含非法、侵犯隐私信息的信息。因此，大型模型采用安全对齐等技术抵御恶意攻击者的攻击。然而，大型模型通过拒绝回复的强防御机制容易被攻击者识别，并用于加强攻击者的能力。在本文中，我们提出了一种多智能体攻击者-伪装者博弈方法，实现一种弱防御机制，使大型模型能够安全地回复攻击者并隐藏防御意图。首先，我们构建一个多智能体框架来模拟攻击和防御情景，扮演不同的角色，负责攻击、伪装",
    "tldr": "通过多智能体攻击者-伪装者博弈方法，实现一种弱防御机制，使大型模型能够安全地回复攻击者并隐藏防御意图",
    "en_tdlr": "Achieving a weak defense mechanism for large models to safely reply to attackers and hide defense intent through a multi-agent attacker-disguiser game approach."
}