{
    "title": "Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data",
    "abstract": "arXiv:2404.02422v1 Announce Type: new  Abstract: Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.",
    "link": "https://arxiv.org/abs/2404.02422",
    "context": "Title: Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data\nAbstract: arXiv:2404.02422v1 Announce Type: new  Abstract: Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve competitive results in Text Classification tasks. In-Context Learning (ICL) typically achieves better accuracy than the 0-shot setting, but it pays in terms of efficiency, due to the longer input prompt. In this paper, we propose a strategy to make LLMs as efficient as 0-shot text classifiers, while getting comparable or better accuracy than ICL. Our solution targets the low resource setting, i.e., when only 4 examples per class are available. Using a single LLM and few-shot real data we perform a sequence of generation, filtering and Parameter-Efficient Fine-Tuning steps to create a robust and efficient classifier. Experimental results show that our approach leads to competitive results on multiple text classification datasets.",
    "path": "papers/24/04/2404.02422.json",
    "total_tokens": 759,
    "translated_title": "使用PEFT和合成数据增强低资源LLMs分类",
    "translated_abstract": "大语言模型在0-shot或few-shot设置下，在文本分类任务中取得竞争性成果。在上下文学习(ICL)中，通常比0-shot设置获得更好的准确性，但这是以效率为代价的，因为需要更长的输入提示。本文提出了一种策略，可以使LLMs像0-shot文本分类器一样高效，同时获得与ICL相当或更好的准确性。我们的解决方案针对资源稀缺的情况，即每类只有4个示例可用。使用单个LLM和少量真实数据，我们执行一系列生成、过滤和参数高效微调步骤，从而创建一个强大而高效的分类器。实验结果表明，我们的方法在多个文本分类数据集上取得了竞争性结果。",
    "tldr": "提出了一种策略，通过PEFT和合成数据增强低资源LLMs分类器，实现了与0-shot文本分类器相媲美或更好的准确性。"
}