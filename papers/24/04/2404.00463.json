{
    "title": "Addressing Both Statistical and Causal Gender Fairness in NLP Models",
    "abstract": "arXiv:2404.00463v1 Announce Type: new  Abstract: Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.",
    "link": "https://arxiv.org/abs/2404.00463",
    "context": "Title: Addressing Both Statistical and Causal Gender Fairness in NLP Models\nAbstract: arXiv:2404.00463v1 Announce Type: new  Abstract: Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.",
    "path": "papers/24/04/2404.00463.json",
    "total_tokens": 828,
    "translated_title": "在NLP模型中解决统计和因果性别公平性问题",
    "translated_abstract": "统计公平性规定对每个受保护群体有相同的结果，而因果公平性要求模型对个体的预测不受其受保护特征的影响。反事实数据增强（CDA）对于减少NLP模型中的偏见是有效的，然而使用CDA训练的模型通常只基于与因果公平性概念密切相关的指标进行评估；同样，为促进统计公平性而设计的基于抽样的方法很少受到因果公平性的评估。在这项工作中，我们评估了在NLP模型中处理性别偏见的统计性和因果性去偏置方法，发现虽然这些方法能够降低偏见，但并不一定会改善其他偏见指标。我们展示了统计性和因果性去偏置技术的组合能够减少通过这两种类型指标衡量的偏见。",
    "tldr": "本研究评估了在NLP模型中同时处理统计和因果性别公平性偏见的方法，发现结合统计和因果性去偏置技术能够有效减少偏见。"
}