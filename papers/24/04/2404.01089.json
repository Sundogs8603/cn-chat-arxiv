{
    "title": "Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On",
    "abstract": "arXiv:2404.01089v1 Announce Type: cross  Abstract: Image-based virtual try-on is an increasingly important task for online shopping. It aims to synthesize images of a specific person wearing a specified garment. Diffusion model-based approaches have recently become popular, as they are excellent at image synthesis tasks. However, these approaches usually employ additional image encoders and rely on the cross-attention mechanism for texture transfer from the garment to the person image, which affects the try-on's efficiency and fidelity. To address these issues, we propose an Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the fidelity of the results and introduces no additional image encoders. Accordingly, we make contributions from two aspects. First, we propose to concatenate the masked person and reference garment images along the spatial dimension and utilize the resulting image as the input for the diffusion model's denoising UNet. This enables the orig",
    "link": "https://arxiv.org/abs/2404.01089",
    "context": "Title: Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On\nAbstract: arXiv:2404.01089v1 Announce Type: cross  Abstract: Image-based virtual try-on is an increasingly important task for online shopping. It aims to synthesize images of a specific person wearing a specified garment. Diffusion model-based approaches have recently become popular, as they are excellent at image synthesis tasks. However, these approaches usually employ additional image encoders and rely on the cross-attention mechanism for texture transfer from the garment to the person image, which affects the try-on's efficiency and fidelity. To address these issues, we propose an Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the fidelity of the results and introduces no additional image encoders. Accordingly, we make contributions from two aspects. First, we propose to concatenate the masked person and reference garment images along the spatial dimension and utilize the resulting image as the input for the diffusion model's denoising UNet. This enables the orig",
    "path": "papers/24/04/2404.01089.json",
    "total_tokens": 859,
    "translated_title": "高保真虚拟试穿的保持纹理扩散模型",
    "translated_abstract": "图像虚拟试穿是在线购物中越来越重要的任务。它旨在合成特定人物穿着指定服装的图像。最近，基于扩散模型的方法变得流行起来，因为它们在图像合成任务中表现出色。然而，这些方法通常使用额外的图像编码器，并依赖交叉注意机制进行从服装到人物图像的纹理传输，这影响了试穿的效率和保真度。为了解决这些问题，我们提出了一种保持纹理扩散（TPD）模型进行虚拟试穿，它增强了结果的保真度并不引入额外的图像编码器。因此，我们从两个方面做出了贡献。首先，我们建议沿空间维度连接遮罩人物和参考服装图像，并利用生成的图像作为扩散模型去噪UNet的输入。",
    "tldr": "我们提出了一种Texture-Preserving Diffusion（TPD）模型，通过在空间维度连接遮罩人物和参考服装图像，增强了虚拟试穿的保真度，同时不引入额外的图像编码器。",
    "en_tdlr": "We propose a Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the fidelity of the results by concatenating masked person and reference garment images along the spatial dimension without introducing additional image encoders."
}