{
    "title": "Can Language Models Recognize Convincing Arguments?",
    "abstract": "arXiv:2404.00750v1 Announce Type: new  Abstract: The remarkable and ever-increasing capabilities of Large Language Models (LLMs) have raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda. To gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans, we propose studying their performance on the related task of detecting convincing arguments. We extend a dataset by Durmus & Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, even surpassing human performance. The data and code released with ",
    "link": "https://arxiv.org/abs/2404.00750",
    "context": "Title: Can Language Models Recognize Convincing Arguments?\nAbstract: arXiv:2404.00750v1 Announce Type: new  Abstract: The remarkable and ever-increasing capabilities of Large Language Models (LLMs) have raised concerns about their potential misuse for creating personalized, convincing misinformation and propaganda. To gain insights into LLMs' persuasive capabilities without directly engaging in experimentation with humans, we propose studying their performance on the related task of detecting convincing arguments. We extend a dataset by Durmus & Cardie (2018) with debates, votes, and user traits and propose tasks measuring LLMs' ability to (1) distinguish between strong and weak arguments, (2) predict stances based on beliefs and demographic characteristics, and (3) determine the appeal of an argument to an individual based on their traits. We show that LLMs perform on par with humans in these tasks and that combining predictions from different LLMs yields significant performance gains, even surpassing human performance. The data and code released with ",
    "path": "papers/24/04/2404.00750.json",
    "total_tokens": 875,
    "translated_title": "大语言模型能识别令人信服的论点吗？",
    "translated_abstract": "大型语言模型（LLMs）的显著且不断增强的能力引发了人们对它们可能被滥用用来创造个性化、令人信服的虚假信息和宣传的担忧。为了深入了解LLMs的说服能力，而又不直接与人类进行实验，我们提出研究它们在检测令人信服的论点任务上的表现。我们通过添加辩论、投票和用户特征来扩展了Durmus和Cardie（2018）的数据集，并提出了衡量LLMs能力的任务，包括（1）区分强势和弱势论点，（2）基于信念和人口特征预测立场，以及（3）根据个人特征确定对一个论点的吸引力。我们发现LLMs在这些任务中表现与人类不相上下，并且结合不同LLMs的预测可以获得显著的性能提升，甚至超过人类的表现。随文附带发布的数据和代码。",
    "tldr": "大语言模型不仅能够在识别和区分强势和弱势论点方面表现良好，还可以根据用户的信念和人口特征预测其立场，并确定论点对个人的吸引力。"
}