{
    "title": "DailyMAE: Towards Pretraining Masked Autoencoders in One Day",
    "abstract": "arXiv:2404.00509v1 Announce Type: new  Abstract: Recently, masked image modeling (MIM), an important self-supervised learning (SSL) method, has drawn attention for its effectiveness in learning data representation from unlabeled data. Numerous studies underscore the advantages of MIM, highlighting how models pretrained on extensive datasets can enhance the performance of downstream tasks. However, the high computational demands of pretraining pose significant challenges, particularly within academic environments, thereby impeding the SSL research progress. In this study, we propose efficient training recipes for MIM based SSL that focuses on mitigating data loading bottlenecks and employing progressive training techniques and other tricks to closely maintain pretraining performance. Our library enables the training of a MAE-Base/16 model on the ImageNet 1K dataset for 800 epochs within just 18 hours, using a single machine equipped with 8 A100 GPUs. By achieving speed gains of up to 5.",
    "link": "https://arxiv.org/abs/2404.00509",
    "context": "Title: DailyMAE: Towards Pretraining Masked Autoencoders in One Day\nAbstract: arXiv:2404.00509v1 Announce Type: new  Abstract: Recently, masked image modeling (MIM), an important self-supervised learning (SSL) method, has drawn attention for its effectiveness in learning data representation from unlabeled data. Numerous studies underscore the advantages of MIM, highlighting how models pretrained on extensive datasets can enhance the performance of downstream tasks. However, the high computational demands of pretraining pose significant challenges, particularly within academic environments, thereby impeding the SSL research progress. In this study, we propose efficient training recipes for MIM based SSL that focuses on mitigating data loading bottlenecks and employing progressive training techniques and other tricks to closely maintain pretraining performance. Our library enables the training of a MAE-Base/16 model on the ImageNet 1K dataset for 800 epochs within just 18 hours, using a single machine equipped with 8 A100 GPUs. By achieving speed gains of up to 5.",
    "path": "papers/24/04/2404.00509.json",
    "total_tokens": 905,
    "translated_title": "DailyMAE：朝着一天预训练的遮蔽自动编码器迈进",
    "translated_abstract": "最近，遮蔽图像建模（MIM）作为一种重要的自监督学习（SSL）方法，因其在从未标记数据中学习数据表示方面的有效性而引起关注。许多研究强调了MIM的优势，突显出在广泛数据集上预训练的模型如何提高下游任务的性能。然而，预训练的高计算需求带来了重大挑战，特别是在学术环境中，从而阻碍了SSL研究的进展。在本研究中，我们提出了用于MIM的高效训练配方的建议，重点是减轻数据加载瓶颈，并采用渐进训练技术和其他技巧以紧密维持预训练性能。我们的库使得在仅18小时内通过单台配备8颗A100 GPU的机器训练MAE-Base/16模型在ImageNet 1K数据集上进行800个epochs成为可能。通过获得高达5倍的速度提升。",
    "tldr": "该研究提出了一种针对遮蔽自动编码器（MAE）的高效训练方法，使得能够在短短18小时内使用单台机器训练模型，取得了高达5倍的速度提升。",
    "en_tdlr": "This study introduces an efficient training approach for Masked Autoencoders (MAE), allowing the training of models in just 18 hours on a single machine and achieving up to 5 times speed gains."
}