{
    "title": "Low-resource neural machine translation with morphological modeling",
    "abstract": "arXiv:2404.02392v1 Announce Type: new  Abstract: Morphological modeling in neural machine translation (NMT) is a promising approach to achieving open-vocabulary machine translation for morphologically-rich languages. However, existing methods such as sub-word tokenization and character-based models are limited to the surface forms of the words. In this work, we propose a framework-solution for modeling complex morphology in low-resource settings. A two-tier transformer architecture is chosen to encode morphological information at the inputs. At the target-side output, a multi-task multi-label training scheme coupled with a beam search-based decoder are found to improve machine translation performance. An attention augmentation scheme to the transformer model is proposed in a generic form to allow integration of pre-trained language models and also facilitate modeling of word order relationships between the source and target languages. Several data augmentation techniques are evaluated ",
    "link": "https://arxiv.org/abs/2404.02392",
    "context": "Title: Low-resource neural machine translation with morphological modeling\nAbstract: arXiv:2404.02392v1 Announce Type: new  Abstract: Morphological modeling in neural machine translation (NMT) is a promising approach to achieving open-vocabulary machine translation for morphologically-rich languages. However, existing methods such as sub-word tokenization and character-based models are limited to the surface forms of the words. In this work, we propose a framework-solution for modeling complex morphology in low-resource settings. A two-tier transformer architecture is chosen to encode morphological information at the inputs. At the target-side output, a multi-task multi-label training scheme coupled with a beam search-based decoder are found to improve machine translation performance. An attention augmentation scheme to the transformer model is proposed in a generic form to allow integration of pre-trained language models and also facilitate modeling of word order relationships between the source and target languages. Several data augmentation techniques are evaluated ",
    "path": "papers/24/04/2404.02392.json",
    "total_tokens": 849,
    "translated_title": "具有形态建模的低资源神经机器翻译",
    "translated_abstract": "在神经机器翻译（NMT）中进行形态建模是实现形态丰富语言开放词汇机器翻译的一种有前景的方法。然而，现有的方法如子词标记化和基于字符的模型局限于单词的表面形式。在这项工作中，我们提出了一个用于在低资源环境中建模复杂形态的框架解决方案。选择了一个两级变压器架构来对输入的形态信息进行编码。在目标端输出时，多任务多标签训练方案结合基于beam search的解码器被发现可以提高机器翻译性能。提出了一种变压器模型的注意力增强方案，以通用形式允许集成预训练语言模型，并促进源语言和目标语言之间的单词顺序关系建模。评估了几种数据增强技术。",
    "tldr": "该论文提出了一种在低资源环境中建模复杂形态的神经机器翻译方法，通过两级变压器架构编码形态信息，并利用多任务多标签训练方案和基于beam search的解码器来提高翻译性能。"
}