{
    "title": "When does Subagging Work?",
    "abstract": "arXiv:2404.01832v1 Announce Type: cross  Abstract: We study the effectiveness of subagging, or subsample aggregating, on regression trees, a popular non-parametric method in machine learning. First, we give sufficient conditions for pointwise consistency of trees. We formalize that (i) the bias depends on the diameter of cells, hence trees with few splits tend to be biased, and (ii) the variance depends on the number of observations in cells, hence trees with many splits tend to have large variance. While these statements for bias and variance are known to hold globally in the covariate space, we show that, under some constraints, they are also true locally. Second, we compare the performance of subagging to that of trees across different numbers of splits. We find that (1) for any given number of splits, subagging improves upon a single tree, and (2) this improvement is larger for many splits than it is for few splits. However, (3) a single tree grown at optimal size can outperform su",
    "link": "https://arxiv.org/abs/2404.01832",
    "context": "Title: When does Subagging Work?\nAbstract: arXiv:2404.01832v1 Announce Type: cross  Abstract: We study the effectiveness of subagging, or subsample aggregating, on regression trees, a popular non-parametric method in machine learning. First, we give sufficient conditions for pointwise consistency of trees. We formalize that (i) the bias depends on the diameter of cells, hence trees with few splits tend to be biased, and (ii) the variance depends on the number of observations in cells, hence trees with many splits tend to have large variance. While these statements for bias and variance are known to hold globally in the covariate space, we show that, under some constraints, they are also true locally. Second, we compare the performance of subagging to that of trees across different numbers of splits. We find that (1) for any given number of splits, subagging improves upon a single tree, and (2) this improvement is larger for many splits than it is for few splits. However, (3) a single tree grown at optimal size can outperform su",
    "path": "papers/24/04/2404.01832.json",
    "total_tokens": 951,
    "translated_title": "何时使用Subagging？",
    "translated_abstract": "我们研究了在机器学习中一种流行的非参数方法——回归树上，子抽样聚合（subagging）的有效性。首先，我们给出了树的逐点一致性的充分条件。我们明确了（i）偏差取决于单元的直径，因此，具有少数分裂的树倾向于存在偏差，以及（ii）方差取决于单元中的观测数量，因此，具有许多分裂的树倾向于具有较大的方差。虽然这些关于偏差和方差的陈述在协变量空间中是全局适用的，我们展示了，在某些约束条件下，它们在局部也是成立的。第二，我们比较了子抽样聚合和具有不同分裂数的树的性能。我们发现，对于任何给定的分裂数，子抽样聚合都优于单棵树，并且这种改进在较多分裂的情况下比较少分裂的情况下更大。然而，一个以最佳大小生长的单棵树可以优于子抽样聚合。",
    "tldr": "研究展示了在机器学习中一种流行的非参数方法——回归树上，子抽样聚合（subagging）的有效性，并发现对于任何给定的分裂数，subagging都可以优于单棵树，并且在较多分裂的情况下改进更大。"
}