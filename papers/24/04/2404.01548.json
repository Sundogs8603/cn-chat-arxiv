{
    "title": "mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning",
    "abstract": "arXiv:2404.01548v1 Announce Type: cross  Abstract: In the fields of computer vision and natural language processing, multimodal chart question-answering, especially involving color, structure, and textless charts, poses significant challenges. Traditional methods, which typically involve either direct multimodal processing or a table-to-text conversion followed by language model analysis, have limitations in effectively handling these complex scenarios. This paper introduces a novel multimodal chart question-answering model, specifically designed to address these intricate tasks. Our model integrates visual and linguistic processing, overcoming the constraints of existing methods. We adopt a dual-phase training approach: the initial phase focuses on aligning image and text representations, while the subsequent phase concentrates on optimizing the model's interpretative and analytical abilities in chart-related queries. This approach has demonstrated superior performance on multiple pub",
    "link": "https://arxiv.org/abs/2404.01548",
    "context": "Title: mChartQA: A universal benchmark for multimodal Chart Question Answer based on Vision-Language Alignment and Reasoning\nAbstract: arXiv:2404.01548v1 Announce Type: cross  Abstract: In the fields of computer vision and natural language processing, multimodal chart question-answering, especially involving color, structure, and textless charts, poses significant challenges. Traditional methods, which typically involve either direct multimodal processing or a table-to-text conversion followed by language model analysis, have limitations in effectively handling these complex scenarios. This paper introduces a novel multimodal chart question-answering model, specifically designed to address these intricate tasks. Our model integrates visual and linguistic processing, overcoming the constraints of existing methods. We adopt a dual-phase training approach: the initial phase focuses on aligning image and text representations, while the subsequent phase concentrates on optimizing the model's interpretative and analytical abilities in chart-related queries. This approach has demonstrated superior performance on multiple pub",
    "path": "papers/24/04/2404.01548.json",
    "total_tokens": 838,
    "translated_title": "mChartQA：基于视觉-语言对齐和推理的多模态图表问答通用基准",
    "translated_abstract": "在计算机视觉和自然语言处理领域，多模态图表问答，特别是涉及颜色、结构和无文本图表，带来了重大挑战。传统方法通常涉及直接多模态处理或表格到文本转换，然后进行语言模型分析，但在有效处理这些复杂场景方面存在局限性。本文介绍了一种新颖的多模态图表问答模型，专门设计来解决这些复杂任务。我们的模型整合了视觉和语言处理，克服了现有方法的限制。我们采用了双阶段训练方法：初始阶段专注于调整图像和文本表示，随后的阶段集中于优化模型在图表相关查询中的解释和分析能力。这种方法在多个公开基准上展现出优越的性能。",
    "tldr": "本文提出了一种新颖的多模态图表问答模型，采用双阶段训练方法，通过整合视觉和语言处理解决多模态问答中复杂的颜色、结构和无文本图表挑战。",
    "en_tdlr": "This paper introduces a novel multimodal chart question-answering model with a dual-phase training approach that integrates visual and linguistic processing to address complex challenges in color, structure, and textless charts in multimodal question answering."
}