{
    "title": "ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model",
    "abstract": "arXiv:2404.02534v1 Announce Type: cross  Abstract: In recent years, the development of pre-trained language models (PLMs) has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate knowledge transfer across diverse languages. However, this progress has predominantly bypassed the inclusion of very-low resource languages, creating a notable void in the multilingual landscape. This paper addresses this gap by introducing four tailored PLMs specifically finetuned for Angolan languages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In this paper, we survey the role of informed embedding initialization and synthetic data in enhancing the performance of MAFT models in downstream tasks. We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA (an effective embedding initialization) by 12.3 and 3.8 points respectively.",
    "link": "https://arxiv.org/abs/2404.02534",
    "context": "Title: ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model\nAbstract: arXiv:2404.02534v1 Announce Type: cross  Abstract: In recent years, the development of pre-trained language models (PLMs) has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate knowledge transfer across diverse languages. However, this progress has predominantly bypassed the inclusion of very-low resource languages, creating a notable void in the multilingual landscape. This paper addresses this gap by introducing four tailored PLMs specifically finetuned for Angolan languages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In this paper, we survey the role of informed embedding initialization and synthetic data in enhancing the performance of MAFT models in downstream tasks. We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA (an effective embedding initialization) by 12.3 and 3.8 points respectively.",
    "path": "papers/24/04/2404.02534.json",
    "total_tokens": 880,
    "translated_title": "ANGOFA：利用OFA嵌入初始化和合成数据的安哥拉语言模型",
    "translated_abstract": "近年来，预训练语言模型（PLMs）的发展势头迅猛，展示了它们超越语言障碍、促进跨多种语言的知识转移的能力。然而，这一进展主要忽视了极低资源语言的包含，导致多语言景观中出现明显的空白。本文通过引入四个定制的PLM，专门为安哥拉语言进行微调，采用多语言自适应微调（MAFT）方法，以填补这一空白。我们调查了信息嵌入初始化和合成数据在增强MAFT模型在下游任务中性能方面的作用。我们将基线提高了12.3个百分点，超过了通过MAFT开发的SOTA AfroXLMR-base和有效嵌入初始化OFA分别提高了3.8个百分点。",
    "tldr": "本文介绍了四个专门针对安哥拉语言进行微调的PLM，并使用多语言自适应微调（MAFT）方法，通过采用知情嵌入初始化和合成数据，提高了MAFT模型在下游任务中的性能，将基线提高了12.3个百分点，超越了SOTA AfroXLMR-base和OFA。",
    "en_tdlr": "This paper introduces four tailored PLMs specifically finetuned for Angolan languages using Multilingual Adaptive Fine-tuning (MAFT), improving performance in downstream tasks through informed embedding initialization and synthetic data, surpassing SOTA AfroXLMR-base and OFA by 12.3 and 3.8 points respectively."
}