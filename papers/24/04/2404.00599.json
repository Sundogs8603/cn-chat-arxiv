{
    "title": "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories",
    "abstract": "arXiv:2404.00599v1 Announce Type: cross  Abstract: How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repo",
    "link": "https://arxiv.org/abs/2404.00599",
    "context": "Title: EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories\nAbstract: arXiv:2404.00599v1 Announce Type: cross  Abstract: How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repo",
    "path": "papers/24/04/2404.00599.json",
    "total_tokens": 903,
    "translated_title": "EvoCodeBench: 一个与现实世界代码库对齐的进化代码生成基准",
    "translated_abstract": "arXiv:2404.00599v1 公告类型: 跨界 摘要: 如何评估大型语言模型(LLMs)在代码生成中的表现是一个悬而未决的问题。现有的基准测试表现出与现实世界代码库的差距，不足以评估LLMs的编码能力。本文提出了一个新的基准测试 - EvoCodeBench来解决前述问题，其具有三个主要进展。(1) EvoCodeBench在多个维度上与现实世界库对齐，例如代码分布和依赖分布。(2) EvoCodeBench提供了全面的注释(例如需求、参考代码和参考依赖)，以及强大的评估度量标准(例如Pass@k和Recall@k)。(3) EvoCodeBench是一个不断发展的基准测试，以避免数据泄漏。我们构建了一个自动化流水线，从最新的库中更新EvoCodeBench。我们发布了第一个版本 - EvoCodeBench-2403，其中包含来自25个现实世界库的275个样本。基于EvoCodeBench，我们提出repo",
    "tldr": "EvoCodeBench 是一个与现实世界代码库对齐的进化代码生成基准，具有完善的注释和评估度量标准，同时避免数据泄露。",
    "en_tdlr": "EvoCodeBench is an evolving code generation benchmark aligned with real-world code repositories, featuring comprehensive annotations and evaluation metrics while avoiding data leakage."
}