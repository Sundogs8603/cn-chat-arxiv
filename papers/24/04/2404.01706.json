{
    "title": "Polarity Calibration for Opinion Summarization",
    "abstract": "arXiv:2404.01706v1 Announce Type: new  Abstract: Opinion summarization is automatically generating summaries from a variety of subjective information, such as product reviews or political opinions. The challenge of opinions summarization lies in presenting divergent or even conflicting opinions. We conduct an analysis of previous summarization models, which reveals their inclination to amplify the polarity bias, emphasizing the majority opinions while ignoring the minority opinions. To address this issue and make the summarizer express both sides of opinions, we introduce the concept of polarity calibration, which aims to align the polarity of output summary with that of input text. Specifically, we develop a reinforcement training approach for polarity calibration. This approach feeds the polarity distance between output summary and input text as reward into the summarizer, and also balance polarity calibration with content preservation and language naturality. We evaluate our Polarit",
    "link": "https://arxiv.org/abs/2404.01706",
    "context": "Title: Polarity Calibration for Opinion Summarization\nAbstract: arXiv:2404.01706v1 Announce Type: new  Abstract: Opinion summarization is automatically generating summaries from a variety of subjective information, such as product reviews or political opinions. The challenge of opinions summarization lies in presenting divergent or even conflicting opinions. We conduct an analysis of previous summarization models, which reveals their inclination to amplify the polarity bias, emphasizing the majority opinions while ignoring the minority opinions. To address this issue and make the summarizer express both sides of opinions, we introduce the concept of polarity calibration, which aims to align the polarity of output summary with that of input text. Specifically, we develop a reinforcement training approach for polarity calibration. This approach feeds the polarity distance between output summary and input text as reward into the summarizer, and also balance polarity calibration with content preservation and language naturality. We evaluate our Polarit",
    "path": "papers/24/04/2404.01706.json",
    "total_tokens": 806,
    "translated_title": "观点总结的极性校准",
    "translated_abstract": "Opinion summarization 是自动从各种主观信息中生成摘要，如产品评论或政治观点。观点总结的挑战在于呈现不同或甚至相互矛盾的观点。我们对先前的总结模型进行了分析，发现它们倾向于放大极性偏见，强调大多数意见，而忽略少数派观点。为解决这个问题并让总结器表达两方观点，我们引入了极性校准的概念，旨在使输出摘要的极性与输入文本一致。具体来说，我们开发了一种强化训练方法用于极性校准。该方法将输出摘要与输入文本之间的极性距离作为奖励输入到总结器中，并平衡极性校准、内容保留和语言自然性。我们评估了我们的 Polarit",
    "tldr": "引入极性校准概念，开发强化训练方法，通过平衡极性校准、内容保留和语言自然性，解决观点总结中放大极性偏见的问题。",
    "en_tdlr": "Introducing the concept of polarity calibration and developing a reinforcement training approach to address the issue of amplifying polarity bias in opinion summarization by balancing polarity calibration with content preservation and language naturality."
}