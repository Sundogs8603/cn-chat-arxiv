{
    "title": "LLM meets Vision-Language Models for Zero-Shot One-Class Classification",
    "abstract": "arXiv:2404.00675v1 Announce Type: cross  Abstract: We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to ",
    "link": "https://arxiv.org/abs/2404.00675",
    "context": "Title: LLM meets Vision-Language Models for Zero-Shot One-Class Classification\nAbstract: arXiv:2404.00675v1 Announce Type: cross  Abstract: We consider the problem of zero-shot one-class visual classification. In this setting, only the label of the target class is available, and the goal is to discriminate between positive and negative query samples without requiring any validation example from the target task. We propose a two-step solution that first queries large language models for visually confusing objects and then relies on vision-language pre-trained models (e.g., CLIP) to perform classification. By adapting large-scale vision benchmarks, we demonstrate the ability of the proposed method to outperform adapted off-the-shelf alternatives in this setting. Namely, we propose a realistic benchmark where negative query samples are drawn from the same original dataset as positive ones, including a granularity-controlled version of iNaturalist, where negative samples are at a fixed distance in the taxonomy tree from the positive ones. Our work shows that it is possible to ",
    "path": "papers/24/04/2404.00675.json",
    "total_tokens": 856,
    "translated_title": "LLM meets Vision-Language Models用于零样本单类分类",
    "translated_abstract": "我们考虑零样本单类视觉分类问题。在这种情况下，仅目标类别的标签可用，并且目标是在不需要来自目标任务的任何验证示例的情况下区分正负查询样本。我们提出了一个两步解决方案，首先查询大型语言模型以查找在视觉上令人困惑的对象，然后依赖于视觉语言预训练模型（例如CLIP）进行分类。通过调整大规模视觉基准，我们展示了所提出的方法在此设置中优于自适应的现成替代方法的能力。具体而言，我们提出了一个实际的基准，其中负查询样本从与正查询样本相同的原始数据集中获取，包括iNaturalist的细粒度控制版本，其中负样本在分类树中与正样本相距固定距离。我们的工作表明，可以通过查询大型语言模型并依赖视觉-语言预训练模型来解决零样本单类分类问题。",
    "tldr": "提出了一种两步解决方案，结合大型语言模型和视觉-语言预训练模型，用于零样本单类分类问题，并在实际基准测试中表现出优越性能。",
    "en_tdlr": "Proposed a two-step solution that combines large language models and vision-language pre-trained models for zero-shot one-class classification, demonstrating superior performance in practical benchmarks."
}