{
    "title": "Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack",
    "abstract": "arXiv:2404.01907v1 Announce Type: new  Abstract: With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks such as paraphrasing. In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model's robustness against such attacks. The empirical results reveal that the current detection mode",
    "link": "https://arxiv.org/abs/2404.01907",
    "context": "Title: Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack\nAbstract: arXiv:2404.01907v1 Announce Type: new  Abstract: With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks such as paraphrasing. In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model's robustness against such attacks. The empirical results reveal that the current detection mode",
    "path": "papers/24/04/2404.01907.json",
    "total_tokens": 906,
    "translated_title": "人性化机器生成的内容：通过对抗攻击规避AI文本检测",
    "translated_abstract": "随着大型语言模型（LLMs）的发展，检测文本是否由机器生成在面对诸如误传信息、保护知识产权和预防学术抄袭等恶意用例时变得日益具有挑战性。虽然经过良好训练的文本检测器在未知测试数据上展现了有希望的性能，但最近的研究表明，这些检测器在处理对抗攻击（如释义）时存在漏洞。本文提出了一个更广泛类别的对抗攻击框架，旨在对机器生成的内容进行微小扰动以规避检测。我们考虑了两种攻击设置：白盒和黑盒，并在动态场景中采用对抗学习来评估当前检测模型对此类攻击的鲁棒性潜力增强。实证结果表明，目前的检测模型",
    "tldr": "本文提出了一个旨在对机器生成的文本进行微小扰动以规避检测的广泛对抗攻击框架，通过白盒和黑盒两种攻击设置以及对抗学习在动态场景中的应用，评估了当前检测模型对此类攻击的鲁棒性潜力增强"
}