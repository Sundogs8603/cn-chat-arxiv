{
    "title": "Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models",
    "abstract": "arXiv:2404.01231v1 Announce Type: cross  Abstract: It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly a",
    "link": "https://arxiv.org/abs/2404.01231",
    "context": "Title: Privacy Backdoors: Enhancing Membership Inference through Poisoning Pre-trained Models\nAbstract: arXiv:2404.01231v1 Announce Type: cross  Abstract: It is commonplace to produce application-specific models by fine-tuning large pre-trained models using a small bespoke dataset. The widespread availability of foundation model checkpoints on the web poses considerable risks, including the vulnerability to backdoor attacks. In this paper, we unveil a new vulnerability: the privacy backdoor attack. This black-box privacy attack aims to amplify the privacy leakage that arises when fine-tuning a model: when a victim fine-tunes a backdoored model, their training data will be leaked at a significantly higher rate than if they had fine-tuned a typical model. We conduct extensive experiments on various datasets and models, including both vision-language models (CLIP) and large language models, demonstrating the broad applicability and effectiveness of such an attack. Additionally, we carry out multiple ablation studies with different fine-tuning methods and inference strategies to thoroughly a",
    "path": "papers/24/04/2404.01231.json",
    "total_tokens": 875,
    "translated_title": "隐私后门: 通过污染预训练模型增强成员推理",
    "translated_abstract": "通过使用小型定制数据集微调大型预训练模型来生成特定应用程序模型已经司空见惯。网络上基础模型检查点的广泛可用性存在重大风险，包括易受后门攻击的脆弱性。本文揭示了一种新的漏洞：隐私后门攻击。这种黑盒隐私攻击旨在增强模型微调时产生的隐私泄露：当受害者微调一个带有后门的模型时，他们的训练数据泄露速率会比微调典型模型时显著提高。我们在各种数据集和模型上进行了大量实验，包括视觉-语言模型（CLIP）和大型语言模型，展示了这种攻击的广泛适用性和有效性。此外，我们进行了多个消融研究，使用不同的微调方法和推理策略进行彻底分析。",
    "tldr": "本文揭示了一种新的隐私后门攻击漏洞，通过污染预训练模型来增强成员推理，在多个数据集和模型上进行了广泛实验，并展示了攻击的广泛适用性和有效性。",
    "en_tdlr": "This paper reveals a new privacy backdoor attack vulnerability, enhancing membership inference through poisoning pre-trained models, conducting extensive experiments on various datasets and models, and demonstrating the broad applicability and effectiveness of the attack."
}