{
    "title": "Conditional Pseudo-Reversible Normalizing Flow for Surrogate Modeling in Quantifying Uncertainty Propagation",
    "abstract": "arXiv:2404.00502v1 Announce Type: new  Abstract: We introduce a conditional pseudo-reversible normalizing flow for constructing surrogate models of a physical model polluted by additive noise to efficiently quantify forward and inverse uncertainty propagation. Existing surrogate modeling approaches usually focus on approximating the deterministic component of physical model. However, this strategy necessitates knowledge of noise and resorts to auxiliary sampling methods for quantifying inverse uncertainty propagation. In this work, we develop the conditional pseudo-reversible normalizing flow model to directly learn and efficiently generate samples from the conditional probability density functions. The training process utilizes dataset consisting of input-output pairs without requiring prior knowledge about the noise and the function. Our model, once trained, can generate samples from any conditional probability density functions whose high probability regions are covered by the train",
    "link": "https://arxiv.org/abs/2404.00502",
    "context": "Title: Conditional Pseudo-Reversible Normalizing Flow for Surrogate Modeling in Quantifying Uncertainty Propagation\nAbstract: arXiv:2404.00502v1 Announce Type: new  Abstract: We introduce a conditional pseudo-reversible normalizing flow for constructing surrogate models of a physical model polluted by additive noise to efficiently quantify forward and inverse uncertainty propagation. Existing surrogate modeling approaches usually focus on approximating the deterministic component of physical model. However, this strategy necessitates knowledge of noise and resorts to auxiliary sampling methods for quantifying inverse uncertainty propagation. In this work, we develop the conditional pseudo-reversible normalizing flow model to directly learn and efficiently generate samples from the conditional probability density functions. The training process utilizes dataset consisting of input-output pairs without requiring prior knowledge about the noise and the function. Our model, once trained, can generate samples from any conditional probability density functions whose high probability regions are covered by the train",
    "path": "papers/24/04/2404.00502.json",
    "total_tokens": 821,
    "translated_title": "条件伪可逆标准化流在量化不确定传播中的代理建模中的应用",
    "translated_abstract": "我们引入了一种条件伪可逆标准化流用于构建物理模型受附加噪声污染的代理模型，以有效量化正向和反向不确定传播。现有的代理建模方法通常专注于近似物理模型的确定性部分。然而，这种策略需要对噪声有所了解，并且借助辅助采样方法来量化反向不确定传播。在这项工作中，我们开发了条件伪可逆标准化流模型，以直接学习和高效地生成样本从条件概率密度函数。训练过程利用由输入-输出对组成的数据集，无需事先了解噪声和函数。一旦训练完成，我们的模型可以从任何高概率区域被训练覆盖的条件概率密度函数中生成样本。",
    "tldr": "该论文引入了一种条件伪可逆标准化流模型，可以有效构建代理模型，无需事先了解噪声和函数，直接学习并高效生成样本，用于量化不确定性传播中的正向和反向传播。",
    "en_tdlr": "This paper introduces a conditional pseudo-reversible normalizing flow model for constructing surrogate models efficiently quantifying forward and inverse uncertainty propagation without prior knowledge of noise and function, by directly learning and generating samples from conditional probability density functions."
}