{
    "title": "uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?",
    "abstract": "arXiv:2404.02474v1 Announce Type: cross  Abstract: Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. F",
    "link": "https://arxiv.org/abs/2404.02474",
    "context": "Title: uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?\nAbstract: arXiv:2404.02474v1 Announce Type: cross  Abstract: Inspired by human cognition, Jiang et al.(2023c) create a benchmark for assessing LLMs' lateral thinking-thinking outside the box. Building upon this benchmark, we investigate how different prompting methods enhance LLMs' performance on this task to reveal their inherent power for outside-the-box thinking ability. Through participating in SemEval-2024, task 9, Sentence Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT) and direct prompting, enhancing with informative descriptions, and employing contextualizing prompts using a retrieval augmented generation (RAG) pipeline. Our experiments involve three LLMs including GPT-3.5, GPT-4, and Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and options using GPT-4, validated by humans for quality. Findings indicate that compressed informative prompts enhance performance. Dynamic in-context learning enhances model performance significantly. F",
    "path": "papers/24/04/2404.02474.json",
    "total_tokens": 927,
    "translated_title": "uTeBC-NLP在SemEval-2024任务9中：LLMs能成为横向思考者吗？",
    "translated_abstract": "受人类认知启发，Jiang等人（2023c）创建了一个用于评估LLMs横向思维（超越思维定势）的基准。在这一基准的基础上，我们研究了不同的提示方法如何增强LLMs在这一任务上的表现，以揭示其固有的超越思维能力。通过参加SemEval-2024的第9项任务，即句子拼图子任务，我们探讨了提示工程方法：思维链（CoT）和直接提示，使用信息性描述进行增强，并利用检索增强生成（RAG）管道进行情境化提示。我们的实验涉及三种LLMs，包括GPT-3.5、GPT-4和Zephyr-7B-beta。我们使用GPT-4生成了谜题和选项之间的思维路径数据集，并通过人类进行了质量验证。研究结果表明，压缩的信息性提示能够提升模型性能。动态的情境学习显著提升了模型性能。",
    "tldr": "通过研究提示工程方法如何增强LLMs在横向思考任务上的表现，揭示了其固有的超越思维能力，并发现压缩的信息性提示和动态的情境学习显著提升了模型性能。"
}