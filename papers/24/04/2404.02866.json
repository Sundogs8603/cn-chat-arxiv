{
    "title": "Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds",
    "abstract": "arXiv:2404.02866v1 Announce Type: new  Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as \"features\" (or, less commonly, as \"embeddings\" or \"feature embeddings\"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, \"MNIST\" and \"CIFAR-10,\" which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guar",
    "link": "https://arxiv.org/abs/2404.02866",
    "context": "Title: Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds\nAbstract: arXiv:2404.02866v1 Announce Type: new  Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as \"features\" (or, less commonly, as \"embeddings\" or \"feature embeddings\"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, \"MNIST\" and \"CIFAR-10,\" which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guar",
    "path": "papers/24/04/2404.02866.json",
    "total_tokens": 797,
    "translated_title": "通过Hammersley-Chapman-Robbins界限保证机密性",
    "translated_abstract": "在深度神经网络推断过程中通过向最后几层的激活添加噪声来保护隐私是可能的。这些层中的激活被称为“特征”（少见的称为“嵌入”或“特征嵌入”）。添加的噪声有助于防止从嘈杂的特征中重建输入。通过对所有可能的无偏估计量的方差进行下限估计，量化了由此添加的噪声产生的机密性。经典不等式Hammersley和Chapman以及Robbins提供便利的、可计算的界限-- HCR界限。数值实验表明，对于包含10个类别的图像分类数据集“MNIST”和“CIFAR-10”，HCR界限在小型神经网络上表现良好。HCR界限似乎单独无法保证",
    "tldr": "通过添加噪声到最后层的激活来保护隐私，使用HCR界限可量化保护机密性的可信度",
    "en_tdlr": "Protect privacy during inference by adding noise to activations in last layers, with HCR bounds quantifying the confidentiality arising from the noise."
}