{
    "title": "Convergence Analysis of Flow Matching in Latent Space with Transformers",
    "abstract": "arXiv:2404.02538v1 Announce Type: cross  Abstract: We present theoretical convergence guarantees for ODE-based generative models, specifically flow matching. We use a pre-trained autoencoder network to map high-dimensional original inputs to a low-dimensional latent space, where a transformer network is trained to predict the velocity field of the transformation from a standard normal distribution to the target latent distribution. Our error analysis demonstrates the effectiveness of this approach, showing that the distribution of samples generated via estimated ODE flow converges to the target distribution in the Wasserstein-2 distance under mild and practical assumptions. Furthermore, we show that arbitrary smooth functions can be effectively approximated by transformer networks with Lipschitz continuity, which may be of independent interest.",
    "link": "https://arxiv.org/abs/2404.02538",
    "context": "Title: Convergence Analysis of Flow Matching in Latent Space with Transformers\nAbstract: arXiv:2404.02538v1 Announce Type: cross  Abstract: We present theoretical convergence guarantees for ODE-based generative models, specifically flow matching. We use a pre-trained autoencoder network to map high-dimensional original inputs to a low-dimensional latent space, where a transformer network is trained to predict the velocity field of the transformation from a standard normal distribution to the target latent distribution. Our error analysis demonstrates the effectiveness of this approach, showing that the distribution of samples generated via estimated ODE flow converges to the target distribution in the Wasserstein-2 distance under mild and practical assumptions. Furthermore, we show that arbitrary smooth functions can be effectively approximated by transformer networks with Lipschitz continuity, which may be of independent interest.",
    "path": "papers/24/04/2404.02538.json",
    "total_tokens": 799,
    "translated_title": "流匹配在潜空间中的收敛性分析与Transformer",
    "translated_abstract": "我们提出了ODE-based生成模型，特别是流匹配的理论收敛性保证。我们使用预训练的自编码器网络将高维原始输入映射到低维潜空间，其中一个Transformer网络被训练来预测从标准正态分布到目标潜空间分布的变换速度场。我们的误差分析展示了这种方法的有效性，表明通过估计的ODE流生成样本的分布在温斯坦-2距离下收敛到目标分布，这在温和且实际的假设下成立。此外，我们展示了具有利普希茨连续性的Transformer网络可以有效地逼近任意光滑函数，这可能是独立感兴趣的。",
    "tldr": "该研究提出了一种基于ODE的生成模型中，使用Transformer实现流匹配在潜空间中的理论收敛性保证，并展示了其在估计ODE流生成样本分布时的有效性，同时还证明了具有利普希茨连续性的Transformer网络可以有效逼近任意光滑函数。",
    "en_tdlr": "This study presents theoretical convergence guarantees for ODE-based generative models using Transformers to achieve flow matching in latent space, demonstrating its effectiveness in generating sample distributions via estimated ODE flow and showing the capability of Transformer networks with Lipschitz continuity to effectively approximate arbitrary smooth functions."
}