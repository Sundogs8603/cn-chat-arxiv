{
    "title": "Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark",
    "abstract": "arXiv:2404.00216v1 Announce Type: cross  Abstract: The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3\\%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of pres",
    "link": "https://arxiv.org/abs/2404.00216",
    "context": "Title: Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark\nAbstract: arXiv:2404.00216v1 Announce Type: cross  Abstract: The rapid development of large language models (LLMs) enables them to convey factual knowledge in a more human-like fashion. Extensive efforts have been made to reduce factual hallucinations by modifying LLMs with factuality decoding. However, they also pose risks of hindering knowledge updates, as they make models overly confident in known facts. In this work, we first revisite the current factuality decoding methods and verified their effectiveness in enhancing factual accuracy. Subsequently, we conduct further evaluation of several strong factuality decoding methods on the knowledge editing benchmark. All these decoding methods significantly diminish the performance of llama2 models compared to their original decoding, with the largest decrease being a staggering 81.3\\%. This further indicates that the current existing decoding methods still cannot perfectly address the factual hallucinations, as they overlook the importance of pres",
    "path": "papers/24/04/2404.00216.json",
    "total_tokens": 880,
    "translated_title": "大型语言模型中的事实解码：在知识编辑基准上的评估",
    "translated_abstract": "大型语言模型（LLMs）的快速发展使它们能够以更类似于人类的方式传达事实知识。人们已经做出了大量努力来通过修改LLMs并降低事实幻觉来提高事实准确性。然而，这些修改也存在阻碍知识更新的风险，因为它们使模型对已知事实过于自信。本文首先重新审视当前的事实解码方法，并验证了它们在提高事实准确性方面的有效性。随后，我们对几种强大的事实解码方法在知识编辑基准上进行进一步评估。所有这些解码方法与其原始解码相比均显着降低了llama2模型的性能，其中最大的降低幅度达到惊人的81.3\\%。这进一步表明，当前的解码方法仍无法完全解决事实幻觉问题，因为它们忽视了先验知识的重要性。",
    "tldr": "大型语言模型通过事实解码方法提高了事实准确性，然而，这些方法使模型对已知事实过于自信，进一步评估显示在知识编辑基准上所有解码方法均显著降低了模型性能。",
    "en_tdlr": "Large language models enhance factual accuracy through factuality decoding methods, but these methods make models overly confident in known facts. Further evaluation on the knowledge editing benchmark demonstrates a significant decrease in model performance for all decoding methods."
}