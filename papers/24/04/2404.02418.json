{
    "title": "Auxiliary task demands mask the capabilities of smaller language models",
    "abstract": "arXiv:2404.02418v1 Announce Type: cross  Abstract: Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of \"task demands\" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying competence, combined with the model's ability to interpret and perform the task given its available resources. Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This \"demand gap\" is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpret",
    "link": "https://arxiv.org/abs/2404.02418",
    "context": "Title: Auxiliary task demands mask the capabilities of smaller language models\nAbstract: arXiv:2404.02418v1 Announce Type: cross  Abstract: Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of \"task demands\" -- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying competence, combined with the model's ability to interpret and perform the task given its available resources. Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This \"demand gap\" is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpret",
    "path": "papers/24/04/2404.02418.json",
    "total_tokens": 888,
    "translated_title": "辅助任务需求掩盖了较小语言模型的能力",
    "translated_abstract": "发展心理学家们对认知能力如语言理解或心灵理论何时出现进行了争论。这些辩论常常关注\"任务需求\"的概念--执行特定评估时所伴随的辅助挑战--这些挑战可能掩盖了儿童的潜在能力。当衡量语言模型（LMs）的能力时，同样的问题也会出现：任务表现取决于模型的基本能力，结合了模型解释和执行任务的能力以及其可用资源。在这里，我们展示了对于类比推理、反思推理、单词预测和语法判断，具有更大任务需求的评估方法会比降低需求的评估得到更低的性能。这种\"需求差距\"在参数较少、训练数据较少的模型中最为显著。我们的结果表明，LM的性能不应被解释为",
    "tldr": "较小语言模型对类比推理、反思推理、单词预测和语法判断的表现受辅助任务需求的影响，评估方法的任务需求越大，性能越低，这种\"需求差距\"在参数较少、训练数据较少的模型中尤为显著",
    "en_tdlr": "Smaller language models' performance on tasks like analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments is influenced by auxiliary task demands; evaluations with higher task demands result in lower performance, with this \"demand gap\" being most notable in models with fewer parameters and less training data."
}