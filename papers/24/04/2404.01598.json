{
    "title": "Extremum-Seeking Action Selection for Accelerating Policy Optimization",
    "abstract": "arXiv:2404.01598v1 Announce Type: cross  Abstract: Reinforcement learning for control over continuous spaces typically uses high-entropy stochastic policies, such as Gaussian distributions, for local exploration and estimating policy gradient to optimize performance. Many robotic control problems deal with complex unstable dynamics, where applying actions that are off the feasible control manifolds can quickly lead to undesirable divergence. In such cases, most samples taken from the ambient action space generate low-value trajectories that hardly contribute to policy improvement, resulting in slow or failed learning. We propose to improve action selection in this model-free RL setting by introducing additional adaptive control steps based on Extremum-Seeking Control (ESC). On each action sampled from stochastic policies, we apply sinusoidal perturbations and query for estimated Q-values as the response signal. Based on ESC, we then dynamically improve the sampled actions to be closer ",
    "link": "https://arxiv.org/abs/2404.01598",
    "context": "Title: Extremum-Seeking Action Selection for Accelerating Policy Optimization\nAbstract: arXiv:2404.01598v1 Announce Type: cross  Abstract: Reinforcement learning for control over continuous spaces typically uses high-entropy stochastic policies, such as Gaussian distributions, for local exploration and estimating policy gradient to optimize performance. Many robotic control problems deal with complex unstable dynamics, where applying actions that are off the feasible control manifolds can quickly lead to undesirable divergence. In such cases, most samples taken from the ambient action space generate low-value trajectories that hardly contribute to policy improvement, resulting in slow or failed learning. We propose to improve action selection in this model-free RL setting by introducing additional adaptive control steps based on Extremum-Seeking Control (ESC). On each action sampled from stochastic policies, we apply sinusoidal perturbations and query for estimated Q-values as the response signal. Based on ESC, we then dynamically improve the sampled actions to be closer ",
    "path": "papers/24/04/2404.01598.json",
    "total_tokens": 858,
    "translated_title": "用于加速策略优化的极值寻找动作选择",
    "translated_abstract": "在连续空间上进行控制的强化学习通常使用高熵的随机策略，如高斯分布，用于局部探索和估计策略梯度以优化性能。许多机器人控制问题涉及复杂不稳定的动力学，其中施加在可行控制流形之外的动作很快会导致不良发散。在这种情况下，大多数从环境动作空间中采样的样本生成的轨迹价值较低，几乎没有贡献于策略改进，导致学习缓慢或失败。我们提出在这种无模型RL设置中通过引入基于极值寻找控制（ESC）的附加自适应控制步骤来改善动作选择。对于从随机策略采样的每个动作，我们应用正弦扰动并查询估计的Q值作为响应信号。根据ESC，我们动态改进采样的动作以使之更接近理想动作。",
    "tldr": "提出了在无模型强化学习中改进动作选择的方法，通过引入极值寻找控制（ESC）进行自适应控制步骤，以加速策略优化。",
    "en_tdlr": "Proposed a method to improve action selection in model-free reinforcement learning by introducing Extremum-Seeking Control (ESC) for adaptive control steps to accelerate policy optimization."
}