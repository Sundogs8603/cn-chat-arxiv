{
    "title": "Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy",
    "abstract": "arXiv:2404.01830v1 Announce Type: cross  Abstract: We introduce a novel doubly-robust (DR) off-policy evaluation (OPE) estimator for Markov decision processes, DRUnknown, designed for situations where both the logging policy and the value function are unknown. The proposed estimator initially estimates the logging policy and then estimates the value function model by minimizing the asymptotic variance of the estimator while considering the estimating effect of the logging policy. When the logging policy model is correctly specified, DRUnknown achieves the smallest asymptotic variance within the class containing existing OPE estimators. When the value function model is also correctly specified, DRUnknown is optimal as its asymptotic variance reaches the semiparametric lower bound. We present experimental results conducted in contextual bandits and reinforcement learning to compare the performance of DRUnknown with that of existing methods.",
    "link": "https://arxiv.org/abs/2404.01830",
    "context": "Title: Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy\nAbstract: arXiv:2404.01830v1 Announce Type: cross  Abstract: We introduce a novel doubly-robust (DR) off-policy evaluation (OPE) estimator for Markov decision processes, DRUnknown, designed for situations where both the logging policy and the value function are unknown. The proposed estimator initially estimates the logging policy and then estimates the value function model by minimizing the asymptotic variance of the estimator while considering the estimating effect of the logging policy. When the logging policy model is correctly specified, DRUnknown achieves the smallest asymptotic variance within the class containing existing OPE estimators. When the value function model is also correctly specified, DRUnknown is optimal as its asymptotic variance reaches the semiparametric lower bound. We present experimental results conducted in contextual bandits and reinforcement learning to compare the performance of DRUnknown with that of existing methods.",
    "path": "papers/24/04/2404.01830.json",
    "total_tokens": 806,
    "translated_title": "估计记录策略的双重稳健离线评估",
    "translated_abstract": "我们介绍了一种用于马尔可夫决策过程的新颖的双重稳健（DR）离线评估（OPE）估计器DRUnknown，旨在应对记录策略和价值函数均未知的情况。该估计器首先估计记录策略，然后通过最小化估计器的渐近方差来估计价值函数模型，同时考虑记录策略的估计效果。当记录策略模型正确指定时，DRUnknown在现有OPE估计器类中达到最小的渐近方差。当价值函数模型也被正确指定时，DRUnknown是最优的，因为它的渐近方差达到了半参数下界。我们在情境臂和强化学习中进行了实验结果，比较了DRUnknown与现有方法的性能。",
    "tldr": "提出了一种适用于未知记录策略和价值函数的双重稳健离线评估估计器DRUnknown，实现了最小渐近方差和半参数下界下最佳性能。",
    "en_tdlr": "Proposed a novel doubly-robust off-policy evaluation estimator DRUnknown for situations with unknown logging policy and value function, achieving optimal performance with smallest asymptotic variance and semiparametric lower bound when both models are correctly specified."
}