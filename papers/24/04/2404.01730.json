{
    "title": "Asymptotics of Language Model Alignment",
    "abstract": "arXiv:2404.01730v1 Announce Type: new  Abstract: Let $p$ denote a generative language model. Let $r$ denote a reward model that returns a scalar that captures the degree at which a draw from $p$ is preferred. The goal of language model alignment is to alter $p$ to a new distribution $\\phi$ that results in a higher expected reward while keeping $\\phi$ close to $p.$ A popular alignment method is the KL-constrained reinforcement learning (RL), which chooses a distribution $\\phi_\\Delta$ that maximizes $E_{\\phi_{\\Delta}} r(y)$ subject to a relative entropy constraint $KL(\\phi_\\Delta || p) \\leq \\Delta.$ Another simple alignment method is best-of-$N$, where $N$ samples are drawn from $p$ and one with highest reward is selected. In this paper, we offer a closed-form characterization of the optimal KL-constrained RL solution. We demonstrate that any alignment method that achieves a comparable trade-off between KL divergence and reward must approximate the optimal KL-constrained RL solution in t",
    "link": "https://arxiv.org/abs/2404.01730",
    "context": "Title: Asymptotics of Language Model Alignment\nAbstract: arXiv:2404.01730v1 Announce Type: new  Abstract: Let $p$ denote a generative language model. Let $r$ denote a reward model that returns a scalar that captures the degree at which a draw from $p$ is preferred. The goal of language model alignment is to alter $p$ to a new distribution $\\phi$ that results in a higher expected reward while keeping $\\phi$ close to $p.$ A popular alignment method is the KL-constrained reinforcement learning (RL), which chooses a distribution $\\phi_\\Delta$ that maximizes $E_{\\phi_{\\Delta}} r(y)$ subject to a relative entropy constraint $KL(\\phi_\\Delta || p) \\leq \\Delta.$ Another simple alignment method is best-of-$N$, where $N$ samples are drawn from $p$ and one with highest reward is selected. In this paper, we offer a closed-form characterization of the optimal KL-constrained RL solution. We demonstrate that any alignment method that achieves a comparable trade-off between KL divergence and reward must approximate the optimal KL-constrained RL solution in t",
    "path": "papers/24/04/2404.01730.json",
    "total_tokens": 895,
    "translated_title": "语言模型对齐的渐进研究",
    "translated_abstract": "让$p$表示一个生成式语言模型。让$r$表示一个奖励模型，返回一个标量，捕捉从$p$中抽取的内容被偏好的程度。语言模型对齐的目标是改变$p$为一个新的分布$\\phi$，使得期望奖励更高，同时保持$\\phi$接近$p$。一种流行的对齐方法是KL约束的强化学习（RL），选择一个分布$\\phi_\\Delta$，最大化$E_{\\phi_{\\Delta}} r(y)$，同时满足相对熵约束$KL(\\phi_\\Delta || p) \\leq \\Delta$。另一种简单的对齐方法是最佳-$N$，从$p$中抽取$N$个样本，并选择奖励最高的一个。在本文中，我们提供了最优KL约束的RL解的闭合形式刻画。我们证明了任何实现KL散度和奖励之间可比较的权衡的对齐方法，必须近似最优KL约束的RL解。",
    "tldr": "本文提供了对最优KL约束的强化学习解的闭合形式刻画，证明了实现KL散度和奖励之间权衡的对齐方法必须近似最优KL约束的RL解。",
    "en_tdlr": "This paper presents a closed-form characterization of the optimal KL-constrained RL solution and demonstrates that any alignment method achieving a comparable trade-off between KL divergence and reward must approximate the optimal KL-constrained RL solution."
}