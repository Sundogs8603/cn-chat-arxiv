{
    "title": "Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation",
    "abstract": "arXiv:2404.01334v1 Announce Type: new  Abstract: In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under co",
    "link": "https://arxiv.org/abs/2404.01334",
    "context": "Title: Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation\nAbstract: arXiv:2404.01334v1 Announce Type: new  Abstract: In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under co",
    "path": "papers/24/04/2404.01334.json",
    "total_tokens": 842,
    "translated_title": "使用LLMs增强NER数据集：迈向自动化和精细化标注",
    "translated_abstract": "在自然语言处理（NLP）领域，命名实体识别（NER）被认为是一项关键技术，在各种应用中被广泛应用。传统的用于为NER模型标注数据集的方法面临着高成本和数据集质量变化的挑战。本研究介绍了一种新型的混合标注方法，将人力工作与大型语言模型（LLMs）的能力相结合。这种方法不仅旨在改善手动注释中固有的噪音，如遗漏，从而提高NER模型的性能，而且还以一种具有成本效益的方式实现这一目标。此外，通过采用标签混合策略，它解决了LLM-based注释中遇到的类别不平衡问题。通过对多个数据集的分析，这种方法一直表现出比传统注释方法更优异的性能，即使在co",
    "tldr": "本研究引入了一种新颖的混合标注方法，将人力工作与大型语言模型相结合，旨在提高NER模型的性能，并以成本效益的方式实现这一目标。",
    "en_tdlr": "This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs) to enhance the performance of NER models in a cost-effective manner."
}