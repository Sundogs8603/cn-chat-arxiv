{
    "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
    "abstract": "arXiv:2404.02655v1 Announce Type: new  Abstract: Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \\textit{Uncertainty} about the question and the \\textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \\textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into",
    "link": "https://arxiv.org/abs/2404.02655",
    "context": "Title: Calibrating the Confidence of Large Language Models by Eliciting Fidelity\nAbstract: arXiv:2404.02655v1 Announce Type: new  Abstract: Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \\textit{Uncertainty} about the question and the \\textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \\textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into",
    "path": "papers/24/04/2404.02655.json",
    "total_tokens": 846,
    "translated_title": "通过诱导忠实性校准大型语言模型的置信度",
    "translated_abstract": "使用RLHF等技术优化的大型语言模型已经取得了良好的对齐，既有帮助性又无害。然而，在对齐之后，这些语言模型经常表现出过度自信，表达的置信度并不准确地与其正确率校准。在本文中，我们将语言模型的置信度分解为关于问题的\\textit{不确定性}和对语言模型生成的答案的\\textit{忠实性}。然后，我们提出了一种即插即用的方法来估计语言模型的置信度。通过在四个MCQA数据集上对6个RLHF-LMs进行实验，我们的方法表现出很好的校准性能。此外，我们提出了两个新颖的度量标准，IPR和CE，来评估模型的校准性，并对\\textit{真正校准的置信度}进行了详细讨论。我们的方法可以作为一个强有力的基线，希望这项工作能提供一些见解。",
    "tldr": "本文通过将语言模型的置信度分解为问题的不确定性和对答案的忠实性，提出了一种估计语言模型置信度的即插即用方法，经实验证明具有良好的校准性能。"
}