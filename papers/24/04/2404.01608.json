{
    "title": "FAIRM: Learning invariant representations for algorithmic fairness and domain generalization with minimax optimality",
    "abstract": "arXiv:2404.01608v1 Announce Type: cross  Abstract: Machine learning methods often assume that the test data have the same distribution as the training data. However, this assumption may not hold due to multiple levels of heterogeneity in applications, raising issues in algorithmic fairness and domain generalization. In this work, we address the problem of fair and generalizable machine learning by invariant principles. We propose a training environment-based oracle, FAIRM, which has desirable fairness and domain generalization properties under a diversity-type condition. We then provide an empirical FAIRM with finite-sample theoretical guarantees under weak distributional assumptions. We then develop efficient algorithms to realize FAIRM in linear models and demonstrate the nonasymptotic performance with minimax optimality. We evaluate our method in numerical experiments with synthetic data and MNIST data and show that it outperforms its counterparts.",
    "link": "https://arxiv.org/abs/2404.01608",
    "context": "Title: FAIRM: Learning invariant representations for algorithmic fairness and domain generalization with minimax optimality\nAbstract: arXiv:2404.01608v1 Announce Type: cross  Abstract: Machine learning methods often assume that the test data have the same distribution as the training data. However, this assumption may not hold due to multiple levels of heterogeneity in applications, raising issues in algorithmic fairness and domain generalization. In this work, we address the problem of fair and generalizable machine learning by invariant principles. We propose a training environment-based oracle, FAIRM, which has desirable fairness and domain generalization properties under a diversity-type condition. We then provide an empirical FAIRM with finite-sample theoretical guarantees under weak distributional assumptions. We then develop efficient algorithms to realize FAIRM in linear models and demonstrate the nonasymptotic performance with minimax optimality. We evaluate our method in numerical experiments with synthetic data and MNIST data and show that it outperforms its counterparts.",
    "path": "papers/24/04/2404.01608.json",
    "total_tokens": 861,
    "translated_title": "FAIRM: 学习不变表示以实现算法公平性和域泛化的极小最优性",
    "translated_abstract": "机器学习方法通常假设测试数据与训练数据具有相同的分布。然而，由于应用中存在多个层次的异质性，这一假设可能不成立，从而引发算法公平性和域泛化方面的问题。在这项工作中，我们通过不变性原则解决了公平且具有泛化能力的机器学习问题。我们提出了一个基于训练环境的oracle，FAIRM，它在多样性类型条件下具有理想的公平性和域泛化特性。然后，我们在弱分布假设下提供了一个具有有限样本理论保证的经验FAIRM。我们还开发了有效的算法来在线性模型中实现FAIRM，并展示了具有极小最优性的非渐近性能。我们在合成数据和MNIST数据的数值实验中评估了我们的方法，并展示了其优于对应方法的表现。",
    "tldr": "提出了一种通过不变性原则解决公平和泛化机器学习问题的方法，包括基于训练环境的oracle FAIRM，以及在线性模型中实现FAIRM的高效算法，在实验中表现出极小最优性。",
    "en_tdlr": "Proposed a method to address fairness and generalization issues in machine learning through invariant principles, including the training environment-based oracle FAIRM and efficient algorithms for implementing FAIRM in linear models, demonstrating nonasymptotic performance with minimax optimality in experiments."
}