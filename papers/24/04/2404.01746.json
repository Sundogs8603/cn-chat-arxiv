{
    "title": "Towards Scalable & Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation",
    "abstract": "arXiv:2404.01746v1 Announce Type: cross  Abstract: Real-world driving involves intricate interactions among vehicles navigating through dense traffic scenarios. Recent research focuses on enhancing the interaction awareness of autonomous vehicles to leverage these interactions in decision-making. These interaction-aware planners rely on neural-network-based prediction models to capture inter-vehicle interactions, aiming to integrate these predictions with traditional control techniques such as Model Predictive Control. However, this integration of deep learning-based models with traditional control paradigms often results in computationally demanding optimization problems, relying on heuristic methods. This study introduces a principled and efficient method for combining deep learning with constrained optimization, employing knowledge distillation to train smaller and more efficient networks, thereby mitigating complexity. We demonstrate that these refined networks maintain the problem",
    "link": "https://arxiv.org/abs/2404.01746",
    "context": "Title: Towards Scalable & Efficient Interaction-Aware Planning in Autonomous Vehicles using Knowledge Distillation\nAbstract: arXiv:2404.01746v1 Announce Type: cross  Abstract: Real-world driving involves intricate interactions among vehicles navigating through dense traffic scenarios. Recent research focuses on enhancing the interaction awareness of autonomous vehicles to leverage these interactions in decision-making. These interaction-aware planners rely on neural-network-based prediction models to capture inter-vehicle interactions, aiming to integrate these predictions with traditional control techniques such as Model Predictive Control. However, this integration of deep learning-based models with traditional control paradigms often results in computationally demanding optimization problems, relying on heuristic methods. This study introduces a principled and efficient method for combining deep learning with constrained optimization, employing knowledge distillation to train smaller and more efficient networks, thereby mitigating complexity. We demonstrate that these refined networks maintain the problem",
    "path": "papers/24/04/2404.01746.json",
    "total_tokens": 835,
    "translated_title": "面向自动驾驶车辆的可扩展高效交互感知规划使用知识蒸馏",
    "translated_abstract": "真实世界的驾驶涉及车辆在拥挤交通场景中相互复杂的互动。最近的研究关注于增强自动驾驶车辆的交互意识，以利用这些互动来进行决策制定。这些交互感知规划器依赖于基于神经网络的预测模型来捕获车辆间的相互作用，旨在将这些预测与传统控制技术如模型预测控制相整合。然而，深度学习模型与传统控制范式的整合往往导致计算密集型的优化问题，依赖于启发式方法。本研究引入了一种原则性和高效的方法，将深度学习与约束优化相结合，采用知识蒸馏来训练更小、更高效的网络，从而减轻复杂性。我们证明了这些优化后的网络能够维持问题的...",
    "tldr": "引入了一种结合深度学习和约束优化的高效方法，利用知识蒸馏训练更小更高效的网络，从而减轻复杂性",
    "en_tdlr": "Introduces an efficient method combining deep learning with constrained optimization, using knowledge distillation to train smaller and more efficient networks, thereby mitigating complexity."
}