{
    "title": "Weakly-supervised Audio Separation via Bi-modal Semantic Similarity",
    "abstract": "arXiv:2404.01740v1 Announce Type: cross  Abstract: Conditional sound separation in multi-source audio mixtures without having access to single source sound data during training is a long standing challenge. Existing mix-and-separate based methods suffer from significant performance drop with multi-source training mixtures due to the lack of supervision signal for single source separation cases during training. However, in the case of language-conditional audio separation, we do have access to corresponding text descriptions for each audio mixture in our training data, which can be seen as (rough) representations of the audio samples in the language modality. To this end, in this paper, we propose a generic bi-modal separation framework which can enhance the existing unsupervised frameworks to separate single-source signals in a target modality (i.e., audio) using the easily separable corresponding signals in the conditioning modality (i.e., language), without having access to single-so",
    "link": "https://arxiv.org/abs/2404.01740",
    "context": "Title: Weakly-supervised Audio Separation via Bi-modal Semantic Similarity\nAbstract: arXiv:2404.01740v1 Announce Type: cross  Abstract: Conditional sound separation in multi-source audio mixtures without having access to single source sound data during training is a long standing challenge. Existing mix-and-separate based methods suffer from significant performance drop with multi-source training mixtures due to the lack of supervision signal for single source separation cases during training. However, in the case of language-conditional audio separation, we do have access to corresponding text descriptions for each audio mixture in our training data, which can be seen as (rough) representations of the audio samples in the language modality. To this end, in this paper, we propose a generic bi-modal separation framework which can enhance the existing unsupervised frameworks to separate single-source signals in a target modality (i.e., audio) using the easily separable corresponding signals in the conditioning modality (i.e., language), without having access to single-so",
    "path": "papers/24/04/2404.01740.json",
    "total_tokens": 858,
    "translated_title": "通过双模态语义相似性进行弱监督音频分离",
    "translated_abstract": "多源音频混合物中的条件声音分离，在训练过程中没有单源音频数据的情况下是一个长期存在的挑战。现有的基于混合与分离的方法在训练过程中由于缺乏对单源分离情况的监督信号而在多源训练混合物中表现出显著的性能下降。然而，在语言条件音频分离的情况下，我们可以访问训练数据中每个音频混合物的相应文本描述，这可以被视为语言模态中音频样本的（粗略）表示。因此，在本文中，我们提出了一个通用的双模态分离框架，该框架可以增强现有的无监督框架，以使用在条件模态（即语言）中易于分离的对应信号来分离目标模态（即音频）中的单源信号，而无需访问单源信号。进行训练。",
    "tldr": "本文提出了通过双模态语义相似性进行弱监督音频分离的框架，可在没有单独源音频数据的情况下通过语言模态中的信息实现音频信号的分离。"
}