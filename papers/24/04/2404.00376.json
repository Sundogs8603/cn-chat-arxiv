{
    "title": "Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks",
    "abstract": "arXiv:2404.00376v1 Announce Type: new  Abstract: While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field. Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step reasoning capabilities required for solving complex medical problems. To address this, we introduce Meerkat-7B, a novel medical AI system with 7 billion parameters. Meerkat-7B was trained using our new synthetic dataset consisting of high-quality chain-of-thought reasoning paths sourced from 18 medical textbooks, along with diverse instruction-following datasets. Our system achieved remarkable accuracy across seven medical benchmarks, surpassing GPT-3.5 by 13.1%, as well as outperforming the previous best 7B models such as MediTron-7B and BioMistral-7B by 13.4% and 9.8%, respectively. Notab",
    "link": "https://arxiv.org/abs/2404.00376",
    "context": "Title: Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks\nAbstract: arXiv:2404.00376v1 Announce Type: new  Abstract: While recent advancements in commercial large language models (LM) have shown promising results in medical tasks, their closed-source nature poses significant privacy and security concerns, hindering their widespread use in the medical field. Despite efforts to create open-source models, their limited parameters often result in insufficient multi-step reasoning capabilities required for solving complex medical problems. To address this, we introduce Meerkat-7B, a novel medical AI system with 7 billion parameters. Meerkat-7B was trained using our new synthetic dataset consisting of high-quality chain-of-thought reasoning paths sourced from 18 medical textbooks, along with diverse instruction-following datasets. Our system achieved remarkable accuracy across seven medical benchmarks, surpassing GPT-3.5 by 13.1%, as well as outperforming the previous best 7B models such as MediTron-7B and BioMistral-7B by 13.4% and 9.8%, respectively. Notab",
    "path": "papers/24/04/2404.00376.json",
    "total_tokens": 922,
    "translated_title": "小型语言模型从医学教材中学习增强推理能力",
    "translated_abstract": "最近商用大型语言模型（LM）在医学任务中取得了有希望的成果，但其闭源性质引发了重要的隐私和安全问题，阻碍了它们在医学领域的广泛应用。针对这一问题，我们引入了Meerkat-7B，一个包含70亿参数的新型医学人工智能系统。Meerkat-7B使用我们新的合成数据集进行训练，该数据集包含从18本医学教材中获取的高质量思维链推理路径，以及多样的遵循指令数据集。我们的系统在七个医学基准测试中取得了显著的准确性，超过了GPT-3.5 13.1%，同时也优于以往最好的7B模型MediTron-7B和BioMistral-7B分别达到了13.4%和9.8%。",
    "tldr": "通过从医学教材提取的推理路径和多样化的遵循指令数据集，我们引入了具有70亿参数的Meerkat-7B医学人工智能系统，成功解决了商用大型语言模型在医学任务上隐私和推理能力不足的问题，取得了优于先前7B模型的显著成果。",
    "en_tdlr": "By extracting reasoning paths from medical textbooks and utilizing diverse instruction-following datasets, we introduce Meerkat-7B, a medical AI system with 7 billion parameters, which addresses the privacy and reasoning limitations of commercial large language models in medical tasks, achieving remarkable performance surpassing previous 7B models."
}