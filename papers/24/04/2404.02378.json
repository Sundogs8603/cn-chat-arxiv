{
    "title": "Faster Convergence of Stochastic Accelerated Gradient Descent under Interpolation",
    "abstract": "arXiv:2404.02378v1 Announce Type: cross  Abstract: We prove new convergence rates for a generalized version of stochastic Nesterov acceleration under interpolation conditions. Unlike previous analyses, our approach accelerates any stochastic gradient method which makes sufficient progress in expectation. The proof, which proceeds using the estimating sequences framework, applies to both convex and strongly convex functions and is easily specialized to accelerated SGD under the strong growth condition. In this special case, our analysis reduces the dependence on the strong growth constant from $\\rho$ to $\\sqrt{\\rho}$ as compared to prior work. This improvement is comparable to a square-root of the condition number in the worst case and address criticism that guarantees for stochastic acceleration could be worse than those for SGD.",
    "link": "https://arxiv.org/abs/2404.02378",
    "context": "Title: Faster Convergence of Stochastic Accelerated Gradient Descent under Interpolation\nAbstract: arXiv:2404.02378v1 Announce Type: cross  Abstract: We prove new convergence rates for a generalized version of stochastic Nesterov acceleration under interpolation conditions. Unlike previous analyses, our approach accelerates any stochastic gradient method which makes sufficient progress in expectation. The proof, which proceeds using the estimating sequences framework, applies to both convex and strongly convex functions and is easily specialized to accelerated SGD under the strong growth condition. In this special case, our analysis reduces the dependence on the strong growth constant from $\\rho$ to $\\sqrt{\\rho}$ as compared to prior work. This improvement is comparable to a square-root of the condition number in the worst case and address criticism that guarantees for stochastic acceleration could be worse than those for SGD.",
    "path": "papers/24/04/2404.02378.json",
    "total_tokens": 740,
    "translated_title": "针对插值条件下随机加速梯度下降的更快收敛速度",
    "translated_abstract": "我们证明了在插值条件下对随机Nesterov加速的一般化版本的新收敛速度。与先前的分析不同，我们的方法加速了任何在期望中取得足够进展的随机梯度方法。证明使用估计序列框架进行，适用于凸函数和强凸函数，并可轻松专门用于强增长条件下的加速SGD。在这种特殊情况下，与先前的工作相比，我们的分析将强增长常数的依赖性从$\\rho$减少到$\\sqrt{\\rho}$。这种改进在最坏情况下相当于条件数的平方根，并解决了关于随机加速的保证可能比SGD更差的批评。",
    "tldr": "该论文证明了在插值条件下对随机加速的一般化版本的新收敛速度，在强增长条件下的加速SGD中取得了显著改进。",
    "en_tdlr": "The paper demonstrates new convergence rates for a generalized version of stochastic Nesterov acceleration under interpolation conditions, showing significant improvement in accelerated SGD under strong growth conditions."
}