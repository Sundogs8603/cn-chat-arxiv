{
    "title": "Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models",
    "abstract": "arXiv:2404.00884v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query.",
    "link": "https://arxiv.org/abs/2404.00884",
    "context": "Title: Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models\nAbstract: arXiv:2404.00884v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query.",
    "path": "papers/24/04/2404.00884.json",
    "total_tokens": 905,
    "translated_title": "自我演示: 在大型语言模型中引出示范之外的泛化能力",
    "translated_abstract": "大型语言模型(LLMs)展示了在上下文学习(ICL)方面的良好能力，仅凭借少量示范就能迅速适应新任务。然而，当前的少样本方法严重依赖高质量、特定查询的示范，而这种示范通常缺乏。面对示范之外的查询，依赖手工制定示范或外部检索器的方法可能会失败。为了弥合有限示范和示范之外查询之间的差距，我们提出了自我演示(Self-Demos)，这是一种通过面向查询的示范生成来引出LLMs中固有的泛化能力的新型提示方法。生成的示范策略性地插值了现有示范和给定的查询，将查询从示范之外变为示范内。为了评估我们方法的有效性，我们人工构建了OOD-Toolset数据集，其中包含超过300个真实API和1000个实例，每个实例包括三个工具使用示例作为示范和一个示范之外查询。",
    "tldr": "提出了自我演示(Self-Demos)方法，在大型语言模型中通过生成查询感知的示范，引出固有的泛化能力，并构建了用于评估该方法有效性的数据集OOD-Toolset。",
    "en_tdlr": "Introduced Self-Demos method to elicit the inherent generalizability in large language models by generating query-aware demonstrations, and constructed the OOD-Toolset dataset for evaluating the effectiveness of this approach."
}