{
    "title": "Noise-Aware Training of Layout-Aware Language Models",
    "abstract": "arXiv:2404.00488v1 Announce Type: cross  Abstract: A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degr",
    "link": "https://arxiv.org/abs/2404.00488",
    "context": "Title: Noise-Aware Training of Layout-Aware Language Models\nAbstract: arXiv:2404.00488v1 Announce Type: cross  Abstract: A visually rich document (VRD) utilizes visual features along with linguistic cues to disseminate information. Training a custom extractor that identifies named entities from a document requires a large number of instances of the target document type annotated at textual and visual modalities. This is an expensive bottleneck in enterprise scenarios, where we want to train custom extractors for thousands of different document types in a scalable way. Pre-training an extractor model on unlabeled instances of the target document type, followed by a fine-tuning step on human-labeled instances does not work in these scenarios, as it surpasses the maximum allowable training time allocated for the extractor. We address this scenario by proposing a Noise-Aware Training method or NAT in this paper. Instead of acquiring expensive human-labeled documents, NAT utilizes weakly labeled documents to train an extractor in a scalable way. To avoid degr",
    "path": "papers/24/04/2404.00488.json",
    "total_tokens": 880,
    "translated_title": "噪声感知的布局感知语言模型训练",
    "translated_abstract": "一篇视觉丰富的文档（VRD）利用视觉特征和语言线索传播信息。训练一个能够从文档中识别命名实体的自定义提取器需要大量标记为文本和视觉模态的目标文档实例。在企业场景中，我们希望以可扩展的方式为成千上万种不同文档类型训练自定义提取器，这是一个昂贵的瓶颈。在未标记目标文档实例上预训练提取器模型，然后在人工标记实例上进行微调，在这些情况下是行不通的，因为它超出了为提取器分配的最大允许训练时间。本文提出了一种噪声感知训练方法（NAT）来解决这个场景。NAT利用弱标记文档以可扩展的方式训练提取器，而不是获取昂贵的人工标记文档。",
    "tldr": "本论文提出了一种噪声感知训练方法，可以以可扩展的方式使用弱标记文档训练提取器，从而避免了在企业场景中训练大量不同文档类型的自定义提取器所需的昂贵人工标记成本。",
    "en_tdlr": "This paper introduces a Noise-Aware Training method that utilizes weakly labeled documents to train extractors in a scalable way, avoiding the expensive human labeling cost required to train custom extractors for a wide range of document types in enterprise scenarios."
}