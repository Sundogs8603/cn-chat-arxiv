{
    "title": "Prompting for Numerical Sequences: A Case Study on Market Comment Generation",
    "abstract": "arXiv:2404.02466v1 Announce Type: cross  Abstract: Large language models (LLMs) have been applied to a wide range of data-to-text generation tasks, including tables, graphs, and time-series numerical data-to-text settings. While research on generating prompts for structured data such as tables and graphs is gaining momentum, in-depth investigations into prompting for time-series numerical data are lacking. Therefore, this study explores various input representations, including sequences of tokens and structured formats such as HTML, LaTeX, and Python-style codes. In our experiments, we focus on the task of Market Comment Generation, which involves taking a numerical sequence of stock prices as input and generating a corresponding market comment. Contrary to our expectations, the results show that prompts resembling programming languages yield better outcomes, whereas those similar to natural languages and longer formats, such as HTML and LaTeX, are less effective. Our findings offer in",
    "link": "https://arxiv.org/abs/2404.02466",
    "context": "Title: Prompting for Numerical Sequences: A Case Study on Market Comment Generation\nAbstract: arXiv:2404.02466v1 Announce Type: cross  Abstract: Large language models (LLMs) have been applied to a wide range of data-to-text generation tasks, including tables, graphs, and time-series numerical data-to-text settings. While research on generating prompts for structured data such as tables and graphs is gaining momentum, in-depth investigations into prompting for time-series numerical data are lacking. Therefore, this study explores various input representations, including sequences of tokens and structured formats such as HTML, LaTeX, and Python-style codes. In our experiments, we focus on the task of Market Comment Generation, which involves taking a numerical sequence of stock prices as input and generating a corresponding market comment. Contrary to our expectations, the results show that prompts resembling programming languages yield better outcomes, whereas those similar to natural languages and longer formats, such as HTML and LaTeX, are less effective. Our findings offer in",
    "path": "papers/24/04/2404.02466.json",
    "total_tokens": 807,
    "translated_title": "提示数值序列：市场评论生成案例研究",
    "translated_abstract": "大型语言模型已被应用于广泛的数据转文本生成任务，包括表格、图表和时间序列数值数据转文本设置。然而，对于生成表格和图表等结构化数据的提示的研究正在增长中，对于时间序列数值数据的提示的深入研究却不足。因此，本研究探讨了各种输入表示，包括令牌序列和结构化格式如HTML、LaTeX和Python样式代码。在我们的实验中，我们专注于“市场评论生成”任务，该任务涉及将股价数值序列作为输入，生成相应的市场评论。与我们的预期相反，结果表明类似编程语言的提示产生更好的结果，而类似自然语言和较长格式（如HTML和LaTeX）的提示效果较差。我们的发现提供了一种方法",
    "tldr": "本研究探讨了针对市场评论生成任务的不同输入表示方法，发现类似编程语言的提示效果更好，而类似自然语言和较长格式的提示效果较差。",
    "en_tdlr": "This study explores various input representations for the task of Market Comment Generation and finds that prompts resembling programming languages yield better outcomes, whereas those similar to natural languages and longer formats are less effective."
}