{
    "title": "Insights from the Use of Previously Unseen Neural Architecture Search Datasets",
    "abstract": "arXiv:2404.02189v1 Announce Type: cross  Abstract: The boundless possibility of neural networks which can be used to solve a problem -- each with different performance -- leads to a situation where a Deep Learning expert is required to identify the best neural network. This goes against the hope of removing the need for experts. Neural Architecture Search (NAS) offers a solution to this by automatically identifying the best architecture. However, to date, NAS work has focused on a small set of datasets which we argue are not representative of real-world problems. We introduce eight new datasets created for a series of NAS Challenges: AddNIST, Language, MultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These datasets and challenges are developed to direct attention to issues in NAS development and to encourage authors to consider how their models will perform on datasets unknown to them at development time. We present experimentation using standard Deep Learning met",
    "link": "https://arxiv.org/abs/2404.02189",
    "context": "Title: Insights from the Use of Previously Unseen Neural Architecture Search Datasets\nAbstract: arXiv:2404.02189v1 Announce Type: cross  Abstract: The boundless possibility of neural networks which can be used to solve a problem -- each with different performance -- leads to a situation where a Deep Learning expert is required to identify the best neural network. This goes against the hope of removing the need for experts. Neural Architecture Search (NAS) offers a solution to this by automatically identifying the best architecture. However, to date, NAS work has focused on a small set of datasets which we argue are not representative of real-world problems. We introduce eight new datasets created for a series of NAS Challenges: AddNIST, Language, MultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These datasets and challenges are developed to direct attention to issues in NAS development and to encourage authors to consider how their models will perform on datasets unknown to them at development time. We present experimentation using standard Deep Learning met",
    "path": "papers/24/04/2404.02189.json",
    "total_tokens": 806,
    "translated_title": "从之前未见的神经架构搜索数据集中获得的见解",
    "translated_abstract": "无限可能的神经网络可以用来解决问题，每个神经网络的性能不同，因此需要深度学习专家来确定最佳神经网络，这违背了消除专家需求的希望。神经架构搜索（NAS）通过自动识别最佳架构来解决这一问题。然而，迄今为止，NAS的工作集中在一小组数据集上，我们认为这些数据集并不能代表真实世界的问题。我们引入了八个新数据集，用于一系列NAS挑战：AddNIST，Language，MultNIST，CIFARTile，Gutenberg，Isabella，GeoClassing 和 Chesseract。这些数据集和挑战旨在引起NAS开发中的注意和鼓励作者考虑他们的模型在开发时未知数据集上的表现。我们展示了使用标准深度学习方法的实验。",
    "tldr": "提出了八个新的神经架构搜索数据集，旨在引起在NAS开发中的关注并鼓励作者考虑模型在开发时未知数据集上的表现。",
    "en_tdlr": "Eight new neural architecture search datasets are introduced to draw attention in NAS development and encourage authors to consider model performance on datasets unknown at development time."
}