{
    "title": "Scalable Model Editing via Customized Expert Networks",
    "abstract": "arXiv:2404.02699v1 Announce Type: new  Abstract: Addressing the issue of hallucinations and outdated knowledge in large language models is critical for their reliable application. Model Editing presents a promising avenue for mitigating these challenges in a cost-effective manner. However, existing methods often suffer from unsatisfactory generalization and unintended effects on unrelated samples. To overcome these limitations, we introduce a novel approach: Scalable Model Editing via Customized Expert Networks (SCEN), which is a two-stage continuous training paradigm. Specifically, in the first stage, we train lightweight expert networks individually for each piece of knowledge that needs to be updated. Subsequently, we train a corresponding neuron for each expert to control the activation state of that expert. Our experiments on two different sizes of open-source large language models, the Llama2 7B and 13B, achieve state-of-the-art results compared to existing mainstream Model Editi",
    "link": "https://arxiv.org/abs/2404.02699",
    "context": "Title: Scalable Model Editing via Customized Expert Networks\nAbstract: arXiv:2404.02699v1 Announce Type: new  Abstract: Addressing the issue of hallucinations and outdated knowledge in large language models is critical for their reliable application. Model Editing presents a promising avenue for mitigating these challenges in a cost-effective manner. However, existing methods often suffer from unsatisfactory generalization and unintended effects on unrelated samples. To overcome these limitations, we introduce a novel approach: Scalable Model Editing via Customized Expert Networks (SCEN), which is a two-stage continuous training paradigm. Specifically, in the first stage, we train lightweight expert networks individually for each piece of knowledge that needs to be updated. Subsequently, we train a corresponding neuron for each expert to control the activation state of that expert. Our experiments on two different sizes of open-source large language models, the Llama2 7B and 13B, achieve state-of-the-art results compared to existing mainstream Model Editi",
    "path": "papers/24/04/2404.02699.json",
    "total_tokens": 844,
    "translated_title": "通过定制化专家网络实现可扩展的模型编辑",
    "translated_abstract": "大型语言模型中存在幻觉和过时知识的问题对于其可靠应用至关重要。模型编辑是一个有前途的途径，可以以成本效益的方式减轻这些挑战。然而，现有方法经常受到不令人满意的泛化和对不相关样本的意外影响。为了克服这些限制，我们引入了一种新方法：通过定制化专家网络实现可扩展的模型编辑（SCEN），这是一个有两个阶段的连续训练范式。具体地，在第一个阶段，我们为每个需要更新的知识片段单独训练轻量级专家网络。随后，我们训练每个专家对应的神经元来控制该专家的激活状态。我们在两种不同规模的开源大型语言模型，Llama2 7B 和 13B 上的实验证实，相比现有主流的模型编辑方法取得了最先进的结果。",
    "tldr": "通过引入SCEN方法，使用定制的专家网络实现了可扩展的模型编辑，解决了大型语言模型中的幻觉和过时知识问题，并在实验证实中取得了最先进的结果。",
    "en_tdlr": "The paper presents SCEN, a novel approach utilizing customized expert networks for scalable model editing, addressing hallucinations and outdated knowledge in large language models, achieving state-of-the-art results in experiments."
}