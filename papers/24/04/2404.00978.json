{
    "title": "Prior Constraints-based Reward Model Training for Aligning Large Language Models",
    "abstract": "arXiv:2404.00978v1 Announce Type: new  Abstract: Reinforcement learning with human feedback for aligning large language models (LLMs) trains a reward model typically using ranking loss with comparison pairs.However, the training procedure suffers from an inherent problem: the uncontrolled scaling of reward scores during reinforcement learning due to the lack of constraints while training the reward model.This paper proposes a Prior Constraints-based Reward Model (namely PCRM) training method to mitigate this problem. PCRM incorporates prior constraints, specifically, length ratio and cosine similarity between outputs of each comparison pair, during reward model training to regulate optimization magnitude and control score margins. We comprehensively evaluate PCRM by examining its rank correlation with human preferences and its effectiveness in aligning LLMs via RL. Experimental results demonstrate that PCRM significantly improves alignment performance by effectively constraining reward",
    "link": "https://arxiv.org/abs/2404.00978",
    "context": "Title: Prior Constraints-based Reward Model Training for Aligning Large Language Models\nAbstract: arXiv:2404.00978v1 Announce Type: new  Abstract: Reinforcement learning with human feedback for aligning large language models (LLMs) trains a reward model typically using ranking loss with comparison pairs.However, the training procedure suffers from an inherent problem: the uncontrolled scaling of reward scores during reinforcement learning due to the lack of constraints while training the reward model.This paper proposes a Prior Constraints-based Reward Model (namely PCRM) training method to mitigate this problem. PCRM incorporates prior constraints, specifically, length ratio and cosine similarity between outputs of each comparison pair, during reward model training to regulate optimization magnitude and control score margins. We comprehensively evaluate PCRM by examining its rank correlation with human preferences and its effectiveness in aligning LLMs via RL. Experimental results demonstrate that PCRM significantly improves alignment performance by effectively constraining reward",
    "path": "papers/24/04/2404.00978.json",
    "total_tokens": 812,
    "translated_title": "基于先验约束的奖励模型训练以对齐大尺寸语言模型",
    "translated_abstract": "使用人类反馈的强化学习方法来对齐大型语言模型（LLMs）通常训练一个奖励模型，该模型使用比较对来计算排名损失。然而，训练过程存在一个固有问题：由于缺乏约束，奖励分数在强化学习过程中呈现不受控制的扩展。本文提出了一种基于先验约束的奖励模型（PCRM）训练方法来缓解这一问题。PCRM在奖励模型训练中融合了先验约束，具体来说是每个比较对输出之间的长度比和余弦相似性，以调节优化幅度并控制得分差距。我们通过检查PCRM与人类偏好的排名相关性以及通过RL对LLMs对齐的有效性来全面评估PCRM。实验结果表明，PCRM通过有效地约束奖励显著提升了对齐性能。",
    "tldr": "本文提出了基于先验约束的奖励模型训练方法，有效改善了对齐大型语言模型的性能。"
}