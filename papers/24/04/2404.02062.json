{
    "title": "Digital Forgetting in Large Language Models: A Survey of Unlearning Methods",
    "abstract": "arXiv:2404.02062v1 Announce Type: cross  Abstract: The objective of digital forgetting is, given a model with undesirable knowledge or behavior, obtain a new model where the detected issues are no longer present. The motivations for forgetting include privacy protection, copyright protection, elimination of biases and discrimination, and prevention of harmful content generation. Effective digital forgetting has to be effective (meaning how well the new model has forgotten the undesired knowledge/behavior), retain the performance of the original model on the desirable tasks, and be scalable (in particular forgetting has to be more efficient than retraining from scratch on just the tasks/data to be retained). This survey focuses on forgetting in large language models (LLMs). We first provide background on LLMs, including their components, the types of LLMs, and their usual training pipeline. Second, we describe the motivations, types, and desired properties of digital forgetting. Third, ",
    "link": "https://arxiv.org/abs/2404.02062",
    "context": "Title: Digital Forgetting in Large Language Models: A Survey of Unlearning Methods\nAbstract: arXiv:2404.02062v1 Announce Type: cross  Abstract: The objective of digital forgetting is, given a model with undesirable knowledge or behavior, obtain a new model where the detected issues are no longer present. The motivations for forgetting include privacy protection, copyright protection, elimination of biases and discrimination, and prevention of harmful content generation. Effective digital forgetting has to be effective (meaning how well the new model has forgotten the undesired knowledge/behavior), retain the performance of the original model on the desirable tasks, and be scalable (in particular forgetting has to be more efficient than retraining from scratch on just the tasks/data to be retained). This survey focuses on forgetting in large language models (LLMs). We first provide background on LLMs, including their components, the types of LLMs, and their usual training pipeline. Second, we describe the motivations, types, and desired properties of digital forgetting. Third, ",
    "path": "papers/24/04/2404.02062.json",
    "total_tokens": 879,
    "translated_title": "大规模语言模型中的数字遗忘：遗忘方法综述",
    "translated_abstract": "数字遗忘的目标是，在给定一个存在不良知识或行为的模型的情况下，得到一个新模型，其中不再存在检测到的问题。遗忘的动机包括隐私保护、版权保护、消除偏见和歧视以及预防有害内容生成。有效的数字遗忘必须是有效的（即新模型多么好地遗忘了不良知识/行为），保留原始模型在理想任务上的性能，并且具有可伸缩性（特别是遗忘必须比仅重新训练要有效，仅重新训练需要保留的任务/数据）。本综述聚焦于大型语言模型（LLMs）中的遗忘。首先，我们介绍LLMs的背景，包括它们的组成部分、LLMs的类型以及它们通常的训练流程。其次，我们描述数字遗忘的动机、类型和期望的属性。",
    "tldr": "本综述聚焦于大型语言模型中的数字遗忘，旨在获得一种遗忘方法，能有效地消除模型中的不良知识或行为，同时保留原始模型在理想任务上的性能，并具有可伸缩性。",
    "en_tdlr": "This survey focuses on digital forgetting in large language models, aiming to obtain a forgetting method that can effectively eliminate undesirable knowledge or behavior in the model, while retaining the performance of the original model on desirable tasks and being scalable."
}