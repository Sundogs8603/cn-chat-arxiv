{
    "title": "DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations",
    "abstract": "arXiv:2404.00557v1 Announce Type: new  Abstract: Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context. In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.",
    "link": "https://arxiv.org/abs/2404.00557",
    "context": "Title: DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented Dialogue Representations\nAbstract: arXiv:2404.00557v1 Announce Type: new  Abstract: Language models pre-trained on general text have achieved impressive results in diverse fields. Yet, the distinct linguistic characteristics of task-oriented dialogues (TOD) compared to general text limit the practical utility of existing language models. Current task-oriented dialogue pre-training methods overlook the one-to-many property of conversations, where multiple responses can be appropriate given the same conversation context. In this paper, we propose a novel dialogue pre-training model called DivTOD, which collaborates with LLMs to learn diverse task-oriented dialogue representations. DivTOD guides LLMs in transferring diverse knowledge to smaller models while removing domain knowledge that contradicts task-oriented dialogues. Experiments show that our model outperforms strong TOD baselines on various downstream dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.",
    "path": "papers/24/04/2404.00557.json",
    "total_tokens": 883,
    "translated_title": "DivTOD: 发挥LLM的力量，为任务导向对话表示多样化释放潜力",
    "translated_abstract": "在各个领域普遍采用的在通用文本上预先训练的语言模型取得了令人印象深刻的成果。然而，与通用文本相比，任务导向对话（TOD）具有明显的语言特征，这限制了现有语言模型的实际效用。当前的任务导向对话预训练方法忽视了对话的一对多属性，即在相同对话上下文下，可以有多个适当的回复。在本文中，我们提出了一种名为DivTOD的新型对话预训练模型，它与LLM共同学习多样化的任务导向对话表示。DivTOD指导LLM在向较小模型传递多样化知识的同时，消除与任务导向对话相矛盾的领域知识。实验表明，我们的模型在各种下游对话任务上胜过强大的TOD基线，并学习了任务导向对话的内在多样性。",
    "tldr": "DivTOD 提出了一种新颖的对话预训练模型，与LLMs合作学习多样的任务导向对话表示，实验表明该模型在各种下游对话任务上优于强基线，并学习了任务导向对话的内在多样性。",
    "en_tdlr": "DivTOD proposes a novel dialogue pre-training model that collaborates with LLMs to learn diverse task-oriented dialogue representations, showing superior performance over strong baselines on various downstream dialogue tasks and capturing the intrinsic diversity of task-oriented dialogues."
}