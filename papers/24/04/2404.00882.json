{
    "title": "Metric Learning to Accelerate Convergence of Operator Splitting Methods for Differentiable Parametric Programming",
    "abstract": "arXiv:2404.00882v1 Announce Type: new  Abstract: Recent work has shown a variety of ways in which machine learning can be used to accelerate the solution of constrained optimization problems. Increasing demand for real-time decision-making capabilities in applications such as artificial intelligence and optimal control has led to a variety of approaches, based on distinct strategies. This work proposes a novel approach to learning optimization, in which the underlying metric space of a proximal operator splitting algorithm is learned so as to maximize its convergence rate. While prior works in optimization theory have derived optimal metrics for limited classes of problems, the results do not extend to many practical problem forms including general Quadratic Programming (QP). This paper shows how differentiable optimization can enable the end-to-end learning of proximal metrics, enhancing the convergence of proximal algorithms for QP problems beyond what is possible based on known theo",
    "link": "https://arxiv.org/abs/2404.00882",
    "context": "Title: Metric Learning to Accelerate Convergence of Operator Splitting Methods for Differentiable Parametric Programming\nAbstract: arXiv:2404.00882v1 Announce Type: new  Abstract: Recent work has shown a variety of ways in which machine learning can be used to accelerate the solution of constrained optimization problems. Increasing demand for real-time decision-making capabilities in applications such as artificial intelligence and optimal control has led to a variety of approaches, based on distinct strategies. This work proposes a novel approach to learning optimization, in which the underlying metric space of a proximal operator splitting algorithm is learned so as to maximize its convergence rate. While prior works in optimization theory have derived optimal metrics for limited classes of problems, the results do not extend to many practical problem forms including general Quadratic Programming (QP). This paper shows how differentiable optimization can enable the end-to-end learning of proximal metrics, enhancing the convergence of proximal algorithms for QP problems beyond what is possible based on known theo",
    "path": "papers/24/04/2404.00882.json",
    "total_tokens": 810,
    "translated_title": "度量学习以加速可微参数规划的算子分裂方法收敛",
    "translated_abstract": "最近的研究表明机器学习可以用于加速解决约束优化问题的多种方法。在诸如人工智能和最优控制等应用中对实时决策能力的不断增强，已导致基于不同策略的各种方法的出现。本研究提出了一种学习优化的新方法，其中学习了一个基于近端算子分裂算法的度量空间，以最大化其收敛速度。虽然之前的优化理论研究得出了一些特定问题类的最优度量，但这些结果并不能推广到许多实际问题形式，包括一般的二次规划（QP）问题。本文展示了可微优化如何实现端到端学习近端度量，提升近端算法在QP问题上的收敛性，超越了基于已知理论的可能性。",
    "tldr": "该研究提出了一种新方法，通过学习优化算子分裂算法的度量空间，从而最大化其收敛速度，特别适用于二次规划问题。"
}