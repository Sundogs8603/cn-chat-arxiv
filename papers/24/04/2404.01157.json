{
    "title": "Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training",
    "abstract": "arXiv:2404.01157v1 Announce Type: new  Abstract: Prominent works in the field of Natural Language Processing have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance. However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training LLMs. Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models. Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, in this paper, we evaluate the CO2 emissions of well-known large language models, which have an especially high carbon footprint due to their significant amount of model parameters. We arg",
    "link": "https://arxiv.org/abs/2404.01157",
    "context": "Title: Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade Offs in Large Language Model Training\nAbstract: arXiv:2404.01157v1 Announce Type: new  Abstract: Prominent works in the field of Natural Language Processing have long attempted to create new innovative models by improving upon previous model training approaches, altering model architecture, and developing more in-depth datasets to better their performance. However, with the quickly advancing field of NLP comes increased greenhouse gas emissions, posing concerns over the environmental damage caused by training LLMs. Gaining a comprehensive understanding of the various costs, particularly those pertaining to environmental aspects, that are associated with artificial intelligence serves as the foundational basis for ensuring safe AI models. Currently, investigations into the CO2 emissions of AI models remain an emerging area of research, and as such, in this paper, we evaluate the CO2 emissions of well-known large language models, which have an especially high carbon footprint due to their significant amount of model parameters. We arg",
    "path": "papers/24/04/2404.01157.json",
    "total_tokens": 819,
    "translated_title": "绿色AI：探讨大型语言模型训练中的碳足迹、缓解策略和权衡",
    "translated_abstract": "自然语言处理领域的重要研究长期以来一直不断尝试通过改进先前的模型训练方法、改变模型架构和开发更深入的数据集来创造新的创新模型。然而，随着NLP领域的快速发展，温室气体排放量不断增加，引发了人们对LLMs训练造成的环境损害的担忧。对人工智能相关成本的全面了解，尤其是与环境相关的成本，是确保安全AI模型的基础。目前，对AI模型的CO2排放的调查仍然是一个新兴的研究领域，因此，在本文中，我们评估了知名大型语言模型的CO2排放量，这些模型由于其大量的模型参数而导致其碳足迹特别高。",
    "tldr": "该研究评估了大型语言模型的CO2排放量，强调了这些模型由于其大量的模型参数导致其碳足迹特别高。",
    "en_tdlr": "The study evaluates the CO2 emissions of large language models, emphasizing their high carbon footprint due to a significant amount of model parameters."
}