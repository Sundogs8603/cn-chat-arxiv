{
    "title": "ParaICL: Towards Robust Parallel In-Context Learning",
    "abstract": "arXiv:2404.00570v1 Announce Type: new  Abstract: Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstr",
    "link": "https://arxiv.org/abs/2404.00570",
    "context": "Title: ParaICL: Towards Robust Parallel In-Context Learning\nAbstract: arXiv:2404.00570v1 Announce Type: new  Abstract: Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstr",
    "path": "papers/24/04/2404.00570.json",
    "total_tokens": 800,
    "translated_title": "ParaICL：面向鲁棒性的并行上下文学习",
    "translated_abstract": "大型语言模型(LLMs)已经成为自然语言处理(NLP)领域的常态，在少样本上下文学习(ICL)方面表现出色。然而，ICL的成功很大程度上取决于少样本演示示例的选择，使得选择过程变得愈发关键。现有方法已经开始优化这些示例的数量和语义相似性以提高ICL性能。然而，我们的初步实验表明，ICL的有效性受到输入上下文长度的限制。此外，不同的少样本演示示例组合可以显著提升对不同测试样本的准确性。为了解决这个问题，我们提出了一种名为parallel in-context learning (ParaICL)的新方法，可以有效利用所有示例而不会超出可管理的输入上下文长度。ParaICL采用并行批处理来分发演示",
    "tldr": "提出了一种名为ParaICL的新方法，通过并行批处理来有效利用所有示例，在不超过可管理的输入上下文长度的情况下显著提升了不同测试样本的准确性。",
    "en_tdlr": "Proposed a novel method named ParaICL, which significantly boosts accuracy across different test samples by effectively utilizing all examples through parallel batching without exceeding the manageable input context length."
}