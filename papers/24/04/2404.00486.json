{
    "title": "Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs",
    "abstract": "arXiv:2404.00486v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts w",
    "link": "https://arxiv.org/abs/2404.00486",
    "context": "Title: Dialectical Alignment: Resolving the Tension of 3H and Security Threats of LLMs\nAbstract: arXiv:2404.00486v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), ensuring they embody the principles of being helpful, honest, and harmless (3H), known as Human Alignment, becomes crucial. While existing alignment methods like RLHF, DPO, etc., effectively fine-tune LLMs to match preferences in the preference dataset, they often lead LLMs to highly receptive human input and external evidence, even when this information is poisoned. This leads to a tendency for LLMs to be Adaptive Chameleons when external evidence conflicts with their parametric memory. This exacerbates the risk of LLM being attacked by external poisoned data, which poses a significant security risk to LLM system applications such as Retrieval-augmented generation (RAG). To address the challenge, we propose a novel framework: Dialectical Alignment (DA), which (1) utilizes AI feedback to identify optimal strategies for LLMs to navigate inter-context conflicts and context-memory conflicts w",
    "path": "papers/24/04/2404.00486.json",
    "total_tokens": 899,
    "translated_title": "辩证对齐：解决3H紧张与LLM安全威胁",
    "translated_abstract": "随着大型语言模型（LLM）的兴起，确保它们体现有益、诚实和无害（3H）原则，即人类对齐，变得至关重要。现有的对齐方法如RLHF、DPO等有效地微调LLM以匹配偏好数据集中的偏好，但往往会使LLM对高度接受人类输入和外部证据，即使这些信息是有毒的。这导致LLM倾向于成为适应变色龙，当外部证据与其参数性记忆冲突时。这加剧了LLM遭受外部有毒数据攻击的风险，对LLM系统应用（如检索增强生成）构成了重大安全风险。为了解决这一挑战，我们提出了一个新颖的框架：辩证对齐（DA），它（1）利用人工智能反馈来确定LLM导航相互文本冲突和上下文记忆冲突的最佳策略",
    "tldr": "提出了辩证对齐（DA）框架来解决LLM在面临外部有毒数据时的适应性变色龙问题，增强了其对抗外部攻击的安全性",
    "en_tdlr": "Proposed the Dialectical Alignment (DA) framework to address the adaptability chameleon issue of LLM when facing external poisoned data, enhancing its security against external attacks."
}