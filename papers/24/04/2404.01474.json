{
    "title": "Finding Replicable Human Evaluations via Stable Ranking Probability",
    "abstract": "arXiv:2404.01474v1 Announce Type: new  Abstract: Reliable human evaluation is critical to the development of successful natural language generation models, but achieving it is notoriously difficult. Stability is a crucial requirement when ranking systems by quality: consistent ranking of systems across repeated evaluations is not just desirable, but essential. Without it, there is no reliable foundation for hill-climbing or product launch decisions. In this paper, we use machine translation and its state-of-the-art human evaluation framework, MQM, as a case study to understand how to set up reliable human evaluations that yield stable conclusions. We investigate the optimal configurations for item allocation to raters, number of ratings per item, and score normalization. Our study on two language pairs provides concrete recommendations for designing replicable human evaluation studies. We also collect and release the largest publicly available dataset of multi-segment translations rate",
    "link": "https://arxiv.org/abs/2404.01474",
    "context": "Title: Finding Replicable Human Evaluations via Stable Ranking Probability\nAbstract: arXiv:2404.01474v1 Announce Type: new  Abstract: Reliable human evaluation is critical to the development of successful natural language generation models, but achieving it is notoriously difficult. Stability is a crucial requirement when ranking systems by quality: consistent ranking of systems across repeated evaluations is not just desirable, but essential. Without it, there is no reliable foundation for hill-climbing or product launch decisions. In this paper, we use machine translation and its state-of-the-art human evaluation framework, MQM, as a case study to understand how to set up reliable human evaluations that yield stable conclusions. We investigate the optimal configurations for item allocation to raters, number of ratings per item, and score normalization. Our study on two language pairs provides concrete recommendations for designing replicable human evaluation studies. We also collect and release the largest publicly available dataset of multi-segment translations rate",
    "path": "papers/24/04/2404.01474.json",
    "total_tokens": 881,
    "translated_title": "通过稳定排名概率寻找可复制的人类评估",
    "translated_abstract": "可靠的人类评估对于成功开发自然语言生成模型至关重要，但实现可靠人类评估却极为困难。稳定性对于通过质量对系统进行排名至关重要：在重复评估中系统的一致排名不仅仅是可取的，而且是必不可少的。缺乏这一点，便无法为爬坡或产品推出决策提供可靠基础。本文以机器翻译及其最先进的人类评估框架MQM作为案例研究，以了解如何设立可靠的人类评估以得出稳定的结论。我们研究了分配给评估者的项目配置、每个项目的评分次数以及得分归一化的最佳配置。我们在两种语言对上进行的研究提供了针对设计可复制的人类评估研究的具体建议。我们还收集并发布了最大的公开可用的多段翻译评分数据集。",
    "tldr": "本研究通过机器翻译和其最先进的人类评估框架MQM作为案例研究，提出了关于设置可靠人类评估以得出稳定结论的方法，以及针对设计可复制人类评估研究的具体建议。",
    "en_tdlr": "This study investigates reliable human evaluation setups using machine translation and the state-of-the-art human evaluation framework MQM, providing methods for stable conclusions and concrete recommendations for replicable human evaluation studies."
}