{
    "title": "TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression",
    "abstract": "arXiv:2404.01153v1 Announce Type: cross  Abstract: The main challenge that sets transfer learning apart from traditional supervised learning is the distribution shift, reflected as the shift between the source and target models and that between the marginal covariate distributions. In this work, we tackle model shifts in the presence of covariate shifts in the high-dimensional regression setting. Specifically, we propose a two-step method with a novel fused-regularizer that effectively leverages samples from source tasks to improve the learning performance on a target task with limited samples. Nonasymptotic bound is provided for the estimation error of the target model, showing the robustness of the proposed method to covariate shifts. We further establish conditions under which the estimator is minimax-optimal. Additionally, we extend the method to a distributed setting, allowing for a pretraining-finetuning strategy, requiring just one round of communication while retaining the esti",
    "link": "https://arxiv.org/abs/2404.01153",
    "context": "Title: TransFusion: Covariate-Shift Robust Transfer Learning for High-Dimensional Regression\nAbstract: arXiv:2404.01153v1 Announce Type: cross  Abstract: The main challenge that sets transfer learning apart from traditional supervised learning is the distribution shift, reflected as the shift between the source and target models and that between the marginal covariate distributions. In this work, we tackle model shifts in the presence of covariate shifts in the high-dimensional regression setting. Specifically, we propose a two-step method with a novel fused-regularizer that effectively leverages samples from source tasks to improve the learning performance on a target task with limited samples. Nonasymptotic bound is provided for the estimation error of the target model, showing the robustness of the proposed method to covariate shifts. We further establish conditions under which the estimator is minimax-optimal. Additionally, we extend the method to a distributed setting, allowing for a pretraining-finetuning strategy, requiring just one round of communication while retaining the esti",
    "path": "papers/24/04/2404.01153.json",
    "total_tokens": 858,
    "translated_title": "TransFusion：用于高维回归的抗协变量转移学习",
    "translated_abstract": "传统监督学习与转移学习的主要挑战在于分布偏移，体现为源模型和目标模型之间的偏移以及边际协变量分布之间的偏移。在这项工作中，我们在高维回归设置中处理存在协变量变化的模型变化。具体来说，我们提出了一个两步法方法，使用一种新颖的融合正则化器，有效利用来自源任务的样本来提高在具有有限样本的目标任务上的学习性能。提供了目标模型估计误差的非渐近界限，显示了所提方法对协变量转移的稳健性。我们进一步确定了估计器是最小-最大最优的条件。此外，我们将该方法扩展到分布式设置，允许进行预训练和微调策略，仅需一轮通信即可保留估计",
    "tldr": "提出了一种新型融合正则化器的两步法方法，有效处理高维回归中的模型偏移和协变量转移，提高了目标任务的学习性能，具有稳健性并满足最小-最大最优条件。",
    "en_tdlr": "Proposed a two-step method with a novel fused-regularizer to effectively address model shifts and covariate shifts in high-dimensional regression, improving learning performance on the target task, exhibiting robustness and satisfying minimax optimality conditions."
}