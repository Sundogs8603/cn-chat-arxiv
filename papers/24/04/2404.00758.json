{
    "title": "From Robustness to Improved Generalization and Calibration in Pre-trained Language Models",
    "abstract": "arXiv:2404.00758v1 Announce Type: new  Abstract: Enhancing generalization and uncertainty quantification in pre-trained language models (PLMs) is crucial for their effectiveness and reliability. Building on machine learning research that established the importance of robustness for improving generalization, we investigate the role of representation smoothness, achieved via Jacobian and Hessian regularization, in enhancing PLM performance. Although such regularization methods have proven effective in computer vision, their application in natural language processing (NLP), where PLM inputs are derived from a discrete domain, poses unique challenges. We introduce a novel two-phase regularization approach, JacHess, which minimizes the norms of the Jacobian and Hessian matrices within PLM intermediate representations relative to their inputs. Our evaluation using the GLUE benchmark demonstrates that JacHess significantly improves in-domain generalization and calibration in PLMs, outperformi",
    "link": "https://arxiv.org/abs/2404.00758",
    "context": "Title: From Robustness to Improved Generalization and Calibration in Pre-trained Language Models\nAbstract: arXiv:2404.00758v1 Announce Type: new  Abstract: Enhancing generalization and uncertainty quantification in pre-trained language models (PLMs) is crucial for their effectiveness and reliability. Building on machine learning research that established the importance of robustness for improving generalization, we investigate the role of representation smoothness, achieved via Jacobian and Hessian regularization, in enhancing PLM performance. Although such regularization methods have proven effective in computer vision, their application in natural language processing (NLP), where PLM inputs are derived from a discrete domain, poses unique challenges. We introduce a novel two-phase regularization approach, JacHess, which minimizes the norms of the Jacobian and Hessian matrices within PLM intermediate representations relative to their inputs. Our evaluation using the GLUE benchmark demonstrates that JacHess significantly improves in-domain generalization and calibration in PLMs, outperformi",
    "path": "papers/24/04/2404.00758.json",
    "total_tokens": 826,
    "translated_title": "从鲁棒性到改进的泛化和校准：预训练语言模型中的研究",
    "translated_abstract": "增强预训练语言模型（PLMs）中的泛化能力和不确定性量化对于其有效性和可靠性至关重要。在机器学习研究奠定了鲁棒性对于改善泛化的重要性基础上，我们调查了通过雅可比和黑塞正则化实现的表征平滑度在提高PLM性能方面的作用。尽管此类正则化方法在计算机视觉中证明有效，但在自然语言处理（NLP）中的应用，其中PLM输入源自离散域，提出了独特挑战。我们引入了一种新颖的两阶段正则化方法JacHess，该方法相对于PLM中间表示的输入最小化雅可比和黑塞矩阵的范数。我们的评估使用GLUE基准表明，JacHess显著改善了PLMs的领域内泛化和校准，表现优越。",
    "tldr": "通过引入JacHess方法，在预训练语言模型中实现了在领域内泛化和校准方面的显著改进",
    "en_tdlr": "Significant improvement in in-domain generalization and calibration in pre-trained language models achieved by introducing the JacHess method."
}