{
    "title": "Masked Completion via Structured Diffusion with White-Box Transformers",
    "abstract": "arXiv:2404.02446v1 Announce Type: new  Abstract: Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn representations by solving simple pretext tasks, then use the representations as foundations for downstream tasks. These networks are empirically designed; as such, they are usually not interpretable, their representations are not structured, and their designs are potentially redundant. White-box deep networks, in which each layer explicitly identifies and transforms structures in the data, present a promising alternative. However, existing white-box architectures have only been shown to work at scale in supervised settings with labeled data, such as classification. In this work, we provide the first instantiation of the white-box design paradigm that can be applied to large-scale unsupervised representation learning. We do this by exploiting a fundamental connection between diffusion, compression, and (masked) completion, deriving",
    "link": "https://arxiv.org/abs/2404.02446",
    "context": "Title: Masked Completion via Structured Diffusion with White-Box Transformers\nAbstract: arXiv:2404.02446v1 Announce Type: new  Abstract: Modern learning frameworks often train deep neural networks with massive amounts of unlabeled data to learn representations by solving simple pretext tasks, then use the representations as foundations for downstream tasks. These networks are empirically designed; as such, they are usually not interpretable, their representations are not structured, and their designs are potentially redundant. White-box deep networks, in which each layer explicitly identifies and transforms structures in the data, present a promising alternative. However, existing white-box architectures have only been shown to work at scale in supervised settings with labeled data, such as classification. In this work, we provide the first instantiation of the white-box design paradigm that can be applied to large-scale unsupervised representation learning. We do this by exploiting a fundamental connection between diffusion, compression, and (masked) completion, deriving",
    "path": "papers/24/04/2404.02446.json",
    "total_tokens": 605,
    "translated_title": "通过带有白盒变换器的结构扩散进行蒙面补全",
    "translated_abstract": "现代学习框架通常使用大量无标签数据训练深度神经网络，通过解决简单的前置任务学习表示，然后将这些表示用作下游任务的基础。本文提供了第一个白盒设计范例的实例，可以应用于大规模无监督表示学习。",
    "tldr": "该论文提出了一种可以应用于大规模无监督表示学习的白盒设计范例。",
    "en_tdlr": "This paper introduces a white-box design paradigm that can be applied to large-scale unsupervised representation learning."
}