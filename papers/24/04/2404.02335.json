{
    "title": "Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation",
    "abstract": "arXiv:2404.02335v1 Announce Type: cross  Abstract: The rapid expansion of texts' volume and diversity presents formidable challenges in multi-domain settings. These challenges are also visible in the Persian name entity recognition (NER) settings. Traditional approaches, either employing a unified model for multiple domains or individual models for each domain, frequently pose significant limitations. Single models often struggle to capture the nuances of diverse domains, while utilizing multiple large models can lead to resource constraints, rendering the training of a model for each domain virtually impractical. Therefore, this paper introduces a novel approach composed of one core model with multiple sets of domain-specific parameters. We utilize techniques such as prompt tuning and adapters, combined with the incorporation of additional layers, to add parameters that we can train for the specific domains. This enables the model to perform comparably to individual models for each do",
    "link": "https://arxiv.org/abs/2404.02335",
    "context": "Title: Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource Multi-Domain Adaptation\nAbstract: arXiv:2404.02335v1 Announce Type: cross  Abstract: The rapid expansion of texts' volume and diversity presents formidable challenges in multi-domain settings. These challenges are also visible in the Persian name entity recognition (NER) settings. Traditional approaches, either employing a unified model for multiple domains or individual models for each domain, frequently pose significant limitations. Single models often struggle to capture the nuances of diverse domains, while utilizing multiple large models can lead to resource constraints, rendering the training of a model for each domain virtually impractical. Therefore, this paper introduces a novel approach composed of one core model with multiple sets of domain-specific parameters. We utilize techniques such as prompt tuning and adapters, combined with the incorporation of additional layers, to add parameters that we can train for the specific domains. This enables the model to perform comparably to individual models for each do",
    "path": "papers/24/04/2404.02335.json",
    "total_tokens": 911,
    "translated_title": "Multi-BERT：利用适配器和提示调整进行低资源多领域适应",
    "translated_abstract": "文本量和多样性的急剧扩展提出了多领域环境中的巨大挑战。这些挑战在波斯语命名实体识别（NER）设置中也很明显。传统方法，无论是使用一个统一模型来处理多个领域，还是为每个领域使用单独的模型，经常会出现显著的限制。单一模型通常难以捕捉各种领域的细微差别，而使用多个大型模型可能会导致资源限制，使为每个领域训练模型几乎不切实际。因此，本文介绍了一种由一个核心模型和多套领域特定参数组成的新颖方法。我们利用提示调整和适配器等技术，结合引入额外层，添加我们可以为特定领域训练的参数。这使得模型能够在性能上与为每个领域训练的单独模型相媲美。",
    "tldr": "提出了一种新颖的方法，使用一个核心模型和多套领域特定参数，结合提示调整和适配器技术，以及额外层次来实现低资源多领域适应，使得模型能够完成与每个领域单独训练模型相媲美的任务。",
    "en_tdlr": "Introduced a novel approach using one core model with multiple sets of domain-specific parameters, incorporating prompt tuning and adapters techniques, as well as additional layers to achieve low-resource multi-domain adaptation, allowing the model to perform comparably to individually trained models for each domain."
}