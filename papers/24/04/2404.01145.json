{
    "title": "Sequential-in-time training of nonlinear parametrizations for solving time-dependent partial differential equations",
    "abstract": "arXiv:2404.01145v1 Announce Type: cross  Abstract: Sequential-in-time methods solve a sequence of training problems to fit nonlinear parametrizations such as neural networks to approximate solution trajectories of partial differential equations over time. This work shows that sequential-in-time training methods can be understood broadly as either optimize-then-discretize (OtD) or discretize-then-optimize (DtO) schemes, which are well known concepts in numerical analysis. The unifying perspective leads to novel stability and a posteriori error analysis results that provide insights into theoretical and numerical aspects that are inherent to either OtD or DtO schemes such as the tangent space collapse phenomenon, which is a form of over-fitting. Additionally, the unified perspective facilitates establishing connections between variants of sequential-in-time training methods, which is demonstrated by identifying natural gradient descent methods on energy functionals as OtD schemes applied",
    "link": "https://arxiv.org/abs/2404.01145",
    "context": "Title: Sequential-in-time training of nonlinear parametrizations for solving time-dependent partial differential equations\nAbstract: arXiv:2404.01145v1 Announce Type: cross  Abstract: Sequential-in-time methods solve a sequence of training problems to fit nonlinear parametrizations such as neural networks to approximate solution trajectories of partial differential equations over time. This work shows that sequential-in-time training methods can be understood broadly as either optimize-then-discretize (OtD) or discretize-then-optimize (DtO) schemes, which are well known concepts in numerical analysis. The unifying perspective leads to novel stability and a posteriori error analysis results that provide insights into theoretical and numerical aspects that are inherent to either OtD or DtO schemes such as the tangent space collapse phenomenon, which is a form of over-fitting. Additionally, the unified perspective facilitates establishing connections between variants of sequential-in-time training methods, which is demonstrated by identifying natural gradient descent methods on energy functionals as OtD schemes applied",
    "path": "papers/24/04/2404.01145.json",
    "total_tokens": 848,
    "translated_title": "在解决时变偏微分方程时进行非线性参数化的逐时间训练",
    "translated_abstract": "逐时间顺序方法解决一系列训练问题，以拟合非线性参数化（如神经网络）以近似解的轨迹随时间变化的偏微分方程。这项工作表明，逐时间训练方法可以广泛理解为优化-离散化（OtD）或离散化-优化（DtO）方案，这在数值分析中是众所周知的概念。统一的观点导致了提出新的稳定性和后验误差分析结果，提供了对OtD或DtO方案本质的理论和数值方面洞察力，例如正切空间坍缩现象，这是一种过拟合形式。此外，统一的观点有助于建立逐时间训练方法的变体之间的连接，通过确定将自然梯度下降方法应用为OtD方案的能量泛函。",
    "tldr": "该研究显示逐时间训练方法可以被广泛理解为优化-离散化或离散化-优化方案，在稳定性和误差分析方面取得新颖结果，同时有助于建立方法之间的连接。",
    "en_tdlr": "The research demonstrates that sequential-in-time training methods can be broadly understood as optimize-then-discretize or discretize-then-optimize schemes, yielding novel results in stability and error analysis, while facilitating connections between these methods."
}