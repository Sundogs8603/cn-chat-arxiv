{
    "title": "Large-Scale Non-convex Stochastic Constrained Distributionally Robust Optimization",
    "abstract": "arXiv:2404.01200v1 Announce Type: cross  Abstract: Distributionally robust optimization (DRO) is a powerful framework for training robust models against data distribution shifts. This paper focuses on constrained DRO, which has an explicit characterization of the robustness level. Existing studies on constrained DRO mostly focus on convex loss function, and exclude the practical and challenging case with non-convex loss function, e.g., neural network. This paper develops a stochastic algorithm and its performance analysis for non-convex constrained DRO. The computational complexity of our stochastic algorithm at each iteration is independent of the overall dataset size, and thus is suitable for large-scale applications. We focus on the general Cressie-Read family divergence defined uncertainty set which includes $\\chi^2$-divergences as a special case. We prove that our algorithm finds an $\\epsilon$-stationary point with a computational complexity of $\\mathcal O(\\epsilon^{-3k_*-5})$, wh",
    "link": "https://arxiv.org/abs/2404.01200",
    "context": "Title: Large-Scale Non-convex Stochastic Constrained Distributionally Robust Optimization\nAbstract: arXiv:2404.01200v1 Announce Type: cross  Abstract: Distributionally robust optimization (DRO) is a powerful framework for training robust models against data distribution shifts. This paper focuses on constrained DRO, which has an explicit characterization of the robustness level. Existing studies on constrained DRO mostly focus on convex loss function, and exclude the practical and challenging case with non-convex loss function, e.g., neural network. This paper develops a stochastic algorithm and its performance analysis for non-convex constrained DRO. The computational complexity of our stochastic algorithm at each iteration is independent of the overall dataset size, and thus is suitable for large-scale applications. We focus on the general Cressie-Read family divergence defined uncertainty set which includes $\\chi^2$-divergences as a special case. We prove that our algorithm finds an $\\epsilon$-stationary point with a computational complexity of $\\mathcal O(\\epsilon^{-3k_*-5})$, wh",
    "path": "papers/24/04/2404.01200.json",
    "total_tokens": 853,
    "translated_title": "大规模非凸随机约束分布鲁棒优化",
    "translated_abstract": "分布鲁棒优化（DRO）是针对数据分布变化训练健壮模型的强大框架。本文关注具有鲁棒性水平明确特征的约束DRO。现有研究主要集中在具有凸损失函数的约束DRO上，并排除了具有非凸损失函数（如神经网络）的实践和具有挑战性的情况。本文为非凸约束DRO开发了一种随机算法及其性能分析。我们的随机算法在每次迭代的计算复杂度与整体数据集大小独立无关，因此适用于大规模应用。我们侧重于将Cressie-Read家族散度定义的不确定性集成中包含$\\chi^2$-散度作为特例。我们证明了我们的算法在计算复杂度为$\\mathcal O(\\epsilon^{-3k_*-5})$的情况下找到了一个$\\epsilon$-稳定点。",
    "tldr": "本文开发了一种用于非凸约束分布鲁棒优化的随机算法，其计算复杂度与整体数据集大小无关，适用于大规模应用。",
    "en_tdlr": "This paper introduces a stochastic algorithm for non-convex constrained distributionally robust optimization, with computational complexity independent of the overall dataset size, suitable for large-scale applications."
}