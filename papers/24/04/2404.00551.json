{
    "title": "Convergence of Continuous Normalizing Flows for Learning Probability Distributions",
    "abstract": "arXiv:2404.00551v1 Announce Type: cross  Abstract: Continuous normalizing flows (CNFs) are a generative method for learning probability distributions, which is based on ordinary differential equations. This method has shown remarkable empirical success across various applications, including large-scale image synthesis, protein structure prediction, and molecule generation. In this work, we study the theoretical properties of CNFs with linear interpolation in learning probability distributions from a finite random sample, using a flow matching objective function. We establish non-asymptotic error bounds for the distribution estimator based on CNFs, in terms of the Wasserstein-2 distance. The key assumption in our analysis is that the target distribution satisfies one of the following three conditions: it either has a bounded support, is strongly log-concave, or is a finite or infinite mixture of Gaussian distributions. We present a convergence analysis framework that encompasses the err",
    "link": "https://arxiv.org/abs/2404.00551",
    "context": "Title: Convergence of Continuous Normalizing Flows for Learning Probability Distributions\nAbstract: arXiv:2404.00551v1 Announce Type: cross  Abstract: Continuous normalizing flows (CNFs) are a generative method for learning probability distributions, which is based on ordinary differential equations. This method has shown remarkable empirical success across various applications, including large-scale image synthesis, protein structure prediction, and molecule generation. In this work, we study the theoretical properties of CNFs with linear interpolation in learning probability distributions from a finite random sample, using a flow matching objective function. We establish non-asymptotic error bounds for the distribution estimator based on CNFs, in terms of the Wasserstein-2 distance. The key assumption in our analysis is that the target distribution satisfies one of the following three conditions: it either has a bounded support, is strongly log-concave, or is a finite or infinite mixture of Gaussian distributions. We present a convergence analysis framework that encompasses the err",
    "path": "papers/24/04/2404.00551.json",
    "total_tokens": 822,
    "translated_title": "连续正规化流在学习概率分布中的收敛性",
    "translated_abstract": "连续正规化流（CNFs）是一种基于常微分方程的学习概率分布的生成方法。这种方法在各种应用中表现出显著的经验成功，包括大规模图像合成、蛋白质结构预测和分子生成。本文研究了具有线性插值的CNFs在从有限随机样本中学习概率分布时的理论性质，使用了流匹配目标函数。我们建立了基于CNFs的分布估计器的非渐近误差界，以Wasserstein-2距离表示。我们分析的关键假设是目标分布满足以下三个条件之一：要么具有有界支持，要么是强对数凹的，要么是有限或无限混合的高斯分布。我们提出了一个包含了误差收敛分析框架",
    "tldr": "本文研究了具有线性插值的CNFs在从有限随机样本中学习概率分布时的理论性质，建立了非渐近误差界，并提供了收敛分析框架。",
    "en_tdlr": "This paper studies the theoretical properties of CNFs with linear interpolation in learning probability distributions from a finite random sample, establishes non-asymptotic error bounds, and provides a convergence analysis framework."
}