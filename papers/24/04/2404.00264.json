{
    "title": "DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation",
    "abstract": "arXiv:2404.00264v1 Announce Type: new  Abstract: Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outp",
    "link": "https://arxiv.org/abs/2404.00264",
    "context": "Title: DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation\nAbstract: arXiv:2404.00264v1 Announce Type: new  Abstract: Dataset distillation aims to compress a training dataset by creating a small number of informative synthetic samples such that neural networks trained on them perform as well as those trained on the original training dataset. Current text dataset distillation methods create each synthetic sample as a sequence of word embeddings instead of a text to apply gradient-based optimization; however, such embedding-level distilled datasets cannot be used for training other models whose word embedding weights are different from the model used for distillation. To address this issue, we propose a novel text dataset distillation approach, called Distilling dataset into Language Model (DiLM), which trains a language model to generate informative synthetic training samples as text data, instead of directly optimizing synthetic samples. We evaluated DiLM on various text classification datasets and showed that distilled synthetic datasets from DiLM outp",
    "path": "papers/24/04/2404.00264.json",
    "total_tokens": 879,
    "translated_title": "将数据集提炼为语言模型，用于文本级数据集蒸馏",
    "translated_abstract": "数据集蒸馏旨在通过创建少量信息丰富的合成样本来压缩训练数据集，从而使得在其上训练的神经网络的性能能够与在原始训练数据集上训练的网络一样好。当前的文本数据集蒸馏方法将每个合成样本创建为词嵌入序列而不是文本，以应用基于梯度的优化；然而，这种嵌入级别的蒸馏数据集无法用于训练其他模型，其词嵌入权重不同于用于蒸馏的模型。为解决这一问题，本文提出了一种新颖的文本数据集蒸馏方法，称为Distilling dataset into Language Model（DiLM），该方法训练语言模型以生成信息丰富的文本数据作为合成训练样本，而不是直接优化合成样本。我们在各种文本分类数据集上评估了DiLM，并展示了从DiLM 中蒸馏得到的合成数据集的优秀表现。",
    "tldr": "提出了一种名为DiLM的文本数据集蒸馏方法，通过训练语言模型生成文本数据作为合成训练样本，解决了嵌入级别蒸馏数据集无法用于训练其他模型的问题。",
    "en_tdlr": "Proposed a text dataset distillation method named DiLM, which trains a language model to generate text data as synthetic training samples, addressing the issue of embedding-level distilled datasets being incompatible with training other models."
}