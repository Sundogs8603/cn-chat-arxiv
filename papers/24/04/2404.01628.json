{
    "title": "Learning Equi-angular Representations for Online Continual Learning",
    "abstract": "arXiv:2404.01628v1 Announce Type: cross  Abstract: Online continual learning suffers from an underfitted solution due to insufficient training for prompt model update (e.g., single-epoch training). To address the challenge, we propose an efficient online continual learning method using the neural collapse phenomenon. In particular, we induce neural collapse to form a simplex equiangular tight frame (ETF) structure in the representation space so that the continuously learned model with a single epoch can better fit to the streamed data by proposing preparatory data training and residual correction in the representation space. With an extensive set of empirical validations using CIFAR-10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we show that our proposed method outperforms state-of-the-art methods by a noticeable margin in various online continual learning scenarios such as disjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups.",
    "link": "https://arxiv.org/abs/2404.01628",
    "context": "Title: Learning Equi-angular Representations for Online Continual Learning\nAbstract: arXiv:2404.01628v1 Announce Type: cross  Abstract: Online continual learning suffers from an underfitted solution due to insufficient training for prompt model update (e.g., single-epoch training). To address the challenge, we propose an efficient online continual learning method using the neural collapse phenomenon. In particular, we induce neural collapse to form a simplex equiangular tight frame (ETF) structure in the representation space so that the continuously learned model with a single epoch can better fit to the streamed data by proposing preparatory data training and residual correction in the representation space. With an extensive set of empirical validations using CIFAR-10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we show that our proposed method outperforms state-of-the-art methods by a noticeable margin in various online continual learning scenarios such as disjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups.",
    "path": "papers/24/04/2404.01628.json",
    "total_tokens": 905,
    "translated_title": "学习等角表示进行在线连续学习",
    "translated_abstract": "在线连续学习存在欠拟合解决方案的问题，因为由于不及时的模型更新培训不足（例如，单周期训练）。为了解决这一挑战，我们提出了一种有效的在线连续学习方法，利用神经坍缩现象。具体地，我们诱导神经坍缩形成表示空间中的单纯等角紧框架（ETF）结构，以便通过在表示空间中提出预备数据训练和残差修正来更好地使经过单周期学习的持续学习模型适应流媒体数据。通过使用CIFAR-10/100、TinyImageNet、ImageNet-200和ImageNet-1K进行大量实证验证，我们展示了我们提出的方法在各种在线连续学习场景（如不相交和高斯调度连续（即无边界）数据设置）中均较领先方法表现出显著优势。",
    "tldr": "使用神经坍缩现象引入神经坍缩来形成表示空间中的等角紧框架结构，通过提出预备数据训练和残差修正，使得单周期学习的连续学习模型能更好地适应流数据，这种方法在在线连续学习中取得了明显的优势。",
    "en_tdlr": "By inducing neural collapse to form a simplex equiangular tight frame (ETF) structure in the representation space, proposing preparatory data training and residual correction, the method enables the continuously learned model with a single epoch to better fit to the streamed data, outperforming state-of-the-art methods in various online continual learning scenarios."
}