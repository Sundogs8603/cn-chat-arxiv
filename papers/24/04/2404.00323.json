{
    "title": "CLIP-driven Outliers Synthesis for few-shot OOD detection",
    "abstract": "arXiv:2404.00323v1 Announce Type: cross  Abstract: Few-shot OOD detection focuses on recognizing out-of-distribution (OOD) images that belong to classes unseen during training, with the use of only a small number of labeled in-distribution (ID) images. Up to now, a mainstream strategy is based on large-scale vision-language models, such as CLIP. However, these methods overlook a crucial issue: the lack of reliable OOD supervision information, which can lead to biased boundaries between in-distribution (ID) and OOD. To tackle this problem, we propose CLIP-driven Outliers Synthesis~(CLIP-OS). Firstly, CLIP-OS enhances patch-level features' perception by newly proposed patch uniform convolution, and adaptively obtains the proportion of ID-relevant information by employing CLIP-surgery-discrepancy, thus achieving separation between ID-relevant and ID-irrelevant. Next, CLIP-OS synthesizes reliable OOD data by mixing up ID-relevant features from different classes to provide OOD supervision i",
    "link": "https://arxiv.org/abs/2404.00323",
    "context": "Title: CLIP-driven Outliers Synthesis for few-shot OOD detection\nAbstract: arXiv:2404.00323v1 Announce Type: cross  Abstract: Few-shot OOD detection focuses on recognizing out-of-distribution (OOD) images that belong to classes unseen during training, with the use of only a small number of labeled in-distribution (ID) images. Up to now, a mainstream strategy is based on large-scale vision-language models, such as CLIP. However, these methods overlook a crucial issue: the lack of reliable OOD supervision information, which can lead to biased boundaries between in-distribution (ID) and OOD. To tackle this problem, we propose CLIP-driven Outliers Synthesis~(CLIP-OS). Firstly, CLIP-OS enhances patch-level features' perception by newly proposed patch uniform convolution, and adaptively obtains the proportion of ID-relevant information by employing CLIP-surgery-discrepancy, thus achieving separation between ID-relevant and ID-irrelevant. Next, CLIP-OS synthesizes reliable OOD data by mixing up ID-relevant features from different classes to provide OOD supervision i",
    "path": "papers/24/04/2404.00323.json",
    "total_tokens": 936,
    "translated_title": "基于CLIP的离群值合成用于少样本场景下的OOD检测",
    "translated_abstract": "少样本场景下的OOD检测着重于识别那些在训练过程中未见过的类别的、仅使用少量标记的ID图像的out-of-distribution (OOD) 图像。到目前为止，主流的策略是基于大规模视觉-语言模型，如CLIP。然而，这些方法忽视了一个关键问题：缺乏可靠的OOD监督信息，这可能导致在ID和OOD之间产生偏见的边界。为了解决这个问题，我们提出了基于CLIP的离群值合成（CLIP-OS）。首先，CLIP-OS通过新提出的patch uniform convolution增强了patch级特征的感知能力，并通过采用CLIP-surgery-discrepancy自适应地获取ID相关信息的比例，从而实现了ID相关信息和ID不相关信息之间的分离。接下来，CLIP-OS通过混合来自不同类别的ID相关特征来合成可靠的OOD数据，以提供OOD监督信息。",
    "tldr": "提出了一种基于CLIP的离群值合成方法（CLIP-OS），能够在少样本场景下的OOD检测中解决缺乏可靠OOD监督信息的问题，实现了ID相关信息和ID不相关信息的分离，通过混合ID相关特征合成可靠的OOD数据。",
    "en_tdlr": "Proposed CLIP-driven Outliers Synthesis (CLIP-OS) method addresses the lack of reliable OOD supervision information in few-shot OOD detection, achieving separation between ID-relevant and ID-irrelevant information and synthesizing reliable OOD data by mixing ID-relevant features from different classes."
}