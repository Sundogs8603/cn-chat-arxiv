{
    "title": "Rehearsal-Free Modular and Compositional Continual Learning for Language Models",
    "abstract": "arXiv:2404.00790v1 Announce Type: cross  Abstract: Continual learning aims at incrementally acquiring new knowledge while not forgetting existing knowledge. To overcome catastrophic forgetting, methods are either rehearsal-based, i.e., store data examples from previous tasks for data replay, or isolate parameters dedicated to each task. However, rehearsal-based methods raise privacy and memory issues, and parameter-isolation continual learning does not consider interaction between tasks, thus hindering knowledge transfer. In this work, we propose MoCL, a rehearsal-free Modular and Compositional Continual Learning framework which continually adds new modules to language models and composes them with existing modules. Experiments on various benchmarks show that MoCL outperforms state of the art and effectively facilitates knowledge transfer.",
    "link": "https://arxiv.org/abs/2404.00790",
    "context": "Title: Rehearsal-Free Modular and Compositional Continual Learning for Language Models\nAbstract: arXiv:2404.00790v1 Announce Type: cross  Abstract: Continual learning aims at incrementally acquiring new knowledge while not forgetting existing knowledge. To overcome catastrophic forgetting, methods are either rehearsal-based, i.e., store data examples from previous tasks for data replay, or isolate parameters dedicated to each task. However, rehearsal-based methods raise privacy and memory issues, and parameter-isolation continual learning does not consider interaction between tasks, thus hindering knowledge transfer. In this work, we propose MoCL, a rehearsal-free Modular and Compositional Continual Learning framework which continually adds new modules to language models and composes them with existing modules. Experiments on various benchmarks show that MoCL outperforms state of the art and effectively facilitates knowledge transfer.",
    "path": "papers/24/04/2404.00790.json",
    "total_tokens": 821,
    "translated_title": "无需排练的模块化和组合式持续学习对语言模型的应用",
    "translated_abstract": "持续学习旨在在不遗忘现有知识的情况下，逐步获取新的知识。为了克服灾难性遗忘，方法要么基于排练，即存储来自先前任务的数据示例以进行数据重播，要么将参数隔离分配给每个任务。然而，基于排练的方法会引发隐私和内存问题，参数隔离的持续学习方法不考虑任务之间的相互作用，从而阻碍知识转移。在这项工作中，我们提出了MoCL，一个无需排练的模块化和组合式持续学习框架，该框架不断向语言模型添加新的模块，并将其与现有模块组合在一起。在各种基准测试中的实验表明，MoCL优于现有技术，并有效促进了知识转移。",
    "tldr": "提出了一种无需排练的模块化和组合式持续学习框架，可以持续向语言模型添加新模块并将其与现有模块组合，实验证明该框架优于现有技术并有效推动知识转移。",
    "en_tdlr": "Proposed a rehearsal-free modular and compositional continual learning framework that continuously adds new modules to language models and composes them with existing modules, with experiments showing its superiority over existing techniques and effectively facilitating knowledge transfer."
}