{
    "title": "Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News Article using Transformer-based Models",
    "abstract": "arXiv:2404.00386v1 Announce Type: new  Abstract: In this paper, we describe the different approaches explored by the Jetsons team for the Multi-Lingual ESG Impact Duration Inference (ML-ESG-3) shared task. The shared task focuses on predicting the duration and type of the ESG impact of a news article. The shared task dataset consists of 2,059 news titles and articles in English, French, Korean, and Japanese languages. For the impact duration classification task, we fine-tuned XLM-RoBERTa with a custom fine-tuning strategy and using self-training and DeBERTa-v3 using only English translations. These models individually ranked first on the leaderboard for Korean and Japanese and in an ensemble for the English language, respectively. For the impact type classification task, our XLM-RoBERTa model fine-tuned using a custom fine-tuning strategy ranked first for the English language.",
    "link": "https://arxiv.org/abs/2404.00386",
    "context": "Title: Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News Article using Transformer-based Models\nAbstract: arXiv:2404.00386v1 Announce Type: new  Abstract: In this paper, we describe the different approaches explored by the Jetsons team for the Multi-Lingual ESG Impact Duration Inference (ML-ESG-3) shared task. The shared task focuses on predicting the duration and type of the ESG impact of a news article. The shared task dataset consists of 2,059 news titles and articles in English, French, Korean, and Japanese languages. For the impact duration classification task, we fine-tuned XLM-RoBERTa with a custom fine-tuning strategy and using self-training and DeBERTa-v3 using only English translations. These models individually ranked first on the leaderboard for Korean and Japanese and in an ensemble for the English language, respectively. For the impact type classification task, our XLM-RoBERTa model fine-tuned using a custom fine-tuning strategy ranked first for the English language.",
    "path": "papers/24/04/2404.00386.json",
    "total_tokens": 874,
    "translated_title": "《杰森斯队在FinNLP 2024:基于Transformer模型，探索新闻文章的ESG影响理解》",
    "translated_abstract": "本文描述了杰森斯队针对多语言ESG影响时长推断（ML-ESG-3）共享任务所探索的不同方法。该共享任务专注于预测新闻文章的ESG影响时长和类型。共享任务的数据集包括2,059篇英语、法语、韩语和日语新闻标题和文章。针对影响时长分类任务，我们采用自定义微调策略对XLM-RoBERTa和DeBERTa-v3进行微调，并分别使用自训练和仅使用英语翻译。这些模型分别在韩语和日语的排行榜上排名第一，并在英语语言的集成中排名第一。针对影响类型分类任务，我们的XLM-RoBERTa模型通过自定义微调策略在英语语言中排名第一。",
    "tldr": "本文描述了Jetsons团队在多语言ESG影响时长推断共享任务中探索的不同方法，包括使用XLM-RoBERTa和DeBERTa-v3对影响时长进行预测，并在英语语言中取得领先地位。",
    "en_tdlr": "This paper describes the different approaches explored by the Jetsons team for the Multi-Lingual ESG Impact Duration Inference (ML-ESG-3) shared task, including predicting impact duration using XLM-RoBERTa and DeBERTa-v3 and achieving leadership in the English language."
}