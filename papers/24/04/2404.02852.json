{
    "title": "Toward Inference-optimal Mixture-of-Expert Large Language Models",
    "abstract": "arXiv:2404.02852v1 Announce Type: new  Abstract: Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers. Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric b",
    "link": "https://arxiv.org/abs/2404.02852",
    "context": "Title: Toward Inference-optimal Mixture-of-Expert Large Language Models\nAbstract: arXiv:2404.02852v1 Announce Type: new  Abstract: Mixture-of-Expert (MoE) based large language models (LLMs), such as the recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size without suffering from the quadratic growth of training cost of dense transformers. Like dense models, training MoEs requires answering the same question: given a training budget, what is the optimal allocation on the model size and number of tokens? We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric b",
    "path": "papers/24/04/2404.02852.json",
    "total_tokens": 863,
    "translated_title": "朝向推理最佳的混合专家大型语言模型",
    "translated_abstract": "Mixture-of-Expert（MoE）大型语言模型（LLMs），如最近的Mixtral和DeepSeek-MoE，展示了在缩放模型大小时不会遭受密集变压器训练成本的二次增长的巨大潜力。与密集模型一样，训练MoEs需要回答同样的问题：在给定的训练预算下，模型大小和标记数的最佳分配是多少？我们研究了关于模型性能、模型大小、数据集大小和专家程度之间关系的MoE-based LLMs的缩放定律。回应先前研究MoE在不同情境下的研究，我们观察到增加专家数量的递减回报，但这似乎表明我们应该扩展专家数量直至饱和，因为训练成本会保持恒定，这在推理时间中存在问题。我们提出通过引入推理效率作为另一个度量标准来修改MoE的缩放定律",
    "tldr": "MoE-based大型语言模型的研究强调了推理时间与专家数量之间的平衡问题，提出了引入推理效率作为缩放定律的调整方案",
    "en_tdlr": "The study on MoE-based large language models highlights the trade-off between inference time and the number of experts, proposing the introduction of inference efficiency as an adjustment to the scaling law."
}