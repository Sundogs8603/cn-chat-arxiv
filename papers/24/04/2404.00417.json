{
    "title": "Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation",
    "abstract": "arXiv:2404.00417v1 Announce Type: cross  Abstract: To accommodate real-world dynamics, artificial intelligence systems need to cope with sequentially arriving content in an online manner. Beyond regular Continual Learning (CL) attempting to address catastrophic forgetting with offline training of each task, Online Continual Learning (OCL) is a more challenging yet realistic setting that performs CL in a one-pass data stream. Current OCL methods primarily rely on memory replay of old training samples. However, a notable gap from CL to OCL stems from the additional overfitting-underfitting dilemma associated with the use of rehearsal buffers: the inadequate learning of new training samples (underfitting) and the repeated learning of a few old training samples (overfitting). To this end, we introduce a novel approach, Multi-level Online Sequential Experts (MOSE), which cultivates the model as stacked sub-experts, integrating multi-level supervision and reverse self-distillation. Supervisi",
    "link": "https://arxiv.org/abs/2404.00417",
    "context": "Title: Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation\nAbstract: arXiv:2404.00417v1 Announce Type: cross  Abstract: To accommodate real-world dynamics, artificial intelligence systems need to cope with sequentially arriving content in an online manner. Beyond regular Continual Learning (CL) attempting to address catastrophic forgetting with offline training of each task, Online Continual Learning (OCL) is a more challenging yet realistic setting that performs CL in a one-pass data stream. Current OCL methods primarily rely on memory replay of old training samples. However, a notable gap from CL to OCL stems from the additional overfitting-underfitting dilemma associated with the use of rehearsal buffers: the inadequate learning of new training samples (underfitting) and the repeated learning of a few old training samples (overfitting). To this end, we introduce a novel approach, Multi-level Online Sequential Experts (MOSE), which cultivates the model as stacked sub-experts, integrating multi-level supervision and reverse self-distillation. Supervisi",
    "path": "papers/24/04/2404.00417.json",
    "total_tokens": 913,
    "translated_title": "在线持续学习中推进潜在专业知识的编排：多层监督和反向自蒸馏",
    "translated_abstract": "为了适应现实世界的动态变化，人工智能系统需要以在线方式处理连续到达的内容。在正常持续学习（CL）试图通过离线训练每个任务来解决灾难性遗忘的情况之外，在一次数据流中执行CL的在线持续学习（OCL）是一种更具挑战性但更现实的设置。当前的OCL方法主要依赖于旧训练样本的内存重放。然而，从CL到OCL的一个显着差距源于与重演缓冲区的使用相关的过度拟合-欠拟合困境：对新训练样本的学习不足（欠拟合）以及对少量旧训练样本的重复学习（过拟合）。为此，我们引入了一种新颖的方法，即多级在线顺序专家（MOSE），它将模型作为堆叠的子专家进行培育，并整合了多级监督和反向自蒸馏。",
    "tldr": "引入了Multi-level Online Sequential Experts (MOSE)方法，通过多层监督和反向自蒸馏，解决了在线持续学习中新旧训练样本学习不足和重复学习的问题。",
    "en_tdlr": "The paper introduces the Multi-level Online Sequential Experts (MOSE) method, which addresses the issue of inadequate learning of new and repeated learning of old training samples in online continual learning through multi-level supervision and reverse self-distillation."
}