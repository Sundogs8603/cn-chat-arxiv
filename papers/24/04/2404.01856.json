{
    "title": "Poro 34B and the Blessing of Multilinguality",
    "abstract": "arXiv:2404.01856v1 Announce Type: new  Abstract: The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels i",
    "link": "https://arxiv.org/abs/2404.01856",
    "context": "Title: Poro 34B and the Blessing of Multilinguality\nAbstract: arXiv:2404.01856v1 Announce Type: new  Abstract: The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels i",
    "path": "papers/24/04/2404.01856.json",
    "total_tokens": 828,
    "translated_title": "Poro 34B和多语种的祝福",
    "translated_abstract": "最先进大型语言模型的预训练现在需要数万亿字的文本，这比绝大多数语言可获得的文本数量多几个数量级。尽管包含多种语言的文本是获取更多预训练数据的明显方法，但多语种往往被视为一种诅咒，大多数模型训练工作仍然主要集中在个别大语种上。我们相信多语种可以是一种祝福，并且应该有可能通过多语种训练显著提高小语种的模型能力。在这项研究中，我们介绍了Poro 34B，这是一个在1万亿个芬兰语、英语和编程语言标记上进行训练的拥有340亿参数的模型，并证明了多语种训练方法可以产生一个模型，不仅在芬兰语的现有模型能力上取得了显著进展，而且在表现方面表现出色。",
    "tldr": "多语种训练的Poro 34B模型在芬兰语等小语种上取得了显著进展，并具有比现有模型更出色的能力。",
    "en_tdlr": "The multilingual training of the Poro 34B model has significantly advanced capabilities for small languages such as Finnish, surpassing existing models."
}