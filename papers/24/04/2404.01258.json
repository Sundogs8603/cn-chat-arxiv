{
    "title": "Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward",
    "abstract": "arXiv:2404.01258v1 Announce Type: cross  Abstract: Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mec",
    "link": "https://arxiv.org/abs/2404.01258",
    "context": "Title: Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward\nAbstract: arXiv:2404.01258v1 Announce Type: cross  Abstract: Preference modeling techniques, such as direct preference optimization (DPO), has shown effective in enhancing the generalization abilities of large language model (LLM). However, in tasks involving video instruction-following, providing informative feedback, especially for detecting hallucinations in generated responses, remains a significant challenge. Previous studies have explored using large large multimodal models (LMMs) as reward models to guide preference modeling, but their ability to accurately assess the factuality of generated responses compared to corresponding videos has not been conclusively established. This paper introduces a novel framework that utilizes detailed video captions as a proxy of video content, enabling language models to incorporate this information as supporting evidence for scoring video Question Answering (QA) predictions. Our approach demonstrates robust alignment with OpenAI GPT-4V model's reward mec",
    "path": "papers/24/04/2404.01258.json",
    "total_tokens": 852,
    "translated_title": "语言模型奖励下的视频大型多模态模型直接偏好优化",
    "translated_abstract": "偏好建模技术，如直接偏好优化（DPO），已证明在增强大型语言模型（LLM）的泛化能力方面是有效的。然而，在涉及视频指令跟随的任务中，提供信息丰富的反馈，特别是用于检测生成的响应中的幻觉，仍然是一个重要挑战。先前的研究已经探讨了使用大型多模态模型（LMM）作为奖励模型来指导偏好建模，但它们准确评估生成响应的事实性与对应视频相比的能力尚未得出结论。本文介绍了一个新颖的框架，利用详细的视频标题作为视频内容的代理，使语言模型能够将这些信息作为支持证据来为视频问答（QA）预测打分。我们的方法表现出与OpenAI GPT-4V模型的奖励机制的稳健对齐",
    "tldr": "本研究提出了一种新的框架，利用详细的视频标题作为视频内容的代理，使得语言模型在评分视频问答（QA）预测时能够融入这些信息作为支持证据。"
}