{
    "title": "Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning",
    "abstract": "arXiv:2404.00051v1 Announce Type: new  Abstract: Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via cont",
    "link": "https://arxiv.org/abs/2404.00051",
    "context": "Title: Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal Knowledge Graph Reasoning\nAbstract: arXiv:2404.00051v1 Announce Type: new  Abstract: Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing facts for incomplete TKGs in complex scenarios (e.g., transductive and inductive settings), which has been gaining increasing attention. Recently, to mitigate dependence on structured connections in TKGs, text-based methods have been developed to utilize rich linguistic information from entity descriptions. However, suffering from the enormous parameters and inflexibility of pre-trained language models, existing text-based methods struggle to balance the textual knowledge and temporal information with computationally expensive purpose-built training strategies. To tap the potential of text-based models for TKGR in various complex scenarios, we propose ChapTER, a Contrastive historical modeling framework with prefix-tuning for TEmporal Reasoning. ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to strike a textual-temporal balance via cont",
    "path": "papers/24/04/2404.00051.json",
    "total_tokens": 793,
    "translated_title": "Deja vu: 使用前缀调整进行对比历史建模，用于时间知识图推理",
    "translated_abstract": "时间知识图推理（TKGR）是在复杂场景中为不完整的TKG推断缺失事实的任务（例如，传导和归纳设置），越来越受到关注。最近，为了减少TKG中结构连接的依赖性，已开发了基于文本的方法，利用实体描述中丰富的语言信息。然而，由于预训练语言模型的巨大参数和不灵活性，现有的基于文本的方法在计算昂贵且目的建立的训练策略上很难平衡文本知识和时间信息。为了发掘文本模型在各种复杂场景中用于TKGR的潜力，我们提出了ChapTER，一个具有前缀调整对比历史建模框架，用于时间推理。",
    "tldr": "提出了ChapTER，一个对比历史建模框架，通过前缀调整实现文本与时间的平衡，用于时间知识图推理。",
    "en_tdlr": "Introducing ChapTER, a framework for contrastive historical modeling that strikes a balance between text and time through prefix tuning, for temporal knowledge graph reasoning."
}