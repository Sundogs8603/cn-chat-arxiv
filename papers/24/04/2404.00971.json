{
    "title": "Exploring and Evaluating Hallucinations in LLM-Powered Code Generation",
    "abstract": "arXiv:2404.00971v1 Announce Type: cross  Abstract: The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinatio",
    "link": "https://arxiv.org/abs/2404.00971",
    "context": "Title: Exploring and Evaluating Hallucinations in LLM-Powered Code Generation\nAbstract: arXiv:2404.00971v1 Announce Type: cross  Abstract: The rise of Large Language Models (LLMs) has significantly advanced many applications on software engineering tasks, particularly in code generation. Despite the promising performance, LLMs are prone to generate hallucinations, which means LLMs might produce outputs that deviate from users' intent, exhibit internal inconsistencies, or misalign with the factual knowledge, making the deployment of LLMs potentially risky in a wide range of applications. Existing work mainly focuses on investing the hallucination in the domain of natural language generation (NLG), leaving a gap in understanding the types and extent of hallucinations in the context of code generation. To bridge the gap, we conducted a thematic analysis of the LLM-generated code to summarize and categorize the hallucinations present in it. Our study established a comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5 primary categories of hallucinatio",
    "path": "papers/24/04/2404.00971.json",
    "total_tokens": 793,
    "translated_title": "探索和评估LLM驱动的代码生成中的幻觉",
    "translated_abstract": "大型语言模型（LLMs）的崛起已经极大地推动了软件工程任务中许多应用的发展，特别是在代码生成方面。尽管表现出色，LLMs容易产生幻觉，即LLMs可能产生与用户意图偏离、表现出内部不一致或与事实知识不符的输出，使得在广泛应用中部署LLMs可能存在风险。现有研究主要集中在自然语言生成（NLG）领域的幻觉，缺乏对代码生成环境中幻觉类型和程度的理解。为了填补这一空白，我们对LLM生成的代码进行了主题分析，总结和归类其中存在的幻觉。我们的研究建立了LLM生成的代码中幻觉的全面分类法，涵盖了5个主要幻觉类别。",
    "tldr": "本研究通过主题分析对LLM生成的代码中的幻觉进行了总结和分类，建立了代码中幻觉的全面分类法。",
    "en_tdlr": "This study conducted a thematic analysis of hallucinations in LLM-generated code to establish a comprehensive taxonomy of hallucinations."
}