{
    "title": "Can Biases in ImageNet Models Explain Generalization?",
    "abstract": "arXiv:2404.01509v1 Announce Type: cross  Abstract: The robust generalization of models to rare, in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods. For image classification, this manifests in the existence of adversarial attacks, the performance drops on distorted images, and a lack of generalization to concepts such as sketches. The current understanding of generalization in neural networks is very limited, but some biases that differentiate models from human vision have been identified and might be causing these limitations. Consequently, several attempts with varying success have been made to reduce these biases during training to improve generalization. We take a step back and sanity-check these attempts. Fixing the architecture to the well-established ResNet-50, we perform a large-scale study on 48 ImageNet models obtained via different ",
    "link": "https://arxiv.org/abs/2404.01509",
    "context": "Title: Can Biases in ImageNet Models Explain Generalization?\nAbstract: arXiv:2404.01509v1 Announce Type: cross  Abstract: The robust generalization of models to rare, in-distribution (ID) samples drawn from the long tail of the training distribution and to out-of-training-distribution (OOD) samples is one of the major challenges of current deep learning methods. For image classification, this manifests in the existence of adversarial attacks, the performance drops on distorted images, and a lack of generalization to concepts such as sketches. The current understanding of generalization in neural networks is very limited, but some biases that differentiate models from human vision have been identified and might be causing these limitations. Consequently, several attempts with varying success have been made to reduce these biases during training to improve generalization. We take a step back and sanity-check these attempts. Fixing the architecture to the well-established ResNet-50, we perform a large-scale study on 48 ImageNet models obtained via different ",
    "path": "papers/24/04/2404.01509.json",
    "total_tokens": 772,
    "translated_title": "图像网模型中的偏见能解释泛化吗？",
    "translated_abstract": "深度学习方法面临的主要挑战之一是模型对来自训练分布长尾的稀有内部分布（ID）样本和训练分布之外（OOD）样本的强大泛化能力。对于图像分类，这体现在对扭曲图像的攻击、性能下降以及对概念（如草图）的泛化不足。目前对神经网络泛化的理解非常有限，但发现了一些区别模型与人类视觉的偏见，这些偏见可能导致这些限制。因此，已经尝试了多种不同成功程度的方法来减少这些训练中的偏见以改善泛化性能。我们在已建立的ResNet-50架构上进行大规模研究，在48个通过不同获取途径获得的ImageNet模型上进行了实验。",
    "tldr": "图像网模型的偏见是否能够解释模型的泛化问题，对此进行了大规模研究。",
    "en_tdlr": "Investigated whether biases in ImageNet models can explain generalization issues through a large-scale study."
}