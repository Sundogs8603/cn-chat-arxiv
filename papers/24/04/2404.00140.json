{
    "title": "Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks",
    "abstract": "arXiv:2404.00140v1 Announce Type: new  Abstract: Explainability algorithms aimed at interpreting decision-making AI systems usually consider balancing two critical dimensions: 1) \\textit{faithfulness}, where explanations accurately reflect the model's inference process. 2) \\textit{plausibility}, where explanations are consistent with domain experts. However, the question arises: do faithfulness and plausibility inherently conflict? In this study, through a comprehensive quantitative comparison between the explanations from the selected explainability methods and expert-level interpretations across three NLP tasks: sentiment analysis, intent detection, and topic labeling, we demonstrate that traditional perturbation-based methods Shapley value and LIME could attain greater faithfulness and plausibility. Our findings suggest that rather than optimizing for one dimension at the expense of the other, we could seek to optimize explainability algorithms with dual objectives to achieve high l",
    "link": "https://arxiv.org/abs/2404.00140",
    "context": "Title: Does Faithfulness Conflict with Plausibility? An Empirical Study in Explainable AI across NLP Tasks\nAbstract: arXiv:2404.00140v1 Announce Type: new  Abstract: Explainability algorithms aimed at interpreting decision-making AI systems usually consider balancing two critical dimensions: 1) \\textit{faithfulness}, where explanations accurately reflect the model's inference process. 2) \\textit{plausibility}, where explanations are consistent with domain experts. However, the question arises: do faithfulness and plausibility inherently conflict? In this study, through a comprehensive quantitative comparison between the explanations from the selected explainability methods and expert-level interpretations across three NLP tasks: sentiment analysis, intent detection, and topic labeling, we demonstrate that traditional perturbation-based methods Shapley value and LIME could attain greater faithfulness and plausibility. Our findings suggest that rather than optimizing for one dimension at the expense of the other, we could seek to optimize explainability algorithms with dual objectives to achieve high l",
    "path": "papers/24/04/2404.00140.json",
    "total_tokens": 802,
    "translated_title": "信实性与可信度是否存在冲突？一项关于自然语言处理任务中可解释人工智能的实证研究",
    "translated_abstract": "旨在解释决策型人工智能系统的可解释性算法通常要平衡两个关键方面：1）\\textit{信实性}，即解释必须准确反映模型的推理过程；2）\\textit{可信度}，即解释必须与领域专家保持一致。然而，一个问题出现了：信实性和可信度是否从根本上存在冲突？通过在情感分析、意图检测和主题标注三个NLP任务中选定的解释方法与专家级解释之间的全面量化比较，我们证明传统的扰动方法Shapley值和LIME可以实现更高的信实性和可信度。我们的发现表明，我们可以寻求通过双重目标优化可解释性算法来实现高",
    "tldr": "传统的扰动方法Shapley值和LIME可实现更高的信实性和可信度，建议优化可解释性算法以实现高效的双重目标"
}