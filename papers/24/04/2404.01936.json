{
    "title": "Settling Time vs. Accuracy Tradeoffs for Clustering Big Data",
    "abstract": "arXiv:2404.01936v1 Announce Type: new  Abstract: We study the theoretical and practical runtime limits of k-means and k-median clustering on large datasets. Since effectively all clustering methods are slower than the time it takes to read the dataset, the fastest approach is to quickly compress the data and perform the clustering on the compressed representation. Unfortunately, there is no universal best choice for compressing the number of points - while random sampling runs in sublinear time and coresets provide theoretical guarantees, the former does not enforce accuracy while the latter is too slow as the numbers of points and clusters grow. Indeed, it has been conjectured that any sensitivity-based coreset construction requires super-linear time in the dataset size. We examine this relationship by first showing that there does exist an algorithm that obtains coresets via sensitivity sampling in effectively linear time - within log-factors of the time it takes to read the data. An",
    "link": "https://arxiv.org/abs/2404.01936",
    "context": "Title: Settling Time vs. Accuracy Tradeoffs for Clustering Big Data\nAbstract: arXiv:2404.01936v1 Announce Type: new  Abstract: We study the theoretical and practical runtime limits of k-means and k-median clustering on large datasets. Since effectively all clustering methods are slower than the time it takes to read the dataset, the fastest approach is to quickly compress the data and perform the clustering on the compressed representation. Unfortunately, there is no universal best choice for compressing the number of points - while random sampling runs in sublinear time and coresets provide theoretical guarantees, the former does not enforce accuracy while the latter is too slow as the numbers of points and clusters grow. Indeed, it has been conjectured that any sensitivity-based coreset construction requires super-linear time in the dataset size. We examine this relationship by first showing that there does exist an algorithm that obtains coresets via sensitivity sampling in effectively linear time - within log-factors of the time it takes to read the data. An",
    "path": "papers/24/04/2404.01936.json",
    "total_tokens": 804,
    "translated_title": "大数据聚类中的时间与准确性权衡",
    "translated_abstract": "我们研究了在大型数据集上进行k-means和k-median聚类的理论和实际运行时限制。由于几乎所有的聚类方法都比读取数据集所需的时间更慢，最快的方法是快速压缩数据，并在压缩表示上执行聚类。不幸的是，并没有通用的最佳选择来压缩点数 - 尽管随机抽样的运行时间为次线性时间且核心集提供了理论保证，但前者并不强制准确性，而后者随着数据点数和聚类数的增长而太慢。实际上，人们推测任何基于灵敏性的核心集构建都需要超线性时间才能对数据集大小进行处理。我们首先证明存在一种算法，通过灵敏性抽样在有效线性时间内获得核心集 - 仅在读取数据所需时间的对数因子内。",
    "tldr": "在大数据聚类中，研究了时间和准确性的权衡，提出了一种可以在有效线性时间内获得核心集的算法。",
    "en_tdlr": "A study on the tradeoffs between time and accuracy in clustering big data, presenting an algorithm that can obtain coresets in effectively linear time."
}