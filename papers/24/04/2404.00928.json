{
    "title": "Instance-Aware Group Quantization for Vision Transformers",
    "abstract": "arXiv:2404.00928v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) is an efficient model compression technique that quantizes a pretrained full-precision model using only a small calibration set of unlabeled samples without retraining. PTQ methods for convolutional neural networks (CNNs) provide quantization results comparable to full-precision counterparts. Directly applying them to vision transformers (ViTs), however, incurs severe performance degradation, mainly due to the differences in architectures between CNNs and ViTs. In particular, the distribution of activations for each channel vary drastically according to input instances, making PTQ methods for CNNs inappropriate for ViTs. To address this, we introduce instance-aware group quantization for ViTs (IGQ-ViT). To this end, we propose to split the channels of activation maps into multiple groups dynamically for each input instance, such that activations within each group share similar statistical properties. We",
    "link": "https://arxiv.org/abs/2404.00928",
    "context": "Title: Instance-Aware Group Quantization for Vision Transformers\nAbstract: arXiv:2404.00928v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) is an efficient model compression technique that quantizes a pretrained full-precision model using only a small calibration set of unlabeled samples without retraining. PTQ methods for convolutional neural networks (CNNs) provide quantization results comparable to full-precision counterparts. Directly applying them to vision transformers (ViTs), however, incurs severe performance degradation, mainly due to the differences in architectures between CNNs and ViTs. In particular, the distribution of activations for each channel vary drastically according to input instances, making PTQ methods for CNNs inappropriate for ViTs. To address this, we introduce instance-aware group quantization for ViTs (IGQ-ViT). To this end, we propose to split the channels of activation maps into multiple groups dynamically for each input instance, such that activations within each group share similar statistical properties. We",
    "path": "papers/24/04/2404.00928.json",
    "total_tokens": 849,
    "translated_title": "视觉Transformer的实例感知组量化",
    "translated_abstract": "后训练量化（PTQ）是一种高效的模型压缩技术，它使用仅有少量未标记样本的校准集对预训练的全精度模型进行量化，而无需重新训练。对卷积神经网络（CNNs）的PTQ方法提供了与全精度对应物可比的量化结果。然而，直接将它们应用于视觉Transformer（ViTs），会导致严重的性能下降，主要是由于CNNs和ViTs之间的架构差异。特别是，每个通道的激活分布根据输入实例大大变化，使得CNNs的PTQ方法不适用于ViTs。为了解决这个问题，我们引入了适用于ViTs的实例感知组量化（IGQ-ViT）。为此，我们提出将激活映射的通道动态地分成多个组，以便为每个输入实例，使得每组内的激活具有相似的统计特性。",
    "tldr": "IGQ-ViT是一种面向视觉Transformer的实例感知组量化方法，通过动态地将激活映射的通道分成多个组，使得每个输入实例内的激活具有相似统计特性。",
    "en_tdlr": "IGQ-ViT is an instance-aware group quantization method for Vision Transformers, dynamically splitting the channels of activation maps into multiple groups to ensure similar statistical properties within each input instance."
}