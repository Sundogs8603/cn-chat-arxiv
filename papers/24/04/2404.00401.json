{
    "title": "How Robust are the Tabular QA Models for Scientific Tables? A Study using Customized Dataset",
    "abstract": "arXiv:2404.00401v1 Announce Type: new  Abstract: Question-answering (QA) on hybrid scientific tabular and textual data deals with scientific information, and relies on complex numerical reasoning. In recent years, while tabular QA has seen rapid progress, understanding their robustness on scientific information is lacking due to absence of any benchmark dataset. To investigate the robustness of the existing state-of-the-art QA models on scientific hybrid tabular data, we propose a new dataset, \"SciTabQA\", consisting of 822 question-answer pairs from scientific tables and their descriptions. With the help of this dataset, we assess the state-of-the-art Tabular QA models based on their ability (i) to use heterogeneous information requiring both structured data (table) and unstructured data (text) and (ii) to perform complex scientific reasoning tasks. In essence, we check the capability of the models to interpret scientific tables and text. Our experiments show that \"SciTabQA\" is an inno",
    "link": "https://arxiv.org/abs/2404.00401",
    "context": "Title: How Robust are the Tabular QA Models for Scientific Tables? A Study using Customized Dataset\nAbstract: arXiv:2404.00401v1 Announce Type: new  Abstract: Question-answering (QA) on hybrid scientific tabular and textual data deals with scientific information, and relies on complex numerical reasoning. In recent years, while tabular QA has seen rapid progress, understanding their robustness on scientific information is lacking due to absence of any benchmark dataset. To investigate the robustness of the existing state-of-the-art QA models on scientific hybrid tabular data, we propose a new dataset, \"SciTabQA\", consisting of 822 question-answer pairs from scientific tables and their descriptions. With the help of this dataset, we assess the state-of-the-art Tabular QA models based on their ability (i) to use heterogeneous information requiring both structured data (table) and unstructured data (text) and (ii) to perform complex scientific reasoning tasks. In essence, we check the capability of the models to interpret scientific tables and text. Our experiments show that \"SciTabQA\" is an inno",
    "path": "papers/24/04/2404.00401.json",
    "total_tokens": 884,
    "translated_title": "科学表格问答模型的鲁棒性有多强？一项使用定制数据集的研究",
    "translated_abstract": "在混合科学表格和文本数据上进行问答(QA)涉及科学信息，并依赖于复杂的数值推理。近年来，虽然表格QA取得了快速进展，但由于缺乏任何基准数据集，对它们在科学信息上的鲁棒性缺乏理解。为了研究现有最先进的QA模型在科学混合表格数据上的鲁棒性，我们提出了一个新数据集“SciTabQA”，其中包含来自科学表格及其描述的822个问答对。借助这个数据集，我们评估了基于最先进Tabular QA模型的能力，即(i)利用需要结构化数据(表格)和非结构化数据(文本)的异构信息以及(ii)执行复杂的科学推理任务。本质上，我们检查模型解释科学表格和文本的能力。我们的实验表明，“SciTabQA”是一项创新的工作。",
    "tldr": "通过提出新的数据集\"SciTabQA\"，研究了现有科学混合表格数据上最先进Tabular QA模型的鲁棒性，评估了其在解释科学表格和文本方面的能力。"
}