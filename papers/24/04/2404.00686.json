{
    "title": "Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning",
    "abstract": "arXiv:2404.00686v1 Announce Type: new  Abstract: Accounting for the uncertainty of value functions boosts exploration in Reinforcement Learning (RL). Our work introduces Maximum Mean Discrepancy Q-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertainty propagation during Temporal Difference (TD) updates. MMD-QL uses the MMD barycenter for this purpose, as MMD provides a tighter estimate of closeness between probability measures than the Wasserstein distance. Firstly, we establish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) under the average loss metric. Concerning the accumulated rewards, experiments on tabular environments show that MMD-QL outperforms WQL and other algorithms. Secondly, we incorporate deep networks into MMD-QL to create MMD Q-Network (MMD-QN). Making reasonable assumptions, we analyze the convergence rates of MMD-QN using function approximation. Empirical results on challenging Atari games demonstrate that MMD-QN performs well comp",
    "link": "https://arxiv.org/abs/2404.00686",
    "context": "Title: Utilizing Maximum Mean Discrepancy Barycenter for Propagating the Uncertainty of Value Functions in Reinforcement Learning\nAbstract: arXiv:2404.00686v1 Announce Type: new  Abstract: Accounting for the uncertainty of value functions boosts exploration in Reinforcement Learning (RL). Our work introduces Maximum Mean Discrepancy Q-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertainty propagation during Temporal Difference (TD) updates. MMD-QL uses the MMD barycenter for this purpose, as MMD provides a tighter estimate of closeness between probability measures than the Wasserstein distance. Firstly, we establish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) under the average loss metric. Concerning the accumulated rewards, experiments on tabular environments show that MMD-QL outperforms WQL and other algorithms. Secondly, we incorporate deep networks into MMD-QL to create MMD Q-Network (MMD-QN). Making reasonable assumptions, we analyze the convergence rates of MMD-QN using function approximation. Empirical results on challenging Atari games demonstrate that MMD-QN performs well comp",
    "path": "papers/24/04/2404.00686.json",
    "total_tokens": 1009,
    "translated_title": "利用最大均值差异重心在强化学习中传播价值函数的不确定性",
    "translated_abstract": "考虑到价值函数的不确定性可以促进强化学习中的探索。我们的工作引入了最大均值差异Q学习（MMD-QL），以改进Wasserstein Q学习（WQL），用于在时间差分（TD）更新期间传播不确定性。MMD-QL使用MMD重心来实现这一目的，因为MMD提供了比Wasserstein距离更紧的概率度量之间的接近度估计。首先，我们证明了在平均损失度量下，MMD-QL在马尔可夫决策过程（MDP）中是“可能近似正确”的。在考虑到累积奖励的情况下，对表格环境进行的实验表明，MMD-QL优于WQL和其他算法。其次，我们将深度网络纳入MMD-QL中，创建MMD Q网络（MMD-QN）。通过合理假设，我们分析了MMD-QN在函数逼近中的收敛速度。在具有挑战性的Atari游戏上的实证结果表明，MMD-QN表现良好。",
    "tldr": "这项工作引入了最大均值差异Q学习（MMD-QL）来改进强化学习中价值函数不确定性的传播，通过使用MMD重心，实现了比Wasserstein距离更紧的概率度量，在实验中表现优于其他算法，并结合深度网络创造了MMD Q网络（MMD-QN）。",
    "en_tdlr": "This work introduces Maximum Mean Discrepancy Q-Learning (MMD-QL) to enhance the propagation of uncertainty in value functions in reinforcement learning, achieving a tighter estimate of closeness between probability measures than Wasserstein distance, outperforming other algorithms in experiments, and incorporating deep networks to create MMD Q-Network (MMD-QN)."
}