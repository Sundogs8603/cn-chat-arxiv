{
    "title": "Privacy Backdoors: Stealing Data with Corrupted Pretrained Models",
    "abstract": "arXiv:2404.00473v1 Announce Type: cross  Abstract: Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.",
    "link": "https://arxiv.org/abs/2404.00473",
    "context": "Title: Privacy Backdoors: Stealing Data with Corrupted Pretrained Models\nAbstract: arXiv:2404.00473v1 Announce Type: cross  Abstract: Practitioners commonly download pretrained machine learning models from open repositories and finetune them to fit specific applications. We show that this practice introduces a new risk of privacy backdoors. By tampering with a pretrained model's weights, an attacker can fully compromise the privacy of the finetuning data. We show how to build privacy backdoors for a variety of models, including transformers, which enable an attacker to reconstruct individual finetuning samples, with a guaranteed success! We further show that backdoored models allow for tight privacy attacks on models trained with differential privacy (DP). The common optimistic practice of training DP models with loose privacy guarantees is thus insecure if the model is not trusted. Overall, our work highlights a crucial and overlooked supply chain attack on machine learning privacy.",
    "path": "papers/24/04/2404.00473.json",
    "total_tokens": 838,
    "translated_title": "隐私后门：使用损坏的预训练模型窃取数据",
    "translated_abstract": "实践者通常从开放仓库下载预训练的机器学习模型，并进行微调以适应特定应用。我们展示了这种实践引入了新的隐私后门风险。通过篡改预训练模型的权重，攻击者可以完全损害微调数据的隐私性。我们展示了如何为各种模型构建隐私后门，包括transformers，使攻击者能够重构单个微调样本，且成功担保！我们进一步展示，带有后门的模型允许对使用差分隐私（DP）训练的模型进行严格的隐私攻击。如果模型不受信任，则典型的用宽松隐私保证训练DP模型的乐观做法是不安全的。总体而言，我们的工作突出了对机器学习隐私的一种关键且被忽视的供应链攻击。",
    "tldr": "预训练模型的权重遭到篡改后，可以构建隐私后门，完全损害微调数据的隐私性，进而对使用差分隐私训练的模型进行严格的隐私攻击。",
    "en_tdlr": "Adversaries can build privacy backdoors by tampering with the weights of pretrained models, compromising the privacy of finetuning data, and enabling tight privacy attacks on differentially private models."
}