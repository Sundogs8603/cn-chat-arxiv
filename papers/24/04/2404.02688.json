{
    "title": "Reinforcement Learning in Categorical Cybernetics",
    "abstract": "arXiv:2404.02688v1 Announce Type: new  Abstract: We show that several major algorithms of reinforcement learning (RL) fit into the framework of categorical cybernetics, that is to say, parametrised bidirectional processes. We build on our previous work in which we show that value iteration can be represented by precomposition with a certain optic. The outline of the main construction in this paper is: (1) We extend the Bellman operators to parametrised optics that apply to action-value functions and depend on a sample. (2) We apply a representable contravariant functor, obtaining a parametrised function that applies the Bellman iteration. (3) This parametrised function becomes the backward pass of another parametrised optic that represents the model, which interacts with an environment via an agent. Thus, parametrised optics appear in two different ways in our construction, with one becoming part of the other. As we show, many of the major classes of algorithms in RL can be seen as dif",
    "link": "https://arxiv.org/abs/2404.02688",
    "context": "Title: Reinforcement Learning in Categorical Cybernetics\nAbstract: arXiv:2404.02688v1 Announce Type: new  Abstract: We show that several major algorithms of reinforcement learning (RL) fit into the framework of categorical cybernetics, that is to say, parametrised bidirectional processes. We build on our previous work in which we show that value iteration can be represented by precomposition with a certain optic. The outline of the main construction in this paper is: (1) We extend the Bellman operators to parametrised optics that apply to action-value functions and depend on a sample. (2) We apply a representable contravariant functor, obtaining a parametrised function that applies the Bellman iteration. (3) This parametrised function becomes the backward pass of another parametrised optic that represents the model, which interacts with an environment via an agent. Thus, parametrised optics appear in two different ways in our construction, with one becoming part of the other. As we show, many of the major classes of algorithms in RL can be seen as dif",
    "path": "papers/24/04/2404.02688.json",
    "total_tokens": 793,
    "translated_title": "在分类控制原理中的强化学习",
    "translated_abstract": "我们展示了几种主要的强化学习（RL）算法适用于分类控制原理框架，即参数化的双向过程。我们在此前的工作基础上展开，其中我们展示了价值迭代可以通过预合成特定的光学表示。本文的主要构造概述如下：（1）我们将Bellman算子扩展到适用于动作值函数并依赖于样本的参数化光学。 （2）我们应用一个可表示的逆变子函子，得到一个应用Bellman迭代的参数化函数。（3）该参数化函数成为另一个代表模型的参数化光学的反向传递，通过代理与环境进行交互。因此，在我们的构造中，参数化光学以两种不同的方式出现，其中一种成为另一种的一部分。",
    "tldr": "强化学习算法可以被归纳到分类控制原理框架中，通过参数化的光学相互作用，展示了新的构造方法。",
    "en_tdlr": "Reinforcement learning algorithms can be categorized into the framework of categorical cybernetics, showing a novel construction through the interaction of parameterized optics."
}