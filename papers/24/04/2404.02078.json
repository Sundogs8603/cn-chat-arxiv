{
    "title": "Advancing LLM Reasoning Generalists with Preference Trees",
    "abstract": "arXiv:2404.02078v1 Announce Type: new  Abstract: We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting o",
    "link": "https://arxiv.org/abs/2404.02078",
    "context": "Title: Advancing LLM Reasoning Generalists with Preference Trees\nAbstract: arXiv:2404.02078v1 Announce Type: new  Abstract: We introduce Eurus, a suite of large language models (LLMs) optimized for reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve state-of-the-art results among open-source models on a diverse set of benchmarks covering mathematics, code generation, and logical reasoning problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a comprehensive benchmarking across 12 tests covering five tasks, and achieves a 33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging benchmarks, substantially outperforming existing open-source models by margins more than 13.3%. The strong performance of Eurus can be primarily attributed to UltraInteract, our newly-curated large-scale, high-quality alignment dataset specifically designed for complex reasoning tasks. UltraInteract can be used in both supervised fine-tuning and preference learning. For each instruction, it includes a preference tree consisting o",
    "path": "papers/24/04/2404.02078.json",
    "total_tokens": 872,
    "translated_title": "通过首选树推进LLM推理通用性",
    "translated_abstract": "我们介绍了Eurus，一套专为推理优化的大语言模型（LLMs）。经过Mistral-7B和CodeLlama-70B的微调，Eurus模型在涵盖数学、代码生成和逻辑推理问题的多样化基准测试中取得了业界领先的成果。值得注意的是，Eurus-70B在通过涵盖五项任务的12个测试的全面基准测试中击败了GPT-3.5 Turbo，并在LeetCode和TheoremQA两个具有挑战性的基准测试中分别实现了33.3%和32.6%的pass@1准确率，明显优于现有开源模型超过13.3%的边际。Eurus的强大性能主要归功于我们新设计的大规模、高质量对齐数据集UltraInteract，该数据集专门为复杂推理任务而设计。UltraInteract可用于监督微调和首选学习。对于每个指令，它包括一个首选树。",
    "tldr": "新推出的Eurus模型通过基于首选树的推理优化，在多项基准测试中取得了业界领先的成果，尤其在击败了GPT-3.5 Turbo的基准测试中表现突出。",
    "en_tdlr": "The newly introduced Eurus models optimize reasoning based on preference trees and achieve state-of-the-art results in multiple benchmarks, notably outperforming GPT-3.5 Turbo."
}