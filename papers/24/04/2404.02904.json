{
    "title": "ALOHa: A New Measure for Hallucination in Captioning Models",
    "abstract": "arXiv:2404.02904v1 Announce Type: cross  Abstract: Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene. The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations. Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories.",
    "link": "https://arxiv.org/abs/2404.02904",
    "context": "Title: ALOHa: A New Measure for Hallucination in Captioning Models\nAbstract: arXiv:2404.02904v1 Announce Type: cross  Abstract: Despite recent advances in multimodal pre-training for visual description, state-of-the-art models still produce captions containing errors, such as hallucinating objects not present in a scene. The existing prominent metric for object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa, which leverages large language models (LLMs) to measure object hallucinations. Specifically, we use an LLM to extract groundable objects from a candidate caption, measure their semantic similarity to reference objects from captions and object detections, and use Hungarian matching to produce a final hallucination score. We show that ALOHa correctly identifies 13.6% more hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO Captions annotated for hallucinations, and 30.8% more on nocaps, where objects extend beyond MS COCO categories.",
    "path": "papers/24/04/2404.02904.json",
    "total_tokens": 919,
    "translated_title": "ALOHa：测量图像字幕模型中幻觉的新标准",
    "translated_abstract": "尽管在视觉描述的多模态预训练方面取得了近期的进展，但最先进的模型仍会产生包含错误的字幕，比如在场景中存在幻觉对象。现有的主要幻觉对象度量标准CHAIR，仅限于一组固定的MS COCO对象和同义词。在这项工作中，我们提出了一种现代化的开放词汇度量标准ALOHa，利用大型语言模型（LLM）来衡量对象幻觉。具体地，我们使用LLM从候选字幕中提取可连接的对象，衡量它们与字幕和对象检测中参考对象的语义相似度，并使用匈牙利匹配生成最终的幻觉得分。我们展示了ALOHa在HAT上比CHAIR在一个新的用于幻觉标记的MS COCO字幕的金标准子集上正确识别了更多的幻觉对象（多出13.6%），在nocaps上（其中对象超出了MS COCO类别）识别了更多的幻觉对象（多至30.8%）。",
    "tldr": "提出了一种新的用于测量图像字幕模型中幻觉的标准ALOHa，利用大型语言模型来测量幻觉对象，并成功识别比现有指标CHAIR更多的幻觉对象。",
    "en_tdlr": "Introduced a new standard ALOHa for measuring hallucinations in captioning models, leveraging large language models to measure hallucinated objects and successfully identifying more hallucinated objects than the existing metric CHAIR."
}