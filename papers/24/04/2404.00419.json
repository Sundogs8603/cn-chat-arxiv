{
    "title": "Do Vision-Language Models Understand Compound Nouns?",
    "abstract": "arXiv:2404.00419v1 Announce Type: cross  Abstract: Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., lab coat) as well as they understand nouns (e.g., lab)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs' limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple ",
    "link": "https://arxiv.org/abs/2404.00419",
    "context": "Title: Do Vision-Language Models Understand Compound Nouns?\nAbstract: arXiv:2404.00419v1 Announce Type: cross  Abstract: Open-vocabulary vision-language models (VLMs) like CLIP, trained using contrastive loss, have emerged as a promising new paradigm for text-to-image retrieval. However, do VLMs understand compound nouns (CNs) (e.g., lab coat) as well as they understand nouns (e.g., lab)? We curate Compun, a novel benchmark with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in interpreting CNs. The Compun benchmark challenges a VLM for text-to-image retrieval where, given a text prompt with a CN, the task is to select the correct image that shows the CN among a pair of distractor images that show the constituent nouns that make up the CN. Next, we perform an in-depth analysis to highlight CLIPs' limited understanding of certain types of CNs. Finally, we present an alternative framework that moves beyond hand-written templates for text prompts widely used by CLIP-like models. We employ a Large Language Model to generate multiple ",
    "path": "papers/24/04/2404.00419.json",
    "total_tokens": 902,
    "translated_title": "视觉-语言模型是否理解复合名词？",
    "translated_abstract": "开放词汇的视觉-语言模型（VLMs）如CLIP，使用对比损失进行训练，已经成为一种有前景的新文本到图像检索范式。然而，VLMs是否像理解名词（如实验室）一样理解复合名词（CNs）（如实验室外套）？我们策划了一个名为Compun的新基准测试，其中包含400个独特且常用的CNs，以评估VLMs在解释CNs方面的有效性。Compun基准测试挑战VLM进行文本到图像检索，给定一段包含CN的文本提示，任务是从一对显示构成CN的名词的干扰图像中选择正确显示该CN的图像。接下来，我们进行深入分析，突出CLIP对某些类型的CN的理解能力有限。最后，我们提出了一个超越CLIP类模型广泛使用的手写模板的替代框架。我们使用大型语言模型生成多个",
    "tldr": "该研究策划了一个名为Compun的新基准测试，通过文本提示和图像选择任务评估VLMs在理解复合名词方面的表现，并展现了CLIP在某些类型的复合名词理解上存在局限性。",
    "en_tdlr": "This study introduces a new benchmark called Compun to evaluate the performance of VLMs in understanding compound nouns through textual prompts and image selection tasks, and reveals the limitations of CLIP in understanding certain types of compound nouns."
}