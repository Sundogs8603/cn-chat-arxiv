{
    "title": "Noise Masking Attacks and Defenses for Pretrained Speech Models",
    "abstract": "arXiv:2404.02052v1 Announce Type: new  Abstract: Speech models are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage. Our work considers noise masking attacks, introduced by Amid et al. 2022, which attack automatic speech recognition (ASR) models by requesting a transcript of an utterance which is partially replaced with noise. They show that when a record has been seen at training time, the model will transcribe the noisy record with its memorized sensitive transcript. In our work, we extend these attacks beyond ASR models, to attack pretrained speech encoders. Our method fine-tunes the encoder to produce an ASR model, and then performs noise masking on this model, which we find recovers private information from the pretraining data, despite the model never having seen transcripts at pretraining time! We show how to improve the precision of these attacks and investigate a number of countermeasures to our attacks.",
    "link": "https://arxiv.org/abs/2404.02052",
    "context": "Title: Noise Masking Attacks and Defenses for Pretrained Speech Models\nAbstract: arXiv:2404.02052v1 Announce Type: new  Abstract: Speech models are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage. Our work considers noise masking attacks, introduced by Amid et al. 2022, which attack automatic speech recognition (ASR) models by requesting a transcript of an utterance which is partially replaced with noise. They show that when a record has been seen at training time, the model will transcribe the noisy record with its memorized sensitive transcript. In our work, we extend these attacks beyond ASR models, to attack pretrained speech encoders. Our method fine-tunes the encoder to produce an ASR model, and then performs noise masking on this model, which we find recovers private information from the pretraining data, despite the model never having seen transcripts at pretraining time! We show how to improve the precision of these attacks and investigate a number of countermeasures to our attacks.",
    "path": "papers/24/04/2404.02052.json",
    "total_tokens": 881,
    "translated_title": "预训练语音模型的噪声遮蔽攻击与防御",
    "translated_abstract": "语音模型经常在敏感数据上进行训练以提高模型性能，但这可能导致潜在的隐私泄露。在本工作中，我们考虑了Amid等人于2022年提出的噪声遮蔽攻击，这些攻击针对自动语音识别（ASR）模型，通过请求一段部分被噪声替换的话语的转录来实施攻击。他们表明，当训练时看到了一条记录，模型将用其记忆的敏感转录转录这个带有噪声的记录。在我们的工作中，我们将这些攻击扩展到预训练语音编码器，并且我们的方法微调编码器以生成一个ASR模型，然后在该模型上进行噪声遮蔽，我们发现这种方法可以从预训练数据中恢复私人信息，尽管模型在预训练时从未看过转录！我们展示了如何提高这些攻击的精度，并研究了一些针对我们攻击的对策。",
    "tldr": "本研究提出了针对预训练语音编码器的噪声遮蔽攻击，并深入探讨了攻击的精度提升和反制措施。",
    "en_tdlr": "This study introduces noise masking attacks against pretrained speech encoders and investigates precision enhancements and countermeasures against the attacks."
}