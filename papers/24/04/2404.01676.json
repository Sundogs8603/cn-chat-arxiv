{
    "title": "Incentives in Private Collaborative Machine Learning",
    "abstract": "arXiv:2404.01676v1 Announce Type: new  Abstract: Collaborative machine learning involves training models on data from multiple parties but must incentivize their participation. Existing data valuation methods fairly value and reward each party based on shared data or model parameters but neglect the privacy risks involved. To address this, we introduce differential privacy (DP) as an incentive. Each party can select its required DP guarantee and perturb its sufficient statistic (SS) accordingly. The mediator values the perturbed SS by the Bayesian surprise it elicits about the model parameters. As our valuation function enforces a privacy-valuation trade-off, parties are deterred from selecting excessive DP guarantees that reduce the utility of the grand coalition's model. Finally, the mediator rewards each party with different posterior samples of the model parameters. Such rewards still satisfy existing incentives like fairness but additionally preserve DP and a high similarity to th",
    "link": "https://arxiv.org/abs/2404.01676",
    "context": "Title: Incentives in Private Collaborative Machine Learning\nAbstract: arXiv:2404.01676v1 Announce Type: new  Abstract: Collaborative machine learning involves training models on data from multiple parties but must incentivize their participation. Existing data valuation methods fairly value and reward each party based on shared data or model parameters but neglect the privacy risks involved. To address this, we introduce differential privacy (DP) as an incentive. Each party can select its required DP guarantee and perturb its sufficient statistic (SS) accordingly. The mediator values the perturbed SS by the Bayesian surprise it elicits about the model parameters. As our valuation function enforces a privacy-valuation trade-off, parties are deterred from selecting excessive DP guarantees that reduce the utility of the grand coalition's model. Finally, the mediator rewards each party with different posterior samples of the model parameters. Such rewards still satisfy existing incentives like fairness but additionally preserve DP and a high similarity to th",
    "path": "papers/24/04/2404.01676.json",
    "total_tokens": 874,
    "translated_title": "私人合作机器学习中的激励机制",
    "translated_abstract": "协作机器学习涉及在来自多方数据的模型上训练，但必须激励各方参与。现有数据估值方法公平地根据共享数据或模型参数价值和奖励每个参与方，但忽视了涉及的隐私风险。为了解决这一问题，我们引入差分隐私（DP）作为一种激励机制。每个参与方可以选择其需要的DP保证，并相应地扰动其充分统计量（SS）。中介者通过模型参数所引发的贝叶斯惊讶值来价值化扰动的SS。由于我们的估值函数强制实施隐私-估值权衡，各方被阻止选择过高的DP保证，以减少大联盟模型的效用。最后，中介者用不同的后验模型参数样本奖励每个参与方。这种奖励仍然满足公平等现有激励要求，但另外还保护差分隐私和高度相似性。",
    "tldr": "引入差分隐私作为激励机制，中介者通过贝叶斯惊讶值来价值化扰动的充分统计量，实现隐私-估值权衡，同时保护差分隐私和高度相似性。"
}