{
    "title": "PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison",
    "abstract": "arXiv:2404.01015v1 Announce Type: new  Abstract: Building a reliable and automated evaluation metric is a necessary but challenging problem for open-domain dialogue systems. Recent studies proposed evaluation metrics that assess generated responses by considering their relevance to previous dialogue histories. Although effective, these metrics evaluate individual responses directly rather than considering their relative quality compared to other responses. To handle this, we propose PairEval, a novel dialogue evaluation metric for assessing responses by comparing their quality against responses in different conversations. PairEval is built on top of open-sourced and moderate-size language models, and we make them specialized in pairwise comparison between dialogue responses. Extensive experiments on multiple benchmarks demonstrate that our metric exhibits a higher correlation with human judgments than baseline metrics. We also find that the proposed comparative metric is more robust in",
    "link": "https://arxiv.org/abs/2404.01015",
    "context": "Title: PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison\nAbstract: arXiv:2404.01015v1 Announce Type: new  Abstract: Building a reliable and automated evaluation metric is a necessary but challenging problem for open-domain dialogue systems. Recent studies proposed evaluation metrics that assess generated responses by considering their relevance to previous dialogue histories. Although effective, these metrics evaluate individual responses directly rather than considering their relative quality compared to other responses. To handle this, we propose PairEval, a novel dialogue evaluation metric for assessing responses by comparing their quality against responses in different conversations. PairEval is built on top of open-sourced and moderate-size language models, and we make them specialized in pairwise comparison between dialogue responses. Extensive experiments on multiple benchmarks demonstrate that our metric exhibits a higher correlation with human judgments than baseline metrics. We also find that the proposed comparative metric is more robust in",
    "path": "papers/24/04/2404.01015.json",
    "total_tokens": 816,
    "translated_title": "PairEval：使用两两比较进行开放域对话评估",
    "translated_abstract": "arXiv:2404.01015v1 公告类型：新 建立可靠且自动化的评估指标是开放域对话系统中必不可少但具有挑战性的问题。最近的研究提出了评估指标，通过考虑生成的回复与之前的对话历史的相关性来评估这些回复。尽管有效，但这些指标直接评估单个回复，而未考虑其相对质量与其他回复相比的情况。为了解决这个问题，我们提出了PairEval，一种新颖的对话评估指标，通过将回复的质量与不同对话中的回复进行比较来评估。PairEval建立在开源和中等规模的语言模型之上，并使其专门化于对话回复之间的两两比较。在多个基准测试上进行了大量实验证明，我们的指标与人类判断呈现出更高的相关性超过基线指标。我们还发现，所提出的比较性指标在",
    "tldr": "提出了PairEval，一种新颖的对话评估指标，通过将回复的质量与不同对话中的回复进行比较来评估，与人类判断具有更高的相关性。",
    "en_tdlr": "Introduced PairEval, a novel dialogue evaluation metric that assesses responses by comparing their quality against responses in different conversations, exhibiting higher correlation with human judgments."
}