{
    "title": "Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models",
    "abstract": "arXiv:2404.01295v1 Announce Type: cross  Abstract: As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM. We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs. Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.",
    "link": "https://arxiv.org/abs/2404.01295",
    "context": "Title: Towards Safety and Helpfulness Balanced Responses via Controllable Large Language Models\nAbstract: arXiv:2404.01295v1 Announce Type: cross  Abstract: As large language models (LLMs) become easily accessible nowadays, the trade-off between safety and helpfulness can significantly impact user experience. A model that prioritizes safety will cause users to feel less engaged and assisted while prioritizing helpfulness will potentially cause harm. Possible harms include teaching people how to build a bomb, exposing youth to inappropriate content, and hurting users' mental health. In this work, we propose to balance safety and helpfulness in diverse use cases by controlling both attributes in LLM. We explore training-free and fine-tuning methods that do not require extra human annotations and analyze the challenges of controlling safety and helpfulness in LLMs. Our experiments demonstrate that our method can rewind a learned model and unlock its controllability.",
    "path": "papers/24/04/2404.01295.json",
    "total_tokens": 790,
    "translated_title": "通过可控大语言模型实现安全和帮助平衡的回应",
    "translated_abstract": "随着大型语言模型(LLMs)如今变得容易获得，安全和帮助之间的权衡会显著影响用户体验。一个优先考虑安全的模型会导致用户感到较少参与和被协助，而优先考虑帮助性则可能导致伤害。可能的危害包括教授人们如何制造炸弹，向青少年暴露不当内容以及伤害用户的心理健康。在这项工作中，我们提出通过控制LLMs中的两个属性来在不同的用例中平衡安全性和帮助性。我们探讨了不需要额外人员注释的训练无关和微调方法，并分析了控制LLMs中安全性和帮助性的挑战。我们的实验证明了我们的方法可以重置已学习的模型并解锁其可控性。",
    "tldr": "通过控制大语言模型中的属性，实现安全和帮助之间的平衡，并且可以在不同的应用场景中实现这种平衡。",
    "en_tdlr": "Balancing safety and helpfulness through controlling attributes in large language models, allowing for this balance to be achieved in various use cases."
}