{
    "title": "GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs",
    "abstract": "arXiv:2404.00189v1 Announce Type: new  Abstract: This study introduces GPTA, a Large Language Model assistance training framework, that enhances the training of downstream task models via prefix prompt. By minimizing data exposure to LLM, the framework addresses the security and legal challenges of applying LLM in downstream task model training. GPTA utilizes a new synergistic training approach, optimizing the downstream models with parameter gradients and LLMs with the novel ``dialogue gradient''. The framework not only demonstrates significant improvements in model performance across six NLP benchmark datasets, but also reduces overfitting in low-resource scenarios effectively. The detailed analyses further validate that our pioneer framework provides a cost-efficient and adaptive method for downstream task model training with LLM support.",
    "link": "https://arxiv.org/abs/2404.00189",
    "context": "Title: GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream Neural Network Enhancement with LLMs\nAbstract: arXiv:2404.00189v1 Announce Type: new  Abstract: This study introduces GPTA, a Large Language Model assistance training framework, that enhances the training of downstream task models via prefix prompt. By minimizing data exposure to LLM, the framework addresses the security and legal challenges of applying LLM in downstream task model training. GPTA utilizes a new synergistic training approach, optimizing the downstream models with parameter gradients and LLMs with the novel ``dialogue gradient''. The framework not only demonstrates significant improvements in model performance across six NLP benchmark datasets, but also reduces overfitting in low-resource scenarios effectively. The detailed analyses further validate that our pioneer framework provides a cost-efficient and adaptive method for downstream task model training with LLM support.",
    "path": "papers/24/04/2404.00189.json",
    "total_tokens": 826,
    "translated_title": "GPTA：生成提示调整助手用于LLM的协同下游神经网络增强",
    "translated_abstract": "本研究介绍了GPTA，一种大型语言模型辅助训练框架，通过前缀提示增强了下游任务模型的训练。通过最小化LLM对数据的暴露，该框架解决了在下游任务模型训练中应用LLM所面临的安全和法律挑战。GPTA利用一种新的协同训练方法，通过参数梯度优化下游模型和LLM，新的“对话梯度”。该框架不仅在六个NLP基准数据集上展示出模型性能的显著改进，而且有效地减少了低资源场景下的过拟合。详细分析进一步验证了我们的首创性框架为具有LLM支持的下游任务模型训练提供了一种成本高效且适应性强的方法。",
    "tldr": "GPTA引入了一个大型语言模型辅助训练框架，通过前缀提示增强了下游任务模型的训练，不仅显著提高了模型性能，还有效减少了低资源场景下的过拟合。",
    "en_tdlr": "GPTA introduces a large language model assistance training framework that enhances the training of downstream task models via prefix prompts, significantly improving model performance and reducing overfitting in low-resource scenarios."
}