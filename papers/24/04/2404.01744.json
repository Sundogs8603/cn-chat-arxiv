{
    "title": "Octopus v2: On-device language model for super agent",
    "abstract": "arXiv:2404.01744v1 Announce Type: new  Abstract: Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisite",
    "link": "https://arxiv.org/abs/2404.01744",
    "context": "Title: Octopus v2: On-device language model for super agent\nAbstract: arXiv:2404.01744v1 Announce Type: new  Abstract: Language models have shown effectiveness in a variety of software applications, particularly in tasks related to automatic workflow. These models possess the crucial ability to call functions, which is essential in creating AI agents. Despite the high performance of large-scale language models in cloud environments, they are often associated with concerns over privacy and cost. Current on-device models for function calling face issues with latency and accuracy. Our research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95\\%. When compared to Llama-7B with a RAG-based function calling mechanism, our method enhances latency by 35-fold. This method reduces the latency to levels deemed suitable for deployment across a variety of edge devices in production environments, aligning with the performance requisite",
    "path": "papers/24/04/2404.01744.json",
    "total_tokens": 868,
    "translated_title": "Octopus v2：用于超级代理的设备上语言模型",
    "translated_abstract": "语言模型在各种软件应用中展现出了高效性，特别是与自动工作流相关的任务。这些模型具有调用函数的关键能力，在创建AI代理时至关重要。尽管大规模语言模型在云环境中表现出色，但往往存在着隐私和成本方面的担忧。当前用于函数调用的设备上模型面临延迟和准确性问题。我们的研究提出了一种新方法，使具有20亿参数的设备上模型在准确性和延迟方面超越了GPT-4，并将上下文长度缩减了95%。与基于RAG的函数调用机制的Llama-7B相比，我们的方法将延迟提高了35倍。这种方法将延迟降低到适合在生产环境中的各种边缘设备上部署的水平上，符合性能要求。",
    "tldr": "该研究提出了一种新方法，通过将具有20亿参数的设备上模型赋予超越GPT-4的准确性和延迟性能，并将上下文长度缩减95％，从而解决了函数调用中的延迟和准确性问题。",
    "en_tdlr": "This research presents a new method that empowers an on-device model with 2 billion parameters to surpass the performance of GPT-4 in both accuracy and latency, and decrease the context length by 95%, addressing issues with latency and accuracy in function calling."
}