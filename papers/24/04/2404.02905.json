{
    "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction",
    "abstract": "arXiv:2404.02905v1 Announce Type: cross  Abstract: We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling la",
    "link": "https://arxiv.org/abs/2404.02905",
    "context": "Title: Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction\nAbstract: arXiv:2404.02905v1 Announce Type: cross  Abstract: We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling la",
    "path": "papers/24/04/2404.02905.json",
    "total_tokens": 920,
    "translated_title": "可视自回归建模：通过下一尺度预测实现可伸缩图像生成",
    "translated_abstract": "我们提出了可视自回归建模（VAR），这是一种重新定义自回归学习在图像上的生成范式，将其作为从粗粒度到细粒度的“下一尺度预测”或“下一分辨率预测”，偏离了标准的光栅扫描“下一个令牌预测”。这种简单直观的方法允许自回归（AR）变压器快速学习视觉分布并具有良好泛化能力：VAR首次使AR模型在图像生成方面超越了扩散变压器。在ImageNet 256x256基准测试中，VAR通过将Frechet入侵距离（FID）从18.65提高到1.80，将inception分数（IS）从80.4提高到356.4，推断速度加快了大约20倍，显着改善了AR基线。经验证，VAR在图像质量、推断速度、数据效率和可伸缩性等多个维度上优于扩散变压器（DiT）。扩大VAR模型的规模显示出明显的幂律扩展能力。",
    "tldr": "VAR重新定义了图像上的自回归学习，通过粗到细的“下一尺度预测”实现快速学习视觉分布并超越了扩散变压器。",
    "en_tdlr": "VAR redefines autoregressive learning on images with coarse-to-fine \"next-scale prediction\", enabling fast learning of visual distributions and surpassing diffusion transformers."
}