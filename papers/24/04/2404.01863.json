{
    "title": "Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models",
    "abstract": "arXiv:2404.01863v1 Announce Type: cross  Abstract: Fine-tuning text-to-image models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent. However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of fine-tuned models, a phenomenon known as reward overoptimization. To investigate this issue in depth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which comprises a diverse collection of text prompts, images, and human annotations. Our evaluation of several state-of-the-art reward models on this benchmark reveals their frequent misalignment with human assessment. We empirically demonstrate that overoptimization occurs notably when a poorly aligned reward model is used as the fine-tuning objective. To address this, we propose TextNorm, a simple method that enhances alignment based on a measure of reward model confidence estimated across ",
    "link": "https://arxiv.org/abs/2404.01863",
    "context": "Title: Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models\nAbstract: arXiv:2404.01863v1 Announce Type: cross  Abstract: Fine-tuning text-to-image models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent. However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of fine-tuned models, a phenomenon known as reward overoptimization. To investigate this issue in depth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which comprises a diverse collection of text prompts, images, and human annotations. Our evaluation of several state-of-the-art reward models on this benchmark reveals their frequent misalignment with human assessment. We empirically demonstrate that overoptimization occurs notably when a poorly aligned reward model is used as the fine-tuning objective. To address this, we propose TextNorm, a simple method that enhances alignment based on a measure of reward model confidence estimated across ",
    "path": "papers/24/04/2404.01863.json",
    "total_tokens": 886,
    "translated_title": "面向细化文本到图像模型的置信度感知奖励优化",
    "translated_abstract": "在人类反馈数据训练的奖励函数上对细化文本到图像模型进行微调已被证明可以有效地使模型行为与人类意图一致。然而，过度优化使用这些奖励模型，作为简单的替代目标，可能会损害细化模型的性能，这一现象被称为奖励过度优化。为深入研究这个问题，我们引入了Text-Image Alignment Assessment (TIA2)基准测试，它包括一系列文本提示、图像和人类注释。我们在该基准测试上评估了几种最先进的奖励模型，发现它们经常与人类评估不一致。我们从实证角度证明，当使用一个与人类评估不一致的奖励模型作为微调目标时，奖励过度优化尤为明显。为应对这个问题，我们提出了TextNorm，一种简单的方法，它基于估计的奖励模型置信度来增强对齐性。",
    "tldr": "使用奖励函数训练的细化文本到图像模型可能会因为奖励过度优化而损害性能，为了解决这一问题，提出了一种基于奖励模型置信度的对齐性增强方法。"
}