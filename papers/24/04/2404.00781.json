{
    "title": "Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning",
    "abstract": "arXiv:2404.00781v1 Announce Type: cross  Abstract: Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over ta",
    "link": "https://arxiv.org/abs/2404.00781",
    "context": "Title: Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning\nAbstract: arXiv:2404.00781v1 Announce Type: cross  Abstract: Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over ta",
    "path": "papers/24/04/2404.00781.json",
    "total_tokens": 801,
    "translated_title": "处理连续学习中的可塑性丧失和灾难性遗忘",
    "translated_abstract": "深度表示学习方法在连续学习中存在困难，既遭受有用单元的灾难性遗忘，又因僵化和无用单元导致可塑性丢失。虽然许多方法分别解决这两个问题，但目前只有少数方法能同时处理这两个问题。本文引入了基于效用的扰动梯度下降（UPGD）作为一种用于表示持续学习的新方法。UPGD结合了梯度更新和扰动，它对更有用的单元应用较小的修改，保护它们免受遗忘，对不太有用的单元应用较大的修改，恢复它们的可塑性。",
    "tldr": "本文提出了一种新方法，即基于效用的扰动梯度下降（UPGD），通过在梯度更新中应用不同大小的扰动，保护有用单元以防遗忘，同时恢复不太有用单元的可塑性。",
    "en_tdlr": "This paper introduces a novel approach, Utility-based Perturbed Gradient Descent (UPGD), which protects useful units from forgetting by applying smaller modifications during gradient updates and rejuvenates less useful units by applying larger modifications."
}