{
    "title": "Aligning Large Language Models with Recommendation Knowledge",
    "abstract": "arXiv:2404.00245v1 Announce Type: new  Abstract: Large language models (LLMs) have recently been used as backbones for recommender systems. However, their performance often lags behind conventional methods in standard tasks like retrieval. We attribute this to a mismatch between LLMs' knowledge and the knowledge crucial for effective recommendations. While LLMs excel at natural language reasoning, they cannot model complex user-item interactions inherent in recommendation tasks. We propose bridging the knowledge gap and equipping LLMs with recommendation-specific knowledge to address this. Operations such as Masked Item Modeling (MIM) and Bayesian Personalized Ranking (BPR) have found success in conventional recommender systems. Inspired by this, we simulate these operations through natural language to generate auxiliary-task data samples that encode item correlations and user preferences. Fine-tuning LLMs on such auxiliary-task data samples and incorporating more informative recommend",
    "link": "https://arxiv.org/abs/2404.00245",
    "context": "Title: Aligning Large Language Models with Recommendation Knowledge\nAbstract: arXiv:2404.00245v1 Announce Type: new  Abstract: Large language models (LLMs) have recently been used as backbones for recommender systems. However, their performance often lags behind conventional methods in standard tasks like retrieval. We attribute this to a mismatch between LLMs' knowledge and the knowledge crucial for effective recommendations. While LLMs excel at natural language reasoning, they cannot model complex user-item interactions inherent in recommendation tasks. We propose bridging the knowledge gap and equipping LLMs with recommendation-specific knowledge to address this. Operations such as Masked Item Modeling (MIM) and Bayesian Personalized Ranking (BPR) have found success in conventional recommender systems. Inspired by this, we simulate these operations through natural language to generate auxiliary-task data samples that encode item correlations and user preferences. Fine-tuning LLMs on such auxiliary-task data samples and incorporating more informative recommend",
    "path": "papers/24/04/2404.00245.json",
    "total_tokens": 828,
    "translated_title": "将大型语言模型与推荐知识进行对齐",
    "translated_abstract": "最近，大型语言模型(LLMs)被用作推荐系统的主干。然而，它们在标准任务(如检索)中的性能往往落后于传统方法。我们认为这是由于LLMs知识与有效推荐所需知识之间的不匹配。LLMs擅长自然语言推理，但无法建模推荐任务中固有的复杂用户-物品交互。为了解决这个问题，我们提出了弥合知识差距，为LLMs提供推荐特定知识的方案。诸如遮盖物品建模(MIM)和贝叶斯个性化排名(BPR)之类的操作在传统推荐系统中取得了成功。受此启发，我们通过自然语言模拟这些操作，生成编码物品相关性和用户偏好的辅助任务数据样本。在这种辅助任务数据样本上对LLMs进行微调，并融入更多信息性推荐",
    "tldr": "将大型语言模型与推荐知识对齐，通过模拟传统推荐系统操作生成辅助任务数据样本来为大型语言模型提供推荐特定知识，以提高推荐性能。",
    "en_tdlr": "Aligning large language models with recommendation knowledge by generating auxiliary task data samples through simulating traditional recommender system operations to enhance recommendation performance."
}