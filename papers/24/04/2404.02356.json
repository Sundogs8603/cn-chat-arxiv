{
    "title": "Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors",
    "abstract": "arXiv:2404.02356v1 Announce Type: new  Abstract: Data poisoning backdoor attacks can cause undesirable behaviors in large language models (LLMs), and defending against them is of increasing importance. Existing defense mechanisms often assume that only one type of trigger is adopted by the attacker, while defending against multiple simultaneous and independent trigger types necessitates general defense frameworks and is relatively unexplored. In this paper, we propose Nested Product of Experts(NPoE) defense framework, which involves a mixture of experts (MoE) as a trigger-only ensemble within the PoE defense framework to simultaneously defend against multiple trigger types. During NPoE training, the main model is trained in an ensemble with a mixture of smaller expert models that learn the features of backdoor triggers. At inference time, only the main model is used. Experimental results on sentiment analysis, hate speech detection, and question classification tasks demonstrate that NP",
    "link": "https://arxiv.org/abs/2404.02356",
    "context": "Title: Two Heads are Better than One: Nested PoE for Robust Defense Against Multi-Backdoors\nAbstract: arXiv:2404.02356v1 Announce Type: new  Abstract: Data poisoning backdoor attacks can cause undesirable behaviors in large language models (LLMs), and defending against them is of increasing importance. Existing defense mechanisms often assume that only one type of trigger is adopted by the attacker, while defending against multiple simultaneous and independent trigger types necessitates general defense frameworks and is relatively unexplored. In this paper, we propose Nested Product of Experts(NPoE) defense framework, which involves a mixture of experts (MoE) as a trigger-only ensemble within the PoE defense framework to simultaneously defend against multiple trigger types. During NPoE training, the main model is trained in an ensemble with a mixture of smaller expert models that learn the features of backdoor triggers. At inference time, only the main model is used. Experimental results on sentiment analysis, hate speech detection, and question classification tasks demonstrate that NP",
    "path": "papers/24/04/2404.02356.json",
    "total_tokens": 857,
    "translated_title": "两个好头胜过一个: 嵌套PoE用于强力防御多后门攻击",
    "translated_abstract": "数据毒化后门攻击会导致大型语言模型（LLMs）产生不良行为，并且防御这些攻击变得越来越重要。现有的防御机制通常假定攻击者只采用一种触发器类型，而同时防御多种同时且独立的触发器类型则需要通用的防御框架，且相对较少探索。在本文中，我们提出了Nested Product of Experts(NPoE)防御框架，其中涉及将混合的专家模型（MoE）作为触发器集成到PoE防御框架中，以同时防御多种触发器类型。NPoE训练过程中，主模型与一组 smaller 专家模型集成训练，专家模型学习后门触发器的特征。在推断时，只使用主模型。在情感分析、仇恨言论检测和问题分类任务上的实验结果表明，NP",
    "tldr": "提出了一种嵌套专家集成防御框架(NPoE)，可同时防御多种后门触发器类型，实验结果表明其在多个任务上的有效性。",
    "en_tdlr": "Proposed a Nested Product of Experts (NPoE) defense framework that can defend against multiple types of backdoor triggers simultaneously, with experimental results demonstrating its effectiveness across multiple tasks."
}