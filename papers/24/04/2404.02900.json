{
    "title": "DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets",
    "abstract": "arXiv:2404.02900v1 Announce Type: cross  Abstract: Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks. In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks. However, unlike Convolutional Neural Networks (CNN), ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a large amount of data for pre-training. Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively. However, limited literature discusses the use of ViT for datasets with long-tailed imbalances. In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an efficient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes. Thi",
    "link": "https://arxiv.org/abs/2404.02900",
    "context": "Title: DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets\nAbstract: arXiv:2404.02900v1 Announce Type: cross  Abstract: Vision Transformer (ViT) has emerged as a prominent architecture for various computer vision tasks. In ViT, we divide the input image into patch tokens and process them through a stack of self attention blocks. However, unlike Convolutional Neural Networks (CNN), ViTs simple architecture has no informative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a large amount of data for pre-training. Various data efficient approaches (DeiT) have been proposed to train ViT on balanced datasets effectively. However, limited literature discusses the use of ViT for datasets with long-tailed imbalances. In this work, we introduce DeiT-LT to tackle the problem of training ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an efficient and effective way of distillation from CNN via distillation DIST token by using out-of-distribution images and re-weighting the distillation loss to enhance focus on tail classes. Thi",
    "path": "papers/24/04/2404.02900.json",
    "total_tokens": 886,
    "translated_title": "DeiT-LT蒸馏重返，用于长尾数据集上的Vision Transformer训练",
    "translated_abstract": "Vision Transformer（ViT）已经成为各种计算机视觉任务中突出的架构。在 ViT 中，我们将输入图像分成补丁令牌，并通过一堆自我注意块进行处理。然而，与卷积神经网络（CNN）不同，ViT 的简单架构没有信息性归纳偏差（例如局部性等）。由于这个原因，ViT 需要大量数据进行预训练。已经提出了各种数据有效的方法（DeiT）来有效地训练平衡的数据集上的ViT。然而，文献中很少讨论使用ViT来处理长尾不平衡数据集。在这项工作中，我们引入DeiT-LT来解决从头开始训练长尾数据集上的ViT的问题。在 DeiT-LT 中，我们通过使用超出分布图像和重新加权蒸馏损失，引入了一种有效的蒸馏方式，通过蒸馏 DIST 令牌从CNN进行蒸馏，以增强对尾部类别的关注。",
    "tldr": "DeiT-LT通过引入一种有效的蒸馏方式，将CNN蒸馏到ViT中，以应对长尾数据集上训练ViT时的困难。",
    "en_tdlr": "DeiT-LT introduces an effective distillation method by distilling CNN into ViT to address the challenges of training ViT on long-tailed datasets."
}