{
    "title": "Make Continual Learning Stronger via C-Flat",
    "abstract": "arXiv:2404.00986v1 Announce Type: new  Abstract: Model generalization ability upon incrementally acquiring dynamically updating knowledge from sequentially arriving tasks is crucial to tackle the sensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape sharpness minimization seeking for flat minima lying in neighborhoods with uniform low loss or smooth gradient is proven to be a strong training regime improving model generalization compared with loss minimization based optimizer like SGD. Yet only a few works have discussed this training regime for CL, proving that dedicated designed zeroth-order sharpness optimizer can improve CL performance. In this work, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer an",
    "link": "https://arxiv.org/abs/2404.00986",
    "context": "Title: Make Continual Learning Stronger via C-Flat\nAbstract: arXiv:2404.00986v1 Announce Type: new  Abstract: Model generalization ability upon incrementally acquiring dynamically updating knowledge from sequentially arriving tasks is crucial to tackle the sensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape sharpness minimization seeking for flat minima lying in neighborhoods with uniform low loss or smooth gradient is proven to be a strong training regime improving model generalization compared with loss minimization based optimizer like SGD. Yet only a few works have discussed this training regime for CL, proving that dedicated designed zeroth-order sharpness optimizer can improve CL performance. In this work, we propose a Continual Flatness (C-Flat) method featuring a flatter loss landscape tailored for CL. C-Flat could be easily called with only one line of code and is plug-and-play to any CL methods. A general framework of C-Flat applied to all CL categories and a thorough comparison with loss minima optimizer an",
    "path": "papers/24/04/2404.00986.json",
    "total_tokens": 903,
    "translated_title": "通过C-Flat使持续学习更强大",
    "translated_abstract": "持续学习中模型的泛化能力对于处理连续到达任务的动态更新知识是至关重要的，为了解决持续学习中的敏感性-稳定性困境。研究证明，通过最小化权重损失景观的陡峭度，寻找位于具有统一低损失或平稳梯度的邻域中的平坦最小值，是一种强大的训练方式，相较于基于损失最小化的优化器如SGD来提高模型的泛化性。然而，只有少数作品讨论了这种训练方式在持续学习中的应用，证明特定设计的零阶陡峭度优化器可以提升持续学习性能。在这项工作中，我们提出了一种名为Continual Flatness（C-Flat）的方法，具有为持续学习定制的更平坦的损失景观。C-Flat只需一行代码即可轻松调用，并可与任何持续学习方法插播。C-Flat应用于所有持续学习类别的一般框架，并与损失最小化优化器进行了彻底比较。",
    "tldr": "通过C-Flat方法，我们提出了一种更平坦的损失景观，可用于持续学习，简化了模型训练过程并提高了模型泛化能力。",
    "en_tdlr": "We introduce a flatter loss landscape method, C-Flat, for continual learning to simplify model training process and enhance model generalization."
}