{
    "title": "Revisiting Random Weight Perturbation for Efficiently Improving Generalization",
    "abstract": "arXiv:2404.00357v1 Announce Type: new  Abstract: Improving the generalization ability of modern deep neural networks (DNNs) is a fundamental challenge in machine learning. Two branches of methods have been proposed to seek flat minima and improve generalization: one led by sharpness-aware minimization (SAM) minimizes the worst-case neighborhood loss through adversarial weight perturbation (AWP), and the other minimizes the expected Bayes objective with random weight perturbation (RWP). While RWP offers advantages in computation and is closely linked to AWP on a mathematical basis, its empirical performance has consistently lagged behind that of AWP. In this paper, we revisit the use of RWP for improving generalization and propose improvements from two perspectives: i) the trade-off between generalization and convergence and ii) the random perturbation generation. Through extensive experimental evaluations, we demonstrate that our enhanced RWP methods achieve greater efficiency in enhan",
    "link": "https://arxiv.org/abs/2404.00357",
    "context": "Title: Revisiting Random Weight Perturbation for Efficiently Improving Generalization\nAbstract: arXiv:2404.00357v1 Announce Type: new  Abstract: Improving the generalization ability of modern deep neural networks (DNNs) is a fundamental challenge in machine learning. Two branches of methods have been proposed to seek flat minima and improve generalization: one led by sharpness-aware minimization (SAM) minimizes the worst-case neighborhood loss through adversarial weight perturbation (AWP), and the other minimizes the expected Bayes objective with random weight perturbation (RWP). While RWP offers advantages in computation and is closely linked to AWP on a mathematical basis, its empirical performance has consistently lagged behind that of AWP. In this paper, we revisit the use of RWP for improving generalization and propose improvements from two perspectives: i) the trade-off between generalization and convergence and ii) the random perturbation generation. Through extensive experimental evaluations, we demonstrate that our enhanced RWP methods achieve greater efficiency in enhan",
    "path": "papers/24/04/2404.00357.json",
    "total_tokens": 842,
    "translated_title": "重新审视随机权重扰动以有效改善泛化能力",
    "translated_abstract": "现代深度神经网络（DNNs）的泛化能力是机器学习中的一个基本挑战。两种方法的分支已被提出以寻求平坦最小值并改善泛化：一种是由锐度感知最小化（SAM）领导的方法，通过对抗权重扰动（AWP）最小化最坏情况邻域损失；另一种是通过随机权重扰动（RWP）最小化期望贝叶斯目标。尽管RWP在计算方面具有优势，并且在数学基础上与AWP有密切联系，但其实证性能一直落后于AWP。在本文中，我们重新审视了使用RWP改善泛化的方法，并从两个角度提出了改进: i）泛化和收敛之间的权衡以及ii）随机扰动生成。通过大量实验评估，我们证明了我们增强的RWP方法在提高效率方面表现更优秀。",
    "tldr": "重新审视随机权重扰动方法用于改善深度神经网络的泛化能力，通过两方面的改进实现更高效的效果。",
    "en_tdlr": "Revisiting the use of random weight perturbation methods for improving the generalization ability of deep neural networks, achieving greater efficiency through improvements in two aspects."
}