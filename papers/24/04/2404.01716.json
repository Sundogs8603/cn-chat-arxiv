{
    "title": "Effective internal language model training and fusion for factorized transducer model",
    "abstract": "arXiv:2404.01716v1 Announce Type: cross  Abstract: The internal language model (ILM) of the neural transducer has been widely studied. In most prior work, it is mainly used for estimating the ILM score and is subsequently subtracted during inference to facilitate improved integration with external language models. Recently, various of factorized transducer models have been proposed, which explicitly embrace a standalone internal language model for non-blank token prediction. However, even with the adoption of factorized transducer models, limited improvement has been observed compared to shallow fusion. In this paper, we propose a novel ILM training and decoding strategy for factorized transducer models, which effectively combines the blank, acoustic and ILM scores. Our experiments show a 17% relative improvement over the standard decoding method when utilizing a well-trained ILM and the proposed decoding strategy on LibriSpeech datasets. Furthermore, when compared to a strong RNN-T ba",
    "link": "https://arxiv.org/abs/2404.01716",
    "context": "Title: Effective internal language model training and fusion for factorized transducer model\nAbstract: arXiv:2404.01716v1 Announce Type: cross  Abstract: The internal language model (ILM) of the neural transducer has been widely studied. In most prior work, it is mainly used for estimating the ILM score and is subsequently subtracted during inference to facilitate improved integration with external language models. Recently, various of factorized transducer models have been proposed, which explicitly embrace a standalone internal language model for non-blank token prediction. However, even with the adoption of factorized transducer models, limited improvement has been observed compared to shallow fusion. In this paper, we propose a novel ILM training and decoding strategy for factorized transducer models, which effectively combines the blank, acoustic and ILM scores. Our experiments show a 17% relative improvement over the standard decoding method when utilizing a well-trained ILM and the proposed decoding strategy on LibriSpeech datasets. Furthermore, when compared to a strong RNN-T ba",
    "path": "papers/24/04/2404.01716.json",
    "total_tokens": 876,
    "translated_title": "有效的内部语言模型训练和融合对于分解转录模型",
    "translated_abstract": "神经转录器的内部语言模型（ILM）已被广泛研究。在大多数先前的工作中，它主要用于估计ILM分数，并在推理过程中随后被减去，以促进与外部语言模型更好的集成。最近，已提出了各种分解转录模型，明确采用独立的内部语言模型用于非空白令牌预测。然而，即使采用了分解转录模型，与浅层融合相比，改进仍然有限。在本文中，我们提出了一种新颖的ILM训练和解码策略，用于分解转录模型，有效地结合了空白、声学和ILM分数。我们的实验表明，在LibriSpeech数据集上利用经过良好训练的ILM和所提出的解码策略时，相对于标准解码方法，有17%的相对改善。此外，与强RNN-T ba相比",
    "tldr": "提出了一种对于分解转录模型的有效ILM训练和解码策略，能够显著改善与标准解码方法的性能，并在LibriSpeech数据集上取得了17%的相对改善。",
    "en_tdlr": "Proposed an effective ILM training and decoding strategy for factorized transducer models, resulting in a significant improvement over the standard decoding method and achieving a 17% relative improvement on the LibriSpeech dataset."
}