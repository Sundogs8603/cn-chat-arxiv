{
    "title": "Hallucination Diversity-Aware Active Learning for Text Summarization",
    "abstract": "arXiv:2404.01588v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning",
    "link": "https://arxiv.org/abs/2404.01588",
    "context": "Title: Hallucination Diversity-Aware Active Learning for Text Summarization\nAbstract: arXiv:2404.01588v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning",
    "path": "papers/24/04/2404.01588.json",
    "total_tokens": 866,
    "translated_title": "基于幻觉多样性的文本摘要主动学习",
    "translated_abstract": "大型语言模型（LLMs）已经表现出生成幻觉输出的倾向，即在事实上不正确或不支持的文本。现有的减轻幻觉的方法通常需要昂贵的人类注释来识别和纠正LLMs输出中的幻觉。此外，大多数这些方法专注于特定类型的幻觉，例如实体或标记错误，这限制了它们在解决LLMs输出中展示的各种类型的幻觉方面的有效性。据我们所知，本文提出了第一个旨在减轻LLMs幻觉的主动学习框架，降低了对幻觉所需的昂贵人类注释。通过在文本摘要中衡量语义框架、议论和内容可验证性错误中的细粒度幻觉，我们提出了 HAllucination Diversity-Aware Sampling（HADAS）来选择多样化的幻觉，以供LLM微调的主动学习注释。",
    "tldr": "本文首次提出了一种基于幻觉多样性的主动学习框架，用于减轻大型语言模型（LLMs）在文本摘要中产生的幻觉，减少了昂贵的人类注释需求。",
    "en_tdlr": "The paper introduces a novel active learning framework based on hallucination diversity to mitigate hallucinations generated by Large Language Models (LLMs) in text summarization, reducing the need for costly human annotations."
}