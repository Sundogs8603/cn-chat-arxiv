{
    "title": "Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via Winning Tickets",
    "abstract": "arXiv:2404.01242v1 Announce Type: new  Abstract: Current soft prompt methods yield limited performance when applied to small-sized models (fewer than a billion parameters). Deep prompt-tuning, which entails prepending parameters in each layer for enhanced efficacy, presents a solution for prompting small-sized models, albeit requiring carefully designed implementation. In this paper, we introduce the Lottery Ticket Prompt-learning (LTP) framework that integrates winning tickets with soft prompts. The LTP offers a simpler implementation and requires only a one-time execution. We demonstrate LTP on cross-lingual tasks, where prior works rely on external tools like human-designed multilingual templates and bilingual dictionaries, which may not be feasible in a low-resource regime. Specifically, we select a subset of parameters that have been changed the most during the fine-tuning with the Masked Language Modeling objective. Then, we prepend soft prompts to the original pre-trained langua",
    "link": "https://arxiv.org/abs/2404.01242",
    "context": "Title: Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via Winning Tickets\nAbstract: arXiv:2404.01242v1 Announce Type: new  Abstract: Current soft prompt methods yield limited performance when applied to small-sized models (fewer than a billion parameters). Deep prompt-tuning, which entails prepending parameters in each layer for enhanced efficacy, presents a solution for prompting small-sized models, albeit requiring carefully designed implementation. In this paper, we introduce the Lottery Ticket Prompt-learning (LTP) framework that integrates winning tickets with soft prompts. The LTP offers a simpler implementation and requires only a one-time execution. We demonstrate LTP on cross-lingual tasks, where prior works rely on external tools like human-designed multilingual templates and bilingual dictionaries, which may not be feasible in a low-resource regime. Specifically, we select a subset of parameters that have been changed the most during the fine-tuning with the Masked Language Modeling objective. Then, we prepend soft prompts to the original pre-trained langua",
    "path": "papers/24/04/2404.01242.json",
    "total_tokens": 832,
    "translated_title": "通过胜出的票方法有效地引导小型语言模型进行跨语言任务",
    "translated_abstract": "当前软提示方法在应用于小型模型（少于十亿个参数）时性能有限。深度提示调整涉及在每层中添加参数以增强效力，为小型模型提供了一种提示的解决方案，尽管需要精心设计的实施。在本文中，我们介绍了将中奖票与软提示相结合的Lottery Ticket Prompt-learning（LTP）框架。LTP提供了更简单的实施，并且只需要执行一次。我们在跨语言任务上展示了LTP，先前的工作依赖于外部工具，如人工设计的多语言模板和双语词典，这在资源有限的情况下可能不可行。具体来说，我们选择了在使用掩码语言建模目标进行微调时变化最大的一部分参数，然后在原始预训练语言模型之前添加软提示。",
    "tldr": "该论文引入了Lottery Ticket Prompt-learning（LTP）框架，通过将中奖票与软提示相结合，为小型语言模型提供了一种更简单、只需一次执行的提示方法，从而有效地进行跨语言任务。",
    "en_tdlr": "This paper introduces the Lottery Ticket Prompt-learning (LTP) framework, which integrates winning tickets with soft prompts to provide a simpler and one-time execution prompt method for small language models to effectively perform cross-lingual tasks."
}