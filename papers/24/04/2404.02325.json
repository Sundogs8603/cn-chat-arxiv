{
    "title": "Heat Death of Generative Models in Closed-Loop Learning",
    "abstract": "arXiv:2404.02325v1 Announce Type: new  Abstract: Improvement and adoption of generative machine learning models is rapidly accelerating, as exemplified by the popularity of LLMs (Large Language Models) for text, and diffusion models for image generation.As generative models become widespread, data they generate is incorporated into shared content through the public web. This opens the question of what happens when data generated by a model is fed back to the model in subsequent training campaigns. This is a question about the stability of the training process, whether the distribution of publicly accessible content, which we refer to as \"knowledge\", remains stable or collapses.   Small scale empirical experiments reported in the literature show that this closed-loop training process is prone to degenerating. Models may start producing gibberish data, or sample from only a small subset of the desired data distribution (a phenomenon referred to as mode collapse). So far there has been on",
    "link": "https://arxiv.org/abs/2404.02325",
    "context": "Title: Heat Death of Generative Models in Closed-Loop Learning\nAbstract: arXiv:2404.02325v1 Announce Type: new  Abstract: Improvement and adoption of generative machine learning models is rapidly accelerating, as exemplified by the popularity of LLMs (Large Language Models) for text, and diffusion models for image generation.As generative models become widespread, data they generate is incorporated into shared content through the public web. This opens the question of what happens when data generated by a model is fed back to the model in subsequent training campaigns. This is a question about the stability of the training process, whether the distribution of publicly accessible content, which we refer to as \"knowledge\", remains stable or collapses.   Small scale empirical experiments reported in the literature show that this closed-loop training process is prone to degenerating. Models may start producing gibberish data, or sample from only a small subset of the desired data distribution (a phenomenon referred to as mode collapse). So far there has been on",
    "path": "papers/24/04/2404.02325.json",
    "total_tokens": 795,
    "translated_title": "闭环学习中生成模型的热力学死亡",
    "translated_abstract": "生成式机器学习模型的改进和采纳正在迅速加速，例如文本中LLM（大型语言模型）的流行以及图像生成中的扩散模型。随着生成模型的普及，它们生成的数据被整合到公共网络中的共享内容中。这引发了一个问题：当模型生成的数据被送回到模型进行后续训练时会发生什么。这是一个关于训练过程稳定性的问题，即公共可访问内容的分布（我们称之为“知识”）是否保持稳定还是崩溃。文献中报道的小规模实证实验显示，这种闭环训练过程容易退化。模型可能开始生成无意义的数据，或者仅从所需数据分布的一小部分中采样（称为模式崩溃现象）。",
    "tldr": "生成模型的闭环训练过程容易产生退化现象，模型可能开始生成无意义的数据或仅从所需数据分布的一小部分中采样。"
}