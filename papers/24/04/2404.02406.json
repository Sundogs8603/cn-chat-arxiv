{
    "title": "Exploring Backdoor Vulnerabilities of Chat Models",
    "abstract": "arXiv:2404.02406v1 Announce Type: cross  Abstract: Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on LLMs predominantly focus on instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs",
    "link": "https://arxiv.org/abs/2404.02406",
    "context": "Title: Exploring Backdoor Vulnerabilities of Chat Models\nAbstract: arXiv:2404.02406v1 Announce Type: cross  Abstract: Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on LLMs predominantly focus on instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs",
    "path": "papers/24/04/2404.02406.json",
    "total_tokens": 796,
    "translated_title": "探讨聊天模型的后门漏洞",
    "translated_abstract": "最近的研究表明，大型语言模型（LLMs）容易受到一种称为后门攻击的安全威胁。当前对LLMs的后门研究主要集中在针对指令调整的LLMs，而忽略了另一种现实场景，即将LLMs在多轮对话数据上微调为聊天模型。聊天模型被广泛应用于各种实际场景，因此聊天模型的安全性值得越来越多的关注。不幸的是，我们指出，灵活的多轮交互格式增加了触发设计的灵活性，并增加了聊天模型对后门攻击的脆弱性。在这项工作中，我们揭示并实现了一种新颖的聊天模型后门攻击方法，通过将多个触发场景分布在用户输入中",
    "tldr": "聊天模型因为多轮交互格式的灵活性增加了对后门攻击的脆弱性，该论文揭示并实现了一种新颖的后门攻击方法",
    "en_tdlr": "The flexibility of multi-turn interaction in chat models increases vulnerability to backdoor attacks, and this paper reveals and implements a novel backdoor attacking method."
}