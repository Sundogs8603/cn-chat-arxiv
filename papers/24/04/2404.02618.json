{
    "title": "Diffexplainer: Towards Cross-modal Global Explanations with Diffusion Models",
    "abstract": "arXiv:2404.02618v1 Announce Type: cross  Abstract: We present DiffExplainer, a novel framework that, leveraging language-vision models, enables multimodal global explainability. DiffExplainer employs diffusion models conditioned on optimized text prompts, synthesizing images that maximize class outputs and hidden features of a classifier, thus providing a visual tool for explaining decisions. Moreover, the analysis of generated visual descriptions allows for automatic identification of biases and spurious features, as opposed to traditional methods that often rely on manual intervention. The cross-modal transferability of language-vision models also enables the possibility to describe decisions in a more human-interpretable way, i.e., through text. We conduct comprehensive experiments, which include an extensive user study, demonstrating the effectiveness of DiffExplainer on 1) the generation of high-quality images explaining model decisions, surpassing existing activation maximization",
    "link": "https://arxiv.org/abs/2404.02618",
    "context": "Title: Diffexplainer: Towards Cross-modal Global Explanations with Diffusion Models\nAbstract: arXiv:2404.02618v1 Announce Type: cross  Abstract: We present DiffExplainer, a novel framework that, leveraging language-vision models, enables multimodal global explainability. DiffExplainer employs diffusion models conditioned on optimized text prompts, synthesizing images that maximize class outputs and hidden features of a classifier, thus providing a visual tool for explaining decisions. Moreover, the analysis of generated visual descriptions allows for automatic identification of biases and spurious features, as opposed to traditional methods that often rely on manual intervention. The cross-modal transferability of language-vision models also enables the possibility to describe decisions in a more human-interpretable way, i.e., through text. We conduct comprehensive experiments, which include an extensive user study, demonstrating the effectiveness of DiffExplainer on 1) the generation of high-quality images explaining model decisions, surpassing existing activation maximization",
    "path": "papers/24/04/2404.02618.json",
    "total_tokens": 880,
    "translated_title": "Diffexplainer：基于扩散模型的跨模态全局解释研究",
    "translated_abstract": "我们提出了DiffExplainer，这是一个新颖的框架，利用语言-视觉模型实现多模态全局可解释性。DiffExplainer利用经过优化的文本提示条件化的扩散模型，合成最大化分类器输出和隐藏特征的图像，从而提供一个解释决策的可视化工具。此外，对生成的视觉描述的分析允许自动识别偏见和虚假特征，而不像传统方法那样常常依赖于手动干预。语言-视觉模型的跨模态可迁移性还使得可以通过文本以更具人类可解释性的方式描述决策。我们进行了综合实验，包括广泛的用户研究，展示了DiffExplainer在生成解释模型决策的高质量图像方面的有效性，超过了现有的激活最大化方法。",
    "tldr": "Diffexplainer提出了一种新颖框架，利用语言-视觉模型实现多模态全局可解释性，并通过优化的文本提示条件化的扩散模型，合成图像来解释分类器的决策，同时提供一个对决策解释的视觉工具，并能自动识别偏见和虚假特征。",
    "en_tdlr": "Diffexplainer proposes a novel framework for achieving cross-modal global explainability using language-vision models, synthesizing images to explain classifier decisions based on optimized text prompts from diffusion models, and automatically identifying biases and spurious features, surpassing existing activation maximization methods."
}