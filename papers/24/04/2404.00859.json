{
    "title": "Do language models plan ahead for future tokens?",
    "abstract": "arXiv:2404.00859v1 Announce Type: cross  Abstract: Do transformers \"think ahead\" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.",
    "link": "https://arxiv.org/abs/2404.00859",
    "context": "Title: Do language models plan ahead for future tokens?\nAbstract: arXiv:2404.00859v1 Announce Type: cross  Abstract: Do transformers \"think ahead\" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.",
    "path": "papers/24/04/2404.00859.json",
    "total_tokens": 824,
    "translated_title": "语言模型是否提前为未来标记进行规划？",
    "translated_abstract": "arXiv:2404.00859v1 公告类型：跨领域 摘要：在给定位置的推理过程中，变压器是否会“提前思考”？已知变压器在$t$的前向传递的隐藏状态中准备信息，然后在未来的前向传递$t+\\tau$中使用。我们提出了两种解释这种现象的可能性：预缓存，即训练中存在的非对角梯度项导致模型在$t$计算与当前推理任务无关但对未来有用的特征，以及面包屑，即与时间步长$t$最相关的特征已经与那些将最有利于时间步长$t+\\tau$的特征相同。我们通过训练不将梯度传播到过去时间步的语言模型来测试这些假设，这种方案我们正式称为短视训练。在合成数据设置中，我们发现了预缓存的明确证据。在自回归语言建模设置中，我们的实验更多地支持了面包屑假设。",
    "tldr": "语言模型在推理过程中会提前准备未来标记所需的信息，可能是通过预缓存或面包屑的方式实现。",
    "en_tdlr": "Language models prepare information for future tokens during inference, potentially through pre-caching or breadcrumbs mechanisms."
}