{
    "title": "Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration",
    "abstract": "arXiv:2404.00651v1 Announce Type: cross  Abstract: Recent advancements in deep reinforcement learning (RL) have demonstrated notable progress in sample efficiency, spanning both model-based and model-free paradigms. Despite the identification and mitigation of specific bottlenecks in prior works, the agent's exploration ability remains under-emphasized in the realm of sample-efficient RL. This paper investigates how to achieve sample-efficient exploration in continuous control tasks. We introduce an RL algorithm that incorporates a predictive model and off-policy learning elements, where an online planner enhanced by a novelty-aware terminal value function is employed for sample collection. Leveraging the forward predictive error within a latent state space, we derive an intrinsic reward without incurring parameters overhead. This reward establishes a solid connection to model uncertainty, allowing the agent to effectively overcome the asymptotic performance gap. Through extensive expe",
    "link": "https://arxiv.org/abs/2404.00651",
    "context": "Title: Learning Off-policy with Model-based Intrinsic Motivation For Active Online Exploration\nAbstract: arXiv:2404.00651v1 Announce Type: cross  Abstract: Recent advancements in deep reinforcement learning (RL) have demonstrated notable progress in sample efficiency, spanning both model-based and model-free paradigms. Despite the identification and mitigation of specific bottlenecks in prior works, the agent's exploration ability remains under-emphasized in the realm of sample-efficient RL. This paper investigates how to achieve sample-efficient exploration in continuous control tasks. We introduce an RL algorithm that incorporates a predictive model and off-policy learning elements, where an online planner enhanced by a novelty-aware terminal value function is employed for sample collection. Leveraging the forward predictive error within a latent state space, we derive an intrinsic reward without incurring parameters overhead. This reward establishes a solid connection to model uncertainty, allowing the agent to effectively overcome the asymptotic performance gap. Through extensive expe",
    "path": "papers/24/04/2404.00651.json",
    "total_tokens": 788,
    "translated_title": "通过基于模型的内在动机学习离线策略以实现主动在线探索",
    "translated_abstract": "在深度强化学习领域，最近的进展在样本效率方面取得了显着进展，涵盖了基于模型的方法和无模型的方法。本文研究了如何在连续控制任务中实现样本有效的探索。我们引入了一种强化学习算法，结合了预测模型和离线学习元素，其中在线规划器通过一种新颖感知的终端价值函数用于样本收集。利用潜在状态空间中的前向预测错误，我们推导出了一种内在奖励，而不会产生参数开销。该奖励建立了与模型不确定性的牢固联系，使代理能够有效地克服渐近性能差距。",
    "tldr": "本文提出了一种结合预测模型和离线学习元素的强化学习算法，通过内在奖励与模型不确定性的关联，在连续控制任务中实现了样本有效的探索。",
    "en_tdlr": "This paper presents a reinforcement learning algorithm that combines a predictive model and off-policy learning elements to achieve sample-efficient exploration in continuous control tasks by establishing a connection between intrinsic reward and model uncertainty."
}