{
    "title": "Stochastic Constrained Decentralized Optimization for Machine Learning with Fewer Data Oracles: a Gradient Sliding Approach",
    "abstract": "arXiv:2404.02511v1 Announce Type: cross  Abstract: In modern decentralized applications, ensuring communication efficiency and privacy for the users are the key challenges. In order to train machine-learning models, the algorithm has to communicate to the data center and sample data for its gradient computation, thus exposing the data and increasing the communication cost. This gives rise to the need for a decentralized optimization algorithm that is communication-efficient and minimizes the number of gradient computations. To this end, we propose the primal-dual sliding with conditional gradient sliding framework, which is communication-efficient and achieves an $\\varepsilon$-approximate solution with the optimal gradient complexity of $O(1/\\sqrt{\\varepsilon}+\\sigma^2/{\\varepsilon^2})$ and $O(\\log(1/\\varepsilon)+\\sigma^2/\\varepsilon)$ for the convex and strongly convex setting respectively and an LO (Linear Optimization) complexity of $O(1/\\varepsilon^2)$ for both settings given a sto",
    "link": "https://arxiv.org/abs/2404.02511",
    "context": "Title: Stochastic Constrained Decentralized Optimization for Machine Learning with Fewer Data Oracles: a Gradient Sliding Approach\nAbstract: arXiv:2404.02511v1 Announce Type: cross  Abstract: In modern decentralized applications, ensuring communication efficiency and privacy for the users are the key challenges. In order to train machine-learning models, the algorithm has to communicate to the data center and sample data for its gradient computation, thus exposing the data and increasing the communication cost. This gives rise to the need for a decentralized optimization algorithm that is communication-efficient and minimizes the number of gradient computations. To this end, we propose the primal-dual sliding with conditional gradient sliding framework, which is communication-efficient and achieves an $\\varepsilon$-approximate solution with the optimal gradient complexity of $O(1/\\sqrt{\\varepsilon}+\\sigma^2/{\\varepsilon^2})$ and $O(\\log(1/\\varepsilon)+\\sigma^2/\\varepsilon)$ for the convex and strongly convex setting respectively and an LO (Linear Optimization) complexity of $O(1/\\varepsilon^2)$ for both settings given a sto",
    "path": "papers/24/04/2404.02511.json",
    "total_tokens": 897,
    "translated_title": "具有较少数据神谕的机器学习随机约束分散优化：一种梯度滑动方法",
    "translated_abstract": "在现代分散式应用程序中，确保用户的通信效率和隐私是关键挑战。为了训练机器学习模型，算法必须与数据中心通信，并采样数据进行梯度计算，从而暴露数据并增加通信成本。这导致了需要一种通信高效且最小化梯度计算次数的分散式优化算法。为此，我们提出了原始-对偶滑动与条件梯度滑动框架，该框架通信高效，实现了具有梯度复杂度的 $\\varepsilon$-近似解$O(1/\\sqrt{\\varepsilon}+\\sigma^2/{\\varepsilon^2})$ 和 $O(\\log(1/\\varepsilon)+\\sigma^2/\\varepsilon)$分别适用于凸和强凸设置，以及给定 随机 的LO (线性优化)复杂度，都是$O(1/\\varepsilon^2)$。",
    "tldr": "提出了一种原始-对偶滑动与条件梯度滑动框架，实现了通信高效和最小化梯度计算次数的分散式优化算法，具有优化的梯度和线性优化复杂度。",
    "en_tdlr": "Proposed a primal-dual sliding with conditional gradient sliding framework, achieving communication efficiency and minimizing the number of gradient computations in decentralized optimization, with optimized gradient and linear optimization complexities."
}