{
    "title": "A General and Efficient Training for Transformer via Token Expansion",
    "abstract": "arXiv:2404.00672v1 Announce Type: cross  Abstract: The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an \"initialization-expansion-merging\" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e",
    "link": "https://arxiv.org/abs/2404.00672",
    "context": "Title: A General and Efficient Training for Transformer via Token Expansion\nAbstract: arXiv:2404.00672v1 Announce Type: cross  Abstract: The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we propose a novel token growth scheme Token Expansion (termed ToE) to achieve consistent training acceleration for ViTs. We introduce an \"initialization-expansion-merging\" pipeline to maintain the integrity of the intermediate feature distribution of original transformers, preventing the loss of crucial learnable information in the training process. ToE can not only be seamlessly integrated into the training and fine-tuning process of transformers (e",
    "path": "papers/24/04/2404.00672.json",
    "total_tokens": 652,
    "translated_title": "通过令牌扩展实现Transformer的一般高效训练",
    "translated_abstract": "Vision Transformer（ViT）通常需要极大的训练成本才能取得显著性能。本文提出一种新的令牌生长方案Token Expansion (ToE)，以实现ViT的一致性训练加速。我们引入了一个“初始化-扩展-合并”管道，以保持原始Transformer的中间特征分布的完整性，防止在训练过程中丢失关键的可学习信息。",
    "tldr": "本文提出了一种名为ToE的令牌生长方案，旨在通过加速一致性训练来改善ViT的训练效果。",
    "en_tdlr": "This paper proposes a token growth scheme called ToE to improve the training efficiency of ViTs through consistent training acceleration."
}