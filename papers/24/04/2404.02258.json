{
    "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models",
    "abstract": "arXiv:2404.02258v1 Announce Type: cross  Abstract: Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but d",
    "link": "https://arxiv.org/abs/2404.02258",
    "context": "Title: Mixture-of-Depths: Dynamically allocating compute in transformer-based language models\nAbstract: arXiv:2404.02258v1 Announce Type: cross  Abstract: Transformer-based language models spread FLOPs uniformly across input sequences. In this work we demonstrate that transformers can instead learn to dynamically allocate FLOPs (or compute) to specific positions in a sequence, optimising the allocation along the sequence for different layers across the model depth. Our method enforces a total compute budget by capping the number of tokens ($k$) that can participate in the self-attention and MLP computations at a given layer. The tokens to be processed are determined by the network using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple procedure uses a static computation graph with known tensor sizes, unlike other conditional computation techniques. Nevertheless, since the identities of the $k$ tokens are fluid, this method can expend FLOPs non-uniformly across the time and model depth dimensions. Thus, compute expenditure is entirely predictable in sum total, but d",
    "path": "papers/24/04/2404.02258.json",
    "total_tokens": 830,
    "translated_title": "Mixture-of-Depths: 在基于Transformer的语言模型中动态分配计算",
    "translated_abstract": "基于Transformer的语言模型通常会将FLOPs均匀分布在输入序列中。本文展示了transformers可以学习动态地将FLOPs（或计算）分配给序列中的特定位置，优化各模型层深度上的序列分配。我们的方法通过设定在给定层中可参与自注意力和MLP计算的令牌数（$k$）来实施总计算预算。要处理的令牌由网络使用top-$k$路由机制确定。由于$k$是事先定义的，这种简单的过程使用具有已知张量大小的静态计算图，不同于其他条件计算技术。然而，由于$k$令牌的标识是不固定的，该方法可以非均匀地跨时间和模型深度维度分配FLOPs。因此，总体而言，计算支出完全可预测，但d",
    "tldr": "本研究提出了一种新的方法，即Mixture-of-Depths，可以在Transformer的语言模型中动态分配FLOPs以优化模型深度上不同层的序列分配。",
    "en_tdlr": "This work presents a novel method, Mixture-of-Depths, for dynamically allocating FLOPs in transformer-based language models to optimize sequence allocation across different layers of model depth."
}