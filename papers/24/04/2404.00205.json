{
    "title": "Conceptual and Unbiased Reasoning in Language Models",
    "abstract": "arXiv:2404.00205v1 Announce Type: new  Abstract: Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we bridge this gap and propose a novel conceptualization framework that forces models to perform conceptual reasoning on abstract questions and generate solutions in a verifiable symbolic space. Using this framework as an analytical tool, we show that existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods. We then discuss how models can improve since high-level abstract reasoning is key to unbiased and generalizable decision-making. We propose two techniques to add trustworthy induction signals by generating familiar questions with similar underlying reasoning paths and asking models to perform self-ref",
    "link": "https://arxiv.org/abs/2404.00205",
    "context": "Title: Conceptual and Unbiased Reasoning in Language Models\nAbstract: arXiv:2404.00205v1 Announce Type: new  Abstract: Conceptual reasoning, the ability to reason in abstract and high-level perspectives, is key to generalization in human cognition. However, limited study has been done on large language models' capability to perform conceptual reasoning. In this work, we bridge this gap and propose a novel conceptualization framework that forces models to perform conceptual reasoning on abstract questions and generate solutions in a verifiable symbolic space. Using this framework as an analytical tool, we show that existing large language models fall short on conceptual reasoning, dropping 9% to 28% on various benchmarks compared to direct inference methods. We then discuss how models can improve since high-level abstract reasoning is key to unbiased and generalizable decision-making. We propose two techniques to add trustworthy induction signals by generating familiar questions with similar underlying reasoning paths and asking models to perform self-ref",
    "path": "papers/24/04/2404.00205.json",
    "total_tokens": 905,
    "translated_title": "语言模型中的概念性和无偏推理",
    "translated_abstract": "概念推理，即在抽象和高层次视角进行推理的能力，是人类认知中泛化的关键。然而，对于大型语言模型在执行概念推理方面的能力进行了有限的研究。在这项工作中，我们弥合了这一差距，并提出了一个新颖的概念化框架，强制模型在抽象问题上进行概念推理，并在可验证的符号空间中生成解决方案。利用这个框架作为分析工具，我们展示了现有大型语言模型在概念推理方面的不足，与直接推理方法相比，在各种基准测试中下降了9%至28%。然后我们讨论了模型如何改进，因为高级抽象推理是无偏和泛化决策的关键。我们提出了两种技术，通过生成具有类似潜在推理路径的熟悉问题并要求模型执行自我参照，来添加可信的归纳信号。",
    "tldr": "提出了一个新颖的概念化框架，强制语言模型在抽象问题上进行概念推理，揭示现有大型语言模型在概念推理方面的不足，并探讨了如何通过改进模型来实现高级抽象推理，从而促进无偏和泛化决策。",
    "en_tdlr": "Introduced a novel conceptualization framework that forces language models to perform conceptual reasoning on abstract questions, revealed the shortcomings of existing large language models in conceptual reasoning, and discussed how to improve models to enable high-level abstract reasoning for unbiased and generalizable decision-making."
}