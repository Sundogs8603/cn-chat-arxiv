{
    "title": "On Difficulties of Attention Factorization through Shared Memory",
    "abstract": "arXiv:2404.00798v1 Announce Type: new  Abstract: Transformers have revolutionized deep learning in numerous fields, including natural language processing, computer vision, and audio processing. Their strength lies in their attention mechanism, which allows for the discovering of complex input relationships. However, this mechanism's quadratic time and memory complexity pose challenges for larger inputs. Researchers are now investigating models like Linear Unified Nested Attention (Luna) or Memory Augmented Transformer, which leverage external learnable memory to either reduce the attention computation complexity down to linear, or to propagate information between chunks in chunk-wise processing. Our findings challenge the conventional thinking on these models, revealing that interfacing with the memory directly through an attention operation is suboptimal, and that the performance may be considerably improved by filtering the input signal before communicating with memory.",
    "link": "https://arxiv.org/abs/2404.00798",
    "context": "Title: On Difficulties of Attention Factorization through Shared Memory\nAbstract: arXiv:2404.00798v1 Announce Type: new  Abstract: Transformers have revolutionized deep learning in numerous fields, including natural language processing, computer vision, and audio processing. Their strength lies in their attention mechanism, which allows for the discovering of complex input relationships. However, this mechanism's quadratic time and memory complexity pose challenges for larger inputs. Researchers are now investigating models like Linear Unified Nested Attention (Luna) or Memory Augmented Transformer, which leverage external learnable memory to either reduce the attention computation complexity down to linear, or to propagate information between chunks in chunk-wise processing. Our findings challenge the conventional thinking on these models, revealing that interfacing with the memory directly through an attention operation is suboptimal, and that the performance may be considerably improved by filtering the input signal before communicating with memory.",
    "path": "papers/24/04/2404.00798.json",
    "total_tokens": 776,
    "translated_title": "在通过共享内存进行注意力因子分解的困难",
    "translated_abstract": "Transformers在包括自然语言处理、计算机视觉和音频处理在内的许多领域中革命了深度学习。它们的优势在于其注意力机制，允许发现复杂的输入关系。然而，这种机制的二次时间和内存复杂性为更大的输入提出了挑战。研究人员现在正在研究诸如线性统一嵌套注意力（Luna）或Memory Augmented Transformer等模型，这些模型利用外部可学习内存，将注意力计算复杂性降低到线性，或者在以块为单位的处理中在块之间传播信息。我们的研究挑战了这些模型的传统思维，揭示了通过各种操作直接与内存接口的方法并不是最佳选择，并且通过在与内存通信之前过滤输入信号，性能可能会得到显着改善。",
    "tldr": "通过过滤输入信号来优化与内存通信，可以显着提高模型性能，挑战了使用注意力机制的传统思维。",
    "en_tdlr": "Filtering the input signal before communicating with memory significantly improves model performance, challenging the traditional thinking on utilizing attention mechanism."
}