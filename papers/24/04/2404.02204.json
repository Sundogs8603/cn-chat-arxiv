{
    "title": "Emergent Abilities in Reduced-Scale Generative Language Models",
    "abstract": "arXiv:2404.02204v1 Announce Type: new  Abstract: Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study investigates if such emergent properties are strictly tied to model size or can be demonstrated by smaller models trained on reduced-scale data. To explore this, we simplify pre-training data and pre-train 36 causal language models with parameters varying from 1 million to 165 million parameters. We show that models trained on this simplified pre-training data demonstrate enhanced zero-shot capabilities across various tasks in simplified language, achieving performance comparable to that of pre-trained models six times larger on unrestricted language. This suggests that downscaling the language allows zero-shot learning capabilities to emerge in models with limited size. Additionally, we f",
    "link": "https://arxiv.org/abs/2404.02204",
    "context": "Title: Emergent Abilities in Reduced-Scale Generative Language Models\nAbstract: arXiv:2404.02204v1 Announce Type: new  Abstract: Large language models can solve new tasks without task-specific fine-tuning. This ability, also known as in-context learning (ICL), is considered an emergent ability and is primarily seen in large language models with billions of parameters. This study investigates if such emergent properties are strictly tied to model size or can be demonstrated by smaller models trained on reduced-scale data. To explore this, we simplify pre-training data and pre-train 36 causal language models with parameters varying from 1 million to 165 million parameters. We show that models trained on this simplified pre-training data demonstrate enhanced zero-shot capabilities across various tasks in simplified language, achieving performance comparable to that of pre-trained models six times larger on unrestricted language. This suggests that downscaling the language allows zero-shot learning capabilities to emerge in models with limited size. Additionally, we f",
    "path": "papers/24/04/2404.02204.json",
    "total_tokens": 816,
    "translated_title": "减小规模生成语言模型中的新能力",
    "translated_abstract": "大语言模型可以在不需要特定任务微调的情况下解决新任务。这种能力，也被称为上下文学习（ICL），被认为是一种新兴能力，主要出现在拥有数十亿参数的大语言模型中。本研究探讨了这种新兴属性是否严格与模型大小相关，或者可以通过在减小规模数据上训练的较小模型来展示。为了探索这一点，我们简化了预训练数据，对36个因果语言模型进行了预训练，参数从100万到1.65亿不等。我们展示了在这种简化的预训练数据上训练的模型在简化语言中表现出增强的零样本能力，实现了与在自由语言上六倍大的预训练模型相当的性能。这表明，缩小语言规模可以使具有有限大小的模型出现零样本学习能力。此外，我们f",
    "tldr": "减小规模数据训练的较小语言模型展示了增强的零样本能力，可在简化语言中实现与大型模型相当的性能。",
    "en_tdlr": "Smaller language models trained on reduced-scale data demonstrate enhanced zero-shot capabilities, achieving performance comparable to that of large models in simplified language."
}