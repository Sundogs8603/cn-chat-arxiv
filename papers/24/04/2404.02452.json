{
    "title": "Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations",
    "abstract": "arXiv:2404.02452v1 Announce Type: new  Abstract: Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss. To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language. In this paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing In-Context Cross-lingual Transfer (IC-XLT). The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language. Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming prompt-based models in the Zero and Few-shot scenarios adapted through fine-tuning. Moreover, we show that when source-lang",
    "link": "https://arxiv.org/abs/2404.02452",
    "context": "Title: Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations\nAbstract: arXiv:2404.02452v1 Announce Type: new  Abstract: Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a source language to make predictions in another language, often with a performance loss. To alleviate this, additional improvements can be achieved through subsequent adaptation using examples in the target language. In this paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer in the classification task by introducing In-Context Cross-lingual Transfer (IC-XLT). The novel concept involves training a model to learn from context examples and subsequently adapting it during inference to a target language by prepending a One-Shot context demonstration in that language. Our results show that IC-XLT successfully leverages target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model, outperforming prompt-based models in the Zero and Few-shot scenarios adapted through fine-tuning. Moreover, we show that when source-lang",
    "path": "papers/24/04/2404.02452.json",
    "total_tokens": 878,
    "translated_title": "通过上下文一次性演示进行自适应跨语言文本分类",
    "translated_abstract": "零样本跨语言转移（ZS-XLT）利用在源语言中训练的模型在另一种语言中做预测，通常会导致性能损失。为了减轻这种情况，通过在目标语言中使用示例进行后续适应可以实现额外的改进。本文在分类任务中利用上下文调整（ICT）来进行一次性跨语言转移，引入上下文跨语言转移（IC-XLT）。这一创新概念涉及训练一个模型从上下文示例中学习，然后在推理过程中通过在目标语言中插入一次性上下文演示来对其进行适应。我们的结果表明，IC-XLT成功利用目标语言示例，改进了评估的mT5模型的跨语言能力，在适用于零和少量样本场景的情况下，通过微调来适应的基于提示的模型。此外，我们还表明，当源语言时",
    "tldr": "本论文引入了一种新颖的概念，即通过在目标语言中插入一次性上下文演示，从而成功利用目标语言示例来改进评估的mT5模型的跨语言能力。",
    "en_tdlr": "This paper introduces a novel concept of leveraging target-language examples to improve the cross-lingual capabilities of the evaluated mT5 model by prepending a One-Shot context demonstration in that language."
}