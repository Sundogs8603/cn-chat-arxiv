{
    "title": "Scaling Properties of Speech Language Models",
    "abstract": "arXiv:2404.00685v1 Announce Type: cross  Abstract: Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost",
    "link": "https://arxiv.org/abs/2404.00685",
    "context": "Title: Scaling Properties of Speech Language Models\nAbstract: arXiv:2404.00685v1 Announce Type: cross  Abstract: Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost",
    "path": "papers/24/04/2404.00685.json",
    "total_tokens": 735,
    "translated_title": "语音语言模型的尺度特性",
    "translated_abstract": "语音语言模型（SLM）旨在从原始音频中学习语言，而无需文本资源。尽管取得了显著进展，我们当前的模型表现出弱的句法和语义能力。然而，如果神经语言模型的尺度特性对语音模态成立，这些能力将随着训练所使用的计算量增加而提高。本文利用模型的尺度行为来估计我们当前方法将产生具有文本-based大型语言模型（LLMs）英语熟练度的SLM的尺度。",
    "tldr": "通过研究语音语言模型的尺度特性，可以预测其语言性能的扩展速度，与文本-based大型语言模型相比，语音语言模型的语言性能扩展速度慢三个数量级。"
}