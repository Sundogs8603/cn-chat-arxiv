{
    "title": "Transfer Learning with Point Transformers",
    "abstract": "arXiv:2404.00846v1 Announce Type: cross  Abstract: Point Transformers are near state-of-the-art models for classification, segmentation, and detection tasks on Point Cloud data. They utilize a self attention based mechanism to model large range spatial dependencies between multiple point sets. In this project we explore two things: classification performance of these attention based networks on ModelNet10 dataset and then, we use the trained model to classify 3D MNIST dataset after finetuning. We also train the model from scratch on 3D MNIST dataset to compare the performance of finetuned and from-scratch model on the MNIST dataset. We observe that since the two datasets have a large difference in the degree of the distributions, transfer learned models do not outperform the from-scratch models in this case. Although we do expect transfer learned models to converge faster since they already know the lower level edges, corners, etc features from the ModelNet10 dataset.",
    "link": "https://arxiv.org/abs/2404.00846",
    "context": "Title: Transfer Learning with Point Transformers\nAbstract: arXiv:2404.00846v1 Announce Type: cross  Abstract: Point Transformers are near state-of-the-art models for classification, segmentation, and detection tasks on Point Cloud data. They utilize a self attention based mechanism to model large range spatial dependencies between multiple point sets. In this project we explore two things: classification performance of these attention based networks on ModelNet10 dataset and then, we use the trained model to classify 3D MNIST dataset after finetuning. We also train the model from scratch on 3D MNIST dataset to compare the performance of finetuned and from-scratch model on the MNIST dataset. We observe that since the two datasets have a large difference in the degree of the distributions, transfer learned models do not outperform the from-scratch models in this case. Although we do expect transfer learned models to converge faster since they already know the lower level edges, corners, etc features from the ModelNet10 dataset.",
    "path": "papers/24/04/2404.00846.json",
    "total_tokens": 856,
    "translated_title": "基于点变换器的迁移学习",
    "translated_abstract": "点变换器是近乎处于最先进水平的模型，用于处理点云数据上的分类、分割和检测任务。它们利用基于自注意力机制来模拟多个点集之间的大范围空间依赖关系。在这个项目中，我们探讨了两件事：这些基于注意力的网络在ModelNet10数据集上的分类性能，然后，我们使用训练好的模型来对3D MNIST数据集进行分类微调。我们还从头开始训练模型在3D MNIST数据集上，以比较微调和从头开始模型在MNIST数据集上的性能。我们观察到，由于这两个数据集在分布程度上存在很大的差异，迁移学习模型在这种情况下并没有超越从头开始模型。尽管我们预期迁移学习模型会收敛更快，因为它们已经从ModelNet10数据集中知道了边缘、角等底层特征。",
    "tldr": "基于点变换器的迁移学习模型在处理点云数据分类等任务时表现良好，但在不同数据集上迁移学习效果并不总是优于从头开始训练模型，因为数据集之间的分布差异较大。",
    "en_tdlr": "Transfer learning with point transformers shows good performance on tasks like classification on point cloud data, but does not always outperform models trained from scratch on different datasets due to large distribution differences."
}