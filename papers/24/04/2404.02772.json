{
    "title": "FPT: Feature Prompt Tuning for Few-shot Readability Assessment",
    "abstract": "arXiv:2404.02772v1 Announce Type: new  Abstract: Prompt-based methods have achieved promising results in most few-shot text classification tasks. However, for readability assessment tasks, traditional prompt methods lackcrucial linguistic knowledge, which has already been proven to be essential. Moreover, previous studies on utilizing linguistic features have shown non-robust performance in few-shot settings and may even impair model performance.To address these issues, we propose a novel prompt-based tuning framework that incorporates rich linguistic knowledge, called Feature Prompt Tuning (FPT). Specifically, we extract linguistic features from the text and embed them into trainable soft prompts. Further, we devise a new loss function to calibrate the similarity ranking order between categories. Experimental results demonstrate that our proposed method FTP not only exhibits a significant performance improvement over the prior best prompt-based tuning approaches, but also surpasses th",
    "link": "https://arxiv.org/abs/2404.02772",
    "context": "Title: FPT: Feature Prompt Tuning for Few-shot Readability Assessment\nAbstract: arXiv:2404.02772v1 Announce Type: new  Abstract: Prompt-based methods have achieved promising results in most few-shot text classification tasks. However, for readability assessment tasks, traditional prompt methods lackcrucial linguistic knowledge, which has already been proven to be essential. Moreover, previous studies on utilizing linguistic features have shown non-robust performance in few-shot settings and may even impair model performance.To address these issues, we propose a novel prompt-based tuning framework that incorporates rich linguistic knowledge, called Feature Prompt Tuning (FPT). Specifically, we extract linguistic features from the text and embed them into trainable soft prompts. Further, we devise a new loss function to calibrate the similarity ranking order between categories. Experimental results demonstrate that our proposed method FTP not only exhibits a significant performance improvement over the prior best prompt-based tuning approaches, but also surpasses th",
    "path": "papers/24/04/2404.02772.json",
    "total_tokens": 853,
    "translated_title": "FPT:特征提示调整用于少样本可读性评估",
    "translated_abstract": "基于提示的方法在大多数少样本文本分类任务中取得了有希望的结果。然而，在可读性评估任务中，传统的提示方法缺乏关键的语言知识，而已经被证明是必不可少的。此外，先前关于利用语言特征的研究显示，在少样本设置中具有非稳健性能，甚至可能损害模型性能。为了解决这些问题，我们提出了一个新颖的基于提示的调整框架，即具有丰富语言知识的特征提示调整（FPT）。具体地，我们从文本中提取语言特征，并将其嵌入可训练的软提示中。此外，我们设计了一个新的损失函数来校准类别之间的相似性排名顺序。实验结果表明，我们提出的FTP方法不仅在先前最佳的基于提示的调整方法上表现出显著的性能提升，而且超过了他们。",
    "tldr": "FPT提出了一种新颖的基于提示的调整框架，通过将语言特征嵌入训练软提示并设计新的损失函数，在少样本设置下改善了可读性评估任务的性能。",
    "en_tdlr": "FPT proposes a novel prompt-based tuning framework that improves the performance of readability assessment tasks in few-shot settings by embedding linguistic features into trainable soft prompts and designing a new loss function to calibrate the similarity ranking order between categories."
}