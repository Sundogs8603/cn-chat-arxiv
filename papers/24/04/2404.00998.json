{
    "title": "LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation",
    "abstract": "arXiv:2404.00998v1 Announce Type: cross  Abstract: Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced ",
    "link": "https://arxiv.org/abs/2404.00998",
    "context": "Title: LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report Generation\nAbstract: arXiv:2404.00998v1 Announce Type: cross  Abstract: Evaluating generated radiology reports is crucial for the development of radiology AI, but existing metrics fail to reflect the task's clinical requirements. This study proposes a novel evaluation framework using large language models (LLMs) to compare radiology reports for assessment. We compare the performance of various LLMs and demonstrate that, when using GPT-4, our proposed metric achieves evaluation consistency close to that of radiologists. Furthermore, to reduce costs and improve accessibility, making this method practical, we construct a dataset using LLM evaluation results and perform knowledge distillation to train a smaller model. The distilled model achieves evaluation capabilities comparable to GPT-4. Our framework and distilled model offer an accessible and efficient evaluation method for radiology report generation, facilitating the development of more clinically relevant models. The model will be further open-sourced ",
    "path": "papers/24/04/2404.00998.json",
    "total_tokens": 914,
    "translated_title": "LLM-RadJudge: 实现X射线报告生成的放射科医师级评估",
    "translated_abstract": "在放射学AI的发展中，评估生成的放射学报告至关重要，但现有指标无法反映任务的临床要求。本研究提出了一种新的评估框架，使用大型语言模型（LLMs）来比较放射学报告以进行评估。我们比较了各种LLMs的性能，并证明，当使用GPT-4时，我们提出的指标实现了接近放射科医师评估一致性的表现。此外，为了降低成本并提高可访问性，使该方法实用化，我们利用LLM评估结果构建数据集，并进行知识蒸馏以训练一个较小的模型。蒸馏模型实现了与GPT-4相当的评估能力。我们的框架和蒸馏模型为放射学报告生成提供了一种可访问且高效的评估方法，促进了开发更具临床相关性模型。该模型将进一步开源。",
    "tldr": "该研究提出了一个使用大型语言模型进行放射学报告评估的新框架，通过GPT-4实现了与放射科医师评估一致性接近的效果，同时利用知识蒸馏训练的较小模型也达到了类似的评估能力。",
    "en_tdlr": "This study proposes a novel framework using large language models for evaluating radiology reports, achieving evaluation consistency close to that of radiologists with GPT-4, and a smaller model trained using knowledge distillation also achieves comparable evaluation capabilities."
}