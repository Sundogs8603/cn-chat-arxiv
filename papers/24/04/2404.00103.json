{
    "title": "PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks",
    "abstract": "arXiv:2404.00103v1 Announce Type: new  Abstract: Low-precision quantization is recognized for its efficacy in neural network optimization. Our analysis reveals that non-quantized elementwise operations which are prevalent in layers such as parameterized activation functions, batch normalization, and quantization scaling dominate the inference cost of low-precision models. These non-quantized elementwise operations are commonly overlooked in SOTA efficiency metrics such as Arithmetic Computation Effort (ACE). In this paper, we propose ACEv2 - an extended version of ACE which offers a better alignment with the inference cost of quantized models and their energy consumption on ML hardware. Moreover, we introduce PikeLPN, a model that addresses these efficiency issues by applying quantization to both elementwise operations and multiply-accumulate operations. In particular, we present a novel quantization technique for batch normalization layers named QuantNorm which allows for quantizing t",
    "link": "https://arxiv.org/abs/2404.00103",
    "context": "Title: PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks\nAbstract: arXiv:2404.00103v1 Announce Type: new  Abstract: Low-precision quantization is recognized for its efficacy in neural network optimization. Our analysis reveals that non-quantized elementwise operations which are prevalent in layers such as parameterized activation functions, batch normalization, and quantization scaling dominate the inference cost of low-precision models. These non-quantized elementwise operations are commonly overlooked in SOTA efficiency metrics such as Arithmetic Computation Effort (ACE). In this paper, we propose ACEv2 - an extended version of ACE which offers a better alignment with the inference cost of quantized models and their energy consumption on ML hardware. Moreover, we introduce PikeLPN, a model that addresses these efficiency issues by applying quantization to both elementwise operations and multiply-accumulate operations. In particular, we present a novel quantization technique for batch normalization layers named QuantNorm which allows for quantizing t",
    "path": "papers/24/04/2404.00103.json",
    "total_tokens": 924,
    "translated_title": "PikeLPN: 缓解低精度神经网络的被忽视的低效问题",
    "translated_abstract": "低精度量化以其优化神经网络的功效而闻名。我们的分析揭示了非量化的逐元素操作，在诸如参数化激活函数、批量归一化和量化缩放等层中普遍存在，并且主导了低精度模型的推理成本。这些非量化的逐元素操作通常被忽视于基于算术计算工作量（ACE）等最先进的效率度量中。在本文中，我们提出了ACEv2 - 一个ACE的扩展版本，能更好地与量化模型的推理成本以及它们在ML硬件上的能耗相匹配。此外，我们介绍了PikeLPN，一个通过将量化应用于逐元素操作和乘累积操作来解决这些效率问题的模型。特别地，我们提出了一种新颖的针对批量归一化层的量化技术，名为QuantNorm，可以实现对批量归一化操作的量化。",
    "tldr": "本文提出了一个ACEv2的扩展版本，与量化模型的推理成本和在ML硬件上的能耗更匹配，同时引入了PikeLPN模型，通过将量化应用于逐元素操作和乘累积操作，解决了低精度神经网络中被忽视的低效问题。",
    "en_tdlr": "This paper introduces an extended version of ACE, ACEv2, which better aligns with the inference cost and energy consumption on ML hardware of quantized models, and also presents PikeLPN model that addresses overlooked inefficiencies in low-precision neural networks by applying quantization to elementwise and multiply-accumulate operations."
}