{
    "title": "LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction",
    "abstract": "arXiv:2404.00913v1 Announce Type: cross  Abstract: Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs. In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information. Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure. We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts. LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-",
    "link": "https://arxiv.org/abs/2404.00913",
    "context": "Title: LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction\nAbstract: arXiv:2404.00913v1 Announce Type: cross  Abstract: Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA, which introduce extra modules or additional input sequences to inject new skills or knowledge, may compromise the innate abilities of LLMs. In this paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs' potential to better follow instructions by gradually paying more attention to worthwhile information. Specifically, the LLaMA-Excitor does not directly change the intermediate hidden state during the self-attention calculation of the transformer structure. We designed the Excitor block as a bypass module for the similarity score computation in LLMs' self-attention to reconstruct keys and change the importance of values by learnable prompts. LLaMA-Excitor ensures a self-adaptive allocation of additional attention to input instructions, thus effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on low-quality instruction-",
    "path": "papers/24/04/2404.00913.json",
    "total_tokens": 865,
    "translated_title": "LLaMA-Excitor: 通过间接特征交互进行通用指令调整",
    "translated_abstract": "现有的用于微调LLM（如Adapter、Prefix-tuning和LoRA）的方法引入了额外的模块或附加输入序列，以注入新技能或知识，但可能会损害LLM的固有能力。本文提出了LLaMA-Excitor，这是一种轻量级方法，通过逐渐更多地关注有价值的信息来激发LLM更好地遵循指令。具体来说，LLaMA-Excitor在transformer结构的自注意力计算过程中不直接改变中间隐藏状态。我们设计了Excitor块作为用于LLM自注意力中相似度计算的旁路模块，通过可学习的提示重新构建keys并改变values的重要性。LLaMA-Excitor确保自适应地将额外的注意力分配给输入指令，从而在低质量指令上微调LLM时有效地保留LLM的预训练知识。",
    "tldr": "LLaMA-Excitor是一种轻量级方法，通过逐渐更多地关注有价值的信息来激发LLM更好地遵循指令，而不直接改变中间隐藏状态，有效保留预训练知识。",
    "en_tdlr": "LLaMA-Excitor is a lightweight method that stimulates LLMs to better follow instructions by gradually paying more attention to worthwhile information without directly changing the intermediate hidden state, effectively preserving pre-trained knowledge."
}