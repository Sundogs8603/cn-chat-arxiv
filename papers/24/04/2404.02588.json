{
    "title": "Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages",
    "abstract": "arXiv:2404.02588v1 Announce Type: new  Abstract: Spoken Language Understanding (SLU) models are a core component of voice assistants (VA), such as Alexa, Bixby, and Google Assistant. In this paper, we introduce a pipeline designed to extend SLU systems to new languages, utilizing Large Language Models (LLMs) that we fine-tune for machine translation of slot-annotated SLU training data. Our approach improved on the MultiATIS++ benchmark, a primary multi-language SLU dataset, in the cloud scenario using an mBERT model. Specifically, we saw an improvement in the Overall Accuracy metric: from 53% to 62.18%, compared to the existing state-of-the-art method, Fine and Coarse-grained Multi-Task Learning Framework (FC-MTLF). In the on-device scenario (tiny and not pretrained SLU), our method improved the Overall Accuracy from 5.31% to 22.06% over the baseline Global-Local Contrastive Learning Framework (GL-CLeF) method. Contrary to both FC-MTLF and GL-CLeF, our LLM-based machine translation doe",
    "link": "https://arxiv.org/abs/2404.02588",
    "context": "Title: Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages\nAbstract: arXiv:2404.02588v1 Announce Type: new  Abstract: Spoken Language Understanding (SLU) models are a core component of voice assistants (VA), such as Alexa, Bixby, and Google Assistant. In this paper, we introduce a pipeline designed to extend SLU systems to new languages, utilizing Large Language Models (LLMs) that we fine-tune for machine translation of slot-annotated SLU training data. Our approach improved on the MultiATIS++ benchmark, a primary multi-language SLU dataset, in the cloud scenario using an mBERT model. Specifically, we saw an improvement in the Overall Accuracy metric: from 53% to 62.18%, compared to the existing state-of-the-art method, Fine and Coarse-grained Multi-Task Learning Framework (FC-MTLF). In the on-device scenario (tiny and not pretrained SLU), our method improved the Overall Accuracy from 5.31% to 22.06% over the baseline Global-Local Contrastive Learning Framework (GL-CLeF) method. Contrary to both FC-MTLF and GL-CLeF, our LLM-based machine translation doe",
    "path": "papers/24/04/2404.02588.json",
    "total_tokens": 872,
    "translated_title": "大型语言模型用于将口语理解系统扩展到新语言",
    "translated_abstract": "口语理解（SLU）模型是语音助手（如Alexa、Bixby和Google Assistant）的核心组件。本文介绍了一种旨在利用大型语言模型（LLMs）将SLU系统扩展到新语言的流水线，通过对机器翻译进行微调以处理槽标注的SLU训练数据。我们的方法在云场景中使用mBERT模型改进了MultiATIS ++基准测试，这是一个主要的多语言SLU数据集。具体而言，与现有的最先进方法Fine and Coarse-grained Multi-Task Learning Framework（FC-MTLF）相比，我们在“总体准确率”指标上取得了进步：从53%提高到62.18%。在设备上的情景中（小型且未预训练的SLU），我们的方法使“总体准确率”指标从5.31%提高到22.06%，超过了基线Global-Local Contrastive Learning Framework（GL-CLeF）方法。与FC-MTLF和GL-CLeF方法不同，我们基于LLM的机器翻译技术…（待续）",
    "tldr": "通过利用大型语言模型并对其进行微调，我们提出了一种新的方法将口语理解系统扩展到新语言，改进了多语言口语理解数据集的性能。"
}