{
    "title": "Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting in Transformers",
    "abstract": "arXiv:2404.01317v1 Announce Type: cross  Abstract: Pretraining language models on large text corpora is a common practice in natural language processing. Fine-tuning of these models is then performed to achieve the best results on a variety of tasks. In this paper, we investigate the problem of catastrophic forgetting in transformer neural networks and question the common practice of fine-tuning with a flat learning rate for the entire network in this context. We perform a hyperparameter optimization process to find learning rate distributions that are better than a flat learning rate. We combine the learning rate distributions thus found and show that they generalize to better performance with respect to the problem of catastrophic forgetting. We validate these learning rate distributions with a variety of NLP benchmarks from the GLUE dataset.",
    "link": "https://arxiv.org/abs/2404.01317",
    "context": "Title: Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting in Transformers\nAbstract: arXiv:2404.01317v1 Announce Type: cross  Abstract: Pretraining language models on large text corpora is a common practice in natural language processing. Fine-tuning of these models is then performed to achieve the best results on a variety of tasks. In this paper, we investigate the problem of catastrophic forgetting in transformer neural networks and question the common practice of fine-tuning with a flat learning rate for the entire network in this context. We perform a hyperparameter optimization process to find learning rate distributions that are better than a flat learning rate. We combine the learning rate distributions thus found and show that they generalize to better performance with respect to the problem of catastrophic forgetting. We validate these learning rate distributions with a variety of NLP benchmarks from the GLUE dataset.",
    "path": "papers/24/04/2404.01317.json",
    "total_tokens": 743,
    "translated_title": "在Transformer中智能学习率分布以减少灾难性遗忘",
    "translated_abstract": "在自然语言处理中，对大型文本语料库进行语言模型的预训练是一种常见做法。然后对这些模型进行微调以在各种任务上取得最佳结果。本文研究了Transformer神经网络中灾难性遗忘的问题，并质疑在这种情况下对整个网络采用相同学习率的微调常见做法。我们进行了超参数优化过程，找到了比平坦学习率更好的学习率分布。我们结合这些学习率分布，并展示它们对灾难性遗忘问题的性能表现更好。我们使用GLUE数据集中的各种自然语言处理基准验证了这些学习率分布。",
    "tldr": "本文研究了在Transformer神经网络中的灾难性遗忘问题，通过智能学习率分布取得了比平坦学习率更好的性能，并在GLUE数据集中得到验证。",
    "en_tdlr": "This paper investigates catastrophic forgetting in Transformer neural networks, achieves better performance than flat learning rate through intelligent learning rate distribution, and validates it on GLUE dataset."
}