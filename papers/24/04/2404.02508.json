{
    "title": "VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments",
    "abstract": "arXiv:2404.02508v1 Announce Type: cross  Abstract: Individuals with visual impairments, encompassing both partial and total difficulties in visual perception, are referred to as visually impaired (VI) people. An estimated 2.2 billion individuals worldwide are affected by visual impairments. Recent advancements in multi-modal large language models (MLLMs) have showcased their extraordinary capabilities across various domains. It is desirable to help VI individuals with MLLMs' great capabilities of visual understanding and reasoning. However, it is challenging for VI people to use MLLMs due to the difficulties in capturing the desirable images to fulfill their daily requests. For example, the target object is not fully or partially placed in the image. This paper explores how to leverage MLLMs for VI individuals to provide visual-question answers. VIAssist can identify undesired images and provide detailed actions. Finally, VIAssist can provide reliable answers to users' queries based on",
    "link": "https://arxiv.org/abs/2404.02508",
    "context": "Title: VIAssist: Adapting Multi-modal Large Language Models for Users with Visual Impairments\nAbstract: arXiv:2404.02508v1 Announce Type: cross  Abstract: Individuals with visual impairments, encompassing both partial and total difficulties in visual perception, are referred to as visually impaired (VI) people. An estimated 2.2 billion individuals worldwide are affected by visual impairments. Recent advancements in multi-modal large language models (MLLMs) have showcased their extraordinary capabilities across various domains. It is desirable to help VI individuals with MLLMs' great capabilities of visual understanding and reasoning. However, it is challenging for VI people to use MLLMs due to the difficulties in capturing the desirable images to fulfill their daily requests. For example, the target object is not fully or partially placed in the image. This paper explores how to leverage MLLMs for VI individuals to provide visual-question answers. VIAssist can identify undesired images and provide detailed actions. Finally, VIAssist can provide reliable answers to users' queries based on",
    "path": "papers/24/04/2404.02508.json",
    "total_tokens": 901,
    "translated_title": "VIAssist：为视觉障碍用户调整多模大语言模型",
    "translated_abstract": "具有视觉障碍的个体，包括视觉感知方面的部分或完全困难，被称为视障人士。全球估计有22亿人受视力障碍影响。近年来，多模大语言模型（MLLMs）的发展展示了它们在各个领域的非凡能力。希望通过MLLMs的视觉理解和推理能力来帮助视障人士。然而，由于难以捕捉理想图像以满足他们的日常需求，视障人士使用MLLMs具有挑战性。例如，目标对象未完全或部分放置在图像中。本文探讨了如何利用MLLMs为视障人士提供视觉问题答案。VIAssist能够识别不受欢迎的图像并提供详细的操作。最后，VIAssist可以根据用户的查询提供可靠的答案。",
    "tldr": "本论文研究了如何利用多模大语言模型（MLLMs）为视障人士提供视觉问题答案，提出了 VIAssist 系统，可以识别不受欢迎的图像并提供详细的操作，最终依据用户查询提供可靠的答案。",
    "en_tdlr": "This paper explores how to leverage multi-modal large language models (MLLMs) to provide visual-question answers for visually impaired individuals, introducing the VIAssist system that can identify undesired images and provide detailed actions, ultimately offering reliable answers based on user queries."
}