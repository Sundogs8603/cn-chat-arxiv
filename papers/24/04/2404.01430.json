{
    "title": "Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs",
    "abstract": "arXiv:2404.01430v1 Announce Type: cross  Abstract: Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augm",
    "link": "https://arxiv.org/abs/2404.01430",
    "context": "Title: Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing Positional Bias in LLMs\nAbstract: arXiv:2404.01430v1 Announce Type: cross  Abstract: Recent advances in large language models (LLMs) have enhanced their ability to process long input contexts. This development is particularly crucial for tasks that involve retrieving knowledge from an external datastore, which can result in long inputs. However, recent studies show a positional bias in LLMs, demonstrating varying performance depending on the location of useful information within the input sequence. In this study, we conduct extensive experiments to investigate the root causes of positional bias. Our findings indicate that the primary contributor to LLM positional bias stems from the inherent positional preferences of different models. We demonstrate that merely employing prompt-based solutions is inadequate for overcoming the positional preferences. To address this positional bias issue of a pre-trained LLM, we developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach which is composed of a data augm",
    "path": "papers/24/04/2404.01430.json",
    "total_tokens": 816,
    "translated_title": "降低LLMs中位置偏差的面向位置的参数高效微调方法",
    "translated_abstract": "大型语言模型（LLMs）的最新进展增强了它们处理长输入上下文的能力。对于涉及从外部数据存储库检索知识的任务，这一进展尤为关键，因为可能涉及长输入。然而，最近的研究显示LLMs存在位置偏差，表明其性能会根据输入序列中有用信息的位置而变化。本研究进行了大量实验，以调查位置偏差的根本原因。我们的发现表明，LLMs的位置偏差的主要贡献者源于不同模型的固有位置偏好。我们证明，仅仅采用基于提示的解决方案无法克服位置偏好。为了解决预训练LLMs的位置偏差问题，我们开发了一种面向位置的参数高效微调（PAPEFT）方法，该方法包含一个数据增广。",
    "tldr": "本研究发现LLMs的位置偏差主要源于不同模型的固有位置偏好，并提出了一种面向位置的参数高效微调方法来解决这一问题。",
    "en_tdlr": "This study identifies the positional bias in LLMs primarily stems from the inherent positional preferences of different models and proposes a position-aware parameter efficient fine-tuning approach to address this issue."
}