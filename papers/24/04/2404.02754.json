{
    "title": "Continual Learning of Numerous Tasks from Long-tail Distributions",
    "abstract": "arXiv:2404.02754v1 Announce Type: new  Abstract: Continual learning, an important aspect of artificial intelligence and machine learning research, focuses on developing models that learn and adapt to new tasks while retaining previously acquired knowledge. Existing continual learning algorithms usually involve a small number of tasks with uniform sizes and may not accurately represent real-world learning scenarios. In this paper, we investigate the performance of continual learning algorithms with a large number of tasks drawn from a task distribution that is long-tail in terms of task sizes. We design one synthetic dataset and two real-world continual learning datasets to evaluate the performance of existing algorithms in such a setting. Moreover, we study an overlooked factor in continual learning, the optimizer states, e.g. first and second moments in the Adam optimizer, and investigate how it can be used to improve continual learning performance. We propose a method that reuses the",
    "link": "https://arxiv.org/abs/2404.02754",
    "context": "Title: Continual Learning of Numerous Tasks from Long-tail Distributions\nAbstract: arXiv:2404.02754v1 Announce Type: new  Abstract: Continual learning, an important aspect of artificial intelligence and machine learning research, focuses on developing models that learn and adapt to new tasks while retaining previously acquired knowledge. Existing continual learning algorithms usually involve a small number of tasks with uniform sizes and may not accurately represent real-world learning scenarios. In this paper, we investigate the performance of continual learning algorithms with a large number of tasks drawn from a task distribution that is long-tail in terms of task sizes. We design one synthetic dataset and two real-world continual learning datasets to evaluate the performance of existing algorithms in such a setting. Moreover, we study an overlooked factor in continual learning, the optimizer states, e.g. first and second moments in the Adam optimizer, and investigate how it can be used to improve continual learning performance. We propose a method that reuses the",
    "path": "papers/24/04/2404.02754.json",
    "total_tokens": 805,
    "translated_title": "从长尾分布中不断学习大量任务",
    "translated_abstract": "Continual learning，作为人工智能和机器学习研究的一个重要方面，致力于开发能够学习并适应新任务同时保留先前获得知识的模型。现有的不断学习算法通常涉及少量任务，这些任务规模相同，并且可能无法准确地代表现实世界的学习场景。本文研究具有大量任务的不断学习算法在长尾任务大小分布下的表现。我们设计了一个合成数据集和两个真实世界的不断学习数据集，以评估现有算法在这种场景下的表现。此外，我们研究了一个在不断学习中被忽视的因素，即优化器状态，如Adam优化器中的第一和第二时刻，并探讨如何利用它来提高不断学习的性能。我们提出了一种方法，用于重复利用",
    "tldr": "本文研究了在长尾分布下大量任务的不断学习算法表现，并提出了利用优化器状态改进不断学习性能的方法。",
    "en_tdlr": "This paper investigates the performance of continual learning algorithms with a large number of tasks drawn from a long-tail distribution, and proposes a method to improve continual learning performance by utilizing optimizer states."
}