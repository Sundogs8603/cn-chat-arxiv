{
    "title": "Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts",
    "abstract": "arXiv:2404.02022v1 Announce Type: new  Abstract: In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks. It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs. With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Contex",
    "link": "https://arxiv.org/abs/2404.02022",
    "context": "Title: Improving Retrieval Augmented Open-Domain Question-Answering with Vectorized Contexts\nAbstract: arXiv:2404.02022v1 Announce Type: new  Abstract: In the era of large language models, applying techniques such as Retrieval Augmented Generation can better address Open-Domain Question-Answering problems. Due to constraints including model sizes and computing resources, the length of context is often limited, and it becomes challenging to empower the model to cover overlong contexts while answering questions from open domains. This paper proposes a general and convenient method to covering longer contexts in Open-Domain Question-Answering tasks. It leverages a small encoder language model that effectively encodes contexts, and the encoding applies cross-attention with origin inputs. With our method, the origin language models can cover several times longer contexts while keeping the computing requirements close to the baseline. Our experiments demonstrate that after fine-tuning, there is improved performance across two held-in datasets, four held-out datasets, and also in two In Contex",
    "path": "papers/24/04/2404.02022.json",
    "total_tokens": 825,
    "translated_title": "优化向量化上下文的检索增强开放领域问答",
    "translated_abstract": "在大型语言模型时代，应用检索增强生成等技术可以更好地解决开放领域问答问题。由于模型大小和计算资源等约束，上下文长度通常受限，让模型覆盖过长的上下文并回答来自开放领域的问题变得具有挑战性。本文提出了一种在开放领域问答任务中覆盖更长上下文的通用、方便方法。它利用一个小型编码器语言模型有效编码上下文，并对原始输入应用交叉注意力。通过我们的方法，原始语言模型可以覆盖几倍长的上下文，同时保持与基线接近的计算需求。我们的实验表明，在微调后，性能在两个保存的数据集、四个保留的数据集以及两个In Context",
    "tldr": "本文提出一种通用且便利的方法，通过利用小型编码器语言模型和交叉注意力，使原始语言模型可以覆盖更长的上下文，从而提高开放领域问答任务的性能。",
    "en_tdlr": "This paper proposes a general and convenient method to improve the performance of open-domain question-answering tasks by enabling the original language models to cover longer contexts using a small encoder language model and cross-attention mechanism."
}