{
    "title": "360+x: A Panoptic Multi-modal Scene Understanding Dataset",
    "abstract": "arXiv:2404.00989v1 Announce Type: cross  Abstract: Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. Figure 1 offers a glimpse of all 28 scene categories of our 360+x dataset. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis, we presented 5 differe",
    "link": "https://arxiv.org/abs/2404.00989",
    "context": "Title: 360+x: A Panoptic Multi-modal Scene Understanding Dataset\nAbstract: arXiv:2404.00989v1 Announce Type: cross  Abstract: Human perception of the world is shaped by a multitude of viewpoints and modalities. While many existing datasets focus on scene understanding from a certain perspective (e.g. egocentric or third-person views), our dataset offers a panoptic perspective (i.e. multiple viewpoints with multiple data modalities). Specifically, we encapsulate third-person panoramic and front views, as well as egocentric monocular/binocular views with rich modalities including video, multi-channel audio, directional binaural delay, location data and textual scene descriptions within each scene captured, presenting comprehensive observation of the world. Figure 1 offers a glimpse of all 28 scene categories of our 360+x dataset. To the best of our knowledge, this is the first database that covers multiple viewpoints with multiple data modalities to mimic how daily information is accessed in the real world. Through our benchmark analysis, we presented 5 differe",
    "path": "papers/24/04/2404.00989.json",
    "total_tokens": 784,
    "translated_title": "360+x：一个全景多模态场景理解数据集",
    "translated_abstract": "人类对世界的感知受到多种视角和模态的影响。虽然许多现有数据集侧重于从某种视角（例如自我中心或第三人称视角）理解场景，但我们的数据集提供了一个全景视角（即多视角和多数据模态）。具体而言，我们捕捉了第三人称全景和前视图，以及具有视频、多通道音频、定向双耳延迟、位置数据和文本场景描述等丰富模态的自我的单眼/双眼视图，呈现了对世界的全面观察。图1展示了我们的360+x数据集的所有28个场景类别。据我们所知，这是第一个涵盖多个视角和多个数据模态的数据库，模拟了现实世界中日常信息的获取方式。通过我们的基准分析，我们提出了5种不同",
    "tldr": "该数据集是第一个融合多个视角和多个数据模态以模拟现实世界中日常信息获取方式的数据库。",
    "en_tdlr": "The dataset is the first to combine multiple viewpoints and multiple data modalities to mimic how daily information is accessed in the real world."
}