{
    "title": "Extensive Self-Contrast Enables Feedback-Free Language Model Alignment",
    "abstract": "arXiv:2404.00604v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform",
    "link": "https://arxiv.org/abs/2404.00604",
    "context": "Title: Extensive Self-Contrast Enables Feedback-Free Language Model Alignment\nAbstract: arXiv:2404.00604v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has been a central technique for recent large language model (LLM) alignment. However, its heavy dependence on costly human or LLM-as-Judge preference feedback could stymie its wider applications. In this work, we introduce Self-Contrast, a feedback-free large language model alignment method via exploiting extensive self-generated negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast leverages the LLM itself to generate massive diverse candidates, and harnesses a pre-trained embedding model to filter multiple negatives according to text similarity. Theoretically, we illustrate that in this setting, merely scaling negative responses can still effectively approximate situations with more balanced positive and negative preference annotations. Our experiments with direct preference optimization (DPO) on three datasets show that, Self-Contrast could consistently outperform",
    "path": "papers/24/04/2404.00604.json",
    "total_tokens": 863,
    "translated_title": "广泛的自对比使得无需反馈的语言模型对齐成为可能",
    "translated_abstract": "人类反馈的强化学习（RLHF）一直是最近大型语言模型（LLM）对齐的核心技术。然而，其严重依赖昂贵的人类或LLM作为评判者的偏好反馈可能会阻碍其更广泛的应用。在这项工作中，我们引入了Self-Contrast，一种通过利用广泛自动生成的负例来进行无需反馈的大型语言模型对齐方法。仅通过监督的微调（SFT）目标，Self-Contrast利用LLM本身生成大量多样的候选项，并利用预训练的嵌入模型根据文本相似性过滤多个负例。理论上，我们证明了在这种设置中，仅仅扩大负面回应仍然可以有效地近似具有更平衡的正面和负面偏好注释的情况。我们对三个数据集进行了直接偏好优化（DPO）的实验表明，Self-Contrast能够始终优于",
    "tldr": "本论文介绍了一种利用广泛自对比生成负例的无需反馈的大型语言模型对齐方法，该方法在实验中表现优于直接偏好优化方法。",
    "en_tdlr": "This paper introduces a feedback-free large language model alignment method that utilizes extensive self-contrast to generate negatives, which outperforms direct preference optimization in experiments."
}