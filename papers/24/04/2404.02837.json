{
    "title": "Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models",
    "abstract": "arXiv:2404.02837v1 Announce Type: new  Abstract: This paper reveals the phenomenon of parameter heterogeneity in large language models (LLMs). We find that a small subset of ``cherry'' parameters exhibit a disproportionately large influence on model performance, while the vast majority of parameters have minimal impact. This heterogeneity is found to be prevalent across different model families, scales, and types. Motivated by this observation, we propose CherryQ, a novel quantization method that unifies the optimization of mixed-precision parameters. CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision. Extensive experiments demonstrate the effectiveness of CherryQ. CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance. Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts. Th",
    "link": "https://arxiv.org/abs/2404.02837",
    "context": "Title: Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models\nAbstract: arXiv:2404.02837v1 Announce Type: new  Abstract: This paper reveals the phenomenon of parameter heterogeneity in large language models (LLMs). We find that a small subset of ``cherry'' parameters exhibit a disproportionately large influence on model performance, while the vast majority of parameters have minimal impact. This heterogeneity is found to be prevalent across different model families, scales, and types. Motivated by this observation, we propose CherryQ, a novel quantization method that unifies the optimization of mixed-precision parameters. CherryQ identifies and preserves the critical cherry parameters in high precision while aggressively quantizing the remaining parameters to low precision. Extensive experiments demonstrate the effectiveness of CherryQ. CherryQ outperforms existing quantization approaches in terms of perplexity and downstream task performance. Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance compared to their 16-bit counterparts. Th",
    "path": "papers/24/04/2404.02837.json",
    "total_tokens": 860,
    "translated_title": "最后收官：大语言模型中的参数异质性和量化",
    "translated_abstract": "这篇论文揭示了大型语言模型（LLMs）中参数异质性的现象。我们发现，少量“樱桃”参数对模型性能产生了不成比例的巨大影响，而绝大多数参数的影响较小。这种异质性在不同模型系列、规模和类型中普遍存在。在这一观察的基础上，我们提出了CherryQ，一种新颖的量化方法，统一了混合精度参数的优化。CherryQ能够识别并保留高精度下关键的樱桃参数，同时将其余参数积极量化为低精度。大量实验证明了CherryQ的有效性。CherryQ在困惑度和下游任务性能方面优于现有的量化方法。值得注意的是，我们的3位量化Vicuna-1.5与它们的16位对应物相比表现出色。",
    "tldr": "论文揭示了在大语言模型中存在参数异质性的现象，提出了一种名为CherryQ的量化方法，该方法能够在保留关键参数的同时将其余参数高效量化至低精度，在性能方面明显优于现有方法。",
    "en_tdlr": "The paper reveals the phenomenon of parameter heterogeneity in large language models and proposes a quantization method called CherryQ, which can aggressively quantize most parameters to low precision while preserving critical parameters, outperforming existing methods in terms of performance."
}