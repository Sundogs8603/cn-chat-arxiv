{
    "title": "ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback",
    "abstract": "arXiv:2404.00934v1 Announce Type: new  Abstract: ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. Fo",
    "link": "https://arxiv.org/abs/2404.00934",
    "context": "Title: ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback\nAbstract: arXiv:2404.00934v1 Announce Type: new  Abstract: ChatGLM is a free-to-use AI service powered by the ChatGLM family of large language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline -- a reinforcement learning from human feedback (RLHF) system -- designed to enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses three major components: the collection of human preference data, the training of the reward model, and the optimization of policies. Throughout the process of integrating ChatGLM-RLHF into production, we encountered and addressed several unprecedented challenges. We introduce the strategies to mitigate reward variance for stabilized large-scale training, implement model parallelism with fused gradient-descent, and design regularization constraints to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF brings significant improvements in alignment tasks compared to the supervised fine-tuned (SFT) version of ChatGLM. Fo",
    "path": "papers/24/04/2404.00934.json",
    "total_tokens": 968,
    "translated_title": "ChatGLM-RLHF：将大型语言模型与人类反馈对齐的实践",
    "translated_abstract": "arXiv:2404.00934v1 公告类型：新的 摘要：ChatGLM是由大型语言模型（LLMs）家族提供支持的免费人工智能服务。本文介绍了ChatGLM-RLHF流水线--一种从人类反馈中强化学习（RLHF）系统--旨在增强ChatGLM与人类偏好的对齐性。ChatGLM-RLHF包含三个主要组成部分：人类偏好数据的收集，奖励模型的训练，以及策略的优化。在将ChatGLM-RLHF整合到生产环境的过程中，我们遇到并解决了一些前所未有的挑战。我们引入了减少奖励方差以实现稳定的大规模训练的策略，实现了带有融合梯度下降的模型并行性，并设计了正则化约束以避免LLMs中的灾难性遗忘。实验表明，与ChatGLM的受监督微调（SFT）版本相比，ChatGLM-RLHF在对齐任务中带来了显著的改进。",
    "tldr": "ChatGLM-RLHF是一种从人类反馈中强化学习系统，通过收集人类偏好数据、训练奖励模型和优化策略等方法来增强大型语言模型ChatGLM与人类偏好的对齐性，在实验中显示出显著的改进。",
    "en_tdlr": "ChatGLM-RLHF is a reinforcement learning system from human feedback designed to enhance the alignment of the large language model ChatGLM with human preferences, showing significant improvements in experiments by collecting human preference data, training reward models, and optimizing strategies."
}