{
    "title": "Rethinking the Relationship between Recurrent and Non-Recurrent Neural Networks: A Study in Sparsity",
    "abstract": "arXiv:2404.00880v1 Announce Type: new  Abstract: Neural networks (NN) can be divided into two broad categories, recurrent and non-recurrent. Both types of neural networks are popular and extensively studied, but they are often treated as distinct families of machine learning algorithms. In this position paper, we argue that there is a closer relationship between these two types of neural networks than is normally appreciated. We show that many common neural network models, such as Recurrent Neural Networks (RNN), Multi-Layer Perceptrons (MLP), and even deep multi-layer transformers, can all be represented as iterative maps.   The close relationship between RNNs and other types of NNs should not be surprising. In particular, RNNs are known to be Turing complete, and therefore capable of representing any computable function (such as any other types of NNs), but herein we argue that the relationship runs deeper and is more practical than this. For example, RNNs are often thought to be mor",
    "link": "https://arxiv.org/abs/2404.00880",
    "context": "Title: Rethinking the Relationship between Recurrent and Non-Recurrent Neural Networks: A Study in Sparsity\nAbstract: arXiv:2404.00880v1 Announce Type: new  Abstract: Neural networks (NN) can be divided into two broad categories, recurrent and non-recurrent. Both types of neural networks are popular and extensively studied, but they are often treated as distinct families of machine learning algorithms. In this position paper, we argue that there is a closer relationship between these two types of neural networks than is normally appreciated. We show that many common neural network models, such as Recurrent Neural Networks (RNN), Multi-Layer Perceptrons (MLP), and even deep multi-layer transformers, can all be represented as iterative maps.   The close relationship between RNNs and other types of NNs should not be surprising. In particular, RNNs are known to be Turing complete, and therefore capable of representing any computable function (such as any other types of NNs), but herein we argue that the relationship runs deeper and is more practical than this. For example, RNNs are often thought to be mor",
    "path": "papers/24/04/2404.00880.json",
    "total_tokens": 771,
    "translated_title": "重新思考循环神经网络和非循环神经网络之间的关系：稀疏性研究",
    "translated_abstract": "神经网络（NN）可以分为两大类，即循环和非循环。这两种神经网络类型都很受欢迎并广泛研究，但通常将它们视为机器学习算法的不同家族。在这篇立场论文中，我们认为循环神经网络和其他类型的神经网络之间存在比通常认识更紧密的关系。我们展示了许多常见的神经网络模型，如循环神经网络（RNN）、多层感知器（MLP）甚至深度多层变压器都可以表示为迭代映射。",
    "tldr": "循环神经网络（RNN）与其他类型的神经网络之间存在更紧密的关系，这种关系比我们通常认为的更实际，并且具有更深层次的联系。",
    "en_tdlr": "There is a closer and more practical relationship between Recurrent Neural Networks (RNN) and other types of neural networks than commonly appreciated."
}