{
    "title": "MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks",
    "abstract": "arXiv:2404.00457v1 Announce Type: new  Abstract: Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract \"important information\", i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme. We construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to iden",
    "link": "https://arxiv.org/abs/2404.00457",
    "context": "Title: MetaIE: Distilling a Meta Model from LLM for All Kinds of Information Extraction Tasks\nAbstract: arXiv:2404.00457v1 Announce Type: new  Abstract: Information extraction (IE) is a fundamental area in natural language processing where prompting large language models (LLMs), even with in-context examples, cannot defeat small LMs tuned on very small IE datasets. We observe that IE tasks, such as named entity recognition and relation extraction, all focus on extracting important information, which can be formalized as a label-to-span matching. In this paper, we propose a novel framework MetaIE to build a small LM as meta-model by learning to extract \"important information\", i.e., the meta-understanding of IE, so that this meta-model can be adapted to all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains the small LM via a symbolic distillation from an LLM following the label-to-span scheme. We construct the distillation dataset via sampling sentences from language model pre-training datasets (e.g., OpenWebText in our implementation) and prompting an LLM to iden",
    "path": "papers/24/04/2404.00457.json",
    "total_tokens": 881,
    "translated_title": "MetaIE: 从大型语言模型中提炼元模型，针对所有类型的信息抽取任务",
    "translated_abstract": "信息抽取（IE）是自然语言处理中的一个基础领域，在这个领域中，即使使用了具有上下文示例的大型语言模型（LLMs），也无法击败在非常小的IE数据集上调整了的小型LM。我们观察到，诸如命名实体识别和关系抽取等IE任务都集中在提取重要信息，这可以被形式化为标签到跨度的匹配。在本文中，我们提出了一个新颖的框架MetaIE，通过学习提取“重要信息”（即IE的元理解），来构建一个小型LM作为元模型，从而使得这个元模型可以有效且高效地适应各种IE任务。具体来说，MetaIE通过符号蒸馏从LLM中获取小型LM，遵循标签到跨度的方案。我们通过从语言模型预训练数据集（例如，我们的实现中的OpenWebText）中对句子进行采样构建蒸馏数据集，并提示一个LLM来识别...",
    "tldr": "提出了MetaIE框架，通过从大型语言模型中进行符号蒸馏，构建一个小型LM作为元模型，实现对所有类型的信息抽取任务的高效适应。",
    "en_tdlr": "Introduced MetaIE framework, which distills a small LM as a meta-model from large language models, enabling effective adaptation to all types of information extraction tasks."
}