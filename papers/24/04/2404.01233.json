{
    "title": "Optimal Ridge Regularization for Out-of-Distribution Prediction",
    "abstract": "arXiv:2404.01233v1 Announce Type: cross  Abstract: We study the behavior of optimal ridge regularization and optimal ridge risk for out-of-distribution prediction, where the test distribution deviates arbitrarily from the train distribution. We establish general conditions that determine the sign of the optimal regularization level under covariate and regression shifts. These conditions capture the alignment between the covariance and signal structures in the train and test data and reveal stark differences compared to the in-distribution setting. For example, a negative regularization level can be optimal under covariate shift or regression shift, even when the training features are isotropic or the design is underparameterized. Furthermore, we prove that the optimally-tuned risk is monotonic in the data aspect ratio, even in the out-of-distribution setting and when optimizing over negative regularization levels. In general, our results do not make any modeling assumptions for the tra",
    "link": "https://arxiv.org/abs/2404.01233",
    "context": "Title: Optimal Ridge Regularization for Out-of-Distribution Prediction\nAbstract: arXiv:2404.01233v1 Announce Type: cross  Abstract: We study the behavior of optimal ridge regularization and optimal ridge risk for out-of-distribution prediction, where the test distribution deviates arbitrarily from the train distribution. We establish general conditions that determine the sign of the optimal regularization level under covariate and regression shifts. These conditions capture the alignment between the covariance and signal structures in the train and test data and reveal stark differences compared to the in-distribution setting. For example, a negative regularization level can be optimal under covariate shift or regression shift, even when the training features are isotropic or the design is underparameterized. Furthermore, we prove that the optimally-tuned risk is monotonic in the data aspect ratio, even in the out-of-distribution setting and when optimizing over negative regularization levels. In general, our results do not make any modeling assumptions for the tra",
    "path": "papers/24/04/2404.01233.json",
    "total_tokens": 841,
    "translated_title": "针对分布外预测的最优岭回归正则化",
    "translated_abstract": "我们研究了针对分布外预测的最优岭回归正则化和最优岭风险的行为，其中测试分布与训练分布任意偏离。我们建立了确定在协变量和回归偏移下最优正则化水平符号的一般条件。这些条件捕捉了训练数据和测试数据之间协方差和信号结构之间的对齐，并揭示了与在分布内设置相比的鲜明差异。例如，在协变量偏移或回归偏移下，即使训练特征是各向同性的或设计是欠参数化的，负正则化水平也可能是最优的。此外，我们证明了在数据纵横比中，甚至在最优化负正则化水平时，最优调整的风险是单调的，即在分布外设置中也是如此。总的来说，我们的结果对训练数据没有做出任何建模假设。",
    "tldr": "研究了针对分布外预测的最优岭回归正则化下的行为，并建立了确定最优正则化水平的一般条件，揭示了与分布内设置的鲜明差异。",
    "en_tdlr": "Investigated the behavior of optimal ridge regularization for out-of-distribution prediction and established general conditions determining the optimal regularization level, revealing stark differences compared to the in-distribution setting."
}