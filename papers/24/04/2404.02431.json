{
    "title": "On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons",
    "abstract": "arXiv:2404.02431v1 Announce Type: new  Abstract: Current decoder-based pre-trained language models (PLMs) successfully demonstrate multilingual capabilities. However, it is unclear how these models handle multilingualism. We analyze the neuron-level internal behavior of multilingual decoder-based PLMs, Specifically examining the existence of neurons that fire ``uniquely for each language'' within decoder-only multilingual PLMs. We analyze six languages: English, German, French, Spanish, Chinese, and Japanese, and show that language-specific neurons are unique, with a slight overlap (< 5%) between languages. These neurons are mainly distributed in the models' first and last few layers. This trend remains consistent across languages and models. Additionally, we tamper with less than 1% of the total neurons in each model during inference and demonstrate that tampering with a few language-specific neurons drastically changes the probability of target language occurrence in text generation.",
    "link": "https://arxiv.org/abs/2404.02431",
    "context": "Title: On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons\nAbstract: arXiv:2404.02431v1 Announce Type: new  Abstract: Current decoder-based pre-trained language models (PLMs) successfully demonstrate multilingual capabilities. However, it is unclear how these models handle multilingualism. We analyze the neuron-level internal behavior of multilingual decoder-based PLMs, Specifically examining the existence of neurons that fire ``uniquely for each language'' within decoder-only multilingual PLMs. We analyze six languages: English, German, French, Spanish, Chinese, and Japanese, and show that language-specific neurons are unique, with a slight overlap (< 5%) between languages. These neurons are mainly distributed in the models' first and last few layers. This trend remains consistent across languages and models. Additionally, we tamper with less than 1% of the total neurons in each model during inference and demonstrate that tampering with a few language-specific neurons drastically changes the probability of target language occurrence in text generation.",
    "path": "papers/24/04/2404.02431.json",
    "total_tokens": 938,
    "translated_title": "关于基于解码器的预训练语言模型的多语言能力：寻找和控制语言特定神经元",
    "translated_abstract": "当前的基于解码器的预训练语言模型（PLMs）成功展示了多语言能力。然而，这些模型如何处理多语言仍不清楚。我们分析了多语言基于解码器的PLMs的神经元级内部行为，具体检查了在仅解码器的多语言PLMs中是否存在为“每种语言独特激活”的神经元。我们分析了六种语言：英语、德语、法语、西班牙语、中文和日语，并表明语言特定的神经元是独特的，在语言之间有轻微的重叠（<5%）。这些神经元主要分布在模型的前几层和最后几层。这一趋势在不同语言和模型中保持一致。此外，我们在推断过程中干扰了每个模型少于1％的神经元，并证明干扰少量语言特定神经元会显著改变文本生成中目标语言出现的概率。",
    "tldr": "该研究分析了基于解码器的预训练语言模型在多语言处理中的神经元级内部行为，发现模型中存在特定于每种语言的神经元，干扰这些语言特定的神经元会显著改变文本生成中目标语言出现的概率。"
}