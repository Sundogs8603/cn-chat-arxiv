{
    "title": "Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models",
    "abstract": "arXiv:2404.01992v1 Announce Type: new  Abstract: Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARE-LAMA - a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. CONPARE-LAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval ",
    "link": "https://arxiv.org/abs/2404.01992",
    "context": "Title: Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary Information on Knowledge Retrieval from Pretrained Language Models\nAbstract: arXiv:2404.01992v1 Announce Type: new  Abstract: Pre-trained Language Models (PLMs) are known to contain various kinds of knowledge. One method to infer relational knowledge is through the use of cloze-style prompts, where a model is tasked to predict missing subjects or objects. Typically, designing these prompts is a tedious task because small differences in syntax or semantics can have a substantial impact on knowledge retrieval performance. Simultaneously, evaluating the impact of either prompt syntax or information is challenging due to their interdependence. We designed CONPARE-LAMA - a dedicated probe, consisting of 34 million distinct prompts that facilitate comparison across minimal paraphrases. These paraphrases follow a unified meta-template enabling the controlled variation of syntax and semantics across arbitrary relations. CONPARE-LAMA enables insights into the independent impact of either syntactical form or semantic information of paraphrases on the knowledge retrieval ",
    "path": "papers/24/04/2404.01992.json",
    "total_tokens": 816,
    "translated_title": "对释义的剖析：提示语法和补充信息对从预训练语言模型中检索知识的影响",
    "translated_abstract": "预训练语言模型（PLMs）被认为包含各种知识。一种推断关系知识的方法是通过使用填空式提示，模型被要求预测缺失的主语或宾语。设计这些提示通常是一项繁琐的任务，因为语法或语义上的细微差异可能会对知识检索性能产生重大影响。同时，评估提示语法或信息的影响是具有挑战性的，因为它们之间的相互依赖性。我们设计了CONPARE-LAMA - 一个专门的探针，由3400万个不同的提示组成，可以在最小释义之间进行比较。这些释义遵循统一的元模板，可以在任意关系中通过语法和语义的受控变化。CONPARE-LAMA使人能够洞察释义的语法形式或语义信息对知识检索的独立影响",
    "tldr": "设计了CONPARE-LAMA，确定了释义的语法和语义信息对知识检索的独立影响",
    "en_tdlr": "CONPARE-LAMA was designed to determine the independent impact of syntax and semantic information of paraphrases on knowledge retrieval"
}