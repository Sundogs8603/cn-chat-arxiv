{
    "title": "NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning",
    "abstract": "arXiv:2404.00459v1 Announce Type: new  Abstract: Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of \"42\", we suggest using \"{2:42}\" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstr",
    "link": "https://arxiv.org/abs/2404.00459",
    "context": "Title: NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning\nAbstract: arXiv:2404.00459v1 Announce Type: new  Abstract: Language models struggle with handling numerical data and performing arithmetic operations. We hypothesize that this limitation can be partially attributed to non-intuitive textual numbers representation. When a digit is read or generated by a causal language model it does not know its place value (e.g. thousands vs. hundreds) until the entire number is processed. To address this issue, we propose a simple adjustment to how numbers are represented by including the count of digits before each number. For instance, instead of \"42\", we suggest using \"{2:42}\" as the new format. This approach, which we term NumeroLogic, offers an added advantage in number generation by serving as a Chain of Thought (CoT). By requiring the model to consider the number of digits first, it enhances the reasoning process before generating the actual number. We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic formatting. We further demonstr",
    "path": "papers/24/04/2404.00459.json",
    "total_tokens": 761,
    "translated_title": "NumeroLogic：增强LLMs数字推理的数字编码",
    "translated_abstract": "论文指出，语言模型在处理数值数据和执行算术运算时面临困难。我们假设这种限制部分归因于非直观的文本数字表示。为了解决这个问题，我们提出了一种简单的调整方法，即在每个数字前包含数字的位数计数。例如，我们建议使用\"{2:42}\"代替\"42\"作为新的格式。我们将这种方法称为NumeroLogic，它在数字生成中作为“思维链”提供了额外优势。通过要求模型首先考虑数字的位数，它增强了生成实际数字之前的推理过程。我们使用算术任务来展示NumeroLogic格式的有效性。",
    "tldr": "NumeroLogic提出了一种新的数字表示方法，通过在每个数字前包含数字的位数计数，为语言模型的数字推理能力提供了增强，从而改善了生成实际数字之前的推理过程。",
    "en_tdlr": "NumeroLogic proposes a new way of representing numbers by including the count of digits before each number, enhancing the numerical reasoning ability of language models and improving the reasoning process before generating the actual number."
}