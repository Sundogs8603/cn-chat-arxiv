{
    "title": "MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models",
    "abstract": "arXiv:2404.00511v1 Announce Type: new  Abstract: This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. We propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of our multimodal approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: https://github.com/MIPS-COLT/MER-MCE.git",
    "link": "https://arxiv.org/abs/2404.00511",
    "context": "Title: MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in Conversations with Multimodal Language Models\nAbstract: arXiv:2404.00511v1 Announce Type: new  Abstract: This paper presents our winning submission to Subtask 2 of SemEval 2024 Task 3 on multimodal emotion cause analysis in conversations. We propose a novel Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction (MER-MCE) framework that integrates text, audio, and visual modalities using specialized emotion encoders. Our approach sets itself apart from top-performing teams by leveraging modality-specific features for enhanced emotion understanding and causality inference. Experimental evaluation demonstrates the advantages of our multimodal approach, with our submission achieving a competitive weighted F1 score of 0.3435, ranking third with a margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team. Project: https://github.com/MIPS-COLT/MER-MCE.git",
    "path": "papers/24/04/2404.00511.json",
    "total_tokens": 868,
    "translated_title": "MIPS在SemEval-2024任务3中的表现：使用多模态语言模型在对话中进行多模态情绪-原因对提取",
    "translated_abstract": "本文介绍了我们在SemEval 2024任务3的子任务2中关于对话中多模态情绪原因分析的获奖提交。我们提出了一种新颖的多模态情绪识别和多模态情绪原因提取（MER-MCE）框架，该框架利用专门的情绪编码器整合文本、音频和视觉三种模态。我们的方法通过利用模态特定特征提升情绪理解和因果推理，使自己脱颖而出。实验评估表明了我们多模态方法的优势，我们的提交取得了竞争性的加权F1分数为0.3435，在0.0339之后排名第一的团队，仅在0.0025之后排名第二。项目链接：https://github.com/MIPS-COLT/MER-MCE.git",
    "tldr": "本文提出了一种集成文本、音频和视觉模态的多模态情绪识别和多模态情绪原因提取框架，通过利用专门的情绪编码器和模态特定特征，实现了提升情绪理解和因果推理的竞争性成果。",
    "en_tdlr": "This paper proposes a novel framework for multimodal emotion recognition and extraction by integrating text, audio, and visual modalities, achieving competitive results in emotion understanding and causality inference through specialized emotion encoders and modality-specific features."
}