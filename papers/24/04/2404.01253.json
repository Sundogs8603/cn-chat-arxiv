{
    "title": "UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing",
    "abstract": "arXiv:2404.01253v1 Announce Type: new  Abstract: Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge. In this work, we focus on the factual probing performance over unseen prompts from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge. We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen prompts. We propose an adapter-based framework, UniArk, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters. Extensive experiments show that UniArk can significantly improve the model's out-of-domain generalisation as well as consistency under various prompts. Additionally, we construct ParaTrex, a large-scale and diverse dataset for measuring the incon",
    "link": "https://arxiv.org/abs/2404.01253",
    "context": "Title: UniArk: Improving Generalisation and Consistency for Factual Knowledge Extraction through Debiasing\nAbstract: arXiv:2404.01253v1 Announce Type: new  Abstract: Several recent papers have investigated the potential of language models as knowledge bases as well as the existence of severe biases when extracting factual knowledge. In this work, we focus on the factual probing performance over unseen prompts from tuning, and using a probabilistic view we show the inherent misalignment between pre-training and downstream tuning objectives in language models for probing knowledge. We hypothesize that simultaneously debiasing these objectives can be the key to generalisation over unseen prompts. We propose an adapter-based framework, UniArk, for generalised and consistent factual knowledge extraction through simple methods without introducing extra parameters. Extensive experiments show that UniArk can significantly improve the model's out-of-domain generalisation as well as consistency under various prompts. Additionally, we construct ParaTrex, a large-scale and diverse dataset for measuring the incon",
    "path": "papers/24/04/2404.01253.json",
    "total_tokens": 808,
    "translated_title": "UniArk: 通过去偏差提高事实知识提取的泛化性和一致性",
    "translated_abstract": "最近一些论文探讨了将语言模型作为知识库的潜力，以及在提取事实知识时存在严重偏见的存在。本文聚焦于从微调中观察事实探测性能，并使用概率视角展示了语言模型中的预训练和下游微调目标之间固有的错位。我们假设同时去偏差这些目标可能是泛化到未见提示的关键。我们提出了一个基于适配器的框架UniArk，通过简单方法实现泛化和一致的事实知识提取，而不引入额外参数。大量实验表明UniArk可以显著提高模型在领域外的泛化性能，以及在各种提示下的一致性。此外，我们构建了ParaTrex，一个用于测量矛盾的大规模多样化数据集",
    "tldr": "通过UniArk框架，本文提出了一种基于适配器的解决方案，通过去偏差的方式提高了模型在未见提示下的泛化性和一致性",
    "en_tdlr": "This paper introduces UniArk, an adapter-based framework, to improve the model's generalization and consistency under unseen prompts through debiasing."
}