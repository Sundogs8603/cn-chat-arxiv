{
    "title": "Class-Incremental Few-Shot Event Detection",
    "abstract": "arXiv:2404.01767v1 Announce Type: new  Abstract: Event detection is one of the fundamental tasks in information extraction and knowledge graph. However, a realistic event detection system often needs to deal with new event classes constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental few-shot event detection. Nevertheless, this task faces two problems, i.e., old knowledge forgetting and new class overfitting. To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD. Specifically, to handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teac",
    "link": "https://arxiv.org/abs/2404.01767",
    "context": "Title: Class-Incremental Few-Shot Event Detection\nAbstract: arXiv:2404.01767v1 Announce Type: new  Abstract: Event detection is one of the fundamental tasks in information extraction and knowledge graph. However, a realistic event detection system often needs to deal with new event classes constantly. These new classes usually have only a few labeled instances as it is time-consuming and labor-intensive to annotate a large number of unlabeled instances. Therefore, this paper proposes a new task, called class-incremental few-shot event detection. Nevertheless, this task faces two problems, i.e., old knowledge forgetting and new class overfitting. To solve these problems, this paper further presents a novel knowledge distillation and prompt learning based method, called Prompt-KD. Specifically, to handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions, and the father teac",
    "path": "papers/24/04/2404.01767.json",
    "total_tokens": 812,
    "translated_title": "增量式少样本事件检测",
    "translated_abstract": "事件检测是信息抽取和知识图谱中的基本任务之一。然而，一个现实的事件检测系统经常需要不断处理新的事件类别。这些新类别通常只有少量标记实例，因为标注大量未标记实例是耗时且劳动密集的。因此，本文提出了一个新任务，称为增量式少样本事件检测。然而，这个任务面临着两个问题，即老知识遗忘和新类别过拟合。为了解决这些问题，本文进一步提出了一种基于知识蒸馏和提示学习的全新方法，称为Prompt-KD。具体来说，为了处理关于老知识遗忘的问题，Prompt-KD开发了一种基于注意力的多教师知识蒸馏框架，其中在所有学习会话中重复使用在基础类别上预先训练的祖先教师模型，父教师...",
    "tldr": "提出了增量式少样本事件检测任务，并通过提出的Prompt-KD方法解决了老知识遗忘和新类别过拟合问题。",
    "en_tdlr": "Introduced the task of class-incremental few-shot event detection and addressed the issues of old knowledge forgetting and new class overfitting through the proposed Prompt-KD method."
}