{
    "title": "Communication Efficient Distributed Training with Distributed Lion",
    "abstract": "arXiv:2404.00438v1 Announce Type: cross  Abstract: The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages on memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires communicating binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large model",
    "link": "https://arxiv.org/abs/2404.00438",
    "context": "Title: Communication Efficient Distributed Training with Distributed Lion\nAbstract: arXiv:2404.00438v1 Announce Type: cross  Abstract: The Lion optimizer has been a promising competitor with the AdamW for training large AI models, with advantages on memory, computation, and sample efficiency. In this paper, we introduce Distributed Lion, an innovative adaptation of Lion for distributed training environments. Leveraging the sign operator in Lion, our Distributed Lion only requires communicating binary or lower-precision vectors between workers to the center server, significantly reducing the communication cost. Our theoretical analysis confirms Distributed Lion's convergence properties. Empirical results demonstrate its robustness across a range of tasks, worker counts, and batch sizes, on both vision and language problems. Notably, Distributed Lion attains comparable performance to standard Lion or AdamW optimizers applied on aggregated gradients, but with significantly reduced communication bandwidth. This feature is particularly advantageous for training large model",
    "path": "papers/24/04/2404.00438.json",
    "total_tokens": 853,
    "translated_title": "使用分布式狮子进行高效通信的分布式训练",
    "translated_abstract": "Lion优化器在训练大型AI模型方面与AdamW有一定竞争力，具有在内存、计算和样本效率上的优势。本文介绍了分布式狮子，这是狮子在分布式训练环境中的创新性改进。利用狮子中的符号操作符，我们的分布式狮子只需要在工作节点和中心服务器之间传递二进制或低精度向量，显著降低了通信成本。我们的理论分析证实了分布式狮子的收敛性质。实证结果表明，它在多种任务、工作者数量和批量大小上表现稳健，在视觉和语言问题上表现出色。值得注意的是，分布式狮子在聚合梯度上达到了与标准狮子或AdamW优化器相当的性能，但通信带宽显著减少。这个特性对于训练大型模型尤为有利。",
    "tldr": "分布式狮子是对 Lion 进行了创新性改进，利用符号操作符降低了通信成本，在分布式训练中取得了与标准 Lion 或 AdamW 优化器相当的性能，并显著减少了通信带宽。"
}