{
    "title": "Deconstructing In-Context Learning: Understanding Prompts via Corruption",
    "abstract": "arXiv:2404.02054v1 Announce Type: new  Abstract: The ability of large language models (LLMs) to \"learn in context\" based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt ",
    "link": "https://arxiv.org/abs/2404.02054",
    "context": "Title: Deconstructing In-Context Learning: Understanding Prompts via Corruption\nAbstract: arXiv:2404.02054v1 Announce Type: new  Abstract: The ability of large language models (LLMs) to \"learn in context\" based on the provided prompt has led to an explosive growth in their use, culminating in the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI assistants are known to be robust to minor prompt modifications, mostly due to alignment techniques that use human feedback. In contrast, the underlying pre-trained LLMs they use as a backbone are known to be brittle in this respect. Building high-quality backbone models remains a core challenge, and a common approach to assessing their quality is to conduct few-shot evaluation. Such evaluation is notorious for being highly sensitive to minor prompt modifications, as well as the choice of specific in-context examples. Prior work has examined how modifying different elements of the prompt can affect model performance. However, these earlier studies tended to concentrate on a limited number of specific prompt ",
    "path": "papers/24/04/2404.02054.json",
    "total_tokens": 941,
    "translated_title": "拆解上下文学习: 通过破坏理解提示",
    "translated_abstract": "大型语言模型（LLMs）根据提供的提示“在上下文中学习”的能力已经导致它们的使用数量急剧增长，最终导致AI助手如ChatGPT、Claude和Bard的大量出现。这些AI助手被认为对提示的轻微修改具有鲁棒性，主要是由于使用了人类反馈的对齐技术。相比之下，它们使用作为骨干的基础预训练LLMs被认为在这方面比较脆弱。构建高质量的骨干模型仍然是一个核心挑战，评估其质量的常见方法是进行少样本评估。这种评估以对轻微提示修改和特定上下文示例选择的高度敏感而臭名昭著。先前的研究已经考察了修改提示的不同元素如何影响模型性能。然而，这些较早的研究往往集中在有限数量的具体提示上。",
    "tldr": "大型语言模型的能力在上下文中学习已经导致AI助手的急剧增长，其鲁棒性部分归因于对齐技术，然而这些助手使用的预训练模型在这方面却较为脆弱，构建高质量的骨干模型仍然是一个挑战。",
    "en_tdlr": "The ability of large language models to learn in context has led to the proliferation of AI assistants, with their robustness partly attributed to alignment techniques, while the backbone pre-trained models they use are known to be fragile in this regard, posing a challenge in building high-quality backbone models."
}