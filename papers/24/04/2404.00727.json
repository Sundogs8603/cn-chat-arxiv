{
    "title": "A Controlled Reevaluation of Coreference Resolution Models",
    "abstract": "arXiv:2404.00727v1 Announce Type: new  Abstract: All state-of-the-art coreference resolution (CR) models involve finetuning a pretrained language model. Whether the superior performance of one CR model over another is due to the choice of language model or other factors, such as the task-specific architecture, is difficult or impossible to determine due to lack of a standardized experimental setup. To resolve this ambiguity, we systematically evaluate five CR models and control for certain design decisions including the pretrained language model used by each. When controlling for language model size, encoder-based CR models outperform more recent decoder-based models in terms of both accuracy and inference speed. Surprisingly, among encoder-based CR models, more recent models are not always more accurate, and the oldest CR model that we test generalizes the best to out-of-domain textual genres. We conclude that controlling for the choice of language model reduces most, but not all, of ",
    "link": "https://arxiv.org/abs/2404.00727",
    "context": "Title: A Controlled Reevaluation of Coreference Resolution Models\nAbstract: arXiv:2404.00727v1 Announce Type: new  Abstract: All state-of-the-art coreference resolution (CR) models involve finetuning a pretrained language model. Whether the superior performance of one CR model over another is due to the choice of language model or other factors, such as the task-specific architecture, is difficult or impossible to determine due to lack of a standardized experimental setup. To resolve this ambiguity, we systematically evaluate five CR models and control for certain design decisions including the pretrained language model used by each. When controlling for language model size, encoder-based CR models outperform more recent decoder-based models in terms of both accuracy and inference speed. Surprisingly, among encoder-based CR models, more recent models are not always more accurate, and the oldest CR model that we test generalizes the best to out-of-domain textual genres. We conclude that controlling for the choice of language model reduces most, but not all, of ",
    "path": "papers/24/04/2404.00727.json",
    "total_tokens": 899,
    "translated_title": "核指代解析模型的受控重新评估",
    "translated_abstract": "所有最先进的核指代解析（CR）模型都涉及微调预训练语言模型。一个CR模型优于另一个的出色性能是由语言模型选择还是其他因素（如特定于任务的架构）造成的，由于缺乏标准化的实验设置，这是很难或不可能确定的。为了解决这种模糊性，我们系统评估了五个CR模型，并控制了一些设计决策，包括每个模型使用的预训练语言模型。当控制语言模型大小时，基于编码器的CR模型在准确性和推理速度方面优于更近期的基于解码器的模型。令人惊讶的是，在基于编码器的CR模型中，较近期的模型并不总是更准确，而我们测试的最老的CR模型在跨域文本体裁中表现最佳。我们得出结论，控制语言模型的选择可以减少大部分，但并非全部，",
    "tldr": "基于对预训练语言模型大小的控制，我们发现基于编码器的核指代解析模型在准确性和推理速度方面优于更近期的基于解码器的模型，而在基于编码器的模型中，最老的模型在跨域文本体裁中表现最佳。",
    "en_tdlr": "Controlled for the size of pretrained language models, we found that encoder-based coreference resolution models outperform more recent decoder-based models in terms of both accuracy and inference speed, with the oldest model in the encoder-based category generalizing the best to out-of-domain textual genres."
}