{
    "title": "CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP",
    "abstract": "arXiv:2404.00415v1 Announce Type: new  Abstract: We present CoDa (Constrained Generation based Data Augmentation), a controllable, effective, and training-free data augmentation technique for low-resource (data-scarce) NLP. Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints. Precisely, we extract a set of simple constraints from every instance in the low-resource dataset and verbalize them to prompt an LLM to generate novel and diverse training instances. Our findings reveal that synthetic data that follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or fine-tuning with complex algorithms that eventually make the model biased toward the small number of training instances. Additionally, CoDa is the first framework that provides users explicit control ove",
    "link": "https://arxiv.org/abs/2404.00415",
    "context": "Title: CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP\nAbstract: arXiv:2404.00415v1 Announce Type: new  Abstract: We present CoDa (Constrained Generation based Data Augmentation), a controllable, effective, and training-free data augmentation technique for low-resource (data-scarce) NLP. Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints. Precisely, we extract a set of simple constraints from every instance in the low-resource dataset and verbalize them to prompt an LLM to generate novel and diverse training instances. Our findings reveal that synthetic data that follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or fine-tuning with complex algorithms that eventually make the model biased toward the small number of training instances. Additionally, CoDa is the first framework that provides users explicit control ove",
    "path": "papers/24/04/2404.00415.json",
    "total_tokens": 867,
    "translated_title": "CoDa:基于约束生成的数据增强技术用于低资源NLP",
    "translated_abstract": "我们提出了CoDa（基于约束生成的数据增强），这是一种可控、有效且无需训练的数据增强技术，适用于低资源（数据稀缺）NLP。我们的方法基于提示现有的大型语言模型（LLMs）遵循指令来生成满足一组约束的文本。具体而言，我们从低资源数据集中的每个实例中提取一组简单的约束，将其表达为提示语，以促使LLM生成新颖且多样化的训练实例。我们的研究结果显示，符合下游数据集中简单约束的合成数据可以作为高效的增强，而CoDa可以在不需要复杂解码时约束生成技术或使用复杂算法微调的情况下实现这一点，这样可以避免模型偏向于少量训练实例。此外，CoDa 是首个提供用户明确控制的框架。",
    "tldr": "CoDa是一种针对低资源NLP提出的基于约束生成的数据增强技术，通过提示大型语言模型生成符合一组约束的文本，有效提升数据增强效果，避免模型偏向少量训练数据。"
}