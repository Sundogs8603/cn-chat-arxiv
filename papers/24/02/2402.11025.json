{
    "title": "Training Bayesian Neural Networks with Sparse Subspace Variational Inference",
    "abstract": "arXiv:2402.11025v1 Announce Type: new  Abstract: Bayesian neural networks (BNNs) offer uncertainty quantification but come with the downside of substantially increased training and inference costs. Sparse BNNs have been investigated for efficient inference, typically by either slowly introducing sparsity throughout the training or by post-training compression of dense BNNs. The dilemma of how to cut down massive training costs remains, particularly given the requirement to learn about the uncertainty. To solve this challenge, we introduce Sparse Subspace Variational Inference (SSVI), the first fully sparse BNN framework that maintains a consistently highly sparse Bayesian model throughout the training and inference phases. Starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters. While basis selection is characterized as a non-differentiable problem, we approximate the opti",
    "link": "https://arxiv.org/abs/2402.11025",
    "context": "Title: Training Bayesian Neural Networks with Sparse Subspace Variational Inference\nAbstract: arXiv:2402.11025v1 Announce Type: new  Abstract: Bayesian neural networks (BNNs) offer uncertainty quantification but come with the downside of substantially increased training and inference costs. Sparse BNNs have been investigated for efficient inference, typically by either slowly introducing sparsity throughout the training or by post-training compression of dense BNNs. The dilemma of how to cut down massive training costs remains, particularly given the requirement to learn about the uncertainty. To solve this challenge, we introduce Sparse Subspace Variational Inference (SSVI), the first fully sparse BNN framework that maintains a consistently highly sparse Bayesian model throughout the training and inference phases. Starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters. While basis selection is characterized as a non-differentiable problem, we approximate the opti",
    "path": "papers/24/02/2402.11025.json",
    "total_tokens": 863,
    "translated_title": "使用稀疏子空间变分推断训练贝叶斯神经网络",
    "translated_abstract": "贝叶斯神经网络（BNN）提供了不确定性量化，但代价是大幅增加训练和推断成本。稀疏BNN已被研究用于高效推断，通常通过在训练过程中逐渐引入稀疏性或通过后续对密集BNN进行压缩。然而，如何降低巨大的训练成本仍然是一个难题，特别是考虑到需要学习不确定性。为了解决这一挑战，我们引入了稀疏子空间变分推断（SSVI），这是第一个在训练和推断阶段始终保持高度稀疏的贝叶斯模型的全稀疏BNN框架。我们的方法从一个随机初始化的低维稀疏子空间开始，交替优化稀疏子空间基向量的选择以及相关参数。尽管基向量选择被描述为一个不可微分的问题，我们近似求解该问题。",
    "tldr": "提出了稀疏子空间变分推断（SSVI），这是第一个在训练和推断阶段始终保持高度稀疏的贝叶斯模型的全稀疏BNN框架",
    "en_tdlr": "Introduced Sparse Subspace Variational Inference (SSVI), the first fully sparse BNN framework that maintains a consistently highly sparse Bayesian model throughout the training and inference phases."
}