{
    "title": "OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models",
    "abstract": "arXiv:2402.13524v1 Announce Type: new  Abstract: Modern large language models (LLMs) should generally benefit individuals from various cultural backgrounds around the world. However, most recent advanced generative evaluation benchmarks tailed for LLMs mainly focus on English. To this end, we introduce OMGEval, the first Open-source Multilingual Generative test set that can assess the capability of LLMs in different languages. For each language, OMGEval provides 804 open-ended questions, covering a wide range of important capabilities of LLMs, such as general knowledge, logical reasoning, and so on. Each question is rigorously verified by human annotators. Notably, to sufficiently reflect the compatibility of LLMs in different cultural backgrounds, we perform localization for each non-English language. Specifically, the current version of OMGEval includes 5 languages (i.e., Zh, Ru, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator to automatically score different mo",
    "link": "https://arxiv.org/abs/2402.13524",
    "context": "Title: OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models\nAbstract: arXiv:2402.13524v1 Announce Type: new  Abstract: Modern large language models (LLMs) should generally benefit individuals from various cultural backgrounds around the world. However, most recent advanced generative evaluation benchmarks tailed for LLMs mainly focus on English. To this end, we introduce OMGEval, the first Open-source Multilingual Generative test set that can assess the capability of LLMs in different languages. For each language, OMGEval provides 804 open-ended questions, covering a wide range of important capabilities of LLMs, such as general knowledge, logical reasoning, and so on. Each question is rigorously verified by human annotators. Notably, to sufficiently reflect the compatibility of LLMs in different cultural backgrounds, we perform localization for each non-English language. Specifically, the current version of OMGEval includes 5 languages (i.e., Zh, Ru, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator to automatically score different mo",
    "path": "papers/24/02/2402.13524.json",
    "total_tokens": 873,
    "translated_title": "OMGEval：面向大型语言模型的开放多语言生成评估基准",
    "translated_abstract": "现代大型语言模型（LLMs）应该普遍受益于全球各种文化背景的个人。然而，大多数最近的先进生成评估基准主要专注于英语的LLMs。为此，我们推出了OMGEval，第一个可以评估LLMs在不同语言中能力的开放源多语言生成测试集。对于每种语言，OMGEval提供了804个开放式问题，涵盖了LLMs的广泛重要能力，如常识、逻辑推理等。每个问题都经过人类标注者的严格验证。值得注意的是，为了充分反映LLMs在不同文化背景下的兼容性，我们为每种非英语语言进行本地化。具体而言，当前版本的OMGEval包括5种语言（即：中文、俄文、法语、西班牙文、阿拉伯文）。在AlpacaEval之后，我们使用GPT-4作为裁判自动评分不同的模型。",
    "tldr": "OMGEval是第一个可以评估大型语言模型在不同语言中能力的开放源多语言生成测试集，涵盖了广泛重要能力并进行了本地化处理。",
    "en_tdlr": "OMGEval is the first open multilingual generative evaluation benchmark that assesses the capability of large language models in different languages, covering a wide range of important capabilities and performing localization."
}