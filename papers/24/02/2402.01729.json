{
    "title": "Contextualization Distillation from Large Language Model for Knowledge Graph Completion",
    "abstract": "While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip",
    "link": "https://arxiv.org/abs/2402.01729",
    "context": "Title: Contextualization Distillation from Large Language Model for Knowledge Graph Completion\nAbstract: While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pip",
    "path": "papers/24/02/2402.01729.json",
    "total_tokens": 897,
    "translated_title": "从大型语言模型中提取上下文信息用于知识图谱补全",
    "translated_abstract": "虽然文本信息显著提高了预训练语言模型（PLMs）在知识图谱补全（KGC）中的性能，但现有语料库从维基百科文章或同义词定义中收集的静态和噪声性质常常限制了基于PLM的KGC模型的潜力。为了克服这些挑战，我们提出了上下文化蒸馏策略，这是一种通用的可插入和可播放的方法，与判别和生成的KGC框架兼容。我们的方法首先指导大型语言模型（LLMs）将紧凑的结构化三元组转换为上下文丰富的段落。随后，我们引入了两个定制的辅助任务，重建和上下文化，使较小的KGC模型能够吸收这些丰富的三元组中的见解。对多种数据集和KGC技术的全面评估突出了我们方法的功效和适应性，揭示了无论基础管道如何，始终能提高性能。",
    "tldr": "本论文介绍了一种从大型语言模型中提取上下文信息用于知识图谱补全的策略，该策略能克服现有语料库的限制，显著提高了预训练语言模型在知识图谱补全中的性能。"
}