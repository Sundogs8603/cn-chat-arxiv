{
    "title": "Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition",
    "abstract": "arXiv:2402.15175v1 Announce Type: new  Abstract: Recent studies have uncovered intriguing phenomena in deep learning, such as grokking, double descent, and emergent abilities in large language models, which challenge human intuition and are crucial for a deeper understanding of neural models. In this paper, we present a comprehensive framework that provides a unified view of these three phenomena, focusing on the competition between memorization and generalization circuits. This approach, initially employed to explain grokking, is extended in our work to encompass a wider range of model sizes and training data volumes. Our framework delineates four distinct training dynamics, each depending on varying combinations of model size and training data quantity. Utilizing this framework, we provide a detailed analysis of the double descent phenomenon and propose two verifiable predictions regarding its occurrence, both substantiated by our experimental results. Moreover, we expand our framewo",
    "link": "https://arxiv.org/abs/2402.15175",
    "context": "Title: Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition\nAbstract: arXiv:2402.15175v1 Announce Type: new  Abstract: Recent studies have uncovered intriguing phenomena in deep learning, such as grokking, double descent, and emergent abilities in large language models, which challenge human intuition and are crucial for a deeper understanding of neural models. In this paper, we present a comprehensive framework that provides a unified view of these three phenomena, focusing on the competition between memorization and generalization circuits. This approach, initially employed to explain grokking, is extended in our work to encompass a wider range of model sizes and training data volumes. Our framework delineates four distinct training dynamics, each depending on varying combinations of model size and training data quantity. Utilizing this framework, we provide a detailed analysis of the double descent phenomenon and propose two verifiable predictions regarding its occurrence, both substantiated by our experimental results. Moreover, we expand our framewo",
    "path": "papers/24/02/2402.15175.json",
    "total_tokens": 872,
    "translated_title": "统一理解Grokking、双降和新兴能力：来自电路竞争视角的观点",
    "translated_abstract": "最近的研究揭示了深度学习中一些有趣的现象，如Grokking、双重下降以及大型语言模型中的新兴能力，这些挑战了人类的直觉，并对神经模型的更深入理解至关重要。在本文中，我们提出了一个全面的框架，提供了对这三种现象的统一观点，重点关注记忆和泛化电路之间的竞争。这种方法最初用于解释Grokking，我们在工作中将其扩展到了更广泛的模型大小和训练数据量。我们的框架勾勒出了四种不同的训练动态，每种都取决于模型大小和训练数据数量的不同组合。利用这一框架，我们对双重下降现象进行了详细分析，并提出了两个关于其发生的可验证预测，均得到我们实验结果的支持。此外，我们扩展了我们的框架",
    "tldr": "提供了一个全面框架，统一解释了Grokking、双重下降和新兴能力这三种现象，着重探讨了记忆和泛化电路之间的竞争。",
    "en_tdlr": "A comprehensive framework is presented to provide a unified view of Grokking, double descent, and emergent abilities, focusing on the competition between memorization and generalization circuits."
}