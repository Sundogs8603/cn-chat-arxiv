{
    "title": "Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining",
    "abstract": "Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed speci",
    "link": "https://arxiv.org/abs/2402.03302",
    "context": "Title: Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining\nAbstract: Accurate medical image segmentation demands the integration of multi-scale information, spanning from local features to global dependencies. However, it is challenging for existing methods to model long-range global information, where convolutional neural networks (CNNs) are constrained by their local receptive fields, and vision transformers (ViTs) suffer from high quadratic complexity of their attention mechanism. Recently, Mamba-based models have gained great attention for their impressive ability in long sequence modeling. Several studies have demonstrated that these models can outperform popular vision models in various tasks, offering higher accuracy, lower memory consumption, and less computational burden. However, existing Mamba-based models are mostly trained from scratch and do not explore the power of pretraining, which has been proven to be quite effective for data-efficient medical image analysis. This paper introduces a novel Mamba-based model, Swin-UMamba, designed speci",
    "path": "papers/24/02/2402.03302.json",
    "total_tokens": 950,
    "translated_title": "Swin-UMamba：以Mamba为基础的具有ImageNet预训练的UNet模型",
    "translated_abstract": "准确的医学图像分割需要整合从局部特征到全局依赖的多尺度信息。然而，现有方法很难建模长距离的全局信息，卷积神经网络受到其局部感受野的限制，而视觉变换器的注意力机制受到高二次复杂性的影响。最近，基于Mamba的模型因其在长序列建模方面的出色能力而受到广泛关注。几项研究表明，这些模型在各种任务中能够胜过流行的视觉模型，提供更高的准确性、更低的内存消耗和更少的计算负担。然而，现有的基于Mamba的模型大多是从头开始训练，没有充分利用预训练的威力，而预训练已被证明对于高效的医学图像分析非常有效。本文介绍了一种新颖的基于Mamba的模型Swin-UMamba，专为医学图像分割任务而设计。",
    "tldr": "Swin-UMamba是一种以Mamba为基础的新型UNet模型，通过结合局部特征和全局依赖的多尺度信息来实现准确的医学图像分割。与现有方法相比，Swin-UMamba具有更高的准确性、较低的内存消耗和更少的计算负担，并充分利用了预训练的威力。"
}