{
    "title": "On dimensionality of feature vectors in MPNNs",
    "abstract": "We revisit the classical result of Morris et al.~(AAAI'19) that message-passing graphs neural networks (MPNNs) are equal in their distinguishing power to the Weisfeiler--Leman (WL) isomorphism test.   Morris et al.~show their simulation result with ReLU activation function and $O(n)$-dimensional feature vectors, where $n$ is the number of nodes of the graph. Recently, by introducing randomness into the architecture, Aamand et al.~(NeurIPS'22) were able to improve this bound to $O(\\log n)$-dimensional feature vectors, although at the expense of guaranteeing perfect simulation only with high probability.   In all these constructions, to guarantee equivalence to the WL test, the dimension of feature vectors in the MPNN has to increase with the size of the graphs. However, architectures used in practice have feature vectors of constant dimension. Thus, there is a gap between the guarantees provided by these results and the actual characteristics of architectures used in practice. In this p",
    "link": "https://arxiv.org/abs/2402.03966",
    "context": "Title: On dimensionality of feature vectors in MPNNs\nAbstract: We revisit the classical result of Morris et al.~(AAAI'19) that message-passing graphs neural networks (MPNNs) are equal in their distinguishing power to the Weisfeiler--Leman (WL) isomorphism test.   Morris et al.~show their simulation result with ReLU activation function and $O(n)$-dimensional feature vectors, where $n$ is the number of nodes of the graph. Recently, by introducing randomness into the architecture, Aamand et al.~(NeurIPS'22) were able to improve this bound to $O(\\log n)$-dimensional feature vectors, although at the expense of guaranteeing perfect simulation only with high probability.   In all these constructions, to guarantee equivalence to the WL test, the dimension of feature vectors in the MPNN has to increase with the size of the graphs. However, architectures used in practice have feature vectors of constant dimension. Thus, there is a gap between the guarantees provided by these results and the actual characteristics of architectures used in practice. In this p",
    "path": "papers/24/02/2402.03966.json",
    "total_tokens": 848,
    "translated_title": "关于MPNN中特征向量的维度",
    "translated_abstract": "我们重新考察了Morris等人（AAAI'19）关于消息传递图神经网络（MPNNs）与Weisfeiler-Leman（WL）同构测试在区分能力上相等的经典结果。Morris等人展示了使用ReLU激活函数和$O(n)$维特征向量的仿真结果，其中$n$是图的节点数。最近，通过将随机性引入到架构中，Aamand等人（NeurIPS'22）能够将特征向量的维度提高到$O(\\log n)$，尽管以高概率保证完全模拟的开销也增加了。在所有这些构造中，为了保证与WL测试的等价性，MPNN中的特征向量维度必须随着图的大小增加。然而，实际使用的架构具有恒定维度的特征向量。因此，这些结果提供的保证与实际使用的架构特性之间存在差距。",
    "tldr": "这篇论文重新考察了消息传递图神经网络（MPNN）特征向量的维度问题，发现实际使用的架构与理论保证存在差距。"
}