{
    "title": "Leveraging Continuously Differentiable Activation Functions for Learning in Quantized Noisy Environments",
    "abstract": "Real-world analog systems intrinsically suffer from noise that can impede model convergence and accuracy on a variety of deep learning models. We demonstrate that differentiable activations like GELU and SiLU enable robust propagation of gradients which help to mitigate analog quantization error that is ubiquitous to all analog systems. We perform analysis and training of convolutional, linear, and transformer networks in the presence of quantized noise. Here, we are able to demonstrate that continuously differentiable activation functions are significantly more noise resilient over conventional rectified activations. As in the case of ReLU, the error in gradients are 100x higher than those in GELU near zero. Our findings provide guidance for selecting appropriate activations to realize performant and reliable hardware implementations across several machine learning domains such as computer vision, signal processing, and beyond.",
    "link": "https://arxiv.org/abs/2402.02593",
    "context": "Title: Leveraging Continuously Differentiable Activation Functions for Learning in Quantized Noisy Environments\nAbstract: Real-world analog systems intrinsically suffer from noise that can impede model convergence and accuracy on a variety of deep learning models. We demonstrate that differentiable activations like GELU and SiLU enable robust propagation of gradients which help to mitigate analog quantization error that is ubiquitous to all analog systems. We perform analysis and training of convolutional, linear, and transformer networks in the presence of quantized noise. Here, we are able to demonstrate that continuously differentiable activation functions are significantly more noise resilient over conventional rectified activations. As in the case of ReLU, the error in gradients are 100x higher than those in GELU near zero. Our findings provide guidance for selecting appropriate activations to realize performant and reliable hardware implementations across several machine learning domains such as computer vision, signal processing, and beyond.",
    "path": "papers/24/02/2402.02593.json",
    "total_tokens": 859,
    "translated_title": "在量化噪声环境中利用连续可微激活函数进行学习的优化",
    "translated_abstract": "实际世界中的模拟系统固有地受到噪声的影响，这可能会阻碍各种深度学习模型的收敛性和准确性。我们证明了像GELU和SiLU这样的可微激活函数可以稳健地传播梯度，有助于减轻普遍存在于所有模拟系统中的模拟量化误差。我们在量化噪声存在的情况下进行了卷积、线性和Transformer网络的分析和训练。我们能够证明，与传统的修正线性激活函数相比，连续可微激活函数在抗噪声方面具有显著优势。与ReLU相比，在接近零时梯度误差高出100倍。我们的研究结果为选择适当的激活函数提供了指导，以实现在计算机视觉、信号处理等多个机器学习领域中具有高性能和可靠性的硬件实现。",
    "tldr": "在量化噪声环境中，利用连续可微激活函数进行学习可以减轻模拟量化误差，为计算机视觉、信号处理等多个机器学习领域的硬件实现提供了指导。"
}