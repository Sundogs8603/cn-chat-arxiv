{
    "title": "$C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding",
    "abstract": "arXiv:2402.15991v1 Announce Type: new  Abstract: Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cas",
    "link": "https://arxiv.org/abs/2402.15991",
    "context": "Title: $C^3$: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding\nAbstract: arXiv:2402.15991v1 Announce Type: new  Abstract: Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cas",
    "path": "papers/24/02/2402.15991.json",
    "total_tokens": 869,
    "translated_title": "$C^3$: 用于高效跨语言自然语言理解的置信度校准模型级联",
    "translated_abstract": "跨语言自然语言理解(NLU)是自然语言处理(NLP)中的一个关键任务。最近的进展已经看到多语言预训练语言模型(mPLMs)显著增强了这些任务的性能。然而，mPLMs在推断期间需要大量资源并产生高计算成本，这给在真实世界和实时系统中部署带来挑战。现有的模型级联方法通过贪婪地基于模型置信度得分从各种模型中选择能够处理当前输入的最轻量模型来增强推断效率。然而，深度模型往往表现出过度自信，而且置信度分布在不同语言之间有所变化。这导致较小模型发出自信但不正确的预测，阻碍它们有效地在测试语言中进行泛化。在本研究中，我们引入了一个置信度校准模型级联。",
    "tldr": "本研究提出了一种置信度校准模型级联方法，用于增强跨语言自然语言理解任务中小模型的推断效率并解决深度模型过度自信和跨语言置信度分布变化的问题。",
    "en_tdlr": "This study introduces a confidence calibration model cascade method to enhance the inference efficiency of small models in cross-lingual natural language understanding tasks and address the issues of overconfidence in deep models and variability in confidence distributions across languages."
}