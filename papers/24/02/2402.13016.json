{
    "title": "Understanding the effects of language-specific class imbalance in multilingual fine-tuning",
    "abstract": "arXiv:2402.13016v1 Announce Type: new  Abstract: We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages. We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features. We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects. These results create awareness of the negative effects of language-specific class imbalance in multilingual fine-tuning and the way in which the model learns to rely on the separation of languages to perform the task.",
    "link": "https://arxiv.org/abs/2402.13016",
    "context": "Title: Understanding the effects of language-specific class imbalance in multilingual fine-tuning\nAbstract: arXiv:2402.13016v1 Announce Type: new  Abstract: We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages. We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features. We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects. These results create awareness of the negative effects of language-specific class imbalance in multilingual fine-tuning and the way in which the model learns to rely on the separation of languages to perform the task.",
    "path": "papers/24/02/2402.13016.json",
    "total_tokens": 779,
    "translated_title": "理解多语言微调中特定语言不平衡的影响",
    "translated_abstract": "我们研究了现实生活中多语言分类数据集中经常存在的一种不平衡类型的影响：跨语言标签的分布不均匀。我们展示了微调基于transformer的大型语言模型(LLM)在具有这种不平衡的数据集上会导致性能下降，潜在空间中语言分离更加明显，并促进了无信息特征的生成。我们修改了传统的类别加权方法，通过分别计算每种语言的类别权重来缓解这些不利影响。这些结果意识到了多语言微调中特定语言不平衡的负面影响，以及模型学习依赖语言分离来执行任务的方式。",
    "tldr": "微调transformer模型时，针对多语言数据集中个别语言标签不平衡的问题，通过为每种语言分别计算类别权重，可以缓解性能下降、语言分离更明显和无信息特征促进等不良影响。"
}