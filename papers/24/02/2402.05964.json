{
    "title": "A Survey on Transformer Compression",
    "abstract": "Large models based on the Transformer architecture play increasingly vital roles in artificial intelligence, particularly within the realms of natural language processing (NLP) and computer vision (CV). Model compression methods reduce their memory and computational cost, which is a necessary step to implement the transformer models on practical devices. Given the unique architecture of transformer, featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are required. The efficiency of these compression methods is also paramount, as it is usually impractical to retrain large models on the entire training dataset.This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to transformer models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design. In each category, we discuss compression methods",
    "link": "https://arxiv.org/abs/2402.05964",
    "context": "Title: A Survey on Transformer Compression\nAbstract: Large models based on the Transformer architecture play increasingly vital roles in artificial intelligence, particularly within the realms of natural language processing (NLP) and computer vision (CV). Model compression methods reduce their memory and computational cost, which is a necessary step to implement the transformer models on practical devices. Given the unique architecture of transformer, featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are required. The efficiency of these compression methods is also paramount, as it is usually impractical to retrain large models on the entire training dataset.This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to transformer models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design. In each category, we discuss compression methods",
    "path": "papers/24/02/2402.05964.json",
    "total_tokens": 839,
    "translated_title": "《Transformer压缩调研》",
    "translated_abstract": "基于Transformer架构的大型模型在人工智能领域，特别是自然语言处理（NLP）和计算机视觉（CV）领域中扮演着日益重要的角色。模型压缩方法可以减少模型的内存和计算成本，是在实际设备上实现Transformer模型的必要步骤。鉴于Transformer的独特架构，具有交替的注意力和前馈神经网络（FFN）模块，需要特定的压缩技术。这些压缩方法的效率也至关重要，因为重新训练整个训练数据集上的大型模型往往是不切实际的。本调研提供了对最近压缩方法的全面回顾，特别关注它们在Transformer模型中的应用。压缩方法主要分为修剪、量化、知识蒸馏和高效架构设计四个类别。在每个类别中，我们讨论了压缩方法",
    "tldr": "《Transformer压缩调研》是对最近压缩方法的全面回顾，特别关注它们在Transformer模型中的应用。压缩方法主要分为修剪、量化、知识蒸馏和高效架构设计四个类别。",
    "en_tdlr": "《A Survey on Transformer Compression》is a comprehensive review of recent compression methods, focusing on their application to transformer models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design."
}