{
    "title": "Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers",
    "abstract": "arXiv:2402.17564v1 Announce Type: new  Abstract: Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement. In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. Focused on the two aspects, we borrow the theoretical framework and learning methods from gradient-based optimization to design improved strategies for LLM-based prompt optimizers. By systematically analyzing a rich set of improvement strategies, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the op",
    "link": "https://arxiv.org/abs/2402.17564",
    "context": "Title: Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers\nAbstract: arXiv:2402.17564v1 Announce Type: new  Abstract: Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement. In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. Focused on the two aspects, we borrow the theoretical framework and learning methods from gradient-based optimization to design improved strategies for LLM-based prompt optimizers. By systematically analyzing a rich set of improvement strategies, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the op",
    "path": "papers/24/02/2402.17564.json",
    "total_tokens": 884,
    "translated_title": "将大型语言模型释放为提示优化器的潜力：与基于梯度的模型优化器的类比分析",
    "translated_abstract": "自动提示优化是提高大型语言模型（LLMs）性能的重要方法。最近的研究表明，使用LLMs作为提示优化器具有潜力，可以通过迭代改进生成改进的任务提示。本文提出了一个新颖的视角，通过与基于梯度的模型优化器进行类比来研究基于LLM的提示优化器的设计。为了连接这两种方法，我们确定模型参数学习中的两个关键因素：更新方向和更新方法。专注于这两个方面，我们借鉴了梯度优化的理论框架和学习方法，设计了改进的LLM-based提示优化器策略。通过系统分析丰富的改进策略，我们进一步开发了一个能力强大的基于梯度启发的LLM-based提示优化器，称为GPO。",
    "tldr": "本文提出了一个新颖的视角，将大型语言模型作为提示优化器来改进任务提示，通过类比基于梯度的模型优化器，设计了改进的LLM-based提示优化器策略，并开发了一种强大的基于梯度启发的LLM-based提示优化器GPO。",
    "en_tdlr": "This paper proposes a novel perspective of utilizing large language models as prompt optimizers to improve task prompts, draws an analogy with gradient-based model optimizers, designs improved strategies for LLM-based prompt optimizers, and develops a powerful Gradient-inspired LLM-based Prompt Optimizer called GPO."
}