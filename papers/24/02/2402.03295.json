{
    "title": "Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks",
    "abstract": "Second-order optimization approaches like the generalized Gauss-Newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices. Albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning. The major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix. These requirements are infeasible even with state-of-the-art hardware. In this work, we propose Ginger, an eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our method enjoys efficient linear memory and time complexity for each iteration. Instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate. We provide the convergence result of Ginger for non-convex objectives. Our experiments on different tasks with different model architectures verify the effectiveness of our method. Our code i",
    "link": "https://arxiv.org/abs/2402.03295",
    "context": "Title: Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks\nAbstract: Second-order optimization approaches like the generalized Gauss-Newton method are considered more powerful as they utilize the curvature information of the objective function with preconditioning matrices. Albeit offering tempting theoretical benefits, they are not easily applicable to modern deep learning. The major reason is due to the quadratic memory and cubic time complexity to compute the inverse of the matrix. These requirements are infeasible even with state-of-the-art hardware. In this work, we propose Ginger, an eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our method enjoys efficient linear memory and time complexity for each iteration. Instead of approximating the conditioning matrix, we directly maintain its inverse to make the approximation more accurate. We provide the convergence result of Ginger for non-convex objectives. Our experiments on different tasks with different model architectures verify the effectiveness of our method. Our code i",
    "path": "papers/24/02/2402.03295.json",
    "total_tokens": 869,
    "translated_title": "Ginger: 一种用于通用神经网络的线性复杂度高效曲率近似方法",
    "translated_abstract": "二阶优化方法，如广义高斯牛顿法，由于利用了目标函数的曲率信息和预处理矩阵，被认为更加强大。尽管在理论上具有诱人的优势，但它们不易应用于现代深度学习。主要原因是计算矩阵的逆所需的二次内存和三次时间复杂度是不可行的，即使使用先进的硬件也不行。在这项工作中，我们提出了Ginger，一种用于广义高斯牛顿矩阵逆的特征分解方法。我们的方法在每次迭代中具有高效的线性内存和时间复杂度。我们直接维护条件矩阵的逆，以使近似更加准确，而不是近似条件矩阵。我们提供了Ginger在非凸目标上的收敛结果。我们在不同任务和不同模型架构上的实验证实了我们方法的有效性。我们的代码...",
    "tldr": "Ginger是一种用于通用神经网络的高效曲率近似方法，具有线性复杂度。它通过特征分解来逆向计算广义高斯牛顿矩阵，避免了传统方法中的高内存和高时间复杂度问题。",
    "en_tdlr": "Ginger is an efficient curvature approximation method with linear complexity for general neural networks. It computes the inverse of the generalized Gauss-Newton matrix through eigendecomposition, avoiding the high memory and time complexity issues in traditional methods."
}