{
    "title": "Exploring Group and Symmetry Principles in Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across a wide range of applications; however, assessing their reasoning capabilities remains a significant challenge. In this paper, we introduce a framework grounded in group and symmetry principles, which have played a crucial role in fields such as physics and mathematics, and offer another way to evaluate their capabilities. While the proposed framework is general, to showcase the benefits of employing these properties, we focus on arithmetic reasoning and investigate the performance of these models on four group properties: closure, identity, inverse, and associativity. Our findings reveal that LLMs studied in this work struggle to preserve group properties across different test regimes. In the closure test, we observe biases towards specific outputs and an abrupt degradation in their performance from 100% to 0% after a specific sequence length. They also perform poorly in the identity test, which represents add",
    "link": "https://arxiv.org/abs/2402.06120",
    "context": "Title: Exploring Group and Symmetry Principles in Large Language Models\nAbstract: Large Language Models (LLMs) have demonstrated impressive performance across a wide range of applications; however, assessing their reasoning capabilities remains a significant challenge. In this paper, we introduce a framework grounded in group and symmetry principles, which have played a crucial role in fields such as physics and mathematics, and offer another way to evaluate their capabilities. While the proposed framework is general, to showcase the benefits of employing these properties, we focus on arithmetic reasoning and investigate the performance of these models on four group properties: closure, identity, inverse, and associativity. Our findings reveal that LLMs studied in this work struggle to preserve group properties across different test regimes. In the closure test, we observe biases towards specific outputs and an abrupt degradation in their performance from 100% to 0% after a specific sequence length. They also perform poorly in the identity test, which represents add",
    "path": "papers/24/02/2402.06120.json",
    "total_tokens": 878,
    "translated_title": "探索大型语言模型中的群组和对称性原理",
    "translated_abstract": "大型语言模型（LLM）在广泛的应用中展示了令人瞩目的性能，然而评估它们的推理能力仍然是一个重大挑战。在本文中，我们引入了一个以群组和对称性原理为基础的框架，这些原理在物理学和数学等领域发挥着关键作用，并提供了另一种评估它们能力的方式。虽然提出的框架是通用的，为了展示使用这些属性的好处，我们关注算术推理，并研究这些模型在四个群组属性（封闭性、恒等性、逆性和结合性）上的性能。我们的发现表明，在本研究中研究的LLM在不同的测试方案中难以保持群组属性。在封闭性测试中，我们观察到对特定输出的偏见，并在特定的序列长度后从100％的性能迅速下降到0％。它们在恒等性测试中表现不佳，代表了相加得到原数的属性。",
    "tldr": "本文提出了一个基于群组和对称性原理的框架，以评估大型语言模型的推理能力。通过研究四个群组属性，发现这些模型在保持群组属性方面表现不佳。",
    "en_tdlr": "This paper introduces a framework based on group and symmetry principles to evaluate the reasoning capabilities of large language models. The study reveals that these models struggle to preserve group properties and perform poorly in arithmetic reasoning tests."
}