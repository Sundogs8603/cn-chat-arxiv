{
    "title": "DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation",
    "abstract": "arXiv:2402.11120v1 Announce Type: new  Abstract: Distribution shifts and adversarial examples are two major challenges for deploying machine learning models. While these challenges have been studied individually, their combination is an important topic that remains relatively under-explored. In this work, we study the problem of adversarial robustness under a common setting of distribution shift - unsupervised domain adaptation (UDA). Specifically, given a labeled source domain $D_S$ and an unlabeled target domain $D_T$ with related but different distributions, the goal is to obtain an adversarially robust model for $D_T$. The absence of target domain labels poses a unique challenge, as conventional adversarial robustness defenses cannot be directly applied to $D_T$. To address this challenge, we first establish a generalization bound for the adversarial target loss, which consists of (i) terms related to the loss on the data, and (ii) a measure of worst-case domain divergence. Motivat",
    "link": "https://arxiv.org/abs/2402.11120",
    "context": "Title: DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation\nAbstract: arXiv:2402.11120v1 Announce Type: new  Abstract: Distribution shifts and adversarial examples are two major challenges for deploying machine learning models. While these challenges have been studied individually, their combination is an important topic that remains relatively under-explored. In this work, we study the problem of adversarial robustness under a common setting of distribution shift - unsupervised domain adaptation (UDA). Specifically, given a labeled source domain $D_S$ and an unlabeled target domain $D_T$ with related but different distributions, the goal is to obtain an adversarially robust model for $D_T$. The absence of target domain labels poses a unique challenge, as conventional adversarial robustness defenses cannot be directly applied to $D_T$. To address this challenge, we first establish a generalization bound for the adversarial target loss, which consists of (i) terms related to the loss on the data, and (ii) a measure of worst-case domain divergence. Motivat",
    "path": "papers/24/02/2402.11120.json",
    "total_tokens": 910,
    "translated_title": "DART: 一种面向对抗鲁棒的无监督领域自适应的原则性方法",
    "translated_abstract": "分布转移和对抗样本是部署机器学习模型面临的两个主要挑战。虽然这些挑战已被分别研究，但它们的结合仍然是一个相对未被充分探索的重要主题。本文研究了在一个常见的分布转移设置下对抗鲁棒性的问题，即无监督领域自适应（UDA）。具体地，给定一个带标签的源域 $D_S$ 和一个带有相关但不同分布的未标记目标域 $D_T$，目标是为 $D_T$ 获得一个对抗鲁棒的模型。目标域标签的缺失提出了一个独特的挑战，因为传统的对抗鲁棒性防御不能直接应用于 $D_T$。为了解决这一挑战，我们首先建立了对抗目标损失的泛化界限，其中包括与数据损失相关的项和最坏情况域分歧的度量。",
    "tldr": "本文探讨了无监督领域自适应中对抗鲁棒性的问题，通过建立对抗目标损失的泛化界限来解决目标域标签缺失带来的挑战。",
    "en_tdlr": "This paper investigates adversarial robustness in unsupervised domain adaptation and addresses the challenge of missing target domain labels by establishing a generalization bound for the adversarial target loss."
}