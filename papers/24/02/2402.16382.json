{
    "title": "Immunization against harmful fine-tuning attacks",
    "abstract": "arXiv:2402.16382v1 Announce Type: new  Abstract: Approaches to aligning large language models (LLMs) with human values has focused on correcting misalignment that emerges from pretraining. However, this focus overlooks another source of misalignment: bad actors might purposely fine-tune LLMs to achieve harmful goals. In this paper, we present an emerging threat model that has arisen from alignment circumvention and fine-tuning attacks. However, lacking in previous works is a clear presentation of the conditions for effective defence. We propose a set of conditions for effective defence against harmful fine-tuning in LLMs called \"Immunization conditions,\" which help us understand how we would construct and measure future defences. Using this formal framework for defence, we offer a synthesis of different research directions that might be persued to prevent harmful fine-tuning attacks and provide a demonstration of how to use these conditions experimentally showing early results of using",
    "link": "https://arxiv.org/abs/2402.16382",
    "context": "Title: Immunization against harmful fine-tuning attacks\nAbstract: arXiv:2402.16382v1 Announce Type: new  Abstract: Approaches to aligning large language models (LLMs) with human values has focused on correcting misalignment that emerges from pretraining. However, this focus overlooks another source of misalignment: bad actors might purposely fine-tune LLMs to achieve harmful goals. In this paper, we present an emerging threat model that has arisen from alignment circumvention and fine-tuning attacks. However, lacking in previous works is a clear presentation of the conditions for effective defence. We propose a set of conditions for effective defence against harmful fine-tuning in LLMs called \"Immunization conditions,\" which help us understand how we would construct and measure future defences. Using this formal framework for defence, we offer a synthesis of different research directions that might be persued to prevent harmful fine-tuning attacks and provide a demonstration of how to use these conditions experimentally showing early results of using",
    "path": "papers/24/02/2402.16382.json",
    "total_tokens": 834,
    "translated_title": "防范有害微调攻击",
    "translated_abstract": "大型语言模型（LLMs）与人类价值观的调整方法主要集中在纠正预训练中出现的不一致。然而，这种关注忽略了另一种不一致的来源：恶意行为者可能有意对LLMs进行微调以实现有害目标。本文提出了一种新兴的威胁模型，该模型源于对齐规避和微调攻击。然而，以前的作品缺乏有效防御条件的清晰呈现。我们提出了对抗LLMs中有害微调的有效防御条件集，称为“免疫条件”，这有助于我们了解如何构建和衡量未来的防御措施。利用这种防御的形式框架，我们提供了不同研究方向的综合，以防止有害微调攻击，并展示了如何在实验中使用这些条件的早期结果。",
    "tldr": "本文提出了一种用于防范大型语言模型中有害微调攻击的免疫条件集，以帮助理解如何构建和衡量未来的防御措施。",
    "en_tdlr": "The paper introduces a set of \"Immunization conditions\" for defending against harmful fine-tuning attacks in large language models, aiding in understanding how to construct and measure future defenses."
}