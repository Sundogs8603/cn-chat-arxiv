{
    "title": "Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks",
    "abstract": "arXiv:2402.09109v1 Announce Type: cross Abstract: Spiking Neural Networks (SNNs) have been recently integrated into Transformer architectures due to their potential to reduce computational demands and to improve power efficiency. Yet, the implementation of the attention mechanism using spiking signals on general-purpose computing platforms remains inefficient. In this paper, we propose a novel framework leveraging stochastic computing (SC) to effectively execute the dot-product attention for SNN-based Transformers. We demonstrate that our approach can achieve high classification accuracy ($83.53\\%$) on CIFAR-10 within 10 time steps, which is comparable to the performance of a baseline artificial neural network implementation ($83.66\\%$). We estimate that the proposed SC approach can lead to over $6.3\\times$ reduction in computing energy and $1.7\\times$ reduction in memory access costs for a digital CMOS-based ASIC design. We experimentally validate our stochastic attention block design",
    "link": "https://arxiv.org/abs/2402.09109",
    "context": "Title: Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks\nAbstract: arXiv:2402.09109v1 Announce Type: cross Abstract: Spiking Neural Networks (SNNs) have been recently integrated into Transformer architectures due to their potential to reduce computational demands and to improve power efficiency. Yet, the implementation of the attention mechanism using spiking signals on general-purpose computing platforms remains inefficient. In this paper, we propose a novel framework leveraging stochastic computing (SC) to effectively execute the dot-product attention for SNN-based Transformers. We demonstrate that our approach can achieve high classification accuracy ($83.53\\%$) on CIFAR-10 within 10 time steps, which is comparable to the performance of a baseline artificial neural network implementation ($83.66\\%$). We estimate that the proposed SC approach can lead to over $6.3\\times$ reduction in computing energy and $1.7\\times$ reduction in memory access costs for a digital CMOS-based ASIC design. We experimentally validate our stochastic attention block design",
    "path": "papers/24/02/2402.09109.json",
    "total_tokens": 853,
    "translated_title": "随机尖峰关注：在尖峰网络中利用随机计算加速关注机制",
    "translated_abstract": "最近，由于尖峰神经网络（SNN）在减少计算需求和提高功耗效率方面的潜力，已将其整合到Transformer体系结构中。然而，使用尖峰信号在通用计算平台上实现关注机制仍然低效。在本文中，我们提出了一种利用随机计算（SC）来有效执行基于SNN的Transformer的点积注意力的新框架。我们证明，我们的方法能够在10个时间步内在CIFAR-10上达到高分类准确率（83.53%），这与基线人工神经网络实现的性能（83.66%）相当。我们估计，该SC方法可以导致CMOS数字ASIC设计中计算能量减少6.3倍，内存访问成本减少1.7倍。我们通过实验证实了我们的随机注意力块设计。",
    "tldr": "本文提出了一种利用随机计算加速尖峰网络中的关注机制的新框架，证明该方法在CIFAR-10上能够达到高分类准确率，同时大大降低了计算能量和内存访问成本。"
}