{
    "title": "More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning",
    "abstract": "In this paper, we prove that Distributional Reinforcement Learning (DistRL), which learns the return distribution, can obtain second-order bounds in both online and offline RL in general settings with function approximation. Second-order bounds are instance-dependent bounds that scale with the variance of return, which we prove are tighter than the previously known small-loss bounds of distributional RL. To the best of our knowledge, our results are the first second-order bounds for low-rank MDPs and for offline RL. When specializing to contextual bandits (one-step RL problem), we show that a distributional learning based optimism algorithm achieves a second-order worst-case regret bound, and a second-order gap dependent bound, simultaneously. We also empirically demonstrate the benefit of DistRL in contextual bandits on real-world datasets. We highlight that our analysis with DistRL is relatively simple, follows the general framework of optimism in the face of uncertainty and does not",
    "link": "https://arxiv.org/abs/2402.07198",
    "context": "Title: More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning\nAbstract: In this paper, we prove that Distributional Reinforcement Learning (DistRL), which learns the return distribution, can obtain second-order bounds in both online and offline RL in general settings with function approximation. Second-order bounds are instance-dependent bounds that scale with the variance of return, which we prove are tighter than the previously known small-loss bounds of distributional RL. To the best of our knowledge, our results are the first second-order bounds for low-rank MDPs and for offline RL. When specializing to contextual bandits (one-step RL problem), we show that a distributional learning based optimism algorithm achieves a second-order worst-case regret bound, and a second-order gap dependent bound, simultaneously. We also empirically demonstrate the benefit of DistRL in contextual bandits on real-world datasets. We highlight that our analysis with DistRL is relatively simple, follows the general framework of optimism in the face of uncertainty and does not",
    "path": "papers/24/02/2402.07198.json",
    "total_tokens": 902,
    "translated_title": "分布式强化学习的更多好处：强化学习的二阶界限",
    "translated_abstract": "在这篇论文中，我们证明了分布式强化学习（DistRL）可以在具有函数逼近的一般设置中，在在线和离线强化学习中获得二阶界限。二阶界限是与返回值的方差成比例的实例相关的界限。我们证明了这些界限比传统的分布式强化学习中的小损失界限更紧。据我们所知，我们的结果是对于低秩MDP和离线强化学习的首个二阶界限。当特化为情境赌博机时（一步强化学习问题），我们展示了基于分布学习的乐观算法可以同时实现二阶最坏情况遗憾界限和二阶间隔依赖界限。我们还通过真实世界数据集在情境赌博机中实证了DistRL的好处。需要注意的是，我们对DistRL的分析相对简单，遵循了面对不确定性的乐观性的一般框架，并且不依赖于特定的情境赌博机模型。",
    "tldr": "这篇论文证明了分布式强化学习可以在在线和离线强化学习中获得二阶界限，这些界限更紧，同时还通过情境赌博机问题的实证表明了DistRL的好处。"
}