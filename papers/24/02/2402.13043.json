{
    "title": "Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries",
    "abstract": "arXiv:2402.13043v1 Announce Type: new  Abstract: Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We val",
    "link": "https://arxiv.org/abs/2402.13043",
    "context": "Title: Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries\nAbstract: arXiv:2402.13043v1 Announce Type: new  Abstract: Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We val",
    "path": "papers/24/02/2402.13043.json",
    "total_tokens": 870,
    "translated_title": "用隐式文本摘要提高对话状态跟踪的有效性和效率",
    "translated_abstract": "arXiv:2402.13043v1 公告类型: 新 文摘: 基于大型语言模型（LLM）的小样本对话状态跟踪（DST）依赖于一个有效且高效的对话检索器来查找类似的上下文示例以进行提示学习。先前的作品使用原始对话上下文作为搜索键和查询，并通过对带注释的对话进行微调来实现卓越性能。然而，这种方法不太适合扩展到新的领域或新的注释语言，因为微调数据不可用。为解决这一问题，我们基于对话的文本摘要来处理对话检索任务。采用基于LLM的对话摘要生成器进行查询和关键词生成，实现了有效的最大内积搜索。为避免LLM基于对话摘要生成带来的额外推理成本，我们进一步提炼一个轻量级的对话编码器，该编码器在不解码测试对话摘要的情况下生成查询嵌入向量。",
    "tldr": "使用文本摘要提高对话检索的有效性和效率，通过对话摘要生成器进行查询和关键词生成，进一步提炼轻量级对话编码器以避免额外推理成本",
    "en_tdlr": "Enhancing the effectiveness and efficiency of conversation retrieval by utilizing text summaries, leveraging a conversation summarizer for query and key generation, and distilling a lightweight conversation encoder to mitigate extra inference cost."
}