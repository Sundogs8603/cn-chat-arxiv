{
    "title": "SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning",
    "abstract": "arXiv:2402.11896v1 Announce Type: new  Abstract: Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time. Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs. Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT",
    "link": "https://arxiv.org/abs/2402.11896",
    "context": "Title: SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning\nAbstract: arXiv:2402.11896v1 Announce Type: new  Abstract: Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time. Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs. Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT",
    "path": "papers/24/02/2402.11896.json",
    "total_tokens": 860,
    "translated_title": "SIBO：一个简单的增强器用于参数高效微调",
    "translated_abstract": "微调大型语言模型（LLMs）的所有参数需要大量的计算资源和较长时间。最新的参数高效微调（PEFT）技术，如适配器微调和LoRA，允许只调整这些LLMs的一小部分参数。同时，人们注意到过度平滑的问题削弱了基于Transformer的LLMs的有效性，在下游任务中表现不佳。在本文中，我们提出了SIBO，这是一个简单的增强器，通过注入初始残差来增强PEFT。SIBO直观易懂，并且很容易扩展到一系列最新的PEFT技术，以减轻过度平滑并提高性能。对22个基准数据集进行的大量实验证明，SIBO显著提高了各种强基线模型的性能，比现有的PEFT技术提高了高达15.7%和23.5%。",
    "tldr": "SIBO提出了一个简单的增强器来增强参数高效微调技术，有效解决了Transformer-based LLMs中过度平滑的问题，并在多个基准数据集上取得了显著的性能提升。",
    "en_tdlr": "SIBO proposes a simple booster to enhance parameter-efficient fine-tuning techniques, effectively addressing the issue of over-smoothing in Transformer-based LLMs, and achieving significant performance improvements across multiple benchmark datasets."
}