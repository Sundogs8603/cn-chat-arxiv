{
    "title": "Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-sho",
    "link": "https://arxiv.org/abs/2402.00841",
    "context": "Title: Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?\nAbstract: Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-sho",
    "path": "papers/24/02/2402.00841.json",
    "total_tokens": 990,
    "translated_title": "小型大型语言模型在实际环境中的会议摘要中是否能够超越其体量？",
    "translated_abstract": "大型语言模型（LLMs）在没有明确针对任务特定数据集进行微调的情况下，展示了解决各种任务的出色能力。然而，在实际环境中部署LLMs并非易事，因为需要大量的计算资源。本文研究了在实际工业环境中的会议摘要任务，并通过比较经过微调的小型紧凑LLMs（如FLAN-T5、TinyLLaMA、LiteLLaMA）与零射击较大LLMs（如LLaMA-2、GPT-3.5、PaLM-2）的性能，进行了大量实验以解决利用LLMs在实际环境中的巨大成本问题。我们观察到，大多数小型LLMs，即使经过微调，也无法在会议摘要数据集中超越较大的零射击LLMs。然而，一个值得注意的例外是FLAN-T5（780M个参数），它的性能与许多零射击LLMs持平甚至更好。",
    "tldr": "本文研究了在实际环境中的会议摘要任务，通过比较小型紧凑LLMs和零射击较大LLMs的性能，发现大多数小型LLMs无法超越较大的零射击LLMs，但FLAN-T5是一个例外，它的性能与许多零射击LLMs持平甚至更好。",
    "en_tdlr": "This paper investigates the meeting summarization task in a real-world environment and compares the performance of smaller compact LLMs with larger zero-shot LLMs, finding that most smaller LLMs fail to outperform larger zero-shot LLMs except for FLAN-T5, which performs on par or even better."
}