{
    "title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models",
    "abstract": "arXiv:2402.19427v1 Announce Type: cross  Abstract: Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.",
    "link": "https://arxiv.org/abs/2402.19427",
    "context": "Title: Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models\nAbstract: arXiv:2402.19427v1 Announce Type: cross  Abstract: Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.",
    "path": "papers/24/02/2402.19427.json",
    "total_tokens": 822,
    "translated_title": "Griffin: 将门控线性循环与局部注意力相结合，实现高效语言模型",
    "translated_abstract": "循环神经网络（RNNs）在长序列上具有快速推理和高效扩展的优势，但训练困难且难以扩展。本文提出了Hawk，一种具有门控线性循环的RNN，以及Griffin，一种混合模型，将门控线性循环与局部注意力相结合。Hawk在下游任务的表现超过了Mamba，而Griffin在训练时仅使用了6倍少的令牌数量却与Llama-2的表现相匹配。我们还展示了Griffin在训练期间可以对比训练时长得多的序列进行推断。我们的模型在训练时具有与Transformer相匹配的硬件效率，而在推理过程中具有更低的延迟和更高的吞吐量。我们将Griffin扩展到了14B参数，并解释了如何对我们的模型进行分片以实现高效的分布式训练。",
    "tldr": "提出了 Griffin 模型，将门控线性循环与局部注意力相结合，实现高效的语言模型，该模型在推理过程中具有低延迟和高吞吐量。",
    "en_tdlr": "Introducing Griffin model that combines gated linear recurrences with local attention for efficient language models with low latency and high throughput during inference."
}