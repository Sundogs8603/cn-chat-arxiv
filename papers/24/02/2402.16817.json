{
    "title": "Investigating the Effectiveness of HyperTuning via Gisting",
    "abstract": "arXiv:2402.16817v1 Announce Type: new  Abstract: Gisting (Mu et al., 2023) is a simple method for training models to compress information into fewer token representations using a modified attention mask, and can serve as an economical approach to training Transformer-based hypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks built on Llama-2 models that generates task-specific soft prefixes based on few-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol Tuning datasets, we show that HyperLlama models can effectively compress information from few-shot examples into soft prefixes. However, they still underperform multi-task fine-tuned language models with full attention over few-shot in-context examples. We also show that HyperLlama-generated soft prefixes can serve as better initializations for further prefix tuning. Overall, Gisting-based hypernetworks are economical and easy to implement, but have mixed empirical performance.",
    "link": "https://arxiv.org/abs/2402.16817",
    "context": "Title: Investigating the Effectiveness of HyperTuning via Gisting\nAbstract: arXiv:2402.16817v1 Announce Type: new  Abstract: Gisting (Mu et al., 2023) is a simple method for training models to compress information into fewer token representations using a modified attention mask, and can serve as an economical approach to training Transformer-based hypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks built on Llama-2 models that generates task-specific soft prefixes based on few-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol Tuning datasets, we show that HyperLlama models can effectively compress information from few-shot examples into soft prefixes. However, they still underperform multi-task fine-tuned language models with full attention over few-shot in-context examples. We also show that HyperLlama-generated soft prefixes can serve as better initializations for further prefix tuning. Overall, Gisting-based hypernetworks are economical and easy to implement, but have mixed empirical performance.",
    "path": "papers/24/02/2402.16817.json",
    "total_tokens": 843,
    "translated_title": "研究通过Gisting的HyperTuning的有效性",
    "translated_abstract": "Gisting（Mu等，2023）是一种简单的方法，用于训练模型将信息压缩为更少的标记表示，其使用修改后的注意力蒙版，并可作为训练基于Transformer的超网络的经济方法。我们引入了HyperLlama，这是一组基于Llama-2模型构建的Gisting型超网络，它根据少量输入生成特定于任务的软前缀。在P3、Super-NaturalInstructions和Symbol Tuning数据集上的实验中，我们展示了HyperLlama模型可以有效地将信息从少量示例压缩成软前缀。然而，它们在少量上下文示例上的全注意力下的表现仍不如多任务微调语言模型。我们还表明，HyperLlama生成的软前缀可用作进一步前缀微调的更好初始化。总的来说，基于Gisting的超网络是经济且易于实现的，但在实证表现上有一定不稳定性。",
    "tldr": "Gisting方法可用于训练模型将信息压缩为更少的标记表示，构建的HyperLlama模型可以有效地将信息从少量示例压缩成软前缀，并为进一步的前缀微调提供更好的初始化。",
    "en_tdlr": "The Gisting method can compress information into fewer token representations, and the constructed HyperLlama model can effectively compress information from few-shot examples into soft prefixes, providing better initializations for further prefix tuning."
}