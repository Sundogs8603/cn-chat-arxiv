{
    "title": "BATON: Aligning Text-to-Audio Model with Human Preference Feedback",
    "abstract": "With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention. However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability. To alleviate this issue, we formulate the BATON, a framework designed to enhance the alignment between generated audio and text prompt using human preference feedback. Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback. Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs. Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model. The experiment results demonstrate that our BATON can significantly improve the generation quality of the orig",
    "link": "https://arxiv.org/abs/2402.00744",
    "context": "Title: BATON: Aligning Text-to-Audio Model with Human Preference Feedback\nAbstract: With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention. However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability. To alleviate this issue, we formulate the BATON, a framework designed to enhance the alignment between generated audio and text prompt using human preference feedback. Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback. Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs. Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model. The experiment results demonstrate that our BATON can significantly improve the generation quality of the orig",
    "path": "papers/24/02/2402.00744.json",
    "total_tokens": 893,
    "translated_title": "BATON：将文本到音频模型与人类偏好反馈进行对齐",
    "translated_abstract": "随着AI生成内容（AIGC）的发展，文本到音频模型引起了广泛关注。然而，由于自然语言的固有信息密度和有限的模型理解能力，这些模型很难生成与人类偏好对齐的音频。为了缓解这个问题，我们提出了BATON，这是一个旨在通过人类偏好反馈提高生成音频与文本提示之间对齐度的框架。我们的BATON包括三个关键阶段：首先，我们构建了一个包含提示和相应生成音频的数据集，并基于人类反馈进行了注释。其次，我们引入了一个奖励模型，利用构建的数据集可以模拟人类偏好，通过对输入的文本-音频对分配奖励。最后，我们利用奖励模型对现成的文本到音频模型进行了微调。实验结果表明，我们的BATON可以显著提高原始音频的生成质量。",
    "tldr": "在AI生成内容的背景下，我们提出了BATON框架，利用人类偏好反馈来改善文本到音频模型对齐的问题。通过构建数据集、引入奖励模型和微调现有模型，我们的BATON能够显著提高生成音频的质量。",
    "en_tdlr": "In the context of AI-generated content, we introduce the BATON framework to address the alignment issue in text-to-audio models using human preference feedback. By constructing a dataset, implementing a reward model, and fine-tuning an existing model, our BATON significantly improves the quality of generated audio."
}