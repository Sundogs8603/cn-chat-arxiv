{
    "title": "Robustly Learning Single-Index Models via Alignment Sharpness",
    "abstract": "arXiv:2402.17756v1 Announce Type: new  Abstract: We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model. We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions. This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions. Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation. The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest.",
    "link": "https://arxiv.org/abs/2402.17756",
    "context": "Title: Robustly Learning Single-Index Models via Alignment Sharpness\nAbstract: arXiv:2402.17756v1 Announce Type: new  Abstract: We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model. We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions. This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions. Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation. The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest.",
    "path": "papers/24/02/2402.17756.json",
    "total_tokens": 858,
    "translated_title": "通过对齐锐度稳健学习单指数模型",
    "translated_abstract": "我们研究了在对齐模型下以$L_2^2$损失学习单指数模型的问题。在对齐模型中，我们提出了一种高效的学习算法，实现了对最优损失的常数近似，并且适用于一系列分布（包括对数凹分布）和广泛的单调和Lipschitz连接函数的类。这是首个高效的常数近似对齐学习器，即使对于高斯数据和任何非平凡的连接函数类。以前针对未知连接函数情况的工作要么适用于可实现设置，要么无法达到常数近似。使我们的算法和分析成为可能的主要技术要素是我们称之为对齐锐度的新颖优化局部误差界的概念，这可能具有更广泛的兴趣。",
    "tldr": "通过引入对齐锐度技术，我们提出了一种高效的学习算法，实现了单指数模型的常数近似学习，适用于各种分布和连接函数，是首个适用于高斯数据和任何非平凡连接函数类的稳健学习器。",
    "en_tdlr": "Introducing the concept of alignment sharpness, a novel efficient learning algorithm is proposed for single-index models under the $L_2^2$ loss, achieving a constant factor approximation that works for various distributions and link functions, making it the first robust learner applicable to Gaussian data and any nontrivial class of link functions."
}