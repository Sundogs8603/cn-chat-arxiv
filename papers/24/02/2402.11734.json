{
    "title": "Solving Data-centric Tasks using Large Language Models",
    "abstract": "arXiv:2402.11734v1 Announce Type: cross  Abstract: Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic vari",
    "link": "https://arxiv.org/abs/2402.11734",
    "context": "Title: Solving Data-centric Tasks using Large Language Models\nAbstract: arXiv:2402.11734v1 Announce Type: cross  Abstract: Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic vari",
    "path": "papers/24/02/2402.11734.json",
    "total_tokens": 848,
    "translated_title": "使用大型语言模型解决数据中心任务",
    "translated_abstract": "大型语言模型（LLMs）正在迅速取代像StackOverflow这样的帮助论坛，并且对于非专业程序员和最终用户特别有帮助。这些用户通常对数据中心任务感兴趣，例如电子表格操作和数据处理，如果仅通过自然语言描述传达意图而不包含数据，这些任务很难解决。但是，我们如何决定在提示中包含多少数据和哪些数据？本文对回答这个问题做出了两点贡献。首先，我们创建了一个从StackOverflow帖子中获取的操作表格数据的真实NL-to-code任务数据集。其次，我们引入了一种聚类然后选择提示技术，将输入数据中最具代表性的行添加到LLM提示中。我们的实验表明，LLM的性能确实对传递到提示中的数据量敏感，并且对于具有大量语法变体的任务，传递的数据量较大。",
    "tldr": "本文提出了两点贡献：一是创建了一个真实世界的NL-to-code任务数据集，二是引入了一种聚类然后选择提示技术，从输入数据中添加最具代表性的行到LLM提示中。"
}