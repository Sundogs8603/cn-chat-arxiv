{
    "title": "M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation",
    "abstract": "arXiv:2402.11875v1 Announce Type: new  Abstract: Video-grounded dialogue generation (VDG) requires the system to generate a fluent and accurate answer based on multimodal knowledge. However, the difficulty in multimodal knowledge utilization brings serious hallucinations to VDG models in practice. Although previous works mitigate the hallucination in a variety of ways, they hardly take notice of the importance of the multimodal knowledge anchor answer tokens. In this paper, we reveal via perplexity that different VDG models experience varying hallucinations and exhibit diverse anchor tokens. Based on this observation, we propose M2K-VDG, a model-adaptive multimodal knowledge anchor enhancement framework for hallucination reduction. Furthermore, we introduce the counterfactual effect for more accurate anchor token detection. The experimental results on three popular benchmarks exhibit the superiority of our approach over state-of-the-art methods, demonstrating its effectiveness in reduc",
    "link": "https://arxiv.org/abs/2402.11875",
    "context": "Title: M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation\nAbstract: arXiv:2402.11875v1 Announce Type: new  Abstract: Video-grounded dialogue generation (VDG) requires the system to generate a fluent and accurate answer based on multimodal knowledge. However, the difficulty in multimodal knowledge utilization brings serious hallucinations to VDG models in practice. Although previous works mitigate the hallucination in a variety of ways, they hardly take notice of the importance of the multimodal knowledge anchor answer tokens. In this paper, we reveal via perplexity that different VDG models experience varying hallucinations and exhibit diverse anchor tokens. Based on this observation, we propose M2K-VDG, a model-adaptive multimodal knowledge anchor enhancement framework for hallucination reduction. Furthermore, we introduce the counterfactual effect for more accurate anchor token detection. The experimental results on three popular benchmarks exhibit the superiority of our approach over state-of-the-art methods, demonstrating its effectiveness in reduc",
    "path": "papers/24/02/2402.11875.json",
    "total_tokens": 889,
    "translated_title": "M2K-VDG: 模型自适应多模态知识锚增强视频对话生成",
    "translated_abstract": "视频对话生成（VDG）要求系统基于多模态知识生成流畅准确的答案。然而，在实践中，多模态知识利用的困难给VDG模型带来了严重的幻觉。尽管先前的研究以各种方式缓解了幻觉，但它们很少注意多模态知识锚答案标记的重要性。在本文中，我们通过困惑度揭示了不同VDG模型经历不同幻觉并展示不同的锚标记。基于这一观察，我们提出了M2K-VDG，一种模型自适应的多模态知识锚增强框架用于减少幻觉。此外，我们引入了反事实效应以实现更准确的锚标记检测。三个流行基准测试的实验结果展示了我们方法优于最先进方法的优越性，证明了它在减少幻觉中的有效性。",
    "tldr": "通过分析不同VDG模型的幻觉现象和锚标记的差异，本文提出了M2K-VDG框架，用于自适应增强多模态知识锚以减少幻觉，并在实验中表现出优越性。",
    "en_tdlr": "Analyzing hallucination variations and anchor token differences in VDG models, this paper introduces the M2K-VDG framework to adaptively enhance multimodal knowledge anchors for hallucination reduction, demonstrating superiority in experiments."
}