{
    "title": "Minimum Description Length and Generalization Guarantees for Representation Learning",
    "abstract": "A major challenge in designing efficient statistical supervised learning algorithms is finding representations that perform well not only on available training samples but also on unseen data. While the study of representation learning has spurred much interest, most existing such approaches are heuristic; and very little is known about theoretical generalization guarantees.   In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm in terms of the \"Minimum Description Length\" (MDL) of the labels or the latent variables (representations). Rather than the mutual information between the encoder's input and the representation, which is often believed to reflect the algorithm's generalization capability in the related literature but in fact, falls short of doing so, our new bounds involve the \"multi-letter\" relative entropy between the distribution of the representations (or labels) of t",
    "link": "https://arxiv.org/abs/2402.03254",
    "context": "Title: Minimum Description Length and Generalization Guarantees for Representation Learning\nAbstract: A major challenge in designing efficient statistical supervised learning algorithms is finding representations that perform well not only on available training samples but also on unseen data. While the study of representation learning has spurred much interest, most existing such approaches are heuristic; and very little is known about theoretical generalization guarantees.   In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm in terms of the \"Minimum Description Length\" (MDL) of the labels or the latent variables (representations). Rather than the mutual information between the encoder's input and the representation, which is often believed to reflect the algorithm's generalization capability in the related literature but in fact, falls short of doing so, our new bounds involve the \"multi-letter\" relative entropy between the distribution of the representations (or labels) of t",
    "path": "papers/24/02/2402.03254.json",
    "total_tokens": 817,
    "translated_title": "表示学习的最小描述长度和泛化保证",
    "translated_abstract": "设计高效的统计有监督学习算法的一个主要挑战是找到不仅在可用训练样本上表现良好而且在未见数据上也表现良好的表示形式。尽管表示学习的研究引发了许多兴趣，但大多数现有方法都是启发式的；对于理论上的泛化保证几乎没有什么了解。在本文中，我们建立了一个可压缩性框架，使我们能够通过标签或潜在变量（表示形式）的\"最小描述长度\"（MDL）来推导表示学习算法的泛化误差的上界。与通常被认为反映算法泛化能力的编码器输入和表示之间的互信息相比，我们的新界限涉及表示（或标签）分布之间的\"多字母\"相对熵，在相关文献中对算法的泛化能力的反映还不足。",
    "tldr": "本文提出了一个可压缩性框架，通过计算表示学习算法的泛化误差的上界，改进了现有启发式方法，并提供了关于理论泛化保证的新见解。",
    "en_tdlr": "This paper proposes a compressibility framework that improves existing heuristic methods by calculating upper bounds on the generalization error of representation learning algorithms, offering new insights into theoretical generalization guarantees."
}