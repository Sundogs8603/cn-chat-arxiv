{
    "title": "Data-driven Policy Learning for a Continuous Treatment",
    "abstract": "This paper studies policy learning under the condition of unconfoundedness with a continuous treatment variable. Our research begins by employing kernel-based inverse propensity-weighted (IPW) methods to estimate policy welfare. We aim to approximate the optimal policy within a global policy class characterized by infinite Vapnik-Chervonenkis (VC) dimension. This is achieved through the utilization of a sequence of sieve policy classes, each with finite VC dimension. Preliminary analysis reveals that welfare regret comprises of three components: global welfare deficiency, variance, and bias. This leads to the necessity of simultaneously selecting the optimal bandwidth for estimation and the optimal policy class for welfare approximation. To tackle this challenge, we introduce a semi-data-driven strategy that employs penalization techniques. This approach yields oracle inequalities that adeptly balance the three components of welfare regret without prior knowledge of the welfare deficie",
    "link": "https://arxiv.org/abs/2402.02535",
    "context": "Title: Data-driven Policy Learning for a Continuous Treatment\nAbstract: This paper studies policy learning under the condition of unconfoundedness with a continuous treatment variable. Our research begins by employing kernel-based inverse propensity-weighted (IPW) methods to estimate policy welfare. We aim to approximate the optimal policy within a global policy class characterized by infinite Vapnik-Chervonenkis (VC) dimension. This is achieved through the utilization of a sequence of sieve policy classes, each with finite VC dimension. Preliminary analysis reveals that welfare regret comprises of three components: global welfare deficiency, variance, and bias. This leads to the necessity of simultaneously selecting the optimal bandwidth for estimation and the optimal policy class for welfare approximation. To tackle this challenge, we introduce a semi-data-driven strategy that employs penalization techniques. This approach yields oracle inequalities that adeptly balance the three components of welfare regret without prior knowledge of the welfare deficie",
    "path": "papers/24/02/2402.02535.json",
    "total_tokens": 858,
    "translated_title": "基于数据驱动的连续治疗政策学习",
    "translated_abstract": "本研究针对连续治疗变量条件下的政策学习进行了研究。我们采用基于核方法的倒数估计权重(IPW)方法来估计政策福利。我们的目标是在由无穷的Vapnik-Chervonenkis(VC)维度特征的全局政策类中近似最优政策。通过使用一系列具有有限VC维度的筛选政策类，实现了这一目标。初步分析表明，福利损失包括三个组成部分：全局福利不足、方差和偏差。这导致同时选择估计的最优带宽和福利近似的最优政策类成为必要。为了应对这一挑战，我们引入了一种半数据驱动的策略，采用了惩罚技术。这种方法产生了奥拉克不等式，能够在不事先了解福利不足的情况下，灵活平衡福利损失的三个组成部分。",
    "tldr": "本论文研究了在连续治疗条件下的政策学习，通过使用核方法估计政策福利，并引入一种半数据驱动的策略来平衡福利损失的组成部分。",
    "en_tdlr": "This paper investigates policy learning with a continuous treatment variable, using kernel-based methods to estimate policy welfare and employing a semi-data-driven strategy to balance the components of welfare regret."
}