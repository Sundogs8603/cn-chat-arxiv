{
    "title": "MoDE: A Mixture-of-Experts Model with Mutual Distillation among the Experts",
    "abstract": "The application of mixture-of-experts (MoE) is gaining popularity due to its ability to improve model's performance. In an MoE structure, the gate layer plays a significant role in distinguishing and routing input features to different experts. This enables each expert to specialize in processing their corresponding sub-tasks. However, the gate's routing mechanism also gives rise to narrow vision: the individual MoE's expert fails to use more samples in learning the allocated sub-task, which in turn limits the MoE to further improve its generalization ability. To effectively address this, we propose a method called Mixture-of-Distilled-Expert (MoDE), which applies moderate mutual distillation among experts to enable each expert to pick up more features learned by other experts and gain more accurate perceptions on their original allocated sub-tasks. We conduct plenty experiments including tabular, NLP and CV datasets, which shows MoDE's effectiveness, universality and robustness. Furth",
    "link": "https://rss.arxiv.org/abs/2402.00893",
    "context": "Title: MoDE: A Mixture-of-Experts Model with Mutual Distillation among the Experts\nAbstract: The application of mixture-of-experts (MoE) is gaining popularity due to its ability to improve model's performance. In an MoE structure, the gate layer plays a significant role in distinguishing and routing input features to different experts. This enables each expert to specialize in processing their corresponding sub-tasks. However, the gate's routing mechanism also gives rise to narrow vision: the individual MoE's expert fails to use more samples in learning the allocated sub-task, which in turn limits the MoE to further improve its generalization ability. To effectively address this, we propose a method called Mixture-of-Distilled-Expert (MoDE), which applies moderate mutual distillation among experts to enable each expert to pick up more features learned by other experts and gain more accurate perceptions on their original allocated sub-tasks. We conduct plenty experiments including tabular, NLP and CV datasets, which shows MoDE's effectiveness, universality and robustness. Furth",
    "path": "papers/24/02/2402.00893.json",
    "total_tokens": 882,
    "translated_title": "MoDE:一种具有专家间相互蒸馏的混合专家模型",
    "translated_abstract": "由于其提高模型性能的能力，混合专家(MoE)的应用越来越受欢迎。在MoE结构中，门控层在区分和路由输入特征到不同的专家方面起着重要作用。这使得每个专家能够专注于处理他们对应的子任务。然而，门控的路由机制也会导致狭窄的视野：单个MoE的专家无法使用更多的样本来学习分配的子任务，这反过来会限制MoE进一步提高其泛化能力。为了有效解决这个问题，我们提出了一种称为Mixture-of-Distilled-Expert (MoDE)的方法，它在专家之间应用适度的相互蒸馏，使每个专家能够学习其他专家学到的更多特征，并对其原始分配的子任务获得更准确的认知。我们进行了大量的实验证明了MoDE的有效性、通用性和稳健性。",
    "tldr": "MoDE是一种在混合专家模型中应用专家间相互蒸馏的方法，以提高模型的泛化能力。实验证明MoDE在各种数据集上都有效，并且具有普适性和鲁棒性。",
    "en_tdlr": "MoDE is a method that applies mutual distillation among experts in a mixture-of-experts model to enhance the model's generalization ability. Experiments show that MoDE is effective, universal, and robust across various datasets."
}