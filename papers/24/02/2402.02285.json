{
    "title": "SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking",
    "abstract": "In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \\method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\\method} results in $4-5%$ improvement in Join",
    "link": "https://arxiv.org/abs/2402.02285",
    "context": "Title: SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking\nAbstract: In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \\method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\\method} results in $4-5%$ improvement in Join",
    "path": "papers/24/02/2402.02285.json",
    "total_tokens": 946,
    "translated_title": "SynthDST: 少样本对话状态跟踪所需的全部是合成数据",
    "translated_abstract": "在上下文学习中，大型语言模型（LLM）已成为对话状态跟踪（DST）研究的一个有希望的方向。然而，表现最好的上下文学习方法涉及检索和添加类似的示例到提示中，需要访问标记的训练数据。在多个领域和应用中获取这样的训练数据非常耗时、昂贵，有时是不可行的。虽然零样本学习不需要训练数据，但在少样本设置中明显落后。因此，“我们是否可以为任何对话模式有效地生成合成数据，以实现少样本提示？”针对这个问题，我们提出了一个名为\\method的数据生成框架，专门针对DST，利用LLM。我们的方法只需要对话模式和一些手工对话模板，就能合成自然、连贯和流畅的带有DST注释的对话。使用{\\method}的少样本学习结果显示，Join连通率提升了4-5％。",
    "tldr": "SynthDST是一个针对对话状态跟踪设计的数据生成框架，利用合成数据来实现少样本提示，通过使用少量手工对话模板和对话模式，它能够生成自然、连贯和流畅的带有DST注释的对话，并使Join连通率提升4-5％."
}