{
    "title": "C-ICL: Contrastive In-context Learning for Information Extraction",
    "abstract": "arXiv:2402.11254v1 Announce Type: new  Abstract: Recently, there has been increasing interest in exploring the capabilities of advanced large language models (LLMs) in the field of information extraction (IE), specifically focusing on tasks related to named entity recognition (NER) and relation extraction (RE). Although researchers are exploring the use of few-shot information extraction through in-context learning with LLMs, they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process. In this paper, we present c-ICL, a novel few-shot technique that leverages both correct and incorrect sample constructions to create in-context learning demonstrations. This approach enhances the ability of LLMs to extract entities and relations by utilizing prompts that incorporate not only the positive samples but also the reasoning behind them. This method allows for the identi",
    "link": "https://arxiv.org/abs/2402.11254",
    "context": "Title: C-ICL: Contrastive In-context Learning for Information Extraction\nAbstract: arXiv:2402.11254v1 Announce Type: new  Abstract: Recently, there has been increasing interest in exploring the capabilities of advanced large language models (LLMs) in the field of information extraction (IE), specifically focusing on tasks related to named entity recognition (NER) and relation extraction (RE). Although researchers are exploring the use of few-shot information extraction through in-context learning with LLMs, they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process. In this paper, we present c-ICL, a novel few-shot technique that leverages both correct and incorrect sample constructions to create in-context learning demonstrations. This approach enhances the ability of LLMs to extract entities and relations by utilizing prompts that incorporate not only the positive samples but also the reasoning behind them. This method allows for the identi",
    "path": "papers/24/02/2402.11254.json",
    "total_tokens": 826,
    "translated_title": "C-ICL：对比上下文学习用于信息抽取",
    "translated_abstract": "最近，人们越来越感兴趣于探索先进大型语言模型（LLMs）在信息抽取（IE）领域的能力，特别是专注于命名实体识别（NER）和关系提取（RE）相关的任务。尽管研究人员正通过LLMs进行少样本信息抽取的上下文学习，他们往往只专注于使用正确或正向示例来展示，而忽视了将不正确或负向示例纳入学习过程的潜在价值。在本文中，我们提出了C-ICL，一种利用正确和不正确样本构建进行上下文学习示范的新颖少样本技术。这种方法通过利用不仅包含正样本还包含背后推理的提示，增强了LLMs提取实体和关系的能力。",
    "tldr": "C-ICL提出了一种利用正确和不正确样本构建进行上下文学习示范的新颖少样本技术，通过提示不仅包含正样本还包含背后推理，增强了LLMs提取实体和关系的能力。",
    "en_tdlr": "C-ICL introduces a novel few-shot technique that leverages both correct and incorrect sample constructions for in-context learning demonstrations, enhancing the ability of LLMs to extract entities and relations by utilizing prompts that incorporate not only the positive samples but also the reasoning behind them."
}