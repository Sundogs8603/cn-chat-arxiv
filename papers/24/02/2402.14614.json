{
    "title": "Two Counterexamples to \\textit{Tokenization and the Noiseless Channel}",
    "abstract": "arXiv:2402.14614v1 Announce Type: new  Abstract: In \\textit{Tokenization and the Noiseless Channel} \\cite{zouhar-etal-2023-tokenization}, R\\'enyi efficiency is suggested as an intrinsic mechanism for evaluating a tokenizer: for NLP tasks, the tokenizer which leads to the highest R\\'enyi efficiency of the unigram distribution should be chosen. The R\\'enyi efficiency is thus treated as a predictor of downstream performance (e.g., predicting BLEU for a machine translation task), without the expensive step of training multiple models with different tokenizers. Although useful, the predictive power of this metric is not perfect, and the authors note there are additional qualities of a good tokenization scheme that R\\'enyi efficiency alone cannot capture.   We describe two variants of BPE tokenization which can arbitrarily increase R\\'enyi efficiency while decreasing the downstream model performance. These counterexamples expose cases where R\\'enyi efficiency fails as an intrinsic tokenizati",
    "link": "https://arxiv.org/abs/2402.14614",
    "context": "Title: Two Counterexamples to \\textit{Tokenization and the Noiseless Channel}\nAbstract: arXiv:2402.14614v1 Announce Type: new  Abstract: In \\textit{Tokenization and the Noiseless Channel} \\cite{zouhar-etal-2023-tokenization}, R\\'enyi efficiency is suggested as an intrinsic mechanism for evaluating a tokenizer: for NLP tasks, the tokenizer which leads to the highest R\\'enyi efficiency of the unigram distribution should be chosen. The R\\'enyi efficiency is thus treated as a predictor of downstream performance (e.g., predicting BLEU for a machine translation task), without the expensive step of training multiple models with different tokenizers. Although useful, the predictive power of this metric is not perfect, and the authors note there are additional qualities of a good tokenization scheme that R\\'enyi efficiency alone cannot capture.   We describe two variants of BPE tokenization which can arbitrarily increase R\\'enyi efficiency while decreasing the downstream model performance. These counterexamples expose cases where R\\'enyi efficiency fails as an intrinsic tokenizati",
    "path": "papers/24/02/2402.14614.json",
    "total_tokens": 879,
    "translated_title": "《Tokenization and the Noiseless Channel》的两个反例",
    "translated_abstract": "在《Tokenization and the Noiseless Channel》中，建议使用Rényi效率作为评估分词器的固有机制: 对于NLP任务，应选择导致unigram分布Rényi效率最高的分词器。因此，Rényi效率被视为下游性能的预测器（例如，用于预测机器翻译任务的BLEU），而无需通过训练不同分词器的多个模型这一昂贵的步骤。尽管有用，但这一度量标准的预测能力并不完美，作者指出有其他优秀分词方案的附加特质Rényi效率本身无法捕捉。我们描述了两种BPE分词的变体，可以在增加Rényi效率的同时降低下游模型性能。这些反例揭示了Rényi效率作为固有分词的情况下存在失败的情况。",
    "tldr": "该论文讨论了在《Tokenization and the Noiseless Channel》提出的使用Rényi效率作为分词器评估机制的局限性，并描述了两个BPE分词的反例，展示了Rényi效率无法捕捉到所有优秀分词方案的情况。",
    "en_tdlr": "The paper discusses the limitations of using Rényi efficiency as a tokenizer evaluation mechanism proposed in \"Tokenization and the Noiseless Channel\", and presents two counterexamples of BPE tokenization variants that demonstrate cases where Rényi efficiency fails to capture all characteristics of good tokenization schemes."
}