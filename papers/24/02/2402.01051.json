{
    "title": "Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",
    "abstract": "Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we f",
    "link": "https://rss.arxiv.org/abs/2402.01051",
    "context": "Title: Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model\nAbstract: Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we f",
    "path": "papers/24/02/2402.01051.json",
    "total_tokens": 872,
    "translated_title": "生成、提炼和评估具有基础语言模型风格的动机式反思",
    "translated_abstract": "大型基础语言模型能够在许多任务上高效执行，但由于其庞大的体积和专有所有权，在许多应用中很难部署。许多人会希望将基础模型的特定功能提炼成较小的模型，以便拥有和控制。在开发治疗性聊天机器人时，我们希望提炼一种称为反思倾听的能力，即治疗师生成对客户讲话的反思。这些反思要么重新陈述客户说过的话，要么将其与相关观察、思想或猜测联系起来，以鼓励和引导客户继续思考。在本文中，我们提出了一种从基础语言模型（GPT-4）中提炼反思生成的方法。我们首先展示了使用零样本提示，GPT-4可以以将近100%的成功率生成反思，优于所有先前的方法。使用由GPT-4生成的反思，我们进行了评估",
    "tldr": "本文介绍了一种将基础语言模型中的反思生成能力提炼到较小模型中的方法，并且展示了该方法在生成反思方面的优越性能。",
    "en_tdlr": "This paper presents a method for distilling the capability of generating reflections from a large foundational language model into smaller models, and demonstrates the superior performance of this method in generating reflections."
}