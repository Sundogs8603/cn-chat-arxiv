{
    "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning",
    "abstract": "Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the finetuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya A",
    "link": "https://arxiv.org/abs/2402.06619",
    "context": "Title: Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning\nAbstract: Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the finetuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya A",
    "path": "papers/24/02/2402.06619.json",
    "total_tokens": 897,
    "translated_title": "Aya数据集：用于多语言指令调优的开放访问收藏品",
    "translated_abstract": "数据集是现代人工智能中许多突破的基础。自然语言处理（NLP）领域的许多最近的成就都归功于在多样化任务上对预训练模型进行微调，使得大型语言模型能够响应指令。指令微调（IFT）需要特别构建和注释的数据集。然而，现有的数据集几乎都是以英语为主。在这项工作中，我们的首要目标是通过构建跨越65种语言的人工筛选的指令遵循数据集来弥合语言差距。我们与来自世界各地的流利说者合作，收集指令和完成的自然实例。此外，我们通过在114种语言之间进行模板化和翻译现有数据集，创造了迄今为止规模最大的多语言收藏品，共有5.13亿个实例。总而言之，我们提供了四个关键资源：我们开发并开源了Aya数据集，通过模板化和翻译现有的数据集进行扩展，并将其跨越了114种语言。",
    "tldr": "本研究的主要目标是通过人工筛选的指令遵循数据集来弥合不同语言之间的差距，并且创造了迄今为止最大的多语言收藏品，包括513亿个实例。",
    "en_tdlr": "The primary goal of this study is to bridge the language gap by curating an instruction-following dataset and to create the largest multilingual collection to date, consisting of 5.13 billion instances."
}