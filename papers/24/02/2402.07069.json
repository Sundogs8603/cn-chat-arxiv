{
    "title": "Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine",
    "abstract": "We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-R",
    "link": "https://arxiv.org/abs/2402.07069",
    "context": "Title: Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine\nAbstract: We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-R",
    "path": "papers/24/02/2402.07069.json",
    "total_tokens": 916,
    "translated_title": "使用大型语言模型自动化和加速奖励机器强化学习",
    "translated_abstract": "我们提出了LARL-RM（通过大型语言模型生成的用于奖励机器强化学习的自动机）算法，以将高级知识编码到强化学习中，使用自动机加速强化学习过程。我们的方法使用大型语言模型（LLM）通过提示工程获得高级领域特定知识，而不是直接将高级知识提供给强化学习算法，这需要专家来编码自动机。我们使用思维链和少样本方法进行提示工程，并证明了我们的方法在这些方法下有效。此外，LARL-RM允许完全闭环的强化学习，无需专家来指导和监督学习，因为LARL-RM可以直接使用LLM生成所需的高级知识以完成任务。我们还证明了算法收敛到最优策略的理论保证。我们证明了LARL-RM的实验结果展示了其对常见的强化学习问题具有非常好的性能，并且在一些任务上超越了目前最先进的方法。",
    "tldr": "这篇论文介绍了一种使用大型语言模型自动生成自动机来编码高级知识，加速强化学习过程的算法，并证明了其在多个任务上的有效性和优越性。",
    "en_tdlr": "This paper presents an algorithm, LARL-RM, that uses large language models to automate the encoding of high-level knowledge into reinforcement learning using automaton, resulting in accelerated learning. The algorithm is able to generate the required high-level knowledge directly from the language model, eliminating the need for expert guidance and supervision. The experimental results demonstrate the effectiveness and superiority of the algorithm in various tasks."
}