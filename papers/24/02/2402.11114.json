{
    "title": "Whose Emotions and Moral Sentiments Do Language Models Reflect?",
    "abstract": "arXiv:2402.11114v1 Announce Type: new  Abstract: Language models (LMs) are known to represent the perspectives of some social groups better than others, which may impact their performance, especially on subjective tasks such as content moderation and hate speech detection. To explore how LMs represent different perspectives, existing research focused on positional alignment, i.e., how closely the models mimic the opinions and stances of different groups, e.g., liberals or conservatives. However, human communication also encompasses emotional and moral dimensions. We define the problem of affective alignment, which measures how LMs' emotional and moral tone represents those of different groups. By comparing the affect of responses generated by 36 LMs to the affect of Twitter messages, we observe significant misalignment of LMs with both ideological groups. This misalignment is larger than the partisan divide in the U.S. Even after steering the LMs towards specific ideological perspectiv",
    "link": "https://arxiv.org/abs/2402.11114",
    "context": "Title: Whose Emotions and Moral Sentiments Do Language Models Reflect?\nAbstract: arXiv:2402.11114v1 Announce Type: new  Abstract: Language models (LMs) are known to represent the perspectives of some social groups better than others, which may impact their performance, especially on subjective tasks such as content moderation and hate speech detection. To explore how LMs represent different perspectives, existing research focused on positional alignment, i.e., how closely the models mimic the opinions and stances of different groups, e.g., liberals or conservatives. However, human communication also encompasses emotional and moral dimensions. We define the problem of affective alignment, which measures how LMs' emotional and moral tone represents those of different groups. By comparing the affect of responses generated by 36 LMs to the affect of Twitter messages, we observe significant misalignment of LMs with both ideological groups. This misalignment is larger than the partisan divide in the U.S. Even after steering the LMs towards specific ideological perspectiv",
    "path": "papers/24/02/2402.11114.json",
    "total_tokens": 747,
    "translated_title": "语言模型反映了谁的情感和道德情感？",
    "translated_abstract": "语言模型已知更好地代表一些社会群体的观点，这可能会影响它们的性能，特别是在主观任务上，比如内容管理和仇恨言论检测。我们定义了情感对齐的问题，用来衡量语言模型的情感和道德色调如何代表不同群体的情感。通过比较36个语言模型生成的回应的情感与Twitter消息的情感，我们观察到语言模型与两种意识形态团体存在显著不一致。即使在引导语言模型朝着特定意识形态方向发展之后，这种不一致性也仍然存在。",
    "tldr": "该研究探讨了语言模型在情感和道德维度上如何代表不同群体，发现它们与意识形态团体存在显著的不一致性。",
    "en_tdlr": "This research examines how language models represent different groups in terms of emotional and moral dimensions, revealing significant misalignment with ideological groups."
}