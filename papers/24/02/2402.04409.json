{
    "title": "Towards Fair, Robust and Efficient Client Contribution Evaluation in Federated Learning",
    "abstract": "The performance of clients in Federated Learning (FL) can vary due to various reasons. Assessing the contributions of each client is crucial for client selection and compensation. It is challenging because clients often have non-independent and identically distributed (non-iid) data, leading to potentially noisy or divergent updates. The risk of malicious clients amplifies the challenge especially when there's no access to clients' local data or a benchmark root dataset. In this paper, we introduce a novel method called Fair, Robust, and Efficient Client Assessment (FRECA) for quantifying client contributions in FL. FRECA employs a framework called FedTruth to estimate the global model's ground truth update, balancing contributions from all clients while filtering out impacts from malicious ones. This approach is robust against Byzantine attacks and incorporates a Byzantine-resilient aggregation algorithm. FRECA is also efficient, as it operates solely on local model updates and requir",
    "link": "https://arxiv.org/abs/2402.04409",
    "context": "Title: Towards Fair, Robust and Efficient Client Contribution Evaluation in Federated Learning\nAbstract: The performance of clients in Federated Learning (FL) can vary due to various reasons. Assessing the contributions of each client is crucial for client selection and compensation. It is challenging because clients often have non-independent and identically distributed (non-iid) data, leading to potentially noisy or divergent updates. The risk of malicious clients amplifies the challenge especially when there's no access to clients' local data or a benchmark root dataset. In this paper, we introduce a novel method called Fair, Robust, and Efficient Client Assessment (FRECA) for quantifying client contributions in FL. FRECA employs a framework called FedTruth to estimate the global model's ground truth update, balancing contributions from all clients while filtering out impacts from malicious ones. This approach is robust against Byzantine attacks and incorporates a Byzantine-resilient aggregation algorithm. FRECA is also efficient, as it operates solely on local model updates and requir",
    "path": "papers/24/02/2402.04409.json",
    "total_tokens": 996,
    "translated_title": "进一步实现公平、鲁棒和高效的联邦学习中客户贡献评估",
    "translated_abstract": "由于各种原因，联邦学习（FL）中客户的性能可能会有所不同。评估每个客户的贡献对于客户选择和补偿至关重要。然而，由于客户通常具有非独立和同分布（non-iid）的数据，导致可能存在噪声或发散的更新，因此评估客户贡献具有挑战性。当无法访问客户的本地数据或基准根数据集时，恶意客户的风险会进一步增加。本文介绍了一种名为公平、鲁棒和高效客户评估（FRECA）的新方法，用于量化FL中的客户贡献。FRECA采用一种名为FedTruth的框架来估计全局模型的真实更新，平衡来自所有客户的贡献，并过滤出恶意客户的影响。该方法对拜占庭攻击具有鲁棒性，并采用了拜占庭鲁棒的聚合算法。FRECA还具有高效性，因为它仅仅在本地模型更新上操作，且只需要少量的全局通信。",
    "tldr": "本文提出了一种名为FRECA的方法来评估联邦学习中客户的贡献。该方法使用FedTruth框架估计全局模型的真实更新，平衡来自所有客户的贡献，并排除恶意客户的影响。FRECA对于拜占庭攻击具有鲁棒性，并且具有高效性。",
    "en_tdlr": "This paper introduces a method called FRECA for evaluating client contributions in Federated Learning. FRECA uses the FedTruth framework to estimate the global model's true update, balances contributions from all clients, and filters out the impacts of malicious clients. FRECA is robust against Byzantine attacks and is efficient."
}