{
    "title": "Small But Funny: A Feedback-Driven Approach to Humor Distillation",
    "abstract": "arXiv:2402.18113v1 Announce Type: cross  Abstract: The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a \"teacher\" generating data, as well as a \"critic\" evaluating the student's performance. Our ex",
    "link": "https://arxiv.org/abs/2402.18113",
    "context": "Title: Small But Funny: A Feedback-Driven Approach to Humor Distillation\nAbstract: arXiv:2402.18113v1 Announce Type: cross  Abstract: The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a \"teacher\" generating data, as well as a \"critic\" evaluating the student's performance. Our ex",
    "path": "papers/24/02/2402.18113.json",
    "total_tokens": 909,
    "translated_title": "小而有趣：一种基于反馈的幽默精馏方法",
    "translated_abstract": "大型语言模型（LLMs）的出现揭示了有潜力的语言生成能力，特别是在执行诸如复杂推理和创意写作之类的任务方面。因此，通过模仿教师回答的方式进行精馏已经成为一种流行的技术，用于将LLMs中的知识转移到更易访问的小型语言模型（SLMs）中。虽然这对于简单任务效果很好，但在需要复杂语言理解和创造力的任务上存在实质性的表现差距，比如幽默生成。我们假设这种差距可能源自于创造性任务可能单凭模仿学习是很难的，并探讨一种涉及到教师额外指导的方法，能否产生更高的性能。为了解决这个问题，我们研究了将LLM分配双重角色的效果-作为生成数据的“教师”，以及作为评估学生表现的“评论家”。我们的实验结果表明，这种方法在幽默生成任务上确实产生了更高的性能。",
    "tldr": "通过将大型语言模型分配为“教师”生成数据和“评论家”评估学生表现的双重角色，研究表明这种基于反馈的方法在幽默生成任务中取得了更高的性能。",
    "en_tdlr": "The feedback-driven approach, assigning a dual role to the Large Language Models (LLMs) as a \"teacher\" generating data and as a \"critic\" evaluating the student's performance, demonstrated higher performance in humor generation tasks."
}