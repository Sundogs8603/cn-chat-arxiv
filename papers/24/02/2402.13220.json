{
    "title": "How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts",
    "abstract": "arXiv:2402.13220v1 Announce Type: cross  Abstract: The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our exper",
    "link": "https://arxiv.org/abs/2402.13220",
    "context": "Title: How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts\nAbstract: arXiv:2402.13220v1 Announce Type: cross  Abstract: The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our exper",
    "path": "papers/24/02/2402.13220.json",
    "total_tokens": 969,
    "translated_title": "有多容易欺骗多模态LLMs？关于欺骗性提示的实证分析",
    "translated_abstract": "多模态大型语言模型（MLLMs）的显著进展并没有使它们免疫各种挑战，特别是在处理带有欺骗性信息的提示时，会产生幻觉般的回应。为了定量评估这种脆弱性，我们提出了MAD-Bench，一个精心策划的基准测试，包含850个测试样本，分为6个类别，如不存在的对象、对象数量、空间关系和视觉混淆。我们对流行的MLLMs进行了全面分析，包括GPT-4V、Gemini-Pro，以及开源模型，如LLaVA-1.5和CogVLM。实证研究中，我们观察到GPT-4V和其他模型之间存在着显著的性能差距；之前的鲁棒指令调整模型，如LRV-Instruction和LLaVA-RLHF，在这个新基准测试中并不有效。虽然GPT-4V在MAD-Bench上取得了75.02%的准确率，但其他任何模型在我们的实验中都没有达到这一水平。",
    "tldr": "对多模态LLMs的欺骗性提示进行了实证分析，提出包含850个测试样本的基准测试MAD-Bench，发现GPT-4V在该基准测试上准确率较高，而其他模型性能差距显著。",
    "en_tdlr": "An empirical analysis on deceptive prompts for Multimodal Large Language Models (MLLMs) is conducted, introducing the benchmark MAD-Bench with 850 test samples, revealing significant performance gaps between GPT-4V and other models in accuracy."
}