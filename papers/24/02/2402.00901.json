{
    "title": "Real Sparks of Artificial Intelligence and the Importance of Inner Interpretability",
    "abstract": "The present paper looks at one of the most thorough articles on the intelligence of GPT, research conducted by engineers at Microsoft. Although there is a great deal of value in their work, I will argue that, for familiar philosophical reasons, their methodology, !Blackbox Interpretability\"#is wrongheaded. But there is a better way. There is an exciting and emerging discipline of !Inner Interpretability\"#(and specifically Mechanistic Interpretability) that aims to uncover the internal activations and weights of models in order to understand what they represent and the algorithms they implement. In my view, a crucial mistake in Black-box Interpretability is the failure to appreciate that how processes are carried out matters when it comes to intelligence and understanding. I can#t pretend to have a full story that provides both necessary and sufficient conditions for being intelligent, but I do think that Inner Interpretability dovetails nicely with plausible philosophical views of what",
    "link": "https://rss.arxiv.org/abs/2402.00901",
    "context": "Title: Real Sparks of Artificial Intelligence and the Importance of Inner Interpretability\nAbstract: The present paper looks at one of the most thorough articles on the intelligence of GPT, research conducted by engineers at Microsoft. Although there is a great deal of value in their work, I will argue that, for familiar philosophical reasons, their methodology, !Blackbox Interpretability\"#is wrongheaded. But there is a better way. There is an exciting and emerging discipline of !Inner Interpretability\"#(and specifically Mechanistic Interpretability) that aims to uncover the internal activations and weights of models in order to understand what they represent and the algorithms they implement. In my view, a crucial mistake in Black-box Interpretability is the failure to appreciate that how processes are carried out matters when it comes to intelligence and understanding. I can#t pretend to have a full story that provides both necessary and sufficient conditions for being intelligent, but I do think that Inner Interpretability dovetails nicely with plausible philosophical views of what",
    "path": "papers/24/02/2402.00901.json",
    "total_tokens": 846,
    "translated_title": "人工智能的真正火花与内部可解释性的重要性",
    "translated_abstract": "本文研究了微软工程师对GPT智能的研究中最详细的文章之一。虽然他们的工作具有很大的价值，但我认为出于熟悉的哲学原因，他们的方法论“黑盒解释性”是错误的。但是还有一种更好的方法。有一种令人兴奋且新兴的学科，即“内部可解释性”（具体而言是机械解释性），旨在揭示模型的内部激活和权重，以便理解它们代表什么以及它们实现的算法。在我看来，黑盒解释性的一个重要错误是未能意识到过程的执行方式对于智能和理解至关重要。我不能假装有一个能同时提供必要和充分条件的完整故事来解释智能的存在，但我确实认为内部可解释性与合理的哲学观点相吻合。",
    "tldr": "本文研究了微软工程师对GPT智能的研究，认为黑盒解释性方法论是错误的，提出了内部可解释性的重要性并与哲学观点相吻合。",
    "en_tdlr": "This paper examines the research on GPT intelligence conducted by Microsoft engineers, arguing that the blackbox interpretability approach is flawed and highlighting the importance of inner interpretability, which aligns with plausible philosophical views."
}