{
    "title": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models",
    "abstract": "arXiv:2402.16775v1 Announce Type: new  Abstract: Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \\& capacity, (2) alignment, and (3) efficiency, and conduct extensive experi",
    "link": "https://arxiv.org/abs/2402.16775",
    "context": "Title: A Comprehensive Evaluation of Quantization Strategies for Large Language Models\nAbstract: arXiv:2402.16775v1 Announce Type: new  Abstract: Increasing the number of parameters in large language models (LLMs) usually improves performance in downstream tasks but raises compute and memory costs, making deployment difficult in resource-limited settings. Quantization techniques, which reduce the bits needed for model weights or activations with minimal performance loss, have become popular due to the rise of LLMs. However, most quantization studies use pre-trained LLMs, and the impact of quantization on instruction-tuned LLMs and the relationship between perplexity and benchmark performance of quantized LLMs are not well understood. Evaluation of quantized LLMs is often limited to language modeling and a few classification tasks, leaving their performance on other benchmarks unclear. To address these gaps, we propose a structured evaluation framework consisting of three critical dimensions: (1) knowledge \\& capacity, (2) alignment, and (3) efficiency, and conduct extensive experi",
    "path": "papers/24/02/2402.16775.json",
    "total_tokens": 840,
    "translated_title": "大型语言模型量化策略的全面评估",
    "translated_abstract": "增加大型语言模型（LLMs）中的参数数量通常会在下游任务中提高性能，但会增加计算和存储成本，在资源有限的情况下部署变得困难。量化技术通过减少模型权重或激活所需的位数，并最小化性能损失，已经因LLMs的兴起而变得流行。然而，大多数量化研究使用预训练的LLMs，量化对调整过指令的LLMs的影响以及量化LLMs的困惑度与基准性能之间的关系尚不明确。对量化LLMs的评估通常仅限于语言建模和少数分类任务，其在其他基准上的性能尚不清楚。为填补这些空白，我们提出了一个结构化评估框架，包括三个关键维度：（1）知识和容量，（2）对齐性和（3）效率，并进行了广泛的实验。",
    "tldr": "该研究提出了一个结构化评估框架，针对大型语言模型的量化策略展开全面评估，填补了现有研究中的空白。",
    "en_tdlr": "This study proposes a structured evaluation framework for comprehensive assessment of quantization strategies for large language models, addressing gaps in existing research."
}