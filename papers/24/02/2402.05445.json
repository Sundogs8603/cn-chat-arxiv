{
    "title": "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention",
    "abstract": "The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the ",
    "link": "https://arxiv.org/abs/2402.05445",
    "context": "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\nAbstract: The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the ",
    "path": "papers/24/02/2402.05445.json",
    "total_tokens": 877,
    "translated_title": "准确的LLMs的LoRA-Finetuning量化通过信息保留",
    "translated_abstract": "将LLMs的LoRA-finetuning量化研究得到准确但紧凑的LLMs以便在资源受限的硬件上部署。然而，现有的方法导致量化的LLMs严重退化，甚至无法从LoRA的调优中获益。本文提出了一种新颖的IR-QLoRA，通过信息保留推动带有LoRA的量化LLMs变得高度准确。所提出的IR-QLoRA主要依赖于两种从统一信息视角派生的技术：（1）基于统计的信息校准量化允许LLMs的量化参数精确保留原始信息；（2）基于调优的信息弹性连接使LoRA利用具有多样信息的弹性表示转换。综合实验证明，在2-4位宽下，IR-QLoRA可以显著提高LLaMA和LLaMA2系列的准确性，例如，4位LLaMA-7B相比于...",
    "tldr": "本文提出了一种通过信息保留推动量化LLMs的LoRA-Finetuning的方法IR-QLoRA，使用统计信息校准和调优信息弹性连接来提高模型的准确性。"
}