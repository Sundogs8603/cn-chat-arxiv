{
    "title": "ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs",
    "abstract": "Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing ReLU$^2$ excel across all t",
    "link": "https://arxiv.org/abs/2402.03804",
    "context": "Title: ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs\nAbstract: Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing ReLU$^2$ excel across all t",
    "path": "papers/24/02/2402.03804.json",
    "total_tokens": 929,
    "translated_title": "ReLU^2获胜：发现稀疏LLM的高效激活函数",
    "translated_abstract": "通过动态跳过非活跃神经元的计算，稀疏计算为低资源场景中的大型语言模型（LLM）的推断提供了一种引人注目的解决方案。虽然传统方法注重基于ReLU的LLM，利用激活值中的零，但我们将稀疏LLM的范围扩大到零激活值之外。我们引入了一种通用方法，通过神经元输出幅度和定制的幅度阈值来定义神经元激活，并证明非ReLU LLM也表现出稀疏激活。为了找到最高效的稀疏计算激活函数，我们提出了一个系统框架，从三个方面考察LLM的稀疏性：稀疏性与性能之间的权衡、稀疏性的预测性和硬件亲和性。我们对使用不同激活函数的LLM进行了彻底的实验，包括ReLU, SwiGLU, ReGLU 和 ReLU^2。结果表明，采用ReLU^2的模型在所有方面表现出色。",
    "tldr": "通过动态跳过非活跃神经元的计算，我们提出了一种稀疏LLM的高效激活函数ReLU^2，它在稀疏性与性能、稀疏性的预测性和硬件亲和性等方面表现出色。",
    "en_tdlr": "We propose an efficient activation function ReLU^2 for sparse LLMs, which dynamically skips the computation of inactive neurons and excels in terms of sparsity-performance trade-off, sparsity predictivity, and hardware affinity."
}