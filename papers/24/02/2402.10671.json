{
    "title": "Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm",
    "abstract": "arXiv:2402.10671v1 Announce Type: new  Abstract: In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model's attention. Additionally, the inclusion of self-correcting and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches. Extensive experiments conducted on three da",
    "link": "https://arxiv.org/abs/2402.10671",
    "context": "Title: Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL through Workflow Paradigm\nAbstract: arXiv:2402.10671v1 Announce Type: new  Abstract: In-context learning of large-language models (LLMs) has achieved remarkable success in the field of natural language processing, while extensive case studies reveal that the single-step chain-of-thought prompting approach faces challenges such as attention diffusion and inadequate performance in complex tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the attention and problem-solving scope of LLMs through decomposition. Specifically, the information determination module for eliminating redundant information and the brand-new prompt structure based on problem classification greatly enhance the model's attention. Additionally, the inclusion of self-correcting and active learning modules greatly expands the problem-solving scope of LLMs, hence improving the upper limit of LLM-based approaches. Extensive experiments conducted on three da",
    "path": "papers/24/02/2402.10671.json",
    "total_tokens": 845,
    "translated_title": "通过分解来增强注意力：通过工作流范式改进基于LLM的文本到SQL转换",
    "translated_abstract": "大语言模型（LLMs）的上下文学习在自然语言处理领域取得了显著成功，而广泛的案例研究表明，单步链式思维提示方法在复杂任务（如文本到SQL）中面临注意力扩散和性能不足等挑战。为了改善LLMs在文本到SQL中的上下文学习能力，提出了一种工作流范式方法，旨在通过分解增强LLMs的注意力和问题解决范围。具体来说，用于消除冗余信息的信息确定模块和基于问题分类的全新提示结构极大增强了模型的注意力。此外，引入自校正和主动学习模块极大扩展了LLMs的问题解决范围，从而提高了基于LLM方法的上限。在三个数据集上进行了大量实验。",
    "tldr": "提出了一种通过工作流范式方法来改善LLMs在文本到SQL中的上下文学习能力，通过分解提高了模型的注意力和问题解决范围，进一步提高了基于LLM的方法的上限。",
    "en_tdlr": "Proposed a workflow paradigm method to enhance the contextual learning capabilities of LLMs in text-to-SQL, improving the model's attention and problem-solving scope through decomposition, further enhancing the upper limit of LLM-based approaches."
}