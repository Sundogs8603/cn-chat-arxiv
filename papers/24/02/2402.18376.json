{
    "title": "Tokenization Is More Than Compression",
    "abstract": "arXiv:2402.18376v1 Announce Type: cross  Abstract: Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of ",
    "link": "https://arxiv.org/abs/2402.18376",
    "context": "Title: Tokenization Is More Than Compression\nAbstract: arXiv:2402.18376v1 Announce Type: cross  Abstract: Tokenization is a foundational step in Natural Language Processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of ",
    "path": "papers/24/02/2402.18376.json",
    "total_tokens": 813,
    "translated_title": "Tokenization超越了压缩",
    "translated_abstract": "Tokenization是自然语言处理（NLP）任务中的基础步骤，它连接了原始文本和语言模型。现有的Tokenization方法，如字节对编码（Byte-Pair Encoding，BPE），源自数据压缩领域，并有人认为BPE的有效性源于其将文本压缩为相对较少的标记的能力。我们通过引入PathPiece来测试“更少的标记是否会导致更好的下游性能”这一假设，PathPiece是一种新的分词器，根据给定词汇将文档文本划分为最少数量的标记。通过广泛实验，我们发现这一假设并非成立，对有效Tokenization原因的理解产生了疑问。为了检查哪些其他因素起到作用，我们评估了Tokenization的所有三个阶段（预分词、词汇构造和分割）的设计决策，提供了关于设计的新见解。",
    "tldr": "通过引入新的分词器PathPiece，研究者发现少量标记并不能导致更好的下游性能，这一结果对于 Tokenization 的有效性理解提出了质疑。",
    "en_tdlr": "By introducing the new tokenizer PathPiece, researchers found that a smaller number of tokens does not necessarily lead to better downstream performance, casting doubt on the understanding of the effectiveness of Tokenization."
}