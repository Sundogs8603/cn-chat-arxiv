{
    "title": "InstanceDiffusion: Instance-level Control for Image Generation",
    "abstract": "Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\\text{box}$ for box inputs, ",
    "link": "https://arxiv.org/abs/2402.03290",
    "context": "Title: InstanceDiffusion: Instance-level Control for Image Generation\nAbstract: Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\\text{box}$ for box inputs, ",
    "path": "papers/24/02/2402.03290.json",
    "total_tokens": 819,
    "translated_title": "InstanceDiffusion：图像生成的实例级控制",
    "translated_abstract": "文本到图像的扩散模型可以生成高质量图像，但不能对图像中的单个实例进行精确控制。我们引入InstanceDiffusion，将精确的实例级控制添加到文本到图像的扩散模型中。InstanceDiffusion支持每个实例的自由形式语言条件，并允许以简单的单个点、涂鸦、边界框或复杂的实例分割掩码及其组合方式指定实例位置。我们提出了三个重要的改进文本到图像模型的方法，以实现精确的实例级控制。我们的UniFusion块实现了文本到图像模型的实例级条件，ScaleU块提高了图像的保真度，Multi-instance采样器提高了多实例的生成效果。InstanceDiffusion在每个位置条件下明显超过了专业先进模型。值得注意的是，对于COCO数据集，我们在box输入方面超过了以前的最先进技术20.4%AP50 box。",
    "tldr": "InstanceDiffusion通过添加实例级控制，使文本到图像的扩散模型能够产生高质量图像，并在不同的位置条件下超过了专业先进模型。"
}