{
    "title": "Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman equations",
    "abstract": "arXiv:2402.15285v1 Announce Type: cross  Abstract: Sampling from probability densities is a common challenge in fields such as Uncertainty Quantification (UQ) and Generative Modelling (GM). In GM in particular, the use of reverse-time diffusion processes depending on the log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling tool. In Berner et al. [2022] the authors point out that these log-densities can be obtained by solution of a \\textit{Hamilton-Jacobi-Bellman} (HJB) equation known from stochastic optimal control. While this HJB equation is usually treated with indirect methods such as policy iteration and unsupervised training of black-box architectures like Neural Networks, we propose instead to solve the HJB equation by direct time integration, using compressed polynomials represented in the Tensor Train (TT) format for spatial discretization. Crucially, this method is sample-free, agnostic to normalization constants and can avoid the curse of dimensionalit",
    "link": "https://arxiv.org/abs/2402.15285",
    "context": "Title: Generative Modelling with Tensor Train approximations of Hamilton--Jacobi--Bellman equations\nAbstract: arXiv:2402.15285v1 Announce Type: cross  Abstract: Sampling from probability densities is a common challenge in fields such as Uncertainty Quantification (UQ) and Generative Modelling (GM). In GM in particular, the use of reverse-time diffusion processes depending on the log-densities of Ornstein-Uhlenbeck forward processes are a popular sampling tool. In Berner et al. [2022] the authors point out that these log-densities can be obtained by solution of a \\textit{Hamilton-Jacobi-Bellman} (HJB) equation known from stochastic optimal control. While this HJB equation is usually treated with indirect methods such as policy iteration and unsupervised training of black-box architectures like Neural Networks, we propose instead to solve the HJB equation by direct time integration, using compressed polynomials represented in the Tensor Train (TT) format for spatial discretization. Crucially, this method is sample-free, agnostic to normalization constants and can avoid the curse of dimensionalit",
    "path": "papers/24/02/2402.15285.json",
    "total_tokens": 919,
    "translated_title": "使用张量矩阵逼近哈密尔顿-雅各比-贝尔曼方程的生成建模",
    "translated_abstract": "从概率密度中进行采样在不确定性量化（UQ）和生成建模（GM）等领域中是一项常见挑战。 在GM中，特别流行的采样工具是依赖于Ornstein-Uhlenbeck正向过程的对数密度的逆时间扩散过程。 在Berner等人[2022]中，作者指出这些对数密度可以通过解决源自随机最优控制的哈密尔顿-雅各比-贝尔曼（HJB）方程来获得。 虽然这个HJB方程通常使用间接方法来处理，比如政策迭代和对神经网络这样的黑匣子架构进行无监督训练，我们提出通过直接时间积分来解决HJB方程，使用张量矩阵（TT）格式的压缩多项式进行空间离散化。 这种方法没有样本需求，不依赖于归一化常数，并且可以避免维数灾难。",
    "tldr": "使用张量矩阵逼近哈密尔顿-雅各比-贝尔曼方程，提出了一种在生成建模中解决HJB方程的新方法，该方法无需样本，不依赖于归一化常数，并能避免维数灾难。",
    "en_tdlr": "Proposing a novel method to solve the Hamilton-Jacobi-Bellman equation in generative modeling using tensor train approximations, which is sample-free, agnostic to normalization constants, and can avoid the curse of dimensionality."
}