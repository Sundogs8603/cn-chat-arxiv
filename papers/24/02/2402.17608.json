{
    "title": "Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)",
    "abstract": "arXiv:2402.17608v1 Announce Type: new  Abstract: In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task. In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.",
    "link": "https://arxiv.org/abs/2402.17608",
    "context": "Title: Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)\nAbstract: arXiv:2402.17608v1 Announce Type: new  Abstract: In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task. In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.",
    "path": "papers/24/02/2402.17608.json",
    "total_tokens": 779,
    "translated_title": "语言知识可以增强编码器-解码器模型（如果你允许的话）",
    "translated_abstract": "在本文中，我们探讨了如何通过增加预先训练的编码器-解码器模型，特别是T5模型，与语言知识来预测目标任务的影响。具体而言，我们调查了在中间任务上微调T5模型，该任务预测句子的结构语言属性，是否会改变其在预测句子级复杂度的目标任务中的表现。我们的研究涵盖了在意大利语和英语数据集上进行的各种实验，采用了不同规模的单语和多语T5模型。对两种语言以及跨语言配置的结果表明，基于语言学动机的中间微调通常对目标任务的性能产生积极影响，尤其是当应用于较小模型和数据有限的情况下。",
    "tldr": "通过向T5模型引入语言知识，特别是在结构语言属性的中间任务上进行微调，可以改善对句子级复杂度的预测任务表现，尤其是在资源有限的情况下。",
    "en_tdlr": "Fine-tuning T5 models with linguistic knowledge in an intermediate task predicting structural linguistic properties enhances performance in predicting sentence-level complexity, particularly in scenarios with limited resources."
}