{
    "title": "LLM can Achieve Self-Regulation via Hyperparameter Aware Generation",
    "abstract": "arXiv:2402.11251v1 Announce Type: new  Abstract: In the realm of Large Language Models (LLMs), users commonly employ diverse decoding strategies and adjust hyperparameters to control the generated text. However, a critical question emerges: Are LLMs conscious of the existence of these decoding strategies and capable of regulating themselves? The current decoding generation process often relies on empirical and heuristic manual adjustments to hyperparameters based on types of tasks and demands. However, this process is typically cumbersome, and the decoding hyperparameters may not always be optimal for each sample. To address the aforementioned challenges, we propose a novel text generation paradigm termed Hyperparameter Aware Generation (HAG). By leveraging hyperparameter-aware instruction tuning, the LLM autonomously determines the optimal decoding strategy and configs based on the input samples, enabling self-regulation. Our approach eliminates the need for extensive manual tuning, o",
    "link": "https://arxiv.org/abs/2402.11251",
    "context": "Title: LLM can Achieve Self-Regulation via Hyperparameter Aware Generation\nAbstract: arXiv:2402.11251v1 Announce Type: new  Abstract: In the realm of Large Language Models (LLMs), users commonly employ diverse decoding strategies and adjust hyperparameters to control the generated text. However, a critical question emerges: Are LLMs conscious of the existence of these decoding strategies and capable of regulating themselves? The current decoding generation process often relies on empirical and heuristic manual adjustments to hyperparameters based on types of tasks and demands. However, this process is typically cumbersome, and the decoding hyperparameters may not always be optimal for each sample. To address the aforementioned challenges, we propose a novel text generation paradigm termed Hyperparameter Aware Generation (HAG). By leveraging hyperparameter-aware instruction tuning, the LLM autonomously determines the optimal decoding strategy and configs based on the input samples, enabling self-regulation. Our approach eliminates the need for extensive manual tuning, o",
    "path": "papers/24/02/2402.11251.json",
    "total_tokens": 752,
    "translated_title": "LLM可以通过超参数感知生成实现自我调节",
    "translated_abstract": "在大型语言模型（LLMs）领域，用户通常采用不同的解码策略并调整超参数以控制生成的文本。然而，一个关键问题出现了：LLMs是否意识到这些解码策略的存在并且能够自我调节？当前的解码生成过程通常依赖于根据不同任务和需求调整超参数的经验和启发式手动调整。然而，这个过程通常很繁琐，解码超参数可能并不总是对每个样本都是最佳的。为了解决上述挑战，我们提出了一种称为超参数感知生成（HAG）的新颖文本生成范式。通过利用超参数感知指令调整，LLM可以自主确定基于输入样本的最佳解码策略和配置，实现自我调节。我们的方法消除了大量手动调整的需求。",
    "tldr": "LLM通过超参数感知生成实现自我调节，消除了大量手动调整的需求。",
    "en_tdlr": "LLM achieves self-regulation through hyperparameter aware generation, eliminating the need for extensive manual tuning."
}