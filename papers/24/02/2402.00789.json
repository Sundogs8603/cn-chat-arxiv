{
    "title": "Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces",
    "abstract": "Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantia",
    "link": "https://arxiv.org/abs/2402.00789",
    "context": "Title: Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces\nAbstract: Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantia",
    "path": "papers/24/02/2402.00789.json",
    "total_tokens": 811,
    "translated_title": "Graph-Mamba: 通过选择性状态空间进行长程图序列建模",
    "translated_abstract": "注意力机制在图变换器中广泛用于捕捉节点之间的长程依赖关系。由于二次计算成本的限制，注意力机制在大型图中无法扩展。最近的计算效率改进主要通过使用随机或基于启发式的图子采样进行注意力稀疏化实现，但在数据相关的上下文推理方面效果不佳。状态空间模型（SSM）（如Mamba）因其在序列数据中建模长程依赖关系的效果和效率而受到关注。然而，将SSM适应非序列图数据是一个显著的挑战。在这项工作中，我们介绍了图-Mamba，这是第一个通过将Mamba模块与基于输入的节点选择机制集成，以增强图网络中的长程上下文建模的尝试。具体而言，我们制定了以图为中心的节点优先级和排列策略来增强上下文感知推理，从而实现了实质性的效果改进。",
    "tldr": "Graph-Mamba是第一个尝试通过将Mamba模块与输入相关的节点选择机制集成来增强图网络中长程上下文建模的方法。"
}