{
    "title": "Deep Semantic-Visual Alignment for Zero-Shot Remote Sensing Image Scene Classification",
    "abstract": "Deep neural networks have achieved promising progress in remote sensing (RS) image classification, for which the training process requires abundant samples for each class. However, it is time-consuming and unrealistic to annotate labels for each RS category, given the fact that the RS target database is increasing dynamically. Zero-shot learning (ZSL) allows for identifying novel classes that are not seen during training, which provides a promising solution for the aforementioned problem. However, previous ZSL models mainly depend on manually-labeled attributes or word embeddings extracted from language models to transfer knowledge from seen classes to novel classes. Besides, pioneer ZSL models use convolutional neural networks pre-trained on ImageNet, which focus on the main objects appearing in each image, neglecting the background context that also matters in RS scene classification. To address the above problems, we propose to collect visually detectable attributes automatically. W",
    "link": "https://arxiv.org/abs/2402.02094",
    "context": "Title: Deep Semantic-Visual Alignment for Zero-Shot Remote Sensing Image Scene Classification\nAbstract: Deep neural networks have achieved promising progress in remote sensing (RS) image classification, for which the training process requires abundant samples for each class. However, it is time-consuming and unrealistic to annotate labels for each RS category, given the fact that the RS target database is increasing dynamically. Zero-shot learning (ZSL) allows for identifying novel classes that are not seen during training, which provides a promising solution for the aforementioned problem. However, previous ZSL models mainly depend on manually-labeled attributes or word embeddings extracted from language models to transfer knowledge from seen classes to novel classes. Besides, pioneer ZSL models use convolutional neural networks pre-trained on ImageNet, which focus on the main objects appearing in each image, neglecting the background context that also matters in RS scene classification. To address the above problems, we propose to collect visually detectable attributes automatically. W",
    "path": "papers/24/02/2402.02094.json",
    "total_tokens": 875,
    "translated_title": "针对零样本遥感图像场景分类的深度语义-视觉对齐",
    "translated_abstract": "深度神经网络在遥感图像分类方面取得了显著的进展，然而，训练过程需要每个类别的丰富样本。然而，给定遥感目标数据库的动态增长事实，为每个遥感类别注释标签耗时且不现实。零样本学习（ZSL）可以识别训练期间未见过的新类别，为上述问题提供了有希望的解决方案。然而，先前的ZSL模型主要依赖于手动标记的属性或从语言模型中提取的词嵌入来将知识从已知类别传递到新类别。此外，开创性的ZSL模型使用在ImageNet上预训练的卷积神经网络，这些网络主要关注每个图像中出现的主要对象，忽视了在遥感场景分类中也很重要的背景上下文。为了解决上述问题，我们提出自动收集可视化可检测属性的方法。",
    "tldr": "提出了一种针对零样本遥感图像场景分类的深度语义-视觉对齐方法，通过自动收集可视化可检测属性来解决先前ZSL模型主要依赖于手动标记的属性或词嵌入的问题。",
    "en_tdlr": "A deep semantic-visual alignment method is proposed for zero-shot remote sensing image scene classification, addressing the problem of previous ZSL models relying on manually-labeled attributes or word embeddings by automatically collecting visually detectable attributes."
}