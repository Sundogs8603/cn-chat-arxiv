{
    "title": "Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning",
    "abstract": "arXiv:2402.11237v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than learning the intended task, they tend to draw inconclusive relationships between their inputs and outputs. Shortcut learning is ubiquitous among many failure cases of neural networks, and traces of this phenomenon can be seen in their generalizability issues, domain shift, adversarial vulnerability, and even bias towards majority groups. In this paper, we argue that this commonality in the cause of various DNN issues creates a significant opportunity that should be leveraged to find a unified solution for shortcut learning. To this end, we outline the recent advances in topological data analysis~(TDA), and persistent homology~(PH) in particular, to sketch a unified roadmap for detecting shortcuts in deep learning. We demonstrate our arguments by investigating the topological features of computational graphs in DNNs using two cases of unlearnable examples and bia",
    "link": "https://arxiv.org/abs/2402.11237",
    "context": "Title: Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning\nAbstract: arXiv:2402.11237v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than learning the intended task, they tend to draw inconclusive relationships between their inputs and outputs. Shortcut learning is ubiquitous among many failure cases of neural networks, and traces of this phenomenon can be seen in their generalizability issues, domain shift, adversarial vulnerability, and even bias towards majority groups. In this paper, we argue that this commonality in the cause of various DNN issues creates a significant opportunity that should be leveraged to find a unified solution for shortcut learning. To this end, we outline the recent advances in topological data analysis~(TDA), and persistent homology~(PH) in particular, to sketch a unified roadmap for detecting shortcuts in deep learning. We demonstrate our arguments by investigating the topological features of computational graphs in DNNs using two cases of unlearnable examples and bia",
    "path": "papers/24/02/2402.11237.json",
    "total_tokens": 848,
    "translated_title": "对抗深度学习中快捷方式的统一解决方案",
    "translated_abstract": "深度神经网络(DNNs)容易受到快捷学习的影响：它们倾向于建立输入和输出之间无关的关系，而不是学习预期的任务。快捷学习在神经网络许多失败案例中普遍存在，这一现象的痕迹可见于其泛化问题、领域转移、对抗性脆弱性，甚至对多数群体的偏见。本文认为，各种DNN问题的共同原因为我们提供了一个重要机会，应该利用这一点找到对抗快捷学习的统一解决方案。为此，我们概述了拓扑数据分析(TDA)特别是持续同调(PH)方面的最新进展，为探测深度学习中快捷方式勾画了统一的路线图。我们通过研究DNNs中计算图的拓扑特征，使用无法学习的示例和偏见为两种情况，来证明我们的论点。",
    "tldr": "本文旨在通过利用拓扑数据分析提出一个统一的解决方案，检测深度学习中的快捷学习问题。",
    "en_tdlr": "This paper aims to propose a unified solution for detecting shortcuts in deep learning by leveraging topological data analysis."
}