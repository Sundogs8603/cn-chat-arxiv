{
    "title": "Anchor-based Large Language Models",
    "abstract": "Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM signi",
    "link": "https://arxiv.org/abs/2402.07616",
    "context": "Title: Anchor-based Large Language Models\nAbstract: Large language models (LLMs) predominantly employ decoder-only transformer architectures, necessitating the retention of keys/values information for historical tokens to provide contextual information and avoid redundant computation. However, the substantial size and parameter volume of these LLMs require massive GPU memory. This memory demand increases with the length of the input text, leading to an urgent need for more efficient methods of information storage and processing. This study introduces the Anchor-based LLM (AnLLM), which utilizes an innovative anchor-based self-attention network (AnSAN) and also an anchor-based inference strategy. This approach enables LLMs to compress sequence information into an anchor token, reducing the keys/values cache and enhancing inference efficiency. Experiments show that the AnLLM maintains comparable accuracy with up to 99% keys/values cache reduction and up to 3.5 times faster inference. Despite a minor compromise in accuracy, the AnLLM signi",
    "path": "papers/24/02/2402.07616.json",
    "total_tokens": 878,
    "translated_title": "基于锚点的大型语言模型",
    "translated_abstract": "大型语言模型（LLMs）主要采用仅解码器的转换器架构，需要保留历史标记的键/值信息以提供上下文信息并避免冗余计算。然而，这些LLMs的巨大大小和参数量需要大量的GPU内存。这种内存需求随着输入文本的长度而增加，迫切需要更高效的信息存储和处理方法。本研究介绍了一种基于锚点的LLM（AnLLM），它利用了一种创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略。这种方法使LLMs能够将序列信息压缩成锚点标记，减少键/值缓存并提高推理效率。实验证明，AnLLM在减少键/值缓存高达99%和推理速度提高高达3.5倍的同时，仍保持可比的准确性。尽管牺牲了一些准确性，AnLLM的创新和贡献依然重要。",
    "tldr": "基于锚点的大型语言模型（AnLLM）通过引入创新的基于锚点的自注意力网络（AnSAN）和基于锚点的推理策略，将序列信息压缩到锚点标记中，减少键/值缓存，提高推理效率。"
}