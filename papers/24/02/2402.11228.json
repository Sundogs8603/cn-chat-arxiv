{
    "title": "Adaptive Split Balancing for Optimal Random Forest",
    "abstract": "arXiv:2402.11228v1 Announce Type: cross  Abstract: While random forests are commonly used for regression problems, existing methods often lack adaptability in complex situations or lose optimality under simple, smooth scenarios. In this study, we introduce the adaptive split balancing forest (ASBF), capable of learning tree representations from data while simultaneously achieving minimax optimality under the Lipschitz class. To exploit higher-order smoothness levels, we further propose a localized version that attains the minimax rate under the H\\\"older class $\\mathcal{H}^{q,\\beta}$ for any $q\\in\\mathbb{N}$ and $\\beta\\in(0,1]$. Rather than relying on the widely-used random feature selection, we consider a balanced modification to existing approaches. Our results indicate that an over-reliance on auxiliary randomness may compromise the approximation power of tree models, leading to suboptimal results. Conversely, a less random, more balanced approach demonstrates optimality. Additionall",
    "link": "https://arxiv.org/abs/2402.11228",
    "context": "Title: Adaptive Split Balancing for Optimal Random Forest\nAbstract: arXiv:2402.11228v1 Announce Type: cross  Abstract: While random forests are commonly used for regression problems, existing methods often lack adaptability in complex situations or lose optimality under simple, smooth scenarios. In this study, we introduce the adaptive split balancing forest (ASBF), capable of learning tree representations from data while simultaneously achieving minimax optimality under the Lipschitz class. To exploit higher-order smoothness levels, we further propose a localized version that attains the minimax rate under the H\\\"older class $\\mathcal{H}^{q,\\beta}$ for any $q\\in\\mathbb{N}$ and $\\beta\\in(0,1]$. Rather than relying on the widely-used random feature selection, we consider a balanced modification to existing approaches. Our results indicate that an over-reliance on auxiliary randomness may compromise the approximation power of tree models, leading to suboptimal results. Conversely, a less random, more balanced approach demonstrates optimality. Additionall",
    "path": "papers/24/02/2402.11228.json",
    "total_tokens": 886,
    "translated_title": "自适应分割平衡优化随机森林",
    "translated_abstract": "尽管随机森林通常用于回归问题，但现有方法在复杂情况下缺乏适应性，或在简单、平滑情景下失去最优性。在本研究中，我们介绍了自适应分割平衡森林（ASBF），能够从数据中学习树表示，同时在Lipschitz类下实现极小极优性。为了利用更高阶的平滑性水平，我们进一步提出了一个本地化版本，该版本在任意$q \\in \\mathbb{N}$和$\\beta \\in (0,1]$的Hölder类$\\mathcal{H}^{q,\\beta}$下达到最小极优性。与广泛使用的随机特征选择不同，我们考虑了对现有方法的平衡修改。我们的结果表明，过度依赖辅助随机性可能会损害树模型的逼近能力，导致次优结果。相反，一个更平衡、更少随机的方法表现出最佳性能。",
    "tldr": "介绍了自适应分割平衡森林（ASBF），可在学习树表示的同时，在复杂情况下实现极小极优性，并提出了一个本地化版本，在H\\\"older类下达到最小极优性。",
    "en_tdlr": "Introducing the adaptive split balancing forest (ASBF) capable of learning tree representations while achieving minimax optimality under complex scenarios, and proposing a localized version that attains minimax rate under the H\\\"older class."
}