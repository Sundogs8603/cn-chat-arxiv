{
    "title": "Opening the black box of language acquisition",
    "abstract": "arXiv:2402.11681v1 Announce Type: new  Abstract: Recent advances in large language models using deep learning techniques have renewed interest on how languages can be learned from data. However, it is unclear whether or how these models represent grammatical information from the learned languages. In addition, the models must be pre-trained on large corpora before they can be used. In this work, we propose an alternative, more transparent and cognitively plausible architecture for learning language. Instead of using deep learning, our approach uses a minimal cognitive architecture based on sequence memory and chunking. The learning mechanism is based on the principles of reinforcement learning. We test our architecture on a number of natural-like toy languages. Results show that the model can learn these artificial languages from scratch and extract grammatical information that supports learning. Our study demonstrates the power of this simple architecture and stresses the importance o",
    "link": "https://arxiv.org/abs/2402.11681",
    "context": "Title: Opening the black box of language acquisition\nAbstract: arXiv:2402.11681v1 Announce Type: new  Abstract: Recent advances in large language models using deep learning techniques have renewed interest on how languages can be learned from data. However, it is unclear whether or how these models represent grammatical information from the learned languages. In addition, the models must be pre-trained on large corpora before they can be used. In this work, we propose an alternative, more transparent and cognitively plausible architecture for learning language. Instead of using deep learning, our approach uses a minimal cognitive architecture based on sequence memory and chunking. The learning mechanism is based on the principles of reinforcement learning. We test our architecture on a number of natural-like toy languages. Results show that the model can learn these artificial languages from scratch and extract grammatical information that supports learning. Our study demonstrates the power of this simple architecture and stresses the importance o",
    "path": "papers/24/02/2402.11681.json",
    "total_tokens": 820,
    "translated_title": "打开语言习得的黑匣子",
    "translated_abstract": "最近深度学习技术在大型语言模型方面取得了重大进展，重新激发了人们对语言如何从数据中学习的兴趣。然而，目前尚不清楚这些模型是否以及如何表示所学语言的语法信息。此外，这些模型在使用之前必须在大型语料库上进行预训练。在本研究中，我们提出了一种替代性、更透明且认知合理的学习语言架构。我们的方法不是使用深度学习，而是基于序列记忆和分块的最小认知架构。学习机制基于强化学习原则。我们在多种类似自然语言的玩具语言上测试了我们的架构。结果显示，该模型能够从零开始学习这些人造语言，并提取支持学习的语法信息。我们的研究证明了这种简单架构的强大能力，并强调了其重要性。",
    "tldr": "提出了一种基于序列记忆和分块的最小认知架构，采用强化学习原则，能够学习人造语言并提取支持学习的语法信息。"
}