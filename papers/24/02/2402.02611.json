{
    "title": "PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?",
    "abstract": "Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such",
    "link": "https://arxiv.org/abs/2402.02611",
    "context": "Title: PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?\nAbstract: Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such",
    "path": "papers/24/02/2402.02611.json",
    "total_tokens": 915,
    "translated_title": "PuzzleBench：LLMs能否解决困难的一阶组合推理问题？",
    "translated_abstract": "最近的研究探索了使用LLMs进行推理任务，重点是相对简单的问题，如逻辑问答。在我们的工作中，我们希望解决更复杂的问题，显著扩展这些模型的功能。特别是，我们探讨LLMs是否能够解决困难的一阶组合推理问题，一个例子是流行的数独谜题。这些问题有一个由自然语言描述的基础一阶结构，并且可以实例化为不同大小的实例。此外，这些问题在计算上是密集型的，需要多个推理步骤才能达到解决方案。我们提出了PuzzleBench，一个包含31个这样具有挑战性的谜题的数据集。我们观察到，即使在符号求解器的帮助下，LLMs在我们的基准测试中表现得相当糟糕。作为回应，我们提出了一种新的方法，Puzzle-LM，它将LLMs与符号求解器和程序解释器相结合，使它们能够推理这类问题。",
    "tldr": "本研究通过PuzzleBench数据集探索了LLMs解决困难的一阶组合推理问题的能力，并提出了Puzzle-LM方法，该方法将LLMs与符号求解器和程序解释器相结合，使其能够有效地推理这类问题。"
}