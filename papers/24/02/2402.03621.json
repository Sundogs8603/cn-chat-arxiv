{
    "title": "Neural Network Approximators for Marginal MAP in Probabilistic Circuits",
    "abstract": "Probabilistic circuits (PCs) such as sum-product networks efficiently represent large multi-variate probability distributions. They are preferred in practice over other probabilistic representations such as Bayesian and Markov networks because PCs can solve marginal inference (MAR) tasks in time that scales linearly in the size of the network. Unfortunately, the maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in these models. Inspired by the recent work on using neural networks for generating near-optimal solutions to optimization problems such as integer linear programming, we propose an approach that uses neural networks to approximate (M)MAP inference in PCs. The key idea in our approach is to approximate the cost of an assignment to the query variables using a continuous multilinear function, and then use the latter as a loss function. The two main benefits of our new method are that it is self-supervised and after the neural network is learned, it requires ",
    "link": "https://arxiv.org/abs/2402.03621",
    "context": "Title: Neural Network Approximators for Marginal MAP in Probabilistic Circuits\nAbstract: Probabilistic circuits (PCs) such as sum-product networks efficiently represent large multi-variate probability distributions. They are preferred in practice over other probabilistic representations such as Bayesian and Markov networks because PCs can solve marginal inference (MAR) tasks in time that scales linearly in the size of the network. Unfortunately, the maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in these models. Inspired by the recent work on using neural networks for generating near-optimal solutions to optimization problems such as integer linear programming, we propose an approach that uses neural networks to approximate (M)MAP inference in PCs. The key idea in our approach is to approximate the cost of an assignment to the query variables using a continuous multilinear function, and then use the latter as a loss function. The two main benefits of our new method are that it is self-supervised and after the neural network is learned, it requires ",
    "path": "papers/24/02/2402.03621.json",
    "total_tokens": 874,
    "translated_title": "概率电路中用于边际MAP的神经网络近似器",
    "translated_abstract": "概率电路（PCs）如和积网络以高效地表示大型多变量概率分布。在实践中，与贝叶斯网络和马尔可夫网络等其他概率表示相比，PCs更受青睐，因为PCs可以在网络大小线性扩展的时间内解决边际推理（MAR）任务。然而，最大后验概率（MAP）和边际MAP（MMAP）任务在这些模型中仍然是NP困难的。受最近关于使用神经网络生成接近最优解的优化问题（如整数线性规划）的工作的启发，我们提出了一种方法，该方法使用神经网络来近似PCs中的(M)MAP推理。我们方法的关键思想是使用连续多线性函数来近似查询变量的赋值成本，然后将其用作损失函数。我们的新方法有两个主要优点，即自我监督和在学习神经网络之后，它只需要",
    "tldr": "本文提出了一种使用神经网络近似概率电路中边际MAP推理的方法，该方法通过使用连续多线性函数来估计查询变量的赋值成本并将其作为损失函数，具有自我监督和高效性的优点。"
}