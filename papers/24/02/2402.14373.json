{
    "title": "Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction",
    "abstract": "arXiv:2402.14373v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have been successful in relational extraction (RE) tasks, especially in the few-shot learning. An important problem in the field of RE is long-tailed data, while not much attention is currently paid to this problem using LLM approaches. Therefore, in this paper, we propose SLCoLM, a model collaboration framework, to mitigate the data long-tail problem. In our framework, We use the ``\\textit{Training-Guide-Predict}'' strategy to combine the strengths of pre-trained language models (PLMs) and LLMs, where a task-specific PLM framework acts as a tutor, transfers task knowledge to the LLM, and guides the LLM in performing RE tasks. Our experiments on a RE dataset rich in relation types show that the approach in this paper facilitates RE of long-tail relation types.",
    "link": "https://arxiv.org/abs/2402.14373",
    "context": "Title: Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction\nAbstract: arXiv:2402.14373v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have been successful in relational extraction (RE) tasks, especially in the few-shot learning. An important problem in the field of RE is long-tailed data, while not much attention is currently paid to this problem using LLM approaches. Therefore, in this paper, we propose SLCoLM, a model collaboration framework, to mitigate the data long-tail problem. In our framework, We use the ``\\textit{Training-Guide-Predict}'' strategy to combine the strengths of pre-trained language models (PLMs) and LLMs, where a task-specific PLM framework acts as a tutor, transfers task knowledge to the LLM, and guides the LLM in performing RE tasks. Our experiments on a RE dataset rich in relation types show that the approach in this paper facilitates RE of long-tail relation types.",
    "path": "papers/24/02/2402.14373.json",
    "total_tokens": 853,
    "translated_title": "小语言模型在中文实体关系抽取中是大语言模型的良好向导",
    "translated_abstract": "近年来，大语言模型（LLMs）在关系抽取（RE）任务中取得了成功，尤其是在少样本学习中。关系抽取领域中一个重要问题是长尾数据，然而目前很少有关注使用LLM方法解决这个问题。因此，在本文中，我们提出了SLCoLM，一个模型协作框架，以缓解数据长尾问题。在我们的框架中，我们使用“训练-指导-预测”策略来结合预训练语言模型（PLMs）和LLMs的优势，其中一个特定于任务的PLM框架充当导师，将任务知识转移到LLM，并指导LLM执行RE任务。我们对一个富含关系类型的RE数据集进行的实验表明，本文中的方法促进了长尾关系类型的RE。",
    "tldr": "本文提出了SLCoLM，一个模型协作框架，通过使用“训练-指导-预测”策略结合预训练语言模型和大语言模型，成功缓解了长尾数据问题，促进了实体关系的抽取。",
    "en_tdlr": "In this paper, we propose SLCoLM, a model collaboration framework that successfully mitigates the long-tail data problem and facilitates entity relation extraction by combining pre-trained language models and large language models using the \"Training-Guide-Predict\" strategy."
}