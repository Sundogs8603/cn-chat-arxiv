{
    "title": "EXGC: Bridging Efficiency and Explainability in Graph Condensation",
    "abstract": "Graph representation learning on vast datasets, like web data, has made significant strides. However, the associated computational and storage overheads raise concerns. In sight of this, Graph condensation (GCond) has been introduced to distill these large real datasets into a more concise yet information-rich synthetic graph. Despite acceleration efforts, existing GCond methods mainly grapple with efficiency, especially on expansive web data graphs. Hence, in this work, we pinpoint two major inefficiencies of current paradigms: (1) the concurrent updating of a vast parameter set, and (2) pronounced parameter redundancy. To counteract these two limitations correspondingly, we first (1) employ the Mean-Field variational approximation for convergence acceleration, and then (2) propose the objective of Gradient Information Bottleneck (GDIB) to prune redundancy. By incorporating the leading explanation techniques (e.g., GNNExplainer and GSAT) to instantiate the GDIB, our EXGC, the Efficien",
    "link": "https://arxiv.org/abs/2402.05962",
    "context": "Title: EXGC: Bridging Efficiency and Explainability in Graph Condensation\nAbstract: Graph representation learning on vast datasets, like web data, has made significant strides. However, the associated computational and storage overheads raise concerns. In sight of this, Graph condensation (GCond) has been introduced to distill these large real datasets into a more concise yet information-rich synthetic graph. Despite acceleration efforts, existing GCond methods mainly grapple with efficiency, especially on expansive web data graphs. Hence, in this work, we pinpoint two major inefficiencies of current paradigms: (1) the concurrent updating of a vast parameter set, and (2) pronounced parameter redundancy. To counteract these two limitations correspondingly, we first (1) employ the Mean-Field variational approximation for convergence acceleration, and then (2) propose the objective of Gradient Information Bottleneck (GDIB) to prune redundancy. By incorporating the leading explanation techniques (e.g., GNNExplainer and GSAT) to instantiate the GDIB, our EXGC, the Efficien",
    "path": "papers/24/02/2402.05962.json",
    "total_tokens": 847,
    "translated_title": "EXGC: 在图压缩中平衡效率与可解释性",
    "translated_abstract": "在海量数据集（如网络数据）上进行图表示学习已经取得了显著进展。然而，相关的计算和存储开销引起了人们的关注。为此，引入了图压缩（GCond）来将这些大型真实数据集蒸馏为更简洁但信息丰富的合成图。尽管进行了加速努力，现有的GCond方法主要在海量网络数据图上面临效率问题。因此，在这项工作中，我们指出了当前范例的两个主要不足之处：（1）大量参数集的并发更新，（2）明显的参数冗余。为了相应地克服这两个限制，我们首先采用平均场变分近似进行收敛加速，然后提出梯度信息瓶颈（GDIB）的目标来减少冗余。通过结合领先的解释技术（如GNNExplainer和GSAT）来实例化GDIB，我们的EXGC能够同时提高效率和可解释性。",
    "tldr": "本论文提出了EXGC方法，通过采用平均场变分近似和梯度信息瓶颈目标来提高图压缩的效率和可解释性。",
    "en_tdlr": "This paper proposes the EXGC method, which improves the efficiency and explainability of graph condensation by employing mean-field variational approximation and the objective of gradient information bottleneck."
}