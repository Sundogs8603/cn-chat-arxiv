{
    "title": "Enhancing Stochastic Gradient Descent: A Unified Framework and Novel Acceleration Methods for Faster Convergence",
    "abstract": "Based on SGD, previous works have proposed many algorithms that have improved convergence speed and generalization in stochastic optimization, such as SGDm, AdaGrad, Adam, etc. However, their convergence analysis under non-convex conditions is challenging. In this work, we propose a unified framework to address this issue. For any first-order methods, we interpret the updated direction $g_t$ as the sum of the stochastic subgradient $\\nabla f_t(x_t)$ and an additional acceleration term $\\frac{2|\\langle v_t, \\nabla f_t(x_t) \\rangle|}{\\|v_t\\|_2^2} v_t$, thus we can discuss the convergence by analyzing $\\langle v_t, \\nabla f_t(x_t) \\rangle$. Through our framework, we have discovered two plug-and-play acceleration methods: \\textbf{Reject Accelerating} and \\textbf{Random Vector Accelerating}, we theoretically demonstrate that these two methods can directly lead to an improvement in convergence rate.",
    "link": "https://rss.arxiv.org/abs/2402.01515",
    "context": "Title: Enhancing Stochastic Gradient Descent: A Unified Framework and Novel Acceleration Methods for Faster Convergence\nAbstract: Based on SGD, previous works have proposed many algorithms that have improved convergence speed and generalization in stochastic optimization, such as SGDm, AdaGrad, Adam, etc. However, their convergence analysis under non-convex conditions is challenging. In this work, we propose a unified framework to address this issue. For any first-order methods, we interpret the updated direction $g_t$ as the sum of the stochastic subgradient $\\nabla f_t(x_t)$ and an additional acceleration term $\\frac{2|\\langle v_t, \\nabla f_t(x_t) \\rangle|}{\\|v_t\\|_2^2} v_t$, thus we can discuss the convergence by analyzing $\\langle v_t, \\nabla f_t(x_t) \\rangle$. Through our framework, we have discovered two plug-and-play acceleration methods: \\textbf{Reject Accelerating} and \\textbf{Random Vector Accelerating}, we theoretically demonstrate that these two methods can directly lead to an improvement in convergence rate.",
    "path": "papers/24/02/2402.01515.json",
    "total_tokens": 895,
    "translated_title": "提升随机梯度下降：一种统一框架和用于更快收敛的新型加速方法",
    "translated_abstract": "基于SGD，之前的研究提出了许多算法，在随机优化中改进了收敛速度和泛化性能，例如SGDm，AdaGrad，Adam等。然而，在非凸条件下，它们的收敛分析是具有挑战性的。在本文中，我们提出了一个统一的框架来解决这个问题。对于任何一阶方法，我们将更新方向$g_t$解释为随机次梯度$\\nabla f_t(x_t)$和附加的加速项$\\frac{2|\\langle v_t, \\nabla f_t(x_t) \\rangle|}{\\|v_t\\|_2^2} v_t$的和，因此我们可以通过分析$\\langle v_t, \\nabla f_t(x_t) \\rangle$来讨论收敛性。通过我们的框架，我们发现了两种即插即用的加速方法：\\textbf{拒绝加速}和\\textbf{随机向量加速}，我们在理论上证明了这两种方法可以直接导致收敛速度的提升。",
    "tldr": "本文提出了一个统一框架来解决随机优化中的收敛问题，并提出了两种即插即用的加速方法，在理论上证明了这些方法可以加快收敛速度。",
    "en_tdlr": "This paper proposes a unified framework to address convergence issues in stochastic optimization and introduces two plug-and-play acceleration methods that are proven to improve convergence speed."
}