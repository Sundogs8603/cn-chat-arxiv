{
    "title": "PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation",
    "abstract": "arXiv:2402.18117v1 Announce Type: cross  Abstract: Tremendous breakthroughs have been developed in Semi-Supervised Semantic Segmentation (S4) through contrastive learning. However, due to limited annotations, the guidance on unlabeled images is generated by the model itself, which inevitably exists noise and disturbs the unsupervised training process. To address this issue, we propose a robust contrastive-based S4 framework, termed the Probabilistic Representation Contrastive Learning (PRCL) framework to enhance the robustness of the unsupervised training process. We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in contrastive learning. Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process. Since the GDP contains the information of all representations ",
    "link": "https://arxiv.org/abs/2402.18117",
    "context": "Title: PRCL: Probabilistic Representation Contrastive Learning for Semi-Supervised Semantic Segmentation\nAbstract: arXiv:2402.18117v1 Announce Type: cross  Abstract: Tremendous breakthroughs have been developed in Semi-Supervised Semantic Segmentation (S4) through contrastive learning. However, due to limited annotations, the guidance on unlabeled images is generated by the model itself, which inevitably exists noise and disturbs the unsupervised training process. To address this issue, we propose a robust contrastive-based S4 framework, termed the Probabilistic Representation Contrastive Learning (PRCL) framework to enhance the robustness of the unsupervised training process. We model the pixel-wise representation as Probabilistic Representations (PR) via multivariate Gaussian distribution and tune the contribution of the ambiguous representations to tolerate the risk of inaccurate guidance in contrastive learning. Furthermore, we introduce Global Distribution Prototypes (GDP) by gathering all PRs throughout the whole training process. Since the GDP contains the information of all representations ",
    "path": "papers/24/02/2402.18117.json",
    "total_tokens": 848,
    "translated_title": "PRCL：用于半监督语义分割的概率表示对比学习",
    "translated_abstract": "半监督语义分割（S4）领域取得了巨大突破，通过对比学习生成了一些有限标签的非监督图像数据，并通过模型自身生成的指导对无标签图像进行了指导。然而，这必然存在噪声并扰乱了无监督训练过程。为解决这一问题，提出了一个稳健的基于对比的S4框架，命名为概率表示对比学习（PRCL）框架，以增强无监督训练过程的稳健性。通过多变量高斯分布将像素级表示建模为概率表示（PR），调整模糊表示的贡献以容忍对比学习中不准确指导的风险。此外，通过在整个训练过程中收集所有PR，引入全局分布原型（GDP）。由于GDP包含所有表示的信息",
    "tldr": "提出了PRCL框架，通过将像素级表示建模为概率表示，调整模糊表示的贡献，并引入全局分布原型来增强半监督语义分割的鲁棒性。",
    "en_tdlr": "Proposed PRCL framework enhances the robustness of semi-supervised semantic segmentation by modeling pixel-wise representation as probabilistic representations, adjusting the contribution of ambiguous representations, and introducing global distribution prototypes."
}