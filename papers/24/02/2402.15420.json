{
    "title": "PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning",
    "abstract": "arXiv:2402.15420v1 Announce Type: cross  Abstract: Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights -- state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a u",
    "link": "https://arxiv.org/abs/2402.15420",
    "context": "Title: PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning\nAbstract: arXiv:2402.15420v1 Announce Type: cross  Abstract: Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights -- state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a u",
    "path": "papers/24/02/2402.15420.json",
    "total_tokens": 831,
    "translated_title": "PREDILECT：在强化学习中利用零样本语言推理划分偏好",
    "translated_abstract": "基于偏好的强化学习已经成为机器人学习中的一个新领域，在这个领域中，人类通过对不同状态-动作序列表达偏好来塑造机器人行为。然而，为机器人制定现实政策需要人类对大量查询的响应。本工作通过扩展每个查询收集的信息，包含偏好和可选文本提示，来解决样本效率挑战。为了实现这一目标，我们利用大型语言模型(LLM)的零样本能力来从人类提供的文本中进行推理。为了适应额外的查询信息，我们重新定义了奖励学习目标，包含灵活的重点 —— 包含相对高信息量且与零射样本传递的特征相关的状态-动作对。在仿真场景和实际场景中，我们展示了我们方法的有效性。",
    "tldr": "本文提出了一种在强化学习中利用零样本语言推理来划分偏好的方法，通过扩展查询信息并重新定义奖励学习目标，提高了样本效率。",
    "en_tdlr": "This paper introduces a method for delineating preferences in reinforcement learning using zero-shot language reasoning, which improves sample efficiency by expanding query information and redefining reward learning objectives."
}