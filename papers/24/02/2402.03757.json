{
    "title": "The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs",
    "abstract": "Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation result",
    "link": "https://arxiv.org/abs/2402.03757",
    "context": "Title: The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs\nAbstract: Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation result",
    "path": "papers/24/02/2402.03757.json",
    "total_tokens": 940,
    "translated_title": "本能偏见：虚假图像导致MLLMs产生幻觉",
    "translated_abstract": "大型语言模型（LLMs）近年来取得了显著进展，多模态大型语言模型（MLLMs）的出现使LLMs具备了视觉能力，在各种多模态任务中表现出色。然而，像GPT-4V这样强大的MLLMs在面对某些图像和文本输入时仍然以惊人的方式失败了。本文中，我们确定了一类典型输入，这些输入令MLLMs困惑，它们由高度相关但与答案不一致的图像组成，导致MLLMs产生幻觉。为了量化这种影响，我们提出了CorrelationQA，这是首个评估给定虚假图像的幻觉程度的基准。该基准包含13个类别的7,308个文本-图像对。基于提出的CorrelationQA，我们对9个主流MLLMs进行了深入分析，表明它们普遍受到这种本能偏见的不同程度的影响。我们希望我们精选的基准和评估结果能有所帮助。",
    "tldr": "本论文研究发现，虚假图像会导致多模态大型语言模型产生幻觉，作者提出了评估幻觉程度的基准CorrelationQA，并发现主流多模态大型语言模型普遍受到这种本能偏见的影响。",
    "en_tdlr": "This paper investigates the phenomenon of hallucination in MLLMs caused by spurious images and proposes a benchmark called CorrelationQA to assess the level of hallucination. The study shows that mainstream MLLMs universally suffer from an instinctive bias towards inconsistent but relevant images."
}