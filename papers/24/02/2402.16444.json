{
    "title": "ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors",
    "abstract": "arXiv:2402.16444v1 Announce Type: new  Abstract: The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner. In this paper, we propose ShieldLM, an LLM-based safety detector, which aligns with general human safety standards, supports customizable detection rules, and provides explanations for its decisions. To train ShieldLM, we compile a large bilingual dataset comprising 14,387 query-response pairs, annotating the safety of responses based on various safety standards. Through extensive experiments, we demonstrate that ShieldLM surpasses strong baselines across four test sets, showcasing remarkable customizability and explainability. Besides performing well on standard detection datasets, ShieldLM has also been shown to be effective in real-world situations as a safety evaluator for advanced LLMs. We relea",
    "link": "https://arxiv.org/abs/2402.16444",
    "context": "Title: ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors\nAbstract: arXiv:2402.16444v1 Announce Type: new  Abstract: The safety of Large Language Models (LLMs) has gained increasing attention in recent years, but there still lacks a comprehensive approach for detecting safety issues within LLMs' responses in an aligned, customizable and explainable manner. In this paper, we propose ShieldLM, an LLM-based safety detector, which aligns with general human safety standards, supports customizable detection rules, and provides explanations for its decisions. To train ShieldLM, we compile a large bilingual dataset comprising 14,387 query-response pairs, annotating the safety of responses based on various safety standards. Through extensive experiments, we demonstrate that ShieldLM surpasses strong baselines across four test sets, showcasing remarkable customizability and explainability. Besides performing well on standard detection datasets, ShieldLM has also been shown to be effective in real-world situations as a safety evaluator for advanced LLMs. We relea",
    "path": "papers/24/02/2402.16444.json",
    "total_tokens": 847,
    "translated_title": "ShieldLM: 使LLMs成为对齐、可定制和可解释的安全检测器",
    "translated_abstract": "大型语言模型（LLMs）的安全性近年来受到越来越多关注，但在对LLMs的响应中检测安全问题的方法仍然缺乏一个全面的、对齐、可定制和可解释的方法。在本文中，我们提出了ShieldLM，一个基于LLM的安全检测器，它与一般人类安全标准相符，支持定制化的检测规则，并为其决策提供解释。为了训练ShieldLM，我们编制了一个包含14,387个查询-响应对的大型双语数据集，根据各种安全标准对响应的安全性进行了注释。通过大量实验，我们证明了ShieldLM在四个测试集上超越了强基线，展示了出色的定制性和可解释性。除了在标准检测数据集上表现良好外，ShieldLM还被证明在实际情况中作为先进LLMs的安全评估器是有效的。",
    "tldr": "ShieldLM是一个基于LLM的安全检测器，符合一般人类安全标准，支持定制化的检测规则，并提供决策解释。",
    "en_tdlr": "ShieldLM is an LLM-based safety detector that aligns with general human safety standards, supports customizable detection rules, and provides explanations for its decisions."
}