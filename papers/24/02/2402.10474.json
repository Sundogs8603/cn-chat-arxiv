{
    "title": "One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression",
    "abstract": "arXiv:2402.10474v1 Announce Type: new  Abstract: We study the use of linear regression for multiclass classification in the over-parametrized regime where some of the training data is mislabeled. In such scenarios it is necessary to add an explicit regularization term, $\\lambda f(w)$, for some convex function $f(\\cdot)$, to avoid overfitting the mislabeled data. In our analysis, we assume that the data is sampled from a Gaussian Mixture Model with equal class sizes, and that a proportion $c$ of the training labels is corrupted for each class. Under these assumptions, we prove that the best classification performance is achieved when $f(\\cdot) = \\|\\cdot\\|^2_2$ and $\\lambda \\to \\infty$. We then proceed to analyze the classification errors for $f(\\cdot) = \\|\\cdot\\|_1$ and $f(\\cdot) = \\|\\cdot\\|_\\infty$ in the large $\\lambda$ regime and notice that it is often possible to find sparse and one-bit solutions, respectively, that perform almost as well as the one corresponding to $f(\\cdot) = \\|\\",
    "link": "https://arxiv.org/abs/2402.10474",
    "context": "Title: One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression\nAbstract: arXiv:2402.10474v1 Announce Type: new  Abstract: We study the use of linear regression for multiclass classification in the over-parametrized regime where some of the training data is mislabeled. In such scenarios it is necessary to add an explicit regularization term, $\\lambda f(w)$, for some convex function $f(\\cdot)$, to avoid overfitting the mislabeled data. In our analysis, we assume that the data is sampled from a Gaussian Mixture Model with equal class sizes, and that a proportion $c$ of the training labels is corrupted for each class. Under these assumptions, we prove that the best classification performance is achieved when $f(\\cdot) = \\|\\cdot\\|^2_2$ and $\\lambda \\to \\infty$. We then proceed to analyze the classification errors for $f(\\cdot) = \\|\\cdot\\|_1$ and $f(\\cdot) = \\|\\cdot\\|_\\infty$ in the large $\\lambda$ regime and notice that it is often possible to find sparse and one-bit solutions, respectively, that perform almost as well as the one corresponding to $f(\\cdot) = \\|\\",
    "path": "papers/24/02/2402.10474.json",
    "total_tokens": 955,
    "translated_title": "一位量化和稀疏化用于多类线性分类的正则化回归",
    "translated_abstract": "我们研究了在线性回归中用于多类分类的问题，这些问题在超参数化范围内，训练数据中一些标记错误。在这种情况下，为了避免过度拟合错误标记的数据，需要添加一个显式的正则化项，$\\lambda f(w)$，其中$f(\\cdot)$是某个凸函数。在我们的分析中，我们假设数据是从一个具有相等类大小的高斯混合模型中采样的，并且每个类别的训练标签中有一部分比例为$c$是错误的。在这些假设下，我们证明了当$f(\\cdot) = \\|\\cdot\\|^2_2$且$\\lambda \\to \\infty$时，可以获得最佳的分类性能。然后我们继续分析了在大$\\lambda$范围内$f(\\cdot) = \\|\\cdot\\|_1$和$f(\\cdot) = \\|\\cdot\\|_\\infty$的分类错误，并且注意到通常可以找到稀疏和一位解决方案，分别表现几乎与$f(\\cdot) = \\|\\cdot\\|^2_2$相同。",
    "tldr": "通过正则化回归，在超参数化范围内，根据特定选择的凸函数并适当增加一个正则化项，可以实现稀疏和一位解决方案，其性能几乎与最佳分类性能相同。",
    "en_tdlr": "Sparse and one-bit solutions with performance close to the best classification performance can be achieved via regularized regression in the over-parametrized regime by selecting specific convex functions and adding a regularization term appropriately."
}