{
    "title": "Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity",
    "abstract": "arXiv:2402.13130v1 Announce Type: new  Abstract: While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback. In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant sentence embeddings. The community tacitly stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity (STS). We notice a significant drop in performance when using the ELECTRA discriminator's last layer in comparison to earlier layers. We explore this drop and devise a way to repair ELECTRA's embeddings, proposing a novel truncated model fine-tuning (TMFT) method. TMFT improves the Spearman correlation coefficient by over 8 points while increasing parameter efficiency on the STS benchmark dataset. We extend our analysis to various model sizes and languages. Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, usi",
    "link": "https://arxiv.org/abs/2402.13130",
    "context": "Title: Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity\nAbstract: arXiv:2402.13130v1 Announce Type: new  Abstract: While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback. In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant sentence embeddings. The community tacitly stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity (STS). We notice a significant drop in performance when using the ELECTRA discriminator's last layer in comparison to earlier layers. We explore this drop and devise a way to repair ELECTRA's embeddings, proposing a novel truncated model fine-tuning (TMFT) method. TMFT improves the Spearman correlation coefficient by over 8 points while increasing parameter efficiency on the STS benchmark dataset. We extend our analysis to various model sizes and languages. Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, usi",
    "path": "papers/24/02/2402.13130.json",
    "total_tokens": 871,
    "translated_title": "ELECTRA的句子嵌入是否无法修复？语义文本相似性案例",
    "translated_abstract": "虽然BERT生成具有高质量的句子嵌入向量，但其预训练计算成本是一个明显的缺点。相比之下，ELECTRA提供了一种经济高效的预训练目标和下游任务性能提升，但其句子嵌入向量表现不佳。社区悄然停止使用ELECTRA的句子嵌入向量进行语义文本相似性（STS）任务。我们注意到使用ELECTRA鉴别器的最后一层相对于较早的层时性能显著下降。我们探索了这种下降，并设计了一种修复ELECTRA嵌入向量的方法，提出了一种新颖的截断模型微调（TMFT）方法。在STS基准数据集上，TMFT将Spearman相关系数提高了8个多点，同时提高了参数效率。我们将我们的分析扩展到各种模型大小和语言。此外，我们发现了ELECTRA生成模型的惊人功效，它的性能与BERT持平",
    "tldr": "该研究探索了ELECTRA句子嵌入向量性能问题，并提出了一种新的截断模型微调方法，显著提高了语义文本相似性任务的表现",
    "en_tdlr": "This study investigates the performance issues of ELECTRA sentence embeddings and proposes a new truncated model fine-tuning method that significantly improves semantic textual similarity tasks."
}