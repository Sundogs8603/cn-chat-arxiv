{
    "title": "Deconstructing the Goldilocks Zone of Neural Network Initialization",
    "abstract": "The second-order properties of the training loss have a massive impact on the optimization dynamics of deep learning models. Fort & Scherlis (2019) discovered that a high positive curvature and local convexity of the loss Hessian are associated with highly trainable initial points located in a region coined the \"Goldilocks zone\". Only a handful of subsequent studies touched upon this relationship, so it remains largely unexplained. In this paper, we present a rigorous and comprehensive analysis of the Goldilocks zone for homogeneous neural networks. In particular, we derive the fundamental condition resulting in non-zero positive curvature of the loss Hessian and argue that it is only incidentally related to the initialization norm, contrary to prior beliefs. Further, we relate high positive curvature to model confidence, low initial loss, and a previously unknown type of vanishing cross-entropy loss gradient. To understand the importance of positive curvature for trainability of deep ",
    "link": "https://arxiv.org/abs/2402.03579",
    "context": "Title: Deconstructing the Goldilocks Zone of Neural Network Initialization\nAbstract: The second-order properties of the training loss have a massive impact on the optimization dynamics of deep learning models. Fort & Scherlis (2019) discovered that a high positive curvature and local convexity of the loss Hessian are associated with highly trainable initial points located in a region coined the \"Goldilocks zone\". Only a handful of subsequent studies touched upon this relationship, so it remains largely unexplained. In this paper, we present a rigorous and comprehensive analysis of the Goldilocks zone for homogeneous neural networks. In particular, we derive the fundamental condition resulting in non-zero positive curvature of the loss Hessian and argue that it is only incidentally related to the initialization norm, contrary to prior beliefs. Further, we relate high positive curvature to model confidence, low initial loss, and a previously unknown type of vanishing cross-entropy loss gradient. To understand the importance of positive curvature for trainability of deep ",
    "path": "papers/24/02/2402.03579.json",
    "total_tokens": 917,
    "translated_title": "解构神经网络初始化的“金发女孩区”",
    "translated_abstract": "训练损失的二阶性质对深度学习模型的优化动力学有着巨大影响。Fort＆Scherlis（2019）发现，损失海森矩阵的高正曲率和局部凸性与位于被称为“金发女孩区”的高度可训练的初始点相关。只有少数几项后续研究涉及该关系，因此其仍然未被充分解释。在本文中，我们对均质神经网络的“金发女孩区”进行了严格而全面的分析。特别是，我们推导出导致损失海森矩阵非零正曲率的基本条件，并认为它与初始化范数只是偶然相关，与先前的信念相反。此外，我们将高正曲率与模型置信度、初始损失较低以及一种以前未知的消失的交叉熵损失梯度相关联。为了了解正曲率对深度学习模型的可训练性的重要性，我们研究学习的能力和效率，首先是通过泛化误差和高效率的学习算法的标准度量。",
    "tldr": "我们对神经网络的初始化进行了全面的分析，发现损失海森矩阵的高正曲率与可训练性强的初始点相关。与先前的观念相反，正曲率并不仅仅与初始化范数相关，而与模型置信度、初始损失较低以及一种以前未知的损失梯度消失相关。"
}