{
    "title": "Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision",
    "abstract": "Prompted weak supervision (PromptedWS) applies pre-trained large language models (LLMs) as the basis for labeling functions (LFs) in a weak supervision framework to obtain large labeled datasets. We further extend the use of LLMs in the loop to address one of the key challenges in weak supervision: learning the statistical dependency structure among supervision sources. In this work, we ask the LLM how similar are these prompted LFs. We propose a Structure Refining Module, a simple yet effective first approach based on the similarities of the prompts by taking advantage of the intrinsic structure in the embedding space. At the core of Structure Refining Module are Labeling Function Removal (LaRe) and Correlation Structure Generation (CosGen). Compared to previous methods that learn the dependencies from weak labels, our method finds the dependencies which are intrinsic to the LFs and less dependent on the data. We show that our Structure Refining Module improves the PromptedWS pipeline",
    "link": "https://arxiv.org/abs/2402.01867",
    "context": "Title: Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision\nAbstract: Prompted weak supervision (PromptedWS) applies pre-trained large language models (LLMs) as the basis for labeling functions (LFs) in a weak supervision framework to obtain large labeled datasets. We further extend the use of LLMs in the loop to address one of the key challenges in weak supervision: learning the statistical dependency structure among supervision sources. In this work, we ask the LLM how similar are these prompted LFs. We propose a Structure Refining Module, a simple yet effective first approach based on the similarities of the prompts by taking advantage of the intrinsic structure in the embedding space. At the core of Structure Refining Module are Labeling Function Removal (LaRe) and Correlation Structure Generation (CosGen). Compared to previous methods that learn the dependencies from weak labels, our method finds the dependencies which are intrinsic to the LFs and less dependent on the data. We show that our Structure Refining Module improves the PromptedWS pipeline",
    "path": "papers/24/02/2402.01867.json",
    "total_tokens": 866,
    "translated_title": "利用大型语言模型在提示式弱监督中进行结构学习",
    "translated_abstract": "提示式弱监督（PromptedWS）将预训练的大型语言模型（LLMs）应用于弱监督框架中的标注函数（LFs），以获取大规模标记数据集。我们进一步扩展了LLMs在循环中的使用，以解决弱监督中的一个关键挑战：学习监督源之间的统计依赖结构。在这项工作中，我们询问LLM这些提示式LFs有多相似。我们提出了一个结构细化模块，它是一种基于提示相似性的简单而有效的方法，利用嵌入空间中的内在结构。结构细化模块的核心是标注函数移除（LaRe）和相关结构生成（CosGen）。与之前从弱标签中学习依赖关系的方法相比，我们的方法找到了对LFs内在且不太依赖数据的依赖关系。我们展示了我们的结构细化模块如何改进提示式弱监督流程。",
    "tldr": "这项工作利用大型语言模型在提示式弱监督中学习标注函数之间的统计依赖结构，通过结构细化模块提高了提示式弱监督流程的效果。",
    "en_tdlr": "This work leverages large language models to learn the statistical dependency structure among labeling functions in prompted weak supervision, and improves the effectiveness of the prompted weak supervision pipeline with a structure refining module."
}