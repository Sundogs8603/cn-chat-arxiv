{
    "title": "When Do Off-Policy and On-Policy Policy Gradient Methods Align?",
    "abstract": "arXiv:2402.12034v1 Announce Type: cross  Abstract: Policy gradient methods are widely adopted reinforcement learning algorithms for tasks with continuous action spaces. These methods succeeded in many application domains, however, because of their notorious sample inefficiency their use remains limited to problems where fast and accurate simulations are available. A common way to improve sample efficiency is to modify their objective function to be computable from off-policy samples without importance sampling. A well-established off-policy objective is the excursion objective. This work studies the difference between the excursion objective and the traditional on-policy objective, which we refer to as the on-off gap. We provide the first theoretical analysis showing conditions to reduce the on-off gap while establishing empirical evidence of shortfalls arising when these conditions are not met.",
    "link": "https://arxiv.org/abs/2402.12034",
    "context": "Title: When Do Off-Policy and On-Policy Policy Gradient Methods Align?\nAbstract: arXiv:2402.12034v1 Announce Type: cross  Abstract: Policy gradient methods are widely adopted reinforcement learning algorithms for tasks with continuous action spaces. These methods succeeded in many application domains, however, because of their notorious sample inefficiency their use remains limited to problems where fast and accurate simulations are available. A common way to improve sample efficiency is to modify their objective function to be computable from off-policy samples without importance sampling. A well-established off-policy objective is the excursion objective. This work studies the difference between the excursion objective and the traditional on-policy objective, which we refer to as the on-off gap. We provide the first theoretical analysis showing conditions to reduce the on-off gap while establishing empirical evidence of shortfalls arising when these conditions are not met.",
    "path": "papers/24/02/2402.12034.json",
    "total_tokens": 816,
    "translated_title": "离策略和在策略策略梯度方法何时能够一致？",
    "translated_abstract": "策略梯度方法是广泛采用的在连续动作空间中的强化学习算法。这些方法在许多应用领域取得成功，然而由于其臭名昭著的样本效率低，它们的使用仍然局限于可以快速准确模拟的问题。改进样本效率的常见方法是修改它们的目标函数，使之能够从离策略样本中计算而无需重要性采样。一个成熟的离策略目标就是游荡目标。本文研究了游荡目标与传统在策略目标之间的差异，我们称之为在离之间的差距。我们提供了第一个理论分析，展示了减少在离差距的条件，同时建立了当这些条件未被满足时出现的缺陷的经验证据。",
    "tldr": "该论文研究了离策略和在策略策略梯度方法之间的差异，并首次提出了减小该差距的条件，同时发现在条件不满足时会产生短板。",
    "en_tdlr": "This paper investigates the differences between off-policy and on-policy policy gradient methods and provides the first theoretical analysis on reducing the gap between them, as well as empirical evidence of shortfalls when conditions are not met."
}