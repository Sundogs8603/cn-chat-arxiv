{
    "title": "How Flawed is ECE? An Analysis via Logit Smoothing",
    "abstract": "arXiv:2402.10046v1 Announce Type: new  Abstract: Informally, a model is calibrated if its predictions are correct with a probability that matches the confidence of the prediction. By far the most common method in the literature for measuring calibration is the expected calibration error (ECE). Recent work, however, has pointed out drawbacks of ECE, such as the fact that it is discontinuous in the space of predictors. In this work, we ask: how fundamental are these issues, and what are their impacts on existing results? Towards this end, we completely characterize the discontinuities of ECE with respect to general probability measures on Polish spaces. We then use the nature of these discontinuities to motivate a novel continuous, easily estimated miscalibration metric, which we term Logit-Smoothed ECE (LS-ECE). By comparing the ECE and LS-ECE of pre-trained image classification models, we show in initial experiments that binned ECE closely tracks LS-ECE, indicating that the theoretical",
    "link": "https://arxiv.org/abs/2402.10046",
    "context": "Title: How Flawed is ECE? An Analysis via Logit Smoothing\nAbstract: arXiv:2402.10046v1 Announce Type: new  Abstract: Informally, a model is calibrated if its predictions are correct with a probability that matches the confidence of the prediction. By far the most common method in the literature for measuring calibration is the expected calibration error (ECE). Recent work, however, has pointed out drawbacks of ECE, such as the fact that it is discontinuous in the space of predictors. In this work, we ask: how fundamental are these issues, and what are their impacts on existing results? Towards this end, we completely characterize the discontinuities of ECE with respect to general probability measures on Polish spaces. We then use the nature of these discontinuities to motivate a novel continuous, easily estimated miscalibration metric, which we term Logit-Smoothed ECE (LS-ECE). By comparing the ECE and LS-ECE of pre-trained image classification models, we show in initial experiments that binned ECE closely tracks LS-ECE, indicating that the theoretical",
    "path": "papers/24/02/2402.10046.json",
    "total_tokens": 911,
    "translated_title": "ECE有多大的缺陷？通过对数平滑的分析",
    "translated_abstract": "简而言之，如果一个模型的预测准确率与置信度匹配，那么这个模型就是校准的。在现有文献中，衡量校准性最常见的方法是期望校准误差（ECE）。然而，最近的研究指出了ECE的缺点，例如它在预测者空间中是不连续的。本研究探讨了这些问题有多本质，并分析了它们对现有结果的影响。为此，我们完全描述了ECE对波兰空间上的一般概率测度的不连续性。然后，我们利用这些不连续性提出了一种新的连续、易于估计的误差测度，称为Logit-Smoothed ECE（LS-ECE）。通过比较预训练图像分类模型的ECE和LS-ECE，我们在初步实验中发现，分箱ECE与LS-ECE非常接近，表明理论方面是相符的。",
    "tldr": "本研究通过分析对数平滑，探讨了ECE的缺陷以及对现有结果的影响，并提出了一种新的连续、易于估计的误差测度LS-ECE。通过实验发现，LS-ECE与分箱ECE非常接近。",
    "en_tdlr": "This study analyzes the flaws of ECE and their impact on existing results through logit smoothing, and proposes a new continuous and easily estimated error metric, LS-ECE. Experimental results show that LS-ECE closely aligns with binned ECE."
}