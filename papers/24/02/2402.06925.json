{
    "title": "A Thorough Examination of Decoding Methods in the Era of LLMs",
    "abstract": "Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining opt",
    "link": "https://arxiv.org/abs/2402.06925",
    "context": "Title: A Thorough Examination of Decoding Methods in the Era of LLMs\nAbstract: Decoding methods play an indispensable role in converting language models from next-token predictors into practical task solvers. Prior research on decoding methods, primarily focusing on task-specific models, may not extend to the current era of general-purpose large language models (LLMs). Moreover, the recent influx of decoding strategies has further complicated this landscape. This paper provides a comprehensive and multifaceted analysis of various decoding methods within the context of LLMs, evaluating their performance, robustness to hyperparameter changes, and decoding speeds across a wide range of tasks, models, and deployment environments. Our findings reveal that decoding method performance is notably task-dependent and influenced by factors such as alignment, model size, and quantization. Intriguingly, sensitivity analysis exposes that certain methods achieve superior performance at the cost of extensive hyperparameter tuning, highlighting the trade-off between attaining opt",
    "path": "papers/24/02/2402.06925.json",
    "total_tokens": 920,
    "translated_title": "LLM时代解码方法的综合研究",
    "translated_abstract": "解码方法在将语言模型从下一个标记预测器转换为实际任务解决器中起着不可或缺的作用。以往关于解码方法的研究主要集中在任务特定模型上，可能不适用于当前通用型大型语言模型(LLMs)的时代。此外，最近解码策略的涌入进一步复杂了这个领域。本文在LLMs的背景下，对各种解码方法进行了全面而多方位的分析，评估了它们在各种任务、模型和部署环境中的性能、对超参数变化的鲁棒性以及解码速度。我们的研究结果表明，解码方法的性能明显与任务相关，并受到对齐、模型大小和量化等因素的影响。有趣的是，敏感性分析揭示了某些方法在需要进行大量超参数调整的前提下能够实现更好的性能，突出了在达到最佳性能之间的权衡关系。",
    "tldr": "在LLMs的背景下，本文综合研究了各种解码方法的性能、鲁棒性和解码速度，并发现解码方法的性能与任务相关，受到对齐、模型大小和量化等因素影响；某些方法可以通过大量超参数调整达到更好的性能，但需要权衡取舍。"
}