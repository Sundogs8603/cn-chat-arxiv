{
    "title": "Gradient-enhanced deep Gaussian processes for multifidelity modelling",
    "abstract": "arXiv:2402.16059v1 Announce Type: cross  Abstract: Multifidelity models integrate data from multiple sources to produce a single approximator for the underlying process. Dense low-fidelity samples are used to reduce interpolation error, while sparse high-fidelity samples are used to compensate for bias or noise in the low-fidelity samples. Deep Gaussian processes (GPs) are attractive for multifidelity modelling as they are non-parametric, robust to overfitting, perform well for small datasets, and, critically, can capture nonlinear and input-dependent relationships between data of different fidelities. Many datasets naturally contain gradient data, especially when they are generated by computational models that are compatible with automatic differentiation or have adjoint solutions. Principally, this work extends deep GPs to incorporate gradient data. We demonstrate this method on an analytical test problem and a realistic partial differential equation problem, where we predict the aer",
    "link": "https://arxiv.org/abs/2402.16059",
    "context": "Title: Gradient-enhanced deep Gaussian processes for multifidelity modelling\nAbstract: arXiv:2402.16059v1 Announce Type: cross  Abstract: Multifidelity models integrate data from multiple sources to produce a single approximator for the underlying process. Dense low-fidelity samples are used to reduce interpolation error, while sparse high-fidelity samples are used to compensate for bias or noise in the low-fidelity samples. Deep Gaussian processes (GPs) are attractive for multifidelity modelling as they are non-parametric, robust to overfitting, perform well for small datasets, and, critically, can capture nonlinear and input-dependent relationships between data of different fidelities. Many datasets naturally contain gradient data, especially when they are generated by computational models that are compatible with automatic differentiation or have adjoint solutions. Principally, this work extends deep GPs to incorporate gradient data. We demonstrate this method on an analytical test problem and a realistic partial differential equation problem, where we predict the aer",
    "path": "papers/24/02/2402.16059.json",
    "total_tokens": 825,
    "translated_title": "梯度增强深高斯过程用于多保真建模",
    "translated_abstract": "多保真模型整合来自多个来源的数据，生成底层过程的单一逼近器。密集的低保真样本用于减少插值误差，稀疏的高保真样本用于弥补低保真样本中的偏差或噪音。梯度增强的深高斯过程对多保真建模具有吸引力，因为它们是非参数的，不容易过拟合，在小数据集上表现良好，并且关键是能够捕获不同保真数据之间非线性和输入相关的关系。许多数据集自然包含梯度数据，特别是当它们由与自动微分兼容或具有共轭解的计算模型生成时。本工作主要是将深高斯过程扩展为包含梯度数据。我们在一个分析测试问题和一个现实的偏微分方程问题上演示了这种方法，我们在这两个问题中进行了气动预测",
    "tldr": "这项工作将深高斯过程扩展到包含梯度数据，用于多保真建模，能够捕获不同保真数据之间的非线性和输入相关关系。",
    "en_tdlr": "This work extends deep Gaussian processes to incorporate gradient data for multifidelity modeling, enabling capturing nonlinear and input-dependent relationships between data of different fidelities."
}