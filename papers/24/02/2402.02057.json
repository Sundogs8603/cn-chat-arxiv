{
    "title": "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding",
    "abstract": "Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding",
    "link": "https://arxiv.org/abs/2402.02057",
    "context": "Title: Break the Sequential Dependency of LLM Inference Using Lookahead Decoding\nAbstract: Autoregressive decoding of large language models (LLMs) is memory bandwidth bounded, resulting in high latency and significant wastes of the parallel processing power of modern accelerators. Existing methods for accelerating LLM decoding often require a draft model (e.g., speculative decoding), which is nontrivial to obtain and unable to generalize. In this paper, we introduce Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM decoding without needing auxiliary models or data stores. It allows trading per-step log(FLOPs) to reduce the number of total decoding steps, is more parallelizable on single or multiple modern accelerators, and is compatible with concurrent memory-efficient attention (e.g., FlashAttention). Our implementation of Lookahead decoding can speed up autoregressive decoding by up to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code completion tasks. Our code is avialable at https://github.com/hao-ai-lab/LookaheadDecoding",
    "path": "papers/24/02/2402.02057.json",
    "total_tokens": 1019,
    "translated_title": "打破LLM推理的顺序依赖：使用前瞻解码",
    "translated_abstract": "大型语言模型（LLM）的自回归解码受到内存带宽限制，导致延迟较高，并且浪费了现代加速器的并行处理能力。现有的加速LLM解码的方法通常需要草稿模型（例如，推测解码），这样的模型不易获取且无法推广。在本文中，我们介绍了前瞻解码，一种精确的并行解码算法，可以加速LLM解码，而无需辅助模型或数据存储。它允许交换每步log（FLOPs）以减少总解码步骤的数量，在单个或多个现代加速器上更易于并行化，并且与并发内存高效的注意力机制（例如FlashAttention）兼容。我们的前瞻解码实现可以在MT-bench上加速自回归解码1.8倍，并在多个GPU上实现强扩展性，代码补全任务上加速4倍。我们的代码可在https://github.com/hao-ai-lab/LookaheadDecoding找到。",
    "tldr": "本文介绍了一种称为前瞻解码的精确、并行解码算法，通过交换每步操作数以减少总解码步骤的数量，加速了大型语言模型（LLM）的解码过程。它不需要辅助模型或数据存储，并且与并发内存高效的注意力机制兼容。实验证明，在代码补全任务中，前瞻解码可将自回归解码加速1.8倍，并且在多个GPU上实现强扩展性。"
}