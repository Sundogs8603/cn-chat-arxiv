{
    "title": "Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs",
    "abstract": "arXiv:2402.08733v1 Announce Type: new Abstract: Identifying how much a model ${\\widehat{p}}_{\\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or \"hallucinated\" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\\widehat{p}}_{\\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to \"cheat\" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we pr",
    "link": "https://arxiv.org/abs/2402.08733",
    "context": "Title: Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs\nAbstract: arXiv:2402.08733v1 Announce Type: new Abstract: Identifying how much a model ${\\widehat{p}}_{\\theta}(Y|X)$ knows about the stochastic real-world process $p(Y|X)$ it was trained on is important to ensure it avoids producing incorrect or \"hallucinated\" answers or taking unsafe actions. But this is difficult for generative models because probabilistic predictions do not distinguish between per-response noise (aleatoric uncertainty) and lack of knowledge about the process (epistemic uncertainty), and existing epistemic uncertainty quantification techniques tend to be overconfident when the model underfits. We propose a general strategy for teaching a model to both approximate $p(Y|X)$ and also estimate the remaining gaps between ${\\widehat{p}}_{\\theta}(Y|X)$ and $p(Y|X)$: train it to predict pairs of independent responses drawn from the true conditional distribution, allow it to \"cheat\" by observing one response while predicting the other, then measure how much it cheats. Remarkably, we pr",
    "path": "papers/24/02/2402.08733.json",
    "total_tokens": 914,
    "translated_title": "专家不作弊: 通过预测对偶来学习未知信息",
    "translated_abstract": "模型${\\widehat{p}}_{\\theta}(Y|X)$对于其训练数据$p(Y|X)$的了解程度的准确评估对于避免产生错误或\"虚构\"的答案或采取不安全的行为非常重要，然而这对于生成模型来说是困难的，因为概率预测不能区分每个响应的噪声（广义不确定性）和对过程的不了解（专题不确定性），而现有的专题不确定性量化技术往往在模型欠拟合时过于自信。我们提出了一种通用策略，可以教导模型同时逼近$p(Y|X)$并估计${\\widehat{p}}_{\\theta}(Y|X)$与$p(Y|X)$之间的差距：训练模型预测来自真实条件分布的独立响应对，允许它在预测一个响应时观察另一个响应，然后测量它的作弊程度。令人惊讶的是，我们证明了这种策略可以准确估计模型的专题不确定性。",
    "tldr": "通过预测对偶的方法，我们提出了一种教导模型逼近真实条件分布并估计模型的不确定性的通用策略。"
}