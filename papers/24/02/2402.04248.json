{
    "title": "Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks",
    "abstract": "State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, \\variant, that combines Mamba with attention bloc",
    "link": "https://arxiv.org/abs/2402.04248",
    "context": "Title: Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks\nAbstract: State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, \\variant, that combines Mamba with attention bloc",
    "path": "papers/24/02/2402.04248.json",
    "total_tokens": 862,
    "translated_title": "Mamba能学习如何学习吗？对上下文学习任务进行比较研究",
    "translated_abstract": "状态空间模型（SSMs）被提出作为语言建模中替代Transformer网络的选择，通过引入门控、卷积和基于输入的令牌选择来缓解多头注意力的二次成本。虽然SSMs表现出竞争性能，但与Transformer相比，它们在上下文学习（ICL）能力方面仍然是未充分探索的。在这项研究中，我们评估了SSMs的ICL性能，着重研究了Mamba在各种任务中与Transformer模型的比较。我们的结果表明，在标准回归ICL任务中，SSMs的表现与Transformer相当，而在稀疏奇偶学习等任务中超过了Transformer。然而，在涉及非标准检索功能的任务中，SSMs表现不佳。为了解决这些局限性，我们引入了一种混合模型\\variant，它将Mamba与注意力库结合起来。",
    "tldr": "本研究评估了Mamba在上下文学习任务中的性能，并与Transformer模型进行比较。结果显示，SSMs在标准回归任务中与Transformer性能相当，在稀疏奇偶学习等任务中表现优异，但在涉及非标准检索功能的任务中表现不佳。为了解决这些局限性，引入了一种混合模型。"
}