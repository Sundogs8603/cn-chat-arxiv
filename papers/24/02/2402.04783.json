{
    "title": "Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate Networks",
    "abstract": "Recently, neural networks utilizing periodic activation functions have been proven to demonstrate superior performance in vision tasks compared to traditional ReLU-activated networks. However, there is still a limited understanding of the underlying reasons for this improved performance. In this paper, we aim to address this gap by providing a theoretical understanding of periodically activated networks through an analysis of their Neural Tangent Kernel (NTK). We derive bounds on the minimum eigenvalue of their NTK in the finite width setting, using a fairly general network architecture which requires only one wide layer that grows at least linearly with the number of data samples. Our findings indicate that periodically activated networks are \\textit{notably more well-behaved}, from the NTK perspective, than ReLU activated networks. Additionally, we give an application to the memorization capacity of such networks and verify our theoretical predictions empirically. Our study offers a ",
    "link": "https://arxiv.org/abs/2402.04783",
    "context": "Title: Analyzing the Neural Tangent Kernel of Periodically Activated Coordinate Networks\nAbstract: Recently, neural networks utilizing periodic activation functions have been proven to demonstrate superior performance in vision tasks compared to traditional ReLU-activated networks. However, there is still a limited understanding of the underlying reasons for this improved performance. In this paper, we aim to address this gap by providing a theoretical understanding of periodically activated networks through an analysis of their Neural Tangent Kernel (NTK). We derive bounds on the minimum eigenvalue of their NTK in the finite width setting, using a fairly general network architecture which requires only one wide layer that grows at least linearly with the number of data samples. Our findings indicate that periodically activated networks are \\textit{notably more well-behaved}, from the NTK perspective, than ReLU activated networks. Additionally, we give an application to the memorization capacity of such networks and verify our theoretical predictions empirically. Our study offers a ",
    "path": "papers/24/02/2402.04783.json",
    "total_tokens": 790,
    "translated_title": "周期激活坐标网络的神经切向核分析",
    "translated_abstract": "最近，利用周期性激活函数的神经网络在视觉任务中表现出了比传统的ReLU激活网络更好的性能。然而，对于这种改进性能的原因还存在有限的了解。本文旨在通过分析神经切向核（NTK）来提供对周期性激活网络的理论理解。我们在有限宽度的设置下推导出NTK的最小特征值的上界，使用了一个相对通用的网络架构，只需要一个宽度至少与数据样本数量线性增长的层。我们的发现表明，从NTK的角度来看，周期性激活网络比ReLU激活网络更加“良好”。此外，我们还将这些网络的记忆容量应用于一个案例，并通过实验验证了我们的理论预测。我们的研究提供了一个新的理论视角来解释周期性激活网络的性能提升。",
    "tldr": "通过分析神经切向核（NTK），本文提供了对周期性激活网络的理论理解，并发现周期性激活网络在NTK的角度上比ReLU激活网络更加“良好”。此外，我们还验证了这些网络的记忆容量。"
}