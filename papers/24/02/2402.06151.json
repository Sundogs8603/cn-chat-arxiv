{
    "title": "POTEC: Off-Policy Learning for Large Action Spaces via Two-Stage Policy Decomposition",
    "abstract": "We study off-policy learning (OPL) of contextual bandit policies in large discrete action spaces where existing methods -- most of which rely crucially on reward-regression models or importance-weighted policy gradients -- fail due to excessive bias or variance. To overcome these issues in OPL, we propose a novel two-stage algorithm, called Policy Optimization via Two-Stage Policy Decomposition (POTEC). It leverages clustering in the action space and learns two different policies via policy- and regression-based approaches, respectively. In particular, we derive a novel low-variance gradient estimator that enables to learn a first-stage policy for cluster selection efficiently via a policy-based approach. To select a specific action within the cluster sampled by the first-stage policy, POTEC uses a second-stage policy derived from a regression-based approach within each cluster. We show that a local correctness condition, which only requires that the regression model preserves the rela",
    "link": "https://arxiv.org/abs/2402.06151",
    "context": "Title: POTEC: Off-Policy Learning for Large Action Spaces via Two-Stage Policy Decomposition\nAbstract: We study off-policy learning (OPL) of contextual bandit policies in large discrete action spaces where existing methods -- most of which rely crucially on reward-regression models or importance-weighted policy gradients -- fail due to excessive bias or variance. To overcome these issues in OPL, we propose a novel two-stage algorithm, called Policy Optimization via Two-Stage Policy Decomposition (POTEC). It leverages clustering in the action space and learns two different policies via policy- and regression-based approaches, respectively. In particular, we derive a novel low-variance gradient estimator that enables to learn a first-stage policy for cluster selection efficiently via a policy-based approach. To select a specific action within the cluster sampled by the first-stage policy, POTEC uses a second-stage policy derived from a regression-based approach within each cluster. We show that a local correctness condition, which only requires that the regression model preserves the rela",
    "path": "papers/24/02/2402.06151.json",
    "total_tokens": 867,
    "translated_title": "POTEC:通过两阶段策略分解的大动作空间离策略学习",
    "translated_abstract": "我们研究了在存在大离散动作空间的情境吞噬机制中的离线策略学习(OPL)，现有方法大多依赖于回归模型或重要性加权策略梯度，但由于过高的偏差或方差而失败。为了克服OPL中的这些问题，我们提出了一种新的两阶段算法，称为两阶段策略分解的策略优化(POTEC)。它利用动作空间中的聚类，并分别通过基于策略和回归的方法学习两种不同的策略。特别地，我们推导出一种新颖的低方差梯度估计器，通过基于策略的方法高效地学习第一阶段策略以选择聚类。为了在第一阶段策略采样的聚类中选择特定动作，POTEC在每个聚类中使用来自回归方法的第二阶段策略。我们展示了一种局部正确性条件，该条件仅要求回归模型保持相关性。",
    "tldr": "POTEC提出了一种两阶段策略分解的算法，在大离散动作空间中有效进行离策略学习。该算法利用聚类选择第一阶段策略，并利用回归方法选择每个聚类内的具体动作。"
}