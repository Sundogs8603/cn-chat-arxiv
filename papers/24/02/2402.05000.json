{
    "title": "Pedagogical Alignment of Large Language Models",
    "abstract": "In this paper, we introduce the novel concept of pedagogically aligned Large Language Models (LLMs) that signifies a transformative shift in the application of LLMs within educational contexts. Rather than providing direct responses to user queries, pedagogically-aligned LLMs function as scaffolding tools, breaking complex problems into manageable subproblems and guiding students towards the final answer through constructive feedback and hints. The objective is to equip learners with problem-solving strategies that deepen their understanding and internalization of the subject matter. Previous research in this field has primarily applied the supervised finetuning approach without framing the objective as an alignment problem, hence not employing reinforcement learning through human feedback (RLHF) methods. This study reinterprets the narrative by viewing the task through the lens of alignment and demonstrates how RLHF methods emerge naturally as a superior alternative for aligning LLM b",
    "link": "https://arxiv.org/abs/2402.05000",
    "context": "Title: Pedagogical Alignment of Large Language Models\nAbstract: In this paper, we introduce the novel concept of pedagogically aligned Large Language Models (LLMs) that signifies a transformative shift in the application of LLMs within educational contexts. Rather than providing direct responses to user queries, pedagogically-aligned LLMs function as scaffolding tools, breaking complex problems into manageable subproblems and guiding students towards the final answer through constructive feedback and hints. The objective is to equip learners with problem-solving strategies that deepen their understanding and internalization of the subject matter. Previous research in this field has primarily applied the supervised finetuning approach without framing the objective as an alignment problem, hence not employing reinforcement learning through human feedback (RLHF) methods. This study reinterprets the narrative by viewing the task through the lens of alignment and demonstrates how RLHF methods emerge naturally as a superior alternative for aligning LLM b",
    "path": "papers/24/02/2402.05000.json",
    "total_tokens": 872,
    "translated_title": "大型语言模型的教学对齐",
    "translated_abstract": "在本文中，我们引入了教学对齐的大型语言模型（LLM）的新概念，这在教育背景下应用LLM具有转变性的意义。与直接回答用户问题不同，教学对齐的LLM作为辅助工具，将复杂问题分解为可管理的子问题，并通过建设性的反馈和提示指导学生找到最终答案。其目标是为学习者提供解决问题的策略，以加深他们对主题的理解和内化。先前的研究主要采用了监督微调方法，没有将目标定义为对齐问题，并未使用通过人类反馈的强化学习方法（RLHF）。本研究通过对齐的视角重新解释了这一论述，并展示了RLHF方法作为对齐LLM的优越替代方法。",
    "tldr": "本文介绍了教学对齐的大型语言模型（LLM）的新概念，并探讨了通过建设性反馈和提示指导学生解决复杂问题的方法。相比于传统方法，这种对齐方法以及采用人类反馈的强化学习方法能够更好地对齐LLM，提供更优质的教育支持。",
    "en_tdlr": "This paper introduces the novel concept of pedagogically aligned Large Language Models (LLMs) and explores the use of constructive feedback and hints to guide students in solving complex problems. Compared to traditional methods, this alignment approach and the use of reinforcement learning through human feedback offer better educational support and improve the alignment of LLMs."
}