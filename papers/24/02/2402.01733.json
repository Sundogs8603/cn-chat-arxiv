{
    "title": "Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report",
    "abstract": "Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine.   Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison.   Results: The LLM-RA",
    "link": "https://arxiv.org/abs/2402.01733",
    "context": "Title: Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report\nAbstract: Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine.   Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison.   Results: The LLM-RA",
    "path": "papers/24/02/2402.01733.json",
    "total_tokens": 839,
    "translated_title": "大型语言模型中检索增强生成的开发和测试--案例研究报告",
    "translated_abstract": "目的：大型语言模型(LLMs)在医学应用中具有重要的潜力。检索增强生成(RAG)作为一种有前景的方法，用于定制LLMs中的领域知识。本案例研究介绍了一个专为医疗保健定制的LLM-RAG流程的开发和评估，重点关注术前医学。方法：我们使用了35个术前指南开发了一个LLM-RAG模型，并通过与人工生成的回答进行测试，共评估了1260个回答。RAG流程涉及使用基于Python的LangChain和Llamaindex框架将临床文档转换为文本，并将这些文本处理为块以用于嵌入和检索。利用Pinecone进行向量存储和使用1536维余弦相似度损失度量来优化数据检索，其中选择了嵌入模型。将由初级医生提供的人工生成回答用作比较。结果：LLM-RA",
    "tldr": "RAG模型是一种有前景的方法，用于在大型语言模型中定制领域知识。本研究开发和评估了一个专为医疗保健定制的LLM-RAG流程，重点关注术前医学。"
}