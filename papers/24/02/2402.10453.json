{
    "title": "Steering Conversational Large Language Models for Long Emotional Support Conversations",
    "abstract": "arXiv:2402.10453v1 Announce Type: new  Abstract: In this study, we address the challenge of consistently following emotional support strategies in long conversations by large language models (LLMs). We introduce the Strategy-Relevant Attention (SRA) metric, a model-agnostic measure designed to evaluate the effectiveness of LLMs in adhering to strategic prompts in emotional support contexts. By analyzing conversations within the Emotional Support Conversations dataset (ESConv) using LLaMA models, we demonstrate that SRA is significantly correlated with a model's ability to sustain the outlined strategy throughout the interactions. Our findings reveal that the application of SRA-informed prompts leads to enhanced strategic adherence, resulting in conversations that more reliably exhibit the desired emotional support strategies over longer conversations. Furthermore, we contribute a comprehensive, multi-branch synthetic conversation dataset for ESConv, featuring a variety of strategy cont",
    "link": "https://arxiv.org/abs/2402.10453",
    "context": "Title: Steering Conversational Large Language Models for Long Emotional Support Conversations\nAbstract: arXiv:2402.10453v1 Announce Type: new  Abstract: In this study, we address the challenge of consistently following emotional support strategies in long conversations by large language models (LLMs). We introduce the Strategy-Relevant Attention (SRA) metric, a model-agnostic measure designed to evaluate the effectiveness of LLMs in adhering to strategic prompts in emotional support contexts. By analyzing conversations within the Emotional Support Conversations dataset (ESConv) using LLaMA models, we demonstrate that SRA is significantly correlated with a model's ability to sustain the outlined strategy throughout the interactions. Our findings reveal that the application of SRA-informed prompts leads to enhanced strategic adherence, resulting in conversations that more reliably exhibit the desired emotional support strategies over longer conversations. Furthermore, we contribute a comprehensive, multi-branch synthetic conversation dataset for ESConv, featuring a variety of strategy cont",
    "path": "papers/24/02/2402.10453.json",
    "total_tokens": 872,
    "translated_title": "引导情感支持对话的大型语言模型进行长时间对话",
    "translated_abstract": "在这项研究中，我们解决了大型语言模型（LLMs）在长时间对话中一贯遵循情感支持策略的挑战。我们引入了Strategy-Relevant Attention（SRA）度量，这是一个模型不可知的指标，旨在评估LLMs在情感支持环境中遵循战略提示的有效性。通过使用LLaMA模型分析情感支持对话数据集（ESConv）中的对话，我们证明SRA与模型在整个互动过程中维持所述策略能力密切相关。我们的研究结果显示，应用基于SRA的提示可提高战略依从性，导致对话更可靠地展示长时间对话中所需的情感支持策略。此外，我们贡献了一个全面的、多分支的合成对话数据集，适用于ESConv，其中包含各种策略内容。",
    "tldr": "引入了Strategy-Relevant Attention（SRA）度量，评估大型语言模型在情感支持对话中遵循战略提示的有效性，研究发现应用SRA指导的提示可提高战略依从性，从而使长时间对话更可靠地展示所需的情感支持策略。",
    "en_tdlr": "Introduced the Strategy-Relevant Attention (SRA) metric to evaluate the effectiveness of large language models in adhering to strategic prompts in emotional support conversations, findings show that applying SRA-informed prompts enhances strategic adherence, leading to longer and more reliably exhibit desired emotional support strategies."
}