{
    "title": "Efficient and Scalable Fine-Tune of Language Models for Genome Understanding",
    "abstract": "Although DNA foundation models have advanced the understanding of genomes, they still face significant challenges in the limited scale and diversity of genomic data. This limitation starkly contrasts with the success of natural language foundation models, which thrive on substantially larger scales. Furthermore, genome understanding involves numerous downstream genome annotation tasks with inherent data heterogeneity, thereby necessitating more efficient and robust fine-tuning methods tailored for genomics. Here, we present \\textsc{Lingo}: \\textsc{L}anguage prefix f\\textsc{In}e-tuning for \\textsc{G}en\\textsc{O}mes. Unlike DNA foundation models, \\textsc{Lingo} strategically leverages natural language foundation models' contextual cues, recalibrating their linguistic knowledge to genomic sequences. \\textsc{Lingo} further accommodates numerous, heterogeneous downstream fine-tune tasks by an adaptive rank sampling method that prunes and stochastically reintroduces pruned singular vectors w",
    "link": "https://arxiv.org/abs/2402.08075",
    "context": "Title: Efficient and Scalable Fine-Tune of Language Models for Genome Understanding\nAbstract: Although DNA foundation models have advanced the understanding of genomes, they still face significant challenges in the limited scale and diversity of genomic data. This limitation starkly contrasts with the success of natural language foundation models, which thrive on substantially larger scales. Furthermore, genome understanding involves numerous downstream genome annotation tasks with inherent data heterogeneity, thereby necessitating more efficient and robust fine-tuning methods tailored for genomics. Here, we present \\textsc{Lingo}: \\textsc{L}anguage prefix f\\textsc{In}e-tuning for \\textsc{G}en\\textsc{O}mes. Unlike DNA foundation models, \\textsc{Lingo} strategically leverages natural language foundation models' contextual cues, recalibrating their linguistic knowledge to genomic sequences. \\textsc{Lingo} further accommodates numerous, heterogeneous downstream fine-tune tasks by an adaptive rank sampling method that prunes and stochastically reintroduces pruned singular vectors w",
    "path": "papers/24/02/2402.08075.json",
    "total_tokens": 862,
    "translated_title": "高效可扩展的基因理解语言模型微调方法",
    "translated_abstract": "尽管DNA基因组模型在基因理解方面取得了进展，但在基因组数据规模和多样性方面仍面临重大挑战。这种限制与自然语言模型相比形成鲜明对比，后者在更大规模上取得了成功。此外，基因理解涉及许多具有固有数据异质性的下游基因组注释任务，因此需要更高效且稳健的针对基因组的微调方法。在这里，我们提出了Lingo：语言前缀微调基因组模型。与DNA基因组模型不同，Lingo策略性地利用了自然语言模型的上下文提示，重新校准其在基因组序列方面的语言知识。Lingo通过自适应秩采样方法进一步适应了许多不同的、异质的下游微调任务，该方法剪枝并随机重新引入剪枝的奇异向量。",
    "tldr": "这篇论文提出了一种名为Lingo的高效可扩展的基因理解语言模型微调方法，该方法利用了自然语言模型的上下文提示，并通过自适应秩采样方法适应了基因组注释的多样性任务。"
}