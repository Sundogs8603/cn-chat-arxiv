{
    "title": "Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models",
    "abstract": "arXiv:2402.17184v1 Announce Type: new  Abstract: The accuracy of end-to-end (E2E) automatic speech recognition (ASR) models continues to improve as they are scaled to larger sizes, with some now reaching billions of parameters. Widespread deployment and adoption of these models, however, requires computationally efficient strategies for decoding. In the present work, we study one such strategy: applying multiple frame reduction layers in the encoder to compress encoder outputs into a small number of output frames. While similar techniques have been investigated in previous work, we achieve dramatically more reduction than has previously been demonstrated through the use of multiple funnel reduction layers. Through ablations, we study the impact of various architectural choices in the encoder to identify the most effective strategies. We demonstrate that we can generate one encoder output frame for every 2.56 sec of input speech, without significantly affecting word error rate on a larg",
    "link": "https://arxiv.org/abs/2402.17184",
    "context": "Title: Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models\nAbstract: arXiv:2402.17184v1 Announce Type: new  Abstract: The accuracy of end-to-end (E2E) automatic speech recognition (ASR) models continues to improve as they are scaled to larger sizes, with some now reaching billions of parameters. Widespread deployment and adoption of these models, however, requires computationally efficient strategies for decoding. In the present work, we study one such strategy: applying multiple frame reduction layers in the encoder to compress encoder outputs into a small number of output frames. While similar techniques have been investigated in previous work, we achieve dramatically more reduction than has previously been demonstrated through the use of multiple funnel reduction layers. Through ablations, we study the impact of various architectural choices in the encoder to identify the most effective strategies. We demonstrate that we can generate one encoder output frame for every 2.56 sec of input speech, without significantly affecting word error rate on a larg",
    "path": "papers/24/02/2402.17184.json",
    "total_tokens": 860,
    "translated_title": "极端编码器输出帧率降低：提高大型端到端模型的计算延迟",
    "translated_abstract": "自动语音识别（ASR）端到端（E2E）模型的准确性随着规模扩大而持续提高，一些模型现在已经达到了数十亿个参数。然而，这些模型的广泛部署和采用需要计算效率高的解码策略。在本研究中，我们研究了一种这样的策略：在编码器中应用多个帧降低层，将编码器输出压缩成少量的输出帧。尽管类似的技术在先前的工作中已经得到研究，但我们通过使用多个漏斗降低层实现了比先前展示的更大幅度的减少。通过消融实验，我们研究了编码器中各种架构选择的影响，以确定最有效的策略。我们证明我们可以在每2.56秒的输入语音中生成一个编码器输出帧，而不会显著影响大型端到端模型上的词错误率。",
    "tldr": "通过在编码器中应用多个漏斗降低层，实现了极大程度的输出帧率降低，为大型端到端模型提高了计算效率"
}