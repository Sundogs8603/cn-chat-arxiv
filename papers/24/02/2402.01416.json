{
    "title": "Sequence Shortening for Context-Aware Machine Translation",
    "abstract": "Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that th",
    "link": "https://rss.arxiv.org/abs/2402.01416",
    "context": "Title: Sequence Shortening for Context-Aware Machine Translation\nAbstract: Context-aware Machine Translation aims to improve translations of sentences by incorporating surrounding sentences as context. Towards this task, two main architectures have been applied, namely single-encoder (based on concatenation) and multi-encoder models. In this study, we show that a special case of multi-encoder architecture, where the latent representation of the source sentence is cached and reused as the context in the next step, achieves higher accuracy on the contrastive datasets (where the models have to rank the correct translation among the provided sentences) and comparable BLEU and COMET scores as the single- and multi-encoder approaches. Furthermore, we investigate the application of Sequence Shortening to the cached representations. We test three pooling-based shortening techniques and introduce two novel methods - Latent Grouping and Latent Selecting, where the network learns to group tokens or selects the tokens to be cached as context. Our experiments show that th",
    "path": "papers/24/02/2402.01416.json",
    "total_tokens": 921,
    "translated_title": "上下文感知机器翻译的序列缩短",
    "translated_abstract": "上下文感知机器翻译旨在通过将周围的句子作为上下文来改进句子的翻译。为实现这一目标，已经应用了两种主要架构，即基于串联的单编码器和多编码器模型。在这项研究中，我们展示了多编码器架构的一个特殊情况，在下一步中重用源句子的潜在表示作为上下文，可以在对比数据集上实现更高的准确性（模型必须对提供的句子中的正确翻译进行排序），并且与单编码器和多编码器方法相比具有可比较的BLEU和COMET分数。此外，我们研究了将缓存表示应用于序列缩短。我们测试了三种基于汇聚的缩短技术，并引入了两种新方法——潜在分组和潜在选择，其中网络学习将令牌分组或选择要缓存为上下文。我们的实验表明，缓存表示的序列缩短方法可以进一步提高翻译质量。",
    "tldr": "本研究展示了上下文感知机器翻译的序列缩短方法，通过重用上下文信息可以提高翻译准确性，在对比数据集上表现良好，并且引入了新的潜在分组和潜在选择方法来进一步提高翻译质量。",
    "en_tdlr": "This study demonstrates the application of sequence shortening in context-aware machine translation, which improves translation accuracy by reusing context information. It shows good performance on contrastive datasets and introduces new methods of latent grouping and latent selecting to further enhance translation quality."
}