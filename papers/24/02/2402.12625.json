{
    "title": "Compact NSGA-II for Multi-objective Feature Selection",
    "abstract": "arXiv:2402.12625v1 Announce Type: new  Abstract: Feature selection is an expensive challenging task in machine learning and data mining aimed at removing irrelevant and redundant features. This contributes to an improvement in classification accuracy, as well as the budget and memory requirements for classification, or any other post-processing task conducted after feature selection. In this regard, we define feature selection as a multi-objective binary optimization task with the objectives of maximizing classification accuracy and minimizing the number of selected features. In order to select optimal features, we have proposed a binary Compact NSGA-II (CNSGA-II) algorithm. Compactness represents the population as a probability distribution to enhance evolutionary algorithms not only to be more memory-efficient but also to reduce the number of fitness evaluations. Instead of holding two populations during the optimization process, our proposed method uses several Probability Vectors (",
    "link": "https://arxiv.org/abs/2402.12625",
    "context": "Title: Compact NSGA-II for Multi-objective Feature Selection\nAbstract: arXiv:2402.12625v1 Announce Type: new  Abstract: Feature selection is an expensive challenging task in machine learning and data mining aimed at removing irrelevant and redundant features. This contributes to an improvement in classification accuracy, as well as the budget and memory requirements for classification, or any other post-processing task conducted after feature selection. In this regard, we define feature selection as a multi-objective binary optimization task with the objectives of maximizing classification accuracy and minimizing the number of selected features. In order to select optimal features, we have proposed a binary Compact NSGA-II (CNSGA-II) algorithm. Compactness represents the population as a probability distribution to enhance evolutionary algorithms not only to be more memory-efficient but also to reduce the number of fitness evaluations. Instead of holding two populations during the optimization process, our proposed method uses several Probability Vectors (",
    "path": "papers/24/02/2402.12625.json",
    "total_tokens": 758,
    "translated_title": "多目标特征选择的紧凑型NSGA-II",
    "translated_abstract": "特征选择是机器学习和数据挖掘中一项昂贵且具有挑战性的任务，旨在消除无关和冗余特征。这有助于提高分类准确度，以及分类或特征选择后进行的任何其他后处理任务的预算和内存要求。在这方面，我们将特征选择定义为一个多目标二进制优化任务，其目标是最大化分类准确度和最小化选择的特征数量。为了选择最优特征，我们提出了一种二进制紧凑型NSGA-II（CNSGA-II）算法。紧凑型代表将群体表示为概率分布，以增强进化算法不仅更节省内存，还减少评估的数量。在优化过程中，我们的方法利用若干概率向量，而不是保持两个群体。",
    "tldr": "提出了一种紧凑型NSGA-II算法，用于多目标特征选择，旨在提高分类准确度并减少所选特征数量",
    "en_tdlr": "Proposed a compact NSGA-II algorithm for multi-objective feature selection to improve classification accuracy and reduce the number of selected features."
}