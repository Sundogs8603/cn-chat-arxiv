{
    "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math",
    "abstract": "arXiv:2402.14830v1 Announce Type: cross  Abstract: Mathematical word problem-solving has long been recognized as a complex task for small language models (SLMs). A recent study hypothesized that the smallest model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34 billion parameters. To reach this level of performance with smaller models, researcher often train SLMs to generate Python code or use tools to help avoid calculation errors. Additionally, they employ ensembling, where outputs of up to 100 model runs are combined to arrive at a more accurate result. Result selection is done using consensus, majority vote or a separate a verifier model used in conjunction with the SLM. Ensembling provides a substantial boost in accuracy but at a significant cost increase with multiple calls to the model (e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).   In this work, we present Orca-Math, a 7-billion-parameter SLM based on the Mistral-7B, which achie",
    "link": "https://arxiv.org/abs/2402.14830",
    "context": "Title: Orca-Math: Unlocking the potential of SLMs in Grade School Math\nAbstract: arXiv:2402.14830v1 Announce Type: cross  Abstract: Mathematical word problem-solving has long been recognized as a complex task for small language models (SLMs). A recent study hypothesized that the smallest model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34 billion parameters. To reach this level of performance with smaller models, researcher often train SLMs to generate Python code or use tools to help avoid calculation errors. Additionally, they employ ensembling, where outputs of up to 100 model runs are combined to arrive at a more accurate result. Result selection is done using consensus, majority vote or a separate a verifier model used in conjunction with the SLM. Ensembling provides a substantial boost in accuracy but at a significant cost increase with multiple calls to the model (e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).   In this work, we present Orca-Math, a 7-billion-parameter SLM based on the Mistral-7B, which achie",
    "path": "papers/24/02/2402.14830.json",
    "total_tokens": 885,
    "translated_title": "Orca-Math：释放小语言模型在小学数学中的潜力",
    "translated_abstract": "数学单词问题解决长期以来一直被认为是小语言模型（SLMs）面临的复杂任务。最近的一项研究假设，为了在GSM8K基准测试上实现超过80%的准确度，最小的模型大小需要为340亿个参数。为了用更小的模型达到这一性能水平，研究人员经常训练SLMs生成Python代码或使用工具来帮助避免计算错误。此外，他们使用集成，将多达100次模型运行的输出组合在一起，得到更准确的结果。结果选择是通过共识、多数投票或与SLM一起使用的单独的验证器模型来进行的。集成大大提高了准确性，但随之而来的是对模型的多次调用造成的显著成本增加（例如，Phi-GSM使用前48个来将性能从68.2提升到81.5）。在这项研究中，我们提出了Orca-Math，一个基于Mistral-7B的70亿参数SLM。",
    "tldr": "提出了一个基于Mistral-7B的70亿参数的Orca-Math小语言模型，旨在在小学数学中实现更高的准确度。",
    "en_tdlr": "Proposed a 7-billion-parameter Orca-Math small language model based on Mistral-7B, aiming to achieve higher accuracy in grade school math."
}