{
    "title": "AttributionBench: How Hard is Automatic Attribution Evaluation?",
    "abstract": "arXiv:2402.15089v1 Announce Type: cross  Abstract: Modern generative search engines enhance the reliability of large language model (LLM) responses by providing cited evidence. However, evaluating the answer's attribution, i.e., whether every claim within the generated responses is fully supported by its cited evidence, remains an open problem. This verification, traditionally dependent on costly human evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standardized benchmarks for these methods, we present AttributionBench, a comprehensive benchmark compiled from various existing attribution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art LLMs. Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error c",
    "link": "https://arxiv.org/abs/2402.15089",
    "context": "Title: AttributionBench: How Hard is Automatic Attribution Evaluation?\nAbstract: arXiv:2402.15089v1 Announce Type: cross  Abstract: Modern generative search engines enhance the reliability of large language model (LLM) responses by providing cited evidence. However, evaluating the answer's attribution, i.e., whether every claim within the generated responses is fully supported by its cited evidence, remains an open problem. This verification, traditionally dependent on costly human evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standardized benchmarks for these methods, we present AttributionBench, a comprehensive benchmark compiled from various existing attribution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art LLMs. Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error c",
    "path": "papers/24/02/2402.15089.json",
    "total_tokens": 837,
    "translated_title": "AttributionBench：自动归因评估有多难？",
    "translated_abstract": "现代生成式搜索引擎通过提供引用证据增强了大型语言模型（LLM）响应的可靠性。然而，评估答案的归因，即生成响应中的每个声明是否都得到其引用证据的充分支持，仍然是一个未解决的问题。传统上依赖于昂贵的人工评估的这种验证强调了对自动归因评估方法的迫切需求。为了填补这种方法缺乏标准化基准的差距，我们提出了AttributionBench，这是一个综合性基准，由各种现有的归因数据集编制而成。我们在AttributionBench上的大量实验揭示了自动归因评估面临的挑战，即使对于最先进的LLM也是如此。具体而言，我们的发现表明，即使是经过优化的GPT-3.5在二元分类公式下也只能达到约80%的宏F1分数。更 than 300 error c",
    "tldr": "AttributionBench是一个综合基准，揭示了自动归因评估的挑战，即使对于最先进的语言模型也只能达到80%的准确率。",
    "en_tdlr": "AttributionBench is a comprehensive benchmark that reveals the challenges of automatic attribution evaluation, with even state-of-the-art language models achieving only 80% accuracy."
}