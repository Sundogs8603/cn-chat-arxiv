{
    "title": "Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More",
    "abstract": "Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting pre-trained language models (PLMs) to specific tasks by inserting learnable embeddings, or soft prompts, at the input layer of the PLM, without modifying its parameters. This paper investigates the potential of SPT for cross-lingual transfer. Unlike previous studies on SPT for cross-lingual transfer that often fine-tune both the soft prompt and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft prompt. This does not only reduce the computational cost and storage overhead of full-model fine-tuning, but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages. Moreover, we explore how different factors related to the prompt, such as the length or its reparameterization, affect cross-lingual transfer performance.",
    "link": "https://arxiv.org/abs/2402.03782",
    "context": "Title: Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More\nAbstract: Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting pre-trained language models (PLMs) to specific tasks by inserting learnable embeddings, or soft prompts, at the input layer of the PLM, without modifying its parameters. This paper investigates the potential of SPT for cross-lingual transfer. Unlike previous studies on SPT for cross-lingual transfer that often fine-tune both the soft prompt and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft prompt. This does not only reduce the computational cost and storage overhead of full-model fine-tuning, but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages. Moreover, we explore how different factors related to the prompt, such as the length or its reparameterization, affect cross-lingual transfer performance.",
    "path": "papers/24/02/2402.03782.json",
    "total_tokens": 858,
    "translated_title": "软提示调整用于跨语言迁移：越少越好",
    "translated_abstract": "软提示调整（SPT）是一种有效的参数调整方法，通过在预训练语言模型（PLMs）的输入层插入可学习的嵌入，或软提示，而不修改其参数，来适应特定任务。本文研究了SPT在跨语言迁移方面的潜力。与先前关于SPT跨语言迁移的研究不同，我们坚持SPT的原始意图，即仅训练软提示而保持模型参数不变。这不仅减少了完整模型微调的计算成本和存储开销，而且我们还证明了SPT固有的参数效率能够提高对语言差异较大的语言的跨语言迁移性能。此外，我们还探讨了与提示相关的不同因素，如长度或其重新参数化，对跨语言迁移性能的影响。",
    "tldr": "本研究探索了软提示调整（SPT）在跨语言迁移中的应用。通过仅训练软提示而不调整模型参数，实现了更高的参数效率，并提高了对语言差异较大的语言的跨语言迁移性能。",
    "en_tdlr": "This paper investigates the potential of Soft Prompt Tuning (SPT) for cross-lingual transfer. By training only the soft prompt without modifying the model parameters, this approach achieves higher parameter efficiency and improves cross-lingual transfer performance for linguistically distant languages."
}