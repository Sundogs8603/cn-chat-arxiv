{
    "title": "Pretext Training Algorithms for Event Sequence Data",
    "abstract": "arXiv:2402.10392v1 Announce Type: cross  Abstract: Pretext training followed by task-specific fine-tuning has been a successful approach in vision and language domains. This paper proposes a self-supervised pretext training framework tailored to event sequence data. We introduce a novel alignment verification task that is specialized to event sequences, building on good practices in masked reconstruction and contrastive learning. Our pretext tasks unlock foundational representations that are generalizable across different down-stream tasks, including next-event prediction for temporal point process models, event sequence classification, and missing event interpolation. Experiments on popular public benchmarks demonstrate the potential of the proposed method across different tasks and data domains.",
    "link": "https://arxiv.org/abs/2402.10392",
    "context": "Title: Pretext Training Algorithms for Event Sequence Data\nAbstract: arXiv:2402.10392v1 Announce Type: cross  Abstract: Pretext training followed by task-specific fine-tuning has been a successful approach in vision and language domains. This paper proposes a self-supervised pretext training framework tailored to event sequence data. We introduce a novel alignment verification task that is specialized to event sequences, building on good practices in masked reconstruction and contrastive learning. Our pretext tasks unlock foundational representations that are generalizable across different down-stream tasks, including next-event prediction for temporal point process models, event sequence classification, and missing event interpolation. Experiments on popular public benchmarks demonstrate the potential of the proposed method across different tasks and data domains.",
    "path": "papers/24/02/2402.10392.json",
    "total_tokens": 720,
    "translated_title": "针对事件序列数据的预训练算法",
    "translated_abstract": "预训练后进行特定任务微调在视觉和语言领域取得了成功。本文提出了一个针对事件序列数据定制的自监督预训练框架。我们引入了一项针对事件序列特化的新型对齐验证任务，借鉴了掩码重构和对比学习中的良好实践。我们的预训练任务可以释放基础表示，这些表示可以推广到不同的下游任务，包括用于时间点过程模型的下一事件预测，事件序列分类和缺失事件插值。对流行的公共基准数据集上的实验表明，所提出的方法在不同任务和数据领域上具有潜力。",
    "tldr": "提出了针对事件序列数据的自监督预训练框架，通过引入新颖的对齐验证任务，构建了适用于事件序列的基础表示，可以推广到不同的下游任务和数据领域。",
    "en_tdlr": "Proposed a self-supervised pretext training framework tailored to event sequence data, introducing a novel alignment verification task that builds foundational representations specialized to event sequences, which can be generalized across different downstream tasks and data domains."
}