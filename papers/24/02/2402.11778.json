{
    "title": "Towards Theoretical Understandings of Self-Consuming Generative Models",
    "abstract": "arXiv:2402.11778v1 Announce Type: cross  Abstract: This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declin",
    "link": "https://arxiv.org/abs/2402.11778",
    "context": "Title: Towards Theoretical Understandings of Self-Consuming Generative Models\nAbstract: arXiv:2402.11778v1 Announce Type: cross  Abstract: This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declin",
    "path": "papers/24/02/2402.11778.json",
    "total_tokens": 893,
    "translated_title": "朝向自消耗生成模型的理论理解",
    "translated_abstract": "这篇论文探讨了训练生成模型的新挑战，即在一个自消耗循环中训练模型，其中连续的模型世代通过混合之前世代的真实数据和合成数据来进行递归训练。我们构建了一个理论框架，以严格评估这种训练方案对未来模型学习的数据分布产生的影响。具体来说，我们推导了在不同混合训练场景下，未来模型产生的合成数据分布与原始真实数据分布之间的总变差（TV）距离的界限。我们的分析表明，在混合训练数据集的大小或真实数据比例足够大的条件下，这种距离可以被有效控制。有趣的是，我们进一步揭示了由扩大合成数据量引起的相变，理论上证明了虽然TV距离表现出初始上升，但却逐渐下降。",
    "tldr": "通过构建理论框架，我们探讨了在自消耗循环中训练生成模型对数据分布学习的影响，证明了在足够大的训练数据集大小或真实数据比例条件下，合成数据分布与原始真实数据分布之间的总变差距离能够被有效控制。",
    "en_tdlr": "By constructing a theoretical framework, we explore the impact of training generative models within a self-consuming loop on data distribution learning, demonstrating that the total variation distance between the synthetic data distribution and the original real data distribution can be effectively controlled under conditions of sufficiently large training dataset size or proportion of real data."
}