{
    "title": "Learning fast changing slow in spiking neural networks",
    "abstract": "arXiv:2402.10069v1 Announce Type: cross  Abstract: Reinforcement learning (RL) faces substantial challenges when applied to real-life problems, primarily stemming from the scarcity of available data due to limited interactions with the environment. This limitation is exacerbated by the fact that RL often demands a considerable volume of data for effective learning. The complexity escalates further when implementing RL in recurrent spiking networks, where inherent noise introduced by spikes adds a layer of difficulty. Life-long learning machines must inherently resolve the plasticity-stability paradox. Striking a balance between acquiring new knowledge and maintaining stability is crucial for artificial agents. In this context, we take inspiration from machine learning technology and introduce a biologically plausible implementation of proximal policy optimization, arguing that it significantly alleviates this challenge. Our approach yields two notable advancements: first, the ability t",
    "link": "https://arxiv.org/abs/2402.10069",
    "context": "Title: Learning fast changing slow in spiking neural networks\nAbstract: arXiv:2402.10069v1 Announce Type: cross  Abstract: Reinforcement learning (RL) faces substantial challenges when applied to real-life problems, primarily stemming from the scarcity of available data due to limited interactions with the environment. This limitation is exacerbated by the fact that RL often demands a considerable volume of data for effective learning. The complexity escalates further when implementing RL in recurrent spiking networks, where inherent noise introduced by spikes adds a layer of difficulty. Life-long learning machines must inherently resolve the plasticity-stability paradox. Striking a balance between acquiring new knowledge and maintaining stability is crucial for artificial agents. In this context, we take inspiration from machine learning technology and introduce a biologically plausible implementation of proximal policy optimization, arguing that it significantly alleviates this challenge. Our approach yields two notable advancements: first, the ability t",
    "path": "papers/24/02/2402.10069.json",
    "total_tokens": 816,
    "translated_title": "学习快速变化的慢性在脉冲神经网络中",
    "translated_abstract": "强化学习在现实问题中面临着很大的挑战，主要源于与环境的有限交互导致的可用数据的稀缺性。 RL通常需要大量的数据来进行有效的学习，这使得复杂性进一步增加，尤其是在使用循环脉冲网络实现RL时，脉冲引入的固有噪声增加了难度。终身学习机器在本质上必须解决可塑性-稳定性悖论。在获得新知识和保持稳定之间取得平衡对于人工智能代理至关重要。在这个背景下，我们从机器学习技术中汲取灵感，并引入了一种生物可行的近端策略优化实现，认为它显著减轻了此挑战。我们的方法带来了两个重要的进展：首先，能够...",
    "tldr": "通过近端策略优化实现的生物学可行方法在脉冲神经网络中减轻了强化学习所面临的数据稀缺性和噪声引入的困难。",
    "en_tdlr": "A biologically feasible approach implemented through proximal policy optimization in spiking neural networks alleviates the challenges of data scarcity and noise introduced in reinforcement learning."
}