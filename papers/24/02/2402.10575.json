{
    "title": "Symbolic Autoencoding for Self-Supervised Sequence Learning",
    "abstract": "arXiv:2402.10575v1 Announce Type: cross  Abstract: Traditional language models, adept at next-token prediction in text sequences, often struggle with transduction tasks between distinct symbolic systems, particularly when parallel data is scarce. Addressing this issue, we introduce \\textit{symbolic autoencoding} ($\\Sigma$AE), a self-supervised framework that harnesses the power of abundant unparallel data alongside limited parallel data. $\\Sigma$AE connects two generative models via a discrete bottleneck layer and is optimized end-to-end by minimizing reconstruction loss (simultaneously with supervised loss for the parallel data), such that the sequence generated by the discrete bottleneck can be read out as the transduced input sequence. We also develop gradient-based methods allowing for efficient self-supervised sequence learning despite the discreteness of the bottleneck. Our results demonstrate that $\\Sigma$AE significantly enhances performance on transduction tasks, even with min",
    "link": "https://arxiv.org/abs/2402.10575",
    "context": "Title: Symbolic Autoencoding for Self-Supervised Sequence Learning\nAbstract: arXiv:2402.10575v1 Announce Type: cross  Abstract: Traditional language models, adept at next-token prediction in text sequences, often struggle with transduction tasks between distinct symbolic systems, particularly when parallel data is scarce. Addressing this issue, we introduce \\textit{symbolic autoencoding} ($\\Sigma$AE), a self-supervised framework that harnesses the power of abundant unparallel data alongside limited parallel data. $\\Sigma$AE connects two generative models via a discrete bottleneck layer and is optimized end-to-end by minimizing reconstruction loss (simultaneously with supervised loss for the parallel data), such that the sequence generated by the discrete bottleneck can be read out as the transduced input sequence. We also develop gradient-based methods allowing for efficient self-supervised sequence learning despite the discreteness of the bottleneck. Our results demonstrate that $\\Sigma$AE significantly enhances performance on transduction tasks, even with min",
    "path": "papers/24/02/2402.10575.json",
    "total_tokens": 859,
    "translated_title": "符号自编码用于自监督序列学习",
    "translated_abstract": "传统语言模型擅长预测文本序列中的下一个标记，但在不同符号系统之间执行转导任务时通常会遇到困难，特别是在平行数据稀缺的情况下。为解决这一问题，我们引入了符号自编码（$\\Sigma$AE），这是一个自监督框架，利用了丰富的不平行数据和有限的平行数据。$\\Sigma$AE通过一个离散瓶颈层连接两个生成模型，并通过最小化重构损失（与平行数据的监督损失同时进行优化）进行端到端优化，使得离散瓶颈生成的序列可以被读取为转导的输入序列。我们还开发了基于梯度的方法，实现了尽管存在瓶颈离散性，仍能进行高效的自监督序列学习。我们的结果表明，$\\Sigma$AE显著提高了转导任务的性能，即使使用了最少量的平行数据。",
    "tldr": "符号自编码（$\\Sigma$AE）是一个自监督框架，通过最小化重构损失和平行数据的监督损失来优化连接两个生成模型，实现了在转导任务上显著提升性能。",
    "en_tdlr": "Symbolic autoencoding ($\\Sigma$AE) is a self-supervised framework that optimally connects two generative models by minimizing the reconstruction loss and supervised loss of parallel data, significantly enhancing performance on transduction tasks."
}