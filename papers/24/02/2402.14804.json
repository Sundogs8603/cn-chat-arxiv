{
    "title": "Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset",
    "abstract": "arXiv:2402.14804v1 Announce Type: cross  Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements i",
    "link": "https://arxiv.org/abs/2402.14804",
    "context": "Title: Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset\nAbstract: arXiv:2402.14804v1 Announce Type: cross  Abstract: Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models approaching human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on MATH-V, underscoring the imperative for further advancements i",
    "path": "papers/24/02/2402.14804.json",
    "total_tokens": 874,
    "translated_title": "使用MATH-Vision数据集测量多模态数学推理",
    "translated_abstract": "大型多模态模型（LMMs）的最新进展在视觉背景下的数学推理方面显示出令人鼓舞的结果，这些模型在现有基准测试（如MathVista）上接近人类水平的表现。然而，我们观察到这些基准测试在问题多样性和涵盖学科范围方面存在显着局限性。为了解决这一问题，我们提出了MATH-Vision（MATH-V）数据集，这是一个精心策划的收集了来自真实数学竞赛的3,040个高质量数学问题和视觉背景的数据集。跨越16个不同的数学学科，分为5个难度级别进行评分，我们的数据集为评估LMMs的数学推理能力提供了一套全面且多样化的挑战。通过广泛的实验，我们揭示了当前LMMs与MATH-V上人类表现之间的显著表现差距，并强调了进一步推进的必要性。",
    "tldr": "提出了MATH-Vision（MATH-V）数据集，用于评估大型多模态模型（LMMs）的数学推理能力，通过实验证实了当前LMMs和人类在MATH-V上的表现差距。"
}