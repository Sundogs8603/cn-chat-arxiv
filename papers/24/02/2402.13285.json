{
    "title": "Leveraging PAC-Bayes Theory and Gibbs Distributions for Generalization Bounds with Complexity Measures",
    "abstract": "arXiv:2402.13285v1 Announce Type: cross  Abstract: In statistical learning theory, a generalization bound usually involves a complexity measure imposed by the considered theoretical framework. This limits the scope of such bounds, as other forms of capacity measures or regularizations are used in algorithms. In this paper, we leverage the framework of disintegrated PAC-Bayes bounds to derive a general generalization bound instantiable with arbitrary complexity measures. One trick to prove such a result involves considering a commonly used family of distributions: the Gibbs distributions. Our bound stands in probability jointly over the hypothesis and the learning sample, which allows the complexity to be adapted to the generalization gap as it can be customized to fit both the hypothesis class and the task.",
    "link": "https://arxiv.org/abs/2402.13285",
    "context": "Title: Leveraging PAC-Bayes Theory and Gibbs Distributions for Generalization Bounds with Complexity Measures\nAbstract: arXiv:2402.13285v1 Announce Type: cross  Abstract: In statistical learning theory, a generalization bound usually involves a complexity measure imposed by the considered theoretical framework. This limits the scope of such bounds, as other forms of capacity measures or regularizations are used in algorithms. In this paper, we leverage the framework of disintegrated PAC-Bayes bounds to derive a general generalization bound instantiable with arbitrary complexity measures. One trick to prove such a result involves considering a commonly used family of distributions: the Gibbs distributions. Our bound stands in probability jointly over the hypothesis and the learning sample, which allows the complexity to be adapted to the generalization gap as it can be customized to fit both the hypothesis class and the task.",
    "path": "papers/24/02/2402.13285.json",
    "total_tokens": 682,
    "translated_title": "利用 PAC-Bayes 理论和 Gibbs 分布推导带有复杂度度量的泛化界限",
    "translated_abstract": "在统计学习理论中，泛化界限通常涉及由考虑的理论框架施加的复杂度度量。本文利用了分解的 PAC-Bayes 界限框架，推导出一个可实例化为任意复杂度度量的泛化界限。我们的界限以概率同时涵盖假设和学习样本，可以根据泛化差距调整复杂度，因为它可定制以适应假设类和任务。",
    "tldr": "本文利用 PAC-Bayes 理论和 Gibbs 分布提出了一个新的泛化界限框架，可适用于任意复杂度度量，允许对泛化差距进行定制化调整。",
    "en_tdlr": "This paper introduces a new generalization bound framework leveraging PAC-Bayes theory and Gibbs distributions, applicable to arbitrary complexity measures and allowing customizable adjustments to the generalization gap."
}