{
    "title": "Likelihood-based Mitigation of Evaluation Bias in Large Language Models",
    "abstract": "arXiv:2402.15987v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also impr",
    "link": "https://arxiv.org/abs/2402.15987",
    "context": "Title: Likelihood-based Mitigation of Evaluation Bias in Large Language Models\nAbstract: arXiv:2402.15987v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are widely used to evaluate natural language generation tasks as automated metrics. However, the likelihood, a measure of LLM's plausibility for a sentence, can vary due to superficial differences in sentences, such as word order and sentence structure. It is therefore possible that there might be a likelihood bias if LLMs are used for evaluation: they might overrate sentences with higher likelihoods while underrating those with lower likelihoods. In this paper, we investigate the presence and impact of likelihood bias in LLM-based evaluators. We also propose a method to mitigate the likelihood bias. Our method utilizes highly biased instances as few-shot examples for in-context learning. Our experiments in evaluating the data-to-text and grammatical error correction tasks reveal that several LLMs we test display a likelihood bias. Furthermore, our proposed method successfully mitigates this bias, also impr",
    "path": "papers/24/02/2402.15987.json",
    "total_tokens": 836,
    "translated_title": "基于似然的大型语言模型评估偏差的缓解",
    "translated_abstract": "大型语言模型(LLMs)被广泛用于评估自然语言生成任务的自动化指标。然而，似然作为衡量LLM对句子可信度的指标，可能会因句子表面差异（如词序和句子结构）而变化。因此，如果将LLMs用于评估，可能存在似然偏差：它们可能会高估具有较高似然性的句子，而低估具有较低似然性的句子。本文对LLM评估器中似然偏差的存在和影响进行了研究。我们还提出了一种缓解似然偏差的方法。我们的方法利用高度偏置的实例作为少样本示例进行上下文学习。我们在评估数据到文本和语法错误纠正任务时的实验结果显示，我们测试的几种LLMs显示出似然偏差。此外，我们提出的方法成功地减轻了这种偏差",
    "tldr": "该论文研究了基于大型语言模型（LLM）的评估器中的似然偏差，并提出了一种缓解这种偏差的方法。",
    "en_tdlr": "This paper investigates the likelihood bias in LLM-based evaluators and proposes a method to mitigate this bias."
}