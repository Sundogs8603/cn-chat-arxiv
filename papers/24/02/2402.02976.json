{
    "title": "Boosting, Voting Classifiers and Randomized Sample Compression Schemes",
    "abstract": "In boosting, we aim to leverage multiple weak learners to produce a strong learner. At the center of this paradigm lies the concept of building the strong learner as a voting classifier, which outputs a weighted majority vote of the weak learners. While many successful boosting algorithms, such as the iconic AdaBoost, produce voting classifiers, their theoretical performance has long remained sub-optimal: the best known bounds on the number of training examples necessary for a voting classifier to obtain a given accuracy has so far always contained at least two logarithmic factors above what is known to be achievable by general weak-to-strong learners. In this work, we break this barrier by proposing a randomized boosting algorithm that outputs voting classifiers whose generalization error contains a single logarithmic dependency on the sample size. We obtain this result by building a general framework that extends sample compression methods to support randomized learning algorithms ba",
    "link": "https://arxiv.org/abs/2402.02976",
    "context": "Title: Boosting, Voting Classifiers and Randomized Sample Compression Schemes\nAbstract: In boosting, we aim to leverage multiple weak learners to produce a strong learner. At the center of this paradigm lies the concept of building the strong learner as a voting classifier, which outputs a weighted majority vote of the weak learners. While many successful boosting algorithms, such as the iconic AdaBoost, produce voting classifiers, their theoretical performance has long remained sub-optimal: the best known bounds on the number of training examples necessary for a voting classifier to obtain a given accuracy has so far always contained at least two logarithmic factors above what is known to be achievable by general weak-to-strong learners. In this work, we break this barrier by proposing a randomized boosting algorithm that outputs voting classifiers whose generalization error contains a single logarithmic dependency on the sample size. We obtain this result by building a general framework that extends sample compression methods to support randomized learning algorithms ba",
    "path": "papers/24/02/2402.02976.json",
    "total_tokens": 882,
    "translated_title": "提升，投票分类器和随机采样压缩方案",
    "translated_abstract": "在提升中，我们旨在利用多个弱学习器来产生一个强学习器。这个范式的核心是将强学习器建模为一个投票分类器，它输出弱学习器的加权多数投票。尽管许多成功的提升算法，如标志性的AdaBoost，产生投票分类器，但它们的理论性能长期以来一直不够优化：迄今为止，已知的使投票分类器达到给定准确性所需的训练样本数的最佳界限总是至少包含至多两个对数因子，而这已经超过了一般的弱到强学习器所能实现的范围。在这项工作中，我们通过提出一种随机提升算法打破这一障碍，该算法输出的投票分类器在样本大小上包含单对数依赖的泛化错误。我们通过构建一个通用框架将样本压缩方法扩展到支持随机学习算法来获得这个结果。",
    "tldr": "本研究提出了一种随机提升算法来解决传统提升算法的性能问题，并通过构建一个通用框架将样本压缩方法扩展到支持随机学习算法，实现了在样本大小上具有单对数依赖的泛化错误。",
    "en_tdlr": "This research proposes a randomized boosting algorithm to address the performance issues of traditional boosting algorithms, and achieves a generalization error with a single logarithmic dependency on the sample size by extending sample compression methods to support randomized learning algorithms."
}