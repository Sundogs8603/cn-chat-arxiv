{
    "title": "A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm",
    "abstract": "With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain. In practice, there are two prevailing ways, in which the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained LLMs are fine-tuned a few times independently using the corresponding training samples from each task. (ii) An Integrated Model: Samples from all tasks are employed to fine-tune a pre-trianed LLM unitedly. To address the high computing cost and seesawing issue simultaneously, we propose a unified framework that implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme. For a given cluster of tasks, we design an innovative layer that contai",
    "link": "https://arxiv.org/abs/2402.01684",
    "context": "Title: A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm\nAbstract: With the productive evolution of large language models (LLMs) in the field of natural language processing (NLP), tons of effort has been made to effectively fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or multiple specific domain. In practice, there are two prevailing ways, in which the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained LLMs are fine-tuned a few times independently using the corresponding training samples from each task. (ii) An Integrated Model: Samples from all tasks are employed to fine-tune a pre-trianed LLM unitedly. To address the high computing cost and seesawing issue simultaneously, we propose a unified framework that implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme. For a given cluster of tasks, we design an innovative layer that contai",
    "path": "papers/24/02/2402.01684.json",
    "total_tokens": 902,
    "translated_title": "使用CGC-LORA算法在LLMs中实现1 + N多任务微调模式的框架",
    "translated_abstract": "随着自然语言处理领域中大型语言模型（LLMs）的不断演进，人们为了有效地微调常见的预训练LLMs以完成各种任务，在一个或多个特定领域中进行了大量努力。在实践中，有两种主要的适应方式：（i）多个独立模型：使用每个任务的相应训练样本对预训练LLMs进行独立的微调；（ii）集成模型：使用所有任务的样本来联合微调预训练LLMs 。为了同时解决高计算成本和摇摆问题，我们提出了一个统一的框架，使用一种新颖的定制门控（CGC）低秩自适应（LoRA）算法在LLMs中实现了1 + N多任务微调模式。我们的工作旨在充分利用MTL（即CGC）和PEFT（即LoRA）方案。对于给定的任务集群，我们设计了一个创新的层，其中包含...",
    "tldr": "提出了一个使用CGC-LORA算法在LLMs中实现1 + N多任务微调模式的统一框架，旨在解决高计算成本和摇摆问题。"
}