{
    "title": "Minusformer: Improving Time Series Forecasting by Progressively Learning Residuals",
    "abstract": "In this paper, we find that ubiquitous time series (TS) forecasting models are prone to severe overfitting. To cope with this problem, we embrace a de-redundancy approach to progressively reinstate the intrinsic values of TS for future intervals. Specifically, we renovate the vanilla Transformer by reorienting the information aggregation mechanism from addition to subtraction. Then, we incorporate an auxiliary output branch into each block of the original model to construct a highway leading to the ultimate prediction. The output of subsequent modules in this branch will subtract the previously learned results, enabling the model to learn the residuals of the supervision signal, layer by layer. This designing facilitates the learning-driven implicit progressive decomposition of the input and output streams, empowering the model with heightened versatility, interpretability, and resilience against overfitting. Since all aggregations in the model are minus signs, which is called Minusfor",
    "link": "https://arxiv.org/abs/2402.02332",
    "context": "Title: Minusformer: Improving Time Series Forecasting by Progressively Learning Residuals\nAbstract: In this paper, we find that ubiquitous time series (TS) forecasting models are prone to severe overfitting. To cope with this problem, we embrace a de-redundancy approach to progressively reinstate the intrinsic values of TS for future intervals. Specifically, we renovate the vanilla Transformer by reorienting the information aggregation mechanism from addition to subtraction. Then, we incorporate an auxiliary output branch into each block of the original model to construct a highway leading to the ultimate prediction. The output of subsequent modules in this branch will subtract the previously learned results, enabling the model to learn the residuals of the supervision signal, layer by layer. This designing facilitates the learning-driven implicit progressive decomposition of the input and output streams, empowering the model with heightened versatility, interpretability, and resilience against overfitting. Since all aggregations in the model are minus signs, which is called Minusfor",
    "path": "papers/24/02/2402.02332.json",
    "total_tokens": 890,
    "translated_title": "Minusformer: 通过渐进学习残差来改进时间序列预测",
    "translated_abstract": "在本文中，我们发现普遍存在的时间序列（TS）预测模型容易严重过拟合。为了应对这个问题，我们采用去冗余的方法逐步恢复TS的内在价值以用于未来的时间段。具体而言，我们通过将信息聚合机制从加法转变为减法来改进传统的Transformer模型。然后，我们在原模型的每个模块中加入一个辅助输出分支，构建一条通往最终预测的高速公路。该分支中后续模块的输出将减去先前学习到的结果，使模型能够逐层学习监督信号的残差。这种设计促进了输入和输出流的逐步学习驱动隐式分解，使模型具有更高的灵活性、可解释性和对抗过拟合的韧性。由于模型中的所有聚合都是减号，因此被称为Minusformer。",
    "tldr": "Minusformer通过逐步学习残差来改进时间序列预测。它将Transformer模型中信息聚合机制从加法改为减法，并通过辅助输出分支逐层学习监督信号的残差，从而提高模型的灵活性和对抗过拟合的能力。",
    "en_tdlr": "Minusformer improves time series forecasting by progressively learning residuals. It renovates the vanilla Transformer by changing the information aggregation mechanism from addition to subtraction and incorporates an auxiliary output branch to learn the residuals of the supervision signal layer by layer, enhancing the model's flexibility and resilience against overfitting."
}