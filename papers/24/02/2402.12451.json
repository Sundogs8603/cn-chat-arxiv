{
    "title": "The (R)Evolution of Multimodal Large Language Models: A Survey",
    "abstract": "arXiv:2402.12451v1 Announce Type: cross  Abstract: Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in ",
    "link": "https://arxiv.org/abs/2402.12451",
    "context": "Title: The (R)Evolution of Multimodal Large Language Models: A Survey\nAbstract: arXiv:2402.12451v1 Announce Type: cross  Abstract: Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in ",
    "path": "papers/24/02/2402.12451.json",
    "total_tokens": 771,
    "translated_title": "多模态大型语言模型的革命：一项调查",
    "translated_abstract": "连接文本和视觉模态在生成智能中扮演着重要角色。受大型语言模型成功的启发，目前已有大量研究工作致力于开发多模态大型语言模型（MLLMs）。这些模型可以无缝地整合视觉和文本模态，同时作为输入和输出，提供基于对话的接口和遵循指令的能力。本文全面审查了最近基于视觉的MLLMs，分析了它们的架构选择、多模态对齐策略和训练技术。我们还对这些模型在各种任务上进行了详细分析，包括视觉定位、图像生成和编辑、视觉理解以及特定领域的应用。此外，我们编制并描述了训练数据集和评估基准，对现有模型进行了比较。",
    "tldr": "多模态大型语言模型能够无缝整合视觉和文本模态，为生成智能提供了新的可能性。",
    "en_tdlr": "Multimodal Large Language Models can seamlessly integrate visual and textual modalities, providing new possibilities for generative intelligence."
}