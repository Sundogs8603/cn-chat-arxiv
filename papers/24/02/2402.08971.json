{
    "title": "Structured Language Generation Model for Robust Structure Prediction",
    "abstract": "arXiv:2402.08971v1 Announce Type: new Abstract: We propose Structured Language Generation Model (SLGM), a mixture of new loss function and inference method for better generalization of structured outputs. Previous studies on structure prediction (e.g. NER, RE) make use of explicit dataset information, which would boost performance, yet it might pose challenges to robust generalization in real-world situations. Instead, our model gives generalized format information about data indirectly. With format information, we could reduce sequence-to-sequence problem into classification problem via loss calibration and formatted decoding. Our experimental results showed SLGM successfully maintain performance without dataset information, and showed much less format errors. We also showed our model can work like adapters on individual dataset, with no additional training.",
    "link": "https://arxiv.org/abs/2402.08971",
    "context": "Title: Structured Language Generation Model for Robust Structure Prediction\nAbstract: arXiv:2402.08971v1 Announce Type: new Abstract: We propose Structured Language Generation Model (SLGM), a mixture of new loss function and inference method for better generalization of structured outputs. Previous studies on structure prediction (e.g. NER, RE) make use of explicit dataset information, which would boost performance, yet it might pose challenges to robust generalization in real-world situations. Instead, our model gives generalized format information about data indirectly. With format information, we could reduce sequence-to-sequence problem into classification problem via loss calibration and formatted decoding. Our experimental results showed SLGM successfully maintain performance without dataset information, and showed much less format errors. We also showed our model can work like adapters on individual dataset, with no additional training.",
    "path": "papers/24/02/2402.08971.json",
    "total_tokens": 816,
    "translated_title": "鲁棒结构预测的结构化语言生成模型",
    "translated_abstract": "我们提出了一种结构化语言生成模型（SLGM），通过新的损失函数和推理方法的混合来改善结构化输出的泛化能力。以往的结构预测研究（如NER，RE）利用了显式的数据集信息，这可以提高性能，但可能会对现实世界中的鲁棒泛化性产生挑战。相反，我们的模型间接地提供了有关数据的通用格式信息。利用格式信息，我们可以通过损失校准和格式化解码将序列到序列问题简化为分类问题。我们的实验结果表明，SLGM在没有数据集信息的情况下成功保持了性能，并且显示出较少的格式错误。我们还展示了我们的模型可以像适配器一样在各个数据集上工作，而无需额外的训练。",
    "tldr": "鲁棒结构预测的结构化语言生成模型通过新的损失函数和推理方法的混合，成功提高了结构化输出的泛化能力，并且可以在没有数据集信息的情况下工作，并且减少了格式错误。",
    "en_tdlr": "The Structured Language Generation Model (SLGM) improves the robustness of structure prediction by utilizing a new loss function and inference method, providing generalized format information without explicit dataset information. The model reduces the sequence-to-sequence problem into a classification problem and maintains performance without dataset information, resulting in fewer format errors. Additionally, the model can adapt to individual datasets without additional training."
}