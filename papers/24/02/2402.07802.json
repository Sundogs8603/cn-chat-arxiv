{
    "title": "Towards a mathematical theory for consistency training in diffusion models",
    "abstract": "Consistency models, which were proposed to mitigate the high computational overhead during the sampling phase of diffusion models, facilitate single-step sampling while attaining state-of-the-art empirical performance. When integrated into the training phase, consistency models attempt to train a sequence of consistency functions capable of mapping any point at any time step of the diffusion process to its starting point. Despite the empirical success, a comprehensive theoretical understanding of consistency training remains elusive. This paper takes a first step towards establishing theoretical underpinnings for consistency models. We demonstrate that, in order to generate samples within $\\varepsilon$ proximity to the target in distribution (measured by some Wasserstein metric), it suffices for the number of steps in consistency learning to exceed the order of $d^{5/2}/\\varepsilon$, with $d$ the data dimension. Our theory offers rigorous insights into the validity and efficacy of cons",
    "link": "https://arxiv.org/abs/2402.07802",
    "context": "Title: Towards a mathematical theory for consistency training in diffusion models\nAbstract: Consistency models, which were proposed to mitigate the high computational overhead during the sampling phase of diffusion models, facilitate single-step sampling while attaining state-of-the-art empirical performance. When integrated into the training phase, consistency models attempt to train a sequence of consistency functions capable of mapping any point at any time step of the diffusion process to its starting point. Despite the empirical success, a comprehensive theoretical understanding of consistency training remains elusive. This paper takes a first step towards establishing theoretical underpinnings for consistency models. We demonstrate that, in order to generate samples within $\\varepsilon$ proximity to the target in distribution (measured by some Wasserstein metric), it suffices for the number of steps in consistency learning to exceed the order of $d^{5/2}/\\varepsilon$, with $d$ the data dimension. Our theory offers rigorous insights into the validity and efficacy of cons",
    "path": "papers/24/02/2402.07802.json",
    "total_tokens": 882,
    "translated_title": "面向扩散模型的一种一致性训练数学理论的探索",
    "translated_abstract": "一致性模型被提出来减少扩散模型采样阶段的高计算开销，实现了单步采样并达到了最先进的实证性能。一致性模型在训练阶段被整合进来，试图训练一系列的一致性函数，能够将扩散过程中的任何时间步骤的任何点映射回其起始点。尽管在实证上取得了成功，但关于一致性训练的全面理论理解还是很难得到的。本文对一致性模型的理论基础进行了初步的探索。我们证明，为了在分布中生成与目标在$\\varepsilon$接近的样本（通过某种Wasserstein度量衡量），一致性学习中的步骤数量需要超过$d^{5/2}/\\varepsilon$的阶数，其中$d$是数据维度。我们的理论为一致性模型的有效性和有效性提供了严格的洞察。",
    "tldr": "本文探索了面向扩散模型中一致性训练的理论基础，证明了在一致性学习中，步骤数量需要超过$d^{5/2}/\\varepsilon$的阶数，能够生成与目标分布接近的样本。"
}