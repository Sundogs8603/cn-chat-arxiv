{
    "title": "Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels",
    "abstract": "arXiv:2402.11953v1 Announce Type: cross  Abstract: Machine learning, with its myriad applications, has become an integral component of numerous technological systems. A common practice in this domain is the use of transfer learning, where a pre-trained model's architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it's crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method. Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerp",
    "link": "https://arxiv.org/abs/2402.11953",
    "context": "Title: Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels\nAbstract: arXiv:2402.11953v1 Announce Type: cross  Abstract: Machine learning, with its myriad applications, has become an integral component of numerous technological systems. A common practice in this domain is the use of transfer learning, where a pre-trained model's architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it's crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method. Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerp",
    "path": "papers/24/02/2402.11953.json",
    "total_tokens": 673,
    "translated_title": "通过对抗样本和时间侧信道揭示预训练的CNN模型",
    "translated_abstract": "机器学习在其众多应用中已成为许多技术系统的重要组成部分。在这一领域的常见做法是使用迁移学习，即调整预先训练的模型架构以适应特定任务。本文介绍了一种基于观察结果的方法，该方法认为对抗图像的分类模式可用作窃取模型的手段。此外，对抗图像的分类结合时间侧信道可以导致模型窃取方法。",
    "tldr": "对抗样本的分类模式和时间侧信道的结合可以导致窃取预训练的CNN模型。"
}