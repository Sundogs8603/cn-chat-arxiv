{
    "title": "Closed-Form Bounds for DP-SGD against Record-level Inference",
    "abstract": "arXiv:2402.14397v1 Announce Type: cross  Abstract: Machine learning models trained with differentially-private (DP) algorithms such as DP-SGD enjoy resilience against a wide range of privacy attacks. Although it is possible to derive bounds for some attacks based solely on an $(\\varepsilon,\\delta)$-DP guarantee, meaningful bounds require a small enough privacy budget (i.e., injecting a large amount of noise), which results in a large loss in utility. This paper presents a new approach to evaluate the privacy of machine learning models against specific record-level threats, such as membership and attribute inference, without the indirection through DP. We focus on the popular DP-SGD algorithm, and derive simple closed-form bounds. Our proofs model DP-SGD as an information theoretic channel whose inputs are the secrets that an attacker wants to infer (e.g., membership of a data record) and whose outputs are the intermediate model parameters produced by iterative optimization. We obtain b",
    "link": "https://arxiv.org/abs/2402.14397",
    "context": "Title: Closed-Form Bounds for DP-SGD against Record-level Inference\nAbstract: arXiv:2402.14397v1 Announce Type: cross  Abstract: Machine learning models trained with differentially-private (DP) algorithms such as DP-SGD enjoy resilience against a wide range of privacy attacks. Although it is possible to derive bounds for some attacks based solely on an $(\\varepsilon,\\delta)$-DP guarantee, meaningful bounds require a small enough privacy budget (i.e., injecting a large amount of noise), which results in a large loss in utility. This paper presents a new approach to evaluate the privacy of machine learning models against specific record-level threats, such as membership and attribute inference, without the indirection through DP. We focus on the popular DP-SGD algorithm, and derive simple closed-form bounds. Our proofs model DP-SGD as an information theoretic channel whose inputs are the secrets that an attacker wants to infer (e.g., membership of a data record) and whose outputs are the intermediate model parameters produced by iterative optimization. We obtain b",
    "path": "papers/24/02/2402.14397.json",
    "total_tokens": 830,
    "translated_title": "DP-SGD算法对抗记录层推断的封闭形式界限",
    "translated_abstract": "使用差分隐私（DP）算法（如DP-SGD）训练的机器学习模型对抗各种隐私攻击具有韧性。虽然可以仅基于（ε，δ）-DP保证推导出某些攻击的界限，但有意义的界限需要足够小的隐私预算（即注入大量噪声），这导致效用大幅损失。本文提出了一种新方法，用于评估机器学习模型针对特定记录级威胁（如成员关系和属性推断）的隐私，而无需经过DP的间接连结。我们专注于流行的DP-SGD算法，并推导出简单的闭式界限。我们的证明将DP-SGD建模为一个信息论通道，其输入是攻击者想要推断的秘密（如数据记录的成员关系），输出是迭代优化产生的中间模型参数。",
    "tldr": "该论文提出了一种新方法，通过封闭形式界限评估机器学习模型在特定记录级威胁下的隐私保护，避免了通过DP进行间接评估。",
    "en_tdlr": "This paper introduces a novel approach to evaluating the privacy protection of machine learning models against specific record-level threats through closed-form bounds, bypassing the indirect evaluation through DP."
}