{
    "title": "$\\lambda$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space",
    "abstract": "Despite the recent advances in personalized text-to-image (P-T2I) generative models, subject-driven T2I remains challenging. The primary bottlenecks include 1) Intensive training resource requirements, 2) Hyper-parameter sensitivity leading to inconsistent outputs, and 3) Balancing the intricacies of novel visual concept and composition alignment. We start by re-iterating the core philosophy of T2I diffusion models to address the above limitations. Predominantly, contemporary subject-driven T2I approaches hinge on Latent Diffusion Models (LDMs), which facilitate T2I mapping through cross-attention layers. While LDMs offer distinct advantages, P-T2I methods' reliance on the latent space of these diffusion models significantly escalates resource demands, leading to inconsistent results and necessitating numerous iterations for a single desired image. Recently, ECLIPSE has demonstrated a more resource-efficient pathway for training UnCLIP-based T2I models, circumventing the need for diffu",
    "link": "https://arxiv.org/abs/2402.05195",
    "context": "Title: $\\lambda$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space\nAbstract: Despite the recent advances in personalized text-to-image (P-T2I) generative models, subject-driven T2I remains challenging. The primary bottlenecks include 1) Intensive training resource requirements, 2) Hyper-parameter sensitivity leading to inconsistent outputs, and 3) Balancing the intricacies of novel visual concept and composition alignment. We start by re-iterating the core philosophy of T2I diffusion models to address the above limitations. Predominantly, contemporary subject-driven T2I approaches hinge on Latent Diffusion Models (LDMs), which facilitate T2I mapping through cross-attention layers. While LDMs offer distinct advantages, P-T2I methods' reliance on the latent space of these diffusion models significantly escalates resource demands, leading to inconsistent results and necessitating numerous iterations for a single desired image. Recently, ECLIPSE has demonstrated a more resource-efficient pathway for training UnCLIP-based T2I models, circumventing the need for diffu",
    "path": "papers/24/02/2402.05195.json",
    "total_tokens": 972,
    "translated_title": "$\\lambda$-ECLIPSE: 通过利用CLIP潜空间，基于多概念个性化文本到图像扩散模型",
    "translated_abstract": "尽管个性化文本到图像(P-T2I)生成模型取得了近期的进展，但基于主题的T2I仍然具有挑战性。主要的瓶颈包括：1) 需要大量的训练资源，2) 超参数敏感性导致不一致的输出，以及3) 平衡新的视觉概念和构图对齐的复杂性。我们重新阐述了T2I扩散模型的核心理念，以解决上述限制。主要地，当代的基于主题的T2I方法依赖于潜空间扩散模型(LDMs)，通过交叉注意力层实现T2I映射。虽然LDMs提供了明显的优势，但P-T2I方法对这些扩散模型的潜空间的依赖显著增加了资源需求，导致结果不一致，并需要多次迭代才能得到一个所需的图像。最近，ECLIPSE展示了一种更具资源效率的训练UnCLIP-based T2I模型的路径，避免了需要扩散的需求。",
    "tldr": "$\\lambda$-ECLIPSE通过利用CLIP潜空间，实现了多概念个性化文本到图像的扩散模型。相比于传统方法，它减小了训练资源需求，并提供了更一致、高质量的图像生成结果。",
    "en_tdlr": "$\\lambda$-ECLIPSE leverages CLIP latent space to develop multi-concept personalized text-to-image diffusion models, reducing training resource requirements and providing consistent and high-quality image generation results compared to traditional approaches."
}