{
    "title": "DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning",
    "abstract": "arXiv:2402.15957v1 Announce Type: new  Abstract: We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to approximate inference in environments where the latent state evolves at varying rates. We model episode sessions - parts of the episode where the latent state is fixed - and propose three key modifications to existing meta-RL methods: consistency of latent information within sessions, session masking, and prior latent conditioning. We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuous-control and simulated robot assistive tasks, demonstrating that DynaMITE-RL significantly outperforms state-of-the-art baselines in sample efficiency and inference returns.",
    "link": "https://arxiv.org/abs/2402.15957",
    "context": "Title: DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning\nAbstract: arXiv:2402.15957v1 Announce Type: new  Abstract: We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to approximate inference in environments where the latent state evolves at varying rates. We model episode sessions - parts of the episode where the latent state is fixed - and propose three key modifications to existing meta-RL methods: consistency of latent information within sessions, session masking, and prior latent conditioning. We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuous-control and simulated robot assistive tasks, demonstrating that DynaMITE-RL significantly outperforms state-of-the-art baselines in sample efficiency and inference returns.",
    "path": "papers/24/02/2402.15957.json",
    "total_tokens": 792,
    "translated_title": "DynaMITE-RL: 一种动态模型用于改进时间元元强化学习",
    "translated_abstract": "我们引入了DynaMITE-RL，这是一种元强化学习(meta-RL)方法，用于在潜在状态以不同速率演变的环境中进行近似推理。我们建模剧集会话 - 剧集的部分，在这些部分中，潜在状态是固定的 - 并对现有的meta-RL方法提出了三个关键修改：剧集内部潜在信息的一致性，会话掩码和先验潜在条件。我们在各种领域展示了这些修改的重要性，从离散Gridworld环境到连续控制和模拟机器人辅助任务，证明了DynaMITE-RL在样本效率和推理结果方面显着优于最先进的基线。",
    "tldr": "DynaMITE-RL 提出了一种动态模型元强化学习方法，通过一致性的潜在信息、会话掩码和先验潜在条件等关键修改，在各种领域中实现了比最先进基线更优异的样本效率和推理结果。",
    "en_tdlr": "DynaMITE-RL introduces a dynamic model for meta-reinforcement learning, which significantly outperforms state-of-the-art baselines in sample efficiency and inference returns by proposing key modifications such as consistency of latent information, session masking, and prior latent conditioning across various domains."
}