{
    "title": "TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling",
    "abstract": "Time series pre-training has recently garnered wide attention for its potential to reduce labeling expenses and benefit various downstream tasks. Prior methods are mainly based on pre-training techniques well-acknowledged in vision or language, such as masked modeling and contrastive learning. However, randomly masking time series or calculating series-wise similarity will distort or neglect inherent temporal correlations crucial in time series data. To emphasize temporal correlation modeling, this paper proposes TimeSiam as a simple but effective self-supervised pre-training framework for Time series based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to capture intrinsic temporal correlations between randomly sampled past and current subseries. With a simple data augmentation method (e.g.~masking), TimeSiam can benefit from diverse augmented subseries and learn internal time-dependent representations through a past-to-current reconstruction. Moreover, learnabl",
    "link": "https://arxiv.org/abs/2402.02475",
    "context": "Title: TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling\nAbstract: Time series pre-training has recently garnered wide attention for its potential to reduce labeling expenses and benefit various downstream tasks. Prior methods are mainly based on pre-training techniques well-acknowledged in vision or language, such as masked modeling and contrastive learning. However, randomly masking time series or calculating series-wise similarity will distort or neglect inherent temporal correlations crucial in time series data. To emphasize temporal correlation modeling, this paper proposes TimeSiam as a simple but effective self-supervised pre-training framework for Time series based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to capture intrinsic temporal correlations between randomly sampled past and current subseries. With a simple data augmentation method (e.g.~masking), TimeSiam can benefit from diverse augmented subseries and learn internal time-dependent representations through a past-to-current reconstruction. Moreover, learnabl",
    "path": "papers/24/02/2402.02475.json",
    "total_tokens": 884,
    "translated_title": "TimeSiam：一种用于孪生时间序列建模的预训练框架",
    "translated_abstract": "最近，时间序列的预训练引起了广泛关注，因为它能够降低标注的成本并受益于各种下游任务。先前的方法主要基于在视觉或语言领域广为认可的预训练技术，如遮蔽建模和对比学习。然而，随机遮蔽时间序列或计算序列之间的相似性将导致丢失或忽视时间序列数据中关键的内在时间相关性。为了强调时间相关性建模，本文提出了一种名为TimeSiam的简单但有效的自监督预训练框架，用于基于孪生网络的时间序列。具体而言，TimeSiam对孪生编码器进行预训练，以捕捉随机采样的过去和当前子序列之间的内在时间相关性。通过简单的数据增强方法（例如遮蔽），TimeSiam可以从多样化的增强子序列中受益，并通过从过去到当前的重建来学习内部时序依赖的表示。此外，可学习的...",
    "tldr": "TimeSiam是一种用于时间序列建模的预训练框架，通过使用Siamese网络和简单的数据增强方法，能够捕捉时间序列数据的内在时间相关性，并学习内部时序依赖的表示。"
}