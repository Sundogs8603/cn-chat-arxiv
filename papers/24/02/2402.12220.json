{
    "title": "Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting",
    "abstract": "arXiv:2402.12220v1 Announce Type: cross  Abstract: Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning perfo",
    "link": "https://arxiv.org/abs/2402.12220",
    "context": "Title: Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting\nAbstract: arXiv:2402.12220v1 Announce Type: cross  Abstract: Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning perfo",
    "path": "papers/24/02/2402.12220.json",
    "total_tokens": 883,
    "translated_title": "贝叶斯参数高效微调以克服灾难性遗忘",
    "translated_abstract": "虽然最初是被文本转语音合成模型的自适应所激发，但我们认为更通用的参数高效微调（PEFT）是进行这种自适应的适当框架。然而，灾难性遗忘仍然是PEFT面临的问题，它损害了预训练模型固有的能力。我们证明现有的贝叶斯学习技术可以应用于PEFT，以防止灾难性遗忘，只要能够可微地计算微调层的参数转换。在一系列关于语言建模和语音合成任务的基础性实验中，我们利用建立的拉普拉斯近似，包括对角线和Kronecker分解方法，来正则化PEFT与低秩适应（LoRA）并比较它们在保留预训练知识方面的性能。我们的结果表明，我们的方法可以克服灾难性遗忘，而不会降低微调性能。",
    "tldr": "这项研究展示了如何利用贝叶斯学习技术应用于参数高效微调，以防止灾难性遗忘，实现了预训练知识的保留，并在语言建模和语音合成任务中取得成功。"
}