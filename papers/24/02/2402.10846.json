{
    "title": "FedD2S: Personalized Data-Free Federated Knowledge Distillation",
    "abstract": "arXiv:2402.10846v1 Announce Type: cross  Abstract: This paper addresses the challenge of mitigating data heterogeneity among clients within a Federated Learning (FL) framework. The model-drift issue, arising from the noniid nature of client data, often results in suboptimal personalization of a global model compared to locally trained models for each client. To tackle this challenge, we propose a novel approach named FedD2S for Personalized Federated Learning (pFL), leveraging knowledge distillation. FedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free knowledge distillation process to enhance local model personalization. Through extensive simulations on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and CIFAR100-we compare FedD2S with state-of-the-art FL baselines. The proposed approach demonstrates superior performance, characterized by accelerated convergence and improved fairness among clients. The introduced layer-dropping technique effectively capture",
    "link": "https://arxiv.org/abs/2402.10846",
    "context": "Title: FedD2S: Personalized Data-Free Federated Knowledge Distillation\nAbstract: arXiv:2402.10846v1 Announce Type: cross  Abstract: This paper addresses the challenge of mitigating data heterogeneity among clients within a Federated Learning (FL) framework. The model-drift issue, arising from the noniid nature of client data, often results in suboptimal personalization of a global model compared to locally trained models for each client. To tackle this challenge, we propose a novel approach named FedD2S for Personalized Federated Learning (pFL), leveraging knowledge distillation. FedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free knowledge distillation process to enhance local model personalization. Through extensive simulations on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and CIFAR100-we compare FedD2S with state-of-the-art FL baselines. The proposed approach demonstrates superior performance, characterized by accelerated convergence and improved fairness among clients. The introduced layer-dropping technique effectively capture",
    "path": "papers/24/02/2402.10846.json",
    "total_tokens": 818,
    "translated_title": "FedD2S: 个性化无数据联邦知识蒸馏",
    "translated_abstract": "本文解决了联邦学习（FL）框架中客户端数据异构性的挑战。我们提出了一种名为FedD2S的新方法，用于个性化联邦学习（pFL），利用知识蒸馏。FedD2S在无数据知识蒸馏过程中结合了深到浅的层丢弃机制，以增强本地模型的个性化。通过在不同图像数据集（FEMNIST、CIFAR10、CINIC0和CIFAR100）上进行大量模拟，我们将FedD2S与最先进的FL基线进行了比较。所提出的方法表现出卓越性能，具有加速收敛和改善客户间公平性的特点。引入的层丢弃技术有效捕捉",
    "tldr": "提出了FedD2S方法，通过深到浅的层丢弃机制，在无数据联邦知识蒸馏中增强了本地模型的个性化，表现出卓越性能和改善客户间公平性。"
}