{
    "title": "LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting",
    "abstract": "arXiv:2402.16132v1 Announce Type: cross  Abstract: Time-series forecasting (TSF) finds broad applications in real-world scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency. However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models.",
    "link": "https://arxiv.org/abs/2402.16132",
    "context": "Title: LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting\nAbstract: arXiv:2402.16132v1 Announce Type: cross  Abstract: Time-series forecasting (TSF) finds broad applications in real-world scenarios. Prompting off-the-shelf Large Language Models (LLMs) demonstrates strong zero-shot TSF capabilities while preserving computational efficiency. However, existing prompting methods oversimplify TSF as language next-token predictions, overlooking its dynamic nature and lack of integration with state-of-the-art prompt strategies such as Chain-of-Thought. Thus, we propose LSTPrompt, a novel approach for prompting LLMs in zero-shot TSF tasks. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models.",
    "path": "papers/24/02/2402.16132.json",
    "total_tokens": 886,
    "translated_title": "LSTPrompt: 长短期提示下的大型语言模型作为零-shot时间序列预测器",
    "translated_abstract": "时间序列预测在现实场景中有着广泛的应用。利用现成的大型语言模型进行提示展现了强大的零shot时间序列预测能力，同时保持计算效率。然而，现有的提示方法过分简化了时间序列预测，将其视为语言下一个标记的预测，忽视了其动态性以及与最先进的提示策略（如Chain-of-Thought）的融合。因此，我们提出了LSTPrompt，一种用于在零shot时间序列预测任务中提示LLMs的新方法。LSTPrompt将时间序列预测分解为短期和长期预测子任务，并为每个子任务量身定制提示。LSTPrompt引导LLMs定期重新评估预测机制，以增强适应性。广泛的评估表明，与现有的提示方法相比，LSTPrompt的性能始终更好，并且与基本时间序列预测模型相比具有竞争力。",
    "tldr": "LSTPrompt提出了一种新颖的方法，将时间序列预测任务分解为短期和长期预测子任务，并为每个子任务量身定制提示，旨在提高大型语言模型在零shot时间序列预测中的适应性和性能。",
    "en_tdlr": "LSTPrompt proposes a novel approach that decomposes time-series forecasting tasks into short-term and long-term forecasting sub-tasks, tailoring prompts for each task to enhance adaptability and performance of Large Language Models in zero-shot time series forecasting."
}