{
    "title": "The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank",
    "abstract": "arXiv:2402.13810v1 Announce Type: new  Abstract: Langevin dynamics (LD) is widely used for sampling from distributions and for optimization. In this work, we derive a closed-form expression for the expected loss of preconditioned LD near stationary points of the objective function. We use the fact that at the vicinity of such points, LD reduces to an Ornstein-Uhlenbeck process, which is amenable to convenient mathematical treatment. Our analysis reveals that when the preconditioning matrix satisfies a particular relation with respect to the noise covariance, LD's expected loss becomes proportional to the rank of the objective's Hessian. We illustrate the applicability of this result in the context of neural networks, where the Hessian rank has been shown to capture the complexity of the predictor function but is usually computationally hard to probe. Finally, we use our analysis to compare SGD-like and Adam-like preconditioners and identify the regimes under which each of them leads to",
    "link": "https://arxiv.org/abs/2402.13810",
    "context": "Title: The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank\nAbstract: arXiv:2402.13810v1 Announce Type: new  Abstract: Langevin dynamics (LD) is widely used for sampling from distributions and for optimization. In this work, we derive a closed-form expression for the expected loss of preconditioned LD near stationary points of the objective function. We use the fact that at the vicinity of such points, LD reduces to an Ornstein-Uhlenbeck process, which is amenable to convenient mathematical treatment. Our analysis reveals that when the preconditioning matrix satisfies a particular relation with respect to the noise covariance, LD's expected loss becomes proportional to the rank of the objective's Hessian. We illustrate the applicability of this result in the context of neural networks, where the Hessian rank has been shown to capture the complexity of the predictor function but is usually computationally hard to probe. Finally, we use our analysis to compare SGD-like and Adam-like preconditioners and identify the regimes under which each of them leads to",
    "path": "papers/24/02/2402.13810.json",
    "total_tokens": 816,
    "translated_title": "通过预条件Langevin 动力学的预期损失揭示Hessian秩",
    "translated_abstract": "Langevin 动力学（LD）被广泛用于从分布中抽样和优化。在这项工作中，我们推导出了预条件LD在目标函数的稳定点附近的预期损失的闭式表达式。我们利用了在这些点附近，LD会退化为一种适合便利数学处理的Ornstein-Uhlenbeck过程的事实。我们的分析表明，当预条件矩阵满足与噪声协方差的特定关系时，LD的预期损失将成正比于目标Hessian的秩。我们展示了这个结果在神经网络的背景下的适用性，其中Hessian秩已被证明能够捕捉预测函数的复杂性，但通常难以计算。最后，我们利用我们的分析比较了类似于SGD和类似于Adam的预处理器，并确定了它们各自导致的情况下的区域。",
    "tldr": "预条件Langevin动力学的预期损失与目标函数的Hessian秩成正比，并且在神经网络中具有应用前景。",
    "en_tdlr": "The expected loss of preconditioned Langevin dynamics is proportional to the rank of the objective's Hessian, showing potential applications in neural networks."
}