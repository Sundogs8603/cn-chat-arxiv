{
    "title": "Optimal score estimation via empirical Bayes smoothing",
    "abstract": "We study the problem of estimating the score function of an unknown probability distribution $\\rho^*$ from $n$ independent and identically distributed observations in $d$ dimensions. Assuming that $\\rho^*$ is subgaussian and has a Lipschitz-continuous score function $s^*$, we establish the optimal rate of $\\tilde \\Theta(n^{-\\frac{2}{d+4}})$ for this estimation problem under the loss function $\\|\\hat s - s^*\\|^2_{L^2(\\rho^*)}$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension $d$. Leveraging key insights in empirical Bayes theory as well as a new convergence rate of smoothed empirical distribution in Hellinger distance, we show that a regularized score estimator based on a Gaussian kernel attains this rate, shown optimal by a matching minimax lower bound. We also discuss the implication of our theory on the sample complexity of score-based generativ",
    "link": "https://arxiv.org/abs/2402.07747",
    "context": "Title: Optimal score estimation via empirical Bayes smoothing\nAbstract: We study the problem of estimating the score function of an unknown probability distribution $\\rho^*$ from $n$ independent and identically distributed observations in $d$ dimensions. Assuming that $\\rho^*$ is subgaussian and has a Lipschitz-continuous score function $s^*$, we establish the optimal rate of $\\tilde \\Theta(n^{-\\frac{2}{d+4}})$ for this estimation problem under the loss function $\\|\\hat s - s^*\\|^2_{L^2(\\rho^*)}$ that is commonly used in the score matching literature, highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension $d$. Leveraging key insights in empirical Bayes theory as well as a new convergence rate of smoothed empirical distribution in Hellinger distance, we show that a regularized score estimator based on a Gaussian kernel attains this rate, shown optimal by a matching minimax lower bound. We also discuss the implication of our theory on the sample complexity of score-based generativ",
    "path": "papers/24/02/2402.07747.json",
    "total_tokens": 958,
    "translated_title": "通过经验贝叶斯平滑进行最优分数估计",
    "translated_abstract": "我们研究了从$d$维独立同分布观测中估计未知概率分布$\\rho^*$的分数函数的问题。在假设$\\rho^*$是亚高斯的并且具有Lipschitz连续的分数函数$s^*$的情况下，我们在score matching文献中常用的损失函数$\\|\\hat s - s^*\\|^2_{L^2(\\rho^*)}$下建立了该估计问题的最优速率为$\\tilde \\Theta(n^{-\\frac{2}{d+4}})$，强调了维度$d$的增长对于准确分数估计的样本复杂性呈指数级增长的困境。借助经验贝叶斯理论的关键见解以及平滑经验分布在Hellinger距离下的新收敛速率，我们展示了基于高斯核的正则化分数估计器能够达到该速率，并通过匹配最小值下界证明了其最优性。我们还讨论了我们理论对于基于分数的生成模型的样本复杂性的影响。",
    "tldr": "该论文研究了通过经验贝叶斯平滑在高维数据中估计未知概率分布的分数函数的问题，提出了一种基于高斯核的正则化分数估计器，在score matching损失函数下达到了最优速率，并揭示了维度增长对样本复杂性的指数级影响。",
    "en_tdlr": "This paper investigates the problem of estimating the score function of an unknown probability distribution in high-dimensional data using empirical Bayes smoothing. It proposes a regularized score estimator based on a Gaussian kernel, which achieves the optimal rate under the score matching loss function and highlights the exponential impact of dimensionality on sample complexity."
}