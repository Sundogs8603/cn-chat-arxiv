{
    "title": "Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning",
    "abstract": "arXiv:2402.10810v1 Announce Type: new  Abstract: We study the Constrained Convex Markov Decision Process (MDP), where the goal is to minimize a convex functional of the visitation measure, subject to a convex constraint. Designing algorithms for a constrained convex MDP faces several challenges, including (1) handling the large state space, (2) managing the exploration/exploitation tradeoff, and (3) solving the constrained optimization where the objective and the constraint are both nonlinear functions of the visitation measure. In this work, we present a model-based algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which Lagrangian and Fenchel duality are implemented to reformulate the original constrained problem into an unconstrained primal-dual optimization. Moreover, the primal variables are updated by model-based value iteration following the principle of Optimism in the Face of Uncertainty (OFU), while the dual variables are updated by gradient ascent. Moreover,",
    "link": "https://arxiv.org/abs/2402.10810",
    "context": "Title: Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning\nAbstract: arXiv:2402.10810v1 Announce Type: new  Abstract: We study the Constrained Convex Markov Decision Process (MDP), where the goal is to minimize a convex functional of the visitation measure, subject to a convex constraint. Designing algorithms for a constrained convex MDP faces several challenges, including (1) handling the large state space, (2) managing the exploration/exploitation tradeoff, and (3) solving the constrained optimization where the objective and the constraint are both nonlinear functions of the visitation measure. In this work, we present a model-based algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which Lagrangian and Fenchel duality are implemented to reformulate the original constrained problem into an unconstrained primal-dual optimization. Moreover, the primal variables are updated by model-based value iteration following the principle of Optimism in the Face of Uncertainty (OFU), while the dual variables are updated by gradient ascent. Moreover,",
    "path": "papers/24/02/2402.10810.json",
    "total_tokens": 945,
    "translated_title": "双重对偶：用于受限制强化学习的变分原始对偶策略优化",
    "translated_abstract": "我们研究受限凸马尔可夫决策过程（MDP），目标是最小化访问度量的凸泛函，受到凸约束的限制。为受限凸MDP设计算法面临几个挑战，包括（1）处理大状态空间，（2）管理探索/开拓的权衡，以及（3）解决约束优化，其中目标和约束都是访问度量的非线性函数。在这项工作中，我们提出了一种基于模型的算法，变分原始对偶策略优化（VPDPO），其中Lagrangian 和 Fenchel 对偶被用于将原始受限问题重新公式化为无限制原始-对偶优化。此外，原始变量通过基于模型的值迭代更新，遵循不确定性面前的乐观原则（OFU），而对偶变量则通过梯度上升更新。",
    "tldr": "本文提出了一种基于模型的算法，Variational Primal-Dual Policy Optimization (VPDPO)，通过实现Lagrangian和Fenchel对偶来将原始受限问题重构为无约束的原始-对偶优化，并且采用乐观原则更新原始变量和梯度上升更新对偶变量。",
    "en_tdlr": "The paper introduces a model-based algorithm, Variational Primal-Dual Policy Optimization (VPDPO), which reformulates the original constrained problem into an unconstrained primal-dual optimization using Lagrangian and Fenchel duality, updating primal variables with a model-based value iteration following the principle of Optimism in the Face of Uncertainty (OFU), and updating dual variables with gradient ascent."
}