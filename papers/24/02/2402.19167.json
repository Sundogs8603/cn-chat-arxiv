{
    "title": "Teaching Large Language Models an Unseen Language on the Fly",
    "abstract": "arXiv:2402.19167v1 Announce Type: new  Abstract: Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce \\textsc{DiPMT++}, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and only 5K parallel sentences, \\textsc{DiPMT++} significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. Furthermore, we demonstrate the practical utility of this framework in aiding humans to translate completely unseen languages, which could contribute to the preservation of linguistic diversity.",
    "link": "https://arxiv.org/abs/2402.19167",
    "context": "Title: Teaching Large Language Models an Unseen Language on the Fly\nAbstract: arXiv:2402.19167v1 Announce Type: new  Abstract: Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce \\textsc{DiPMT++}, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and only 5K parallel sentences, \\textsc{DiPMT++} significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. Furthermore, we demonstrate the practical utility of this framework in aiding humans to translate completely unseen languages, which could contribute to the preservation of linguistic diversity.",
    "path": "papers/24/02/2402.19167.json",
    "total_tokens": 957,
    "translated_title": "在需要时教授大型语言模型一种未知语言",
    "translated_abstract": "现有的大型语言模型在支持许多低资源语言方面存在困难，特别是在极低资源语言方面，在这些语言中，有效参数更新所需的训练数据极少。因此，我们研究了LLM是否可以仅通过提示在飞行中学习一种新语言。为了研究这个问题，我们为壮语收集了一个研究套件，这是当前没有LLMs支持的一种语言。我们介绍了一种名为DiPMT++的框架，用于通过上下文学习将LLMs适应看不见的语言。使用一本词典和仅有5K对平行句子，DiPMT++将GPT-4的性能从0提升到16 BLEU，用于汉语到壮语的翻译，并实现了壮语到汉语的32 BLEU。此外，我们展示了这一框架在帮助人类翻译完全未知语言方面的实际用途，这有助于维护语言多样性。",
    "tldr": "通过提示，在飞行中教授大型语言模型一种未知语言，提出DiPMT ++框架，通过上下文学习使LLMs适应看不见的语言，并实现了壮语和汉语之间的翻译性能显著提升，并展示了该框架在帮助人类翻译完全未知语言方面的实用性。",
    "en_tdlr": "Teaching large language models an unseen language on the fly solely through prompting, introducing the DiPMT++ framework for adapting LLMs to unseen languages by in-context learning, significantly enhancing translation performance between Zhuang and Chinese, and demonstrating the practical utility of this framework in aiding humans to translate completely unseen languages."
}