{
    "title": "Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration",
    "abstract": "arXiv:2402.16030v1 Announce Type: cross  Abstract: While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives. This paper delves further into current order-based methods, examining their inefficiencies in utilizing reward values and addressing misalignment issues. Building upon these findings, we propose a novel \\textbf{V}alue-based \\textbf{C}ali\\textbf{B}ration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and stability in diverse settings.",
    "link": "https://arxiv.org/abs/2402.16030",
    "context": "Title: Don't Forget Your Reward Values: Language Model Alignment via Value-based Calibration\nAbstract: arXiv:2402.16030v1 Announce Type: cross  Abstract: While Reinforcement Learning from Human Feedback (RLHF) significantly enhances the generation quality of Large Language Models (LLMs), recent studies have raised concerns regarding the complexity and instability associated with the Proximal Policy Optimization (PPO) algorithm, proposing a series of order-based calibration methods as viable alternatives. This paper delves further into current order-based methods, examining their inefficiencies in utilizing reward values and addressing misalignment issues. Building upon these findings, we propose a novel \\textbf{V}alue-based \\textbf{C}ali\\textbf{B}ration (VCB) method to better align LLMs with human preferences. Experimental results demonstrate that VCB surpasses existing alignment methods on AI assistant and summarization datasets, providing impressive generalizability, robustness, and stability in diverse settings.",
    "path": "papers/24/02/2402.16030.json",
    "total_tokens": 871,
    "translated_title": "不要忘记您的奖励价值: 通过基于价值的校准实现语言模型的对齐",
    "translated_abstract": "从人类反馈中进行强化学习（RLHF）显著提高了大型语言模型（LLM）的生成质量，但最近的研究提出了对近端策略优化（PPO）算法复杂性和不稳定性的担忧，提议一系列基于顺序的校准方法作为可行的替代方法。本文进一步探讨了当前基于顺序的方法，检查它们在利用奖励价值和解决不对齐问题方面的低效性。基于这些发现，我们提出了一种新颖的基于价值的校准（VCB）方法，以更好地使LLMs与人类偏好对齐。实验结果表明，VCB在AI助手和摘要数据集上超越了现有的对齐方法，在各种环境中提供了令人印象深刻的通用性、稳健性和稳定性。",
    "tldr": "本文提出了一种基于价值的校准（VCB）方法，以解决大型语言模型与人类偏好之间的对齐问题，并在实验中表现出比现有方法更好的通用性、稳健性和稳定性。",
    "en_tdlr": "This paper proposes a novel Value-based Calibration (VCB) method to address the alignment between Large Language Models (LLMs) and human preferences, demonstrating superior generalizability, robustness, and stability compared to existing methods."
}