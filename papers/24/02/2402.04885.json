{
    "title": "A Unified Gaussian Process for Branching and Nested Hyperparameter Optimization",
    "abstract": "Choosing appropriate hyperparameters plays a crucial role in the success of neural networks as hyper-parameters directly control the behavior and performance of the training algorithms. To obtain efficient tuning, Bayesian optimization methods based on Gaussian process (GP) models are widely used. Despite numerous applications of Bayesian optimization in deep learning, the existing methodologies are developed based on a convenient but restrictive assumption that the tuning parameters are independent of each other. However, tuning parameters with conditional dependence are common in practice. In this paper, we focus on two types of them: branching and nested parameters. Nested parameters refer to those tuning parameters that exist only within a particular setting of another tuning parameter, and a parameter within which other parameters are nested is called a branching parameter. To capture the conditional dependence between branching and nested parameters, a unified Bayesian optimizati",
    "link": "https://arxiv.org/abs/2402.04885",
    "context": "Title: A Unified Gaussian Process for Branching and Nested Hyperparameter Optimization\nAbstract: Choosing appropriate hyperparameters plays a crucial role in the success of neural networks as hyper-parameters directly control the behavior and performance of the training algorithms. To obtain efficient tuning, Bayesian optimization methods based on Gaussian process (GP) models are widely used. Despite numerous applications of Bayesian optimization in deep learning, the existing methodologies are developed based on a convenient but restrictive assumption that the tuning parameters are independent of each other. However, tuning parameters with conditional dependence are common in practice. In this paper, we focus on two types of them: branching and nested parameters. Nested parameters refer to those tuning parameters that exist only within a particular setting of another tuning parameter, and a parameter within which other parameters are nested is called a branching parameter. To capture the conditional dependence between branching and nested parameters, a unified Bayesian optimizati",
    "path": "papers/24/02/2402.04885.json",
    "total_tokens": 824,
    "translated_title": "一个统一的高斯过程用于分支和嵌套超参数优化",
    "translated_abstract": "在神经网络的成功中，选择合适的超参数起着至关重要的作用，因为超参数直接控制训练算法的行为和性能。为了获得高效的调参，基于高斯过程（GP）模型的贝叶斯优化方法被广泛应用。尽管贝叶斯优化在深度学习中具有许多应用，但现有的方法都基于一个方便但限制性的假设，即调参参数彼此独立。然而，在实践中，条件依赖的调参参数是常见的。本文重点研究了两种类型的调参参数：分支和嵌套参数。嵌套参数指的是那些仅存在于另一个调参参数特定设置中的调参参数，而其它参数在其中嵌套的参数称为分支参数。为了捕捉分支和嵌套参数之间的条件依赖关系，本文提出了一个统一的贝叶斯优化方法。",
    "tldr": "本文提出了一个统一的贝叶斯优化方法，用于处理分支和嵌套参数之间的条件依赖关系，该方法能够有效地优化神经网络的超参数选择和调整。",
    "en_tdlr": "This paper presents a unified Bayesian optimization method for handling the conditional dependence between branching and nested parameters, which efficiently optimizes the selection and tuning of hyperparameters in neural networks."
}