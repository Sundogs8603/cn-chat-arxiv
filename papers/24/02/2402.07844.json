{
    "title": "Mercury: An Efficiency Benchmark for LLM Code Synthesis",
    "abstract": "Despite advancements in evaluating Large Language Models (LLMs) for code synthesis, benchmarks have predominantly focused on functional correctness, overlooking the importance of code efficiency. We present Mercury, the first benchmark designated for assessing the code efficiency of LLM code synthesis tasks. Mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation. Unlike existing benchmarks, Mercury integrates a novel metric Beyond@K to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard. Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, unde",
    "link": "https://arxiv.org/abs/2402.07844",
    "context": "Title: Mercury: An Efficiency Benchmark for LLM Code Synthesis\nAbstract: Despite advancements in evaluating Large Language Models (LLMs) for code synthesis, benchmarks have predominantly focused on functional correctness, overlooking the importance of code efficiency. We present Mercury, the first benchmark designated for assessing the code efficiency of LLM code synthesis tasks. Mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation. Unlike existing benchmarks, Mercury integrates a novel metric Beyond@K to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard. Our findings reveal that while LLMs demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, unde",
    "path": "papers/24/02/2402.07844.json",
    "total_tokens": 828,
    "translated_title": "Mercury: 一种用于LLM代码综合效率评估的基准",
    "translated_abstract": "尽管在评估大型语言模型（LLM）进行代码综合方面取得了进展，但基准主要集中在功能正确性上，忽视了代码效率的重要性。我们提出了Mercury，这是第一个专用于评估LLM代码综合任务的代码效率的基准。Mercury由1,889个涵盖不同难度级别的编程任务组成，还包括生成无限案例的测试用例生成器，以进行全面评估。与现有的基准不同，Mercury集成了一种新的度量标准Beyond@K，以基于历史提交来衡量归一化的代码效率，从而为代码综合提供了新的评估指标，鼓励生成功能正确且计算效率高的代码，体现了现实世界软件开发的标准。我们的研究结果表明，虽然LLM表现出生成功能正确代码的显著能力，但它们在效率输出方面仍存在很大的差距。",
    "tldr": "Mercury提出了一个针对LLM代码综合任务的效率评估基准，通过引入新的度量标准Beyond@K来衡量归一化的代码效率，从而鼓励生成功能正确且计算效率高的代码。"
}