{
    "title": "Benchmarking Retrieval-Augmented Generation for Medicine",
    "abstract": "arXiv:2402.13178v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs ",
    "link": "https://arxiv.org/abs/2402.13178",
    "context": "Title: Benchmarking Retrieval-Augmented Generation for Medicine\nAbstract: arXiv:2402.13178v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs ",
    "path": "papers/24/02/2402.13178.json",
    "total_tokens": 875,
    "translated_title": "用于医学领域的检索增强生成的基准测试",
    "translated_abstract": "大型语言模型(LLMs)在广泛的医学问答任务上取得了最先进的性能，但仍然面临幻觉和过时知识的挑战。检索增强生成(RAG)是一个有前途的解决方案，并得到了广泛采用。然而，RAG系统可能涉及多个灵活的组件，并且缺乏关于各种医学目的的最佳RAG设置的最佳实践。为了系统地评估这些系统，我们提出了医学信息检索增强生成评估(MIRAGE)，这是一个首创的基准测试，包括来自五个医学问答数据集的7,663个问题。利用MIRAGE，我们通过本文介绍的MedRAG工具包，在41种不同语料库、检索器和骨干LLMs的组合上进行了超过1.8万亿的提示标记的大规模实验。总体而言，MedRAG提高了六种不同LLMs的准确性。",
    "tldr": "通过提出首个医学信息检索增强生成评估(MIRAGE)基准测试，并使用MedRAG工具包进行大规模实验，实现了对多个大型语言模型的准确性改进。"
}