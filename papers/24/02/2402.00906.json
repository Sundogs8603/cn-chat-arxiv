{
    "title": "BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic Architectures against Model Inversion Attacks",
    "abstract": "With the mainstream integration of machine learning into security-sensitive domains such as healthcare and finance, concerns about data privacy have intensified. Conventional artificial neural networks (ANNs) have been found vulnerable to several attacks that can leak sensitive data. Particularly, model inversion (MI) attacks enable the reconstruction of data samples that have been used to train the model. Neuromorphic architectures have emerged as a paradigm shift in neural computing, enabling asynchronous and energy-efficient computation. However, little to no existing work has investigated the privacy of neuromorphic architectures against model inversion. Our study is motivated by the intuition that the non-differentiable aspect of spiking neural networks (SNNs) might result in inherent privacy-preserving properties, especially against gradient-based attacks. To investigate this hypothesis, we propose a thorough exploration of SNNs' privacy-preserving capabilities. Specifically, we ",
    "link": "https://rss.arxiv.org/abs/2402.00906",
    "context": "Title: BrainLeaks: On the Privacy-Preserving Properties of Neuromorphic Architectures against Model Inversion Attacks\nAbstract: With the mainstream integration of machine learning into security-sensitive domains such as healthcare and finance, concerns about data privacy have intensified. Conventional artificial neural networks (ANNs) have been found vulnerable to several attacks that can leak sensitive data. Particularly, model inversion (MI) attacks enable the reconstruction of data samples that have been used to train the model. Neuromorphic architectures have emerged as a paradigm shift in neural computing, enabling asynchronous and energy-efficient computation. However, little to no existing work has investigated the privacy of neuromorphic architectures against model inversion. Our study is motivated by the intuition that the non-differentiable aspect of spiking neural networks (SNNs) might result in inherent privacy-preserving properties, especially against gradient-based attacks. To investigate this hypothesis, we propose a thorough exploration of SNNs' privacy-preserving capabilities. Specifically, we ",
    "path": "papers/24/02/2402.00906.json",
    "total_tokens": 885,
    "translated_title": "BrainLeaks: 关于神经形态架构对模型反转攻击的隐私保护性质",
    "translated_abstract": "随着机器学习在医疗保健和金融等安全敏感领域的主流整合，对数据隐私的担忧已经加剧。传统的人工神经网络（ANNs）已被发现容易受到多种泄露敏感数据的攻击。特别是，模型反转（MI）攻击可以重构用于训练模型的数据样本。神经形态架构已经成为神经计算的一种范式转变，实现了异步和节能的计算。然而，几乎没有现有的工作研究了神经形态架构对模型反转的隐私性。我们的研究受到的启示是，脉冲神经网络（SNNs）的不可微特性可能导致固有的隐私保护性质，尤其抵抗基于梯度的攻击。为了研究这一假设，我们提出了对SNNs的隐私保护能力进行全面探索。",
    "tldr": "本研究探索了神经形态架构对模型反转攻击的隐私保护能力，发现脉冲神经网络具有固有的隐私保护性质，并能有效抵抗基于梯度的攻击。",
    "en_tdlr": "This study explores the privacy-preserving capabilities of neuromorphic architectures against model inversion attacks, finding that spiking neural networks have inherent privacy-preserving properties and can effectively resist gradient-based attacks."
}