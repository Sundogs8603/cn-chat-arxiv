{
    "title": "Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss",
    "abstract": "Machine learning models are susceptible to membership inference attacks (MIAs), which aim to infer whether a sample is in the training set. Existing work utilizes gradient ascent to enlarge the loss variance of training data, alleviating the privacy risk. However, optimizing toward a reverse direction may cause the model parameters to oscillate near local minima, leading to instability and suboptimal performance. In this work, we propose a novel method -- Convex-Concave Loss, which enables a high variance of training loss distribution by gradient descent. Our method is motivated by the theoretical analysis that convex losses tend to decrease the loss variance during training. Thus, our key idea behind CCL is to reduce the convexity of loss functions with a concave term. Trained with CCL, neural networks produce losses with high variance for training data, reinforcing the defense against MIAs. Extensive experiments demonstrate the superiority of CCL, achieving state-of-the-art balance i",
    "link": "https://arxiv.org/abs/2402.05453",
    "context": "Title: Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss\nAbstract: Machine learning models are susceptible to membership inference attacks (MIAs), which aim to infer whether a sample is in the training set. Existing work utilizes gradient ascent to enlarge the loss variance of training data, alleviating the privacy risk. However, optimizing toward a reverse direction may cause the model parameters to oscillate near local minima, leading to instability and suboptimal performance. In this work, we propose a novel method -- Convex-Concave Loss, which enables a high variance of training loss distribution by gradient descent. Our method is motivated by the theoretical analysis that convex losses tend to decrease the loss variance during training. Thus, our key idea behind CCL is to reduce the convexity of loss functions with a concave term. Trained with CCL, neural networks produce losses with high variance for training data, reinforcing the defense against MIAs. Extensive experiments demonstrate the superiority of CCL, achieving state-of-the-art balance i",
    "path": "papers/24/02/2402.05453.json",
    "total_tokens": 887,
    "translated_title": "通过凸凹损失函数降低会员推断中的隐私风险",
    "translated_abstract": "机器学习模型容易受到会员推断攻击（MIAs），即推断样本是否在训练集中。现有工作利用梯度上升来增大训练数据的损失方差，缓解隐私风险。然而，向相反方向优化可能导致模型参数在局部最小值附近振荡，导致不稳定和次优性能。在本研究中，我们提出了一种新的方法——凸凹损失函数，在梯度下降的过程中实现了训练损失分布的高方差。我们的方法受到理论分析的启发，凸损失函数在训练过程中倾向于减少损失方差。因此，我们在CCL的背后的关键思想是通过凹函数项减小损失函数的凸性。使用CCL训练的神经网络产生训练数据的高方差损失，加强了对MIAs的防御。大量实验证实了CCL的卓越性能，实现了最新的平衡。",
    "tldr": "本论文提出了一种凸凹损失函数的方法，通过梯度下降实现训练损失的高方差，从而降低会员推断攻击中的隐私风险。",
    "en_tdlr": "This paper proposes a method using convex-concave loss function to achieve high variance of training loss through gradient descent, thereby mitigating privacy risk in membership inference attacks."
}