{
    "title": "Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs",
    "abstract": "In this paper, we introduce \\emph{refined Direct Preference Optimization} (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data. The method involves creating synthetic data using self-critique prompting by a teacher LLM and then utilising a generalized DPO loss function to distil to a student LLM. The loss function incorporates an additional external reward model to improve the quality of synthetic data, making rDPO robust to potential noise in the synthetic dataset. rDPO is shown to be effective in a diverse set of behavioural alignment tasks, such as improved safety, robustness against role-playing, and reduced sycophancy. Code to be released at https://github.com/vicgalle/refined-dpo.",
    "link": "https://arxiv.org/abs/2402.08005",
    "context": "Title: Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs\nAbstract: In this paper, we introduce \\emph{refined Direct Preference Optimization} (rDPO), a method for improving the behavioral alignment of Large Language Models (LLMs) without the need for human-annotated data. The method involves creating synthetic data using self-critique prompting by a teacher LLM and then utilising a generalized DPO loss function to distil to a student LLM. The loss function incorporates an additional external reward model to improve the quality of synthetic data, making rDPO robust to potential noise in the synthetic dataset. rDPO is shown to be effective in a diverse set of behavioural alignment tasks, such as improved safety, robustness against role-playing, and reduced sycophancy. Code to be released at https://github.com/vicgalle/refined-dpo.",
    "path": "papers/24/02/2402.08005.json",
    "total_tokens": 832,
    "translated_title": "带有合成数据的改进型直接优化法用于LLM的行为调整",
    "translated_abstract": "本文介绍了一种改进型直接优化法（rDPO），用于改善大规模语言模型（LLM）的行为调整，无需人工标注数据。该方法通过自我评论提示教师LLM来创建合成数据，然后利用广义DPO损失函数来提纯给学生LLM。损失函数结合了额外的外部奖励模型，以提高合成数据的质量，使rDPO能够抵抗合成数据集中的潜在噪声。rDPO在多种行为调整任务中展现出良好效果，如提高安全性，抵抗角色扮演，降低巴结行为。代码将在https://github.com/vicgalle/refined-dpo上发布。",
    "tldr": "本文提出了一种改进的直接优化法（rDPO），通过使用合成数据来改善大规模语言模型（LLM）的行为调整。这种方法通过自我评论和广义DPO损失函数来优化学生LLM，并利用外部奖励模型提高合成数据质量，从而使rDPO在多个行为调整任务中表现出良好效果。",
    "en_tdlr": "This paper introduces refined Direct Preference Optimization (rDPO), a method that improves the behavioral alignment of Large Language Models (LLMs) by utilizing synthetic data and a generalized DPO loss function. The method is effective in various behavioral alignment tasks and addresses issues such as safety, robustness, and sycophancy."
}