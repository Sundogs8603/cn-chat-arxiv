{
    "title": "A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing",
    "abstract": "Analog In-Memory Computing (AIMC) is an emerging technology for fast and energy-efficient Deep Learning (DL) inference. However, a certain amount of digital post-processing is required to deal with circuit mismatches and non-idealities associated with the memory devices. Efficient near-memory digital logic is critical to retain the high area/energy efficiency and low latency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic with limited parallelization capability and high latency. To overcome these limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on fixed-point arithmetic. It achieves competitive accuracy and higher computing throughput than previous approaches while minimizing the area overhead. Moreover, the NMPU supports standard DL activation steps, such as ReLU and Batch Normalization. We perform a physical implementation of the NMPU design in a 14 nm CMOS technology and provide detailed performance, power, and area assessments. We valid",
    "link": "https://arxiv.org/abs/2402.07549",
    "context": "Title: A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing\nAbstract: Analog In-Memory Computing (AIMC) is an emerging technology for fast and energy-efficient Deep Learning (DL) inference. However, a certain amount of digital post-processing is required to deal with circuit mismatches and non-idealities associated with the memory devices. Efficient near-memory digital logic is critical to retain the high area/energy efficiency and low latency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic with limited parallelization capability and high latency. To overcome these limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on fixed-point arithmetic. It achieves competitive accuracy and higher computing throughput than previous approaches while minimizing the area overhead. Moreover, the NMPU supports standard DL activation steps, such as ReLU and Batch Normalization. We perform a physical implementation of the NMPU design in a 14 nm CMOS technology and provide detailed performance, power, and area assessments. We valid",
    "path": "papers/24/02/2402.07549.json",
    "total_tokens": 939,
    "translated_title": "用于模拟内存计算的精度优化固定点近存储数字处理单元",
    "translated_abstract": "模拟内存计算是一种用于快速和高能效的深度学习推理的新兴技术。然而，需要一定量的数字后处理来处理与内存设备相关的电路不匹配和非理想特性。高效的近存储数字逻辑对于保持模拟内存计算的高面积/能量效率和低延迟至关重要。现有的系统采用有限的并行化能力和高延迟的浮点16位（FP16）算术。为了克服这些限制，我们提出了一种基于固定点算术的近存储数字处理单元（NMPU）。它在保持最小面积开销的同时，实现了竞争性精度和更高的计算吞吐量。此外，NMPU支持标准的深度学习激活步骤，如ReLU和批量归一化。我们使用14nm CMOS技术对NMPU设计进行物理实现，并提供了详细的性能、功耗和面积评估。",
    "tldr": "我们提出了一种精度优化的固定点近存储数字处理单元，用于高效的模拟内存计算。该处理单元在保持高效能量和面积效率以及低延迟的同时，通过支持标准的深度学习激活步骤实现了竞争性的准确性和更高的计算吞吐量。",
    "en_tdlr": "We propose a precision-optimized fixed-point near-memory digital processing unit for efficient analog in-memory computing, which achieves competitive accuracy and higher computing throughput while maintaining high energy and area efficiency, low latency, and supporting standard deep learning activation steps."
}