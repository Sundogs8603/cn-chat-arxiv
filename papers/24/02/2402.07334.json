{
    "title": "Differentially Private Training of Mixture of Experts Models",
    "abstract": "This position paper investigates the integration of Differential Privacy (DP) in the training of Mixture of Experts (MoE) models within the field of natural language processing. As Large Language Models (LLMs) scale to billions of parameters, leveraging expansive datasets, they exhibit enhanced linguistic capabilities and emergent abilities. However, this growth raises significant computational and privacy concerns. Our study addresses these issues by exploring the potential of MoE models, known for their computational efficiency, and the application of DP, a standard for privacy preservation. We present the first known attempt to train MoE models under the constraints of DP, addressing the unique challenges posed by their architecture and the complexities of DP integration. Our initial experimental studies demonstrate that MoE models can be effectively trained with DP, achieving performance that is competitive with their non-private counterparts. This initial study aims to provide val",
    "link": "https://arxiv.org/abs/2402.07334",
    "context": "Title: Differentially Private Training of Mixture of Experts Models\nAbstract: This position paper investigates the integration of Differential Privacy (DP) in the training of Mixture of Experts (MoE) models within the field of natural language processing. As Large Language Models (LLMs) scale to billions of parameters, leveraging expansive datasets, they exhibit enhanced linguistic capabilities and emergent abilities. However, this growth raises significant computational and privacy concerns. Our study addresses these issues by exploring the potential of MoE models, known for their computational efficiency, and the application of DP, a standard for privacy preservation. We present the first known attempt to train MoE models under the constraints of DP, addressing the unique challenges posed by their architecture and the complexities of DP integration. Our initial experimental studies demonstrate that MoE models can be effectively trained with DP, achieving performance that is competitive with their non-private counterparts. This initial study aims to provide val",
    "path": "papers/24/02/2402.07334.json",
    "total_tokens": 898,
    "translated_title": "差分隐私训练混合专家模型",
    "translated_abstract": "本文研究了在自然语言处理领域中，将差分隐私(DP)与混合专家模型(MoE)的训练相结合。随着大型语言模型(LLMs)的参数规模扩大到数十亿，利用庞大的数据集，它们展现出了增强的语言能力和新兴的能力。然而，这种增长引发了重要的计算和隐私问题。我们的研究通过探索MoE模型的潜力（它们以计算效率著称）和应用DP（隐私保护的标准）来解决这些问题。我们首次尝试在DP的约束下训练MoE模型，解决了其架构和DP整合的复杂性所带来的独特挑战。我们的初步实验研究表明，MoE模型可以有效地使用DP进行训练，其性能与非隐私训练的模型相媲美。这项初步研究旨在提供有价值的参考。",
    "tldr": "本文研究了在自然语言处理中，如何使用差分隐私训练混合专家模型。研究通过探索MoE模型的潜力和应用DP解决了计算和隐私问题，并实验证明了MoE模型可以在保护隐私的前提下有效训练。",
    "en_tdlr": "This paper investigates the integration of Differential Privacy in the training of Mixture of Experts models in the field of natural language processing. The study addresses computational and privacy concerns by exploring the potential of MoE models and applying DP, and demonstrates that MoE models can be effectively trained with privacy preservation."
}