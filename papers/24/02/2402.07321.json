{
    "title": "Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs",
    "abstract": "How do transformer-based large language models (LLMs) store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form `Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions. Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer. In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pa",
    "link": "https://arxiv.org/abs/2402.07321",
    "context": "Title: Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs\nAbstract: How do transformer-based large language models (LLMs) store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form `Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions. Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer. In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pa",
    "path": "papers/24/02/2402.07321.json",
    "total_tokens": 861,
    "translated_title": "总结事实：LLMs中背后的加法机制解析事实回忆的过程",
    "translated_abstract": "Transformer-based大型语言模型(LLMs)是如何存储和检索知识的？我们关注最基本的任务形式——事实回忆，模型通过在\"事实：斗兽场位于国家\"等提示中明确呈现存储的事实。我们发现，事实回忆背后的机制故事比以前认为的要复杂。它包含几种不同、独立且性质不同的机制，这些机制通过相加组合，对正确的属性进行构造性干涉。我们将这一通用现象称为加法模式：模型通过加总多个独立的贡献进行计算。每个机制的贡献本身可能是不足够的，但总和会对正确答案产生积极的干涉。此外，我们扩展了直接对logit属性的方法，可以将一个注意头的输出归因给单独的源记号。我们使用这种技术来解开我们所说的\"混合头\"——它们本身是a的一部分.",
    "tldr": "Transformer-based大型语言模型在事实回忆任务中使用加法模式来存储和检索知识，通过相加组合多个独立的机制对正确答案进行构造性干涉。",
    "en_tdlr": "Transformer-based large language models (LLMs) use an additive motif to store and retrieve knowledge in factual recall tasks, combining multiple independent mechanisms in a constructive interference to produce the correct answer."
}