{
    "title": "Towards Cross-Domain Continual Learning",
    "abstract": "arXiv:2402.12490v1 Announce Type: cross  Abstract: Continual learning is a process that involves training learning agents to sequentially master a stream of tasks or classes without revisiting past data. The challenge lies in leveraging previously acquired knowledge to learn new tasks efficiently, while avoiding catastrophic forgetting. Existing methods primarily focus on single domains, restricting their applicability to specific problems.   In this work, we introduce a novel approach called Cross-Domain Continual Learning (CDCL) that addresses the limitations of being limited to single supervised domains. Our method combines inter- and intra-task cross-attention mechanisms within a compact convolutional network. This integration enables the model to maintain alignment with features from previous tasks, thereby delaying the data drift that may occur between tasks, while performing unsupervised cross-domain (UDA) between related domains. By leveraging an intra-task-specific pseudo-labe",
    "link": "https://arxiv.org/abs/2402.12490",
    "context": "Title: Towards Cross-Domain Continual Learning\nAbstract: arXiv:2402.12490v1 Announce Type: cross  Abstract: Continual learning is a process that involves training learning agents to sequentially master a stream of tasks or classes without revisiting past data. The challenge lies in leveraging previously acquired knowledge to learn new tasks efficiently, while avoiding catastrophic forgetting. Existing methods primarily focus on single domains, restricting their applicability to specific problems.   In this work, we introduce a novel approach called Cross-Domain Continual Learning (CDCL) that addresses the limitations of being limited to single supervised domains. Our method combines inter- and intra-task cross-attention mechanisms within a compact convolutional network. This integration enables the model to maintain alignment with features from previous tasks, thereby delaying the data drift that may occur between tasks, while performing unsupervised cross-domain (UDA) between related domains. By leveraging an intra-task-specific pseudo-labe",
    "path": "papers/24/02/2402.12490.json",
    "total_tokens": 874,
    "translated_title": "跨领域持续学习的研究",
    "translated_abstract": "持续学习是一个过程，涉及训练学习代理以顺序地掌握一系列任务或类别，而不需要重新回顾过去的数据。挑战在于利用先前获得的知识有效地学习新任务，同时避免灾难性遗忘。现有方法主要集中在单一领域，限制了它们在特定问题上的适用性。在这项工作中，我们介绍了一种名为跨领域持续学习（CDCL）的新方法，它解决了被限制在单一监督领域的局限性。我们的方法在紧凑的卷积网络中结合了任务间和任务内的交叉注意机制。这种整合使得模型能够与先前任务的特征保持对齐，从而延迟可能发生在任务之间的数据漂移，同时在相关领域之间进行无监督的跨领域学习（UDA）。通过利用任务内具体的伪标签",
    "tldr": "介绍了一种名为跨领域持续学习（CDCL）的新方法，通过整合任务间和任务内的交叉注意机制，在紧凑的卷积网络中延迟数据漂移，实现了无监督的跨领域学习（UDA）。",
    "en_tdlr": "Introduced a novel approach called Cross-Domain Continual Learning (CDCL), which delays data drift by integrating inter- and intra-task cross-attention mechanisms within a compact convolutional network, enabling unsupervised cross-domain learning (UDA)."
}