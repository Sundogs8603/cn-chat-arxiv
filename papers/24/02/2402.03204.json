{
    "title": "Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems",
    "abstract": "We develop a multi-agent reinforcement learning (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference. A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy. To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed. Simulation results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies. Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor",
    "link": "https://arxiv.org/abs/2402.03204",
    "context": "Title: Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell Massive MIMO Systems\nAbstract: We develop a multi-agent reinforcement learning (MARL) algorithm to minimize the total energy consumption of multiple massive MIMO (multiple-input multiple-output) base stations (BSs) in a multi-cell network while preserving the overall quality-of-service (QoS) by making decisions on the multi-level advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is modeled as a decentralized partially observable Markov decision process (DEC-POMDP) to enable collaboration between individual BSs, which is necessary to tackle inter-cell interference. A multi-agent proximal policy optimization (MAPPO) algorithm is designed to learn a collaborative BS control policy. To enhance its scalability, a modified version called MAPPO-neighbor policy is further proposed. Simulation results demonstrate that the trained MAPPO agent achieves better performance compared to baseline policies. Specifically, compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the MAPPO-neighbor",
    "path": "papers/24/02/2402.03204.json",
    "total_tokens": 920,
    "translated_title": "多智能体强化学习在多小区大规模MIMO系统中的能量节省",
    "translated_abstract": "我们开发了一种多智能体强化学习算法，通过对多级高级睡眠模式（ASM）和基站的天线切换进行决策，来最小化多个大规模MIMO（多输入多输出）基站在多小区网络中的总能量消耗，同时保持整体的服务质量（QoS）。将问题建模为分布式部分可观察的马尔可夫决策过程（DEC-POMDP），以实现个体基站之间的协作，这对于处理小区间干扰是必要的。设计了一种多智能体近端策略优化（MAPPO）算法来学习协同基站控制策略。为了提高其可伸缩性，进一步提出了一种称为MAPPO-neighbor策略的修改版本。仿真结果表明，训练的MAPPO智能体相比基线策略取得了更好的性能。特别是，与自动睡眠模式1（符号级休眠）算法相比，MAPPO-neighbor算法实现了更好的性能。",
    "tldr": "该论文提出了一种多智能体强化学习算法，通过优化多个大规模MIMO基站的睡眠模式和天线切换，实现在多小区网络中能量的节省，同时保持服务质量。仿真结果表明，这种算法相比基线策略具有更好的性能。",
    "en_tdlr": "This paper proposes a multi-agent reinforcement learning algorithm to save energy in multi-cell massive MIMO systems by optimizing sleep modes and antenna switching. The algorithm achieves better performance compared to baseline policies, according to simulation results."
}