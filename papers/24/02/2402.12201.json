{
    "title": "Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT",
    "abstract": "arXiv:2402.12201v1 Announce Type: new  Abstract: Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a",
    "link": "https://arxiv.org/abs/2402.12201",
    "context": "Title: Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT\nAbstract: arXiv:2402.12201v1 Announce Type: new  Abstract: Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a",
    "path": "papers/24/02/2402.12201.json",
    "total_tokens": 875,
    "translated_title": "字典学习改进了机械解释中的无补丁电路发现：以奥赛罗-GPT为案例研究",
    "translated_abstract": "稀疏字典学习是一种在机械解释中快速发展的技术，用于攻击叠加并从模型激活中提取更易理解的特征。本文基于提取的更单义特征进一步提出一个问题：我们如何识别连接大量字典特征的电路？我们提出了一个电路发现框架，替代了激活补丁。我们的框架在越界分布方面遭受较小，并在渐近复杂度方面证明更有效。我们框架中的基本单元是从所有模块中写入残余流的字典特征，包括嵌入、注意力输出和MLP输出。从任何对数、字典特征或注意力分数开始，我们成功追踪到所有令牌的较低级别字典特征，并计算它们对这些更具可解释性和局部模型行为的贡献。",
    "tldr": "字典学习技术在机械解释中攻克叠加，并提取更易理解的特征。该论文提出了一种电路发现框架，用于连接大量字典特征，相比于激活补丁，该框架受越界分布影响较小，并在渐近复杂度方面更有效。"
}