{
    "title": "How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study",
    "abstract": "arXiv:2402.16061v1 Announce Type: new  Abstract: Previous work has showcased the intriguing capability of large language models (LLMs) in retrieving facts and processing context knowledge. However, only limited research exists on the layer-wise capability of LLMs to encode knowledge, which challenges our understanding of their internal mechanisms. In this paper, we devote the first attempt to investigate the layer-wise capability of LLMs through probing tasks. We leverage the powerful generative capability of ChatGPT to construct probing datasets, providing diverse and coherent evidence corresponding to various facts. We employ $\\mathcal V$-usable information as the validation metric to better reflect the capability in encoding context knowledge across different layers. Our experiments on conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens ",
    "link": "https://arxiv.org/abs/2402.16061",
    "context": "Title: How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study\nAbstract: arXiv:2402.16061v1 Announce Type: new  Abstract: Previous work has showcased the intriguing capability of large language models (LLMs) in retrieving facts and processing context knowledge. However, only limited research exists on the layer-wise capability of LLMs to encode knowledge, which challenges our understanding of their internal mechanisms. In this paper, we devote the first attempt to investigate the layer-wise capability of LLMs through probing tasks. We leverage the powerful generative capability of ChatGPT to construct probing datasets, providing diverse and coherent evidence corresponding to various facts. We employ $\\mathcal V$-usable information as the validation metric to better reflect the capability in encoding context knowledge across different layers. Our experiments on conflicting and newly acquired knowledge show that LLMs: (1) prefer to encode more context knowledge in the upper layers; (2) primarily encode context knowledge within knowledge-related entity tokens ",
    "path": "papers/24/02/2402.16061.json",
    "total_tokens": 859,
    "translated_title": "大型语言模型如何编码上下文知识？一项逐层探究研究",
    "translated_abstract": "先前的研究展示了大型语言模型（LLMs）在检索事实和处理上下文知识方面的引人注目能力。然而，关于LLMs逐层编码知识的能力的研究有限，这挑战了我们对它们内部机制的理解。在本文中，我们致力于通过探究任务来首次研究LLMs逐层的能力。我们利用ChatGPT强大的生成能力构建探究数据集，提供与各种事实相对应的多样且连贯的证据。我们采用$\\mathcal V$-usable信息作为验证指标，以更好地反映跨不同层编码上下文知识的能力。我们在有冲突和新获得知识方面的实验表明，LLMs：（1）更倾向于在上层编码更多的上下文知识；（2）主要在与知识相关的实体标记内编码上下文知识",
    "tldr": "本文首次通过探究任务研究了大型语言模型逐层编码知识的能力，实验结果显示LLMs更倾向于在上层编码更多的上下文知识。",
    "en_tdlr": "This paper investigates the layer-wise capability of Large Language Models in encoding knowledge through probing tasks for the first time, showing that LLMs tend to encode more context knowledge in the upper layers."
}