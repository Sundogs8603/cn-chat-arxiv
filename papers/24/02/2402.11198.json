{
    "title": "Achieving Linear Speedup in Asynchronous Federated Learning with Heterogeneous Clients",
    "abstract": "arXiv:2402.11198v1 Announce Type: new  Abstract: Federated learning (FL) is an emerging distributed training paradigm that aims to learn a common global model without exchanging or transferring the data that are stored locally at different clients. The Federated Averaging (FedAvg)-based algorithms have gained substantial popularity in FL to reduce the communication overhead, where each client conducts multiple localized iterations before communicating with a central server. In this paper, we focus on FL where the clients have diverse computation and/or communication capabilities. Under this circumstance, FedAvg can be less efficient since it requires all clients that participate in the global aggregation in a round to initiate iterations from the latest global model, and thus the synchronization among fast clients and straggler clients can severely slow down the overall training process. To address this issue, we propose an efficient asynchronous federated learning (AFL) framework call",
    "link": "https://arxiv.org/abs/2402.11198",
    "context": "Title: Achieving Linear Speedup in Asynchronous Federated Learning with Heterogeneous Clients\nAbstract: arXiv:2402.11198v1 Announce Type: new  Abstract: Federated learning (FL) is an emerging distributed training paradigm that aims to learn a common global model without exchanging or transferring the data that are stored locally at different clients. The Federated Averaging (FedAvg)-based algorithms have gained substantial popularity in FL to reduce the communication overhead, where each client conducts multiple localized iterations before communicating with a central server. In this paper, we focus on FL where the clients have diverse computation and/or communication capabilities. Under this circumstance, FedAvg can be less efficient since it requires all clients that participate in the global aggregation in a round to initiate iterations from the latest global model, and thus the synchronization among fast clients and straggler clients can severely slow down the overall training process. To address this issue, we propose an efficient asynchronous federated learning (AFL) framework call",
    "path": "papers/24/02/2402.11198.json",
    "total_tokens": 822,
    "translated_title": "在异步联邦学习中实现具有异构客户端的线性加速度",
    "translated_abstract": "联邦学习(FL)是一种新兴的分布式训练范式，旨在学习一个通用的全局模型，而无需交换或传输存储在不同客户端本地的数据。基于Federated Averaging (FedAvg)的算法在FL中备受欢迎，以减少通信开销，其中每个客户端在与中央服务器通信之前进行多个本地化迭代。本文重点研究了具有不同计算和/或通信能力的客户端的FL。在这种情况下，FedAvg可能不那么高效，因为它要求参与全局聚合的所有客户端在一个回合中从最新的全局模型开始迭代，因此快速客户端和滞后客户端之间的同步会严重拖慢整个训练过程。为了解决这个问题，我们提出了一种称为高效异步联邦学习(AFL)框架",
    "tldr": "提出了一种异步联邦学习(AFL)框架，旨在解决具有异构客户端的联邦学习中同步问题，实现线性加速。",
    "en_tdlr": "Proposed an asynchronous federated learning (AFL) framework to address synchronization issues in federated learning with heterogeneous clients, achieving linear speedup."
}