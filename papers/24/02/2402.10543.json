{
    "title": "Strong hallucinations from negation and how to fix them",
    "abstract": "arXiv:2402.10543v1 Announce Type: cross  Abstract: Despite great performance on many tasks, language models (LMs) still struggle with reasoning, sometimes providing responses that cannot possibly be true because they stem from logical incoherence. We call such responses \\textit{strong hallucinations} and prove that they follow from an LM's computation of its internal representations for logical operators and outputs from those representations. Focusing on negation, we provide a novel solution in which negation is treated not as another element of a latent representation, but as \\textit{an operation over an LM's latent representations that constrains how they may evolve}. We show that our approach improves model performance in cloze prompting and natural language inference tasks with negation without requiring training on sparse negative data.",
    "link": "https://arxiv.org/abs/2402.10543",
    "context": "Title: Strong hallucinations from negation and how to fix them\nAbstract: arXiv:2402.10543v1 Announce Type: cross  Abstract: Despite great performance on many tasks, language models (LMs) still struggle with reasoning, sometimes providing responses that cannot possibly be true because they stem from logical incoherence. We call such responses \\textit{strong hallucinations} and prove that they follow from an LM's computation of its internal representations for logical operators and outputs from those representations. Focusing on negation, we provide a novel solution in which negation is treated not as another element of a latent representation, but as \\textit{an operation over an LM's latent representations that constrains how they may evolve}. We show that our approach improves model performance in cloze prompting and natural language inference tasks with negation without requiring training on sparse negative data.",
    "path": "papers/24/02/2402.10543.json",
    "total_tokens": 764,
    "translated_title": "消除否定导致的强幻觉",
    "translated_abstract": "尽管语言模型（LMs）在许多任务上表现出色，但仍然在推理方面存在困难，有时会提供由于逻辑不连贯而不可能成立的响应。我们称这种响应为\\textit{强幻觉}，并证明它们源于LM计算其内部表示的逻辑运算符和从这些表示中产生的输出。重点关注否定，我们提供了一种新颖的解决方案，其中否定不是作为潜在表示的另一个元素，而是作为\\textit{LM潜在表示上的一个操作，约束它们可能的演变方式}。我们展示了我们的方法改善了在带否定的填空提示和自然语言推理任务中的模型性能，而无需对稀疏负数据进行训练。",
    "tldr": "论文针对语言模型在推理中造成的强幻觉问题，提出了一种处理否定的新方法，可以改善模型性能而无需使用稀疏负数据训练。",
    "en_tdlr": "The paper addresses the issue of strong hallucinations in language models during reasoning, proposing a novel approach to handling negation to improve model performance without the need for training on sparse negative data."
}