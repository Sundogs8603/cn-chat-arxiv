{
    "title": "Breaking MLPerf Training: A Case Study on Optimizing BERT",
    "abstract": "Speeding up the large-scale distributed training is challenging in that it requires improving various components of training including load balancing, communication, optimizers, etc. We present novel approaches for fast large-scale training of BERT model which individually ameliorates each component thereby leading to a new level of BERT training performance. Load balancing is imperative in distributed BERT training since its training datasets are characterized by samples with various lengths. Communication cost, which is proportional to the scale of distributed training, needs to be hidden by useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc., need to be carefully re-evaluated in the context of large-scale distributed training. We propose two new ideas, (1) local presorting based on dataset stratification for load balancing and (2) bucket-wise gradient clipping before allreduce which allows us to benefit from the overlap of gradient computation and synchronization",
    "link": "https://arxiv.org/abs/2402.02447",
    "context": "Title: Breaking MLPerf Training: A Case Study on Optimizing BERT\nAbstract: Speeding up the large-scale distributed training is challenging in that it requires improving various components of training including load balancing, communication, optimizers, etc. We present novel approaches for fast large-scale training of BERT model which individually ameliorates each component thereby leading to a new level of BERT training performance. Load balancing is imperative in distributed BERT training since its training datasets are characterized by samples with various lengths. Communication cost, which is proportional to the scale of distributed training, needs to be hidden by useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc., need to be carefully re-evaluated in the context of large-scale distributed training. We propose two new ideas, (1) local presorting based on dataset stratification for load balancing and (2) bucket-wise gradient clipping before allreduce which allows us to benefit from the overlap of gradient computation and synchronization",
    "path": "papers/24/02/2402.02447.json",
    "total_tokens": 851,
    "translated_title": "打破MLPerf训练：优化BERT的案例研究",
    "translated_abstract": "加速大规模分布式训练具有挑战性，需要改进包括负载平衡、通信、优化器等训练的各个组件。我们提出了用于快速大规模训练BERT模型的新方法，通过改进每个组件，从而实现了BERT训练性能的新水平。在分布式BERT训练中，负载平衡至关重要，因为其训练数据集根据不同长度的样本进行了特征化。与分布式训练的规模成正比的通信成本需要通过有用的计算来隐藏。此外，优化器，如ADAM、LAMB等，需要在大规模分布式训练的背景下进行仔细重新评估。我们提出了两个新想法，即基于数据集分层的本地预排序进行负载平衡和全约减之前的按桶梯度裁剪，从而使我们能够从梯度计算和同步的重叠中获益。",
    "tldr": "通过改进负载平衡、通信和优化器等各个组件，我们提出了用于快速大规模训练BERT模型的新方法，实现了新水平的BERT训练性能。",
    "en_tdlr": "We propose new methods for fast large-scale training of the BERT model by improving various components such as load balancing, communication, and optimizers, achieving a new level of BERT training performance."
}