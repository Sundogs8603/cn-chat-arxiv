{
    "title": "In-Context Data Distillation with TabPFN",
    "abstract": "Foundation models have revolutionized tasks in computer vision and natural language processing. However, in the realm of tabular data, tree-based models like XGBoost continue to dominate. TabPFN, a transformer model tailored for tabular data, mirrors recent foundation models in its exceptional in-context learning capability, being competitive with XGBoost's performance without the need for task-specific training or hyperparameter tuning. Despite its promise, TabPFN's applicability is hindered by its data size constraint, limiting its use in real-world scenarios. To address this, we present in-context data distillation (ICD), a novel methodology that effectively eliminates these constraints by optimizing TabPFN's context. ICD efficiently enables TabPFN to handle significantly larger datasets with a fixed memory budget, improving TabPFN's quadratic memory complexity but at the cost of a linear number of tuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong performanc",
    "link": "https://arxiv.org/abs/2402.06971",
    "context": "Title: In-Context Data Distillation with TabPFN\nAbstract: Foundation models have revolutionized tasks in computer vision and natural language processing. However, in the realm of tabular data, tree-based models like XGBoost continue to dominate. TabPFN, a transformer model tailored for tabular data, mirrors recent foundation models in its exceptional in-context learning capability, being competitive with XGBoost's performance without the need for task-specific training or hyperparameter tuning. Despite its promise, TabPFN's applicability is hindered by its data size constraint, limiting its use in real-world scenarios. To address this, we present in-context data distillation (ICD), a novel methodology that effectively eliminates these constraints by optimizing TabPFN's context. ICD efficiently enables TabPFN to handle significantly larger datasets with a fixed memory budget, improving TabPFN's quadratic memory complexity but at the cost of a linear number of tuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong performanc",
    "path": "papers/24/02/2402.06971.json",
    "total_tokens": 873,
    "translated_title": "使用TabPFN进行上下文数据蒸馏",
    "translated_abstract": "基于tabular数据，基于树的模型（如XGBoost）在该领域仍然占据主导地位。TabPFN是一种专门为tabular数据设计的Transformer模型，其卓越的上下文学习能力与XGBoost相媲美，无需特定任务训练或超参数调整。然而，TabPFN的适用性受到数据规模限制的影响，在实际场景中使用受到限制。为了解决这个问题，我们提出了一种新颖的方法，即上下文数据蒸馏(ICD)，有效地消除了这些限制，通过优化TabPFN的上下文。ICD使得TabPFN能够在有限内存预算下处理更大规模的数据集，提高TabPFN的二次内存复杂度，但代价是线性数量的调优步骤。值得注意的是，经过ICD增强的TabPFN展现出非常强大的性能。",
    "tldr": "TabPFN是一种适用于tabular数据的Transformer模型，具有卓越的上下文学习能力。为了解决TabPFN在实际场景中的数据规模限制问题，我们提出了上下文数据蒸馏(ICD)方法，通过优化TabPFN的上下文，使其能够处理更大规模的数据集，提高性能。"
}