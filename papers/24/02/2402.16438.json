{
    "title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models",
    "abstract": "arXiv:2402.16438v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora. It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts. In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs. Based on LAPE, we conduct comprehensive experiments on two representative LLMs, namely LLaMA-2 and BLOOM. Our findings indicate that LLMs' proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models' top and bottom layers. Furthermore, we showcase the feasibility to \"steer\" the output language of LLMs by selectively activating or deactivatin",
    "link": "https://arxiv.org/abs/2402.16438",
    "context": "Title: Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models\nAbstract: arXiv:2402.16438v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora. It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts. In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions. Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs. Based on LAPE, we conduct comprehensive experiments on two representative LLMs, namely LLaMA-2 and BLOOM. Our findings indicate that LLMs' proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models' top and bottom layers. Furthermore, we showcase the feasibility to \"steer\" the output language of LLMs by selectively activating or deactivatin",
    "path": "papers/24/02/2402.16438.json",
    "total_tokens": 909,
    "translated_title": "语言特定神经元：大型语言模型多语能力的关键",
    "translated_abstract": "大型语言模型(LLMs)展现出显著的多语能力，即使未经过专门策划的多语平行语料库的预训练。解释LLMs处理多语文本的基本机制仍然是一个具有挑战性的问题。在本文中，我们深入研究了LLMs中Transformer架构的构成，以找出语言特定区域。具体而言，我们提出了一种新颖的检测方法，即语言激活概率熵（LAPE），用于识别LLMs内的语言特定神经元。基于LAPE，我们对两个代表性的LLMs，即LLaMA-2和BLOOM进行了全面实验。我们的研究结果表明，LLMs处理特定语言的能力主要是由一小部分神经元决定的，这些神经元主要位于模型的顶部和底部层。此外，我们展示了通过选择性激活或停用神经元来“引导”LLMs的输出语言的可行性。",
    "tldr": "大型语言模型中的语言特定神经元可以解释其多语能力，通过提出语言激活概率熵（LAPE）的检测方法，研究发现LLMs处理特定语言的能力主要由少量神经元决定。",
    "en_tdlr": "Language-specific neurons in large language models can explain their multilingual capabilities, and through proposing the detection method of Language Activation Probability Entropy (LAPE), the research found that the ability of LLMs to process specific languages is mainly determined by a small number of neurons."
}