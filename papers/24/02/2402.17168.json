{
    "title": "Benchmarking Data Science Agents",
    "abstract": "arXiv:2402.17168v1 Announce Type: new  Abstract: In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.",
    "link": "https://arxiv.org/abs/2402.17168",
    "context": "Title: Benchmarking Data Science Agents\nAbstract: arXiv:2402.17168v1 Announce Type: new  Abstract: In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.",
    "path": "papers/24/02/2402.17168.json",
    "total_tokens": 905,
    "translated_title": "数据科学代理基准测试",
    "translated_abstract": "在数据驱动决策的时代，数据分析的复杂性需要数据科学的高级专业知识和工具，这对专家来说也带来了重大挑战。大型语言模型(LLM)作为数据科学代理，已经成为协助人类进行数据分析和处理的有希望的辅助工具。然而，它们的实际有效性仍受限于现实应用的多样需求和复杂的分析过程。在本文中，我们介绍了DSEval--一种新颖的评估范式，以及一系列针对整个数据科学生命周期的代理性能评估的创新基准。通过引入一种新颖的自举注释方法，我们简化了数据集准备流程，改进了评估覆盖范围，并扩展了基准测试的全面性。我们的研究发现揭示了普遍存在的障碍，并提供了关键见解，以指导未来在这一领域的进展。",
    "tldr": "本文引入了DSEval评估范式和一系列创新基准，用于评估数据科学代理在整个数据科学生命周期中的性能，通过引入自举注释方法简化数据集准备流程，改进评估覆盖范围，扩展基准测试的全面性，揭示了普遍存在的障碍并提供了关键见解"
}