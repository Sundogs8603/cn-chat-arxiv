{
    "title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More",
    "abstract": "arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa",
    "link": "https://arxiv.org/abs/2402.12065",
    "context": "Title: WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More\nAbstract: arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa",
    "path": "papers/24/02/2402.12065.json",
    "total_tokens": 832,
    "translated_title": "WKVQuant：量化大型语言模型的参数权重和键值缓存以提高性能",
    "translated_abstract": "大型语言模型（LLMs）面临着部署挑战，主要是由于其巨大的内存需求和自回归文本生成过程的计算需求。本文通过关注LLMs的量化来解决这些挑战，量化是一种通过将模型参数和激活转换为低比特整数来减少内存消耗的技术。我们批判性地分析了现有的量化方法，识别出它们在平衡量化LLMs的准确性和效率方面的局限性。为了超越这些局限性，我们提出了WKVQuant，这是一个专为量化LLMs的参数权重和键值（KV）缓存而设计的PTQ框架。具体而言，我们引入了仅考虑过去的量化以改善注意力计算。此外，我们还介绍了二维量化策略来处理KV缓存的分布，以及一种跨块重建正则化方法以帮助模型压缩。",
    "tldr": "该论文提出了WKVQuant，一种专为大型语言模型设计的量化框架，通过量化权重和键值缓存来改善性能。",
    "en_tdlr": "This paper introduces WKVQuant, a quantization framework designed specifically for large language models to improve performance by quantizing weights and key/value cache."
}