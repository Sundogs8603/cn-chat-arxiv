{
    "title": "ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars for Write Noise Mitigation",
    "abstract": "Transformers have revolutionized various real-world applications from natural language processing to computer vision. However, traditional von-Neumann computing paradigm faces memory and bandwidth limitations in accelerating transformers owing to their massive model sizes. To this end, In-memory Computing (IMC) crossbars based on Non-volatile Memories (NVMs), due to their ability to perform highly parallelized Matrix-Vector-Multiplications (MVMs) with high energy-efficiencies, have emerged as a promising solution for accelerating transformers. However, analog MVM operations in crossbars introduce non-idealities, such as stochastic read & write noise, which affect the inference accuracy of the deployed transformers. Specifically, we find pre-trained Vision Transformers (ViTs) to be vulnerable on crossbars due to the impact of write noise on the dynamically-generated Key (K) and Value (V) matrices in the attention layers, an effect not accounted for in prior studies. We, thus, propose Cl",
    "link": "https://arxiv.org/abs/2402.02586",
    "context": "Title: ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars for Write Noise Mitigation\nAbstract: Transformers have revolutionized various real-world applications from natural language processing to computer vision. However, traditional von-Neumann computing paradigm faces memory and bandwidth limitations in accelerating transformers owing to their massive model sizes. To this end, In-memory Computing (IMC) crossbars based on Non-volatile Memories (NVMs), due to their ability to perform highly parallelized Matrix-Vector-Multiplications (MVMs) with high energy-efficiencies, have emerged as a promising solution for accelerating transformers. However, analog MVM operations in crossbars introduce non-idealities, such as stochastic read & write noise, which affect the inference accuracy of the deployed transformers. Specifically, we find pre-trained Vision Transformers (ViTs) to be vulnerable on crossbars due to the impact of write noise on the dynamically-generated Key (K) and Value (V) matrices in the attention layers, an effect not accounted for in prior studies. We, thus, propose Cl",
    "path": "papers/24/02/2402.02586.json",
    "total_tokens": 928,
    "translated_title": "ClipFormer:用于减轻存储器电阻交叉点上变压器写入噪声的键-值剪辑",
    "translated_abstract": "变压器已经在从自然语言处理到计算机视觉的各种现实应用中带来了革命。然而，由于其庞大的模型大小，传统的冯·诺依曼计算范例在加速变压器时面临着内存和带宽限制。因此，基于非易失性存储器（NVM）的内存计算（IMC）交叉栏，由于其能够以高能效进行高度并行的矩阵-向量乘法（MVM），已成为加速变压器的一种有前景的解决方案。然而，交叉栏中的模拟MVM操作引入了非理想性，如随机读写噪声，这些噪声会影响部署的变压器的推理准确性。具体来说，我们发现预训练的视觉变压器（ViTs）由于写噪声对注意力层中的动态生成的键（K）和值（V）矩阵的影响而容易受到交叉栏的影响，在先前的研究中没有考虑到这种影响。因此，我们提出创新的ClipFormer算法来解决这个问题。",
    "tldr": "ClipFormer算法用于减轻变压器在交叉栏上的写入噪声对注意力层中键值矩阵的影响，提高推理准确性。",
    "en_tdlr": "ClipFormer algorithm is proposed to mitigate the impact of write noise on the key-value matrices in the attention layers of transformers on crossbars, improving inference accuracy."
}