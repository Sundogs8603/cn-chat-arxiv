{
    "title": "Share What You Already Know: Cross-Language-Script Transfer and Alignment for Sentiment Detection in Code-Mixed Data",
    "abstract": "Code-switching entails mixing multiple languages. It is an increasingly occurring phenomenon in social media texts. Usually, code-mixed texts are written in a single script, even though the languages involved have different scripts. Pre-trained multilingual models primarily utilize the data in the native script of the language. In existing studies, the code-switched texts are utilized as they are. However, using the native script for each language can generate better representations of the text owing to the pre-trained knowledge. Therefore, a cross-language-script knowledge sharing architecture utilizing the cross attention and alignment of the representations of text in individual language scripts was proposed in this study. Experimental results on two different datasets containing Nepali-English and Hindi-English code-switched texts, demonstrate the effectiveness of the proposed method. The interpretation of the model using model explainability technique illustrates the sharing of la",
    "link": "https://arxiv.org/abs/2402.04542",
    "context": "Title: Share What You Already Know: Cross-Language-Script Transfer and Alignment for Sentiment Detection in Code-Mixed Data\nAbstract: Code-switching entails mixing multiple languages. It is an increasingly occurring phenomenon in social media texts. Usually, code-mixed texts are written in a single script, even though the languages involved have different scripts. Pre-trained multilingual models primarily utilize the data in the native script of the language. In existing studies, the code-switched texts are utilized as they are. However, using the native script for each language can generate better representations of the text owing to the pre-trained knowledge. Therefore, a cross-language-script knowledge sharing architecture utilizing the cross attention and alignment of the representations of text in individual language scripts was proposed in this study. Experimental results on two different datasets containing Nepali-English and Hindi-English code-switched texts, demonstrate the effectiveness of the proposed method. The interpretation of the model using model explainability technique illustrates the sharing of la",
    "path": "papers/24/02/2402.04542.json",
    "total_tokens": 865,
    "translated_title": "分享您已经知道的东西：用于混合语言数据的情感检测的跨语言-脚本转移和对齐",
    "translated_abstract": "代码混合涉及多种语言的混合。这是社交媒体文本中日益发生的现象。通常，代码混合文本是用单一脚本编写的，即使涉及的语言有不同的脚本。预训练的多语言模型主要利用语言的本机脚本中的数据。在现有研究中，代码混合的文本被直接使用。然而，使用每种语言的本机脚本可以由于预训练的知识而生成更好的文本表示。因此，本研究提出了一种跨语言-脚本知识共享架构，利用各种语言脚本中文本表示的跨关注和对齐。在包含尼泊尔语-英语和印地语-英语代码混合文本的两个不同数据集上的实验结果表明了该方法的有效性。模型解释技术对模型的解释显示了知识共享的过程。",
    "tldr": "本研究提出了一种跨语言-脚本知识共享架构，利用各种语言脚本中文本表示的跨关注和对齐，以提高混合语言数据中的情感检测效果。",
    "en_tdlr": "This study proposes a cross-language-script knowledge sharing architecture that utilizes cross attention and alignment of text representations in different language scripts to improve sentiment detection in code-mixed data."
}