{
    "title": "Smoothness Adaptive Hypothesis Transfer Learning",
    "abstract": "arXiv:2402.14966v1 Announce Type: cross  Abstract: Many existing two-phase kernel-based hypothesis transfer learning algorithms employ the same kernel regularization across phases and rely on the known smoothness of functions to obtain optimality. Therefore, they fail to adapt to the varying and unknown smoothness between the target/source and their offset in practice. In this paper, we address these problems by proposing Smoothness Adaptive Transfer Learning (SATL), a two-phase kernel ridge regression(KRR)-based algorithm. We first prove that employing the misspecified fixed bandwidth Gaussian kernel in target-only KRR learning can achieve minimax optimality and derive an adaptive procedure to the unknown Sobolev smoothness. Leveraging these results, SATL employs Gaussian kernels in both phases so that the estimators can adapt to the unknown smoothness of the target/source and their offset function. We derive the minimax lower bound of the learning problem in excess risk and show that",
    "link": "https://arxiv.org/abs/2402.14966",
    "context": "Title: Smoothness Adaptive Hypothesis Transfer Learning\nAbstract: arXiv:2402.14966v1 Announce Type: cross  Abstract: Many existing two-phase kernel-based hypothesis transfer learning algorithms employ the same kernel regularization across phases and rely on the known smoothness of functions to obtain optimality. Therefore, they fail to adapt to the varying and unknown smoothness between the target/source and their offset in practice. In this paper, we address these problems by proposing Smoothness Adaptive Transfer Learning (SATL), a two-phase kernel ridge regression(KRR)-based algorithm. We first prove that employing the misspecified fixed bandwidth Gaussian kernel in target-only KRR learning can achieve minimax optimality and derive an adaptive procedure to the unknown Sobolev smoothness. Leveraging these results, SATL employs Gaussian kernels in both phases so that the estimators can adapt to the unknown smoothness of the target/source and their offset function. We derive the minimax lower bound of the learning problem in excess risk and show that",
    "path": "papers/24/02/2402.14966.json",
    "total_tokens": 874,
    "translated_title": "光滑自适应假设迁移学习",
    "translated_abstract": "许多现有的基于核的两阶段假设迁移学习算法在不同阶段均采用相同的核正则化，并依赖于函数的已知光滑性来实现最优性。因此，在实践中，它们未能适应目标/源及其偏移之间的变化和未知光滑性。本文通过提出光滑自适应迁移学习（SATL），一个基于两阶段核岭回归（KRR）的算法，解决了这些问题。我们首先证明，在目标专用KRR学习中采用错误指定的固定带宽高斯核可以实现极小化最优性，并推导出一种适应未知Sobolev光滑性的自适应过程。利用这些结果，SATL在两阶段均采用高斯核，以使估计量能够适应目标/源及其偏移函数的未知光滑性。我们推导了学习问题在过量风险中的极小值下限，并表明",
    "tldr": "本文提出了光滑自适应迁移学习（SATL）算法，通过在两个阶段均采用高斯核，使估计器能够适应目标/源及其偏移函数的未知光滑性。",
    "en_tdlr": "This paper introduces Smoothness Adaptive Transfer Learning (SATL) algorithm, which adapts estimators to the unknown smoothness of the target/source and their offset function by using Gaussian kernels in both phases."
}