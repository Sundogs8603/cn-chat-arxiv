{
    "title": "Copyright Traps for Large Language Models",
    "abstract": "arXiv:2402.09363v1 Announce Type: new Abstract: Questions of fair use of copyright-protected content to train Large Language Models (LLMs) are being very actively debated. Document-level inference has been proposed as a new task: inferring from black-box access to the trained model whether a piece of content has been seen during training. SOTA methods however rely on naturally occurring memorization of (part of) the content. While very effective against models that memorize a lot, we hypothesize--and later confirm--that they will not work against models that do not naturally memorize, e.g. medium-size 1B models. We here propose to use copyright traps, the inclusion of fictitious entries in original content, to detect the use of copyrighted materials in LLMs with a focus on models where memorization does not naturally occur. We carefully design an experimental setup, randomly inserting traps into original content (books) and train a 1.3B LLM. We first validate that the use of content in",
    "link": "https://arxiv.org/abs/2402.09363",
    "context": "Title: Copyright Traps for Large Language Models\nAbstract: arXiv:2402.09363v1 Announce Type: new Abstract: Questions of fair use of copyright-protected content to train Large Language Models (LLMs) are being very actively debated. Document-level inference has been proposed as a new task: inferring from black-box access to the trained model whether a piece of content has been seen during training. SOTA methods however rely on naturally occurring memorization of (part of) the content. While very effective against models that memorize a lot, we hypothesize--and later confirm--that they will not work against models that do not naturally memorize, e.g. medium-size 1B models. We here propose to use copyright traps, the inclusion of fictitious entries in original content, to detect the use of copyrighted materials in LLMs with a focus on models where memorization does not naturally occur. We carefully design an experimental setup, randomly inserting traps into original content (books) and train a 1.3B LLM. We first validate that the use of content in",
    "path": "papers/24/02/2402.09363.json",
    "total_tokens": 859,
    "translated_title": "大型语言模型的版权陷阱",
    "translated_abstract": "论文讨论了在训练大型语言模型（LLM）中对受版权保护的内容的合理使用的问题。文章提出了一种新的任务，即在训练模型时通过对模型的黑盒访问来推断一段内容是否在训练中出现过。目前的最优方法依赖于（部分）内容的自然记忆，对于大量记忆的模型非常有效，但我们假设并后来证实，这些方法对于不自然记忆，例如中型 1B 模型将不起作用。为此，我们提出使用版权陷阱来识别LLM中的版权材料使用，重点放在不自然记忆的模型上。我们精心设计了一个实验设置，将陷阱随机插入原始内容（书籍）中，并训练了一个1.3B的LLM模型。我们首先验证了在LLM中使用内容是否会导致陷阱的出现。",
    "tldr": "本论文研究了在大型语言模型（LLM）的训练中对版权保护内容的合理使用问题。提出了使用版权陷阱来识别不自然记忆的模型中的版权材料使用。",
    "en_tdlr": "This paper discusses the fair use of copyright-protected content in training large language models (LLMs). It proposes the use of copyright traps to detect the use of copyrighted materials in LLMs without natural memorization."
}