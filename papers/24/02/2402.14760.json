{
    "title": "Generalizing Reward Modeling for Out-of-Distribution Preference Learning",
    "abstract": "arXiv:2402.14760v1 Announce Type: cross  Abstract: Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model",
    "link": "https://arxiv.org/abs/2402.14760",
    "context": "Title: Generalizing Reward Modeling for Out-of-Distribution Preference Learning\nAbstract: arXiv:2402.14760v1 Announce Type: cross  Abstract: Preference learning (PL) with large language models (LLMs) aims to align the LLMs' generations with human preferences. Previous work on reinforcement learning from human feedback (RLHF) has demonstrated promising results in in-distribution PL. However, due to the difficulty of obtaining human feedback, discretely training reward models for every encountered distribution is challenging. Thus, out-of-distribution (OOD) PL is practically useful for enhancing the generalization ability of LLMs with limited preference feedback. This work addresses OOD PL by optimizing a general reward model through a meta-learning approach. During meta-training, a bilevel optimization algorithm is utilized to learn a reward model capable of guiding policy learning to align with human preferences across various distributions. When encountering a test distribution, the meta-test procedure conducts regularized policy optimization using the learned reward model",
    "path": "papers/24/02/2402.14760.json",
    "total_tokens": 833,
    "translated_title": "泛化奖励建模用于超出分布偏好学习",
    "translated_abstract": "偏好学习(PL)结合大型语言模型(LLMs)旨在使LLMs生成与人类偏好一致。以往有关从人类反馈中学习的强化学习(RLHF)的研究已在分布内的PL中取得了良好结果。然而，由于获取人类反馈的难度，为每个遇到的分布离散训练奖励模型是具有挑战性的。因此，在超出分布(OOD) PL中通过优化通用奖励模型来增强LLMs有限偏好反馈的泛化能力是实用的。本研究通过元学习方法来解决OOD PL问题。在元训练期间，利用双层优化算法来学习一个能够引导策略学习以使之与人类偏好一致的奖励模型。在遇到测试分布时，元测试过程使用学习到的奖励模型进行正则化策略优化。",
    "tldr": "通过元学习方法优化通用奖励模型，以解决超出分布偏好学习问题，并提高LLMs在有限偏好反馈下的泛化能力",
    "en_tdlr": "Optimizing a general reward model through meta-learning to address out-of-distribution preference learning, enhancing the generalization ability of LLMs with limited preference feedback."
}