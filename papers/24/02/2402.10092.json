{
    "title": "Workflow Optimization for Parallel Split Learning",
    "abstract": "arXiv:2402.10092v1 Announce Type: cross  Abstract: Split learning (SL) has been recently proposed as a way to enable resource-constrained devices to train multi-parameter neural networks (NNs) and participate in federated learning (FL). In a nutshell, SL splits the NN model into parts, and allows clients (devices) to offload the largest part as a processing task to a computationally powerful helper. In parallel SL, multiple helpers can process model parts of one or more clients, thus, considerably reducing the maximum training time over all clients (makespan). In this paper, we focus on orchestrating the workflow of this operation, which is critical in highly heterogeneous systems, as our experiments show. In particular, we formulate the joint problem of client-helper assignments and scheduling decisions with the goal of minimizing the training makespan, and we prove that it is NP-hard. We propose a solution method based on the decomposition of the problem by leveraging its inherent sy",
    "link": "https://arxiv.org/abs/2402.10092",
    "context": "Title: Workflow Optimization for Parallel Split Learning\nAbstract: arXiv:2402.10092v1 Announce Type: cross  Abstract: Split learning (SL) has been recently proposed as a way to enable resource-constrained devices to train multi-parameter neural networks (NNs) and participate in federated learning (FL). In a nutshell, SL splits the NN model into parts, and allows clients (devices) to offload the largest part as a processing task to a computationally powerful helper. In parallel SL, multiple helpers can process model parts of one or more clients, thus, considerably reducing the maximum training time over all clients (makespan). In this paper, we focus on orchestrating the workflow of this operation, which is critical in highly heterogeneous systems, as our experiments show. In particular, we formulate the joint problem of client-helper assignments and scheduling decisions with the goal of minimizing the training makespan, and we prove that it is NP-hard. We propose a solution method based on the decomposition of the problem by leveraging its inherent sy",
    "path": "papers/24/02/2402.10092.json",
    "total_tokens": 820,
    "translated_title": "并行分割学习的工作流优化",
    "translated_abstract": "分割学习（SL）最近被提出作为一种让资源受限设备训练多参数神经网络（NNs）并参与联邦学习（FL）的方法。SL将NN模型分割成部分，并允许客户端（设备）将最大部分作为处理任务卸载给计算能力强大的辅助器。在并行SL中，多个辅助器可以处理一个或多个客户端的模型部分，从而大大减少了所有客户端的训练时间（makespan）。本文关注该操作的工作流编排，特别是在高度异构系统中的关键性问题。我们将客户-辅助器分配和调度决策的联合问题形式化，目标是最小化训练时间（makespan），并证明该问题是NP难问题。我们提出了一种基于问题分解的解决方法，利用其固有特性",
    "tldr": "本文提出了一种并行分割学习的工作流优化方法，旨在最小化训练时间，通过将问题分解成客户-辅助器分配和调度决策的联合问题进行求解。",
    "en_tdlr": "This paper proposes a workflow optimization method for parallel split learning, aiming to minimize training time by decomposing the problem into a joint problem of client-helper assignments and scheduling decisions."
}