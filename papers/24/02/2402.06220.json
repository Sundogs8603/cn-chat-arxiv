{
    "title": "A Unified Causal View of Instruction Tuning",
    "abstract": "Instruction tuning on a mixture of tasks has improved zero-shot capabilities in natural language processing (NLP). Nevertheless, existing methods often learn features that exhibit correlations between instruction-formatted samples and target labels, rather than causal relationships. Termed as ``spurious correlation'' in statistics, such a correlation may change drastically in a new task, making the effect from the learned features to be misleading. To this end, we develop a meta Structural Causal Model (meta-SCM) to integrate different NLP tasks under a single causal structure of the data. Specifically, the meta-SCM introduces multiple latent factors that represent properties of source context, only some of which causally influence the target labels for a specific task. The key idea is to learn task-required causal factors and only use those to make predictions for a given task. Theoretically, we prove the causal factor can be identified without mixing information from others. Guided b",
    "link": "https://arxiv.org/abs/2402.06220",
    "context": "Title: A Unified Causal View of Instruction Tuning\nAbstract: Instruction tuning on a mixture of tasks has improved zero-shot capabilities in natural language processing (NLP). Nevertheless, existing methods often learn features that exhibit correlations between instruction-formatted samples and target labels, rather than causal relationships. Termed as ``spurious correlation'' in statistics, such a correlation may change drastically in a new task, making the effect from the learned features to be misleading. To this end, we develop a meta Structural Causal Model (meta-SCM) to integrate different NLP tasks under a single causal structure of the data. Specifically, the meta-SCM introduces multiple latent factors that represent properties of source context, only some of which causally influence the target labels for a specific task. The key idea is to learn task-required causal factors and only use those to make predictions for a given task. Theoretically, we prove the causal factor can be identified without mixing information from others. Guided b",
    "path": "papers/24/02/2402.06220.json",
    "total_tokens": 912,
    "translated_title": "统一的因果视角下的指令调优",
    "translated_abstract": "在自然语言处理（NLP）中，混合任务上的指令调优已经提高了零-shot能力。然而，现有的方法常常学习到了在指令格式的样本和目标标签之间存在相关性，而不是因果关系的特征。这种被统计学上称为“伪相关性”的相关性在新任务中可能会发生巨大变化，使得学习到的特征对结果产生误导。为此，我们开发了一个元结构因果模型（meta-SCM），用于将不同的NLP任务整合到单一的数据因果结构下。具体而言，meta-SCM引入了多个潜在因子来表示源上下文的特性，其中只有一些对特定任务的目标标签产生因果影响。关键思想是学习任务所需的因果因子，只使用这些因素来预测给定任务的结果。理论上，我们证明了可以在不混合其他信息的情况下进行因果因子的识别。受到因果推断的指导，我们提出了uchii等.（2021）:论文+_因果识别+_整合来学习因果影响",
    "tldr": "该论文提出了一个元结构因果模型（meta-SCM）来整合不同的自然语言处理（NLP）任务。通过学习每个任务所需的因果因子并使用这些因子进行预测，从而解决了现有方法中存在的“伪相关性”问题。",
    "en_tdlr": "This paper proposes a meta Structural Causal Model (meta-SCM) that integrates different Natural Language Processing (NLP) tasks. By learning task-specific causal factors and using them for predictions, it addresses the issue of \"spurious correlation\" in existing methods."
}