{
    "title": "MusicRL: Aligning Music Generation to Human Preferences",
    "abstract": "We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning fr",
    "link": "https://arxiv.org/abs/2402.04229",
    "context": "Title: MusicRL: Aligning Music Generation to Human Preferences\nAbstract: We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning fr",
    "path": "papers/24/02/2402.04229.json",
    "total_tokens": 865,
    "translated_title": "MusicRL: 将音乐生成对齐到人类偏好",
    "translated_abstract": "我们提出了MusicRL，这是第一个通过人类反馈进行优化的音乐生成系统。文本到音乐的模型鉴赏度特别主观，因为音乐性的概念以及文本背后的具体意图都是用户相关的（例如，“活力四溢的健身音乐”可以对应复古吉他独奏或者电子流行音乐节拍）。这不仅使得这类模型的监督训练具有挑战性，而且也需要在部署后将连续的人类反馈整合到其微调中。MusicRL是一个经过预训练的离散音频令牌自回归MusicLM（Agostinelli等人，2023）模型，通过强化学习进行微调以最大化序列级奖励。我们从选定的评价者那里设计与文本一致性和音频质量相关的奖励函数，并使用它们将MusicLM微调为MusicRL-R。我们向用户部署MusicLM并收集了一个包含30万个配对偏好的大型数据集。",
    "tldr": "MusicRL是第一个通过人类反馈进行优化的音乐生成系统。它使用强化学习通过离散音频令牌的自回归模型进行微调，以最大化序列级奖励。在部署后，通过从用户收集的配对偏好数据集来进一步优化系统。"
}