{
    "title": "Improving Sentence Embeddings with an Automatically Generated NLI Dataset",
    "abstract": "arXiv:2402.15132v1 Announce Type: new  Abstract: Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL makes great use of fine-tuning with a manually annotated natural language inference (NLI) dataset. We aim to improve sentence embeddings learned in an unsupervised setting by automatically generating an NLI dataset with an LLM and using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed method achieved an average Spearman's rank correlation coefficient of 82.21 with respect to human evaluation, thus outperforming existing methods without using large, manually annotated datasets.",
    "link": "https://arxiv.org/abs/2402.15132",
    "context": "Title: Improving Sentence Embeddings with an Automatically Generated NLI Dataset\nAbstract: arXiv:2402.15132v1 Announce Type: new  Abstract: Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL makes great use of fine-tuning with a manually annotated natural language inference (NLI) dataset. We aim to improve sentence embeddings learned in an unsupervised setting by automatically generating an NLI dataset with an LLM and using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed method achieved an average Spearman's rank correlation coefficient of 82.21 with respect to human evaluation, thus outperforming existing methods without using large, manually annotated datasets.",
    "path": "papers/24/02/2402.15132.json",
    "total_tokens": 744,
    "translated_title": "通过自动生成的NLI数据集改进句子嵌入",
    "translated_abstract": "基于解码器的大型语言模型在自然语言处理的许多任务中表现出了很高的性能。这在句子嵌入学习中同样成立，其中基于解码器的模型PromptEOL 在语义文本相似性（STS）任务中取得了最佳表现。然而，PromptEOL 在很大程度上利用了对自然语言推理（NLI）数据集的手动标注进行微调。我们旨在通过使用LLM自动生成的NLI数据集来改进在无监督设置下学习的句子嵌入，并将其用于微调PromptEOL。在STS任务的实验中，提出的方法在人类评估方面达到了82.21的平均Spearman等级相关系数，从而优于现有方法而无需使用大规模手动注释的数据集。",
    "tldr": "通过自动生成的NLI数据集改进句子嵌入，实验结果表明该方法在STS任务中表现出色，优于现有方法。",
    "en_tdlr": "Improving sentence embeddings with an automatically generated NLI dataset, the proposed method shows superior performance in STS tasks compared to existing methods."
}