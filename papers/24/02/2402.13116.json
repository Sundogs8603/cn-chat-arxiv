{
    "title": "A Survey on Knowledge Distillation of Large Language Models",
    "abstract": "arXiv:2402.13116v1 Announce Type: new  Abstract: This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, i",
    "link": "https://arxiv.org/abs/2402.13116",
    "context": "Title: A Survey on Knowledge Distillation of Large Language Models\nAbstract: arXiv:2402.13116v1 Announce Type: new  Abstract: This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, i",
    "path": "papers/24/02/2402.13116.json",
    "total_tokens": 834,
    "translated_title": "对大型语言模型知识蒸馏的调查",
    "translated_abstract": "本调查对大型语言模型（LLMs）领域中知识蒸馏（KD）技术进行了深入探讨，重点关注KD在将诸如GPT-4之类的专有巨头的复杂能力转移到可访问的开源模型（如LLaMA和Mistral）中起着关键作用。在不断发展的人工智能领域，本项工作阐明了专有和开源LLMs之间的关键差异，展示了KD如何成为第二者赋予第一者先进功能和细致理解的重要媒介。我们的调查围绕算法、技能和垂直化这三个基础支柱精心构建，全面探讨了KD机制、特定认知能力的增强以及它们在不同领域的实际影响。重要的是，调查引导着数据增强（DA）和KD之间错综复杂的相互作用。",
    "tldr": "本调查深入探讨了大型语言模型知识蒸馏技术的重要性，并阐明了将专有巨头的先进功能转移到开源模型中的关键作用。",
    "en_tdlr": "This survey provides an in-depth exploration of the importance of knowledge distillation for large language models, elucidating the crucial role of transferring advanced capabilities from proprietary giants to open-source models."
}