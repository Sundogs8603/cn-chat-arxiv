{
    "title": "Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies",
    "abstract": "arXiv:2402.12673v1 Announce Type: new  Abstract: In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond only worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic hardness in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\\Pi$. This finding prompts us to \\textit{refine} the baseline policy class $\\Pi$ prior to test time, aimin",
    "link": "https://arxiv.org/abs/2402.12673",
    "context": "Title: Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies\nAbstract: arXiv:2402.12673v1 Announce Type: new  Abstract: In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond only worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic hardness in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\\Pi$. This finding prompts us to \\textit{refine} the baseline policy class $\\Pi$ prior to test time, aimin",
    "path": "papers/24/02/2402.12673.json",
    "total_tokens": 924,
    "translated_title": "超越最坏情况攻击：通过非支配策略实现自适应防御的鲁棒RL",
    "translated_abstract": "随着强化学习（RL）在各种现实世界应用中取得的蓬勃发展，人们开始集中关注确保RL策略在测试时对抗性攻击的鲁棒性。目前的方法主要围绕着解决极端情况问题，以应对潜在的最坏情况。尽管这些方法对强攻击具有一定的效果，但往往在没有攻击或只有弱攻击存在时会牺牲性能。为了解决这个问题，我们研究了在广泛接受的状态对抗攻击模型下的策略鲁棒性，将重点从仅限于最坏情况攻击扩展出来。我们首先将测试时的任务正式化为一个后悔最小化问题，并在基准策略来自于通用连续策略类$\\Pi$时，在实现亚线性后悔的困难性问题上做出了阐述。这一发现促使我们在测试之前\\textit{优化}基准策略类$\\Pi$，致力于",
    "tldr": "研究将RL策略鲁棒性扩展至状态对抗攻击模型，超越仅针对最坏情况攻击，提出基于后悔最小化问题的自适应防御方法。",
    "en_tdlr": "This paper extends the robustness of RL policies to the state-adversarial attack model beyond worst-case attacks, proposing an adaptive defense method based on the regret minimization problem."
}