{
    "title": "Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning",
    "abstract": "arXiv:2402.09290v1 Announce Type: cross Abstract: Deep reinforcement learning has demonstrated remarkable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery. Common methodologies in partially-observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state. We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework. At the heart of PSRL is the fusion of both supervised and unsupervised learning. The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time. This yields more interpretable policies that compose state predictions with control. In parallel, it captures an unsupervised latent representation. These two-the semantic state and the latent state-are then fused and ut",
    "link": "https://arxiv.org/abs/2402.09290",
    "context": "Title: Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning\nAbstract: arXiv:2402.09290v1 Announce Type: cross Abstract: Deep reinforcement learning has demonstrated remarkable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery. Common methodologies in partially-observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state. We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework. At the heart of PSRL is the fusion of both supervised and unsupervised learning. The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time. This yields more interpretable policies that compose state predictions with control. In parallel, it captures an unsupervised latent representation. These two-the semantic state and the latent state-are then fused and ut",
    "path": "papers/24/02/2402.09290.json",
    "total_tokens": 902,
    "translated_title": "通过部分监督强化学习学习后验可观察POMDP中的可解释策略",
    "translated_abstract": "深度强化学习在视频游戏、机器人控制、自主驾驶和药物发现等各个领域取得了显着的成就。但在部分可观察的领域中，常见的方法主要依赖于从高维观察（如图像）进行端到端学习，而没有明确推理真实状态。我们提出了一个替代方向，引入了部分监督强化学习（PSRL）框架。PSRL框架的核心是将监督学习和非监督学习进行融合。该方法利用状态估计器从高维观察中提取出有时在训练时完全观察到的监督语义状态信息。这样可以得到更可解释的策略，将状态预测与控制结合起来。同时，它还捕捉到一个非监督潜在表示。这两者-语义状态和潜在状态然后被融合和应用。",
    "tldr": "该论文提出了部分监督强化学习（PSRL）框架，通过融合监督和非监督学习来生成更可解释的策略，同时利用状态估计器提取出监督语义状态信息，以及捕捉潜在状态信息。",
    "en_tdlr": "This paper proposes a Partially Supervised Reinforcement Learning (PSRL) framework, which generates more interpretable policies by integrating supervised and unsupervised learning, and extracts supervised semantic state information using a state estimator, while capturing latent state information."
}