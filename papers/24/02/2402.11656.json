{
    "title": "Integrating Pre-Trained Language Model with Physical Layer Communications",
    "abstract": "arXiv:2402.11656v1 Announce Type: cross  Abstract: The burgeoning field of on-device AI communication, where devices exchange information directly through embedded foundation models, such as language models (LMs), requires robust, efficient, and generalizable communication frameworks. However, integrating these frameworks with existing wireless systems and effectively managing noise and bit errors pose significant challenges. In this work, we introduce a practical on-device AI communication framework, integrated with physical layer (PHY) communication functions, demonstrated through its performance on a link-level simulator. Our framework incorporates end-to-end training with channel noise to enhance resilience, incorporates vector quantized variational autoencoders (VQ-VAE) for efficient and robust communication, and utilizes pre-trained encoder-decoder transformers for improved generalization capabilities. Simulations, across various communication scenarios, reveal that our framework",
    "link": "https://arxiv.org/abs/2402.11656",
    "context": "Title: Integrating Pre-Trained Language Model with Physical Layer Communications\nAbstract: arXiv:2402.11656v1 Announce Type: cross  Abstract: The burgeoning field of on-device AI communication, where devices exchange information directly through embedded foundation models, such as language models (LMs), requires robust, efficient, and generalizable communication frameworks. However, integrating these frameworks with existing wireless systems and effectively managing noise and bit errors pose significant challenges. In this work, we introduce a practical on-device AI communication framework, integrated with physical layer (PHY) communication functions, demonstrated through its performance on a link-level simulator. Our framework incorporates end-to-end training with channel noise to enhance resilience, incorporates vector quantized variational autoencoders (VQ-VAE) for efficient and robust communication, and utilizes pre-trained encoder-decoder transformers for improved generalization capabilities. Simulations, across various communication scenarios, reveal that our framework",
    "path": "papers/24/02/2402.11656.json",
    "total_tokens": 857,
    "translated_title": "将预训练语言模型与物理层通信集成",
    "translated_abstract": "在设备间人工智能通信的新兴领域中，设备直接通过嵌入式基础模型（如语言模型）交换信息，需要强大、高效且通用的通信框架。然而，将这些框架与现有无线系统集成并有效管理噪声和比特误差都面临着重大挑战。在本研究中，我们介绍了一个实用的设备间人工智能通信框架，集成了物理层通信功能，并通过链路级模拟器展示了其性能。我们的框架通过端到端训练结合信道噪声以增强韧性，采用向量量化变分自动编码器（VQ-VAE）实现高效稳健的通信，利用预训练编码-解码Transformer提升通用性能。在各种通信场景的模拟中，我们的框架展现出",
    "tldr": "提出了一个集成了物理层通信功能的实用设备间人工智能通信框架，通过端到端训练结合信道噪声以增强韧性，采用VQ-VAE实现高效稳健的通信，利用预训练Transformer提升通用性能",
    "en_tdlr": "Proposed a practical on-device AI communication framework integrated with physical layer communication functions, enhanced robustness through end-to-end training with channel noise, achieved efficient and robust communication with VQ-VAE, and improved generalization capabilities using pre-trained Transformer."
}