{
    "title": "Language-Based User Profiles for Recommendation",
    "abstract": "arXiv:2402.15623v1 Announce Type: new  Abstract: Most conventional recommendation methods (e.g., matrix factorization) represent user profiles as high-dimensional vectors. Unfortunately, these vectors lack interpretability and steerability, and often perform poorly in cold-start settings. To address these shortcomings, we explore the use of user profiles that are represented as human-readable text. We propose the Language-based Factorization Model (LFM), which is essentially an encoder/decoder model where both the encoder and the decoder are large language models (LLMs). The encoder LLM generates a compact natural-language profile of the user's interests from the user's rating history. The decoder LLM uses this summary profile to complete predictive downstream tasks. We evaluate our LFM approach on the MovieLens dataset, comparing it against matrix factorization and an LLM model that directly predicts from the user's rating history. In cold-start settings, we find that our method can h",
    "link": "https://arxiv.org/abs/2402.15623",
    "context": "Title: Language-Based User Profiles for Recommendation\nAbstract: arXiv:2402.15623v1 Announce Type: new  Abstract: Most conventional recommendation methods (e.g., matrix factorization) represent user profiles as high-dimensional vectors. Unfortunately, these vectors lack interpretability and steerability, and often perform poorly in cold-start settings. To address these shortcomings, we explore the use of user profiles that are represented as human-readable text. We propose the Language-based Factorization Model (LFM), which is essentially an encoder/decoder model where both the encoder and the decoder are large language models (LLMs). The encoder LLM generates a compact natural-language profile of the user's interests from the user's rating history. The decoder LLM uses this summary profile to complete predictive downstream tasks. We evaluate our LFM approach on the MovieLens dataset, comparing it against matrix factorization and an LLM model that directly predicts from the user's rating history. In cold-start settings, we find that our method can h",
    "path": "papers/24/02/2402.15623.json",
    "total_tokens": 820,
    "translated_title": "基于语言的用户偏好推荐方法",
    "translated_abstract": "大多数传统的推荐方法（如矩阵分解）将用户偏好表示为高维向量。不幸的是，这些向量缺乏可解释性和可控性，在冷启动环境下往往表现不佳。为了解决这些缺点，我们探索了使用以人类可读文本表示的用户偏好。我们提出了基于语言的因子分解模型（LFM），它本质上是一个编码器/解码器模型，其中编码器和解码器均为大型语言模型（LLM）。编码器LLM从用户的评分历史生成用户兴趣的简洁自然语言描述。解码器LLM使用这个简要描述来完成预测性的下游任务。我们在MovieLens数据集上评估了LFM方法，将其与矩阵分解和直接从用户评分历史预测的LLM模型进行了比较。在冷启动环境下，我们发现我们的方法能够...",
    "tldr": "通过使用以人类可读文本表示的用户偏好，提出了Language-based Factorization Model (LFM)，在冷启动环境中与传统方法相比取得了更好的表现"
}