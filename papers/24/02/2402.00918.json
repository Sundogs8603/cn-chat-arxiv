{
    "title": "MUSTAN: Multi-scale Temporal Context as Attention for Robust Video Foreground Segmentation",
    "abstract": "Video foreground segmentation (VFS) is an important computer vision task wherein one aims to segment the objects under motion from the background. Most of the current methods are image-based, i.e., rely only on spatial cues while ignoring motion cues. Therefore, they tend to overfit the training data and don't generalize well to out-of-domain (OOD) distribution. To solve the above problem, prior works exploited several cues such as optical flow, background subtraction mask, etc. However, having a video data with annotations like optical flow is a challenging task. In this paper, we utilize the temporal information and the spatial cues from the video data to improve OOD performance. However, the challenge lies in how we model the temporal information given the video data in an interpretable way creates a very noticeable difference. We therefore devise a strategy that integrates the temporal context of the video in the development of VFS. Our approach give rise to deep learning architect",
    "link": "https://rss.arxiv.org/abs/2402.00918",
    "context": "Title: MUSTAN: Multi-scale Temporal Context as Attention for Robust Video Foreground Segmentation\nAbstract: Video foreground segmentation (VFS) is an important computer vision task wherein one aims to segment the objects under motion from the background. Most of the current methods are image-based, i.e., rely only on spatial cues while ignoring motion cues. Therefore, they tend to overfit the training data and don't generalize well to out-of-domain (OOD) distribution. To solve the above problem, prior works exploited several cues such as optical flow, background subtraction mask, etc. However, having a video data with annotations like optical flow is a challenging task. In this paper, we utilize the temporal information and the spatial cues from the video data to improve OOD performance. However, the challenge lies in how we model the temporal information given the video data in an interpretable way creates a very noticeable difference. We therefore devise a strategy that integrates the temporal context of the video in the development of VFS. Our approach give rise to deep learning architect",
    "path": "papers/24/02/2402.00918.json",
    "total_tokens": 866,
    "translated_title": "MUSTAN：基于多尺度时间上下文的注意力机制的鲁棒视频前景分割",
    "translated_abstract": "视频前景分割是一种重要的计算机视觉任务，旨在从背景中分割出运动中的物体。大多数现有方法只依赖于空间线索而忽略了运动线索，因此容易过度拟合训练数据，泛化能力差。为了解决这个问题，先前的研究利用了光流、背景减除遮罩等多种线索。然而，光流等视频数据的标注是一项具有挑战性的任务。本文利用视频数据中的时间信息和空间线索来改善泛化性能，但是如何以可解释的方式对视频数据的时间信息建模是一个挑战。因此，我们设计了一种策略，将视频的时间上下文整合到视频前景分割的开发中。我们的方法产生了深度学习架构。",
    "tldr": "本文提出了一种基于多尺度时间上下文的注意力机制（MUSTAN），用于改善视频前景分割的泛化性能。通过整合视频的时间信息和空间线索，我们的方法在深度学习架构中实现了鲁棒的前景分割。",
    "en_tdlr": "This paper proposes a Multi-scale Temporal Context as Attention (MUSTAN) mechanism to improve the generalization performance of video foreground segmentation. By integrating the temporal information and spatial cues from the video data, our approach achieves robust foreground segmentation in a deep learning architecture."
}