{
    "title": "Evaluation of Predictive Reliability to Foster Trust in Artificial Intelligence. A case study in Multiple Sclerosis",
    "abstract": "arXiv:2402.17554v1 Announce Type: new  Abstract: Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical contexts, such as medicine, requires the implementation of safety measures to reduce risks of harm in case of prediction errors. Spotting ML failures is of paramount importance when ML predictions are used to drive clinical decisions. ML predictive reliability measures the degree of trust of a ML prediction on a new instance, thus allowing decision-makers to accept or reject it based on its reliability. To assess reliability, we propose a method that implements two principles. First, our approach evaluates whether an instance to be classified is coming from the same distribution of the training set. To do this, we leverage Autoencoders (AEs) ability to reconstruct the training set with low error. An instance is considered Out-of-Distribution (OOD) if the AE reconstructs it with a high error. Second, it is evaluated whether the ML classifier has good performances ",
    "link": "https://arxiv.org/abs/2402.17554",
    "context": "Title: Evaluation of Predictive Reliability to Foster Trust in Artificial Intelligence. A case study in Multiple Sclerosis\nAbstract: arXiv:2402.17554v1 Announce Type: new  Abstract: Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical contexts, such as medicine, requires the implementation of safety measures to reduce risks of harm in case of prediction errors. Spotting ML failures is of paramount importance when ML predictions are used to drive clinical decisions. ML predictive reliability measures the degree of trust of a ML prediction on a new instance, thus allowing decision-makers to accept or reject it based on its reliability. To assess reliability, we propose a method that implements two principles. First, our approach evaluates whether an instance to be classified is coming from the same distribution of the training set. To do this, we leverage Autoencoders (AEs) ability to reconstruct the training set with low error. An instance is considered Out-of-Distribution (OOD) if the AE reconstructs it with a high error. Second, it is evaluated whether the ML classifier has good performances ",
    "path": "papers/24/02/2402.17554.json",
    "total_tokens": 855,
    "translated_title": "评估预测可靠性以培养人工智能的信任。多发性硬化症案例研究",
    "translated_abstract": "在关键背景（如医学）中应用人工智能（AI）和机器学习（ML）需要实施安全措施，以降低在预测错误的情况下造成的伤害风险。当ML预测用于指导临床决策时，发现ML失败至关重要。ML预测可靠性衡量了ML预测在新示例上的信任度，从而使决策者能够根据其可靠性接受或拒绝。为了评估可靠性，我们提出了一种实现两个原则的方法。首先，我们的方法评估要分类的示例是否来自训练集的相同分布。为此，我们利用自动编码器（AE）重建具有低误差的训练集的能力。如果AE将示例重构为高误差，则认为该示例是“分布外”（OOD）。其次，评估ML分类器的性能是否良好",
    "tldr": "通过提出一种方法，评估人工智能预测的可靠性，使决策者能够根据其可靠性来接受或拒绝预测结果",
    "en_tdlr": "By proposing a method to evaluate the predictive reliability of artificial intelligence, decision-makers can accept or reject predictions based on their reliability."
}