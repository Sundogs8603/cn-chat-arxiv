{
    "title": "Random features models: a way to study the success of naive imputation",
    "abstract": "Constant (naive) imputation is still widely used in practice as this is a first easy-to-use technique to deal with missing data. Yet, this simple method could be expected to induce a large bias for prediction purposes, as the imputed input may strongly differ from the true underlying data. However, recent works suggest that this bias is low in the context of high-dimensional linear predictors when data is supposed to be missing completely at random (MCAR). This paper completes the picture for linear predictors by confirming the intuition that the bias is negligible  and  that surprisingly naive imputation also remains relevant in very low dimension.To this aim, we consider a unique underlying random features model, which offers a rigorous framework for studying predictive performances, whilst the dimension of the observed features varies.Building on these theoretical results, we establish finite-sample bounds on stochastic gradient (SGD) predictors applied to zero-imputed data, a strat",
    "link": "https://arxiv.org/abs/2402.03839",
    "context": "Title: Random features models: a way to study the success of naive imputation\nAbstract: Constant (naive) imputation is still widely used in practice as this is a first easy-to-use technique to deal with missing data. Yet, this simple method could be expected to induce a large bias for prediction purposes, as the imputed input may strongly differ from the true underlying data. However, recent works suggest that this bias is low in the context of high-dimensional linear predictors when data is supposed to be missing completely at random (MCAR). This paper completes the picture for linear predictors by confirming the intuition that the bias is negligible  and  that surprisingly naive imputation also remains relevant in very low dimension.To this aim, we consider a unique underlying random features model, which offers a rigorous framework for studying predictive performances, whilst the dimension of the observed features varies.Building on these theoretical results, we establish finite-sample bounds on stochastic gradient (SGD) predictors applied to zero-imputed data, a strat",
    "path": "papers/24/02/2402.03839.json",
    "total_tokens": 912,
    "translated_title": "随机特征模型：研究天真插补的成功方法",
    "translated_abstract": "常数（天真）插补作为一种最简单易用的处理缺失数据的技术，仍然被广泛使用。然而，这种简单的方法在预测目的上可能会引起很大的偏差，因为插补的输入可能与真实的基础数据差异很大。然而，最近的研究表明，在高维线性预测器的背景下，当数据被假设为完全随机缺失（MCAR）时，这种偏差较低。本文通过确认直觉，完善了线性预测器的情况，并且令人惊讶的是，天真插补在非常低的维度下仍然是有效的。为此，我们考虑了一个独特的基础随机特征模型，这个模型在观察特征的维度变化的情况下，为研究预测性能提供了严格的框架。基于这些理论结果，我们建立了应用于零插补数据的随机梯度（SGD）预测器的有限样本界限，一种策略",
    "tldr": "这项研究探讨了在高维线性预测器和完全随机缺失数据的背景下，天真插补方法在预测性能上的偏差较低，并且在非常低的维度下依然是有效的。"
}