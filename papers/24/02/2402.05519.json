{
    "title": "Can ChatGPT evaluate research quality?",
    "abstract": "Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research evaluations on journal articles to automate this time-consuming task. Design/methodology/approach: Test the extent to which ChatGPT-4 can assess the quality of journal articles using a case study of the published scoring guidelines of the UK Research Excellence Framework (REF) 2021 to create a research evaluation ChatGPT. This was applied to 51 of my own articles and compared against my own quality judgements. Findings: ChatGPT-4 can produce plausible document summaries and quality evaluation rationales that match the REF criteria. Its overall scores have weak correlations with my self-evaluation scores of the same documents (averaging r=0.281 over 15 iterations, with 8 being statistically significantly different from 0). In contrast, the average scores from the 15 iterations produced a statistically significant positive correlation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds seems more effec",
    "link": "https://arxiv.org/abs/2402.05519",
    "context": "Title: Can ChatGPT evaluate research quality?\nAbstract: Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research evaluations on journal articles to automate this time-consuming task. Design/methodology/approach: Test the extent to which ChatGPT-4 can assess the quality of journal articles using a case study of the published scoring guidelines of the UK Research Excellence Framework (REF) 2021 to create a research evaluation ChatGPT. This was applied to 51 of my own articles and compared against my own quality judgements. Findings: ChatGPT-4 can produce plausible document summaries and quality evaluation rationales that match the REF criteria. Its overall scores have weak correlations with my self-evaluation scores of the same documents (averaging r=0.281 over 15 iterations, with 8 being statistically significantly different from 0). In contrast, the average scores from the 15 iterations produced a statistically significant positive correlation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds seems more effec",
    "path": "papers/24/02/2402.05519.json",
    "total_tokens": 953,
    "translated_title": "ChatGPT能评估研究质量吗？",
    "translated_abstract": "目的: 评估ChatGPT 4.0是否足够准确，能够自动评估期刊论文的研究质量，以节省时间。设计/方法: 通过使用2021年英国研究卓越框架（REF）的发布评分指南创建一个研究评估ChatGPT，并将其应用于我自己的51篇文章中，并与我自己的质量判断进行比较，测试ChatGPT-4评估期刊论文质量的能力。结果: ChatGPT-4可以生成符合REF标准的合理的文档摘要和质量评估理由。它的总体评分与我对相同文章的自我评价得分之间存在较弱的相关性（平均为r=0.281，在15次迭代中平均有8次与0显著不同）。相比之下，15次迭代的平均得分呈现出了0.509的显著正相关。因此，对多轮ChatGPT-4评分求平均似乎更有效。",
    "tldr": "本研究评估了ChatGPT 4.0评估研究质量的准确性，发现其能够产生符合标准的文档摘要和质量评估理由。然而，与作者自我评价得分相比，ChatGPT-4的评分相关性较弱，但多轮评分的平均得分具有显著正相关性。",
    "en_tdlr": "This study assesses the accuracy of ChatGPT 4.0 in evaluating research quality and finds that it can generate document summaries and quality evaluation rationales that align with the standards. However, the correlations between ChatGPT-4 scores and self-evaluation scores are weak, but averaging scores from multiple rounds shows significant positive correlation."
}