{
    "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens",
    "abstract": "arXiv:2402.13753v1 Announce Type: new  Abstract: Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation ",
    "link": "https://arxiv.org/abs/2402.13753",
    "context": "Title: LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens\nAbstract: arXiv:2402.13753v1 Announce Type: new  Abstract: Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation ",
    "path": "papers/24/02/2402.13753.json",
    "total_tokens": 867,
    "translated_title": "将LLM上下文窗口扩展到超过2百万个标记的LongRoPE",
    "translated_abstract": "大上下文窗口是大型语言模型（LLMs）中的一个理想特性。然而，由于高昂的微调成本、长文本稀缺以及新标记位置引入的灾难性值，当前的扩展上下文窗口仅限于约128k个标记。本文介绍了LongRoPE，首次将预训练的LLMs的上下文窗口扩展至令人印象深刻的2048k个标记，通过仅在256k训练长度内最多进行1k次微调步骤，同时保持在原始短上下文窗口下的性能。这是通过三项关键创新实现的：(i) 我们识别并利用了两种形式的位置插值中的非均匀性，通过高效搜索提供更好的微调初始化，并在非微调场景下实现8倍扩展；(ii) 我们引入了一种逐步扩展策略，首先微调256k长度的LLM，然后进行第二次位置插值。",
    "tldr": "LongRoPE首次将预训练的LLM上下文窗口扩展至2048k个标记，通过关键创新实现了这一突破。",
    "en_tdlr": "LongRoPE extends the context window of pre-trained LLMs to an impressive 2048k tokens for the first time, achieved through key innovations."
}