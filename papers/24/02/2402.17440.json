{
    "title": "Principled Architecture-aware Scaling of Hyperparameters",
    "abstract": "arXiv:2402.17440v1 Announce Type: new  Abstract: Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process. Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios. However, most designs or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters. In this work, we precisely characterize the dependence of initializations and maximal learning rates on the network architecture, which includes the network depth, width, convolutional kernel size, and connectivity patterns. By pursuing every parameter to be maximally updated with the same mean squared change in pre-activations, we can generalize our initialization and learning rates across MLPs (multi-layer perception) and CNNs (convolutional neural network) with sophisticated graph topologie",
    "link": "https://arxiv.org/abs/2402.17440",
    "context": "Title: Principled Architecture-aware Scaling of Hyperparameters\nAbstract: arXiv:2402.17440v1 Announce Type: new  Abstract: Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process. Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios. However, most designs or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters. In this work, we precisely characterize the dependence of initializations and maximal learning rates on the network architecture, which includes the network depth, width, convolutional kernel size, and connectivity patterns. By pursuing every parameter to be maximally updated with the same mean squared change in pre-activations, we can generalize our initialization and learning rates across MLPs (multi-layer perception) and CNNs (convolutional neural network) with sophisticated graph topologie",
    "path": "papers/24/02/2402.17440.json",
    "total_tokens": 809,
    "translated_title": "基于原则的架构感知超参数缩放",
    "translated_abstract": "训练高质量的深度神经网络需要选择合适的超参数，这是一个非常重要且昂贵的过程。当前的研究试图自动优化或设计超参数的原则，以便它们能够推广到多样的未知场景。然而，大多数设计或优化方法对网络结构的选择一无所知，因此在很大程度上忽略了神经架构对超参数的影响。本文准确表征了初始化和最大学习速率对网络架构的依赖性，包括网络深度、宽度、卷积核大小和连接模式。通过追求使每个参数在预激活中具有相同的均方变化，我们可以将初始化和学习速率推广到具有复杂图拓扑结构的多层感知器（MLP）和卷积神经网络（CNN）中。",
    "tldr": "通过精确定位初始化和最大学习速率对网络结构的依赖性，本文可以将初始化和学习速率推广到MLP和CNN中，以适用于不同的神经网络架构。",
    "en_tdlr": "By precisely characterizing the dependence of initializations and maximal learning rates on network architecture, this work generalizes the initialization and learning rates to MLPs and CNNs, making them applicable to various neural network architectures."
}