{
    "title": "DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators",
    "abstract": "arXiv:2402.15200v1 Announce Type: new  Abstract: Generally, the decoder-only large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process into three separate phases. During each phase, different continuous prompts are introduced to make LLMs discriminatel",
    "link": "https://arxiv.org/abs/2402.15200",
    "context": "Title: DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators\nAbstract: arXiv:2402.15200v1 Announce Type: new  Abstract: Generally, the decoder-only large language models (LLMs) are adapted to context-aware neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation of the source sentence (i.e., intra-sentence context) and the inter-sentence context as the input, and then to generate the target tokens sequentially. This adaptation strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence contexts with the same priority, despite an apparent difference between the two kinds of contexts. In this paper, we propose an alternative adaptation approach, named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately model and utilize the inter- and intra-sentence context and more effectively adapt LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process into three separate phases. During each phase, different continuous prompts are introduced to make LLMs discriminatel",
    "path": "papers/24/02/2402.15200.json",
    "total_tokens": 832,
    "translated_title": "DeMPT：解码增强的多阶段提示优化，使LLMs成为更好的上下文感知翻译器",
    "translated_abstract": "通常，仅具有解码器的大型语言模型（LLMs）通过连接的方式适应上下文感知神经机器翻译（NMT），其中LLMs将源句（即句内上下文）和句间上下文的连接作为输入，然后顺序生成目标标记。本文提出了一种名为Decoding-enhanced Multi-phase Prompt Tuning（DeMPT）的替代适应方法，以使LLMs能够歧视性地对模组和利用句间和句内上下文，并更有效地将LLMs调整到上下文感知NMT。首先，DeMPT将上下文感知NMT过程分为三个单独阶段。在每个阶段，引入不同的连续提示，使LLMs能够区分地模型。",
    "tldr": "DeMPT提出了解码增强的多阶段提示优化，使得LLMs更好地模拟和利用句间和句内上下文，从而更有效地适应上下文感知NMT。",
    "en_tdlr": "DeMPT introduces decoding-enhanced multi-phase prompt tuning to enable LLMs to better model and utilize inter- and intra-sentence contexts, consequently adapting more effectively to context-aware NMT."
}