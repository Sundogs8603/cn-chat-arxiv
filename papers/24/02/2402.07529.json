{
    "title": "Accelerating Distributed Deep Learning using Lossless Homomorphic Compression",
    "abstract": "As deep neural networks (DNNs) grow in complexity and size, the resultant increase in communication overhead during distributed training has become a significant bottleneck, challenging the scalability of distributed training systems. Existing solutions, while aiming to mitigate this bottleneck through worker-level compression and in-network aggregation, fall short due to their inability to efficiently reconcile the trade-offs between compression effectiveness and computational overhead, hindering overall performance and scalability. In this paper, we introduce a novel compression algorithm that effectively merges worker-level compression with in-network aggregation. Our solution is both homomorphic, allowing for efficient in-network aggregation without CPU/GPU processing, and lossless, ensuring no compromise on training accuracy. Theoretically optimal in compression and computational efficiency, our approach is empirically validated across diverse DNN models such as NCF, LSTM, VGG19, ",
    "link": "https://arxiv.org/abs/2402.07529",
    "context": "Title: Accelerating Distributed Deep Learning using Lossless Homomorphic Compression\nAbstract: As deep neural networks (DNNs) grow in complexity and size, the resultant increase in communication overhead during distributed training has become a significant bottleneck, challenging the scalability of distributed training systems. Existing solutions, while aiming to mitigate this bottleneck through worker-level compression and in-network aggregation, fall short due to their inability to efficiently reconcile the trade-offs between compression effectiveness and computational overhead, hindering overall performance and scalability. In this paper, we introduce a novel compression algorithm that effectively merges worker-level compression with in-network aggregation. Our solution is both homomorphic, allowing for efficient in-network aggregation without CPU/GPU processing, and lossless, ensuring no compromise on training accuracy. Theoretically optimal in compression and computational efficiency, our approach is empirically validated across diverse DNN models such as NCF, LSTM, VGG19, ",
    "path": "papers/24/02/2402.07529.json",
    "total_tokens": 893,
    "translated_title": "使用无损同态压缩加速分布式深度学习",
    "translated_abstract": "随着深度神经网络（DNN）的复杂性和规模增长，分布式训练过程中的通信开销也随之增加，成为挑战分布式训练系统可扩展性的重要瓶颈。现有的解决方案旨在通过工作节点级别的压缩和网络内聚合来缓解这个瓶颈，但由于它们无法有效平衡压缩效果和计算开销之间的权衡，从而阻碍了整体性能和可扩展性。在本文中，我们引入了一种新颖的压缩算法，将工作节点级别的压缩和网络内聚合有效地结合起来。我们的解决方案既是同态的，可以实现高效的网络内聚合，又是无损的，确保在训练精度上没有妥协。在压缩和计算效率上理论上是最优的，我们的方法在多种DNN模型（如NCF，LSTM，VGG19）上进行了实证验证。",
    "tldr": "本论文介绍了一种使用无损同态压缩加速分布式深度学习的新方法，通过结合工作节点级别的压缩和网络内聚合，实现高效的训练过程，并保证了训练精度。实验证明了该方法在各种DNN模型上的优越性能。",
    "en_tdlr": "This paper introduces a novel approach of accelerating distributed deep learning using lossless homomorphic compression, which effectively combines worker-level compression and in-network aggregation to achieve efficient training with preserved accuracy. Experimental results demonstrate the superior performance of this method across various DNN models."
}