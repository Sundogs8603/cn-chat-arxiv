{
    "title": "An information theoretic approach to quantify the stability of feature selection and ranking algorithms",
    "abstract": "Feature selection is a key step when dealing with high dimensional data. In particular, these techniques simplify the process of knowledge discovery from the data by selecting the most relevant features out of the noisy, redundant and irrelevant features. A problem that arises in many of these practical applications is that the outcome of the feature selection algorithm is not stable. Thus, small variations in the data may yield very different feature rankings. Assessing the stability of these methods becomes an important issue in the previously mentioned situations. We propose an information theoretic approach based on the Jensen Shannon divergence to quantify this robustness. Unlike other stability measures, this metric is suitable for different algorithm outcomes: full ranked lists, feature subsets as well as the lesser studied partial ranked lists. This generalized metric quantifies the difference among a whole set of lists with the same size, following a probabilistic approach and",
    "link": "https://arxiv.org/abs/2402.05295",
    "context": "Title: An information theoretic approach to quantify the stability of feature selection and ranking algorithms\nAbstract: Feature selection is a key step when dealing with high dimensional data. In particular, these techniques simplify the process of knowledge discovery from the data by selecting the most relevant features out of the noisy, redundant and irrelevant features. A problem that arises in many of these practical applications is that the outcome of the feature selection algorithm is not stable. Thus, small variations in the data may yield very different feature rankings. Assessing the stability of these methods becomes an important issue in the previously mentioned situations. We propose an information theoretic approach based on the Jensen Shannon divergence to quantify this robustness. Unlike other stability measures, this metric is suitable for different algorithm outcomes: full ranked lists, feature subsets as well as the lesser studied partial ranked lists. This generalized metric quantifies the difference among a whole set of lists with the same size, following a probabilistic approach and",
    "path": "papers/24/02/2402.05295.json",
    "total_tokens": 842,
    "translated_title": "一种量化特征选择和排序算法稳定性的信息论方法",
    "translated_abstract": "特征选择是处理高维数据时的关键步骤。特别是这些技术通过从嘈杂、冗余和无关的特征中选择最相关的特征，简化了从数据中发现知识的过程。在许多实际应用中出现的一个问题是特征选择算法的结果是不稳定的。因此，数据的微小变化可能导致非常不同的特征排序。在上述情况中，评估这些方法的稳定性成为一个重要问题。我们提出了一种基于Jensen Shannon距离的信息论方法来量化这种鲁棒性。与其他稳定性度量不同，这个度量指标适用于不同的算法结果：完整的排名列表、特征子集以及较少研究的部分排名列表。这个广义度量以概率方法量化了相同大小的整个列表集的差异，并提供了反映其中的排序稳定性的结果。",
    "tldr": "本论文提出了一种基于信息论的方法来量化特征选择和排序算法的稳定性。该方法能够评估不同算法结果中的特征排序的稳定性，包括完整的排名列表、特征子集和部分排名列表。"
}