{
    "title": "Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations",
    "abstract": "arXiv:2402.11608v1 Announce Type: new  Abstract: We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to understand how neural systems represent the theoretical features of the objects they process. As a proof-of-concept, we apply MLEMs to neural representations extracted from BERT, and track a wide variety of linguistic features (e.g., tense, subject person, clause type, clause embedding). We find that: (1) linguistic features are ordered: they separate representations of sentences to different degrees in different layers; (2) neural representations are organized hierarchically: in some layers, we find clusters of representations nested within larger clusters, following successively important linguistic features; (3) linguistic features are disentangled in middle layers: distinct, selective units are activated by distinct linguistic features. Methodologically, MLEMs are superior (4) to multivariate decoding methods, being more robust to type-I errors, and (5) to univ",
    "link": "https://arxiv.org/abs/2402.11608",
    "context": "Title: Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations\nAbstract: arXiv:2402.11608v1 Announce Type: new  Abstract: We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to understand how neural systems represent the theoretical features of the objects they process. As a proof-of-concept, we apply MLEMs to neural representations extracted from BERT, and track a wide variety of linguistic features (e.g., tense, subject person, clause type, clause embedding). We find that: (1) linguistic features are ordered: they separate representations of sentences to different degrees in different layers; (2) neural representations are organized hierarchically: in some layers, we find clusters of representations nested within larger clusters, following successively important linguistic features; (3) linguistic features are disentangled in middle layers: distinct, selective units are activated by distinct linguistic features. Methodologically, MLEMs are superior (4) to multivariate decoding methods, being more robust to type-I errors, and (5) to univ",
    "path": "papers/24/02/2402.11608.json",
    "total_tokens": 892,
    "translated_title": "指标学习编码模型识别BERT表示中的语言特征处理特征",
    "translated_abstract": "我们介绍了指标学习编码模型（MLEMs）作为一种理解神经系统如何表示其处理对象的理论特征的新方法。作为概念验证，我们将MLEMs应用于从BERT中提取的神经表示，并跟踪各种语言特征（例如时态、主语人称、从句类型、从句嵌套等）。我们发现：（1）语言特征是有序的：它们在不同层中以不同程度将句子的表示分开；（2）神经表示是分层组织的：在某些层中，我们发现表示的群集嵌套在更大的群集内部，遵循逐渐重要的语言特征；（3）语言特征在中间层中是解耦的：不同的、选择性单位由不同的语言特征激活。在方法论上，MLEMs（4）优于多变量解码方法，更具抗类型-I错误的鲁棒性，（5）优于单变量",
    "tldr": "应用指标学习编码模型（MLEMs）于BERT表示，发现语言特征在不同层中有序分离，神经表示层级组织，中间层解耦，优于其他解码方法。",
    "en_tdlr": "Using Metric-Learning Encoding Models (MLEMs) on BERT representations reveals ordered separation of linguistic features across layers, hierarchical organization of neural representations, and disentanglement of features in middle layers, outperforming other decoding methods."
}