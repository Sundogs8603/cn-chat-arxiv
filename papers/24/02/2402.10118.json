{
    "title": "Reusing Softmax Hardware Unit for GELU Computation in Transformers",
    "abstract": "arXiv:2402.10118v1 Announce Type: cross  Abstract: Transformers have improved drastically the performance of natural language processing (NLP) and computer vision applications. The computation of transformers involves matrix multiplications and non-linear activation functions such as softmax and GELU (Gaussion Error Linear Unit) that are accelerated directly in hardware. Currently, function evaluation is done separately for each function and rarely allows for hardware reuse. To mitigate this problem, in this work, we map the computation of GELU to a softmax operator. In this way, the efficient hardware units designed already for softmax can be reused for computing GELU as well. Computation of GELU can enjoy the inherent vectorized nature of softmax and produce in parallel multiple GELU outcomes. Experimental results show that computing GELU via a pre-existing and incrementally modified softmax hardware unit (a) does not reduce the accuracy of representative NLP applications and (b) all",
    "link": "https://arxiv.org/abs/2402.10118",
    "context": "Title: Reusing Softmax Hardware Unit for GELU Computation in Transformers\nAbstract: arXiv:2402.10118v1 Announce Type: cross  Abstract: Transformers have improved drastically the performance of natural language processing (NLP) and computer vision applications. The computation of transformers involves matrix multiplications and non-linear activation functions such as softmax and GELU (Gaussion Error Linear Unit) that are accelerated directly in hardware. Currently, function evaluation is done separately for each function and rarely allows for hardware reuse. To mitigate this problem, in this work, we map the computation of GELU to a softmax operator. In this way, the efficient hardware units designed already for softmax can be reused for computing GELU as well. Computation of GELU can enjoy the inherent vectorized nature of softmax and produce in parallel multiple GELU outcomes. Experimental results show that computing GELU via a pre-existing and incrementally modified softmax hardware unit (a) does not reduce the accuracy of representative NLP applications and (b) all",
    "path": "papers/24/02/2402.10118.json",
    "total_tokens": 803,
    "translated_title": "在Transformer中重用Softmax硬件单元进行GELU计算",
    "translated_abstract": "Transformers大大提高了自然语言处理（NLP）和计算机视觉应用的性能。Transformer的计算涉及矩阵乘法和非线性激活函数，如softmax和GELU（高斯误差线性单元），这些函数可以直接在硬件中加速。目前，每个函数的计算都是分开完成的，很少能够重复使用硬件。为了解决这个问题，本文将GELU计算映射到softmax运算符上。这样，已经设计用于softmax的高效硬件单元也可以用于计算GELU。GELU的计算可以充分利用softmax的向量化特性，同时并行产生多个GELU的结果。实验结果表明，通过预先存在并逐步修改的softmax硬件单元计算GELU（a）不会降低代表性NLP应用的准确性，（b）全部",
    "tldr": "本文提出了一种在Transformer中重用Softmax硬件单元进行GELU计算的方法，实验证明这种方法不会降低NLP应用的准确性。",
    "en_tdlr": "This paper proposes a method to reuse Softmax hardware units for GELU computation in Transformers, and experimental results show that it does not reduce the accuracy of NLP applications."
}