{
    "title": "Emergence of heavy tails in homogenized stochastic gradient descent",
    "abstract": "It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent, show that it behaves asymptotically heavy-tailed, and give explicit upper and lower bounds on its tail-index. We validate these bounds in numerical experiments and show that they are typically close approximations to the empirical tail-index of SGD iterates. In addition, their explicit form enables us to quantify the interplay between optimization parameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima.",
    "link": "https://rss.arxiv.org/abs/2402.01382",
    "context": "Title: Emergence of heavy tails in homogenized stochastic gradient descent\nAbstract: It has repeatedly been observed that loss minimization by stochastic gradient descent (SGD) leads to heavy-tailed distributions of neural network parameters. Here, we analyze a continuous diffusion approximation of SGD, called homogenized stochastic gradient descent, show that it behaves asymptotically heavy-tailed, and give explicit upper and lower bounds on its tail-index. We validate these bounds in numerical experiments and show that they are typically close approximations to the empirical tail-index of SGD iterates. In addition, their explicit form enables us to quantify the interplay between optimization parameters and the tail-index. Doing so, we contribute to the ongoing discussion on links between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima.",
    "path": "papers/24/02/2402.01382.json",
    "total_tokens": 865,
    "translated_title": "齐次化随机梯度下降中重尾现象的出现",
    "translated_abstract": "随机梯度下降（SGD）通过最小化损失已经被发现导致神经网络参数的重尾分布。在这里，我们分析了SGD的连续扩散逼近，称为齐次化随机梯度下降，证明了它在渐进情况下表现出重尾特性，并给出了关于尾指数的明确上下界。我们在数值实验中验证了这些界并显示它们通常是SGD迭代的经验尾指数的近似。另外，它们的明确形式使我们能够量化优化参数和尾指数之间的相互作用。通过这样做，我们对于关于重尾和神经网络的泛化性能以及SGD避免次优局部最小值能力的联系的讨论做出了贡献。",
    "tldr": "这项研究分析了齐次化随机梯度下降算法，在数值实验中验证了参数尾指数的明确上下界，并量化了优化参数与尾指数之间的相互作用，从而为重尾和神经网络的泛化性能以及SGD避免次优局部最小值能力之间的关系提供了贡献。",
    "en_tdlr": "This research analyzes the homogenized stochastic gradient descent algorithm, validates explicit upper and lower bounds on parameter tail indices in numerical experiments, and quantifies the interplay between optimization parameters and tail indices, contributing to the understanding of the connection between heavy tails and the generalization performance of neural networks as well as the ability of SGD to avoid suboptimal local minima."
}