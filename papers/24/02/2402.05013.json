{
    "title": "Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth",
    "abstract": "Autoencoders are a prominent model in many empirical branches of machine learning and lossy data compression. However, basic theoretical questions remain unanswered even in a shallow two-layer setting. In particular, to what degree does a shallow autoencoder capture the structure of the underlying data distribution? For the prototypical case of the 1-bit compression of sparse Gaussian data, we prove that gradient descent converges to a solution that completely disregards the sparse structure of the input. Namely, the performance of the algorithm is the same as if it was compressing a Gaussian source - with no sparsity. For general data distributions, we give evidence of a phase transition phenomenon in the shape of the gradient descent minimizer, as a function of the data sparsity: below the critical sparsity level, the minimizer is a rotation taken uniformly at random (just like in the compression of non-sparse data); above the critical sparsity, the minimizer is the identity (up to a",
    "link": "https://arxiv.org/abs/2402.05013",
    "context": "Title: Compression of Structured Data with Autoencoders: Provable Benefit of Nonlinearities and Depth\nAbstract: Autoencoders are a prominent model in many empirical branches of machine learning and lossy data compression. However, basic theoretical questions remain unanswered even in a shallow two-layer setting. In particular, to what degree does a shallow autoencoder capture the structure of the underlying data distribution? For the prototypical case of the 1-bit compression of sparse Gaussian data, we prove that gradient descent converges to a solution that completely disregards the sparse structure of the input. Namely, the performance of the algorithm is the same as if it was compressing a Gaussian source - with no sparsity. For general data distributions, we give evidence of a phase transition phenomenon in the shape of the gradient descent minimizer, as a function of the data sparsity: below the critical sparsity level, the minimizer is a rotation taken uniformly at random (just like in the compression of non-sparse data); above the critical sparsity, the minimizer is the identity (up to a",
    "path": "papers/24/02/2402.05013.json",
    "total_tokens": 961,
    "translated_title": "用自编码器对结构化数据进行压缩：非线性和深度的可证明优势",
    "translated_abstract": "自编码器是机器学习和有损数据压缩的多个实证分支中的一个重要模型。然而，即使在浅层两层设置下，基本的理论问题仍然没有答案。特别是，在浅层自编码器中能够多大程度上捕捉到底层数据分布的结构？对于典型的稀疏高斯数据的1位压缩问题，我们证明了梯度下降算法收敛于一个完全忽略输入的稀疏结构的解决方案。换句话说，算法的性能与压缩高斯源（没有稀疏性）的性能相同。对于一般的数据分布，我们给出了关于梯度下降最小化器的形状的相变现象的证据，作为数据稀疏性的函数：在临界稀疏水平以下，最小化器是一个随机均匀选择的旋转（就像非稀疏数据的压缩一样）；在临界稀疏水平以上，最小化器是身份变换（除了一个+1的偏移）。",
    "tldr": "本文证明了在自编码器中，即使采用浅层结构，梯度下降算法会完全忽略稀疏数据的结构，并且对于一般数据分布，我们发现了梯度下降最小化器在数据稀疏性临界点处的相变现象。",
    "en_tdlr": "This paper proves that in autoencoders, even with a shallow structure, the gradient descent algorithm completely ignores the structure of sparse data. For general data distributions, a phase transition phenomenon is observed in the gradient descent minimizer at the critical sparsity level."
}