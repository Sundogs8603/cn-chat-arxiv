{
    "title": "Enhancing the \"Immunity\" of Mixture-of-Experts Networks for Adversarial Defense",
    "abstract": "arXiv:2402.18787v1 Announce Type: new  Abstract: Recent studies have revealed the vulnerability of Deep Neural Networks (DNNs) to adversarial examples, which can easily fool DNNs into making incorrect predictions. To mitigate this deficiency, we propose a novel adversarial defense method called \"Immunity\" (Innovative MoE with MUtual information \\& positioN stabilITY) based on a modified Mixture-of-Experts (MoE) architecture in this work. The key enhancements to the standard MoE are two-fold: 1) integrating of Random Switch Gates (RSGs) to obtain diverse network structures via random permutation of RSG parameters at evaluation time, despite of RSGs being determined after one-time training; 2) devising innovative Mutual Information (MI)-based and Position Stability-based loss functions by capitalizing on Grad-CAM's explanatory power to increase the diversity and the causality of expert networks. Notably, our MI-based loss operates directly on the heatmaps, thereby inducing subtler negati",
    "link": "https://arxiv.org/abs/2402.18787",
    "context": "Title: Enhancing the \"Immunity\" of Mixture-of-Experts Networks for Adversarial Defense\nAbstract: arXiv:2402.18787v1 Announce Type: new  Abstract: Recent studies have revealed the vulnerability of Deep Neural Networks (DNNs) to adversarial examples, which can easily fool DNNs into making incorrect predictions. To mitigate this deficiency, we propose a novel adversarial defense method called \"Immunity\" (Innovative MoE with MUtual information \\& positioN stabilITY) based on a modified Mixture-of-Experts (MoE) architecture in this work. The key enhancements to the standard MoE are two-fold: 1) integrating of Random Switch Gates (RSGs) to obtain diverse network structures via random permutation of RSG parameters at evaluation time, despite of RSGs being determined after one-time training; 2) devising innovative Mutual Information (MI)-based and Position Stability-based loss functions by capitalizing on Grad-CAM's explanatory power to increase the diversity and the causality of expert networks. Notably, our MI-based loss operates directly on the heatmaps, thereby inducing subtler negati",
    "path": "papers/24/02/2402.18787.json",
    "total_tokens": 965,
    "translated_title": "提升混合专家网络的“免疫力”以抵御对抗性攻击",
    "translated_abstract": "近期研究揭示了深度神经网络(DNNs)对抗性示例的易受攻击性，这些示例可以轻松地愚弄DNNs，使其做出错误的预测。为了减少这种缺陷，本文提出了一种基于修改的专家混合(MoE)架构的新型对抗性防御方法称为“免疫力”(Innovative MoE with MUtual information \\& positioN stabilITY)。该方法的关键改进有两方面：1)集成随机切换门(RSGs)，在评估时通过随机排列RSG参数获得多样的网络结构，尽管RSGs在经过一次训练后确定；2)利用Grad-CAM的解释能力，通过设计基于互信息(MI)和位置稳定性的损失函数，增加专家网络的多样性和因果关系。值得注意的是，我们的基于MI的损失直接在热图上运行，从而诱导出更微妙的负向特征。",
    "tldr": "提出了一种名为“免疫力”(Immunity)的新对抗性防御方法，通过修改的专家混合(MoE)架构，结合随机切换门(RSGs)和基于互信息(MI)和位置稳定性的损失函数，增加了专家网络的多样性和因果关系。"
}