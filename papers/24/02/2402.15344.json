{
    "title": "Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates",
    "abstract": "arXiv:2402.15344v1 Announce Type: cross  Abstract: The performance of stochastic gradient descent (SGD), which is the simplest first-order optimizer for training deep neural networks, depends on not only the learning rate but also the batch size. They both affect the number of iterations and the stochastic first-order oracle (SFO) complexity needed for training. In particular, the previous numerical results indicated that, for SGD using a constant learning rate, the number of iterations needed for training decreases when the batch size increases, and the SFO complexity needed for training is minimized at a critical batch size and that it increases once the batch size exceeds that size. Here, we study the relationship between batch size and the iteration and SFO complexities needed for nonconvex optimization in deep learning with SGD using constant or decaying learning rates and show that SGD using the critical batch size minimizes the SFO complexity. We also provide numerical compariso",
    "link": "https://arxiv.org/abs/2402.15344",
    "context": "Title: Iteration and Stochastic First-order Oracle Complexities of Stochastic Gradient Descent using Constant and Decaying Learning Rates\nAbstract: arXiv:2402.15344v1 Announce Type: cross  Abstract: The performance of stochastic gradient descent (SGD), which is the simplest first-order optimizer for training deep neural networks, depends on not only the learning rate but also the batch size. They both affect the number of iterations and the stochastic first-order oracle (SFO) complexity needed for training. In particular, the previous numerical results indicated that, for SGD using a constant learning rate, the number of iterations needed for training decreases when the batch size increases, and the SFO complexity needed for training is minimized at a critical batch size and that it increases once the batch size exceeds that size. Here, we study the relationship between batch size and the iteration and SFO complexities needed for nonconvex optimization in deep learning with SGD using constant or decaying learning rates and show that SGD using the critical batch size minimizes the SFO complexity. We also provide numerical compariso",
    "path": "papers/24/02/2402.15344.json",
    "total_tokens": 857,
    "translated_title": "使用固定和递减学习率的随机梯度下降的迭代和随机一阶预言者复杂度",
    "translated_abstract": "随机梯度下降（SGD）的性能取决于学习率和批量大小，影响训练所需的迭代次数和随机一阶预言者（SFO）复杂度。先前的数值结果表明，对于使用固定学习率的SGD，随着批量大小的增加，训练所需的迭代次数减少，并且SFO复杂度在关键批量大小时最小化，一旦批量大小超过该大小后增加。本文研究了在深度学习中使用固定或递减学习率的SGD进行非凸优化时，批量大小与所需迭代和SFO复杂度之间的关系，并表明使用关键批量大小的SGD可以最小化SFO复杂度。",
    "tldr": "研究了在深度学习中使用固定或递减学习率的SGD进行非凸优化时，批量大小与迭代和SFO复杂度之间的关系，并指出使用关键批量大小的SGD可以最小化SFO复杂度",
    "en_tdlr": "Investigated the relationship between batch size and the iteration and SFO complexities needed for nonconvex optimization in deep learning with SGD using constant or decaying learning rates, showing that using the critical batch size minimizes the SFO complexity."
}