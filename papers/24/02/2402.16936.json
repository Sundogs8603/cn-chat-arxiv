{
    "title": "Disentangled 3D Scene Generation with Layout Learning",
    "abstract": "arXiv:2402.16936v1 Announce Type: cross  Abstract: We introduce a method to generate 3D scenes that are disentangled into their component objects. This disentanglement is unsupervised, relying only on the knowledge of a large pretrained text-to-image model. Our key insight is that objects can be discovered by finding parts of a 3D scene that, when rearranged spatially, still produce valid configurations of the same scene. Concretely, our method jointly optimizes multiple NeRFs from scratch - each representing its own object - along with a set of layouts that composite these objects into scenes. We then encourage these composited scenes to be in-distribution according to the image generator. We show that despite its simplicity, our approach successfully generates 3D scenes decomposed into individual objects, enabling new capabilities in text-to-3D content creation. For results and an interactive demo, see our project page at https://dave.ml/layoutlearning/",
    "link": "https://arxiv.org/abs/2402.16936",
    "context": "Title: Disentangled 3D Scene Generation with Layout Learning\nAbstract: arXiv:2402.16936v1 Announce Type: cross  Abstract: We introduce a method to generate 3D scenes that are disentangled into their component objects. This disentanglement is unsupervised, relying only on the knowledge of a large pretrained text-to-image model. Our key insight is that objects can be discovered by finding parts of a 3D scene that, when rearranged spatially, still produce valid configurations of the same scene. Concretely, our method jointly optimizes multiple NeRFs from scratch - each representing its own object - along with a set of layouts that composite these objects into scenes. We then encourage these composited scenes to be in-distribution according to the image generator. We show that despite its simplicity, our approach successfully generates 3D scenes decomposed into individual objects, enabling new capabilities in text-to-3D content creation. For results and an interactive demo, see our project page at https://dave.ml/layoutlearning/",
    "path": "papers/24/02/2402.16936.json",
    "total_tokens": 827,
    "translated_title": "具有布局学习的去卷积三维场景生成",
    "translated_abstract": "我们介绍了一种方法来生成被分解成其组件对象的三维场景。这种分解是无监督的，仅依赖于一个大型预训练的文本到图像模型的知识。我们的关键洞察是，通过找到一个三维场景的部分，在空间重新布置时仍然产生相同场景的有效配置，可以发现对象。具体来说，我们的方法从头开始联合优化多个 NeRF 模型 - 每个模型代表其自己的对象 - 以及将这些对象组成场景的一组布局。然后，我们鼓励这些合成的场景根据图像生成器保持在分布中。我们展示了，尽管方法简单，但我们的方法成功地生成了分解为单独对象的三维场景，实现了文本到三维内容创建中的新功能。有关结果和交互式演示，请查看我们的项目页面 https://dave.ml/layoutlearning/",
    "tldr": "该方法通过布局学习实现了将三维场景分解为各个单独对象，从而在文本到三维内容创建方面带来了新的功能。",
    "en_tdlr": "The method achieves the disentanglement of 3D scenes into individual objects through layout learning, enabling new capabilities in text-to-3D content creation."
}