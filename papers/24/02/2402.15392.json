{
    "title": "Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms",
    "abstract": "arXiv:2402.15392v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) aims to recover the reward function of an expert agent from demonstrations of behavior. It is well known that the IRL problem is fundamentally ill-posed, i.e., many reward functions can explain the demonstrations. For this reason, IRL has been recently reframed in terms of estimating the feasible reward set, thus, postponing the selection of a single reward. However, so far, the available formulations and algorithmic solutions have been proposed and analyzed mainly for the online setting, where the learner can interact with the environment and query the expert at will. This is clearly unrealistic in most practical applications, where the availability of an offline dataset is a much more common scenario. In this paper, we introduce a novel notion of feasible reward set capturing the opportunities and limitations of the offline setting and we analyze the complexity of its estimation. This requires the i",
    "link": "https://arxiv.org/abs/2402.15392",
    "context": "Title: Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms\nAbstract: arXiv:2402.15392v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) aims to recover the reward function of an expert agent from demonstrations of behavior. It is well known that the IRL problem is fundamentally ill-posed, i.e., many reward functions can explain the demonstrations. For this reason, IRL has been recently reframed in terms of estimating the feasible reward set, thus, postponing the selection of a single reward. However, so far, the available formulations and algorithmic solutions have been proposed and analyzed mainly for the online setting, where the learner can interact with the environment and query the expert at will. This is clearly unrealistic in most practical applications, where the availability of an offline dataset is a much more common scenario. In this paper, we introduce a novel notion of feasible reward set capturing the opportunities and limitations of the offline setting and we analyze the complexity of its estimation. This requires the i",
    "path": "papers/24/02/2402.15392.json",
    "total_tokens": 750,
    "translated_title": "离线逆强化学习：新的解决方案概念和可证明高效算法",
    "translated_abstract": "逆强化学习（IRL）旨在从行为演示中恢复专家代理的奖励函数。目前已经逐渐以估计可行奖励集合作为IRL的新框架，将选择单一奖励推迟。然而，迄今为止，现有的制定和算法解决方案主要针对在线设置提出，并达到分析，这在大多数实际应用中明显不现实，在那里离线数据集更为普遍。本文引入了一个捕捉离线环境机遇和限制的可行奖励集概念，并分析了其估计的复杂性。",
    "tldr": "该论文提出了一种新的可行奖励集概念，以应对离线设定的机会和限制，并分析了其估计的复杂性。",
    "en_tdlr": "This paper introduces a novel notion of feasible reward set to address the opportunities and limitations of the offline setting and analyzes the complexity of its estimation."
}