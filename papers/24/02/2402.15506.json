{
    "title": "AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning",
    "abstract": "arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address these challenges. \\textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \\textbf{xLAM-v0.1}, a large action mode",
    "link": "https://arxiv.org/abs/2402.15506",
    "context": "Title: AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning\nAbstract: arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address these challenges. \\textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \\textbf{xLAM-v0.1}, a large action mode",
    "path": "papers/24/02/2402.15506.json",
    "total_tokens": 843,
    "translated_title": "AgentOhana：为有效智能体学习设计统一数据和训练流水线",
    "translated_abstract": "由大型语言模型（LLMs）提供支持的自主智能体引起了重大研究关注。然而，充分利用LLMs的潜力进行基于智能体的任务面临困难，这是由于具有多轮轨迹的多样化数据源的异构性。在本文中，我们介绍AgentOhana作为解决这些挑战的综合解决方案。AgentOhana从不同环境中聚合智能体轨迹，涵盖了各种情景。它精心地将这些轨迹标准化和统一到一致的格式中，简化了为智能体训练优化的通用数据加载器的创建。通过数据统一，我们的训练流水线在不同数据源之间保持平衡，并在数据集划分和模型训练过程中保持设备之间的独立随机性。此外，我们还介绍了xLAM-v0.1，一个大动作模式",
    "tldr": "AgentOhana提供了一种统一数据和训练流水线的综合解决方案，有助于克服使用大型语言模型（LLMs）进行智能体任务时的挑战。"
}