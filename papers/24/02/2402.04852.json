{
    "title": "Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning",
    "abstract": "In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mast",
    "link": "https://arxiv.org/abs/2402.04852",
    "context": "Title: Multi-Patch Prediction: Adapting LLMs for Time Series Representation Learning\nAbstract: In this study, we present aLLM4TS, an innovative framework that adapts Large Language Models (LLMs) for time-series representation learning. Central to our approach is that we reconceive time-series forecasting as a self-supervised, multi-patch prediction task, which, compared to traditional mask-and-reconstruction methods, captures temporal dynamics in patch representations more effectively. Our strategy encompasses two-stage training: (i). a causal continual pre-training phase on various time-series datasets, anchored on next patch prediction, effectively syncing LLM capabilities with the intricacies of time-series data; (ii). fine-tuning for multi-patch prediction in the targeted time-series context. A distinctive element of our framework is the patch-wise decoding layer, which departs from previous methods reliant on sequence-level decoding. Such a design directly transposes individual patches into temporal sequences, thereby significantly bolstering the model's proficiency in mast",
    "path": "papers/24/02/2402.04852.json",
    "total_tokens": 879,
    "translated_title": "多块预测：适应时间序列表示学习的LLMs方法",
    "translated_abstract": "本研究提出了一个创新的框架aLLM4TS，用于将大型语言模型（LLMs）应用于时间序列表示学习。我们的方法将时间序列预测重新构想为一项自监督的多块预测任务，相比传统的掩码和重构方法，更有效地捕捉了块表示中的时间动态。我们的策略包括两个阶段的训练：(i) 在各种时间序列数据集上进行因果连续预训练阶段，以下一个块预测为锚点，有效地将LLM的能力与时间序列数据的复杂性同步。(ii) 在目标时间序列上进行多块预测的微调。我们框架的一个独特要素是块级解码层，不同于之前依赖于序列级解码的方法。这样的设计直接将单个块转换为时间序列，从而显著增强了模型在掩蔽任务下的能力。",
    "tldr": "本研究提出了aLLM4TS框架，将LLMs应用于时间序列表示学习。通过自监督的多块预测任务，捕捉时间动态特征，并通过特定时间序列上的微调进行进一步优化。"
}