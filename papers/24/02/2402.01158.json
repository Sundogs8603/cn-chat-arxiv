{
    "title": "LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning",
    "abstract": "ChatGPT and other general large language models (LLMs) have achieved remarkable success, but they have also raised concerns about the misuse of AI-generated texts. Existing AI-generated text detection models, such as based on BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor out-of-domain (OOD) detection performance. In this paper, we first collected Chinese text responses generated by human experts and 9 types of LLMs, for which to multiple domains questions, and further created a dataset that mixed human-written sentences and sentences polished by LLMs. We then proposed LLM-Detector, a novel method for both document-level and sentence-level text detection through Instruction Tuning of LLMs. Our method leverages the wealth of knowledge LLMs acquire during pre-training, enabling them to detect the text they generate. Instruction tuning aligns the model's responses with the user's expected text detection tasks. Experimental results show that previous methods struggl",
    "link": "https://rss.arxiv.org/abs/2402.01158",
    "context": "Title: LLM-Detector: Improving AI-Generated Chinese Text Detection with Open-Source LLM Instruction Tuning\nAbstract: ChatGPT and other general large language models (LLMs) have achieved remarkable success, but they have also raised concerns about the misuse of AI-generated texts. Existing AI-generated text detection models, such as based on BERT and RoBERTa, are prone to in-domain over-fitting, leading to poor out-of-domain (OOD) detection performance. In this paper, we first collected Chinese text responses generated by human experts and 9 types of LLMs, for which to multiple domains questions, and further created a dataset that mixed human-written sentences and sentences polished by LLMs. We then proposed LLM-Detector, a novel method for both document-level and sentence-level text detection through Instruction Tuning of LLMs. Our method leverages the wealth of knowledge LLMs acquire during pre-training, enabling them to detect the text they generate. Instruction tuning aligns the model's responses with the user's expected text detection tasks. Experimental results show that previous methods struggl",
    "path": "papers/24/02/2402.01158.json",
    "total_tokens": 929,
    "translated_title": "LLM-Detector: 使用开源LLM指令调整来改进AI生成的中文文本检测",
    "translated_abstract": "ChatGPT和其他通用大型语言模型（LLMs）取得了显著的成功，但也引发了有关滥用人工智能生成文本的担忧。现有的基于BERT和RoBERTa的AI生成文本检测模型容易在领域内过度拟合，导致领域外（OOD）的检测性能差。本文首先收集了人工专家和9种LLMs生成的中文文本回答，针对多个领域的问题，并进一步创建了一个混合了人工编写句子和LLMs润饰句子的数据集。然后，提出了LLM-Detector，一种通过LLMs的指令调整实现文档级和句子级文本检测的新方法。我们的方法利用LLMs在预训练期间获得的丰富知识，使其能够检测它们生成的文本。指令调整将模型的响应与用户的期望文本检测任务保持一致。实验结果表明，之前的方法出现了困难。",
    "tldr": "本文提出了一个名为LLM-Detector的方法，使用开源LLM指令调整来改进AI生成的中文文本检测。通过提供混合了人工编写句子和LLMs润饰句子的数据集，并利用LLMs在预训练期间获得的知识，我们能够在文档级和句子级上实现高效的文本检测。实验结果表明，该方法优于现有的基于BERT和RoBERTa的模型。"
}