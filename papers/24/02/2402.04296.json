{
    "title": "LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\\times$ Faster Inference",
    "abstract": "Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment. In practice, we find that one key barrier to the efficient deployment of HGNNs is the high-order structural dependencies during inference. In this paper, we propose to bridge the gap between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to eliminate the hypergraph dependency of HGNNs and thus reduce computational complexity as well as improve inference speed. Specifically, we introduce LightHGNN and LightHGNN$^+$ for fast inference with low complexity. LightHGNN directly distills the knowledge from teacher HGNNs to student MLPs via soft labels, and LightHGNN$^+$ further explicitly injects reliable high-order correlations into ",
    "link": "https://arxiv.org/abs/2402.04296",
    "context": "Title: LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\\times$ Faster Inference\nAbstract: Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment. In practice, we find that one key barrier to the efficient deployment of HGNNs is the high-order structural dependencies during inference. In this paper, we propose to bridge the gap between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to eliminate the hypergraph dependency of HGNNs and thus reduce computational complexity as well as improve inference speed. Specifically, we introduce LightHGNN and LightHGNN$^+$ for fast inference with low complexity. LightHGNN directly distills the knowledge from teacher HGNNs to student MLPs via soft labels, and LightHGNN$^+$ further explicitly injects reliable high-order correlations into ",
    "path": "papers/24/02/2402.04296.json",
    "total_tokens": 876,
    "translated_title": "光HGNN：将超图神经网络蒸馏成MLPs，推断速度提升100倍",
    "translated_abstract": "最近，由于其在高阶相关性建模方面的优势，超图神经网络(HGNNs)引起了广泛关注并展现了令人满意的性能。然而，我们注意到，超图的高阶建模能力也带来了增加的计算复杂性，这阻碍了其在实际工业部署中的应用。实际上，我们发现HGNNs高阶结构依赖在推断过程中是高效部署的一个关键障碍。在本文中，我们提出了将HGNNs和高效推断的多层感知器(MLPs)联系起来，以消除HGNNs的超图依赖性，从而降低计算复杂性并改善推断速度。具体而言，我们引入了LightHGNN和LightHGNN$^+$，以实现快速推断和低复杂性。LightHGNN通过软标签将知识直接从teacher HGNN蒸馏到student MLPs中，而LightHGNN$^+$则进一步显式地将可靠的高阶相关性注入其中。",
    "tldr": "本论文介绍了一种光HGNN方法，将超图神经网络(HGNNs)转化为Multi-Layer Perceptron (MLPs)以提高推断速度。LightHGNN通过软标签将知识从teacher HGNN蒸馏到student MLPs，而LightHGNN$^+$则注入了可靠的高阶相关性。"
}