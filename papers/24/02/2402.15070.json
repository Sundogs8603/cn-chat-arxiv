{
    "title": "Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting",
    "abstract": "arXiv:2402.15070v1 Announce Type: new  Abstract: One-shot Federated Learning (OFL) has become a promising learning paradigm, enabling the training of a global server model via a single communication round. In OFL, the server model is aggregated by distilling knowledge from all client models (the ensemble), which are also responsible for synthesizing samples for distillation. In this regard, advanced works show that the performance of the server model is intrinsically related to the quality of the synthesized data and the ensemble model. To promote OFL, we introduce a novel framework, Co-Boosting, in which synthesized data and the ensemble model mutually enhance each other progressively. Specifically, Co-Boosting leverages the current ensemble model to synthesize higher-quality samples in an adversarial manner. These hard samples are then employed to promote the quality of the ensemble model by adjusting the ensembling weights for each client model. Consequently, Co-Boosting periodicall",
    "link": "https://arxiv.org/abs/2402.15070",
    "context": "Title: Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting\nAbstract: arXiv:2402.15070v1 Announce Type: new  Abstract: One-shot Federated Learning (OFL) has become a promising learning paradigm, enabling the training of a global server model via a single communication round. In OFL, the server model is aggregated by distilling knowledge from all client models (the ensemble), which are also responsible for synthesizing samples for distillation. In this regard, advanced works show that the performance of the server model is intrinsically related to the quality of the synthesized data and the ensemble model. To promote OFL, we introduce a novel framework, Co-Boosting, in which synthesized data and the ensemble model mutually enhance each other progressively. Specifically, Co-Boosting leverages the current ensemble model to synthesize higher-quality samples in an adversarial manner. These hard samples are then employed to promote the quality of the ensemble model by adjusting the ensembling weights for each client model. Consequently, Co-Boosting periodicall",
    "path": "papers/24/02/2402.15070.json",
    "total_tokens": 843,
    "translated_title": "通过数据和集成协同增强增强一次性联邦学习",
    "translated_abstract": "一次性联邦学习（OFL）已经成为一种有前途的学习范式，通过单一通信轮次实现全局服务器模型的训练。在OFL中，服务器模型通过从所有客户端模型（集成）中提炼知识进行聚合，客户端模型也负责合成用于提炼的样本。在这方面，先进的作品表明，服务器模型的性能与合成数据的质量和集成模型密切相关。为了推广OFL，我们引入了一个新颖的框架，Co-Boosting，其中合成数据和集成模型相互逐步增强对方。具体而言，Co-Boosting利用当前的集成模型以对抗方式合成更高质量的样本。这些困难样本然后被用来通过调整每个客户端模型的集成权重来提升集成模型的质量。因此，Co-Boosting周期性地...",
    "tldr": "在本论文中，提出了一种名为Co-Boosting的新型框架，通过对合成数据和集成模型相互增强，促进了一次性联邦学习的发展。",
    "en_tdlr": "This paper introduces a novel framework called Co-Boosting, which promotes the development of One-Shot Federated Learning by enhancing the synthesized data and the ensemble model mutually."
}