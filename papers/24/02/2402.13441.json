{
    "title": "PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models",
    "abstract": "arXiv:2402.13441v1 Announce Type: new  Abstract: Deep neural networks (DNNs) have proven to be effective models for accurate Memory Access Prediction (MAP), a critical task in mitigating memory latency through data prefetching. However, existing DNN-based MAP models suffer from the challenges such as significant physical storage space and poor inference latency, primarily due to their large number of parameters. These limitations render them impractical for deployment in real-world scenarios. In this paper, we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to compress MAP models while maintaining the prediction performance. The PaCKD approach encompasses three steps: clustering memory access sequences into distinct partitions involving similar patterns, training large pattern-specific teacher models for memory access prediction for each partition, and training a single lightweight student model by distilling the knowledge from the trained pattern-specific teachers. ",
    "link": "https://arxiv.org/abs/2402.13441",
    "context": "Title: PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models\nAbstract: arXiv:2402.13441v1 Announce Type: new  Abstract: Deep neural networks (DNNs) have proven to be effective models for accurate Memory Access Prediction (MAP), a critical task in mitigating memory latency through data prefetching. However, existing DNN-based MAP models suffer from the challenges such as significant physical storage space and poor inference latency, primarily due to their large number of parameters. These limitations render them impractical for deployment in real-world scenarios. In this paper, we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to compress MAP models while maintaining the prediction performance. The PaCKD approach encompasses three steps: clustering memory access sequences into distinct partitions involving similar patterns, training large pattern-specific teacher models for memory access prediction for each partition, and training a single lightweight student model by distilling the knowledge from the trained pattern-specific teachers. ",
    "path": "papers/24/02/2402.13441.json",
    "total_tokens": 919,
    "translated_title": "PaCKD: 模式聚类知识蒸馏用于压缩内存访问预测模型",
    "translated_abstract": "深度神经网络（DNNs）已被证明是用于准确的内存访问预测（MAP）的有效模型，这是通过数据预取来缓解内存延迟的关键任务。然而，现有基于DNN的MAP模型存在诸如显著的物理存储空间和推理延迟不佳等挑战，主要是由于它们庞大的参数数量。这些限制使它们在实际部署中变得不切实际。在本文中，我们提出了一种名为PaCKD的模式聚类知识蒸馏方法，用于压缩MAP模型同时保持预测性能。PaCKD方法包括三个步骤：将内存访问序列聚类到涉及相似模式的不同分区中，为每个分区训练大型模式特定的用于内存访问预测的教师模型，以及通过从经过训练的模式特定教师那里提炼知识来训练单个轻量级学生模型。",
    "tldr": "提出了一种名为PaCKD的模式聚类知识蒸馏方法，用于压缩内存访问预测模型，通过将内存访问序列聚类并训练模式特定的教师模型，最终训练出一个轻量级的学生模型。"
}