{
    "title": "Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning",
    "abstract": "Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prot",
    "link": "https://arxiv.org/abs/2402.03917",
    "context": "Title: Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning\nAbstract: Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prot",
    "path": "papers/24/02/2402.03917.json",
    "total_tokens": 896,
    "translated_title": "冷启动无示例增量学习的弹性特征整合",
    "translated_abstract": "无示例类别增量学习（EFCIL）旨在从一系列任务中学习，而不需要访问先前任务的数据。在本文中，我们考虑了具有挑战性的冷启动场景，在第一个任务中没有足够的数据来学习高质量的骨干网络。对于EFCIL来说，这是特别具有挑战性的，因为它需要高度的可塑性，这会导致特征漂移，在无示例的情况下很难进行补偿。为了解决这个问题，我们提出了一种简单而有效的方法，通过规范在与先前任务高度相关的方向上的漂移，并利用原型来减少任务新鲜度偏差，以整合特征表示。我们的方法被称为弹性特征整合（EFC），它利用基于经验特征矩阵（EFM）的可解二阶近似来处理特征漂移。EFM在特征空间中引入了伪度量，我们使用它来规范重要方向上的特征漂移，并更新高斯原型。",
    "tldr": "这篇论文解决了冷启动场景的无示例增量学习的问题，提出了一种弹性特征整合的方法，通过规范特征漂移并利用原型来减少任务新鲜度偏差。",
    "en_tdlr": "This paper addresses the problem of exemplar-free incremental learning in cold start scenarios and proposes a method called Elastic Feature Consolidation that regulates feature drift and reduces task-recency bias using prototypes."
}