{
    "title": "On the Resurgence of Recurrent Models for Long Sequences: Survey and Research Opportunities in the Transformer Era",
    "abstract": "A longstanding challenge for the Machine Learning community is the one of developing models that are capable of processing and learning from very long sequences of data. The outstanding results of Transformers-based networks (e.g., Large Language Models) promotes the idea of parallel attention as the key to succeed in such a challenge, obfuscating the role of classic sequential processing of Recurrent Models. However, in the last few years, researchers who were concerned by the quadratic complexity of self-attention have been proposing a novel wave of neural models, which gets the best from the two worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State Models emerged as robust approaches to function approximation over time, thus opening a new perspective in learning from sequential data, followed by many people in the field and exploited to implement a special class of (linear) Recurrent Neural Networks. This survey is aimed at providing an overview of these trends ",
    "link": "https://arxiv.org/abs/2402.08132",
    "context": "Title: On the Resurgence of Recurrent Models for Long Sequences: Survey and Research Opportunities in the Transformer Era\nAbstract: A longstanding challenge for the Machine Learning community is the one of developing models that are capable of processing and learning from very long sequences of data. The outstanding results of Transformers-based networks (e.g., Large Language Models) promotes the idea of parallel attention as the key to succeed in such a challenge, obfuscating the role of classic sequential processing of Recurrent Models. However, in the last few years, researchers who were concerned by the quadratic complexity of self-attention have been proposing a novel wave of neural models, which gets the best from the two worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State Models emerged as robust approaches to function approximation over time, thus opening a new perspective in learning from sequential data, followed by many people in the field and exploited to implement a special class of (linear) Recurrent Neural Networks. This survey is aimed at providing an overview of these trends ",
    "path": "papers/24/02/2402.08132.json",
    "total_tokens": 864,
    "translated_title": "《关于循环模型在长序列中的复兴：在Transformer时代的调研和研究机会》",
    "translated_abstract": "机器学习领域长期以来的一个挑战是开发可以处理和学习非常长的数据序列的模型。基于Transformer的网络（例如大型语言模型）的出色结果推动了并行注意力的概念，将经典的顺序处理的循环模型的作用掩盖起来。然而，过去几年中，一些研究人员对自注意力的二次复杂度表示关注，提出了一系列兼顾Transformer和循环网络两个世界优势的新型神经模型。同时，深度空间状态模型作为时间函数逼近的强大方法出现，从而为从序列数据中学习开辟了新的视角，许多领域的研究者对此感兴趣并利用它来实现一类特殊的（线性）循环神经网络。本调研旨在概述这些趋势。",
    "tldr": "这项调研总结了在处理长序列数据方面，循环模型的复兴和与Transformer模型相结合的新型神经模型的发展，以及深度空间状态模型作为时间函数逼近的方法的出现。",
    "en_tdlr": "This survey provides an overview of the resurgence of recurrent models in processing long sequences, the development of novel neural models combining both transformer and recurrent networks, and the emergence of deep space-state models as a function approximation approach for sequential data."
}