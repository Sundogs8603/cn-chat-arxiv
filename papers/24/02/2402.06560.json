{
    "title": "Video Annotator: A framework for efficiently building video classifiers using vision-language models and active learning",
    "abstract": "High-quality and consistent annotations are fundamental to the successful development of robust machine learning models. Traditional data annotation methods are resource-intensive and inefficient, often leading to a reliance on third-party annotators who are not the domain experts. Hard samples, which are usually the most informative for model training, tend to be difficult to label accurately and consistently without business context. These can arise unpredictably during the annotation process, requiring a variable number of iterations and rounds of feedback, leading to unforeseen expenses and time commitments to guarantee quality.   We posit that more direct involvement of domain experts, using a human-in-the-loop system, can resolve many of these practical challenges. We propose a novel framework we call Video Annotator (VA) for annotating, managing, and iterating on video classification datasets. Our approach offers a new paradigm for an end-user-centered model development process,",
    "link": "https://arxiv.org/abs/2402.06560",
    "context": "Title: Video Annotator: A framework for efficiently building video classifiers using vision-language models and active learning\nAbstract: High-quality and consistent annotations are fundamental to the successful development of robust machine learning models. Traditional data annotation methods are resource-intensive and inefficient, often leading to a reliance on third-party annotators who are not the domain experts. Hard samples, which are usually the most informative for model training, tend to be difficult to label accurately and consistently without business context. These can arise unpredictably during the annotation process, requiring a variable number of iterations and rounds of feedback, leading to unforeseen expenses and time commitments to guarantee quality.   We posit that more direct involvement of domain experts, using a human-in-the-loop system, can resolve many of these practical challenges. We propose a novel framework we call Video Annotator (VA) for annotating, managing, and iterating on video classification datasets. Our approach offers a new paradigm for an end-user-centered model development process,",
    "path": "papers/24/02/2402.06560.json",
    "total_tokens": 911,
    "translated_title": "视频标注器：利用视觉语言模型和主动学习构建视频分类器的高效框架",
    "translated_abstract": "高质量和一致的注释对于成功开发稳健的机器学习模型至关重要。传统的数据注释方法耗费资源且效率低下，常常依赖于非领域专家的第三方注释者。对于模型训练而言，通常最具信息量的困难样本往往很难在没有业务背景的情况下进行准确和一致的标注。这些困难样本在注释过程中可能无法预测地出现，需要进行可变次数的迭代和反馈循环，从而导致意想不到的费用和时间成本以保证质量。我们认为，更直接地通过领域专家的参与，使用人在循环系统，可以解决这些实践中的挑战。我们提出了一个新颖的框架，称为视频标注器（VA），用于注释、管理和迭代视频分类数据集。我们的方法为以最终用户为中心的模型开发过程提供了一种新的范式。",
    "tldr": "本论文提出了一个名为视频标注器（VA）的框架，通过利用视觉语言模型和主动学习构建视频分类器，并通过人在循环系统实现了领域专家的直接参与，解决了传统数据注释方法的资源消耗和效率低下的问题。",
    "en_tdlr": "This paper presents a framework called Video Annotator (VA), which efficiently builds video classifiers using vision-language models and active learning, and resolves the resource-intensive and inefficient challenges of traditional data annotation methods through the direct involvement of domain experts in a human-in-the-loop system."
}