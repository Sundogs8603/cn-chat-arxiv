{
    "title": "Improved Generalization of Weight Space Networks via Augmentations",
    "abstract": "Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substanti",
    "link": "https://arxiv.org/abs/2402.04081",
    "context": "Title: Improved Generalization of Weight Space Networks via Augmentations\nAbstract: Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substanti",
    "path": "papers/24/02/2402.04081.json",
    "total_tokens": 855,
    "translated_title": "通过扩充改进权重空间网络的泛化能力",
    "translated_abstract": "深度权重空间（DWS）中的学习是一个新兴的研究方向，神经网络通过处理其他神经网络的权重来进行学习，它在2D和3D神经场（INRs，NeRFs）以及对其他类型神经网络进行推理方面有广泛应用。然而，权重空间模型往往容易受到过拟合的影响。我们通过实证分析了过拟合的原因，并发现一个关键原因是DWS数据集的缺乏多样性。虽然一个给定的对象可以被许多不同的权重配置所表示，但典型的INR训练集未能捕捉到表示同一对象的不同INR之间的变异性。为了解决这个问题，我们探索了权重空间中的数据扩充策略，并提出了适用于权重空间的MixUp方法。我们在两个设置中证明了这些方法的有效性。在分类任务中，它们的性能提升类似于拥有多达10倍的数据量。在自监督对比学习中，它们产生了实质性的改进。",
    "tldr": "通过扩充权重空间的数据集，采用MixUp方法，我们改进了权重空间网络的泛化能力和性能。",
    "en_tdlr": "By augmenting the dataset in weight spaces and using the MixUp method, we improve the generalization and performance of weight space networks."
}