{
    "title": "Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards",
    "abstract": "arXiv:2402.17975v1 Announce Type: new  Abstract: Preference-based reinforcement learning (PbRL) aligns a robot behavior with human preferences via a reward function learned from binary feedback over agent behaviors. We show that dynamics-aware reward functions improve the sample efficiency of PbRL by an order of magnitude. In our experiments we iterate between: (1) learning a dynamics-aware state-action representation (z^{sa}) via a self-supervised temporal consistency task, and (2) bootstrapping the preference-based reward function from (z^{sa}), which results in faster policy learning and better final policy performance. For example, on quadruped-walk, walker-walk, and cheetah-run, with 50 preference labels we achieve the same performance as existing approaches with 500 preference labels, and we recover 83\\% and 66\\% of ground truth reward policy performance versus only 38\\% and 21\\%. The performance gains demonstrate the benefits of explicitly learning a dynamics-aware reward model.",
    "link": "https://arxiv.org/abs/2402.17975",
    "context": "Title: Sample-Efficient Preference-based Reinforcement Learning with Dynamics Aware Rewards\nAbstract: arXiv:2402.17975v1 Announce Type: new  Abstract: Preference-based reinforcement learning (PbRL) aligns a robot behavior with human preferences via a reward function learned from binary feedback over agent behaviors. We show that dynamics-aware reward functions improve the sample efficiency of PbRL by an order of magnitude. In our experiments we iterate between: (1) learning a dynamics-aware state-action representation (z^{sa}) via a self-supervised temporal consistency task, and (2) bootstrapping the preference-based reward function from (z^{sa}), which results in faster policy learning and better final policy performance. For example, on quadruped-walk, walker-walk, and cheetah-run, with 50 preference labels we achieve the same performance as existing approaches with 500 preference labels, and we recover 83\\% and 66\\% of ground truth reward policy performance versus only 38\\% and 21\\%. The performance gains demonstrate the benefits of explicitly learning a dynamics-aware reward model.",
    "path": "papers/24/02/2402.17975.json",
    "total_tokens": 915,
    "translated_title": "基于偏好的强化学习中的样本有效性及动态感知奖励",
    "translated_abstract": "基于偏好的强化学习（PbRL）通过从对代理行为的二进制反馈中学习的奖励函数将机器人行为与人类偏好对齐。我们展示了动态感知奖励函数可以将PbRL的样本效率提高一个数量级。在我们的实验中，我们在学习动态感知状态-动作表示（z^{sa））和基于偏好的奖励函数之间迭代，结果表明这可以加快策略学习并提高最终策略性能。例如，在四足行走、步行和猎豹奔跑等领域，使用50个偏好标签的性能与使用500个偏好标签的现有方法相同，并且我们恢复了83\\%和66\\%的地面真值奖励策略性能，而其他方法只有38\\%和21\\%。这些性能提升展示了明确学习动态感知奖励模型的好处。",
    "tldr": "动态感知奖励函数显著提高了基于偏好的强化学习的样本效率，实验证明只需50个偏好标签即可达到与传统方法500个偏好标签相同的性能，并且能更好地恢复地面真值奖励策略性能。",
    "en_tdlr": "Dynamics-aware reward functions significantly improve the sample efficiency of preference-based reinforcement learning, with experiments showing that achieving the same performance as traditional methods with 500 preference labels only requires 50 preference labels and better recovery of ground truth reward policy performance."
}