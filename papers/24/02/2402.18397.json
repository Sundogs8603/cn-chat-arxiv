{
    "title": "Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models",
    "abstract": "arXiv:2402.18397v1 Announce Type: new  Abstract: Despite the predominance of English in their training data, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks. Diverging from the single text-to-text prompt, our method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Further analysis reveals the influence of evaluation methods and the us",
    "link": "https://arxiv.org/abs/2402.18397",
    "context": "Title: Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models\nAbstract: arXiv:2402.18397v1 Announce Type: new  Abstract: Despite the predominance of English in their training data, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks. Diverging from the single text-to-text prompt, our method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Further analysis reveals the influence of evaluation methods and the us",
    "path": "papers/24/02/2402.18397.json",
    "total_tokens": 873,
    "translated_title": "分解提示：揭示以英语为中心的大型语言模型中的多语言语言结构知识",
    "translated_abstract": "尽管英语在它们的训练数据中占主导地位，类似GPT-3和LLaMA这样以英语为中心的大型语言模型（LLMs）表现出卓越的多语言任务能力，这引发了关于其跨语言能力深度和性质的问题。本文引入了分解提示方法，用以探究这些LLMs在序列标注任务中的语言结构理解能力。与单一文本到文本提示不同，我们的方法为输入句子的每个令牌生成一个单独的提示，询问其语言标签。我们在38种语言的通用依赖词性标注数据集上评估了我们的方法，利用了以英语为中心和多语言LLMs。我们的研究结果表明，分解提示在零次和少次迭代设置下的效力和效率均超过了迭代提示基线。进一步的分析揭示了评估方法和他们之间的影响。",
    "tldr": "通过分解提示方法，这项研究揭示了以英语为中心的大型语言模型在多语言任务上的语言结构理解能力，证实其在零次和少次迭代设置中的高效性和效率。",
    "en_tdlr": "This study reveals the linguistic structure understanding ability of English-centric large language models in multilingual tasks using the decomposed prompting approach, demonstrating its efficacy and efficiency under zero- and few-shot settings."
}