{
    "title": "Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology",
    "abstract": "arXiv:2402.15690v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have gradually become the gateway for people to acquire new knowledge. However, attackers can break the model's security protection (\"jail\") to access restricted information, which is called \"jailbreaking.\" Previous studies have shown the weakness of current LLMs when confronted with such jailbreaking attacks. Nevertheless, comprehension of the intrinsic decision-making mechanism within the LLMs upon receipt of jailbreak prompts is noticeably lacking. Our research provides a psychological explanation of the jailbreak prompts. Drawing on cognitive consistency theory, we argue that the key to jailbreak is guiding the LLM to achieve cognitive coordination in an erroneous direction. Further, we propose an automatic black-box jailbreaking method based on the Foot-in-the-Door (FITD) technique. This method progressively induces the model to answer harmful questions via multi-step incremental prompts. We instantiat",
    "link": "https://arxiv.org/abs/2402.15690",
    "context": "Title: Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology\nAbstract: arXiv:2402.15690v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have gradually become the gateway for people to acquire new knowledge. However, attackers can break the model's security protection (\"jail\") to access restricted information, which is called \"jailbreaking.\" Previous studies have shown the weakness of current LLMs when confronted with such jailbreaking attacks. Nevertheless, comprehension of the intrinsic decision-making mechanism within the LLMs upon receipt of jailbreak prompts is noticeably lacking. Our research provides a psychological explanation of the jailbreak prompts. Drawing on cognitive consistency theory, we argue that the key to jailbreak is guiding the LLM to achieve cognitive coordination in an erroneous direction. Further, we propose an automatic black-box jailbreaking method based on the Foot-in-the-Door (FITD) technique. This method progressively induces the model to answer harmful questions via multi-step incremental prompts. We instantiat",
    "path": "papers/24/02/2402.15690.json",
    "total_tokens": 830,
    "translated_title": "踏进大型语言模型“越狱”的认知心理学",
    "translated_abstract": "大型语言模型（LLMs）渐渐成为人们获取新知识的入口。然而，攻击者可以打破模型的安全保护（“监狱”）以访问受限信息，这称为“越狱”。先前的研究显示了当前LLMs在面对此类越狱攻击时的薄弱性。然而，对LLMs在接收越狱提示时内在决策机制的理解明显欠缺。我们的研究提供了越狱提示的心理解释。借鉴认知一致性理论，我们认为越狱的关键是引导LLMs在错误方向上实现认知协调。此外，我们提出了一种基于门脚-门的自动黑盒越狱方法。这种方法逐步诱导模型通过多步增量提示回答有害问题。",
    "tldr": "该研究通过认知一致性理论为大型语言模型的越狱提示提供了心理解释，并提出了一种基于门脚-门技术的自动黑盒越狱方法。",
    "en_tdlr": "This study provides a psychological explanation for jailbreak prompts in large language models based on cognitive consistency theory and proposes an automatic black-box jailbreaking method using the Foot-in-the-Door technique."
}