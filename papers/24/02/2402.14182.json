{
    "title": "Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization",
    "abstract": "arXiv:2402.14182v1 Announce Type: cross  Abstract: Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability. Informally, we lack a formulaic or intuitive understanding of what and how models learn from code. Explainability of language models can be partially provided if, as the models learn to produce higher-quality code summaries, they also align in deeming the same code parts important as those identified by human programmers. In this paper, we report negative results from our investigation of explainability of language models in code summarization through the lens of human comprehension. We measure human focus on code using eye-tracking metrics such as fixation counts and duration in code summarization tasks. To approximate language model focus, we employ a state-of-the-art model-agnostic, black-box, perturbation-based approach, SHAP (SHapley Additive",
    "link": "https://arxiv.org/abs/2402.14182",
    "context": "Title: Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization\nAbstract: arXiv:2402.14182v1 Announce Type: cross  Abstract: Recent language models have demonstrated proficiency in summarizing source code. However, as in many other domains of machine learning, language models of code lack sufficient explainability. Informally, we lack a formulaic or intuitive understanding of what and how models learn from code. Explainability of language models can be partially provided if, as the models learn to produce higher-quality code summaries, they also align in deeming the same code parts important as those identified by human programmers. In this paper, we report negative results from our investigation of explainability of language models in code summarization through the lens of human comprehension. We measure human focus on code using eye-tracking metrics such as fixation counts and duration in code summarization tasks. To approximate language model focus, we employ a state-of-the-art model-agnostic, black-box, perturbation-based approach, SHAP (SHapley Additive",
    "path": "papers/24/02/2402.14182.json",
    "total_tokens": 887,
    "translated_title": "机器和人类是否关注相似的代码？探索大型语言模型在代码总结中的解释性",
    "translated_abstract": "最近的语言模型展示了在总结源代码方面的熟练能力。然而，与许多其他机器学习领域一样，代码的语言模型缺乏足够的解释性。我们非正式地缺乏对模型如何从代码中学习以及学习了什么的公式化或直觉性理解。如果语言模型学会生成更高质量的代码总结，那么它们在认为相同的代码部分重要时也与人类程序员所识别的部分相一致，可以在一定程度上提供语言模型的解释性。在本文中，我们通过人类理解的视角，报告了我们对语言模型在代码总结中解释性的调查的负面结果。我们使用眼动追踪指标（如注视次数和代码总结任务中的停留时间）来衡量人类对代码的关注。为了近似语言模型的关注重点，我们采用了一种最先进的模型无关、黑盒、基于扰动的方法——SHAP（SHapley Additive）。",
    "tldr": "本研究通过眼动追踪指标和 SHAP 方法对比了人类与语言模型在代码总结中的关注重点，展示了语言模型解释性的负面结果。"
}