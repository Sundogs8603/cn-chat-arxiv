{
    "title": "NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms",
    "abstract": "arXiv:2402.12261v1 Announce Type: new  Abstract: The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks",
    "link": "https://arxiv.org/abs/2402.12261",
    "context": "Title: NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms\nAbstract: arXiv:2402.12261v1 Announce Type: new  Abstract: The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks",
    "path": "papers/24/02/2402.12261.json",
    "total_tokens": 936,
    "translated_title": "NEO-BENCH：使用新词评估大型语言模型的鲁棒性",
    "translated_abstract": "Large Language Models (LLMs)的表现会因模型训练数据与推理过程中看到的新文本之间的时间漂移而退化。本文探讨了导致数据漂移的语言变化中一个不太被研究的方向，即随着时间推移而出现的新词形式——新词。我们通过使用几种流行的收集方法创建了一个多样化的最新英语新词资源。我们通过比较包含新词的句子与将新词替换为现有替代词的几乎相同的句子来分析新词对时间漂移的影响。在句子中引入单个新词时，机器翻译中的模型性能几乎减半。受到这些结果的启发，我们构建了一个基准来评估LLMs对不同自然语言理解任务和模型困惑度中新词的泛化能力。后期知识截止日期的模型产生较低的困惑度，并在下游任务中表现更好。",
    "tldr": "本研究通过创建一个多样化的最新英语新词资源，分析了大型语言模型对于新词的鲁棒性表现，并构建了一个基准用于评估模型对新词的泛化能力，结果显示机器翻译中模型性能会因引入新词而几乎减半。",
    "en_tdlr": "This study examines the robustness of large language models to neologisms by creating a diverse resource of recent English neologisms, constructing a benchmark to evaluate model generalization to new words, and finds that model performance in machine translation is nearly halved when introducing a single neologism."
}