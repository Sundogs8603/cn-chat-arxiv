{
    "title": "Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment",
    "abstract": "arXiv:2402.14016v1 Announce Type: new  Abstract: Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5",
    "link": "https://arxiv.org/abs/2402.14016",
    "context": "Title: Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment\nAbstract: arXiv:2402.14016v1 Announce Type: new  Abstract: Large Language Models (LLMs) are powerful zero-shot assessors and are increasingly used in real-world situations such as for written exams or benchmarking systems. Despite this, no existing work has analyzed the vulnerability of judge-LLMs against adversaries attempting to manipulate outputs. This work presents the first study on the adversarial robustness of assessment LLMs, where we search for short universal phrases that when appended to texts can deceive LLMs to provide high assessment scores. Experiments on SummEval and TopicalChat demonstrate that both LLM-scoring and pairwise LLM-comparative assessment are vulnerable to simple concatenation attacks, where in particular LLM-scoring is very susceptible and can yield maximum assessment scores irrespective of the input text quality. Interestingly, such attacks are transferable and phrases learned on smaller open-source LLMs can be applied to larger closed-source models, such as GPT3.5",
    "path": "papers/24/02/2402.14016.json",
    "total_tokens": 928,
    "translated_title": "LLM作为评判者是否稳健？研究通用对抗攻击对零样点LLM评估的影响",
    "translated_abstract": "大型语言模型（LLMs）是强大的零样点评估者，在实际场景中越来越多地被用于笔试或系统基准测试等情境。尽管如此，目前还没有研究分析对抗试图操纵输出的评判LLMs的脆弱性的工作。这项工作提出了对评估LLMs的对抗鲁棒性的第一项研究，我们寻找短通用短语，当附加到文本时可以欺骗LLMs提供高评分。在SummEval和TopicalChat上的实验表明，LLM评分和两两LLM比较评估都容易受到简单的串联攻击的影响，尤其是LLM评分非常容易受到影响，可以产生最高评分，而不考虑输入文本的质量。有趣的是，这些攻击是可传递的，学到的短语可以应用于更大的封闭源模型，如GPT3.5",
    "tldr": "该研究研究了评估LLM的对抗鲁棒性，发现短通用短语可以欺骗LLMs提供高评分，这种攻击对于从简单的串联攻击到转移学习都是有效的。",
    "en_tdlr": "This study investigates the adversarial robustness of assessment LLMs, finding that short universal phrases can deceive LLMs into providing high assessment scores, with the attacks being effective from simple concatenation attacks to transfer learning."
}