{
    "title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation",
    "abstract": "arXiv:2402.10631v1 Announce Type: new  Abstract: The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit conf",
    "link": "https://arxiv.org/abs/2402.10631",
    "context": "Title: BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation\nAbstract: arXiv:2402.10631v1 Announce Type: new  Abstract: The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit conf",
    "path": "papers/24/02/2402.10631.json",
    "total_tokens": 974,
    "translated_title": "BitDistiller: 通过自蒸馏释放低于4位LLMs的潜力",
    "translated_abstract": "arXiv:2402.10631v1 公告类型: 新内容 摘要: 大型语言模型（LLMs）的升级取得了自然语言处理领域的重大进展，但也带来了重大部署挑战。权重量化已经成为减少内存和计算需求的普遍接受的解决方案。本文介绍了BitDistiller，这是一个将量化感知训练（QAT）与知识蒸馏（KD）相结合的框架，以提升LLMs在极低精度（低于4位）下的性能。具体而言，BitDistiller首先采用了一种定制的非对称量化和剪裁技术，以最大限度地保留量化权重的保真度，然后提出了一种新颖的置信感知Kullback-Leibler散度（CAKLD）目标，以自蒸馏的方式实现更快的收敛和卓越的模型性能。实证评估表明，BitDistiller在3位和2位情况下明显超越了现有方法。",
    "tldr": "BitDistiller框架将量化感知训练（QAT）与知识蒸馏（KD）相结合，通过引入定制的量化和剪裁技术以及置信感知Kullback-Leibler散度（CAKLD）目标，实现了在极低精度下（低于4位）提升LLMs性能。",
    "en_tdlr": "BitDistiller framework combines Quantization-Aware Training (QAT) with Knowledge Distillation (KD), introducing customized quantization and clipping techniques and Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective to enhance the performance of LLMs at ultra-low precisions (sub-4-bit)."
}