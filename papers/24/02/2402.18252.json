{
    "title": "Towards Generalist Prompting for Large Language Models by Mental Models",
    "abstract": "arXiv:2402.18252v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance on many tasks. However, to achieve optimal performance, specially designed prompting methods are still needed. These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks. In this work, we attempt to introduce the concept of generalist prompting, which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of prompts tailored to specific problems. Furthermore, we propose MeMo (Mental Models), an innovative prompting method that is simple-designed yet effectively fulfills the criteria of generalist prompting. MeMo distills the cores of various prompting methods into individual mental models and allows LLMs to autonomously select",
    "link": "https://arxiv.org/abs/2402.18252",
    "context": "Title: Towards Generalist Prompting for Large Language Models by Mental Models\nAbstract: arXiv:2402.18252v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance on many tasks. However, to achieve optimal performance, specially designed prompting methods are still needed. These methods either rely on task-specific few-shot examples that require a certain level of domain knowledge, or are designed to be simple but only perform well on a few types of tasks. In this work, we attempt to introduce the concept of generalist prompting, which operates on the design principle of achieving optimal or near-optimal performance on a wide range of tasks while eliminating the need for manual selection and customization of prompts tailored to specific problems. Furthermore, we propose MeMo (Mental Models), an innovative prompting method that is simple-designed yet effectively fulfills the criteria of generalist prompting. MeMo distills the cores of various prompting methods into individual mental models and allows LLMs to autonomously select",
    "path": "papers/24/02/2402.18252.json",
    "total_tokens": 833,
    "translated_title": "通过心智模型实现大型语言模型的通用提示方法",
    "translated_abstract": "大型语言模型（LLMs）在许多任务上展示出令人印象深刻的性能。然而，为了达到最佳性能，仍然需要特别设计的提示方法。这些方法要么依赖于需要一定领域知识的特定任务少量示例，要么被设计为简单，但只对少数类型任务表现良好。在这项工作中，我们尝试引入通用提示的概念，它的设计原则是在广泛任务范围内实现最佳或接近最佳的性能，同时消除了需要针对特定问题手动选择和定制提示的需求。此外，我们提出了MeMo（心智模型），这是一种简单设计的创新提示方法，能有效地实现通用提示的标准。MeMo将各种提示方法的核心精髓提炼为单个心智模型，并允许LLM自主选择。",
    "tldr": "通过提出通用提示概念和创新的MeMo（心智模型）方法，实现大型语言模型在广泛任务范围内实现最佳性能，消除了手动选择和定制提示的需求。",
    "en_tdlr": "Introducing the concept of generalist prompting and the innovative MeMo (Mental Models) method to achieve optimal performance across a wide range of tasks for large language models, eliminating the need for manual selection and customization of prompts."
}