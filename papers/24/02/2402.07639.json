{
    "title": "Tighter Bounds on the Information Bottleneck with Application to Deep Learning",
    "abstract": "Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space. The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks. This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs. These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs.",
    "link": "https://arxiv.org/abs/2402.07639",
    "context": "Title: Tighter Bounds on the Information Bottleneck with Application to Deep Learning\nAbstract: Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space. The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks. This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs. These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs.",
    "path": "papers/24/02/2402.07639.json",
    "total_tokens": 841,
    "translated_title": "对信息瓶颈的限制更紧的界限，并应用于深度学习",
    "translated_abstract": "深度神经网络（DNNs）通过下游任务、目标函数和其他参数来学习引发的潜在表示。学习到的表示的质量影响着DNN的概括能力和新出现的潜在空间的连贯性。信息瓶颈（IB）提供了一种理论上最优的数据建模框架，但通常是难以处理的。最近的研究工作将DNNs与IB相结合，通过应用VAE-inspire的变分方法来近似相互信息的界限，从而提高对抗攻击的鲁棒性。本文引入了一种新的和更紧的变分界限，提高了以前IB-inspire DNNs的性能。这些进展加强了IB及其变分近似作为数据模型框架的论点，并为分类器DNNs的对抗鲁棒性提供了一种简单的方法。",
    "tldr": "这个论文提出了一个对信息瓶颈更为紧密的变分界限，可以改善以前的基于信息瓶颈的DNNs的性能，并提供了一种简单方法来显著增强分类器DNNs的对抗鲁棒性。",
    "en_tdlr": "This paper introduces a new and tighter variational bound for the Information Bottleneck, improving the performance of previous DNNs inspired by the Information Bottleneck. It also provides a simple method to significantly enhance the adversarial robustness of classifier DNNs."
}