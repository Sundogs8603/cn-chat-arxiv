{
    "title": "Flexible Variational Information Bottleneck: Achieving Diverse Compression with a Single Training",
    "abstract": "Information Bottleneck (IB) is a widely used framework that enables the extraction of information related to a target random variable from a source random variable. In the objective function, IB controls the trade-off between data compression and predictiveness through the Lagrange multiplier $\\beta$. Traditionally, to find the trade-off to be learned, IB requires a search for $\\beta$ through multiple training cycles, which is computationally expensive. In this study, we introduce Flexible Variational Information Bottleneck (FVIB), an innovative framework for classification task that can obtain optimal models for all values of $\\beta$ with single, computationally efficient training. We theoretically demonstrate that across all values of reasonable $\\beta$, FVIB can simultaneously maximize an approximation of the objective function for Variational Information Bottleneck (VIB), the conventional IB method. Then we empirically show that FVIB can learn the VIB objective as effectively as VI",
    "link": "https://rss.arxiv.org/abs/2402.01238",
    "context": "Title: Flexible Variational Information Bottleneck: Achieving Diverse Compression with a Single Training\nAbstract: Information Bottleneck (IB) is a widely used framework that enables the extraction of information related to a target random variable from a source random variable. In the objective function, IB controls the trade-off between data compression and predictiveness through the Lagrange multiplier $\\beta$. Traditionally, to find the trade-off to be learned, IB requires a search for $\\beta$ through multiple training cycles, which is computationally expensive. In this study, we introduce Flexible Variational Information Bottleneck (FVIB), an innovative framework for classification task that can obtain optimal models for all values of $\\beta$ with single, computationally efficient training. We theoretically demonstrate that across all values of reasonable $\\beta$, FVIB can simultaneously maximize an approximation of the objective function for Variational Information Bottleneck (VIB), the conventional IB method. Then we empirically show that FVIB can learn the VIB objective as effectively as VI",
    "path": "papers/24/02/2402.01238.json",
    "total_tokens": 879,
    "translated_title": "灵活的变分信息瓶颈：通过一次训练实现多样化压缩",
    "translated_abstract": "信息瓶颈是一种广泛使用的框架，可以从源随机变量中提取与目标随机变量相关的信息。在目标函数中，通过拉格朗日乘子β，信息瓶颈控制数据压缩和预测性之间的权衡。传统上，为了找到要学习的权衡，信息瓶颈需要通过多个训练周期来搜索β，这在计算上是昂贵的。在本研究中，我们引入了灵活的变分信息瓶颈（FVIB），这是一种用于分类任务的创新框架，可以通过单次计算高效的训练来获得所有β值的最优模型。我们在理论上证明，FVIB可以在合理的β值范围内同时最大化变分信息瓶颈（VIB）的目标函数的近似。然后，我们通过实验证明了FVIB可以像VI一样有效地学习VIB的目标。",
    "tldr": "本研究引入了灵活的变分信息瓶颈（FVIB）框架，通过单次训练即可获得所有β值的最优模型，从而实现多样化压缩，并且在理论和实证方面证明了它的有效性。",
    "en_tdlr": "This study introduces the Flexible Variational Information Bottleneck (FVIB) framework, which achieves diverse compression with a single training by obtaining optimal models for all values of β. The effectiveness of FVIB is theoretically and empirically demonstrated."
}