{
    "title": "Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation",
    "abstract": "arXiv:2402.11907v1 Announce Type: new  Abstract: Aligning large language models (LLMs) with human expectations without human-annotated preference data is an important problem. In this paper, we propose a method to evaluate the response preference by using the output probabilities of response pairs under contrastive prompt pairs, which could achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based on this, we propose an automatic alignment method, Direct Large Model Alignment (DLMA). First, we use contrastive prompt pairs to automatically generate preference data. Then, we continue to evaluate the generated preference data using contrastive prompt pairs and calculate a self-rewarding score. Finally, we use the DPO algorithm to effectively align LLMs by combining this self-rewarding score. In the experimental stage, our DLMA method could surpass the \\texttt{RLHF} method without relying on human-annotated preference data.",
    "link": "https://arxiv.org/abs/2402.11907",
    "context": "Title: Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation\nAbstract: arXiv:2402.11907v1 Announce Type: new  Abstract: Aligning large language models (LLMs) with human expectations without human-annotated preference data is an important problem. In this paper, we propose a method to evaluate the response preference by using the output probabilities of response pairs under contrastive prompt pairs, which could achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based on this, we propose an automatic alignment method, Direct Large Model Alignment (DLMA). First, we use contrastive prompt pairs to automatically generate preference data. Then, we continue to evaluate the generated preference data using contrastive prompt pairs and calculate a self-rewarding score. Finally, we use the DPO algorithm to effectively align LLMs by combining this self-rewarding score. In the experimental stage, our DLMA method could surpass the \\texttt{RLHF} method without relying on human-annotated preference data.",
    "path": "papers/24/02/2402.11907.json",
    "total_tokens": 820,
    "translated_title": "通过自我奖励对比提示精炼直接对齐大型语言模型",
    "translated_abstract": "在这篇论文中，我们提出了一种方法，通过使用对比提示对响应对的输出概率进行评估，从而在LLaMA2-7B和LLaMA2-13B上实现了比RLAIF更好的性能。基于此，我们提出了一种自动对齐方法，即直接大型模型对齐（DLMA）。首先，我们使用对比提示对自动生成的偏好数据。然后，我们继续使用对比提示对生成的偏好数据进行评估并计算自我奖励分数。最后，我们使用DPO算法通过结合这种自我奖励分数来有效地对齐LLMs。在实验阶段，我们的DLMA方法能够在不依赖人工注释的偏好数据的情况下超越RLHF方法。",
    "tldr": "通过对比提示自我奖励方法，提出了一种直接对齐大型语言模型的自动对齐方法，无需依赖人工注释的偏好数据，在实验中表现优于现有方法RLHF。",
    "en_tdlr": "Proposed an automatic alignment method for directly aligning large language models through self-rewarding contrastive prompt distillation, outperforming existing method RLHF in experiments without relying on human-annotated preference data."
}