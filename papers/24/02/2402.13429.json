{
    "title": "Everything You Always Wanted to Know About Storage Compressibility of Pre-Trained ML Models but Were Afraid to Ask",
    "abstract": "arXiv:2402.13429v1 Announce Type: cross  Abstract: As the number of pre-trained machine learning (ML) models is growing exponentially, data reduction tools are not catching up. Existing data reduction techniques are not specifically designed for pre-trained model (PTM) dataset files. This is largely due to a lack of understanding of the patterns and characteristics of these datasets, especially those relevant to data reduction and compressibility.   This paper presents the first, exhaustive analysis to date of PTM datasets on storage compressibility. Our analysis spans different types of data reduction and compression techniques, from hash-based data deduplication, data similarity detection, to dictionary-coding compression. Our analysis explores these techniques at three data granularity levels, from model layers, model chunks, to model parameters. We draw new observations that indicate that modern data reduction tools are not effective when handling PTM datasets. There is a pressing ",
    "link": "https://arxiv.org/abs/2402.13429",
    "context": "Title: Everything You Always Wanted to Know About Storage Compressibility of Pre-Trained ML Models but Were Afraid to Ask\nAbstract: arXiv:2402.13429v1 Announce Type: cross  Abstract: As the number of pre-trained machine learning (ML) models is growing exponentially, data reduction tools are not catching up. Existing data reduction techniques are not specifically designed for pre-trained model (PTM) dataset files. This is largely due to a lack of understanding of the patterns and characteristics of these datasets, especially those relevant to data reduction and compressibility.   This paper presents the first, exhaustive analysis to date of PTM datasets on storage compressibility. Our analysis spans different types of data reduction and compression techniques, from hash-based data deduplication, data similarity detection, to dictionary-coding compression. Our analysis explores these techniques at three data granularity levels, from model layers, model chunks, to model parameters. We draw new observations that indicate that modern data reduction tools are not effective when handling PTM datasets. There is a pressing ",
    "path": "papers/24/02/2402.13429.json",
    "total_tokens": 718,
    "translated_title": "有关预训练机器学习模型存储可压缩性的一切你想知道但却不敢询问的问题",
    "translated_abstract": "随着预训练机器学习（ML）模型数量呈指数增长，数据缩减工具却未能跟上。现有的数据缩减技术并非专门针对预训练模型（PTM）数据集文件而设计。这在很大程度上是由于对这些数据集的模式和特征的理解不足，尤其是与数据缩减和可压缩性相关的特征。",
    "tldr": "这项研究是关于对预训练机器学习模型数据集的存储可压缩性进行首次全面分析，发现现代数据缩减工具在处理PTM数据集时并不有效。",
    "en_tdlr": "This research provides the first comprehensive analysis of the storage compressibility of pre-trained machine learning model datasets, revealing that modern data reduction tools are not effective in handling PTM datasets."
}