{
    "title": "Nearly Optimal Regret for Decentralized Online Convex Optimization",
    "abstract": "arXiv:2402.09173v1 Announce Type: new Abstract: We investigate decentralized online convex optimization (D-OCO), in which a set of local learners are required to minimize a sequence of global loss functions using only local computations and communications. Previous studies have established $O(n^{5/4}\\rho^{-1/2}\\sqrt{T})$ and ${O}(n^{3/2}\\rho^{-1}\\log T)$ regret bounds for convex and strongly convex functions respectively, where $n$ is the number of local learners, $\\rho<1$ is the spectral gap of the communication matrix, and $T$ is the time horizon. However, there exist large gaps from the existing lower bounds, i.e., $\\Omega(n\\sqrt{T})$ for convex functions and $\\Omega(n)$ for strongly convex functions. To fill these gaps, in this paper, we first develop novel D-OCO algorithms that can respectively reduce the regret bounds for convex and strongly convex functions to $\\tilde{O}(n\\rho^{-1/4}\\sqrt{T})$ and $\\tilde{O}(n\\rho^{-1/2}\\log T)$. The primary technique is to design an online acce",
    "link": "https://arxiv.org/abs/2402.09173",
    "context": "Title: Nearly Optimal Regret for Decentralized Online Convex Optimization\nAbstract: arXiv:2402.09173v1 Announce Type: new Abstract: We investigate decentralized online convex optimization (D-OCO), in which a set of local learners are required to minimize a sequence of global loss functions using only local computations and communications. Previous studies have established $O(n^{5/4}\\rho^{-1/2}\\sqrt{T})$ and ${O}(n^{3/2}\\rho^{-1}\\log T)$ regret bounds for convex and strongly convex functions respectively, where $n$ is the number of local learners, $\\rho<1$ is the spectral gap of the communication matrix, and $T$ is the time horizon. However, there exist large gaps from the existing lower bounds, i.e., $\\Omega(n\\sqrt{T})$ for convex functions and $\\Omega(n)$ for strongly convex functions. To fill these gaps, in this paper, we first develop novel D-OCO algorithms that can respectively reduce the regret bounds for convex and strongly convex functions to $\\tilde{O}(n\\rho^{-1/4}\\sqrt{T})$ and $\\tilde{O}(n\\rho^{-1/2}\\log T)$. The primary technique is to design an online acce",
    "path": "papers/24/02/2402.09173.json",
    "total_tokens": 951,
    "translated_title": "基于分布式在线凸优化的近似最优后悔算法",
    "translated_abstract": "我们研究了分布式在线凸优化(D-OCO)，其中一组本地学习器需要使用仅限于本地计算和通信的方法来最小化一系列全局损失函数。以前的研究已经确定了针对凸函数和强凸函数的后悔界限分别为$O(n^{5/4}\\rho^{-1/2}\\sqrt{T})$和${O}(n^{3/2}\\rho^{-1}\\log T)$，其中$n$是本地学习器的数量，$\\rho<1$是通信矩阵的谱间隙，$T$是时间段。然而，对于凸函数存在着较大的间隙，即凸函数的下界为$\\Omega(n\\sqrt{T})$，强凸函数的下界为$\\Omega(n)$。为了填补这些间隙，本文首先开发了新的D-OCO算法，将凸函数和强凸函数的后悔边界分别降低到$\\tilde{O}(n\\rho^{-1/4}\\sqrt{T})$和$\\tilde{O}(n\\rho^{-1/2}\\log T)$。主要技术是设计一种在线可进取的算法。",
    "tldr": "本论文研究了分布式在线凸优化，开发了新的算法来分别降低凸函数和强凸函数的后悔边界，并填补了现有下界之间的差距。",
    "en_tdlr": "This paper investigates decentralized online convex optimization (D-OCO) and develops novel algorithms to reduce regret bounds for convex and strongly convex functions, filling the gaps between existing lower bounds."
}