{
    "title": "Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate",
    "abstract": "Fact-checking research has extensively explored verification but less so the generation of natural-language explanations, crucial for user trust. While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness. To address these challenges, we propose the Multi-Agent Debate Refinement (MADR) framework, leveraging multiple LLMs as agents with diverse roles in an iterative refining process aimed at enhancing faithfulness in generated explanations. MADR ensures that the final explanation undergoes rigorous validation, significantly reducing the likelihood of unfaithful elements and aligning closely with the provided evidence. Experimental results demonstrate that MADR significantly improves the faithfulness of LLM-generated explanations to the evidence, advancing the credib",
    "link": "https://arxiv.org/abs/2402.07401",
    "context": "Title: Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate\nAbstract: Fact-checking research has extensively explored verification but less so the generation of natural-language explanations, crucial for user trust. While Large Language Models (LLMs) excel in text generation, their capability for producing faithful explanations in fact-checking remains underexamined. Our study investigates LLMs' ability to generate such explanations, finding that zero-shot prompts often result in unfaithfulness. To address these challenges, we propose the Multi-Agent Debate Refinement (MADR) framework, leveraging multiple LLMs as agents with diverse roles in an iterative refining process aimed at enhancing faithfulness in generated explanations. MADR ensures that the final explanation undergoes rigorous validation, significantly reducing the likelihood of unfaithful elements and aligning closely with the provided evidence. Experimental results demonstrate that MADR significantly improves the faithfulness of LLM-generated explanations to the evidence, advancing the credib",
    "path": "papers/24/02/2402.07401.json",
    "total_tokens": 1020,
    "translated_title": "能够为事实核查提供忠实解释吗？通过多智能体辩论实现忠实可解释的事实核查",
    "translated_abstract": "事实核查研究广泛探讨了验证方法，但对于生成自然语言解释的研究相对较少，而这对于用户的信任至关重要。虽然大型语言模型在文本生成方面表现出色，但它们在事实核查中生成忠实解释的能力仍未得到充分研究。我们的研究调查了大型语言模型生成这种解释的能力，发现零-shot提示往往导致不忠实的结果。为了解决这些挑战，我们提出了多智能体辩论优化（MADR）框架，利用多个大型语言模型作为代理人，在迭代的优化过程中发挥各自不同的角色，目标是增强生成解释的忠实性。MADR确保最终解释经过严格验证，显著减少了不忠实因素的可能性，并与提供的证据密切对齐。实验结果表明，MADR显著提高了大型语言模型生成的解释与证据的一致性，推动了可信的事实核查方法的发展。",
    "tldr": "本研究调查了大型语言模型在事实核查中生成忠实解释的能力，并发现了零-shot提示常常导致不忠实的结果。为了解决这个问题，我们提出了多智能体辩论优化（MADR）框架，通过迭代的优化过程，利用多个大型语言模型作为代理人，从而显著提高了生成解释的忠实性。",
    "en_tdlr": "This study investigates the capability of Large Language Models (LLMs) to generate faithful explanations in fact-checking and finds that zero-shot prompts often lead to unfaithful results. To address this issue, the Multi-Agent Debate Refinement (MADR) framework is proposed, which leverages multiple LLMs as agents in an iterative optimizing process, significantly improving the faithfulness of generated explanations."
}