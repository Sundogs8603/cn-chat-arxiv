{
    "title": "Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment",
    "abstract": "In recent years, Deep Neural Network models have been developed in different fields, where they have brought many advances. However, they have also started to be used in tasks where risk is critical. A misdiagnosis of these models can lead to serious accidents or even death. This concern has led to an interest among researchers to study possible attacks on these models, discovering a long list of vulnerabilities, from which every model should be defended. The adversarial example attack is a widely known attack among researchers, who have developed several defenses to avoid such a threat. However, these defenses are as opaque as a deep neural network model, how they work is still unknown. This is why visualizing how they change the behavior of the target model is interesting in order to understand more precisely how the performance of the defended model is being modified. For this work, some defenses, against adversarial example attack, have been selected in order to visualize the behav",
    "link": "https://arxiv.org/abs/2402.07496",
    "context": "Title: Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment\nAbstract: In recent years, Deep Neural Network models have been developed in different fields, where they have brought many advances. However, they have also started to be used in tasks where risk is critical. A misdiagnosis of these models can lead to serious accidents or even death. This concern has led to an interest among researchers to study possible attacks on these models, discovering a long list of vulnerabilities, from which every model should be defended. The adversarial example attack is a widely known attack among researchers, who have developed several defenses to avoid such a threat. However, these defenses are as opaque as a deep neural network model, how they work is still unknown. This is why visualizing how they change the behavior of the target model is interesting in order to understand more precisely how the performance of the defended model is being modified. For this work, some defenses, against adversarial example attack, have been selected in order to visualize the behav",
    "path": "papers/24/02/2402.07496.json",
    "total_tokens": 896,
    "translated_title": "通过可视化动态风险评估来理解深度学习对抗对抗样本的防御",
    "translated_abstract": "近年来，深度神经网络模型已经在不同领域取得了许多进展。然而，它们也开始在风险关键的任务中使用。对这些模型的误诊可能导致严重事故甚至死亡。这个问题引起了研究人员的兴趣，他们研究了可能对这些模型进行攻击的攻击，并发现了一长串漏洞，每个模型都应该进行防御。对抗性样本攻击是研究人员广为知晓的一种攻击，他们已经开发了几种防御方法来避免这种威胁。然而，这些防御方法像深度神经网络模型一样不透明，它们的工作方式仍然未知。这就是为什么通过可视化它们如何改变目标模型的行为来了解被防御模型的性能如何被修改是有趣的。在本工作中，选择了一些对抗性样本攻击的防御方法，以便可以对其行为进行可视化分析。",
    "tldr": "通过对深度学习模型的可视化分析，研究了对抗样本攻击的防御方法，以更加精确地了解防御模型的性能如何被修改。",
    "en_tdlr": "This work visualizes the behavior of defended models against adversarial example attacks to understand how the performance is modified, providing insights into defense methods."
}