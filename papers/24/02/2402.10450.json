{
    "title": "PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem",
    "abstract": "arXiv:2402.10450v1 Announce Type: new  Abstract: Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of both multitask imitation learning as well as few-shot imitation learning on unseen tasks. Our code will be released at https:/",
    "link": "https://arxiv.org/abs/2402.10450",
    "context": "Title: PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem\nAbstract: arXiv:2402.10450v1 Announce Type: new  Abstract: Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of both multitask imitation learning as well as few-shot imitation learning on unseen tasks. Our code will be released at https:/",
    "path": "papers/24/02/2402.10450.json",
    "total_tokens": 862,
    "translated_title": "PRISE：将时间动作抽象视为序列压缩问题",
    "translated_abstract": "时间动作抽象以及信念状态表示是序贯决策中的强大知识共享机制。本文提出了一个新颖的观点，将诱导时间动作抽象视为序列压缩问题。为此，我们将LLM训练流水线的一个微妙但至关重要的组成部分 -- 输入标记化通过字节对编码（BPE） -- 带到了连续控制领域中学习可变时间跨度技能的 seemingly distant 任务。我们引入一种称为Primitive Sequence Encoding（PRISE）的方法，该方法将连续动作量化与BPE相结合，学习强大的动作抽象。我们通过实验证明，PRISE从一组机器人操作演示中发现的高级技能显著提升了多任务模仿学习以及在未见任务上的少样本模仿学习的性能。我们的代码将在 https: 放出",
    "tldr": "将时间动作抽象视为序列压缩问题，使用Primitive Sequence Encoding (PRISE)方法结合连续动作量化与BPE来学习强大的动作抽象，并在多任务模仿学习和少样本模仿学习中取得显著性能提升",
    "en_tdlr": "Treating temporal action abstractions as a sequence compression problem, utilizing Primitive Sequence Encoding (PRISE) method that combines continuous action quantization with BPE to learn powerful action abstractions, resulting in significant performance improvements in multitask imitation learning and few-shot imitation learning."
}