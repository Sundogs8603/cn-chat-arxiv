{
    "title": "Towards Efficient Active Learning in NLP via Pretrained Representations",
    "abstract": "arXiv:2402.15613v1 Announce Type: cross  Abstract: Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications. When labeled documents are scarce, active learning helps save annotation efforts but requires retraining of massive models on each acquisition iteration. We drastically expedite this process by using pretrained representations of LLMs within the active learning loop and, once the desired amount of labeled data is acquired, fine-tuning that or even a different pretrained LLM on this labeled data to achieve the best performance. As verified on common text classification benchmarks with pretrained BERT and RoBERTa as the backbone, our strategy yields similar performance to fine-tuning all the way through the active learning loop but is orders of magnitude less computationally expensive. The data acquired with our procedure generalizes across pretrained networks, allowing flexibility in choosing the final model or upda",
    "link": "https://arxiv.org/abs/2402.15613",
    "context": "Title: Towards Efficient Active Learning in NLP via Pretrained Representations\nAbstract: arXiv:2402.15613v1 Announce Type: cross  Abstract: Fine-tuning Large Language Models (LLMs) is now a common approach for text classification in a wide range of applications. When labeled documents are scarce, active learning helps save annotation efforts but requires retraining of massive models on each acquisition iteration. We drastically expedite this process by using pretrained representations of LLMs within the active learning loop and, once the desired amount of labeled data is acquired, fine-tuning that or even a different pretrained LLM on this labeled data to achieve the best performance. As verified on common text classification benchmarks with pretrained BERT and RoBERTa as the backbone, our strategy yields similar performance to fine-tuning all the way through the active learning loop but is orders of magnitude less computationally expensive. The data acquired with our procedure generalizes across pretrained networks, allowing flexibility in choosing the final model or upda",
    "path": "papers/24/02/2402.15613.json",
    "total_tokens": 821,
    "translated_title": "通过预训练表示实现在NLP中的高效主动学习",
    "translated_abstract": "大型语言模型（LLMs）的微调现在是文本分类中常用的方法，在许多应用中都能看到。当标记文档稀缺时，主动学习有助于节省注释工作，但需要在每次获取迭代时重新训练大规模模型。我们通过在主动学习循环中使用LLMs的预训练表示，显著加快了这一过程，一旦获得所需数量的标记数据，就可以对LLMs进行微调，以获得最佳性能。通过在常见的文本分类基准上验证，以预训练的BERT和RoBERTa作为基础，我们的策略产生了与在整个主动学习循环中微调相似的性能，但计算开销降低了数个数量级。使用我们的程序获取的数据可以跨预训练网络进行泛化，从而可以灵活选择最终模型或更新。",
    "tldr": "通过在主动学习循环中使用预训练LLMs的表示，可以显著加快标记数据获取的过程，并通过微调获得最佳性能，同时大大降低计算开销。",
    "en_tdlr": "Pretrained representations of LLMs in active learning loop significantly speed up data acquisition process and achieve optimal performance through fine-tuning, while drastically reducing computational costs."
}