{
    "title": "The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains",
    "abstract": "arXiv:2402.11004v1 Announce Type: new  Abstract: Large language models have the ability to generate text that mimics patterns in their inputs. We introduce a simple Markov Chain sequence modeling task in order to study how this in-context learning (ICL) capability emerges. In our setting, each example is sampled from a Markov chain drawn from a prior distribution over Markov chains. Transformers trained on this task form \\emph{statistical induction heads} which compute accurate next-token probabilities given the bigram statistics of the context. During the course of training, models pass through multiple phases: after an initial stage in which predictions are uniform, they learn to sub-optimally predict using in-context single-token statistics (unigrams); then, there is a rapid phase transition to the correct in-context bigram solution. We conduct an empirical and theoretical investigation of this multi-phase process, showing how successful learning results from the interaction between",
    "link": "https://arxiv.org/abs/2402.11004",
    "context": "Title: The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains\nAbstract: arXiv:2402.11004v1 Announce Type: new  Abstract: Large language models have the ability to generate text that mimics patterns in their inputs. We introduce a simple Markov Chain sequence modeling task in order to study how this in-context learning (ICL) capability emerges. In our setting, each example is sampled from a Markov chain drawn from a prior distribution over Markov chains. Transformers trained on this task form \\emph{statistical induction heads} which compute accurate next-token probabilities given the bigram statistics of the context. During the course of training, models pass through multiple phases: after an initial stage in which predictions are uniform, they learn to sub-optimally predict using in-context single-token statistics (unigrams); then, there is a rapid phase transition to the correct in-context bigram solution. We conduct an empirical and theoretical investigation of this multi-phase process, showing how successful learning results from the interaction between",
    "path": "papers/24/02/2402.11004.json",
    "total_tokens": 834,
    "translated_title": "在上下文学习马尔可夫链的统计归纳头的演变",
    "translated_abstract": "大型语言模型能够生成模仿其输入模式的文本。我们引入了一个简单的马尔可夫链序列建模任务，以研究这种上下文学习（ICL）能力是如何出现的。在我们的设定中，每个例子是从一个从马尔可夫链先验分布中抽取的马尔可夫链中抽取的。在这个任务中训练的Transformer形成了计算给定上下文双字母统计的准确下一个标记概率的\\emph{统计归纳头}。在训练过程中，模型经过多个阶段：在初始阶段，预测是均匀的；他们学会使用上下文单标记统计（一元组）进行次优预测；然后，快速过渡到正确的上下文双字母解决方案。我们对这个多阶段过程进行了经验和理论调查，显示了成功学习是如何来自于",
    "tldr": "研究了大型语言模型如何通过训练在上下文学习中准确预测下一个标记概率，并发现了一个多阶段的过程。"
}