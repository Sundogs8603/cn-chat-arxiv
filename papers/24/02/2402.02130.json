{
    "title": "Rendering Graphs for Graph Reasoning in Multimodal Large Language Models",
    "abstract": "Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on ",
    "link": "https://arxiv.org/abs/2402.02130",
    "context": "Title: Rendering Graphs for Graph Reasoning in Multimodal Large Language Models\nAbstract: Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on ",
    "path": "papers/24/02/2402.02130.json",
    "total_tokens": 833,
    "translated_title": "在多模态大型语言模型中为图推理渲染图形",
    "translated_abstract": "大型语言模型(LLMs)在机器人规划、知识图谱补全和常识推理等任务中越来越多地使用图结构，LLMs能够理解文本格式的图信息，但忽视了丰富的视觉模态，而视觉是人类理解结构信息和进行图推理的直观方式。将图结构表示为视觉图像(即视觉图)的潜在益处和能力仍未被探索。本文在图推理任务中首次引入视觉信息，并提出一个新的基准测试数据集GITQA，其中每个样本是一个元组(图、图像、文本描述)。我们利用最先进的多模态LLMs在GITQA基准测试数据集上进行了大量实验证明，结合文本和视觉信息的结果比单一模态效果更好。此外，在LLaVA-7B/13B模型的微调上表现出色。",
    "tldr": "本文在多模态大型语言模型中为图推理任务引入了视觉信息，并提出了一个新的基准测试数据集GITQA。实验证明，结合文本和视觉信息的结果比单一模态效果更好。"
}