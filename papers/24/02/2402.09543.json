{
    "title": "Rethinking Large Language Model Architectures for Sequential Recommendations",
    "abstract": "arXiv:2402.09543v1 Announce Type: new  Abstract: Recently, sequential recommendation has been adapted to the LLM paradigm to enjoy the power of LLMs. LLM-based methods usually formulate recommendation information into natural language and the model is trained to predict the next item in an auto-regressive manner. Despite their notable success, the substantial computational overhead of inference poses a significant obstacle to their real-world applicability. In this work, we endeavor to streamline existing LLM-based recommendation models and propose a simple yet highly effective model Lite-LLM4Rec. The primary goal of Lite-LLM4Rec is to achieve efficient inference for the sequential recommendation task. Lite-LLM4Rec circumvents the beam search decoding by using a straight item projection head for ranking scores generation. This design stems from our empirical observation that beam search decoding is ultimately unnecessary for sequential recommendations. Additionally, Lite-LLM4Rec introd",
    "link": "https://arxiv.org/abs/2402.09543",
    "context": "Title: Rethinking Large Language Model Architectures for Sequential Recommendations\nAbstract: arXiv:2402.09543v1 Announce Type: new  Abstract: Recently, sequential recommendation has been adapted to the LLM paradigm to enjoy the power of LLMs. LLM-based methods usually formulate recommendation information into natural language and the model is trained to predict the next item in an auto-regressive manner. Despite their notable success, the substantial computational overhead of inference poses a significant obstacle to their real-world applicability. In this work, we endeavor to streamline existing LLM-based recommendation models and propose a simple yet highly effective model Lite-LLM4Rec. The primary goal of Lite-LLM4Rec is to achieve efficient inference for the sequential recommendation task. Lite-LLM4Rec circumvents the beam search decoding by using a straight item projection head for ranking scores generation. This design stems from our empirical observation that beam search decoding is ultimately unnecessary for sequential recommendations. Additionally, Lite-LLM4Rec introd",
    "path": "papers/24/02/2402.09543.json",
    "total_tokens": 943,
    "translated_title": "重新思考用于顺序推荐的大型语言模型架构",
    "translated_abstract": "最近，顺序推荐已经被适应到了LLM范式中，以享受LLM的强大能力。基于LLM的方法通常将推荐信息转化为自然语言，模型被训练成以自回归方式预测下一个项目。尽管它们取得了显著的成功，但是推理的巨大计算开销对于其在实际应用中的可行性构成了重大障碍。在这项工作中，我们努力简化现有的基于LLM的推荐模型，并提出了一种简单而高效的模型Lite-LLM4Rec。Lite-LLM4Rec的主要目标是实现顺序推荐任务的高效推理。Lite-LLM4Rec通过使用直接项目投影头来生成排名分数，避免了波束搜索解码。这个设计源于我们的经验观察，即波束搜索解码对于顺序推荐实际上是不必要的。此外，Lite-LLM4Rec还引入了解码时长的自适应截断机制，以进一步提高推理效率。",
    "tldr": "本文提出了一种名为Lite-LLM4Rec的简化的基于LLM的推荐模型，旨在实现顺序推荐任务的高效推理。Lite-LLM4Rec通过使用直接项目投影头来生成排名分数，避免了波束搜索解码，同时引入了自适应截断机制以提高推理效率。",
    "en_tdlr": "This paper proposes a simplified LLM-based recommendation model called Lite-LLM4Rec, which aims to achieve efficient inference for sequential recommendation tasks. Lite-LLM4Rec avoids beam search decoding by using a direct item projection head for ranking score generation and introduces an adaptive truncation mechanism to further improve inference efficiency."
}