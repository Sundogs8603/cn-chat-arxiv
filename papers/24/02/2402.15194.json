{
    "title": "Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control",
    "abstract": "arXiv:2402.15194v1 Announce Type: cross  Abstract: Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth \"genuine\" reward, as is the case in many practical applications. These challenges, collectively termed \"reward collapse,\" pose",
    "link": "https://arxiv.org/abs/2402.15194",
    "context": "Title: Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized Control\nAbstract: arXiv:2402.15194v1 Announce Type: cross  Abstract: Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth \"genuine\" reward, as is the case in many practical applications. These challenges, collectively termed \"reward collapse,\" pose",
    "path": "papers/24/02/2402.15194.json",
    "total_tokens": 836,
    "translated_title": "连续时间扩散模型的微调作为熵正则化控制",
    "translated_abstract": "扩散模型在捕捉复杂数据分布方面表现出色，例如自然图像和蛋白质的分布。虽然扩散模型经过训练可代表训练数据集中的分布，但我们通常更关注其他属性，例如生成图像的美学质量或生成蛋白质的功能属性。扩散模型可以通过最大化某些奖励函数的价值（例如图像的美学质量）以目标导向的方式进行微调。然而，这些方法可能会导致样本多样性减少，与训练数据分布出现显著偏差，甚至由于利用不完美的奖励函数而导致样本质量较差。在许多实际应用中奖励函数是用于近似真实“真实”奖励的学习模型时，最后一个问题经常会产生。这些挑战总称为“奖励崩溃”。",
    "tldr": "扩散模型的微调方法可以通过最大化奖励函数的价值来以目标导向方式进行微调，但可能会面临奖励崩溃的挑战。",
    "en_tdlr": "The fine-tuning of diffusion models can be done in a goal-directed way by maximizing the value of a reward function, but it may face the challenge of reward collapse."
}