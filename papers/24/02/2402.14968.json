{
    "title": "Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment",
    "abstract": "arXiv:2402.14968v1 Announce Type: cross  Abstract: Despite the general capabilities of Large Language Models (LLMs) like GPT-4 and Llama-2, these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the fine-tuning dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the fine-tuning dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In pa",
    "link": "https://arxiv.org/abs/2402.14968",
    "context": "Title: Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment\nAbstract: arXiv:2402.14968v1 Announce Type: cross  Abstract: Despite the general capabilities of Large Language Models (LLMs) like GPT-4 and Llama-2, these models still request fine-tuning or adaptation with customized data when it comes to meeting the specific business demands and intricacies of tailored use cases. However, this process inevitably introduces new safety threats, particularly against the Fine-tuning based Jailbreak Attack (FJAttack), where incorporating just a few harmful examples into the fine-tuning dataset can significantly compromise the model safety. Though potential defenses have been proposed by incorporating safety examples into the fine-tuning dataset to reduce the safety issues, such approaches require incorporating a substantial amount of safety examples, making it inefficient. To effectively defend against the FJAttack with limited safety examples, we propose a Backdoor Enhanced Safety Alignment method inspired by an analogy with the concept of backdoor attacks. In pa",
    "path": "papers/24/02/2402.14968.json",
    "total_tokens": 827,
    "translated_title": "使用后门增强对齐来缓解微调越狱攻击",
    "translated_abstract": "尽管大型语言模型（LLMs）如GPT-4和Llama-2具有一般能力，但在满足特定业务需求和定制用例的复杂性时，仍然需要对其进行微调或自适应以满足需求。然而，这个过程不可避免地引入了新的安全威胁，特别是针对基于微调的越狱攻击（FJAttack），在这种情况下，将仅几个有害示例纳入微调数据集就可能显着地损害模型的安全性。虽然已经提出了一些潜在的防御方法，例如将安全示例纳入微调数据集以减少安全问题，但这些方法需要纳入大量的安全示例，效率低下。为了有效地针对FJAttack进行防御并只使用有限的安全示例，我们提出了一种灵感来自后门攻击概念的后门增强安全对齐方法。",
    "tldr": "提出了一种通过后门增强对齐方法有效缓解微调越狱攻击，避免需要大量安全示例的低效问题。",
    "en_tdlr": "Proposed a method to mitigate fine-tuning jailbreak attacks effectively with limited safety examples by enhancing alignment with backdoors."
}