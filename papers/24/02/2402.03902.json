{
    "title": "A phase transition between positional and semantic learning in a solvable model of dot-product attention",
    "abstract": "We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning). For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism. On the theoretical side, we study the learning of a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product att",
    "link": "https://arxiv.org/abs/2402.03902",
    "context": "Title: A phase transition between positional and semantic learning in a solvable model of dot-product attention\nAbstract: We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning). For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism. On the theoretical side, we study the learning of a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product att",
    "path": "papers/24/02/2402.03902.json",
    "total_tokens": 897,
    "translated_title": "一个可求解的点积注意力模型中位置学习和语义学习之间的相变",
    "translated_abstract": "我们研究了点积注意力层如何学习位置注意力矩阵（通过各自的位置决定令牌之间的关注）和语义注意力矩阵（通过意义决定令牌之间的关注）。通过算法任务的实验，我们展示了同样简单的架构如何使用位置机制或语义机制来实现解决方案。在理论上，我们研究了具有可训练的绑定和低秩查询和键矩阵的非线性自注意层的学习。在高维数据和相对较大数量的训练样本的渐近极限下，我们提供了非凸经验损失函数全局最小值的闭式表征。我们展示了这个最小值对应于位置机制或语义机制，证明了随着样本复杂性的增加，从位置机制向语义机制的自发相变。最后，我们比较了点积注意力与其他注意力机制的性能。",
    "tldr": "这篇论文研究了点积注意力层如何同时学习位置和语义关注，发现在高维数据和大量训练样本条件下，存在从位置机制到语义机制的相变，并提供了非凸经验损失函数全局最小值的闭式表征。",
    "en_tdlr": "This paper investigates how a dot-product attention layer learns both positional and semantic attention, finding a phase transition from positional to semantic mechanism in high-dimensional data and large training sample conditions, and provides a closed-form characterization of the global minimum of the non-convex empirical loss landscape."
}