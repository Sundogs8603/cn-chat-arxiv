{
    "title": "Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings",
    "abstract": "arXiv:2402.17016v1 Announce Type: cross  Abstract: We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language. These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations.   By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embed",
    "link": "https://arxiv.org/abs/2402.17016",
    "context": "Title: Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings\nAbstract: arXiv:2402.17016v1 Announce Type: cross  Abstract: We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language. These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations.   By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embed",
    "path": "papers/24/02/2402.17016.json",
    "total_tokens": 892,
    "translated_title": "多任务对比学习用于8192标记的双语文本嵌入",
    "translated_abstract": "我们引入了一套新颖的最先进的双语文本嵌入模型，旨在支持英语和另一种目标语言。这些模型能够处理长达8192个标记的文本输入，因此非常适用于一系列自然语言处理任务，如文本检索、聚类和语义文本相似度（STS）计算。通过专注于双语模型并引入独特的多任务学习目标，我们显著改善了在STS任务上的模型表现，超过了现有多语言模型在目标语言理解和跨语言评估任务方面的能力。此外，我们的双语模型更加高效，需要较少的参数和更少的内存，因为它们需要较小的词汇量。此外，我们还扩展了大规模文本嵌入基准（MTEB），包括德语和西班牙语嵌入的基准。",
    "tldr": "通过引入独特的多任务学习目标，研究者设计了用于支持英语和其他目标语言的最先进的双语文本嵌入模型，显著提高了模型在STS任务上的表现，同时在目标语言理解和跨语言评估任务中超越了现有多语言模型。",
    "en_tdlr": "By introducing a unique multi-task learning objective, researchers have designed state-of-the-art bilingual text embedding models to support English and other target languages, significantly improving performance on STS tasks and surpassing existing multilingual models in target language understanding and cross-lingual evaluation tasks."
}