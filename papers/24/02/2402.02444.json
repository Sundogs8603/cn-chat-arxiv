{
    "title": "BECLR: Batch Enhanced Contrastive Few-Shot Learning",
    "abstract": "Learning quickly from very few labeled samples is a fundamental attribute that separates machines and humans in the era of deep representation learning. Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding the reliance on annotations at training time. Intrigued by the success of contrastive learning approaches in the realm of U-FSL, we structurally approach their shortcomings in both pretraining and downstream inference stages. We propose a novel Dynamic Clustered mEmory (DyCE) module to promote a highly separable latent representation space for enhancing positive sampling at the pretraining phase and infusing implicit class-level insights into unsupervised contrastive learning. We then tackle the, somehow overlooked yet critical, issue of sample bias at the few-shot inference stage. We propose an iterative Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate that it efficiently addresses the problem, especially in low-shot scenarios",
    "link": "https://arxiv.org/abs/2402.02444",
    "context": "Title: BECLR: Batch Enhanced Contrastive Few-Shot Learning\nAbstract: Learning quickly from very few labeled samples is a fundamental attribute that separates machines and humans in the era of deep representation learning. Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding the reliance on annotations at training time. Intrigued by the success of contrastive learning approaches in the realm of U-FSL, we structurally approach their shortcomings in both pretraining and downstream inference stages. We propose a novel Dynamic Clustered mEmory (DyCE) module to promote a highly separable latent representation space for enhancing positive sampling at the pretraining phase and infusing implicit class-level insights into unsupervised contrastive learning. We then tackle the, somehow overlooked yet critical, issue of sample bias at the few-shot inference stage. We propose an iterative Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate that it efficiently addresses the problem, especially in low-shot scenarios",
    "path": "papers/24/02/2402.02444.json",
    "total_tokens": 979,
    "translated_title": "BECLR：增强批量对比式少样本学习",
    "translated_abstract": "在深度表示学习时代，快速从很少的标记样本中学习是区分机器和人类的基本特点。无监督少样本学习（U-FSL）希望通过在训练时丢弃对注释的依赖来弥合这一差距。受对比学习方法在U-FSL领域的成功启发，我们从预训练和后续推理阶段结构性地解决了它们的缺点。我们提出了一种新颖的动态聚类内存（DyCE）模块，以提升预训练阶段正样本采样的高度可分离潜在表示空间，并将隐式类别级别的洞察力融入无监督对比学习中。然后，我们解决了一直被忽视但却至关重要的少样本推理阶段的样本偏差问题。我们提出了一种基于迭代最优传输的分布对齐（OpTA）策略，并证明它能够有效解决这个问题，特别是在低样本场景下。",
    "tldr": "本论文提出了一种增强的批量对比式少样本学习方法（BECLR），通过引入动态聚类内存（DyCE）模块和迭代最优传输的分布对齐策略（OpTA），在预训练和推理阶段分别解决了正样本采样和样本偏差问题，提高了无监督少样本学习性能。"
}