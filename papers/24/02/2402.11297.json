{
    "title": "MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal",
    "abstract": "arXiv:2402.11297v1 Announce Type: new  Abstract: Our contribution introduces a groundbreaking multimodal large language model designed to comprehend multi-images, multi-audio, and multi-images-multi-audio within a single multiturn session. Leveraging state-of-the-art models, we utilize the SigLIP encoder for visual inputs and the Whisper Encoder for audio inputs. Notably, this multimodal large language model is bilingual, proficient in understanding both English and Malay simultaneously. We proudly unveil two versions of this model: TinyLlama with 1.1B parameters, and Mistral with 7B parameters. With its ability to navigate diverse modalities and languages, our model represents a significant advancement for the Malaysian context and beyond.   All models released at https://huggingface.co/collections/mesolitica/multimodal-malaysian-llm-65c6f893e03f78fa9e5c8859",
    "link": "https://arxiv.org/abs/2402.11297",
    "context": "Title: MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal\nAbstract: arXiv:2402.11297v1 Announce Type: new  Abstract: Our contribution introduces a groundbreaking multimodal large language model designed to comprehend multi-images, multi-audio, and multi-images-multi-audio within a single multiturn session. Leveraging state-of-the-art models, we utilize the SigLIP encoder for visual inputs and the Whisper Encoder for audio inputs. Notably, this multimodal large language model is bilingual, proficient in understanding both English and Malay simultaneously. We proudly unveil two versions of this model: TinyLlama with 1.1B parameters, and Mistral with 7B parameters. With its ability to navigate diverse modalities and languages, our model represents a significant advancement for the Malaysian context and beyond.   All models released at https://huggingface.co/collections/mesolitica/multimodal-malaysian-llm-65c6f893e03f78fa9e5c8859",
    "path": "papers/24/02/2402.11297.json",
    "total_tokens": 800,
    "translated_title": "MMMModal -- 多图像多音频多轮多模态",
    "translated_abstract": "我们的贡献是引入了一种突破性的多模态大型语言模型，旨在理解多图像、多音频和多图像多音频在单个多轮会话内。利用最先进的模型，我们利用SigLIP编码器用于视觉输入，Whisper编码器用于音频输入。值得注意的是，这种多模态大型语言模型是双语的，能够同时理解英语和马来语。我们自豪地推出了这个模型的两个版本：拥有11亿参数的TinyLlama和拥有70亿参数的Mistral。凭借其能够处理多样的模态和语言的能力，我们的模型代表了马来西亚背景和更广泛领域的重大进展。",
    "tldr": "这个模型是一种多模态大型语言模型，能够理解多图像、多音频和多模态信息，在英语和马来语之间切换，并通过使用SigLIP编码器和Whisper编码器实现了这一功能。",
    "en_tdlr": "This model is a multimodal large language model that can comprehend multi-images, multi-audio, and multi-modal information, switching between English and Malay, and achieves this by utilizing the SigLIP encoder and Whisper encoder."
}