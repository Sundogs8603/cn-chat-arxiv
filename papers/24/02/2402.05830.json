{
    "title": "Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting",
    "abstract": "Time series analysis is vital for numerous applications, and transformers have become increasingly prominent in this domain. Leading methods customize the transformer architecture from NLP and CV, utilizing a patching technique to convert continuous signals into segments. Yet, time series data are uniquely challenging due to significant distribution shifts and intrinsic noise levels. To address these two challenges,we introduce the Sparse Vector Quantized FFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparse vector quantization technique coupled with Reverse Instance Normalization (RevIN) to reduce noise impact and capture sufficient statistics for forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the transformer architecture. Our FFN-free approach trims the parameter count, enhancing computational efficiency and reducing overfitting. Through evaluations across ten benchmark datasets, including the newly introduced CAISO dataset, Sparse-VQ su",
    "link": "https://arxiv.org/abs/2402.05830",
    "context": "Title: Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting\nAbstract: Time series analysis is vital for numerous applications, and transformers have become increasingly prominent in this domain. Leading methods customize the transformer architecture from NLP and CV, utilizing a patching technique to convert continuous signals into segments. Yet, time series data are uniquely challenging due to significant distribution shifts and intrinsic noise levels. To address these two challenges,we introduce the Sparse Vector Quantized FFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparse vector quantization technique coupled with Reverse Instance Normalization (RevIN) to reduce noise impact and capture sufficient statistics for forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the transformer architecture. Our FFN-free approach trims the parameter count, enhancing computational efficiency and reducing overfitting. Through evaluations across ten benchmark datasets, including the newly introduced CAISO dataset, Sparse-VQ su",
    "path": "papers/24/02/2402.05830.json",
    "total_tokens": 897,
    "translated_title": "稀疏向量量化变压器：一种无前馈网络的框架，用于增强时间序列预测",
    "translated_abstract": "时间序列分析对于许多应用非常重要，而变压器在这个领域中变得越来越突出。领先的方法从自然语言处理和计算机视觉中定制了变压器架构，利用修补技术将连续信号转换为片段。然而，由于分布变化和内在噪声水平的显著变化，时间序列数据具有独特的挑战。为了解决这两个挑战，我们引入了稀疏向量量化的FFN-Free变压器（Sparse-VQ）。我们的方法利用稀疏向量量化技术和反实例归一化（RevIN）来减少噪声影响，并捕获足够的统计信息用于预测，作为变压器架构中前馈层（FFN）的替代方法。我们的无FFN方法削减了参数数量，提高了计算效率，并减少了过拟合。通过对十个基准数据集进行评估，包括新引入的CAISO数据集，Sparse-VQ取得了",
    "tldr": "Sparse-VQ是一种无前馈网络的框架，利用稀疏向量量化技术和反实例归一化来减少噪声影响并捕获足够的统计信息，从而提高时间序列预测的性能。"
}