{
    "title": "Prompting Implicit Discourse Relation Annotation",
    "abstract": "Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches. This work investigates several proven prompting techniques to improve ChatGPT's recognition of discourse relations. In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings.",
    "link": "https://arxiv.org/abs/2402.04918",
    "context": "Title: Prompting Implicit Discourse Relation Annotation\nAbstract: Pre-trained large language models, such as ChatGPT, archive outstanding performance in various reasoning tasks without supervised training and were found to have outperformed crowdsourcing workers. Nonetheless, ChatGPT's performance in the task of implicit discourse relation classification, prompted by a standard multiple-choice question, is still far from satisfactory and considerably inferior to state-of-the-art supervised approaches. This work investigates several proven prompting techniques to improve ChatGPT's recognition of discourse relations. In particular, we experimented with breaking down the classification task that involves numerous abstract labels into smaller subtasks. Nonetheless, experiment results show that the inference accuracy hardly changes even with sophisticated prompt engineering, suggesting that implicit discourse relation classification is not yet resolvable under zero-shot or few-shot settings.",
    "path": "papers/24/02/2402.04918.json",
    "total_tokens": 781,
    "translated_title": "促使隐式话语关系注释的研究",
    "translated_abstract": "预训练的大型语言模型，如ChatGPT，在各种推理任务中表现出色，无需监督训练，并且被发现超过了众包工作者。然而，ChatGPT在通过标准的多项选择问题引导的隐式话语关系分类任务中的表现仍然远远不如最先进的监督方法。本研究对几种经过验证的提示技术进行了调查，以改善ChatGPT对话语关系的识别。特别是，我们尝试将涉及大量抽象标签的分类任务分解为较小的子任务。然而，实验结果表明，即使使用复杂的提示工程，推理准确率几乎没有改变，这表明隐式话语关系分类在零样本或少样本设置下尚不可解。",
    "tldr": "本研究旨在提升ChatGPT对隐式话语关系的识别，尝试了多种提示技术，但实验结果显示即使进行复杂的提示工程，隐式话语关系分类在零样本或少样本设置下仍难以解决。",
    "en_tdlr": "This study aims to improve ChatGPT's recognition of implicit discourse relations by investigating various prompting techniques. However, the experiment results suggest that implicit discourse relation classification is still challenging under zero-shot or few-shot settings, even with sophisticated prompt engineering."
}