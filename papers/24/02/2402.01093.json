{
    "title": "Specialized Language Models with Cheap Inference from Limited Domain Data",
    "abstract": "Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.",
    "link": "https://rss.arxiv.org/abs/2402.01093",
    "context": "Title: Specialized Language Models with Cheap Inference from Limited Domain Data\nAbstract: Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.",
    "path": "papers/24/02/2402.01093.json",
    "total_tokens": 943,
    "translated_title": "使用有限领域数据进行廉价推理的专用语言模型",
    "translated_abstract": "大型语言模型已成为一种多才多艺的工具，但在缺乏大规模推理预算和大规模领域内训练集的任务中应用起来具有挑战性。本研究对这些限制进行了形式化，并区分了四个重要的变量：预训练预算（用于在目标领域出现之前进行训练），专用预算（用于在目标领域出现之后进行训练），推理预算和领域内训练集大小。在这些设置中，我们比较了机器学习文献中的不同方法。受到推理成本的限制，我们发现比训练非常大的基本转换器模型的标准做法更好的替代方案。特别是，我们发现超网络和专家混合模型在大型预训练预算下具有更好的困惑度，而在大型专用预算下，训练重要样本数据集上的小型模型更具吸引力。",
    "tldr": "本研究提出了一种使用有限领域数据进行廉价推理的专用语言模型。在研究中，我们通过比较不同的机器学习方法，在推理成本的限制下找到了比训练非常大的基本转换器模型更优的替代方案。具体而言，在大型预训练预算下，超网络和专家混合模型的困惑度更好，而在大型专用预算下，训练重要样本数据集上的小型模型更具吸引力。",
    "en_tdlr": "This paper presents specialized language models that enable cheap inference from limited domain data. By comparing different machine learning approaches, the study finds alternatives to large vanilla transformer models, such as hyper-networks and mixture of experts for large pretraining budgets, and small models trained on importance sampled datasets for large specialization budgets."
}