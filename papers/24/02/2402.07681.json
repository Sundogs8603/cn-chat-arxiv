{
    "title": "Large Language Models \"Ad Referendum\": How Good Are They at Machine Translation in the Legal Domain?",
    "abstract": "This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a tradition-al neural machine translation (NMT) system across four language pairs in the legal domain. It combines automatic evaluation met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy. The re-sults indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations. This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality. The study underscores the evolving capabil-ities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations.",
    "link": "https://arxiv.org/abs/2402.07681",
    "context": "Title: Large Language Models \"Ad Referendum\": How Good Are They at Machine Translation in the Legal Domain?\nAbstract: This study evaluates the machine translation (MT) quality of two state-of-the-art large language models (LLMs) against a tradition-al neural machine translation (NMT) system across four language pairs in the legal domain. It combines automatic evaluation met-rics (AEMs) and human evaluation (HE) by professional transla-tors to assess translation ranking, fluency and adequacy. The re-sults indicate that while Google Translate generally outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4, comparably or slightly better in terms of producing contextually adequate and fluent translations. This discrepancy suggests LLMs' potential in handling specialized legal terminology and context, highlighting the importance of human evaluation methods in assessing MT quality. The study underscores the evolving capabil-ities of LLMs in specialized domains and calls for reevaluation of traditional AEMs to better capture the nuances of LLM-generated translations.",
    "path": "papers/24/02/2402.07681.json",
    "total_tokens": 999,
    "translated_title": "大型语言模型“评审”: 在法律领域的机器翻译效果如何？",
    "translated_abstract": "本研究评估了两种最先进的大型语言模型（LLMs）与传统神经机器翻译（NMT）系统在法律领域四种语言对中的机器翻译质量。研究结合了自动评估指标（AEMs）和专业翻译人员进行的人工评估（HE），评估了翻译排名、流畅性和足够性。结果表明，虽然谷歌翻译在AEMs方面通常优于LLMs，但人工评估者认为LLMs，特别是GPT-4，在产生上下文足够且流畅的译文方面相当或略好于谷歌翻译。这种差异表明LLMs在处理专业法律术语和背景方面的潜力，凸显了人工评估方法在评估机器翻译质量方面的重要性。本研究强调了LLMs在专业领域的不断发展能力，并呼吁重新评估传统AEMs以更好地捕捉LLMs生成的翻译的细微差别。",
    "tldr": "本研究评估了两种大型语言模型和传统神经机器翻译系统在法律领域的机器翻译质量，结果显示大型语言模型在产生上下文足够且流畅的译文方面表现优异，强调了人工评估方法在评估机器翻译质量中的重要性。",
    "en_tdlr": "This study evaluates the machine translation quality of two large language models and a traditional neural machine translation system in the legal domain. The results indicate that large language models perform well in producing contextually adequate and fluent translations, highlighting the importance of human evaluation methods in assessing machine translation quality."
}