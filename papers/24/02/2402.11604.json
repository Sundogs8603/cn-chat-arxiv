{
    "title": "Self-evolving Autoencoder Embedded Q-Network",
    "abstract": "arXiv:2402.11604v1 Announce Type: new  Abstract: In the realm of sequential decision-making tasks, the exploration capability of a reinforcement learning (RL) agent is paramount for achieving high rewards through interactions with the environment. To enhance this crucial ability, we propose SAQN, a novel approach wherein a self-evolving autoencoder (SA) is embedded with a Q-Network (QN). In SAQN, the self-evolving autoencoder architecture adapts and evolves as the agent explores the environment. This evolution enables the autoencoder to capture a diverse range of raw observations and represent them effectively in its latent space. By leveraging the disentangled states extracted from the encoder generated latent space, the QN is trained to determine optimal actions that improve rewards. During the evolution of the autoencoder architecture, a bias-variance regulatory strategy is employed to elicit the optimal response from the RL agent. This strategy involves two key components: (i) fost",
    "link": "https://arxiv.org/abs/2402.11604",
    "context": "Title: Self-evolving Autoencoder Embedded Q-Network\nAbstract: arXiv:2402.11604v1 Announce Type: new  Abstract: In the realm of sequential decision-making tasks, the exploration capability of a reinforcement learning (RL) agent is paramount for achieving high rewards through interactions with the environment. To enhance this crucial ability, we propose SAQN, a novel approach wherein a self-evolving autoencoder (SA) is embedded with a Q-Network (QN). In SAQN, the self-evolving autoencoder architecture adapts and evolves as the agent explores the environment. This evolution enables the autoencoder to capture a diverse range of raw observations and represent them effectively in its latent space. By leveraging the disentangled states extracted from the encoder generated latent space, the QN is trained to determine optimal actions that improve rewards. During the evolution of the autoencoder architecture, a bias-variance regulatory strategy is employed to elicit the optimal response from the RL agent. This strategy involves two key components: (i) fost",
    "path": "papers/24/02/2402.11604.json",
    "total_tokens": 835,
    "translated_title": "自进化自动编码器嵌入的 Q 网络",
    "translated_abstract": "在序贯决策任务领域，强化学习代理的探索能力对通过与环境的交互获得高奖励至关重要。为增强这一关键能力，我们提出了一种新方法 SAQN，在其中将一个自进化自动编码器（SA）嵌入到一个 Q 网络（QN）中。在 SAQN 中，自进化自动编码器架构随着代理探索环境而调整和进化。这种进化使得自动编码器能够捕捉各种原始观测并有效地在其潜在空间中表示它们。通过利用从编码器生成的潜在空间中提取的分解状态，QN 被训练以确定改善奖励的最佳行动。在自动编码器架构的进化过程中，采用偏差-方差调节策略来引导强化学习代理做出最佳反应。这个策略涉及两个关键组成部分：（i）促进",
    "tldr": "提出了一种将自进化自动编码器嵌入 Q 网络以增强强化学习代理探索能力的新方法"
}