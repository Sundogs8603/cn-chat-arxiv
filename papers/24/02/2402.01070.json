{
    "title": "FedShift: Tackling Dual Heterogeneity Problem of Federated Learning via Weight Shift Aggregation",
    "abstract": "Federated Learning (FL) offers a compelling method for training machine learning models with a focus on preserving data privacy. The presence of system heterogeneity and statistical heterogeneity, recognized challenges in FL, arises from the diversity of client hardware, network, and dataset distribution. This diversity can critically affect the training pace and the performance of models. While many studies address either system or statistical heterogeneity by introducing communication-efficient or stable convergence algorithms, addressing these challenges in isolation often leads to compromises due to unaddressed heterogeneity. In response, this paper introduces FedShift, a novel algorithm designed to enhance both the training speed and the models' accuracy in a dual heterogeneity scenario. Our solution can improve client engagement through quantization and mitigate the adverse effects on performance typically associated with quantization by employing a shifting technique. This techn",
    "link": "https://rss.arxiv.org/abs/2402.01070",
    "context": "Title: FedShift: Tackling Dual Heterogeneity Problem of Federated Learning via Weight Shift Aggregation\nAbstract: Federated Learning (FL) offers a compelling method for training machine learning models with a focus on preserving data privacy. The presence of system heterogeneity and statistical heterogeneity, recognized challenges in FL, arises from the diversity of client hardware, network, and dataset distribution. This diversity can critically affect the training pace and the performance of models. While many studies address either system or statistical heterogeneity by introducing communication-efficient or stable convergence algorithms, addressing these challenges in isolation often leads to compromises due to unaddressed heterogeneity. In response, this paper introduces FedShift, a novel algorithm designed to enhance both the training speed and the models' accuracy in a dual heterogeneity scenario. Our solution can improve client engagement through quantization and mitigate the adverse effects on performance typically associated with quantization by employing a shifting technique. This techn",
    "path": "papers/24/02/2402.01070.json",
    "total_tokens": 825,
    "translated_title": "FedShift: 通过权重迁移聚合解决联邦学习的双重异质性问题",
    "translated_abstract": "联邦学习（FL）提供了一种训练机器学习模型并注重保护数据隐私的有力方法。FL中存在的系统异质性和统计异质性问题源于客户端硬件、网络和数据集分布的多样性。这种多样性可能会严重影响训练速度和模型性能。虽然许多研究通过引入通信效率或稳定收敛算法来解决系统或统计异质性问题，但单独解决这些挑战往往会导致妥协，因为异质性问题未得到解决。为此，本文介绍了一种名为FedShift的新算法，旨在在双重异质性场景中提高训练速度和模型的准确性。我们的解决方案通过量化改善客户参与度，并通过应用迁移技术来缓解量化通常导致的性能不良影响。",
    "tldr": "本文介绍了一种名为FedShift的算法，通过权重迁移聚合来解决联邦学习中的异质性问题，并提高训练速度和模型准确性。",
    "en_tdlr": "This paper introduces FedShift, an algorithm that tackles the heterogeneity problem in federated learning through weight shift aggregation, improving both training speed and model accuracy."
}