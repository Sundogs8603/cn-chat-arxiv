{
    "title": "Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs",
    "abstract": "arXiv:2402.12052v1 Announce Type: new  Abstract: The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the mis",
    "link": "https://arxiv.org/abs/2402.12052",
    "context": "Title: Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs\nAbstract: arXiv:2402.12052v1 Announce Type: new  Abstract: The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the mis",
    "path": "papers/24/02/2402.12052.json",
    "total_tokens": 848,
    "translated_title": "小模型，大见解：利用精简代理模型确定LLMs何时以及为何检索",
    "translated_abstract": "arXiv:2402.12052v1 公告类型:新摘要: 大型语言模型（LLMs）与搜索引擎的整合代表了知识获取方法的重要发展。然而，确定LLM已经具备的知识和需要搜索引擎帮助的知识仍然是一个未解决的问题。大多数现有方法通过LLM本身预处理答案或推理的结果来解决这个问题，但这带来了过高的计算成本。本文介绍了一种新颖的协作方法，即SlimPLM，通过精简代理模型检测LLMs中缺失的知识，以增强LLMs的知识获取过程。我们采用参数远远更少的代理模型，并将其答案视为启发式答案。然后利用启发式答案来预测回答用户问题所需的知识，以及LLM中已知和未知的知识。我们只为未知知识进行检索",
    "tldr": "本文介绍了一种新的协作方法SlimPLM，通过精简代理模型检测LLMs中缺失的知识，以增强LLMs的知识获取过程",
    "en_tdlr": "This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process."
}