{
    "title": "Collaborative Learning with Different Labeling Functions",
    "abstract": "arXiv:2402.10445v1 Announce Type: new  Abstract: We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.   We show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class.   In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, ",
    "link": "https://arxiv.org/abs/2402.10445",
    "context": "Title: Collaborative Learning with Different Labeling Functions\nAbstract: arXiv:2402.10445v1 Announce Type: new  Abstract: We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.   We show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class.   In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, ",
    "path": "papers/24/02/2402.10445.json",
    "total_tokens": 734,
    "translated_title": "使用不同标注函数的协作学习",
    "translated_abstract": "我们研究了一种 Collaborative PAC Learning 的变体，在这种情况下，我们旨在学习每个$n$个数据分布的准确分类器，同时最小化从它们总共抽取的样本数量。与通常的协作学习设置不同，不假设存在一个同时对所有分布准确的单一分类器。我们表明，当数据分布满足较弱的可实现性假设时，仍然可以实现高效的学习。我们给出了一种基于经验风险最小化(ERM)的学习算法，应用于假设类的一个自然增强，分析依赖于对该增强类的VC维的上界。",
    "tldr": "研究了使用不同标注函数的协作学习中，基于经验风险最小化算法在增强假设类上的高效学习方法。",
    "en_tdlr": "This paper investigates collaborative learning with different labeling functions and proposes an efficient learning method based on the Empirical Risk Minimization algorithm on an augmented hypothesis class."
}