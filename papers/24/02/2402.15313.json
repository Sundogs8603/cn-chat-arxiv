{
    "title": "ArabianGPT: Native Arabic GPT-based Large Language",
    "abstract": "arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t",
    "link": "https://arxiv.org/abs/2402.15313",
    "context": "Title: ArabianGPT: Native Arabic GPT-based Large Language\nAbstract: arXiv:2402.15313v1 Announce Type: cross  Abstract: The predominance of English and Latin-based large language models (LLMs) has led to a notable deficit in native Arabic LLMs. This discrepancy is accentuated by the prevalent inclusion of English tokens in existing Arabic models, detracting from their efficacy in processing native Arabic's intricate morphology and syntax. Consequently, there is a theoretical and practical imperative for developing LLMs predominantly focused on Arabic linguistic elements. To address this gap, this paper proposes ArabianGPT, a series of transformer-based models within the ArabianLLM suite designed explicitly for Arabic. These models, including ArabianGPT-0.1B and ArabianGPT-0.3B, vary in size and complexity, aligning with the nuanced linguistic characteristics of Arabic. The AraNizer tokenizer, integral to these models, addresses the unique morphological aspects of Arabic script, ensuring more accurate text processing. Empirical results from fine-tuning t",
    "path": "papers/24/02/2402.15313.json",
    "total_tokens": 755,
    "translated_title": "ArabianGPT：基于原生阿拉伯语的大型语言模型",
    "translated_abstract": "英语和拉丁语为主导的大型语言模型（LLMs）的主导地位导致了本土阿拉伯语LLMs的显著不足。本文提出ArabianGPT，这是一系列基于Transformer的模型，专门为阿拉伯语设计而成。这些模型包括ArabianGPT-0.1B和ArabianGPT-0.3B，大小和复杂性不同，与阿拉伯语的微妙语言特征相契合。",
    "tldr": "提出了ArabianGPT，这是一系列专门为阿拉伯语设计的基于Transformer的模型，包括大小和复杂性不同的ArabianGPT-0.1B和ArabianGPT-0.3B，帮助弥补了本土阿拉伯语大型语言模型的不足。",
    "en_tdlr": "Introducing ArabianGPT, a series of transformer-based models specifically designed for Arabic language, including ArabianGPT-0.1B and ArabianGPT-0.3B, to address the deficiency of native Arabic large language models."
}