{
    "title": "Spectral Filters, Dark Signals, and Attention Sinks",
    "abstract": "arXiv:2402.09221v1 Announce Type: new Abstract: Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens. We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands. We find that the signals exchanged in the tail end of the spectrum are responsible for attention sinking (Xiao et al. 2023), of which we provide an explanation. We find that the loss of pretrained models can be kept low despite suppressing sizable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved. Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum.",
    "link": "https://arxiv.org/abs/2402.09221",
    "context": "Title: Spectral Filters, Dark Signals, and Attention Sinks\nAbstract: arXiv:2402.09221v1 Announce Type: new Abstract: Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens. We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands. We find that the signals exchanged in the tail end of the spectrum are responsible for attention sinking (Xiao et al. 2023), of which we provide an explanation. We find that the loss of pretrained models can be kept low despite suppressing sizable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved. Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum.",
    "path": "papers/24/02/2402.09221.json",
    "total_tokens": 860,
    "translated_title": "频谱滤波器、暗信号和注意力陷阱",
    "translated_abstract": "arXiv:2402.09221v1 公告类型：新的 摘要：将中间表示投影到词汇表上是一种越来越流行的转换器型LLM的解释工具，也称为logit lens。我们对这种方法进行了定量扩展，并基于将词汇嵌入和解嵌矩阵的奇异向量分成波段来定义中间表示的频谱滤波器。我们发现，在频谱的尾部交换的信号负责注意力陷阱（Xiao et al. 2023），我们对此进行了解释。我们发现，尽管以层为基础抑制了嵌入频谱的相当大部分，但预训练模型的损失仍然可以保持较低，只要保持注意力陷阱即可。最后，我们发现在吸引多个令牌注意力的令牌表示在频谱的尾部具有较大的投影。",
    "tldr": "这项研究通过定义频谱滤波器和注意力陷阱，揭示了将中间表示投影到词汇表的解释工具对于transformer-based LLMs的重要性，并发现了预训练模型中特定频谱区域的损失对于保持低损失是可行的。"
}