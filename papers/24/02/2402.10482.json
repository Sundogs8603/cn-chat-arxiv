{
    "title": "Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise",
    "abstract": "arXiv:2402.10482v1 Announce Type: new  Abstract: Self-distillation (SD) is the process of training a student model using the outputs of a teacher model, with both models sharing the same architecture. Our study theoretically examines SD in multi-class classification with cross-entropy loss, exploring both multi-round SD and SD with refined teacher outputs, inspired by partial label learning (PLL). By deriving a closed-form solution for the student model's outputs, we discover that SD essentially functions as label averaging among instances with high feature correlations. Initially beneficial, this averaging helps the model focus on feature clusters correlated with a given instance for predicting the label. However, it leads to diminishing performance with increasing distillation rounds. Additionally, we demonstrate SD's effectiveness in label noise scenarios and identify the label corruption condition and minimum number of distillation rounds needed to achieve 100% classification accur",
    "link": "https://arxiv.org/abs/2402.10482",
    "context": "Title: Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise\nAbstract: arXiv:2402.10482v1 Announce Type: new  Abstract: Self-distillation (SD) is the process of training a student model using the outputs of a teacher model, with both models sharing the same architecture. Our study theoretically examines SD in multi-class classification with cross-entropy loss, exploring both multi-round SD and SD with refined teacher outputs, inspired by partial label learning (PLL). By deriving a closed-form solution for the student model's outputs, we discover that SD essentially functions as label averaging among instances with high feature correlations. Initially beneficial, this averaging helps the model focus on feature clusters correlated with a given instance for predicting the label. However, it leads to diminishing performance with increasing distillation rounds. Additionally, we demonstrate SD's effectiveness in label noise scenarios and identify the label corruption condition and minimum number of distillation rounds needed to achieve 100% classification accur",
    "path": "papers/24/02/2402.10482.json",
    "total_tokens": 948,
    "translated_title": "理解带有标签噪音的多类别分类中的自蒸馏和部分标签学习",
    "translated_abstract": "自蒸馏（SD）是使用教师模型的输出训练学生模型的过程，两个模型共享相同的架构。我们的研究从理论上考察了使用交叉熵损失的多类别分类中的SD，探索了多轮SD和具有精炼教师输出的SD，这些灵感来自部分标签学习（PLL）。通过推导学生模型输出的封闭形式解，我们发现SD本质上是在具有高特征相关性的实例之间进行标签平均。最初有益的平均化有助于模型专注于与给定实例相关联的特征簇以预测标签。然而，随着蒸馏轮次的增加，性能会下降。此外，我们展示了SD在标签噪声情景中的有效性，并确定实现100%分类准确率所需的标签损坏条件和最小蒸馏轮次数。",
    "tldr": "自蒸馏在多类别分类中扮演着标签平均化的角色，有助于模型关注与特定实例相关的特征簇以预测标签，但随着蒸馏轮次增加，性能会降低。此外，在标签噪声情景下自蒸馏被证明是有效的，找到了实现100%分类准确率所需的最小蒸馏轮次。",
    "en_tdlr": "Self-distillation plays a role of label averaging in multi-class classification, helping the model focus on feature clusters correlated with a given instance for label prediction, but performance decreases with increasing distillation rounds. Additionally, self-distillation is proven effective in label noise scenarios, with identified minimum distillation rounds required for achieving 100% classification accuracy."
}