{
    "title": "Neural Multigrid Architectures",
    "abstract": "We propose a convenient matrix-free neural architecture for the multigrid method. The architecture is simple enough to be implemented in less than fifty lines of code, yet it encompasses a large number of distinct multigrid solvers. We argue that a fixed neural network without dense layers can not realize an efficient iterative method. Because of that, standard training protocols do not lead to competitive solvers. To overcome this difficulty, we use parameter sharing and serialization of layers. The resulting network can be trained on linear problems with thousands of unknowns and retains its efficiency on problems with millions of unknowns. From the point of view of numerical linear algebra network's training corresponds to finding optimal smoothers for the geometric multigrid method. We demonstrate our approach on a few second-order elliptic equations. For tested linear systems, we obtain from two to five times smaller spectral radius of the error propagation matrix compare to a bas",
    "link": "https://arxiv.org/abs/2402.05563",
    "context": "Title: Neural Multigrid Architectures\nAbstract: We propose a convenient matrix-free neural architecture for the multigrid method. The architecture is simple enough to be implemented in less than fifty lines of code, yet it encompasses a large number of distinct multigrid solvers. We argue that a fixed neural network without dense layers can not realize an efficient iterative method. Because of that, standard training protocols do not lead to competitive solvers. To overcome this difficulty, we use parameter sharing and serialization of layers. The resulting network can be trained on linear problems with thousands of unknowns and retains its efficiency on problems with millions of unknowns. From the point of view of numerical linear algebra network's training corresponds to finding optimal smoothers for the geometric multigrid method. We demonstrate our approach on a few second-order elliptic equations. For tested linear systems, we obtain from two to five times smaller spectral radius of the error propagation matrix compare to a bas",
    "path": "papers/24/02/2402.05563.json",
    "total_tokens": 1035,
    "translated_title": "神经多重网格结构",
    "translated_abstract": "我们提出了一种方便的无矩阵神经网络多重网格方法。该架构足够简单，可以在不到五十行代码的情况下实施，并且包含大量不同的多重网格求解器。我们认为，没有密集层的固定神经网络无法实现高效的迭代方法。因此，标准的训练协议不能产生具有竞争力的求解器。为了克服这个困难，我们使用参数共享和层的序列化。所得到的网络可以在拥有数千个未知数的线性问题上进行训练，并在具有数百万个未知数的问题上保持其效率。从数值线性代数的角度来看，网络的训练对应于为几何多重网格方法寻找最佳光滑器。我们在几个二阶椭圆方程上演示了我们的方法。对于测试的线性系统，与基准相比，我们得到了两到五倍较小的误差传播矩阵的谱半径。",
    "tldr": "我们提出了一种简单且高效的神经网络多重网格方法，通过参数共享和层序列化实现了高效的训练，并在具有成千上万个未知数的线性问题以及具有数百万个未知数的问题上保持其效率。同时，在数值线性代数中，该网络的训练方法可以寻找最佳的光滑器，以提高几何多重网格方法的效果。我们在几个二阶椭圆方程上的实验结果表明，与基准相比，我们的方法能够显著降低误差传播矩阵的谱半径。",
    "en_tdlr": "We propose a simple and efficient neural network multigrid method that achieves high efficiency through parameter sharing and layer serialization, and maintains its efficiency on linear problems with thousands of unknowns and problems with millions of unknowns. In addition, the training method of the network can find the optimal smoother for geometric multigrid methods in numerical linear algebra. Experimental results on several second-order elliptic equations demonstrate that our method significantly reduces the spectral radius of the error propagation matrix compared to the baseline."
}