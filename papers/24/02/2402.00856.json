{
    "title": "Towards Efficient and Exact Optimization of Language Model Alignment",
    "abstract": "The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient op",
    "link": "https://arxiv.org/abs/2402.00856",
    "context": "Title: Towards Efficient and Exact Optimization of Language Model Alignment\nAbstract: The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient op",
    "path": "papers/24/02/2402.00856.json",
    "total_tokens": 888,
    "translated_title": "实现语言模型对齐的高效精确优化方法",
    "translated_abstract": "将语言模型与人类偏好进行对齐对于其在实际任务中的应用至关重要。该问题被建模为优化模型策略，以最大化反映人类偏好的预期奖励，并尽量减小与初始策略的偏差。尽管强化学习（RL）被认为是一种直接的解决方案，但其策略更新的方差很高，阻碍了高效的策略改进。最近，直接偏好优化（DPO）被提出以直接从偏好数据中优化策略。尽管实现简单，DPO是基于不一定能在实践中实现的最优策略导出的，这削弱了其收敛到预期解决方案的能力。本文提出了一种高效精确优化（EXO）的对齐目标方法。我们证明了对于策略的任意参数化，EXO保证渐近地与RL算法的优化方向一致，并且能够实现高效优化。",
    "tldr": "本论文提出了一种实现语言模型对齐的高效精确优化方法。通过证明，该方法能够在策略参数化任意的情况下，渐近地与强化学习算法的优化方向一致，并且能够实现高效优化。"
}