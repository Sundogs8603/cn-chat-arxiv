{
    "title": "Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM",
    "abstract": "Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated test suites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt's approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a",
    "link": "https://arxiv.org/abs/2402.00097",
    "context": "Title: Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM\nAbstract: Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent work using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated test suites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt's approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a",
    "path": "papers/24/02/2402.00097.json",
    "total_tokens": 894,
    "translated_title": "代码感知提示：基于LLM的回归设置下覆盖率导向的测试生成研究",
    "translated_abstract": "测试在确保软件质量方面起着至关重要的作用，然而传统的基于搜索的软件测试方法经常在复杂的软件单元上遇到困难，达不到最佳的测试覆盖率。最近使用大型语言模型（LLMs）进行测试生成的研究一直致力于通过优化测试生成上下文和纠正模型输出中的错误来改进生成质量，但使用了固定的提示策略，即提示模型在没有额外指导的情况下生成测试。因此，LLM生成的测试套件仍然存在低覆盖率的问题。在本文中，我们提出了SymPrompt，一种用于LLMs的代码感知提示策略来进行测试生成。SymPrompt的方法是基于最近的研究，该研究证明了LLMs在以多步方式思考问题时可以解决更复杂的逻辑问题。我们将这种方法应用于测试生成，将测试套件生成过程分解为多阶段序列，每个阶段都由一种驱动策略来推动。",
    "tldr": "本文提出了一种代码感知提示策略（SymPrompt），用于基于LLM的测试生成，通过将测试生成过程分解为多阶段序列，并以驱动策略推动每个阶段，改善了测试生成的覆盖率。"
}