{
    "title": "SAE: Single Architecture Ensemble Neural Networks",
    "abstract": "Ensembles of separate neural networks (NNs) have shown superior accuracy and confidence calibration over single NN across tasks. Recent methods compress ensembles within a single network via early exits or multi-input multi-output frameworks. However, the landscape of these methods is fragmented thus far, making it difficult to choose the right approach for a given task. Furthermore, the algorithmic performance of these methods is behind the ensemble of separate NNs and requires extensive architecture tuning. We propose a novel methodology unifying these approaches into a Single Architecture Ensemble (SAE). Our method learns the optimal number and depth of exits per ensemble input in a single NN. This enables the SAE framework to flexibly tailor its configuration for a given architecture or application. We evaluate SAEs on image classification and regression across various network architecture types and sizes. We demonstrate competitive accuracy or confidence calibration to baselines w",
    "link": "https://arxiv.org/abs/2402.06580",
    "context": "Title: SAE: Single Architecture Ensemble Neural Networks\nAbstract: Ensembles of separate neural networks (NNs) have shown superior accuracy and confidence calibration over single NN across tasks. Recent methods compress ensembles within a single network via early exits or multi-input multi-output frameworks. However, the landscape of these methods is fragmented thus far, making it difficult to choose the right approach for a given task. Furthermore, the algorithmic performance of these methods is behind the ensemble of separate NNs and requires extensive architecture tuning. We propose a novel methodology unifying these approaches into a Single Architecture Ensemble (SAE). Our method learns the optimal number and depth of exits per ensemble input in a single NN. This enables the SAE framework to flexibly tailor its configuration for a given architecture or application. We evaluate SAEs on image classification and regression across various network architecture types and sizes. We demonstrate competitive accuracy or confidence calibration to baselines w",
    "path": "papers/24/02/2402.06580.json",
    "total_tokens": 838,
    "translated_title": "SAE: 单一架构集合神经网络",
    "translated_abstract": "单一神经网络架构的集合能够在任务上显示出优越的准确性和置信度校准。最近的方法通过提前退出或多输入多输出框架将集合压缩到单一网络中。然而，这些方法的景观迄今为止是零散的，因此很难选择适合特定任务的方法。此外，这些方法的算法性能落后于独立神经网络的集合，并需要广泛的架构调整。我们提出了一种新的方法，将这些方法统一到单一架构集合中。我们的方法在单一神经网络中学习集合输入的最佳退出数量和深度。这使得SAE框架可以根据特定架构或应用程序灵活地定制其配置。我们评估了在各种网络架构类型和大小上进行图像分类和回归的SAE。我们展示了与基线相当的准确性或置信度校准。",
    "tldr": "SAE是一种单一架构集合神经网络方法，通过学习集合输入的最佳退出数量和深度，在任务上显示出优越的准确性和置信度校准。它能够根据特定架构或应用程序灵活地定制其配置。",
    "en_tdlr": "SAE is a single architecture ensemble neural network method that achieves superior accuracy and confidence calibration by learning the optimal number and depth of exits for ensemble inputs. It can flexibly tailor its configuration based on specific architectures or applications."
}