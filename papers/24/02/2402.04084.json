{
    "title": "Provably learning a multi-head attention layer",
    "abstract": "The multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models. Given a sequence length $k$, attention matrices $\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and projection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times d}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to \\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X}) \\triangleq \\sum^m_{i=1} \\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$. In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem:   - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns",
    "link": "https://arxiv.org/abs/2402.04084",
    "context": "Title: Provably learning a multi-head attention layer\nAbstract: The multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models. Given a sequence length $k$, attention matrices $\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and projection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times d}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to \\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X}) \\triangleq \\sum^m_{i=1} \\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$. In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem:   - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns",
    "path": "papers/24/02/2402.04084.json",
    "total_tokens": 1015,
    "translated_title": "可证学习多头注意力层",
    "translated_abstract": "多头注意力层是变形器架构的关键组件之一，使其与传统的前馈模型有所区别。通过给定序列长度 $k$，注意力矩阵 $\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$，以及投影矩阵 $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times d}$，相应的多头注意力层 $F: \\mathbb{R}^{k\\times d}\\to \\mathbb{R}^{k\\times d}$ 通过 $F(\\mathbf{X}) \\triangleq \\sum^m_{i=1} \\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$ 来转化长度为 $k$ 的 $d$ 维令牌序列 $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$。在这项工作中，我们开始研究通过随机示例可证学习多头注意力层，并为该问题给出了首个非平凡的上下界限制：假设 $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ 满足某些非退化条件，我们提供了一个 $(dk)^{O(m^3)}$ 时间复杂度的算法，用于学习。",
    "tldr": "该论文介绍了可证学习多头注意力层的研究，并给出了该问题的非平凡的上下界限制。",
    "en_tdlr": "This paper presents the study of provably learning a multi-head attention layer and provides the first nontrivial upper and lower bounds for this problem."
}