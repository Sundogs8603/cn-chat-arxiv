{
    "title": "ACTER: Diverse and Actionable Counterfactual Sequences for Explaining and Diagnosing RL Policies",
    "abstract": "Understanding how failure occurs and how it can be prevented in reinforcement learning (RL) is necessary to enable debugging, maintain user trust, and develop personalized policies. Counterfactual reasoning has often been used to assign blame and understand failure by searching for the closest possible world in which the failure is avoided. However, current counterfactual state explanations in RL can only explain an outcome using just the current state features and offer no actionable recourse on how a negative outcome could have been prevented. In this work, we propose ACTER (Actionable Counterfactual Sequences for Explaining Reinforcement Learning Outcomes), an algorithm for generating counterfactual sequences that provides actionable advice on how failure can be avoided. ACTER investigates actions leading to a failure and uses the evolutionary algorithm NSGA-II to generate counterfactual sequences of actions that prevent it with minimal changes and high certainty even in stochastic ",
    "link": "https://arxiv.org/abs/2402.06503",
    "context": "Title: ACTER: Diverse and Actionable Counterfactual Sequences for Explaining and Diagnosing RL Policies\nAbstract: Understanding how failure occurs and how it can be prevented in reinforcement learning (RL) is necessary to enable debugging, maintain user trust, and develop personalized policies. Counterfactual reasoning has often been used to assign blame and understand failure by searching for the closest possible world in which the failure is avoided. However, current counterfactual state explanations in RL can only explain an outcome using just the current state features and offer no actionable recourse on how a negative outcome could have been prevented. In this work, we propose ACTER (Actionable Counterfactual Sequences for Explaining Reinforcement Learning Outcomes), an algorithm for generating counterfactual sequences that provides actionable advice on how failure can be avoided. ACTER investigates actions leading to a failure and uses the evolutionary algorithm NSGA-II to generate counterfactual sequences of actions that prevent it with minimal changes and high certainty even in stochastic ",
    "path": "papers/24/02/2402.06503.json",
    "total_tokens": 802,
    "translated_title": "ACTER: 用于解释和诊断RL策略的多样且可行的反事实序列",
    "translated_abstract": "了解强化学习（RL）中的失败如何发生以及如何防止是为了实现调试、维护用户信任和开发个性化策略而必要的。反事实推理经常被用来归咎和理解失败，通过寻找最接近的可能世界以避免失败。然而，当前RL中的反事实状态解释只能使用当前状态特征来解释结果，并不能提供关于如何预防负结果的可行性措施。在这项工作中，我们提出了ACTER（用于解释强化学习结果的可行反事实序列）算法，该算法生成可行的反事实序列，提供了关于如何避免失败的可行建议。ACTER研究导致失败的动作，并使用进化算法NSGA-II生成可以最小化改变且具有高确定性的反事实动作序列，以防止失败，即使在随机情况下也是如此。",
    "tldr": "ACTER是一个算法，用于生成可行的反事实序列，提供关于如何避免RL策略失败的可行建议。",
    "en_tdlr": "ACTER is an algorithm that generates actionable counterfactual sequences to provide feasible advice on avoiding failures in RL policies."
}