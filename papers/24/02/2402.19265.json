{
    "title": "Learning Logic Specifications for Policy Guidance in POMDPs: an Inductive Logic Programming Approach",
    "abstract": "arXiv:2402.19265v1 Announce Type: new  Abstract: Partially Observable Markov Decision Processes (POMDPs) are a powerful framework for planning under uncertainty. They allow to model state uncertainty as a belief probability distribution. Approximate solvers based on Monte Carlo sampling show great success to relax the computational demand and perform online planning. However, scaling to complex realistic domains with many actions and long planning horizons is still a major challenge, and a key point to achieve good performance is guiding the action-selection process with domain-dependent policy heuristics which are tailored for the specific application domain. We propose to learn high-quality heuristics from POMDP traces of executions generated by any solver. We convert the belief-action pairs to a logical semantics, and exploit data- and time-efficient Inductive Logic Programming (ILP) to generate interpretable belief-based policy specifications, which are then used as online heuristi",
    "link": "https://arxiv.org/abs/2402.19265",
    "context": "Title: Learning Logic Specifications for Policy Guidance in POMDPs: an Inductive Logic Programming Approach\nAbstract: arXiv:2402.19265v1 Announce Type: new  Abstract: Partially Observable Markov Decision Processes (POMDPs) are a powerful framework for planning under uncertainty. They allow to model state uncertainty as a belief probability distribution. Approximate solvers based on Monte Carlo sampling show great success to relax the computational demand and perform online planning. However, scaling to complex realistic domains with many actions and long planning horizons is still a major challenge, and a key point to achieve good performance is guiding the action-selection process with domain-dependent policy heuristics which are tailored for the specific application domain. We propose to learn high-quality heuristics from POMDP traces of executions generated by any solver. We convert the belief-action pairs to a logical semantics, and exploit data- and time-efficient Inductive Logic Programming (ILP) to generate interpretable belief-based policy specifications, which are then used as online heuristi",
    "path": "papers/24/02/2402.19265.json",
    "total_tokens": 834,
    "translated_title": "在POMDPs中学习逻辑规范以指导政策：归纳逻辑编程方法",
    "translated_abstract": "部分可观察马尔可夫决策过程（POMDPs）是一个强大的不确定性规划框架，允许将状态不确定性建模为信念概率分布。基于蒙特卡洛采样的近似求解器显示出很大成功，以放宽计算需求并执行在线规划。然而，扩展到具有许多动作和长期规划视野的复杂现实域仍然是一个重大挑战，实现良好性能的关键点是通过定制特定应用域的领域相关策略启发来引导行动选择过程。我们提出从由任何求解器生成的POMDP执行痕迹中学习高质量启发式。我们将信念-动作对转换为逻辑语义，并利用数据和时间高效的归纳逻辑编程（ILP）生成可解释的基于信念的策略规范，然后将其用作在线启发式。",
    "tldr": "通过归纳逻辑编程方法，从POMDP执行痕迹中学习高质量启发式，以指导政策选择过程。"
}