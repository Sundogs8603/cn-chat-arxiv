{
    "title": "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning",
    "abstract": "arXiv:2402.11690v1 Announce Type: new  Abstract: Despite vision-language models' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional ",
    "link": "https://arxiv.org/abs/2402.11690",
    "context": "Title: Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning\nAbstract: arXiv:2402.11690v1 Announce Type: new  Abstract: Despite vision-language models' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional ",
    "path": "papers/24/02/2402.11690.json",
    "total_tokens": 870,
    "translated_title": "Vision-Flan：扩展视觉指导调整中的人类标记任务",
    "translated_abstract": "尽管视觉-语言模型（VLMs）作为多功能视觉助手具有显著的能力，但现有VLM框架中仍存在两个重大挑战：（1）在预训练和视觉指导调整中缺乏任务多样性，以及（2）在GPT-4合成指导调整数据中存在注释错误和偏见。这两个挑战导致问题，如泛化能力差、幻觉和灾难性遗忘。为解决这些挑战，我们构建了Vision-Flan，这是迄今为止最多样化的公开可用视觉指导调整数据集，包括187个多样化任务和从学术数据集中提取的1,664,261个实例，每个任务都附带专家撰写的指导。此外，我们提出了一个两阶段指导调整框架，其中VLM首先在Vision-Flan上进行微调，然后在GPT-4合成数据上进一步进行调整。我们发现这种两阶段调整框架明显优于传统方法。",
    "tldr": "构建了最多样化的视觉指导调整数据集Vision-Flan，提出了两阶段指导调整框架，显著优于传统方法"
}