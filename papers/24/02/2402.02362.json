{
    "title": "Unification of Symmetries Inside Neural Networks: Transformer, Feedforward and Neural ODE",
    "abstract": "Understanding the inner workings of neural networks, including transformers, remains one of the most challenging puzzles in machine learning. This study introduces a novel approach by applying the principles of gauge symmetries, a key concept in physics, to neural network architectures. By regarding model functions as physical observables, we find that parametric redundancies of various machine learning models can be interpreted as gauge symmetries. We mathematically formulate the parametric redundancies in neural ODEs, and find that their gauge symmetries are given by spacetime diffeomorphisms, which play a fundamental role in Einstein's theory of gravity. Viewing neural ODEs as a continuum version of feedforward neural networks, we show that the parametric redundancies in feedforward neural networks are indeed lifted to diffeomorphisms in neural ODEs. We further extend our analysis to transformer models, finding natural correspondences with neural ODEs and their gauge symmetries. The",
    "link": "https://arxiv.org/abs/2402.02362",
    "context": "Title: Unification of Symmetries Inside Neural Networks: Transformer, Feedforward and Neural ODE\nAbstract: Understanding the inner workings of neural networks, including transformers, remains one of the most challenging puzzles in machine learning. This study introduces a novel approach by applying the principles of gauge symmetries, a key concept in physics, to neural network architectures. By regarding model functions as physical observables, we find that parametric redundancies of various machine learning models can be interpreted as gauge symmetries. We mathematically formulate the parametric redundancies in neural ODEs, and find that their gauge symmetries are given by spacetime diffeomorphisms, which play a fundamental role in Einstein's theory of gravity. Viewing neural ODEs as a continuum version of feedforward neural networks, we show that the parametric redundancies in feedforward neural networks are indeed lifted to diffeomorphisms in neural ODEs. We further extend our analysis to transformer models, finding natural correspondences with neural ODEs and their gauge symmetries. The",
    "path": "papers/24/02/2402.02362.json",
    "total_tokens": 975,
    "translated_title": "神经网络内部的对称性统一：Transformer, 前馈和神经ODE",
    "translated_abstract": "理解神经网络内部运作，包括transformers，仍然是机器学习中最具挑战性的难题之一。本研究将物理学中的规范对称性原理应用于神经网络架构，提出了一种新颖的方法。通过将模型函数视为物理可观测量，发现各种机器学习模型的参数冗余可以解释为规范对称性。我们在神经ODE中数学形式化了参数冗余，并发现它们的规范对称性由时空微分同胚给出，这在爱因斯坦的引力理论中起着基础性作用。我们将神经ODE视为连续版本的前馈神经网络，证明了前馈神经网络中的参数冗余确实在神经ODE中升级为微分同胚。我们进一步将分析扩展到Transformer模型，找到了与神经ODE及其规范对称性的自然对应关系。",
    "tldr": "本研究通过应用物理学中的规范对称性原理，将其应用于神经网络架构，发现各种机器学习模型的参数冗余可以解释为规范对称性。并证明了前馈神经网络中的参数冗余在神经ODE中升级为微分同胚，并找到了Transformer模型与神经ODE及其规范对称性的自然对应关系。"
}