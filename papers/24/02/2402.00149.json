{
    "title": "The Impact of Language Adapters in Cross-Lingual Transfer for NLU",
    "abstract": "Modular deep learning has been proposed for the efficient adaption of pre-trained models to new tasks, domains and languages. In particular, combining language adapters with task adapters has shown potential where no supervised data exists for a language. In this paper, we explore the role of language adapters in zero-shot cross-lingual transfer for natural language understanding (NLU) benchmarks. We study the effect of including a target-language adapter in detailed ablation studies with two multilingual models and three multilingual datasets. Our results show that the effect of target-language adapters is highly inconsistent across tasks, languages and models. Retaining the source-language adapter instead often leads to an equivalent, and sometimes to a better, performance. Removing the language adapter after training has only a weak negative effect, indicating that the language adapters do not have a strong impact on the predictions.",
    "link": "https://arxiv.org/abs/2402.00149",
    "context": "Title: The Impact of Language Adapters in Cross-Lingual Transfer for NLU\nAbstract: Modular deep learning has been proposed for the efficient adaption of pre-trained models to new tasks, domains and languages. In particular, combining language adapters with task adapters has shown potential where no supervised data exists for a language. In this paper, we explore the role of language adapters in zero-shot cross-lingual transfer for natural language understanding (NLU) benchmarks. We study the effect of including a target-language adapter in detailed ablation studies with two multilingual models and three multilingual datasets. Our results show that the effect of target-language adapters is highly inconsistent across tasks, languages and models. Retaining the source-language adapter instead often leads to an equivalent, and sometimes to a better, performance. Removing the language adapter after training has only a weak negative effect, indicating that the language adapters do not have a strong impact on the predictions.",
    "path": "papers/24/02/2402.00149.json",
    "total_tokens": 815,
    "translated_title": "语言适配器在跨语言NLU传输中的影响",
    "translated_abstract": "提出了模块化深度学习用于将预训练模型高效地适用于新的任务、领域和语言。特别是，在没有针对某种语言的监督数据的情况下，将语言适配器与任务适配器结合起来展示了潜力。本文研究了在自然语言理解（NLU）基准测试中零样本跨语言传输中语言适配器的作用。我们通过详细的消融研究，使用两个多语言模型和三个多语言数据集，探讨了包含目标语言适配器的效果。我们的结果表明，目标语言适配器的效果在任务、语言和模型之间高度不一致。保留源语言适配器通常会导致相同甚至更好的性能。在训练后去掉语言适配器对预测的影响仅有弱的负面效果，表明语言适配器对预测没有强烈的影响。",
    "tldr": "本文研究了在零样本跨语言传输中语言适配器的作用，结果显示其在任务、语言和模型之间的效果不一致。保留源语言适配器通常导致相同或更好的性能。"
}