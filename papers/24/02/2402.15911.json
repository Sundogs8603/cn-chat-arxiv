{
    "title": "PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails",
    "abstract": "arXiv:2402.15911v1 Announce Type: cross  Abstract: Large language models (LLMs) are typically aligned to be harmless to humans. Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content. More recent LLMs often incorporate an additional layer of defense, a Guard Model, which is a second LLM that is designed to check and moderate the output response of the primary LLM. Our key contribution is to show a novel attack strategy, PRP, that is successful against several open-source (e.g., Llama 2) and closed-source (e.g., GPT 3.5) implementations of Guard Models. PRP leverages a two step prefix-based attack that operates by (a) constructing a universal adversarial prefix for the Guard Model, and (b) propagating this prefix to the response. We find that this procedure is effective across multiple threat models, including ones in which the adversary has no access to the Guard Model at all. Our work suggests t",
    "link": "https://arxiv.org/abs/2402.15911",
    "context": "Title: PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails\nAbstract: arXiv:2402.15911v1 Announce Type: cross  Abstract: Large language models (LLMs) are typically aligned to be harmless to humans. Unfortunately, recent work has shown that such models are susceptible to automated jailbreak attacks that induce them to generate harmful content. More recent LLMs often incorporate an additional layer of defense, a Guard Model, which is a second LLM that is designed to check and moderate the output response of the primary LLM. Our key contribution is to show a novel attack strategy, PRP, that is successful against several open-source (e.g., Llama 2) and closed-source (e.g., GPT 3.5) implementations of Guard Models. PRP leverages a two step prefix-based attack that operates by (a) constructing a universal adversarial prefix for the Guard Model, and (b) propagating this prefix to the response. We find that this procedure is effective across multiple threat models, including ones in which the adversary has no access to the Guard Model at all. Our work suggests t",
    "path": "papers/24/02/2402.15911.json",
    "total_tokens": 896,
    "translated_title": "PRP：传播通用扰动以攻击大型语言模型的守护栏",
    "translated_abstract": "大型语言模型（LLMs）通常被设计为对人类无害。不幸的是，最近的研究表明，这些模型容易受到自动越狱攻击的影响，诱使它们生成有害内容。最近的LLMs通常包含了一个额外的防御层，即守护模型，这是第二个LLM，旨在检查和调节主要LLM的输出响应。我们的主要贡献是展示了一种新颖的攻击策略PRP，该策略成功地针对几种开源（如Llama 2）和闭源（如GPT 3.5）守护模型实施。PRP利用了一种两步基于前缀的攻击，通过（a）构建守护模型的通用对抗前缀，并（b）将此前缀传播到响应中。我们发现这一过程在多种威胁模型中都是有效的，包括对手根本无法访问守护模型的模型。我们的工作暗示了...",
    "tldr": "PRP攻击策略成功地针对多种开源和闭源的守护模型实施了两步前缀攻击，有效跨越多个威胁模型。",
    "en_tdlr": "PRP attack strategy successfully implemented a two-step prefix attack against various open-source and closed-source guard models, effectively crossing multiple threat models."
}