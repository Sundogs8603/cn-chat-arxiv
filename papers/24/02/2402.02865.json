{
    "title": "On combining acoustic and modulation spectrograms in an attention LSTM-based system for speech intelligibility level classification",
    "abstract": "Speech intelligibility can be affected by multiple factors, such as noisy environments, channel distortions or physiological issues. In this work, we deal with the problem of automatic prediction of the speech intelligibility level in this latter case. Starting from our previous work, a non-intrusive system based on LSTM networks with attention mechanism designed for this task, we present two main contributions. In the first one, it is proposed the use of per-frame modulation spectrograms as input features, instead of compact representations derived from them that discard important temporal information. In the second one, two different strategies for the combination of per-frame acoustic log-mel and modulation spectrograms into the LSTM framework are explored: at decision level or late fusion and at utterance level or Weighted-Pooling (WP) fusion. The proposed models are evaluated with the UA-Speech database that contains dysarthric speech with different degrees of severity. On the one",
    "link": "https://arxiv.org/abs/2402.02865",
    "context": "Title: On combining acoustic and modulation spectrograms in an attention LSTM-based system for speech intelligibility level classification\nAbstract: Speech intelligibility can be affected by multiple factors, such as noisy environments, channel distortions or physiological issues. In this work, we deal with the problem of automatic prediction of the speech intelligibility level in this latter case. Starting from our previous work, a non-intrusive system based on LSTM networks with attention mechanism designed for this task, we present two main contributions. In the first one, it is proposed the use of per-frame modulation spectrograms as input features, instead of compact representations derived from them that discard important temporal information. In the second one, two different strategies for the combination of per-frame acoustic log-mel and modulation spectrograms into the LSTM framework are explored: at decision level or late fusion and at utterance level or Weighted-Pooling (WP) fusion. The proposed models are evaluated with the UA-Speech database that contains dysarthric speech with different degrees of severity. On the one",
    "path": "papers/24/02/2402.02865.json",
    "total_tokens": 908,
    "translated_title": "使用声学和调制谱图结合的注意力LSTM系统对语音可懂性级别进行分类的研究",
    "translated_abstract": "语音可懂性受到多种因素的影响，如嘈杂环境、信道失真或生理问题。本研究针对后一种情况，解决了自动预测语音可懂性级别的问题。在我们以前的工作的基础上，提出了两个主要贡献。第一，提出了使用逐帧调制谱图作为输入特征，而不是从中获取丢弃重要时序信息的紧凑表示。第二，探讨了将逐帧声学对数梅尔和调制谱图在LSTM框架中进行决策级融合或后期融合以及话语级加权池化（WP）融合的两种不同策略。提出的模型在包含不同严重程度的口吃语音的UA-Speech数据库上进行评估。",
    "tldr": "本研究探讨了使用声学和调制谱图相结合的注意力LSTM系统对语音可懂性级别进行分类的方法，并提出了使用逐帧调制谱图作为输入特征以及两种不同的融合策略。模型在包含不同严重程度的口吃语音的UA-Speech数据库上进行了评估。"
}