{
    "title": "Direct Language Model Alignment from Online AI Feedback",
    "abstract": "Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF ",
    "link": "https://arxiv.org/abs/2402.04792",
    "context": "Title: Direct Language Model Alignment from Online AI Feedback\nAbstract: Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF ",
    "path": "papers/24/02/2402.04792.json",
    "total_tokens": 893,
    "translated_title": "来自在线人工智能反馈的直接语言模型对齐",
    "translated_abstract": "最近，直接对齐偏好（DAP）方法如DPO已成为对于从人类反馈中进行增强学习的高效替代方法，不要求单独的奖励模型。然而，DAP方法中使用的偏好数据集通常在训练之前收集，并且从不更新，因此反馈纯粹是离线的。此外，这些数据集中的回应通常是从一个与被对齐的语言模型不同的语言模型中采样的，由于模型在训练过程中会变化，对齐阶段必然是非策略的。在本研究中，我们认为在线反馈是关键，可以改善DAP方法。我们的方法，在线人工智能反馈（OAIF），使用LLM作为标注器：在每个训练迭代中，我们从当前模型中采样两个回应，并提示LLM标注器选择哪个更受欢迎，从而提供在线反馈。尽管简单，但通过多个任务中的人工评估，我们证明OAIF优于离线DAP和RLHF",
    "tldr": "本论文提出了一种名为OAIF的方法，通过在线反馈来改善DAP方法，该方法使用LLM作为标注器，从而在多个任务中展示出优于离线DAP和RLHF的性能",
    "en_tdlr": "This paper proposes a method called OAIF that improves DAP methods by utilizing online feedback from an LLM as an annotator, demonstrating superior performance compared to offline DAP and RLHF in multiple tasks through human evaluation."
}