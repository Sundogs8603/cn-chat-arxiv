{
    "title": "TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks",
    "abstract": "arXiv:2402.11137v1 Announce Type: new  Abstract: While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs by developing context optimization techniques for PFNs. Specifically, we propose TuneTables, a novel prompt-tuning strategy that compresses large datasets into a smaller learned context. TuneTables scales TabPFN to be competitive with state-of-the-art tabular classification methods on larger datas",
    "link": "https://arxiv.org/abs/2402.11137",
    "context": "Title: TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks\nAbstract: arXiv:2402.11137v1 Announce Type: new  Abstract: While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs by developing context optimization techniques for PFNs. Specifically, we propose TuneTables, a novel prompt-tuning strategy that compresses large datasets into a smaller learned context. TuneTables scales TabPFN to be competitive with state-of-the-art tabular classification methods on larger datas",
    "path": "papers/24/02/2402.11137.json",
    "total_tokens": 844,
    "translated_title": "TuneTables：可扩展先验数据拟合网络的上下文优化",
    "translated_abstract": "针对表格分类传统上依赖于从零开始训练的问题，最近提出了一个名为先验数据拟合网络（PFN）的突破性方法，挑战了这种方法。类似于大型语言模型，PFN利用预训练和上下文学习，在单次前向传递中在新任务上取得强大表现。然而，当前的PFN存在限制，阻碍了它们的广泛采用。特别是，TabPFN在小型表格数据集上取得非常强劲的性能，但并不适用于数据集大小大于1000的预测。在这项工作中，我们通过为PFN开发上下文优化技术，克服了这些限制，大幅提高了PFN的性能。具体来说，我们提出了TuneTables，一种将大型数据集压缩为较小学习上下文的新型提示调整策略。TuneTables将TabPFN扩展到与更大数据上的最先进表格分类方法相竞争。",
    "tldr": "提出了TuneTables上下文优化技术，通过开发一种新的提示调整策略，将TabPFN扩展到与更大数据上的最先进表格分类方法相竞争。",
    "en_tdlr": "Introduced context optimization techniques TuneTables, which extends TabPFN to compete with state-of-the-art tabular classification methods on larger datasets by developing a new prompt-tuning strategy."
}