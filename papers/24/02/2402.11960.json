{
    "title": "DB-LLM: Accurate Dual-Binarization for Efficient LLMs",
    "abstract": "arXiv:2402.11960v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs. However, existing ultra-low-bit quantization always causes severe accuracy drops. In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit quantization and present a novel Dual-Binarization method for LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while reta",
    "link": "https://arxiv.org/abs/2402.11960",
    "context": "Title: DB-LLM: Accurate Dual-Binarization for Efficient LLMs\nAbstract: arXiv:2402.11960v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs. However, existing ultra-low-bit quantization always causes severe accuracy drops. In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit quantization and present a novel Dual-Binarization method for LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while reta",
    "path": "papers/24/02/2402.11960.json",
    "total_tokens": 892,
    "translated_title": "DB-LLM: 高效LLM的准确双二值化",
    "translated_abstract": "大型语言模型(LLMs)显著推进了自然语言处理领域，然而高昂的内存和计算开销阻碍了它们的实际部署。量化成为改善LLMs计算效率的最有效方法之一。然而，现有的超低比特量化总是导致严重的精度下降。本文通过实证研究缓解了超低比特量化的微观和宏观特性，提出了一种新颖的LLMs双二值化方法，即DB-LLM。对于微观层面，我们考虑了2位宽度的准确性优势和二值化的效率优势，引入了灵活双二值化(FDB)。通过将2位量化权重分为两组独立的二进制数集，FDB确保了表示的准确性并引入了灵活性，利用二值化的高效位操作同时保留了",
    "tldr": "本文提出了一种名为DB-LLM的新颖双二值化方法，通过引入灵活双二值化(FDB)来平衡2位宽度的精度优势和二值化的效率优势，从而在提高LLMs的计算效率的同时保持了准确性。",
    "en_tdlr": "This paper presents a novel Dual-Binarization method for LLMs, namely DB-LLM, which balances the accuracy advantage of 2-bit-width with the efficiency advantage of binarization by introducing Flexible Dual Binarization (FDB), aiming to improve the computational efficiency of LLMs while maintaining accuracy."
}