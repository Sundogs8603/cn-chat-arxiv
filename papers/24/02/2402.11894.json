{
    "title": "Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation",
    "abstract": "arXiv:2402.11894v1 Announce Type: new  Abstract: Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing benchmarks. On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues. In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poo",
    "link": "https://arxiv.org/abs/2402.11894",
    "context": "Title: Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation\nAbstract: arXiv:2402.11894v1 Announce Type: new  Abstract: Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing benchmarks. On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues. In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poo",
    "path": "papers/24/02/2402.11894.json",
    "total_tokens": 824,
    "translated_title": "你见过我吗？自动化数据集更新以实现可靠及及时的评估",
    "translated_abstract": "由于大型语言模型（LLMs）的能力不断扩大和预训练数据的增加，LLMs面临着日益严重的评估挑战。一方面，数据泄漏问题导致对现有基准的过度估计。另一方面，定期手动整理数据集成本高昂。本文提出自动化数据集更新以实现可靠及及时的评估。基本思想是基于现有样本生成看不见的高质量测试样本以减轻泄漏问题。具体而言，我们提出了两种策略并进行了系统验证。第一种是模仿策略，利用LLMs创建类似现有样本的新样本，最大程度地保留原始数据集的风格。我们的实验表明它在多次实例中的评估稳定性以及在大多数情况下处理数据泄漏问题的有效性。",
    "tldr": "本文提出自动化数据集更新策略，利用两种系统验证过的策略生成看不见和高质量的测试样本，以缓解数据泄漏问题并实现评估稳定性。",
    "en_tdlr": "This paper proposes an automated dataset update strategy that generates unseen and high-quality testing samples using two systematically verified strategies to mitigate data leakage issues and achieve evaluation stability."
}