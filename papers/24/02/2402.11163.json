{
    "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
    "abstract": "arXiv:2402.11163v1 Announce Type: new  Abstract: In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLM",
    "link": "https://arxiv.org/abs/2402.11163",
    "context": "Title: KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph\nAbstract: arXiv:2402.11163v1 Announce Type: new  Abstract: In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLM",
    "path": "papers/24/02/2402.11163.json",
    "total_tokens": 856,
    "translated_title": "KG-Agent: 一种用于在知识图谱上进行复杂推理的高效自主代理框架",
    "translated_abstract": "在这篇论文中，我们旨在提高大型语言模型（LLMs）在知识图谱（KGs）上回答复杂问题的推理能力。受到现有设计LLMs和KG之间交互策略方法的启发，我们提出了一种名为KG-Agent的自主LLM代理框架，该框架使得小型LLM能够在完成对KG的推理过程中主动做出决策。在KG-Agent中，我们整合了LLM、多功能工具箱、基于KG的执行器和知识存储器，并开发了一个迭代机制，该机制自主选择工具然后更新内存以进行在KG上的推理。为了保证有效性，我们利用程序语言来制定在KG上的多跳推理过程，并合成基于代码的指令数据集来微调基础LLM。大量实验表明，仅使用10K个样本来调整LLaMA-7B比使用更大LLM的最先进方法表现更好。",
    "tldr": "提出了一种名为KG-Agent的高效自主代理框架，该框架通过在KG上进行推理来提高大型语言模型的复杂问题回答能力",
    "en_tdlr": "Introducing KG-Agent, an efficient autonomous agent framework, which enhances the reasoning ability of large language models in answering complex questions by reasoning over knowledge graphs."
}