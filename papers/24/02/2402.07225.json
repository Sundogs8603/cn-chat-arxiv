{
    "title": "Rethinking Graph Masked Autoencoders through Alignment and Uniformity",
    "abstract": "Self-supervised learning on graphs can be bifurcated into contrastive and generative methods. Contrastive methods, also known as graph contrastive learning (GCL), have dominated graph self-supervised learning in the past few years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles the momentum behind generative methods. Despite the empirical success of GraphMAE, there is still a dearth of theoretical understanding regarding its efficacy. Moreover, while both generative and contrastive methods have been shown to be effective, their connections and differences have yet to be thoroughly investigated. Therefore, we theoretically build a bridge between GraphMAE and GCL, and prove that the node-level reconstruction objective in GraphMAE implicitly performs context-level GCL. Based on our theoretical analysis, we further identify the limitations of the GraphMAE from the perspectives of alignment and uniformity, which have been considered as two key properties of high-qual",
    "link": "https://arxiv.org/abs/2402.07225",
    "context": "Title: Rethinking Graph Masked Autoencoders through Alignment and Uniformity\nAbstract: Self-supervised learning on graphs can be bifurcated into contrastive and generative methods. Contrastive methods, also known as graph contrastive learning (GCL), have dominated graph self-supervised learning in the past few years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles the momentum behind generative methods. Despite the empirical success of GraphMAE, there is still a dearth of theoretical understanding regarding its efficacy. Moreover, while both generative and contrastive methods have been shown to be effective, their connections and differences have yet to be thoroughly investigated. Therefore, we theoretically build a bridge between GraphMAE and GCL, and prove that the node-level reconstruction objective in GraphMAE implicitly performs context-level GCL. Based on our theoretical analysis, we further identify the limitations of the GraphMAE from the perspectives of alignment and uniformity, which have been considered as two key properties of high-qual",
    "path": "papers/24/02/2402.07225.json",
    "total_tokens": 911,
    "translated_title": "通过对齐和一致性重新思考图形遮罩自编码器",
    "translated_abstract": "自监督学习在图中可以分为对比和生成两种方法。过去几年，对比方法，也被称为图对比学习（GCL），在图的自监督学习中占据了主导地位，但最近出现的图形遮罩自编码器（GraphMAE）重新点燃了生成方法的动力。尽管GraphMAE在实证上取得了成功，但对其有效性仍缺乏理论理解。此外，虽然生成和对比方法都被证明是有效的，但它们之间的联系和差异尚未得到全面研究。因此，我们在理论上建立了GraphMAE和GCL之间的桥梁，并证明了GraphMAE中的节点级重构目标隐含地执行了上下文级GCL。基于我们的理论分析，我们进一步从对齐和一致性的角度识别了GraphMAE的局限性，这被认为是高质量图的两个关键属性之一。",
    "tldr": "通过对齐和一致性重新思考图形遮罩自编码器对图的自监督学习方法进行了理论分析，揭示了GraphMAE中的节点级重构目标实际上执行了上下文级对比学习，并指出了GraphMAE在对齐和一致性方面的局限性。"
}