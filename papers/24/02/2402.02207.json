{
    "title": "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models",
    "abstract": "Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empi",
    "link": "https://arxiv.org/abs/2402.02207",
    "context": "Title: Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models\nAbstract: Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safe instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empi",
    "path": "papers/24/02/2402.02207.json",
    "total_tokens": 1086,
    "translated_title": "安全微调几乎零成本：大规模语言模型视觉基线",
    "translated_abstract": "当前的大规模语言模型（VLLMs）具有非凡的能力，但容易生成有害内容，并且容易受到简单越狱攻击的影响。我们的初步分析发现，这是由于在视觉语言指令微调过程中存在有害数据，并且VLLM的微调会导致对底层LLM之前学习的安全对齐性的遗忘。为了解决这个问题，我们首先策划了一个包含各种有害类别的视觉语言安全指令遵循数据集VLGuard。我们的实验表明，将该数据集集成到标准的视觉语言微调中或将其用于事后微调，可以有效地对齐VLLMs的安全性。这种对齐是在对模型的有益性几乎没有影响甚至有所提升的情况下实现的。我们安全微调数据集的多样性使其成为对现有VLLMs进行安全测试，训练新模型或保护预训练VLLMs的宝贵资源。",
    "tldr": "该论文研究了大规模语言模型在安全微调过程中存在的问题，并提出了一种可行的解决方案。他们首先策划了一个包含各种有害类别的视觉语言安全指令遵循数据集VLGuard，并通过实验证明将该数据集集成到视觉语言微调中或进行事后微调，可以有效地对齐VLLMs的安全性。这对模型的有益性几乎没有影响甚至有所提升。这项研究的贡献是提供了一个有价值的资源，用于对现有VLLMs进行安全测试、训练新模型或保护预训练VLLMs。",
    "en_tdlr": "This paper investigates the issues of safety fine-tuning in large language models and proposes a feasible solution. They curate a vision-language safe instruction-following dataset VLGuard, covering various harmful categories, and demonstrate that integrating this dataset into fine-tuning effectively aligns the safety of the models with minimal impact on their helpfulness. The contribution of this research is the provision of a valuable resource for safety-testing existing models, training new models, or safeguarding pre-trained models."
}