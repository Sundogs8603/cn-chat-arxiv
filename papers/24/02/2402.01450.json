{
    "title": "Improving importance estimation in covariate shift for providing accurate prediction error",
    "abstract": "In traditional Machine Learning, the algorithms predictions are based on the assumption that the data follows the same distribution in both the training and the test datasets. However, in real world data this condition does not hold and, for instance, the distribution of the covariates changes whereas the conditional distribution of the targets remains unchanged. This situation is called covariate shift problem where standard error estimation may be no longer accurate. In this context, the importance is a measure commonly used to alleviate the influence of covariate shift on error estimations. The main drawback is that it is not easy to compute. The Kullback-Leibler Importance Estimation Procedure (KLIEP) is capable of estimating importance in a promising way. Despite its good performance, it fails to ignore target information, since it only includes the covariates information for computing the importance. In this direction, this paper explores the potential performance improvement if ",
    "link": "https://rss.arxiv.org/abs/2402.01450",
    "context": "Title: Improving importance estimation in covariate shift for providing accurate prediction error\nAbstract: In traditional Machine Learning, the algorithms predictions are based on the assumption that the data follows the same distribution in both the training and the test datasets. However, in real world data this condition does not hold and, for instance, the distribution of the covariates changes whereas the conditional distribution of the targets remains unchanged. This situation is called covariate shift problem where standard error estimation may be no longer accurate. In this context, the importance is a measure commonly used to alleviate the influence of covariate shift on error estimations. The main drawback is that it is not easy to compute. The Kullback-Leibler Importance Estimation Procedure (KLIEP) is capable of estimating importance in a promising way. Despite its good performance, it fails to ignore target information, since it only includes the covariates information for computing the importance. In this direction, this paper explores the potential performance improvement if ",
    "path": "papers/24/02/2402.01450.json",
    "total_tokens": 832,
    "translated_title": "在协变量偏移中改进重要性估计以提供准确的预测误差",
    "translated_abstract": "在传统的机器学习中，算法的预测基于训练集和测试集中的数据遵循相同的分布的假设。然而，在现实世界的数据中，这个条件并不成立，例如，协变量的分布发生了变化，而目标的条件分布保持不变。这种情况被称为协变量偏移问题，标准误差估计可能不再准确。在这种情况下，重要性是一种常用的度量，用于减轻协变量偏移对误差估计的影响。主要的缺点是它不容易计算。Kullback-Leibler重要性估计过程（KLIEP）能够以一种有希望的方式估计重要性。尽管它的性能很好，但它无法忽略目标信息，因为它只包括用于计算重要性的协变量信息。在这个方向上，本文探讨了如果在计算重要性时包括目标信息的潜在性能改进。",
    "tldr": "该论文研究了在协变量偏移问题中改进重要性估计以提高预测误差的准确性。"
}