{
    "title": "Asymptotics of feature learning in two-layer networks after one gradient-step",
    "abstract": "In this manuscript we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging a connection from (Ba et al., 2022) with a non-linear spiked matrix model and recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error in the high-dimensional limit where the number of samples $n$, the width $p$ and the input dimension $d$ grow at a proportional rate. We characterize exactly how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime. To our knowledge, our results provides the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime $\\eta=\\Theta_{d}(d)$, beyond",
    "link": "https://arxiv.org/abs/2402.04980",
    "context": "Title: Asymptotics of feature learning in two-layer networks after one gradient-step\nAbstract: In this manuscript we investigate the problem of how two-layer neural networks learn features from data, and improve over the kernel regime, after being trained with a single gradient descent step. Leveraging a connection from (Ba et al., 2022) with a non-linear spiked matrix model and recent progress on Gaussian universality (Dandi et al., 2023), we provide an exact asymptotic description of the generalization error in the high-dimensional limit where the number of samples $n$, the width $p$ and the input dimension $d$ grow at a proportional rate. We characterize exactly how adapting to the data is crucial for the network to efficiently learn non-linear functions in the direction of the gradient -- where at initialization it can only express linear functions in this regime. To our knowledge, our results provides the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime $\\eta=\\Theta_{d}(d)$, beyond",
    "path": "papers/24/02/2402.04980.json",
    "total_tokens": 952,
    "translated_title": "一次梯度下降步骤后两层神经网络在特征学习中的渐近性质",
    "translated_abstract": "本文研究了两层神经网络在从数据中学习特征，并在使用单一梯度下降步骤训练后如何改进核心方法的问题。借助于（Ba et al., 2022）与非线性尖峰矩阵模型的关联以及对高斯泛化性的最新进展（Dandi et al., 2023），我们在样本数$n$、宽度$p$和输入维度$d$成比例增长的高维极限下，给出了一种精确的一致性误差描述。我们准确地刻画了适应数据对于网络在梯度方向上高效学习非线性函数的重要性——在初始化阶段，网络只能表达线性函数。据我们所知，我们的结果提供了在大学习率$\\eta=\\Theta_{d}(d)$的情况下特征学习对于两层神经网络泛化的首个准确描述，超越了核心方法。",
    "tldr": "通过研究两层神经网络在一次梯度下降步骤后的特征学习，我们提供了在高维极限下通用化误差的精确渐近描述，并发现在适应数据的情况下，网络能够高效地学习梯度方向上的非线性函数。",
    "en_tdlr": "Studying the feature learning in two-layer neural networks after one gradient descent step, we provide an exact asymptotic description of the generalization error in the high-dimensional limit and find that adapting to the data is crucial for efficient learning of nonlinear functions in the direction of the gradient. This work presents the first tight description of the impact of feature learning in the generalization of two-layer neural networks in the large learning rate regime."
}