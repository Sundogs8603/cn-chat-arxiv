{
    "title": "How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?",
    "abstract": "arXiv:2402.13470v1 Announce Type: new  Abstract: Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain. The past few years have seen generative language models (LMs), instruction finetuning, and few-shot learning become foci of NLP research. As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction finetuning has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks. Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of relation extraction. Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora? (2) Do models instruction finetuned on biomedical datasets outperform those finetuned on assorted datasets or those simply pretrained? We tackle these questions",
    "link": "https://arxiv.org/abs/2402.13470",
    "context": "Title: How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?\nAbstract: arXiv:2402.13470v1 Announce Type: new  Abstract: Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain. The past few years have seen generative language models (LMs), instruction finetuning, and few-shot learning become foci of NLP research. As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction finetuning has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks. Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of relation extraction. Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora? (2) Do models instruction finetuned on biomedical datasets outperform those finetuned on assorted datasets or those simply pretrained? We tackle these questions",
    "path": "papers/24/02/2402.13470.json",
    "total_tokens": 963,
    "translated_title": "语言模型和生物医学关系提取中的领域特异性有多重要？",
    "translated_abstract": "高价值、数据丰富的生物医学领域常常会使用最前沿的通用自然语言处理技术。过去几年来，生成式语言模型、指导微调和少样本学习成为自然语言处理研究的焦点。因此，预训练于生物医学语料库的生成式语言模型不断涌现，同时也尝试对生物医学指导微调，希望领域特异性可以改善下游任务的性能。鉴于训练这些模型所需的非平凡努力，我们研究它们在关系提取这一关键生物医学自然语言处理任务中是否存在任何益处。具体来说，我们探讨了两个问题：(1) 在生物医学语料库上训练的语言模型是否优于在通用领域语料库上训练的模型？(2) 在生物医学数据集上进行指导微调的模型是否优于在各种数据集上进行微调或者仅仅预训练的模型？我们解决这些问题。",
    "tldr": "研究探讨了在生物医学关系提取任务中领域特异性对于语言模型和指导微调的重要性，对比了在生物医学领域与通用领域训练的模型效果，并探讨了在生物医学数据集上指导微调的模型在性能上的优势。",
    "en_tdlr": "The study investigates the importance of domain specificity in language models and instruction finetuning for biomedical relation extraction, comparing the performance of models trained on biomedical and general domain corpora, and exploring the advantages of instruction finetuning on biomedical datasets."
}