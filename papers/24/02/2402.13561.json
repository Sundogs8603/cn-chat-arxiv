{
    "title": "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment",
    "abstract": "arXiv:2402.13561v1 Announce Type: new  Abstract: Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the ",
    "link": "https://arxiv.org/abs/2402.13561",
    "context": "Title: Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment\nAbstract: arXiv:2402.13561v1 Announce Type: new  Abstract: Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the ",
    "path": "papers/24/02/2402.13561.json",
    "total_tokens": 897,
    "translated_title": "认知视觉语言映射器：通过增强视觉知识对齐推进多模态理解",
    "translated_abstract": "评估和反思当前大型多模态模型（LMMs）的现状，我们观察到广泛使用的视觉语言投影方法（如Q-former或MLP）侧重于图像-文本描述的对齐，但忽略了视觉知识维度的对齐，即将视觉与其相关知识连接起来。视觉知识在分析、推断和解释视觉信息方面起着重要作用，有助于提高基于知识的视觉问题答案的准确性。本文主要探讨通过视觉语言知识对齐来改进LMMs，特别针对挑战知识型视觉问答（VQA）。为此，我们提出了一个认知视觉语言映射器（CVLM），其中包含一个预训练的视觉知识对齐器（VKA）和一个用于多模态指令调节阶段的细粒度知识适配器（FKA）。具体来说，我们基于",
    "tldr": "该论文提出了一种认知视觉语言映射器（CVLM），通过增强视觉知识对齐，在多模态理解中取得了重要进展，特别是在挑战知识型视觉问题回答方面。",
    "en_tdlr": "The paper introduces a Cognitive Visual-Language Mapper (CVLM) that advances multimodal comprehension by enhancing visual knowledge alignment, particularly in challenging knowledge-based visual question answering."
}