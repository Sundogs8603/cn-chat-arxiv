{
    "title": "ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models",
    "abstract": "arXiv:2402.14660v1 Announce Type: cross  Abstract: This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grai",
    "link": "https://arxiv.org/abs/2402.14660",
    "context": "Title: ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models\nAbstract: arXiv:2402.14660v1 Announce Type: cross  Abstract: This paper introduces ConceptMath, a bilingual (English and Chinese), fine-grained benchmark that evaluates concept-wise mathematical reasoning of Large Language Models (LLMs). Unlike traditional benchmarks that evaluate general mathematical reasoning with an average accuracy, ConceptMath systematically organizes math problems under a hierarchy of math concepts, so that mathematical reasoning can be evaluated at different granularity with concept-wise accuracies. Based on our ConcepthMath, we evaluate a broad range of LLMs, and we observe existing LLMs, though achieving high average accuracies on traditional benchmarks, exhibit significant performance variations across different math concepts and may even fail catastrophically on the most basic ones. Besides, we also introduce an efficient fine-tuning strategy to enhance the weaknesses of existing LLMs. Finally, we hope ConceptMath could guide the developers to understand the fine-grai",
    "path": "papers/24/02/2402.14660.json",
    "total_tokens": 903,
    "translated_title": "ConceptMath：用于衡量大型语言模型数学推理能力的双语概念评测基准",
    "translated_abstract": "本文介绍了ConceptMath，这是一个双语（英语和中文），细粒度的基准测试，用来评估大型语言模型（LLMs）的概念性数学推理能力。与评估一般数学推理的传统基准不同，ConceptMath将数学问题系统地组织在数学概念的层次结构下，从而可以以概念为单位准确性评估数学推理。基于我们的ConceptMath，我们评估了广泛范围的LLMs，并观察到现有的LLMs尽管在传统基准上取得了高平均准确性，但在不同数学概念上表现出显著的性能差异，甚至可能在最基本的概念上出现严重失败。此外，我们还介绍了一种有效的微调策略来增强现有LLMs的弱点。最后，我们希望ConceptMath能够指导开发者理解细致的数学推理能力。",
    "tldr": "介绍了ConceptMath，一种双语的细粒度基准测试，用于评估大型语言模型的概念性数学推理能力，并发现现有模型在不同数学概念上存在显著性能差异，甚至可能在最基本的概念上出现失败。",
    "en_tdlr": "Introduced ConceptMath, a bilingual fine-grained benchmark for evaluating the mathematical reasoning of Large Language Models, revealed significant performance variations across different math concepts in existing models, and even failures on the most basic concepts."
}