{
    "title": "Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation",
    "abstract": "arXiv:2402.15399v1 Announce Type: new  Abstract: We study off-dynamics Reinforcement Learning (RL), where the policy is trained on a source domain and deployed to a distinct target domain. We aim to solve this problem via online distributionally robust Markov decision processes (DRMDPs), where the learning algorithm actively interacts with the source domain while seeking the optimal performance under the worst possible dynamics that is within an uncertainty set of the source domain's transition kernel. We provide the first study on online DRMDPs with function approximation for off-dynamics RL. We find that DRMDPs' dual formulation can induce nonlinearity, even when the nominal transition kernel is linear, leading to error propagation. By designing a $d$-rectangular uncertainty set using the total variation distance, we remove this additional nonlinearity and bypass the error propagation. We then introduce DR-LSVI-UCB, the first provably efficient online DRMDP algorithm for off-dynamics",
    "link": "https://arxiv.org/abs/2402.15399",
    "context": "Title: Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation\nAbstract: arXiv:2402.15399v1 Announce Type: new  Abstract: We study off-dynamics Reinforcement Learning (RL), where the policy is trained on a source domain and deployed to a distinct target domain. We aim to solve this problem via online distributionally robust Markov decision processes (DRMDPs), where the learning algorithm actively interacts with the source domain while seeking the optimal performance under the worst possible dynamics that is within an uncertainty set of the source domain's transition kernel. We provide the first study on online DRMDPs with function approximation for off-dynamics RL. We find that DRMDPs' dual formulation can induce nonlinearity, even when the nominal transition kernel is linear, leading to error propagation. By designing a $d$-rectangular uncertainty set using the total variation distance, we remove this additional nonlinearity and bypass the error propagation. We then introduce DR-LSVI-UCB, the first provably efficient online DRMDP algorithm for off-dynamics",
    "path": "papers/24/02/2402.15399.json",
    "total_tokens": 974,
    "translated_title": "分布鲁棒的离策略强化学习：具有线性函数逼近的高效性证明",
    "translated_abstract": "我们研究离策略强化学习（RL），其中策略在源域上进行训练，然后部署到不同的目标域。我们旨在通过在线分布鲁棒马尔可夫决策过程（DRMDPs）来解决这个问题，其中学习算法在与源域交互时，寻求在源域转移核的不确定性集合内的最差动态下的最佳性能。我们首次研究了离策略RL中具有函数逼近的在线DRMDPs。我们发现DRMDPs的对偶形式可能会引入非线性，即使名义转移核是线性的，也会导致误差传播。通过设计一个$d$-矩形不确定性集合，使用总变差距离，我们去除了这种额外的非线性，避免了误差传播。然后我们引入了DR-LSVI-UCB，这是第一个针对离策略具有线性函数逼近的在线DRMDP算法，并且具有可证明的高效性。",
    "tldr": "研究了离策略强化学习中的在线分布鲁棒马尔可夫决策过程，设计了可以去除非线性和避免误差传播的$d$-矩形不确定性集合，并提出了第一个针对离策略具有线性函数逼近的在线DRMDP算法，并且具有高效性。",
    "en_tdlr": "Studied online distributionally robust Markov decision processes in off-dynamics reinforcement learning, designed a $d$-rectangular uncertainty set to remove nonlinearity and bypass error propagation, introduced the first provably efficient online DRMDP algorithm for off-dynamics with linear function approximation."
}