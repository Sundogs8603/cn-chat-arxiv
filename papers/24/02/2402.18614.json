{
    "title": "Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains",
    "abstract": "arXiv:2402.18614v1 Announce Type: new  Abstract: The recently discovered Neural collapse (NC) phenomenon states that the last-layer weights of Deep Neural Networks (DNN), converge to the so-called Equiangular Tight Frame (ETF) simplex, at the terminal phase of their training. This ETF geometry is equivalent to vanishing within-class variability of the last layer activations. Inspired by NC properties, we explore in this paper the transferability of DNN models trained with their last layer weight fixed according to ETF. This enforces class separation by eliminating class covariance information, effectively providing implicit regularization. We show that DNN models trained with such a fixed classifier significantly improve transfer performance, particularly on out-of-domain datasets. On a broad range of fine-grained image classification datasets, our approach outperforms i) baseline methods that do not perform any covariance regularization (up to 22%), as well as ii) methods that explici",
    "link": "https://arxiv.org/abs/2402.18614",
    "context": "Title: Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains\nAbstract: arXiv:2402.18614v1 Announce Type: new  Abstract: The recently discovered Neural collapse (NC) phenomenon states that the last-layer weights of Deep Neural Networks (DNN), converge to the so-called Equiangular Tight Frame (ETF) simplex, at the terminal phase of their training. This ETF geometry is equivalent to vanishing within-class variability of the last layer activations. Inspired by NC properties, we explore in this paper the transferability of DNN models trained with their last layer weight fixed according to ETF. This enforces class separation by eliminating class covariance information, effectively providing implicit regularization. We show that DNN models trained with such a fixed classifier significantly improve transfer performance, particularly on out-of-domain datasets. On a broad range of fine-grained image classification datasets, our approach outperforms i) baseline methods that do not perform any covariance regularization (up to 22%), as well as ii) methods that explici",
    "path": "papers/24/02/2402.18614.json",
    "total_tokens": 821,
    "translated_title": "用固定的随机分类器训练的深度神经网络模型在不同领域之间转移更好",
    "translated_abstract": "最近发现的神经坍缩（NC）现象表明，深度神经网络（DNN）的最后一层权重在训练的最后阶段会收敛到所谓的等角紧框架（ETF）单纯形。这种ETF几何形状相当于最后一层激活的类内变化消失。受NC属性启发，本文探讨了使用最后一层权重固定为ETF的DNN模型的可转移性。这通过消除类协方差信息强化了类别分离，有效提供了隐式正则化。我们展示了使用这种固定分类器训练的DNN模型显著改善了转移性能，特别是在跨领域数据集上。在广泛的细粒度图像分类数据集上，我们的方法优于没有执行任何协方差正则化的基线方法（高达22%），以及明确方法",
    "tldr": "DNN模型利用ETF进行训练，在最后一层权重固定的情况下，显著提高了跨域数据集上的转移性能。",
    "en_tdlr": "DNN models trained with ETF and fixed last-layer weights significantly improve transfer performance across domains."
}