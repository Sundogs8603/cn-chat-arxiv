{
    "title": "From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers",
    "abstract": "Pretrained Language Models (PLMs) have become the de facto starting point for fine-tuning on downstream tasks. However, as model sizes continue to increase, traditional fine-tuning of all parameters becomes challenging. To address this, parameter-efficient fine-tuning (PEFT) methods have gained popularity as a means to adapt PLMs effectively. In parallel, recent studies have revealed the presence of activation sparsity within the intermediate outputs of the multilayer perception (MLP) blocks in transformers. Low activation density enables efficient model inference on sparsity-aware hardware. Building upon this insight, in this work, we propose a novel density loss that encourages higher activation sparsity (equivalently, lower activation density) in the pre-trained models. We demonstrate the effectiveness of our approach by utilizing mainstream PEFT techniques including QLoRA, LoRA, Adapter, Prompt/Prefix Tuning to facilitate efficient model adaptation across diverse downstream tasks. ",
    "link": "https://arxiv.org/abs/2402.01911",
    "context": "Title: From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers\nAbstract: Pretrained Language Models (PLMs) have become the de facto starting point for fine-tuning on downstream tasks. However, as model sizes continue to increase, traditional fine-tuning of all parameters becomes challenging. To address this, parameter-efficient fine-tuning (PEFT) methods have gained popularity as a means to adapt PLMs effectively. In parallel, recent studies have revealed the presence of activation sparsity within the intermediate outputs of the multilayer perception (MLP) blocks in transformers. Low activation density enables efficient model inference on sparsity-aware hardware. Building upon this insight, in this work, we propose a novel density loss that encourages higher activation sparsity (equivalently, lower activation density) in the pre-trained models. We demonstrate the effectiveness of our approach by utilizing mainstream PEFT techniques including QLoRA, LoRA, Adapter, Prompt/Prefix Tuning to facilitate efficient model adaptation across diverse downstream tasks. ",
    "path": "papers/24/02/2402.01911.json",
    "total_tokens": 1004,
    "translated_title": "从PEFT到DEFT：用于减少变压器中激活密度的参数高效微调",
    "translated_abstract": "预训练语言模型（PLMs）已成为下游任务微调的事实上的起点。然而，随着模型规模的增加，传统的全参数微调变得困难。为了解决这个问题，参数高效微调（PEFT）方法作为有效适应PLMs的手段而变得流行。与此同时，最近的研究揭示了变压器中多层感知（MLP）模块的中间输出中存在的激活稀疏性。低激活密度能够在支持稀疏感知硬件上实现高效模型推断。基于这一观察，我们在工作中提出了一种新的密度损失，鼓励预训练模型中更高的激活稀疏性（等价于更低的激活密度）。我们通过利用包括QLoRA、LoRA、Adapter、Prompt/Prefix Tuning在内的主流PEFT技术，展示了我们方法的有效性，以促进在多样的下游任务中实现高效的模型适应。",
    "tldr": "本论文提出了一种用于减少变压器模型中激活密度的参数高效微调方法 DEFT。研究发现预训练模型中存在激活稀疏性，并通过引入新的密度损失来促进更高的激活稀疏性。通过应用主流的PEFT技术，包括QLoRA、LoRA、Adapter、Prompt/Prefix Tuning，实验证明了该方法在不同下游任务中的有效性。",
    "en_tdlr": "This paper proposes a parameter-efficient fine-tuning method called DEFT for reducing activation density in transformers. The study reveals the presence of activation sparsity in pretrained models and introduces a new density loss to encourage higher activation sparsity. The effectiveness of the method is demonstrated through the application of mainstream PEFT techniques, including QLoRA, LoRA, Adapter, and Prompt/Prefix Tuning, across diverse downstream tasks."
}