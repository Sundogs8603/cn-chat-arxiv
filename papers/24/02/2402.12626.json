{
    "title": "Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors",
    "abstract": "arXiv:2402.12626v1 Announce Type: new  Abstract: Machine learning models have achieved great success in supervised learning tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible. Recently, many practitioners have shifted to self-supervised learning methods that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data. However, such a process may also raise concerns regarding data poisoning attacks. For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end supervised learning. In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-t",
    "link": "https://arxiv.org/abs/2402.12626",
    "context": "Title: Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors\nAbstract: arXiv:2402.12626v1 Announce Type: new  Abstract: Machine learning models have achieved great success in supervised learning tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible. Recently, many practitioners have shifted to self-supervised learning methods that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data. However, such a process may also raise concerns regarding data poisoning attacks. For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end supervised learning. In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-t",
    "path": "papers/24/02/2402.12626.json",
    "total_tokens": 823,
    "translated_title": "针对预训练特征提取器的任意数据毒化攻击研究",
    "translated_abstract": "机器学习模型在监督学习任务中取得了巨大成功，这需要大量标记数据，但这并不总是可行的。最近，许多从业者转向自监督学习方法，利用廉价的未标记数据通过预训练学习一个通用特征提取器，可以简单地通过训练一个额外的线性层并使用有限的标记数据来应用于个性化的下游任务。然而，这一过程也可能引发对数据毒化攻击的担忧。例如，任意数据毒化攻击旨在通过将少量毒化数据注入训练集来降低模型效用，这对机器学习模型构成安全风险，但目前仅在端到端监督学习中进行了研究。本文扩展了对应用预训练的下游任务的任意攻击威胁的探讨。",
    "tldr": "本文研究了针对预训练特征提取器的任意数据毒化攻击，探讨了这种攻击对机器学习模型的安全风险和影响。",
    "en_tdlr": "This paper investigates indiscriminate data poisoning attacks on pre-trained feature extractors, exploring the security risks and impacts of such attacks on machine learning models."
}