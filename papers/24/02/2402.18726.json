{
    "title": "Unveiling Privacy, Memorization, and Input Curvature Links",
    "abstract": "arXiv:2402.18726v1 Announce Type: cross  Abstract: Deep Neural Nets (DNNs) have become a pervasive tool for solving many emerging problems. However, they tend to overfit to and memorize the training set. Memorization is of keen interest since it is closely related to several concepts such as generalization, noisy learning, and privacy. To study memorization, Feldman (2019) proposed a formal score, however its computational requirements limit its practical use. Recent research has shown empirical evidence linking input loss curvature (measured by the trace of the loss Hessian w.r.t inputs) and memorization. It was shown to be ~3 orders of magnitude more efficient than calculating the memorization score. However, there is a lack of theoretical understanding linking memorization with input loss curvature. In this paper, we not only investigate this connection but also extend our analysis to establish theoretical links between differential privacy, memorization, and input loss curvature. F",
    "link": "https://arxiv.org/abs/2402.18726",
    "context": "Title: Unveiling Privacy, Memorization, and Input Curvature Links\nAbstract: arXiv:2402.18726v1 Announce Type: cross  Abstract: Deep Neural Nets (DNNs) have become a pervasive tool for solving many emerging problems. However, they tend to overfit to and memorize the training set. Memorization is of keen interest since it is closely related to several concepts such as generalization, noisy learning, and privacy. To study memorization, Feldman (2019) proposed a formal score, however its computational requirements limit its practical use. Recent research has shown empirical evidence linking input loss curvature (measured by the trace of the loss Hessian w.r.t inputs) and memorization. It was shown to be ~3 orders of magnitude more efficient than calculating the memorization score. However, there is a lack of theoretical understanding linking memorization with input loss curvature. In this paper, we not only investigate this connection but also extend our analysis to establish theoretical links between differential privacy, memorization, and input loss curvature. F",
    "path": "papers/24/02/2402.18726.json",
    "total_tokens": 812,
    "translated_title": "揭示隐私、记忆和输入曲率之间的联系",
    "translated_abstract": "深度神经网络(DNNs)已成为解决许多新兴问题的普遍工具。然而，它们往往会过度拟合和记忆训练集。记忆是一个备受关注的问题，因为它与诸多概念如泛化、有噪学习和隐私密切相关。最近的研究显示了输入损失曲率（通过损失Hessian矩阵对输入的迹进行测量）与记忆之间的经验性联系。它被证明比计算记忆分数要高效约3个数量级。然而，目前缺乏将记忆与输入损失曲率联系起来的理论理解。本文不仅研究了这种联系，还扩展了我们的分析，建立了差分隐私、记忆和输入损失曲率之间的理论联系。",
    "tldr": "本研究揭示了记忆与输入损失曲率之间的联系，并建立了差分隐私、记忆和输入损失曲率之间的理论联系。",
    "en_tdlr": "This study reveals the connection between memorization and input loss curvature, and establishes theoretical links between differential privacy, memorization, and input loss curvature."
}