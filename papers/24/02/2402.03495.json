{
    "title": "Partially Stochastic Infinitely Deep Bayesian Neural Networks",
    "abstract": "In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators",
    "link": "https://arxiv.org/abs/2402.03495",
    "context": "Title: Partially Stochastic Infinitely Deep Bayesian Neural Networks\nAbstract: In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators",
    "path": "papers/24/02/2402.03495.json",
    "total_tokens": 837,
    "translated_title": "部分随机的无限深度贝叶斯神经网络",
    "translated_abstract": "在本文中，我们提出了一种部分随机的无限深度贝叶斯神经网络，这是一种将部分随机性整合到无限深度神经网络框架中的新型架构。我们的新型架构旨在改善现有架构在训练和推理时间上的计算效率限制。为实现这一目标，我们利用了部分随机性在无限深度极限下的优势，包括全随机性的好处，如鲁棒性、不确定性量化和内存效率，同时改善了它们在训练和推理时间上的计算效率限制。我们提出了多种架构配置，提供了网络设计的灵活性，包括不同的权重划分方法。我们还通过确立我们的网络家族符合通用条件分布近似器的数学保证，对我们的模型的表达能力进行了证明。",
    "tldr": "本文提出了一种部分随机性的无限深度贝叶斯神经网络，通过在网络框架中整合部分随机性，改善现有架构在训练和推理时间上的计算效率限制，并提供了多种灵活的网络设计配置，同时通过数学证明确保了模型的表达能力。"
}