{
    "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
    "abstract": "Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribut",
    "link": "https://arxiv.org/abs/2402.02750",
    "context": "Title: KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache\nAbstract: Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPU's SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process. A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, we conducted a comprehensive study on the element distribut",
    "path": "papers/24/02/2402.02750.json",
    "total_tokens": 865,
    "translated_title": "KIVI：一种无需调整的非对称2位量化KV缓存技术",
    "translated_abstract": "高效地为大型语言模型（LLMs）提供服务需要将许多请求批量处理以减少每个请求的成本。然而，存储注意力键和值以避免重新计算的键值（KV）缓存显著增加了内存需求，并成为速度和内存使用的新瓶颈。这种内存需求随着批处理大小和上下文长度的增加而增加。此外，推断速度受到KV缓存大小的限制，因为GPU的SRAM必须从主GPU内存中加载整个KV缓存以生成每个标记，导致计算核心在此过程中处于空闲状态。减小KV缓存大小的一个直接而有效的解决方案是量化，通过减少KV缓存所需的总字节数来实现。然而，目前缺乏对KV缓存元素分布进行深入研究以了解KV缓存量化的难度和限制。为了弥补这一空白，我们开展了一项全面的元素分布研究。。。",
    "tldr": "该论文提出了一种无需调整的非对称2位量化KV缓存技术，以解决存储注意力键和值的内存需求增加和推断速度受限问题。",
    "en_tdlr": "This paper presents a tuning-free asymmetric 2bit quantization technique for KV cache, addressing the memory demand increase and limited inference speed caused by storing attention keys and values."
}