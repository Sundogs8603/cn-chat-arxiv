{
    "title": "Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning",
    "abstract": "arXiv:2402.17263v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential. The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theor",
    "link": "https://arxiv.org/abs/2402.17263",
    "context": "Title: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning\nAbstract: arXiv:2402.17263v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential. The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theor",
    "path": "papers/24/02/2402.17263.json",
    "total_tokens": 848,
    "translated_title": "迷你集成低秩适配器用于参数高效微调",
    "translated_abstract": "参数高效微调（PEFT）是一种用于定制预训练大型语言模型（LLMs）的流行方法，尤其是在模型规模和任务多样性增加的情况下。低秩适应（LoRA）基于这样一个思想，即适应过程在本质上是低维的，即可以用相对较少的参数表示重要的模型变化。然而，与全参数微调相比，降低秩会遇到特定任务的泛化误差方面的挑战。我们提出了MELoRA，一种迷你集成低秩适配器，使用更少的可训练参数同时保持更高的秩，从而提供改进的性能潜力。其核心思想是冻结原始的预训练权重，并训练一组仅具有少量参数的迷你LoRA。这可以捕捉迷你LoRA之间的重要多样性程度，从而促进更好的泛化能力。",
    "tldr": "提出了MELoRA，一种迷你集成低秩适配器，通过使用更少的可训练参数同时保持更高的秩，从而提供改进的性能潜力。"
}