{
    "title": "Distilling Large Language Models for Text-Attributed Graph Learning",
    "abstract": "arXiv:2402.12022v1 Announce Type: new  Abstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents. Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning. To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale. Extensive experiments validate the efficacy of our proposed ",
    "link": "https://arxiv.org/abs/2402.12022",
    "context": "Title: Distilling Large Language Models for Text-Attributed Graph Learning\nAbstract: arXiv:2402.12022v1 Announce Type: new  Abstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents. Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning. To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale. Extensive experiments validate the efficacy of our proposed ",
    "path": "papers/24/02/2402.12022.json",
    "total_tokens": 776,
    "translated_title": "将大型语言模型压缩用于文本属性图学习",
    "translated_abstract": "文本属性图（TAGs）是连接的文本文档图。图模型可以有效学习TAGs，但它们的训练严重依赖于人工标注的标签，在许多应用中这些标签很少或甚至不可用。大型语言模型（LLMs）最近在少样本和零样本TAG学习中展示了显著能力，但它们存在可伸缩性、成本和隐私问题。因此，在这项工作中，我们专注于通过将LLMs的能力传授给TAG学习中的本地图模型，从而协同LLMs和图模型的互补优势。",
    "tldr": "本研究旨在将大型语言模型和图模型的优势相结合，通过将LLMs的能力压缩到 TAG 学习的本地图模型中，解决它们之间的固有差距。",
    "en_tdlr": "This work aims to synergize the strengths of large language models and graph models by distilling the power of LLMs into a local graph model for TAG learning, bridging the inherent gaps between them."
}