{
    "title": "LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs",
    "abstract": "arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res",
    "link": "https://arxiv.org/abs/2402.11804",
    "context": "Title: LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs\nAbstract: arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res",
    "path": "papers/24/02/2402.11804.json",
    "total_tokens": 896,
    "translated_title": "LLM作为提示器：在任意知识图上进行低资源归纳推理",
    "translated_abstract": "知识图（KG）归纳推理旨在推断训练期间未见过的新KG中缺失的事实，在各种应用中被广泛采用。KG归纳推理的一个关键挑战是处理在文本和结构方面都稀缺的低资源场景。本文尝试利用大型语言模型（LLMs）来解决这一挑战。具体来说，我们利用最先进的LLMs生成图形结构提示，以增强预训练的图神经网络（GNNs），从而为KG归纳推理方法带来新的方法论见解，以及在实践中具有很高的普适性。在方法论方面，我们引入了一种新颖的预训练和提示框架ProLINK，旨在在任意KG上进行低资源归纳推理，而无需额外训练。在实践方面，我们在36个低资源数据集上对我们的方法进行了实验评估。",
    "tldr": "本文利用大型语言模型（LLMs）生成图形结构提示，以增强预训练的图神经网络（GNNs），提出一种新的方法论见解，实现了在任意知识图上进行低资源归纳推理的高通用性。",
    "en_tdlr": "This paper proposes a novel approach using Large Language Models (LLMs) to generate graph-structural prompts to enhance pre-trained Graph Neural Networks (GNNs), providing new methodological insights for low-resource inductive reasoning on arbitrary knowledge graphs with high generalizability."
}