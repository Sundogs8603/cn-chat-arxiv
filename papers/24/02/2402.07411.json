{
    "title": "Potential-Based Reward Shaping For Intrinsic Motivation",
    "abstract": "Recently there has been a proliferation of intrinsic motivation (IM) reward-shaping methods to learn in complex and sparse-reward environments. These methods can often inadvertently change the set of optimal policies in an environment, leading to suboptimal behavior. Previous work on mitigating the risks of reward shaping, particularly through potential-based reward shaping (PBRS), has not been applicable to many IM methods, as they are often complex, trainable functions themselves, and therefore dependent on a wider set of variables than the traditional reward functions that PBRS was developed for. We present an extension to PBRS that we prove preserves the set of optimal policies under a more general set of functions than has been previously proven. We also present {\\em Potential-Based Intrinsic Motivation} (PBIM), a method for converting IM rewards into a potential-based form that is useable without altering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff Walk",
    "link": "https://arxiv.org/abs/2402.07411",
    "context": "Title: Potential-Based Reward Shaping For Intrinsic Motivation\nAbstract: Recently there has been a proliferation of intrinsic motivation (IM) reward-shaping methods to learn in complex and sparse-reward environments. These methods can often inadvertently change the set of optimal policies in an environment, leading to suboptimal behavior. Previous work on mitigating the risks of reward shaping, particularly through potential-based reward shaping (PBRS), has not been applicable to many IM methods, as they are often complex, trainable functions themselves, and therefore dependent on a wider set of variables than the traditional reward functions that PBRS was developed for. We present an extension to PBRS that we prove preserves the set of optimal policies under a more general set of functions than has been previously proven. We also present {\\em Potential-Based Intrinsic Motivation} (PBIM), a method for converting IM rewards into a potential-based form that is useable without altering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff Walk",
    "path": "papers/24/02/2402.07411.json",
    "total_tokens": 854,
    "translated_title": "潜势引导的奖励塑造用于内在动机",
    "translated_abstract": "最近，在复杂且稀疏奖励环境下，内在动机（IM）奖励塑造方法的数量激增。这些方法往往会无意中改变环境中的最优策略集，导致次优行为产生。以往关于减轻奖励塑造风险的研究，特别是潜势引导的奖励塑造（PBRS），并不能适用于许多IM方法，因为它们通常是复杂的可训练函数，因此依赖于比PBRS开发时更广泛的变量集。我们提出了对PBRS的扩展，证明了在更一般的函数集下保留了最优策略集。我们还提出了一种称为潜势引导的内在动机（PBIM）的方法，可以将IM奖励转换为可用的潜势形式，而不会改变最优策略集。在MiniGrid DoorKey和Cliff Walk中进行了测试。",
    "tldr": "该论文提出了潜势引导的奖励塑造方法用于处理内在动机，在复杂和稀疏奖励环境下能避免改变最优策略集导致次优行为的问题。",
    "en_tdlr": "This paper introduces potential-based reward shaping method for dealing with intrinsic motivation, which avoids changing the set of optimal policies and resulting in suboptimal behavior in complex and sparse-reward environments."
}