{
    "title": "Large Language Models to Enhance Bayesian Optimization",
    "abstract": "Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \\texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that \\texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modelin",
    "link": "https://arxiv.org/abs/2402.03921",
    "context": "Title: Large Language Models to Enhance Bayesian Optimization\nAbstract: Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \\texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that \\texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modelin",
    "path": "papers/24/02/2402.03921.json",
    "total_tokens": 958,
    "translated_title": "用大型语言模型增强贝叶斯优化",
    "translated_abstract": "贝叶斯优化（BO）是一种优化复杂和昂贵的黑盒函数的强大方法。它在许多应用中的重要性得到了强调，特别是超参数调优，但其有效性取决于有效地平衡勘探和开发。尽管在BO方法方面取得了重大进展，但平衡这一问题仍然是一个微妙的过程。在这个背景下，我们提出了一个新方法LLAMBO，它将大型语言模型（LLM）的能力与BO相结合。在高层次上，我们用自然语言的方式来描述BO问题，使LLM能够根据历史评估提出有前景的解决方案。更具体地说，我们探讨了如何结合LLM的上下文理解、少样本学习能力和领域知识，来增强基于模型的BO的各个组成部分。我们的研究结果表明，LLAMBO在零样本热启动方面是有效的，并且可以改善代理模型的性能。",
    "tldr": "通过结合大型语言模型（LLM）的能力，我们提出了一种名为LLAMBO的新方法，将其应用于贝叶斯优化（BO）。通过用自然语言描述BO问题，并利用LLM的上下文理解、少样本学习能力和领域知识，LLAMBO能够提供有前景的解决方案，并且在零样本热启动方面表现出良好的效果。",
    "en_tdlr": "We propose a novel approach called LLAMBO that combines the capabilities of large language models (LLM) with Bayesian optimization (BO). By framing the BO problem in natural language terms and leveraging the contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs, LLAMBO can provide promising solutions and performs well in zero-shot warmstarting."
}