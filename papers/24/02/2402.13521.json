{
    "title": "Test-Driven Development for Code Generation",
    "abstract": "arXiv:2402.13521v1 Announce Type: cross  Abstract: Large language models (LLMs) like GPT4, have shown proficiency in generating code snippets from problem statements. Traditionally software development by humans followed a similar methodology of writing code from problem statements or requirements. However, in the past, there have been several studies that have shown the value of test-driven development (TDD) where humans write tests based on problem statements before the code for the functionality is written. In the context of LLM-based code generation, one obvious benefit of TDD is that the developer then knows for sure if the generated code has passed all the given tests or not. Therefore, in this paper, we want to empirically evaluate the hypothesis: giving the problem statements and tests as input to GPT4 is better than just giving the problem statement as input. To test our hypothesis, we build a framework TGen. In our experiments on the MBPP, HumanEval and CodeChef datasets, we ",
    "link": "https://arxiv.org/abs/2402.13521",
    "context": "Title: Test-Driven Development for Code Generation\nAbstract: arXiv:2402.13521v1 Announce Type: cross  Abstract: Large language models (LLMs) like GPT4, have shown proficiency in generating code snippets from problem statements. Traditionally software development by humans followed a similar methodology of writing code from problem statements or requirements. However, in the past, there have been several studies that have shown the value of test-driven development (TDD) where humans write tests based on problem statements before the code for the functionality is written. In the context of LLM-based code generation, one obvious benefit of TDD is that the developer then knows for sure if the generated code has passed all the given tests or not. Therefore, in this paper, we want to empirically evaluate the hypothesis: giving the problem statements and tests as input to GPT4 is better than just giving the problem statement as input. To test our hypothesis, we build a framework TGen. In our experiments on the MBPP, HumanEval and CodeChef datasets, we ",
    "path": "papers/24/02/2402.13521.json",
    "total_tokens": 835,
    "translated_title": "面向代码生成的测试驱动开发",
    "translated_abstract": "arXiv:2402.13521v1 公告类型:交叉摘要:大型语言模型（LLMs）如GPT4已经展示出了从问题描述中生成代码片段的能力。传统上，人类编写软件的方法类似于从问题描述或需求中编写代码。然而，过去有几项研究已经显示了测试驱动开发（TDD）的价值，即在编写功能代码之前，人类根据问题描述编写测试。在基于LLM的代码生成环境中，TDD的一个明显好处是开发人员可以确切知道生成的代码是否通过了所有给定的测试。因此，在本文中，我们希望通过实证评估假设：将问题描述和测试作为GPT4的输入要优于仅将问题描述作为输入。为了测试我们的假设，我们建立了一个名为TGen的框架。在MBPP、HumanEval和CodeChef数据集上的实验中，",
    "tldr": "本文旨在研究通过测试驱动开发（TDD）方法，将问题描述和测试作为输入是否优于仅将问题描述作为输入，以提高基于大型语言模型（LLM）的代码生成效果。"
}