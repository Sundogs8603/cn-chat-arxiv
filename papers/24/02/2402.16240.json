{
    "title": "High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs",
    "abstract": "arXiv:2402.16240v1 Announce Type: new  Abstract: We investigate node representation learning on text-attributed graphs (TAGs), where nodes are associated with text information. Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have exhibited their power in encoding network and text signals, respectively, less attention has been paid to delicately coupling these two types of models on TAGs. Specifically, existing GNNs rarely model text in each node in a contextualized way; existing PLMs can hardly be applied to characterize graph structures due to their sequence architecture. To address these challenges, we propose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive Selective Coding method that integrates GNNs and PLMs into a unified model. Different from previous \"cascaded architectures\" that directly add GNN layers upon a PLM, our HASH-CODE relies on five self-supervised optimization objectives to facilitate thorough mutual e",
    "link": "https://arxiv.org/abs/2402.16240",
    "context": "Title: High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs\nAbstract: arXiv:2402.16240v1 Announce Type: new  Abstract: We investigate node representation learning on text-attributed graphs (TAGs), where nodes are associated with text information. Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have exhibited their power in encoding network and text signals, respectively, less attention has been paid to delicately coupling these two types of models on TAGs. Specifically, existing GNNs rarely model text in each node in a contextualized way; existing PLMs can hardly be applied to characterize graph structures due to their sequence architecture. To address these challenges, we propose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive Selective Coding method that integrates GNNs and PLMs into a unified model. Different from previous \"cascaded architectures\" that directly add GNN layers upon a PLM, our HASH-CODE relies on five self-supervised optimization objectives to facilitate thorough mutual e",
    "path": "papers/24/02/2402.16240.json",
    "total_tokens": 911,
    "translated_title": "针对文本属性图的高频感知分层对比选择编码用于表示学习",
    "translated_abstract": "我们研究了在文本属性图（TAGs）上的节点表示学习，其中节点关联有文本信息。尽管最近关于图神经网络（GNNs）和预训练语言模型（PLMs）的研究展示了它们在编码网络和文本信号方面的强大能力，但对于精细地将这两种模型耦合在TAGs上的注意力较少。具体而言，现有的GNNs很少以一种情境化的方式对每个节点中的文本进行建模；现有的PLMs由于其序列架构，几乎无法应用于表征图结构。为了解决这些挑战，我们提出了HASH-CODE，一种高频感知的谱分层对比选择编码方法，将GNNs和PLMs整合到统一模型中。与之前的“级联架构”不同，直接在PLM之上添加GNN层的方法不同，我们的HASH-CODE依靠五个自监督优化目标，以促进彻底的相互学习。",
    "tldr": "提出了一种名为HASH-CODE的高频感知分层对比选择编码方法，将图神经网络（GNNs）和预训练语言模型（PLMs）相结合，解决了在文本属性图上节点表示学习中的挑战。",
    "en_tdlr": "Proposed a method named HASH-CODE that integrates graph neural networks (GNNs) and pretrained language models (PLMs) to address the challenges of node representation learning on text-attributed graphs."
}