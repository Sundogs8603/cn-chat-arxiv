{
    "title": "NPSVC++: Nonparallel Classifiers Encounter Representation Learning",
    "abstract": "This paper focuses on a specific family of classifiers called nonparallel support vector classifiers (NPSVCs). Different from typical classifiers, the training of an NPSVC involves the minimization of multiple objectives, resulting in the potential concerns of feature suboptimality and class dependency. Consequently, no effective learning scheme has been established to improve NPSVCs' performance through representation learning, especially deep learning. To break this bottleneck, we develop NPSVC++ based on multi-objective optimization, enabling the end-to-end learning of NPSVC and its features. By pursuing Pareto optimality, NPSVC++ theoretically ensures feature optimality across classes, hence effectively overcoming the two issues above. A general learning procedure via duality optimization is proposed, based on which we provide two applicable instances, K-NPSVC++ and D-NPSVC++. The experiments show their superiority over the existing methods and verify the efficacy of NPSVC++.",
    "link": "https://arxiv.org/abs/2402.06010",
    "context": "Title: NPSVC++: Nonparallel Classifiers Encounter Representation Learning\nAbstract: This paper focuses on a specific family of classifiers called nonparallel support vector classifiers (NPSVCs). Different from typical classifiers, the training of an NPSVC involves the minimization of multiple objectives, resulting in the potential concerns of feature suboptimality and class dependency. Consequently, no effective learning scheme has been established to improve NPSVCs' performance through representation learning, especially deep learning. To break this bottleneck, we develop NPSVC++ based on multi-objective optimization, enabling the end-to-end learning of NPSVC and its features. By pursuing Pareto optimality, NPSVC++ theoretically ensures feature optimality across classes, hence effectively overcoming the two issues above. A general learning procedure via duality optimization is proposed, based on which we provide two applicable instances, K-NPSVC++ and D-NPSVC++. The experiments show their superiority over the existing methods and verify the efficacy of NPSVC++.",
    "path": "papers/24/02/2402.06010.json",
    "total_tokens": 954,
    "translated_title": "NPSVC++: 非并行分类器遇到表示学习",
    "translated_abstract": "本文侧重于一种特定的分类器家族，称为非并行支持向量分类器(NPSVCs)。与典型的分类器不同，NPSVC的训练涉及多目标的最小化，导致特征次优和类别依赖的潜在问题。因此，尚未建立有效的学习方案来通过表示学习，特别是深度学习来改善NPSVC的性能。为了突破这一瓶颈，我们基于多目标优化开发了NPSVC++，实现了NPSVC及其特征的端到端学习。通过追求帕累托最优，NPSVC++在理论上确保了跨类别的特征优化，从而有效地克服了上述两个问题。我们提出了一种基于对偶优化的通用学习过程，并基于此提供了两个可应用的实例，K-NPSVC++和D-NPSVC++。实验证明了它们在现有方法上的优越性，并验证了NPSVC++的有效性。",
    "tldr": "本文研究了一种称为非并行支持向量分类器(NPSVCs)的分类器家族，提出了NPSVC++，基于多目标优化。NPSVC++通过表示学习实现了NPSVC及其特征的端到端学习，追求帕累托最优，有效地解决了特征次优和类别依赖的问题，在实验证明了其优越性。",
    "en_tdlr": "In this paper, the authors focus on a specific family of classifiers called nonparallel support vector classifiers (NPSVCs), and propose NPSVC++ based on multi-objective optimization. NPSVC++ enables end-to-end learning of NPSVC and its features by representation learning, pursuing Pareto optimality and effectively addressing feature suboptimality and class dependency, as demonstrated by experiments."
}