{
    "title": "Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models",
    "abstract": "arXiv:2402.10189v1 Announce Type: new  Abstract: In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-cont",
    "link": "https://arxiv.org/abs/2402.10189",
    "context": "Title: Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models\nAbstract: arXiv:2402.10189v1 Announce Type: new  Abstract: In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-cont",
    "path": "papers/24/02/2402.10189.json",
    "total_tokens": 857,
    "translated_title": "大型语言模型的上下文学习中的不确定性分解和量化",
    "translated_abstract": "上下文学习已经成为大型语言模型（LLM）的突破性能力，并通过在提示中提供一些与任务相关的演示来彻底改变了各个领域。然而，LLM响应中的可信问题，如幻觉，也被积极讨论。现有工作致力于量化LLM响应中的不确定性，但往往忽视LLM的复杂性和上下文学习的独特性。在这项工作中，我们深入研究了与上下文学习相关的LLM预测不确定性，并强调这种不确定性可能来自于提供的演示（aleatoric不确定性）和与模型配置相关的模糊性（epistemic不确定性）。我们提出了一种新的公式和相应的估计方法来量化这两种类型的不确定性。该方法为理解上下文学习里的预测提供了一种无监督的方式。",
    "tldr": "本文研究了大型语言模型（LLM）上下文学习中的不确定性，并提出了一种新的方法来量化这种不确定性，包括演示产生的不确定性和模型配置的模糊性。",
    "en_tdlr": "This paper investigates the uncertainty in in-context learning of large language models (LLMs) and proposes a novel method to quantify both the uncertainty stemming from provided demonstrations and the ambiguity tied to model configurations."
}