{
    "title": "Label-Noise Robust Diffusion Models",
    "abstract": "arXiv:2402.17517v1 Announce Type: new  Abstract: Conditional diffusion models have shown remarkable performance in various generative tasks, but training them requires large-scale datasets that often contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to condition mismatch and quality degradation of generated data. This paper proposes Transition-aware weighted Denoising Score Matching (TDSM) for training conditional diffusion models with noisy labels, which is the first study in the line of diffusion models. The TDSM objective contains a weighted sum of score networks, incorporating instance-wise and time-dependent label transition probabilities. We introduce a transition-aware weight estimator, which leverages a time-dependent noisy-label classifier distinctively customized to the diffusion process. Through experiments across various datasets and noisy label settings, TDSM improves the quality of generated samples aligned with given conditions. Furthermore, our ",
    "link": "https://arxiv.org/abs/2402.17517",
    "context": "Title: Label-Noise Robust Diffusion Models\nAbstract: arXiv:2402.17517v1 Announce Type: new  Abstract: Conditional diffusion models have shown remarkable performance in various generative tasks, but training them requires large-scale datasets that often contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to condition mismatch and quality degradation of generated data. This paper proposes Transition-aware weighted Denoising Score Matching (TDSM) for training conditional diffusion models with noisy labels, which is the first study in the line of diffusion models. The TDSM objective contains a weighted sum of score networks, incorporating instance-wise and time-dependent label transition probabilities. We introduce a transition-aware weight estimator, which leverages a time-dependent noisy-label classifier distinctively customized to the diffusion process. Through experiments across various datasets and noisy label settings, TDSM improves the quality of generated samples aligned with given conditions. Furthermore, our ",
    "path": "papers/24/02/2402.17517.json",
    "total_tokens": 786,
    "translated_title": "标签噪声鲁棒扩散模型",
    "translated_abstract": "有条件扩散模型在各种生成任务中表现出色，但训练它们需要包含有噪声的条件输入的大规模数据集，即嘈杂标签。本文提出了用于训练有噪声标签的条件扩散模型的Transition-aware weighted Denoising Score Matching (TDSM)，这是扩散模型领域中的首次研究。TDSM目标包含得分网络的加权和，结合了实例级和时间相关的标签转移概率。我们引入了一种过渡感知的权重估计器，利用了一个与扩散过程明显定制化的时间相关嘈杂标签分类器。通过在各种数据集和嘈杂标签设置上的实验，TDSM提高了生成的样本质量与给定条件一致。",
    "tldr": "Transition-aware weighted Denoising Score Matching（TDSM）是用于训练带有嘈杂标签的条件扩散模型的新方法，通过加权得分网络和过渡概率来提高生成样本质量。",
    "en_tdlr": "Transition-aware weighted Denoising Score Matching (TDSM) is a novel approach for training conditional diffusion models with noisy labels, improving the quality of generated samples through weighted score networks and transition probabilities."
}