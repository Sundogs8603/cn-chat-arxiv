{
    "title": "LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition",
    "abstract": "arXiv:2402.14568v1 Announce Type: new  Abstract: Despite the impressive capabilities of large language models (LLMs), their performance on information extraction tasks is still not entirely satisfactory. However, their remarkable rewriting capabilities and extensive world knowledge offer valuable insights to improve these tasks. In this paper, we propose $LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot NER task. To overcome the limitations of existing data augmentation methods that compromise semantic integrity and address the uncertainty inherent in LLM-generated text, we leverage the distinctive characteristics of the NER task by augmenting the original data at both the contextual and entity levels. Our approach involves employing 14 contextual rewriting strategies, designing entity replacements of the same type, and incorporating noise injection to enhance robustness. Extensive experiments demonstrate the effectiveness of our approach in enhancing NER mod",
    "link": "https://arxiv.org/abs/2402.14568",
    "context": "Title: LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition\nAbstract: arXiv:2402.14568v1 Announce Type: new  Abstract: Despite the impressive capabilities of large language models (LLMs), their performance on information extraction tasks is still not entirely satisfactory. However, their remarkable rewriting capabilities and extensive world knowledge offer valuable insights to improve these tasks. In this paper, we propose $LLM-DA$, a novel data augmentation technique based on LLMs for the few-shot NER task. To overcome the limitations of existing data augmentation methods that compromise semantic integrity and address the uncertainty inherent in LLM-generated text, we leverage the distinctive characteristics of the NER task by augmenting the original data at both the contextual and entity levels. Our approach involves employing 14 contextual rewriting strategies, designing entity replacements of the same type, and incorporating noise injection to enhance robustness. Extensive experiments demonstrate the effectiveness of our approach in enhancing NER mod",
    "path": "papers/24/02/2402.14568.json",
    "total_tokens": 865,
    "translated_title": "LLM-DA: 基于大型语言模型的数据增强在少样本命名实体识别中的应用",
    "translated_abstract": "尽管大型语言模型(LLMs)具有令人印象深刻的能力，但它们在信息抽取任务上的表现仍然不完全令人满意。然而，它们出色的重写能力和广泛的世界知识为改进这些任务提供了宝贵的见解。在本文中，我们提出了$LLM-DA$，一种基于LLMs的新颖数据增强技术，用于少样本NER任务。为了克服现有数据增强方法的局限性，这些方法会损害语义完整性并解决LLM生成的文本中固有的不确定性，我们利用NER任务的独特特征，在上下文和实体层面上增强原始数据。我们的方法涉及使用14种上下文重写策略，设计相同类型的实体替换，并引入噪声注入来增强鲁棒性。广泛的实验表明了我们方法在增强NER性能方面的有效性。",
    "tldr": "提出了一种新的数据增强技术$LLM-DA$，基于大型语言模型，用于少样本NER任务，在上下文和实体层面增强数据，展示了显著的性能提升。",
    "en_tdlr": "Proposed a new data augmentation technique $LLM-DA$, based on large language models, for few-shot NER task, augmenting data at both contextual and entity levels, demonstrating significant performance improvements."
}