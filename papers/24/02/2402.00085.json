{
    "title": "Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning",
    "abstract": "Training task-oriented dialog agents based on reinforcement learning is time-consuming and requires a large number of interactions with real users. How to grasp dialog policy within limited dialog experiences remains an obstacle that makes the agent training process less efficient. In addition, most previous frameworks start training by randomly choosing training samples, which differs from the human learning method and hurts the efficiency and stability of training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a curiosity-driven curriculum learning framework based on a state-of-the-art model-based reinforcement learning dialog model, Deep Dyna-Q (DDQ). Furthermore, we designed learning schedules for SC-DDQ and DDQ, respectively, following two opposite training strategies: classic curriculum learning and its reverse version. Our results show that by introducing scheduled learning and curiosity, the new framework leads to a significant improvement over the DDQ and Dee",
    "link": "https://arxiv.org/abs/2402.00085",
    "context": "Title: Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy Learning\nAbstract: Training task-oriented dialog agents based on reinforcement learning is time-consuming and requires a large number of interactions with real users. How to grasp dialog policy within limited dialog experiences remains an obstacle that makes the agent training process less efficient. In addition, most previous frameworks start training by randomly choosing training samples, which differs from the human learning method and hurts the efficiency and stability of training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a curiosity-driven curriculum learning framework based on a state-of-the-art model-based reinforcement learning dialog model, Deep Dyna-Q (DDQ). Furthermore, we designed learning schedules for SC-DDQ and DDQ, respectively, following two opposite training strategies: classic curriculum learning and its reverse version. Our results show that by introducing scheduled learning and curiosity, the new framework leads to a significant improvement over the DDQ and Dee",
    "path": "papers/24/02/2402.00085.json",
    "total_tokens": 885,
    "translated_title": "预定好奇心-深度动态-Q：对话策略学习的高效探索",
    "translated_abstract": "基于强化学习的任务导向对话代理的培训是耗时的，并需要与真实用户进行大量的交互。如何在有限的对话经验中掌握对话策略仍然是使代理培训过程更加高效的障碍。此外，大多数先前的框架通过随机选择培训样本开始培训，这与人类学习方法不同，损害了培训的效率和稳定性。因此，我们提出了一种基于最先进的基于模型的强化学习对话模型Deep Dyna-Q(DDQ)的预定好奇心-深度动态-Q (SC-DDQ)的好奇心驱动课程学习框架。此外，我们分别为SC-DDQ和DDQ设计了学习计划，遵循两种相反的培训策略：经典课程学习及其逆向版本。我们的结果表明，通过引入预定学习和好奇心，新框架在DDQ和Dee的基础上取得了显著的改进。",
    "tldr": "本论文提出了Scheduled Curiosity-Deep Dyna-Q (SC-DDQ)框架，通过引入预定学习和好奇心，显著提高了基于强化学习的任务导向对话代理的效果。",
    "en_tdlr": "This paper proposes the Scheduled Curiosity-Deep Dyna-Q (SC-DDQ) framework, which significantly improves the performance of task-oriented dialog agents based on reinforcement learning by introducing scheduled learning and curiosity."
}