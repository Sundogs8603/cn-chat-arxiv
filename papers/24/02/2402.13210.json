{
    "title": "Bayesian Reward Models for LLM Alignment",
    "abstract": "arXiv:2402.13210v1 Announce Type: new  Abstract: To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data. We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback). However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data. It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimi",
    "link": "https://arxiv.org/abs/2402.13210",
    "context": "Title: Bayesian Reward Models for LLM Alignment\nAbstract: arXiv:2402.13210v1 Announce Type: new  Abstract: To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data. We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback). However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data. It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimi",
    "path": "papers/24/02/2402.13210.json",
    "total_tokens": 810,
    "translated_title": "Bayesian Reward Models for LLM Alignment",
    "translated_abstract": "为了确保大型语言模型（LLM）的回复有益且无毒，通常我们会在人类偏好数据上微调奖励模型。然后我们选择具有高奖励的策略回复（best-of-n抽样），或者进一步优化策略以生成具有高奖励的回复（从人类反馈中进行强化学习）。然而，这个过程容易受到奖励过度优化或攻击的影响，选定的回复由于奖励模型中的错误而具有高奖励，而不是真实偏好。这一问题在提示或回复偏离训练数据时尤为严重。我们通过训练贝叶斯奖励模型来缓解这些问题，这种模型在远离训练数据分布时会产生更高的不确定性。因此，我们使用Laplace-LoRA（Yang等人，2024）训练了贝叶斯奖励模型，发现由此产生的不确定性估计可以成功缓解奖励的过度优化。",
    "tldr": "通过训练贝叶斯奖励模型，可以成功缓解奖励的过度优化问题。",
    "en_tdlr": "Training Bayesian reward models can successfully mitigate reward overoptimization."
}