{
    "title": "Skill or Luck? Return Decomposition via Advantage Functions",
    "abstract": "arXiv:2402.12874v1 Announce Type: new  Abstract: Learning from off-policy data is essential for sample-efficient reinforcement learning. In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent's actions (skill) and parts outside of the agent's control (luck). Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The resulting method can learn from off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions. We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important. Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to sub",
    "link": "https://arxiv.org/abs/2402.12874",
    "context": "Title: Skill or Luck? Return Decomposition via Advantage Functions\nAbstract: arXiv:2402.12874v1 Announce Type: new  Abstract: Learning from off-policy data is essential for sample-efficient reinforcement learning. In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent's actions (skill) and parts outside of the agent's control (luck). Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The resulting method can learn from off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions. We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important. Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to sub",
    "path": "papers/24/02/2402.12874.json",
    "total_tokens": 878,
    "translated_title": "技能还是运气？通过优势函数对回报进行分解",
    "translated_abstract": "学习来自离线数据对于高效的强化学习至关重要。在本工作中，我们基于这样的洞察力，即优势函数可以被理解为动作对回报的因果影响，并展示了这使我们能够将轨迹的回报分解为由代理的动作（技能）引起的部分和代理无法控制的部分（运气）。此外，这种分解使我们能够自然地将直接优势估计（DAE）扩展到离线设置（离线DAE）。由此产生的方法可以从离线轨迹中学习，而无需依赖重要性采样技术或截断离线动作。我们建立离线DAE与先前方法之间的联系，以展示它如何加速学习以及当所提出的离线校正何时重要。最后，我们使用MinAtar环境来说明忽略离线校正可能导致子",
    "tldr": "优势函数的创新在于将回报分解为代理动作引起的部分（技能）和代理无法控制的部分（运气），进而扩展了直接优势估计到离线环境，使得从离线轨迹学习更加高效。"
}