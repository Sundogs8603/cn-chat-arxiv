{
    "title": "Do Transformer World Models Give Better Policy Gradients?",
    "abstract": "A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimizat",
    "link": "https://arxiv.org/abs/2402.05290",
    "context": "Title: Do Transformer World Models Give Better Policy Gradients?\nAbstract: A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimizat",
    "path": "papers/24/02/2402.05290.json",
    "total_tokens": 985,
    "translated_title": "变形器世界模型是否可以给出更好的策略梯度？",
    "translated_abstract": "对于强化学习来说，一种自然的方法是通过展开神经网络世界模型来预测未来的奖励，并通过计算图进行反向传播以学习策略。然而，由于典型的世界模型产生了难以优化的损失地形，这种方法在长时间跨度上通常变得不可行。变形器已知可以高效地传播长时间跨度的梯度：它们是否可以解决这个问题呢？令人惊讶的是，我们发现常用的变形器世界模型会产生迂回的梯度路径，这对于长距离的策略梯度是有害的。为了应对这个挑战，我们提出了一类称为Actions World Models (AWMs)的世界模型，旨在提供更直接的梯度传播路径。我们将这种AWMs集成到一个策略梯度的框架中，强调了网络架构与策略梯度更新之间的关系。我们证明了AWMs可以产生可优化的梯度路径。",
    "tldr": "在强化学习中，通过使用变形器世界模型来预测未来奖励并进行策略梯度学习通常变得不可行。研究人员发现常用的变形器世界模型会产生迂回的梯度路径，对于长距离的策略梯度是有害的。为了解决这个问题，他们提出了一种名为Actions World Models (AWMs)的世界模型，可以提供更直接的梯度传播路径。",
    "en_tdlr": "In reinforcement learning, using transformer world models to predict future rewards and learn policy gradients often becomes impractical. Researchers found that commonly used transformer world models produce circuitous gradient paths, which are detrimental for long-range policy gradients. To address this issue, they proposed a world model called Actions World Models (AWMs) that provides more direct routes for gradient propagation."
}