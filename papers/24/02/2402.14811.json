{
    "title": "Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking",
    "abstract": "arXiv:2402.14811v1 Announce Type: new  Abstract: Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit implements entity tracking. In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionalit",
    "link": "https://arxiv.org/abs/2402.14811",
    "context": "Title: Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking\nAbstract: arXiv:2402.14811v1 Announce Type: new  Abstract: Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit implements entity tracking. In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionalit",
    "path": "papers/24/02/2402.14811.json",
    "total_tokens": 836,
    "translated_title": "微调增强现有机制：实体跟踪案例研究",
    "translated_abstract": "细化在诸如遵循指令、生成代码和数学等广义任务上已经显示出增强语言模型在一系列任务上的性能。然而，关于这种微调如何影响这些模型中内部计算的解释仍然难以捉摸。我们研究了微调如何影响语言模型中实现的内部机制。作为一个案例研究，我们探讨了实体跟踪的特性，这是语言理解的一个重要方面，在数学上进行了微调的模型在性能上有显著提升。我们识别出了实现实体跟踪的机制，并显示出（i）原始模型和其精细调整版本主要实现实体跟踪的是相同的电路。事实上，原始模型的实体跟踪电路在经过微调的版本上的性能优于完整的原始模型。（ii）所有模型的电路实现大致相同的功能",
    "tldr": "通过对语言模型进行微调，我们研究了如何影响实体跟踪等内部机制，并发现微调能够在数学任务上实现明显的性能提升。",
    "en_tdlr": "By fine-tuning language models, we study how it affects internal mechanisms like entity tracking, showing significant performance improvements in mathematics tasks."
}