{
    "title": "ScreenAgent: A Vision Language Model-driven Computer Control Agent",
    "abstract": "Existing Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphics User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing a variety of daily computer tasks. Finally, we trained a model, Screen",
    "link": "https://arxiv.org/abs/2402.07945",
    "context": "Title: ScreenAgent: A Vision Language Model-driven Computer Control Agent\nAbstract: Existing Large Language Models (LLM) can invoke a variety of tools and APIs to complete complex tasks. The computer, as the most powerful and universal tool, could potentially be controlled directly by a trained LLM agent. Powered by the computer, we can hopefully build a more generalized agent to assist humans in various daily digital works. In this paper, we construct an environment for a Vision Language Model (VLM) agent to interact with a real computer screen. Within this environment, the agent can observe screenshots and manipulate the Graphics User Interface (GUI) by outputting mouse and keyboard actions. We also design an automated control pipeline that includes planning, acting, and reflecting phases, guiding the agent to continuously interact with the environment and complete multi-step tasks. Additionally, we construct the ScreenAgent Dataset, which collects screenshots and action sequences when completing a variety of daily computer tasks. Finally, we trained a model, Screen",
    "path": "papers/24/02/2402.07945.json",
    "total_tokens": 871,
    "translated_title": "ScreenAgent:一种基于视觉语言模型驱动的计算机控制代理",
    "translated_abstract": "现有的大型语言模型(LLM)可以调用各种工具和API来完成复杂任务。作为最强大和通用的工具，计算机可能被训练有素的LLM代理直接控制。由计算机驱动，我们希望构建一个更通用的代理，在各种日常数字工作中协助人类。在本文中，我们构建了一个环境，用于让视觉语言模型(VLM)代理与真实的计算机屏幕进行交互。在这个环境中，代理可以观察屏幕截图，并通过输出鼠标和键盘动作来操作图形用户界面(GUI)。我们还设计了一个自动控制流水线，包括规划、行动和反思阶段，指导代理不断与环境交互，完成多步任务。此外，我们构建了ScreenAgent数据集，该数据集收集了完成各种日常计算机任务时的屏幕截图和动作序列。最后，我们训练了一个模型，ScreenAgent。",
    "tldr": "本文介绍了一种基于视觉语言模型的计算机控制代理ScreenAgent，该代理可以通过观察屏幕截图和输出鼠标键盘动作与计算机屏幕进行交互，完成多步任务。"
}