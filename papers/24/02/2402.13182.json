{
    "title": "Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness",
    "abstract": "arXiv:2402.13182v1 Announce Type: new  Abstract: We consider distributed kernel bandits where $N$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space. Each agent sequentially queries the function to obtain noisy observations at the query points. Agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $T$ and aggregating over agents. We develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $N$ and $T$. The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. Working together with the sparse approximation of the GP model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.",
    "link": "https://arxiv.org/abs/2402.13182",
    "context": "Title: Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness\nAbstract: arXiv:2402.13182v1 Announce Type: new  Abstract: We consider distributed kernel bandits where $N$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space. Each agent sequentially queries the function to obtain noisy observations at the query points. Agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $T$ and aggregating over agents. We develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $N$ and $T$. The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. Working together with the sparse approximation of the GP model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.",
    "path": "papers/24/02/2402.13182.json",
    "total_tokens": 836,
    "translated_title": "使用共享随机性的均匀采样实现分布式核赌博机中的最优次序遗憾",
    "translated_abstract": "我们考虑分布式核赌博机问题，其中$N$个代理旨在协同最大化存在于再生核希尔伯特空间中的未知奖励函数。每个代理依次查询该函数，以在查询点处获得嘈杂的观测值。代理可以通过中央服务器共享信息，目的是最小化随着时间$T$累积并汇总在代理之间的遗憾。我们开发了第一个算法，该算法实现了在通信成本对$N$和$T$均为亚线性的情况下实现了最优遗憾次序（由集中式学习定义）。所提出算法的关键特点是局部代理的均匀探索和与中央服务器的共享随机性。与GP模型的稀疏逼近一起工作，这两个关键组件使得能够以通信的衰减速率保持中央设置的学习速率。",
    "tldr": "提出的算法在分布式核赌博机问题中实现了最优次序遗憾，并且通信成本对于代理数量和时间都是亚线性的。"
}