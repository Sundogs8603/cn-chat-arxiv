{
    "title": "Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models",
    "abstract": "arXiv:2402.17671v1 Announce Type: new  Abstract: As foundation models (FMs) continue to shape the landscape of AI, the in-context learning (ICL) paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency. Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem. In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within ICL frameworks, focusing on four key methodologies, each with its corresponding subgoals. We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent ICL environment, thereby unlocking their vast potential.",
    "link": "https://arxiv.org/abs/2402.17671",
    "context": "Title: Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models\nAbstract: arXiv:2402.17671v1 Announce Type: new  Abstract: As foundation models (FMs) continue to shape the landscape of AI, the in-context learning (ICL) paradigm thrives but also encounters issues such as toxicity, hallucination, disparity, adversarial vulnerability, and inconsistency. Ensuring the reliability and responsibility of FMs is crucial for the sustainable development of the AI ecosystem. In this concise overview, we investigate recent advancements in enhancing the reliability and trustworthiness of FMs within ICL frameworks, focusing on four key methodologies, each with its corresponding subgoals. We sincerely hope this paper can provide valuable insights for researchers and practitioners endeavoring to build safe and dependable FMs and foster a stable and consistent ICL environment, thereby unlocking their vast potential.",
    "path": "papers/24/02/2402.17671.json",
    "total_tokens": 828,
    "translated_title": "加强上下文学习以确保可靠性：对基础模型的简要概述",
    "translated_abstract": "随着基础模型（FMs）继续塑造人工智能的格局，上下文学习（ICL）范式蓬勃发展，但也遇到了毒性、幻觉、差异、对抗性脆弱性和不一致性等问题。确保FMs的可靠性和责任性对于人工智能生态系统的可持续发展至关重要。在这篇简明概述中，我们调查了增强FMs在ICL框架内可靠性和可信度的最新进展，重点关注四种关键方法论，每种方法论都有其相应的子目标。我们真诚希望本文能为致力于构建安全可靠FMs并促进稳定一致的ICL环境、从而释放其巨大潜力的研究人员和从业者提供宝贵的见解。",
    "tldr": "论文回顾了增强基础模型可靠性和可信度的最新进展，着重于四种关键方法论，为构建安全可靠的FMs和促进稳定一致的ICL环境提供了有价值的见解。",
    "en_tdlr": "The paper reviews recent advancements in enhancing the reliability and trustworthiness of foundation models, focusing on four key methodologies and providing valuable insights for building safe and dependable FMs and fostering a stable and consistent ICL environment."
}