{
    "title": "CodeS: Towards Building Open-source Language Models for Text-to-SQL",
    "abstract": "arXiv:2402.16347v1 Announce Type: new  Abstract: Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema lin",
    "link": "https://arxiv.org/abs/2402.16347",
    "context": "Title: CodeS: Towards Building Open-source Language Models for Text-to-SQL\nAbstract: arXiv:2402.16347v1 Announce Type: new  Abstract: Language models have shown promising performance on the task of translating natural language questions into SQL queries (Text-to-SQL). However, most of the state-of-the-art (SOTA) approaches rely on powerful yet closed-source large language models (LLMs), such as ChatGPT and GPT-4, which may have the limitations of unclear model architectures, data privacy risks, and expensive inference overheads. To address the limitations, we introduce CodeS, a series of pre-trained language models with parameters ranging from 1B to 15B, specifically designed for the text-to-SQL task. CodeS is a fully open-source language model, which achieves superior accuracy with much smaller parameter sizes. This paper studies the research challenges in building CodeS. To enhance the SQL generation abilities of CodeS, we adopt an incremental pre-training approach using a specifically curated SQL-centric corpus. Based on this, we address the challenges of schema lin",
    "path": "papers/24/02/2402.16347.json",
    "total_tokens": 879,
    "translated_title": "CodeS：构建用于文本到SQL的开源语言模型",
    "translated_abstract": "语言模型在将自然语言问题翻译为SQL查询（Text-to-SQL）任务中表现出有希望的性能。然而，大多数最先进的方法依赖于强大但封闭源的大型语言模型（LLMs），如ChatGPT和GPT-4，可能存在模型架构不明确、数据隐私风险和昂贵的推理开销等局限性。为解决这些问题，我们引入了CodeS，一系列针对文本到SQL任务的预训练语言模型，其参数范围从1B到15B不等。CodeS是一个完全开源的语言模型，能以更小的参数规模实现更高的准确性。本文研究了构建CodeS时面临的研究挑战。为增强CodeS的SQL生成能力，我们采用了一个特别设计的SQL中心语料库进行渐进预训练。基于此，我们解决了架构链接挑战等问题。",
    "tldr": "CodeS是一系列用于文本到SQL任务的开源语言模型，通过较小的参数规模实现了更高的准确性，采用了具有挑战性的SQL中心语料库进行渐进预训练来增强其SQL生成能力。",
    "en_tdlr": "CodeS is a series of open-source language models designed for the task of text-to-SQL, achieving superior accuracy with smaller parameter sizes and enhancing SQL generation abilities through incremental pre-training with a challenging SQL-centric corpus."
}