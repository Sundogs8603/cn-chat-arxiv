{
    "title": "ELAD: Explanation-Guided Large Language Models Active Distillation",
    "abstract": "arXiv:2402.13098v1 Announce Type: cross  Abstract: The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation. In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance. To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps. Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the stu",
    "link": "https://arxiv.org/abs/2402.13098",
    "context": "Title: ELAD: Explanation-Guided Large Language Models Active Distillation\nAbstract: arXiv:2402.13098v1 Announce Type: cross  Abstract: The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation. In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance. To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps. Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the stu",
    "path": "papers/24/02/2402.13098.json",
    "total_tokens": 840,
    "translated_title": "ELAD: 解释引导的大型语言模型主动蒸馏",
    "translated_abstract": "大型语言模型（LLMs）的部署和应用受到它们的内存效率、计算要求和高成本的API推断的阻碍。传统的蒸馏方法往往未能确定知识是否已经被充分转移，可能导致高成本或不完整的蒸馏。本文提出了一种名为Explanation-Guided LLMs Active Distillation（ELAD）框架，采用主动学习策略来优化注释成本和模型性能之间的平衡。为了改进有效的样本选择，我们引入了一种基于解释的样本选择方法，通过利用解释步骤中的不确定性来识别挑战其推理的样本。此外，我们提出了一种定制的LLM-注释解释修订技术，其中教师模型检测并纠正学生模型中的缺陷。",
    "tldr": "ELAD提出了一种Explanation-Guided LLMs Active Distillation框架，通过主动学习策略优化注释成本和模型性能之间的平衡，并引入了基于解释的样本选择方法和LLM-注释解释修订技术。",
    "en_tdlr": "ELAD introduces an Explanation-Guided LLMs Active Distillation framework that optimizes the balance between annotation costs and model performance using active learning strategy, and it includes an explanation-guided sample selection method and LLM-annotated explanation revision technique."
}