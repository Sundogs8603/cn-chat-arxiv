{
    "title": "Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses",
    "abstract": "Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the oth",
    "link": "https://arxiv.org/abs/2402.02648",
    "context": "Title: Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses\nAbstract: Large Language Models (LLMs) frequently suffer from knowledge-intensive questions, often being inconsistent by providing different outputs despite given the same input. The response quality worsens when the user expresses a firm opposing stance which causes the LLMs to adjust its response despite the correct initial one. These behaviors decrease the reliability and validity of the responses provided by these models. In this paper, we attempt to 1) raise awareness of the inherent risks that follow from overly relying on AI agents like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate more from the actual answer and 2) suggest a novel prompting method, Recursive Chain of Feedback (R-CoF), that we are conducting further study. The CoF system takes in an open-ended multi-step question. Then, we repetitively provide meaningless feedback requesting another attempt. Our preliminary experiments show that such feedback only decreases the quality of the response. On the oth",
    "path": "papers/24/02/2402.02648.json",
    "total_tokens": 926,
    "translated_title": "链式反馈：缓解回答不一致性的影响",
    "translated_abstract": "大型语言模型（LLMs）在回答知识密集型问题时经常出现不一致的情况，即使输入相同，也会提供不同的输出。当用户表达坚决相反的立场时，LLMs调整其回答的质量会变差，尽管初始回答是正确的。这些行为降低了这些模型提供的回答的可靠性和有效性。在本文中，我们试图：1）通过展示链式反馈（CoF）如何导致LLMs更加偏离实际答案，引起过度依赖ChatGPT等AI代理带来的固有风险；2）提出一种新的提示方法，递归链式反馈（R-CoF），我们正在进行进一步研究。CoF系统接收一个开放式多步问题，然后我们重复提供无意义的反馈，要求再次尝试。我们的初步实验表明，这种反馈只会降低回答的质量。另一方面，",
    "tldr": "本文提出了链式反馈（CoF）方法，以缓解大型语言模型在回答问题中出现不一致性的问题。同时，作者还提出了递归链式反馈（R-CoF）方法，并提到了进一步的研究。这些方法可以提高回答的可靠性和有效性。",
    "en_tdlr": "This paper proposes the Chain-of-Feedback (CoF) method to mitigate the inconsistency issue in large language models (LLMs) when answering questions. Additionally, the authors suggest the Recursive Chain of Feedback (R-CoF) method and mention further study. These methods aim to improve the reliability and validity of the responses."
}