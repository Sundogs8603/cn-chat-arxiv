{
    "title": "Q-Probe: A Lightweight Approach to Reward Maximization for Language Models",
    "abstract": "arXiv:2402.14688v1 Announce Type: new  Abstract: We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either. The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes. Moreover, a Q-probe can be ",
    "link": "https://arxiv.org/abs/2402.14688",
    "context": "Title: Q-Probe: A Lightweight Approach to Reward Maximization for Language Models\nAbstract: arXiv:2402.14688v1 Announce Type: new  Abstract: We present an approach called Q-probing to adapt a pre-trained language model to maximize a task-specific reward function. At a high level, Q-probing sits between heavier approaches such as finetuning and lighter approaches such as few shot prompting, but can also be combined with either. The idea is to learn a simple linear function on a model's embedding space that can be used to reweight candidate completions. We theoretically show that this sampling procedure is equivalent to a KL-constrained maximization of the Q-probe as the number of samples increases. To train the Q-probes we consider either reward modeling or a class of novel direct policy learning objectives based on importance weighted policy gradients. With this technique, we see gains in domains with ground-truth rewards (code generation) as well as implicit rewards defined by preference data, even outperforming finetuning in data-limited regimes. Moreover, a Q-probe can be ",
    "path": "papers/24/02/2402.14688.json",
    "total_tokens": 922,
    "translated_title": "Q-Probe: 一种轻量级方法，用于语言模型的奖励最大化",
    "translated_abstract": "我们提出了一种称为Q-probing的方法，用于调整预训练语言模型以最大化任务特定的奖励函数。在高层次上，Q-probing位于像微调这样较重的方法和像少量提示这样较轻的方法之间，但也可以与任一种方法结合使用。其想法是在模型的嵌入空间上学习一个简单的线性函数，该函数可用于重新加权候选完成。我们从理论上证明，随着样本数量的增加，这种采样过程等同于Q-probe的KL约束最大化。为了训练Q-probes，我们考虑奖励建模或基于重要性加权策略梯度的一类新型直接策略学习目标。通过这种技术，我们看到在具有基于地面真实奖励（代码生成）以及由偏好数据定义的隐式奖励的领域中获得收益，甚至在数据有限的情况下胜过微调。此外，Q-probe可以",
    "tldr": "Q-Probe是一种轻量级方法，通过学习简单的线性函数在模型的嵌入空间中重新加权候选完成，从而调整预训练语言模型以最大化任务特定的奖励函数，在各种领域中获得显著收益。",
    "en_tdlr": "Q-Probe is a lightweight approach that maximizes task-specific reward function for language models by learning a simple linear function to reweight candidate completions in the embedding space of the model, achieving significant gains in various domains."
}