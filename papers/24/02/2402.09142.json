{
    "title": "When Representations Align: Universality in Representation Learning Dynamics",
    "abstract": "arXiv:2402.09142v1 Announce Type: new Abstract: Deep neural networks come in many sizes and architectures. The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations. Yet, recent results have shown that different architectures learn representations with striking qualitative similarities. Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions. This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization. We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits ",
    "link": "https://arxiv.org/abs/2402.09142",
    "context": "Title: When Representations Align: Universality in Representation Learning Dynamics\nAbstract: arXiv:2402.09142v1 Announce Type: new Abstract: Deep neural networks come in many sizes and architectures. The choice of architecture, in conjunction with the dataset and learning algorithm, is commonly understood to affect the learned neural representations. Yet, recent results have shown that different architectures learn representations with striking qualitative similarities. Here we derive an effective theory of representation learning under the assumption that the encoding map from input to hidden representation and the decoding map from representation to output are arbitrary smooth functions. This theory schematizes representation learning dynamics in the regime of complex, large architectures, where hidden representations are not strongly constrained by the parametrization. We show through experiments that the effective theory describes aspects of representation learning dynamics across a range of deep networks with different activation functions and architectures, and exhibits ",
    "path": "papers/24/02/2402.09142.json",
    "total_tokens": 781,
    "translated_title": "当表示对齐时：表示学习动力学中的普遍性",
    "translated_abstract": "深度神经网络有许多不同的大小和结构。架构的选择，结合数据集和学习算法，普遍认为影响了学习到的神经表示。然而，最近的研究结果显示，不同的架构学习到的表示具有惊人的定性相似性。在这里，我们在将输入到隐藏表示的编码映射和从表示到输出的解码映射都是任意光滑函数的假设下，推导了表示学习的有效理论。在复杂和大型架构的情况下，隐藏表示没有被参数化强约束，该理论概括了表示学习动力学。我们通过实验证明，这个有效理论描述了具有不同激活函数和架构的深度网络中表示学习动力学的一些方面。",
    "tldr": "本研究提出了一个有效的表示学习理论，该理论假设了编码映射和解码映射为任意光滑函数，并且能够描述复杂且大型架构中的表示学习动力学。",
    "en_tdlr": "This study presents an effective theory of representation learning that assumes arbitrary smooth functions for the encoding and decoding maps, and it can describe the dynamics of representation learning in complex and large architectures."
}