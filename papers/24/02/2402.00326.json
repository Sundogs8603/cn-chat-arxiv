{
    "title": "PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks",
    "abstract": "While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initializatio",
    "link": "https://arxiv.org/abs/2402.00326",
    "context": "Title: PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks\nAbstract: While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initializatio",
    "path": "papers/24/02/2402.00326.json",
    "total_tokens": 933,
    "translated_title": "PirateNets：采用残差自适应网络的物理知识驱动深度学习",
    "translated_abstract": "虽然物理知识驱动神经网络(PINNs)已成为解决由偏微分方程(PDEs)控制的正向和反向问题的流行深度学习框架，但在采用更大和更深的神经网络架构时，它们的性能会下降。我们的研究发现，这种反直觉行为的根源在于使用不适合的初始化方案的多层感知机(MLP)网络结构，导致网络导数的可训练性较差，并最终导致PDE残差损失的不稳定最小化。为了解决这个问题，我们提出了物理知识驱动残差自适应网络(PirateNets)，这是一种新型架构，旨在促进深度PINN模型的稳定和高效训练。PirateNets利用一种新颖的自适应残差连接，允许网络作为浅层网络进行初始化，并在训练过程中逐渐加深。我们还展示了所提出的初始化方案可以提高PINN模型的训练效果并改善性能。",
    "tldr": "PirateNets是一种物理知识驱动的深度学习框架，解决了多层感知机网络在较大深度时性能下降的问题，通过引入自适应残差连接实现了稳定和高效的训练，并提升了模型的性能。",
    "en_tdlr": "PirateNets is a physics-informed deep learning framework that addresses the performance degradation issue of multi-layer perceptron networks in deeper architectures. It achieves stable and efficient training by introducing adaptive residual connections and improves the model's performance."
}