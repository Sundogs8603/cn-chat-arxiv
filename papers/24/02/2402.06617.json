{
    "title": "FaBERT: Pre-training BERT on Persian Blogs",
    "abstract": "We introduce FaBERT, a Persian BERT-base model pre-trained on the HmBlogs corpus, encompassing both informal and formal Persian texts. FaBERT is designed to excel in traditional Natural Language Understanding (NLU) tasks, addressing the intricacies of diverse sentence structures and linguistic styles prevalent in the Persian language. In our comprehensive evaluation of FaBERT on 12 datasets in various downstream tasks, encompassing Sentiment Analysis (SA), Named Entity Recognition (NER), Natural Language Inference (NLI), Question Answering (QA), and Question Paraphrasing (QP), it consistently demonstrated improved performance, all achieved within a compact model size. The findings highlight the importance of utilizing diverse and cleaned corpora, such as HmBlogs, to enhance the performance of language models like BERT in Persian Natural Language Processing (NLP) applications. FaBERT is openly accessible at https://huggingface.co/sbunlp/fabert",
    "link": "https://arxiv.org/abs/2402.06617",
    "context": "Title: FaBERT: Pre-training BERT on Persian Blogs\nAbstract: We introduce FaBERT, a Persian BERT-base model pre-trained on the HmBlogs corpus, encompassing both informal and formal Persian texts. FaBERT is designed to excel in traditional Natural Language Understanding (NLU) tasks, addressing the intricacies of diverse sentence structures and linguistic styles prevalent in the Persian language. In our comprehensive evaluation of FaBERT on 12 datasets in various downstream tasks, encompassing Sentiment Analysis (SA), Named Entity Recognition (NER), Natural Language Inference (NLI), Question Answering (QA), and Question Paraphrasing (QP), it consistently demonstrated improved performance, all achieved within a compact model size. The findings highlight the importance of utilizing diverse and cleaned corpora, such as HmBlogs, to enhance the performance of language models like BERT in Persian Natural Language Processing (NLP) applications. FaBERT is openly accessible at https://huggingface.co/sbunlp/fabert",
    "path": "papers/24/02/2402.06617.json",
    "total_tokens": 901,
    "translated_title": "FaBERT: 基于波斯语博客的BERT预训练模型",
    "translated_abstract": "我们介绍了FaBERT，一个基于波斯语HmBlogs语料库进行预训练的波斯语BERT-base模型，包括非正式和正式的波斯语文本。FaBERT旨在在传统的自然语言理解（NLU）任务中表现出色，解决波斯语中常见的句子结构和语言风格的复杂问题。在我们对FaBERT在12个数据集上进行的全面评估中，涵盖了情感分析（SA）、命名实体识别（NER）、自然语言推理（NLI）、问答系统（QA）和问题改写（QP）等不同的下游任务，它始终展示出改进的性能，并且模型尺寸较小。研究结果强调了利用多样化和清理好的语料库（如HmBlogs）来提高类似BERT在波斯语自然语言处理（NLP）应用中的性能的重要性。FaBERT可以在https://huggingface.co/sbunlp/fabert上免费获得。",
    "tldr": "本论文介绍了FaBERT，一个基于波斯语博客进行预训练的BERT模型。它在各种下游任务中展示出更好的性能，并且强调了利用多样化和清理好的语料库来提高在波斯语自然语言处理应用中的性能的重要性。",
    "en_tdlr": "This paper introduces FaBERT, a BERT model pre-trained on Persian blogs. It demonstrates improved performance in various downstream tasks and emphasizes the importance of utilizing diverse and cleaned corpora to enhance performance in Persian natural language processing applications."
}