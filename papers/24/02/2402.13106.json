{
    "title": "On Generalization Bounds for Deep Compound Gaussian Neural Networks",
    "abstract": "arXiv:2402.13106v1 Announce Type: cross  Abstract: Algorithm unfolding or unrolling is the technique of constructing a deep neural network (DNN) from an iterative algorithm. Unrolled DNNs often provide better interpretability and superior empirical performance over standard DNNs in signal estimation tasks. An important theoretical question, which has only recently received attention, is the development of generalization error bounds for unrolled DNNs. These bounds deliver theoretical and practical insights into the performance of a DNN on empirical datasets that are distinct from, but sampled from, the probability density generating the DNN training data. In this paper, we develop novel generalization error bounds for a class of unrolled DNNs that are informed by a compound Gaussian prior. These compound Gaussian networks have been shown to outperform comparative standard and unfolded deep neural networks in compressive sensing and tomographic imaging problems. The generalization error",
    "link": "https://arxiv.org/abs/2402.13106",
    "context": "Title: On Generalization Bounds for Deep Compound Gaussian Neural Networks\nAbstract: arXiv:2402.13106v1 Announce Type: cross  Abstract: Algorithm unfolding or unrolling is the technique of constructing a deep neural network (DNN) from an iterative algorithm. Unrolled DNNs often provide better interpretability and superior empirical performance over standard DNNs in signal estimation tasks. An important theoretical question, which has only recently received attention, is the development of generalization error bounds for unrolled DNNs. These bounds deliver theoretical and practical insights into the performance of a DNN on empirical datasets that are distinct from, but sampled from, the probability density generating the DNN training data. In this paper, we develop novel generalization error bounds for a class of unrolled DNNs that are informed by a compound Gaussian prior. These compound Gaussian networks have been shown to outperform comparative standard and unfolded deep neural networks in compressive sensing and tomographic imaging problems. The generalization error",
    "path": "papers/24/02/2402.13106.json",
    "total_tokens": 809,
    "translated_title": "关于深度复合高斯神经网络的泛化界限",
    "translated_abstract": "算法展开或滚动是一种从迭代算法构建深度神经网络（DNN）的技术。展开的DNN在信号估计任务中通常比标准DNN提供更好的可解释性和更优越的经验性能。一个重要的理论问题是最近才引起关注的是为展开的DNN开发泛化误差界限。这些界限提供了理论和实际洞察，说明了DNN在与生成DNN训练数据的概率密度不同但采样自其中的经验数据集上的表现。在本文中，我们为一类受复合高斯先验启发的展开DNN开发了新颖的泛化误差界限。已经显示这些复合高斯网络在压缩感知和层析成像问题中优于比较的标准和展开的深度神经网络。",
    "tldr": "本文针对受复合高斯先验启发的展开DNN，提出了新颖的泛化误差界限，这些网络在压缩感知和层析成像问题中表现出色。",
    "en_tdlr": "Novel generalization error bounds are developed for a class of unrolled DNNs informed by a compound Gaussian prior, which have shown superior performance in compressive sensing and tomographic imaging problems."
}