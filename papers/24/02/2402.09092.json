{
    "title": "Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks",
    "abstract": "arXiv:2402.09092v1 Announce Type: new Abstract: Neural networks have proven to be a highly effective tool for solving complex problems in many areas of life. Recently, their importance and practical usability have further been reinforced with the advent of deep learning. One of the important conditions for the success of neural networks is the choice of an appropriate activation function introducing non-linearity into the model. Many types of these functions have been proposed in the literature in the past, but there is no single comprehensive source containing their exhaustive overview. The absence of this overview, even in our experience, leads to redundancy and the unintentional rediscovery of already existing activation functions. To bridge this gap, our paper presents an extensive survey involving 400 activation functions, which is several times larger in scale than previous surveys. Our comprehensive compilation also references these surveys; however, its main goal is to provide ",
    "link": "https://arxiv.org/abs/2402.09092",
    "context": "Title: Three Decades of Activations: A Comprehensive Survey of 400 Activation Functions for Neural Networks\nAbstract: arXiv:2402.09092v1 Announce Type: new Abstract: Neural networks have proven to be a highly effective tool for solving complex problems in many areas of life. Recently, their importance and practical usability have further been reinforced with the advent of deep learning. One of the important conditions for the success of neural networks is the choice of an appropriate activation function introducing non-linearity into the model. Many types of these functions have been proposed in the literature in the past, but there is no single comprehensive source containing their exhaustive overview. The absence of this overview, even in our experience, leads to redundancy and the unintentional rediscovery of already existing activation functions. To bridge this gap, our paper presents an extensive survey involving 400 activation functions, which is several times larger in scale than previous surveys. Our comprehensive compilation also references these surveys; however, its main goal is to provide ",
    "path": "papers/24/02/2402.09092.json",
    "total_tokens": 861,
    "translated_title": "三十年的激活函数：400个神经网络激活函数综合调查",
    "translated_abstract": "神经网络在许多领域中已被证明是解决复杂问题的高效工具。深度学习的出现进一步加强了它们的重要性和实际可用性。神经网络成功的重要条件之一是选择适当的激活函数引入非线性模型。过去文献中提出了许多类型的激活函数，但没有单一的综合来源包含它们的详尽概述。即使在我们的经验中，缺乏这个概述也会导致冗余和无意中重新发现已有的激活函数。为了弥补这个差距，我们的论文提供了一个涉及400个激活函数的广泛调查，比之前的调查规模大几倍。我们的综合汇编还引用了这些调查；然而，它的主要目标是提供一个详尽的概述和分类的方法，以便研究人员能够更好地了解和选择适当的激活函数。",
    "tldr": "这篇论文是关于对400个神经网络激活函数进行全面调查，为研究人员提供一个详尽的概述和分类的方法，以便更好地选择适当的激活函数。",
    "en_tdlr": "The paper presents a comprehensive survey of 400 activation functions for neural networks, providing researchers with a detailed overview and classification approach for better selection of suitable activation functions."
}