{
    "title": "Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding",
    "abstract": "arXiv:2402.15300v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallu",
    "link": "https://arxiv.org/abs/2402.15300",
    "context": "Title: Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding\nAbstract: arXiv:2402.15300v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallu",
    "path": "papers/24/02/2402.15300.json",
    "total_tokens": 817,
    "translated_title": "见证为信：通过CLIP引导解码缓解大型视觉-语言模型中的幻觉",
    "translated_abstract": "大型视觉-语言模型(LVLMs)容易出现对象幻觉，即生成的文本包含不存在的对象，严重限制了它们的可靠性和实用性。我们首先对句子级LVLM幻觉进行实证分析，发现与图像的CLIP相似性作为一个比单词可能性更强大、更稳健的幻觉指示器。基于这一发现，我们提出了CLIP引导解码（CGD）方法，这是一种简单但有效的无需训练的方法，用于减少解码时的对象幻觉。CGD利用CLIP来引导模型的解码过程，通过增强生成文本与图像的视觉联系。实验表明，CGD有效地减轻了对象幻觉。",
    "tldr": "CLIP相似性作为更强大和更稳健的幻觉指标，研究提出了CLIP引导解码（CGD）方法，在大型视觉-语言模型中有效减少对象幻觉。",
    "en_tdlr": "By utilizing CLIP similarity as a stronger and more robust indicator of hallucination, the study introduces CLIP-Guided Decoding (CGD) approach to effectively reduce object hallucination in large Vision-Language Models."
}