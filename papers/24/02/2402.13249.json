{
    "title": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization",
    "abstract": "arXiv:2402.13249v1 Announce Type: cross  Abstract: Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a cu",
    "link": "https://arxiv.org/abs/2402.13249",
    "context": "Title: TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization\nAbstract: arXiv:2402.13249v1 Announce Type: cross  Abstract: Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a cu",
    "path": "papers/24/02/2402.13249.json",
    "total_tokens": 878,
    "translated_title": "TofuEval：评估LLM在主题对话摘要中的幻觉",
    "translated_abstract": "单文档新闻摘要在忠实度方面取得了长足进步，这得益于对事实一致性或幻觉评估的研究。我们探讨了这些进展是否能延伸到其他文本摘要领域。我们提出了一个新的主题对话摘要评估基准，由不同规模的LLMs生成。我们提供了关于这些摘要的事实一致性的二元句级人类注释，以及对事实不一致句子的详细解释。我们的分析表明，现有的LLMs在对话领域存在大量事实错误的幻觉，无论模型大小如何。另一方面，当LLMs（包括GPT-4）充当二元事实评估器时，它们表现不佳，且可以被当前最先进的专门事实评估度量所超越。最后，我们对幻觉类型进行了分析",
    "tldr": "论文提出了一个新的主题对话摘要评估基准TofuEval，研究发现现有的LLMs在对话领域存在大量事实错误的幻觉，并表明当LLMs充当事实评估器时，其表现不佳。",
    "en_tdlr": "The paper introduces a new evaluation benchmark TofuEval for topic-focused dialogue summarization and reveals that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, indicating poor performance when serving as factual evaluators."
}