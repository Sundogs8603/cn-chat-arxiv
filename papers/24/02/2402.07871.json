{
    "title": "Scaling Laws for Fine-Grained Mixture of Experts",
    "abstract": "Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.",
    "link": "https://arxiv.org/abs/2402.07871",
    "context": "Title: Scaling Laws for Fine-Grained Mixture of Experts\nAbstract: Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.",
    "path": "papers/24/02/2402.07871.json",
    "total_tokens": 1021,
    "translated_title": "细粒度混合专家模型的标度律",
    "translated_abstract": "混合专家（MoE）模型已成为减少大型语言模型计算成本的主要解决方案。在这项工作中，我们分析了它们的标度特性，并纳入了更广泛的变量范围。具体地，我们引入了一个新的超参数，称为粒度，通过调整粒度可以精确控制专家的大小。基于此，我们建立了细粒度MoE的标度律，考虑了训练标记数、模型大小和粒度。利用这些定律，我们推导出了给定计算预算下的最佳训练配置。我们的研究结果不仅表明MoE模型始终优于密集变压器模型，而且还凸显了在扩大模型大小和训练预算时，密集和MoE模型之间的效率差距在扩大。此外，我们证明了将MoE中专家的大小设置为与前馈层相同的常见做法在几乎任何计算预算下都不是最优的。",
    "tldr": "本研究分析了细粒度混合专家模型的标度特性，并引入了粒度作为新的超参数，通过调整粒度可以精确控制专家的大小。研究结果显示，MoE模型在效果上始终优于密集变压器模型，并且随着模型大小和训练预算的增大，密集和MoE模型之间的效率差距也在增大。同时，将MoE中专家的大小设置为与前馈层相同的常见做法在几乎任何计算预算下都不是最优的。",
    "en_tdlr": "This study analyzes the scaling properties of fine-grained Mixture of Experts (MoE) models, introducing a new hyperparameter called granularity for precise control over the size of the experts. The findings show that MoE models consistently outperform dense Transformers, and the efficiency gap between them widens as the model size and training budget increase. The common practice of setting expert size in MoE to match the feed-forward layer is not optimal at almost any computational budget."
}