{
    "title": "Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning",
    "abstract": "arXiv:2402.09542v1 Announce Type: new  Abstract: In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online continual learning me",
    "link": "https://arxiv.org/abs/2402.09542",
    "context": "Title: Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning\nAbstract: arXiv:2402.09542v1 Announce Type: new  Abstract: In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online continual learning me",
    "path": "papers/24/02/2402.09542.json",
    "total_tokens": 921,
    "translated_title": "逐层近端回放：一种在线连续学习的近端点方法",
    "translated_abstract": "在在线连续学习中，神经网络逐步从非独立同分布的数据流中学习。几乎所有的在线连续学习方法都使用经验回放来同时防止灾难性遗忘和过度拟合先前的数据。我们的工作展示了这种方法的一个局限性：使用经验回放训练的网络往往具有不稳定的优化轨迹，影响其整体准确度。令人惊讶的是，即使回放缓冲区存储了所有先前的训练样本，这些不稳定性仍然存在，这表明这个问题与灾难性遗忘是无关的。我们通过对优化几何的简单修改来最小化这些不稳定性。我们的解决方案，逐层近端回放（LPR），在只允许逐渐改变过去数据的隐藏激活的同时，平衡了从新数据和回放数据中的学习。我们证明了LPR在基于回放的在线连续学习方法中持续改进。",
    "tldr": "这项工作针对在线连续学习中经验回放造成的优化不稳定问题进行了改进，提出了一种逐层近端回放（LPR）方法，通过优化几何的修改来平衡新数据和回放数据的学习，从而改善了回放式在线连续学习方法的准确性。",
    "en_tdlr": "This work addresses the optimization instability issue caused by experience replay in online continual learning, proposing a Layerwise Proximal Replay (LPR) method that balances learning from new and replay data by modifying the optimization geometry, improving the accuracy of replay-based online continual learning methods."
}