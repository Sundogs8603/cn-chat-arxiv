{
    "title": "Depth Separation in Norm-Bounded Infinite-Width Neural Networks",
    "abstract": "arXiv:2402.08808v1 Announce Type: new Abstract: We study depth separation in infinite-width neural networks, where complexity is controlled by the overall squared $\\ell_2$-norm of the weights (sum of squares of all weights in the network). Whereas previous depth separation results focused on separation in terms of width, such results do not give insight into whether depth determines if it is possible to learn a network that generalizes well even when the network width is unbounded. Here, we study separation in terms of the sample complexity required for learnability. Specifically, we show that there are functions that are learnable with sample complexity polynomial in the input dimension by norm-controlled depth-3 ReLU networks, yet are not learnable with sub-exponential sample complexity by norm-controlled depth-2 ReLU networks (with any value for the norm). We also show that a similar statement in the reverse direction is not possible: any function learnable with polynomial sample co",
    "link": "https://arxiv.org/abs/2402.08808",
    "context": "Title: Depth Separation in Norm-Bounded Infinite-Width Neural Networks\nAbstract: arXiv:2402.08808v1 Announce Type: new Abstract: We study depth separation in infinite-width neural networks, where complexity is controlled by the overall squared $\\ell_2$-norm of the weights (sum of squares of all weights in the network). Whereas previous depth separation results focused on separation in terms of width, such results do not give insight into whether depth determines if it is possible to learn a network that generalizes well even when the network width is unbounded. Here, we study separation in terms of the sample complexity required for learnability. Specifically, we show that there are functions that are learnable with sample complexity polynomial in the input dimension by norm-controlled depth-3 ReLU networks, yet are not learnable with sub-exponential sample complexity by norm-controlled depth-2 ReLU networks (with any value for the norm). We also show that a similar statement in the reverse direction is not possible: any function learnable with polynomial sample co",
    "path": "papers/24/02/2402.08808.json",
    "total_tokens": 886,
    "translated_title": "在规范有界的无限宽度神经网络中的深度分隔问题",
    "translated_abstract": "我们研究了在无限宽度神经网络中的深度分隔问题，其中复杂性由权重的整体二次$\\ell_2$范数控制（网络中所有权重的平方和）。之前的深度分隔结果主要关注宽度方面的分隔，这些结果无法说明深度是否决定了在宽度无限制的情况下能否学习出适用于广义上的好泛化性能的网络。在这里，我们研究学习可行性的样本复杂度方面的分隔。具体来说，我们表明有些函数可以通过控制范数的深度3 ReLU网络以多项式复杂度的样本量进行学习，但不能通过控制范数的深度2 ReLU网络（任何范数值）以亚指数复杂度进行学习。同时我们还表明类似的逆向说法是不可能成立的：任何可以通过多项式样本复杂度进行学习的函数，并不能通过亚指数样本复杂度进行学习。",
    "tldr": "本研究探讨了在无限宽度神经网络中的深度分隔问题，并发现在特定条件下，用深度为3的ReLU网络学习比用深度为2的ReLU网络学习要更高效。"
}