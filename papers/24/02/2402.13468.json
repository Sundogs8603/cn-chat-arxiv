{
    "title": "STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning",
    "abstract": "arXiv:2402.13468v1 Announce Type: cross  Abstract: As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10\\%-24\\%$ and rare-class F-1 score by $17\\%-40\\%$ on multiple text classification datasets over commo",
    "link": "https://arxiv.org/abs/2402.13468",
    "context": "Title: STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning\nAbstract: arXiv:2402.13468v1 Announce Type: cross  Abstract: As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10\\%-24\\%$ and rare-class F-1 score by $17\\%-40\\%$ on multiple text classification datasets over commo",
    "path": "papers/24/02/2402.13468.json",
    "total_tokens": 848,
    "translated_title": "STENCIL：基于次模互信息的冷启动主动学习弱监督",
    "translated_abstract": "随着在NLP应用中对预训练模型进行监督微调越来越受欢迎，需要更大量的标注数据，特别是在大型语言模型的参数计数增加时。主动学习试图挖掘和注释未标记的实例以最大限度地快速改善模型性能，是减少注释成本的常见选择；然而，大多数方法通常忽视类别不平衡，并且要么假设可以访问初始标注数据，要么要求改进稀有类之前需要多轮主动学习选择。我们提出了STENCIL，它利用一组文本示例和最近提出的次模互信息来选择一组弱标记的稀有类实例，然后由标注者对其进行强标记。我们展示了STENCIL在多个文本分类数据集上将整体准确率提高了10%-24%，将稀有类F-1分数提高了17%-40%。",
    "tldr": "STENCIL利用次模互信息选择弱标记的稀有类实例，并通过标注者强标记，提高了文本分类数据集上的准确率和稀有类F-1分数。",
    "en_tdlr": "STENCIL uses submodular mutual information to select weakly labeled rare-class instances, which are then strongly labeled by annotators, improving accuracy and rare-class F-1 scores on text classification datasets."
}