{
    "title": "GenLens: A Systematic Evaluation of Visual GenAI Model Outputs",
    "abstract": "The rapid development of generative AI (GenAI) models in computer vision necessitates effective evaluation methods to ensure their quality and fairness. Existing tools primarily focus on dataset quality assurance and model explainability, leaving a significant gap in GenAI output evaluation during model development. Current practices often depend on developers' subjective visual assessments, which may lack scalability and generalizability. This paper bridges this gap by conducting a formative study with GenAI model developers in an industrial setting. Our findings led to the development of GenLens, a visual analytic interface designed for the systematic evaluation of GenAI model outputs during the early stages of model development. GenLens offers a quantifiable approach for overviewing and annotating failure cases, customizing issue tags and classifications, and aggregating annotations from multiple users to enhance collaboration. A user study with model developers reveals that GenLens",
    "link": "https://arxiv.org/abs/2402.03700",
    "context": "Title: GenLens: A Systematic Evaluation of Visual GenAI Model Outputs\nAbstract: The rapid development of generative AI (GenAI) models in computer vision necessitates effective evaluation methods to ensure their quality and fairness. Existing tools primarily focus on dataset quality assurance and model explainability, leaving a significant gap in GenAI output evaluation during model development. Current practices often depend on developers' subjective visual assessments, which may lack scalability and generalizability. This paper bridges this gap by conducting a formative study with GenAI model developers in an industrial setting. Our findings led to the development of GenLens, a visual analytic interface designed for the systematic evaluation of GenAI model outputs during the early stages of model development. GenLens offers a quantifiable approach for overviewing and annotating failure cases, customizing issue tags and classifications, and aggregating annotations from multiple users to enhance collaboration. A user study with model developers reveals that GenLens",
    "path": "papers/24/02/2402.03700.json",
    "total_tokens": 985,
    "translated_title": "GenLens:一种对视觉GenAI模型输出进行系统评估的方法",
    "translated_abstract": "计算机视觉领域生成式AI（GenAI）模型的快速发展需要有效的评估方法来确保其质量和公平性。现有工具主要关注数据集质量保证和模型可解释性，在模型开发期间在GenAI输出评估方面存在重大差距。当前的做法往往依赖于开发人员的主观视觉评估，可能缺乏可伸缩性和普适性。本文通过在工业环境中与GenAI模型开发者进行形成研究来填补这一差距。我们的研究结果促使我们设计了GenLens，这是一个专为在模型开发的早期阶段对GenAI模型输出进行系统评估而设计的视觉分析界面。GenLens提供了一种可量化的方法来查看和注释失败案例，定制问题标签和分类，并从多个用户聚合注释以增强协作。与模型开发人员进行的用户研究显示GenLens能够提高评估GenAI模型输出的准确性和效率。",
    "tldr": "这个论文介绍了一种名为GenLens的系统，该系统旨在对GenAI模型的输出进行系统评估。作者进行了一项形成性研究，发现当前的评估方法存在缺陷，并开发了GenLens作为解决方案。GenLens提供了一种可量化的方法来查看和注释GenAI模型输出中的失败案例，并能够定制问题标签和分类。用户研究表明，GenLens能够提高评估准确性和效率。",
    "en_tdlr": "This paper introduces GenLens, a system for systematically evaluating the outputs of GenAI models. The authors conducted a formative study and identified the shortcomings of current evaluation methods, leading to the development of GenLens as a solution. GenLens offers a quantifiable approach for reviewing and annotating failure cases in GenAI model outputs, as well as customizable issue tags and classifications. User studies demonstrate that GenLens improves evaluation accuracy and efficiency."
}