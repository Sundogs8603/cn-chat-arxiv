{
    "title": "DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching",
    "abstract": "In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and traje",
    "link": "https://arxiv.org/abs/2402.02439",
    "context": "Title: DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based Trajectory Stitching\nAbstract: In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and traje",
    "path": "papers/24/02/2402.02439.json",
    "total_tokens": 941,
    "translated_title": "DiffStitch: 使用基于扩散的轨迹拼接提升离线强化学习",
    "translated_abstract": "在离线强化学习中，学习策略的性能高度依赖于离线数据集的质量。然而，在许多情况下，离线数据集只包含了非常有限的最佳轨迹，这给离线强化学习算法带来了挑战，因为智能体必须获得到达高奖励区域的能力。为了解决这个问题，我们引入了基于扩散的轨迹拼接（DiffStitch），这是一个新颖的基于扩散的数据增强流水线，它可以系统地生成轨迹之间的拼接转换。DiffStitch可以有效地连接低奖励轨迹和高奖励轨迹，形成全局最优轨迹，以解决离线强化学习算法所面临的挑战。在D4RL数据集上进行的实证实验表明，DiffStitch在各种强化学习方法中都具有有效性。值得注意的是，DiffStitch在一步方法（IQL）、模仿学习方法（TD3+BC）和轨迹方法（PPO）的性能方面都有显著的改进。",
    "tldr": "DiffStitch是一种使用基于扩散的轨迹拼接提升离线强化学习的方法。它通过有效地连接低奖励轨迹和高奖励轨迹，形成全局最优轨迹，以提高离线强化学习算法的性能。",
    "en_tdlr": "DiffStitch is a method that boosts offline reinforcement learning using diffusion-based trajectory stitching. It effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to improve the performance of offline RL algorithms."
}