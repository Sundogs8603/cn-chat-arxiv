{
    "title": "Neural Policy Style Transfer",
    "abstract": "Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories. We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure. Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content. The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter. Different policies are defined via Deep Q-Network architectures. These models are trained using demonstrations through Inverse Reinforcement Learning. Two different sets of user demonstrations are performed, one for content and other for style. Different styles are encoded as defined by user demonstrations. The generated policy is the result of feeding a content poli",
    "link": "https://arxiv.org/abs/2402.00677",
    "context": "Title: Neural Policy Style Transfer\nAbstract: Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories. We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure. Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content. The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter. Different policies are defined via Deep Q-Network architectures. These models are trained using demonstrations through Inverse Reinforcement Learning. Two different sets of user demonstrations are performed, one for content and other for style. Different styles are encoded as defined by user demonstrations. The generated policy is the result of feeding a content poli",
    "path": "papers/24/02/2402.00677.json",
    "total_tokens": 956,
    "translated_title": "神经策略风格转换",
    "translated_abstract": "风格转换已经在许多领域中被提出：美术、自然语言处理和固定轨迹。本文将这个概念扩展到了深度强化学习架构中的控制策略。每个网络都被训练成最大化预期的奖励，通常编码了一种行为的目标，可以描述为内容。深度神经网络的表达能力使得可以编码第二个任务，可以描述为风格。提出了神经策略风格转换（NPST）算法，用于将一个策略的风格转移到另一个策略，同时保持后者的内容。通过深度 Q-Network 架构定义了不同的策略。使用逆强化学习通过演示进行模型训练。进行了两组不同的用户演示，一组为内容，另一组为风格。不同的风格通过用户演示进行编码。生成的策略是通过将内容策略输送到一个生成器中来得到的。",
    "tldr": "本研究提出了神经策略风格转换算法，通过深度强化学习来实现控制策略的风格转换。通过训练不同的网络来最大化预期奖励，同时编码了行为的目标和风格，从而将一个策略的风格转移到另一个策略而保持其内容不变。通过逆强化学习和用户演示实现模型的训练和风格的编码。",
    "en_tdlr": "This study proposes a Neural Policy Style Transfer algorithm, which uses deep reinforcement learning to transfer the style of one policy to another while preserving the content. By training different networks to maximize expected rewards and encode both the goal and style of actions, the algorithm achieves style transfer in a deep reinforcement learning infrastructure. The models are trained using inverse reinforcement learning and user demonstrations to encode different styles."
}