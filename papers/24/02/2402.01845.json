{
    "title": "Multi-Armed Bandits with Interference",
    "abstract": "Experimentation with interference poses a significant challenge in contemporary online platforms. Prior research on experimentation with interference has concentrated on the final output of a policy. The cumulative performance, while equally crucial, is less well understood. To address this gap, we introduce the problem of {\\em Multi-armed Bandits with Interference} (MABI), where the learner assigns an arm to each of $N$ experimental units over a time horizon of $T$ rounds. The reward of each unit in each round depends on the treatments of {\\em all} units, where the influence of a unit decays in the spatial distance between units. Furthermore, we employ a general setup wherein the reward functions are chosen by an adversary and may vary arbitrarily across rounds and units. We first show that switchback policies achieve an optimal {\\em expected} regret $\\tilde O(\\sqrt T)$ against the best fixed-arm policy. Nonetheless, the regret (as a random variable) for any switchback policy suffers ",
    "link": "https://arxiv.org/abs/2402.01845",
    "context": "Title: Multi-Armed Bandits with Interference\nAbstract: Experimentation with interference poses a significant challenge in contemporary online platforms. Prior research on experimentation with interference has concentrated on the final output of a policy. The cumulative performance, while equally crucial, is less well understood. To address this gap, we introduce the problem of {\\em Multi-armed Bandits with Interference} (MABI), where the learner assigns an arm to each of $N$ experimental units over a time horizon of $T$ rounds. The reward of each unit in each round depends on the treatments of {\\em all} units, where the influence of a unit decays in the spatial distance between units. Furthermore, we employ a general setup wherein the reward functions are chosen by an adversary and may vary arbitrarily across rounds and units. We first show that switchback policies achieve an optimal {\\em expected} regret $\\tilde O(\\sqrt T)$ against the best fixed-arm policy. Nonetheless, the regret (as a random variable) for any switchback policy suffers ",
    "path": "papers/24/02/2402.01845.json",
    "total_tokens": 1023,
    "translated_title": "具有干扰的多臂赌博机问题",
    "translated_abstract": "在当代在线平台上，与干扰进行实验是一个重大挑战。以往有关干扰实验的研究集中在政策的最终输出上，而对于累计性能则了解不足。为了填补这一空白，我们引入了“具有干扰的多臂赌博机”（MABI）问题，在时间段为T轮的情况下，学习者为N个实验单元中的每个分配一个臂。每个单元在每一轮的回报取决于“所有”单元的治疗方式，而单元之间的空间距离会导致单元的影响力逐渐衰减。此外，我们使用了一个通用设置，其中回报函数由对手选择，并且在轮次和单元之间可以任意变化。我们首先证明了转换政策能够对最佳固定臂政策实现最优的“预期”遗憾，遗憾值为$O(\\sqrt T)$。然而，任何一个转换政策的遗憾（作为一个随机变量）都会遭受一定的遗憾现象。",
    "tldr": "这篇论文研究了在在线平台中与干扰进行的实验。在多臂赌博机问题中，学习者分配不同的臂给每个实验单元，根据单元之间的空间距离和对手选择的匹配函数来决定每个单元在每轮的回报。研究发现，转换政策能够实现最佳的预期遗憾，但任何转换政策都会遭受一定的遗憾现象。",
    "en_tdlr": "This paper investigates experimentation with interference in contemporary online platforms. In the problem of multi-armed bandits with interference, the learner assigns arms to each experimental unit, and the reward of each unit in each round depends on the spatial distance between units and the adversary's choice of reward functions. It is found that switchback policies achieve optimal expected regret, but suffer from regret as a random variable."
}