{
    "title": "Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration",
    "abstract": "Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the s",
    "link": "https://arxiv.org/abs/2402.00367",
    "context": "Title: Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration\nAbstract: Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the s",
    "path": "papers/24/02/2402.00367.json",
    "total_tokens": 946,
    "translated_title": "不要幻觉，持观：通过多LLM协作识别LLM知识盲区",
    "translated_abstract": "尽管存在扩展大型语言模型（LLM）知识的努力，但由于知识的不断演化，LLM知识盲区——LLM中缺失或过时的信息可能会一直存在。在这项工作中，我们研究了识别LLM知识盲区和在存在知识盲区时放弃回答问题的方法。我们首先通过模型校准或适应的现有方法进行改进，并分析它们在避免生成低置信度输出方面的能力。受到它们在自我反思和过度依赖保留集方面的失败的启发，我们提出了两种基于模型协作的新方法，即LLM探测其他LLM的知识盲区，无论是合作还是竞争。通过在四个包含多样知识领域的问答任务上对三个LLM进行广泛实验，我们证明揭示LLM知识盲区的合作和竞争方法在放弃准确度方面取得了高达19.3％的提高。",
    "tldr": "本论文研究了识别大型语言模型（LLM）知识盲区的方法，并提出了两种基于LLM协作的新方法，通过这些方法可以在面对知识盲区时放弃回答问题。实验证明，这些方法在提高放弃准确度方面取得了高达19.3％的改进。"
}