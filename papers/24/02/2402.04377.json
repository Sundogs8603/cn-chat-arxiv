{
    "title": "$\\texttt{NeRCC}$: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems",
    "abstract": "Resilience against stragglers is a critical element of prediction serving systems, tasked with executing inferences on input data for a pre-trained machine-learning model. In this paper, we propose NeRCC, as a general straggler-resistant framework for approximate coded computing. NeRCC includes three layers: (1) encoding regression and sampling, which generates coded data points, as a combination of original data points, (2) computing, in which a cluster of workers run inference on the coded data points, (3) decoding regression and sampling, which approximately recovers the predictions of the original data points from the available predictions on the coded data points. We argue that the overall objective of the framework reveals an underlying interconnection between two regression models in the encoding and decoding layers. We propose a solution to the nested regressions problem by summarizing their dependence on two regularization terms that are jointly optimized. Our extensive experi",
    "link": "https://arxiv.org/abs/2402.04377",
    "context": "Title: $\\texttt{NeRCC}$: Nested-Regression Coded Computing for Resilient Distributed Prediction Serving Systems\nAbstract: Resilience against stragglers is a critical element of prediction serving systems, tasked with executing inferences on input data for a pre-trained machine-learning model. In this paper, we propose NeRCC, as a general straggler-resistant framework for approximate coded computing. NeRCC includes three layers: (1) encoding regression and sampling, which generates coded data points, as a combination of original data points, (2) computing, in which a cluster of workers run inference on the coded data points, (3) decoding regression and sampling, which approximately recovers the predictions of the original data points from the available predictions on the coded data points. We argue that the overall objective of the framework reveals an underlying interconnection between two regression models in the encoding and decoding layers. We propose a solution to the nested regressions problem by summarizing their dependence on two regularization terms that are jointly optimized. Our extensive experi",
    "path": "papers/24/02/2402.04377.json",
    "total_tokens": 892,
    "translated_title": "$\\texttt{NeRCC}$: 内嵌回归编码计算用于具有弹性的分布式预测服务系统",
    "translated_abstract": "对抗拖尾节点(stragglers)是预测服务系统的一个重要特征，任务是在预先训练的机器学习模型上执行输入数据的推理。在本文中，我们提出了一种名为NeRCC的通用的抗拖尾节点的近似编码计算框架。NeRCC包括三个层次：(1)回归编码和抽样，生成编码数据点，作为原始数据点的组合；(2)计算，其中一个工作集群在编码数据点上运行推理；(3)回归解码和抽样，从编码数据点的可用预测中近似恢复出原始数据点的预测结果。我们认为框架的总体目标揭示了编码和解码层中两个回归模型之间的潜在相互关系。我们提出了一个解决嵌套回归问题的方法，通过总结它们对两个联合优化的正则化项的依赖关系。",
    "tldr": "NeRCC是一个通用的抗拖尾节点的近似编码计算框架，包括回归编码、计算和回归解码三个层次，通过优化两个正则化项的依赖关系来解决嵌套回归问题。"
}