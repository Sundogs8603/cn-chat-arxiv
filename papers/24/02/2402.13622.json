{
    "title": "Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression",
    "abstract": "arXiv:2402.13622v1 Announce Type: cross  Abstract: We investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional supervised regression tasks. We provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and logistic regression, taking the limit where the number of samples $n$ and dimension $d$ of the covariates grow at a comparable fixed rate $\\alpha\\!=\\! n/d$. Our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when $\\alpha$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime $\\alpha\\!<\\!1$ relevant to modern machine learning practice, their predictions are",
    "link": "https://arxiv.org/abs/2402.13622",
    "context": "Title: Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression\nAbstract: arXiv:2402.13622v1 Announce Type: cross  Abstract: We investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional supervised regression tasks. We provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and logistic regression, taking the limit where the number of samples $n$ and dimension $d$ of the covariates grow at a comparable fixed rate $\\alpha\\!=\\! n/d$. Our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when $\\alpha$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime $\\alpha\\!<\\!1$ relevant to modern machine learning practice, their predictions are",
    "path": "papers/24/02/2402.13622.json",
    "total_tokens": 911,
    "translated_title": "在高维正则化回归中对自举和子抽样的分析",
    "translated_abstract": "我们研究了用于估计统计模型不确定性的流行重抽样方法，如子抽样、自举和jackknife，以及它们在高维监督回归任务中的性能。在广义线性模型的情境下，例如岭回归和逻辑回归，我们对这些方法估计的偏差和方差提供了紧致的渐近描述，考虑到样本数量$n$和协变量维度$d$以可比固定速率$\\alpha\\!=\\! n/d$增长的极限情况。我们的发现有三个方面：i）在高维情况下，重抽样方法存在问题，并表现出这些情况典型的双峰行为；ii）只有在$\\alpha$足够大时，它们才提供一致可靠的误差估计（我们给出收敛率）；iii）在现代机器学习实践中相关的超参数化区域$\\alpha\\!<\\!1$，它们的预测是",
    "tldr": "重要发现包括高维情况下重抽样方法的问题，仅当$\\alpha$足够大时提供一致可靠的误差估计，以及在超参数化区域$\\alpha\\!<\\!1$的情况下它们的预测表现",
    "en_tdlr": "Key findings include the issues with resampling methods in high dimensions, the consistent and reliable error estimations only when $\\alpha$ is large enough, and their performance in the over-parametrized regime $\\alpha\\!<\\!1$ relevant to modern machine learning practice."
}