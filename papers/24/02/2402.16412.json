{
    "title": "TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis",
    "abstract": "arXiv:2402.16412v1 Announce Type: new  Abstract: The field of general time series analysis has recently begun to explore unified modeling, where a common architectural backbone can be retrained on a specific task for a specific dataset. In this work, we approach unification from a complementary vantage point: unification across tasks and domains. To this end, we explore the impact of discrete, learnt, time series data representations that enable generalist, cross-domain training. Our method, TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a self-supervised manner. TOTEM works across multiple tasks and domains with minimal to no tuning. We study the efficacy of TOTEM with an extensive evaluation on 17 real world time series datasets across 3 tasks. We evaluate both the specialist (i.e., training a model on each domain) and generalist (i.e., trainin",
    "link": "https://arxiv.org/abs/2402.16412",
    "context": "Title: TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis\nAbstract: arXiv:2402.16412v1 Announce Type: new  Abstract: The field of general time series analysis has recently begun to explore unified modeling, where a common architectural backbone can be retrained on a specific task for a specific dataset. In this work, we approach unification from a complementary vantage point: unification across tasks and domains. To this end, we explore the impact of discrete, learnt, time series data representations that enable generalist, cross-domain training. Our method, TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a self-supervised manner. TOTEM works across multiple tasks and domains with minimal to no tuning. We study the efficacy of TOTEM with an extensive evaluation on 17 real world time series datasets across 3 tasks. We evaluate both the specialist (i.e., training a model on each domain) and generalist (i.e., trainin",
    "path": "papers/24/02/2402.16412.json",
    "total_tokens": 921,
    "translated_title": "TOTEM：用于一般时间序列分析的令牌化时间序列嵌入",
    "translated_abstract": "最近，一般时间序列分析领域开始探索统一建模，其中一个通用的架构可以在特定任务和特定数据集上进行重新训练。本文从一个互补的角度接近统一化：跨任务和领域的统一化。为此，我们探讨了离散、学习到的时间序列数据表示对启用通用、跨领域训练的影响。我们的方法TOTEM，即TOkenized Time Series EMbeddings，提出了一种简单的标记器架构，通过以自监督方式学习的离散矢量化表示嵌入来自不同领域的时间序列数据。TOTEM可以跨多个任务和领域工作，几乎不需要调整。我们通过对3个任务上的17个真实世界时间序列数据集进行广泛评估来研究TOTEM的有效性。我们评估了专家（即在每个领域训练模型）和通用（即训练）的TOTEM。",
    "tldr": "TOTEM提出了一种简单的令牌化架构，通过自监督学习的离散矢量化表示嵌入不同领域的时间序列数据，能够实现通用、跨领域训练，在多个任务和领域上进行广泛评估。",
    "en_tdlr": "TOTEM proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a self-supervised manner, enabling generalist, cross-domain training and extensive evaluation across multiple tasks and domains."
}