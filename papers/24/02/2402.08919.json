{
    "title": "Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding",
    "abstract": "arXiv:2402.08919v1 Announce Type: cross Abstract: Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-finders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to define and compute a notion of \"conceptual similarity\" among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base multi-modal model to generate \"explanations\" (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two h",
    "link": "https://arxiv.org/abs/2402.08919",
    "context": "Title: Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding\nAbstract: arXiv:2402.08919v1 Announce Type: cross Abstract: Quantifying the degree of similarity between images is a key copyright issue for image-based machine learning. In legal doctrine however, determining the degree of similarity between works requires subjective analysis, and fact-finders (judges and juries) can demonstrate considerable variability in these subjective judgement calls. Images that are structurally similar can be deemed dissimilar, whereas images of completely different scenes can be deemed similar enough to support a claim of copying. We seek to define and compute a notion of \"conceptual similarity\" among images that captures high-level relations even among images that do not share repeated elements or visually similar components. The idea is to use a base multi-modal model to generate \"explanations\" (captions) of visual data at increasing levels of complexity. Then, similarity can be measured by the length of the caption needed to discriminate between the two images: Two h",
    "path": "papers/24/02/2402.08919.json",
    "total_tokens": 867,
    "translated_title": "可解释的限制复杂性描绘自动编码的概念相似度度量",
    "translated_abstract": "对于基于图像的机器学习而言，衡量图像之间相似度的程度是一个关键的版权问题。然而，在法律原则中，确定作品之间的相似度需要主观分析，事实裁决者（法官和陪审团）在这些主观判断中可能存在相当大的变异性。在结构上相似的图像可能被认为是不相似的，而完全不同的场景图像可能被认为足够相似以支持剽窃的指控。我们希望定义和计算图像之间的“概念相似度”，即使这些图像没有重复元素或视觉相似组件也能捕捉到高层次的关系。这个想法是使用一个基本的多模态模型生成对视觉数据的“解释”（标题），并在逐渐增加的复杂性水平上进行计算。然后，可以通过需要区分这两个图像的标题长度来衡量相似度。",
    "tldr": "这篇论文提出了一种可解释的限制复杂性描绘自动编码方法，用于衡量图像之间的概念相似度，以便解决基于图像的机器学习中的版权问题。",
    "en_tdlr": "This paper proposes an interpretable complexity-constrained descriptive auto-encoding method to quantify conceptual similarity between images, aiming to address copyright issues in image-based machine learning."
}