{
    "title": "Analyzing Adversarial Inputs in Deep Reinforcement Learning",
    "abstract": "In recent years, Deep Reinforcement Learning (DRL) has become a popular paradigm in machine learning due to its successful applications to real-world and complex systems. However, even the state-of-the-art DRL models have been shown to suffer from reliability concerns -- for example, their susceptibility to adversarial inputs, i.e., small and abundant input perturbations that can fool the models into making unpredictable and potentially dangerous decisions. This drawback limits the deployment of DRL systems in safety-critical contexts, where even a small error cannot be tolerated. In this work, we present a comprehensive analysis of the characterization of adversarial inputs, through the lens of formal verification. Specifically, we introduce a novel metric, the Adversarial Rate, to classify models based on their susceptibility to such perturbations, and present a set of tools and algorithms for its computation. Our analysis empirically demonstrates how adversarial inputs can affect th",
    "link": "https://arxiv.org/abs/2402.05284",
    "context": "Title: Analyzing Adversarial Inputs in Deep Reinforcement Learning\nAbstract: In recent years, Deep Reinforcement Learning (DRL) has become a popular paradigm in machine learning due to its successful applications to real-world and complex systems. However, even the state-of-the-art DRL models have been shown to suffer from reliability concerns -- for example, their susceptibility to adversarial inputs, i.e., small and abundant input perturbations that can fool the models into making unpredictable and potentially dangerous decisions. This drawback limits the deployment of DRL systems in safety-critical contexts, where even a small error cannot be tolerated. In this work, we present a comprehensive analysis of the characterization of adversarial inputs, through the lens of formal verification. Specifically, we introduce a novel metric, the Adversarial Rate, to classify models based on their susceptibility to such perturbations, and present a set of tools and algorithms for its computation. Our analysis empirically demonstrates how adversarial inputs can affect th",
    "path": "papers/24/02/2402.05284.json",
    "total_tokens": 837,
    "translated_title": "分析深度强化学习中的对抗输入",
    "translated_abstract": "近年来，深度强化学习（DRL）由于在实际和复杂系统中取得的成功应用而成为机器学习中受欢迎的范例。然而，即使最先进的DRL模型也被证明存在可靠性问题，例如对抗输入的敏感性，即小型且大量的输入扰动会导致模型做出不可预测且潜在危险的决策。这个缺点限制了DRL系统在安全关键环境中的部署，即使是小的错误都是不可容忍的。在这项工作中，我们通过形式验证的视角提出了对对抗输入进行分类的新度量标准——对抗率，并提出了一套用于计算对抗率的工具和算法。我们的分析通过实验证明了对抗输入对DRL模型的影响。",
    "tldr": "这篇论文通过形式验证的视角，分析了深度强化学习中对抗输入的特征，并提出了一个新的度量标准——对抗率，以及计算该度量标准的一套工具和算法。",
    "en_tdlr": "This paper provides a comprehensive analysis of adversarial inputs in deep reinforcement learning through the lens of formal verification, introducing a novel metric, the Adversarial Rate, for classifying models and presenting a set of tools and algorithms for its computation. The empirical analysis demonstrates the impact of adversarial inputs on DRL models."
}