{
    "title": "Dependable Distributed Training of Compressed Machine Learning Models",
    "abstract": "arXiv:2402.14346v1 Announce Type: cross  Abstract: The existing work on the distributed training of machine learning (ML) models has consistently overlooked the distribution of the achieved learning quality, focusing instead on its average value. This leads to a poor dependability}of the resulting ML models, whose performance may be much worse than expected. We fill this gap by proposing DepL, a framework for dependable learning orchestration, able to make high-quality, efficient decisions on (i) the data to leverage for learning, (ii) the models to use and when to switch among them, and (iii) the clusters of nodes, and the resources thereof, to exploit. For concreteness, we consider as possible available models a full DNN and its compressed versions. Unlike previous studies, DepL guarantees that a target learning quality is reached with a target probability, while keeping the training cost at a minimum. We prove that DepL has constant competitive ratio and polynomial complexity, and s",
    "link": "https://arxiv.org/abs/2402.14346",
    "context": "Title: Dependable Distributed Training of Compressed Machine Learning Models\nAbstract: arXiv:2402.14346v1 Announce Type: cross  Abstract: The existing work on the distributed training of machine learning (ML) models has consistently overlooked the distribution of the achieved learning quality, focusing instead on its average value. This leads to a poor dependability}of the resulting ML models, whose performance may be much worse than expected. We fill this gap by proposing DepL, a framework for dependable learning orchestration, able to make high-quality, efficient decisions on (i) the data to leverage for learning, (ii) the models to use and when to switch among them, and (iii) the clusters of nodes, and the resources thereof, to exploit. For concreteness, we consider as possible available models a full DNN and its compressed versions. Unlike previous studies, DepL guarantees that a target learning quality is reached with a target probability, while keeping the training cost at a minimum. We prove that DepL has constant competitive ratio and polynomial complexity, and s",
    "path": "papers/24/02/2402.14346.json",
    "total_tokens": 831,
    "translated_title": "可靠的分布式压缩机器学习模型训练",
    "translated_abstract": "有关机器学习（ML）模型分布式训练的现有工作一直忽视了实现学习质量的分布，而是专注于其平均值。 这导致了所得ML模型的可靠性差，其性能可能比预期的要差得多。 我们通过提出DepL来填补这一空白，这是一个可靠的学习编排框架，能够就（i）用于学习的数据，（ii）要使用的模型及何时在它们之间切换，以及（iii）要利用的节点集群及其资源做出高质量高效的决策。 具体而言，我们考虑可能的可用模型为完整的DNN及其压缩版本。 与以前的研究不同，DepL保证以目标概率实现目标学习质量，同时保持训练成本最低。 我们证明DepL具有常数竞争比率和多项式复杂度。",
    "tldr": "提出了DepL框架，实现了可靠的学习编排，能够确保以最低训练成本达到目标学习质量。",
    "en_tdlr": "Introduced DepL framework for dependable learning orchestration, ensuring target learning quality with minimum training cost."
}