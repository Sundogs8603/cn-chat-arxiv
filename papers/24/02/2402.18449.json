{
    "title": "HOP to the Next Tasks and Domains for Continual Learning in NLP",
    "abstract": "arXiv:2402.18449v1 Announce Type: cross  Abstract: Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones. Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework. Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem. Extensive experimental campaign on 4 NLP applications, 5 benchmarks and ",
    "link": "https://arxiv.org/abs/2402.18449",
    "context": "Title: HOP to the Next Tasks and Domains for Continual Learning in NLP\nAbstract: arXiv:2402.18449v1 Announce Type: cross  Abstract: Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and domains) by transferring knowledge acquired on previous problems, whilst avoiding forgetting of past ones. Different from previous approaches which focused on CL for one NLP task or domain in a specific use-case, in this paper, we address a more general CL setting to learn from a sequence of problems in a unique framework. Our method, HOP, permits to hop across tasks and domains by addressing the CL problem along three directions: (i) we employ a set of adapters to generalize a large pre-trained model to unseen problems, (ii) we compute high-order moments over the distribution of embedded representations to distinguish independent and correlated statistics across different tasks and domains, (iii) we process this enriched information with auxiliary heads specialized for each end problem. Extensive experimental campaign on 4 NLP applications, 5 benchmarks and ",
    "path": "papers/24/02/2402.18449.json",
    "total_tokens": 782,
    "translated_title": "HOP到自然语言处理中连续学习的下一个任务和领域",
    "translated_abstract": "连续学习（CL）旨在通过转移先前问题中获得的知识来学习一系列问题（即任务和领域），同时避免遗忘过去的问题。与先前专注于特定用例中一个NLP任务或领域的CL方法不同，本文针对一个更通用的CL设置，从一个唯一的框架中学习一系列问题。我们的方法HOP通过沿三个方向解决CL问题来允许在任务和领域之间跳跃：（i）我们使用一组适配器将大型预训练模型推广到未见问题，（ii）我们计算嵌入表示分布上的高阶矩以区分不同任务和领域之间的独立和相关统计数据，（iii）我们通过为每个最终问题专门设计的辅助头处理这些丰富信息。我们在4个NLP应用程序，5个基准测试和...",
    "tldr": "该方法HOP在连续学习中引入了三个方向以在自然语言处理中跨任务和领域进行学习。"
}