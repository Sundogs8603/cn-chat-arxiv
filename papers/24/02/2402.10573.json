{
    "title": "LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty",
    "abstract": "arXiv:2402.10573v1 Announce Type: new  Abstract: Named Entity Recognition (NER) serves as a fundamental task in natural language understanding, bearing direct implications for web content analysis, search engines, and information retrieval systems. Fine-tuned NER models exhibit satisfactory performance on standard NER benchmarks. However, due to limited fine-tuning data and lack of knowledge, it performs poorly on unseen entity recognition. As a result, the usability and reliability of NER models in web-related applications are compromised. Instead, Large Language Models (LLMs) like GPT-4 possess extensive external knowledge, but research indicates that they lack specialty for NER tasks. Furthermore, non-public and large-scale weights make tuning LLMs difficult. To address these challenges, we propose a framework that combines small fine-tuned models with LLMs (LinkNER) and an uncertainty-based linking strategy called RDC that enables fine-tuned models to complement black-box LLMs, ach",
    "link": "https://arxiv.org/abs/2402.10573",
    "context": "Title: LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty\nAbstract: arXiv:2402.10573v1 Announce Type: new  Abstract: Named Entity Recognition (NER) serves as a fundamental task in natural language understanding, bearing direct implications for web content analysis, search engines, and information retrieval systems. Fine-tuned NER models exhibit satisfactory performance on standard NER benchmarks. However, due to limited fine-tuning data and lack of knowledge, it performs poorly on unseen entity recognition. As a result, the usability and reliability of NER models in web-related applications are compromised. Instead, Large Language Models (LLMs) like GPT-4 possess extensive external knowledge, but research indicates that they lack specialty for NER tasks. Furthermore, non-public and large-scale weights make tuning LLMs difficult. To address these challenges, we propose a framework that combines small fine-tuned models with LLMs (LinkNER) and an uncertainty-based linking strategy called RDC that enables fine-tuned models to complement black-box LLMs, ach",
    "path": "papers/24/02/2402.10573.json",
    "total_tokens": 877,
    "translated_title": "LinkNER: 使用不确定性将本地命名实体识别模型与大语言模型进行链接",
    "translated_abstract": "命名实体识别（NER）作为自然语言理解中的基本任务，直接影响着网络内容分析、搜索引擎和信息检索系统。微调后的NER模型在标准NER基准上表现出令人满意的性能。然而，由于有限的微调数据和缺乏知识，它在未见实体识别上表现不佳。因此，NER模型在网络相关应用中的可用性和可靠性受到影响。相反，像GPT-4这样的大型语言模型（LLM）具有丰富的外部知识，但研究表明它们缺乏NER任务的专业性。此外，私有和大规模权重使LLM的调整困难。为了解决这些挑战，我们提出了一个框架，结合了小型微调模型和LLMs（LinkNER），以及一种基于不确定性的链接策略RDC，使微调模型能够补充黑盒LLMs。",
    "tldr": "提出了一种结合小型微调模型和大型语言模型的LinkNER框架，通过不确定性的链接策略RDC，使微调模型能够补充黑盒LLMs",
    "en_tdlr": "Proposed a framework, LinkNER, that combines small fine-tuned models with Large Language Models (LLMs) and an uncertainty-based linking strategy called RDC to enable fine-tuned models to complement black-box LLMs."
}