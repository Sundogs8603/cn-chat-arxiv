{
    "title": "Humanoid Locomotion as Next Token Prediction",
    "abstract": "arXiv:2402.19469v1 Announce Type: cross  Abstract: We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backwar",
    "link": "https://arxiv.org/abs/2402.19469",
    "context": "Title: Humanoid Locomotion as Next Token Prediction\nAbstract: arXiv:2402.19469v1 Announce Type: cross  Abstract: We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backwar",
    "path": "papers/24/02/2402.19469.json",
    "total_tokens": 907,
    "translated_title": "人形机器人运动作为下一个标记预测",
    "translated_abstract": "我们将现实世界的人形控制解释为一个下一个标记预测问题，类似于预测语言中的下一个单词。我们的模型是一个经过自回归预测传感器轨迹训练的因果变压器。为了考虑数据的多模态性质，我们以模态对齐的方式进行预测，对于每个输入标记，从相同的模态预测下一个标记。这种一般性显示使我们能够利用缺失模态的数据，例如没有动作的视频轨迹。我们在模拟轨迹集合上训练我们的模型，这些轨迹来自先前的神经网络策略、基于模型的控制器、动作捕捉数据和人类的YouTube视频。我们展示了我们的模型使一个全尺寸的人形机器人能够在旧金山行走，甚至没有受到器官。我们的模型可以在只有27小时行走数据的情况下转移到现实世界，并且可以推广到训练时没有看到的命令，比如向后走。",
    "tldr": "该研究将人形控制问题转化为下一个标记预测问题，通过因果变压器训练实现传感器轨迹的自回归预测，可在缺失模态数据下训练，并成功使人形机器人完成步行任务。",
    "en_tdlr": "This study frames humanoid control as a next token prediction problem, trains a causal transformer for autoregressive prediction of sensorimotor trajectories, can train on data with missing modalities, and enables a full-sized humanoid to walk successfully."
}