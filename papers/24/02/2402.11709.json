{
    "title": "GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network",
    "abstract": "arXiv:2402.11709v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-ba",
    "link": "https://arxiv.org/abs/2402.11709",
    "context": "Title: GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network\nAbstract: arXiv:2402.11709v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-ba",
    "path": "papers/24/02/2402.11709.json",
    "total_tokens": 871,
    "translated_title": "GNNavi：通过图神经网络导航大型语言模型中的信息流",
    "translated_abstract": "大型语言模型（LLMs）在接收示范输入时表现出强大的上下文学习能力（ICL）。然而，微调仍然至关重要以进一步增强其适应性。基于提示的微调方法在数据稀缺情况下证明是有效的微调方法，但对计算资源的高需求限制了其实用性。我们通过引入基于提示的参数高效微调（PEFT）方法来解决这个问题。GNNavi利用了有关ICL信息流动态的见解，表明标签词在提示中作为信息传播的锚点。GNNavi利用图神经网络（GNN）层精确地引导信息流的汇聚和分布，在处理提示时将期望的信息流硬编码到GNN中。我们在使用GPT-2和Llama2进行文本分类任务的实验中发现，GNNavi超越了标准提示式微调的性能。",
    "tldr": "GNNavi通过引入基于提示的参数高效微调（PEFT）方法和图神经网络（GNN）层，准确引导信息流的汇聚和分布，在大型语言模型中导航信息流动态，超越了标准提示式微调的性能。",
    "en_tdlr": "GNNavi navigates the information flow dynamics in large language models by introducing a prompt-based parameter-efficient fine-tuning approach (PEFT) and a Graph Neural Network (GNN) layer, surpassing the performance of standard prompt-based fine-tuning."
}