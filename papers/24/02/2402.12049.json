{
    "title": "Reinforcement Learning for Optimal Execution when Liquidity is Time-Varying",
    "abstract": "arXiv:2402.12049v1 Announce Type: new  Abstract: Optimal execution is an important problem faced by any trader. Most solutions are based on the assumption of constant market impact, while liquidity is known to be dynamic. Moreover, models with time-varying liquidity typically assume that it is observable, despite the fact that, in reality, it is latent and hard to measure in real time. In this paper we show that the use of Double Deep Q-learning, a form of Reinforcement Learning based on neural networks, is able to learn optimal trading policies when liquidity is time-varying. Specifically, we consider an Almgren-Chriss framework with temporary and permanent impact parameters following several deterministic and stochastic dynamics. Using extensive numerical experiments, we show that the trained algorithm learns the optimal policy when the analytical solution is available, and overcomes benchmarks and approximated solutions when the solution is not available.",
    "link": "https://arxiv.org/abs/2402.12049",
    "context": "Title: Reinforcement Learning for Optimal Execution when Liquidity is Time-Varying\nAbstract: arXiv:2402.12049v1 Announce Type: new  Abstract: Optimal execution is an important problem faced by any trader. Most solutions are based on the assumption of constant market impact, while liquidity is known to be dynamic. Moreover, models with time-varying liquidity typically assume that it is observable, despite the fact that, in reality, it is latent and hard to measure in real time. In this paper we show that the use of Double Deep Q-learning, a form of Reinforcement Learning based on neural networks, is able to learn optimal trading policies when liquidity is time-varying. Specifically, we consider an Almgren-Chriss framework with temporary and permanent impact parameters following several deterministic and stochastic dynamics. Using extensive numerical experiments, we show that the trained algorithm learns the optimal policy when the analytical solution is available, and overcomes benchmarks and approximated solutions when the solution is not available.",
    "path": "papers/24/02/2402.12049.json",
    "total_tokens": 794,
    "translated_title": "当流动性是时变的时，强化学习用于最优执行问题",
    "translated_abstract": "最优执行是任何交易者面临的重要问题。大多数解决方案基于市场冲击恒定的假设，而流动性已知是动态的。此外，具有时变流动性的模型通常假设是可观察的，尽管实际上它是潜在的且难以实时测量。在本文中，我们展示了基于神经网络的双深度 Q 学习的使用能够在流动性时变时学习到最优交易策略。具体来说，我们考虑了一个具有临时和永久冲击参数的Almgren-Chriss框架，这些参数遵循几种确定性和随机动态。通过大量的数值实验，我们展示了训练算法在解析解可用时学习到最优策略，并且在解决方案不可用时超越了基准和近似解。",
    "tldr": "本研究使用双深度 Q 学习来学习最优交易策略，有效应对流动性时变的问题。",
    "en_tdlr": "This study utilizes Double Deep Q-learning to learn optimal trading policies in the face of time-varying liquidity."
}