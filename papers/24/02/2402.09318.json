{
    "title": "Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio",
    "abstract": "arXiv:2402.09318v1 Announce Type: cross Abstract: We present PECMAE, an interpretable model for music audio classification based on prototype learning. Our model is based on a previous method, APNet, which jointly learns an autoencoder and a prototypical network. Instead, we propose to decouple both training processes. This enables us to leverage existing self-supervised autoencoders pre-trained on much larger data (EnCodecMAE), providing representations with better generalization. APNet allows prototypes' reconstruction to waveforms for interpretability relying on the nearest training data samples. In contrast, we explore using a diffusion decoder that allows reconstruction without such dependency. We evaluate our method on datasets for music instrument classification (Medley-Solos-DB) and genre recognition (GTZAN and a larger in-house dataset), the latter being a more challenging task not addressed with prototypical networks before. We find that the prototype-based models preserve mo",
    "link": "https://arxiv.org/abs/2402.09318",
    "context": "Title: Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio\nAbstract: arXiv:2402.09318v1 Announce Type: cross Abstract: We present PECMAE, an interpretable model for music audio classification based on prototype learning. Our model is based on a previous method, APNet, which jointly learns an autoencoder and a prototypical network. Instead, we propose to decouple both training processes. This enables us to leverage existing self-supervised autoencoders pre-trained on much larger data (EnCodecMAE), providing representations with better generalization. APNet allows prototypes' reconstruction to waveforms for interpretability relying on the nearest training data samples. In contrast, we explore using a diffusion decoder that allows reconstruction without such dependency. We evaluate our method on datasets for music instrument classification (Medley-Solos-DB) and genre recognition (GTZAN and a larger in-house dataset), the latter being a more challenging task not addressed with prototypical networks before. We find that the prototype-based models preserve mo",
    "path": "papers/24/02/2402.09318.json",
    "total_tokens": 940,
    "translated_title": "利用预训练自编码器进行可解释的音乐音频原型学习",
    "translated_abstract": "我们提出了一种基于原型学习的音乐音频分类的可解释模型PECMAE。我们的模型基于之前的方法APNet，该方法联合学习自编码器和原型网络。相反，我们提出将这两个训练过程分离，这样我们就可以利用已在更大数据上预训练的自监督自编码器（EnCodecMAE）提供更好的泛化表示。APNet使得原型可以通过最近的训练数据样本进行波形重构以实现可解释性。相反，我们探索使用扩散解码器，它允许在没有这种依赖的情况下进行重构。我们在音乐乐器分类（Medley-Solos-DB）和流派识别（GTZAN和一个更大的内部数据集）数据集上评估我们的方法，后者是一个以前未使用原型网络解决的更具挑战的任务。我们发现基于原型的模型可以保留更多关于音频特征的信息，从而提高了分类性能。",
    "tldr": "提出了一种基于预训练自编码器的可解释音乐音频原型学习模型PECMAE，并通过使用扩散解码器提升了重构效果。在音乐乐器分类和流派识别数据集上的实验结果表明，该模型能够保留更多关于音频特征的信息，提高分类性能。",
    "en_tdlr": "Introducing PECMAE, an interpretable model for music audio classification based on pre-trained autoencoders and prototype learning. By decoupling the training processes, utilizing a diffusion decoder, and evaluating on challenging datasets, the model preserves more audio feature information and improves classification performance."
}