{
    "title": "$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning",
    "abstract": "arXiv:2402.10150v1 Announce Type: new  Abstract: In self-supervised contrastive learning, a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback-Leibler (KL)-based mutual information. In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides the popular cosine similarity, can we design a better similarity function? We provide answers to both questions by generalizing the KL-based mutual information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL) using the $f$-divergences. To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance. For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel,",
    "link": "https://arxiv.org/abs/2402.10150",
    "context": "Title: $f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning\nAbstract: arXiv:2402.10150v1 Announce Type: new  Abstract: In self-supervised contrastive learning, a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback-Leibler (KL)-based mutual information. In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides the popular cosine similarity, can we design a better similarity function? We provide answers to both questions by generalizing the KL-based mutual information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL) using the $f$-divergences. To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance. For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel,",
    "path": "papers/24/02/2402.10150.json",
    "total_tokens": 942,
    "translated_title": "$f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning",
    "translated_abstract": "在自监督对比学习中，一种被广泛采用的目标函数是InfoNCE，它使用启发式的余弦相似度进行表示比较，并且与最大化基于KL的互信息密切相关。本文旨在回答两个有趣的问题：(1)我们能否超越基于KL的目标？(2)除了流行的余弦相似度，我们能否设计出更好的相似度函数？我们通过将基于KL的互信息推广为$f$-Mutual Information in Contrastive Learning ($f$-MICL)，使用$f$-divergences来回答这两个问题。针对第一个问题，我们提供了一系列$f$-MICL目标函数，它们具有与InfoNCE相似的良好特性（如对齐和均匀性），同时在性能上达到类似甚至更优的效果。对于第二个问题，假设联合特征分布与高斯核成比例。",
    "tldr": "本文提出了一种名为$f$-MICL的方法，用于理解和推广基于InfoNCE的对比学习。通过使用$f$-divergences将基于KL的互信息推广为$f$-Mutual Information in Contrastive Learning ($f$-MICL)，我们回答了超越基于KL的目标函数以及设计更好相似度函数的问题。"
}