{
    "title": "PROSAC: Provably Safe Certification for Machine Learning Models under Adversarial Attacks",
    "abstract": "It is widely known that state-of-the-art machine learning models, including vision and language models, can be seriously compromised by adversarial perturbations. It is therefore increasingly relevant to develop capabilities to certify their performance in the presence of the most effective adversarial attacks. Our paper offers a new approach to certify the performance of machine learning models in the presence of adversarial attacks with population level risk guarantees. In particular, we introduce the notion of $(\\alpha,\\zeta)$ machine learning model safety. We propose a hypothesis testing procedure, based on the availability of a calibration set, to derive statistical guarantees providing that the probability of declaring that the adversarial (population) risk of a machine learning model is less than $\\alpha$ (i.e. the model is safe), while the model is in fact unsafe (i.e. the model adversarial population risk is higher than $\\alpha$), is less than $\\zeta$. We also propose Bayesian",
    "link": "https://arxiv.org/abs/2402.02629",
    "context": "Title: PROSAC: Provably Safe Certification for Machine Learning Models under Adversarial Attacks\nAbstract: It is widely known that state-of-the-art machine learning models, including vision and language models, can be seriously compromised by adversarial perturbations. It is therefore increasingly relevant to develop capabilities to certify their performance in the presence of the most effective adversarial attacks. Our paper offers a new approach to certify the performance of machine learning models in the presence of adversarial attacks with population level risk guarantees. In particular, we introduce the notion of $(\\alpha,\\zeta)$ machine learning model safety. We propose a hypothesis testing procedure, based on the availability of a calibration set, to derive statistical guarantees providing that the probability of declaring that the adversarial (population) risk of a machine learning model is less than $\\alpha$ (i.e. the model is safe), while the model is in fact unsafe (i.e. the model adversarial population risk is higher than $\\alpha$), is less than $\\zeta$. We also propose Bayesian",
    "path": "papers/24/02/2402.02629.json",
    "total_tokens": 916,
    "translated_title": "PROSAC：针对对抗攻击下的机器学习模型的可证明安全认证",
    "tldr": "本文提出了一种能够在对抗攻击下认证机器学习模型性能的方法，该方法基于校准集进行假设检验，并提供了统计保证，使得在模型的对抗群体风险高于给定阈值时，宣称该模型是安全的概率低于预先设定的阈值。",
    "en_tdlr": "This paper proposes a method to certify the performance of machine learning models under adversarial attacks. The method uses hypothesis testing based on a calibration set and provides statistical guarantees that the probability of declaring the model as safe when its adversarial population risk is higher than a given threshold is lower than a pre-defined value."
}