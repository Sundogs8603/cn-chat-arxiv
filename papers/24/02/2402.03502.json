{
    "title": "How Does Unlabeled Data Provably Help Out-of-Distribution Detection?",
    "abstract": "Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data. This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier. Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection. This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness. The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data. Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm. Our the",
    "link": "https://arxiv.org/abs/2402.03502",
    "context": "Title: How Does Unlabeled Data Provably Help Out-of-Distribution Detection?\nAbstract: Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data. This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier. Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection. This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness. The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data. Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm. Our the",
    "path": "papers/24/02/2402.03502.json",
    "total_tokens": 881,
    "translated_title": "未标记数据如何在超出分布检测中发挥可证明的作用？",
    "translated_abstract": "使用未标记数据对机器学习模型进行正则化已经显示出在检测超出分布（OOD）数据方面改进安全性和可靠性的潜力。利用野外未标记数据的能力是非常困难的，因为内分布（ID）和OOD数据的异质性。缺乏清洁的OOD样本集合在学习最优OOD分类器方面存在重大挑战。目前，缺乏对未标记数据如何帮助OOD检测的研究。本文通过引入新的学习框架SAL（Separate And Learn），填补了这一空白，该框架在理论上具有强大的保证和实证的有效性。该框架将候选异常点从未标记数据中分离出来，并使用候选异常点和标记的ID数据训练OOD分类器。从可分离性和可学习性的角度，我们在理论上提供了严格的错误界限，正式证明了我们算法中的两个组成部分。",
    "tldr": "本文介绍了一种新的学习框架SAL（Separate And Learn），通过利用未标记数据和标记数据，分离并训练异常点和OOD分类器，理论上提供了强大的保证和严格的错误界限。"
}