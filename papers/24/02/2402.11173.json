{
    "title": "How to Make the Gradients Small Privately: Improved Rates for Differentially Private Non-Convex Optimization",
    "abstract": "arXiv:2402.11173v1 Announce Type: new  Abstract: We provide a simple and flexible framework for designing differentially private algorithms to find approximate stationary points of non-convex loss functions. Our framework is based on using a private approximate risk minimizer to \"warm start\" another private algorithm for finding stationary points. We use this framework to obtain improved, and sometimes optimal, rates for several classes of non-convex loss functions. First, we obtain improved rates for finding stationary points of smooth non-convex empirical loss functions. Second, we specialize to quasar-convex functions, which generalize star-convex functions and arise in learning dynamical systems and training some neural nets. We achieve the optimal rate for this class. Third, we give an optimal algorithm for finding stationary points of functions satisfying the Kurdyka-Lojasiewicz (KL) condition. For example, over-parameterized neural networks often satisfy this condition. Fourth, ",
    "link": "https://arxiv.org/abs/2402.11173",
    "context": "Title: How to Make the Gradients Small Privately: Improved Rates for Differentially Private Non-Convex Optimization\nAbstract: arXiv:2402.11173v1 Announce Type: new  Abstract: We provide a simple and flexible framework for designing differentially private algorithms to find approximate stationary points of non-convex loss functions. Our framework is based on using a private approximate risk minimizer to \"warm start\" another private algorithm for finding stationary points. We use this framework to obtain improved, and sometimes optimal, rates for several classes of non-convex loss functions. First, we obtain improved rates for finding stationary points of smooth non-convex empirical loss functions. Second, we specialize to quasar-convex functions, which generalize star-convex functions and arise in learning dynamical systems and training some neural nets. We achieve the optimal rate for this class. Third, we give an optimal algorithm for finding stationary points of functions satisfying the Kurdyka-Lojasiewicz (KL) condition. For example, over-parameterized neural networks often satisfy this condition. Fourth, ",
    "path": "papers/24/02/2402.11173.json",
    "total_tokens": 878,
    "translated_title": "如何在隐私条件下使梯度变得更小：改进的差分隐私非凸优化速率",
    "translated_abstract": "我们提供了一个简单灵活的框架，用于设计具有差分隐私算法，以找到非凸损失函数的近似稳定点。我们的框架基于使用私有的近似风险最小化器来“热启动”另一个用于寻找稳定点的私有算法。我们利用这个框架来获得对几类非凸损失函数的改进甚至是最优速率。首先，我们改进了寻找平滑非凸经验损失函数稳定点的速率。其次，我们专门针对夸萨-凸函数，这种函数概括了星-凸函数，并在学习动态系统和训练一些神经网络时出现。我们为这个类别实现了最优速率。第三，我们提供了一种对满足Kurdyka-Lojasiewicz（KL）条件的函数寻找稳定点的最优算法。例如，超参数化神经网络经常满足这个条件。",
    "tldr": "提出了一种设计具有差分隐私算法的简单灵活框架，用于寻找非凸损失函数的近似稳定点，并获得了改进和有时是最优的速率。",
    "en_tdlr": "Introduced a simple and flexible framework for designing differentially private algorithms to find approximate stationary points of non-convex loss functions, achieving improved and sometimes optimal rates."
}