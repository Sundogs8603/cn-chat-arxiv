{
    "title": "Fixed width treelike neural networks capacity analysis -- generic activations",
    "abstract": "We consider the capacity of \\emph{treelike committee machines} (TCM) neural networks. Relying on Random Duality Theory (RDT), \\cite{Stojnictcmspnncaprdt23} recently introduced a generic framework for their capacity analysis. An upgrade based on the so-called \\emph{partially lifted} RDT (pl RDT) was then presented in \\cite{Stojnictcmspnncapliftedrdt23}. Both lines of work focused on the networks with the most typical, \\emph{sign}, activations. Here, on the other hand, we focus on networks with other, more general, types of activations and show that the frameworks of \\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} are sufficiently powerful to enable handling of such scenarios as well. In addition to the standard \\emph{linear} activations, we uncover that particularly convenient results can be obtained for two very commonly used activations, namely, the \\emph{quadratic} and \\emph{rectified linear unit (ReLU)} ones. In more concrete terms, for each of these activations, we obtai",
    "link": "https://arxiv.org/abs/2402.05696",
    "context": "Title: Fixed width treelike neural networks capacity analysis -- generic activations\nAbstract: We consider the capacity of \\emph{treelike committee machines} (TCM) neural networks. Relying on Random Duality Theory (RDT), \\cite{Stojnictcmspnncaprdt23} recently introduced a generic framework for their capacity analysis. An upgrade based on the so-called \\emph{partially lifted} RDT (pl RDT) was then presented in \\cite{Stojnictcmspnncapliftedrdt23}. Both lines of work focused on the networks with the most typical, \\emph{sign}, activations. Here, on the other hand, we focus on networks with other, more general, types of activations and show that the frameworks of \\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} are sufficiently powerful to enable handling of such scenarios as well. In addition to the standard \\emph{linear} activations, we uncover that particularly convenient results can be obtained for two very commonly used activations, namely, the \\emph{quadratic} and \\emph{rectified linear unit (ReLU)} ones. In more concrete terms, for each of these activations, we obtai",
    "path": "papers/24/02/2402.05696.json",
    "total_tokens": 977,
    "translated_title": "固定宽度的树状神经网络容量分析-通用激活函数",
    "translated_abstract": "我们考虑了树状委员会机（TCM）神经网络的容量。基于随机对偶理论（RDT），\\cite{Stojnictcmspnncaprdt23}最近提出了一个通用的框架用于它们的容量分析。然后，在\\cite{Stojnictcmspnncapliftedrdt23}中提出了一种基于所谓的“部分提升”RDT（pl RDT）的升级版本。这两个工作方向都着重于具有最典型的“符号”激活函数的网络。然而，在这里，我们专注于具有其他更一般类型激活函数的网络，并且证明了\\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23}的框架足够强大，可以处理这些情况。除了标准的“线性”激活函数外，我们发现两个广泛使用的激活函数，即“二次”和“修正线性单元（ReLU）”，可以得到特别方便的结果。更具体地说，对于每个激活函数，我们获得了",
    "tldr": "这篇论文研究了树状神经网络的容量，基于Random Duality Theory提出了通用的容量分析框架，并证明该框架适用于其他类型的激活函数，如二次和ReLU函数。",
    "en_tdlr": "This paper investigates the capacity of treelike neural networks, introduces a generic framework for capacity analysis based on Random Duality Theory, and demonstrates its applicability to various types of activation functions, including quadratic and rectified linear unit (ReLU)."
}