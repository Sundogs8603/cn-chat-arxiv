{
    "title": "D\\'ej\\`a Vu Memorization in Vision-Language Models",
    "abstract": "Vision-Language Models (VLMs) have emerged as the state-of-the-art representation learning solution, with myriads of downstream applications such as image classification, retrieval and generation. A natural question is whether these models memorize their training data, which also has implications for generalization. We propose a new method for measuring memorization in VLMs, which we call d\\'ej\\`a vu memorization. For VLMs trained on image-caption pairs, we show that the model indeed retains information about individual objects in the training images beyond what can be inferred from correlations or the image caption. We evaluate d\\'ej\\`a vu memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs. Finally, we show that text randomization considerably mitigates memorization while only moderately impacting the model's downstream task performance.",
    "link": "https://arxiv.org/abs/2402.02103",
    "context": "Title: D\\'ej\\`a Vu Memorization in Vision-Language Models\nAbstract: Vision-Language Models (VLMs) have emerged as the state-of-the-art representation learning solution, with myriads of downstream applications such as image classification, retrieval and generation. A natural question is whether these models memorize their training data, which also has implications for generalization. We propose a new method for measuring memorization in VLMs, which we call d\\'ej\\`a vu memorization. For VLMs trained on image-caption pairs, we show that the model indeed retains information about individual objects in the training images beyond what can be inferred from correlations or the image caption. We evaluate d\\'ej\\`a vu memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs. Finally, we show that text randomization considerably mitigates memorization while only moderately impacting the model's downstream task performance.",
    "path": "papers/24/02/2402.02103.json",
    "total_tokens": 911,
    "translated_title": "视觉语言模型中的心理现象记忆",
    "translated_abstract": "视觉语言模型（VLM）作为最先进的表示学习解决方案出现，具有诸多下游应用，如图像分类、检索和生成。一个自然的问题是这些模型是否会记忆训练数据，这也对泛化有着影响。我们提出了一种衡量VLMs中记忆的新方法，称之为心理现象记忆。对于在图像-标题对上训练的VLMs，我们展示了该模型确实保留了关于训练图像中个别对象的信息，超出了从相关性或图像标题中可以推断出的范畴。我们在样本和总体水平上评估了心理现象记忆，并展示了OpenCLIP在多达5000万个图像-标题对上训练时的显著性。最后，我们展示了文本随机化在很大程度上减轻了记忆，同时对模型的下游任务性能产生了适度影响。",
    "tldr": "这项研究提出了一种新方法来衡量视觉语言模型中的记忆现象，并发现对于使用图像-标题对进行训练的VLMs，模型确实会保留关于训练图像中的个别对象的信息，文本随机化可以在很大程度上减轻记忆现象而对模型的下游任务性能影响较小。",
    "en_tdlr": "This study proposes a new method for measuring memorization in vision-language models and finds that for VLMs trained on image-caption pairs, the model indeed retains information about individual objects in the training images, and text randomization can greatly mitigate memorization while only moderately impacting the model's downstream task performance."
}