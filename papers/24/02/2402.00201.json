{
    "title": "An Experiment on Feature Selection using Logistic Regression",
    "abstract": "In supervised machine learning, feature selection plays a very important role by potentially enhancing explainability and performance as measured by computing time and accuracy-related metrics. In this paper, we investigate a method for feature selection based on the well-known L1 and L2 regularization strategies associated with logistic regression (LR). It is well known that the learned coefficients, which serve as weights, can be used to rank the features. Our approach is to synthesize the findings of L1 and L2 regularization. For our experiment, we chose the CIC-IDS2018 dataset owing partly to its size and also to the existence of two problematic classes that are hard to separate. We report first with the exclusion of one of them and then with its inclusion. We ranked features first with L1 and then with L2, and then compared logistic regression with L1 (LR+L1) against that with L2 (LR+L2) by varying the sizes of the feature sets for each of the two rankings. We found no significant",
    "link": "https://arxiv.org/abs/2402.00201",
    "context": "Title: An Experiment on Feature Selection using Logistic Regression\nAbstract: In supervised machine learning, feature selection plays a very important role by potentially enhancing explainability and performance as measured by computing time and accuracy-related metrics. In this paper, we investigate a method for feature selection based on the well-known L1 and L2 regularization strategies associated with logistic regression (LR). It is well known that the learned coefficients, which serve as weights, can be used to rank the features. Our approach is to synthesize the findings of L1 and L2 regularization. For our experiment, we chose the CIC-IDS2018 dataset owing partly to its size and also to the existence of two problematic classes that are hard to separate. We report first with the exclusion of one of them and then with its inclusion. We ranked features first with L1 and then with L2, and then compared logistic regression with L1 (LR+L1) against that with L2 (LR+L2) by varying the sizes of the feature sets for each of the two rankings. We found no significant",
    "path": "papers/24/02/2402.00201.json",
    "total_tokens": 866,
    "translated_title": "用逻辑回归进行特征选择的实验",
    "translated_abstract": "在监督机器学习中，特征选择在提高可解释性和以计算时间和准确性相关度量为衡量标准方面起着非常重要的作用。本文基于逻辑回归（LR）中广为人知的L1和L2正则化策略，研究了一种特征选择方法。众所周知，学到的系数可以用作权重来对特征进行排序。我们的方法是综合L1和L2正则化的结果。在我们的实验中，我们选择了CIC-IDS2018数据集，部分原因是其规模，同时因为存在两个难以区分的问题类别。我们首先排除其中一个问题类别，然后再包含它。我们先通过L1进行特征排序，然后再通过L2进行特征排序，并通过改变两个排序中特征集的大小来比较带有L1的逻辑回归（LR+L1）和带有L2的逻辑回归（LR+L2）。我们没有发现显著的结果。",
    "tldr": "这篇论文探讨了一种基于逻辑回归的特征选择方法，通过综合L1和L2正则化的结果，对CIC-IDS2018数据集进行了实验。没有发现显著结果。"
}