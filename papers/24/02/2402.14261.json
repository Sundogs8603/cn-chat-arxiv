{
    "title": "Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming",
    "abstract": "arXiv:2402.14261v1 Announce Type: cross  Abstract: The integration of Large Language Models (LLMs) into Development Environments (IDEs) has become a focal point in modern software development. LLMs such as OpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment developer productivity by serving as intelligent, chat-driven programming assistants. However, utilizing LLMs out of the box is unlikely to be optimal for any given scenario. Rather, each system requires the LLM to be honed to its set of heuristics to ensure the best performance. In this paper, we introduce the Copilot evaluation harness: a set of data and tools for evaluating LLM-guided IDE interactions, covering various programming scenarios and languages. We propose our metrics as a more robust and information-dense evaluation than previous state of the art evaluation systems. We design and compute both static and execution based success metrics for scenarios encompassing a wide range of developer tasks, ",
    "link": "https://arxiv.org/abs/2402.14261",
    "context": "Title: Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming\nAbstract: arXiv:2402.14261v1 Announce Type: cross  Abstract: The integration of Large Language Models (LLMs) into Development Environments (IDEs) has become a focal point in modern software development. LLMs such as OpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment developer productivity by serving as intelligent, chat-driven programming assistants. However, utilizing LLMs out of the box is unlikely to be optimal for any given scenario. Rather, each system requires the LLM to be honed to its set of heuristics to ensure the best performance. In this paper, we introduce the Copilot evaluation harness: a set of data and tools for evaluating LLM-guided IDE interactions, covering various programming scenarios and languages. We propose our metrics as a more robust and information-dense evaluation than previous state of the art evaluation systems. We design and compute both static and execution based success metrics for scenarios encompassing a wide range of developer tasks, ",
    "path": "papers/24/02/2402.14261.json",
    "total_tokens": 841,
    "translated_title": "Copilot评估工具：评估LLM引导的软件编程",
    "translated_abstract": "将大型语言模型（LLMs）整合到开发环境（IDEs）已成为现代软件开发的焦点。LLMs，如OpenAI GPT-3.5/4和Code Llama，能够作为智能的、基于聊天的编程助手，显著提高开发人员的生产力。然而，直接使用LLMs可能并不适用于任何场景。相反，每个系统都需要对LLMs进行调整以适应其启发式集，以确保获得最佳性能。本文引入了Copilot评估工具：一套用于评估LLM引导的IDE交互的数据和工具，涵盖各种编程场景和语言。我们提出的度量标准比先前的最先进的评估系统更为稳健和信息密集。我们为涵盖广泛的开发人员任务范围的情景设计并计算了静态和基于执行的成功度量标准。",
    "tldr": "本文介绍了Copilot评估工具，用于评估LLM引导的IDE交互，在各种编程场景和语言中提供更为稳健和信息密集的评估。",
    "en_tdlr": "This paper introduces the Copilot evaluation harness for evaluating LLM-guided IDE interactions, providing more robust and information-dense evaluation in various programming scenarios and languages."
}