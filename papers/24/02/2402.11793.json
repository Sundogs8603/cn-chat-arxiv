{
    "title": "Generative Kaleidoscopic Networks",
    "abstract": "arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\\in\\mathbb{R}^D$ to itself $f_\\mathcal{N}(x)\\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\\in\\mathbb{R}^D$ and recursively applies $f_\\mathcal{N}(\\cdots f_\\mathcal{N}(z)\\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper ",
    "link": "https://arxiv.org/abs/2402.11793",
    "context": "Title: Generative Kaleidoscopic Networks\nAbstract: arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\\in\\mathbb{R}^D$ to itself $f_\\mathcal{N}(x)\\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\\in\\mathbb{R}^D$ and recursively applies $f_\\mathcal{N}(\\cdots f_\\mathcal{N}(z)\\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper ",
    "path": "papers/24/02/2402.11793.json",
    "total_tokens": 928,
    "translated_title": "生成万花筒网络",
    "translated_abstract": "发现深层ReLU网络（或多层感知器架构）表现出“过度泛化”现象。也就是说，那些在训练过程中没有看到的输入的输出值被映射到了在学习过程中观察到的输出范围附近。换句话说，多层感知器学习了一对多的映射，这种效应在增加层数或多层感知器的深度时更为明显。我们利用了深层ReLU网络的这一特性来设计一个数据集万花筒，称为“生成万花筒网络”。简而言之，如果我们学习一个多层感知器将输入 $x\\in\\mathbb{R}^D$ 映射到自身 $f_\\mathcal{N}(x)\\rightarrow x$，那么“万花筒采样”过程将从随机输入噪声 $z\\in\\mathbb{R}^D$ 开始，并递归地应用 $f_\\mathcal{N}(\\cdots f_\\mathcal{N}(z)\\cdots )$。经过燃烧期后，我们开始观察来自输入分布的样本，我们发现更深的",
    "tldr": "发现深层ReLU网络表现出过度泛化现象，利用这一特性设计了“生成万花筒网络”，通过递归映射随机输入噪声生成样本。",
    "en_tdlr": "Discovered over-generalization in Deep ReLU networks and utilized this to design \"Generative Kaleidoscopic Networks\" for recursive mapping of random input noise to generate samples."
}