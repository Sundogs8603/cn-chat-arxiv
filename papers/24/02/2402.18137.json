{
    "title": "DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning",
    "abstract": "arXiv:2402.18137v1 Announce Type: cross  Abstract: Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNC",
    "link": "https://arxiv.org/abs/2402.18137",
    "context": "Title: DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning\nAbstract: arXiv:2402.18137v1 Announce Type: cross  Abstract: Multimodal pretraining has emerged as an effective strategy for the trinity of goals of representation learning in autonomous robots: 1) extracting both local and global task progression information; 2) enforcing temporal consistency of visual representation; 3) capturing trajectory-level language grounding. Most existing methods approach these via separate objectives, which often reach sub-optimal solutions. In this paper, we propose a universal unified objective that can simultaneously extract meaningful task progression information from image sequences and seamlessly align them with language instructions. We discover that via implicit preferences, where a visual trajectory inherently aligns better with its corresponding language instruction than mismatched pairs, the popular Bradley-Terry model can transform into representation learning through proper reward reparameterizations. The resulted framework, DecisionNCE, mirrors an InfoNC",
    "path": "papers/24/02/2402.18137.json",
    "total_tokens": 792,
    "translated_title": "DecisionNCE: 通过隐式偏好学习实体多模态表示",
    "translated_abstract": "多模态预训练已被证明是自主机器人中表示学习的三大目标：1）提取局部和全局任务进展信息；2）强化视觉表示的时间一致性；3）捕获轨迹级语言基础的有效策略。大部分已有方法通过不同的目标来处理这些问题，往往导致次优解。本文提出了一个通用统一目标，可以同时从图像序列中提取有意义的任务进展信息，并将它们与语言指令无缝对齐。我们发现，通过隐式偏好，在视觉轨迹与其对应的语言指令相比不匹配对更好地对齐时，流行的 Bradley-Terry 模型可以通过适当的奖励重新参数化而变为表示学习。结果产生的 DecisionNCE 框架，类似于 InfoNC",
    "tldr": "本文提出了 DecisionNCE 框架，通过隐式偏好学习实体多模态表示，实现了提取任务进展信息和与语言指令对齐的有效方法",
    "en_tdlr": "The paper introduces the DecisionNCE framework, which learns embodied multimodal representations via implicit preferences, providing an effective way to extract task progression information and align with language instructions."
}