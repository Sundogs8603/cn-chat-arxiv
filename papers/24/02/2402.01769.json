{
    "title": "Redefining \"Hallucination\" in LLMs: Towards a psychology-informed framework for mitigating misinformation",
    "abstract": "In recent years, large language models (LLMs) have become incredibly popular, with ChatGPT for example being used by over a billion users. While these models exhibit remarkable language understanding and logical prowess, a notable challenge surfaces in the form of \"hallucinations.\" This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base. However, we question the appropriateness of the term \"hallucination\" in LLMs, proposing a psychological taxonomy based on cognitive biases and other psychological phenomena. Our approach offers a more fine-grained understanding of this phenomenon, allowing for targeted solutions. By leveraging insights from how humans internally resolve similar challenges, we aim to develop strategies to mitigate LLM hallucinations. This interdisciplinary approach seeks to move beyond conventional terminology, providing a nuanced understanding and actionable pathways for imp",
    "link": "https://arxiv.org/abs/2402.01769",
    "context": "Title: Redefining \"Hallucination\" in LLMs: Towards a psychology-informed framework for mitigating misinformation\nAbstract: In recent years, large language models (LLMs) have become incredibly popular, with ChatGPT for example being used by over a billion users. While these models exhibit remarkable language understanding and logical prowess, a notable challenge surfaces in the form of \"hallucinations.\" This phenomenon results in LLMs outputting misinformation in a confident manner, which can lead to devastating consequences with such a large user base. However, we question the appropriateness of the term \"hallucination\" in LLMs, proposing a psychological taxonomy based on cognitive biases and other psychological phenomena. Our approach offers a more fine-grained understanding of this phenomenon, allowing for targeted solutions. By leveraging insights from how humans internally resolve similar challenges, we aim to develop strategies to mitigate LLM hallucinations. This interdisciplinary approach seeks to move beyond conventional terminology, providing a nuanced understanding and actionable pathways for imp",
    "path": "papers/24/02/2402.01769.json",
    "total_tokens": 941,
    "translated_title": "重新定义LLMs中的“幻觉”：构建心理学为基础的减轻误导的框架",
    "translated_abstract": "近年来，大型语言模型（LLMs）变得非常受欢迎，例如ChatGPT已经被超过十亿的用户使用。虽然这些模型展现出了出色的语言理解和逻辑能力，但在“幻觉”方面存在一个显著挑战。这一现象导致LLMs以自信的方式输出误导信息，而这可以在如此庞大的用户群体中产生灾难性后果。然而，我们对LLMs中使用“幻觉”一词的适当性提出了质疑，并提出了基于认知偏差和其他心理现象的心理分类法。我们的方法提供了对这一现象更详细的理解，从而能够提供有针对性的解决方案。通过利用人类内部解决类似挑战的见解，我们旨在开发策略来减轻LLMs的幻觉。这种跨学科的方法旨在超越传统的术语，为深入理解和可操作的路径提供细致入微的认识。",
    "tldr": "该研究旨在重新定义LLMs中的“幻觉”，提出了心理学分类法，以更详细地理解和解决LLMs输出误导信息的挑战。通过借鉴人类处理类似挑战的方式，研究旨在开发策略以减轻LLMs中的幻觉。",
    "en_tdlr": "This research aims to redefine \"hallucination\" in LLMs and proposes a psychological taxonomy to better understand and address the challenges of LLMs outputting misinformation. By leveraging insights from how humans handle similar challenges, the study aims to develop strategies to mitigate LLM hallucinations."
}