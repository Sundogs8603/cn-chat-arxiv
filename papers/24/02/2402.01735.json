{
    "title": "VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models",
    "abstract": "Visually Impaired Assistance (VIA) aims to automatically help visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (\\textbf{VIALM}). In this task, given an \\textit{image} illustrating the physical environments and a \\textit{linguistic request} from a VI user, VIALM aims to output step-by-step \\textit{guidance} to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities",
    "link": "https://arxiv.org/abs/2402.01735",
    "context": "Title: VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models\nAbstract: Visually Impaired Assistance (VIA) aims to automatically help visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (\\textbf{VIALM}). In this task, given an \\textit{image} illustrating the physical environments and a \\textit{linguistic request} from a VI user, VIALM aims to output step-by-step \\textit{guidance} to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities",
    "path": "papers/24/02/2402.01735.json",
    "total_tokens": 909,
    "translated_title": "VIALM：关于具有大型模型的视觉障碍辅助的调查和基准研究",
    "translated_abstract": "视觉障碍辅助 (VIA) 旨在自动帮助视觉障碍者 (VI) 处理日常活动。VIA 的进展主要依赖于计算机视觉 (CV) 和自然语言处理 (NLP) 的发展，二者都展示了利用大型模型 (LMs) 的前沿范式。此外，LMs 展现出出色的多模态能力，可以应对诸如具身机器人等具有挑战性的物理任务。为了研究最先进 (SOTA) LMs 在VIA应用中的潜力和局限性，我们针对具有LMs的VIA任务（VIALM）进行了广泛的研究。在这个任务中，给定一个说明物理环境的图像和视觉障碍者用户的语言请求，VIALM旨在输出逐步引导，以在环境中帮助视觉障碍用户完成请求。该研究包括对近期LM研究的调查和对选定LMs能力的基准实验的检查。",
    "tldr": "这项研究调查了具有大型模型的视觉障碍辅助，并通过基准实验评估了模型的能力，进一步推动了视觉障碍辅助技术的发展。",
    "en_tdlr": "This study surveyed visually impaired assistance with large models (VIALM) and benchmarked the capabilities of the models, further advancing the development of visually impaired assistance technology."
}