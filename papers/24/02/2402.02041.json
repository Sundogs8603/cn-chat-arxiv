{
    "title": "$\\alpha$-Divergence Loss Function for Neural Density Ratio Estimation",
    "abstract": "Recently, neural networks have produced state-of-the-art results for density-ratio estimation (DRE), a fundamental technique in machine learning. However, existing methods bear optimization issues that arise from the loss functions of DRE: a large sample requirement of Kullback--Leibler (KL)-divergence, vanishing of train loss gradients, and biased gradients of the loss functions. Thus, an $\\alpha$-divergence loss function ($\\alpha$-Div) that offers concise implementation and stable optimization is proposed in this paper. Furthermore, technical justifications for the proposed loss function are presented. The stability of the proposed loss function is empirically demonstrated and the estimation accuracy of DRE tasks is investigated. Additionally, this study presents a sample requirement for DRE using the proposed loss function in terms of the upper bound of $L_1$ error, which connects a curse of dimensionality as a common problem in high-dimensional DRE tasks.",
    "link": "https://arxiv.org/abs/2402.02041",
    "context": "Title: $\\alpha$-Divergence Loss Function for Neural Density Ratio Estimation\nAbstract: Recently, neural networks have produced state-of-the-art results for density-ratio estimation (DRE), a fundamental technique in machine learning. However, existing methods bear optimization issues that arise from the loss functions of DRE: a large sample requirement of Kullback--Leibler (KL)-divergence, vanishing of train loss gradients, and biased gradients of the loss functions. Thus, an $\\alpha$-divergence loss function ($\\alpha$-Div) that offers concise implementation and stable optimization is proposed in this paper. Furthermore, technical justifications for the proposed loss function are presented. The stability of the proposed loss function is empirically demonstrated and the estimation accuracy of DRE tasks is investigated. Additionally, this study presents a sample requirement for DRE using the proposed loss function in terms of the upper bound of $L_1$ error, which connects a curse of dimensionality as a common problem in high-dimensional DRE tasks.",
    "path": "papers/24/02/2402.02041.json",
    "total_tokens": 927,
    "translated_title": "用于神经密度比估计的$\\alpha$-散度损失函数",
    "translated_abstract": "最近，神经网络在机器学习中的基础技术密度比估计(DRE)方面取得了最先进的结果。然而，现有方法因DRE的损失函数而出现了优化问题：KL散度需要大样本，训练损失梯度消失，损失函数梯度有偏。因此，本文提出了一种提供简洁实现和稳定优化的$\\alpha$-散度损失函数($\\alpha$-Div)。此外，还给出了对所提出的损失函数的技术验证。实验证明了所提出的损失函数的稳定性，并研究了DRE任务的估计准确性。此外，本研究还提出了使用所提出的损失函数进行DRE的样本要求，以$L_1$误差的上界联系起来，该上界将高维度DRE任务中的维度诅咒作为一个共同问题。",
    "tldr": "本文提出了一种应用于神经密度比估计的$\\alpha$-散度损失函数($\\alpha$-Div)，通过简洁实现和稳定优化解决了现有方法中存在的优化问题。实验证明了这种损失函数的稳定性，并提出了对DRE任务的估计准确性的研究，同时给出了样本要求的解决方案。",
    "en_tdlr": "This paper proposes an $\\alpha$-divergence loss function ($\\alpha$-Div) for neural density ratio estimation (DRE), which addresses optimization issues in existing methods. The stability of the proposed loss function is empirically demonstrated, and the estimation accuracy of DRE tasks is investigated. Additionally, a solution for the sample requirement is provided."
}