{
    "title": "The Common Stability Mechanism behind most Self-Supervised Learning Approaches",
    "abstract": "arXiv:2402.14957v1 Announce Type: cross  Abstract: Last couple of years have witnessed a tremendous progress in self-supervised learning (SSL), the success of which can be attributed to the introduction of useful inductive biases in the learning process to learn meaningful visual representations while avoiding collapse. These inductive biases and constraints manifest themselves in the form of different optimization formulations in the SSL techniques, e.g. by utilizing negative examples in a contrastive formulation, or exponential moving average and predictor in BYOL and SimSiam. In this paper, we provide a framework to explain the stability mechanism of these different SSL techniques: i) we discuss the working mechanism of contrastive techniques like SimCLR, non-contrastive techniques like BYOL, SWAV, SimSiam, Barlow Twins, and DINO; ii) we provide an argument that despite different formulations these methods implicitly optimize a similar objective function, i.e. minimizing the magnitu",
    "link": "https://arxiv.org/abs/2402.14957",
    "context": "Title: The Common Stability Mechanism behind most Self-Supervised Learning Approaches\nAbstract: arXiv:2402.14957v1 Announce Type: cross  Abstract: Last couple of years have witnessed a tremendous progress in self-supervised learning (SSL), the success of which can be attributed to the introduction of useful inductive biases in the learning process to learn meaningful visual representations while avoiding collapse. These inductive biases and constraints manifest themselves in the form of different optimization formulations in the SSL techniques, e.g. by utilizing negative examples in a contrastive formulation, or exponential moving average and predictor in BYOL and SimSiam. In this paper, we provide a framework to explain the stability mechanism of these different SSL techniques: i) we discuss the working mechanism of contrastive techniques like SimCLR, non-contrastive techniques like BYOL, SWAV, SimSiam, Barlow Twins, and DINO; ii) we provide an argument that despite different formulations these methods implicitly optimize a similar objective function, i.e. minimizing the magnitu",
    "path": "papers/24/02/2402.14957.json",
    "total_tokens": 786,
    "translated_title": "大多数自监督学习方法背后的共同稳定机制",
    "translated_abstract": "过去几年见证了自监督学习（SSL）领域的巨大进展，其成功归功于在学习过程中引入了有用的归纳偏差，以学习有意义的视觉表示，同时避免坍缩。本文提供了一个框架来解释这些不同SSL技术的稳定机制，讨论了像SimCLR这样的对比技术、像BYOL、SWAV、SimSiam、Barlow Twins和DINO这样的非对比技术的工作机制，并提出了一个论点，即尽管有不同的公式，这些方法隐式优化了类似的目标函数，即最小化幅度。",
    "tldr": "自监督学习方法的共同稳定机制是通过引入有用的归纳偏差来学习有意义的视觉表示，并避免坍缩，这些方法尽管有不同的公式，但隐式优化了类似的目标函数。",
    "en_tdlr": "The common stability mechanism behind most self-supervised learning approaches is to learn meaningful visual representations by introducing useful inductive biases and avoiding collapse, where these methods implicitly optimize a similar objective function despite different formulations."
}