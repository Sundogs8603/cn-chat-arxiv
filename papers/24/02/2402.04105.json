{
    "title": "Measuring Implicit Bias in Explicitly Unbiased Large Language Models",
    "abstract": "Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both of these challenges by introducing two measures of bias inspired by psychology: LLM Implicit Association Test (IAT) Bias, which is a prompt-based method for revealing implicit bias; and LLM Decision Bias for detecting subtle discrimination in decision-making tasks. Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others). Our prompt-based measure of implicit bias correlates wit",
    "link": "https://arxiv.org/abs/2402.04105",
    "context": "Title: Measuring Implicit Bias in Explicitly Unbiased Large Language Models\nAbstract: Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both of these challenges by introducing two measures of bias inspired by psychology: LLM Implicit Association Test (IAT) Bias, which is a prompt-based method for revealing implicit bias; and LLM Decision Bias for detecting subtle discrimination in decision-making tasks. Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others). Our prompt-based measure of implicit bias correlates wit",
    "path": "papers/24/02/2402.04105.json",
    "total_tokens": 947,
    "translated_title": "在明确无偏大型语言模型中测量隐性偏见",
    "translated_abstract": "大型语言模型（LLMs）能够通过明确的偏见测试，但仍然可能存在隐性偏见，类似于持有平等主义信念的人们却表现出微妙的偏见。测量这种隐性偏见是一项挑战：随着LLMs变得越来越专有，可能无法访问它们的嵌入，并应用现有的偏见测量方法；此外，隐性偏见主要是一个问题，如果它们影响了这些系统所做的实际决策。我们通过引入受心理学启发的两个偏见测量方法来应对这两个挑战：LLMs隐含联想测试（IAT）偏见是一种基于提示的测量隐性偏见的方法；LLMs决策偏见用于检测决策任务中的微妙歧视。使用这些测量方法，我们发现了6个LLMs在4个社会领域（种族、性别、宗教、健康）和21个类别（武器、罪罚、科学、职业等）中普遍存在人类化的刻板印象偏见。我们的基于提示的隐性偏见测量方法与实际决策中的隐性偏见相关。",
    "tldr": "通过引入受心理学启发的两个偏见测量方法，我们在明确无偏大型语言模型中发现了普遍存在的人类化的刻板印象偏见，并与实际决策中的隐性偏见相关。"
}