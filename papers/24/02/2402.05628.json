{
    "title": "RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization",
    "abstract": "Large transformer models have demonstrated remarkable success. Post-training quantization (PTQ), which requires only a small dataset for calibration and avoids end-to-end retraining, is a promising solution for compressing these large models. Regrettably, existing PTQ methods typically exhibit non-trivial performance loss. We find that the performance bottleneck stems from over-consideration of hardware compatibility in the quantization process, compelling them to reluctantly employ simple quantizers, albeit at the expense of accuracy. With the above insights, we propose RepQuant, a novel PTQ framework with quantization-inference decoupling paradigm to address the above issues. RepQuant employs complex quantizers in the quantization process and simplified quantizers in the inference process, and performs mathematically equivalent transformations between the two through quantization scale reparameterization, thus ensuring both accurate quantization and efficient inference. More specific",
    "link": "https://arxiv.org/abs/2402.05628",
    "context": "Title: RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization\nAbstract: Large transformer models have demonstrated remarkable success. Post-training quantization (PTQ), which requires only a small dataset for calibration and avoids end-to-end retraining, is a promising solution for compressing these large models. Regrettably, existing PTQ methods typically exhibit non-trivial performance loss. We find that the performance bottleneck stems from over-consideration of hardware compatibility in the quantization process, compelling them to reluctantly employ simple quantizers, albeit at the expense of accuracy. With the above insights, we propose RepQuant, a novel PTQ framework with quantization-inference decoupling paradigm to address the above issues. RepQuant employs complex quantizers in the quantization process and simplified quantizers in the inference process, and performs mathematically equivalent transformations between the two through quantization scale reparameterization, thus ensuring both accurate quantization and efficient inference. More specific",
    "path": "papers/24/02/2402.05628.json",
    "total_tokens": 862,
    "translated_title": "RepQuant: 通过比例重参数化实现大型Transformer模型的准确后训练量化",
    "translated_abstract": "大型Transformer模型已经展示了卓越的成功。后训练量化（PTQ）是一种有前景的解决方案，用于压缩这些大型模型，它只需要一个小型数据集进行校准，并避免端到端的重新训练。不幸的是，现有的PTQ方法通常会导致非常明显的性能损失。我们发现，性能瓶颈来自于在量化过程中过分考虑硬件兼容性，这迫使它们不情愿地使用简单的量化器，虽然以牺牲准确性为代价。基于上述观察，我们提出了RepQuant，一个新的PTQ框架，通过采用量化-推理解耦范式来解决上述问题。RepQuant在量化过程中使用复杂的量化器，在推理过程中使用简化的量化器，并通过量化比例重参数化在两者之间进行数学上等价的转换，从而确保准确的量化和高效的推理。",
    "tldr": "RepQuant提出了一种新的后训练量化框架，通过采用量化-推理解耦的范式和量化比例重参数化的方法，实现了准确的量化和高效的推理。",
    "en_tdlr": "RepQuant proposes a novel post-training quantization framework that achieves accurate quantization and efficient inference by adopting the paradigm of quantization-inference decoupling and quantization scale reparameterization."
}