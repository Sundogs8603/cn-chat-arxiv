{
    "title": "DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer",
    "abstract": "Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or Transformer architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a Transformer-based network equipped with novel biased conditional attention modules. These modules serve as substitutes for the traditional self/cross-attention in standard Transformers, incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions. We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm. Experiments show our model not only achieves",
    "link": "https://arxiv.org/abs/2402.05712",
    "context": "Title: DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer\nAbstract: Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or Transformer architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a Transformer-based network equipped with novel biased conditional attention modules. These modules serve as substitutes for the traditional self/cross-attention in standard Transformers, incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions. We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm. Experiments show our model not only achieves",
    "path": "papers/24/02/2402.05712.json",
    "total_tokens": 911,
    "translated_title": "DiffSpeaker: 使用扩散变换器进行语音驱动的三维面部动画",
    "translated_abstract": "语音驱动的三维面部动画对于许多多媒体应用非常重要。最近的研究表明，使用扩散模型或变换器架构都在这个任务中表现出了潜力。然而，它们的简单聚合并没有带来改进的性能。我们怀疑这是由于缺乏配对的音频-4D数据，这对于变换器在扩散框架内有效地作为去噪器进行工作非常关键。为了解决这个问题，我们提出了DiffSpeaker，这是一种基于变换器的网络，配备了新颖的有偏条件注意模块。这些模块作为传统变换器中自注意力/交叉注意力的替代品，融入了经过深思熟虑的偏见，以使注意机制集中在既与任务相关又与扩散相关的条件上。我们还在扩散范式中探讨了精确的唇部同步和非语言面部表情之间的权衡。实验证明我们的模型不仅实现了准确的唇部同步，而且实现了有效的非语言面部表达。",
    "tldr": "DiffSpeaker是一种使用扩散变换器和自注意力模块的语音驱动3D面部动画网络，通过解决配对音频-4D数据的缺乏问题，实现了准确的唇部同步和非语言面部表达。",
    "en_tdlr": "DiffSpeaker is a speech-driven 3D facial animation network that uses a diffusion transformer and self-attention modules to achieve accurate lip synchronization and non-verbal facial expression by addressing the shortage of paired audio-4D data."
}