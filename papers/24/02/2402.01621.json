{
    "title": "Stochastic Two Points Method for Deep Model Zeroth-order Optimization",
    "abstract": "Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions. The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms",
    "link": "https://rss.arxiv.org/abs/2402.01621",
    "context": "Title: Stochastic Two Points Method for Deep Model Zeroth-order Optimization\nAbstract: Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions. The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms",
    "path": "papers/24/02/2402.01621.json",
    "total_tokens": 886,
    "translated_title": "针对深度模型零阶优化的随机两点法",
    "translated_abstract": "大型基础模型，例如大型语言模型，在各种应用场景中表现出色。由于硬件预算或缺乏反向传播的访问权限，构建或完全微调这样的大模型通常是不可行的。零阶方法为解决这一挑战提供了一种有希望的方向，它只需要前向传递来更新模型。本文在无梯度情形下引入了一种高效的随机两点（S2P）方法。我们在一般和放松的平滑性假设下提出了S2P的理论收敛性质。理论性质还揭示了更快、更稳定的S2P变体——加速S2P（AS2P），通过利用我们的新收敛性质，更好地表示了深度模型在训练中的动力学。我们全面的实证结果表明，AS2P在优化大型深度模型（包括语言模型）的目标上非常有效，并且优于其他方法。",
    "tldr": "本文介绍了一种针对大型深度模型的零阶优化方法——随机两点法，通过前向传递来更新模型。并且通过理论分析和实验证明了其在优化目标上的高效性并超越其他方法。",
    "en_tdlr": "This paper introduces the Stochastic Two-Point (S2P) method, a zeroth-order optimization approach for large deep models, where only forward passes are needed to update the model. The theoretical analysis and empirical results demonstrate the effectiveness of the proposed method in optimizing objectives and outperforming other methods."
}