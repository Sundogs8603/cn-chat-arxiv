{
    "title": "Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs",
    "abstract": "arXiv:2402.14740v1 Announce Type: new  Abstract: AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. \\textsc{Proximal Policy Optimization} (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the \\textit{formulation} of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed \"RL-free\" methods such a",
    "link": "https://arxiv.org/abs/2402.14740",
    "context": "Title: Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs\nAbstract: arXiv:2402.14740v1 Announce Type: new  Abstract: AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. \\textsc{Proximal Policy Optimization} (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the \\textit{formulation} of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed \"RL-free\" methods such a",
    "path": "papers/24/02/2402.14740.json",
    "total_tokens": 871,
    "translated_title": "回归基础: 重新审视LLMs中学习人类反馈的REINFORCE风格优化",
    "translated_abstract": "arXiv:2402.14740v1 公告类型: 新的 摘要: AI对齐被视为大型语言模型中从人类反馈中学习的至关重要组成部分。 \\textsc{Proximal Policy Optimization} (PPO)已被最新文献定位为RLHF中RL部分的典范方法。然而，它既涉及高计算成本又涉及敏感的超参数调整。我们认为触发了PPO发展的大多数动机原则在RLHF中并非实践上的关注重点，并提倡一种更少计算消耗的方法，该方法保持甚至提高了性能。我们重新审视了在RL的环境中根据人类偏好进行对齐的\\textit{公式}。以简单性为指导原则，我们展示了PPO的许多组件在RLHF环境中是不必要的，并且远远更简单的REINFORCE风格的优化变体表现出比PPO和新提出的“RL-free”方法更好的性能。",
    "tldr": "在大型语言模型中，重新审视REINFORCE风格优化对于学习人类反馈具有重要意义，简化优化方法可以提高性能。",
    "en_tdlr": "Revisiting REINFORCE style optimization is crucial for learning from human feedback in large language models, and simplifying the optimization method can improve performance."
}