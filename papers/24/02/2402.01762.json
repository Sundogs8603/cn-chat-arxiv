{
    "title": "Commercial AI, Conflict, and Moral Responsibility: A theoretical analysis and practical approach to the moral responsibilities associated with dual-use AI technology",
    "abstract": "This paper presents a theoretical analysis and practical approach to the moral responsibilities when developing AI systems for non-military applications that may nonetheless be used for conflict applications. We argue that AI represents a form of crossover technology that is different from previous historical examples of dual- or multi-use technology as it has a multiplicative effect across other technologies. As a result, existing analyses of ethical responsibilities around dual-use technologies do not necessarily work for AI systems. We instead argue that stakeholders involved in the AI system lifecycle are morally responsible for uses of their systems that are reasonably foreseeable. The core idea is that an agent's moral responsibility for some action is not necessarily determined by their intentions alone; we must also consider what the agent could reasonably have foreseen to be potential outcomes of their action, such as the potential use of a system in conflict even when it is n",
    "link": "https://arxiv.org/abs/2402.01762",
    "context": "Title: Commercial AI, Conflict, and Moral Responsibility: A theoretical analysis and practical approach to the moral responsibilities associated with dual-use AI technology\nAbstract: This paper presents a theoretical analysis and practical approach to the moral responsibilities when developing AI systems for non-military applications that may nonetheless be used for conflict applications. We argue that AI represents a form of crossover technology that is different from previous historical examples of dual- or multi-use technology as it has a multiplicative effect across other technologies. As a result, existing analyses of ethical responsibilities around dual-use technologies do not necessarily work for AI systems. We instead argue that stakeholders involved in the AI system lifecycle are morally responsible for uses of their systems that are reasonably foreseeable. The core idea is that an agent's moral responsibility for some action is not necessarily determined by their intentions alone; we must also consider what the agent could reasonably have foreseen to be potential outcomes of their action, such as the potential use of a system in conflict even when it is n",
    "path": "papers/24/02/2402.01762.json",
    "total_tokens": 871,
    "translated_title": "商业人工智能、冲突和道德责任：关于双重用途人工智能技术所涉及的道德责任的理论分析和实践方法",
    "translated_abstract": "本文提出了关于开发非军事应用中可能被用于冲突应用的人工智能系统时涉及的道德责任的理论分析和实践方法。我们认为，人工智能代表了一种与之前的双重或多重用途技术不同的交叉技术，因为它在其他技术上具有乘法效应。因此，现有关于双重用途技术的道德责任分析不一定适用于人工智能系统。我们认为，参与人工智能系统生命周期的利益相关者对于他们系统的合理可预见的使用负有道德责任。核心思想是，一个行动主体的道德责任不仅仅取决于他们的意图，我们还必须考虑行动主体合理可预见的行动结果，如系统在冲突中的潜在用途等。",
    "tldr": "本文对开发非军事应用中可能被用于冲突的人工智能系统时的道德责任进行了理论分析和实践方法的探讨，并认为利益相关者对于合理可预见的系统使用负有道德责任。"
}