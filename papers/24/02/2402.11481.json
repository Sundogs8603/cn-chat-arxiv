{
    "title": "DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics",
    "abstract": "arXiv:2402.11481v1 Announce Type: new  Abstract: Structured data offers a sophisticated mechanism for the organization of information. Existing methodologies for the text-serialization of structured data in the context of large language models fail to adequately address the heterogeneity inherent in key-value structured data. These methods are not ideal and frequently result in larger input sizes and poor adaptability to input changes. In this paper, we introduce DictLLM, an innovative framework designed to improve the modeling of key-value structured data, like medical laboratory reports, for generating medical diagnoses. DictLLM integrates three key components: (1) group positional encoding to maintain permutation invariance, (2) hierarchical attention bias to capture the inherent bias in structured data, and (3) an optimal transport alignment layer that aligns the embedding generated by the dictionary encoder with the LLM, thereby producing a sequence of fixed-length virtual tokens.",
    "link": "https://arxiv.org/abs/2402.11481",
    "context": "Title: DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics\nAbstract: arXiv:2402.11481v1 Announce Type: new  Abstract: Structured data offers a sophisticated mechanism for the organization of information. Existing methodologies for the text-serialization of structured data in the context of large language models fail to adequately address the heterogeneity inherent in key-value structured data. These methods are not ideal and frequently result in larger input sizes and poor adaptability to input changes. In this paper, we introduce DictLLM, an innovative framework designed to improve the modeling of key-value structured data, like medical laboratory reports, for generating medical diagnoses. DictLLM integrates three key components: (1) group positional encoding to maintain permutation invariance, (2) hierarchical attention bias to capture the inherent bias in structured data, and (3) an optimal transport alignment layer that aligns the embedding generated by the dictionary encoder with the LLM, thereby producing a sequence of fixed-length virtual tokens.",
    "path": "papers/24/02/2402.11481.json",
    "total_tokens": 858,
    "translated_title": "DictLLM: 利用大型语言模型操纵键值数据结构以增强医学诊断",
    "translated_abstract": "结构化数据提供了一种复杂的信息组织机制。在大型语言模型的文本序列化结构化数据的现有方法未能充分解决键值结构化数据固有的异质性问题。这些方法并不理想，经常导致更大的输入尺寸和对输入更改的适应性较差。在本文中，我们介绍了DictLLM，这是一个创新性框架，旨在改进键值结构化数据（如医学实验室报告）的建模，以生成医学诊断。DictLLM整合了三个关键组件：（1）组位置编码以保持置换不变性，（2）层次注意偏差以捕捉结构化数据的固有偏差，以及（3）一个优化传输对齐层，将字典编码器生成的嵌入与LLM对齐，从而产生一系列固定长度的虚拟标记。",
    "tldr": "DictLLM是一个创新性框架，旨在改进键值结构化数据的建模，用于生成医学诊断。它包括组位置编码、层次注意偏差和优化传输对齐层。"
}