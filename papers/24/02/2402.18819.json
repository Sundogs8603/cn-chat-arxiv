{
    "title": "Dual Operating Modes of In-Context Learning",
    "abstract": "arXiv:2402.18819v1 Announce Type: new  Abstract: In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time. We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously. Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples. Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution",
    "link": "https://arxiv.org/abs/2402.18819",
    "context": "Title: Dual Operating Modes of In-Context Learning\nAbstract: arXiv:2402.18819v1 Announce Type: new  Abstract: In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time. We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously. Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples. Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution",
    "path": "papers/24/02/2402.18819.json",
    "total_tokens": 875,
    "translated_title": "In-Context Learning的双重运行模式",
    "translated_abstract": "In-Context Learning (ICL)展示了双重运行模式：任务学习，即从上下文样本中获取新技能，和任务检索，即查找并激活相关的预训练技能。最近的理论研究探讨了各种数学模型来分析ICL，但现有模型一次只能解释一种操作模式。我们引入了一个概率模型，可以同时解释ICL的双重运行模式。专注于线性函数的上下文学习，通过引入多个任务组和任务相关的输入分布扩展现有模型用于预训练数据。然后分析在平方损失下表现最优的预训练模型的行为，即给定上下文样本标签的最小均方误差估计器。将预训练任务分布视为先验，将上下文示例视为观测值，我们推导出任务后验分布的闭式表达式。",
    "tldr": "该论文介绍了In-Context Learning的双重运行模式，通过引入概率模型同时解释了任务学习和任务检索，对线性函数的上下文学习进行了扩展，分析了优化预训练模型在平方损失下的行为，并推导出了任务后验分布的闭式表达式。",
    "en_tdlr": "This paper introduces the dual operating modes of In-Context Learning by introducing a probabilistic model that explains both task learning and task retrieval simultaneously, extends the in-context learning of linear functions, analyzes the behavior of the optimally pretrained model under squared loss, and derives the closed-form expression of the task posterior distribution."
}