{
    "title": "Attacking Large Language Models with Projected Gradient Descent",
    "abstract": "arXiv:2402.09154v1 Announce Type: new Abstract: Current LLM alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.",
    "link": "https://arxiv.org/abs/2402.09154",
    "context": "Title: Attacking Large Language Models with Projected Gradient Descent\nAbstract: arXiv:2402.09154v1 Announce Type: new Abstract: Current LLM alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.",
    "path": "papers/24/02/2402.09154.json",
    "total_tokens": 789,
    "translated_title": "用投影梯度下降攻击大型语言模型",
    "translated_abstract": "当前的大型语言模型对特定设计的对抗性提示很容易被破解。虽然使用离散优化制作对抗性提示非常有效，但这种攻击通常需要超过100,000次的语言模型调用。这种高计算成本使得它们不适用于定量分析和对抗性训练。为了解决这个问题，我们重新考虑了对连续松弛的输入提示使用投影梯度下降（PGD）的方法。尽管先前使用普通梯度攻击的尝试基本失败，但我们表明，仔细控制连续松弛引入的误差极大地提升了它们的效力。我们的LLMs的PGD速度比最先进的离散优化快一个数量级，以达到相同的毁灭性攻击结果。",
    "tldr": "本研究通过使用投影梯度下降方法，以连续松弛的输入提示来攻击大型语言模型，取得了比离散优化更快的速度，实现了相同的毁灭性攻击效果。",
    "en_tdlr": "This study attacks large language models using Projected Gradient Descent (PGD) on continuously relaxed input prompts, achieving faster speeds than discrete optimization while achieving the same devastating attack results."
}