{
    "title": "ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update",
    "abstract": "In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL). DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning. However, they typically perform much worse than current state-of-the-art (SOTA) methods that solely use action-level behavior constraint. After revisiting DICE-based methods, we find there exist two gradient terms when learning the value function using true-gradient update: forward gradient (taken on the current state) and backward gradient (taken on the next state). Using forward gradient bears a large similarity to many offline RL methods, and thus can be regarded as applying action-level constraint. However, directly adding the backward gradient may degenerate or cancel out its effect if these two gradients have conflicting directions. To resolve this issue, we propose a simple yet effective modi",
    "link": "https://arxiv.org/abs/2402.00348",
    "context": "Title: ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update\nAbstract: In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL). DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning. However, they typically perform much worse than current state-of-the-art (SOTA) methods that solely use action-level behavior constraint. After revisiting DICE-based methods, we find there exist two gradient terms when learning the value function using true-gradient update: forward gradient (taken on the current state) and backward gradient (taken on the next state). Using forward gradient bears a large similarity to many offline RL methods, and thus can be regarded as applying action-level constraint. However, directly adding the backward gradient may degenerate or cancel out its effect if these two gradients have conflicting directions. To resolve this issue, we propose a simple yet effective modi",
    "path": "papers/24/02/2402.00348.json",
    "total_tokens": 921,
    "translated_title": "ODICE: 通过正交梯度更新揭示分布校正估计的奥秘",
    "translated_abstract": "在这项研究中，我们调查了分布校正估计（DICE）方法，这是离线强化学习（RL）和模仿学习（IL）中重要的研究方向。基于DICE的方法对状态行为级别的行为约束施加了，这对于离线学习是一个理想的选择。然而，它们通常比仅使用动作级别行为约束的当前最先进方法表现得更差。在重新审视了基于DICE的方法后，我们发现在使用真梯度更新学习值函数时存在两个梯度项：前向梯度（在当前状态上）和后向梯度（在下一个状态上）。使用前向梯度与许多离线RL方法有很大的相似之处，因此可以被视为应用动作级别约束。然而，如果这两个梯度有相互冲突的方向，直接加上后向梯度可能会退化或抵消其效果。为了解决这个问题，我们提出了一个简单但有效的修正方法。",
    "tldr": "ODICE研究了离线强化学习和模仿学习中重要的分布校正估计（DICE）方法，并发现在使用真梯度更新学习值函数时存在前向梯度和后向梯度两个梯度项。为了解决这个问题，他们提出了一种简单但有效的修正方法。"
}