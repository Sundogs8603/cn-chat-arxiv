{
    "title": "RelayAttention for Efficient Large Language Model Serving with Long System Prompts",
    "abstract": "arXiv:2402.14808v1 Announce Type: new  Abstract: Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once ",
    "link": "https://arxiv.org/abs/2402.14808",
    "context": "Title: RelayAttention for Efficient Large Language Model Serving with Long System Prompts\nAbstract: arXiv:2402.14808v1 Announce Type: new  Abstract: Practical large language model (LLM) services may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across numerous requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t. the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once ",
    "path": "papers/24/02/2402.14808.json",
    "total_tokens": 837,
    "translated_title": "RelayAttention：用于高效实现大型语言模型服务与长系统提示的论文",
    "translated_abstract": "实际的大型语言模型（LLM）服务可能涉及一个长的系统提示，其中包含任务的指示、示例和知识文档，并在许多请求中复用。然而，长系统提示会导致吞吐量/延迟瓶颈，因为生成下一个标记的成本随着序列长度的增长而增加。本文旨在提高涉及长系统提示的LLM服务的效率。我们的关键观察是，处理这些系统提示在现有因果注意力计算算法中需要大量冗余的内存访问。具体来说，对于批量请求，系统提示的缓存隐藏状态（即键-值对）被多次从芯片外的DRAM传输到芯片上的SRAM，每次对应一个单独的请求。为了消除这种冗余，我们提出了RelayAttention，一种注意力算法，允许仅从DRAM读取这些隐藏状态一次。",
    "tldr": "本论文提出的RelayAttention算法旨在改善涉及长系统提示的大型语言模型服务的效率，通过一次性从DRAM读取隐藏状态来消除现有因果注意力算法中的内存访问冗余。",
    "en_tdlr": "The proposed RelayAttention algorithm aims to improve the efficiency of large language model services with long system prompts by eliminating memory access redundancy in existing causal attention algorithms through reading hidden states from DRAM once."
}