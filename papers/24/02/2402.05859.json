{
    "title": "Learning to Route Among Specialized Experts for Zero-Shot Generalization",
    "abstract": "Recently, there has been a widespread proliferation of \"expert\" language models that are specialized to a specific task or domain through parameter-efficient fine-tuning. How can we recycle large collections of expert language models to improve zero-shot generalization to unseen tasks? In this work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), which learns to route among specialized modules that were produced through parameter-efficient fine-tuning. Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model. Crucially, our method is post-hoc - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained. In experiments covering a range ",
    "link": "https://arxiv.org/abs/2402.05859",
    "context": "Title: Learning to Route Among Specialized Experts for Zero-Shot Generalization\nAbstract: Recently, there has been a widespread proliferation of \"expert\" language models that are specialized to a specific task or domain through parameter-efficient fine-tuning. How can we recycle large collections of expert language models to improve zero-shot generalization to unseen tasks? In this work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), which learns to route among specialized modules that were produced through parameter-efficient fine-tuning. Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model. Crucially, our method is post-hoc - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained. In experiments covering a range ",
    "path": "papers/24/02/2402.05859.json",
    "total_tokens": 826,
    "translated_title": "学习通过专家路由来实现零样本泛化",
    "translated_abstract": "近年来，“专家”语言模型的广泛应用通过参数有效的微调，使其专门用于特定的任务或领域。我们如何重用大量的专家语言模型来提高在未见任务上的零样本泛化能力呢？在这项工作中，我们提出后续自适应逐标记门控机制，它通过学习路由于通过参数有效微调生成的专家模块之间。与过去学习在专业模型之间路由的方法不同，后续自适应逐标记门控机制探讨了如果通过对每个令牌和模型中的每个层进行自适应选择不同的专家，零样本泛化是否会得到改善的可能性。关键是，我们的方法是后续的，不需要同时访问用于创建专业模型的数据集，而且在训练每个专家模型后只需要适量的额外计算。",
    "tldr": "提出了后续自适应逐标记门控机制（PHATGOOSE），通过学习路由于参数有效微调生成的专家模块之间，从而提高在未见任务上的零样本泛化能力。",
    "en_tdlr": "Introducing PHATGOOSE, a post-hoc adaptive tokenwise gating mechanism that improves zero-shot generalization by learning to route among specialized modules produced through parameter-efficient fine-tuning."
}