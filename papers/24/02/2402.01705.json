{
    "title": "Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation",
    "abstract": "Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement ",
    "link": "https://arxiv.org/abs/2402.01705",
    "context": "Title: Beyond Behaviorist Representational Harms: A Plan for Measurement and Mitigation\nAbstract: Algorithmic harms are commonly categorized as either allocative or representational. This study specifically addresses the latter, focusing on an examination of current definitions of representational harms to discern what is included and what is not. This analysis motivates our expansion beyond behavioral definitions to encompass harms to cognitive and affective states. The paper outlines high-level requirements for measurement: identifying the necessary expertise to implement this approach and illustrating it through a case study. Our work highlights the unique vulnerabilities of large language models to perpetrating representational harms, particularly when these harms go unmeasured and unmitigated. The work concludes by presenting proposed mitigations and delineating when to employ them. The overarching aim of this research is to establish a framework for broadening the definition of representational harms and to translate insights from fairness research into practical measurement ",
    "path": "papers/24/02/2402.01705.json",
    "total_tokens": 875,
    "translated_title": "超越行为主义的表征伤害：度量和减轻计划",
    "translated_abstract": "算法伤害通常被分为配置性或表征性。本研究专门针对后者，重点在于对当前表征性伤害定义的审查，以确定其中包含什么和不包含什么。这个分析促使我们扩展超越行为主义的定义范围，包括对认知和情感状态的伤害。本文概述了度量的高级要求：确定实施这种方法所需的专业知识，并通过案例研究进行说明。我们的工作凸显了大型语言模型在实施表征性伤害时的独特脆弱性，特别是当这些伤害未被度量和减轻时。该研究通过提出减轻措施并界定何时使用它们来结束。这项研究的总体目标是建立一个框架，扩大表征性伤害的定义，并将公平研究的见解转化为实际的度量方法。",
    "tldr": "本研究超越行为主义的定义范围，提出了一种度量和减轻表征性伤害的框架，强调了大型语言模型在实施这些伤害时的脆弱性，并提出了减轻措施的建议。",
    "en_tdlr": "This study expands the definition of representational harms beyond behaviorist perspectives, proposes a framework for measuring and mitigating these harms, highlights the vulnerability of large language models in perpetrating such harms, and presents proposed mitigations."
}