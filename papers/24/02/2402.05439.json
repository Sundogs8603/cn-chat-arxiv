{
    "title": "Learning Uncertainty-Aware Temporally-Extended Actions",
    "abstract": "In reinforcement learning, temporal abstraction in the action space, exemplified by action repetition, is a technique to facilitate policy learning through extended actions. However, a primary limitation in previous studies of action repetition is its potential to degrade performance, particularly when sub-optimal actions are repeated. This issue often negates the advantages of action repetition. To address this, we propose a novel algorithm named Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to accurately measure uncertainty during action extension. This feature allows policies to strategically choose between emphasizing exploration or adopting an uncertainty-averse approach, tailored to their specific needs. We demonstrate the effectiveness of UTE through experiments in Gridworld and Atari 2600 environments. Our findings show that UTE outperforms existing action repetition algorithms, effectively mitigating their inherent limitations and significantly enhan",
    "link": "https://arxiv.org/abs/2402.05439",
    "context": "Title: Learning Uncertainty-Aware Temporally-Extended Actions\nAbstract: In reinforcement learning, temporal abstraction in the action space, exemplified by action repetition, is a technique to facilitate policy learning through extended actions. However, a primary limitation in previous studies of action repetition is its potential to degrade performance, particularly when sub-optimal actions are repeated. This issue often negates the advantages of action repetition. To address this, we propose a novel algorithm named Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to accurately measure uncertainty during action extension. This feature allows policies to strategically choose between emphasizing exploration or adopting an uncertainty-averse approach, tailored to their specific needs. We demonstrate the effectiveness of UTE through experiments in Gridworld and Atari 2600 environments. Our findings show that UTE outperforms existing action repetition algorithms, effectively mitigating their inherent limitations and significantly enhan",
    "path": "papers/24/02/2402.05439.json",
    "total_tokens": 854,
    "translated_title": "学习不确定性感知的时间扩展动作",
    "translated_abstract": "在强化学习中，动作空间中的时间抽象，例如动作重复，是一种通过扩展动作促进策略学习的技术。然而，以前的动作重复研究存在一个主要限制，即当重复次优动作时可能降低性能。这个问题经常抵消了动作重复的优势。为了解决这个问题，我们提出了一种名为不确定性感知时间扩展（UTE）的新算法。UTE使用集成方法在动作扩展期间准确地测量不确定性。这个特性允许策略根据其特定需求，在强调探索或采取不确定性-抵制方法之间进行选择。我们通过在Gridworld和Atari 2600环境中进行实验展示了UTE的有效性。我们的研究结果表明，UTE优于现有的动作重复算法，有效地缓解了它们固有的局限性，并显著提高了效果。",
    "tldr": "我们提出了一种名为不确定性感知时间扩展（UTE）的算法，在强化学习中解决了动作重复可能降低性能的问题，通过测量不确定性从而让策略根据需求进行选择，实验证明UTE优于现有算法。"
}