{
    "title": "Training Artificial Neural Networks by Coordinate Search Algorithm",
    "abstract": "arXiv:2402.12646v1 Announce Type: cross  Abstract: Training Artificial Neural Networks poses a challenging and critical problem in machine learning. Despite the effectiveness of gradient-based learning methods, such as Stochastic Gradient Descent (SGD), in training neural networks, they do have several limitations. For instance, they require differentiable activation functions, and cannot optimize a model based on several independent non-differentiable loss functions simultaneously; for example, the F1-score, which is used during testing, can be used during training when a gradient-free optimization algorithm is utilized. Furthermore, the training in any DNN can be possible with a small size of the training dataset. To address these concerns, we propose an efficient version of the gradient-free Coordinate Search (CS) algorithm, an instance of General Pattern Search methods, for training neural networks. The proposed algorithm can be used with non-differentiable activation functions and",
    "link": "https://arxiv.org/abs/2402.12646",
    "context": "Title: Training Artificial Neural Networks by Coordinate Search Algorithm\nAbstract: arXiv:2402.12646v1 Announce Type: cross  Abstract: Training Artificial Neural Networks poses a challenging and critical problem in machine learning. Despite the effectiveness of gradient-based learning methods, such as Stochastic Gradient Descent (SGD), in training neural networks, they do have several limitations. For instance, they require differentiable activation functions, and cannot optimize a model based on several independent non-differentiable loss functions simultaneously; for example, the F1-score, which is used during testing, can be used during training when a gradient-free optimization algorithm is utilized. Furthermore, the training in any DNN can be possible with a small size of the training dataset. To address these concerns, we propose an efficient version of the gradient-free Coordinate Search (CS) algorithm, an instance of General Pattern Search methods, for training neural networks. The proposed algorithm can be used with non-differentiable activation functions and",
    "path": "papers/24/02/2402.12646.json",
    "total_tokens": 793,
    "translated_title": "通过坐标搜索算法训练人工神经网络",
    "translated_abstract": "训练人工神经网络在机器学习中是一个具有挑战性和关键性的问题。 尽管梯度下降等基于梯度的学习方法在训练神经网络方面有效，但它们也存在一些限制。 例如，它们需要可微激活函数，并且不能基于多个独立的非可微损失函数同时优化模型；例如，在测试期间使用的 F1 分数可以在训练期间使用，当采用无梯度优化算法时。 此外，任何 DNN 中的训练可能只需很少量的训练数据集。 为了解决这些问题，我们提出了非梯度坐标搜索（CS）算法的高效版本，它是通用模式搜索方法的一种实例，用于训练神经网络。",
    "tldr": "通过提出的高效版本的非梯度坐标搜索（CS）算法，我们可以训练神经网络，解决了需要可微激活函数和同时优化多个非可微损失函数的问题。",
    "en_tdlr": "By proposing an efficient version of the gradient-free Coordinate Search (CS) algorithm, we can train neural networks, addressing the need for differentiable activation functions and optimizing multiple non-differentiable loss functions simultaneously."
}