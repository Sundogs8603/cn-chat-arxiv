{
    "title": "Architectural Strategies for the optimization of Physics-Informed Neural Networks",
    "abstract": "Physics-informed neural networks (PINNs) offer a promising avenue for tackling both forward and inverse problems in partial differential equations (PDEs) by incorporating deep learning with fundamental physics principles. Despite their remarkable empirical success, PINNs have garnered a reputation for their notorious training challenges across a spectrum of PDEs. In this work, we delve into the intricacies of PINN optimization from a neural architecture perspective. Leveraging the Neural Tangent Kernel (NTK), our study reveals that Gaussian activations surpass several alternate activations when it comes to effectively training PINNs. Building on insights from numerical linear algebra, we introduce a preconditioned neural architecture, showcasing how such tailored architectures enhance the optimization process. Our theoretical findings are substantiated through rigorous validation against established PDEs within the scientific literature.",
    "link": "https://arxiv.org/abs/2402.02711",
    "context": "Title: Architectural Strategies for the optimization of Physics-Informed Neural Networks\nAbstract: Physics-informed neural networks (PINNs) offer a promising avenue for tackling both forward and inverse problems in partial differential equations (PDEs) by incorporating deep learning with fundamental physics principles. Despite their remarkable empirical success, PINNs have garnered a reputation for their notorious training challenges across a spectrum of PDEs. In this work, we delve into the intricacies of PINN optimization from a neural architecture perspective. Leveraging the Neural Tangent Kernel (NTK), our study reveals that Gaussian activations surpass several alternate activations when it comes to effectively training PINNs. Building on insights from numerical linear algebra, we introduce a preconditioned neural architecture, showcasing how such tailored architectures enhance the optimization process. Our theoretical findings are substantiated through rigorous validation against established PDEs within the scientific literature.",
    "path": "papers/24/02/2402.02711.json",
    "total_tokens": 937,
    "translated_title": "用于优化物理信息神经网络的架构策略",
    "translated_abstract": "物理信息神经网络（PINNs）通过将深度学习与基础物理原理结合起来，为解决偏微分方程（PDEs）中的前向和反向问题提供了一个有希望的途径。尽管PINNs在实践中取得了显著的成功，但它们在一系列PDEs的训练中也因其困难而声名狼藉。在这项工作中，我们从神经架构的角度深入探讨了PINN优化的复杂性。利用神经切向核（NTK），我们的研究揭示了在有效训练PINNs时，高斯激活函数优于几种替代激活函数。借鉴数值线性代数的见解，我们引入了一种经过预处理的神经架构，展示了这种定制架构如何增强优化过程。我们的理论发现通过对科学文献中已有的PDEs进行严格验证得到了证实。",
    "tldr": "本研究从神经架构的角度研究了物理信息神经网络（PINNs）的优化。通过利用神经切向核（NTK），我们发现高斯激活函数在有效训练PINNs时超过其他替代激活函数。此外，我们引入了一种经过预处理的神经架构，进一步增强了优化过程。这些发现在多个PDEs的验证中得到了证实。",
    "en_tdlr": "This study investigates the optimization of Physics-Informed Neural Networks (PINNs) from a neural architecture perspective. By leveraging the Neural Tangent Kernel (NTK), it is found that Gaussian activations outperform alternative activations in effectively training PINNs. Additionally, a preconditioned neural architecture is introduced to enhance the optimization process. These findings are rigorously validated against various established PDEs."
}