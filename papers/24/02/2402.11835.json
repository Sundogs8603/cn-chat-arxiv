{
    "title": "Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization",
    "abstract": "arXiv:2402.11835v1 Announce Type: new  Abstract: We propose ABCs (Adaptive Branching through Child stationarity), a best-of-both-worlds algorithm combining Boltzmann Q-learning (BQL), a classic reinforcement learning algorithm for single-agent domains, and counterfactual regret minimization (CFR), a central algorithm for learning in multi-agent domains. ABCs adaptively chooses what fraction of the environment to explore each iteration by measuring the stationarity of the environment's reward and transition dynamics. In Markov decision processes, ABCs converges to the optimal policy with at most an O(A) factor slowdown compared to BQL, where A is the number of actions in the environment. In two-player zero-sum games, ABCs is guaranteed to converge to a Nash equilibrium (assuming access to a perfect oracle for detecting stationarity), while BQL has no such guarantees. Empirically, ABCs demonstrates strong performance when benchmarked across environments drawn from the OpenSpiel game libr",
    "link": "https://arxiv.org/abs/2402.11835",
    "context": "Title: Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization\nAbstract: arXiv:2402.11835v1 Announce Type: new  Abstract: We propose ABCs (Adaptive Branching through Child stationarity), a best-of-both-worlds algorithm combining Boltzmann Q-learning (BQL), a classic reinforcement learning algorithm for single-agent domains, and counterfactual regret minimization (CFR), a central algorithm for learning in multi-agent domains. ABCs adaptively chooses what fraction of the environment to explore each iteration by measuring the stationarity of the environment's reward and transition dynamics. In Markov decision processes, ABCs converges to the optimal policy with at most an O(A) factor slowdown compared to BQL, where A is the number of actions in the environment. In two-player zero-sum games, ABCs is guaranteed to converge to a Nash equilibrium (assuming access to a perfect oracle for detecting stationarity), while BQL has no such guarantees. Empirically, ABCs demonstrates strong performance when benchmarked across environments drawn from the OpenSpiel game libr",
    "path": "papers/24/02/2402.11835.json",
    "total_tokens": 888,
    "translated_title": "简单如ABC：统一Boltzmann Q-Learning和反事实遗憾最小化",
    "translated_abstract": "我们提出了ABCs（通过子站点稳定性进行自适应分支），这是一种结合了Boltzmann Q-learning（一种经典的单一智能体领域强化学习算法）和反事实遗憾最小化（CFR）的双赢算法。ABCs通过测量环境奖励和转换动态的稳定性，自适应地选择每次迭代要探索环境的比例。在马尔可夫决策过程中，ABCs与BQL相比最多只慢一个O（A）因子就能收敛到最优策略，其中A是环境中的动作数量。在两人零和博弈中，ABCs有保证收敛到纳什均衡（假设可以访问一个完美的神谕来检测稳定性），而BQL没有这样的保证。从经验上看，ABCs在OpenSpiel游戏库环境中表现出强劲的性能。",
    "tldr": "ABCs算法结合了Boltzmann Q-learning和反事实遗憾最小化，通过测量环境稳定性自适应选择探索比例，在单一智能体和多智能体领域表现出色。",
    "en_tdlr": "The ABCs algorithm combines Boltzmann Q-learning and counterfactual regret minimization, adaptively choosing exploration ratios based on environment stability, showing strong performance in both single-agent and multi-agent domains."
}