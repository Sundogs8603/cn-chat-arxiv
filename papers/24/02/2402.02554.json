{
    "title": "DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers",
    "abstract": "Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification techniques have been proposed to address this issue. These techniques employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system's resources, while maintaining its stealthiness. Our e",
    "link": "https://arxiv.org/abs/2402.02554",
    "context": "Title: DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers\nAbstract: Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification techniques have been proposed to address this issue. These techniques employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system's resources, while maintaining its stealthiness. Our e",
    "path": "papers/24/02/2402.02554.json",
    "total_tokens": 911,
    "translated_title": "DeSparsify：对视觉Transformer中的Token稀疏化机制进行的对抗攻击",
    "translated_abstract": "视觉Transformer在计算机视觉领域做出了巨大贡献，展现出在各种任务（如图像分类、目标检测）中的最先进性能。然而，它们的高计算要求随使用的Token数量呈二次增长。为解决这个问题，提出了Token稀疏化技术。这些技术采用了一种依赖输入的策略，将无关的Token从计算流程中丢弃，提高模型的效率。然而，它们的动态性和平均情况假设使它们容易受到一种新的威胁 - 经过精心制作的对抗样本，能够欺骗稀疏化机制，导致最坏情况的性能。在本文中，我们提出了一种攻击方法DeSparsify，针对使用Token稀疏化机制的视觉Transformer的可用性。该攻击旨在耗尽操作系统的资源，同时保持隐蔽性。",
    "tldr": "本文提出了一种对抗攻击方法DeSparsify，针对使用Token稀疏化机制的视觉Transformer，通过精心制作的对抗样本欺骗稀疏化机制，导致最坏情况的性能，以此耗尽操作系统的资源并保持隐蔽性。",
    "en_tdlr": "This paper proposes an adversarial attack method, DeSparsify, targeting vision transformers that use token sparsification mechanisms. It exhausts the operating system's resources and maintains stealthiness by crafting adversarial examples that deceive the sparsification mechanism, leading to worst-case performance."
}