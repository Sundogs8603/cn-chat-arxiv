{
    "title": "Studying LLM Performance on Closed- and Open-source Data",
    "abstract": "arXiv:2402.15100v1 Announce Type: cross  Abstract: Large Language models (LLMs) are finding wide use in software engineering practice. These models are extremely data-hungry, and are largely trained on open-source (OSS) code distributed with permissive licenses. In terms of actual use however, a great deal of software development still occurs in the for-profit/proprietary sphere, where the code under development is not, and never has been, in the public domain; thus, many developers, do their work, and use LLMs, in settings where the models may not be as familiar with the code under development. In such settings, do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds? In this paper, we examine this issue using proprietary, closed-source software data from Microsoft, where most proprietary code is in C# and C++. We find that performance for C# changes little from OSS --> proprieta",
    "link": "https://arxiv.org/abs/2402.15100",
    "context": "Title: Studying LLM Performance on Closed- and Open-source Data\nAbstract: arXiv:2402.15100v1 Announce Type: cross  Abstract: Large Language models (LLMs) are finding wide use in software engineering practice. These models are extremely data-hungry, and are largely trained on open-source (OSS) code distributed with permissive licenses. In terms of actual use however, a great deal of software development still occurs in the for-profit/proprietary sphere, where the code under development is not, and never has been, in the public domain; thus, many developers, do their work, and use LLMs, in settings where the models may not be as familiar with the code under development. In such settings, do LLMs work as well as they do for OSS code? If not, what are the differences? When performance differs, what are the possible causes, and are there work-arounds? In this paper, we examine this issue using proprietary, closed-source software data from Microsoft, where most proprietary code is in C# and C++. We find that performance for C# changes little from OSS --> proprieta",
    "path": "papers/24/02/2402.15100.json",
    "total_tokens": 839,
    "translated_title": "研究LLM在闭源和开源数据上的性能表现",
    "translated_abstract": "大型语言模型（LLMs）在软件工程实践中得到广泛应用。这些模型对数据需求极高，主要是在具有宽松许可的开源（OSS）代码上进行训练。然而，实际使用中，很多软件开发仍然发生在盈利/专有领域，其中正在开发的代码不是也从未在公共领域中。因此，许多开发人员在LLMs在不熟悉正在开发的代码的情况下进行工作和使用。在这样的情况下，LLMs的性能是否和在OSS代码上一样好呢？若不是，有什么不同？当性能不同时，可能的原因是什么，是否有解决方法？本文使用来自微软的专有闭源软件数据作为研究对象，大部分专有代码采用C#和C++。我们发现C#的性能从OSS --> proprietary变化不大。",
    "tldr": "本文研究了LLM在闭源和开源数据上的性能表现，发现在闭源软件数据上，C#的性能变化不大。",
    "en_tdlr": "This paper studies the performance of LLM on closed-source and open-source data, and finds that the performance of C# changes little on closed-source software data."
}