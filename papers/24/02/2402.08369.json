{
    "title": "One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill",
    "abstract": "One-shot imitation is to learn a new task from a single demonstration, yet it is a challenging problem to adopt it for complex tasks with the high domain diversity inherent in a non-stationary environment. To tackle the problem, we explore the compositionality of complex tasks, and present a novel skill-based imitation learning framework enabling one-shot imitation and zero-shot adaptation; from a single demonstration for a complex unseen task, a semantic skill sequence is inferred and then each skill in the sequence is converted into an action sequence optimized for environmental hidden dynamics that can vary over time. Specifically, we leverage a vision-language model to learn a semantic skill set from offline video datasets, where each skill is represented on the vision-language embedding space, and adapt meta-learning with dynamics inference to enable zero-shot skill adaptation. We evaluate our framework with various one-shot imitation scenarios for extended multi-stage Meta-world ",
    "link": "https://arxiv.org/abs/2402.08369",
    "context": "Title: One-shot Imitation in a Non-Stationary Environment via Multi-Modal Skill\nAbstract: One-shot imitation is to learn a new task from a single demonstration, yet it is a challenging problem to adopt it for complex tasks with the high domain diversity inherent in a non-stationary environment. To tackle the problem, we explore the compositionality of complex tasks, and present a novel skill-based imitation learning framework enabling one-shot imitation and zero-shot adaptation; from a single demonstration for a complex unseen task, a semantic skill sequence is inferred and then each skill in the sequence is converted into an action sequence optimized for environmental hidden dynamics that can vary over time. Specifically, we leverage a vision-language model to learn a semantic skill set from offline video datasets, where each skill is represented on the vision-language embedding space, and adapt meta-learning with dynamics inference to enable zero-shot skill adaptation. We evaluate our framework with various one-shot imitation scenarios for extended multi-stage Meta-world ",
    "path": "papers/24/02/2402.08369.json",
    "total_tokens": 989,
    "translated_title": "在非平稳环境中通过多模态技能实现一次性模仿",
    "translated_abstract": "一次性模仿是从单个演示中学习新任务的方法，然而，在非平稳环境中采用它来解决复杂任务的挑战性问题是很困难的，因为这种环境具有高域多样性。为了解决这个问题，我们探索了复杂任务的组合性，并提出了一种新颖的基于技能的模仿学习框架，实现一次性模仿和零次适应；通过单个演示来推断复杂的未见任务中的语义技能序列，然后将序列中的每个技能转化为经过优化的行动序列，以适应可能随时间变化的环境隐含动力学。具体而言，我们利用视觉语言模型从离线视频数据集中学习语义技能集，其中每个技能在视觉语言嵌入空间上表示，并利用动力学推理的元学习实现零次技能适应。我们使用各种一次性模仿场景对我们的框架进行评估，用于扩展的多阶段元世界。",
    "tldr": "本研究提出一种基于技能的模仿学习框架，在非平稳环境中实现一次性模仿和零次适应。通过从单个演示中推断出语义技能序列，并将其转换为经过优化的行动序列，以适应环境中隐含的动力学变化。实验结果表明，该框架在不同的一次性模仿场景下表现出良好的性能。",
    "en_tdlr": "This study proposes a skill-based imitation learning framework that enables one-shot imitation and zero-shot adaptation in a non-stationary environment. By inferring a semantic skill sequence from a single demonstration and converting it into an optimized action sequence, the framework adapts to the hidden dynamics of the environment that can vary over time. Experimental results demonstrate the framework's strong performance in various one-shot imitation scenarios."
}