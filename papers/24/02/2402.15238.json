{
    "title": "GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?",
    "abstract": "arXiv:2402.15238v1 Announce Type: new  Abstract: Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind \"You are just a [slur] to me.\" However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs). We employ an additional natural language inference (NLI) model to verify the generations",
    "link": "https://arxiv.org/abs/2402.15238",
    "context": "Title: GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?\nAbstract: arXiv:2402.15238v1 Announce Type: new  Abstract: Online hate detection suffers from biases incurred in data sampling, annotation, and model pre-training. Therefore, measuring the averaged performance over all examples in held-out test data is inadequate. Instead, we must identify specific model weaknesses and be informed when it is more likely to fail. A recent proposal in this direction is HateCheck, a suite for testing fine-grained model functionalities on synthesized data generated using templates of the kind \"You are just a [slur] to me.\" However, despite enabling more detailed diagnostic insights, the HateCheck test cases are often generic and have simplistic sentence structures that do not match the real-world data. To address this limitation, we propose GPT-HateCheck, a framework to generate more diverse and realistic functional tests from scratch by instructing large language models (LLMs). We employ an additional natural language inference (NLI) model to verify the generations",
    "path": "papers/24/02/2402.15238.json",
    "total_tokens": 883,
    "translated_title": "GPT-HateCheck: LLMs能否为仇恨言论检测编写更好的功能测试？",
    "translated_abstract": "在线仇恨检测受到数据采样、注释和模型预训练中引入的偏见的影响。因此，仅测量在留存测试数据中所有示例上的平均性能是不足够的。相反，我们必须识别特定模型的弱点，并在其更有可能失败时获得信息。本文提出了一个最近的方向，即HateCheck，这是一个用于在使用模板生成的合成数据上测试精细粒度模型功能的套件，模板的形式为“你只是一个[骂人的词]对我来说”。然而，尽管HateCheck允许获得更详细的诊断见解，但其测试用例通常是通用的，并且具有简单的句子结构，不符合真实世界数据。为解决这一局限性，我们提出了GPT-HateCheck，一个框架，通过指导大型语言模型（LLMs）从头开始生成更多样化和现实的功能测试。我们采用额外的自然语言推理（NLI）模型来验证生成。",
    "tldr": "GPT-HateCheck提出了一个框架，通过指导大型语言模型从头开始生成更多样化和现实的功能测试，以解决现有测试案例过于通用简单的问题。",
    "en_tdlr": "GPT-HateCheck proposes a framework to generate more diverse and realistic functional tests from scratch by instructing large language models, addressing the issue of existing test cases being too generic and simplistic."
}