{
    "title": "Low-Rank Extragradient Methods for Scalable Semidefinite Optimization",
    "abstract": "arXiv:2402.09081v1 Announce Type: cross Abstract: We consider several classes of highly important semidefinite optimization problems that involve both a convex objective function (smooth or nonsmooth) and additional linear or nonlinear smooth and convex constraints, which are ubiquitous in statistics, machine learning, combinatorial optimization, and other domains. We focus on high-dimensional and plausible settings in which the problem admits a low-rank solution which also satisfies a low-rank complementarity condition. We provide several theoretical results proving that, under these circumstances, the well-known Extragradient method, when initialized in the proximity of an optimal primal-dual solution, converges to a solution of the constrained optimization problem with its standard convergence rates guarantees, using only low-rank singular value decompositions (SVD) to project onto the positive semidefinite cone, as opposed to computationally-prohibitive full-rank SVDs required in w",
    "link": "https://arxiv.org/abs/2402.09081",
    "context": "Title: Low-Rank Extragradient Methods for Scalable Semidefinite Optimization\nAbstract: arXiv:2402.09081v1 Announce Type: cross Abstract: We consider several classes of highly important semidefinite optimization problems that involve both a convex objective function (smooth or nonsmooth) and additional linear or nonlinear smooth and convex constraints, which are ubiquitous in statistics, machine learning, combinatorial optimization, and other domains. We focus on high-dimensional and plausible settings in which the problem admits a low-rank solution which also satisfies a low-rank complementarity condition. We provide several theoretical results proving that, under these circumstances, the well-known Extragradient method, when initialized in the proximity of an optimal primal-dual solution, converges to a solution of the constrained optimization problem with its standard convergence rates guarantees, using only low-rank singular value decompositions (SVD) to project onto the positive semidefinite cone, as opposed to computationally-prohibitive full-rank SVDs required in w",
    "path": "papers/24/02/2402.09081.json",
    "total_tokens": 858,
    "translated_title": "低秩外推梯度方法用于可扩展的半定规划问题",
    "translated_abstract": "我们考虑了几类非常重要的半定规划问题，这些问题既包括凸目标函数（平滑或非平滑），又包括额外的线性或非线性平滑凸约束，这些问题在统计学、机器学习、组合优化等领域都很常见。我们关注高维和可能的情境，其中问题具有低秩解，并满足低秩互补条件。我们提供了几个理论结果，证明在这些情况下，已知的外推梯度方法，在接近最优原始对偶解的情况下初始化时，收敛于带有标准收敛速度保证的约束优化问题的解，仅使用低秩奇异值分解（SVD）来投影到半正定锥，而不是计算上限的全秩SVD所需的操作。",
    "tldr": "本文研究了低秩外推梯度方法在可扩展半定规划问题上的应用，通过使用低秩奇异值分解来投影到半正定锥，取得了收敛于约束优化问题解的理论结果。",
    "en_tdlr": "This paper investigates the application of low-rank extragradient methods on scalable semidefinite optimization problems. Theoretical results are presented showing that by using low-rank singular value decompositions to project onto the positive semidefinite cone, the method converges to the solution of the constrained optimization problem."
}