{
    "title": "Entropy-regularized Point-based Value Iteration",
    "abstract": "arXiv:2402.09388v1 Announce Type: new Abstract: Model-based planners for partially observable problems must accommodate both model uncertainty during planning and goal uncertainty during objective inference. However, model-based planners may be brittle under these types of uncertainty because they rely on an exact model and tend to commit to a single optimal behavior. Inspired by results in the model-free setting, we propose an entropy-regularized model-based planner for partially observable problems. Entropy regularization promotes policy robustness for planning and objective inference by encouraging policies to be no more committed to a single action than necessary. We evaluate the robustness and objective inference performance of entropy-regularized policies in three problem domains. Our results show that entropy-regularized policies outperform non-entropy-regularized baselines in terms of higher expected returns under modeling errors and higher accuracy during objective inference.",
    "link": "https://arxiv.org/abs/2402.09388",
    "context": "Title: Entropy-regularized Point-based Value Iteration\nAbstract: arXiv:2402.09388v1 Announce Type: new Abstract: Model-based planners for partially observable problems must accommodate both model uncertainty during planning and goal uncertainty during objective inference. However, model-based planners may be brittle under these types of uncertainty because they rely on an exact model and tend to commit to a single optimal behavior. Inspired by results in the model-free setting, we propose an entropy-regularized model-based planner for partially observable problems. Entropy regularization promotes policy robustness for planning and objective inference by encouraging policies to be no more committed to a single action than necessary. We evaluate the robustness and objective inference performance of entropy-regularized policies in three problem domains. Our results show that entropy-regularized policies outperform non-entropy-regularized baselines in terms of higher expected returns under modeling errors and higher accuracy during objective inference.",
    "path": "papers/24/02/2402.09388.json",
    "total_tokens": 848,
    "translated_title": "熵正则化的基于点的价值迭代",
    "translated_abstract": "面对模型不确定性和目标不确定性，部分可观测问题的基于模型的规划器必须适应两者。然而，这些类型的不确定性可能导致基于模型的规划器变得脆弱，因为它们依赖于精确的模型并倾向于承诺一个单一的最优行为。受到无模型设置中的结果的启发，我们提出了一种熵正则化的基于模型的规划器来解决部分可观测问题。熵正则化通过鼓励策略在规划和目标推理中尽可能不承诺一个单一的动作来促进策略的鲁棒性。我们评估了三个问题领域中熵正则化策略的鲁棒性和目标推理性能。我们的结果表明，在建模错误和目标推理准确性方面，熵正则化策略优于非熵正则化的基准策略。",
    "tldr": "在部分可观测问题领域，我们提出了一种熵正则化的基于模型的规划器，在规划和目标推理中通过鼓励策略不承诺一个单一的动作来提高鲁棒性和目标推理性能。",
    "en_tdlr": "In the domain of partially observable problems, we propose an entropy-regularized model-based planner that improves robustness and objective inference performance by encouraging policies not to commit to a single action."
}