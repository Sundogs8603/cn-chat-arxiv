{
    "title": "FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint",
    "abstract": "arXiv:2402.18605v1 Announce Type: new  Abstract: Meta-learning problem is usually formulated as a bi-level optimization in which the task-specific and the meta-parameters are updated in the inner and outer loops of optimization, respectively. However, performing the optimization in the Riemannian space, where the parameters and meta-parameters are located on Riemannian manifolds is computationally intensive. Unlike the Euclidean methods, the Riemannian backpropagation needs computing the second-order derivatives that include backward computations through the Riemannian operators such as retraction and orthogonal projection. This paper introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold. Our method significantly reduces the computational load and memory footprint. We show how using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbon",
    "link": "https://arxiv.org/abs/2402.18605",
    "context": "Title: FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint\nAbstract: arXiv:2402.18605v1 Announce Type: new  Abstract: Meta-learning problem is usually formulated as a bi-level optimization in which the task-specific and the meta-parameters are updated in the inner and outer loops of optimization, respectively. However, performing the optimization in the Riemannian space, where the parameters and meta-parameters are located on Riemannian manifolds is computationally intensive. Unlike the Euclidean methods, the Riemannian backpropagation needs computing the second-order derivatives that include backward computations through the Riemannian operators such as retraction and orthogonal projection. This paper introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold. Our method significantly reduces the computational load and memory footprint. We show how using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbon",
    "path": "papers/24/02/2402.18605.json",
    "total_tokens": 840,
    "translated_title": "FORML：一种具有正交约束的流形海森自由元学习方法",
    "translated_abstract": "元学习问题通常被表述为一个双层优化问题，其中任务特定参数和元参数分别在优化的内部和外部循环中进行更新。然而，在黎曼空间中进行优化，参数和元参数位于黎曼流形上，这在计算上是非常密集的。与欧几里德方法不同，黎曼反向传播需要计算包括通过黎曼算子（如收缩和正交投影）的二阶导数。本文介绍了一种使用斯蒂夫尔流形上的导数的一阶近似的海森自由方法。我们的方法显著减少了计算负载和内存占用。我们展示了如何使用一个斯蒂夫尔全连接层作为骨干网络的最后分类层参数上的正交约束。",
    "tldr": "该论文介绍了一种 FORML 方法，使用斯蒂夫尔流形上的一阶导数近似，通过引入海森自由方法来降低计算负担和内存占用，并在元学习中实现参数正交约束。"
}