{
    "title": "LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments",
    "abstract": "arXiv:2402.16499v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive ex",
    "link": "https://arxiv.org/abs/2402.16499",
    "context": "Title: LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments\nAbstract: arXiv:2402.16499v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive ex",
    "path": "papers/24/02/2402.16499.json",
    "total_tokens": 827,
    "translated_title": "在动态多代理环境中评估大型语言模型的能力的LLMArena",
    "translated_abstract": "最近，大型语言模型（LLMs）的进展揭示了它们实现拥有人类水平智能的自主代理的潜力。然而，用于评估LLM代理的现有基准要么使用静态数据集，可能导致数据泄漏，要么仅关注单一代理场景，忽略了多代理互动的复杂性。缺乏一个基准，可以评估LLM代理在多代理动态环境中的多样化能力。为此，我们引入了LLMArena，这是一个新颖且易于扩展的框架，用于评估LLM在多代理动态环境中的多样化能力。LLMArena包含七个不同的游戏环境，采用Trueskill评分来评估LLM代理中关键能力，包括空间推理、战略规划、数字推理、风险评估、沟通、对手建模和团队协作。我们进行了大量的实验。",
    "tldr": "LLMArena是一个新颖且易于扩展的框架，用于评估大型语言模型在多代理动态环境中的多样化能力。",
    "en_tdlr": "LLMArena is a novel and easily extensible framework for evaluating the diverse capabilities of large language models in dynamic multi-agent environments."
}