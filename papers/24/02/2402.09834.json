{
    "title": "All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining",
    "abstract": "arXiv:2402.09834v1 Announce Type: new  Abstract: Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approa",
    "link": "https://arxiv.org/abs/2402.09834",
    "context": "Title: All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining\nAbstract: arXiv:2402.09834v1 Announce Type: new  Abstract: Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approa",
    "path": "papers/24/02/2402.09834.json",
    "total_tokens": 902,
    "translated_title": "一体化与多功能性：一种简单而有效的跨领域图预训练方法",
    "translated_abstract": "大型语言模型（LLMs）已经在计算机视觉（CV）和自然语言处理（NLP）领域取得了重大突破。LLMs最显著的进展之一是，在广泛且多样化的数据集上训练了单一模型，这些数据跨越多个领域，这种范式被称为“一体化”。这种方法使LLMs具备了超强的泛化能力，有助于理解各种数据分布。借助这些能力，单一的LLM在各种领域展现出了出色的多功能性，这种范式被称为“多功能一体化”。然而，将这个想法应用于图领域仍然面临着巨大的挑战，跨领域预训练经常导致负迁移。这个问题在少样本学习场景中尤为重要，因为训练数据的匮乏需要引入外部知识源。为了应对这个挑战，我们提出了一种新颖的方法。",
    "tldr": "本研究提出了一种简单而有效的跨领域图预训练方法，通过一体化和多功能性，使得大型语言模型在各个领域具备了超强的泛化能力。",
    "en_tdlr": "This paper introduces a simple yet effective method for cross-domain graph pretraining, enabling large language models to have strong generalization capabilities across different domains through the concepts of \"all in one\" and \"one for all\"."
}