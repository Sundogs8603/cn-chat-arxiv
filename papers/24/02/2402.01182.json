{
    "title": "In-Context Learning for Few-Shot Nested Named Entity Recognition",
    "abstract": "In nested Named entity recognition (NER), entities are nested with each other, and thus requiring more data annotations to address. This leads to the development of few-shot nested NER, where the prevalence of pretrained language models with in-context learning (ICL) offers promising solutions. In this work, we introduce an effective and innovative ICL framework for the setting of few-shot nested NER. We improve the ICL prompt by devising a novel example demonstration selection mechanism, EnDe retriever. In EnDe retriever, we employ contrastive learning to perform three types of representation learning, in terms of semantic similarity, boundary similarity, and label similarity, to generate high-quality demonstration examples. Extensive experiments over three nested NER and four flat NER datasets demonstrate the efficacy of our system.",
    "link": "https://rss.arxiv.org/abs/2402.01182",
    "context": "Title: In-Context Learning for Few-Shot Nested Named Entity Recognition\nAbstract: In nested Named entity recognition (NER), entities are nested with each other, and thus requiring more data annotations to address. This leads to the development of few-shot nested NER, where the prevalence of pretrained language models with in-context learning (ICL) offers promising solutions. In this work, we introduce an effective and innovative ICL framework for the setting of few-shot nested NER. We improve the ICL prompt by devising a novel example demonstration selection mechanism, EnDe retriever. In EnDe retriever, we employ contrastive learning to perform three types of representation learning, in terms of semantic similarity, boundary similarity, and label similarity, to generate high-quality demonstration examples. Extensive experiments over three nested NER and four flat NER datasets demonstrate the efficacy of our system.",
    "path": "papers/24/02/2402.01182.json",
    "total_tokens": 850,
    "translated_title": "针对少样本嵌套命名实体识别的上下文学习",
    "translated_abstract": "在嵌套命名实体识别（NER）中，实体与实体相互嵌套，因此需要更多的数据注释来解决。这促使发展出少样本嵌套NER，其中具有上下文学习（ICL）的预训练语言模型的普及提供了有前途的解决方案。在这项工作中，我们引入了一个有效和创新的ICL框架，用于少样本嵌套NER的设定。我们通过设计一种新颖的示例演示选择机制EnDe retriever改进了ICL提示。在EnDe检索器中，我们利用对比学习进行三种类型的表示学习，分别是语义相似度、边界相似度和标签相似度，以生成高质量的演示示例。对三个嵌套NER和四个扁平NER数据集进行了大量实验，证明了我们系统的有效性。",
    "tldr": "本研究提出了一种针对少样本嵌套命名实体识别的上下文学习框架，并通过引入EnDe检索器和对比学习机制改进了示例演示选择，实现了高质量的演示示例。在多个数据集上进行的实验验证了系统的有效性。",
    "en_tdlr": "This study introduces an in-context learning framework for few-shot nested named entity recognition (NER), where a novel example demonstration selection mechanism and contrastive learning are employed to generate high-quality demonstration examples. Extensive experiments validate the efficacy of the system across multiple datasets."
}