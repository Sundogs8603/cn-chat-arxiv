{
    "title": "Stable LM 2 1.6B Technical Report",
    "abstract": "arXiv:2402.17834v1 Announce Type: new  Abstract: We introduce StableLM 2 1.6B, the first in a new generation of our language model series. In this technical report, we present in detail the data and training procedure leading to the base and instruction-tuned versions of StableLM 2 1.6B. The weights for both models are available via Hugging Face for anyone to download and use. The report contains thorough evaluations of these models, including zero- and few-shot benchmarks, multilingual benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of publishing this report, StableLM 2 1.6B was the state-of-the-art open model under 2B parameters by a significant margin. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.",
    "link": "https://arxiv.org/abs/2402.17834",
    "context": "Title: Stable LM 2 1.6B Technical Report\nAbstract: arXiv:2402.17834v1 Announce Type: new  Abstract: We introduce StableLM 2 1.6B, the first in a new generation of our language model series. In this technical report, we present in detail the data and training procedure leading to the base and instruction-tuned versions of StableLM 2 1.6B. The weights for both models are available via Hugging Face for anyone to download and use. The report contains thorough evaluations of these models, including zero- and few-shot benchmarks, multilingual benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time of publishing this report, StableLM 2 1.6B was the state-of-the-art open model under 2B parameters by a significant margin. Given its appealing small size, we also provide throughput measurements on a number of edge devices. In addition, we open source several quantized checkpoints and provide their performance metrics compared to the original model.",
    "path": "papers/24/02/2402.17834.json",
    "total_tokens": 904,
    "translated_title": "稳定LM 2 1.6B技术报告",
    "translated_abstract": "我们介绍了稳定LM 2 1.6B，这是我们语言模型系列的新一代产品。在这份技术报告中，我们详细介绍了导致StableLM 2 1.6B基础版本和指导调优版本的数据和训练过程。这两个模型的权重均可通过Hugging Face下载和使用。该报告包含了对这些模型的彻底评估，包括零 shot 和少 shot 基准测试，多语言基准测试，以及重点放在多轮对话的 MT 基准测试上。在发布本报告时，StableLM 2 1.6B是具有显著优势的 2B 参数范围内最先进的开源模型。鉴于其吸引人的小尺寸，我们还提供了在多种边缘设备上的吞吐量测试。此外，我们开源了几个量化检查点，并提供了它们与原始模型的性能指标比较。",
    "tldr": "稳定LM 2 1.6B是语言模型系列中的新一代产品，在该报告中详细介绍了其数据、训练过程和性能评估，该模型在发布时是2B参数范围内最先进的开源模型之一，并提供了下载链接和性能对比数据。",
    "en_tdlr": "StableLM 2 1.6B is the next generation of language models, detailed data, training procedures, and performance evaluations presented in the report. It was the state-of-the-art open model under 2B parameters at the time of publication, offering download links and performance metrics for comparison."
}