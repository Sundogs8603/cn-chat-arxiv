{
    "title": "SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding",
    "abstract": "Distributed learning is an effective approach to accelerate model training using multiple workers. However, substantial communication delays emerge between workers and a parameter server due to massive costs associated with communicating gradients. SignSGD with majority voting (signSGD-MV) is a simple yet effective optimizer that reduces communication costs through one-bit quantization, yet the convergence rates considerably decrease as adversarial workers increase. In this paper, we show that the convergence rate is invariant as the number of adversarial workers increases, provided that the number of adversarial workers is smaller than that of benign workers. The key idea showing this counter-intuitive result is our novel signSGD with federated defense (signSGD-FD). Unlike the traditional approaches, signSGD-FD exploits the gradient information sent by adversarial workers with the proper weights, which are obtained through gradient sign decoding. Experimental results demonstrate signS",
    "link": "https://rss.arxiv.org/abs/2402.01340",
    "context": "Title: SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding\nAbstract: Distributed learning is an effective approach to accelerate model training using multiple workers. However, substantial communication delays emerge between workers and a parameter server due to massive costs associated with communicating gradients. SignSGD with majority voting (signSGD-MV) is a simple yet effective optimizer that reduces communication costs through one-bit quantization, yet the convergence rates considerably decrease as adversarial workers increase. In this paper, we show that the convergence rate is invariant as the number of adversarial workers increases, provided that the number of adversarial workers is smaller than that of benign workers. The key idea showing this counter-intuitive result is our novel signSGD with federated defense (signSGD-FD). Unlike the traditional approaches, signSGD-FD exploits the gradient information sent by adversarial workers with the proper weights, which are obtained through gradient sign decoding. Experimental results demonstrate signS",
    "path": "papers/24/02/2402.01340.json",
    "total_tokens": 876,
    "translated_title": "使用联邦防御的SignSGD：通过梯度符号解码来利用对抗攻击",
    "translated_abstract": "分布式学习是一种使用多个计算资源加速模型训练的有效方法。然而，由于梯度通信的巨大成本，工作节点和参数服务器之间存在着大量的通信延迟。SignSGD与多数投票（SignSGD-MV）是一种简单而有效的优化器，通过一位量化来减少通信成本，但是在对抗性工作节点增加时，收敛速度明显减慢。本文展示了在对抗性工作节点数量小于良性工作节点数量的情况下，收敛速度保持不变。这一反直觉的结果是由我们的新颖的SignSGD与联邦防御（SignSGD-FD）产生的关键思想所展示的。与传统方法不同，SignSGD-FD利用由对抗性工作节点发送的梯度信息，使用通过梯度符号解码获得的适当权重。实验结果证明了SignSGD-FD的有效性。",
    "tldr": "本研究提出了一种名为SignSGD-FD的技术，在对抗性工作节点增加时保持了收敛率不变，利用了通过梯度符号解码获得的对抗性工作节点的梯度信息。实验证明了该技术的有效性。"
}