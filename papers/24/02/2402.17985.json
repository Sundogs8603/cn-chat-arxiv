{
    "title": "FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization",
    "abstract": "arXiv:2402.17985v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance across various tasks. However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance. Recently, there have been some efficient attempts to quantize LLMs, yet inference with large batch size or long sequence still has the issue of being compute-bound. Fine-grained quantization methods have showcased their proficiency in achieving low-bit quantization for LLMs, while requiring FP16 data type for linear layer computations, which is time-consuming when dealing with large batch size or long sequence. In this paper, we introduce a method called FlattenQuant, which significantly reduces the maximum value of the tensor by flattening the large channels in the tensor, to achieve low bit per-tensor quantization with minimal accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits to achieve 48.29% ",
    "link": "https://arxiv.org/abs/2402.17985",
    "context": "Title: FlattenQuant: Breaking Through the Inference Compute-bound for Large Language Models with Per-tensor Quantization\nAbstract: arXiv:2402.17985v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance across various tasks. However, the latency of inference and the large GPU memory consumption of LLMs restrict their deployment performance. Recently, there have been some efficient attempts to quantize LLMs, yet inference with large batch size or long sequence still has the issue of being compute-bound. Fine-grained quantization methods have showcased their proficiency in achieving low-bit quantization for LLMs, while requiring FP16 data type for linear layer computations, which is time-consuming when dealing with large batch size or long sequence. In this paper, we introduce a method called FlattenQuant, which significantly reduces the maximum value of the tensor by flattening the large channels in the tensor, to achieve low bit per-tensor quantization with minimal accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits to achieve 48.29% ",
    "path": "papers/24/02/2402.17985.json",
    "total_tokens": 678,
    "translated_title": "FlattenQuant: 使用分张量量化打破大型语言模型推断计算限制",
    "translated_abstract": "大型语言模型(LLMs)在各种任务中展现出领先的性能，然而，推断的延迟和LLMs的大GPU内存消耗限制了它们的部署性能。本文提出了一种名为FlattenQuant的方法，通过对张量中的大通道进行展平来显著降低张量的最大值，实现了低比特每张量量化，减小了准确性损失。",
    "tldr": "FlattenQuant方法通过展平张量中的大通道，实现了低比特每张量量化，降低了准确性损失",
    "en_tdlr": "FlattenQuant method reduces the maximum value of the tensor by flattening large channels, achieving low-bit per-tensor quantization with minimal accuracy loss."
}