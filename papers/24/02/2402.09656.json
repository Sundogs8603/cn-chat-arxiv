{
    "title": "The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse",
    "abstract": "arXiv:2402.09656v1 Announce Type: new  Abstract: Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream task performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse",
    "link": "https://arxiv.org/abs/2402.09656",
    "context": "Title: The Butterfly Effect of Model Editing: Few Edits Can Trigger Large Language Models Collapse\nAbstract: arXiv:2402.09656v1 Announce Type: new  Abstract: Although model editing has shown promise in revising knowledge in Large Language Models (LLMs), its impact on the inherent capabilities of LLMs is often overlooked. In this work, we reveal a critical phenomenon: even a single edit can trigger model collapse, manifesting as significant performance degradation in various benchmark tasks. However, benchmarking LLMs after each edit, while necessary to prevent such collapses, is impractically time-consuming and resource-intensive. To mitigate this, we propose using perplexity as a surrogate metric, validated by extensive experiments demonstrating its strong correlation with downstream task performance. We further conduct an in-depth study on sequential editing, a practical setting for real-world scenarios, across various editing methods and LLMs, focusing on hard cases from our previous single edit studies. The results indicate that nearly all examined editing methods result in model collapse",
    "path": "papers/24/02/2402.09656.json",
    "total_tokens": 896,
    "translated_title": "模型编辑的蝴蝶效应：少量编辑可能引发大规模语言模型崩溃",
    "translated_abstract": "虽然模型编辑已显示出在大型语言模型（LLMs）中修订知识的潜力，但其对LLMs的内在能力的影响常常被忽视。在这项工作中，我们揭示了一个关键现象：即使只进行一个编辑，也可以引发模型崩溃，表现为各种基准任务中性能显著下降。然而，在每次编辑后对LLMs进行基准测试虽然必要，但耗时且资源密集。为了缓解这个问题，我们提出使用困惑度作为替代指标，并通过大量实验证实其与下游任务性能的强相关性。我们还对顺序编辑进行了深入研究，这是实际场景中的一种常见情况，涵盖了来自我们之前单次编辑研究中的困难案例。结果表明，几乎所有研究的编辑方法都导致模型崩溃。",
    "tldr": "尽管模型编辑在大型语言模型中显示出修订知识的潜力，但少量编辑可以触发模型崩溃，导致性能显著下降。我们提出使用困惑度作为替代指标，并通过实验证实其与下游任务性能的强相关性。",
    "en_tdlr": "Despite the potential of model editing in revising knowledge in large language models (LLMs), even a few edits can trigger model collapse, resulting in significant performance degradation. We propose using perplexity as a surrogate metric and validate its strong correlation with downstream task performance."
}