{
    "title": "CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer",
    "abstract": "arXiv:2402.17360v1 Announce Type: cross  Abstract: The ability to estimate joint parameters is essential for various applications in robotics and computer vision. In this paper, we propose CAPT: category-level articulation estimation from a point cloud using Transformer. CAPT uses an end-to-end transformer-based architecture for joint parameter and state estimation of articulated objects from a single point cloud. The proposed CAPT methods accurately estimate joint parameters and states for various articulated objects with high precision and robustness. The paper also introduces a motion loss approach, which improves articulation estimation performance by emphasizing the dynamic features of articulated objects. Additionally, the paper presents a double voting strategy to provide the framework with coarse-to-fine parameter estimation. Experimental results on several category datasets demonstrate that our methods outperform existing alternatives for articulation estimation. Our research ",
    "link": "https://arxiv.org/abs/2402.17360",
    "context": "Title: CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer\nAbstract: arXiv:2402.17360v1 Announce Type: cross  Abstract: The ability to estimate joint parameters is essential for various applications in robotics and computer vision. In this paper, we propose CAPT: category-level articulation estimation from a point cloud using Transformer. CAPT uses an end-to-end transformer-based architecture for joint parameter and state estimation of articulated objects from a single point cloud. The proposed CAPT methods accurately estimate joint parameters and states for various articulated objects with high precision and robustness. The paper also introduces a motion loss approach, which improves articulation estimation performance by emphasizing the dynamic features of articulated objects. Additionally, the paper presents a double voting strategy to provide the framework with coarse-to-fine parameter estimation. Experimental results on several category datasets demonstrate that our methods outperform existing alternatives for articulation estimation. Our research ",
    "path": "papers/24/02/2402.17360.json",
    "total_tokens": 814,
    "translated_title": "CAPT: 使用Transformer从单个点云估计类别级联的关节参数",
    "translated_abstract": "估计关节参数对于机器人学和计算机视觉中的各种应用至关重要。本文提出了CAPT：使用Transformer从点云中估计类别级联关节参数。CAPT使用端到端的基于Transformer的架构，从单个点云中对联动物体的关节参数和状态进行估计。所提出的CAPT方法可以准确估计各种联动物体的关节参数和状态，具有高精度和鲁棒性。该论文还提出了一种运动损失方法，通过强调联动物体的动态特征来改善关节参数估计性能。此外，该论文提出了双重投票策略，为框架提供由粗到细的参数估计。在几个类别数据集上的实验结果表明，我们的方法在关节参数估计方面优于现有的其他选择。",
    "tldr": "提出了CAPT方法，使用Transformer从单个点云中准确估计各种联动物体的关节参数和状态，引入了运动损失方法和双重投票策略，实验结果表明其优于现有的其他选择",
    "en_tdlr": "Introduced CAPT, a method using Transformer to accurately estimate joint parameters and states of various articulated objects from a single point cloud, with the introduction of motion loss approach and double voting strategy, demonstrating superiority over existing alternatives in experimental results."
}