{
    "title": "Adaptive Activation Functions for Predictive Modeling with Sparse Experimental Data",
    "abstract": "A pivotal aspect in the design of neural networks lies in selecting activation functions, crucial for introducing nonlinear structures that capture intricate input-output patterns. While the effectiveness of adaptive or trainable activation functions has been studied in domains with ample data, like image classification problems, significant gaps persist in understanding their influence on classification accuracy and predictive uncertainty in settings characterized by limited data availability. This research aims to address these gaps by investigating the use of two types of adaptive activation functions. These functions incorporate shared and individual trainable parameters per hidden layer and are examined in three testbeds derived from additive manufacturing problems containing fewer than one hundred training instances. Our investigation reveals that adaptive activation functions, such as Exponential Linear Unit (ELU) and Softplus, with individual trainable parameters, result in acc",
    "link": "https://arxiv.org/abs/2402.05401",
    "context": "Title: Adaptive Activation Functions for Predictive Modeling with Sparse Experimental Data\nAbstract: A pivotal aspect in the design of neural networks lies in selecting activation functions, crucial for introducing nonlinear structures that capture intricate input-output patterns. While the effectiveness of adaptive or trainable activation functions has been studied in domains with ample data, like image classification problems, significant gaps persist in understanding their influence on classification accuracy and predictive uncertainty in settings characterized by limited data availability. This research aims to address these gaps by investigating the use of two types of adaptive activation functions. These functions incorporate shared and individual trainable parameters per hidden layer and are examined in three testbeds derived from additive manufacturing problems containing fewer than one hundred training instances. Our investigation reveals that adaptive activation functions, such as Exponential Linear Unit (ELU) and Softplus, with individual trainable parameters, result in acc",
    "path": "papers/24/02/2402.05401.json",
    "total_tokens": 845,
    "translated_title": "自适应激活函数在稀疏实验数据预测建模中的应用",
    "translated_abstract": "神经网络设计的关键在于选择激活函数，用于引入能够捕捉复杂输入-输出模式的非线性结构。虽然在具有充足数据的领域（例如图像分类问题）中研究了自适应或可调激活函数的有效性，但在数据有限的情况下，对其对分类准确性和预测不确定性的影响仍然存在重大差距。本研究旨在通过研究两种类型的自适应激活函数来填补这些差距。这些函数在每个隐藏层中引入了共享和个体可调参数，并在包含少于一百个训练实例的三个测试平台中进行了探究。我们的研究揭示了个体可调参数的自适应激活函数（例如指数线性单位（ELU）和软加）在预测模型中具有较高的准确性。",
    "tldr": "本研究通过研究自适应激活函数在稀疏实验数据预测建模中的应用，填补了当前对其影响了解不足的重要差距，并揭示出个体可调参数的自适应激活函数在预测模型中具有较高的准确性。",
    "en_tdlr": "This research fills significant gaps in understanding the influence of adaptive activation functions by investigating their application in predictive modeling with sparse experimental data, revealing that adaptive activation functions with individual trainable parameters have higher accuracy in predictive models."
}