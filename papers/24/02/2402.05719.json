{
    "title": "Exact capacity of the \\emph{wide} hidden layer treelike neural networks with generic activations",
    "abstract": "Recent progress in studying \\emph{treelike committee machines} (TCM) neural networks (NN) in \\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23} showed that the Random Duality Theory (RDT) and its a \\emph{partially lifted}(pl RDT) variant are powerful tools that can be used for very precise networks capacity analysis. Here, we consider \\emph{wide} hidden layer networks and uncover that certain aspects of numerical difficulties faced in \\cite{Stojnictcmspnncapdiffactrdt23} miraculously disappear. In particular, we employ recently developed \\emph{fully lifted} (fl) RDT to characterize the \\emph{wide} ($d\\rightarrow \\infty$) TCM nets capacity. We obtain explicit, closed form, capacity characterizations for a very generic class of the hidden layer activations. While the utilized approach significantly lowers the amount of the needed numerical evaluations, the ultimate fl RDT usefulness and success still require a solid portion of the residual numerical ",
    "link": "https://arxiv.org/abs/2402.05719",
    "context": "Title: Exact capacity of the \\emph{wide} hidden layer treelike neural networks with generic activations\nAbstract: Recent progress in studying \\emph{treelike committee machines} (TCM) neural networks (NN) in \\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23} showed that the Random Duality Theory (RDT) and its a \\emph{partially lifted}(pl RDT) variant are powerful tools that can be used for very precise networks capacity analysis. Here, we consider \\emph{wide} hidden layer networks and uncover that certain aspects of numerical difficulties faced in \\cite{Stojnictcmspnncapdiffactrdt23} miraculously disappear. In particular, we employ recently developed \\emph{fully lifted} (fl) RDT to characterize the \\emph{wide} ($d\\rightarrow \\infty$) TCM nets capacity. We obtain explicit, closed form, capacity characterizations for a very generic class of the hidden layer activations. While the utilized approach significantly lowers the amount of the needed numerical evaluations, the ultimate fl RDT usefulness and success still require a solid portion of the residual numerical ",
    "path": "papers/24/02/2402.05719.json",
    "total_tokens": 941,
    "translated_title": "宽隐藏层树状神经网络中具有通用激活函数的容量准确性",
    "translated_abstract": "最近对于树状委员会机器(TCM)神经网络的研究表明，随机二重性理论(RDT)及其部分提升变种(pl RDT)是能够用于非常精确的网络容量分析的强大工具。在本文中，我们考虑了宽隐藏层网络，并发现\\cite{Stojnictcmspnncapdiffactrdt23}中面临的某些数值困难奇迹般地消失了。特别是，我们采用最近发展的全面提升(fl) RDT来表征宽($d\\rightarrow \\infty$) TCM网络的容量。我们获得了一类非常通用的隐藏层激活函数的显式、闭式容量刻画。尽管所使用的方法显著降低了所需的数值评估量，但最终的fl RDT的实用性和成功仍然需要可靠的数值计算。",
    "tldr": "该论文研究了宽隐藏层树状神经网络的容量，采用全面提升的随机二重性理论(fl RDT)来对宽(TCM)网络的容量进行刻画，得到了一类通用激活函数的显式、闭式容量计算公式。"
}