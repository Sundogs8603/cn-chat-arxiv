{
    "title": "Long-Context Language Modeling with Parallel Context Encoding",
    "abstract": "arXiv:2402.16617v1 Announce Type: new  Abstract: Extending large language models (LLMs) to process longer inputs is crucial for numerous applications. However, the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window. We introduce Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only LLMs to extend their context window. CEPE adopts a small encoder to process long inputs chunk by chunk and enables the frozen decoder to leverage additional contexts via cross-attention. CEPE is efficient, generalizable, and versatile: trained with 8K-token documents, CEPE extends the context window of LLAMA-2 to 128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE yields strong performance on language modeling and in-context learning. CEPE also excels in retrieval-augmented applications, while existing long-context models degenerat",
    "link": "https://arxiv.org/abs/2402.16617",
    "context": "Title: Long-Context Language Modeling with Parallel Context Encoding\nAbstract: arXiv:2402.16617v1 Announce Type: new  Abstract: Extending large language models (LLMs) to process longer inputs is crucial for numerous applications. However, the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window. We introduce Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only LLMs to extend their context window. CEPE adopts a small encoder to process long inputs chunk by chunk and enables the frozen decoder to leverage additional contexts via cross-attention. CEPE is efficient, generalizable, and versatile: trained with 8K-token documents, CEPE extends the context window of LLAMA-2 to 128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE yields strong performance on language modeling and in-context learning. CEPE also excels in retrieval-augmented applications, while existing long-context models degenerat",
    "path": "papers/24/02/2402.16617.json",
    "total_tokens": 898,
    "translated_title": "具有并行上下文编码的长上下文语言建模",
    "translated_abstract": "将大型语言模型（LLMs）扩展到处理更长的输入对于许多应用至关重要。然而，transformers的巨大计算成本，以及位置编码的有限泛化能力，限制了它们的上下文窗口的大小。我们引入了一种称为Context Expansion with Parallel Encoding（CEPE）的框架，可以应用于任何现有的仅解码器LLMs，以扩展它们的上下文窗口。CEPE采用一个小型编码器来分块处理长输入，并通过交叉注意力使冻结的解码器能够利用额外的上下文。CEPE高效、通用且多功能：通过使用8K标记文档进行训练，CEPE将LLAMA-2的上下文窗口扩展到128K标记，仅使用1/6的内存即可获得10倍的吞吐量。CEPE在语言建模和上下文学习方面表现出强大性能。CEPE在检索增强应用中也表现出色，而现有的长上下文模型在这方面则退化。",
    "tldr": "提出了一种名为CEPE的框架，通过并行编码扩展了现有仅解码器LLMs的上下文窗口，显著降低了计算成本并在语言建模和上下文学习中取得了强大性能表现。",
    "en_tdlr": "Introducing a framework named CEPE that extends the context window of existing decoder-only LLMs through parallel encoding, significantly reducing computational cost and achieving strong performance in language modeling and in-context learning."
}