{
    "title": "Revisiting Knowledge Distillation for Autoregressive Language Models",
    "abstract": "arXiv:2402.11890v1 Announce Type: new  Abstract: Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve ",
    "link": "https://arxiv.org/abs/2402.11890",
    "context": "Title: Revisiting Knowledge Distillation for Autoregressive Language Models\nAbstract: arXiv:2402.11890v1 Announce Type: new  Abstract: Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve ",
    "path": "papers/24/02/2402.11890.json",
    "total_tokens": 905,
    "translated_title": "重新审视自回归语言模型的知识蒸馏",
    "translated_abstract": "知识蒸馏（KD）是一种常见的方法，用于通过训练较小的学生模型来压缩教师模型，以减少推理成本和内存占用。然而，在自回归语言模型（LMs）的背景下，我们在实证中发现较大的教师LMs可能会导致较差的学生。为解决这个问题，我们进行了一系列分析，并揭示了不同记号具有不同的教学方式，忽视这一点将导致性能下降。在此基础上，我们提出了一种简单而有效的自适应教学方法（ATKD）来改善KD。ATKD的核心是减少死记硬背的学习，使教学更加多样化和灵活。对8个LM任务的大量实验表明，在ATKD的帮助下，各种基线KD方法在所有模型类型和规模上均可以实现一致且显著的性能提升（最高可达+3.04%的平均分数）。",
    "tldr": "提出一种自适应教学方法（ATKD），以改善自回归语言模型的知识蒸馏，帮助各种基线KD方法在所有模型类型和规模上实现一致且显著的性能提升。"
}