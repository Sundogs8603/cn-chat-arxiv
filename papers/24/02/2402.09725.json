{
    "title": "Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization",
    "abstract": "arXiv:2402.09725v1 Announce Type: cross  Abstract: Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens. However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases. In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR). We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions. We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference. The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores c",
    "link": "https://arxiv.org/abs/2402.09725",
    "context": "Title: Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization\nAbstract: arXiv:2402.09725v1 Announce Type: cross  Abstract: Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens. However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases. In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR). We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions. We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference. The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores c",
    "path": "papers/24/02/2402.09725.json",
    "total_tokens": 870,
    "translated_title": "通过错误暴露和一致性正则化改进非自回归机器翻译",
    "translated_abstract": "作为IR-NAT（基于迭代改进的NAT）框架之一，条件掩码语言模型（CMLM）采用掩码预测范式来重新预测掩码低置信度的标记。然而，CMLM在训练和推理之间存在数据分布不一致的问题，观察到的标记在这两种情况下生成方式不同。本文提出使用错误暴露和一致性正则化（EECR）的训练方法来解决这个问题。我们在训练过程中基于模型预测构建混合序列，并提出在不完美观测条件下针对掩码标记进行优化。我们还设计了一种一致性学习方法，以限制不同观测情况下掩码标记的数据分布，缩小训练和推理之间的差距。在五个翻译基准上的实验证明，平均改进了0.68和0.40的BLEU得分。",
    "tldr": "本论文提出使用错误暴露和一致性正则化的训练方法来改进非自回归机器翻译中的数据分布不一致问题，并取得了显著的BLEU得分提升。",
    "en_tdlr": "This paper proposes using the training methods of error exposure and consistency regularization to improve the issue of data distribution discrepancy in non-autoregressive machine translation, and achieves significant improvements in BLEU scores."
}