{
    "title": "Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision",
    "abstract": "arXiv:2402.12691v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved remarkable success thanks to scalability on large text corpora, but have some drawback in training efficiency. In contrast, Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance thanks to syntactic supervision, but have trouble with scalability. Thus, given these complementary advantages of LLMs and SLMs, it is necessary to develop an architecture that integrates the scalability of LLMs with the training efficiency of SLMs, namely Syntactic Large Language Models (SLLM). In this paper, we propose a novel method dubbed tree-planting: implicitly \"plant\" trees into attention weights of Transformer LMs to reflect syntactic structures of natural language. Specifically, Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which learn syntax on small treebanks via tree-planting and then scale on large text corpora via conti",
    "link": "https://arxiv.org/abs/2402.12691",
    "context": "Title: Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision\nAbstract: arXiv:2402.12691v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved remarkable success thanks to scalability on large text corpora, but have some drawback in training efficiency. In contrast, Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance thanks to syntactic supervision, but have trouble with scalability. Thus, given these complementary advantages of LLMs and SLMs, it is necessary to develop an architecture that integrates the scalability of LLMs with the training efficiency of SLMs, namely Syntactic Large Language Models (SLLM). In this paper, we propose a novel method dubbed tree-planting: implicitly \"plant\" trees into attention weights of Transformer LMs to reflect syntactic structures of natural language. Specifically, Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which learn syntax on small treebanks via tree-planting and then scale on large text corpora via conti",
    "path": "papers/24/02/2402.12691.json",
    "total_tokens": 942,
    "translated_title": "树种植变压器：具有隐式句法监督的大型语言模型",
    "translated_abstract": "大型语言模型（LLMs）在大规模文本语料上的可扩展性取得了显著成功，但在训练效率方面存在一些缺点。相比之下，句法语言模型（SLMs）可以通过句法监督高效训练，达到相对较高的性能，但在可扩展性方面存在困难。因此，鉴于LLMs和SLMs的互补优势，有必要开发一种将LLMs的可扩展性与SLMs的训练效率结合起来的体系结构，即句法大型语言模型（SLLM）。在本文中，我们提出一种名为“树种植”的新方法：在Transformer LMs的注意权重中暗示地“种植”树木，以反映自然语言的句法结构。具体而言，通过树种植训练的Transformer LMs将被称为树种植变压器（TPT），它们通过树种植在小型树库上学习语法，然后通过继续在大规模文本语料上进行缩放。",
    "tldr": "提出了一种新方法 Tree-Planted Transformers (TPT)，通过在 Transformer LMs 的注意权重中隐式地“种植”树木来反映自然语言的句法结构，实现了句法大型语言模型（SLLM）的结合。",
    "en_tdlr": "Introduced a new method Tree-Planted Transformers (TPT) that implicitly \"plants\" trees into attention weights of Transformer LMs to reflect syntactic structures of natural language, achieving the integration of Syntactic Large Language Models (SLLM)."
}