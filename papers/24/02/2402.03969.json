{
    "title": "In-context learning agents are asymmetric belief updaters",
    "abstract": "We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology. We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition.",
    "link": "https://arxiv.org/abs/2402.03969",
    "context": "Title: In-context learning agents are asymmetric belief updaters\nAbstract: We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology. We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition.",
    "path": "papers/24/02/2402.03969.json",
    "total_tokens": 850,
    "translated_title": "上下文学习智能体是不对称的信念更新者",
    "translated_abstract": "本文研究了大型语言模型（LLMs）的上下文学习动态，通过从认知心理学中引入的三个仪器学习任务进行适应。我们发现LLMs以不对称的方式更新其信念，并且从比预期好的结果中学到的东西比从比预期差的结果中学到的东西更多。此外，我们还发现当学习关于反事实反馈时，这种效应会逆转，并且在没有涉及代理性时消失。通过研究通过元-强化学习获得的理想化的上下文学习智能体，我们进一步证实了这些发现，发现类似的模式。总的来说，我们的结果有助于我们理解上下文学习的工作方式，突出显示问题的框架显著影响学习的过程，这也是人类认知中观察到的现象。",
    "tldr": "本研究发现大型语言模型以不对称方式更新其信念，更多地从比预期好的结果中学到。同时，我们还发现学习关于反事实反馈时，这种效应会逆转。这些结果有助于我们理解上下文学习的工作方式，并揭示了问题框架如何影响学习过程。"
}