{
    "title": "SoFA: Shielded On-the-fly Alignment via Priority Rule Following",
    "abstract": "arXiv:2402.17358v1 Announce Type: new  Abstract: The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that ",
    "link": "https://arxiv.org/abs/2402.17358",
    "context": "Title: SoFA: Shielded On-the-fly Alignment via Priority Rule Following\nAbstract: arXiv:2402.17358v1 Announce Type: new  Abstract: The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that ",
    "path": "papers/24/02/2402.17358.json",
    "total_tokens": 909,
    "translated_title": "SoFA：通过优先规则跟随进行屏蔽式即时对齐",
    "translated_abstract": "在大型语言模型（LLMs）中的对齐问题涉及将它们适应广泛的人类价值观。由于偏好的多样性和监管标准的挑战，这一要求使现有的对齐方法面临困难。本文介绍了一种新颖的对齐范式，即优先规则跟随，它将规则定义为每次对话中的主要控制机制，并将其优先于用户指令。我们的初步分析显示，即使像GPT-4这样先进的LLMs也存在理解和优先考虑规则的缺陷。因此，我们提出了PriorityDistill，一种从LLM模拟中萃取优先跟随信号的半自动方法，以确保规则的强大整合和遵守。我们的实验表明，这种方法不仅有效地通过仅使用一个通用规则来最小化不对齐问题，而且可以平滑地适应各种未见规则，确保它们免受劫持。",
    "tldr": "本文提出了一种新颖的对齐范式——优先规则跟随，在对话中将规则作为主要控制机制，以确保对齐并提出了PriorityDistill，实现了从LLM模拟中提取优先跟随信号的方法，确保规则的强大整合和遵守"
}