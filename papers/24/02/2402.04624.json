{
    "title": "MEMORYLLM: Towards Self-Updatable Large Language Models",
    "abstract": "Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer. MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates.",
    "link": "https://arxiv.org/abs/2402.04624",
    "context": "Title: MEMORYLLM: Towards Self-Updatable Large Language Models\nAbstract: Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer. MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates.",
    "path": "papers/24/02/2402.04624.json",
    "total_tokens": 838,
    "translated_title": "MEMORYLLM：面向自更新的大型语言模型",
    "translated_abstract": "现有的大型语言模型（LLMs）在部署后通常保持静态，这可能会使向模型中注入新知识变得困难。我们旨在构建包含相当比例的可自更新参数的模型，使模型能够有效且高效地整合新知识。为此，我们引入了MEMORYLLM，它是一个由Transformer和固定大小的内存池组成的模型，位于Transformer的潜空间内。MEMORYLLM可以通过文本知识进行自我更新并记忆先前注入的知识。我们的评估证明了MEMORYLLM有效地整合新知识的能力，其性能在模型编辑基准上得到了证实。同时，该模型还表现出长期信息保留能力，这在我们自定义的评估和长上下文基准中得到了验证。MEMORYLLM在进行了近百万次内存更新后，没有任何性能下降的迹象，显示出操作的完整性。",
    "tldr": "MEMORYLLM是一个自更新的大型语言模型，其中包括Transformer和一个固定大小的内存池，能够有效地整合新知识并保持长期信息保留能力。即使在近百万次内存更新后，MEMORYLLM仍能保持操作的完整性。"
}