{
    "title": "Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach",
    "abstract": "Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text-generation tasks. To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of LLMs when editing their output. The editing actions in this framework are specifically devised for text generation. Extensive experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5's performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.",
    "link": "https://arxiv.org/abs/2402.04609",
    "context": "Title: Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach\nAbstract: Post-editing has proven effective in improving the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4, particularly when direct updating of their parameters to enhance text quality is infeasible or expensive. However, relying solely on smaller language models for post-editing can limit the LLMs' ability to generalize across domains. Moreover, the editing strategies in these methods are not optimally designed for text-generation tasks. To address these limitations, we propose a neural programmer-interpreter approach that preserves the domain generalization ability of LLMs when editing their output. The editing actions in this framework are specifically devised for text generation. Extensive experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5's performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art (SOTA) LLM post-editing methods in cross-domain settings.",
    "path": "papers/24/02/2402.04609.json",
    "total_tokens": 951,
    "translated_title": "通过LLM后编辑改进跨领域低资源文本生成：一种程序员-解释器方法",
    "translated_abstract": "后编辑已被证明能够有效改善大型语言模型（LLM），如GPT-3.5或GPT-4，生成的文本质量，特别是在更新其参数以提升文本质量不可行或昂贵的情况下。然而，仅依赖较小的语言模型进行后编辑可能会限制LLM在领域间的泛化能力。此外，这些方法中的编辑策略对于文本生成任务并不是最佳设计。为了解决这些限制，我们提出了一种神经程序员-解释器方法，在编辑LLM的输出时保留了其领域泛化能力。该框架中的编辑操作专门用于文本生成。大量实验证明，在交叉领域环境中，程序员-解释器显著提升了GPT-3.5在逻辑形式到文本转换和低资源机器翻译中的性能，超过了其他最先进的LLM后编辑方法。",
    "tldr": "本研究提出了一种神经程序员-解释器方法，通过后编辑来改进大型语言模型在文本生成任务中的性能。实验证明，在交叉领域环境中，该方法显著超越其他最先进的后编辑方法，提升了GPT-3.5在逻辑形式到文本转换和低资源机器翻译中的性能。"
}