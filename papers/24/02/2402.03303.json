{
    "title": "Nevermind: Instruction Override and Moderation in Large Language Models",
    "abstract": "Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/ja",
    "link": "https://arxiv.org/abs/2402.03303",
    "context": "Title: Nevermind: Instruction Override and Moderation in Large Language Models\nAbstract: Given the impressive capabilities of recent Large Language Models (LLMs), we investigate and benchmark the most popular proprietary and different sized open source models on the task of explicit instruction following in conflicting situations, e.g. overrides. These include the ability of the model to override the knowledge within the weights of the model, the ability to override (or moderate) extracted knowledge in the prompt, and lastly the ability to perform a full jailbreak. Experimentation performed suggest several key findings to improve instruction following - larger models perform the best in following instructions that override internal and contextual instructions, and are obedient, even to a fault. When scaling to longer contexts via rope scaling, a significant buffer needs to be maintained from the edge of the perplexity cliff in order to maintain instruction following capabilities. Finally, we observe improving instruction following, and subsequently instruction overrides/ja",
    "path": "papers/24/02/2402.03303.json",
    "total_tokens": 879,
    "translated_title": "没关系：大语言模型中的指令覆盖和调节",
    "translated_abstract": "鉴于近期大语言模型（LLMs）的令人印象深刻的能力，我们对最流行的专有模型和不同大小的开源模型进行了调查和基准测试，以解决在冲突情况下的明确指令遵循任务，例如覆盖。这些包括模型在其权重中覆盖知识的能力，覆盖（或调节）提示中提取的知识的能力，以及进行完全越狱的能力。实验表明，可以改进指令遵循的几个关键发现 - 较大的模型在遵循覆盖内部和上下文指令方面表现最佳，并且非常服从，甚至有些过度。当通过绳索扩展来扩展到更长的上下文时，需要保持与困惑边缘的显著缓冲区，以保持指令遵循能力。最后，我们观察到指令遵循的改善，以及随之而来的指令覆盖/越狱。",
    "tldr": "大语言模型具有覆盖和调节指令的能力，较大的模型在覆盖内部和上下文指令方面表现最佳，并且在绳索扩展时需要保持缓冲区来保持指令遵循能力。",
    "en_tdlr": "Large language models have the ability to override and moderate instructions, with larger models performing the best in overriding internal and contextual instructions, and a buffer needs to be maintained during rope scaling to maintain instruction following capabilities."
}