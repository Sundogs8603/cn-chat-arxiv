{
    "title": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration",
    "abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across",
    "link": "https://arxiv.org/abs/2402.04978",
    "context": "Title: An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration\nAbstract: While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across",
    "path": "papers/24/02/2402.04978.json",
    "total_tokens": 899,
    "translated_title": "通过知识图谱集成协作的增强型基于提示的LLM推理方案",
    "translated_abstract": "虽然大型语言模型（LLMs）在许多自然语言处理（NLP）任务中展现出极好的性能，但在实际应用中遇到了一些挑战，包括虚构问题、知识更新不足以及推理过程的透明度有限。为了克服这些限制，本研究创新性地提出了一种协作训练自由的推理方案，其中知识图谱（KG）和LLMs之间密切合作。该方案首先使用LLMs迭代地探索KG，选择性地检索与任务相关的知识子图以支持推理。然后引导LLMs进一步组合内在的隐式知识，在子图上进行推理，并明确阐述推理过程。通过这种协作方法，我们的方案实现了更可靠的基于知识的推理，并便于追踪推理结果。实验结果表明，我们的方案在各项指标上取得了显著进展。",
    "tldr": "本文提出了一种通过知识图谱和LLMs合作训练的推理方案，该方案解决了大型语言模型在实际应用中的虚构问题、知识更新不足以及推理过程的透明度有限等挑战，实现了更可靠的知识推理和推理结果追踪。",
    "en_tdlr": "This paper proposes a collaborative training scheme involving knowledge graphs and LLMs to address challenges faced by large language models in practical applications, such as hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. Through this scheme, more reliable knowledge-based reasoning and tracing of reasoning results are achieved."
}