{
    "title": "GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning",
    "abstract": "arXiv:2402.16829v1 Announce Type: cross  Abstract: Embedding models are integral to AI applications like semantic search, personalized recommendations, and retrieval augmented generation for LLMs, necessitating high-quality training data. However, the limited scalability of manual data curation prompts the need for automated methods to ensure data integrity. Traditional unsupervised triplet mining automates training data generation, crucial for embedding model training, yet inadvertently injects biases and noise, thereby degrading model performance. Addressing this, we introduce GISTEmbed, a novel strategy that enhances in-batch negative selection during contrastive training through a guide model. This approach departs from reliance on random sampling and equal utility assumption of batch negatives, significantly reducing noise from data quality issues and improving model fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB), GISTEmbed showcases consistent perfor",
    "link": "https://arxiv.org/abs/2402.16829",
    "context": "Title: GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning\nAbstract: arXiv:2402.16829v1 Announce Type: cross  Abstract: Embedding models are integral to AI applications like semantic search, personalized recommendations, and retrieval augmented generation for LLMs, necessitating high-quality training data. However, the limited scalability of manual data curation prompts the need for automated methods to ensure data integrity. Traditional unsupervised triplet mining automates training data generation, crucial for embedding model training, yet inadvertently injects biases and noise, thereby degrading model performance. Addressing this, we introduce GISTEmbed, a novel strategy that enhances in-batch negative selection during contrastive training through a guide model. This approach departs from reliance on random sampling and equal utility assumption of batch negatives, significantly reducing noise from data quality issues and improving model fine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB), GISTEmbed showcases consistent perfor",
    "path": "papers/24/02/2402.16829.json",
    "total_tokens": 867,
    "translated_title": "GISTEmbed：文本嵌入微调中引导样本内训练负例选择",
    "translated_abstract": "嵌入模型对于语义搜索、个性化推荐以及生成模型的检索增强等AI应用至关重要，这需要高质量的训练数据。然而，手动数据整理的有限可扩展性促使我们需要自动化方法来确保数据完整性。传统的无监督三元组挖掘自动生成训练数据，对于嵌入模型训练至关重要，但不慎引入偏见和噪声，从而降低模型性能。针对这一问题，我们引入了GISTEmbed，一种通过引导模型在对比训练期间增强批内负例选择的新策略。这种方法摆脱了对于随机抽样和批负例等效用假设的依赖，显著降低了由数据质量问题引起的噪声，提高了模型微调效果。通过与 Massive Text Embedding Benchmark (MTEB) 进行基准测试，GISTEmbed 展示了一致的表现。",
    "tldr": "GISTEmbed通过引导模型增强批内负例选择，摆脱随机采样和等效用假设，降低数据质量问题带来的噪声，从而提高模型微调效果。",
    "en_tdlr": "GISTEmbed enhances in-batch negative selection through a guide model, departing from random sampling and equal utility assumption, reducing noise from data quality issues to improve model fine-tuning performance."
}