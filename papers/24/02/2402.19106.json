{
    "title": "A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval",
    "abstract": "arXiv:2402.19106v1 Announce Type: cross  Abstract: Video databases from the internet are a valuable source of text-audio retrieval datasets. However, given that sound and vision streams represent different \"views\" of the data, treating visual descriptions as audio descriptions is far from optimal. Even if audio class labels are present, they commonly are not very detailed, making them unsuited for text-audio retrieval. To exploit relevant audio information from video-text datasets, we introduce a methodology for generating audio-centric descriptions using Large Language Models (LLMs). In this work, we consider the egocentric video setting and propose three new text-audio retrieval benchmarks based on the EpicMIR and EgoMCQ tasks, and on the EpicSounds dataset. Our approach for obtaining audio-centric descriptions gives significantly higher zero-shot performance than using the original visual-centric descriptions. Furthermore, we show that using the same prompts, we can successfully emp",
    "link": "https://arxiv.org/abs/2402.19106",
    "context": "Title: A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval\nAbstract: arXiv:2402.19106v1 Announce Type: cross  Abstract: Video databases from the internet are a valuable source of text-audio retrieval datasets. However, given that sound and vision streams represent different \"views\" of the data, treating visual descriptions as audio descriptions is far from optimal. Even if audio class labels are present, they commonly are not very detailed, making them unsuited for text-audio retrieval. To exploit relevant audio information from video-text datasets, we introduce a methodology for generating audio-centric descriptions using Large Language Models (LLMs). In this work, we consider the egocentric video setting and propose three new text-audio retrieval benchmarks based on the EpicMIR and EgoMCQ tasks, and on the EpicSounds dataset. Our approach for obtaining audio-centric descriptions gives significantly higher zero-shot performance than using the original visual-centric descriptions. Furthermore, we show that using the same prompts, we can successfully emp",
    "path": "papers/24/02/2402.19106.json",
    "total_tokens": 815,
    "translated_title": "一种声音方法：利用大型语言模型为自我中心文本 - 音频检索生成音频描述",
    "translated_abstract": "互联网视频数据库是文本-音频检索数据集的宝贵来源。然而，考虑到声音和视觉流代表数据的不同“视图”，将视觉描述视为音频描述远非最佳选择。即使存在音频类标签，它们通常也不太详细，使其不适用于文本-音频检索。为了利用视频文本数据集中的相关音频信息，我们引入了一种利用大型语言模型（LLM）生成以音频为中心描述的方法。在这项工作中，我们考虑了自我中心视频设置，并提出了基于EpicMIR和EgoMCQ任务以及EpicSounds数据集的三个新的文本-音频检索基准。我们获得音频为中心的描述的方法比使用原始视觉为中心的描述具有显着更高的零样本性能。此外，我们展示了使用相同提示，我们可以成功地",
    "tldr": "使用大型语言模型生成音频描述的方法在自我中心视频设置下的文本-音频检索任务中表现出更高的零样本性能。",
    "en_tdlr": "The method of using Large Language Models to generate audio descriptions shows significantly higher zero-shot performance in text-audio retrieval tasks in egocentric video settings."
}