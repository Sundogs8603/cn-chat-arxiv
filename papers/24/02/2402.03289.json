{
    "title": "Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS",
    "abstract": "Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency. This is due to the lack of PPA awareness in conventional transformer decoding algorithms. In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code. Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models. For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product.",
    "link": "https://arxiv.org/abs/2402.03289",
    "context": "Title: Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS\nAbstract: Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and suboptimal power, performance, and area (PPA) efficiency. This is due to the lack of PPA awareness in conventional transformer decoding algorithms. In response, we present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code. Empirical evaluation with a fine-tuned language model on RTL codesets shows that our proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models. For the largest design generated by the state-of-the-art LLM (16-bit adder), our technique can achieve a 31.8% improvement in the area-delay product.",
    "path": "papers/24/02/2402.03289.json",
    "total_tokens": 854,
    "translated_title": "让每一步都有价值：使用MCTS的LLM基础高质量RTL代码生成",
    "translated_abstract": "现有的用于寄存器传输级代码生成的大型语言模型(LLM)面临编译失败和亚最优功耗、性能和面积(PPA)效率等挑战。这是由于传统变换器解码算法缺乏对PPA的意识所致。为此，我们提出了一种自动变换器解码算法，它通过蒙特卡罗树搜索来进行前瞻，引导变换器生成可编译的、功能正确的、PPA优化的代码。在RTL代码集上使用经过微调的语言模型进行的实证评估表明，与仅使用提示的方法相比，我们提出的技术一致地生成功能正确的代码，并有效解决了朴素大型语言模型对PPA不敏感的缺点。对于最先进的LLM生成的最大设计（16位加法器），我们的技术可以在面积延迟乘积上实现31.8%的改进。",
    "tldr": "本文介绍了一种使用蒙特卡罗树搜索进行前瞻的自动变换器解码算法，可解决现有大型语言模型在RTL代码生成中存在的编译失败和PPA不敏感的问题，并在性能上取得了显著改进。",
    "en_tdlr": "This paper presents an automated transformer decoding algorithm using Monte Carlo tree-search for lookahead, addressing the challenges of compilation failures and PPA unawareness in existing large language models for RTL code generation, and achieving significant improvements in performance."
}