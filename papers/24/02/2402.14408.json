{
    "title": "Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching",
    "abstract": "arXiv:2402.14408v1 Announce Type: new  Abstract: Pre-trained language models have revolutionized the natural language understanding landscape, most notably BERT (Bidirectional Encoder Representations from Transformers). However, a significant challenge remains for low-resource languages, where limited data hinders the effective training of such models. This work presents a novel approach to bridge this gap by transferring BERT capabilities from high-resource to low-resource languages using vocabulary matching. We conduct experiments on the Silesian and Kashubian languages and demonstrate the effectiveness of our approach to improve the performance of BERT models even when the target language has minimal training data. Our results highlight the potential of the proposed technique to effectively train BERT models for low-resource languages, thus democratizing access to advanced language understanding models.",
    "link": "https://arxiv.org/abs/2402.14408",
    "context": "Title: Transferring BERT Capabilities from High-Resource to Low-Resource Languages Using Vocabulary Matching\nAbstract: arXiv:2402.14408v1 Announce Type: new  Abstract: Pre-trained language models have revolutionized the natural language understanding landscape, most notably BERT (Bidirectional Encoder Representations from Transformers). However, a significant challenge remains for low-resource languages, where limited data hinders the effective training of such models. This work presents a novel approach to bridge this gap by transferring BERT capabilities from high-resource to low-resource languages using vocabulary matching. We conduct experiments on the Silesian and Kashubian languages and demonstrate the effectiveness of our approach to improve the performance of BERT models even when the target language has minimal training data. Our results highlight the potential of the proposed technique to effectively train BERT models for low-resource languages, thus democratizing access to advanced language understanding models.",
    "path": "papers/24/02/2402.14408.json",
    "total_tokens": 780,
    "translated_title": "将BERT能力从高资源语言转移到低资源语言使用词汇匹配",
    "translated_abstract": "预训练语言模型已经彻底改变了自然语言理解领域，其中最为显著的是BERT（双向编码器来自Transformer）。然而，对于低资源语言仍存在一个重要挑战，即有限的数据阻碍了这类模型的有效训练。本文提出了一种新颖的方法，通过词汇匹配将BERT的能力从高资源语言转移到低资源语言，以弥合这一差距。我们在西里西亚语和卡舒比亚语上进行了实验，并展示了我们的方法的有效性，即使目标语言仅有很少的训练数据，也能改善BERT模型的性能。我们的结果突显了该技术的潜力，能够有效训练低资源语言的BERT模型，从而使得先进的语言理解模型更具民主性地获得。",
    "tldr": "通过词汇匹配将BERT能力从高资源语言转移到低资源语言，有效弥合低资源语言训练困难的差距。",
    "en_tdlr": "Bridging the gap in training difficulty for low-resource languages by transferring BERT capabilities from high-resource languages using vocabulary matching."
}