{
    "title": "Expediting In-Network Federated Learning by Voting-Based Consensus Model Compression",
    "abstract": "Recently, federated learning (FL) has gained momentum because of its capability in preserving data privacy. To conduct model training by FL, multiple clients exchange model updates with a parameter server via Internet. To accelerate the communication speed, it has been explored to deploy a programmable switch (PS) in lieu of the parameter server to coordinate clients. The challenge to deploy the PS in FL lies in its scarce memory space, prohibiting running memory consuming aggregation algorithms on the PS. To overcome this challenge, we propose Federated Learning in-network Aggregation with Compression (FediAC) algorithm, consisting of two phases: client voting and model aggregating. In the former phase, clients report their significant model update indices to the PS to estimate global significant model updates. In the latter phase, clients upload global significant model updates to the PS for aggregation. FediAC consumes much less memory space and communication traffic than existing w",
    "link": "https://arxiv.org/abs/2402.03815",
    "context": "Title: Expediting In-Network Federated Learning by Voting-Based Consensus Model Compression\nAbstract: Recently, federated learning (FL) has gained momentum because of its capability in preserving data privacy. To conduct model training by FL, multiple clients exchange model updates with a parameter server via Internet. To accelerate the communication speed, it has been explored to deploy a programmable switch (PS) in lieu of the parameter server to coordinate clients. The challenge to deploy the PS in FL lies in its scarce memory space, prohibiting running memory consuming aggregation algorithms on the PS. To overcome this challenge, we propose Federated Learning in-network Aggregation with Compression (FediAC) algorithm, consisting of two phases: client voting and model aggregating. In the former phase, clients report their significant model update indices to the PS to estimate global significant model updates. In the latter phase, clients upload global significant model updates to the PS for aggregation. FediAC consumes much less memory space and communication traffic than existing w",
    "path": "papers/24/02/2402.03815.json",
    "total_tokens": 861,
    "translated_title": "基于投票一致性模型压缩的促进网络内联合学习",
    "translated_abstract": "最近，由于其保护数据隐私的能力，联合学习（FL）越来越受关注。为了通过FL进行模型训练，多个客户端通过互联网与参数服务器交换模型更新。为了加速通信速度，研究人员探索了在参数服务器的位置部署可编程交换机（PS）以协调客户端。在FL中部署PS的挑战在于其内存空间稀缺，无法在PS上运行消耗大量内存的聚合算法。为了克服这个挑战，我们提出了联合学习网络内聚合压缩（FediAC）算法，它包含两个阶段：客户端投票和模型聚合。在前一阶段，客户端向PS报告其重要模型更新的索引，以估计全局重要模型更新。在后一阶段，客户端将全局重要模型更新上传到PS进行聚合。与现有的方法相比，FediAC消耗的内存空间和通信流量较少。",
    "tldr": "本论文提出了一种基于投票一致性模型压缩的促进网络内联合学习的算法，通过客户端投票和模型聚合来减少内存空间和通信流量。"
}