{
    "title": "Decentralized Sum-of-Nonconvex Optimization",
    "abstract": "We consider the optimization problem of minimizing the sum-of-nonconvex function, i.e., a convex function that is the average of nonconvex components. The existing stochastic algorithms for such a problem only focus on a single machine and the centralized scenario. In this paper, we study the sum-of-nonconvex optimization in the decentralized setting. We present a new theoretical analysis of the PMGT-SVRG algorithm for this problem and prove the linear convergence of their approach. However, the convergence rate of the PMGT-SVRG algorithm has a linear dependency on the condition number, which is undesirable for the ill-conditioned problem. To remedy this issue, we propose an accelerated stochastic decentralized first-order algorithm by incorporating the techniques of acceleration, gradient tracking, and multi-consensus mixing into the SVRG algorithm. The convergence rate of the proposed method has a square-root dependency on the condition number. The numerical experiments validate the ",
    "link": "https://arxiv.org/abs/2402.02356",
    "context": "Title: Decentralized Sum-of-Nonconvex Optimization\nAbstract: We consider the optimization problem of minimizing the sum-of-nonconvex function, i.e., a convex function that is the average of nonconvex components. The existing stochastic algorithms for such a problem only focus on a single machine and the centralized scenario. In this paper, we study the sum-of-nonconvex optimization in the decentralized setting. We present a new theoretical analysis of the PMGT-SVRG algorithm for this problem and prove the linear convergence of their approach. However, the convergence rate of the PMGT-SVRG algorithm has a linear dependency on the condition number, which is undesirable for the ill-conditioned problem. To remedy this issue, we propose an accelerated stochastic decentralized first-order algorithm by incorporating the techniques of acceleration, gradient tracking, and multi-consensus mixing into the SVRG algorithm. The convergence rate of the proposed method has a square-root dependency on the condition number. The numerical experiments validate the ",
    "path": "papers/24/02/2402.02356.json",
    "total_tokens": 806,
    "translated_title": "分散的非凸求和优化",
    "translated_abstract": "我们考虑最小化非凸函数的求和问题，即将非凸组分的平均值作为凸函数。现有的针对这类问题的随机算法只关注单台机器和集中式场景。本文研究了分散场景下的非凸求和优化问题。我们对该问题的PMGT-SVRG算法进行了新的理论分析，并证明了他们方法的线性收敛性。然而，PMGT-SVRG算法的收敛速度与条件数呈线性依赖关系，在条件数较差的问题中是不可取的。为了解决这个问题，我们提出了一种加速的随机分散一阶算法，将加速、梯度追踪和多一致性混合技术融入SVRG算法中。所提方法的收敛速度与条件数呈平方根依赖关系。数值实验证实了我们方法的有效性。",
    "tldr": "本论文研究了分散场景下的非凸求和优化问题，并提出了一种具有加速效果的随机分散一阶算法。数值实验证实了该算法的有效性。",
    "en_tdlr": "This paper studies the problem of decentralized sum-of-nonconvex optimization and proposes an accelerated stochastic decentralized first-order algorithm. Numerical experiments validate the effectiveness of the proposed algorithm."
}