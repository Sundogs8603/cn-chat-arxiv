{
    "title": "Trained quantum neural networks are Gaussian processes",
    "abstract": "arXiv:2402.08726v1 Announce Type: cross Abstract: We study quantum neural networks made by parametric one-qubit gates and fixed two-qubit gates in the limit of infinite width, where the generated function is the expectation value of the sum of single-qubit observables over all the qubits. First, we prove that the probability distribution of the function generated by the untrained network with randomly initialized parameters converges in distribution to a Gaussian process whenever each measured qubit is correlated only with few other measured qubits. Then, we analytically characterize the training of the network via gradient descent with square loss on supervised learning problems. We prove that, as long as the network is not affected by barren plateaus, the trained network can perfectly fit the training set and that the probability distribution of the function generated after training still converges in distribution to a Gaussian process. Finally, we consider the statistical noise of t",
    "link": "https://arxiv.org/abs/2402.08726",
    "context": "Title: Trained quantum neural networks are Gaussian processes\nAbstract: arXiv:2402.08726v1 Announce Type: cross Abstract: We study quantum neural networks made by parametric one-qubit gates and fixed two-qubit gates in the limit of infinite width, where the generated function is the expectation value of the sum of single-qubit observables over all the qubits. First, we prove that the probability distribution of the function generated by the untrained network with randomly initialized parameters converges in distribution to a Gaussian process whenever each measured qubit is correlated only with few other measured qubits. Then, we analytically characterize the training of the network via gradient descent with square loss on supervised learning problems. We prove that, as long as the network is not affected by barren plateaus, the trained network can perfectly fit the training set and that the probability distribution of the function generated after training still converges in distribution to a Gaussian process. Finally, we consider the statistical noise of t",
    "path": "papers/24/02/2402.08726.json",
    "total_tokens": 958,
    "translated_title": "训练的量子神经网络是高斯过程",
    "translated_abstract": "我们研究了由参数化的一比特门和固定的两比特门构成的量子神经网络，在无穷宽度的极限下，生成的函数是所有比特上单比特可观测量的期望值的和。首先，我们证明了具有随机初始化参数的未训练网络所生成的函数的概率分布在每个测量比特仅与少数其他测量比特相关的情况下收敛于高斯过程。然后，我们通过梯度下降和平方损失函数对网络的训练进行了分析。我们证明，只要网络没有受到贫瘠高原的影响，训练后的网络可以完美地拟合训练集，训练后所生成的函数的概率分布仍然收敛于高斯过程。最后，我们考虑了统计噪声的影响。",
    "tldr": "本文研究了训练的量子神经网络，证明了当每个测量的比特仅与少数其他测量的比特相关时，未训练的网络生成的函数的概率分布收敛于高斯过程；通过分析梯度下降训练网络，证明了训练后的网络能够完美拟合训练集，并且训练后生成的函数的概率分布仍然收敛于高斯过程；同时考虑了统计噪声的影响。"
}