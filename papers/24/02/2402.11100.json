{
    "title": "When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models",
    "abstract": "arXiv:2402.11100v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have made remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning questions that FLUB focuses on mainly consist of the tricky, humorous, and misleading questions collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights ",
    "link": "https://arxiv.org/abs/2402.11100",
    "context": "Title: When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models\nAbstract: arXiv:2402.11100v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have made remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning questions that FLUB focuses on mainly consist of the tricky, humorous, and misleading questions collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights ",
    "path": "papers/24/02/2402.11100.json",
    "total_tokens": 806,
    "translated_title": "当大型语言模型遇到狡猾问题：用于大型语言模型的谬误理解基准",
    "translated_abstract": "最近，大型语言模型(LLMs)在语言理解和生成方面取得了显著进展。本文通过提出一个名为FaLlacy Understanding Benchmark (FLUB)的基准来挑战LLMs的推理和理解能力，其中包含易于人类理解但难于模型把握的狡猾问题。具体来说，FLUB专注于从真实互联网环境中收集到的棘手、幽默和误导性问题。我们设计了三个难度递增的任务，用于评估LLMs的谬误理解能力。基于FLUB，我们研究了多个代表性的和先进的LLMs的表现，表明我们的FLUB是具有挑战性的并值得未来进一步研究的。",
    "tldr": "该论文提出了一个谬误理解基准FLUB，挑战大型语言模型在推理和理解能力上，重点是通过设计狡猾问题评估LLMs的谬误理解能力。",
    "en_tdlr": "This paper introduces a fallacy understanding benchmark FLUB to challenge large language models in reasoning and understanding abilities, focusing on evaluating the fallacy understanding ability of LLMs through designing cunning questions."
}