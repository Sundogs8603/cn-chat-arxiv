{
    "title": "A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety objectives beyond classification accuracy, such as predictive uncertainty. Yet, the effectiveness of CLIP models on such safety-related features is less-explored. Driven by the above, this work comprehensively investigates the safety objectives of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs. To this end, we study 83 CLIP models and 127 ImageNet classifiers. They are diverse in architecture, (pre)training distribution and training strategies. We consider 10 visual factors (e.g., shape and pattern), 5 ",
    "link": "https://arxiv.org/abs/2402.07410",
    "context": "Title: A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)\nAbstract: Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety objectives beyond classification accuracy, such as predictive uncertainty. Yet, the effectiveness of CLIP models on such safety-related features is less-explored. Driven by the above, this work comprehensively investigates the safety objectives of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs. To this end, we study 83 CLIP models and 127 ImageNet classifiers. They are diverse in architecture, (pre)training distribution and training strategies. We consider 10 visual factors (e.g., shape and pattern), 5 ",
    "path": "papers/24/02/2402.07410.json",
    "total_tokens": 913,
    "translated_title": "《深入解析对比语言-图像预训练(CLIP)模型的健壮性》",
    "translated_abstract": "对比语言-图像预训练(CLIP)模型在多个具有挑战性的分布转移中展现出了卓越的泛化能力。然而，关于其在特定视觉因素变化下的鲁棒性仍有待进一步探索。在现实世界的应用中，可靠且安全的系统必须考虑除分类准确性之外的其他安全目标，如预测的不确定性。然而，CLIP模型在这些与安全相关的特征上的有效性较少被探讨。出于上述原因，本研究全面调查了CLIP模型的安全目标，特别关注三个关键属性：对视觉因素变化的弹性，校准的不确定性估计，以及检测异常输入的能力。为此，我们研究了83个CLIP模型和127个ImageNet分类器。它们在架构、（预）训练分布和训练策略上都具有多样性。我们考虑了10个视觉因素（例如形状和图案），5个...",
    "tldr": "本研究深入分析了对比语言-图像预训练(CLIP)模型的健壮性，重点关注了其在特定视觉因素变化、不确定性估计和异常输入检测等安全目标上的表现。",
    "en_tdlr": "This study comprehensively investigates the robustness of Contrastive Language-Image Pre-training (CLIP) models, specifically focusing on their performance in terms of variations in specific visual factors, uncertainty estimation, and detection of anomalous inputs."
}