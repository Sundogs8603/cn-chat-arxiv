{
    "title": "\"My Answer is C\": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models",
    "abstract": "arXiv:2402.14499v1 Announce Type: new  Abstract: The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with \"Sure\" or refusing to answer. Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavil",
    "link": "https://arxiv.org/abs/2402.14499",
    "context": "Title: \"My Answer is C\": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models\nAbstract: arXiv:2402.14499v1 Announce Type: new  Abstract: The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions (MCQ) to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with \"Sure\" or refusing to answer. Consequently, MCQ evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60%. Models heavil",
    "path": "papers/24/02/2402.14499.json",
    "total_tokens": 889,
    "translated_title": "\"我的答案是C\": 指令调整的语言模型中的第一个令牌概率与文本答案不匹配",
    "translated_abstract": "arXiv:2402.14499v1 公告类型: 新的 摘要: 语言生成的开放性质使得评估自回归大型语言模型（LLMs）具有挑战性。一种常见的评估方法是使用多项选择题（MCQ）限制响应空间。然后通过排名候选答案的第一个令牌预测的对数概率来评估模型。然而，第一个令牌可能不一致地反映最终的响应输出，因为模型具有多样化的响应风格，例如以\"确定\"开头或拒绝回答。因此，MCQ评估无法表明模型与用户互动时的行为。但差距有多大呢？我们评估了第一个令牌评估在几个维度上与文本输出的一致性，即最终选项选择、拒绝率、选择分布和在提示扰动下的稳健性。我们的结果显示，这两种方法在所有维度上严重不一致，达到60%以上的不匹配率。模型非常",
    "tldr": "第一个令牌预测不一定代表最终文本输出，在评估大型语言模型时存在严重的不一致性，影响模型行为与用户互动。",
    "en_tdlr": "First-token predictions may not match the final text output, leading to severe misalignment in evaluating large language models and their interactions with users."
}