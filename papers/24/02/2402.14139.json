{
    "title": "NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning",
    "abstract": "arXiv:2402.14139v1 Announce Type: new  Abstract: Efficient on-device convolutional neural network (CNN) training in resource-constrained mobile and edge environments is an open challenge. Backpropagation is the standard approach adopted, but it is GPU memory intensive due to its strong inter-layer dependencies that demand intermediate activations across the entire CNN model to be retained in GPU memory. This necessitates smaller batch sizes to make training possible within the available GPU memory budget, but in turn, results in a substantially high and impractical training time. We introduce NeuroFlux, a novel CNN training system tailored for memory-constrained scenarios. We develop two novel opportunities: firstly, adaptive auxiliary networks that employ a variable number of filters to reduce GPU memory usage, and secondly, block-specific adaptive batch sizes, which not only cater to the GPU memory constraints but also accelerate the training process. NeuroFlux segments the CNNs into",
    "link": "https://arxiv.org/abs/2402.14139",
    "context": "Title: NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning\nAbstract: arXiv:2402.14139v1 Announce Type: new  Abstract: Efficient on-device convolutional neural network (CNN) training in resource-constrained mobile and edge environments is an open challenge. Backpropagation is the standard approach adopted, but it is GPU memory intensive due to its strong inter-layer dependencies that demand intermediate activations across the entire CNN model to be retained in GPU memory. This necessitates smaller batch sizes to make training possible within the available GPU memory budget, but in turn, results in a substantially high and impractical training time. We introduce NeuroFlux, a novel CNN training system tailored for memory-constrained scenarios. We develop two novel opportunities: firstly, adaptive auxiliary networks that employ a variable number of filters to reduce GPU memory usage, and secondly, block-specific adaptive batch sizes, which not only cater to the GPU memory constraints but also accelerate the training process. NeuroFlux segments the CNNs into",
    "path": "papers/24/02/2402.14139.json",
    "total_tokens": 717,
    "translated_title": "NeuroFlux: 使用自适应局部学习进行高效CNN训练",
    "translated_abstract": "在资源受限的移动和边缘环境中进行高效的设备内卷积神经网络（CNN）训练是一个挑战。本文介绍了NeuroFlux，这是一个为内存受限场景量身定制的CNN训练系统。我们提出了两个创新机遇：第一是采用可变数量滤波器的自适应辅助网络，以减少GPU内存的使用；第二是针对块特定的自适应批处理大小，既满足GPU内存限制，又加速训练过程。",
    "tldr": "NeuroFlux是一个为内存受限场景量身定制的CNN训练系统，提出了自适应辅助网络和块特定的自适应批处理大小的创新机遇。",
    "en_tdlr": "NeuroFlux is a CNN training system tailored for memory-constrained scenarios, introducing innovative opportunities of adaptive auxiliary networks and block-specific adaptive batch sizes."
}