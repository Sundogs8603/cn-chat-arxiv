{
    "title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification",
    "abstract": "arXiv:2402.14158v1 Announce Type: new  Abstract: Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced.",
    "link": "https://arxiv.org/abs/2402.14158",
    "context": "Title: TOOLVERIFIER: Generalization to New Tools via Self-Verification\nAbstract: arXiv:2402.14158v1 Announce Type: new  Abstract: Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced.",
    "path": "papers/24/02/2402.14158.json",
    "total_tokens": 839,
    "translated_title": "TOOLVERIFIER: 通过自验证实现对新工具的泛化",
    "translated_abstract": "将语言模型教会如何使用工具是迈向构建通用助手的重要里程碑，但仍然是一个未解之谜。虽然在针对特定工具的微调方面取得了显著进展，但语言模型仍然在如何从仅有少数示例中强大地使用新工具方面遇到困难。在这项工作中，我们引入了一种自验证方法，通过在工具选择和参数生成过程中进行对比性自问问题来区分近似的候选工具。我们利用Llama-2 70B构建了合成的高质量自生成数据，用于实现这一目标，我们打算将其公开发布。在ToolBench基准测试中的4项任务上进行了大量实验，包括17个未见过的工具，展示了在少样本基线测试中平均提升了22%，即使在候选工具之间的区别微妙之处的情况下也是如此。",
    "tldr": "通过自验证方法，本研究提出了一种区分新工具的方法，通过对比性自问问题来实现工具选择和参数生成，进一步提升了语言模型对新工具的学习能力。"
}