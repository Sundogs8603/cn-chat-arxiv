{
    "title": "Adaptive proximal gradient methods are universal without approximation",
    "abstract": "We show that adaptive proximal gradient methods for convex problems are not restricted to traditional Lipschitzian assumptions. Our analysis reveals that a class of linesearch-free methods is still convergent under mere local H\\\"older gradient continuity, covering in particular continuously differentiable semi-algebraic functions. To mitigate the lack of local Lipschitz continuity, popular approaches revolve around $\\varepsilon$-oracles and/or linesearch procedures. In contrast, we exploit plain H\\\"older inequalities not entailing any approximation, all while retaining the linesearch-free nature of adaptive schemes. Furthermore, we prove full sequence convergence without prior knowledge of local H\\\"older constants nor of the order of H\\\"older continuity. In numerical experiments we present comparisons to baseline methods on diverse tasks from machine learning covering both the locally and the globally H\\\"older setting.",
    "link": "https://arxiv.org/abs/2402.06271",
    "context": "Title: Adaptive proximal gradient methods are universal without approximation\nAbstract: We show that adaptive proximal gradient methods for convex problems are not restricted to traditional Lipschitzian assumptions. Our analysis reveals that a class of linesearch-free methods is still convergent under mere local H\\\"older gradient continuity, covering in particular continuously differentiable semi-algebraic functions. To mitigate the lack of local Lipschitz continuity, popular approaches revolve around $\\varepsilon$-oracles and/or linesearch procedures. In contrast, we exploit plain H\\\"older inequalities not entailing any approximation, all while retaining the linesearch-free nature of adaptive schemes. Furthermore, we prove full sequence convergence without prior knowledge of local H\\\"older constants nor of the order of H\\\"older continuity. In numerical experiments we present comparisons to baseline methods on diverse tasks from machine learning covering both the locally and the globally H\\\"older setting.",
    "path": "papers/24/02/2402.06271.json",
    "total_tokens": 944,
    "translated_title": "适应性近端梯度方法在没有近似的情况下是通用的",
    "translated_abstract": "我们证明，对于凸问题，适应性近端梯度方法不受传统利普希兹假设的限制。我们的分析揭示了一类无需线搜索的方法在仅具有局部H\\\"older梯度连续性的情况下仍然收敛，特别适用于连续可微的半代数函数。为了弥补缺乏局部利普希兹连续性的问题，常见的方法包括$\\varepsilon$-oracle和/或线搜索步骤。相反，我们利用普通的H\\\"older不等式而不涉及任何近似，同时保持适应性方案无需线搜索的特性。此外，我们证明了不需要先验知识的局部H\\\"older常数或H\\\"older连续性的顺序，也可以实现完全的序列收敛性。在数值实验中，我们对机器学习中的不同任务进行了基准方法的比较，涵盖了局部和全局 H\\\"older 设置。",
    "tldr": "该论文证明了适应性近端梯度方法对于凸问题不受传统假设的限制，并且可以在局部梯度H\\\"older连续性条件下收敛，同时避免了线搜索步骤和近似的使用。对局部H\\\"older常数和H\\\"older连续性顺序的先验知识也不是必需的。在数值实验中，与基准方法进行了对比实验，涵盖了局部和全局 H\\\"older 设置。",
    "en_tdlr": "This paper shows that adaptive proximal gradient methods for convex problems are not restricted to traditional assumptions, and can converge under local gradient H\\\"older continuity without line search and approximation. Prior knowledge of local constants or order of H\\\"older continuity is not required. Numerical experiments compare the methods to baseline approaches in both locally and globally H\\\"older settings."
}