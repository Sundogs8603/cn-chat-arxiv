{
    "title": "State Space Models for Event Cameras",
    "abstract": "arXiv:2402.15584v1 Announce Type: cross  Abstract: Today, state-of-the-art deep neural networks that process event-camera data first convert a temporal window of events into dense, grid-like input representations. As such, they exhibit poor generalizability when deployed at higher inference frequencies (i.e., smaller temporal windows) than the ones they were trained on. We address this challenge by introducing state-space models (SSMs) with learnable timescale parameters to event-based vision. This design adapts to varying frequencies without the need to retrain the network at different frequencies. Additionally, we investigate two strategies to counteract aliasing effects when deploying the model at higher frequencies. We comprehensively evaluate our approach against existing methods based on RNN and Transformer architectures across various benchmarks, including Gen1 and 1 Mpx event camera datasets. Our results demonstrate that SSM-based models train 33% faster and also exhibit minima",
    "link": "https://arxiv.org/abs/2402.15584",
    "context": "Title: State Space Models for Event Cameras\nAbstract: arXiv:2402.15584v1 Announce Type: cross  Abstract: Today, state-of-the-art deep neural networks that process event-camera data first convert a temporal window of events into dense, grid-like input representations. As such, they exhibit poor generalizability when deployed at higher inference frequencies (i.e., smaller temporal windows) than the ones they were trained on. We address this challenge by introducing state-space models (SSMs) with learnable timescale parameters to event-based vision. This design adapts to varying frequencies without the need to retrain the network at different frequencies. Additionally, we investigate two strategies to counteract aliasing effects when deploying the model at higher frequencies. We comprehensively evaluate our approach against existing methods based on RNN and Transformer architectures across various benchmarks, including Gen1 and 1 Mpx event camera datasets. Our results demonstrate that SSM-based models train 33% faster and also exhibit minima",
    "path": "papers/24/02/2402.15584.json",
    "total_tokens": 839,
    "translated_title": "事件相机的状态空间模型",
    "translated_abstract": "如今，处理事件相机数据的最先进的深度神经网络首先将一段时间内的事件转换为稠密的网格状输入表示。因此，在部署推断频率高于它们训练时的频率（即时间窗口较小）时，它们表现出较差的泛化能力。我们通过引入具有可学习时间尺度参数的状态空间模型（SSMs）来应对这一挑战。这种设计适应不同频率而无需在不同频率下重新训练网络。此外，我们研究了两种对抗混叠效应的策略，当在更高频率下部署模型时。我们对我们的方法进行了全面评估，对抗基于RNN和Transformer架构的现有方法，包括Gen1和1 Mpx事件相机数据集。我们的结果表明，基于SSM的模型训练速度快33%，同时也表现出最小值。",
    "tldr": "通过引入具有可学习时间尺度参数的状态空间模型（SSMs），以适应不同频率而无需重新训练网络，并研究了两种对抗混叠效应的策略，该方法训练速度快33%。",
    "en_tdlr": "State-space models (SSMs) with learnable timescale parameters are introduced to adapt to different frequencies without retraining the network, along with two strategies to counteract aliasing effects, resulting in 33% faster training."
}