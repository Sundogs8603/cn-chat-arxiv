{
    "title": "Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization",
    "abstract": "arXiv:2402.10342v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level el",
    "link": "https://arxiv.org/abs/2402.10342",
    "context": "Title: Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization\nAbstract: arXiv:2402.10342v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level el",
    "path": "papers/24/02/2402.10342.json",
    "total_tokens": 936,
    "translated_title": "在RLHF中基于探索驱动的策略优化：关于有效数据利用的理论洞见",
    "translated_abstract": "强化学习从人类反馈（RLHF）在依赖少量人类反馈的情况下取得了令人印象深刻的经验成功。然而，对于这种现象存在着有限的理论证明。此外，尽管最近的经验成功采用了基于策略的算法，但大多数最近的研究仍侧重于基于价值的算法。在这项工作中，我们考虑了基于策略优化（PO-RLHF）的RLHF算法。该算法基于流行的策略覆盖-策略梯度（PC-PG）算法，该算法假设对奖励函数有知识。在PO-RLHF中，不假设知道奖励函数，并且该算法依赖于基于轨迹的比较反馈来推断奖励函数。我们为PO-RLHF提供了低查询复杂度的性能界限，这为解释为什么少量的人类反馈可能足以在RLHF中获得良好性能提供了洞见。一个关键的创新是我们的轨迹级el",
    "tldr": "本研究提出了一个基于探索驱动策略优化的RLHF算法，通过轨迹比较反馈推断奖励函数，为解释少量人类反馈足以实现良好性能提供了理论洞见",
    "en_tdlr": "This paper introduces a RLHF algorithm based on exploration-driven policy optimization, which infers the reward function through trajectory comparison feedback, providing theoretical insights into why a small amount of human feedback may be sufficient for good performance in RLHF."
}