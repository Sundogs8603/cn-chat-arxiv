{
    "title": "Random Projection Neural Networks of Best Approximation: Convergence theory and practical applications",
    "abstract": "arXiv:2402.11397v1 Announce Type: new  Abstract: We investigate the concept of Best Approximation for Feedforward Neural Networks (FNN) and explore their convergence properties through the lens of Random Projection (RPNNs). RPNNs have predetermined and fixed, once and for all, internal weights and biases, offering computational efficiency. We demonstrate that there exists a choice of external weights, for any family of such RPNNs, with non-polynomial infinitely differentiable activation functions, that exhibit an exponential convergence rate when approximating any infinitely differentiable function. For illustration purposes, we test the proposed RPNN-based function approximation, with parsimoniously chosen basis functions, across five benchmark function approximation problems. Results show that RPNNs achieve comparable performance to established methods such as Legendre Polynomials, highlighting their potential for efficient and accurate function approximation.",
    "link": "https://arxiv.org/abs/2402.11397",
    "context": "Title: Random Projection Neural Networks of Best Approximation: Convergence theory and practical applications\nAbstract: arXiv:2402.11397v1 Announce Type: new  Abstract: We investigate the concept of Best Approximation for Feedforward Neural Networks (FNN) and explore their convergence properties through the lens of Random Projection (RPNNs). RPNNs have predetermined and fixed, once and for all, internal weights and biases, offering computational efficiency. We demonstrate that there exists a choice of external weights, for any family of such RPNNs, with non-polynomial infinitely differentiable activation functions, that exhibit an exponential convergence rate when approximating any infinitely differentiable function. For illustration purposes, we test the proposed RPNN-based function approximation, with parsimoniously chosen basis functions, across five benchmark function approximation problems. Results show that RPNNs achieve comparable performance to established methods such as Legendre Polynomials, highlighting their potential for efficient and accurate function approximation.",
    "path": "papers/24/02/2402.11397.json",
    "total_tokens": 817,
    "translated_title": "最佳逼近的随机投影神经网络：收敛理论和实际应用",
    "translated_abstract": "我们研究了前馈神经网络（FNN）的最佳逼近概念，并通过随机投影神经网络（RPNNs）的视角探讨它们的收敛性质。RPNNs具有预先确定且固定的内部权重和偏置，提供了计算效率。我们证明了对于任何一族具有非多项式无穷可微激活函数的RPNNs，存在外部权重的选择，展现出指数收敛速率来逼近任意无穷可微函数。为了说明，我们在五个基准函数逼近问题上测试了基于RPNN的函数逼近，选择了简洁的基函数。结果显示，RPNNs达到了与勒让德多项式等已建立方法相媲美的性能，突显了它们在高效准确函数逼近方面的潜力。",
    "tldr": "RPNNs具有固定内部权重和偏置，通过选择外部权重，它们展现出指数收敛速率来逼近任意无穷可微函数，在函数逼近问题上表现出高效准确的潜力",
    "en_tdlr": "RPNNs with fixed internal weights and biases show exponential convergence rate when approximating any infinitely differentiable function through the choice of external weights, highlighting their potential for efficient and accurate function approximation."
}