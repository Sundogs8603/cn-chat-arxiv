{
    "title": "LLM-based Federated Recommendation",
    "abstract": "arXiv:2402.09959v1 Announce Type: new  Abstract: Large Language Models (LLMs), with their advanced contextual understanding abilities, have demonstrated considerable potential in enhancing recommendation systems via fine-tuning methods. However, fine-tuning requires users' behavior data, which poses considerable privacy risks due to the incorporation of sensitive user information. The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues. To mitigate these privacy issues, Federated Learning for Recommendation (Fed4Rec) has emerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-based recommendation presents two main challenges: first, an increase in the imbalance of performance across clients, affecting the system's efficiency over time, and second, a high demand on clients' computational and storage resources for local training and inference of LLMs.   To address these challenges, we introduce a Privacy-Preserving LL",
    "link": "https://arxiv.org/abs/2402.09959",
    "context": "Title: LLM-based Federated Recommendation\nAbstract: arXiv:2402.09959v1 Announce Type: new  Abstract: Large Language Models (LLMs), with their advanced contextual understanding abilities, have demonstrated considerable potential in enhancing recommendation systems via fine-tuning methods. However, fine-tuning requires users' behavior data, which poses considerable privacy risks due to the incorporation of sensitive user information. The unintended disclosure of such data could infringe upon data protection laws and give rise to ethical issues. To mitigate these privacy issues, Federated Learning for Recommendation (Fed4Rec) has emerged as a promising approach. Nevertheless, applying Fed4Rec to LLM-based recommendation presents two main challenges: first, an increase in the imbalance of performance across clients, affecting the system's efficiency over time, and second, a high demand on clients' computational and storage resources for local training and inference of LLMs.   To address these challenges, we introduce a Privacy-Preserving LL",
    "path": "papers/24/02/2402.09959.json",
    "total_tokens": 798,
    "translated_title": "基于LLM的联邦推荐系统",
    "translated_abstract": "大规模语言模型（LLM）通过微调方法展示了改进推荐系统的巨大潜力，具备先进的上下文理解能力。然而，微调需要用户行为数据，这会带来隐私风险，因为包含了敏感用户信息。这些数据的意外泄露可能侵犯数据保护法，并引发伦理问题。为了减轻这些隐私问题，联邦学习推荐系统（Fed4Rec）被提出作为一种有前景的方法。然而，将Fed4Rec应用于基于LLM的推荐系统面临两个主要挑战：首先，客户端性能不平衡加剧，影响系统的效率；其次，对于本地训练和推理LLM，对客户端的计算和存储资源需求很高。",
    "tldr": "这项研究介绍了一种基于LLM的联邦推荐系统，用于提高推荐系统的性能和隐私保护。面临的挑战是客户端性能不平衡和对计算资源的高需求。",
    "en_tdlr": "This research introduces a Federated Recommendation system based on Large Language Models (LLMs) aiming to enhance recommendation performance and preserve privacy. The challenges include performance imbalance across clients and high demand for computational resources."
}