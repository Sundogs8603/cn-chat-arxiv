{
    "title": "GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence",
    "abstract": "arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre",
    "link": "https://arxiv.org/abs/2402.12566",
    "context": "Title: GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence\nAbstract: arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre",
    "path": "papers/24/02/2402.12566.json",
    "total_tokens": 846,
    "translated_title": "GenAudit：利用证据修复语言模型输出中的事实错误",
    "translated_abstract": "LLMs即使可以访问参考文档，也可能生成事实不准确的陈述。在高风险应用中（例如基于文档的医疗保健或金融问答），这样的错误可能具有危险性。我们提出了GenAudit -- 一个旨在帮助检查基于文档任务语言模型响应的工具。GenAudit通过修订或删除未被参考文档支持的声明，同时为看似被证据支持的事实提供来自参考文献的证据，来建议修改LLM响应。我们训练模型来执行这些任务，并设计了一个交互界面，向用户呈现建议的修改和证据。通过人工评分员的全面评估显示，GenAudit在总结不同领域文档时能够检测出8种不同的LLM输出中的错误。为确保系统能够标记大多数错误，我们提出了一种方法，可以提高错误召回率，同时最小化对预处理的影响。",
    "tldr": "GenAudit是一个工具，通过修订或删除未被参考文献支持的声明，并提供来自参考文献的证据，帮助修复语言模型输出中的事实错误。",
    "en_tdlr": "GenAudit is a tool that helps to fix factual errors in language model outputs by revising or removing unsupported claims and providing evidence from reference documents."
}