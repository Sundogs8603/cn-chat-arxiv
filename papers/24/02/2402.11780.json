{
    "title": "Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware",
    "abstract": "arXiv:2402.11780v1 Announce Type: cross  Abstract: With the recent growth in demand for large-scale deep neural networks, compute in-memory (CiM) has come up as a prominent solution to alleviate bandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman architectures. However, the construction of CiM hardware poses a challenge as any specific memory hierarchy in terms of cache sizes and memory bandwidth at different interfaces may not be ideally matched to any neural network's attributes such as tensor dimension and arithmetic intensity, thus leading to suboptimal and under-performing systems. Despite the success of neural architecture search (NAS) techniques in yielding efficient sub-networks for a given hardware metric budget (e.g., DNN execution time or latency), it assumes the hardware configuration to be frozen, often yielding sub-optimal sub-networks for a given budget. In this paper, we present CiMNet, a framework that jointly searches for optimal sub-networks and ",
    "link": "https://arxiv.org/abs/2402.11780",
    "context": "Title: Towards Joint Optimization for DNN Architecture and Configuration for Compute-In-Memory Hardware\nAbstract: arXiv:2402.11780v1 Announce Type: cross  Abstract: With the recent growth in demand for large-scale deep neural networks, compute in-memory (CiM) has come up as a prominent solution to alleviate bandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman architectures. However, the construction of CiM hardware poses a challenge as any specific memory hierarchy in terms of cache sizes and memory bandwidth at different interfaces may not be ideally matched to any neural network's attributes such as tensor dimension and arithmetic intensity, thus leading to suboptimal and under-performing systems. Despite the success of neural architecture search (NAS) techniques in yielding efficient sub-networks for a given hardware metric budget (e.g., DNN execution time or latency), it assumes the hardware configuration to be frozen, often yielding sub-optimal sub-networks for a given budget. In this paper, we present CiMNet, a framework that jointly searches for optimal sub-networks and ",
    "path": "papers/24/02/2402.11780.json",
    "total_tokens": 847,
    "translated_title": "面向计算存储硬件的深度神经网络架构和配置联合优化",
    "translated_abstract": "随着大规模深度神经网络需求的增长，计算存储（CiM）作为一种突出解决方案，有助于缓解约束冯·诺依曼体系结构的带宽和芯片内部连接瓶颈。然而，CiM硬件的构建面临挑战，因为在不同接口的缓存大小和内存带宽方面的任何特定内存层次结构可能不理想地匹配任何神经网络的属性，如张量维度和算术强度，因此导致次优和表现不佳的系统。 尽管神经架构搜索（NAS）技术在为给定硬件度量预算（例如DNN执行时间或延迟）产生高效子网络方面取得成功，但它假设硬件配置被冻结，通常为给定预算产生次优子网络。 本文提出了CiMNet，一个联合搜索最优子网络和",
    "tldr": "本文提出了CiMNet，一个旨在联合优化深度神经网络架构和配置的框架，以解决计算存储硬件构建中的挑战。"
}