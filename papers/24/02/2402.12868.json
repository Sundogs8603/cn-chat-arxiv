{
    "title": "Fast Rates in Online Convex Optimization by Exploiting the Curvature of Feasible Sets",
    "abstract": "arXiv:2402.12868v1 Announce Type: new  Abstract: In this paper, we explore online convex optimization (OCO) and introduce a new analysis that provides fast rates by exploiting the curvature of feasible sets. In online linear optimization, it is known that if the average gradient of loss functions is larger than a certain value, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret. This paper reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets. We first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret upper bound of $O(\\rho \\log T)$ in stochastic environments. Here, $\\rho > 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set. Our approach, unlike existing ones, can work directly with co",
    "link": "https://arxiv.org/abs/2402.12868",
    "context": "Title: Fast Rates in Online Convex Optimization by Exploiting the Curvature of Feasible Sets\nAbstract: arXiv:2402.12868v1 Announce Type: new  Abstract: In this paper, we explore online convex optimization (OCO) and introduce a new analysis that provides fast rates by exploiting the curvature of feasible sets. In online linear optimization, it is known that if the average gradient of loss functions is larger than a certain value, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret. This paper reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets. We first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret upper bound of $O(\\rho \\log T)$ in stochastic environments. Here, $\\rho > 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set. Our approach, unlike existing ones, can work directly with co",
    "path": "papers/24/02/2402.12868.json",
    "total_tokens": 711,
    "translated_title": "通过利用可行集的曲率，在在线凸优化中实现快速收敛速度",
    "translated_abstract": "在本文中，我们探讨了在线凸优化（OCO），介绍了一种通过利用可行集的曲率提供快速收敛速度的新分析。我们首先证明，如果最优决策位于可行集的边界上且基础损失函数的梯度非零，则算法在随机环境中可以达到$O(\\rho \\log T)$的遗憾上界。其中，$\\rho > 0$是包含最优决策并围绕可行集的最小球体的半径。",
    "tldr": "该论文提出了一种新的分析方法，通过利用可行集的曲率，在在线凸优化中实现了快速收敛速度。",
    "en_tdlr": "This paper introduces a new analysis that achieves fast convergence rates in online convex optimization by exploiting the curvature of feasible sets."
}