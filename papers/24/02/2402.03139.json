{
    "title": "Enhancing Neural Subset Selection: Integrating Background Information into Set Representations",
    "abstract": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an \\textit{invariant sufficient statistic} of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific super",
    "link": "https://arxiv.org/abs/2402.03139",
    "context": "Title: Enhancing Neural Subset Selection: Integrating Background Information into Set Representations\nAbstract: Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an \\textit{invariant sufficient statistic} of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific super",
    "path": "papers/24/02/2402.03139.json",
    "total_tokens": 793,
    "translated_title": "提升神经子集选择：将背景信息融入到集合表示中",
    "translated_abstract": "学习神经子集选择任务，如AI辅助药物发现中的化合物选择，已经在各种应用中变得越来越重要。该领域中现有的方法主要集中于构建模型，捕捉效用函数值与其相应超集中子集之间的关系。然而，这些方法在利用神经网络建模集合函数时往往忽视了超集中包含的有价值信息。在这项工作中，我们采用概率论的观点来解决这个问题。我们的理论发现表明，当目标值在输入集合和子集的条件下时，将超集的不变量统计量纳入所关注的子集是有效学习的关键。这确保输出值对于子集及其相应的超集的排列是不变的，从而能够识别特定的超级子集。",
    "tldr": "这项研究提出了一种能够将背景信息融入神经子集选择任务中的方法，通过将超集的不变量统计量纳入所关注的子集，实现了对特定超级子集的识别。"
}