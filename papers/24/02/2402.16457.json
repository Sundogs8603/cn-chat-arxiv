{
    "title": "RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering",
    "abstract": "arXiv:2402.16457v1 Announce Type: new  Abstract: Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet e",
    "link": "https://arxiv.org/abs/2402.16457",
    "context": "Title: RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering\nAbstract: arXiv:2402.16457v1 Announce Type: new  Abstract: Adaptive retrieval-augmented generation (ARAG) aims to dynamically determine the necessity of retrieval for queries instead of retrieving indiscriminately to enhance the efficiency and relevance of the sourced information. However, previous works largely overlook the evaluation of ARAG approaches, leading to their effectiveness being understudied. This work presents a benchmark, RetrievalQA, comprising 1,271 short-form questions covering new world and long-tail knowledge. The knowledge necessary to answer the questions is absent from LLMs; therefore, external information must be retrieved to answer correctly. This makes RetrievalQA a suitable testbed to evaluate existing ARAG methods. We observe that calibration-based methods heavily rely on threshold tuning, while vanilla prompting is inadequate for guiding LLMs to make reliable retrieval decisions. Based on our findings, we propose Time-Aware Adaptive Retrieval (TA-ARE), a simple yet e",
    "path": "papers/24/02/2402.16457.json",
    "total_tokens": 864,
    "translated_title": "RetrievalQA：评估自适应检索增强生成技术用于短文开放领域问答",
    "translated_abstract": "自适应检索增强生成（ARAG）旨在动态确定查询是否需要检索，而不是无选择地检索，以增强信息的效率和相关性。然而，先前的研究在评估ARAG方法方面大多被忽视，导致它们的有效性未受到充分研究。本研究提出一种基准，RetrievalQA，包含1,271个涵盖新世界和长尾知识的短文问题。回答这些问题所需的知识不在LLM中；因此，必须检索外部信息来正确回答。这使得RetrievalQA成为评估现有ARAG方法的合适测试平台。我们发现基于校准的方法严重依赖于阈值调整，而普通提示无法有效引导LLMs做出可靠的检索决策。基于我们的发现，提出了一种简单而有效的Time-Aware Adaptive Retrieval（TA-ARE）",
    "tldr": "本研究提出了一个新的基准RetrievalQA，用于评估自适应检索增强生成技术，发现现有方法存在问题并提出了一种新的方法TA-ARE",
    "en_tdlr": "This study introduces a new benchmark, RetrievalQA, for evaluating adaptive retrieval-augmented generation techniques, identifies issues with existing methods, and proposes a new approach TA-ARE."
}