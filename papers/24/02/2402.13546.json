{
    "title": "LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs",
    "abstract": "arXiv:2402.13546v1 Announce Type: new  Abstract: Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector a",
    "link": "https://arxiv.org/abs/2402.13546",
    "context": "Title: LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs\nAbstract: arXiv:2402.13546v1 Announce Type: new  Abstract: Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector a",
    "path": "papers/24/02/2402.13546.json",
    "total_tokens": 883,
    "translated_title": "LLMs与长视频相遇：在LLMs中利用互动式视觉适配器推进长视频理解",
    "translated_abstract": "长视频理解是多媒体和人工智能交叉领域中一项重要且持续挑战。利用大型语言模型(LLMs)来理解视频成为一种新兴且有前景的方法。然而，由于视频令牌数量庞大，这种方法导致计算成本高，视觉清晰度降低，还面临着在回答视频相关问题时出现无关视觉令牌所带来的挑战。为了缓解这些问题，我们在LLMs中提出了一个交互式视觉适配器(IVA)，旨在增强与细粒度视觉元素的交互。具体来说，我们首先通过利用视觉编码器和预训练因果变换器将长视频转换为时间视频令牌，然后将它们与视频说明一起输入LLMs。随后，我们集成了IVA，其中包含一个轻量级的时间帧选择器",
    "tldr": "介绍了一个交互式视觉适配器（IVA），用于在LLMs中增强对细粒度视觉元素的交互，并解决了长视频理解中的计算成本高、视觉清晰度降低和无关视觉令牌带来的挑战。",
    "en_tdlr": "Introduced an Interactive Visual Adapter (IVA) to enhance interaction with fine-grained visual elements in LLMs, addressing the high computational costs, reduced visual clarity, and challenges from irrelevant visual tokens in long video comprehension."
}