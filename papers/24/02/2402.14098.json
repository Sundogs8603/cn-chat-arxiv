{
    "title": "Intriguing Properties of Modern GANs",
    "abstract": "arXiv:2402.14098v1 Announce Type: new  Abstract: Modern GANs achieve remarkable performance in terms of generating realistic and diverse samples. This has led many to believe that ``GANs capture the training data manifold''. In this work we show that this interpretation is wrong. We empirically show that the manifold learned by modern GANs does not fit the training distribution: specifically the manifold does not pass through the training examples and passes closer to out-of-distribution images than to in-distribution images. We also investigate the distribution over images implied by the prior over the latent codes and study whether modern GANs learn a density that approximates the training distribution. Surprisingly, we find that the learned density is very far from the data distribution and that GANs tend to assign higher density to out-of-distribution images. Finally, we demonstrate that the set of images used to train modern GANs are often not part of the typical set described by ",
    "link": "https://arxiv.org/abs/2402.14098",
    "context": "Title: Intriguing Properties of Modern GANs\nAbstract: arXiv:2402.14098v1 Announce Type: new  Abstract: Modern GANs achieve remarkable performance in terms of generating realistic and diverse samples. This has led many to believe that ``GANs capture the training data manifold''. In this work we show that this interpretation is wrong. We empirically show that the manifold learned by modern GANs does not fit the training distribution: specifically the manifold does not pass through the training examples and passes closer to out-of-distribution images than to in-distribution images. We also investigate the distribution over images implied by the prior over the latent codes and study whether modern GANs learn a density that approximates the training distribution. Surprisingly, we find that the learned density is very far from the data distribution and that GANs tend to assign higher density to out-of-distribution images. Finally, we demonstrate that the set of images used to train modern GANs are often not part of the typical set described by ",
    "path": "papers/24/02/2402.14098.json",
    "total_tokens": 828,
    "translated_title": "现代生成对抗网络的有趣属性",
    "translated_abstract": "现代生成对抗网络在生成逼真且多样化样本方面取得了显著的性能。这引发了许多人认为“生成对抗网络捕捉了训练数据流形”。在这项工作中，我们表明这种解释是错误的。我们在经验上展示了现代生成对抗网络学习的流形不适合训练分布：具体而言，该流形不经过训练样本，而是更接近于分布之外的图像。我们还研究了由潜在编码上的先验隐含的图像分布，并研究现代生成对抗网络是否学习了一个逼近训练分布的密度。令人惊讶的是，我们发现学习到的密度与数据分布相差甚远，生成对抗网络倾向于将更高的密度分配给分布之外的图像。最后，我们证明用于训练现代生成对抗网络的图像集通常不属于典型描述的集合。",
    "tldr": "现代生成对抗网络学习的流形不符合训练数据分布，学习到的密度与数据分布相差甚远。",
    "en_tdlr": "Modern GANs learn a manifold that does not fit the training data distribution and the learned density is very far from the data distribution."
}