{
    "title": "PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization",
    "abstract": "arXiv:2402.16141v1 Announce Type: new  Abstract: Supervised fine-tuning is the most common method to adapt large language models (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive computational resources. Recently, parameter-efficient fine-tuning (PEFT) methods have been widely studied due to its cost-effectiveness. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low-dimensional. Although LoRA fine-tuning is effective, there is still a performance gap compared to full fine-tuning, since its weight update is limited to low-rank matrices. In order to break the low-rank bottleneck in LoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank update matrices multiple times to achieve a higher update rank. PLoRA has multiple training stages. During each stage, we still update only the LoRA weights. However, at the end of each stage, we unload the LoRA weights into the backbone parameters and then",
    "link": "https://arxiv.org/abs/2402.16141",
    "context": "Title: PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization\nAbstract: arXiv:2402.16141v1 Announce Type: new  Abstract: Supervised fine-tuning is the most common method to adapt large language models (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive computational resources. Recently, parameter-efficient fine-tuning (PEFT) methods have been widely studied due to its cost-effectiveness. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low-dimensional. Although LoRA fine-tuning is effective, there is still a performance gap compared to full fine-tuning, since its weight update is limited to low-rank matrices. In order to break the low-rank bottleneck in LoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank update matrices multiple times to achieve a higher update rank. PLoRA has multiple training stages. During each stage, we still update only the LoRA weights. However, at the end of each stage, we unload the LoRA weights into the backbone parameters and then",
    "path": "papers/24/02/2402.16141.json",
    "total_tokens": 885,
    "translated_title": "PeriodicLoRA: 打破LoRA优化中的低秩瓶颈",
    "translated_abstract": "监督微调是使大型语言模型（LLMs）适应下游任务的常见方法，但全微调LLMs需要大量计算资源。最近，由于其成本效益，参数高效微调（PEFT）方法得到了广泛研究。 LoRA是最广泛使用的方法之一，它假设优化过程本质上是低维的。虽然LoRA微调是有效的，但与全微调相比仍存在性能差距，因为其权重更新仅限于低秩矩阵。为了打破LoRA优化中的低秩瓶颈，我们提出了PeriodicLoRA（PLoRA），它多次累积低秩更新矩阵以实现更高的更新秩。PLoRA具有多个训练阶段。在每个阶段，我们仍然仅更新LoRA权重。然而，在每个阶段结束时，我们将LoRA权重卸载到骨干参数中，然后...",
    "tldr": "提出了PeriodicLoRA（PLoRA）来打破LoRA优化中的低秩瓶颈，通过多次累积低秩更新矩阵来实现更高的更新秩，从而提高性能。",
    "en_tdlr": "Introduced PeriodicLoRA (PLoRA) to break the low-rank bottleneck in LoRA optimization by accumulating low-rank update matrices multiple times to achieve a higher update rank and improve performance."
}