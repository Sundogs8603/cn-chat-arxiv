{
    "title": "Learning Diverse Policies with Soft Self-Generated Guidance",
    "abstract": "Reinforcement learning (RL) with sparse and deceptive rewards is challenging because non-zero rewards are rarely obtained. Hence, the gradient calculated by the agent can be stochastic and without valid information. Recent studies that utilize memory buffers of previous experiences can lead to a more efficient learning process. However, existing methods often require these experiences to be successful and may overly exploit them, which can cause the agent to adopt suboptimal behaviors. This paper develops an approach that uses diverse past trajectories for faster and more efficient online RL, even if these trajectories are suboptimal or not highly rewarded. The proposed algorithm combines a policy improvement step with an additional exploration step using offline demonstration data. The main contribution of this paper is that by regarding diverse past trajectories as guidance, instead of imitating them, our method directs its policy to follow and expand past trajectories while still be",
    "link": "https://arxiv.org/abs/2402.04539",
    "context": "Title: Learning Diverse Policies with Soft Self-Generated Guidance\nAbstract: Reinforcement learning (RL) with sparse and deceptive rewards is challenging because non-zero rewards are rarely obtained. Hence, the gradient calculated by the agent can be stochastic and without valid information. Recent studies that utilize memory buffers of previous experiences can lead to a more efficient learning process. However, existing methods often require these experiences to be successful and may overly exploit them, which can cause the agent to adopt suboptimal behaviors. This paper develops an approach that uses diverse past trajectories for faster and more efficient online RL, even if these trajectories are suboptimal or not highly rewarded. The proposed algorithm combines a policy improvement step with an additional exploration step using offline demonstration data. The main contribution of this paper is that by regarding diverse past trajectories as guidance, instead of imitating them, our method directs its policy to follow and expand past trajectories while still be",
    "path": "papers/24/02/2402.04539.json",
    "total_tokens": 872,
    "translated_title": "使用软件自生成的引导学习多样化策略",
    "translated_abstract": "强化学习中，稀疏和具有误导性的奖励使得学习变得困难，因为几乎很少能够获得非零奖励。因此，智能体计算的梯度可能是随机的且缺乏有效信息。最近的研究利用先前经验的内存缓冲区可以使学习过程更高效。然而，现有方法通常要求这些经验必须成功，并可能过度利用它们，这可能导致智能体采取次优的行为。本文提出了一种方法，即使用多样化的过去轨迹进行更快、更高效的在线强化学习，即使这些轨迹是次优的或没有高奖励。所提出的算法结合了策略改进步骤和使用离线演示数据的额外探索步骤。本文的主要贡献是，将多样化的过去轨迹视为引导而不是模仿它们，我们的方法使策略跟随和扩展过去的轨迹，同时仍保持",
    "tldr": "本文提出了一种使用多样化的过去轨迹作为引导的方法，以实现更快、更高效的在线强化学习，即使这些轨迹是次优的或没有高奖励。通过将过去轨迹视为引导，而不是模仿它们，本方法可以使策略跟随和扩展过去的轨迹同时仍保持"
}