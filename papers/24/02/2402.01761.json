{
    "title": "Rethinking Interpretability in the Era of Large Language Models",
    "abstract": "Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs.   In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We high",
    "link": "https://arxiv.org/abs/2402.01761",
    "context": "Title: Rethinking Interpretability in the Era of Large Language Models\nAbstract: Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs.   In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We high",
    "path": "papers/24/02/2402.01761.json",
    "total_tokens": 807,
    "translated_title": "在大语言模型时代重新思考可解释性",
    "translated_abstract": "可解释的机器学习在过去十年中成为一个热门领域，受到越来越大的数据集和深度神经网络的崛起的推动。与此同时，大语言模型（LLMs）在各种任务中展示出了卓越的能力，为重新思考可解释机器学习的机会提供了可能。值得注意的是，以自然语言解释的能力使得LLMs能够扩展给人类的规模和复杂性上的模式。然而，这些新的能力也带来了新的挑战，比如虚构的解释和巨大的计算成本。在这篇立场论文中，我们首先回顾了评估新兴LLM解释领域的现有方法（包括解释LLM和使用LLM进行解释）。我们认为，尽管存在局限性，LLMs能够重新定义解释性，涵盖更广泛的应用领域，包括对LLMs本身的审计。",
    "tldr": "大语言模型具有以自然语言解释的能力，能够重新定义解释性，并且在多个应用中展示出巨大潜力。",
    "en_tdlr": "Large language models have the ability to explain in natural language, redefining interpretability with immense potential across multiple applications."
}