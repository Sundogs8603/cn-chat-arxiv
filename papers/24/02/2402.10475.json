{
    "title": "Fundamental Benefit of Alternating Updates in Minimax Optimization",
    "abstract": "arXiv:2402.10475v1 Announce Type: cross  Abstract: The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global convergence rates. To address this theory-practice gap, we present fine-grained convergence analyses of both algorithms for strongly-convex-strongly-concave and Lipschitz-gradient objectives. Our new iteration complexity upper bound of Alt-GDA is strictly smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster. Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main idea is to alternately take gradients from extrapolations of the iterates. We show that Alex-GDA satisfies a smaller it",
    "link": "https://arxiv.org/abs/2402.10475",
    "context": "Title: Fundamental Benefit of Alternating Updates in Minimax Optimization\nAbstract: arXiv:2402.10475v1 Announce Type: cross  Abstract: The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global convergence rates. To address this theory-practice gap, we present fine-grained convergence analyses of both algorithms for strongly-convex-strongly-concave and Lipschitz-gradient objectives. Our new iteration complexity upper bound of Alt-GDA is strictly smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster. Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main idea is to alternately take gradients from extrapolations of the iterates. We show that Alex-GDA satisfies a smaller it",
    "path": "papers/24/02/2402.10475.json",
    "total_tokens": 892,
    "translated_title": "极小化优化中交替更新的基本利益",
    "translated_abstract": "Gradient Descent-Ascent（GDA）算法旨在解决极小化优化问题，其中采用下降和上升步骤，分为同时进行（Sim-GDA）或交替进行（Alt-GDA）。 Alt-GDA通常收敛更快，但两者之间的性能差距尚未在理论上得到很好的理解，尤其是在全局收敛速率方面。为了解决这种理论与实践之间的差距，我们针对强凸强凹和Lipschitz梯度目标提出了对两种算法的细粒度收敛分析。我们的Alt-GDA的新迭代复杂性上界严格小于Sim-GDA的下界；即Alt-GDA被证明更快。此外，我们提出了交替外推GDA（Alex-GDA），这是一个包含Sim-GDA和Alt-GDA的通用算法框架，其主要思想是轮流从迭代的外推中获取梯度。我们展示了Alex-GDA满足更小的迭代复杂度。",
    "tldr": "Alt-GDA算法被证明更快，提出了交替外推GDA（Alex-GDA）通用算法框架，以轮流从迭代的外推中获取梯度。"
}