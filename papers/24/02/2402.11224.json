{
    "title": "Neural Networks with (Low-Precision) Polynomial Approximations: New Insights and Techniques for Accuracy Improvement",
    "abstract": "arXiv:2402.11224v1 Announce Type: new  Abstract: Replacing non-polynomial functions (e.g., non-linear activation functions such as ReLU) in a neural network with their polynomial approximations is a standard practice in privacy-preserving machine learning. The resulting neural network, called polynomial approximation of neural network (PANN) in this paper, is compatible with advanced cryptosystems to enable privacy-preserving model inference. Using ``highly precise'' approximation, state-of-the-art PANN offers similar inference accuracy as the underlying backbone model. However, little is known about the effect of approximation, and existing literature often determined the required approximation precision empirically. In this paper, we initiate the investigation of PANN as a standalone object. Specifically, our contribution is two-fold. Firstly, we provide an explanation on the effect of approximate error in PANN. In particular, we discovered that (1) PANN is susceptible to some type o",
    "link": "https://arxiv.org/abs/2402.11224",
    "context": "Title: Neural Networks with (Low-Precision) Polynomial Approximations: New Insights and Techniques for Accuracy Improvement\nAbstract: arXiv:2402.11224v1 Announce Type: new  Abstract: Replacing non-polynomial functions (e.g., non-linear activation functions such as ReLU) in a neural network with their polynomial approximations is a standard practice in privacy-preserving machine learning. The resulting neural network, called polynomial approximation of neural network (PANN) in this paper, is compatible with advanced cryptosystems to enable privacy-preserving model inference. Using ``highly precise'' approximation, state-of-the-art PANN offers similar inference accuracy as the underlying backbone model. However, little is known about the effect of approximation, and existing literature often determined the required approximation precision empirically. In this paper, we initiate the investigation of PANN as a standalone object. Specifically, our contribution is two-fold. Firstly, we provide an explanation on the effect of approximate error in PANN. In particular, we discovered that (1) PANN is susceptible to some type o",
    "path": "papers/24/02/2402.11224.json",
    "total_tokens": 836,
    "translated_title": "具有（低精度）多项式逼近的神经网络：准确性提高的新见解和技术",
    "translated_abstract": "在神经网络中，用多项式逼近替换非多项式函数（例如非线性激活函数，如ReLU）是隐私保护机器学习中的标准做法。本文中称之为神经网络的多项式逼近（PANN）的结果神经网络与先进的密码系统兼容，实现隐私保护模型推断。利用“高精度”逼近，最先进的PANN提供了与基础骨干模型相似的推断准确性。然而，关于逼近的影响知之甚少，并且现有文献通常是通过实证确定所需的逼近精度。在本文中，我们开始对PANN进行独立对象的研究。具体来说，我们的贡献是双重的。首先，我们解释了PANN中近似误差的影响。特别是，我们发现PANN容易受到某种类型的...",
    "tldr": "本文揭示了神经网络的多项式逼近作为一种独立对象的研究，发现PANN对某些类型的逼近误差..."
}