{
    "title": "Quantized Embedding Vectors for Controllable Diffusion Language Models",
    "abstract": "arXiv:2402.10107v1 Announce Type: cross  Abstract: Improving the controllability, portability, and inference speed of diffusion language models (DLMs) is a key challenge in natural language generation. While recent research has shown significant success in complex text generation with language models, the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models. To mitigate these issues, numerous well-established methods were proposed for neural network quantization. To further enhance their portability of independent deployment as well as improve their stability evaluated by language perplexity, we propose a novel approach called the Quantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM builds upon the recent successful controllable DLMs by remodeling the task-specific embedding space via quantization. This leads to a gradient-based controller for the generat",
    "link": "https://arxiv.org/abs/2402.10107",
    "context": "Title: Quantized Embedding Vectors for Controllable Diffusion Language Models\nAbstract: arXiv:2402.10107v1 Announce Type: cross  Abstract: Improving the controllability, portability, and inference speed of diffusion language models (DLMs) is a key challenge in natural language generation. While recent research has shown significant success in complex text generation with language models, the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models. To mitigate these issues, numerous well-established methods were proposed for neural network quantization. To further enhance their portability of independent deployment as well as improve their stability evaluated by language perplexity, we propose a novel approach called the Quantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM builds upon the recent successful controllable DLMs by remodeling the task-specific embedding space via quantization. This leads to a gradient-based controller for the generat",
    "path": "papers/24/02/2402.10107.json",
    "total_tokens": 757,
    "translated_title": "可控扩散语言模型的量化嵌入向量",
    "translated_abstract": "改善扩散语言模型的可控性、可移植性和推理速度是自然语言生成中的一个关键挑战。尽管最近的研究在语言模型的复杂文本生成方面取得了显著成功，但内存和计算能力仍然非常苛刻，无法满足预期，这自然导致模型的可移植性和稳定性较低。为了解决这些问题，我们提出了一种新的方法，称为量化嵌入可控扩散语言模型（QE-CDLM）。QE-CDLM基于最近成功的可控DLM，通过量化重建了任务特定的嵌入空间。这导致了一种基于梯度的生成式控制器。",
    "tldr": "本论文提出了一种量化嵌入可控扩散语言模型（QE-CDLM），通过重建任务特定的嵌入空间来改善扩散语言模型的可控性、可移植性和稳定性。"
}