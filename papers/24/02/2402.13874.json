{
    "title": "$\\texttt{Se}^2$: $\\textit{Se}$quential Example $\\textit{Se}$lection for In-Context Learning",
    "abstract": "arXiv:2402.13874v1 Announce Type: new  Abstract: The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the \"select then organize\" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\\textit{se}$quential $\\textit{se}$lection problem and introduce $\\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories i",
    "link": "https://arxiv.org/abs/2402.13874",
    "context": "Title: $\\texttt{Se}^2$: $\\textit{Se}$quential Example $\\textit{Se}$lection for In-Context Learning\nAbstract: arXiv:2402.13874v1 Announce Type: new  Abstract: The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the \"select then organize\" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\\textit{se}$quential $\\textit{se}$lection problem and introduce $\\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories i",
    "path": "papers/24/02/2402.13874.json",
    "total_tokens": 906,
    "translated_title": "$\\texttt{Se}^2$: $\\textit{Se}$quential Example $\\textit{Se}$lection for In-Context Learning",
    "translated_abstract": "众所周知，大型语言模型（LLMs）在上下文学习（ICL）中具有出色的能力，但需要通过示例示范来激活。以往的研究广泛探讨了用于ICL的示例选择，主要遵循“先选择再组织”的范式，这些方法往往忽视示例之间的内在关系，存在训练和推理之间的不一致性。本文将问题表述为一个序贯选择问题，并引入$\\texttt{Se}^2$，这是一种顺序感知方法，利用LLM对不同上下文的反馈，有助于捕捉示例之间的相互关系和序列信息，显著丰富ICL提示的上下文相关性和相关性。同时，我们利用束搜索来寻找和构建示例序列，增强了质量和多样性。我们在8个不同类别中的23个NLP任务上进行了大量实验",
    "tldr": "本文提出了$\\texttt{Se}^2$，一种顺序感知方法，利用大型语言模型的反馈帮助捕捉示例之间的相互关系和序列信息，显著丰富了上下文学习提示的相关性和相关性。",
    "en_tdlr": "This paper introduces $\\texttt{Se}^2$, a sequential-aware method that leverages feedback from large language models to capture inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of in-context learning prompts."
}