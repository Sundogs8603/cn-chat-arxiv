{
    "title": "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL",
    "abstract": "arXiv:2402.19446v1 Announce Type: cross  Abstract: A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or \"agent\" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fin",
    "link": "https://arxiv.org/abs/2402.19446",
    "context": "Title: ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL\nAbstract: arXiv:2402.19446v1 Announce Type: cross  Abstract: A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or \"agent\" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fin",
    "path": "papers/24/02/2402.19446.json",
    "total_tokens": 665,
    "translated_title": "ArCHer: 通过分层多轮强化学习训练语言模型代理",
    "translated_abstract": "大型语言模型（LLMs）的一个广泛应用案例是目标导向的决策任务（或“代理”任务），在这些任务中，LLM不仅需要为给定提示生成完成，而且需要在多轮交互中做出智能决策以完成任务（例如，与网络交互，使用工具或提供客户支持）。本文提出了一个用于构建LLMs的多轮强化学习算法的框架。",
    "tldr": "本文提出了一个用于构建LLMs的多轮强化学习算法的框架",
    "en_tdlr": "This paper proposes a framework for building multi-turn reinforcement learning algorithms for LLMs."
}