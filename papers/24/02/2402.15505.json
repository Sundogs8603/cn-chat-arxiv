{
    "title": "Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts",
    "abstract": "arXiv:2402.15505v1 Announce Type: cross  Abstract: Steering the behavior of a strong model pre-trained on internet-scale data can be difficult due to the scarcity of competent supervisors. Recent studies reveal that, despite supervisory noises, a strong student model may surpass its weak teacher when fine-tuned on specific objectives. Yet, the effectiveness of such weak-to-strong generalization remains limited, especially in the presence of large capability gaps. In this paper, we propose to address this challenge by harnessing a diverse set of specialized teachers, instead of a single generalist one, that collectively supervises the strong student. Our approach resembles the classical hierarchical mixture of experts, with two components tailored for co-supervision: (i) we progressively alternate student training and teacher assignment, leveraging the growth of the strong student to identify plausible supervisions; (ii) we conservatively enforce teacher-student and local-global consist",
    "link": "https://arxiv.org/abs/2402.15505",
    "context": "Title: Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts\nAbstract: arXiv:2402.15505v1 Announce Type: cross  Abstract: Steering the behavior of a strong model pre-trained on internet-scale data can be difficult due to the scarcity of competent supervisors. Recent studies reveal that, despite supervisory noises, a strong student model may surpass its weak teacher when fine-tuned on specific objectives. Yet, the effectiveness of such weak-to-strong generalization remains limited, especially in the presence of large capability gaps. In this paper, we propose to address this challenge by harnessing a diverse set of specialized teachers, instead of a single generalist one, that collectively supervises the strong student. Our approach resembles the classical hierarchical mixture of experts, with two components tailored for co-supervision: (i) we progressively alternate student training and teacher assignment, leveraging the growth of the strong student to identify plausible supervisions; (ii) we conservatively enforce teacher-student and local-global consist",
    "path": "papers/24/02/2402.15505.json",
    "total_tokens": 906,
    "translated_title": "Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts",
    "translated_abstract": "强有力的模型经过在互联网规模数据上的预训练后，由于缺乏胜任的监督者，在引导其行为时可能会变得困难。最近的研究表明，尽管存在监督噪声，一个强大的学生模型在针对特定目标进行微调后可能会超越其弱教师。然而，这种从弱到强的泛化效果仍然有限，特别是在存在巨大能力差距的情况下。在本文中，我们提出通过利用一组多样化的专家教师，而不是单一的通才教师，共同监督强大的学生模型来应对这一挑战。我们的方法类似于传统的分层专家混合模型，其中包含两个针对协同监督的组件：(i)我们逐步交替进行学生训练和教师分配，利用强大学生模型的增长来识别可能的监督方式；(ii)我们谨慎地强化教师-学生和局部-全局的一致性。",
    "tldr": "提出了一种通过使用分层专家混合模型来改善弱到强泛化的协同监督学习方法，利用一组多样化的专家教师共同监督强大的学生模型。",
    "en_tdlr": "Introduced a co-supervised learning approach that improves weak-to-strong generalization by utilizing a hierarchical mixture of experts, where a diverse set of specialized teachers collectively supervises a strong student model."
}