{
    "title": "Bidirectional Generative Pre-training for Improving Time Series Representation Learning",
    "abstract": "arXiv:2402.09558v1 Announce Type: new  Abstract: Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-s",
    "link": "https://arxiv.org/abs/2402.09558",
    "context": "Title: Bidirectional Generative Pre-training for Improving Time Series Representation Learning\nAbstract: arXiv:2402.09558v1 Announce Type: new  Abstract: Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-s",
    "path": "papers/24/02/2402.09558.json",
    "total_tokens": 865,
    "translated_title": "提高时间序列表示学习的双向生成预训练模型",
    "translated_abstract": "学习时间序列表示以用于判别任务一直是一项长期的挑战。当前的预训练方法要么是单向的下一个标记预测，要么是随机屏蔽标记预测。我们提出了一种新颖的架构，称为双向及时生成预训练Transformer（BiTimelyGPT），它通过交替的Transformer层在时间序列数据上进行了下一个标记和上一个标记的预测。这种预训练任务保留了时间序列的原始分布和数据形状。此外，全秩前向和后向注意力矩阵具有更具表现力的表示能力。 使用生物信号数据，BiTimelyGPT在预测神经功能、疾病诊断和生理病征方面表现出了优越性能。通过可视化注意力热图，我们观察到预训练的BiTimelyGPT能够从时间序列中识别出具有判别性的片段。",
    "tldr": "这项论文提出了一种名为BiTimelyGPT的模型，通过双向的预训练任务在时间序列数据上学习表示，展示了优越的性能，可用于神经功能预测、疾病诊断和生理病征识别。",
    "en_tdlr": "This paper introduces a model called BiTimelyGPT, which learns representations on time series data through a bidirectional pre-training task, demonstrating superior performance for predicting neurological functionality, disease diagnosis, and physiological sign identification."
}