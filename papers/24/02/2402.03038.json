{
    "title": "Automatic Combination of Sample Selection Strategies for Few-Shot Learning",
    "abstract": "In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a str",
    "link": "https://arxiv.org/abs/2402.03038",
    "context": "Title: Automatic Combination of Sample Selection Strategies for Few-Shot Learning\nAbstract: In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a str",
    "path": "papers/24/02/2402.03038.json",
    "total_tokens": 833,
    "translated_title": "自动组合样本选择策略用于少样本学习",
    "translated_abstract": "在少样本学习中，如元学习、少样本微调或上下文学习中，用于训练模型的有限样本数量对整体成功具有显著影响。尽管存在大量的样本选择策略，但它们对少样本学习性能的影响尚不十分明确，因为大部分只被在典型的监督设置中进行了评估。本文通过对8个图像和6个文本数据集上的5种少样本学习方法，彻底研究了20种样本选择策略对性能的影响。此外，我们提出了一种新的自动组合样本选择策略的方法（ACSESS），它充分利用了个体策略的优势和互补信息。实验结果表明，我们的方法始终优于个体选择策略，以及最近提出的上下文学习支持样本选择方法。",
    "tldr": "本文研究了20种样本选择策略对少样本学习性能的影响，并提出了一种自动组合样本选择策略的方法（ACSESS），在多个数据集上证明了其优越性能。",
    "en_tdlr": "This paper investigates the impact of 20 sample selection strategies on the performance of few-shot learning and proposes a method for automatic combination of these strategies (ACSESS), which consistently outperforms individual strategies across multiple datasets."
}