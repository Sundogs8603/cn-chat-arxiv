{
    "title": "Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content",
    "abstract": "arXiv:2402.13926v1 Announce Type: cross  Abstract: The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts. In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks. In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives. The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs. In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations.",
    "link": "https://arxiv.org/abs/2402.13926",
    "context": "Title: Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content\nAbstract: arXiv:2402.13926v1 Announce Type: cross  Abstract: The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts. In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks. In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives. The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs. In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations.",
    "path": "papers/24/02/2402.13926.json",
    "total_tokens": 846,
    "translated_title": "大型语言模型易受诱饵-转换攻击的危害内容生成研究",
    "translated_abstract": "大型语言模型（LLMs）生成欺骗性和有害内容所带来的风险已经引起了相当多的研究，但即使是安全的生成也可能导致问题降级影响。在我们的研究中，我们将焦点转移到即使来自LLMs的安全文本也可以通过诱饵-转换攻击轻松转变为潜在危险内容。在这种攻击中，用户首先用安全问题提示LLMs，然后利用简单的查找和替换后处理技术将输出操纵成有害叙事。这种方法在生成有毒内容方面的惊人有效性突出了在开发可靠的LLMs安全防护栏时面临的重大挑战。特别是，我们强调，专注于逐字的LLMs输出的安全性是不够的，我们还需要考虑后处理转换。",
    "tldr": "大型语言模型可能受到诱饵-转换攻击的威胁，甚至安全生成的文本也能轻易转变为有害内容，强调在LLMs的安全防护中需要考虑后处理转换。",
    "en_tdlr": "Large language models are vulnerable to bait-and-switch attacks, where even safe text can be easily manipulated into harmful content, emphasizing the need to consider post-hoc transformations in safety guardrails for LLMs."
}