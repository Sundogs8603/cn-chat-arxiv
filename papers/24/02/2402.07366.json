{
    "title": "Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing",
    "abstract": "Federated learning (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling. Typically, FL algorithms involve clients training their local models using stochastic gradient descent (SGD), which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions. In this work, we propose a message passing based Bayesian federated learning (BFL) framework to avoid these drawbacks.Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured model compression. Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression. The central server aggregates local posterior distributions to update ",
    "link": "https://arxiv.org/abs/2402.07366",
    "context": "Title: Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing\nAbstract: Federated learning (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling. Typically, FL algorithms involve clients training their local models using stochastic gradient descent (SGD), which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions. In this work, we propose a message passing based Bayesian federated learning (BFL) framework to avoid these drawbacks.Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured model compression. Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression. The central server aggregates local posterior distributions to update ",
    "path": "papers/24/02/2402.07366.json",
    "total_tokens": 966,
    "translated_title": "通过期望最大化和Turbo Deep近似消息传递的贝叶斯联邦学习",
    "translated_abstract": "联邦学习是一种机器学习范 paradigm，在这种范式中，客户端拥有分散的训练数据，而中央服务器则负责聚合和调度。通常情况下，联邦学习算法涉及客户端使用随机梯度下降（SGD）来训练他们的本地模型，但这带来了收敛速度较慢和容易陷入次优解的问题。在这项工作中，我们提出了一种基于消息传递的贝叶斯联邦学习（BFL）框架来避免这些缺点。具体而言，我们将深度神经网络（DNN）的学习和压缩问题建模为稀疏贝叶斯推断问题，其中采用了分组稀疏先验以实现结构化模型压缩。然后，我们提出了一种高效的 BFL 算法，名为 EMTDAMP，其中将期望最大化（EM）和 Turbo Deep 近似消息传递（TDAMP）结合起来实现分布式学习和压缩。中央服务器聚合本地后验分布以实现更新。",
    "tldr": "本文提出了一种基于消息传递的贝叶斯联邦学习（BFL）框架，通过结合期望最大化和Turbo Deep近似消息传递（TDAMP）实现分布式学习和压缩。该框架在处理联邦学习算法的缺点上有着显著的改进。"
}