{
    "title": "ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models",
    "abstract": "Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)",
    "link": "https://arxiv.org/abs/2402.00794",
    "context": "Title: ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models\nAbstract: Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent)",
    "path": "papers/24/02/2402.00794.json",
    "total_tokens": 854,
    "translated_title": "ReAGent: 一个面向生成语言模型的模型无关特征归因方法",
    "translated_abstract": "特征归因方法（FAs），如梯度和注意力机制，被广泛应用于确定所有输入特征对模型预测的重要性。现有自然语言处理领域的研究主要集中在为仅有编码器的语言模型（LMs）开发和测试FAs，用于分类任务。然而，尚不清楚在文本生成上是否可以使用这些FAs来处理仅有解码器的模型，因为模型架构和任务设置之间存在固有差异。此外，先前的研究已经证明，没有一个通用的FA适用于所有模型和任务。这使得针对大型LMs选择FA计算上非常昂贵，因为输入重要性的推导通常需要多个前向和反向传递，包括可能是限制性的梯度计算。为了解决这些问题，我们提出了一种面向生成LMs的模型无关FA，称为递归归因生成器（ReAGent）。",
    "tldr": "本论文介绍了一种面向生成语言模型的模型无关特征归因方法，称为ReAGent。该方法解决了现有方法在文本生成中的适用性问题，并提供了计算上效率更高的选择。",
    "en_tdlr": "This paper presents a model-agnostic feature attribution method for generative language models called ReAGent. It addresses the applicability issue of existing methods in text generation and provides a computationally efficient alternative."
}