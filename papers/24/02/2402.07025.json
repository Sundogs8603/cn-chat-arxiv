{
    "title": "Generalization Error of Graph Neural Networks in the Mean-field Regime",
    "abstract": "This work provides a theoretical framework for assessing the generalization error of graph classification tasks via graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. We explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks. Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks. We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our under",
    "link": "https://arxiv.org/abs/2402.07025",
    "context": "Title: Generalization Error of Graph Neural Networks in the Mean-field Regime\nAbstract: This work provides a theoretical framework for assessing the generalization error of graph classification tasks via graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. We explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks. Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks. We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our under",
    "path": "papers/24/02/2402.07025.json",
    "total_tokens": 864,
    "translated_title": "均场极限下图神经网络的泛化误差",
    "translated_abstract": "该工作提供了一个理论框架，用于评估在过参数化的情况下通过图神经网络进行图分类任务的泛化误差，即参数数量超过数据点数量的情况。我们研究了两种广泛使用的图神经网络类型：图卷积神经网络和消息传递图神经网络。在本研究之前，关于过参数化情况下泛化误差的现有界限缺乏信息，限制了我们对过参数化网络性能的理解。我们的创新方法是在均场极限下推导出上界，以评估这些图神经网络的泛化误差。我们建立了以$O(1/n)$收敛速度的上界，其中$n$是图样本的数量。这些上界为在具有挑战性的过参数化情况下网络在未见数据上的性能提供了理论上的保证，从而对我们的理解做出了贡献。",
    "tldr": "该论文在均场极限下提供了一个理论框架，评估了图神经网络在过参数化情况下的泛化误差，通过推导出收敛速度为$O(1/n)$的上界，为我们对网络在未见数据上的性能提供了理论保证。",
    "en_tdlr": "This paper provides a theoretical framework for evaluating the generalization error of graph neural networks in the over-parameterized regime. By deriving upper bounds with a convergence rate of $O(1/n)$ within the mean-field limit, it offers theoretical assurance of network performance on unseen data."
}