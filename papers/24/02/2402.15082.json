{
    "title": "PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning",
    "abstract": "arXiv:2402.15082v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for adapting pre-trained language models to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or distill shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient fine-tuning framework based on multi-task transfer learning. PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a gated unit, measuring the correlation between the target and each source task using task ",
    "link": "https://arxiv.org/abs/2402.15082",
    "context": "Title: PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning\nAbstract: arXiv:2402.15082v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for adapting pre-trained language models to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or distill shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient fine-tuning framework based on multi-task transfer learning. PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a gated unit, measuring the correlation between the target and each source task using task ",
    "path": "papers/24/02/2402.15082.json",
    "total_tokens": 897,
    "translated_title": "PEMT: 多任务相关性引导的专家混合模型实现了参数高效的迁移学习",
    "translated_abstract": "针对参数高效微调（PEFT）作为将预训练语言模型有效地适应各种任务的方法已经崛起。最近，人们对从一个或多个任务转移知识到下游目标任务以实现性能提升产生了越来越浓厚的兴趣。然而，当前方法通常要么在各个任务上训练适配器，要么从源任务中提取共享知识，未能充分利用任务特定知识和源任务与目标任务之间的相关性。为了克服这些限制，我们提出了PEMT，这是一种基于多任务迁移学习的创新参数高效微调框架。PEMT将专家混合（MoE）框架扩展为源任务上训练的适配器的加权组合以捕获可转移知识。这些权重由一个门控单元确定，利用任务之间的相关性来测量目标任务和每个源任务之间的相关性。",
    "tldr": "PEMT 是一种基于多任务迁移学习的参数高效微调框架，通过扩展专家混合框架，以权重组合捕获源任务上训练的适配器，从而有效利用任务特定知识和源目标任务之间的相关性。",
    "en_tdlr": "PEMT is a parameter-efficient fine-tuning framework based on multi-task transfer learning, extending the mixture-of-experts framework to capture transferable knowledge through a weighted combination of adapters trained on source tasks, effectively utilizing task-specific knowledge and the correlation between source and target tasks."
}