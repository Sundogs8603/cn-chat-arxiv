{
    "title": "Multi Task Inverse Reinforcement Learning for Common Sense Reward",
    "abstract": "arXiv:2402.11367v1 Announce Type: new  Abstract: One of the challenges in applying reinforcement learning in a complex real-world environment lies in providing the agent with a sufficiently detailed reward function. Any misalignment between the reward and the desired behavior can result in unwanted outcomes. This may lead to issues like \"reward hacking\" where the agent maximizes rewards by unintended behavior. In this work, we propose to disentangle the reward into two distinct parts. A simple task-specific reward, outlining the particulars of the task at hand, and an unknown common-sense reward, indicating the expected behavior of the agent within the environment. We then explore how this common-sense reward can be learned from expert demonstrations. We first show that inverse reinforcement learning, even when it succeeds in training an agent, does not learn a useful reward function. That is, training a new agent with the learned reward does not impair the desired behaviors. We then d",
    "link": "https://arxiv.org/abs/2402.11367",
    "context": "Title: Multi Task Inverse Reinforcement Learning for Common Sense Reward\nAbstract: arXiv:2402.11367v1 Announce Type: new  Abstract: One of the challenges in applying reinforcement learning in a complex real-world environment lies in providing the agent with a sufficiently detailed reward function. Any misalignment between the reward and the desired behavior can result in unwanted outcomes. This may lead to issues like \"reward hacking\" where the agent maximizes rewards by unintended behavior. In this work, we propose to disentangle the reward into two distinct parts. A simple task-specific reward, outlining the particulars of the task at hand, and an unknown common-sense reward, indicating the expected behavior of the agent within the environment. We then explore how this common-sense reward can be learned from expert demonstrations. We first show that inverse reinforcement learning, even when it succeeds in training an agent, does not learn a useful reward function. That is, training a new agent with the learned reward does not impair the desired behaviors. We then d",
    "path": "papers/24/02/2402.11367.json",
    "total_tokens": 854,
    "translated_title": "多任务逆强化学习用于常识奖励",
    "translated_abstract": "在将强化学习应用于复杂的现实环境中，一个挑战在于为agent提供足够详细的奖励函数。奖励与期望行为之间的不一致可能导致意外结果，如“奖励篡改”，agent通过意外行为最大化奖励。本文提出将奖励分解为两个明确部分：一个简单的任务特定奖励，概述了当前任务的细节；以及一个未知的常识奖励，指示agent在环境中的预期行为。我们探讨了如何从专家演示中学习这种常识奖励。我们首先展示，即使逆强化学习成功训练了一个代理，也不会学到一个有用的奖励函数。也就是说，使用学到的奖励训练新代理不会影响期望行为。",
    "tldr": "将奖励分解为任务特定奖励和常识奖励，探索如何从专家演示中学习常识奖励；研究发现，逆强化学习成功训练代理后，并不会学到有用的奖励函数。"
}