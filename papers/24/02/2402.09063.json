{
    "title": "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space",
    "abstract": "arXiv:2402.09063v1 Announce Type: new Abstract: Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the embedding space attack, which directly attacks the continuous embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across m",
    "link": "https://arxiv.org/abs/2402.09063",
    "context": "Title: Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space\nAbstract: arXiv:2402.09063v1 Announce Type: new Abstract: Current research in adversarial robustness of LLMs focuses on discrete input manipulations in the natural language space, which can be directly transferred to closed-source models. However, this approach neglects the steady progression of open-source models. As open-source models advance in capability, ensuring their safety also becomes increasingly imperative. Yet, attacks tailored to open-source LLMs that exploit full model access remain largely unexplored. We address this research gap and propose the embedding space attack, which directly attacks the continuous embedding representation of input tokens. We find that embedding space attacks circumvent model alignments and trigger harmful behaviors more efficiently than discrete attacks or model fine-tuning. Furthermore, we present a novel threat model in the context of unlearning and show that embedding space attacks can extract supposedly deleted information from unlearned LLMs across m",
    "path": "papers/24/02/2402.09063.json",
    "total_tokens": 902,
    "translated_title": "软提示威胁：通过嵌入空间对开源LLMs进行安全对齐攻击和遗忘",
    "translated_abstract": "当前对LLMs的敌对鲁棒性研究专注于自然语言空间中的离散输入操纵，这些操纵可以直接转移到闭源模型中。然而，这种方法忽视了开源模型的持续进展。随着开源模型能力的提升，确保其安全性也变得越来越重要。然而，针对开源LLMs的攻击，利用完全模型访问权限的方式仍然很少被探索。我们填补了这一研究空白，并提出了嵌入空间攻击，直接攻击输入令牌的连续嵌入表示。我们发现，嵌入空间攻击比离散攻击或模型微调更有效地绕过模型对齐并触发有害行为。此外，我们在遗忘的背景下提出了一种新的威胁模型，并展示了嵌入空间攻击在从未经学习的LLMs中提取应该删除的信息方面的能力。",
    "tldr": "该论文提出了一种新的嵌入空间攻击方法，针对开源LLMs进行攻击，绕过模型对齐并在遗忘的情况下提取信息，比传统的离散攻击更高效。"
}