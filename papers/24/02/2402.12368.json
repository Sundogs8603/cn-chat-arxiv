{
    "title": "A synthetic data approach for domain generalization of NLI models",
    "abstract": "arXiv:2402.12368v1 Announce Type: new  Abstract: Natural Language Inference (NLI) remains an important benchmark task for LLMs. NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text. There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We present an in-depth exploration of the problem of domain generalization of NLI models. We demonstrate a new approach for generating synthetic NLI data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data ($685$K synthetic examples) have the b",
    "link": "https://arxiv.org/abs/2402.12368",
    "context": "Title: A synthetic data approach for domain generalization of NLI models\nAbstract: arXiv:2402.12368v1 Announce Type: new  Abstract: Natural Language Inference (NLI) remains an important benchmark task for LLMs. NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text. There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We present an in-depth exploration of the problem of domain generalization of NLI models. We demonstrate a new approach for generating synthetic NLI data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data ($685$K synthetic examples) have the b",
    "path": "papers/24/02/2402.12368.json",
    "total_tokens": 862,
    "translated_title": "一种用于NLI模型领域泛化的合成数据方法",
    "translated_abstract": "自然语言推理（NLI）仍然是LLMs的一个重要基准任务。 NLI数据集是迁移学习到其他语义任务的跳板，而NLI模型是识别模型生成文本忠实性的标准工具。 今天有几个大规模的NLI数据集，通过在这些集合上进行爬坡，模型已经取得了很大的改进。 然而，它们在分布/领域数据上的实际性能尚不很清楚。 我们对NLI模型领域泛化问题进行了深入探讨。 我们展示了一种在多个领域和长度生成合成NLI数据的新方法，这些数据迄今为止尚未被现有训练集覆盖。 生成的示例具有有意义的前提，假设以创造性的方式形成，而不是简单地对几个前提标记进行编辑，标签的准确率很高。 我们展示了在这些数据上训练的模型（685K个合成示例）具有",
    "tldr": "本研究提出了一种新方法，通过生成多样领域和长度的合成NLI数据，解决了NLI模型在领域泛化方面的问题。",
    "en_tdlr": "This study presents a new approach to address the issue of domain generalization of NLI models by generating synthetic NLI data in diverse domains and lengths."
}