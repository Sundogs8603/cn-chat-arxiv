{
    "title": "Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition",
    "abstract": "arXiv:2402.16247v1 Announce Type: cross  Abstract: In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community. This observation led to research into Zero-Shot Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training. However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the zero-shot setting. In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions. We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a 'joiner' agent to learn from a dataset of interactions between agents in a target community. We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learni",
    "link": "https://arxiv.org/abs/2402.16247",
    "context": "Title: Learning Translations: Emergent Communication Pretraining for Cooperative Language Acquisition\nAbstract: arXiv:2402.16247v1 Announce Type: cross  Abstract: In Emergent Communication (EC) agents learn to communicate with one another, but the protocols that they develop are specialised to their training community. This observation led to research into Zero-Shot Coordination (ZSC) for learning communication strategies that are robust to agents not encountered during training. However, ZSC typically assumes that no prior data is available about the agents that will be encountered in the zero-shot setting. In many cases, this presents an unnecessarily hard problem and rules out communication via preestablished conventions. We propose a novel AI challenge called a Cooperative Language Acquisition Problem (CLAP) in which the ZSC assumptions are relaxed by allowing a 'joiner' agent to learn from a dataset of interactions between agents in a target community. We propose and compare two methods for solving CLAPs: Imitation Learning (IL), and Emergent Communication pretraining and Translation Learni",
    "path": "papers/24/02/2402.16247.json",
    "total_tokens": 887,
    "translated_title": "学习翻译：应对合作语言习得的新兴沟通预训练",
    "translated_abstract": "在新兴沟通中，代理学习彼此进行沟通，但他们制定的协议是针对他们的训练群体的。这一观察结果导致了对于学习对未在训练中遇到的代理稳健的沟通策略的Zero-Shot Coordination（ZSC）的研究。但是，ZSC通常假设关于在零-shot设置中会遇到的代理的先前数据是无法获得的。在许多情况下，这提出了一个不必要的棘手问题，并排除了通过预先建立的约定进行沟通。我们提出了一个名为合作语言习得问题（CLAP）的新颖人工智能挑战，在其中通过允许“加入者”代理从目标社区内代理之间的互动数据集中学习来放宽了ZSC假设。我们提出并比较了解决CLAPs的两种方法：模仿学习（IL）和新兴沟通的预训练和翻译学习。",
    "tldr": "提出了一个名为合作语言习得问题（CLAP）的新颖人工智能挑战，通过允许代理在目标社区中从互动数据集中学习，放宽了Zero-Shot Coordination假设。",
    "en_tdlr": "Proposed a novel AI challenge called Cooperative Language Acquisition Problem (CLAP), relaxing the Zero-Shot Coordination assumptions by allowing agents to learn from interactions in a target community dataset."
}