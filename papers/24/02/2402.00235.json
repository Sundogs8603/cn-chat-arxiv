{
    "title": "Exploring the limits of decoder-only models trained on public speech recognition corpora",
    "abstract": "The emergence of industrial-scale speech recognition (ASR) models such as Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio only proprietary data respectively, has led to a stronger need for large scale public ASR corpora and competitive open source pipelines. Unlike the said models, large language models are typically based on Transformer decoders, and it remains unclear if decoder-only models trained on public data alone can deliver competitive performance. In this work, we investigate factors such as choice of training datasets and modeling components necessary for obtaining the best performance using public English ASR corpora alone. Our Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper (OWSM) on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets. We release our codebase and model checkpoints under permissive license.",
    "link": "https://arxiv.org/abs/2402.00235",
    "context": "Title: Exploring the limits of decoder-only models trained on public speech recognition corpora\nAbstract: The emergence of industrial-scale speech recognition (ASR) models such as Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio only proprietary data respectively, has led to a stronger need for large scale public ASR corpora and competitive open source pipelines. Unlike the said models, large language models are typically based on Transformer decoders, and it remains unclear if decoder-only models trained on public data alone can deliver competitive performance. In this work, we investigate factors such as choice of training datasets and modeling components necessary for obtaining the best performance using public English ASR corpora alone. Our Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper (OWSM) on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets. We release our codebase and model checkpoints under permissive license.",
    "path": "papers/24/02/2402.00235.json",
    "total_tokens": 935,
    "translated_title": "探索仅基于解码器训练的模型在公共语音识别语料库上的极限",
    "translated_abstract": "工业规模的语音识别模型（如Whisper和USM）的出现，这些模型分别训练于100万小时的弱标注数据和1200万小时的纯音频专有数据，导致了对大规模公共语音识别语料库和竞争性开源流程的更强需求。与上述模型不同，大型语言模型通常基于Transformer解码器，目前尚不清楚仅使用公共数据训练的解码器模型是否能够提供具有竞争力的性能。在这项工作中，我们研究了诸如训练数据集的选择和建模组件等因素，以获得仅使用公共英语语音识别语料库的最佳性能。我们的Decoder-Only Transformer for ASR (DOTA)模型在几乎所有英语语音识别基准测试中全面优于Whisper的编码器-解码器开源复制(OWSM)，并且在15个测试集中的7个中胜过Whisper large-v3。我们以宽松的许可证发布我们的代码库和模型检查点。",
    "tldr": "本研究探索了仅基于解码器训练的模型在公共语音识别语料库上的极限，开发了名为DOTA的模型，在大多数英语语音识别基准测试中优于其他开源模型，并公开了代码库和模型检查点。",
    "en_tdlr": "This study explores the limits of decoder-only models trained on public speech recognition corpora and develops a model called DOTA, which outperforms other open source models in most English ASR benchmarks. The codebase and model checkpoints are released under a permissive license."
}