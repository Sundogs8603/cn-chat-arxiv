{
    "title": "Improved Quantization Strategies for Managing Heavy-tailed Gradients in Distributed Learning",
    "abstract": "Gradient compression has surfaced as a key technique to address the challenge of communication efficiency in distributed learning. In distributed deep learning, however, it is observed that gradient distributions are heavy-tailed, with outliers significantly influencing the design of compression strategies. Existing parameter quantization methods experience performance degradation when this heavy-tailed feature is ignored. In this paper, we introduce a novel compression scheme specifically engineered for heavy-tailed gradients, which effectively combines gradient truncation with quantization. This scheme is adeptly implemented within a communication-limited distributed Stochastic Gradient Descent (SGD) framework. We consider a general family of heavy-tail gradients that follow a power-law distribution, we aim to minimize the error resulting from quantization, thereby determining optimal values for two critical parameters: the truncation threshold and the quantization density. We provid",
    "link": "https://arxiv.org/abs/2402.01798",
    "context": "Title: Improved Quantization Strategies for Managing Heavy-tailed Gradients in Distributed Learning\nAbstract: Gradient compression has surfaced as a key technique to address the challenge of communication efficiency in distributed learning. In distributed deep learning, however, it is observed that gradient distributions are heavy-tailed, with outliers significantly influencing the design of compression strategies. Existing parameter quantization methods experience performance degradation when this heavy-tailed feature is ignored. In this paper, we introduce a novel compression scheme specifically engineered for heavy-tailed gradients, which effectively combines gradient truncation with quantization. This scheme is adeptly implemented within a communication-limited distributed Stochastic Gradient Descent (SGD) framework. We consider a general family of heavy-tail gradients that follow a power-law distribution, we aim to minimize the error resulting from quantization, thereby determining optimal values for two critical parameters: the truncation threshold and the quantization density. We provid",
    "path": "papers/24/02/2402.01798.json",
    "total_tokens": 865,
    "translated_title": "改进的量化策略用于管理分布式学习中的重尾梯度",
    "translated_abstract": "梯度压缩已经成为分布式学习中解决通信效率挑战的关键技术。然而，在分布式深度学习中，观察到梯度分布呈重尾分布，离群值显著影响压缩策略的设计。现有的参数量化方法在忽略重尾特征时会出现性能下降。在本文中，我们引入了一种新的针对重尾梯度的压缩方案，将梯度截断与量化巧妙结合。该方案巧妙地实现在一个通信受限的分布式随机梯度下降（SGD）框架中。我们考虑了一类遵循幂律分布的重尾梯度，旨在最小化由量化引起的误差，从而确定两个关键参数的最优值：截断阈值和量化密度。",
    "tldr": "本论文介绍了一种针对重尾梯度的新型压缩方案，在分布式学习中解决了梯度分布不均匀的挑战，并且将梯度截断和量化相结合，通过优化关键参数的值来减小量化引起的误差。"
}