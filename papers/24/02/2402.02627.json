{
    "title": "Stability Analysis of Various Symbolic Rule Extraction Methods from Recurrent Neural Network",
    "abstract": "This paper analyzes two competing rule extraction methodologies: quantization and equivalence query. We trained $3600$ RNN models, extracting $18000$ DFA with a quantization approach (k-means and SOM) and $3600$ DFA by equivalence query($L^{*}$) methods across $10$ initialization seeds. We sampled the datasets from $7$ Tomita and $4$ Dyck grammars and trained them on $4$ RNN cells: LSTM, GRU, O2RNN, and MIRNN. The observations from our experiments establish the superior performance of O2RNN and quantization-based rule extraction over others. $L^{*}$, primarily proposed for regular grammars, performs similarly to quantization methods for Tomita languages when neural networks are perfectly trained. However, for partially trained RNNs, $L^{*}$ shows instability in the number of states in DFA, e.g., for Tomita 5 and Tomita 6 languages, $L^{*}$ produced more than $100$ states. In contrast, quantization methods result in rules with number of states very close to ground truth DFA. Among RNN c",
    "link": "https://arxiv.org/abs/2402.02627",
    "context": "Title: Stability Analysis of Various Symbolic Rule Extraction Methods from Recurrent Neural Network\nAbstract: This paper analyzes two competing rule extraction methodologies: quantization and equivalence query. We trained $3600$ RNN models, extracting $18000$ DFA with a quantization approach (k-means and SOM) and $3600$ DFA by equivalence query($L^{*}$) methods across $10$ initialization seeds. We sampled the datasets from $7$ Tomita and $4$ Dyck grammars and trained them on $4$ RNN cells: LSTM, GRU, O2RNN, and MIRNN. The observations from our experiments establish the superior performance of O2RNN and quantization-based rule extraction over others. $L^{*}$, primarily proposed for regular grammars, performs similarly to quantization methods for Tomita languages when neural networks are perfectly trained. However, for partially trained RNNs, $L^{*}$ shows instability in the number of states in DFA, e.g., for Tomita 5 and Tomita 6 languages, $L^{*}$ produced more than $100$ states. In contrast, quantization methods result in rules with number of states very close to ground truth DFA. Among RNN c",
    "path": "papers/24/02/2402.02627.json",
    "total_tokens": 943,
    "translated_title": "对多种基于循环神经网络的符号规则提取方法的稳定性分析",
    "translated_abstract": "本文分析了两种竞争的规则提取方法：量化和等价查询。我们使用量化方法（k-means和SOM）从3600个RNN模型中提取了18000个DFA，使用等价查询（$L^{*}$）方法从3600个DFA模型中提取了10个初始化种子。我们从7个Tomita语法和4个Dyck语法中抽样得到了数据集，并使用4个RNN单元（LSTM、GRU、O2RNN和MIRNN）进行训练。我们的实验观察结果表明，O2RNN和基于量化的规则提取方法表现优于其他方法。当神经网络完全训练时，$L^{*}$在Tomita语言上的表现与量化方法类似。然而，对于部分训练的RNN，$L^{*}$在DFA状态数量上表现不稳定，例如对于Tomita 5和Tomita 6语言，$L^{*}$产生了超过100个状态的结果。相反，量化方法的规则状态数量非常接近真实的DFA。在RNN中，O2RNN和quantization-based规则提取方法表现出色。",
    "tldr": "本文分析了基于循环神经网络的两种规则提取方法：量化和等价查询。实验结果表明，O2RNN和基于量化的规则提取方法在稳定性和性能方面优于其他方法。",
    "en_tdlr": "This paper analyzes two competing rule extraction methodologies from recurrent neural network: quantization and equivalence query. The experiments demonstrate that O2RNN and quantization-based rule extraction outperform other methods in terms of stability and performance."
}