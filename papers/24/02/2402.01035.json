{
    "title": "Getting the most out of your tokenizer for pre-training and domain adaptation",
    "abstract": "Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to ",
    "link": "https://rss.arxiv.org/abs/2402.01035",
    "context": "Title: Getting the most out of your tokenizer for pre-training and domain adaptation\nAbstract: Tokenization is an understudied and often neglected component of modern LLMs. Most published works use a single tokenizer for all experiments, often borrowed from another model, without performing ablations or analysis to optimize tokenization. Moreover, the tokenizer is generally kept unchanged when fine-tuning a base model. In this paper, we show that the size, pre-tokenization regular expression, and training data of a tokenizer can significantly impact the model's generation speed, effective context size, memory usage, and downstream performance. We train specialized Byte-Pair Encoding code tokenizers, and conduct extensive ablations on the impact of tokenizer design on the performance of LLMs for code generation tasks such as HumanEval and MBPP, and provide recommendations for tokenizer hyper-parameters selection and switching the tokenizer in a pre-trained LLM. We perform our experiments on models trained from scratch and from pre-trained models, verifying their applicability to ",
    "path": "papers/24/02/2402.01035.json",
    "total_tokens": 907,
    "translated_title": "充分利用分词器进行预训练和领域适应",
    "translated_abstract": "分词是现代LLM中鲜为人知且常被忽视的组成部分。大多数已发表的作品在所有实验中都使用同一个分词器，通常是从另一个模型借用而来的，并没有进行消融或分析来优化分词。此外，在微调基础模型时，分词器通常保持不变。在本文中，我们展示了分词器的大小、预标记正则表达式和训练数据对模型的生成速度、有效上下文大小、内存使用和下游性能均有重要影响。我们训练了专用的字节对编码分词器，并对分词器设计对代码生成任务（如HumanEval和MBPP）中LLM性能影响进行了广泛的消融，提供了分词器超参数选择和在预训练LLM中切换分词器的建议。我们在从头开始训练和从预训练模型中进行了实验，验证了它们对各种任务和模型的适用性。",
    "tldr": "本文通过训练专用分词器，对分词器设计进行了消毒和分析，发现分词器的大小、正则表达式和训练数据对模型性能有重要影响，并提供了相应的超参数选择建议和切换分词器的方法。",
    "en_tdlr": "This paper investigates the impact of tokenizer design on the performance of language models, specifically focusing on code generation tasks. It demonstrates that the size, regular expression, and training data of a tokenizer significantly affect model generation speed, effective context size, memory usage, and downstream performance. The paper provides recommendations for tokenizer hyper-parameter selection and switching in pre-trained language models."
}