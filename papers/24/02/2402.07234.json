{
    "title": "CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain",
    "abstract": "Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream LLMs in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain--CPSDbench. CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of LLMs across four key dimensions: text classification, information extraction, question answering, and text generation. Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security. Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the fu",
    "link": "https://arxiv.org/abs/2402.07234",
    "context": "Title: CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain\nAbstract: Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream LLMs in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain--CPSDbench. CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of LLMs across four key dimensions: text classification, information extraction, question answering, and text generation. Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security. Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the fu",
    "path": "papers/24/02/2402.07234.json",
    "total_tokens": 930,
    "translated_title": "CPSDBench：一个针对中国公共安全领域的大型语言模型评估基准和基线",
    "translated_abstract": "大型语言模型（LLMs）在多个应用领域展示了显著的潜力和效果。为了评估主流LLMs在公共安全任务中的性能，本研究旨在构建一个专门针对中国公共安全领域的评估基准——CPSDBench。CPSDBench整合了从现实场景中收集到的与公共安全相关的数据集，支持对LLMs在文本分类、信息提取、问题回答和文本生成四个关键维度上进行全面评估。此外，本研究还引入了一套创新的评估指标，旨在更精确地量化LLMs在执行与公共安全相关任务上的效力。通过本研究中的深入分析和评估，我们不仅增强了对现有模型在解决公共安全问题方面性能优势和局限性的理解，还为未来的研究提供了参考。",
    "tldr": "CPSDBench是一个专门为中国公共安全领域量身定制的大型语言模型评估基准，通过整合实际场景中收集的公共安全相关数据集，针对文本分类、信息提取、问题回答和文本生成四个关键维度全面评估LLMs的性能，并引入创新的评估指标，提高了对现有模型在解决公共安全问题方面性能的理解。"
}