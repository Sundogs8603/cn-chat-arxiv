{
    "title": "Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills",
    "abstract": "Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-",
    "link": "https://arxiv.org/abs/2402.03244",
    "context": "Title: Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills\nAbstract: Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-",
    "path": "papers/24/02/2402.03244.json",
    "total_tokens": 967,
    "translated_title": "技能集优化：通过可转移技能增强语言模型行为",
    "translated_abstract": "近期，大型语言模型（LLMs）已被用于交互环境中的顺序决策。然而，利用环境奖励信号来不断改进LLM演员的表现并不简单。我们提出了技能集优化（SSO）来通过构建和完善可转移技能集来提高LLM演员的性能。SSO通过提取具有高奖励的共同子轨迹并生成子目标和说明来构建技能。这些技能在上下文中提供给LLM演员，以强化具有高奖励的行为。然后，SSO通过修剪不再产生高奖励的技能来进一步完善技能集。我们在经典视频游戏NetHack和文本环境ScienceWorld中评估了我们的方法，以展示SSO优化技能集并进行上下文策略改进的能力。在我们的自定义NetHack任务中，SSO的性能超过基准方法40%，并超过了先前的最新状态。",
    "tldr": "本论文提出了一种技能集优化（SSO）方法，通过构建和完善可转移的技能集来提高大型语言模型（LLM）的性能。该方法通过提取高奖励的共同子轨迹，生成子目标和说明，并在上下文中提供给LLM演员，以强化行为。实验结果显示，SSO在不同环境中能够优化技能集，并实现上下文策略改进。",
    "en_tdlr": "This paper proposes a Skill Set Optimization (SSO) method to improve the performance of Large Language Models (LLMs) by constructing and refining a set of transferable skills. The method extracts common subtrajectories with high rewards, generates subgoals and instructions for each skill, and reinforces behaviors with high rewards in the LLM actor. Experimental results show that SSO can optimize the skill set and achieve contextual policy improvement in different environments."
}