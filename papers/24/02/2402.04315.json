{
    "title": "Training Language Models to Generate Text with Citations via Fine-grained Rewards",
    "abstract": "While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly prompted LLMs to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller LLMs. In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common LLM training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on Question Answering (QA) datasets taken from the ALCE benchmark and validate the model's gene",
    "link": "https://arxiv.org/abs/2402.04315",
    "context": "Title: Training Language Models to Generate Text with Citations via Fine-grained Rewards\nAbstract: While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly prompted LLMs to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller LLMs. In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common LLM training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on Question Answering (QA) datasets taken from the ALCE benchmark and validate the model's gene",
    "path": "papers/24/02/2402.04315.json",
    "total_tokens": 845,
    "translated_title": "使用细粒度奖励训练语言模型生成带引用文本",
    "translated_abstract": "最近的大型语言模型（LLMs）在回答用户查询方面非常有用，但容易产生幻觉，并且它们的回答常常缺乏可靠来源的引用。解决这些问题的直观方法是将外部文档的引用作为证据包含在文本中。虽然以前的研究直接促使LLMs生成引用文本，但它们的性能远非令人满意，尤其是对于较小的LLMs。在这项工作中，我们提出了一种有效的训练框架，使用细粒度奖励教授LLMs生成高度支持和相关的引用，同时确保其响应的正确性。我们还对将这些细粒度奖励应用于常见的LLMs训练策略进行了系统分析，证明其相对于传统做法的优势。我们在从ALCE基准测试中获取的问答（QA）数据集上进行了大量实验，并验证了模型的生成能力。",
    "tldr": "本文提出了一种使用细粒度奖励训练语言模型生成高质量引用的有效框架，并在常见的大型语言模型训练策略上进行了实证分析。"
}