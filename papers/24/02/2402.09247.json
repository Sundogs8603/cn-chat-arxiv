{
    "title": "Momentum Approximation in Asynchronous Private Federated Learning",
    "abstract": "arXiv:2402.09247v1 Announce Type: new Abstract: Asynchronous protocols have been shown to improve the scalability of federated learning (FL) with a massive number of clients. Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL. However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance. It is still unclear how to effective combinie these two techniques together to achieve a win-win. In this paper, we find that asynchrony introduces implicit bias to momentum updates. In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates. Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost. We empirically demonstrate that on benchmark FL datasets, momentum appro",
    "link": "https://arxiv.org/abs/2402.09247",
    "context": "Title: Momentum Approximation in Asynchronous Private Federated Learning\nAbstract: arXiv:2402.09247v1 Announce Type: new Abstract: Asynchronous protocols have been shown to improve the scalability of federated learning (FL) with a massive number of clients. Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL. However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance. It is still unclear how to effective combinie these two techniques together to achieve a win-win. In this paper, we find that asynchrony introduces implicit bias to momentum updates. In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates. Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost. We empirically demonstrate that on benchmark FL datasets, momentum appro",
    "path": "papers/24/02/2402.09247.json",
    "total_tokens": 882,
    "translated_title": "异步私有联邦学习中的动量近似",
    "translated_abstract": "异步协议已被证明能够提高大规模客户端联邦学习（FL）的可扩展性。同时，基于动量的方法可以在同步FL中实现最佳模型质量。然而，在异步FL算法中简单地应用动量会导致收敛速度变慢和模型性能下降。如何有效地结合这两种技术以实现双赢目前尚不清楚。在本文中，我们发现异步性引入了对动量更新的隐含偏差。为了解决这个问题，我们提出了动量近似，通过找到所有历史模型更新的最佳加权平均值来最小化偏差。动量近似与安全聚合和差分隐私是兼容的，并且可以在生产的FL系统中很容易地集成，只需较小的通信和存储成本。我们在基准FL数据集上进行了实证研究，证明了动量近似在性能上的改进效果。",
    "tldr": "本文提出了动量近似方法，在异步私有联邦学习（FL）中有效结合了动量和异步协议的技术，通过最小化动量更新的偏差来改进模型性能。实证研究证明了动量近似在基准FL数据集上的有效性。",
    "en_tdlr": "This paper proposes a momentum approximation method that effectively combines momentum and asynchronous protocols in asynchronous private federated learning (FL), improving model performance by minimizing the bias introduced by momentum updates. Empirical studies demonstrate the effectiveness of the momentum approximation on benchmark FL datasets."
}