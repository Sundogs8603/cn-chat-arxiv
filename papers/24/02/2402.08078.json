{
    "title": "Large Language Models as Agents in Two-Player Games",
    "abstract": "By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies. This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems. We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.",
    "link": "https://arxiv.org/abs/2402.08078",
    "context": "Title: Large Language Models as Agents in Two-Player Games\nAbstract: By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies. This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems. We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.",
    "path": "papers/24/02/2402.08078.json",
    "total_tokens": 843,
    "translated_title": "大型语言模型作为两人游戏中的代理",
    "translated_abstract": "通过在一个统一的机器学习范式中正式定义大型语言模型（LLMs）的训练过程，该过程通常包括预训练、有监督微调和强化学习与人类反馈，在推进LLM技术方面可以获得关键性的见解。本文描述了LLM的训练方法与在博弈论、强化学习和多智能体系统中研究的两人游戏代理开发所采用的策略之间的相似之处。我们提出了一种将LLM学习过程重新概念化为基于语言的游戏中的代理学习的框架。这一框架揭示了LLM开发中的成功与挑战的创新视角，提供了对解决对齐问题和其他战略考虑的新理解。此外，我们的两人游戏方法为训练LLMs提供了新的数据准备和机器学习技术的启示。",
    "tldr": "通过将大型语言模型的训练过程重新概念化为基于语言的两人游戏中的代理学习，我们能够得到关键的见解，并提供了新的方法和技术来推进大型语言模型的发展。",
    "en_tdlr": "By reconceptualizing the training process of large language models as agent learning in language-based two-player games, this paper provides key insights and new methods and techniques to advance the development of large language models."
}