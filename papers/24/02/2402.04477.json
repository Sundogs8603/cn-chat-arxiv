{
    "title": "Detecting Mode Collapse in Language Models via Narration",
    "abstract": "No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively consistent personae on language models via instruction tuning and reinforcement learning from human feedback (RLHF), but whether aligned models retain the ability to model an arbitrary virtual author has received little scrutiny. By studying 4,374 stories sampled from three OpenAI language models, we show successive versions of GPT-3 suffer from increasing degrees of \"mode collapse\" whereby overf",
    "link": "https://arxiv.org/abs/2402.04477",
    "context": "Title: Detecting Mode Collapse in Language Models via Narration\nAbstract: No two authors write alike. Personal flourishes invoked in written narratives, from lexicon to rhetorical devices, imply a particular author--what literary theorists label the implied or virtual author; distinct from the real author or narrator of a text. Early large language models trained on unfiltered training sets drawn from a variety of discordant sources yielded incoherent personalities, problematic for conversational tasks but proving useful for sampling literature from multiple perspectives. Successes in alignment research in recent years have allowed researchers to impose subjectively consistent personae on language models via instruction tuning and reinforcement learning from human feedback (RLHF), but whether aligned models retain the ability to model an arbitrary virtual author has received little scrutiny. By studying 4,374 stories sampled from three OpenAI language models, we show successive versions of GPT-3 suffer from increasing degrees of \"mode collapse\" whereby overf",
    "path": "papers/24/02/2402.04477.json",
    "total_tokens": 863,
    "translated_title": "通过叙述检测语言模型中的模式崩溃",
    "translated_abstract": "没有两个作者写作方式相同。从词汇到修辞手法，在书面叙述中呈现出的个人特色，暗示了一位特定的作者，文学理论家将其称为隐含或虚拟作者，与文本的实际作者或叙述者不同。早期使用来自各种不协调来源的未经过滤的训练集进行训练的大型语言模型产生了不连贯的个性，这对于对话任务来说是有问题的，但对于从多个观点采样文学却是有用的。近年来，在对齐研究方面取得的成功使研究人员能够通过指导调整和从人类反馈中进行强化学习（RLHF）来对语言模型施加主观一致的人物形象，但对齐模型是否保留了对模拟任意虚拟作者的能力几乎没有受到审查。通过研究来自三个OpenAI语言模型的4,374个故事，我们展示了随着GPT-3的更新版本，越来越多的“模式崩溃”现象的发生。",
    "tldr": "通过研究来自三个OpenAI语言模型的故事，我们发现GPT-3的更新版本逐渐出现了“模式崩溃”的现象。",
    "en_tdlr": "By studying stories from three OpenAI language models, we discovered that successive versions of GPT-3 suffer from increasing degrees of \"mode collapse.\""
}