{
    "title": "Low-Rank Learning by Design: the Role of Network Architecture and Activation Linearity in Gradient Rank Collapse",
    "abstract": "Our understanding of learning dynamics of deep neural networks (DNNs) remains incomplete. Recent research has begun to uncover the mathematical principles underlying these networks, including the phenomenon of \"Neural Collapse\", where linear classifiers within DNNs converge to specific geometrical structures during late-stage training. However, the role of geometric constraints in learning extends beyond this terminal phase. For instance, gradients in fully-connected layers naturally develop a low-rank structure due to the accumulation of rank-one outer products over a training batch. Despite the attention given to methods that exploit this structure for memory saving or regularization, the emergence of low-rank learning as an inherent aspect of certain DNN architectures has been under-explored. In this paper, we conduct a comprehensive study of gradient rank in DNNs, examining how architectural choices and structure of the data effect gradient rank bounds. Our theoretical analysis pro",
    "link": "https://arxiv.org/abs/2402.06751",
    "context": "Title: Low-Rank Learning by Design: the Role of Network Architecture and Activation Linearity in Gradient Rank Collapse\nAbstract: Our understanding of learning dynamics of deep neural networks (DNNs) remains incomplete. Recent research has begun to uncover the mathematical principles underlying these networks, including the phenomenon of \"Neural Collapse\", where linear classifiers within DNNs converge to specific geometrical structures during late-stage training. However, the role of geometric constraints in learning extends beyond this terminal phase. For instance, gradients in fully-connected layers naturally develop a low-rank structure due to the accumulation of rank-one outer products over a training batch. Despite the attention given to methods that exploit this structure for memory saving or regularization, the emergence of low-rank learning as an inherent aspect of certain DNN architectures has been under-explored. In this paper, we conduct a comprehensive study of gradient rank in DNNs, examining how architectural choices and structure of the data effect gradient rank bounds. Our theoretical analysis pro",
    "path": "papers/24/02/2402.06751.json",
    "total_tokens": 915,
    "translated_title": "通过设计实现低秩学习：网络架构和激活线性在梯度秩塌陷中的作用",
    "translated_abstract": "我们对深度神经网络（DNNs）的学习动态的理解仍然不完整。最近的研究已经开始揭示了支持这些网络的数学原理，包括“神经坍塌”现象，即在训练的最后阶段，DNN内的线性分类器会收敛到特定的几何结构。然而，几何约束在学习中的作用不仅限于这个终止阶段。例如，全连接层中的梯度自然会由于训练批次上的秩一外积的累积而形成一个低秩结构。尽管已经注意到利用这种结构节省内存或进行正则化的方法，但低秩学习作为某些DNN架构固有的一个方面的出现尚未被充分探讨。在本文中，我们对DNNs中的梯度秩进行了全面研究，考察了架构选择和数据结构对梯度秩界的影响。我们的理论分析证明了低秩学习是某些DNN架构的固有特征。",
    "tldr": "本文对DNNs中的梯度秩进行了全面研究，发现低秩学习是某些DNN架构固有的特征，而不仅仅是训练的最后阶段的现象。",
    "en_tdlr": "This paper presents a comprehensive study of gradient rank in DNNs and discovers that low-rank learning is an inherent aspect of certain DNN architectures, extending beyond the terminal phase of training."
}