{
    "title": "Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL",
    "abstract": "Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an important problem in RL that focuses on acquiring diverse goal-oriented skills solely from pre-collected behavior datasets. In this setting, the reward feedback is typically absent except when the goal is achieved, which makes it difficult to learn policies especially from a finite dataset of suboptimal behaviors. In addition, realistic scenarios involve long-horizon planning, which necessitates the extraction of useful skills within sub-trajectories. Recently, the conditional diffusion model has been shown to be a promising approach to generate high-quality long-horizon plans for RL. However, their practicality for the goal-conditioned setting is still limited due to a number of technical assumptions made by the methods. In this paper, we propose SSD (Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method that leverages the conditional diffusion model to address these limitations. In summary, we ",
    "link": "https://arxiv.org/abs/2402.07226",
    "context": "Title: Stitching Sub-Trajectories with Conditional Diffusion Model for Goal-Conditioned Offline RL\nAbstract: Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an important problem in RL that focuses on acquiring diverse goal-oriented skills solely from pre-collected behavior datasets. In this setting, the reward feedback is typically absent except when the goal is achieved, which makes it difficult to learn policies especially from a finite dataset of suboptimal behaviors. In addition, realistic scenarios involve long-horizon planning, which necessitates the extraction of useful skills within sub-trajectories. Recently, the conditional diffusion model has been shown to be a promising approach to generate high-quality long-horizon plans for RL. However, their practicality for the goal-conditioned setting is still limited due to a number of technical assumptions made by the methods. In this paper, we propose SSD (Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method that leverages the conditional diffusion model to address these limitations. In summary, we ",
    "path": "papers/24/02/2402.07226.json",
    "total_tokens": 901,
    "translated_title": "用条件扩散模型拼接子轨迹以实现目标条件下的离线强化学习",
    "translated_abstract": "离线目标条件下的强化学习(Offline GCRL)是强化学习中一个重要问题，其专注于从预先收集的行为数据集中获取各种面向目标的技能。在这种设定中，奖励反馈通常只在达成目标时存在，这使得从有限的次优行为数据集中学习策略变得困难。此外，现实场景涉及长时程规划，这需要在子轨迹中提取有用的技能。最近，条件扩散模型已被证明是生成强化学习高质量长时程计划的一种有前景的方法。然而，由于这些方法中存在一些技术假设，其在目标条件设定下的实用性仍然有限。在本文中，我们提出了SSD(用扩散实现子轨迹拼接)，这是一种基于模型的离线GCRL方法，利用条件扩散模型来解决这些限制。",
    "tldr": "本文提出了一种名为SSD的模型，该模型利用条件扩散模型来解决离线目标条件下强化学习中的问题，包括从次优行为数据集学习策略的困难以及在子轨迹中提取有用技能的需求。",
    "en_tdlr": "This paper proposes a model called SSD, which leverages the conditional diffusion model to address problems in offline goal-conditioned reinforcement learning, including the challenge of learning policies from suboptimal behavior datasets and the need for extracting useful skills within sub-trajectories."
}