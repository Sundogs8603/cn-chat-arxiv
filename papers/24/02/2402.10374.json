{
    "title": "Revisiting Experience Replayable Conditions",
    "abstract": "arXiv:2402.10374v1 Announce Type: new  Abstract: Experience replay (ER) used in (deep) reinforcement learning is considered to be applicable only to off-policy algorithms. However, there have been some cases in which ER has been applied for on-policy algorithms, suggesting that off-policyness might be a sufficient condition for applying ER. This paper reconsiders more strict \"experience replayable conditions\" (ERC) and proposes the way of modifying the existing algorithms to satisfy ERC. To this end, instability of policy improvements is assumed to be a key in ERC. The instability factors are revealed from the viewpoint of metric learning as i) repulsive forces from negative samples and ii) replays of inappropriate experiences. Accordingly, the corresponding stabilization tricks are derived. As a result, it is confirmed through numerical simulations that the proposed stabilization tricks make ER applicable to an advantage actor-critic, an on-policy algorithm. In addition, its learning ",
    "link": "https://arxiv.org/abs/2402.10374",
    "context": "Title: Revisiting Experience Replayable Conditions\nAbstract: arXiv:2402.10374v1 Announce Type: new  Abstract: Experience replay (ER) used in (deep) reinforcement learning is considered to be applicable only to off-policy algorithms. However, there have been some cases in which ER has been applied for on-policy algorithms, suggesting that off-policyness might be a sufficient condition for applying ER. This paper reconsiders more strict \"experience replayable conditions\" (ERC) and proposes the way of modifying the existing algorithms to satisfy ERC. To this end, instability of policy improvements is assumed to be a key in ERC. The instability factors are revealed from the viewpoint of metric learning as i) repulsive forces from negative samples and ii) replays of inappropriate experiences. Accordingly, the corresponding stabilization tricks are derived. As a result, it is confirmed through numerical simulations that the proposed stabilization tricks make ER applicable to an advantage actor-critic, an on-policy algorithm. In addition, its learning ",
    "path": "papers/24/02/2402.10374.json",
    "total_tokens": 828,
    "translated_title": "重新审视经验重放条件",
    "translated_abstract": "深度强化学习中使用的经验重放（ER）被认为仅适用于离策略算法。然而，已经有一些情况下将ER应用于在策略算法中，这表明离策略性可能是应用ER的充分条件。本文重新审视了更严格的“经验重放条件”（ERC），并提出了修改现有算法以满足ERC的方法。为此，假设政策改进的不稳定性是ERC的关键。从度量学习的角度揭示了不稳定因素，包括i）来自负样本的排斥力和ii）不当经验的重放。因此，导出了相应的稳定化技巧。通过数值模拟验证，所提出的稳定化技巧使得ER适用于优势演员-评论家算法，一种在策略算法。此外，其学习",
    "tldr": "本文提出了更严格的“经验重放条件”，并揭示了政策改进的不稳定性因素，从而导出了相应的稳定化技巧，最终使得经验重放可应用于优势演员-评论家算法。"
}