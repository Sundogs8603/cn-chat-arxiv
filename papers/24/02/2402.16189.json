{
    "title": "One-stage Prompt-based Continual Learning",
    "abstract": "arXiv:2402.16189v1 Announce Type: cross  Abstract: Prompt-based Continual Learning (PCL) has gained considerable attention as a promising continual learning solution as it achieves state-of-the-art performance while preventing privacy violation and memory overhead issues. Nonetheless, existing PCL approaches face significant computational burdens because of two Vision Transformer (ViT) feed-forward stages; one is for the query ViT that generates a prompt query to select prompts inside a prompt pool; the other one is a backbone ViT that mixes information between selected prompts and image tokens. To address this, we introduce a one-stage PCL framework by directly using the intermediate layer's token embedding as a prompt query. This design removes the need for an additional feed-forward stage for query ViT, resulting in ~50% computational cost reduction for both training and inference with marginal accuracy drop < 1%. We further introduce a Query-Pool Regularization (QR) loss that regul",
    "link": "https://arxiv.org/abs/2402.16189",
    "context": "Title: One-stage Prompt-based Continual Learning\nAbstract: arXiv:2402.16189v1 Announce Type: cross  Abstract: Prompt-based Continual Learning (PCL) has gained considerable attention as a promising continual learning solution as it achieves state-of-the-art performance while preventing privacy violation and memory overhead issues. Nonetheless, existing PCL approaches face significant computational burdens because of two Vision Transformer (ViT) feed-forward stages; one is for the query ViT that generates a prompt query to select prompts inside a prompt pool; the other one is a backbone ViT that mixes information between selected prompts and image tokens. To address this, we introduce a one-stage PCL framework by directly using the intermediate layer's token embedding as a prompt query. This design removes the need for an additional feed-forward stage for query ViT, resulting in ~50% computational cost reduction for both training and inference with marginal accuracy drop < 1%. We further introduce a Query-Pool Regularization (QR) loss that regul",
    "path": "papers/24/02/2402.16189.json",
    "total_tokens": 873,
    "translated_title": "基于提示的单阶段持续学习",
    "translated_abstract": "提示驱动的持续学习（PCL）作为一种有望实现最先进性能的持续学习解决方案，因其在防止隐私侵犯和内存开销问题方面的表现而受到广泛关注。然而，现有的PCL方法面临着重大的计算负担，因为其中涉及两个ViT前馈阶段；一个用于生成提示查询以选择提示池中的提示的查询ViT，另一个是骨干ViT，用于在选择的提示和图像标记之间混合信息。为解决这个问题，我们通过直接使用中间层的令牌嵌入作为提示查询，引入了一种单阶段PCL框架。这种设计消除了查询ViT的额外前向阶段的需要，从而在训练和推理阶段将计算成本降低了约50%，并且在准确度下降不到1%的情况下。我们进一步引入了一种查询池正则化（QR）损失，用来规范",
    "tldr": "通过直接使用中间层的令牌嵌入作为提示查询，这项研究引入了一种单阶段PCL框架，可以在减少50%计算成本的同时，保持准确度下降小于1%。"
}