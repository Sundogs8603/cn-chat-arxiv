{
    "title": "Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations",
    "abstract": "The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.",
    "link": "https://arxiv.org/abs/2402.05713",
    "context": "Title: Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations\nAbstract: The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.",
    "path": "papers/24/02/2402.05713.json",
    "total_tokens": 980,
    "translated_title": "明明就在眼前：对弱势患者群体进行不可检测的对抗性偏见攻击",
    "translated_abstract": "人工智能在放射学中的广泛应用揭示了深度学习模型加剧对弱势患者群体的临床偏见的风险。虽然先前的文献主要关注训练的深度学习模型所展示的偏见的量化，但针对特定人口群体的对抗性偏见攻击以及其在临床环境中的影响仍然是一个未被充分研究的医学影像领域。在这项工作中，我们证明了针对人口统计学标签的毒化攻击可以向深度学习模型引入对抗性的诊断不足偏见，并在不影响整体模型性能的情况下降低对被低估群体的性能。此外，我们的结果在多个性能指标和人口群体（如性别、年龄以及其交叉子群）上表明，群体对于不可检测的对抗性偏见攻击的脆弱性与其在模型的训练数据中的表征直接相关。",
    "tldr": "该研究发现在医学影像中，可以通过针对特定人群的标签污染攻击来破坏深度学习模型的性能，并引入对抗性的诊断不足偏见。研究结果还表明，人群在训练数据中的表示对于不可检测的对抗性偏见攻击的脆弱性直接相关。"
}