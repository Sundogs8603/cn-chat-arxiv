{
    "title": "\"Understanding AI\": Semantic Grounding in Large Language Models",
    "abstract": "arXiv:2402.10992v1 Announce Type: cross  Abstract: Do LLMs understand the meaning of the texts they generate? Do they possess a semantic grounding? And how could we understand whether and what they understand? I start the paper with the observation that we have recently witnessed a generative turn in AI, since generative models, including LLMs, are key for self-supervised learning. To assess the question of semantic grounding, I distinguish and discuss five methodological ways. The most promising way is to apply core assumptions of theories of meaning in philosophy of mind and language to LLMs. Grounding proves to be a gradual affair with a three-dimensional distinction between functional, social and causal grounding. LLMs show basic evidence in all three dimensions. A strong argument is that LLMs develop world models. Hence, LLMs are neither stochastic parrots nor semantic zombies, but already understand the language they generate, at least in an elementary sense.",
    "link": "https://arxiv.org/abs/2402.10992",
    "context": "Title: \"Understanding AI\": Semantic Grounding in Large Language Models\nAbstract: arXiv:2402.10992v1 Announce Type: cross  Abstract: Do LLMs understand the meaning of the texts they generate? Do they possess a semantic grounding? And how could we understand whether and what they understand? I start the paper with the observation that we have recently witnessed a generative turn in AI, since generative models, including LLMs, are key for self-supervised learning. To assess the question of semantic grounding, I distinguish and discuss five methodological ways. The most promising way is to apply core assumptions of theories of meaning in philosophy of mind and language to LLMs. Grounding proves to be a gradual affair with a three-dimensional distinction between functional, social and causal grounding. LLMs show basic evidence in all three dimensions. A strong argument is that LLMs develop world models. Hence, LLMs are neither stochastic parrots nor semantic zombies, but already understand the language they generate, at least in an elementary sense.",
    "path": "papers/24/02/2402.10992.json",
    "total_tokens": 912,
    "translated_title": "在大型语言模型中理解人工智能：语义基础",
    "translated_abstract": "近年来我们目睹了人工智能的生成式转变，生成模型，包括大型语言模型（LLMs），对于自监督学习至关重要。我们提出一个问题，LLMs是否理解其生成的文本的含义？它们是否具有语义基础？我们如何了解它们是否理解以及理解的是什么？本文探讨了对语义基础问题的评估，区分和讨论了五种方法。其中最有前景的方法是将心灵哲学和语言学中关于含义的核心假设应用于LLMs。我们发现，语义基础是一个渐进的过程，包括功能性、社会性和因果性三个维度的区分。LLMs在这三个维度上展现出基本证据。一个强有力的论据是LLMs会形成世界模型。因此，LLMs既不是随机的鹦鹉，也不是语义僵尸，而是至少在基本层面上已经理解它们生成的语言。",
    "tldr": "大型语言模型（LLMs）展现了对语义的渐进理解，通过应用心灵哲学和语言学中关于含义的核心假设，研究发现LLMs不仅仅是生成文本的工具，而是在某种程度上已经理解了它们生成的语言。",
    "en_tdlr": "Large Language Models (LLMs) show a gradual understanding of semantics, as they apply core assumptions of theories of meaning in philosophy of mind and language, indicating that LLMs are not just text generators, but already understand the language they generate to some extent."
}