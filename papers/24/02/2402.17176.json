{
    "title": "DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection",
    "abstract": "arXiv:2402.17176v1 Announce Type: new  Abstract: Model-X knockoff, among various feature selection methods, received much attention recently due to its guarantee on false discovery rate (FDR) control. Subsequent to its introduction in parametric design, knockoff is advanced to handle arbitrary data distributions using deep learning-based generative modeling. However, we observed that current implementations of the deep Model-X knockoff framework exhibit limitations. Notably, the \"swap property\" that knockoffs necessitate frequently encounter challenges on sample level, leading to a diminished selection power. To overcome, we develop \"Deep Dependency Regularized Knockoff (DeepDRK)\", a distribution-free deep learning method that strikes a balance between FDR and power. In DeepDRK, a generative model grounded in a transformer architecture is introduced to better achieve the \"swap property\". Novel efficient regularization techniques are also proposed to reach higher power. Our model outper",
    "link": "https://arxiv.org/abs/2402.17176",
    "context": "Title: DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection\nAbstract: arXiv:2402.17176v1 Announce Type: new  Abstract: Model-X knockoff, among various feature selection methods, received much attention recently due to its guarantee on false discovery rate (FDR) control. Subsequent to its introduction in parametric design, knockoff is advanced to handle arbitrary data distributions using deep learning-based generative modeling. However, we observed that current implementations of the deep Model-X knockoff framework exhibit limitations. Notably, the \"swap property\" that knockoffs necessitate frequently encounter challenges on sample level, leading to a diminished selection power. To overcome, we develop \"Deep Dependency Regularized Knockoff (DeepDRK)\", a distribution-free deep learning method that strikes a balance between FDR and power. In DeepDRK, a generative model grounded in a transformer architecture is introduced to better achieve the \"swap property\". Novel efficient regularization techniques are also proposed to reach higher power. Our model outper",
    "path": "papers/24/02/2402.17176.json",
    "total_tokens": 881,
    "translated_title": "DeepDRK:深度依赖正则化 Knockoff 用于特征选择",
    "translated_abstract": "arXiv:2402.17176v1 公告类型:新 摘要: Model-X knockoff，在各种特征选择方法中，由于其对假发现率（FDR）控制的保证而最近受到广泛关注。在参数设计中引入后，knockoff被发展为使用基于深度学习的生成建模来处理任意数据分布。然而，我们观察到目前深度Model-X knockoff框架的实现存在局限性。值得注意的是，knockoffs所需的“交换属性”经常在样本级别遇到挑战，导致选择能力下降。为了克服这一问题，我们开发了“深度依赖正则化Knockoff（DeepDRK）”，这是一种不依赖分布的深度学习方法，可以在FDR和能力之间取得平衡。在DeepDRK中，引入了一种基于Transformer架构的生成模型，以更好地实现“交换属性”。还提出了新颖有效的正则化技术，以获得更高的能力。",
    "tldr": "DeepDRK是一种分布无关的深度学习方法，通过引入基于Transformer架构的生成模型以实现“交换属性”，并提出新颖有效的正则化技术，取得了在FDR和能力之间取得平衡。",
    "en_tdlr": "DeepDRK is a distribution-free deep learning method that achieves a balance between FDR and power by introducing a generative model based on Transformer architecture to achieve the \"swap property\" and proposing novel efficient regularization techniques."
}