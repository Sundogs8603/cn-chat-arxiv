{
    "title": "Preconditioners for the Stochastic Training of Implicit Neural Representations",
    "abstract": "arXiv:2402.08784v1 Announce Type: cross Abstract: Implicit neural representations have emerged as a powerful technique for encoding complex continuous multidimensional signals as neural networks, enabling a wide range of applications in computer vision, robotics, and geometry. While Adam is commonly used for training due to its stochastic proficiency, it entails lengthy training durations. To address this, we explore alternative optimization techniques for accelerated training without sacrificing accuracy. Traditional second-order optimizers like L-BFGS are suboptimal in stochastic settings, making them unsuitable for large-scale data sets. Instead, we propose stochastic training using curvature-aware diagonal preconditioners, showcasing their effectiveness across various signal modalities such as images, shape reconstruction, and Neural Radiance Fields (NeRF).",
    "link": "https://arxiv.org/abs/2402.08784",
    "context": "Title: Preconditioners for the Stochastic Training of Implicit Neural Representations\nAbstract: arXiv:2402.08784v1 Announce Type: cross Abstract: Implicit neural representations have emerged as a powerful technique for encoding complex continuous multidimensional signals as neural networks, enabling a wide range of applications in computer vision, robotics, and geometry. While Adam is commonly used for training due to its stochastic proficiency, it entails lengthy training durations. To address this, we explore alternative optimization techniques for accelerated training without sacrificing accuracy. Traditional second-order optimizers like L-BFGS are suboptimal in stochastic settings, making them unsuitable for large-scale data sets. Instead, we propose stochastic training using curvature-aware diagonal preconditioners, showcasing their effectiveness across various signal modalities such as images, shape reconstruction, and Neural Radiance Fields (NeRF).",
    "path": "papers/24/02/2402.08784.json",
    "total_tokens": 792,
    "translated_title": "隐式神经表示的随机训练的预处理器",
    "translated_abstract": "隐式神经表示已经成为一种强大的技术，用于将复杂连续多维信号编码为神经网络，从而实现计算机视觉、机器人学和几何学等广泛应用。尽管Adam由于其随机的高效性而被广泛应用于训练中，但其训练时间往往较长。为了解决这个问题，我们探索了在加速训练的同时不损失准确性的替代优化技术。传统的二阶优化器如L-BFGS在随机环境中效果不佳，因此不适用于大规模数据集。相反，我们提出了使用曲率感知对角预处理器进行随机训练，展示了它们在图像、形状重建和神经辐射场等各种信号模态中的有效性。",
    "tldr": "本论文提出了一种新的随机训练方法，通过使用曲率感知对角预处理器，在不损失准确性的情况下加速了隐式神经表示的训练过程，适用于多个信号模态。",
    "en_tdlr": "This paper proposes a novel method for stochastic training of implicit neural representations, using curvature-aware diagonal preconditioners to accelerate training without sacrificing accuracy, and it is effective across various signal modalities."
}