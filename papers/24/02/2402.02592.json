{
    "title": "Unified Training of Universal Time Series Forecasting Transformers",
    "abstract": "Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) fea",
    "link": "https://arxiv.org/abs/2402.02592",
    "context": "Title: Unified Training of Universal Time Series Forecasting Transformers\nAbstract: Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) fea",
    "path": "papers/24/02/2402.02592.json",
    "total_tokens": 884,
    "translated_title": "统一训练通用时间序列预测Transformer",
    "translated_abstract": "传统上，时间序列预测的深度学习在一个数据集中一模型的框架下运作，限制了其能够利用大型预训练模型的突破性影响的潜力。通用预测的概念，源于在大量时间序列数据集上进行预训练，设想一个能够处理各种下游预测任务的单一大型时间序列模型。然而，构建这样的模型对于时间序列数据存在独特的挑战，包括：i) 跨频率学习，ii) 适应多变量时间序列中任意数量的变量，以及iii) 解决大规模数据固有的不同分布特性。为了解决这些挑战，我们对传统的时间序列Transformer架构进行了新颖的增强，提出了基于Masked Encoder的通用时间序列预测Transformer（Moirai）。在我们新引入的大规模开放时间序列存档（LOTSA）数据集上进行训练。",
    "tldr": "本研究提出了一种改进的时间序列Transformer架构，名为Moirai，以解决时间序列预测中的跨频率学习、适应多变量时间序列以及解决大规模数据固有的不同分布特性等挑战，从而实现了统一训练通用时间序列预测Transformer。",
    "en_tdlr": "This research proposes an enhanced time series Transformer architecture, named Moirai, to address challenges such as cross-frequency learning, accommodating multivariate time series, and handling varying distributional properties inherent in large-scale data, thus achieving unified training of universal time series forecasting Transformers."
}