{
    "title": "Understanding Practical Membership Privacy of Deep Learning",
    "abstract": "We apply a state-of-the-art membership inference attack (MIA) to systematically test the practical privacy vulnerability of fine-tuning large image classification models.We focus on understanding the properties of data sets and samples that make them vulnerable to membership inference. In terms of data set properties, we find a strong power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. For an individual sample, large gradients at the end of training are strongly correlated with MIA vulnerability.",
    "link": "https://arxiv.org/abs/2402.06674",
    "context": "Title: Understanding Practical Membership Privacy of Deep Learning\nAbstract: We apply a state-of-the-art membership inference attack (MIA) to systematically test the practical privacy vulnerability of fine-tuning large image classification models.We focus on understanding the properties of data sets and samples that make them vulnerable to membership inference. In terms of data set properties, we find a strong power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. For an individual sample, large gradients at the end of training are strongly correlated with MIA vulnerability.",
    "path": "papers/24/02/2402.06674.json",
    "total_tokens": 732,
    "translated_title": "理解深度学习的实际成员隐私",
    "translated_abstract": "我们应用最先进的成员推理攻击（MIA）来系统地测试细调大型图像分类模型的实际隐私漏洞。我们的重点是理解使数据集和样本容易受到成员推理攻击的特性。在数据集特性方面，我们发现数据中每个类别的示例数量与成员推理攻击的漏洞之间存在强烈的幂律依赖关系，这是以攻击的真阳性率（在低假阳性率下测量）来衡量的。对于个别样本而言，在训练结束时产生的大梯度与成员推理攻击的漏洞之间存在很强的相关性。",
    "tldr": "该论文利用最先进的成员推理攻击方法系统地测试了细调大型图像分类模型的实际隐私漏洞，并发现数据集中每个类别的示例数量以及训练结束时的大梯度与成员推理攻击的漏洞之间存在关联。",
    "en_tdlr": "This paper systematically tests the practical privacy vulnerability of fine-tuning large image classification models using state-of-the-art membership inference attack (MIA), and discovers a strong correlation between the number of examples per class in the data and the vulnerability of membership inference attack as well as the large gradients at the end of training."
}