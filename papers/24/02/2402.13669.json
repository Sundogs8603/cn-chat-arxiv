{
    "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
    "abstract": "arXiv:2402.13669v1 Announce Type: new  Abstract: The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs",
    "link": "https://arxiv.org/abs/2402.13669",
    "context": "Title: Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning\nAbstract: arXiv:2402.13669v1 Announce Type: new  Abstract: The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs",
    "path": "papers/24/02/2402.13669.json",
    "total_tokens": 858,
    "translated_title": "自蒸馏桥接语言模型微调中的分布差距",
    "translated_abstract": "大型语言模型（LLMs）的兴起彻底改变了自然语言处理，但将它们微调为特定任务常常面临在平衡性能和保留一般指示遵循能力之间的挑战。在本文中，我们认为任务数据集与LLMs之间的分布差距是主要的潜在原因。为了解决这一问题，我们引入了自蒸馏微调（SDFT），这是一种通过用模型本身生成的精简数据集引导微调以匹配其原始分布来桥接分布差距的新方法。在各种基准测试上对Llama-2-chat模型的实验结果表明，SDFT有效地缓解了灾难性遗忘，同时在下游任务上实现了与普通微调相媲美甚至更优越的性能。此外，SDFT展示了保持LLMs的有益性和安全对齐的潜力",
    "tldr": "SDFT是一种通过用模型本身生成的精简数据集来桥接分布差距的新方法，在缓解灾难性遗忘的同时，在下游任务上实现了与普通微调相媲美甚至更优越的性能",
    "en_tdlr": "SDFT is a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself, effectively mitigating catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to vanilla fine-tuning."
}