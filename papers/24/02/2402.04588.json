{
    "title": "UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset",
    "abstract": "Open-source large language models (LLMs) have gained significant strength across diverse fields. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual supervised fine-tuning. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. For language-specific abilities, we introduce a knowledge-grounded data augmentation approach to elicit more culture-specific knowledge of LLMs, improving their ability to serve users from different countries. For language-agnostic abilities, we find through experiments that modern LLMs exhibit strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic SFT data w",
    "link": "https://arxiv.org/abs/2402.04588",
    "context": "Title: UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset\nAbstract: Open-source large language models (LLMs) have gained significant strength across diverse fields. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual supervised fine-tuning. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. For language-specific abilities, we introduce a knowledge-grounded data augmentation approach to elicit more culture-specific knowledge of LLMs, improving their ability to serve users from different countries. For language-agnostic abilities, we find through experiments that modern LLMs exhibit strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic SFT data w",
    "path": "papers/24/02/2402.04588.json",
    "total_tokens": 883,
    "translated_title": "UltraLink: 一个开源的知识增强多语言监督微调数据集",
    "translated_abstract": "开源的大型语言模型(LLMs)在不同领域取得了显著的优势。然而，大部分研究主要集中在英文上，对于多语言监督微调的研究还相对有限。因此，在本研究中我们构建了一个开源的多语言监督微调数据集。与之前简单翻译英文指令的方法不同，我们考虑了LLMs的语言特定和语言无关能力。对于语言特定能力，我们引入了一个基于知识的数据增强方法，以提取LLMs更多的文化特定知识，提高它们为不同国家用户服务的能力。对于语言无关能力，通过实验发现现代LLMs展现出很强的跨语言迁移能力，因此多次学习相同内容的多种语言并不必要。因此，我们可以大幅减少语言无关SFT数据集。",
    "tldr": "本论文构建了一个开源的多语言监督微调数据集UltraLink，通过引入基于知识的数据增强方法提升了语言模型在文化特定知识上的能力，同时发现现代语言模型具有强大的跨语言迁移能力，减少了语言无关数据集的需求。",
    "en_tdlr": "This paper presents UltraLink, an open-source multilingual supervised fine-tuning dataset, which improves the language model's ability in culture-specific knowledge through knowledge-grounded data augmentation and reduces the need for language-agnostic dataset by leveraging the strong cross-lingual transfer capabilities of modern language models."
}