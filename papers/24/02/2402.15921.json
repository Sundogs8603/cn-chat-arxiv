{
    "title": "Pretraining Strategy for Neural Potentials",
    "abstract": "arXiv:2402.15921v1 Announce Type: new  Abstract: We propose a mask pretraining method for Graph Neural Networks (GNNs) to improve their performance on fitting potential energy surfaces, particularly in water systems. GNNs are pretrained by recovering spatial information related to masked-out atoms from molecules, then transferred and finetuned on atomic forcefields. Through such pretraining, GNNs learn meaningful prior about structural and underlying physical information of molecule systems that are useful for downstream tasks. From comprehensive experiments and ablation studies, we show that the proposed method improves the accuracy and convergence speed compared to GNNs trained from scratch or using other pretraining techniques such as denoising. On the other hand, our pretraining method is suitable for both energy-centric and force-centric GNNs. This approach showcases its potential to enhance the performance and data efficiency of GNNs in fitting molecular force fields.",
    "link": "https://arxiv.org/abs/2402.15921",
    "context": "Title: Pretraining Strategy for Neural Potentials\nAbstract: arXiv:2402.15921v1 Announce Type: new  Abstract: We propose a mask pretraining method for Graph Neural Networks (GNNs) to improve their performance on fitting potential energy surfaces, particularly in water systems. GNNs are pretrained by recovering spatial information related to masked-out atoms from molecules, then transferred and finetuned on atomic forcefields. Through such pretraining, GNNs learn meaningful prior about structural and underlying physical information of molecule systems that are useful for downstream tasks. From comprehensive experiments and ablation studies, we show that the proposed method improves the accuracy and convergence speed compared to GNNs trained from scratch or using other pretraining techniques such as denoising. On the other hand, our pretraining method is suitable for both energy-centric and force-centric GNNs. This approach showcases its potential to enhance the performance and data efficiency of GNNs in fitting molecular force fields.",
    "path": "papers/24/02/2402.15921.json",
    "total_tokens": 859,
    "translated_title": "神经势预训练策略",
    "translated_abstract": "我们提出了一种面向图神经网络(GNNs)的掩码预训练方法，以提高其在拟合势能表面方面的性能，特别是在水系统中。GNNs通过从分子中恢复与掩码的原子相关的空间信息进行预训练，然后在原子力场上进行转移和微调。通过这种预训练，GNNs学习了关于分子系统的结构和潜在物理信息的有意义先验，对于下游任务是有用的。通过全面的实验和消融研究，我们展示了所提出的方法相对于从头开始训练或使用其他预训练技术（如去噪）的GNNs，在精度和收敛速度上的提高。另一方面，我们的预训练方法适用于以能量为中心和以力为中心的GNNs。这种方法展示了它在拟合分子力场方面提高了GNNs的性能和数据效率的潜力。",
    "tldr": "通过提出的面向图神经网络的掩码预训练方法，改善了GNN在拟合水系统势能表面方面的性能，并在精度和收敛速度上优于从头开始训练或使用其他预训练技术。",
    "en_tdlr": "The proposed mask pretraining method enhances the performance of Graph Neural Networks in fitting potential energy surfaces of water systems, showing improvements in accuracy and convergence speed compared to training from scratch or using other pretraining techniques."
}