{
    "title": "Efficient Models for the Detection of Hate, Abuse and Profanity",
    "abstract": "Large Language Models (LLMs) are the cornerstone for many Natural Language Processing (NLP) tasks like sentiment analysis, document classification, named entity recognition, question answering, summarization, etc. LLMs are often trained on data which originates from the web. This data is prone to having content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP, please refer to the Appendix. Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content. For example, when the open-source RoBERTa model (specifically, the RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted to replace the mask token in `I do not know that Persian people are that MASK` it returns the word `stupid` with the highest score. This is unacceptable in civil discourse.The detection of Hate, Abuse and Profanity in text is a vital component of creating civil and unbiased LLMs, which is needed not only f",
    "link": "https://arxiv.org/abs/2402.05624",
    "context": "Title: Efficient Models for the Detection of Hate, Abuse and Profanity\nAbstract: Large Language Models (LLMs) are the cornerstone for many Natural Language Processing (NLP) tasks like sentiment analysis, document classification, named entity recognition, question answering, summarization, etc. LLMs are often trained on data which originates from the web. This data is prone to having content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP, please refer to the Appendix. Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content. For example, when the open-source RoBERTa model (specifically, the RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted to replace the mask token in `I do not know that Persian people are that MASK` it returns the word `stupid` with the highest score. This is unacceptable in civil discourse.The detection of Hate, Abuse and Profanity in text is a vital component of creating civil and unbiased LLMs, which is needed not only f",
    "path": "papers/24/02/2402.05624.json",
    "total_tokens": 834,
    "translated_title": "针对仇恨、辱骂和亵渎检测的高效模型",
    "translated_abstract": "大型语言模型(LLM)是许多自然语言处理(NLP)任务的基石，如情感分析、文档分类、命名实体识别、问答、摘要等。LLM通常在来自网络的数据上进行训练。这些数据容易包含仇恨、辱骂和亵渎(HAP)内容。由于LLM在训练过程中接触到HAP内容，模型会学习到并生成带有仇恨或亵渎内容。例如，当使用HuggingFace(Transformers库的开源RoBERTa模型(具体来说，是RoBERTa基础模型))来替换句子`I do not know that Persian people are that MASK`中的掩码标记时，它返回得分最高的词为`stupid`。这在文明对话中是不可接受的。文本中的仇恨、辱骂和亵渎的检测是创建文明和没有偏见的LLM的重要组成部分。",
    "tldr": "这篇论文提出了针对仇恨、辱骂和亵渎检测的高效模型，因为大型语言模型在训练过程中可能学习到这些负面内容并生成不合适的文本。"
}