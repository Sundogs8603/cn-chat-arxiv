{
    "title": "Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear Unit (TeLU)",
    "abstract": "In this paper, we introduce the Hyperbolic Tangent Exponential Linear Unit (TeLU), a novel neural network activation function, represented as $f(x) = x{\\cdot}tanh(e^x)$. TeLU is designed to overcome the limitations of conventional activation functions like ReLU, GELU, and Mish by addressing the vanishing and, to an extent, the exploding gradient problems. Our theoretical analysis and empirical assessments reveal that TeLU outperforms existing activation functions in stability and robustness, effectively adjusting activation outputs' mean towards zero for enhanced training stability and convergence. Extensive evaluations against popular activation functions (ReLU, GELU, SiLU, Mish, Logish, Smish) across advanced architectures, including Resnet-50, demonstrate TeLU's lower variance and superior performance, even under hyperparameter conditions optimized for other functions. In large-scale tests with challenging datasets like CIFAR-10, CIFAR-100, and TinyImageNet, encompassing 860 scenari",
    "link": "https://arxiv.org/abs/2402.02790",
    "context": "Title: Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear Unit (TeLU)\nAbstract: In this paper, we introduce the Hyperbolic Tangent Exponential Linear Unit (TeLU), a novel neural network activation function, represented as $f(x) = x{\\cdot}tanh(e^x)$. TeLU is designed to overcome the limitations of conventional activation functions like ReLU, GELU, and Mish by addressing the vanishing and, to an extent, the exploding gradient problems. Our theoretical analysis and empirical assessments reveal that TeLU outperforms existing activation functions in stability and robustness, effectively adjusting activation outputs' mean towards zero for enhanced training stability and convergence. Extensive evaluations against popular activation functions (ReLU, GELU, SiLU, Mish, Logish, Smish) across advanced architectures, including Resnet-50, demonstrate TeLU's lower variance and superior performance, even under hyperparameter conditions optimized for other functions. In large-scale tests with challenging datasets like CIFAR-10, CIFAR-100, and TinyImageNet, encompassing 860 scenari",
    "path": "papers/24/02/2402.02790.json",
    "total_tokens": 1023,
    "translated_title": "通过双曲正切指数线性单元（TeLU）实现稳定和稳健的深度学习",
    "translated_abstract": "本文引入了一种新颖的神经网络激活函数——双曲正切指数线性单元（TeLU），表示为$f(x) = x{\\cdot}tanh(e^x)$。TeLU旨在克服传统激活函数（如ReLU、GELU和Mish）的局限性，解决梯度消失和爆炸问题。我们的理论分析和实证评估表明，TeLU在稳定性和鲁棒性方面优于现有的激活函数，有效地将激活输出的均值调整为零，增强了训练的稳定性和收敛性。对包括Resnet-50在内的先进架构进行了广泛评估，与流行的激活函数（ReLU、GELU、SiLU、Mish、Logish、Smish）进行了对比，结果显示TeLU具有较低的方差和优秀的性能，即使在针对其他函数进行优化的超参数条件下也是如此。在包括CIFAR-10、CIFAR-100和TinyImageNet在内的具有挑战性的大规模测试中，涵盖了860个场景。",
    "tldr": "本文提出了一种新的神经网络激活函数——双曲正切指数线性单元（TeLU），通过解决传统激活函数的局限性，如梯度消失和爆炸问题，实现了稳定和稳健的深度学习。与流行的激活函数相比，TeLU在稳定性和性能上具有更优异的表现，并在大规模测试中验证了其优越性能。",
    "en_tdlr": "This paper introduces a novel neural network activation function called Hyperbolic Tangent Exponential Linear Unit (TeLU), which overcomes the limitations of traditional activation functions and achieves stable and robust deep learning. Compared to popular activation functions, TeLU outperforms them in terms of stability and performance, as validated in large-scale tests."
}