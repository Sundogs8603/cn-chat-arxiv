{
    "title": "Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish",
    "abstract": "arXiv:2402.09759v1 Announce Type: cross  Abstract: This study explores the potential of fine-tuning foundational English Large Language Models (LLMs) for generating Polish text. The first step involves Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB, consisting of 276 million Polish tokens. The LAPT is followed by additional fine-tuning aimed at solving nine KLEJ challenges. Our trained model Curie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02 among decoder-based Polish models but also closely rivals the performance of the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9 tasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn Polish. The LAPT was completed in less than five days using a consumer GPU, highlighting the method's efficiency. The proficiency of the model in Polish was significantly enhanced, demonstrating the viability of this approach for adding new languages to existing LLMs",
    "link": "https://arxiv.org/abs/2402.09759",
    "context": "Title: Efficient Language Adaptive Pre-training: Extending State-of-the-Art Large Language Models for Polish\nAbstract: arXiv:2402.09759v1 Announce Type: cross  Abstract: This study explores the potential of fine-tuning foundational English Large Language Models (LLMs) for generating Polish text. The first step involves Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB, consisting of 276 million Polish tokens. The LAPT is followed by additional fine-tuning aimed at solving nine KLEJ challenges. Our trained model Curie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02 among decoder-based Polish models but also closely rivals the performance of the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9 tasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn Polish. The LAPT was completed in less than five days using a consumer GPU, highlighting the method's efficiency. The proficiency of the model in Polish was significantly enhanced, demonstrating the viability of this approach for adding new languages to existing LLMs",
    "path": "papers/24/02/2402.09759.json",
    "total_tokens": 966,
    "translated_title": "高效的语言自适应预训练：扩展最新的大规模语言模型用于波兰语",
    "translated_abstract": "本研究探讨了将基础英文大规模语言模型（LLMs）微调为生成波兰文的潜力。首先，通过对3.11 GB高质量数据集进行语言自适应预训练（LAPT），该数据集包含2.76亿个波兰语tokens。LAPT后进行了额外的微调，旨在解决九个KLEJ挑战。我们训练的模型Curie-7B-v1不仅在基于解码器的波兰模型中具有最低的困惑度3.02，而且在8个任务中与最好的波兰编码器-解码器模型之间的差距不到2%。Curie-7B-v1仅使用典型数据集大小的约2-3%来学习波兰语。LAPT在不到五天的时间内使用普通GPU完成，凸显了该方法的高效性。模型在波兰语方面的熟练度显著提高，证明了该方法在将新语言添加到现有LLMs中的可行性。",
    "tldr": "本研究使用高效的语言自适应预训练方法，成功将基础英文大规模语言模型应用于生成波兰文，并在困惑度和任务表现上取得了显著的改进，为向现有语言模型添加新语言开辟了新途径。"
}