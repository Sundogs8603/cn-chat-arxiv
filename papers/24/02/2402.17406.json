{
    "title": "LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning",
    "abstract": "arXiv:2402.17406v1 Announce Type: cross  Abstract: Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts. Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning. Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts. This feature serves as temporal coding, curbing the risk of forgetting parameter",
    "link": "https://arxiv.org/abs/2402.17406",
    "context": "Title: LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning\nAbstract: arXiv:2402.17406v1 Announce Type: cross  Abstract: Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts. Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning. Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts. This feature serves as temporal coding, curbing the risk of forgetting parameter",
    "path": "papers/24/02/2402.17406.json",
    "total_tokens": 827,
    "translated_title": "LSPT：用于视觉表示学习的长期空间提示调整",
    "translated_abstract": "视觉提示调整（VPT）技术因其通过专用的可学习令牌（称为提示）将预训练的视觉Transformer（ViT）调整到下游视觉任务而闻名。在自监督视觉Transformer中使用的当代VPT方法通常默认引入来源自模型先前块的新可学习提示或门控提示令牌。这种方法的一个关键缺失是未利用长距离先前块作为每个自监督ViT内提示的潜力来源。为了弥补这一重要差距，我们引入了长期空间提示调整（LSPT）- 一种革命性的视觉表示学习方法。 LSPT从人类大脑的复杂性中汲取灵感，巧妙地结合了长期门控提示。这个特性作为时间编码，减轻了遗忘参数的风险。",
    "tldr": "LSPT是一种革命性的视觉表示学习方法，通过引入长期门控提示，巧妙地利用长距离先前块作为提示的潜在来源，减轻了遗忘参数的风险。",
    "en_tdlr": "LSPT is a revolutionary approach to visual representation learning that ingeniously incorporates long-term gated prompts, effectively utilizing long-range previous blocks as potential sources for prompts and mitigating the risk of forgetting parameters."
}