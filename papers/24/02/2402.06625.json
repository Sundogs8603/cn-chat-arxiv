{
    "title": "Understanding the Effects of Iterative Prompting on Truthfulness",
    "abstract": "The development of Large Language Models (LLMs) has notably transformed numerous sectors, offering impressive text generation capabilities. Yet, the reliability and truthfulness of these models remain pressing concerns. To this end, we investigate iterative prompting, a strategy hypothesized to refine LLM responses, assessing its impact on LLM truthfulness, an area which has not been thoroughly explored. Our extensive experiments delve into the intricacies of iterative prompting variants, examining their influence on the accuracy and calibration of model responses. Our findings reveal that naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors. In response to these challenges, we introduce several prompting variants designed to address the identified issues. These variants demonstrate marked improvements over existing baselines, signaling a promising direction for future research. Our work provides a nuanced understanding of iterative pr",
    "link": "https://arxiv.org/abs/2402.06625",
    "context": "Title: Understanding the Effects of Iterative Prompting on Truthfulness\nAbstract: The development of Large Language Models (LLMs) has notably transformed numerous sectors, offering impressive text generation capabilities. Yet, the reliability and truthfulness of these models remain pressing concerns. To this end, we investigate iterative prompting, a strategy hypothesized to refine LLM responses, assessing its impact on LLM truthfulness, an area which has not been thoroughly explored. Our extensive experiments delve into the intricacies of iterative prompting variants, examining their influence on the accuracy and calibration of model responses. Our findings reveal that naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors. In response to these challenges, we introduce several prompting variants designed to address the identified issues. These variants demonstrate marked improvements over existing baselines, signaling a promising direction for future research. Our work provides a nuanced understanding of iterative pr",
    "path": "papers/24/02/2402.06625.json",
    "total_tokens": 924,
    "translated_title": "理解迭代提示对真实性的影响",
    "translated_abstract": "大型语言模型（LLMs）的发展已经显著改变了许多领域，提供了令人印象深刻的文本生成能力。然而，这些模型的可靠性和真实性仍然是一个紧迫的问题。为此，我们研究了迭代提示，这是一种被假设可以提高LLM响应精确度的策略，并评估其对LLM真实性的影响，这个领域尚没有得到充分探索。我们进行了大量实验证明了迭代提示变体的复杂性，以及它们对模型响应的准确性和校准性的影响。我们的研究发现，简单的提示方法严重损害了真实性，导致校准误差加剧。为了应对这些挑战，我们提出了几个旨在解决已确定问题的提示变体。这些变体相对于现有基线模型表现出显著的改进，为未来的研究指明了一个有希望的方向。我们的工作提供了对迭代提示的细致理解。",
    "tldr": "本研究探讨了迭代提示对大型语言模型（LLMs）真实性的影响，并发现简单的提示方法严重损害了真实性。我们提出了几个提示变体来改善这一问题，并取得了显著的改进。这为未来的研究指明了一个有希望的方向。",
    "en_tdlr": "This study investigates the impact of iterative prompting on the truthfulness of Large Language Models (LLMs) and reveals that simple prompting methods significantly undermine truthfulness. Several prompting variants are introduced to address this issue and demonstrate marked improvements, signaling a promising direction for future research."
}