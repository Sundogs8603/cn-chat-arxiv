{
    "title": "Operator Learning: Algorithms and Analysis",
    "abstract": "arXiv:2402.15715v1 Announce Type: new  Abstract: Operator learning refers to the application of ideas from machine learning to approximate (typically nonlinear) operators mapping between Banach spaces of functions. Such operators often arise from physical models expressed in terms of partial differential equations (PDEs). In this context, such approximate operators hold great potential as efficient surrogate models to complement traditional numerical methods in many-query tasks. Being data-driven, they also enable model discovery when a mathematical description in terms of a PDE is not available. This review focuses primarily on neural operators, built on the success of deep neural networks in the approximation of functions defined on finite dimensional Euclidean spaces. Empirically, neural operators have shown success in a variety of applications, but our theoretical understanding remains incomplete. This review article summarizes recent progress and the current state of our theoretic",
    "link": "https://arxiv.org/abs/2402.15715",
    "context": "Title: Operator Learning: Algorithms and Analysis\nAbstract: arXiv:2402.15715v1 Announce Type: new  Abstract: Operator learning refers to the application of ideas from machine learning to approximate (typically nonlinear) operators mapping between Banach spaces of functions. Such operators often arise from physical models expressed in terms of partial differential equations (PDEs). In this context, such approximate operators hold great potential as efficient surrogate models to complement traditional numerical methods in many-query tasks. Being data-driven, they also enable model discovery when a mathematical description in terms of a PDE is not available. This review focuses primarily on neural operators, built on the success of deep neural networks in the approximation of functions defined on finite dimensional Euclidean spaces. Empirically, neural operators have shown success in a variety of applications, but our theoretical understanding remains incomplete. This review article summarizes recent progress and the current state of our theoretic",
    "path": "papers/24/02/2402.15715.json",
    "total_tokens": 806,
    "translated_title": "操作符学习：算法与分析",
    "translated_abstract": "操作符学习指的是将机器学习的思想应用于近似（通常是非线性的）在范数空间中映射的算子，这些算子通常来自于用偏微分方程（PDEs）表达的物理模型。在这种情况下，这种近似算子作为高效的替代模型在许多查询任务中具有巨大潜力，以补充传统的数值方法。由于是数据驱动的，它们还可以在数学上无法描述的情况下进行模型发现。这篇综述主要关注神经算子，建立在深度神经网络在有限维欧几里得空间上定义函数近似方面的成功基础上。从经验上看，神经算子在各种应用中已经显示出成功，但我们的理论理解仍然不完整。本综述文章总结了最近的进展和我们理论的现状。",
    "tldr": "神经算子是近似非线性算子映射的机器学习方法，可作为高效替代模型应用于多查询任务，尤其在没有数学描述的情况下进行模型发现。",
    "en_tdlr": "Neural operators, as a machine learning method for approximating nonlinear operator mappings, can serve as efficient surrogate models for many-query tasks, especially for model discovery when a mathematical description is not available."
}