{
    "title": "A Differentiable POGLM with Forward-Backward Message Passing",
    "abstract": "The partially observable generalized linear model (POGLM) is a powerful tool for understanding neural connectivity under the assumption of existing hidden neurons. With spike trains only recorded from visible neurons, existing works use variational inference to learn POGLM meanwhile presenting the difficulty of learning this latent variable model. There are two main issues: (1) the sampled Poisson hidden spike count hinders the use of the pathwise gradient estimator in VI; and (2) the existing design of the variational model is neither expressive nor time-efficient, which further affects the performance. For (1), we propose a new differentiable POGLM, which enables the pathwise gradient estimator, better than the score function gradient estimator used in existing works. For (2), we propose the forward-backward message-passing sampling scheme for the variational model. Comprehensive experiments show that our differentiable POGLMs with our forward-backward message passing produce a bette",
    "link": "https://rss.arxiv.org/abs/2402.01263",
    "context": "Title: A Differentiable POGLM with Forward-Backward Message Passing\nAbstract: The partially observable generalized linear model (POGLM) is a powerful tool for understanding neural connectivity under the assumption of existing hidden neurons. With spike trains only recorded from visible neurons, existing works use variational inference to learn POGLM meanwhile presenting the difficulty of learning this latent variable model. There are two main issues: (1) the sampled Poisson hidden spike count hinders the use of the pathwise gradient estimator in VI; and (2) the existing design of the variational model is neither expressive nor time-efficient, which further affects the performance. For (1), we propose a new differentiable POGLM, which enables the pathwise gradient estimator, better than the score function gradient estimator used in existing works. For (2), we propose the forward-backward message-passing sampling scheme for the variational model. Comprehensive experiments show that our differentiable POGLMs with our forward-backward message passing produce a bette",
    "path": "papers/24/02/2402.01263.json",
    "total_tokens": 836,
    "translated_title": "具有前向-后向消息传递的可微分POGLM",
    "translated_abstract": "在隐含神经元存在的假设下，部分可观察广义线性模型（POGLM）是理解神经连接的强大工具。现有的工作利用变分推断来学习POGLM，但学习这种潜变量模型存在困难。存在两个主要问题：（1）采样的泊松隐藏尖峰数量阻碍了使用路径梯度估计器进行变分推断；（2）现有的变分模型设计既不具有表达性也不具有时间效率，进一步影响了性能。针对问题（1），我们提出了一种新的可微分POGLM，可以使用路径梯度估计器，优于现有工作中使用的得分函数梯度估计器。针对问题（2），我们提出了前向-后向消息传递采样方案用于变分模型。综合实验表明，我们的可微分POGLM与我们的前向-后向消息传递产生了更好的结果。",
    "tldr": "提出了一种具有前向-后向消息传递的可微分POGLM模型，解决了现有POGLM学习中的路径梯度估计和变分模型设计等问题。",
    "en_tdlr": "Proposed a differentiable POGLM model with forward-backward message passing, addressing issues in existing POGLM learning such as pathwise gradient estimation and variational model design."
}