{
    "title": "GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding",
    "abstract": "Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models.",
    "link": "https://arxiv.org/abs/2402.02082",
    "context": "Title: GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding\nAbstract: Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models.",
    "path": "papers/24/02/2402.02082.json",
    "total_tokens": 880,
    "translated_title": "拥有CaPE的GliDe：一种简化的快速推理解码方法",
    "translated_abstract": "快速推理解码是一种利用小而高效的草稿模型来减少LLM延迟的相对较新的解码框架。在本研究中，我们介绍了GliDe和CaPE，这两种简化的快速推理解码方法可以进一步提高冻结LLM的解码速度。具体而言，GliDe是一个修改过的草稿模型架构，可以重用目标LLM中的缓存键和值，而CaPE是一种借助草稿模型的置信度分数来选择额外候选令牌进行验证的扩展方法。对不同基准测试的大量实验证明，我们提出的GliDe草稿模型显著降低了预期的解码延迟。使用墙上时间进行额外评估显示，GliDe可以将Vicuna模型加速至2.17倍，并利用CaPE进一步提高至2.61倍。我们将发布我们的代码、数据和训练好的草稿模型。",
    "tldr": "本研究提出了GliDe和CaPE两种简化的快速推理解码方法，通过重用缓存键和值以及利用置信度分数选择额外候选令牌进行验证，显著降低了LLM的解码延迟。",
    "en_tdlr": "This study introduces GliDe and CaPE, two simplified methods for accelerating speculative decoding, which significantly reduce the decoding latency of a frozen LLM by reusing cached keys and values and selecting additional candidate tokens for verification based on confidence scores."
}