{
    "title": "Curiosity-driven Red-teaming for Large Language Models",
    "abstract": "arXiv:2402.19464v1 Announce Type: cross  Abstract: Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \\textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases an",
    "link": "https://arxiv.org/abs/2402.19464",
    "context": "Title: Curiosity-driven Red-teaming for Large Language Models\nAbstract: arXiv:2402.19464v1 Announce Type: cross  Abstract: Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \\textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases an",
    "path": "papers/24/02/2402.19464.json",
    "total_tokens": 871,
    "translated_title": "大语言模型的好奇驱动的红队对抗",
    "translated_abstract": "大型语言模型（LLMs）在许多自然语言应用中具有巨大潜力，但存在生成不正确或有毒内容的风险。为了探究LLM何时生成不需要的内容，当前的范例是招募一个人类测试者\\textit{红队}来设计输入提示（即测试案例），这些提示可以引出LLMs的不良反应。然而，仅依赖人类测试者是昂贵且耗时的。近期的研究通过训练一个单独的采用强化学习（RL）的红队LLM自动化红队对抗，生成最大化引出目标LLMs不良响应的测试案例。然而，当前的RL方法只能生成少量有效的测试案例，导致对引出目标LLMs不良响应提示范围的覆盖率较低。为了克服这一限制，我们将增加生成测试案例覆盖范围的问题与.",
    "tldr": "研究提出了一种新方法，能够通过训练红队LLM，自动化生成测试案例，以最大化引出目标LLM不良响应，以解决当前RL方法生成测试案例覆盖范围较低的问题。",
    "en_tdlr": "The study introduces a novel approach to automate the generation of test cases by training a red team LLM to maximize undesirable responses from the target LLM, addressing the low coverage issue in current RL methods for generating test cases."
}