{
    "title": "Provably Efficient Partially Observable Risk-Sensitive Reinforcement Learning with Hindsight Observation",
    "abstract": "arXiv:2402.18149v1 Announce Type: new  Abstract: This work pioneers regret analysis of risk-sensitive reinforcement learning in partially observable environments with hindsight observation, addressing a gap in theoretical exploration. We introduce a novel formulation that integrates hindsight observations into a Partially Observable Markov Decision Process (POMDP) framework, where the goal is to optimize accumulated reward under the entropic risk measure. We develop the first provably efficient RL algorithm tailored for this setting. We also prove by rigorous analysis that our algorithm achieves polynomial regret $\\tilde{O}\\left(\\frac{e^{|{\\gamma}|H}-1}{|{\\gamma}|H}H^2\\sqrt{KHS^2OA}\\right)$, which outperforms or matches existing upper bounds when the model degenerates to risk-neutral or fully observable settings. We adopt the method of change-of-measure and develop a novel analytical tool of beta vectors to streamline mathematical derivations. These techniques are of particular interes",
    "link": "https://arxiv.org/abs/2402.18149",
    "context": "Title: Provably Efficient Partially Observable Risk-Sensitive Reinforcement Learning with Hindsight Observation\nAbstract: arXiv:2402.18149v1 Announce Type: new  Abstract: This work pioneers regret analysis of risk-sensitive reinforcement learning in partially observable environments with hindsight observation, addressing a gap in theoretical exploration. We introduce a novel formulation that integrates hindsight observations into a Partially Observable Markov Decision Process (POMDP) framework, where the goal is to optimize accumulated reward under the entropic risk measure. We develop the first provably efficient RL algorithm tailored for this setting. We also prove by rigorous analysis that our algorithm achieves polynomial regret $\\tilde{O}\\left(\\frac{e^{|{\\gamma}|H}-1}{|{\\gamma}|H}H^2\\sqrt{KHS^2OA}\\right)$, which outperforms or matches existing upper bounds when the model degenerates to risk-neutral or fully observable settings. We adopt the method of change-of-measure and develop a novel analytical tool of beta vectors to streamline mathematical derivations. These techniques are of particular interes",
    "path": "papers/24/02/2402.18149.json",
    "total_tokens": 924,
    "translated_title": "在具有事后观察的部分可观察风险敏感强化学习中具有可证的高效性",
    "translated_abstract": "这项工作在带有事后观察的部分可观察环境中探索了风险敏感强化学习的后悔分析，填补了理论探索中的空白。我们引入了一种新颖的方案，将事后观察整合到部分可观察马尔可夫决策过程（POMDP）框架中，目标是在熵风险度量下优化累积奖励。我们开发了针对这种情况的第一个可证高效RL算法。我们还通过严格分析证明，我们的算法实现了多项式后悔$\\tilde{O}\\left(\\frac{e^{|{\\gamma}|H}-1}{|{\\gamma}|H}H^2\\sqrt{KHS^2OA}\\right)$，当模型退化为风险中性或完全可观测设置时，它超越或匹配现有的上限。我们采用变换测度方法，并开发了一种新颖的beta向量分析工具来简化数学推导。这些技术特别引人注目。",
    "tldr": "本研究提出了一种在部分可观察环境中带有事后观察的风险敏感强化学习的新方法，设计了第一个特定算法并证明其在此设置下取得了高效性能。",
    "en_tdlr": "This work introduces a novel approach for risk-sensitive reinforcement learning with hindsight observation in partially observable environments, develops the first specialized algorithm tailored for this setting, and proves its efficiency with a polynomial regret bound."
}