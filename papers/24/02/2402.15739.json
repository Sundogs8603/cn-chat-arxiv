{
    "title": "Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery",
    "abstract": "arXiv:2402.15739v1 Announce Type: new  Abstract: We study contextual bandits with low-rank structure where, in each round, if the (context, arm) pair $(i,j)\\in [m]\\times [n]$ is selected, the learner observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward matrix. Successive contexts are generated randomly in an i.i.d. manner and are revealed to the learner. For such bandits, we present efficient algorithms for policy evaluation, best policy identification and regret minimization. For policy evaluation and best policy identification, we show that our algorithms are nearly minimax optimal. For instance, the number of samples required to return an $\\varepsilon$-optimal policy with probability at least $1-\\delta$ typically scales as ${m+n\\over \\varepsilon^2}\\log(1/\\delta)$. Our regret minimization algorithm enjoys minimax guarantees scaling as $r^{7/4}(m+n)^{3/4}\\sqrt{T}$, which improves over existing algorithms. All the proposed algorithms consist of two phases: they",
    "link": "https://arxiv.org/abs/2402.15739",
    "context": "Title: Low-Rank Bandits via Tight Two-to-Infinity Singular Subspace Recovery\nAbstract: arXiv:2402.15739v1 Announce Type: new  Abstract: We study contextual bandits with low-rank structure where, in each round, if the (context, arm) pair $(i,j)\\in [m]\\times [n]$ is selected, the learner observes a noisy sample of the $(i,j)$-th entry of an unknown low-rank reward matrix. Successive contexts are generated randomly in an i.i.d. manner and are revealed to the learner. For such bandits, we present efficient algorithms for policy evaluation, best policy identification and regret minimization. For policy evaluation and best policy identification, we show that our algorithms are nearly minimax optimal. For instance, the number of samples required to return an $\\varepsilon$-optimal policy with probability at least $1-\\delta$ typically scales as ${m+n\\over \\varepsilon^2}\\log(1/\\delta)$. Our regret minimization algorithm enjoys minimax guarantees scaling as $r^{7/4}(m+n)^{3/4}\\sqrt{T}$, which improves over existing algorithms. All the proposed algorithms consist of two phases: they",
    "path": "papers/24/02/2402.15739.json",
    "total_tokens": 1038,
    "translated_title": "低秩赌徒通过紧绝对到无穷奇异子空间恢复",
    "translated_abstract": "我们研究了具有低秩结构的情境赌徒问题，在每一轮中，如果选择了(情境，动作)对$(i,j)\\in [m]\\times [n]$，学习者会观察一个未知低秩奖励矩阵的$(i,j)$-th入口的嘈杂样本。连续的情境以独立同分布的方式随机生成并透露给学习者。对于这样的赌徒问题，我们提出了高效的算法用于策略评估、最佳策略识别和遗憾最小化。对于策略评估和最佳策略识别，我们展示了我们的算法几乎是极小极大最优的。例如，为了以至少$1-\\delta$的概率返回一个$\\varepsilon$-最佳策略，通常需要的样本数大致按照${m+n\\over \\varepsilon^2}\\log(1/\\delta)$来衡量。我们的遗憾最小化算法享有的极小极大保证按照$r^{7/4}(m+n)^{3/4}\\sqrt{T}$缩放，这优于现有算法。所有提出的算法包括两个阶段：",
    "tldr": "该论文介绍了一种解决低秩环境中具有上下文信息的赌徒问题的高效算法，其中包括策略评估、最佳策略识别和遗憾最小化，并且在最佳策略识别和策略评估方面的算法几乎是极小极大最优的。",
    "en_tdlr": "This paper presents efficient algorithms for addressing contextual bandit problems in low-rank settings, including policy evaluation, best policy identification, and regret minimization, with the algorithms for best policy identification and policy evaluation being nearly minimax optimal."
}