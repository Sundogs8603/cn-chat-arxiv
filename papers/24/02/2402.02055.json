{
    "title": "Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning",
    "abstract": "In recent years, data selection has emerged as a core issue for large-scale visual-language model pretraining, especially on noisy web-curated datasets. One widely adopted strategy assigns quality scores such as CLIP similarity for each sample and retains the data pairs with the highest scores. However, these approaches are agnostic of data distribution and always fail to select the most informative samples. To solve this problem, we propose a simple yet theoretically principled metric named Variance Alignment Score (VAS), which has the form $\\langle \\Sigma_{\\text{test}}, \\Sigma_i\\rangle$. Here, $\\Sigma_{\\text{test}}$ represents the target (cross-)covariance matrix we aim to align, potentially based on prior knowledge, while $\\Sigma_i$ denotes the tensor product of single or multi-modal representations for the $i$-th sample. We further design a new data selection method that maximizes the total VAS. We provide theoretical analysis in a simplified setting to demonstrate the theoretical ",
    "link": "https://arxiv.org/abs/2402.02055",
    "context": "Title: Variance Alignment Score: A Simple But Tough-to-Beat Data Selection Method for Multimodal Contrastive Learning\nAbstract: In recent years, data selection has emerged as a core issue for large-scale visual-language model pretraining, especially on noisy web-curated datasets. One widely adopted strategy assigns quality scores such as CLIP similarity for each sample and retains the data pairs with the highest scores. However, these approaches are agnostic of data distribution and always fail to select the most informative samples. To solve this problem, we propose a simple yet theoretically principled metric named Variance Alignment Score (VAS), which has the form $\\langle \\Sigma_{\\text{test}}, \\Sigma_i\\rangle$. Here, $\\Sigma_{\\text{test}}$ represents the target (cross-)covariance matrix we aim to align, potentially based on prior knowledge, while $\\Sigma_i$ denotes the tensor product of single or multi-modal representations for the $i$-th sample. We further design a new data selection method that maximizes the total VAS. We provide theoretical analysis in a simplified setting to demonstrate the theoretical ",
    "path": "papers/24/02/2402.02055.json",
    "total_tokens": 870,
    "translated_title": "方差对齐分数: 一种简单但难以超越的多模式对比学习数据选择方法",
    "translated_abstract": "近年来，数据选择已经成为大规模视觉-语言模型预训练的核心问题，特别是在嘈杂的网络抽样数据集上。一种广泛采用的策略是为每个样本分配质量分数，如CLIP相似度，并保留具有最高分数的数据对。然而，这些方法对数据分布是无知的，始终无法选择最具信息量的样本。为了解决这个问题，我们提出了一种简单但有理论原则支持的度量方法，名为方差对齐分数(Variance Alignment Score，VAS)，它的形式是$\\langle \\Sigma_{\\text{test}}, \\Sigma_i\\rangle$。这里，$\\Sigma_{\\text{test}}$表示我们希望对齐的目标（交叉）协方差矩阵，可能基于先验知识，而$\\Sigma_i$表示第$i$个样本的单模态或多模态表示的张量积。我们进一步设计了一种新的数据选择方法，最大化总的VAS。我们在简化的设置下进行了理论分析来证明这个方法的理论优势。",
    "tldr": "提出了一种简单但有理论原则支持的度量方法——方差对齐分数(Variance Alignment Score，VAS)，用于解决数据选择问题。该方法通过最大化总的VAS来选择最具信息量的样本。"
}