{
    "title": "RQP-SGD: Differential Private Machine Learning through Noisy SGD and Randomized Quantization",
    "abstract": "The rise of IoT devices has prompted the demand for deploying machine learning at-the-edge with real-time, efficient, and secure data processing. In this context, implementing machine learning (ML) models with real-valued weight parameters can prove to be impractical particularly for large models, and there is a need to train models with quantized discrete weights. At the same time, these low-dimensional models also need to preserve privacy of the underlying dataset. In this work, we present RQP-SGD, a new approach for privacy-preserving quantization to train machine learning models for low-memory ML-at-the-edge. This approach combines differentially private stochastic gradient descent (DP-SGD) with randomized quantization, providing a measurable privacy guarantee in machine learning. In particular, we study the utility convergence of implementing RQP-SGD on ML tasks with convex objectives and quantization constraints and demonstrate its efficacy over deterministic quantization. Throug",
    "link": "https://arxiv.org/abs/2402.06606",
    "context": "Title: RQP-SGD: Differential Private Machine Learning through Noisy SGD and Randomized Quantization\nAbstract: The rise of IoT devices has prompted the demand for deploying machine learning at-the-edge with real-time, efficient, and secure data processing. In this context, implementing machine learning (ML) models with real-valued weight parameters can prove to be impractical particularly for large models, and there is a need to train models with quantized discrete weights. At the same time, these low-dimensional models also need to preserve privacy of the underlying dataset. In this work, we present RQP-SGD, a new approach for privacy-preserving quantization to train machine learning models for low-memory ML-at-the-edge. This approach combines differentially private stochastic gradient descent (DP-SGD) with randomized quantization, providing a measurable privacy guarantee in machine learning. In particular, we study the utility convergence of implementing RQP-SGD on ML tasks with convex objectives and quantization constraints and demonstrate its efficacy over deterministic quantization. Throug",
    "path": "papers/24/02/2402.06606.json",
    "total_tokens": 944,
    "translated_title": "RQP-SGD：通过嘈杂的随机梯度下降和随机量化实现差分隐私的机器学习",
    "translated_abstract": "物联网设备的兴起促使了对在边缘部署实时、高效、安全数据处理的机器学习的需求。在这种情况下，使用实值权重参数实现机器学习模型可能在大型模型上变得不切实际，因此有必要使用量化离散权重来训练模型。同时，这些低维模型也需要保护底层数据集的隐私。在这项工作中，我们提出了RQP-SGD，一种用于低内存边缘机器学习模型训练的隐私保护量化的新方法。该方法将差分隐私随机梯度下降（DP-SGD）与随机量化相结合，在机器学习中提供了可衡量的隐私保证。特别地，我们研究了在具有凸目标和量化约束的ML任务上实施RQP-SGD的效用收敛性，并证明其相对确定性量化的功效。",
    "tldr": "RQP-SGD是一种结合了差分隐私随机梯度下降和随机量化的新方法，用于在边缘部署的低内存机器学习模型训练中实现隐私保护。通过研究其在具有凸目标和量化约束的ML任务上的效用收敛性，并证明了其相对确定性量化的有效性。"
}