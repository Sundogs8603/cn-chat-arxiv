{
    "title": "Align Your Intents: Offline Imitation Learning via Optimal Transport",
    "abstract": "arXiv:2402.13037v1 Announce Type: cross  Abstract: Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and im",
    "link": "https://arxiv.org/abs/2402.13037",
    "context": "Title: Align Your Intents: Offline Imitation Learning via Optimal Transport\nAbstract: arXiv:2402.13037v1 Announce Type: cross  Abstract: Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and im",
    "path": "papers/24/02/2402.13037.json",
    "total_tokens": 770,
    "translated_title": "对齐您的意图：通过最优传输的离线模仿学习",
    "translated_abstract": "离线强化学习（RL）通过学习预先收集的数据来解决顺序决策问题，而无需与环境进行交互。我们展示出，即使缺乏明确的奖励或动作标签，模仿代理也可以仅通过观察专家来学习所需的行为。在我们的方法AILOT（通过最优传输对齐模仿学习）中，我们使用意图的特殊状态表示形式，其中包含数据内的两两空间距离。在给定这种表示形式的情况下，我们通过专家和代理轨迹之间的最优传输距离定义内在奖励函数。我们报告称AILOT在D4RL基准测试上优于最先进的离线模仿学习算法。",
    "tldr": "通过最优传输的离线模仿学习方法AILOT，可以在缺乏明确奖励的情况下，仅通过观察专家学习所需的行为。"
}