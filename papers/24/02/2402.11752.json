{
    "title": "Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing",
    "abstract": "arXiv:2402.11752v1 Announce Type: cross  Abstract: It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance.",
    "link": "https://arxiv.org/abs/2402.11752",
    "context": "Title: Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing\nAbstract: arXiv:2402.11752v1 Announce Type: cross  Abstract: It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance.",
    "path": "papers/24/02/2402.11752.json",
    "total_tokens": 892,
    "translated_title": "对角化SGD：通过重新参数化和平滑实现非可微模型的快速收敛SGD",
    "translated_abstract": "众所周知，对于非可微模型，展现出较低方差的重新参数化梯度估计器在实践中存在偏差。这可能危及基于梯度的优化方法（如随机梯度下降SGD）的正确性。我们引入了一个简单的语法框架来分块地定义非可微函数，并提出了一种系统方法，以获得使重新参数化梯度估计器无偏的平滑。我们的主要贡献是一种新颖的SGD变体，对角化随机梯度下降，它在优化过程中逐步提高平滑近似的准确性，并证明收敛到未平滑（原始）目标的稳定点。我们的实证评估显示，与现有技术相比，我们的方法简单、快速、稳定，并且在工作规范化方差上实现了数量级的降低。",
    "tldr": "引入了Diagonalisation Stochastic Gradient Descent（对角化SGD），通过重新参数化和平滑实现非可微模型的快速收敛SGD，在实证评估中表现出简单、快速、稳定，并且取得了数量级的工作规范化方差降低。",
    "en_tdlr": "Introduced Diagonalisation Stochastic Gradient Descent (Diagonalisation SGD) for fast and convergent SGD for non-differentiable models via reparameterisation and smoothing, showing simplicity, speed, stability, and orders of magnitude reduction in work-normalised variance in empirical evaluation."
}