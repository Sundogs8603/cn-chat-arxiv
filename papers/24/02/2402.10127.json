{
    "title": "Nonlinear spiked covariance matrices and signal propagation in deep neural networks",
    "abstract": "arXiv:2402.10127v1 Announce Type: cross  Abstract: Many recent works have studied the eigenvalue spectrum of the Conjugate Kernel (CK) defined by the nonlinear feature map of a feedforward neural network. However, existing results only establish weak convergence of the empirical eigenvalue distribution, and fall short of providing precise quantitative characterizations of the ''spike'' eigenvalues and eigenvectors that often capture the low-dimensional signal structure of the learning problem. In this work, we characterize these signal eigenvalues and eigenvectors for a nonlinear version of the spiked covariance model, including the CK as a special case. Using this general result, we give a quantitative description of how spiked eigenstructure in the input data propagates through the hidden layers of a neural network with random weights. As a second application, we study a simple regime of representation learning where the weight matrix develops a rank-one signal component over trainin",
    "link": "https://arxiv.org/abs/2402.10127",
    "context": "Title: Nonlinear spiked covariance matrices and signal propagation in deep neural networks\nAbstract: arXiv:2402.10127v1 Announce Type: cross  Abstract: Many recent works have studied the eigenvalue spectrum of the Conjugate Kernel (CK) defined by the nonlinear feature map of a feedforward neural network. However, existing results only establish weak convergence of the empirical eigenvalue distribution, and fall short of providing precise quantitative characterizations of the ''spike'' eigenvalues and eigenvectors that often capture the low-dimensional signal structure of the learning problem. In this work, we characterize these signal eigenvalues and eigenvectors for a nonlinear version of the spiked covariance model, including the CK as a special case. Using this general result, we give a quantitative description of how spiked eigenstructure in the input data propagates through the hidden layers of a neural network with random weights. As a second application, we study a simple regime of representation learning where the weight matrix develops a rank-one signal component over trainin",
    "path": "papers/24/02/2402.10127.json",
    "total_tokens": 961,
    "translated_title": "非线性尖峰协方差矩阵与深度神经网络中的信号传播",
    "translated_abstract": "许多最近的研究都研究了由前馈神经网络的非线性特征映射定义的共轭核（CK）的特征值谱。然而，现有的结果只能建立经验特征值分布的弱收敛性，并没有提供对通常捕捉学习问题的低维信号结构的“尖峰”特征值和特征向量的精确定量描述。在这项工作中，我们对非线性版本的尖峰协方差模型（包括CK作为特例）进行了这些信号特征值和特征向量的表征。利用这个一般结果，我们定量描述了输入数据中的尖峰特征结构如何通过具有随机权重的神经网络的隐藏层传播。作为第二个应用，我们研究了表示学习的一个简单情境，其中权重矩阵在训练过程中发展出一个秩为一的信号分量。",
    "tldr": "该论文研究了非线性尖峰协方差矩阵与深度神经网络中的信号传播。通过对尖峰特征结构的定量描述，揭示了输入数据中的低维信号结构如何经过神经网络的隐藏层传播。此外，研究了一种表示学习的简单情境，其中权重矩阵发展出一个秩为一的信号分量。",
    "en_tdlr": "This paper investigates the signal propagation in deep neural networks with nonlinear spiked covariance matrices. It provides a quantitative description of how low-dimensional signal structure in the input data propagates through the hidden layers of a neural network. Additionally, it studies a simple regime of representation learning where the weight matrix develops a rank-one signal component."
}