{
    "title": "Disclosure and Mitigation of Gender Bias in LLMs",
    "abstract": "arXiv:2402.11190v1 Announce Type: new  Abstract: Large Language Models (LLMs) can generate biased responses. Yet previous direct probing techniques contain either gender mentions or predefined gender stereotypes, which are challenging to comprehensively collect. Hence, we propose an indirect probing framework based on conditional generation. This approach aims to induce LLMs to disclose their gender bias even without explicit gender or stereotype mentions. We explore three distinct strategies to disclose explicit and implicit gender bias in LLMs. Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs. In addition, an increased model size or model alignment amplifies bias in most cases. Furthermore, we investigate three methods to mitigate bias in LLMs via Hyperparameter Tuning, Instruction Guiding, and Debias Tuning. Remarkably, these methods prove effective even in the absence of explici",
    "link": "https://arxiv.org/abs/2402.11190",
    "context": "Title: Disclosure and Mitigation of Gender Bias in LLMs\nAbstract: arXiv:2402.11190v1 Announce Type: new  Abstract: Large Language Models (LLMs) can generate biased responses. Yet previous direct probing techniques contain either gender mentions or predefined gender stereotypes, which are challenging to comprehensively collect. Hence, we propose an indirect probing framework based on conditional generation. This approach aims to induce LLMs to disclose their gender bias even without explicit gender or stereotype mentions. We explore three distinct strategies to disclose explicit and implicit gender bias in LLMs. Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs. In addition, an increased model size or model alignment amplifies bias in most cases. Furthermore, we investigate three methods to mitigate bias in LLMs via Hyperparameter Tuning, Instruction Guiding, and Debias Tuning. Remarkably, these methods prove effective even in the absence of explici",
    "path": "papers/24/02/2402.11190.json",
    "total_tokens": 891,
    "translated_title": "大型语言模型（LLMs）中的性别偏见披露和缓解",
    "translated_abstract": "大型语言模型（LLMs）可能会生成带有偏见的回复。然而，先前的直接探测技术包含性别提及或预定义的性别刻板印象，这些很难全面收集。因此，我们提出了一种基于条件生成的间接探测框架。这种方法旨在诱使LLMs披露其性别偏见，即使没有明确的性别或刻板印象提及。我们探索了三种不同的策略，以披露LLMs中的显性和隐性性别偏见。我们的实验表明，在大多数情况下，所有经过测试的LLMs均表现出明确和/或隐性性别偏见，即使输入中没有性别刻板印象。此外，模型尺寸增加或模型对齐会在大多数情况下放大偏见。此外，我们通过超参数调整、指导指引和去偏调整来研究三种缓解LLMs偏见的方法。值得注意的是，即使在没有明确性别刻板印象的情况下，这些方法也被证明是有效的。",
    "tldr": "提出了一种基于条件生成的间接探测框架，揭示了LLMs中的显性和隐性性别偏见，并探讨了三种缓解LLMs偏见的方法。",
    "en_tdlr": "Proposed an indirect probing framework based on conditional generation to reveal explicit and implicit gender bias in LLMs, and investigated three methods to mitigate bias in LLMs."
}