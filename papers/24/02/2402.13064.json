{
    "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models",
    "abstract": "arXiv:2402.13064v1 Announce Type: new  Abstract: We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate dive",
    "link": "https://arxiv.org/abs/2402.13064",
    "context": "Title: Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models\nAbstract: arXiv:2402.13064v1 Announce Type: new  Abstract: We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate dive",
    "path": "papers/24/02/2402.13064.json",
    "total_tokens": 871,
    "translated_title": "从零开始合成数据：通用指导调整用于语言模型",
    "translated_abstract": "我们引入了广义指导调整（称为GLAN），这是一种用于大型语言模型（LLMs）的指导调整的通用且可扩展的方法。与先前依赖于种子示例或现有数据集来构建指导调整数据的工作不同，GLAN仅利用事先策划的人类知识和能力分类法作为输入，并在所有学科领域生成大规模合成指导数据。具体来说，受人类教育系统中系统结构的启发，我们通过半自动方式，利用LLMs的帮助，将人类知识和能力分解为各种领域、子领域，最终到不同学科，构建了分类法。随后，我们为每个学科生成了一个全面的科目列表，并利用LLMs再次设计了适合每个科目的教学大纲。通过在大纲的每节课上详细介绍细粒度的关键概念，我们能够生成深入...",
    "tldr": "该研究提出了一种称为GLAN的广义指导调整方法，通过利用人类知识和能力分类法来生成大规模合成指导数据，为语言模型提供指导，从而实现对各个学科领域的广泛适用。"
}