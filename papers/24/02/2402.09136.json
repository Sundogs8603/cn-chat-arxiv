{
    "title": "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning",
    "abstract": "arXiv:2402.09136v1 Announce Type: cross Abstract: Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Several instruction tuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs. (2) Improving one's ability to evaluate the correctness of code solutions also enhances their ability to create it.",
    "link": "https://arxiv.org/abs/2402.09136",
    "context": "Title: DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning\nAbstract: arXiv:2402.09136v1 Announce Type: cross Abstract: Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Several instruction tuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs. (2) Improving one's ability to evaluate the correctness of code solutions also enhances their ability to create it.",
    "path": "papers/24/02/2402.09136.json",
    "total_tokens": 786,
    "translated_title": "DolphCoder: 用多样化和多目标指导调整进行回声定位的代码大型语言模型",
    "translated_abstract": "Code Large Language Models (Code LLMs)在代码相关任务中表现出色。已经提出了几种指导调整方法，以提高预训练Code LLM的代码生成性能。在本文中，我们引入了一种具有自我评估的多样化指令模型（DolphCoder）用于代码生成。它学习多样化的指令目标，并结合代码评估目标来增强其代码生成能力。我们的模型在HumanEval和MBPP基准测试中取得了优异的性能，为未来的代码指令调整工作提供了新的见解。我们的主要发现是：（1）通过增加具有不同推理路径的多样化响应来增强LLMs的代码能力。 （2）提高评估代码解决方案正确性的能力也会增强其创造代码的能力。",
    "tldr": "DolphCoder通过多样化的指导和自我评估提高了代码大型语言模型的生成能力，并在HumanEval和MBPP基准测试中取得了优异的性能。",
    "en_tdlr": "DolphCoder enhances the code generation ability of Code Large Language Models (Code LLMs) through diverse instruction targets and self-evaluation, achieving superior performance on HumanEval and MBPP benchmarks."
}