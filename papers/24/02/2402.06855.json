{
    "title": "For Better or For Worse? Learning Minimum Variance Features With Label Augmentation",
    "abstract": "Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label during model training. In this work, we analyze the role played by the label augmentation aspect of such methods. We prove that linear models on linearly separable data trained with label augmentation learn only the minimum variance features in the data, while standard training (which includes weight decay) can learn higher variance features. An important consequence of our results is negative: label smoothing and Mixup can be less robust to adversarial perturbations of the training data when compared to standard training. We verify that our theory reflects practice via a range of experiments on synthetic data and image classification benchmarks.",
    "link": "https://arxiv.org/abs/2402.06855",
    "context": "Title: For Better or For Worse? Learning Minimum Variance Features With Label Augmentation\nAbstract: Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label during model training. In this work, we analyze the role played by the label augmentation aspect of such methods. We prove that linear models on linearly separable data trained with label augmentation learn only the minimum variance features in the data, while standard training (which includes weight decay) can learn higher variance features. An important consequence of our results is negative: label smoothing and Mixup can be less robust to adversarial perturbations of the training data when compared to standard training. We verify that our theory reflects practice via a range of experiments on synthetic data and image classification benchmarks.",
    "path": "papers/24/02/2402.06855.json",
    "total_tokens": 914,
    "translated_title": "更好还是更差？通过标签增强学习最小方差特征",
    "translated_abstract": "在过去的十年中，数据增强对于成功地训练深度学习模型在分类任务上发挥了关键作用。数据增强技术中的一个重要子类-包括标签平滑和Mixup-涉及在模型训练过程中修改输入数据和输入标签。在这项工作中，我们分析了此类方法中标签增强的作用。我们证明了在线性可分数据上使用标签增强训练的线性模型只能学习到最小方差特征，而标准训练（包括权重衰减）可以学习到更高方差特征。我们的结果的一个重要后果是消极的：与标准训练相比，标签平滑和Mixup对于训练数据的对抗扰动可能不太鲁棒。我们通过对合成数据和图像分类基准的一系列实验证明了我们的理论与实践的一致性。",
    "tldr": "本研究分析了标签增强方法中标签增强的作用。研究证明，在线性可分数据上使用标签增强训练的线性模型只能学习到最小方差特征，而标准训练可以学习到更高方差特征。此外，标签平滑和Mixup对于训练数据的对抗扰动可能不太鲁棒。",
    "en_tdlr": "This study analyzes the role of label augmentation in methods such as label smoothing and Mixup. It shows that linear models trained with label augmentation only learn minimum variance features on linearly separable data, while standard training can learn higher variance features. Additionally, label smoothing and Mixup may be less robust to adversarial perturbations of the training data."
}