{
    "title": "Debating with More Persuasive LLMs Leads to More Truthful Answers",
    "abstract": "Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \\textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\\% and 88\\% accuracy respectively (naive baselines obtain 48\\% and 60\\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit",
    "link": "https://arxiv.org/abs/2402.06782",
    "context": "Title: Debating with More Persuasive LLMs Leads to More Truthful Answers\nAbstract: Common methods for aligning large language models (LLMs) with desired behaviour heavily rely on human-labelled data. However, as models grow increasingly sophisticated, they will surpass human expertise, and the role of human evaluation will evolve into non-experts overseeing experts. In anticipation of this, we ask: can weaker models assess the correctness of stronger models? We investigate this question in an analogous setting, where stronger models (experts) possess the necessary information to answer questions and weaker models (non-experts) lack this information. The method we evaluate is \\textit{debate}, where two LLM experts each argue for a different answer, and a non-expert selects the answer. We find that debate consistently helps both non-expert models and humans answer questions, achieving 76\\% and 88\\% accuracy respectively (naive baselines obtain 48\\% and 60\\%). Furthermore, optimising expert debaters for persuasiveness in an unsupervised manner improves non-expert abilit",
    "path": "papers/24/02/2402.06782.json",
    "total_tokens": 893,
    "translated_title": "与更有说服力的LLMs辩论会导致更真实的回答",
    "translated_abstract": "与所需行为一致的大型语言模型（LLM）的常见方法主要依赖于人工标注的数据。然而，随着模型变得越来越复杂，它们将超过人类专业知识，人类评估的角色将演变为非专家监督专家。在此之前，我们问：更弱的模型能评估更强的模型的正确性吗？我们在类似的环境中调查了这个问题，其中更强的模型（专家）拥有回答问题所需的信息，而更弱的模型（非专家）缺乏这些信息。我们评估的方法是\\textit{辩论}，其中两个LLM专家分别支持不同的答案，一个非专家选择答案。我们发现辩论 consistently帮助非专家模型和人类回答问题，分别达到76%和88%的准确性（朴素基准分别为48%和60%）。此外，以无监督方式优化专家辩论者的说服力会提高非专家的能力。",
    "tldr": "本文研究了更弱的语言模型是否能评估更强的模型的正确性。研究发现，通过进行辩论，非专家模型和人类回答问题的准确性都有所提高。",
    "en_tdlr": "This paper investigates whether weaker language models can assess the correctness of stronger models. The study finds that conducting debates helps improve accuracy of both non-expert models and humans in answering questions."
}