{
    "title": "Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning",
    "abstract": "In finetuning a large pretrained model to downstream tasks, parameter-efficient fine-tuning (PEFT) methods can effectively finetune pretrained models with few trainable parameters, but suffer from high GPU memory consumption and slow training speed. Because learnable parameters from these methods are entangled with the pretrained model, gradients related to the frozen pretrained model's parameters have to be computed and stored during finetuning. We propose Low-rank Attention Side-Tuning (LAST), which disentangles the trainable module from the pretrained model by freezing not only parameters but also outputs of the pretrained network. LAST trains a side-network composed of only low-rank self-attention modules. By viewing the pretrained model as a frozen feature extractor, the side-network takes intermediate output from the pretrained model and focus on learning task-specific knowledge. We also show that LAST can be highly parallel across multiple optimization objectives, making it very",
    "link": "https://arxiv.org/abs/2402.04009",
    "context": "Title: Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning\nAbstract: In finetuning a large pretrained model to downstream tasks, parameter-efficient fine-tuning (PEFT) methods can effectively finetune pretrained models with few trainable parameters, but suffer from high GPU memory consumption and slow training speed. Because learnable parameters from these methods are entangled with the pretrained model, gradients related to the frozen pretrained model's parameters have to be computed and stored during finetuning. We propose Low-rank Attention Side-Tuning (LAST), which disentangles the trainable module from the pretrained model by freezing not only parameters but also outputs of the pretrained network. LAST trains a side-network composed of only low-rank self-attention modules. By viewing the pretrained model as a frozen feature extractor, the side-network takes intermediate output from the pretrained model and focus on learning task-specific knowledge. We also show that LAST can be highly parallel across multiple optimization objectives, making it very",
    "path": "papers/24/02/2402.04009.json",
    "total_tokens": 892,
    "translated_title": "低秩注意力侧向调整用于参数高效微调",
    "translated_abstract": "在将大型预训练模型微调为下游任务时，参数高效微调（PEFT）方法可以有效地用少量可训练参数对预训练模型进行微调，但会导致显存占用高和训练速度慢的问题。由于这些方法的可学习参数与预训练模型相互缠结，微调过程中必须计算并存储与预训练模型冻结参数相关的梯度。我们提出了低秩注意力侧向调整（LAST）方法，通过不仅冻结参数，而且冻结预训练网络的输出，将可训练模块与预训练模型解耦。LAST训练一个只由低秩自注意力模块组成的侧向网络。将预训练模型视为冻结的特征提取器，侧向网络接受预训练模型的中间输出，并专注于学习任务特定的知识。我们还展示了LAST能够在多个优化目标之间高度并行化，从而具有很高的效率。",
    "tldr": "LAST方法通过冻结预训练模型的输出和参数，将可训练模块与预训练模型解耦，利用低秩自注意力模块训练一个侧向网络，从而实现参数高效微调，并具有高度并行化的效率。"
}