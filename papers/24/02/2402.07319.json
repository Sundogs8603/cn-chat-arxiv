{
    "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
    "abstract": "In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the oth",
    "link": "https://arxiv.org/abs/2402.07319",
    "context": "Title: ODIN: Disentangled Reward Mitigates Hacking in RLHF\nAbstract: In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the oth",
    "path": "papers/24/02/2402.07319.json",
    "total_tokens": 918,
    "translated_title": "ODIN: 脱耦奖励缓解RLHF中的黑客攻击",
    "translated_abstract": "在这项工作中，我们研究了在LLMs上从人类反馈的强化学习中出现的响应长度上的奖励黑客问题。LLMs的格式良好但不太有用的回复往往会欺骗LLMs甚至人类评估者以获得高分。同样的问题也存在于RL中的某些奖励模型中。为了解决训练和评估中的挑战，我们建立了一个更可靠的评估协议，用于比较不同训练配置之间的LLM评估分数和通过改变训练超参数得到的响应长度之间的权衡。基于这个评估，我们进行了大规模研究，结果揭示了RL中用于减轻长度偏差的超参数和技巧的有效性。我们进一步提出通过在共享特征表示上联合训练两个线性头来改进奖励模型，以预测奖励，一个训练来与长度相关，另一个训练来与内容相关。",
    "tldr": "本研究解决了强化学习中的奖励黑客问题，针对回复长度这一挑战，通过建立可靠的评估协议和改进奖励模型的方法，提出了减轻长度偏差的超参数和技巧，并进行了大规模研究。"
}