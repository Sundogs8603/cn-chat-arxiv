{
    "title": "Supervised Contrastive Representation Learning: Landscape Analysis with Unconstrained Features",
    "abstract": "arXiv:2402.18884v1 Announce Type: new  Abstract: Recent findings reveal that over-parameterized deep neural networks, trained beyond zero training-error, exhibit a distinctive structural pattern at the final layer, termed as Neural-collapse (NC). These results indicate that the final hidden-layer outputs in such networks display minimal within-class variations over the training set. While existing research extensively investigates this phenomenon under cross-entropy loss, there are fewer studies focusing on its contrastive counterpart, supervised contrastive (SC) loss. Through the lens of NC, this paper employs an analytical approach to study the solutions derived from optimizing the SC loss. We adopt the unconstrained features model (UFM) as a representative proxy for unveiling NC-related phenomena in sufficiently over-parameterized deep networks. We show that, despite the non-convexity of SC loss minimization, all local minima are global minima. Furthermore, the minimizer is unique (",
    "link": "https://arxiv.org/abs/2402.18884",
    "context": "Title: Supervised Contrastive Representation Learning: Landscape Analysis with Unconstrained Features\nAbstract: arXiv:2402.18884v1 Announce Type: new  Abstract: Recent findings reveal that over-parameterized deep neural networks, trained beyond zero training-error, exhibit a distinctive structural pattern at the final layer, termed as Neural-collapse (NC). These results indicate that the final hidden-layer outputs in such networks display minimal within-class variations over the training set. While existing research extensively investigates this phenomenon under cross-entropy loss, there are fewer studies focusing on its contrastive counterpart, supervised contrastive (SC) loss. Through the lens of NC, this paper employs an analytical approach to study the solutions derived from optimizing the SC loss. We adopt the unconstrained features model (UFM) as a representative proxy for unveiling NC-related phenomena in sufficiently over-parameterized deep networks. We show that, despite the non-convexity of SC loss minimization, all local minima are global minima. Furthermore, the minimizer is unique (",
    "path": "papers/24/02/2402.18884.json",
    "total_tokens": 882,
    "translated_title": "监督对比表示学习：具有不受限制特征的景观分析",
    "translated_abstract": "最近的研究发现，在超参数化的深度神经网络中，经过零训练误差训练后的网络，在最后一层呈现出严格的结构模式，被称为神经坍塌（NC）。这些结果表明，在这种网络中，最终隐藏层输出在训练集上显示出最小的类内变化。虽然现有研究在交叉熵损失下广泛探讨了这一现象，但关于其对应的对比损失——监督对比（SC）损失的研究较少。本文通过NC的视角，采用分析方法研究了优化SC损失所得解决方案。我们采用不受限制特征模型（UFM）作为代表性代理，揭示了在充分超参数化的深度网络中衍生的与NC相关现象。我们展示了，尽管SC损失最小化是非凸的，但所有局部最小值都是全局最小值。此外，最小化器是唯一的",
    "tldr": "通过监督对比表示学习，在超参数化的深度神经网络中研究解决方案，揭示了最小化SC损失的全局最小值和唯一最小化器。"
}