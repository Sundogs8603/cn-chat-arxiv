{
    "title": "LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition",
    "abstract": "The Vision Transformer (ViT) excels in accuracy when handling high-resolution images, yet it confronts the challenge of significant spatial redundancy, leading to increased computational and memory requirements. To address this, we present the Localization and Focus Vision Transformer (LF-ViT). This model operates by strategically curtailing computational demands without impinging on performance. In the Localization phase, a reduced-resolution image is processed; if a definitive prediction remains elusive, our pioneering Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively identifying and spotlighting class-discriminative regions based on initial findings. Subsequently, in the Focus phase, this designated region is used from the original image to enhance recognition. Uniquely, LF-ViT employs consistent parameters across both phases, ensuring seamless end-to-end optimization. Our empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs by",
    "link": "https://arxiv.org/abs/2402.00033",
    "context": "Title: LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition\nAbstract: The Vision Transformer (ViT) excels in accuracy when handling high-resolution images, yet it confronts the challenge of significant spatial redundancy, leading to increased computational and memory requirements. To address this, we present the Localization and Focus Vision Transformer (LF-ViT). This model operates by strategically curtailing computational demands without impinging on performance. In the Localization phase, a reduced-resolution image is processed; if a definitive prediction remains elusive, our pioneering Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively identifying and spotlighting class-discriminative regions based on initial findings. Subsequently, in the Focus phase, this designated region is used from the original image to enhance recognition. Uniquely, LF-ViT employs consistent parameters across both phases, ensuring seamless end-to-end optimization. Our empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs by",
    "path": "papers/24/02/2402.00033.json",
    "total_tokens": 867,
    "translated_title": "LF-ViT: 为提高图像识别效率减少视觉变换器中的空间冗余",
    "translated_abstract": "视觉变换器（ViT）在处理高分辨率图像时具有出色的准确性，但面临显著的空间冗余挑战，导致计算和内存需求增加。为解决这个问题，我们提出了局部化和聚焦视觉变换器（LF-ViT）。该模型通过有策略地削减计算需求，而不影响性能的方式进行操作。在局部化阶段，我们处理降低分辨率的图像；如果仍无法得出明确的预测结果，我们采用创新的邻近全局类别注意力（NGCA）机制，根据初步发现有效地识别和突出特定类别区域。随后，在聚焦阶段，我们利用原始图像中的指定区域来增强识别结果。独特的是，LF-ViT在两个阶段都使用一致的参数，确保无缝的端到端优化。我们的实证测试证实了LF-ViT的威力：它将Deit-S的FLOPs显著减少。",
    "tldr": "LF-ViT模型通过局部化和聚焦的方式减少计算需求，同时提高图像识别效率，显著降低了Deit-S模型的FLOPs。"
}