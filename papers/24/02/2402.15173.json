{
    "title": "Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer",
    "abstract": "arXiv:2402.15173v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for fine-tuning, which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer which is the first work to leverage the diagonal Hessian to enhance zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the expensive memory cost and only increases one forward pass per step. Extensive experiments on various models (350M~66B parameters) indicate that HiZOO improves model convergence, significantly reducing training steps and effectively enhancing model accuracy. Moreover, we visualize the optimization trajectories of HiZOO on test functions, il",
    "link": "https://arxiv.org/abs/2402.15173",
    "context": "Title: Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer\nAbstract: arXiv:2402.15173v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for fine-tuning, which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer which is the first work to leverage the diagonal Hessian to enhance zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the expensive memory cost and only increases one forward pass per step. Extensive experiments on various models (350M~66B parameters) indicate that HiZOO improves model convergence, significantly reducing training steps and effectively enhancing model accuracy. Moreover, we visualize the optimization trajectories of HiZOO on test functions, il",
    "path": "papers/24/02/2402.15173.json",
    "total_tokens": 863,
    "translated_title": "无痛人工大语言模型的二阶微调：一种基于Hessian信息的零阶优化器",
    "translated_abstract": "通过背向传播过程对大型语言模型（LLMs）进行微调，通常需要昂贵的GPU内存。最近的研究转向使用零阶优化器进行微调，通过两次前向传递显著节省内存。然而，这些优化器受不同维度之间参数曲率的异质性困扰。在这项工作中，我们提出了HiZOO，一种对角Hessian信息的零阶优化器，这是第一项利用对角Hessian增强零阶优化器进行LLMs微调的工作。HiZOO避免了昂贵的内存成本，并且每步只增加了一个前向传递。对各种模型（350M〜66B参数）进行的大量实验表明，HiZOO提高了模型收敛速度，显著减少了训练步骤，并有效提高了模型准确性。此外，我们可视化了HiZOO在测试函数上的优化轨迹，",
    "tldr": "提出了HiZOO，一种对角Hessian信息的零阶优化器，以增强LLMs微调过程中的模型收敛速度和准确性",
    "en_tdlr": "Introduced HiZOO, a diagonal Hessian informed zeroth-order optimizer to enhance model convergence and accuracy during LLMs fine-tuning."
}