{
    "title": "MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models",
    "abstract": "Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely-used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods th",
    "link": "https://rss.arxiv.org/abs/2402.01620",
    "context": "Title: MAGDi: Structured Distillation of Multi-Agent Interaction Graphs Improves Reasoning in Smaller Language Models\nAbstract: Multi-agent interactions between Large Language Model (LLM) agents have shown major improvements on diverse reasoning tasks. However, these involve long generations from multiple models across several rounds, making them expensive. Moreover, these multi-agent approaches fail to provide a final, single model for efficient inference. To address this, we introduce MAGDi, a new method for structured distillation of the reasoning interactions between multiple LLMs into smaller LMs. MAGDi teaches smaller models by representing multi-agent interactions as graphs, augmenting a base student model with a graph encoder, and distilling knowledge using three objective functions: next-token prediction, a contrastive loss between correct and incorrect reasoning, and a graph-based objective to model the interaction structure. Experiments on seven widely-used commonsense and math reasoning benchmarks show that MAGDi improves the reasoning capabilities of smaller models, outperforming several methods th",
    "path": "papers/24/02/2402.01620.json",
    "total_tokens": 849,
    "translated_title": "MAGDi：结构化蒸馏多智能体交互图在较小的语言模型中改善推理能力",
    "translated_abstract": "大语言模型（LLM）智能体之间的多智能体交互在各种推理任务中表现出重大改进。然而，这些方法涉及多个模型之间的长时间生成，成本高昂。此外，这些多智能体方法无法提供一个最终的、单一的模型进行高效推理。为了解决这个问题，我们引入了MAGDi，一种新的方法，用于将多个LLM之间的推理交互结构化蒸馏到较小的模型中。MAGDi通过将多智能体交互表示为图形，使用图形编码器增强基础学生模型，并使用三个目标函数进行知识蒸馏：下一个令牌预测、正确和错误推理之间的对比损失以及基于图形的目标来建模交互结构。在七个广泛使用的常识和数学推理基准测试上的实验结果表明，MAGDi改善了较小模型的推理能力，优于几种方法。",
    "tldr": "MAGDi是一种结构化蒸馏方法，通过将多个大语言模型之间的推理交互表示为图形，来改善较小语言模型的推理能力。"
}