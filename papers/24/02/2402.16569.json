{
    "title": "Pretrained Visual Uncertainties",
    "abstract": "arXiv:2402.16569v2 Announce Type: replace-cross  Abstract: Accurate uncertainty estimation is vital to trustworthy machine learning, yet uncertainties typically have to be learned for each task anew. This work introduces the first pretrained uncertainty modules for vision models. Similar to standard pretraining this enables the zero-shot transfer of uncertainties learned on a large pretraining dataset to specialized downstream datasets. We enable our large-scale pretraining on ImageNet-21k by solving a gradient conflict in previous uncertainty modules and accelerating the training by up to 180x. We find that the pretrained uncertainties generalize to unseen datasets. In scrutinizing the learned uncertainties, we find that they capture aleatoric uncertainty, disentangled from epistemic components. We demonstrate that this enables safe retrieval and uncertainty-aware dataset visualization. To encourage applications to further problems and domains, we release all pretrained checkpoints an",
    "link": "https://arxiv.org/abs/2402.16569",
    "context": "Title: Pretrained Visual Uncertainties\nAbstract: arXiv:2402.16569v2 Announce Type: replace-cross  Abstract: Accurate uncertainty estimation is vital to trustworthy machine learning, yet uncertainties typically have to be learned for each task anew. This work introduces the first pretrained uncertainty modules for vision models. Similar to standard pretraining this enables the zero-shot transfer of uncertainties learned on a large pretraining dataset to specialized downstream datasets. We enable our large-scale pretraining on ImageNet-21k by solving a gradient conflict in previous uncertainty modules and accelerating the training by up to 180x. We find that the pretrained uncertainties generalize to unseen datasets. In scrutinizing the learned uncertainties, we find that they capture aleatoric uncertainty, disentangled from epistemic components. We demonstrate that this enables safe retrieval and uncertainty-aware dataset visualization. To encourage applications to further problems and domains, we release all pretrained checkpoints an",
    "path": "papers/24/02/2402.16569.json",
    "total_tokens": 870,
    "translated_title": "预训练的视觉不确定性",
    "translated_abstract": "准确的不确定性估计对于可信任的机器学习至关重要，然而通常不确定性必须针对每个任务重新学习。这项工作引入了第一个针对视觉模型的预训练不确定性模块。类似于标准的预训练，这使得在大型预训练数据集上学习的不确定性可以零-shot转移到专门的下游数据集。我们通过解决以前不确定性模块中的梯度冲突并将训练加速高达180倍来实现在ImageNet-21k上的大规模预训练。我们发现预训练的不确定性可以泛化到未知数据集。在审查学习到的不确定性时，我们发现其捕获了与认知成分分离的非认知不确定性。我们证明这使得安全检索和对不确定性敏感的数据集可视化成为可能。为了鼓励将应用拓展到更多问题和领域，我们公开发布了所有预训练的检查点。",
    "tldr": "这项工作引入了第一个针对视觉模型的预训练不确定性模块，实现了不确定性的零-shot转移并加速了训练，能泛化到未知数据集，使得安全检索和对不确定性敏感的数据集可视化成为可能。",
    "en_tdlr": "This work introduces the first pretrained uncertainty modules for vision models, enabling zero-shot transfer of uncertainties learned on a large pretraining dataset, accelerating training process, and generalizing to unseen datasets for safe retrieval and uncertainty-aware dataset visualization."
}