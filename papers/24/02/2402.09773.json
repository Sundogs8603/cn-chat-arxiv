{
    "title": "NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models",
    "abstract": "arXiv:2402.09773v1 Announce Type: new  Abstract: The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware. Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models. Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach a",
    "link": "https://arxiv.org/abs/2402.09773",
    "context": "Title: NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models\nAbstract: arXiv:2402.09773v1 Announce Type: new  Abstract: The considerable size of Large Language Models (LLMs) presents notable deployment challenges, particularly on resource-constrained hardware. Structured pruning, offers an effective means to compress LLMs, thereby reducing storage costs and enhancing inference speed for more efficient utilization. In this work, we study data-efficient and resource-efficient structure pruning methods to obtain smaller yet still powerful models. Knowledge Distillation is well-suited for pruning, as the intact model can serve as an excellent teacher for pruned students. However, it becomes challenging in the context of LLMs due to memory constraints. To address this, we propose an efficient progressive Numerous-teacher pruning method (NutePrune). NutePrune mitigates excessive memory costs by loading only one intact model and integrating it with various masks and LoRA modules, enabling it to seamlessly switch between teacher and student roles. This approach a",
    "path": "papers/24/02/2402.09773.json",
    "total_tokens": 885,
    "translated_title": "NutePrune: 高效的大型语言模型逐渐剪枝方法，多个教师参与",
    "translated_abstract": "大型语言模型（LLMs）的巨大尺寸给资源受限硬件带来了显著的部署挑战。结构剪枝为压缩LLMs提供了一种有效的方式，从而降低存储成本，提升推断速度，实现更高效的利用。在这项工作中，我们研究了数据效率和资源效率的结构剪枝方法，以获取更小但依然强大的模型。知识蒸馏非常适合剪枝，因为完整的模型可以作为剪枝后的学生的优秀教师。然而，在LLMs的背景下，由于内存限制，这变得具有挑战性。为了解决这个问题，我们提出了一种高效逐渐剪枝方法（NutePrune）。NutePrune通过只加载一个完整模型并将其与各种掩码和LoRA模块集成，在教师和学生角色之间无缝切换，从而减轻了过多的内存开销。",
    "tldr": "NutePrune是一种高效逐渐剪枝方法，通过加载一个完整模型并将其与掩码和LoRA模块集成，实现了在大型语言模型中进行高效的结构剪枝。",
    "en_tdlr": "NutePrune is an efficient progressive pruning method for large language models, which achieves efficient structural pruning by loading one intact model and integrating it with masks and LoRA modules."
}