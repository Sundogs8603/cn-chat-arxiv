{
    "title": "Limitations of Agents Simulated by Predictive Models",
    "abstract": "There is increasing focus on adapting predictive models into agent-like systems, most notably AI assistants based on language models. We outline two structural reasons for why these models can fail when turned into agents. First, we discuss auto-suggestive delusions. Prior work has shown theoretically that models fail to imitate agents that generated the training data if the agents relied on hidden observations: the hidden observations act as confounding variables, and the models treat actions they generate as evidence for nonexistent observations. Second, we introduce and formally study a related, novel limitation: predictor-policy incoherence. When a model generates a sequence of actions, the model's implicit prediction of the policy that generated those actions can serve as a confounding variable. The result is that models choose actions as if they expect future actions to be suboptimal, causing them to be overly conservative. We show that both of those failures are fixed by includi",
    "link": "https://arxiv.org/abs/2402.05829",
    "context": "Title: Limitations of Agents Simulated by Predictive Models\nAbstract: There is increasing focus on adapting predictive models into agent-like systems, most notably AI assistants based on language models. We outline two structural reasons for why these models can fail when turned into agents. First, we discuss auto-suggestive delusions. Prior work has shown theoretically that models fail to imitate agents that generated the training data if the agents relied on hidden observations: the hidden observations act as confounding variables, and the models treat actions they generate as evidence for nonexistent observations. Second, we introduce and formally study a related, novel limitation: predictor-policy incoherence. When a model generates a sequence of actions, the model's implicit prediction of the policy that generated those actions can serve as a confounding variable. The result is that models choose actions as if they expect future actions to be suboptimal, causing them to be overly conservative. We show that both of those failures are fixed by includi",
    "path": "papers/24/02/2402.05829.json",
    "total_tokens": 960,
    "translated_title": "预测模型模拟的代理的局限性",
    "translated_abstract": "越来越多的关注点是将预测模型应用于类似于代理的系统，特别是基于语言模型的AI助手。我们概述了这些模型在转变成代理时可能失败的两个结构上的原因。首先，我们讨论了自动建议妄想症。先前的研究从理论上表明，如果代理依赖于隐藏观测数据，模型无法模仿生成训练数据的代理:隐藏观测作为混淆变量，模型将其生成的动作视为不存在观测的证据。其次，我们引入并正式研究了一个相关的新限制:预测-策略不一致性。当一个模型生成一系列动作时，模型对生成这些动作的策略的隐含预测可以作为混淆变量。结果是，模型选择动作时，好像它们预期未来的动作是次优的，导致它们过于保守。我们证明了包含这两种故障的修复。",
    "tldr": "预测模型模拟的代理存在两个结构上的局限性，分别是自动建议妄想症和预测-策略不一致性。前者是由于隐藏观测作为混淆变量，模型将生成的动作视为不存在观测的证据；后者是由于模型的隐含预测导致选择过于保守。这些故障可以通过纳入修复。",
    "en_tdlr": "The limitations of agents simulated by predictive models include auto-suggestive delusions and predictor-policy incoherence. The former arises when the models treat their generated actions as evidence for nonexistent observations due to hidden observations acting as confounding variables. The latter occurs when models choose overly conservative actions based on implicit predictions, assuming future actions to be suboptimal. These failures can be remedied by including necessary adjustments."
}