{
    "title": "The Matrix: A Bayesian learning model for LLMs",
    "abstract": "In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights",
    "link": "https://arxiv.org/abs/2402.03175",
    "context": "Title: The Matrix: A Bayesian learning model for LLMs\nAbstract: In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights",
    "path": "papers/24/02/2402.03175.json",
    "total_tokens": 907,
    "translated_title": "The Matrix: 一个用于LLMs的贝叶斯学习模型",
    "translated_abstract": "本文介绍了一个用于理解大型语言模型（LLMs）行为的贝叶斯学习模型。我们探索了基于预测下一个标记的LLM的优化指标，并开发了一个以此原则为基础的新型模型。我们的方法涉及构建一个由先验和多项式转移概率矩阵表示的理想生成文本模型，并研究LLMs如何逼近该矩阵。我们讨论了嵌入和多项式分布之间的映射的连续性，并提出了Dirichlet逼近定理来逼近任何先验。此外，我们演示了LLMs的文本生成如何与贝叶斯学习原理一致，并深入探讨了其在上下文学习中的影响，具体解释了为什么在更大的模型中出现了上下文学习，其中提示被视为需要更新的样本。我们的发现表明，LLMs的行为与贝叶斯学习一致，提供了新的见解。",
    "tldr": "本文介绍了一个贝叶斯学习模型，用于理解大型语言模型（LLMs）的行为。研究探索了LLMs的优化指标，并开发了一个新的基于预测下一个标记的模型。实验结果表明，LLMs的行为与贝叶斯学习一致，为上下文学习提供了新的见解。",
    "en_tdlr": "This paper introduces a Bayesian learning model to understand the behavior of Large Language Models (LLMs). The findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights into in-context learning."
}