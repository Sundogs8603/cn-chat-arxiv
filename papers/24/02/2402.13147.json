{
    "title": "SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations",
    "abstract": "arXiv:2402.13147v1 Announce Type: cross  Abstract: We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that ",
    "link": "https://arxiv.org/abs/2402.13147",
    "context": "Title: SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations\nAbstract: arXiv:2402.13147v1 Announce Type: cross  Abstract: We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that ",
    "path": "papers/24/02/2402.13147.json",
    "total_tokens": 858,
    "translated_title": "SubIQ: 逆向软 Q 学习用于获得次优演示的离线模仿",
    "translated_abstract": "我们考虑了离线模仿学习（IL），旨在从专家演示中模仿专家的行为，而无需与环境进行进一步交互。在离线 IL 中的一个主要挑战是处理仅涵盖状态-动作空间的一小部分的专家演示的有限支持。我们考虑离线 IL，其中专家演示受到限制，但是由更大规模的次优演示集合补充。大部分现有的用于此设置的离线 IL 方法基于行为克隆或分布匹配，其目的是将模仿策略的占用分布与专家策略的占用分布匹配。这种方法往往存在过拟合问题，因为专家演示有限，无法准确表示任何占用分布。另一方面，由于次优演示集合规模更大，有很高的可能性",
    "tldr": "逆向软 Q 学习用于获得次优演示的离线模仿挑战了离线 IL 中有限支持专家演示的问题，并提出了一种解决方案以匹配次优演示集合的占用分布",
    "en_tdlr": "Inverse Soft Q learning for offline imitation with suboptimal demonstrations addresses the limited support of expert demonstrations in offline IL and proposes a solution to match the occupancy distribution of the suboptimal demonstration set."
}