{
    "title": "Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency",
    "abstract": "arXiv:2402.15926v1 Announce Type: new  Abstract: We consider gradient descent (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly -- in $\\mathcal{O}(\\eta)$ steps -- and subsequently achieves an $\\tilde{\\mathcal{O}}(1 / (\\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an accelerated loss of $\\tilde{\\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\\eta:= \\Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\\tilde{\\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (SGD) with a large stepsize, under sui",
    "link": "https://arxiv.org/abs/2402.15926",
    "context": "Title: Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency\nAbstract: arXiv:2402.15926v1 Announce Type: new  Abstract: We consider gradient descent (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly -- in $\\mathcal{O}(\\eta)$ steps -- and subsequently achieves an $\\tilde{\\mathcal{O}}(1 / (\\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an accelerated loss of $\\tilde{\\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\\eta:= \\Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\\tilde{\\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (SGD) with a large stepsize, under sui",
    "path": "papers/24/02/2402.15926.json",
    "total_tokens": 1000,
    "translated_title": "逻辑回归的大步梯度下降：损失的非单调性提高了优化效率",
    "translated_abstract": "我们考虑了梯度下降（GD）与具有线性可分数据的逻辑回归结合使用的恒定步长情况，其中恒定步长$\\eta$非常大，以至于损失在初始阶段会震荡。我们展示了GD在$\\mathcal{O}(\\eta)$步内迅速退出这种初始震荡阶段，并在额外的$t$步之后实现了一个$\\tilde{\\mathcal{O}}(1 / (\\eta t) )$的收敛速率。我们的结果意味着，给定$T$步的预算，使用积极的步长$\\eta:= \\Theta( T)$，无需使用任何动量或变步长调度器，GD可以实现一个$\\tilde{\\mathcal{O}}(1/T^2)$的加速损失。我们的证明技术多才多艺，还可以处理一般分类损失函数（其中需要指数尾部来实现$\\tilde{\\mathcal{O}}(1/T^2)$的加速）、神经切线核区域的非线性预测器，以及具有大步长的在线随机梯度下降（SGD）。",
    "tldr": "该研究表明对于具有线性可分数据的逻辑回归问题，设置一个恒定但较大的步长，在初始震荡后可以实现较快的收敛，并且在一定步骤后可以达到加速的收敛速率，这种方法无需动量或变步长调度器。",
    "en_tdlr": "The study demonstrates that for logistic regression with linearly separable data, using a constant but large step size can achieve rapid convergence after initial oscillations and accelerated convergence rate after a certain number of steps, without the need for momentum or variable step size schedulers."
}