{
    "title": "United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once",
    "abstract": "arXiv:2402.15404v1 Announce Type: new  Abstract: In natural language processing and vision, pretraining is utilized to learn effective representations. Unfortunately, the success of pretraining does not easily carry over to time series due to potential mismatch between sources and target. Actually, common belief is that multi-dataset pretraining does not work for time series! Au contraire, we introduce a new self-supervised contrastive pretraining approach to learn one encoding from many unlabeled and diverse time series datasets, so that the single learned representation can then be reused in several target domains for, say, classification. Specifically, we propose the XD-MixUp interpolation method and the Soft Interpolation Contextual Contrasting (SICC) loss. Empirically, this outperforms both supervised training and other self-supervised pretraining methods when finetuning on low-data regimes. This disproves the common belief: We can actually learn from multiple time series datasets",
    "link": "https://arxiv.org/abs/2402.15404",
    "context": "Title: United We Pretrain, Divided We Fail! Representation Learning for Time Series by Pretraining on 75 Datasets at Once\nAbstract: arXiv:2402.15404v1 Announce Type: new  Abstract: In natural language processing and vision, pretraining is utilized to learn effective representations. Unfortunately, the success of pretraining does not easily carry over to time series due to potential mismatch between sources and target. Actually, common belief is that multi-dataset pretraining does not work for time series! Au contraire, we introduce a new self-supervised contrastive pretraining approach to learn one encoding from many unlabeled and diverse time series datasets, so that the single learned representation can then be reused in several target domains for, say, classification. Specifically, we propose the XD-MixUp interpolation method and the Soft Interpolation Contextual Contrasting (SICC) loss. Empirically, this outperforms both supervised training and other self-supervised pretraining methods when finetuning on low-data regimes. This disproves the common belief: We can actually learn from multiple time series datasets",
    "path": "papers/24/02/2402.15404.json",
    "total_tokens": 941,
    "translated_title": "聚而预训练，分而不胜！通过同时在75个数据集上预训练进行时间序列的表示学习",
    "translated_abstract": "在自然语言处理和视觉领域，预训练被用于学习有效的表示。然而，由于来源和目标之间可能存在的不匹配，预训练的成功并不容易应用于时间序列。事实上，普遍认为多数据集预训练在时间序列中并不奏效！相反地，我们引入了一种新的自监督对比预训练方法，可以从许多未标记和多样化的时间序列数据集中学习一个编码，然后可以在多个目标域（例如分类）中重复使用单一学到的表示。具体来说，我们提出了XD-MixUp插值方法和Soft插值上下文对比（SICC）损失。经验证，这优于在低数据情况下进行微调时的监督训练和其他自监督预训练方法。这一事实证明了常见看法的错误：我们实际上可以从多个时间序列数据集中进行学习。",
    "tldr": "引入了一种新的自监督对比预训练方法，通过在多个未标记和多样化的时间序列数据集上学习一个编码，证明了这种方法在低数据情境下优于监督训练和其他自监督预训练方法，颠覆了常见认知，时间序列可以从多个数据集中学习",
    "en_tdlr": "Introducing a new self-supervised contrastive pretraining approach to learn one encoding from many unlabeled and diverse time series datasets, this method outperforms supervised training and other self-supervised pretraining methods in low-data regimes, challenging the common belief that time series cannot benefit from learning across multiple datasets."
}