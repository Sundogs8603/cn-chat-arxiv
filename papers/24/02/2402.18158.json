{
    "title": "Evaluating Quantized Large Language Models",
    "abstract": "arXiv:2402.18158v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their appli",
    "link": "https://arxiv.org/abs/2402.18158",
    "context": "Title: Evaluating Quantized Large Language Models\nAbstract: arXiv:2402.18158v1 Announce Type: cross  Abstract: Post-training quantization (PTQ) has emerged as a promising technique to reduce the cost of large language models (LLMs). Specifically, PTQ can effectively mitigate memory consumption and reduce computational overhead in LLMs. To meet the requirements of both high efficiency and performance across diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to guide the selection of quantization methods. This paper presents a thorough evaluation of these factors by evaluating the effect of PTQ on Weight, Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon, Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with parameters ranging from 125M to 180B. The evaluation encompasses five types of tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization methods to demonstrate their appli",
    "path": "papers/24/02/2402.18158.json",
    "total_tokens": 872,
    "translated_title": "评估量化大型语言模型",
    "translated_abstract": "后训练量化（PTQ）已经成为减少大型语言模型（LLMs）成本的一种有前景的技术，具体地，PTQ可以有效地减轻LLMs中的内存消耗并降低计算开销。为了满足各种场景下高效率和性能的要求，对量化LLMs进行全面评估是必要的，以指导量化方法的选择。本文通过评估PTQ对11个模型系列（包括OPT、LLaMA2、Falcon、Bloomz、Mistral、ChatGLM、Vicuna、LongChat、StableLM、Gemma和Mamba）的权重、激活和KV缓存的影响，范围从125M到180B，全面评估了这些因素。评估涵盖了五种类型的任务：基础NLP、突然出现的能力、可靠性、对话和长上下文任务。此外，我们还评估了最先进的量化方法，以展示它们的应用。",
    "tldr": "该论文通过全面评估后训练量化对权重、激活和KV缓存的影响，以指导选择量化方法，并对11种模型系列进行了评估，覆盖了多种任务类型。",
    "en_tdlr": "This paper provides a comprehensive evaluation of the impact of post-training quantization on weight, activation, and KV cache to guide the selection of quantization methods, and evaluates 11 model families across various tasks."
}