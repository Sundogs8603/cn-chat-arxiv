{
    "title": "\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors",
    "abstract": "Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is avail",
    "link": "https://arxiv.org/abs/2402.04210",
    "context": "Title: \"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors\nAbstract: Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is avail",
    "path": "papers/24/02/2402.04210.json",
    "total_tokens": 911,
    "translated_title": "“任务成功”远远不够：探究将视频-语言模型作为行为批评者以捕捉不良代理行为的使用",
    "translated_abstract": "大规模生成模型被证明对于抽样有意义的候选解决方案很有用，然而它们经常忽视任务约束和用户偏好。当模型与外部验证者结合，并根据验证反馈逐步或逐渐得出最终解决方案时，它们的完全能力更好地被利用。在具身化人工智能的背景下，验证通常仅涉及评估指令中指定的目标条件是否已满足。然而，为了将这些代理者无缝地融入日常生活，必须考虑到更广泛的约束和偏好，超越仅任务成功（例如，机器人应该谨慎地抓住面包，以避免明显的变形）。然而，鉴于机器人任务的无限范围，构建类似于用于显式知识任务（如围棋和定理证明）的脚本化验证器是不可行的。这引出了一个问题：当没有可靠的验证者可用时，何时可以信任代理的行为？",
    "tldr": "本文探究了将视频-语言模型作为行为批评者用于捕捉不良代理行为的可行性，以解决大规模生成模型忽视任务约束和用户偏好的问题。",
    "en_tdlr": "This paper investigates the feasibility of using video-language models as behavior critics to catch undesirable agent behaviors, aiming to address the issue of task constraints and user preferences being overlooked by large-scale generative models."
}