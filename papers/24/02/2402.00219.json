{
    "title": "FedCore: Straggler-Free Federated Learning with Distributed Coresets",
    "abstract": "Federated learning (FL) is a machine learning paradigm that allows multiple clients to collaboratively train a shared model while keeping their data on-premise. However, the straggler issue, due to slow clients, often hinders the efficiency and scalability of FL. This paper presents FedCore, an algorithm that innovatively tackles the straggler problem via the decentralized selection of coresets, representative subsets of a dataset. Contrary to existing centralized coreset methods, FedCore creates coresets directly on each client in a distributed manner, ensuring privacy preservation in FL. FedCore translates the coreset optimization problem into a more tractable k-medoids clustering problem and operates distributedly on each client. Theoretical analysis confirms FedCore's convergence, and practical evaluations demonstrate an 8x reduction in FL training time, without compromising model accuracy. Our extensive evaluations also show that FedCore generalizes well to existing FL frameworks.",
    "link": "https://arxiv.org/abs/2402.00219",
    "context": "Title: FedCore: Straggler-Free Federated Learning with Distributed Coresets\nAbstract: Federated learning (FL) is a machine learning paradigm that allows multiple clients to collaboratively train a shared model while keeping their data on-premise. However, the straggler issue, due to slow clients, often hinders the efficiency and scalability of FL. This paper presents FedCore, an algorithm that innovatively tackles the straggler problem via the decentralized selection of coresets, representative subsets of a dataset. Contrary to existing centralized coreset methods, FedCore creates coresets directly on each client in a distributed manner, ensuring privacy preservation in FL. FedCore translates the coreset optimization problem into a more tractable k-medoids clustering problem and operates distributedly on each client. Theoretical analysis confirms FedCore's convergence, and practical evaluations demonstrate an 8x reduction in FL training time, without compromising model accuracy. Our extensive evaluations also show that FedCore generalizes well to existing FL frameworks.",
    "path": "papers/24/02/2402.00219.json",
    "total_tokens": 847,
    "translated_title": "FedCore: 使用分布式核心集解决无拖车现象的联邦学习",
    "translated_abstract": "联邦学习（FL）是一种机器学习范例，允许多个客户端在保留自己的数据的前提下，共同训练一个共享模型。然而，由于慢速客户端，拖车现象经常影响FL的效率和可扩展性。本文提出了FedCore，一种通过分布式选择核心集（数据集的代表子集）创新地解决拖车问题的算法。与现有的集中式核心集方法不同，FedCore以分布式方式直接在每个客户端上创建核心集，确保在FL中保护隐私。FedCore将核心集优化问题转化为更易处理的k-medoids聚类问题，并在每个客户端上进行分布式操作。理论分析证实了FedCore的收敛性，实际评估显示FL训练时间减少了8倍，而模型准确性没有降低。我们广泛的评估还表明，FedCore对现有的FL框架具有很好的泛化性。",
    "tldr": "FedCore是一种通过分布式选择核心集解决联邦学习中慢速客户端问题的算法，可以显著减少训练时间，并保护隐私。"
}