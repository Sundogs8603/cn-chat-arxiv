{
    "title": "MLTCP: Congestion Control for DNN Training",
    "abstract": "arXiv:2402.09589v1 Announce Type: cross  Abstract: We present MLTCP, a technique to augment today's congestion control algorithms to accelerate DNN training jobs in shared GPU clusters. MLTCP enables the communication phases of jobs that compete for network bandwidth to interleave with each other, thereby utilizing the network efficiently. At the heart of MLTCP lies a very simple principle based on a key conceptual insight: DNN training flows should scale their congestion window size based on the number of bytes sent at each training iteration. We show that integrating this principle into today's congestion control protocols is straightforward: by adding 30-60 lines of code to Reno, CUBIC, or DCQCN, MLTCP stabilizes flows of different jobs into an interleaved state within a few training iterations, regardless of the number of competing flows or the start time of each flow. Our experiments with popular DNN training jobs demonstrate that enabling MLTCP accelerates the average and 99th pe",
    "link": "https://arxiv.org/abs/2402.09589",
    "context": "Title: MLTCP: Congestion Control for DNN Training\nAbstract: arXiv:2402.09589v1 Announce Type: cross  Abstract: We present MLTCP, a technique to augment today's congestion control algorithms to accelerate DNN training jobs in shared GPU clusters. MLTCP enables the communication phases of jobs that compete for network bandwidth to interleave with each other, thereby utilizing the network efficiently. At the heart of MLTCP lies a very simple principle based on a key conceptual insight: DNN training flows should scale their congestion window size based on the number of bytes sent at each training iteration. We show that integrating this principle into today's congestion control protocols is straightforward: by adding 30-60 lines of code to Reno, CUBIC, or DCQCN, MLTCP stabilizes flows of different jobs into an interleaved state within a few training iterations, regardless of the number of competing flows or the start time of each flow. Our experiments with popular DNN training jobs demonstrate that enabling MLTCP accelerates the average and 99th pe",
    "path": "papers/24/02/2402.09589.json",
    "total_tokens": 911,
    "translated_title": "MLTCP:用于DNN训练的拥塞控制技术",
    "translated_abstract": "我们提出了MLTCP，一种技术来增强当前的拥塞控制算法，以加速在共享GPU集群中进行的DNN训练作业。MLTCP使竞争网络带宽的作业的通信阶段相互交错，从而高效利用网络。MLTCP的核心是一个基于关键概念洞察的非常简单的原则：DNN训练流应该根据每个训练迭代发送的字节数来缩放其拥塞窗口大小。我们展示了将这个原则整合到当前的拥塞控制协议中是直接的：通过在Reno、CUBIC或DCQCN中添加30-60行代码，MLTCP可以在几个训练迭代内将不同作业的流稳定地转化为交错状态，无论竞争流的数量或每个流的开始时间如何。我们对流行的DNN训练作业进行的实验表明，启用MLTCP可以加快平均和99th pe的结束时间",
    "tldr": "MLTCP是一种用于加速共享GPU集群中的DNN训练作业的拥塞控制技术，通过在每个训练迭代发送的字节数进行缩放，使不同作业的流能够高效利用网络极大地加快训练作业的完成时间。",
    "en_tdlr": "MLTCP is a congestion control technique for accelerating DNN training jobs in shared GPU clusters. It enables flows of different jobs to efficiently utilize the network by scaling their congestion window size based on the number of bytes sent at each training iteration. This significantly reduces the completion time of the training jobs."
}