{
    "title": "Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees",
    "abstract": "Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty. We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field. In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric. To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D LASA datasets.",
    "link": "https://arxiv.org/abs/2402.08090",
    "context": "Title: Learning Neural Contracting Dynamics: Extended Linearization and Global Guarantees\nAbstract: Global stability and robustness guarantees in learned dynamical systems are essential to ensure well-behavedness of the systems in the face of uncertainty. We present Extended Linearized Contracting Dynamics (ELCD), the first neural network-based dynamical system with global contractivity guarantees in arbitrary metrics. The key feature of ELCD is a parametrization of the extended linearization of the nonlinear vector field. In its most basic form, ELCD is guaranteed to be (i) globally exponentially stable, (ii) equilibrium contracting, and (iii) globally contracting with respect to some metric. To allow for contraction with respect to more general metrics in the data space, we train diffeomorphisms between the data space and a latent space and enforce contractivity in the latent space, which ensures global contractivity in the data space. We demonstrate the performance of ELCD on the $2$D, $4$D, and $8$D LASA datasets.",
    "path": "papers/24/02/2402.08090.json",
    "total_tokens": 939,
    "translated_title": "学习神经收缩动力学：扩展线性化和全局保证",
    "translated_abstract": "在学习的动态系统中，全局稳定性和鲁棒性保证对于确保系统在面对不确定性时的良好行为至关重要。我们提出了扩展线性化收缩动力学（ELCD），该系统是第一个具有任意度量下全局收缩性保证的基于神经网络的动力系统。ELCD的关键特性是非线性向量场扩展线性化的参数化。在其最基本形式下，ELCD保证全局指数稳定、平衡收缩以及在某些度量下全局收缩。为了实现在数据空间中相对于更一般度量的收缩，我们训练数据空间和潜在空间之间的微分同胚，并在潜在空间中强制收缩性，从而确保数据空间的全局收缩性。我们在2D、4D和8D的LASA数据集上展示了ELCD的性能。",
    "tldr": "本论文提出了扩展线性化收缩动力学（ELCD），是第一个具有全局收缩性保证的神经网络动力系统，通过参数化非线性向量场的扩展线性化实现。通过在数据空间和潜在空间之间训练微分同胚，并在潜在空间中强制收缩性，ELCD能在面对不确定性时保持全局稳定性和鲁棒性。"
}