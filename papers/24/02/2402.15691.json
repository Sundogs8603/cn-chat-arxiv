{
    "title": "Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles",
    "abstract": "arXiv:2402.15691v1 Announce Type: new  Abstract: Gradient boosting of prediction rules is an efficient approach to learn potentially interpretable yet accurate probabilistic models. However, actual interpretability requires to limit the number and size of the generated rules, and existing boosting variants are not designed for this purpose. Though corrective boosting refits all rule weights in each iteration to minimise prediction risk, the included rule conditions tend to be sub-optimal, because commonly used objective functions fail to anticipate this refitting. Here, we address this issue by a new objective function that measures the angle between the risk gradient vector and the projection of the condition output vector onto the orthogonal complement of the already selected conditions. This approach correctly approximate the ideal update of adding the risk gradient itself to the model and favours the inclusion of more general and thus shorter rules. As we demonstrate using a wide r",
    "link": "https://arxiv.org/abs/2402.15691",
    "context": "Title: Orthogonal Gradient Boosting for Simpler Additive Rule Ensembles\nAbstract: arXiv:2402.15691v1 Announce Type: new  Abstract: Gradient boosting of prediction rules is an efficient approach to learn potentially interpretable yet accurate probabilistic models. However, actual interpretability requires to limit the number and size of the generated rules, and existing boosting variants are not designed for this purpose. Though corrective boosting refits all rule weights in each iteration to minimise prediction risk, the included rule conditions tend to be sub-optimal, because commonly used objective functions fail to anticipate this refitting. Here, we address this issue by a new objective function that measures the angle between the risk gradient vector and the projection of the condition output vector onto the orthogonal complement of the already selected conditions. This approach correctly approximate the ideal update of adding the risk gradient itself to the model and favours the inclusion of more general and thus shorter rules. As we demonstrate using a wide r",
    "path": "papers/24/02/2402.15691.json",
    "total_tokens": 721,
    "translated_title": "正交梯度提升用于简化的加法规则集合",
    "translated_abstract": "预测规则的梯度提升是一种学习潜在可解释且准确的概率模型的高效方法。然而，实际的可解释性需要限制生成的规则数量和大小，现有的提升变体并非为此目的而设计。本文通过一个新的目标函数来解决这个问题，该目标函数衡量了风险梯度向量与条件输出向量在已选择条件的正交补上的投影的夹角，从而正确逼近将风险梯度本身添加到模型的理想更新，并倾向于包括更一般且更短的规则。",
    "tldr": "提出了一种正交梯度提升方法，通过新的目标函数促进生成更加简化的加法规则集合，提高了模型的解释性和准确性。",
    "en_tdlr": "Introduced an orthogonal gradient boosting method that promotes the generation of simplified additive rule ensembles through a new objective function, enhancing the interpretability and accuracy of the model."
}