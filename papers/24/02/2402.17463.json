{
    "title": "Training-Free Long-Context Scaling of Large Language Models",
    "abstract": "arXiv:2402.17463v1 Announce Type: new  Abstract: The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attai",
    "link": "https://arxiv.org/abs/2402.17463",
    "context": "Title: Training-Free Long-Context Scaling of Large Language Models\nAbstract: arXiv:2402.17463v1 Announce Type: new  Abstract: The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attai",
    "path": "papers/24/02/2402.17463.json",
    "total_tokens": 878,
    "translated_title": "无须训练的大语言模型长上下文扩展",
    "translated_abstract": "大语言模型（LLMs）在处理和生成连贯文本时，当输入令牌数量超过它们的预训练长度时，其能力会明显减弱。鉴于使用更长序列进行大规模模型微调的昂贵开销，我们提出了Dual Chunk Attention（DCA），它使Llama2 70B能够支持超过100k令牌的上下文窗口，而无需持续训练。通过将长序列的注意力计算分解为基于块的模块，DCA成功捕获了相同块内（Intra-Chunk）和不同块之间（Inter-Chunk）令牌的相对位置信息，并能与Flash Attention无缝集成。除了其惊人的外推能力外，DCA在实际长上下文任务上实现了与或甚至优于微调模型相当的性能。与专有模型相比，我们的无须训练的70B模型取得了",
    "tldr": "提出了一种名为Dual Chunk Attention (DCA)的方法，可以使Llama2 70B在不需要持续训练的情况下支持超过100k令牌的上下文窗口，能够在长上下文任务中取得与微调模型相媲美甚至更好的性能。",
    "en_tdlr": "Proposed Dual Chunk Attention (DCA) method allows Llama2 70B to support context windows of more than 100k tokens without continual training, achieving performance on practical long-context tasks comparable to or even better than that of finetuned models."
}