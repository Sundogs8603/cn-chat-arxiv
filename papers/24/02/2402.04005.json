{
    "title": "Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning",
    "abstract": "As machine learning becomes more prominent there is a growing demand to perform several inference tasks in parallel. Running a dedicated model for each task is computationally expensive and therefore there is a great interest in multi-task learning (MTL). MTL aims at learning a single model that solves several tasks efficiently. Optimizing MTL models is often achieved by computing a single gradient per task and aggregating them for obtaining a combined update direction. However, these approaches do not consider an important aspect, the sensitivity in the gradient dimensions. Here, we introduce a novel gradient aggregation approach using Bayesian inference. We place a probability distribution over the task-specific parameters, which in turn induce a distribution over the gradients of the tasks. This additional valuable information allows us to quantify the uncertainty in each of the gradients dimensions, which can then be factored in when aggregating them. We empirically demonstrate the",
    "link": "https://arxiv.org/abs/2402.04005",
    "context": "Title: Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning\nAbstract: As machine learning becomes more prominent there is a growing demand to perform several inference tasks in parallel. Running a dedicated model for each task is computationally expensive and therefore there is a great interest in multi-task learning (MTL). MTL aims at learning a single model that solves several tasks efficiently. Optimizing MTL models is often achieved by computing a single gradient per task and aggregating them for obtaining a combined update direction. However, these approaches do not consider an important aspect, the sensitivity in the gradient dimensions. Here, we introduce a novel gradient aggregation approach using Bayesian inference. We place a probability distribution over the task-specific parameters, which in turn induce a distribution over the gradients of the tasks. This additional valuable information allows us to quantify the uncertainty in each of the gradients dimensions, which can then be factored in when aggregating them. We empirically demonstrate the",
    "path": "papers/24/02/2402.04005.json",
    "total_tokens": 849,
    "translated_title": "贝叶斯不确定性用于多任务学习中的梯度聚合",
    "translated_abstract": "随着机器学习的日益突出，需要同时执行多个推理任务的需求也在增长。为每个任务运行专用模型在计算上十分昂贵，因此对多任务学习（MTL）的兴趣也越来越大。MTL的目标是学习一个能高效解决多个任务的单个模型。通过计算每个任务的单一梯度并将它们聚合起来以获得结合的更新方向来优化MTL模型。然而，这些方法并没有考虑到一个重要的方面，即梯度维度的敏感性。在这里，我们引入了一种新颖的利用贝叶斯推断的梯度聚合方法。我们为任务特定参数放置一个概率分布，这又引起了任务梯度的分布。这些额外的有价值的信息使我们能够量化每个梯度维度中的不确定性，从而在聚合它们时将其纳入考虑。我们通过实证方法证明了我们的方法的效果。",
    "tldr": "这篇论文提出了一种利用贝叶斯推断的梯度聚合方法，通过引入概率分布来量化梯度维度的不确定性，在多任务学习中获得更好的效果。"
}