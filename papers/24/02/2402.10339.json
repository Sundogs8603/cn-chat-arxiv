{
    "title": "What to Do When Your Discrete Optimization Is the Size of a Neural Network?",
    "abstract": "arXiv:2402.10339v1 Announce Type: new  Abstract: Oftentimes, machine learning applications using neural networks involve solving discrete optimization problems, such as in pruning, parameter-isolation-based continual learning and training of binary networks. Still, these discrete problems are combinatorial in nature and are also not amenable to gradient-based optimization. Additionally, classical approaches used in discrete settings do not scale well to large neural networks, forcing scientists and empiricists to rely on alternative methods. Among these, two main distinct sources of top-down information can be used to lead the model to good solutions: (1) extrapolating gradient information from points outside of the solution set (2) comparing evaluations between members of a subset of the valid solutions. We take continuation path (CP) methods to represent using purely the former and Monte Carlo (MC) methods to represent the latter, while also noting that some hybrid methods combine th",
    "link": "https://arxiv.org/abs/2402.10339",
    "context": "Title: What to Do When Your Discrete Optimization Is the Size of a Neural Network?\nAbstract: arXiv:2402.10339v1 Announce Type: new  Abstract: Oftentimes, machine learning applications using neural networks involve solving discrete optimization problems, such as in pruning, parameter-isolation-based continual learning and training of binary networks. Still, these discrete problems are combinatorial in nature and are also not amenable to gradient-based optimization. Additionally, classical approaches used in discrete settings do not scale well to large neural networks, forcing scientists and empiricists to rely on alternative methods. Among these, two main distinct sources of top-down information can be used to lead the model to good solutions: (1) extrapolating gradient information from points outside of the solution set (2) comparing evaluations between members of a subset of the valid solutions. We take continuation path (CP) methods to represent using purely the former and Monte Carlo (MC) methods to represent the latter, while also noting that some hybrid methods combine th",
    "path": "papers/24/02/2402.10339.json",
    "total_tokens": 854,
    "translated_title": "当你的离散优化问题的规模等同于一个神经网络时该怎么办？",
    "translated_abstract": "在许多机器学习应用中，使用神经网络涉及解决离散优化问题，比如在剪枝、基于参数隔离的持续学习和二进制网络训练中。然而，这些离散问题具有组合特性，不适合使用基于梯度的优化方法。此外，在离散设置中经典方法不适用于大型神经网络，迫使科学家和实证者依赖于替代方法。在这些方法中，有两个主要不同的顶级信息来源可以用来引导模型达到良好的解决方案：（1）从解决方案集之外的点外推梯度信息（2）在一组有效解决方案的成员之间比较评估。我们采用继续路径（CP）方法代表纯粹使用前者，蒙特卡罗（MC）方法代表后者，同时也指出一些混合方法结合了这两者。",
    "tldr": "论文讨论了在大型神经网络中处理离散优化问题的方法，提出了使用继续路径方法和蒙特卡罗方法来引导模型达到良好解决方案的两种不同途径",
    "en_tdlr": "The paper discusses approaches for handling discrete optimization problems in large neural networks, proposing the use of continuation path and Monte Carlo methods as two distinct ways to guide the model towards good solutions."
}