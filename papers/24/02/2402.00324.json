{
    "title": "A Consistent Lebesgue Measure for Multi-label Learning",
    "abstract": "Multi-label loss functions are usually non-differentiable, requiring surrogate loss functions for gradient-based optimisation. The consistency of surrogate loss functions is not proven and is exacerbated by the conflicting nature of multi-label loss functions. To directly learn from multiple related, yet potentially conflicting multi-label loss functions, we propose a Consistent Lebesgue Measure-based Multi-label Learner (CLML) and prove that CLML can achieve theoretical consistency under a Bayes risk framework. Empirical evidence supports our theory by demonstrating that: (1) CLML can consistently achieve state-of-the-art results; (2) the primary performance factor is the Lebesgue measure design, as CLML optimises a simpler feedforward model without additional label graph, perturbation-based conditioning, or semantic embeddings; and (3) an analysis of the results not only distinguishes CLML's effectiveness but also highlights inconsistencies between the surrogate and the desired loss ",
    "link": "https://arxiv.org/abs/2402.00324",
    "context": "Title: A Consistent Lebesgue Measure for Multi-label Learning\nAbstract: Multi-label loss functions are usually non-differentiable, requiring surrogate loss functions for gradient-based optimisation. The consistency of surrogate loss functions is not proven and is exacerbated by the conflicting nature of multi-label loss functions. To directly learn from multiple related, yet potentially conflicting multi-label loss functions, we propose a Consistent Lebesgue Measure-based Multi-label Learner (CLML) and prove that CLML can achieve theoretical consistency under a Bayes risk framework. Empirical evidence supports our theory by demonstrating that: (1) CLML can consistently achieve state-of-the-art results; (2) the primary performance factor is the Lebesgue measure design, as CLML optimises a simpler feedforward model without additional label graph, perturbation-based conditioning, or semantic embeddings; and (3) an analysis of the results not only distinguishes CLML's effectiveness but also highlights inconsistencies between the surrogate and the desired loss ",
    "path": "papers/24/02/2402.00324.json",
    "total_tokens": 910,
    "translated_title": "多标签学习的一致勒贝格测度",
    "translated_abstract": "多标签损失函数通常是非可微的，需要替代损失函数进行梯度优化。 替代损失函数的一致性尚未证明，并且由于多标签损失函数的冲突性质而变得更加严重。 为了直接从多个相关但潜在冲突的多标签损失函数中学习，我们提出了一种基于一致勒贝格测度的多标签学习器（CLML），并证明了在贝叶斯风险框架下CLML可以实现理论上的一致性。 经验证据支持我们的理论，通过展示：（1）CLML可以一贯地取得最先进的结果；（2）主要的性能因素是勒贝格测度设计，因为CLML优化了一个更简单的前馈模型，没有额外的标签图、基于扰动的条件或语义嵌入；以及（3）结果的分析不仅可以区分CLML的有效性，还凸显了替代损失函数与期望损失函数之间的不一致性。",
    "tldr": "提出了一种基于一致勒贝格测度的多标签学习器(CLML)，通过理论证明该方法可以实现一致性。实验证据表明，CLML可以一致地取得最先进的结果，其主要性能因素是勒贝格测度设计。",
    "en_tdlr": "A Consistent Lebesgue Measure-based Multi-label Learner (CLML) is proposed for directly learning from multiple related, yet potentially conflicting multi-label loss functions. The method achieves theoretical consistency and consistently achieves state-of-the-art results, with the Lebesgue measure design being the key performance factor."
}