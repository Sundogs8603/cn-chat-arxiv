{
    "title": "The Virtues of Pessimism in Inverse Reinforcement Learning",
    "abstract": "Inverse Reinforcement Learning (IRL) is a powerful framework for learning complex behaviors from expert demonstrations. However, it traditionally requires repeatedly solving a computationally expensive reinforcement learning (RL) problem in its inner loop. It is desirable to reduce the exploration burden by leveraging expert demonstrations in the inner-loop RL. As an example, recent work resets the learner to expert states in order to inform the learner of high-reward expert states. However, such an approach is infeasible in the real world. In this work, we consider an alternative approach to speeding up the RL subroutine in IRL: \\emph{pessimism}, i.e., staying close to the expert's data distribution, instantiated via the use of offline RL algorithms. We formalize a connection between offline RL and IRL, enabling us to use an arbitrary offline RL algorithm to improve the sample efficiency of IRL. We validate our theory experimentally by demonstrating a strong correlation between the ef",
    "link": "https://arxiv.org/abs/2402.02616",
    "context": "Title: The Virtues of Pessimism in Inverse Reinforcement Learning\nAbstract: Inverse Reinforcement Learning (IRL) is a powerful framework for learning complex behaviors from expert demonstrations. However, it traditionally requires repeatedly solving a computationally expensive reinforcement learning (RL) problem in its inner loop. It is desirable to reduce the exploration burden by leveraging expert demonstrations in the inner-loop RL. As an example, recent work resets the learner to expert states in order to inform the learner of high-reward expert states. However, such an approach is infeasible in the real world. In this work, we consider an alternative approach to speeding up the RL subroutine in IRL: \\emph{pessimism}, i.e., staying close to the expert's data distribution, instantiated via the use of offline RL algorithms. We formalize a connection between offline RL and IRL, enabling us to use an arbitrary offline RL algorithm to improve the sample efficiency of IRL. We validate our theory experimentally by demonstrating a strong correlation between the ef",
    "path": "papers/24/02/2402.02616.json",
    "total_tokens": 899,
    "translated_title": "逆推强化学习中悲观主义的优势",
    "translated_abstract": "逆推强化学习（IRL）是从专家演示中学习复杂行为的强大框架。然而，传统上需要在内循环中反复解决计算成本昂贵的强化学习（RL）问题。通过利用专家演示来减少探索负担在内循环的RL中非常可取。例如，最近的工作通过将学习者重置到专家状态来指导学习者在高回报专家状态下工作。然而，这样的方法在真实世界中不可行。在这项工作中，我们考虑了一种加速IRL中RL子程序的替代方法：悲观主义，即保持与专家数据分布接近，通过离线RL算法来实现。我们在离线RL和IRL之间形成了一个连接，使我们能够使用任意离线RL算法来提高IRL的样本效率。我们通过实验证明了我们的理论，展示了样本效率和探索负担之间的强相关性。",
    "tldr": "本论文提出了一种使用离线强化学习算法来加速逆推强化学习中强化学习子程序的替代方法，通过保持与专家数据分布接近的悲观主义策略，提高了逆推强化学习的样本效率。"
}