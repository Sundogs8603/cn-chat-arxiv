{
    "title": "Corridor Geometry in Gradient-Based Optimization",
    "abstract": "arXiv:2402.08818v1 Announce Type: cross Abstract: We characterize regions of a loss surface as corridors when the continuous curves of steepest descent -- the solutions of the gradient flow -- become straight lines. We show that corridors provide insights into gradient-based optimization, since corridors are exactly the regions where gradient descent and the gradient flow follow the same trajectory, while the loss decreases linearly. As a result, inside corridors there are no implicit regularization effects or training instabilities that have been shown to occur due to the drift between gradient descent and the gradient flow. Using the loss linear decrease on corridors, we devise a learning rate adaptation scheme for gradient descent; we call this scheme Corridor Learning Rate (CLR). The CLR formulation coincides with a special case of Polyak step-size, discovered in the context of convex optimization. The Polyak step-size has been shown recently to have also good convergence propertie",
    "link": "https://arxiv.org/abs/2402.08818",
    "context": "Title: Corridor Geometry in Gradient-Based Optimization\nAbstract: arXiv:2402.08818v1 Announce Type: cross Abstract: We characterize regions of a loss surface as corridors when the continuous curves of steepest descent -- the solutions of the gradient flow -- become straight lines. We show that corridors provide insights into gradient-based optimization, since corridors are exactly the regions where gradient descent and the gradient flow follow the same trajectory, while the loss decreases linearly. As a result, inside corridors there are no implicit regularization effects or training instabilities that have been shown to occur due to the drift between gradient descent and the gradient flow. Using the loss linear decrease on corridors, we devise a learning rate adaptation scheme for gradient descent; we call this scheme Corridor Learning Rate (CLR). The CLR formulation coincides with a special case of Polyak step-size, discovered in the context of convex optimization. The Polyak step-size has been shown recently to have also good convergence propertie",
    "path": "papers/24/02/2402.08818.json",
    "total_tokens": 899,
    "translated_title": "基于梯度优化中的走廊几何",
    "translated_abstract": "本文通过将最陡下降的连续曲线，即梯度流的解，变成直线，将损失曲面的区域划分为走廊。我们表明走廊能够提供关于梯度下降优化的洞见，因为走廊正是梯度下降和梯度流遵循相同轨迹且损失线性下降的区域。因此，在走廊内部，不存在因梯度下降和梯度流之间的漂移而导致的隐式正则化效应或训练不稳定性。基于走廊上损失的线性下降，我们设计了一种适用于梯度下降的学习率自适应策略，我们称之为走廊学习率(CLR)。CLR的形式与凸优化上最近发现的Polyak步长特例一致。Polyak步长近期已被证明具有良好的收敛性质。",
    "tldr": "本文研究了基于梯度优化中的走廊几何，发现走廊可以提供有关梯度下降优化的洞见，并提出了一种适用于梯度下降的学习率自适应策略CLR，该策略与凸优化中的Polyak步长特例一致。",
    "en_tdlr": "This paper investigates corridor geometry in gradient-based optimization, and finds that corridors provide insights into gradient descent optimization. The paper proposes a learning rate adaptation scheme called Corridor Learning Rate (CLR), which coincides with a special case of Polyak step-size in convex optimization."
}