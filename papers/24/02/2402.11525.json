{
    "title": "Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution",
    "abstract": "arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \\textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \\textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions ",
    "link": "https://arxiv.org/abs/2402.11525",
    "context": "Title: Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution\nAbstract: arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \\textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \\textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions ",
    "path": "papers/24/02/2402.11525.json",
    "total_tokens": 898,
    "translated_title": "用RLHF推进翻译偏好建模：迈向成本效益解决方案",
    "translated_abstract": "arXiv:2402.11525v1 公告类型：新 真实性、表达力和优雅是机器翻译中不断追求的目标。然而，传统的度量标准如BLEU并不严格符合人类对翻译质量的偏好。本文探讨了利用强化学习与人类反馈（RLHF）来提高翻译质量。收集人类对翻译之间的比较的大规模高质量数据集并不容易，尤其对于低资源语言。为解决这一问题，我们提出了一种成本效益的偏好学习策略，通过区分人类和机器翻译来优化奖励模型。通过这种方式，奖励模型学习机器翻译与人类之间的不足之处，并指导随后对机器翻译的改进。实验结果表明，RLHF可以有效地提升翻译质量，这种改进也有益于其他翻译方向。",
    "tldr": "提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。",
    "en_tdlr": "Proposed a cost-effective preference learning strategy using reinforcement learning with human feedback (RLHF) to improve translation quality, optimizing reward models by distinguishing between human and machine translations to guide improvements in machine translation."
}