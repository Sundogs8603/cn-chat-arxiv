{
    "title": "CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models",
    "abstract": "arXiv:2402.15021v1 Announce Type: cross  Abstract: Recent years have witnessed a significant increase in the performance of Vision and Language tasks. Foundational Vision-Language Models (VLMs), such as CLIP, have been leveraged in multiple settings and demonstrated remarkable performance across several tasks. Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways. However, no evidence exists that any VLM, including large-scale single-stream models such as GPT-4V, identifies compositions successfully. In this paper, we introduce a framework to significantly improve the ability of existing models to encode compositional language, with over 10% absolute improvement on compositionality benchmarks, while maintaining or improving the performance on standard object-recognition and retrieval benchmarks. Our code and pre-trained models are publicly available at https://github.com/netflix/",
    "link": "https://arxiv.org/abs/2402.15021",
    "context": "Title: CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models\nAbstract: arXiv:2402.15021v1 Announce Type: cross  Abstract: Recent years have witnessed a significant increase in the performance of Vision and Language tasks. Foundational Vision-Language Models (VLMs), such as CLIP, have been leveraged in multiple settings and demonstrated remarkable performance across several tasks. Such models excel at object-centric recognition yet learn text representations that seem invariant to word order, failing to compose known concepts in novel ways. However, no evidence exists that any VLM, including large-scale single-stream models such as GPT-4V, identifies compositions successfully. In this paper, we introduce a framework to significantly improve the ability of existing models to encode compositional language, with over 10% absolute improvement on compositionality benchmarks, while maintaining or improving the performance on standard object-recognition and retrieval benchmarks. Our code and pre-trained models are publicly available at https://github.com/netflix/",
    "path": "papers/24/02/2402.15021.json",
    "total_tokens": 848,
    "translated_title": "CLoVe: 在对比视觉-语言模型中编码组合性语言",
    "translated_abstract": "近年来，视觉和语言任务的表现显著提升。基础视觉-语言模型（VLMs）如CLIP已在多个设置中得到应用，并展示了在几个任务上的显著性能。这些模型擅长于对象中心识别，但学习的文本表示似乎对词序不变，未能以新颖方式组成已知概念。然而，没有证据表明任何VLM，包括大规模单流模型如GPT-4V，成功识别组合。本文介绍了一个框架，显著提高了现有模型编码组合性语言的能力，在组合性基准上取得了超过10% 的绝对改进，同时在标准对象识别和检索基准上保持或提高了性能。我们的代码和预训练模型可在https://github.com/netflix/处公开获取。",
    "tldr": "本文提出了一个框架，显著提高现有模型编码组合性语言的能力，在组合性基准上取得超过10% 的绝对改进，同时保持或提高了在标准对象识别和检索基准上的性能。"
}