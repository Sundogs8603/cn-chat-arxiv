{
    "title": "KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark",
    "abstract": "arXiv:2402.17377v1 Announce Type: new  Abstract: As language models are often deployed as chatbot assistants, it becomes a virtue for models to engage in conversations in a user's first language. While these models are trained on a wide range of languages, a comprehensive evaluation of their proficiency in low-resource languages such as Korean has been lacking. In this work, we introduce KoDialogBench, a benchmark designed to assess language models' conversational capabilities in Korean. To this end, we collect native Korean dialogues on daily topics from public sources, or translate dialogues from other languages. We then structure these conversations into diverse test datasets, spanning from dialogue comprehension to response selection tasks. Leveraging the proposed benchmark, we conduct extensive evaluations and analyses of various language models to measure a foundational understanding of Korean dialogues. Experimental results indicate that there exists significant room for improve",
    "link": "https://arxiv.org/abs/2402.17377",
    "context": "Title: KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark\nAbstract: arXiv:2402.17377v1 Announce Type: new  Abstract: As language models are often deployed as chatbot assistants, it becomes a virtue for models to engage in conversations in a user's first language. While these models are trained on a wide range of languages, a comprehensive evaluation of their proficiency in low-resource languages such as Korean has been lacking. In this work, we introduce KoDialogBench, a benchmark designed to assess language models' conversational capabilities in Korean. To this end, we collect native Korean dialogues on daily topics from public sources, or translate dialogues from other languages. We then structure these conversations into diverse test datasets, spanning from dialogue comprehension to response selection tasks. Leveraging the proposed benchmark, we conduct extensive evaluations and analyses of various language models to measure a foundational understanding of Korean dialogues. Experimental results indicate that there exists significant room for improve",
    "path": "papers/24/02/2402.17377.json",
    "total_tokens": 834,
    "translated_title": "KoDialogBench: 用韩语对话基准评估语言模型的会话理解能力",
    "translated_abstract": "由于语言模型通常被部署为聊天机器人助手，模型在用户的母语中进行对话变得至关重要。尽管这些模型在训练时涵盖了多种语言，但对它们在韩语等低资源语言的熟练程度缺乏全面评估。在这项工作中，我们引入了KoDialogBench，该基准旨在评估语言模型在韩语对话中的会话能力。为此，我们从公共来源收集日常话题的韩语对话，或将其他语言的对话进行翻译。然后，我们将这些对话结构化为不同的测试数据集，涵盖了从对话理解到响应选择任务的广泛范围。利用提出的基准，我们进行了各种语言模型的广泛评估和分析，以衡量对韩语对话的基础理解。实验结果表明，存在明显的改进空间。",
    "tldr": "介绍了一个名为KoDialogBench的基准，用于评估语言模型在韩语对话中的会话能力，实验结果表明存在改进空间。"
}