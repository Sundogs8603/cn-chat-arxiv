{
    "title": "Theoretical Understanding of Learning from Adversarial Perturbations",
    "abstract": "arXiv:2402.10470v1 Announce Type: new  Abstract: It is not fully understood why adversarial examples can deceive neural networks and transfer between different networks. To elucidate this, several studies have hypothesized that adversarial perturbations, while appearing as noises, contain class features. This is supported by empirical evidence showing that networks trained on mislabeled adversarial examples can still generalize well to correctly labeled test samples. However, a theoretical understanding of how perturbations include class features and contribute to generalization is limited. In this study, we provide a theoretical framework for understanding learning from perturbations using a one-hidden-layer network trained on mutually orthogonal samples. Our results highlight that various adversarial perturbations, even perturbations of a few pixels, contain sufficient class features for generalization. Moreover, we reveal that the decision boundary when learning from perturbations m",
    "link": "https://arxiv.org/abs/2402.10470",
    "context": "Title: Theoretical Understanding of Learning from Adversarial Perturbations\nAbstract: arXiv:2402.10470v1 Announce Type: new  Abstract: It is not fully understood why adversarial examples can deceive neural networks and transfer between different networks. To elucidate this, several studies have hypothesized that adversarial perturbations, while appearing as noises, contain class features. This is supported by empirical evidence showing that networks trained on mislabeled adversarial examples can still generalize well to correctly labeled test samples. However, a theoretical understanding of how perturbations include class features and contribute to generalization is limited. In this study, we provide a theoretical framework for understanding learning from perturbations using a one-hidden-layer network trained on mutually orthogonal samples. Our results highlight that various adversarial perturbations, even perturbations of a few pixels, contain sufficient class features for generalization. Moreover, we reveal that the decision boundary when learning from perturbations m",
    "path": "papers/24/02/2402.10470.json",
    "total_tokens": 880,
    "translated_title": "对从对抗性扰动中学习的理论理解",
    "translated_abstract": "尚未完全理解为什么对抗性示例可以欺骗神经网络并在不同网络之间传递。为了阐明这一点，几项研究假设，尽管对抗性扰动看似是噪音，但实际上包含类特征。这得到了通过实证证据支持，即对于在错误标记的对抗性示例上训练的网络仍然可以很好地推广到正确标记的测试样本。然而，对于扰动如何包含类特征并促进泛化的理论理解是有限的。在本研究中，我们提供了一个理论框架，用于理解通过在相互正交样本上训练的单隐藏层网络从扰动中学习。我们的结果突显了各种对抗性扰动，甚至是几个像素的扰动，均包含足够的类特征用于泛化。此外，我们揭示了从扰动学习时的决策边界",
    "tldr": "研究提供了一个理论框架，表明各种对抗性扰动（甚至是几个像素的扰动）包含足够的类特征用于泛化，进一步揭示了从扰动中学习时的决策边界",
    "en_tdlr": "The study provides a theoretical framework showing various adversarial perturbations (even perturbations of a few pixels) contain sufficient class features for generalization, further revealing the decision boundary when learning from perturbations."
}