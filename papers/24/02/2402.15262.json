{
    "title": "Dynamic Memory Based Adaptive Optimization",
    "abstract": "arXiv:2402.15262v1 Announce Type: cross  Abstract: Define an optimizer as having memory $k$ if it stores $k$ dynamically changing vectors in the parameter space. Classical SGD has memory $0$, momentum SGD optimizer has $1$ and Adam optimizer has $2$. We address the following questions: How can optimizers make use of more memory units? What information should be stored in them? How to use them for the learning steps? As an approach to the last question, we introduce a general method called \"Retrospective Learning Law Correction\" or shortly RLLC. This method is designed to calculate a dynamically varying linear combination (called learning law) of memory units, which themselves may evolve arbitrarily. We demonstrate RLLC on optimizers whose memory units have linear update rules and small memory ($\\leq 4$ memory units). Our experiments show that in a variety of standard problems, these optimizers outperform the above mentioned three classical optimizers. We conclude that RLLC is a promisi",
    "link": "https://arxiv.org/abs/2402.15262",
    "context": "Title: Dynamic Memory Based Adaptive Optimization\nAbstract: arXiv:2402.15262v1 Announce Type: cross  Abstract: Define an optimizer as having memory $k$ if it stores $k$ dynamically changing vectors in the parameter space. Classical SGD has memory $0$, momentum SGD optimizer has $1$ and Adam optimizer has $2$. We address the following questions: How can optimizers make use of more memory units? What information should be stored in them? How to use them for the learning steps? As an approach to the last question, we introduce a general method called \"Retrospective Learning Law Correction\" or shortly RLLC. This method is designed to calculate a dynamically varying linear combination (called learning law) of memory units, which themselves may evolve arbitrarily. We demonstrate RLLC on optimizers whose memory units have linear update rules and small memory ($\\leq 4$ memory units). Our experiments show that in a variety of standard problems, these optimizers outperform the above mentioned three classical optimizers. We conclude that RLLC is a promisi",
    "path": "papers/24/02/2402.15262.json",
    "total_tokens": 896,
    "translated_title": "基于动态内存的自适应优化",
    "translated_abstract": "将优化器定义为在参数空间中存储$k$个动态变化向量的具有内存$k$的优化器。经典的SGD优化器具有内存$0$，动量SGD优化器具有$1$，Adam优化器具有$2$。本文探讨了以下问题：优化器如何利用更多内存单元？应该在其中存储哪些信息？如何将它们用于学习步骤？作为最后一个问题的方法，我们介绍了一种称为“回顾式学习法律修正”或简称RLLC的通用方法。该方法旨在计算内存单元的动态变化线性组合（称为学习法则），这些内存单元本身可能会任意演变。我们在内存单元具有线性更新规则和小内存（$\\leq 4$内存单元）的优化器上展示了RLLC。我们的实验表明，在各种标准问题中，这些优化器表现优于上述三种经典优化器。我们得出结论，RLLC是一种有前途的方法。",
    "tldr": "提出了一种称为“回顾式学习法律修正”的通用方法，用于计算内存单元的动态变化线性组合，能够在具有线性更新规则和小内存的优化器中取得优于经典优化器的性能。",
    "en_tdlr": "Introduced a general method called \"Retrospective Learning Law Correction\" for calculating dynamically varying linear combination of memory units, which can outperform classical optimizers in optimizers with linear update rules and small memory."
}