{
    "title": "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations",
    "abstract": "Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art ",
    "link": "https://arxiv.org/abs/2402.04236",
    "context": "Title: CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations\nAbstract: Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art ",
    "path": "papers/24/02/2402.04236.json",
    "total_tokens": 886,
    "translated_title": "CogCoM: 通过一系列的操作训练大规模视觉语言模型，并深入细节",
    "translated_abstract": "视觉语言模型（VLM）通过广泛的训练，在将视觉指令与答案对齐方面展示了广泛的可行性。然而，这种确定性的对齐导致模型忽视了关键的视觉推理，并导致在细致的视觉问题和不忠实的响应方面失败。在本文中，我们提出了一种称为“操作链”的机制，使VLM能够通过一系列的操作来解决问题，其中每个操作都指的是对视觉输入的操作，可以是通过先前训练获得的内在能力（例如，基础）或者是模仿类人行为（例如，放大）。这个机制鼓励VLM生成带有证据的视觉推理的忠实的响应，并允许用户在可解释的路径上追踪错误的原因。因此，我们训练了CogCoM，一个具有内置推理机制的17B通用VLM。实验证明，我们的模型达到了最先进的水平。",
    "tldr": "本文介绍了CogCoM，一个具备操作链机制的大规模视觉语言模型，通过一系列操作解决视觉问题，并以其证据性的视觉推理能力实现忠实的响应。",
    "en_tdlr": "This paper introduces CogCoM, a large-scale vision-language model with a mechanism called Chain of Manipulations, which solves visual problems through a series of operations and achieves faithful responses with its evidential visual reasoning ability."
}