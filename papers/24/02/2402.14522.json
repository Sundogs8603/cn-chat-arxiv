{
    "title": "Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond",
    "abstract": "arXiv:2402.14522v1 Announce Type: new  Abstract: Task embedding, a meta-learning technique that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner. Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs. To unleash the power of task embedding in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-mo",
    "link": "https://arxiv.org/abs/2402.14522",
    "context": "Title: Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond\nAbstract: arXiv:2402.14522v1 Announce Type: new  Abstract: Task embedding, a meta-learning technique that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner. Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs. To unleash the power of task embedding in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-mo",
    "path": "papers/24/02/2402.14522.json",
    "total_tokens": 772,
    "translated_title": "跨越多个模型的统一任务嵌入：弥合基于提示的大型语言模型及其它模型的差距",
    "translated_abstract": "任务嵌入是一种捕捉任务特定信息的元学习技术，已经变得流行起来，特别是在多任务学习、模型编辑和可解释性等领域。文章提出了一种名为统一任务嵌入（FUTE）的框架，该框架能够协调来自各种模型（包括较小的语言模型和具有不同提示的LLMs）的任务嵌入，使其处于单一向量空间。这种统一性使得可以比较和分析不同模型之间的相似性，扩展了现有任务嵌入方法在解决多模型应用中的范围和效用。",
    "tldr": "提出了一种框架用于统一不同模型的任务嵌入，使得任务嵌入可以跨越各种模型，并在单一向量空间内进行比较和分析。",
    "en_tdlr": "A framework is proposed to unify task embeddings from different models, enabling comparisons and analysis across various models within a single vector space."
}