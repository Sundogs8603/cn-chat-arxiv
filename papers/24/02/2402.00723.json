{
    "title": "Improving Semantic Control in Discrete Latent Spaces with Transformer Quantized Variational Autoencoders",
    "abstract": "Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon. Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism. To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and generation in Transformer-based VAEs. In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAEs to guide the self-attention mechanism in T5 at the token-level, exploiting its full generalization capabilities. Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of controllability and prese",
    "link": "https://arxiv.org/abs/2402.00723",
    "context": "Title: Improving Semantic Control in Discrete Latent Spaces with Transformer Quantized Variational Autoencoders\nAbstract: Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon. Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism. To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and generation in Transformer-based VAEs. In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAEs to guide the self-attention mechanism in T5 at the token-level, exploiting its full generalization capabilities. Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of controllability and prese",
    "path": "papers/24/02/2402.00723.json",
    "total_tokens": 964,
    "translated_title": "用Transformer量化变分自编码器改进离散潜在空间中的语义控制",
    "translated_abstract": "在自动变分编码器（VAE）的潜在空间中实现精确的语义控制对于NLP领域的下游任务非常重要，因为底层的生成机制可以更好地定位、解释和改进。然而，最近的研究在实现一致的结果方面存在困难，主要是由于变分瓶颈中不可避免的语义信息丢失以及解码机制的有限控制。为了克服这些挑战，我们研究了基于向量量化的变分自编码器（VQVAE）中的离散潜在空间，以改进Transformer-based VAEs中的语义控制和生成能力。具体而言，我们提出了一种新颖的模型T5VQVAE，利用VQVAE的可控性来指导T5中的自注意机制，以实现令人满意的生成效果。实验结果表明，在可控性和生成效果方面，T5VQVAE优于现有的VAE模型，包括Optimus。",
    "tldr": "本研究利用Transformer量化变分自编码器（VQVAEs）中离散潜在空间的可控性，提出了T5VQVAE模型，通过指导T5中的自注意机制实现更好的生成效果。实验证明，T5VQVAE在可控性和生成效果上优于现有模型。"
}