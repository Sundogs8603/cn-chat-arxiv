{
    "title": "Repetition Improves Language Model Embeddings",
    "abstract": "arXiv:2402.15449v1 Announce Type: new  Abstract: Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, \"echo embeddings,\" in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared",
    "link": "https://arxiv.org/abs/2402.15449",
    "context": "Title: Repetition Improves Language Model Embeddings\nAbstract: arXiv:2402.15449v1 Announce Type: new  Abstract: Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, \"echo embeddings,\" in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared",
    "path": "papers/24/02/2402.15449.json",
    "total_tokens": 881,
    "translated_title": "重复改善语言模型嵌入",
    "translated_abstract": "最近改进从自回归大型语言模型（LLMs）中提取文本嵌入的方法主要集中在改进数据、骨干预训练语言模型或通过指令改进任务差异化上。在这项工作中，我们解决了自回归模型的一个架构限制：令牌嵌入不能包含来自输入中后续令牌的信息。为了解决这一限制，我们提出了一种简单的方法，“回声嵌入”，其中我们在上下文中将输入重复两次，并从第二次出现中提取嵌入。我们展示了早期令牌的回声嵌入可以编码关于后续令牌的信息，从而使我们能够最大程度地利用高质量的LLMs进行嵌入。在MTEB排行榜上，回声嵌入在零射击中比经典嵌入提高了超过9%，在微调时提高了约0.7%。使用Mistral-7B模型的回声嵌入实现了与当前最先进模型的比较。",
    "tldr": "回声嵌入方法通过重复输入来提取信息，解决了自回归模型无法包含后续令牌信息的限制，实验结果表明其能够最大程度充分利用高质量的语言模型进行嵌入。",
    "en_tdlr": "The \"echo embeddings\" approach addresses the limitation of autoregressive models by repeating input to extract information, allowing to leverage high-quality language models to the fullest."
}