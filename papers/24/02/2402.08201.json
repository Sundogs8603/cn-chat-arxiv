{
    "title": "Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap",
    "abstract": "Doubly robust methods hold considerable promise for off-policy evaluation in Markov decision processes (MDPs) under sequential ignorability: They have been shown to converge as $1/\\sqrt{T}$ with the horizon $T$, to be statistically efficient in large samples, and to allow for modular implementation where preliminary estimation tasks can be executed using standard reinforcement learning techniques. Existing results, however, make heavy use of a strong distributional overlap assumption whereby the stationary distributions of the target policy and the data-collection policy are within a bounded factor of each other -- and this assumption is typically only credible when the state space of the MDP is bounded. In this paper, we re-visit the task of off-policy evaluation in MDPs under a weaker notion of distributional overlap, and introduce a class of truncated doubly robust (TDR) estimators which we find to perform well in this setting. When the distribution ratio of the target and data-coll",
    "link": "https://arxiv.org/abs/2402.08201",
    "context": "Title: Off-Policy Evaluation in Markov Decision Processes under Weak Distributional Overlap\nAbstract: Doubly robust methods hold considerable promise for off-policy evaluation in Markov decision processes (MDPs) under sequential ignorability: They have been shown to converge as $1/\\sqrt{T}$ with the horizon $T$, to be statistically efficient in large samples, and to allow for modular implementation where preliminary estimation tasks can be executed using standard reinforcement learning techniques. Existing results, however, make heavy use of a strong distributional overlap assumption whereby the stationary distributions of the target policy and the data-collection policy are within a bounded factor of each other -- and this assumption is typically only credible when the state space of the MDP is bounded. In this paper, we re-visit the task of off-policy evaluation in MDPs under a weaker notion of distributional overlap, and introduce a class of truncated doubly robust (TDR) estimators which we find to perform well in this setting. When the distribution ratio of the target and data-coll",
    "path": "papers/24/02/2402.08201.json",
    "total_tokens": 898,
    "translated_title": "弱分布重叠下马尔可夫决策过程中的离策略评估",
    "translated_abstract": "在马尔可夫决策过程（MDP）中，双重稳健方法在序列可忽略性下对离策略评估具有很大的潜力：它们已经证明了随着时长T的收敛速度为$1/\\sqrt{T}$，在大样本中具有统计效率，并且可以通过标准强化学习技术执行预估任务，具有模块化实现的能力。然而，现有结果在很大程度上使用了强分布重叠假设，即目标政策和数据收集政策的稳态分布相差在有限因子内，而这个假设通常只在MDP的状态空间有界时才可信。在本文中，我们重新审视了在弱分布重叠概念下的MDP离策略评估任务，并引入了一类截断双重稳健（TDR）估计器，在这种情况下表现良好。当目标和数据收集的分布比率有界时，我们证明了这些估计器的一致性。",
    "tldr": "本文研究了弱分布重叠下马尔可夫决策过程中的离策略评估问题，并提出了一种截断双重稳健（TDR）估计器，在这种情况下表现良好。"
}