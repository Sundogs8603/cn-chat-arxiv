{
    "title": "CapHuman: Capture Your Moments in Parallel Universes",
    "abstract": "We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align\" paradigm, which enables generalizable identity preservation for new individuals without cum",
    "link": "https://arxiv.org/abs/2402.00627",
    "context": "Title: CapHuman: Capture Your Moments in Parallel Universes\nAbstract: We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align\" paradigm, which enables generalizable identity preservation for new individuals without cum",
    "path": "papers/24/02/2402.00627.json",
    "total_tokens": 884,
    "translated_title": "CapHuman: 在平行宇宙中捕捉你的瞬间",
    "translated_abstract": "我们关注一种新颖的以人为中心的图像合成任务，即仅给定一个参考面部照片，期望能够在不同情境中生成具有多样的头部位置、姿势和面部表情的特定个体图像。为了实现这一目标，我们认为我们的生成模型应具备以下有利特征：（1）对世界和人类社会有强大的视觉和语义理解，用于基本物体和人类图像的生成；（2）可泛化的身份保留能力；（3）灵活细粒度的头部控制。最近，大型预训练的文本到图像扩散模型展现了显著的结果，成为一个强大的生成基础。基于此，我们旨在释放预训练模型的上述两种能力。在这项工作中，我们提出了一个名为CapHuman的新框架。我们采用了“编码然后学习对齐”的范式，为新个体实现了可泛化的身份保留能力。",
    "tldr": "CapHuman是一个新框架，通过“编码然后学习对齐”的范式实现了可泛化的身份保留能力，用于在不同情境中生成具有多样的头部位置、姿势和面部表情的特定个体图像。",
    "en_tdlr": "CapHuman is a new framework that achieves generalizable identity preservation by adopting the \"encode then learn to align\" paradigm, used for generating specific individual images with diverse head positions, poses, and facial expressions in different contexts."
}