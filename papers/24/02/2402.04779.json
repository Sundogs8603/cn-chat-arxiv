{
    "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
    "abstract": "The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both ",
    "link": "https://arxiv.org/abs/2402.04779",
    "context": "Title: StableMask: Refining Causal Masking in Decoder-only Transformer\nAbstract: The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both ",
    "path": "papers/24/02/2402.04779.json",
    "total_tokens": 819,
    "translated_title": "StableMask: 在仅解码Transformer中改进因果屏蔽的方法",
    "translated_abstract": "在语言建模中，仅解码Transformer架构中采用因果屏蔽和相对位置编码（RPE）已成为事实上的选择。尽管其在各种任务中表现出色，但我们发现了两个限制：首先，即使当前嵌入具有足够的自包含信息，它要求所有注意力分数都为非零且总和为1。这强迫模型对特定的标记分配不成比例的过度关注。其次，基于RPE的Transformer在编码绝对位置信息方面的能力有限，因此在位置关键任务中受到限制。在这项工作中，我们提出了StableMask：一种无参数的方法，通过改进因果屏蔽来解决这两个限制。它引入了伪注意力值来平衡注意力分布，并通过逐渐减小的屏蔽比率来编码绝对位置信息。StableMask的有效性得到了验证。",
    "tldr": "StableMask是一种在仅解码Transformer中改进因果屏蔽的无参数方法，通过引入伪注意力值来平衡注意力分布，并通过逐渐减小的屏蔽比率来编码绝对位置信息。",
    "en_tdlr": "StableMask is a parameter-free method that improves causal masking in decoder-only Transformers. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio."
}