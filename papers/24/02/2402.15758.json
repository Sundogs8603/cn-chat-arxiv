{
    "title": "Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens",
    "abstract": "arXiv:2402.15758v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their widespread application is hindered by the resource-intensive decoding process. To address this challenge, current approaches have incorporated additional decoding heads to enable parallel prediction of multiple subsequent tokens, thereby achieving inference acceleration. Nevertheless, the accuracy of these decoding heads falls short of the auto-regressive decoding approach.   In light of these limitations, we propose Chimera, a novel framework specifically designed for speculative sampling. Within this framework, we introduce a lightweight draft model that effectively utilizes previously generated tokens to predict subsequent words. To ensure both accuracy and efficiency, we present two strategies within the lightweight draft model. Firstly, we focus on capturing short-range dependencies at the bottom layer. Secondly, we leverage",
    "link": "https://arxiv.org/abs/2402.15758",
    "context": "Title: Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens\nAbstract: arXiv:2402.15758v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their widespread application is hindered by the resource-intensive decoding process. To address this challenge, current approaches have incorporated additional decoding heads to enable parallel prediction of multiple subsequent tokens, thereby achieving inference acceleration. Nevertheless, the accuracy of these decoding heads falls short of the auto-regressive decoding approach.   In light of these limitations, we propose Chimera, a novel framework specifically designed for speculative sampling. Within this framework, we introduce a lightweight draft model that effectively utilizes previously generated tokens to predict subsequent words. To ensure both accuracy and efficiency, we present two strategies within the lightweight draft model. Firstly, we focus on capturing short-range dependencies at the bottom layer. Secondly, we leverage",
    "path": "papers/24/02/2402.15758.json",
    "total_tokens": 873,
    "translated_title": "Chimera: 融合所有令牌的无损解码方法，加速大型语言模型推理",
    "translated_abstract": "大型语言模型（LLMs）在各种任务中展示了显著的能力。然而，它们的广泛应用被资源密集型的解码过程所阻碍。为了解决这一挑战，目前的方法已经合并了额外的解码头，以实现对多个后续令牌的并行预测，从而实现推理加速。然而，这些解码头的准确性远不及自回归解码方法。鉴于这些限制，我们提出了Chimera，这是一个专门为推测采样设计的新框架。在这个框架内，我们引入了一个轻量级的草稿模型，能够有效利用先前生成的令牌来预测后续单词。为了确保准确性和效率，我们在轻量级草稿模型中提出了两种策略。首先，我们专注于在底层捕获短程依赖性。其次，我们利用",
    "tldr": "提出了Chimera框架，用于加速大型语言模型推理，通过引入轻量级的草稿模型和两种策略，利用先前生成的令牌来预测后续单词，以解决解码过程中的准确性和效率问题",
    "en_tdlr": "Introduced the Chimera framework for accelerating large language model inference by leveraging a lightweight draft model and two strategies to predict subsequent words using previously generated tokens, addressing accuracy and efficiency issues in the decoding process."
}