{
    "title": "Topic Modeling as Multi-Objective Contrastive Optimization",
    "abstract": "Recent representation learning approaches enhance neural topic models by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the contrastive learning objective that contrasts pairs of input documents. However, document-level contrastive learning might capture low-level mutual information, such as word ratio, which disturbs topic modeling. Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the contrastive loss which attempts to learn topic representations that generalize among input documents. To address these issues, we first introduce a novel contrastive learning method oriented towards sets of topic vectors to capture useful semantics that are shared among a set of input documents. Secondly, we explicitly cast contrastive topic modeling as a gradient-based multi-objective optimization problem, with the goal of achieving a Pareto stationary solution that b",
    "link": "https://arxiv.org/abs/2402.07577",
    "context": "Title: Topic Modeling as Multi-Objective Contrastive Optimization\nAbstract: Recent representation learning approaches enhance neural topic models by optimizing the weighted linear combination of the evidence lower bound (ELBO) of the log-likelihood and the contrastive learning objective that contrasts pairs of input documents. However, document-level contrastive learning might capture low-level mutual information, such as word ratio, which disturbs topic modeling. Moreover, there is a potential conflict between the ELBO loss that memorizes input details for better reconstruction quality, and the contrastive loss which attempts to learn topic representations that generalize among input documents. To address these issues, we first introduce a novel contrastive learning method oriented towards sets of topic vectors to capture useful semantics that are shared among a set of input documents. Secondly, we explicitly cast contrastive topic modeling as a gradient-based multi-objective optimization problem, with the goal of achieving a Pareto stationary solution that b",
    "path": "papers/24/02/2402.07577.json",
    "total_tokens": 871,
    "translated_title": "主题建模作为多目标对比优化方法",
    "translated_abstract": "最近的表示学习方法通过优化对数似然的证据下界（ELBO）和对比学习目标的加权线性组合来增强神经主题模型。然而，文档级对比学习可能捕捉到低级别的互信息，例如词比例，这会干扰主题建模。此外，ELBO损失旨在记忆输入细节以获得更好的重构质量，而对比损失则试图学习在输入文档之间泛化的主题表示，二者存在潜在冲突。为了解决这些问题，首先我们引入了一种新颖的面向主题向量集合的对比学习方法，以捕捉一组输入文档之间共享的有用语义。其次，我们将对比主题建模明确提出为一个基于梯度的多目标优化问题，目标是实现帕累托平稳解决方案。",
    "tldr": "该论文介绍了一种新颖的主题建模方法，通过优化对数似然的证据下界和对比学习目标的加权线性组合，将对比主题建模作为一种多目标优化问题，旨在获得能够捕捉共享语义并克服低级别互信息干扰的主题向量集合。",
    "en_tdlr": "This paper introduces a novel approach to topic modeling by optimizing the weighted linear combination of the evidence lower bound and the contrastive learning objective. It explicitly formulates contrastive topic modeling as a multi-objective optimization problem, aiming to achieve a set of topic vectors that capture shared semantics and overcome low-level mutual information interference."
}