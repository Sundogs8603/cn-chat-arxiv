{
    "title": "Multimodal Transformer With a Low-Computational-Cost Guarantee",
    "abstract": "arXiv:2402.15096v1 Announce Type: new  Abstract: Transformer-based models have significantly improved performance across a range of multimodal understanding tasks, such as visual question answering and action recognition. However, multimodal Transformers significantly suffer from a quadratic complexity of the multi-head attention with the input sequence length, especially as the number of modalities increases. To address this, we introduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal attention mechanism that aims to reduce computational cost during training and inference with minimal performance loss. Specifically, by assigning different multimodal attention patterns to each attention head, LoCoMT can flexibly control multimodal signals and theoretically ensures a reduced computational cost compared to existing multimodal Transformer variants. Experimental results on two multimodal datasets, namely Audioset and MedVidCL demonstrate that LoCoMT not only reduces GFLOPs bu",
    "link": "https://arxiv.org/abs/2402.15096",
    "context": "Title: Multimodal Transformer With a Low-Computational-Cost Guarantee\nAbstract: arXiv:2402.15096v1 Announce Type: new  Abstract: Transformer-based models have significantly improved performance across a range of multimodal understanding tasks, such as visual question answering and action recognition. However, multimodal Transformers significantly suffer from a quadratic complexity of the multi-head attention with the input sequence length, especially as the number of modalities increases. To address this, we introduce Low-Cost Multimodal Transformer (LoCoMT), a novel multimodal attention mechanism that aims to reduce computational cost during training and inference with minimal performance loss. Specifically, by assigning different multimodal attention patterns to each attention head, LoCoMT can flexibly control multimodal signals and theoretically ensures a reduced computational cost compared to existing multimodal Transformer variants. Experimental results on two multimodal datasets, namely Audioset and MedVidCL demonstrate that LoCoMT not only reduces GFLOPs bu",
    "path": "papers/24/02/2402.15096.json",
    "total_tokens": 817,
    "translated_title": "具有低计算成本保证的多模态Transformer",
    "translated_abstract": "基于Transformer的模型显著提高了各种多模态理解任务的性能，如视觉问题回答和动作识别。然而，多模态Transformer在多头注意力机制的复杂度方面存在问题，特别是随着模态数量的增加，存在二次复杂度。为了解决这个问题，我们引入了低成本多模态Transformer（LoCoMT），这是一种旨在在训练和推断过程中减少计算成本的新型多模态注意力机制，并且能够实现最小性能损失。具体地，通过为每个注意力头分配不同的多模态注意力模式，LoCoMT能够灵活控制多模态信号，并在理论上确保相对于现有多模态Transformer变体减少计算成本。在两个多模态数据集Audioset和MedVidCL上的实验结果表明，LoCoMT不仅减少了GFLOPs，",
    "tldr": "提出了一种低成本多模态Transformer (LoCoMT) 机制，通过分配不同的多模态注意力模式给每个注意力头，实现了在减少计算成本的同时最小化性能损失",
    "en_tdlr": "Proposed a low-cost multimodal Transformer mechanism, LoCoMT, which, by assigning different multimodal attention patterns to each attention head, achieves a reduced computational cost while minimizing performance loss."
}