{
    "title": "Universal Neural Functionals",
    "abstract": "A challenging problem in many modern machine learning tasks is to process weight-space features, i.e., to transform or extract information from the weights and gradients of a neural network. Recent works have developed promising weight-space models that are equivariant to the permutation symmetries of simple feedforward networks. However, they are not applicable to general architectures, since the permutation symmetries of a weight space can be complicated by recurrence or residual connections. This work proposes an algorithm that automatically constructs permutation equivariant models, which we refer to as universal neural functionals (UNFs), for any weight space. Among other applications, we demonstrate how UNFs can be substituted into existing learned optimizer designs, and find promising improvements over prior methods when optimizing small image classifiers and language models. Our results suggest that learned optimizers can benefit from considering the (symmetry) structure of the",
    "link": "https://arxiv.org/abs/2402.05232",
    "context": "Title: Universal Neural Functionals\nAbstract: A challenging problem in many modern machine learning tasks is to process weight-space features, i.e., to transform or extract information from the weights and gradients of a neural network. Recent works have developed promising weight-space models that are equivariant to the permutation symmetries of simple feedforward networks. However, they are not applicable to general architectures, since the permutation symmetries of a weight space can be complicated by recurrence or residual connections. This work proposes an algorithm that automatically constructs permutation equivariant models, which we refer to as universal neural functionals (UNFs), for any weight space. Among other applications, we demonstrate how UNFs can be substituted into existing learned optimizer designs, and find promising improvements over prior methods when optimizing small image classifiers and language models. Our results suggest that learned optimizers can benefit from considering the (symmetry) structure of the",
    "path": "papers/24/02/2402.05232.json",
    "total_tokens": 799,
    "translated_title": "通用神经功能",
    "translated_abstract": "在现代许多机器学习任务中，一个具有挑战性的问题是处理权重空间特征，即从神经网络的权重和梯度中转换或提取信息。最近的研究已经开发出了一些有希望的权重空间模型，这些模型对简单的前馈网络的置换对称性是等变的。然而，它们对于普通架构并不适用，因为权重空间的置换对称性可能会因循环或残差连接而变得复杂。本文提出了一种算法，自动构建置换等变模型，我们称之为通用神经功能（UNFs），适用于任何权重空间。在其他应用中，我们展示了如何将UNFs替代现有的学习优化器设计，并在优化小型图像分类器和语言模型时发现有希望的改进。我们的结果表明，学习优化器可以从考虑（对称）结构的角度受益。",
    "tldr": "本文提出了通用神经功能（UNFs），一种能够自动构建适用于任何权重空间的置换等变模型的算法。实验结果显示，在优化小型图像分类器和语言模型时，UNFs能够取得有希望的改进，为学习优化器设计提供了新的思路。"
}