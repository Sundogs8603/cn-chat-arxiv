{
    "title": "Multilingual E5 Text Embeddings: A Technical Report",
    "abstract": "This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at https://github.com/microsoft/unilm/tree/master/e5 .",
    "link": "https://arxiv.org/abs/2402.05672",
    "context": "Title: Multilingual E5 Text Embeddings: A Technical Report\nAbstract: This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, released in mid-2023. Three embedding models of different sizes (small / base / large) are provided, offering a balance between the inference efficiency and embedding quality. The training procedure adheres to the English E5 model recipe, involving contrastive pre-training on 1 billion multilingual text pairs, followed by fine-tuning on a combination of labeled datasets. Additionally, we introduce a new instruction-tuned embedding model, whose performance is on par with state-of-the-art, English-only models of similar sizes. Information regarding the model release can be found at https://github.com/microsoft/unilm/tree/master/e5 .",
    "path": "papers/24/02/2402.05672.json",
    "total_tokens": 810,
    "translated_title": "多语言E5文本嵌入：一项技术报告",
    "translated_abstract": "本技术报告介绍了开源的多语言E5文本嵌入模型的训练方法和评估结果，该模型于2023年中期发布。提供了三种不同大小（小/基础/大）的嵌入模型，平衡了推理效率和嵌入质量。训练过程遵循英文E5模型的配方，涉及10亿个多语言文本对的对比预训练，然后在一系列标记数据集上进行微调。此外，我们还引入了一种新的以指令为导向的嵌入模型，性能与相似大小的最先进的仅英文模型相当。有关模型发布的信息可以在https://github.com/microsoft/unilm/tree/master/e5找到。",
    "tldr": "这项技术报告介绍了开源的多语言E5文本嵌入模型的训练方法和评估结果，包括三种不同大小的模型和一种新的以指令为导向的嵌入模型。模型在推理效率和嵌入质量上取得了平衡，性能与同等大小的最先进的仅英文模型相当。",
    "en_tdlr": "This technical report presents the training methodology and evaluation results of the open-source multilingual E5 text embedding models, including three different sizes of models and a new instruction-tuned embedding model. The models achieve a balance between inference efficiency and embedding quality, performing on par with state-of-the-art, English-only models of similar sizes."
}