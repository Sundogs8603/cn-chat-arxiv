{
    "title": "Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models",
    "abstract": "arXiv:2402.11122v1 Announce Type: cross  Abstract: Memory Editing (ME) has emerged as an efficient method to modify erroneous facts or inject new facts into Large Language Models (LLMs). Two mainstream ME methods exist: parameter-modifying ME and parameter-preserving ME (integrating extra modules while preserving original parameters). Regrettably, previous studies on ME evaluation have two critical limitations: (i) evaluating LLMs with single edit only, neglecting the need for continuous editing, and (ii) evaluations focusing solely on basic factual triples, overlooking broader LLM capabilities like logical reasoning and reading understanding. This study addresses these limitations with contributions threefold: (i) We explore how ME affects a wide range of fundamental capabilities of LLMs under sequential editing. Experimental results reveal an intriguing phenomenon: Most parameter-modifying ME consistently degrade performance across all tasks after a few sequential edits. In contrast,",
    "link": "https://arxiv.org/abs/2402.11122",
    "context": "Title: Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models\nAbstract: arXiv:2402.11122v1 Announce Type: cross  Abstract: Memory Editing (ME) has emerged as an efficient method to modify erroneous facts or inject new facts into Large Language Models (LLMs). Two mainstream ME methods exist: parameter-modifying ME and parameter-preserving ME (integrating extra modules while preserving original parameters). Regrettably, previous studies on ME evaluation have two critical limitations: (i) evaluating LLMs with single edit only, neglecting the need for continuous editing, and (ii) evaluations focusing solely on basic factual triples, overlooking broader LLM capabilities like logical reasoning and reading understanding. This study addresses these limitations with contributions threefold: (i) We explore how ME affects a wide range of fundamental capabilities of LLMs under sequential editing. Experimental results reveal an intriguing phenomenon: Most parameter-modifying ME consistently degrade performance across all tasks after a few sequential edits. In contrast,",
    "path": "papers/24/02/2402.11122.json",
    "total_tokens": 855,
    "translated_title": "导航双重面：对大型语言模型中顺序记忆编辑的全面评估",
    "translated_abstract": "记忆编辑（ME）已经成为修改大型语言模型（LLMs）中错误事实或注入新事实的有效方法。存在两种主流ME方法：修改参数ME和保留参数ME（在保留原始参数的同时整合额外模块）。遗憾的是，先前对ME评估的研究存在两个关键限制：（i）仅评估带有单个编辑的LLMs，忽略了持续编辑的需要，以及（ii）评估仅关注基本事实三元组，忽视了更广泛的LLM能力，如逻辑推理和阅读理解。本研究通过以下三点解决了这些限制：（i）我们探索了ME如何影响LLMs的广泛基本能力在顺序编辑下。实验结果揭示了一个有趣的现象：大多数修改参数ME在几次顺序编辑后一贯降低所有任务的表现。相比之下，",
    "tldr": "这项研究全面评估了大型语言模型中顺序记忆编辑的影响，发现修改参数ME可能会导致所有任务表现不佳，而保留参数ME则能够保持较好性能。",
    "en_tdlr": "This study comprehensively evaluates the impact of sequential memory editing in large language models, finding that parameter-modifying ME may lead to poor performance across all tasks while parameter-preserving ME can maintain better performance."
}