{
    "title": "Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution",
    "abstract": "In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy. However, in many real-world applications, the instant reward signals are not obtainable by the agent. Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory. In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions. To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained. We provide the theoretical study to establish the connection between RLBR and standa",
    "link": "https://arxiv.org/abs/2402.03771",
    "context": "Title: Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution\nAbstract: In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy. However, in many real-world applications, the instant reward signals are not obtainable by the agent. Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory. In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions. To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained. We provide the theoretical study to establish the connection between RLBR and standa",
    "path": "papers/24/02/2402.03771.json",
    "total_tokens": 917,
    "translated_title": "来自打包奖励的强化学习：一种基于Transformer的实例级奖励重新分配方法",
    "translated_abstract": "在强化学习中，每个动作的即时奖励信号将为代理生成，以便代理学习如何最大化累积奖励以获取最优策略。然而，在许多实际应用中，代理无法获取即时奖励信号。相反，学习器只在路径的结束处获取奖励，其中路径的部分序列被定义为一个包。在这种情况下，学习器必须面对探索包中未知即时奖励的显著困难，这不能通过现有方法解决，包括仅考虑完整路径并忽略内部奖励分布的轨迹方法。为了正式研究这种情况，我们引入了一种新的强化学习设置，称为来自打包奖励的强化学习（Reinforcement Learning from Bagged Rewards，RLBR），只能获取序列的打包奖励。我们进行了理论研究，建立了RLBR与标准强化学习之间的联系。",
    "tldr": "该论文提出了一种名为强化学习来自打包奖励的新问题，其中学习器只能获取序列的打包奖励，在这种情况下探索未知的即时奖励是一项困难的任务，并引入了基于Transformer的方法来解决这个问题。",
    "en_tdlr": "This paper introduces a new problem in reinforcement learning called Reinforcement Learning from Bagged Rewards (RLBR), where the learner can only access the bagged rewards of sequences. It addresses the challenge of exploring unknown instant rewards in bags and proposes a Transformer-based approach to solve this problem."
}