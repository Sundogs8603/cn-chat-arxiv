{
    "title": "CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision",
    "abstract": "arXiv:2402.16928v1 Announce Type: cross  Abstract: Binary code representation learning has shown significant performance in binary analysis tasks. But existing solutions often have poor transferability, particularly in few-shot and zero-shot scenarios where few or no training samples are available for the tasks. To address this problem, we present CLAP (Contrastive Language-Assembly Pre-training), which employs natural language supervision to learn better representations of binary code (i.e., assembly code) and get better transferability. At the core, our approach boosts superior transfer learning capabilities by effectively aligning binary code with their semantics explanations (in natural language), resulting a model able to generate better embeddings for binary code. To enable this alignment training, we then propose an efficient dataset engine that could automatically generate a large and diverse dataset comprising of binary code and corresponding natural language explanations. We ",
    "link": "https://arxiv.org/abs/2402.16928",
    "context": "Title: CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision\nAbstract: arXiv:2402.16928v1 Announce Type: cross  Abstract: Binary code representation learning has shown significant performance in binary analysis tasks. But existing solutions often have poor transferability, particularly in few-shot and zero-shot scenarios where few or no training samples are available for the tasks. To address this problem, we present CLAP (Contrastive Language-Assembly Pre-training), which employs natural language supervision to learn better representations of binary code (i.e., assembly code) and get better transferability. At the core, our approach boosts superior transfer learning capabilities by effectively aligning binary code with their semantics explanations (in natural language), resulting a model able to generate better embeddings for binary code. To enable this alignment training, we then propose an efficient dataset engine that could automatically generate a large and diverse dataset comprising of binary code and corresponding natural language explanations. We ",
    "path": "papers/24/02/2402.16928.json",
    "total_tokens": 813,
    "translated_title": "使用自然语言监督学习可转移的二进制代码表示的CLAP",
    "translated_abstract": "二进制代码表示学习在二进制分析任务中显示出显著性能。但现有解决方案在迁移性上往往较差，特别是在少样本和零样本情景下，任务的训练样本很少或不存在时。为了解决这个问题，我们提出了CLAP（对比语言-汇编预训练），它利用自然语言监督来学习更好的二进制代码（即汇编代码）表示，并获得更好的迁移性。从核心上讲，我们的方法通过有效地将二进制代码与它们的语义解释（自然语言中）相对齐，提高了卓越的迁移学习能力，从而使模型能够为二进制代码生成更好的嵌入。为了实现这种对齐训练，我们随后提出了一种能够自动生成大量和多样化的数据集的高效数据集引擎，包括二进制代码和相应的自然语言解释。",
    "tldr": "CLAP通过自然语言监督学习二进制代码的转移表示，提高了在少样本和零样本情景下的迁移性能。",
    "en_tdlr": "CLAP improves transferability of binary code representations in few-shot and zero-shot scenarios by learning with natural language supervision."
}