{
    "title": "Unveiling Vulnerability of Self-Attention",
    "abstract": "arXiv:2402.16470v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) are shown to be vulnerable to minor word changes, which poses a big threat to real-world systems. While previous studies directly focus on manipulating word inputs, they are limited by their means of generating adversarial samples, lacking generalization to versatile real-world attack. This paper studies the basic structure of transformer-based PLMs, the self-attention (SA) mechanism. (1) We propose a powerful perturbation technique \\textit{HackAttend}, which perturbs the attention scores within the SA matrices via meticulously crafted attention masks. We show that state-of-the-art PLMs fall into heavy vulnerability that minor attention perturbations $(1\\%)$ can produce a very high attack success rate $(98\\%)$. Our paper expands the conventional text attack of word perturbations to more general structural perturbations. (2) We introduce \\textit{S-Attend}, a novel smoothing technique that effectively mak",
    "link": "https://arxiv.org/abs/2402.16470",
    "context": "Title: Unveiling Vulnerability of Self-Attention\nAbstract: arXiv:2402.16470v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) are shown to be vulnerable to minor word changes, which poses a big threat to real-world systems. While previous studies directly focus on manipulating word inputs, they are limited by their means of generating adversarial samples, lacking generalization to versatile real-world attack. This paper studies the basic structure of transformer-based PLMs, the self-attention (SA) mechanism. (1) We propose a powerful perturbation technique \\textit{HackAttend}, which perturbs the attention scores within the SA matrices via meticulously crafted attention masks. We show that state-of-the-art PLMs fall into heavy vulnerability that minor attention perturbations $(1\\%)$ can produce a very high attack success rate $(98\\%)$. Our paper expands the conventional text attack of word perturbations to more general structural perturbations. (2) We introduce \\textit{S-Attend}, a novel smoothing technique that effectively mak",
    "path": "papers/24/02/2402.16470.json",
    "total_tokens": 799,
    "translated_title": "揭示自注意力脆弱性",
    "translated_abstract": "预训练语言模型（PLMs）被发现对微小词语变化具有脆弱性，这对现实世界系统构成了巨大威胁。本文研究了基于Transformer的PLMs的基本结构，即自注意力（SA）机制。我们提出了一种强大的扰动技术HackAttend，通过精心设计的注意力蒙版扰动SA矩阵内的注意力分数。我们展示了最先进的PLMs存在严重脆弱性，微小的注意力扰动（1%）就能产生非常高的攻击成功率（98%）。我们的论文将传统的文本攻击从词汇扰动扩展到更广泛的结构扰动。",
    "tldr": "本文通过提出HackAttend扰动技术，揭示了最先进的PLMs因微小的注意力扰动而产生的高攻击成功率，将文本攻击从词汇扰动扩展到结构扰动。"
}