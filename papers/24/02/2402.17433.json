{
    "title": "Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder",
    "abstract": "arXiv:2402.17433v1 Announce Type: new  Abstract: Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modul",
    "link": "https://arxiv.org/abs/2402.17433",
    "context": "Title: Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder\nAbstract: arXiv:2402.17433v1 Announce Type: new  Abstract: Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modul",
    "path": "papers/24/02/2402.17433.json",
    "total_tokens": 902,
    "translated_title": "通过从预训练对比性EEG-文本蒙版自动编码器中转移的表示增强EEG到文本解码",
    "translated_abstract": "从无创脑电图（EEG）重建自然语言具有很大的潜力，作为脑机接口（BCI）的语言解码技术。然而，基于EEG的语言解码仍处于初级阶段，面临诸多技术问题，如：1）缺乏一个能够有效整合跨模态（EEG和文本之间）自学习与EEG特征或文本序列的模内自重构的混合策略；2）未充分利用大型语言模型（LLMs）来增强基于EEG的语言解码。为解决上述问题，我们提出了对比性EEG-文本蒙版自动编码器（CET-MAE），这是一个通过专用的多流编码器在EEG和文本之间以及内部进行复合自监督学习的新型模型。此外，我们开发了一个名为E2T-PTR（使用预训练可转移表示进行EEG到文本解码）的框架，该框架利用预训练模组",
    "tldr": "通过Contrastive EEG-Text Masked Autoencoder（CET-MAE）和E2T-PTR框架，提出了一种新的模型和方法来增强基于EEG的语言解码。",
    "en_tdlr": "A novel model and method were proposed to enhance EEG-based language decoding through CET-MAE and E2T-PTR framework."
}