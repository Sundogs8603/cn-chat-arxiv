{
    "title": "Enhancing Compositional Generalization via Compositional Feature Alignment",
    "abstract": "Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain lab",
    "link": "https://arxiv.org/abs/2402.02851",
    "context": "Title: Enhancing Compositional Generalization via Compositional Feature Alignment\nAbstract: Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain lab",
    "path": "papers/24/02/2402.02851.json",
    "total_tokens": 873,
    "translated_title": "通过组合特征对齐增强组合通用性",
    "translated_abstract": "机器学习模型在现实世界的应用中经常面临数据分布偏移的问题，即训练数据和测试数据分布之间存在差异。在常见的多领域多类别设置中，随着类别和领域数量的增加，很难为每个领域-类别组合收集训练数据。这个挑战自然地引发了对具备组合通用性（CG）能力的模型的探索，即模型可以推广到未见过的领域-类别组合。为了深入研究CG挑战，我们开发了CG-Bench，这是一套从现有实际图像数据集派生的CG基准测试，并观察到目前在基础模型（如CLIP和DINOv2）上流行的预训练-微调范式在这个挑战中存在困难。为了解决这个挑战，我们提出了组合特征对齐（CFA），这是一种简单的两阶段微调技术，它通过在预训练的编码器上学习两个正交线性头部来对齐类别和领域的标签。",
    "tldr": "通过组合特征对齐，增强了模型的组合通用性，使其能够推广到未见过的领域-类别组合。",
    "en_tdlr": "By utilizing compositional feature alignment, the model's compositional generalization ability is enhanced, enabling it to generalize to unseen domain-class combinations."
}