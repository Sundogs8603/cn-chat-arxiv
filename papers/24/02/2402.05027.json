{
    "title": "Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing",
    "abstract": "Graph-based environments pose unique challenges to multi-agent reinforcement learning. In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice. We evaluate our meth",
    "link": "https://arxiv.org/abs/2402.05027",
    "context": "Title: Towards Generalizability of Multi-Agent Reinforcement Learning in Graphs with Recurrent Message Passing\nAbstract: Graph-based environments pose unique challenges to multi-agent reinforcement learning. In decentralized approaches, agents operate within a given graph and make decisions based on partial or outdated observations. The size of the observed neighborhood limits the generalizability to different graphs and affects the reactivity of agents, the quality of the selected actions, and the communication overhead. This work focuses on generalizability and resolves the trade-off in observed neighborhood size with a continuous information flow in the whole graph. We propose a recurrent message-passing model that iterates with the environment's steps and allows nodes to create a global representation of the graph by exchanging messages with their neighbors. Agents receive the resulting learned graph observations based on their location in the graph. Our approach can be used in a decentralized manner at runtime and in combination with a reinforcement learning algorithm of choice. We evaluate our meth",
    "path": "papers/24/02/2402.05027.json",
    "total_tokens": 890,
    "translated_title": "在具有循环消息传递的图中实现多Agent强化学习的泛化能力",
    "translated_abstract": "基于图的环境给多Agent强化学习带来了独特的挑战。在分散式方法中，Agent在给定的图中操作，并根据部分或过时的观察做出决策。观察到的邻域的大小限制了在不同图上的泛化能力，并影响到Agent的反应性、选择的动作质量和通信开销。本研究侧重于泛化能力，并通过在整个图中进行连续的信息流解决了观察到的邻域大小的权衡。我们提出了一种循环消息传递模型，它与环境的步骤迭代，并允许节点通过与其邻居交换消息来创建图的全局表示。根据Agent在图中的位置，Agent接收到基于学习到的图观察结果。我们的方法可以在运行时以分散的方式使用，并与选择的强化学习算法结合使用。我们评估了我们的方法...",
    "tldr": "本论文提出了一种循环消息传递模型，它可以在图中实现多Agent强化学习的泛化能力。这种模型通过在整个图中进行信息流实现了观察邻域大小的平衡，从而提高了Agent的反应性、选择动作的质量和通信效率。",
    "en_tdlr": "This paper proposes a recurrent message-passing model that enables generalizability of multi-agent reinforcement learning in graphs. This model achieves a balance in observed neighborhood size by allowing continuous information flow throughout the whole graph, resulting in improved reactivity, action quality, and communication efficiency for agents."
}