{
    "title": "TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models",
    "abstract": "In this paper we introduce TWIG (Topologically-Weighted Intelligence Generation), a novel, embedding-free paradigm for simulating the output of KGEs that uses a tiny fraction of the parameters. TWIG learns weights from inputs that consist of topological features of the graph data, with no coding for latent representations of entities or edges. Our experiments on the UMLS dataset show that a single TWIG neural network can predict the results of state-of-the-art ComplEx-N3 KGE model nearly exactly on across all hyperparameter configurations. To do this it uses a total of 2590 learnable parameters, but accurately predicts the results of 1215 different hyperparameter combinations with a combined cost of 29,322,000 parameters. Based on these results, we make two claims: 1) that KGEs do not learn latent semantics, but only latent representations of structural patterns; 2) that hyperparameter choice in KGEs is a deterministic function of the KGE model and graph structure. We further hypothesi",
    "link": "https://arxiv.org/abs/2402.06097",
    "context": "Title: TWIG: Towards pre-hoc Hyperparameter Optimisation and Cross-Graph Generalisation via Simulated KGE Models\nAbstract: In this paper we introduce TWIG (Topologically-Weighted Intelligence Generation), a novel, embedding-free paradigm for simulating the output of KGEs that uses a tiny fraction of the parameters. TWIG learns weights from inputs that consist of topological features of the graph data, with no coding for latent representations of entities or edges. Our experiments on the UMLS dataset show that a single TWIG neural network can predict the results of state-of-the-art ComplEx-N3 KGE model nearly exactly on across all hyperparameter configurations. To do this it uses a total of 2590 learnable parameters, but accurately predicts the results of 1215 different hyperparameter combinations with a combined cost of 29,322,000 parameters. Based on these results, we make two claims: 1) that KGEs do not learn latent semantics, but only latent representations of structural patterns; 2) that hyperparameter choice in KGEs is a deterministic function of the KGE model and graph structure. We further hypothesi",
    "path": "papers/24/02/2402.06097.json",
    "total_tokens": 846,
    "translated_title": "TWIG：通过模拟KGE模型实现预先超参数优化和跨图泛化",
    "translated_abstract": "在本文中，我们介绍了一种名为TWIG（Topologically-Weighted Intelligence Generation）的新颖的、无需嵌入的模拟KGE输出的范式，它只使用了一小部分参数。TWIG从图数据的拓扑特征作为输入学习权重，没有对实体或边的潜在表示进行编码。我们在UMLS数据集上的实验结果表明，单个TWIG神经网络几乎可以准确预测所有超参数配置下最先进的ComplEx-N3 KGE模型的结果。为了达到这个目标，它只使用了2590个可学习参数，但准确预测了1215个不同超参数组合的结果，相当于29322000个参数的总成本。",
    "tldr": "这项研究引入了一种名为TWIG的新颖模型，可以通过拓扑特征学习权重来模拟KGE模型的输出，有效减少了参数数量，并具有预先优化超参数和跨图泛化的能力。",
    "en_tdlr": "This paper introduces a novel paradigm called TWIG that simulates the output of KGE models using topological features to learn weights, significantly reducing the number of parameters. TWIG accurately predicts the results of different hyperparameter configurations of state-of-the-art KGE models, demonstrating its capability of pre-hoc hyperparameter optimization and cross-graph generalization."
}