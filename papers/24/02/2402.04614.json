{
    "title": "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models",
    "abstract": "Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness",
    "link": "https://arxiv.org/abs/2402.04614",
    "context": "Title: Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models\nAbstract: Large Language Models (LLMs) are deployed as powerful tools for several natural language processing (NLP) applications. Recent works show that modern LLMs can generate self-explanations (SEs), which elicit their intermediate reasoning steps for explaining their behavior. Self-explanations have seen widespread adoption owing to their conversational and plausible nature. However, there is little to no understanding of their faithfulness. In this work, we discuss the dichotomy between faithfulness and plausibility in SEs generated by LLMs. We argue that while LLMs are adept at generating plausible explanations -- seemingly logical and coherent to human users -- these explanations do not necessarily align with the reasoning processes of the LLMs, raising concerns about their faithfulness. We highlight that the current trend towards increasing the plausibility of explanations, primarily driven by the demand for user-friendly interfaces, may come at the cost of diminishing their faithfulness",
    "path": "papers/24/02/2402.04614.json",
    "total_tokens": 899,
    "translated_title": "信实性与可信度: 关于大型语言模型解释的(不)可靠性",
    "translated_abstract": "大型语言模型(LLMs)被部署为几种自然语言处理（NLP）应用的强大工具。最近的研究显示，现代LLMs可以生成自我解释（SEs），这些SEs揭示了它们解释其行为的中间推理步骤。由于其对话性和可信度的特点，自我解释已广泛应用。然而，我们对其信实性了解甚少。在本研究中，我们讨论了LLMs生成的SEs中信实性和可信度之间的二分法。我们认为，虽然LLMs擅长生成可信的解释-对人类用户来说似乎逻辑和连贯-但这些解释未必与LLMs的推理过程相一致，引发对其信实性的担忧。我们强调，当前趋势是为了用户友好界面的需求而增加解释的可信度，可能会以降低解释的信实性为代价。",
    "tldr": "本文讨论了大型语言模型生成的自我解释中的信实性与可信度之间的差异。虽然这些解释在逻辑上是合乎情理且连贯的，但并不一定与模型的推理过程一致，引发对其信实性的担忧。",
    "en_tdlr": "This paper discusses the discrepancy between faithfulness and plausibility in self-explanations generated by large language models. While these explanations are logically and coherently plausible to human users, they may not necessarily align with the reasoning processes of the models, raising concerns about their faithfulness."
}