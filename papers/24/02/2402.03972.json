{
    "title": "Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning",
    "abstract": "Multi-agent deep reinforcement learning (MADRL) problems often encounter the challenge of sparse rewards. This challenge becomes even more pronounced when coordination among agents is necessary. As performance depends not only on one agent's behavior but rather on the joint behavior of multiple agents, finding an adequate solution becomes significantly harder. In this context, a group of agents can benefit from actively exploring different joint strategies in order to determine the most efficient one. In this paper, we propose an approach for rewarding strategies where agents collectively exhibit novel behaviors. We present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation method that follows the centralized learning with decentralized execution paradigm. JIM rewards joint trajectories based on a centralized measure of novelty designed to function in continuous environments. We demonstrate the strengths of this approach both in a synthetic environment designed to rev",
    "link": "https://arxiv.org/abs/2402.03972",
    "context": "Title: Joint Intrinsic Motivation for Coordinated Exploration in Multi-Agent Deep Reinforcement Learning\nAbstract: Multi-agent deep reinforcement learning (MADRL) problems often encounter the challenge of sparse rewards. This challenge becomes even more pronounced when coordination among agents is necessary. As performance depends not only on one agent's behavior but rather on the joint behavior of multiple agents, finding an adequate solution becomes significantly harder. In this context, a group of agents can benefit from actively exploring different joint strategies in order to determine the most efficient one. In this paper, we propose an approach for rewarding strategies where agents collectively exhibit novel behaviors. We present JIM (Joint Intrinsic Motivation), a multi-agent intrinsic motivation method that follows the centralized learning with decentralized execution paradigm. JIM rewards joint trajectories based on a centralized measure of novelty designed to function in continuous environments. We demonstrate the strengths of this approach both in a synthetic environment designed to rev",
    "path": "papers/24/02/2402.03972.json",
    "total_tokens": 890,
    "translated_title": "多智能体深度强化学习中的联合内在动机以促进协调探索",
    "translated_abstract": "多智能体深度强化学习(MADRL)问题常常面临稀疏奖励的挑战。当智能体之间需要协调时，这个挑战变得更加明显。由于性能不仅取决于一个智能体的行为，而是取决于多个智能体的联合行为，找到一个合适的解决方案变得更加困难。在这种情况下，一组智能体可以通过积极探索不同的联合策略来确定最有效的策略。在本文中，我们提出了一种奖励策略的方法，其中智能体共同展示新的行为。我们介绍了JIM(联合内在动机)，这是一种多智能体内在动机方法，遵循集中学习和分散执行的范式。JIM根据一个针对连续环境设计的集中测量的新颖性奖励联合轨迹。我们在一个合成环境中展示了这种方法的优势。",
    "tldr": "本文提出了一种用于解决多智能体深度强化学习中稀疏奖励和协调探索挑战的方法，通过联合内在动机奖励联合轨迹的新颖性行为，实现了多智能体之间的协作和最优性。"
}