{
    "title": "Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before",
    "abstract": "arXiv:2402.11816v1 Announce Type: cross  Abstract: Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-lea",
    "link": "https://arxiv.org/abs/2402.11816",
    "context": "Title: Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before\nAbstract: arXiv:2402.11816v1 Announce Type: cross  Abstract: Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-lea",
    "path": "papers/24/02/2402.11816.json",
    "total_tokens": 814,
    "translated_title": "避免对比学习中的特征抑制：学习以前未曾学到的内容",
    "translated_abstract": "自监督对比学习已经成为从未标记数据中获取高质量表示的强大方法。然而，最近在标准对比学习（如SimCLR、CLIP中）中发现了特征抑制：在单个端到端训练阶段，对比模型仅捕获对比观点之间的一部分共享信息，而忽略了其他潜在有用的信息。具有特征抑制，对比模型通常无法学习足够适用于各种下游任务的表示。为了减轻特征抑制问题并确保对比模型学习全面的表示，我们开发了一种新颖的多阶对比学习（MCL）框架。与通常会导致特征抑制的标准对比学习不同，MCL逐渐学习以前未探索过的新特征，同时保持已经学到的内容。",
    "tldr": "开发了一种多阶对比学习（MCL）框架，以解决对比学习中的特征抑制问题，并确保模型学习全面的表示。",
    "en_tdlr": "Developed a Multistage Contrastive Learning (MCL) framework to address feature suppression in contrastive learning and ensure comprehensive representation learning."
}