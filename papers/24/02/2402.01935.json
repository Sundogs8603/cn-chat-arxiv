{
    "title": "Code Representation Learning At Scale",
    "abstract": "Recent studies have shown that code language models at scale demonstrate significant performance gains on downstream tasks, i.e., code generation. However, most of the existing works on code representation learning train models at a hundred million parameter scale using very limited pretraining corpora. In this work, we fuel code representation learning with a vast amount of code data via a two-stage pretraining scheme. We first train the encoders via a mix that leverages both randomness in masking language modeling and the structure aspect of programming language. We then enhance the representations via contrastive learning with hard negative and hard positive constructed in an unsupervised manner. We establish an off-the-shelf encoder model that persistently outperforms the existing models on a wide variety of downstream tasks by large margins. To comprehend the factors contributing to successful code representation learning, we conduct detailed ablations and share our findings on (i",
    "link": "https://arxiv.org/abs/2402.01935",
    "context": "Title: Code Representation Learning At Scale\nAbstract: Recent studies have shown that code language models at scale demonstrate significant performance gains on downstream tasks, i.e., code generation. However, most of the existing works on code representation learning train models at a hundred million parameter scale using very limited pretraining corpora. In this work, we fuel code representation learning with a vast amount of code data via a two-stage pretraining scheme. We first train the encoders via a mix that leverages both randomness in masking language modeling and the structure aspect of programming language. We then enhance the representations via contrastive learning with hard negative and hard positive constructed in an unsupervised manner. We establish an off-the-shelf encoder model that persistently outperforms the existing models on a wide variety of downstream tasks by large margins. To comprehend the factors contributing to successful code representation learning, we conduct detailed ablations and share our findings on (i",
    "path": "papers/24/02/2402.01935.json",
    "total_tokens": 851,
    "translated_title": "在规模上进行代码表示学习",
    "translated_abstract": "最近的研究显示，规模上的代码语言模型在下游任务，例如代码生成方面表现出显著的性能提升。然而，目前大部分关于代码表示学习的研究只使用了非常有限的预训练语料库进行了一亿规模参数的模型训练。在这项工作中，我们通过两阶段预训练方案利用大量的代码数据推动代码表示学习。首先，我们通过混合使用语言建模中的随机屏蔽和编程语言结构方面的特点训练编码器。然后，我们通过对比学习以无监督的方式构建的困难负例和困难正例，增强表示。我们建立了一个现成的编码器模型，在各种下游任务上持续以较大的优势击败现有模型。为了理解成功的代码表示学习的因素，我们进行了详细的消融实验，并分享了我们的研究结果。",
    "tldr": "这项工作提出了一个利用大规模代码数据进行代码表示学习的两阶段预训练方案，通过混合语言建模和对比学习增强表示，建立了一个优于现有模型的编码器模型，在各种下游任务上大幅优化了性能。",
    "en_tdlr": "This work presents a two-stage pretraining scheme for code representation learning using a large-scale code dataset. By leveraging language modeling and contrastive learning, the established encoder model outperforms existing models by a large margin on various downstream tasks."
}