{
    "title": "Self-Supervised Contrastive Pre-Training for Multivariate Point Processes",
    "abstract": "Self-supervision is one of the hallmarks of representation learning in the increasingly popular suite of foundation models including large language models such as BERT and GPT-3, but it has not been pursued in the context of multivariate event streams, to the best of our knowledge. We introduce a new paradigm for self-supervised learning for multivariate point processes using a transformer encoder. Specifically, we design a novel pre-training strategy for the encoder where we not only mask random event epochs but also insert randomly sampled \"void\" epochs where an event does not occur; this differs from the typical discrete-time pretext tasks such as word-masking in BERT but expands the effectiveness of masking to better capture continuous-time dynamics. To improve downstream tasks, we introduce a contrasting module that compares real events to simulated void instances. The pre-trained model can subsequently be fine-tuned on a potentially much smaller event dataset, similar conceptuall",
    "link": "https://rss.arxiv.org/abs/2402.00987",
    "context": "Title: Self-Supervised Contrastive Pre-Training for Multivariate Point Processes\nAbstract: Self-supervision is one of the hallmarks of representation learning in the increasingly popular suite of foundation models including large language models such as BERT and GPT-3, but it has not been pursued in the context of multivariate event streams, to the best of our knowledge. We introduce a new paradigm for self-supervised learning for multivariate point processes using a transformer encoder. Specifically, we design a novel pre-training strategy for the encoder where we not only mask random event epochs but also insert randomly sampled \"void\" epochs where an event does not occur; this differs from the typical discrete-time pretext tasks such as word-masking in BERT but expands the effectiveness of masking to better capture continuous-time dynamics. To improve downstream tasks, we introduce a contrasting module that compares real events to simulated void instances. The pre-trained model can subsequently be fine-tuned on a potentially much smaller event dataset, similar conceptuall",
    "path": "papers/24/02/2402.00987.json",
    "total_tokens": 899,
    "translated_title": "自监督对比预训练在多变量事件流中的应用",
    "translated_abstract": "自监督学习是表示学习的一个重要特点，在包括BERT和GPT-3等大型语言模型在内的基础模型中越来越受欢迎，但据我们所知，在多变量事件流的背景下尚未被追求。我们引入了一种新的自监督学习范式，使用变压器编码器对多变量事件流进行预训练。具体而言，我们设计了一种新颖的编码器预训练策略，不仅遮盖了随机事件时段，还插入了随机抽样的“空白”时段，即事件不发生的时段；这与BERT中的词遮盖等典型离散时间预训练任务不同，扩展了遮盖的有效性，以更好地捕捉连续时间动态。为了改进下游任务，我们引入了一个对比模块，将真实事件与模拟的空白实例进行比较。预训练模型随后可以在可能更小的事件数据集上进行微调，类似于概念上的提升。",
    "tldr": "在多变量事件流中，我们提出了一种新的自监督学习范式，使用变压器编码器进行预训练，并引入对比模块来比较真实事件和模拟的空白实例，以提高后续任务的性能。",
    "en_tdlr": "In the context of multivariate event streams, we propose a new paradigm for self-supervised learning using a transformer encoder for pre-training, and introduce a contrasting module to compare real events with simulated void instances for improved downstream tasks."
}