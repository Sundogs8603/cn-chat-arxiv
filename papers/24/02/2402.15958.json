{
    "title": "On the dynamics of three-layer neural networks: initial condensation",
    "abstract": "arXiv:2402.15958v1 Announce Type: new  Abstract: Empirical and theoretical works show that the input weights of two-layer neural networks, when initialized with small values, converge towards isolated orientations. This phenomenon, referred to as condensation, indicates that the gradient descent methods tend to spontaneously reduce the complexity of neural networks during the training process. In this work, we elucidate the mechanisms behind the condensation phenomena occurring in the training of three-layer neural networks and distinguish it from the training of two-layer neural networks. Through rigorous theoretical analysis, we establish the blow-up property of effective dynamics and present a sufficient condition for the occurrence of condensation, findings that are substantiated by experimental results. Additionally, we explore the association between condensation and the low-rank bias observed in deep matrix factorization.",
    "link": "https://arxiv.org/abs/2402.15958",
    "context": "Title: On the dynamics of three-layer neural networks: initial condensation\nAbstract: arXiv:2402.15958v1 Announce Type: new  Abstract: Empirical and theoretical works show that the input weights of two-layer neural networks, when initialized with small values, converge towards isolated orientations. This phenomenon, referred to as condensation, indicates that the gradient descent methods tend to spontaneously reduce the complexity of neural networks during the training process. In this work, we elucidate the mechanisms behind the condensation phenomena occurring in the training of three-layer neural networks and distinguish it from the training of two-layer neural networks. Through rigorous theoretical analysis, we establish the blow-up property of effective dynamics and present a sufficient condition for the occurrence of condensation, findings that are substantiated by experimental results. Additionally, we explore the association between condensation and the low-rank bias observed in deep matrix factorization.",
    "path": "papers/24/02/2402.15958.json",
    "total_tokens": 839,
    "translated_title": "论三层神经网络动力学：初始凝聚",
    "translated_abstract": "经验和理论研究显示，当初始化为小值时，两层神经网络的输入权重会收敛到孤立的方向。这种现象被称为凝聚，表明梯度下降方法在训练过程中往往会自发地减少神经网络的复杂性。 在这项研究中，我们阐明了三层神经网络训练中出现的凝聚现象背后的机制，并将其与两层神经网络的训练进行区分。 通过严格的理论分析，我们建立了有效动力学的爆炸性质，并提出了发生凝聚的充分条件，这些发现得到了实验结果的证实。此外，我们还探讨了凝聚与深度矩阵因子分解中观察到的低秩偏差之间的关联。",
    "tldr": "深入研究三层神经网络训练中的凝聚现象和梯度下降方法自发减少神经网络复杂性的机制，提出有效动力学的爆炸性质和凝聚发生的充分条件，并通过实验证实了这些发现。",
    "en_tdlr": "This work delves into the condensation phenomenon in the training of three-layer neural networks and the spontaneous reduction of network complexity by gradient descent methods, establishing the blow-up property of effective dynamics, presenting a sufficient condition for condensation, all substantiated by experimental results."
}