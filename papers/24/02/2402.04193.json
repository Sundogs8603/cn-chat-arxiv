{
    "title": "Gradient Coding in Decentralized Learning for Evading Stragglers",
    "abstract": "In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.",
    "link": "https://arxiv.org/abs/2402.04193",
    "context": "Title: Gradient Coding in Decentralized Learning for Evading Stragglers\nAbstract: In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.",
    "path": "papers/24/02/2402.04193.json",
    "total_tokens": 837,
    "translated_title": "避免延迟节点的分散式学习中的梯度编码",
    "translated_abstract": "本文考虑了存在延迟节点的分散式学习问题。尽管梯度编码技术已经被开发用于分布式学习以避免延迟节点，即设备使用冗余训练数据发送编码梯度，但是将这些技术直接应用于分散式学习场景是困难的。为了解决这个问题，我们提出了一种新的基于八卦的梯度编码分散式学习方法（GOCO）。在这种方法中，为了避免延迟节点的负面影响，参数向量使用基于随机梯度编码框架的编码梯度进行本地更新，然后以八卦方式进行平均。我们分析了GOCO在强凸损失函数下的收敛性能，并通过仿真结果证明了该方法在学习性能上相对于基准方法的优越性。",
    "tldr": "本文提出了一种新的基于八卦的梯度编码分散式学习方法（GOCO），以解决存在延迟节点的分散式学习问题。与基准方法相比，该方法在学习性能上具有优越性。",
    "en_tdlr": "This paper proposes a new gossip-based decentralized learning method with gradient coding (GOCO) to address the problem of stragglers in decentralized learning. The method outperforms baseline methods in terms of learning performance."
}