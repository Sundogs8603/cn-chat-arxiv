{
    "title": "Regularized boosting with an increasing coefficient magnitude stop criterion as meta-learner in hyperparameter optimization stacking ensemble",
    "abstract": "In Hyperparameter Optimization (HPO), only the hyperparameter configuration with the best performance is chosen after performing several trials, then, discarding the effort of training all the models with every hyperparameter configuration trial and performing an ensemble of all them. This ensemble consists of simply averaging the model predictions or weighting the models by a certain probability. Recently, other more sophisticated ensemble strategies, such as the Caruana method or the stacking strategy has been proposed. On the one hand, the Caruana method performs well in HPO ensemble, since it is not affected by the effects of multicollinearity, which is prevalent in HPO. It just computes the average over a subset of predictions with replacement. But it does not benefit from the generalization power of a learning process. On the other hand, stacking methods include a learning procedure since a meta-learner is required to perform the ensemble. Yet, one hardly finds advice about which",
    "link": "https://rss.arxiv.org/abs/2402.01379",
    "context": "Title: Regularized boosting with an increasing coefficient magnitude stop criterion as meta-learner in hyperparameter optimization stacking ensemble\nAbstract: In Hyperparameter Optimization (HPO), only the hyperparameter configuration with the best performance is chosen after performing several trials, then, discarding the effort of training all the models with every hyperparameter configuration trial and performing an ensemble of all them. This ensemble consists of simply averaging the model predictions or weighting the models by a certain probability. Recently, other more sophisticated ensemble strategies, such as the Caruana method or the stacking strategy has been proposed. On the one hand, the Caruana method performs well in HPO ensemble, since it is not affected by the effects of multicollinearity, which is prevalent in HPO. It just computes the average over a subset of predictions with replacement. But it does not benefit from the generalization power of a learning process. On the other hand, stacking methods include a learning procedure since a meta-learner is required to perform the ensemble. Yet, one hardly finds advice about which",
    "path": "papers/24/02/2402.01379.json",
    "total_tokens": 680,
    "translated_title": "使用增大系数幅度停止准则的正则化增强作为超参数优化集成中的元学习器",
    "translated_abstract": "在超参数优化中，通过多次试验选择性能最佳的超参数配置，并将所有模型进行集成。最近，提出了更复杂的集成策略，如Caruana方法和stacking策略。Caruana方法在超参数优化集成中表现良好，不受多重共线性的影响。而stacking方法需要使用元学习器进行集成。然而，很少有关于如何选择元学习器的建议。",
    "tldr": "本论文提出了一种使用增大系数幅度停止准则的正则化增强作为超参数优化集成中的元学习器的方法。",
    "en_tdlr": "This paper proposes a method that uses regularized boosting with an increasing coefficient magnitude stop criterion as a meta-learner in hyperparameter optimization stacking ensemble."
}