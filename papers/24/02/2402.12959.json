{
    "title": "Prompt Stealing Attacks Against Large Language Models",
    "abstract": "arXiv:2402.12959v1 Announce Type: cross  Abstract: The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries",
    "link": "https://arxiv.org/abs/2402.12959",
    "context": "Title: Prompt Stealing Attacks Against Large Language Models\nAbstract: arXiv:2402.12959v1 Announce Type: cross  Abstract: The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries",
    "path": "papers/24/02/2402.12959.json",
    "total_tokens": 652,
    "translated_title": "大语言模型的提示窃取攻击",
    "translated_abstract": "越来越多领域对大语言模型（LLMs）如ChatGPT的依赖强调了“提示工程”的重要性，这是一种改进模型输出质量的技术。本文提出了一种针对LLMs的新型攻击，称为提示窃取攻击。我们的提示窃取攻击旨在基于生成的答案来窃取这些设计良好的提示。提示窃取攻击包括两个主要模块：参数提取器和提示重构。",
    "tldr": "提出了一种新型攻击，即提示窃取攻击，目的是基于生成的答案窃取设计良好的提示。"
}