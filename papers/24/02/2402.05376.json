{
    "title": "Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks and exhibited impressive reasoning abilities by applying zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature of sentence prefixes during the pre-training phase, existing zero-shot CoT prompting methods that employ identical CoT prompting across all task instances may not be optimal. In this paper, we introduce a novel zero-shot prompting method that leverages evolutionary algorithms to generate diverse promptings for LLMs dynamically. Our approach involves initializing two CoT promptings, performing evolutionary operations based on LLMs to create a varied set, and utilizing the LLMs to select a suitable CoT prompting for a given problem. Additionally, a rewriting operation, guided by the selected CoT prompting, enhances the understanding of the LLMs about the problem. Extensive experiments conducted across ten reasoning datasets demonstrate the superior performance of ",
    "link": "https://arxiv.org/abs/2402.05376",
    "context": "Title: Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models\nAbstract: Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks and exhibited impressive reasoning abilities by applying zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature of sentence prefixes during the pre-training phase, existing zero-shot CoT prompting methods that employ identical CoT prompting across all task instances may not be optimal. In this paper, we introduce a novel zero-shot prompting method that leverages evolutionary algorithms to generate diverse promptings for LLMs dynamically. Our approach involves initializing two CoT promptings, performing evolutionary operations based on LLMs to create a varied set, and utilizing the LLMs to select a suitable CoT prompting for a given problem. Additionally, a rewriting operation, guided by the selected CoT prompting, enhances the understanding of the LLMs about the problem. Extensive experiments conducted across ten reasoning datasets demonstrate the superior performance of ",
    "path": "papers/24/02/2402.05376.json",
    "total_tokens": 865,
    "translated_title": "在大型语言模型中使用进化算法引导的零样本思维链推理",
    "translated_abstract": "大型语言模型（LLM）通过应用零样本思维链推理展示了出色的性能，并展现了令人印象深刻的推理能力。然而，由于预训练阶段中句子前缀的演化性质，现有的零样本思维链推理方法无法在所有任务实例上都采用相同的推理方式，可能不够优化。在本文中，我们引入了一种新的零样本推理方法，利用进化算法动态生成LLM的多样推理方式。我们的方法涉及初始化两个思维链推理方式，基于LLM进行进化操作以生成多样集合，并利用LLM选择适合给定问题的思维链推理方式。此外，通过选定的思维链推理方式引导的重写操作增强LLM对问题的理解。通过对十个推理数据集进行大量实验，证明了我们方法的卓越性能。",
    "tldr": "本论文提出了一种在大型语言模型中使用进化算法引导的零样本思维链推理的方法。通过动态生成多样的推理方式，并通过重写操作增强模型对问题的理解，我们的方法在十个推理数据集上展现出了卓越的性能。",
    "en_tdlr": "This paper introduces a novel method of zero-shot Chain-of-Thought reasoning guided by evolutionary algorithms in large language models (LLMs). By dynamically generating diverse promptings and enhancing model understanding through rewriting operations, our approach demonstrates superior performance across ten reasoning datasets."
}