{
    "title": "Explaining Kernel Clustering via Decision Trees",
    "abstract": "arXiv:2402.09881v1 Announce Type: new  Abstract: Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods. Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees. However, interpretable variants of k-means have limited applicability in practice, where more flexible clustering methods are often needed to obtain useful partitions of the data. In this work, we investigate interpretable kernel clustering, and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means. We further build on previous work on explainable k-means and demonstrate how a suitable choice of features allows preserving interpretability without sacrificing approximation guarantees on the interpret",
    "link": "https://arxiv.org/abs/2402.09881",
    "context": "Title: Explaining Kernel Clustering via Decision Trees\nAbstract: arXiv:2402.09881v1 Announce Type: new  Abstract: Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods. Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees. However, interpretable variants of k-means have limited applicability in practice, where more flexible clustering methods are often needed to obtain useful partitions of the data. In this work, we investigate interpretable kernel clustering, and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means. We further build on previous work on explainable k-means and demonstrate how a suitable choice of features allows preserving interpretability without sacrificing approximation guarantees on the interpret",
    "path": "papers/24/02/2402.09881.json",
    "total_tokens": 830,
    "translated_title": "通过决策树解释核聚类",
    "translated_abstract": "尽管可解释和可解释的机器学习越来越受欢迎，但关于固有可解释聚类方法的工作仍然非常有限。最近，解释经典k-means算法的兴趣激增，导致了使用轴对齐决策树近似k-means聚类的高效算法。然而，可解释的k-means变种在实践中的适用性有限，通常需要更灵活的聚类方法才能获得有用的数据分区。在本研究中，我们研究了可解释的核聚类，并提出了构建决策树来近似kernel k-means引导分区的算法，kernel k-means是k-means的非线性扩展。我们进一步借鉴了关于可解释k-means的先前工作，并展示了如何通过合适的特征选择在不损失解释能力的情况下保持近似保证。",
    "tldr": "这项工作探讨了可解释的核聚类方法，提出了使用决策树近似核k-means聚类分区的算法，并通过合适的特征选择实现了解释性和近似保证的平衡。",
    "en_tdlr": "This work explores interpretable kernel clustering methods and proposes algorithms that use decision trees to approximate partitions in kernel k-means clustering, achieving a balance between interpretability and approximation guarantees through appropriate feature selection."
}