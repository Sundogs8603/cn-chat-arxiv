{
    "title": "Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints",
    "abstract": "We study the problem of multi-agent reinforcement learning (MARL) with adaptivity constraints -- a new problem motivated by real-world applications where deployments of new policies are costly and the number of policy updates must be minimized. For two-player zero-sum Markov Games, we design a (policy) elimination based algorithm that achieves a regret of $\\widetilde{O}(\\sqrt{H^3 S^2 ABK})$, while the batch complexity is only $O(H+\\log\\log K)$. In the above, $S$ denotes the number of states, $A,B$ are the number of actions for the two players respectively, $H$ is the horizon and $K$ is the number of episodes. Furthermore, we prove a batch complexity lower bound $\\Omega(\\frac{H}{\\log_{A}K}+\\log\\log K)$ for all algorithms with $\\widetilde{O}(\\sqrt{K})$ regret bound, which matches our upper bound up to logarithmic factors. As a byproduct, our techniques naturally extend to learning bandit games and reward-free MARL within near optimal batch complexity. To the best of our knowledge, these ",
    "link": "https://rss.arxiv.org/abs/2402.01111",
    "context": "Title: Near-Optimal Reinforcement Learning with Self-Play under Adaptivity Constraints\nAbstract: We study the problem of multi-agent reinforcement learning (MARL) with adaptivity constraints -- a new problem motivated by real-world applications where deployments of new policies are costly and the number of policy updates must be minimized. For two-player zero-sum Markov Games, we design a (policy) elimination based algorithm that achieves a regret of $\\widetilde{O}(\\sqrt{H^3 S^2 ABK})$, while the batch complexity is only $O(H+\\log\\log K)$. In the above, $S$ denotes the number of states, $A,B$ are the number of actions for the two players respectively, $H$ is the horizon and $K$ is the number of episodes. Furthermore, we prove a batch complexity lower bound $\\Omega(\\frac{H}{\\log_{A}K}+\\log\\log K)$ for all algorithms with $\\widetilde{O}(\\sqrt{K})$ regret bound, which matches our upper bound up to logarithmic factors. As a byproduct, our techniques naturally extend to learning bandit games and reward-free MARL within near optimal batch complexity. To the best of our knowledge, these ",
    "path": "papers/24/02/2402.01111.json",
    "total_tokens": 1120,
    "translated_title": "在自适应约束下的自对弈强化学习中的近乎最优解",
    "translated_abstract": "我们研究了具有自适应约束的多智能体强化学习问题（MARL） - 这是一种由实际应用驱动的新问题，其中部署新策略是昂贵的，并且必须最小化策略更新的次数。对于两个玩家的零和马尔可夫博弈，我们设计了一种（策略）基于消除的算法，它在后悔为$\\widetilde{O}(\\sqrt{H^3 S^2 ABK})$的情况下，批量复杂度仅为$O(H+\\log\\log K)$。在上述情况下，$S$表示状态数，$A，B$分别代表两个玩家的行动数，$H$是时间周期，$K$是游戏次数。此外，我们证明了对于所有具有$\\widetilde{O}(\\sqrt{K})$后悔界的算法，一种批量复杂度的下界为$\\Omega(\\frac{H}{\\log_{A}K}+\\log\\log K)$，这与我们的上界在对数因子上匹配。作为副产品，我们的技术自然地扩展到学习赌博博弈和无奖励的近乎最优批量复杂度MARL。据我们所知，这些是迄今为止最好的成果。",
    "tldr": "本文研究了具有自适应约束的多智能体强化学习问题，并提出了一种基于消除算法，将后悔控制在$\\widetilde{O}(\\sqrt{H^3 S^2 ABK})$，批量复杂度为$O(H+\\log\\log K)$。此外，还给出了所有具有$\\widetilde{O}(\\sqrt{K})$后悔界算法的批量复杂度下界。",
    "en_tdlr": "This paper investigates the problem of multi-agent reinforcement learning with adaptivity constraints and proposes an elimination based algorithm that achieves regret of $\\widetilde{O}(\\sqrt{H^3 S^2 ABK})$ and batch complexity of $O(H+\\log\\log K)$. Additionally, a lower bound for batch complexity of algorithms with $\\widetilde{O}(\\sqrt{K})$ regret bound is provided."
}