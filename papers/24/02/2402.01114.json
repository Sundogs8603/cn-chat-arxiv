{
    "title": "Double-Dip: Thwarting Label-Only Membership Inference Attacks with Transfer Learning and Randomization",
    "abstract": "Transfer learning (TL) has been demonstrated to improve DNN model performance when faced with a scarcity of training samples. However, the suitability of TL as a solution to reduce vulnerability of overfitted DNNs to privacy attacks is unexplored. A class of privacy attacks called membership inference attacks (MIAs) aim to determine whether a given sample belongs to the training dataset (member) or not (nonmember). We introduce Double-Dip, a systematic empirical study investigating the use of TL (Stage-1) combined with randomization (Stage-2) to thwart MIAs on overfitted DNNs without degrading classification accuracy. Our study examines the roles of shared feature space and parameter values between source and target models, number of frozen layers, and complexity of pretrained models. We evaluate Double-Dip on three (Target, Source) dataset paris: (i) (CIFAR-10, ImageNet), (ii) (GTSRB, ImageNet), (iii) (CelebA, VGGFace2). We consider four publicly available pretrained DNNs: (a) VGG-19,",
    "link": "https://rss.arxiv.org/abs/2402.01114",
    "context": "Title: Double-Dip: Thwarting Label-Only Membership Inference Attacks with Transfer Learning and Randomization\nAbstract: Transfer learning (TL) has been demonstrated to improve DNN model performance when faced with a scarcity of training samples. However, the suitability of TL as a solution to reduce vulnerability of overfitted DNNs to privacy attacks is unexplored. A class of privacy attacks called membership inference attacks (MIAs) aim to determine whether a given sample belongs to the training dataset (member) or not (nonmember). We introduce Double-Dip, a systematic empirical study investigating the use of TL (Stage-1) combined with randomization (Stage-2) to thwart MIAs on overfitted DNNs without degrading classification accuracy. Our study examines the roles of shared feature space and parameter values between source and target models, number of frozen layers, and complexity of pretrained models. We evaluate Double-Dip on three (Target, Source) dataset paris: (i) (CIFAR-10, ImageNet), (ii) (GTSRB, ImageNet), (iii) (CelebA, VGGFace2). We consider four publicly available pretrained DNNs: (a) VGG-19,",
    "path": "papers/24/02/2402.01114.json",
    "total_tokens": 1043,
    "translated_title": "双重防御：使用迁移学习和随机化来防止仅基于标签的成员推断攻击",
    "translated_abstract": "迁移学习已经被证明在面对训练样本稀缺的情况下，可以提高深度神经网络模型的性能。然而，将迁移学习作为解决过拟合深度神经网络容易受到隐私攻击的方法的合适性尚未被探索。一类隐私攻击称为成员推断攻击旨在确定给定样本是否属于训练数据集（成员）或不属于（非成员）。我们引入了双重防御，一个系统性的实证研究，研究在不降低分类准确度的情况下使用迁移学习（阶段1）结合随机化（阶段2）来防止成员推断攻击对过拟合的深度神经网络的影响。我们的研究考察了源模型和目标模型之间的共享特征空间和参数值、冻结层的数量以及预训练模型的复杂性。我们在三个（目标，源）数据集对上评估了双重防御：（i）（CIFAR-10，ImageNet），（ii）（GTSRB，ImageNet），（iii）（CelebA，VGGFace2）。我们考虑了四个公开可用的预训练深度神经网络：（a）VGG-19，",
    "tldr": "这项研究探索了使用迁移学习和随机化来防止成员推断攻击。通过在过拟合的深度神经网络上应用双重防御，我们可以提高模型的隐私性而不降低准确度。",
    "en_tdlr": "This study investigates the use of transfer learning and randomization to thwart membership inference attacks. By applying the Double-Dip defense on overfitted deep neural networks, we can improve model privacy without compromising accuracy."
}