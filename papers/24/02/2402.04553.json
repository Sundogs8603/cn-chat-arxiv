{
    "title": "Curvature-Informed SGD via General Purpose Lie-Group Preconditioners",
    "abstract": "We present a novel approach to accelerate stochastic gradient descent (SGD) by utilizing curvature information obtained from Hessian-vector products or finite differences of parameters and gradients, similar to the BFGS algorithm. Our approach involves two preconditioners: a matrix-free preconditioner and a low-rank approximation preconditioner. We update both preconditioners online using a criterion that is robust to stochastic gradient noise and does not require line search or damping. To preserve the corresponding symmetry or invariance, our preconditioners are constrained to certain connected Lie groups. The Lie group's equivariance property simplifies the preconditioner fitting process, while its invariance property eliminates the need for damping, which is commonly required in second-order optimizers. As a result, the learning rate for parameter updating and the step size for preconditioner fitting are naturally normalized, and their default values work well in most scenarios. Ou",
    "link": "https://arxiv.org/abs/2402.04553",
    "context": "Title: Curvature-Informed SGD via General Purpose Lie-Group Preconditioners\nAbstract: We present a novel approach to accelerate stochastic gradient descent (SGD) by utilizing curvature information obtained from Hessian-vector products or finite differences of parameters and gradients, similar to the BFGS algorithm. Our approach involves two preconditioners: a matrix-free preconditioner and a low-rank approximation preconditioner. We update both preconditioners online using a criterion that is robust to stochastic gradient noise and does not require line search or damping. To preserve the corresponding symmetry or invariance, our preconditioners are constrained to certain connected Lie groups. The Lie group's equivariance property simplifies the preconditioner fitting process, while its invariance property eliminates the need for damping, which is commonly required in second-order optimizers. As a result, the learning rate for parameter updating and the step size for preconditioner fitting are naturally normalized, and their default values work well in most scenarios. Ou",
    "path": "papers/24/02/2402.04553.json",
    "total_tokens": 947,
    "translated_title": "通过通用李群预处理器的曲率信息优化SGD算法",
    "translated_abstract": "我们提出了一种新的方法来加速随机梯度下降（SGD）算法，该方法利用从黑塞矩阵-向量乘积或参数和梯度的有限差分中获得的曲率信息，类似于BFGS算法。我们的方法涉及两个预处理器：一个无矩阵预处理器和一个低秩近似预处理器。我们使用一种对随机梯度噪声具有鲁棒性且不需要线性搜索或阻尼的准则在线更新两个预处理器。为了保持相应的对称性或不变性，我们的预处理器受限于特定的连通李群。李群的等变性质简化了预处理器的拟合过程，而它的不变性质消除了在二阶优化器中普遍需要的阻尼，从而使参数更新的学习率和预处理器拟合的步长自然地被归一化，并且它们的默认值在大多数情况下效果良好。",
    "tldr": "本研究提出了一种通过利用曲率信息优化随机梯度下降（SGD）算法的新方法。该方法使用无矩阵预处理器和低秩近似预处理器，并在线更新这两种预处理器。通过对称性和不变性的约束，该方法消除了阻尼的需求，使得学习率和步长可以自然归一化，并且在大多数情况下默认值效果良好。",
    "en_tdlr": "This paper presents a novel approach to optimize stochastic gradient descent (SGD) by utilizing curvature information, using matrix-free and low-rank approximation preconditioners. The approach updates the preconditioners online and eliminates the need for damping, resulting in normalized learning rate and step size that work well in most scenarios."
}