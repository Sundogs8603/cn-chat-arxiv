{
    "title": "Tackling Negative Transfer on Graphs",
    "abstract": "arXiv:2402.08907v1 Announce Type: cross Abstract: Transfer learning aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks. However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative transfer. In this paper, we investigate the negative transfer in graph transfer learning, which is important yet underexplored. We reveal that, unlike image or text, negative transfer commonly occurs in graph-structured data, even when source and target graphs share semantic similarities. Specifically, we identify that structural differences significantly amplify the dissimilarities in the node embeddings across graphs. To mitigate this, we bring a new insight: for semantically similar graphs, although structural differences lead to significant distribution shift in node embeddings, their impact on subgraph embeddings could be marginal. Building on this insight, we introduce tw",
    "link": "https://arxiv.org/abs/2402.08907",
    "context": "Title: Tackling Negative Transfer on Graphs\nAbstract: arXiv:2402.08907v1 Announce Type: cross Abstract: Transfer learning aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks. However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative transfer. In this paper, we investigate the negative transfer in graph transfer learning, which is important yet underexplored. We reveal that, unlike image or text, negative transfer commonly occurs in graph-structured data, even when source and target graphs share semantic similarities. Specifically, we identify that structural differences significantly amplify the dissimilarities in the node embeddings across graphs. To mitigate this, we bring a new insight: for semantically similar graphs, although structural differences lead to significant distribution shift in node embeddings, their impact on subgraph embeddings could be marginal. Building on this insight, we introduce tw",
    "path": "papers/24/02/2402.08907.json",
    "total_tokens": 842,
    "translated_title": "解决图上的负迁移问题",
    "translated_abstract": "迁移学习旨在通过利用从其他相关任务中学到的知识来提高目标任务上的学习效果。然而，当源任务和目标任务之间关系不密切时，学习性能可能会受到不利影响，这种现象被称为负迁移。本文研究了在图迁移学习中的负迁移问题，这是一个重要但尚未深入研究的领域。我们发现，在图结构数据中，与图像或文本不同，负迁移经常发生，即使源图和目标图在语义上有相似之处。具体来说，我们发现结构差异会大大增强图中节点嵌入之间的差异。为了缓解这个问题，我们带来了一个新的观点：对于语义相似的图，尽管结构差异会导致节点嵌入的分布差异，但它们对子图嵌入的影响可能较小。基于这个观点，我们引入了tw",
    "tldr": "图迁移学习中的负迁移现象尚未得到充分研究，本文发现在图结构数据中负迁移普遍存在，即使源图和目标图在语义上相似。我们提出了一个新的观点，对于语义相似的图，结构差异对子图嵌入的影响较小。"
}