{
    "title": "Evaluating Large Language Models for Generalization and Robustness via Data Compression",
    "abstract": "Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such a",
    "link": "https://arxiv.org/abs/2402.00861",
    "context": "Title: Evaluating Large Language Models for Generalization and Robustness via Data Compression\nAbstract: Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such a",
    "path": "papers/24/02/2402.00861.json",
    "total_tokens": 857,
    "translated_title": "通过数据压缩评估大型语言模型的泛化性和鲁棒性",
    "translated_abstract": "现有的大型语言模型评估方法面临数据污染、对提示敏感以及基准测试创建成本高等挑战。为了解决这些问题，我们提出了一种基于无损数据压缩的评估方法，测试模型的预测能力在其训练截止日期之后的泛化情况。具体而言，我们收集了从2017年到2023年共83个月的全面测试数据，并根据模型的训练数据截止日期将数据分为训练和测试期。我们使用两个指标进行评估：1）测试期的压缩性能作为对未见数据的泛化能力的衡量；2）训练期和测试期之间的性能差距作为鲁棒性的衡量。我们的实验证明，许多模型的压缩率在截止日期之后显著降低，但像... (内容过长，省略)",
    "tldr": "通过数据压缩评估语言模型的泛化性和鲁棒性。使用无损数据压缩方法，对训练截止日期之后的未见数据进行测试。测试结果表明，很多模型在截止日期之后的压缩率显著降低。",
    "en_tdlr": "Evaluating the generalization and robustness of language models through data compression after their training cutoff. The compression rate of many models significantly decreases after their cutoff date."
}