{
    "title": "Glocal Hypergradient Estimation with Koopman Operator",
    "abstract": "Gradient-based hyperparameter optimization methods update hyperparameters using hypergradients, gradients of a meta criterion with respect to hyperparameters. Previous research used two distinct update strategies: optimizing hyperparameters using global hypergradients obtained after completing model training or local hypergradients derived after every few model updates. While global hypergradients offer reliability, their computational cost is significant; conversely, local hypergradients provide speed but are often suboptimal. In this paper, we propose glocal hypergradient estimation, blending \"global\" quality with \"local\" efficiency. To this end, we use the Koopman operator theory to linearize the dynamics of hypergradients so that the global hypergradients can be efficiently approximated only by using a trajectory of local hypergradients. Consequently, we can optimize hyperparameters greedily using estimated global hypergradients, achieving both reliability and efficiency simultaneo",
    "link": "https://arxiv.org/abs/2402.02741",
    "context": "Title: Glocal Hypergradient Estimation with Koopman Operator\nAbstract: Gradient-based hyperparameter optimization methods update hyperparameters using hypergradients, gradients of a meta criterion with respect to hyperparameters. Previous research used two distinct update strategies: optimizing hyperparameters using global hypergradients obtained after completing model training or local hypergradients derived after every few model updates. While global hypergradients offer reliability, their computational cost is significant; conversely, local hypergradients provide speed but are often suboptimal. In this paper, we propose glocal hypergradient estimation, blending \"global\" quality with \"local\" efficiency. To this end, we use the Koopman operator theory to linearize the dynamics of hypergradients so that the global hypergradients can be efficiently approximated only by using a trajectory of local hypergradients. Consequently, we can optimize hyperparameters greedily using estimated global hypergradients, achieving both reliability and efficiency simultaneo",
    "path": "papers/24/02/2402.02741.json",
    "total_tokens": 873,
    "translated_title": "具有Koopman算子的全局超梯度估计",
    "translated_abstract": "基于梯度的超参数优化方法使用超梯度来更新超参数，即元标准的梯度与超参数的关系。先前的研究使用两种不同的更新策略：一种是使用模型训练完成后得到的全局超梯度来优化超参数，另一种是使用每个模型更新之后得到的局部超梯度。虽然全局超梯度具有可靠性，但计算成本显著；相反，局部超梯度速度快但常常不是最优的。在本文中，我们提出了glocal超梯度估计，将“全局”的质量与“局部”的效率结合起来。为此，我们使用Koopman算子理论来线性化超梯度的动态，以便可以仅通过使用局部超梯度的轨迹来高效地近似全局超梯度。因此，我们可以使用估计的全局超梯度贪婪地优化超参数，同时实现可靠性和效率。",
    "tldr": "本文提出了一种具有Koopman算子的全局超梯度估计方法，通过使用局部超梯度的轨迹来高效地近似全局超梯度，实现了超参数的贪婪优化，兼具可靠性和效率。"
}