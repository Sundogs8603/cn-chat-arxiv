{
    "title": "KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion",
    "abstract": "Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC. They can be categorized into two main classes: triple-based and text-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced entity distributions. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate these limitations, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. It alleviates the long-tail problem without incurring additional training overhead. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. Empirical results on benchmark datasets demonstrate the effectiven",
    "link": "https://arxiv.org/abs/2402.02389",
    "context": "Title: KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion\nAbstract: Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC. They can be categorized into two main classes: triple-based and text-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced entity distributions. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate these limitations, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. It alleviates the long-tail problem without incurring additional training overhead. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. Empirical results on benchmark datasets demonstrate the effectiven",
    "path": "papers/24/02/2402.02389.json",
    "total_tokens": 936,
    "translated_title": "KICGPT: 具备上下文知识的大型语言模型用于知识图谱补全",
    "translated_abstract": "知识图谱补全对于解决知识图谱不完整性和支持下游应用至关重要。已经提出了许多用于知识图谱补全的模型，它们可以分为基于三元组和基于文本的方法两类。基于三元组的方法由于结构信息有限和实体分布不均衡而困难重重。基于文本的方法可以缓解这个问题，但需要昂贵的语言模型训练和特定的知识图谱微调，从而限制了其效率。为了解决这些限制，本文提出了KICGPT，一种集成了大型语言模型(LLM)和基于三元组的知识图谱补全检索器的框架。它可以缓解长尾问题，而不会增加额外的训练开销。KICGPT使用了一种上下文学习策略，称为知识提示，它将结构知识编码为演示，以引导LLM的学习。在基准数据集上的实证结果证明了其有效性。",
    "tldr": "本文提出了KICGPT，它是一个集成了大型语言模型和基于三元组的知识图谱补全检索器的框架。它通过知识提示的上下文学习策略，缓解了长尾问题，并且无需额外的训练开销。实验证明了其有效性。",
    "en_tdlr": "This paper proposes KICGPT, a framework that combines a large language model and a triple-based knowledge graph completion retriever. It addresses the long-tail problem using a context learning strategy called Knowledge Prompt, without incurring additional training overhead. Empirical results demonstrate its effectiveness."
}