{
    "title": "Efficient Unbiased Sparsification",
    "abstract": "arXiv:2402.14925v1 Announce Type: cross  Abstract: An unbiased $m$-sparsification of a vector $p\\in \\mathbb{R}^n$ is a random vector $Q\\in \\mathbb{R}^n$ with mean $p$ that has at most $m<n$ nonzero coordinates. Unbiased sparsification compresses the original vector without introducing bias; it arises in various contexts, such as in federated learning and sampling sparse probability distributions. Ideally, unbiased sparsification should also minimize the expected value of a divergence function $\\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If $Q$ is optimal in this sense, then we call it efficient. Our main results describe efficient unbiased sparsifications for divergences that are either permutation-invariant or additively separable. Surprisingly, the characterization for permutation-invariant divergences is robust to the choice of divergence function, in the sense that our class of optimal $Q$ for squared Euclidean distance coincides with our class of op",
    "link": "https://arxiv.org/abs/2402.14925",
    "context": "Title: Efficient Unbiased Sparsification\nAbstract: arXiv:2402.14925v1 Announce Type: cross  Abstract: An unbiased $m$-sparsification of a vector $p\\in \\mathbb{R}^n$ is a random vector $Q\\in \\mathbb{R}^n$ with mean $p$ that has at most $m<n$ nonzero coordinates. Unbiased sparsification compresses the original vector without introducing bias; it arises in various contexts, such as in federated learning and sampling sparse probability distributions. Ideally, unbiased sparsification should also minimize the expected value of a divergence function $\\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If $Q$ is optimal in this sense, then we call it efficient. Our main results describe efficient unbiased sparsifications for divergences that are either permutation-invariant or additively separable. Surprisingly, the characterization for permutation-invariant divergences is robust to the choice of divergence function, in the sense that our class of optimal $Q$ for squared Euclidean distance coincides with our class of op",
    "path": "papers/24/02/2402.14925.json",
    "total_tokens": 897,
    "translated_title": "高效无偏稀疏化",
    "translated_abstract": "一个向量$p\\in \\mathbb{R}^n$的无偏$m$-稀疏化是一个具有平均值为$p$，最多有$m<n$个非零坐标的随机向量$Q\\in \\mathbb{R}^n。 无偏稀疏化可以压缩原始向量而不引入偏差；它出现在各种情境中，比如联邦学习和采样稀疏概率分布。 理想情况下，无偏稀疏化还应该最小化一个度量$Q$与原始$p$之间距离有多远的分裂函数$\\mathsf{Div}(Q,p)$的期望值。 如果$Q$在这个意义上是最优的，那么我们称之为高效。 我们的主要结果描述了对于既是排列不变又是可加可分的分裂函数的高效无偏稀疏化。 令人惊讶的是，排列不变分裂函数的表征对于分裂函数的选择是健壮的，也就是说，我们针对平方欧氏距离的最优$Q$的类与我们的类重合了op",
    "tldr": "该论文描述了对于排列不变或者可加可分的分裂函数，高效的无偏稀疏化特征。",
    "en_tdlr": "The paper presents efficient unbiased sparsifications for divergences that are either permutation-invariant or additively separable."
}