{
    "title": "Revisiting the Power of Prompt for Visual Tuning",
    "abstract": "Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outpe",
    "link": "https://arxiv.org/abs/2402.02382",
    "context": "Title: Revisiting the Power of Prompt for Visual Tuning\nAbstract: Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outpe",
    "path": "papers/24/02/2402.02382.json",
    "total_tokens": 830,
    "translated_title": "重新审视视觉调整中提示词的力量",
    "translated_abstract": "视觉提示调整（VPT）是一种很有前景的解决方案，它利用可学习的提示词来定制预训练模型，用于下游任务。然而，VPT及其变种经常遇到诸如提示初始化、提示长度和自监督预训练中性能不佳等挑战，阻碍了成功的上下文适应。本研究从探索训练过程中提示词与补丁令牌之间的相关性演变开始。受到提示令牌与补丁令牌之间往往具有高互信息的观察启发，我们提出利用下游令牌原型对提示进行初始化。该策略性初始化明显提高了微调性能，相比于VPT，无需增加计算开销，我们进一步优化令牌构造，使用简化的流程保持了出色的性能。详尽的实验表明，我们提出的方法胜过了其他方法。",
    "tldr": "本研究提出了一种改进的视觉调整方法，通过采用下游令牌原型对提示进行初始化，进一步优化令牌构造，有效解决了视觉提示调整中的挑战，并取得了比其他方法更好的性能。",
    "en_tdlr": "This study proposes an improved visual tuning approach that initializes prompts with downstream token prototypes and optimizes token construction, effectively addressing challenges in visual prompt tuning and achieving superior performance compared to other methods."
}