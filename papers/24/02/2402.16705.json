{
    "title": "SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection",
    "abstract": "arXiv:2402.16705v1 Announce Type: new  Abstract: Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also b",
    "link": "https://arxiv.org/abs/2402.16705",
    "context": "Title: SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection\nAbstract: arXiv:2402.16705v1 Announce Type: new  Abstract: Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also b",
    "path": "papers/24/02/2402.16705.json",
    "total_tokens": 870,
    "translated_title": "SelectIT: 通过基于不确定性的自我反思实现大型语言模型的选择性指导调整",
    "translated_abstract": "指导调整（IT）对于调整大型语言模型（LLMs）以适应人类中心交互至关重要。最近的进展表明，精心选择一小部分高质量的IT数据可以显着提高LLMs的性能。尽管如此，常见方法通常依赖于额外的模型或数据集，这增加了成本并限制了广泛采用。在这项工作中，我们提出了一种新颖的方法，称为SelectIT，它利用LLM本身的基本能力。具体来说，我们利用LLMs中固有的不确定性，更有效地选择高质量的IT数据，而无需额外资源。此外，我们介绍了一种新颖的IT数据集，名为选择性羊驼（Selective Alpaca），通过将SelectIT应用于Alpaca-GPT4数据集而创建。实证结果表明，使用选择性羊驼进行IT可以极大地提升模型性能。SelectIT的稳健性也得到了验证。",
    "tldr": "SelectIT通过利用大型语言模型本身的能力和基于不确定性的方法，提出了一种无需额外资源的高效选择指导调整数据集的方法，进而提升了模型的能力。",
    "en_tdlr": "SelectIT proposes a method for efficiently selecting instruction tuning data without the need for extra resources by leveraging the foundational capabilities of large language models and uncertainty-based approaches, leading to enhanced model performance."
}