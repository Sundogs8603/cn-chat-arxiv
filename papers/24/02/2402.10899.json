{
    "title": "Taxonomy-based CheckList for Large Language Model Evaluation",
    "abstract": "arXiv:2402.10899v1 Announce Type: new  Abstract: As large language models (LLMs) have been used in many downstream tasks, the internal stereotypical representation may affect the fairness of the outputs. In this work, we introduce human knowledge into natural language interventions and study pre-trained language models' (LMs) behaviors within the context of gender bias. Inspired by CheckList behavioral testing, we present a checklist-style task that aims to probe and quantify LMs' unethical behaviors through question-answering (QA). We design three comparison studies to evaluate LMs from four aspects: consistency, biased tendency, model preference, and gender preference switch. We probe one transformer-based QA model trained on SQuAD-v2 dataset and one autoregressive large language model. Our results indicate that transformer-based QA model's biased tendency positively correlates with its consistency, whereas LLM shows the opposite relation. Our proposed task provides the first dataset",
    "link": "https://arxiv.org/abs/2402.10899",
    "context": "Title: Taxonomy-based CheckList for Large Language Model Evaluation\nAbstract: arXiv:2402.10899v1 Announce Type: new  Abstract: As large language models (LLMs) have been used in many downstream tasks, the internal stereotypical representation may affect the fairness of the outputs. In this work, we introduce human knowledge into natural language interventions and study pre-trained language models' (LMs) behaviors within the context of gender bias. Inspired by CheckList behavioral testing, we present a checklist-style task that aims to probe and quantify LMs' unethical behaviors through question-answering (QA). We design three comparison studies to evaluate LMs from four aspects: consistency, biased tendency, model preference, and gender preference switch. We probe one transformer-based QA model trained on SQuAD-v2 dataset and one autoregressive large language model. Our results indicate that transformer-based QA model's biased tendency positively correlates with its consistency, whereas LLM shows the opposite relation. Our proposed task provides the first dataset",
    "path": "papers/24/02/2402.10899.json",
    "total_tokens": 845,
    "translated_title": "基于分类法的大型语言模型评估检查表",
    "translated_abstract": "由于大型语言模型（LLMs）已被用于许多下游任务，内在的刻板印象可能影响输出的公平性。在这项工作中，我们将人类知识引入自然语言干预，并研究预训练语言模型（LMs）在性别偏见上的行为。受CheckList行为测试的启发，我们提出了一项类似检查表的任务，旨在通过问答（QA）探究并量化LMs的不道德行为。我们设计了三项比较研究，以评估LMs的一致性、偏见倾向、模型偏好和性别偏好切换。我们探究了一个基于transformer的QA模型（在SQuAD-v2数据集上训练）和一个自回归的大型语言模型。我们的结果表明，transformer-based QA模型的偏见倾向与其一致性呈正相关，而LLM表现出相反的关系。我们提出的任务提供了第一个数据集",
    "tldr": "本研究在大型语言模型中引入人类知识，通过问答任务探究LM的不道德行为，发现了一致性和偏见倾向之间的关联。",
    "en_tdlr": "This study introduces human knowledge into large language models, explores unethical behaviors through question-answering tasks, and reveals the correlation between consistency and biased tendencies."
}