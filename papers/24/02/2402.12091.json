{
    "title": "Do Large Language Models Understand Logic or Just Mimick Context?",
    "abstract": "arXiv:2402.12091v1 Announce Type: cross  Abstract: Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they ``guess'' the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving a",
    "link": "https://arxiv.org/abs/2402.12091",
    "context": "Title: Do Large Language Models Understand Logic or Just Mimick Context?\nAbstract: arXiv:2402.12091v1 Announce Type: cross  Abstract: Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they ``guess'' the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving a",
    "path": "papers/24/02/2402.12091.json",
    "total_tokens": 793,
    "translated_title": "大型语言模型真正理解逻辑还是仅仅模仿语境？",
    "translated_abstract": "在过去几年中，大型语言模型（LLMs）的能力受到了广泛关注，它们在复杂场景（如逻辑推理和符号推理）中表现出色。其中一个重要因素是在语境学习和少样本提示的益处。然而，使用语境推理的这种模型成功背后的原因尚未被充分探讨。LLMs是否了解逻辑规则以进行推理，还是通过学习一种概率映射通过语境“猜测”答案？本文通过使用反事实方法在两个逻辑推理数据集上研究了LLMs的推理能力，替换语境文本并修改逻辑概念。根据我们的分析，发现LLMs并不真正理解逻辑规则；相反，在语境学习简单增强了这些模型到达",
    "tldr": "大型语言模型在逻辑推理中并不真正理解逻辑规则，而是通过语境学习增强了模型到达结论的可能性",
    "en_tdlr": "Large language models do not truly understand logical rules in reasoning, but enhance the likelihood of arriving at conclusions through in-context learning."
}