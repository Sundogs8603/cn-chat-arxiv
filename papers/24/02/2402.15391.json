{
    "title": "Genie: Generative Interactive Environments",
    "abstract": "arXiv:2402.15391v1 Announce Type: cross  Abstract: We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.",
    "link": "https://arxiv.org/abs/2402.15391",
    "context": "Title: Genie: Generative Interactive Environments\nAbstract: arXiv:2402.15391v1 Announce Type: cross  Abstract: We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.",
    "path": "papers/24/02/2402.15391.json",
    "total_tokens": 856,
    "translated_title": "Genie：生成交互环境",
    "translated_abstract": "我们介绍了Genie，这是第一个通过无监督方式从未标记的互联网视频中训练而成的生成式交互环境。该模型可以被提示生成通过文本、合成图像、照片甚至素描描述的无限种类的动作可控虚拟世界。拥有110亿个参数的Genie可以被视为基础世界模型。其由时空视频标记器、自回归动力学模型以及简单且可扩展的潜在行动模型组成。尽管训练过程中没有使用任何地面真值行动标签或其他通常在世界模型文献中找到的领域特定要求，但Genie使用户可以基于逐帧基础在生成的环境中行动。进一步，所得到的学习潜在行动空间有助于训练代理程序模仿来自未见视频的行为，为未来培训通用代理铺平了道路。",
    "tldr": "Genie是第一个经过无监督训练的生成交互环境，可生成各种动作可控的虚拟世界，并提供了学习潜在行动空间以训练代理程序模仿未见视频行为的可能性。",
    "en_tdlr": "Genie is the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos, capable of generating a variety of action-controllable virtual worlds and providing a learned latent action space for training agents to mimic behaviors from unseen videos."
}