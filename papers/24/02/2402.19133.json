{
    "title": "Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations",
    "abstract": "arXiv:2402.19133v1 Announce Type: new  Abstract: Rationales in the form of manually annotated input spans usually serve as ground truth when evaluating explainability methods in NLP. They are, however, time-consuming and often biased by the annotation process. In this paper, we debate whether human gaze, in the form of webcam-based eye-tracking recordings, poses a valid alternative when evaluating importance scores. We evaluate the additional information provided by gaze data, such as total reading times, gaze entropy, and decoding accuracy with respect to human rationale annotations. We compare WebQAmGaze, a multilingual dataset for information-seeking QA, with attention and explainability-based importance scores for 4 different multilingual Transformer-based language models (mBERT, distil-mBERT, XLMR, and XLMR-L) and 3 languages (English, Spanish, and German). Our pipeline can easily be applied to other tasks and languages. Our findings suggest that gaze data offers valuable linguist",
    "link": "https://arxiv.org/abs/2402.19133",
    "context": "Title: Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale Annotations\nAbstract: arXiv:2402.19133v1 Announce Type: new  Abstract: Rationales in the form of manually annotated input spans usually serve as ground truth when evaluating explainability methods in NLP. They are, however, time-consuming and often biased by the annotation process. In this paper, we debate whether human gaze, in the form of webcam-based eye-tracking recordings, poses a valid alternative when evaluating importance scores. We evaluate the additional information provided by gaze data, such as total reading times, gaze entropy, and decoding accuracy with respect to human rationale annotations. We compare WebQAmGaze, a multilingual dataset for information-seeking QA, with attention and explainability-based importance scores for 4 different multilingual Transformer-based language models (mBERT, distil-mBERT, XLMR, and XLMR-L) and 3 languages (English, Spanish, and German). Our pipeline can easily be applied to other tasks and languages. Our findings suggest that gaze data offers valuable linguist",
    "path": "papers/24/02/2402.19133.json",
    "total_tokens": 919,
    "translated_title": "用基于网络摄像头的凝视数据作为人类原因标注的替代方案的评估",
    "translated_abstract": "在评估自然语言处理中的解释性方法时，以手动注释的输入跨度形式呈现的原因通常作为基准真相。然而，它们耗时且往往受注释过程的影响。在本文中，我们讨论了当评估重要性评分时，人类凝视，即基于网络摄像头的眼动跟踪记录，是否构成一个有效的替代方案。我们评估了凝视数据提供的附加信息，比如总阅读时间、凝视熵以及解码准确性，与人类原因标注相关。我们将WebQAmGaze，一个用于信息检索QA的多语言数据集，与4种不同的多语言Transformer语言模型（mBERT，distil-mBERT，XLMR和XLMR-L）以及3种语言（英语，西班牙语和德语）的注意力和可解释性重要性分数进行比较。我们的流程可以轻松应用于其他任务和语言。我们的研究结果表明，凝视数据提供了宝贵的语言学信息。",
    "tldr": "论文讨论了使用基于网络摄像头的眼动数据作为评估重要性评分的替代方案，通过比较不同语言模型对WebQAmGaze数据集的表现，结果表明凝视数据提供了宝贵的语言学信息。",
    "en_tdlr": "The paper discusses using webcam-based gaze data as an alternative for evaluating importance scores, comparing different language models on the WebQAmGaze dataset, and concludes that gaze data offers valuable linguistic information."
}