{
    "title": "Online Distribution Learning with Local Private Constraints",
    "abstract": "We study the problem of online conditional distribution estimation with \\emph{unbounded} label sets under local differential privacy. Let $\\mathcal{F}$ be a distribution-valued function class with unbounded label set. We aim at estimating an \\emph{unknown} function $f\\in \\mathcal{F}$ in an online fashion so that at time $t$ when the context $\\boldsymbol{x}_t$ is provided we can generate an estimate of $f(\\boldsymbol{x}_t)$ under KL-divergence knowing only a privatized version of the true labels sampling from $f(\\boldsymbol{x}_t)$. The ultimate objective is to minimize the cumulative KL-risk of a finite horizon $T$. We show that under $(\\epsilon,0)$-local differential privacy of the privatized labels, the KL-risk grows as $\\tilde{\\Theta}(\\frac{1}{\\epsilon}\\sqrt{KT})$ upto poly-logarithmic factors where $K=|\\mathcal{F}|$. This is in stark contrast to the $\\tilde{\\Theta}(\\sqrt{T\\log K})$ bound demonstrated by Wu et al. (2023a) for bounded label sets. As a byproduct, our results recover a ",
    "link": "https://arxiv.org/abs/2402.00315",
    "context": "Title: Online Distribution Learning with Local Private Constraints\nAbstract: We study the problem of online conditional distribution estimation with \\emph{unbounded} label sets under local differential privacy. Let $\\mathcal{F}$ be a distribution-valued function class with unbounded label set. We aim at estimating an \\emph{unknown} function $f\\in \\mathcal{F}$ in an online fashion so that at time $t$ when the context $\\boldsymbol{x}_t$ is provided we can generate an estimate of $f(\\boldsymbol{x}_t)$ under KL-divergence knowing only a privatized version of the true labels sampling from $f(\\boldsymbol{x}_t)$. The ultimate objective is to minimize the cumulative KL-risk of a finite horizon $T$. We show that under $(\\epsilon,0)$-local differential privacy of the privatized labels, the KL-risk grows as $\\tilde{\\Theta}(\\frac{1}{\\epsilon}\\sqrt{KT})$ upto poly-logarithmic factors where $K=|\\mathcal{F}|$. This is in stark contrast to the $\\tilde{\\Theta}(\\sqrt{T\\log K})$ bound demonstrated by Wu et al. (2023a) for bounded label sets. As a byproduct, our results recover a ",
    "path": "papers/24/02/2402.00315.json",
    "total_tokens": 1020,
    "translated_title": "在局部私有约束下的在线条件分布学习",
    "translated_abstract": "我们研究了在局部差分隐私下具有无界标签集的在线条件分布估计问题。我们目标是以在线方式估计一个未知的函数$f\\in \\mathcal{F}$，在时间$t$提供了上下文$\\boldsymbol{x}_t$时，我们可以只知道从$f(\\boldsymbol{x}_t)$中取样的真实标签的私有化版本，生成一个$f(\\boldsymbol{x}_t)$的KL散度估计。最终的目标是在有限时间$T$内最小化累积的KL风险。我们证明，在私有化标签的$(\\epsilon,0)$-局部差分隐私下，KL风险增长的速度为$\\tilde{\\Theta}(\\frac{1}{\\epsilon}\\sqrt{KT})$，其中$K=|\\mathcal{F}|$，与Wu等人(2023a)对于有界标签集的$\\tilde{\\Theta}(\\sqrt{T\\log K})$界限形成鲜明对比。作为副产品，我们的结果还恢复出一个...",
    "tldr": "在这项研究中，我们讨论了在局部差分隐私下具有无界标签集的在线条件分布估计问题。我们证明了在$(\\epsilon,0)$-局部差分隐私的情况下，KL风险随着时间的增长速度为$\\tilde{\\Theta}(\\frac{1}{\\epsilon}\\sqrt{KT})$，其中$K=|\\mathcal{F}|$，这与有界标签集的情况形成明显的对比。",
    "en_tdlr": "In this study, we discuss the problem of online conditional distribution estimation with unbounded label sets under local differential privacy. We show that under $(\\epsilon,0)$-local differential privacy, the KL-risk grows as $\\tilde{\\Theta}(\\frac{1}{\\epsilon}\\sqrt{KT})$ in contrast to the $\\tilde{\\Theta}(\\sqrt{T\\log K})$ bound for bounded label sets."
}