{
    "title": "Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima",
    "abstract": "arXiv:2402.11271v1 Announce Type: new  Abstract: The increasing significance of large language and multimodal models in societal information processing has ignited debates on social safety and ethics. However, few studies have approached the analysis of these limitations from the comprehensive perspective of human and artificial intelligence system interactions. This study investigates biases and preferences when humans and large models are used as key links in communication. To achieve this, we design a multimodal dataset and three different experiments to evaluate generative models in their roles as producers and disseminators of information. Our main findings highlight that synthesized information is more likely to be incorporated into model training datasets and messaging than human-generated information. Additionally, large models, when acting as transmitters of information, tend to modify and lose specific content selectively. Conceptually, we present two realistic models of auto",
    "link": "https://arxiv.org/abs/2402.11271",
    "context": "Title: Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima\nAbstract: arXiv:2402.11271v1 Announce Type: new  Abstract: The increasing significance of large language and multimodal models in societal information processing has ignited debates on social safety and ethics. However, few studies have approached the analysis of these limitations from the comprehensive perspective of human and artificial intelligence system interactions. This study investigates biases and preferences when humans and large models are used as key links in communication. To achieve this, we design a multimodal dataset and three different experiments to evaluate generative models in their roles as producers and disseminators of information. Our main findings highlight that synthesized information is more likely to be incorporated into model training datasets and messaging than human-generated information. Additionally, large models, when acting as transmitters of information, tend to modify and lose specific content selectively. Conceptually, we present two realistic models of auto",
    "path": "papers/24/02/2402.11271.json",
    "total_tokens": 817,
    "translated_title": "人工智能与人类在通信时代的互动：自噬使得大型模型实现局部最优",
    "translated_abstract": "随着大型语言和多模态模型在社会信息处理中的重要性日益增加，引发了关于社会安全和伦理的争论。然而，很少有研究从人类和人工智能系统相互作用的综合视角分析这些限制。本研究调查了人类和大型模型在通信中作为关键联系的偏见和偏好。为实现此目的，我们设计了一个多模态数据集和三个不同的实验，评估生成模型在其作为信息生产者和传播者的角色中的表现。我们的主要发现突出显示，合成信息更有可能被纳入模型训练数据集和消息传递中，而人类生成的信息。此外，大型模型在以信息传递者的角色时，倾向于有选择地修改和丢失特定内容。在概念上，我们提出了两种真实的自",
    "tldr": "合成信息更可能被大型模型纳入训练数据集和传播中，大型模型在传递信息时倾向于有选择地修改和丢失特定内容"
}