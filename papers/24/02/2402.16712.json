{
    "title": "Scalable Robust Sparse Principal Component Analysis",
    "abstract": "arXiv:2402.16712v1 Announce Type: new  Abstract: In this work, we propose an optimization framework for estimating a sparse robust one-dimensional subspace. Our objective is to minimize both the representation error and the penalty, in terms of the l1-norm criterion. Given that the problem is NP-hard, we introduce a linear relaxation-based approach. Additionally, we present a novel fitting procedure, utilizing simple ratios and sorting techniques. The proposed algorithm demonstrates a worst-case time complexity of $O(n^2 m \\log n)$ and, in certain instances, achieves global optimality for the sparse robust subspace, thereby exhibiting polynomial time efficiency. Compared to extant methodologies, the proposed algorithm finds the subspace with the lowest discordance, offering a smoother trade-off between sparsity and fit. Its architecture affords scalability, evidenced by a 16-fold improvement in computational speeds for matrices of 2000x2000 over CPU version. Furthermore, this method is",
    "link": "https://arxiv.org/abs/2402.16712",
    "context": "Title: Scalable Robust Sparse Principal Component Analysis\nAbstract: arXiv:2402.16712v1 Announce Type: new  Abstract: In this work, we propose an optimization framework for estimating a sparse robust one-dimensional subspace. Our objective is to minimize both the representation error and the penalty, in terms of the l1-norm criterion. Given that the problem is NP-hard, we introduce a linear relaxation-based approach. Additionally, we present a novel fitting procedure, utilizing simple ratios and sorting techniques. The proposed algorithm demonstrates a worst-case time complexity of $O(n^2 m \\log n)$ and, in certain instances, achieves global optimality for the sparse robust subspace, thereby exhibiting polynomial time efficiency. Compared to extant methodologies, the proposed algorithm finds the subspace with the lowest discordance, offering a smoother trade-off between sparsity and fit. Its architecture affords scalability, evidenced by a 16-fold improvement in computational speeds for matrices of 2000x2000 over CPU version. Furthermore, this method is",
    "path": "papers/24/02/2402.16712.json",
    "total_tokens": 903,
    "translated_title": "可扩展的稳健稀疏主成分分析",
    "translated_abstract": "在这项工作中，我们提出了一个优化框架来估计稀疏稳健的一维子空间。我们的目标是最小化表示误差和l1范数准则下的惩罚。鉴于问题是NP难的，我们引入了一种基于线性松弛的方法。此外，我们提出了一种利用简单比例和排序技术的新型拟合程序。所提出的算法展示了$O(n^2 m \\log n)$的最坏时间复杂度，并且在某些情况下，实现了稀疏稳健子空间的全局最优，从而展示了多项式时间效率。与现有方法相比，所提出的算法找到具有最低不一致性的子空间，提供了在稀疏性和拟合之间更平滑的权衡。其架构具有可扩展性，对于2000x2000的矩阵，计算速度相较CPU版本提升了16倍。此外，这种方法...",
    "tldr": "本文提出了一个优化框架，可在稀疏稳健的情况下估计一维子空间，通过引入线性松弛方法和新颖的拟合程序，实现了全局最优的稳健稀疏子空间，具有多项式时间效率且可扩展性强。",
    "en_tdlr": "This paper proposes an optimization framework for estimating a sparse robust one-dimensional subspace, achieving global optimality for the sparse robust subspace with linear relaxation method and novel fitting procedure, showing polynomial time efficiency and strong scalability."
}