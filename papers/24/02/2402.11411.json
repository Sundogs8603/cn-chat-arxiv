{
    "title": "Aligning Modalities in Vision Large Language Models via Preference Fine-tuning",
    "abstract": "arXiv:2402.11411v1 Announce Type: cross  Abstract: Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucin",
    "link": "https://arxiv.org/abs/2402.11411",
    "context": "Title: Aligning Modalities in Vision Large Language Models via Preference Fine-tuning\nAbstract: arXiv:2402.11411v1 Announce Type: cross  Abstract: Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucin",
    "path": "papers/24/02/2402.11411.json",
    "total_tokens": 831,
    "translated_title": "通过偏好微调在视觉大语言模型中对齐模态",
    "translated_abstract": "指示跟随的视觉大语言模型（VLLMs）最近在各种任务上取得了显著进展。这些方法合并了强大的预训练视觉模型和大型语言模型（LLMs）。由于这些组件是分别训练的，所以需要通过联合训练额外的图像-语言对来对学习的表示进行对齐。这个过程并不完美，可能会导致模型产生幻觉-即使核心LLM非常客观，视觉支撑具有充分完整的表示，也会提供与图像不符合的答案。在这项工作中，我们将幻觉问题定义为一个对齐问题，通过偏好调整来解决。具体来说，我们提出了POVID来产生AI模型的反馈数据。我们使用地面真实指示作为首选响应，采用两阶段方法生成不受欢迎的数据。首先，我们提示GPT-4V注入合理的幻觉。",
    "tldr": "本研究将幻觉问题视为对齐问题，并通过偏好调整解决，提出了POVID方法来生成反馈数据。"
}