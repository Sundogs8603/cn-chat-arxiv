{
    "title": "Towards Optimizing the Costs of LLM Usage",
    "abstract": "Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases.   In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sente",
    "link": "https://arxiv.org/abs/2402.01742",
    "context": "Title: Towards Optimizing the Costs of LLM Usage\nAbstract: Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases.   In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sente",
    "path": "papers/24/02/2402.01742.json",
    "total_tokens": 823,
    "translated_title": "优化LLM使用成本的研究",
    "translated_abstract": "生成式人工智能，特别是LLM在现今广泛应用于各种文件处理任务中，如问答和摘要。然而，不同的LLM在不同任务上具有不同的能力、成本、标记化和延迟。实际上，企业已经在为各自的用例运营或使用LLM而承担巨大的成本。在这项工作中，我们提出通过估计LLM的输出质量（而无需实际调用LLM），然后解决LLM选择的优化例程，以在质量和延迟方面保持成本在预算范围内或最小化成本。我们提出了一个模型来预测LLM在摘要等文件处理任务中的输出质量，随后采用LP取整算法来优化LLM的选择。我们从理论和实证的角度研究了在质量和成本之间权衡的优化问题。",
    "tldr": "本论文提出了一种优化LLM使用成本的方法，通过估计输出质量并解决优化问题，实现在质量和延迟方面保持成本在预算范围内或最小化成本。",
    "en_tdlr": "This paper proposes a method to optimize the costs of LLM usage by estimating output quality and solving an optimization problem, achieving cost control within a budget or minimizing costs while considering quality and latency."
}