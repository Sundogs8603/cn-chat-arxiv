{
    "title": "Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error",
    "abstract": "Establishing robust policies is essential to counter attacks or disturbances affecting deep reinforcement learning (DRL) agents. Recent studies explore state-adversarial robustness and suggest the potential lack of an optimal robust policy (ORP), posing challenges in setting strict robustness constraints. This work further investigates ORP: At first, we introduce a consistency assumption of policy (CAP) stating that optimal actions in the Markov decision process remain consistent with minor perturbations, supported by empirical and theoretical evidence. Building upon CAP, we crucially prove the existence of a deterministic and stationary ORP that aligns with the Bellman optimal policy. Furthermore, we illustrate the necessity of $L^{\\infty}$-norm when minimizing Bellman error to attain ORP. This finding clarifies the vulnerability of prior DRL algorithms that target the Bellman optimal policy with $L^{1}$-norm and motivates us to train a Consistent Adversarial Robust Deep Q-Network (CA",
    "link": "https://arxiv.org/abs/2402.02165",
    "context": "Title: Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error\nAbstract: Establishing robust policies is essential to counter attacks or disturbances affecting deep reinforcement learning (DRL) agents. Recent studies explore state-adversarial robustness and suggest the potential lack of an optimal robust policy (ORP), posing challenges in setting strict robustness constraints. This work further investigates ORP: At first, we introduce a consistency assumption of policy (CAP) stating that optimal actions in the Markov decision process remain consistent with minor perturbations, supported by empirical and theoretical evidence. Building upon CAP, we crucially prove the existence of a deterministic and stationary ORP that aligns with the Bellman optimal policy. Furthermore, we illustrate the necessity of $L^{\\infty}$-norm when minimizing Bellman error to attain ORP. This finding clarifies the vulnerability of prior DRL algorithms that target the Bellman optimal policy with $L^{1}$-norm and motivates us to train a Consistent Adversarial Robust Deep Q-Network (CA",
    "path": "papers/24/02/2402.02165.json",
    "total_tokens": 1053,
    "translated_title": "对抗鲁棒Q学习的最优化研究与贝尔曼无穷误差",
    "translated_abstract": "构建鲁棒策略对于抵御影响深度强化学习（DRL）智能体的攻击或干扰至关重要。最近的研究探索了状态对抗鲁棒性，并暗示了缺乏最优鲁棒策略（ORP）的潜在问题，这给设定严格的鲁棒性约束带来了挑战。本文进一步研究了ORP：首先，我们引入了策略一致性假设（CAP），该假设指出马尔可夫决策过程中的最优动作在微小扰动下保持一致，得到了实证和理论证据的支持。在CAP的基础上，我们关键地证明了一种确定性且稳态的ORP的存在，该ORP与贝尔曼最优策略一致。此外，我们还阐明了在最小化贝尔曼误差以获得ORP时，$L^{\\infty}$-范数的必要性。这一发现澄清了先前针对贝尔曼最优策略使用$L^{1}$-范数的DRL算法的脆弱性，并激励我们训练一种一致性对抗鲁棒深度Q网络（CA）",
    "tldr": "本文研究了对抗鲁棒Q学习的最优化，并证明了一种确定性且稳态的最优鲁棒策略存在，该策略与贝尔曼最优策略一致。同时，阐明了在最小化贝尔曼误差以获得最优鲁棒策略时使用$L^{\\infty}$-范数的必要性。",
    "en_tdlr": "This paper investigates the optimal adversarial robust Q-learning and proves the existence of a deterministic and stationary optimal robust policy that aligns with the Bellman optimal policy. Furthermore, it clarifies the necessity of using the $L^{\\infty}$-norm when minimizing Bellman error to attain the optimal robust policy."
}