{
    "title": "Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?",
    "abstract": "Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is genera",
    "link": "https://arxiv.org/abs/2402.03305",
    "context": "Title: Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?\nAbstract: Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is genera",
    "path": "papers/24/02/2402.03305.json",
    "total_tokens": 896,
    "translated_title": "扩散模型是否学习语义有意义和高效的表示？",
    "translated_abstract": "扩散模型能够以不寻常的方式生成图像，例如宇航员骑在月球上的马，并且有正确的阴影。这些输出表明了模型具有组合泛化的能力，但是模型是如何做到这一点的呢？我们在条件DDPMs上进行了控制实验，学习生成以指定的$x$和$y$位置为中心的2D球形高斯凸起。我们的结果表明，产生语义有意义的潜在表示对于实现高性能至关重要。在学习过程中，模型经历了三个不同的潜在表示阶段：(A阶段)没有潜在结构，(B阶段)一个混乱状态的2D流形，以及(C阶段)一个有序的2D流形。对应于这些阶段，我们发现了 qualitatively 不同的生成行为：1）生成多个凸起，2）生成一个凸起，但$x$和$y$位置不准确，3）生成一个凸起且位置准确。",
    "tldr": "本研究通过实验探究了条件DDPMs学习生成2D球形高斯凸起的过程，在学习的过程中发现了潜在表示的关键，产生了与不同阶段对应的 qualitatively 不同的生成行为。",
    "en_tdlr": "This study explores the process of conditional DDPMs learning to generate 2D spherical Gaussian bumps and finds that the emergence of semantically meaningful latent representations is key to successful performance, with qualitatively different generation behaviors observed in different stages."
}