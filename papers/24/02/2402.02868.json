{
    "title": "Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem",
    "abstract": "Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models. However, fine-tuning reinforcement learning (RL) models remains a challenge. This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: forgetting of pre-trained capabilities. Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of fine-tuning, on which the model behaved well due to pre-training. This way, we lose the anticipated transfer benefits. We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic. Through a detailed empirical analysis of the challenging NetHack and Montezuma's Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the ",
    "link": "https://arxiv.org/abs/2402.02868",
    "context": "Title: Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem\nAbstract: Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models. However, fine-tuning reinforcement learning (RL) models remains a challenge. This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: forgetting of pre-trained capabilities. Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of fine-tuning, on which the model behaved well due to pre-training. This way, we lose the anticipated transfer benefits. We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic. Through a detailed empirical analysis of the challenging NetHack and Montezuma's Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the ",
    "path": "papers/24/02/2402.02868.json",
    "total_tokens": 912,
    "translated_title": "细调强化学习模型暗地里是一种遗忘缓解问题",
    "translated_abstract": "细调是一种广泛应用的技术，允许从预训练模型中转移能力，最近基础模型的成功应用就证明了这一点。然而，细调强化学习（RL）模型仍然是一个挑战。本研究从动作和观察之间的相互作用的角度，将细调阶段未访问到的下游任务状态子空间中的预训练能力遗忘问题作为导致转移效果差的一个具体原因进行了概念化。模型在这个未访问到的状态子空间中的表现良好，但由于预训练使其失去了期望的转移优势。我们确定了该问题发生的条件，表明它是普遍存在的，并且在许多情况下是灾难性的。通过对具有挑战性的NetHack和Montezuma's Revenge环境进行详细的经验分析，我们展示了标准的知识保留技术如何缓解这个问题，从而使我们能充分利用细调的优势。",
    "tldr": "细调强化学习模型中的遗忘问题会导致转移效果差，研究发现常见且具有灾难性后果。通过使用标准的知识保留技术可以缓解这个问题并最大程度地利用细调的优势。"
}