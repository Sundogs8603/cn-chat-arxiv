{
    "title": "Prompting a Pretrained Transformer Can Be a Universal Approximator",
    "abstract": "arXiv:2402.14753v1 Announce Type: cross  Abstract: Despite the widespread adoption of prompting, prompt tuning and prefix-tuning of transformer models, our theoretical understanding of these fine-tuning methods remains limited. A key question is whether one can arbitrarily modify the behavior of pretrained model by prompting or prefix-tuning it. Formally, whether prompting and prefix-tuning a pretrained model can universally approximate sequence-to-sequence functions. This paper answers in the affirmative and demonstrates that much smaller pretrained models than previously thought can be universal approximators when prefixed. In fact, the attention mechanism is uniquely suited for universal approximation with prefix-tuning a single attention head being sufficient to approximate any continuous function. Moreover, any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length. Beyond these density-type results, we also offer Jack",
    "link": "https://arxiv.org/abs/2402.14753",
    "context": "Title: Prompting a Pretrained Transformer Can Be a Universal Approximator\nAbstract: arXiv:2402.14753v1 Announce Type: cross  Abstract: Despite the widespread adoption of prompting, prompt tuning and prefix-tuning of transformer models, our theoretical understanding of these fine-tuning methods remains limited. A key question is whether one can arbitrarily modify the behavior of pretrained model by prompting or prefix-tuning it. Formally, whether prompting and prefix-tuning a pretrained model can universally approximate sequence-to-sequence functions. This paper answers in the affirmative and demonstrates that much smaller pretrained models than previously thought can be universal approximators when prefixed. In fact, the attention mechanism is uniquely suited for universal approximation with prefix-tuning a single attention head being sufficient to approximate any continuous function. Moreover, any sequence-to-sequence function can be approximated by prefixing a transformer with depth linear in the sequence length. Beyond these density-type results, we also offer Jack",
    "path": "papers/24/02/2402.14753.json",
    "total_tokens": 788,
    "translated_title": "Pretrained Transformer的引导可以成为通用逼近器",
    "translated_abstract": "尽管Prompting、Prompt调整和前缀调整transformer模型已经被广泛采用，但我们对这些微调方法的理论理解仍然有限。一个关键问题是是否可以通过提示或前缀调整预训练模型的行为。形式上，提示和前缀调整预训练模型能否普遍逼近序列到序列的函数。本文肯定回答了这个问题，并证明比先前认为的要小得多的预训练模型在添加前缀后可以成为通用逼近器。事实上，注意力机制非常适合于前缀调整，一个单一的注意力头就足以逼近任何连续函数。此外，通过在transformer的深度中添加前缀，任何序列到序列函数都可以被逼近，其深度与序列长度成线性关系。除了这些密度类型结果，我们还提供了Jack...",
    "tldr": "这项研究表明，通过提示或前缀调整Pretrained Transformer可以成为通用逼近器，甚至比之前认为的更小的模型都可以实现这一功能。",
    "en_tdlr": "This study demonstrates that prompting or prefix-tuning a Pretrained Transformer can be a universal approximator, with even smaller models than previously thought able to achieve this functionality."
}