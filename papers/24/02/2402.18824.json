{
    "title": "Batch size invariant Adam",
    "abstract": "arXiv:2402.18824v1 Announce Type: new  Abstract: We propose a batch size invariant version of Adam, for use in large-scale, distributed settings, in which the mini-batch is divided into micro-batches which are distributed among worker nodes. For the v term, standard Adam first computes the average over micro-batch gradients, then squares, while in the batch size invariant Adam proposed here, we first square the micro-batch gradients, then average. Previous work (e.g. Malladi et al. 2022) used an alternative approach that involved a square-root scaling of the learning rate, but this approach requires strong assumptions to work; in particular that the gradient variance dominates the square of the expected gradient. In contrast, the approach proposed here gives batch size invariance without this assumption. We confirm that in practice our scheme gives batch size invariance in a much larger range of scenarios than the previous approach.",
    "link": "https://arxiv.org/abs/2402.18824",
    "context": "Title: Batch size invariant Adam\nAbstract: arXiv:2402.18824v1 Announce Type: new  Abstract: We propose a batch size invariant version of Adam, for use in large-scale, distributed settings, in which the mini-batch is divided into micro-batches which are distributed among worker nodes. For the v term, standard Adam first computes the average over micro-batch gradients, then squares, while in the batch size invariant Adam proposed here, we first square the micro-batch gradients, then average. Previous work (e.g. Malladi et al. 2022) used an alternative approach that involved a square-root scaling of the learning rate, but this approach requires strong assumptions to work; in particular that the gradient variance dominates the square of the expected gradient. In contrast, the approach proposed here gives batch size invariance without this assumption. We confirm that in practice our scheme gives batch size invariance in a much larger range of scenarios than the previous approach.",
    "path": "papers/24/02/2402.18824.json",
    "total_tokens": 801,
    "translated_title": "Adam的批量大小不变版本",
    "translated_abstract": "我们提出了Adam的批量大小不变版本，适用于大规模分布式设置，其中小批量被分成微批量，然后分配给工作节点。在标准Adam中，对v项首先计算微批量梯度的平均值，然后求平方，而在这里提出的批量大小不变的Adam中，我们首先对微批量梯度求平方，然后平均。先前的工作（例如Malladi等人2022年）使用一种涉及学习率的平方根缩放的替代方法，但这种方法需要强大的假设才能起作用；特别是梯度方差主导期望梯度的平方。相比之下，这里提出的方法在不需要这种假设的情况下实现了批量大小不变性。我们证实在实践中，我们的方案在比以前的方法更多的情况下给出了批量大小不变性。",
    "tldr": "提出了Adam的批量大小不变版本，通过改变计算顺序实现批量大小不变性，比之前的方法具有更广泛的适用范围。",
    "en_tdlr": "Proposed a batch size invariant version of Adam, achieving batch size invariance by changing the calculation order, with a broader range of applicability than previous methods."
}