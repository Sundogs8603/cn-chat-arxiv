{
    "title": "Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task",
    "abstract": "arXiv:2402.14494v1 Announce Type: new  Abstract: In a realistic dialogue system, the input information from users is often subject to various types of input perturbations, which affects the slot-filling task. Although rule-based data augmentation methods have achieved satisfactory results, they fail to exhibit the desired generalization when faced with unknown noise disturbances. In this study, we address the challenges posed by input perturbations in slot filling by proposing Noise-BERT, a unified Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and Sentence Noisiness Discrimination, aiming to guide the pre-trained language model in capturing accurate slot information and noise distribution. During fine-tuning, we employ a contrastive learning loss to enhance the semantic representation of entities and labels. Additionally, we introduce an adversarial attack training strategy to i",
    "link": "https://arxiv.org/abs/2402.14494",
    "context": "Title: Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment Pre-training for Noisy Slot Filling Task\nAbstract: arXiv:2402.14494v1 Announce Type: new  Abstract: In a realistic dialogue system, the input information from users is often subject to various types of input perturbations, which affects the slot-filling task. Although rule-based data augmentation methods have achieved satisfactory results, they fail to exhibit the desired generalization when faced with unknown noise disturbances. In this study, we address the challenges posed by input perturbations in slot filling by proposing Noise-BERT, a unified Perturbation-Robust Framework with Noise Alignment Pre-training. Our framework incorporates two Noise Alignment Pre-training tasks: Slot Masked Prediction and Sentence Noisiness Discrimination, aiming to guide the pre-trained language model in capturing accurate slot information and noise distribution. During fine-tuning, we employ a contrastive learning loss to enhance the semantic representation of entities and labels. Additionally, we introduce an adversarial attack training strategy to i",
    "path": "papers/24/02/2402.14494.json",
    "total_tokens": 929,
    "translated_title": "Noise-BERT: 一种具有噪声对齐预训练的统一扰动鲁棒性框架，用于嘈杂的槽填充任务",
    "translated_abstract": "在现实对话系统中，用户输入信息经常遭受各种类型的输入扰动，这影响了槽填充任务。尽管基于规则的数据增强方法已经取得了令人满意的结果，但当面对未知噪声干扰时，它们无法展现出期望的泛化能力。在本研究中，我们通过提出Noise-BERT来解决槽填充中输入扰动带来的挑战，这是一个具有噪声对齐预训练的统一扰动鲁棒性框架。我们的框架包含两个Noise Alignment预训练任务：槽屏蔽预测和句子嘈杂度判别，旨在引导预训练语言模型捕捉准确的槽信息和噪声分布。在微调过程中，我们采用对比学习损失来增强实体和标签的语义表示。此外，我们引入了对抗攻击训练策略以提高语义表示的鲁棒性。",
    "tldr": "提出了Noise-BERT框架，包含噪声对齐预训练任务，通过对比学习损失和对抗攻击训练策略，以提高在嘈杂环境下的槽填充任务表现。",
    "en_tdlr": "Proposed a Noise-BERT framework with noise alignment pre-training tasks, utilizing contrastive learning loss and adversarial attack training strategy to enhance slot filling performance in noisy environments."
}