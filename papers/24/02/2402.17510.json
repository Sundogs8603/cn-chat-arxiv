{
    "title": "Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning",
    "abstract": "arXiv:2402.17510v1 Announce Type: cross  Abstract: Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data. We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcu",
    "link": "https://arxiv.org/abs/2402.17510",
    "context": "Title: Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning\nAbstract: arXiv:2402.17510v1 Announce Type: cross  Abstract: Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data. We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcu",
    "path": "papers/24/02/2402.17510.json",
    "total_tokens": 842,
    "translated_title": "示范和减少视觉语言表示学习中的捷径",
    "translated_abstract": "arXiv:2402.17510v1 公告类型: 跨领域 摘要: 视觉-语言模型(VLMs)主要依赖对比训练来学习图像和标题的通用表示。我们关注的情况是当一个图像与多个标题相关联时，每个标题既包含所有标题共享的信息，又包含关于图像场景的每个标题独特的信息。在这种情况下，尚不清楚对比损失是否足以学习包含标题提供的所有信息的任务最优表示，还是对比学习设置是否鼓励学习最小化对比损失的简单捷径。我们引入了视觉-语言的合成捷径：一种训练和评估框架，在其中我们向图像-文本数据注入合成捷径。我们展示了，从头开始训练或用包含这些合成捷径的数据微调的对比VLMs主要学习代表捷径的特征。",
    "tldr": "在视觉-语言表示学习中，论文提出了一种训练和评估框架，引入了合成捷径来探究对比训练是否足以学习到包含所有信息的任务最优表示。",
    "en_tdlr": "The paper introduces a training and evaluation framework in vision-language representation learning by injecting synthetic shortcuts, aiming to investigate whether contrastive training is sufficient to learn task-optimal representations containing all information."
}