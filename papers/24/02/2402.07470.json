{
    "title": "Pushing The Limit of LLM Capacity for Text Classification",
    "abstract": "The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in text classification under the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperf",
    "link": "https://arxiv.org/abs/2402.07470",
    "context": "Title: Pushing The Limit of LLM Capacity for Text Classification\nAbstract: The value of text classification's future research has encountered challenges and uncertainties, due to the extraordinary efficacy demonstrated by large language models (LLMs) across numerous downstream NLP tasks. In this era of open-ended language modeling, where task boundaries are gradually fading, an urgent question emerges: have we made significant advances in text classification under the full benefit of LLMs? To answer this question, we propose RGPT, an adaptive boosting framework tailored to produce a specialized text classification LLM by recurrently ensembling a pool of strong base learners. The base learners are constructed by adaptively adjusting the distribution of training samples and iteratively fine-tuning LLMs with them. Such base learners are then ensembled to be a specialized text classification LLM, by recurrently incorporating the historical predictions from the previous learners. Through a comprehensive empirical comparison, we show that RGPT significantly outperf",
    "path": "papers/24/02/2402.07470.json",
    "total_tokens": 842,
    "translated_title": "推动文本分类中LLM容量的极限",
    "translated_abstract": "由于大型语言模型（LLM）在众多下游NLP任务中展示出的非凡效果，文本分类未来研究的价值面临着挑战和不确定性。在这个任务边界逐渐模糊的开放式语言建模时代，一个迫切的问题出现了：在充分利用LLM的情况下，我们在文本分类方面取得了重大进展吗？为了回答这个问题，我们提出了RGPT，一个自适应增强框架，旨在通过反复集成一组强基学习者，来生成一个专用的文本分类LLM。基学习者是通过自适应调整训练样本的分布，并反复微调LLM与之构建的。然后，这些基学习者通过反复融合前几个学习者的历史预测结果，形成一个专用的文本分类LLM。通过全面的实证比较，我们展示了RGPT明显胜过其他方法。",
    "tldr": "本论文提出了一个自适应增强框架RGPT，通过反复集成强基学习者，生成一个专用的文本分类LLM。通过实证比较，我们展示了RGPT明显胜过其他方法。",
    "en_tdlr": "This paper proposes an adaptive boosting framework called RGPT to generate a specialized text classification LLM by recurrently ensembling a pool of strong base learners. Through comprehensive empirical comparison, RGPT is shown to outperform other methods significantly."
}