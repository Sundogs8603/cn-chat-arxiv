{
    "title": "Knowledge of Pretrained Language Models on Surface Information of Tokens",
    "abstract": "arXiv:2402.09808v1 Announce Type: new  Abstract: Do pretrained language models have knowledge regarding the surface information of tokens? We examined the surface information stored in word or subword embeddings acquired by pretrained language models from the perspectives of token length, substrings, and token constitution. Additionally, we evaluated the ability of models to generate knowledge regarding token surfaces. We focused on 12 pretrained language models that were mainly trained on English and Japanese corpora. Experimental results demonstrate that pretrained language models have knowledge regarding token length and substrings but not token constitution. Additionally, the results imply that there is a bottleneck on the decoder side in terms of effectively utilizing acquired knowledge.",
    "link": "https://arxiv.org/abs/2402.09808",
    "context": "Title: Knowledge of Pretrained Language Models on Surface Information of Tokens\nAbstract: arXiv:2402.09808v1 Announce Type: new  Abstract: Do pretrained language models have knowledge regarding the surface information of tokens? We examined the surface information stored in word or subword embeddings acquired by pretrained language models from the perspectives of token length, substrings, and token constitution. Additionally, we evaluated the ability of models to generate knowledge regarding token surfaces. We focused on 12 pretrained language models that were mainly trained on English and Japanese corpora. Experimental results demonstrate that pretrained language models have knowledge regarding token length and substrings but not token constitution. Additionally, the results imply that there is a bottleneck on the decoder side in terms of effectively utilizing acquired knowledge.",
    "path": "papers/24/02/2402.09808.json",
    "total_tokens": 747,
    "translated_title": "预训练语言模型对于标记的表面信息的知识",
    "translated_abstract": "我们研究了预训练语言模型是否具有关于标记表面信息的知识。我们从标记长度、子字符串和标记结构的角度检查了预训练语言模型获取的词语或子词嵌入中保存的表面信息。此外，我们评估了模型生成标记表面知识的能力。我们主要关注了12个主要在英语和日语语料库上训练的预训练语言模型。实验结果表明，预训练语言模型对于标记长度和子字符串具有知识，但对于标记结构则没有知识。此外，结果表明，在有效利用获取的知识方面，解码器方面存在瓶颈。",
    "tldr": "该研究探究了预训练语言模型对于标记的表面信息的知识。结果显示，模型具备有关标记长度和子字符串的知识，但对标记结构的知识有限，解码器方面存在瓶颈。",
    "en_tdlr": "This study examines the knowledge of pretrained language models regarding the surface information of tokens. The results demonstrate that the models possess knowledge of token length and substrings, but have limited knowledge of token constitution, indicating a bottleneck on the decoder side in effectively utilizing acquired knowledge."
}