{
    "title": "Careless Whisper: Speech-to-Text Hallucination Harms",
    "abstract": "Speech-to-text services aim to transcribe input audio as accurately as possible. They increasingly play a role in everyday life, for example in personal voice assistants or in customer-company interactions. We evaluate Open AI's Whisper, a state-of-the-art service outperforming industry competitors. While many of Whisper's transcriptions were highly accurate, we found that roughly 1% of audio transcriptions contained entire hallucinated phrases or sentences, which did not exist in any form in the underlying audio. We thematically analyze the Whisper-hallucinated content, finding that 38% of hallucinations include explicit harms such as violence, made up personal information, or false video-based authority. We further provide hypotheses on why hallucinations occur, uncovering potential disparities due to speech type by health status. We call on industry practitioners to ameliorate these language-model-based hallucinations in Whisper, and to raise awareness of potential biases in downstr",
    "link": "https://arxiv.org/abs/2402.08021",
    "context": "Title: Careless Whisper: Speech-to-Text Hallucination Harms\nAbstract: Speech-to-text services aim to transcribe input audio as accurately as possible. They increasingly play a role in everyday life, for example in personal voice assistants or in customer-company interactions. We evaluate Open AI's Whisper, a state-of-the-art service outperforming industry competitors. While many of Whisper's transcriptions were highly accurate, we found that roughly 1% of audio transcriptions contained entire hallucinated phrases or sentences, which did not exist in any form in the underlying audio. We thematically analyze the Whisper-hallucinated content, finding that 38% of hallucinations include explicit harms such as violence, made up personal information, or false video-based authority. We further provide hypotheses on why hallucinations occur, uncovering potential disparities due to speech type by health status. We call on industry practitioners to ameliorate these language-model-based hallucinations in Whisper, and to raise awareness of potential biases in downstr",
    "path": "papers/24/02/2402.08021.json",
    "total_tokens": 1068,
    "translated_title": "不小心的耳语：语音转文本幻觉的危害",
    "translated_abstract": "语音转文本服务旨在尽可能准确地转录输入音频。它们在日常生活中的作用越来越大，例如个人语音助手或公司与客户的互动中。我们评估了开放AI的Whisper，这是一种超越行业竞争对手的最新服务。虽然Whisper的许多转录非常准确，但我们发现大约1％的音频转录包含完全幻觉的短语或句子，这些短语或句子在基础音频中不存在。我们主题化地分析了Whisper幻觉的内容，发现38％的幻觉包含明确的伤害，例如暴力、虚构的个人信息或虚假的基于视频的权威。我们进一步提供了关于幻觉发生的假设，并揭示了由于健康状况而导致的语音类型的潜在差异。我们呼吁行业从业者改善Whisper中基于语言模型的幻觉，并增强对下游潜在偏见的认识。",
    "tldr": "该论文评估了开放AI的语音识别服务Whisper，并指出其中约1%的转录存在完全幻觉的短语或句子。这些幻觉内容中有38%包含明确的伤害，如暴力、虚构的个人信息或虚假的基于视频的权威。研究者进一步提供了幻觉发生的假设，并指出了由于语音类型和健康状况的不同可能导致的潜在差异。他们呼吁行业从业者改善基于语言模型的幻觉，并增强对下游潜在偏见的认识。",
    "en_tdlr": "This paper evaluates Open AI's speech recognition service Whisper and highlights that approximately 1% of the transcriptions contain hallucinated phrases or sentences. Among these hallucinations, 38% involve explicit harms such as violence, fabricated personal information, or false video-based authority. The researchers provide hypotheses on the occurrence of hallucinations and suggest potential disparities due to speech type and health status. They call on industry practitioners to improve language-model-based hallucinations and raise awareness of potential biases in downstream applications."
}