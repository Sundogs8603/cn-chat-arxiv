{
    "title": "A Distributional Analogue to the Successor Representation",
    "abstract": "This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possi",
    "link": "https://arxiv.org/abs/2402.08530",
    "context": "Title: A Distributional Analogue to the Successor Representation\nAbstract: This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process. Analogous to how the successor representation (SR) describes the expected consequences of behaving according to a given policy, our distributional successor measure (SM) describes the distributional consequences of this behaviour. We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning. Moreover, we propose an algorithm that learns the distributional SM from data by minimizing a two-level maximum mean discrepancy. Key to our method are a number of algorithmic techniques that are independently valuable for learning generative models of state. As an illustration of the usefulness of the distributional SM, we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possi",
    "path": "papers/24/02/2402.08530.json",
    "total_tokens": 835,
    "translated_title": "分布式后续表示的分布式类比",
    "translated_abstract": "本文提出了一种新的分布式强化学习方法，它将转换结构和奖励在学习过程中进行了明确的分离。与后续表示（SR）描述按照给定策略行为的期望后果类似，我们的分布式后继度量（SM）描述了这种行为的分布式结果。我们将分布式SM构建为一个分布的分布，并提供了与分布式和基于模型的强化学习相关的理论。此外，我们提出了一种从数据中学习分布式SM的算法，通过最小化两个层次的最大均值差异来实现。我们方法的关键是一些独立有价值的学习状态生成模型的算法技术。作为分布式SM有用性的例证，我们展示了它使得零样本风险敏感策略评估成为可能，这在以前是不可能的。",
    "tldr": "本文提出了一种新的分布式强化学习方法，它通过分离转换结构和奖励，引入了分布式后继度量来描述行为的分布式后果。在实验中展示了该方法的实用性，特别是在零样本风险敏感策略评估方面。",
    "en_tdlr": "This paper presents a new approach to distributional reinforcement learning by introducing a distributional successor measure that describes the distributional consequences of behavior, and illustrates its usefulness, particularly in zero-shot risk-sensitive policy evaluation."
}