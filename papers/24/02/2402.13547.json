{
    "title": "ActiveRAG: Revealing the Treasures of Knowledge via Active Learning",
    "abstract": "arXiv:2402.13547v1 Announce Type: new  Abstract: Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on qu",
    "link": "https://arxiv.org/abs/2402.13547",
    "context": "Title: ActiveRAG: Revealing the Treasures of Knowledge via Active Learning\nAbstract: arXiv:2402.13547v1 Announce Type: new  Abstract: Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on qu",
    "path": "papers/24/02/2402.13547.json",
    "total_tokens": 864,
    "translated_title": "ActiveRAG: 通过主动学习揭示知识的宝藏",
    "translated_abstract": "arXiv:2402.13547v1 公告类型：新摘要：检索增强生成（RAG）引入了一种新的大型语言模型（LLM）范例，有助于解决知识密集型任务。然而，当前的RAG模型将LLMs定位为被动的知识接收器，从而限制了它们学习和理解外部知识的能力。本文提出了ActiveRAG，它是一种创新的RAG框架，从被动知识获取转变为主动学习机制。这种方法利用知识构建机制通过将外部知识与先前获取或记忆的知识相关联来更深入地理解外部知识。随后，它设计了认知联结机制以合并来自思维和知识构建链的成果，从而校准LLMs的内在认知。我们的实验结果表明，ActiveRAG超越了先前的RAG模型，在问题回答上实现了5%的改进。",
    "tldr": "ActiveRAG是一个创新的RAG框架，通过引入主动学习机制，利用知识构建和认知联结机制来提升大型语言模型（LLMs）的内在认知，实现了明显的性能提升。",
    "en_tdlr": "ActiveRAG is an innovative RAG framework that introduces an active learning mechanism, utilizing knowledge construction and cognitive nexus mechanisms to enhance the intrinsic cognition of Large Language Models (LLMs), resulting in a significant performance improvement."
}