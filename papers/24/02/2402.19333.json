{
    "title": "Compact Speech Translation Models via Discrete Speech Units Pretraining",
    "abstract": "arXiv:2402.19333v1 Announce Type: new  Abstract: Using Self-Supervised Learning (SSL) as model initialization is now common to obtain strong results in Speech Translation (ST). However, they also impose a large memory footprint, hindering on-device deployment. In this paper, we leverage the SSL models by pretraining smaller models on their Discrete Speech Units (DSU). We pretrain encoder-decoder models on 1) Filterbank-to-DSU and 2) DSU-to-Translation data, and take the encoder from 1) and the decoder from 2) to initialise a new model, finetuning this on limited speech-translation data. The final model becomes compact by using the DSU pretraining to distil the knowledge of the SSL model. Our method has several benefits over using DSU as model inputs, such as shorter inference pipeline and robustness over (DSU) tokenization. In contrast to ASR pretraining, it does not require transcripts, making it applicable to low-resource settings. Evaluation on CoVoST-2 X-En shows that our method is",
    "link": "https://arxiv.org/abs/2402.19333",
    "context": "Title: Compact Speech Translation Models via Discrete Speech Units Pretraining\nAbstract: arXiv:2402.19333v1 Announce Type: new  Abstract: Using Self-Supervised Learning (SSL) as model initialization is now common to obtain strong results in Speech Translation (ST). However, they also impose a large memory footprint, hindering on-device deployment. In this paper, we leverage the SSL models by pretraining smaller models on their Discrete Speech Units (DSU). We pretrain encoder-decoder models on 1) Filterbank-to-DSU and 2) DSU-to-Translation data, and take the encoder from 1) and the decoder from 2) to initialise a new model, finetuning this on limited speech-translation data. The final model becomes compact by using the DSU pretraining to distil the knowledge of the SSL model. Our method has several benefits over using DSU as model inputs, such as shorter inference pipeline and robustness over (DSU) tokenization. In contrast to ASR pretraining, it does not require transcripts, making it applicable to low-resource settings. Evaluation on CoVoST-2 X-En shows that our method is",
    "path": "papers/24/02/2402.19333.json",
    "total_tokens": 944,
    "translated_title": "通过离散语音单元预训练实现紧凑的语音翻译模型",
    "translated_abstract": "使用自监督学习（SSL）作为模型初始化如今在语音翻译（ST）中获得强大结果是常见的。然而，它们也会占用大量内存，阻碍了设备部署。本文利用SSL模型通过在其离散语音单元（DSU）上预训练较小模型。我们在1）Filterbank-to-DSU和2）DSU-to-Translation数据上预训练编码器-解码器模型，然后取自1）的编码器和来自2）的解码器来初始化一个新模型，在有限的语音翻译数据上微调。通过使用DSU预训练来提炼SSL模型的知识，最终模型变得紧凑。我们的方法相比于使用DSU作为模型输入有几个优点，比如推理管道更短和对（DSU）标记化的鲁棒性。与ASR预训练相比，它不需要转录，使其适用于资源匮乏的环境。在CoVoST-2 X-En上的评估显示我们的方法是",
    "tldr": "通过在离散语音单元上预训练较小模型，以蒸馏SSL模型的知识，实现了紧凑的语音翻译模型，具有短推理管道和适用于低资源环境等优点",
    "en_tdlr": "Achieved compact speech translation models by pretraining smaller models on Discrete Speech Units (DSU) to distil SSL model knowledge, leading to benefits such as shorter inference pipeline and applicability in low-resource settings."
}