{
    "title": "Asymptotic generalization error of a single-layer graph convolutional network",
    "abstract": "While graph convolutional networks show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of supervised fully connected neural networks. In this article, we predict the performances of a single-layer graph convolutional network (GCN) trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit. Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM. We also study the high signal-to-noise ratio limit, detail the convergence rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases.",
    "link": "https://arxiv.org/abs/2402.03818",
    "context": "Title: Asymptotic generalization error of a single-layer graph convolutional network\nAbstract: While graph convolutional networks show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of supervised fully connected neural networks. In this article, we predict the performances of a single-layer graph convolutional network (GCN) trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit. Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM. We also study the high signal-to-noise ratio limit, detail the convergence rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases.",
    "path": "papers/24/02/2402.03818.json",
    "total_tokens": 897,
    "translated_title": "单层图卷积网络的渐进泛化误差",
    "translated_abstract": "虽然图卷积网络在实践中展现出很大的潜力，但是相对于广泛研究的全连接神经网络，关于其泛化特性与样本数量的理论理解仍处于初级阶段。在本文中，我们预测了在高维极限下，基于属性随机块模型（SBM）生成的数据训练的单层图卷积网络（GCN）的性能。之前，仅在Shi等人的文章中考虑了上下文-SBM（CSBM）上的岭回归分析；我们将分析推广到CSBM的任意凸损失和正则化方法，并添加了对另一个数据模型——神经优先SBM的分析。我们还研究了高信噪比极限，并详细介绍了GCN的收敛速度，并且展示了尽管一致，但对于任何考虑的情况都不能达到贝叶斯最优率。",
    "tldr": "本研究针对单层图卷积网络（GCN）在高维极限下的性能进行了预测，并推广了对多种数据模型的分析。我们的研究显示，尽管GCN在收敛速度上是一致的，但在任何情况下都不能达到贝叶斯最优率。",
    "en_tdlr": "This study predicts the performance of a single-layer graph convolutional network (GCN) trained on high-dimensional data and generalizes the analysis to various data models. The results show that although GCN has consistent convergence rates, it does not reach the Bayes-optimal rate in any considered cases."
}