{
    "title": "Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning",
    "abstract": "arXiv:2402.10409v1 Announce Type: cross  Abstract: As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-stro",
    "link": "https://arxiv.org/abs/2402.10409",
    "context": "Title: Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning\nAbstract: arXiv:2402.10409v1 Announce Type: cross  Abstract: As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-stro",
    "path": "papers/24/02/2402.10409.json",
    "total_tokens": 909,
    "translated_title": "通过图表示学习理解大型语言模型调查论文分类法",
    "translated_abstract": "随着大型语言模型（LLMs）的新研究持续进行，难以跟上新的研究和模型。为帮助研究人员综合新研究成果，许多人写了调研论文，但即使这些论文也变得越来越多。本文提出了一种自动将调研论文分配到分类法的方法。我们收集了144篇LLM调研论文的元数据，并探讨了三种范例来对分类法内的论文进行分类。我们的工作表明，在共类别图上利用图结构信息可以显著优于两个范例中的语言模型; 使用LLMs进行预训练语言模型的微调和零-shot/few-shot分类。我们发现我们的模型超过了平均人类识别水平，并且利用较小模型生成的弱标签来微调LLMs（本研究中的GCN等）可能比使用地面实况标签更有效，揭示了从弱到强的潜力。",
    "tldr": "通过图结构信息在共类别图上利用图表示学习技术，可以在LLMs的预训练模型微调和零-shot/few-shot分类方面显著优于语言模型，揭示了弱标签微调LLMs的潜力。"
}