{
    "title": "How Susceptible are Large Language Models to Ideological Manipulation?",
    "abstract": "arXiv:2402.11725v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the inf",
    "link": "https://arxiv.org/abs/2402.11725",
    "context": "Title: How Susceptible are Large Language Models to Ideological Manipulation?\nAbstract: arXiv:2402.11725v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the inf",
    "path": "papers/24/02/2402.11725.json",
    "total_tokens": 925,
    "translated_title": "大型语言模型对意识形态操纵的易感性有多高？",
    "translated_abstract": "大型语言模型(LLMs)具有对公众观念和信息互动施加重要影响的潜力。这引发了关于如果这些模型内的意识形态易受操纵可能带来社会影响的担忧。在这项工作中，我们研究了LLMs在学习和泛化意识形态偏见方面的效果。我们的发现揭示了一个令人担忧的脆弱性：仅接触到少量意识形态驱动的样本就会显著改变LLMs的意识形态。值得注意的是，LLMs表现出惊人的能力，能够从一个主题吸收意识形态并将其泛化到甚至不相关的主题上。LLMs的意识形态容易被扭曲的事实强调了恶意行为者故意毒害训练数据或数据注释者无意引入偏见所带来的风险。这也强调了采取强有力措施以减轻这些威胁的迫切性。",
    "tldr": "大型语言模型展示出令人担忧的易受意识形态操纵的脆弱性，对极少量意识形态驱动样本的暴露就能显著改变其意识形态，并且能够从一个主题吸收意识形态并泛化到其他不相关主题上。",
    "en_tdlr": "Large Language Models exhibit a concerning vulnerability to ideological manipulation, with exposure to a small amount of ideologically driven samples significantly altering their ideology and demonstrating the ability to absorb and generalize ideology from one topic to unrelated ones."
}