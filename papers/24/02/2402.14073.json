{
    "title": "Improving Language Understanding from Screenshots",
    "abstract": "arXiv:2402.14073v1 Announce Type: new  Abstract: An emerging family of language models (LMs), capable of processing both text and images within a single visual view, has the promise to unlock complex tasks such as chart understanding and UI navigation. We refer to these models as screenshot language models. Despite their appeal, existing screenshot LMs substantially lag behind text-only models on language understanding tasks. To close this gap, we adopt a simplified setting where the model inputs are plain-text-rendered screenshots, and we focus on improving the text ability of screenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective, which masks and recovers both image patches of screenshots and text within screenshots. We also conduct extensive ablation studies on masking rates and patch sizes, as well as designs for improving training stability. Our pre-trained model, while solely taking visual inputs, achieves comparable performance with BERT on 6 out of 8 GLUE ",
    "link": "https://arxiv.org/abs/2402.14073",
    "context": "Title: Improving Language Understanding from Screenshots\nAbstract: arXiv:2402.14073v1 Announce Type: new  Abstract: An emerging family of language models (LMs), capable of processing both text and images within a single visual view, has the promise to unlock complex tasks such as chart understanding and UI navigation. We refer to these models as screenshot language models. Despite their appeal, existing screenshot LMs substantially lag behind text-only models on language understanding tasks. To close this gap, we adopt a simplified setting where the model inputs are plain-text-rendered screenshots, and we focus on improving the text ability of screenshot LMs. We propose a novel Patch-and-Text Prediction (PTP) objective, which masks and recovers both image patches of screenshots and text within screenshots. We also conduct extensive ablation studies on masking rates and patch sizes, as well as designs for improving training stability. Our pre-trained model, while solely taking visual inputs, achieves comparable performance with BERT on 6 out of 8 GLUE ",
    "path": "papers/24/02/2402.14073.json",
    "total_tokens": 870,
    "translated_title": "从屏幕截图中提高语言理解能力",
    "translated_abstract": "一种新兴的语言模型家族（LMs）可以处理文本和图像，在单个视觉视图内，有望拓宽图表理解和UI导航等复杂任务。我们称这些模型为屏幕截图语言模型。尽管具有吸引力，但现有的屏幕截图LMs在语言理解任务上明显落后于仅文本的模型。为了弥合这一差距，我们采用了一个简化的设置，其中模型输入是纯文本渲染的屏幕截图，并集中在提高屏幕截图LMs的文本能力。我们提出了一种新颖的Patch-and-Text Prediction（PTP）目标，该目标遮盖和恢复屏幕截图中的图像块和文本。我们还进行了大量消融研究，涉及遮盖率、块大小以及用于提高训练稳定性的设计。我们的预训练模型，仅采用视觉输入，就在8个GLUE中的6个上实现了与BERT相当的性能。",
    "tldr": "本文提出了一种屏幕截图语言模型，通过引入新的Patch-and-Text Prediction（PTP）目标来改善文本能力，并取得了与BERT相当的性能。",
    "en_tdlr": "This paper introduces a screenshot language model, improves text ability by introducing a novel Patch-and-Text Prediction (PTP) objective, and achieves comparable performance with BERT."
}