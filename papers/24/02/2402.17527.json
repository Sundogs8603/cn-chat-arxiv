{
    "title": "Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>",
    "abstract": "arXiv:2402.17527v1 Announce Type: cross  Abstract: Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertaint",
    "link": "https://arxiv.org/abs/2402.17527",
    "context": "Title: Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>\nAbstract: arXiv:2402.17527v1 Announce Type: cross  Abstract: Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertaint",
    "path": "papers/24/02/2402.17527.json",
    "total_tokens": 826,
    "translated_title": "预测下一个词：人类在这项任务中表现出不确定性，而语言模型_____",
    "translated_abstract": "语言模型（LMs）是训练用于为人类生成文本分配概率的统计模型。因此，合理质疑它们是否很好地近似人类展示的语言变化性。这种形式的统计评估在段落级别上很难执行，因为它需要可接受性判断（即，人类评估）或一个强大的自动代理（这是不平凡的）。然而，在单词级别上，通过给定一些上下文，可以通过与一个预先记录的替代单词连续数据集的精确匹配来评估LM的样本。我们利用这一事实，并评估LM重新生成人类（特别是一群英语使用者）在“下一个词预测”任务中展示的变化的能力。这可以被视为一种校准评估，在文本分类的背景下，Baan等人（2022年）将其称为对人类不确定性的校准",
    "tldr": "评估语言模型在预测下一个词时，是否能够复现人类在这项任务中展示的语言变化性",
    "en_tdlr": "Evaluating the ability of language models to replicate the linguistic variability exhibited by humans in predicting the next word task."
}