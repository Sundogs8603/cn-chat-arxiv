{
    "title": "EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models",
    "abstract": "This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.",
    "link": "https://arxiv.org/abs/2402.00518",
    "context": "Title: EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models\nAbstract: This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.",
    "path": "papers/24/02/2402.00518.json",
    "total_tokens": 939,
    "translated_title": "EE-Tuning:一种经济且可扩展的调整早期终止大型语言模型的解决方案",
    "translated_abstract": "本文介绍了EE-Tuning，一种轻量且经济实用的解决方案，可以训练/调整早期终止的大型语言模型（LLMs）。与完整参数的预训练常见方法不同，EE-Tuning通过在参数高效方式下增加额外的早期终止层，与任何预训练（可能是微调）的标准LLM相结合，从而大大降低了计算资源和训练数据的需求。我们通过广泛的性能优化和完全兼容3D并行性的可扩展性，实现了EE-Tuning的卓越训练效率。系统实验证实了EE-Tuning的有效性，证明了在有限的训练预算下可以实现有效的早期终止LLM推理。为了将早期终止LLMs推广到社区，我们在https://github.com/pan-x-c/EE-LLM上发布了EE-Tuning的源代码。",
    "tldr": "该论文介绍了一种经济且可扩展的解决方案EE-Tuning，可以使用较少的计算资源和训练数据针对早期终止大型语言模型进行调整，通过性能优化和3D并行性实现卓越的训练效率。实验证实，即使在有限的训练预算下，也可以实现有效的早期终止LLM推理。",
    "en_tdlr": "This paper presents EE-Tuning, an economical and scalable solution for tuning early-exit large language models (LLMs). By using less computational resources and training data, EE-Tuning achieves outstanding training efficiency through performance optimizations and 3D parallelism. The experimental results confirm that effective early-exit LLM inference can be achieved even with a limited training budget."
}