{
    "title": "Jailbreaking Attack against Multimodal Large Language Model",
    "abstract": "This paper focuses on jailbreaking attacks against multi-modal large language models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to harmful user queries. A maximum likelihood-based algorithm is proposed to find an \\emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs across multiple unseen prompts and images (i.e., data-universal property). Our approach exhibits strong model-transferability, as the generated imgJP can be transferred to jailbreak various models, including MiniGPT-v2, LLaVA, InstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we introduce a construction-based method to harness our approach for LLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art methods. The code is available here. \\textbf{Warning: some content generated by language models may be offensive to some readers.}",
    "link": "https://arxiv.org/abs/2402.02309",
    "context": "Title: Jailbreaking Attack against Multimodal Large Language Model\nAbstract: This paper focuses on jailbreaking attacks against multi-modal large language models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to harmful user queries. A maximum likelihood-based algorithm is proposed to find an \\emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs across multiple unseen prompts and images (i.e., data-universal property). Our approach exhibits strong model-transferability, as the generated imgJP can be transferred to jailbreak various models, including MiniGPT-v2, LLaVA, InstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we introduce a construction-based method to harness our approach for LLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art methods. The code is available here. \\textbf{Warning: some content generated by language models may be offensive to some readers.}",
    "path": "papers/24/02/2402.02309.json",
    "total_tokens": 950,
    "translated_title": "对多模态大型语言模型的越狱攻击",
    "translated_abstract": "本文重点研究针对多模态大型语言模型（MLLMs）的越狱攻击，旨在引导MLLM生成对有害用户查询不当回应。提出了一种基于最大似然的算法来找到“图像越狱提示”（imgJP），实现对MLLM在多个未知提示和图像上的越狱（即数据通用属性）。我们的方法表现出较强的模型可转移性，生成的imgJP可以以黑盒方式转移到各种模型上进行越狱，包括MiniGPT-v2、LLaVA、InstructBLIP和mPLUG-Owl2。此外，我们揭示了MLLM越狱和LLM越狱之间的联系。因此，我们引入了一种基于构建的方法，利用我们的方法进行LLM越狱，展示了比当前最先进的方法更高的效率。代码链接在这里。注意：一些语言模型生成的内容可能对某些读者具有冒犯性。",
    "tldr": "本文提出了一种针对多模态大型语言模型的越狱攻击方法，通过找到“图像越狱提示”，实现在多个未知提示和图像上对语言模型的越狱，并展示了与单模态语言模型的越狱方法之间的联系。同时，通过构建的方法，将该越狱方法应用于单模态语言模型，较现有方法更高效。",
    "en_tdlr": "This paper presents a jailbreaking attack against multi-modal large language models, enabling the models to generate objectionable responses, and reveals a connection to jailbreaking methods for single-modal language models. Furthermore, the proposed method shows higher efficiency in applying the jailbreaking technique to single-modal models."
}