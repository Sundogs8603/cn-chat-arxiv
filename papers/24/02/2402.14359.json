{
    "title": "Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark",
    "abstract": "arXiv:2402.14359v1 Announce Type: new  Abstract: The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with ",
    "link": "https://arxiv.org/abs/2402.14359",
    "context": "Title: Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark\nAbstract: arXiv:2402.14359v1 Announce Type: new  Abstract: The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with ",
    "path": "papers/24/02/2402.14359.json",
    "total_tokens": 854,
    "translated_title": "重新思考科学摘要评估：基于方面感知基准的可解释度指标",
    "translated_abstract": "预训练和大型语言模型（LLMs）的摘要能力在一般领域中得到了广泛验证，但它们在涉及复杂句子和专业知识的科学语料库中的使用较少被评估。该论文提出了科学摘要的概念和实验分析，突出了传统评估方法（如$n$-gram、嵌入比较和问答）在提供解释、把握科学概念或识别关键内容方面的不足之处。随后，我们介绍了Facet-aware Metric（FM），利用LLMs进行高级语义匹配，根据不同方面评估摘要。这种面向方面的方法通过将评估任务分解为更简单的子任务，为摘要提供了全面的评估。鉴于该领域缺乏评估基准，我们精心策划了一个基于方面的科学摘要数据集（FD）。",
    "tldr": "该论文提出了一种基于方面感知的评估指标（FM），利用大型语言模型对摘要进行高级语义匹配，提供了一种全面评估科学摘要的方法。",
    "en_tdlr": "This paper introduces a facet-aware evaluation metric (FM) that utilizes large language models for advanced semantic matching to provide a comprehensive way to evaluate scientific summaries."
}