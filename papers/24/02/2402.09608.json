{
    "title": "Exact, Fast and Expressive Poisson Point Processes via Squared Neural Families",
    "abstract": "arXiv:2402.09608v1 Announce Type: new  Abstract: We introduce squared neural Poisson point processes (SNEPPPs) by parameterising the intensity function by the squared norm of a two layer neural network. When the hidden layer is fixed and the second layer has a single neuron, our approach resembles previous uses of squared Gaussian process or kernel methods, but allowing the hidden layer to be learnt allows for additional flexibility. In many cases of interest, the integrated intensity function admits a closed form and can be computed in quadratic time in the number of hidden neurons. We enumerate a far more extensive number of such cases than has previously been discussed. Our approach is more memory and time efficient than naive implementations of squared or exponentiated kernel methods or Gaussian processes. Maximum likelihood and maximum a posteriori estimates in a reparameterisation of the final layer of the intensity function can be obtained by solving a (strongly) convex optimisa",
    "link": "https://arxiv.org/abs/2402.09608",
    "context": "Title: Exact, Fast and Expressive Poisson Point Processes via Squared Neural Families\nAbstract: arXiv:2402.09608v1 Announce Type: new  Abstract: We introduce squared neural Poisson point processes (SNEPPPs) by parameterising the intensity function by the squared norm of a two layer neural network. When the hidden layer is fixed and the second layer has a single neuron, our approach resembles previous uses of squared Gaussian process or kernel methods, but allowing the hidden layer to be learnt allows for additional flexibility. In many cases of interest, the integrated intensity function admits a closed form and can be computed in quadratic time in the number of hidden neurons. We enumerate a far more extensive number of such cases than has previously been discussed. Our approach is more memory and time efficient than naive implementations of squared or exponentiated kernel methods or Gaussian processes. Maximum likelihood and maximum a posteriori estimates in a reparameterisation of the final layer of the intensity function can be obtained by solving a (strongly) convex optimisa",
    "path": "papers/24/02/2402.09608.json",
    "total_tokens": 895,
    "translated_title": "使用平方神经网络族的精确、快速和表达性泊松点过程",
    "translated_abstract": "我们通过将强度函数的参数化为两层神经网络的平方范数引入了平方神经泊松点过程（SNEPPPs）。当隐藏层被固定且第二层只有一个神经元时，我们的方法类似于之前使用平方高斯过程或核方法，但允许隐藏层学习能够提供额外的灵活性。在许多感兴趣的情况下，积分强度函数可以得到封闭形式，并且可以以二次时间相对于隐藏神经元的数量进行计算。我们列举了比以前讨论过的更多这样的情况。我们的方法比简单实现平方或指数核方法或高斯过程更节约内存和时间。最大似然和最大后验估计可以通过解决（严格）凸优化问题来获得强度函数最终层的参数化重参数化。",
    "tldr": "该论文介绍了使用平方神经网络族的精确、快速和表达性泊松点过程。通过利用两层神经网络的平方范数来参数化强度函数，可以获得更灵活和高效的方法。该方法在计算积分强度函数时具有封闭形式和二次时间复杂度，并且相比于传统方法更节约内存和时间。通过解决凸优化问题，可以获得对强度函数最终层的参数化重参数化的最大似然估计和最大后验估计。"
}