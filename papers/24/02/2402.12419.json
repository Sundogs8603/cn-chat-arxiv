{
    "title": "EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs",
    "abstract": "arXiv:2402.12419v1 Announce Type: cross  Abstract: Existing methods for fine-tuning sparse LLMs often suffer from resource-intensive requirements and high retraining costs. Additionally, many fine-tuning methods often rely on approximations or heuristic optimization strategies, which may lead to suboptimal solutions. To address these issues, we propose an efficient and fast framework for fine-tuning sparse LLMs based on minimizing reconstruction error. Our approach involves sampling a small dataset for calibration and utilizing backpropagation to iteratively optimize block-wise reconstruction error, on a block-by-block basis, aiming for optimal solutions. Extensive experiments on various benchmarks consistently demonstrate the superiority of our method over other baselines. For instance, on the Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a perplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of 75.14. Moreover, with a structured",
    "link": "https://arxiv.org/abs/2402.12419",
    "context": "Title: EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs\nAbstract: arXiv:2402.12419v1 Announce Type: cross  Abstract: Existing methods for fine-tuning sparse LLMs often suffer from resource-intensive requirements and high retraining costs. Additionally, many fine-tuning methods often rely on approximations or heuristic optimization strategies, which may lead to suboptimal solutions. To address these issues, we propose an efficient and fast framework for fine-tuning sparse LLMs based on minimizing reconstruction error. Our approach involves sampling a small dataset for calibration and utilizing backpropagation to iteratively optimize block-wise reconstruction error, on a block-by-block basis, aiming for optimal solutions. Extensive experiments on various benchmarks consistently demonstrate the superiority of our method over other baselines. For instance, on the Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a perplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of 75.14. Moreover, with a structured",
    "path": "papers/24/02/2402.12419.json",
    "total_tokens": 877,
    "translated_title": "EBFT：稀疏LLM的有效和块状微调",
    "translated_abstract": "现有的稀疏LLM微调方法通常需要资源密集型的要求和高昂的重新训练成本。此外，许多微调方法往往依赖于近似或启发式优化策略，这可能导致次优解。为了解决这些问题，我们提出了一种基于最小化重建误差的高效快速微调稀疏LLM的框架。我们的方法涉及对一个小数据集进行采样以进行校准，并利用反向传播逐块地优化块状重建误差，致力于寻求最佳解决方案。对各种基准测试的广泛实验证明，我们的方法在多个基线上始终表现卓越。例如，在Wikitext2数据集上，LLamaV1-7B在70%稀疏度下，我们提出的EBFT取得了16.88的困惑度，超过了75.14的DSnoT的最先进水平。",
    "tldr": "提出了一种有效的块状微调稀疏LLM的框架，通过最小化重建误差并采用反向传播逐块优化解决方案，实验结果表明在各种基准测试中优于其他方法。",
    "en_tdlr": "Introduced an efficient framework for block-wise fine-tuning sparse LLMs by minimizing reconstruction error and optimizing solutions block by block using backpropagation, experimentally outperforming other methods on various benchmarks."
}