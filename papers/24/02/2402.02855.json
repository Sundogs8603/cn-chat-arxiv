{
    "title": "Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation",
    "abstract": "In the realm of deep learning-based recommendation systems, the increasing computational demands, driven by the growing number of users and items, pose a significant challenge to practical deployment. This challenge is primarily twofold: reducing the model size while effectively learning user and item representations for efficient recommendations. Despite considerable advancements in model compression and architecture search, prevalent approaches face notable constraints. These include substantial additional computational costs from pre-training/re-training in model compression and an extensive search space in architecture design. Additionally, managing complexity and adhering to memory constraints is problematic, especially in scenarios with strict time or space limitations. Addressing these issues, this paper introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored for recommendation models. DSL innovatively trains a lightweight sparse model from scratch, periodi",
    "link": "https://arxiv.org/abs/2402.02855",
    "context": "Title: Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation\nAbstract: In the realm of deep learning-based recommendation systems, the increasing computational demands, driven by the growing number of users and items, pose a significant challenge to practical deployment. This challenge is primarily twofold: reducing the model size while effectively learning user and item representations for efficient recommendations. Despite considerable advancements in model compression and architecture search, prevalent approaches face notable constraints. These include substantial additional computational costs from pre-training/re-training in model compression and an extensive search space in architecture design. Additionally, managing complexity and adhering to memory constraints is problematic, especially in scenarios with strict time or space limitations. Addressing these issues, this paper introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored for recommendation models. DSL innovatively trains a lightweight sparse model from scratch, periodi",
    "path": "papers/24/02/2402.02855.json",
    "total_tokens": 849,
    "translated_title": "动态稀疏学习：高效推荐的一种新范式",
    "translated_abstract": "在基于深度学习的推荐系统领域中，日益增长的用户和物品数量所带来的计算需求增加，给实际部署带来了显著挑战。这个挑战主要有两个方面：在降低模型大小的同时，有效学习用户和物品表示以实现高效推荐。尽管模型压缩和架构搜索方面有了相当大的进展，但现有方法面临着明显的限制。其中包括模型压缩中预训练/重新训练的额外计算开销以及架构设计中广泛的搜索空间。此外，在具有严格时间或空间限制的情况下，管理复杂性和遵守内存限制是有问题的。为了解决这些问题，本文引入了一种新的学习范式，称为动态稀疏学习（DSL），专门用于推荐模型。DSL创新性地从头开始训练一个轻量级稀疏模型，周期的重构。",
    "tldr": "本文提出了一种针对推荐模型的新型学习范式，称为动态稀疏学习（DSL），通过从头训练一个轻量级稀疏模型，解决了模型大小和学习效率的问题。"
}