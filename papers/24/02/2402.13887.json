{
    "title": "Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models",
    "abstract": "arXiv:2402.13887v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output pr",
    "link": "https://arxiv.org/abs/2402.13887",
    "context": "Title: Beyond Probabilities: Unveiling the Misalignment in Evaluating Large Language Models\nAbstract: arXiv:2402.13887v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output pr",
    "path": "papers/24/02/2402.13887.json",
    "total_tokens": 817,
    "translated_title": "超越概率：揭示评估大型语言模型中的错位问题",
    "translated_abstract": "大型语言模型（LLMs）在各种应用中展现出卓越的能力，从根本上改变了自然语言处理（NLP）研究的格局。然而，最近的评估框架通常依赖于LLMs的输出概率进行预测，主要是由于计算约束，偏离了真实世界的LLMs使用场景。虽然被广泛采用，基于概率的评估策略的有效性仍是一个开放的研究问题。本研究旨在审查这种基于概率的评估方法在使用LLMs进行多项选择题（MCQs）时的有效性，突显其固有局限性。我们的实证调查显示，普遍的基于概率的评估方法未能与基于生成的预测相适应。此外，当前的评估框架通常通过基于输出预测的预测任务来评估LLMs",
    "tldr": "本研究揭示了在使用大型语言模型进行多项选择题时，基于概率的评估方法与基于生成的预测不相吻合的固有局限性。",
    "en_tdlr": "This study reveals the inherent limitation of probability-based evaluation methods in using large language models for multiple choice questions due to misalignment with generation-based prediction."
}