{
    "title": "Preference-Conditioned Language-Guided Abstraction",
    "abstract": "Learning from demonstrations is a common way for users to teach robots, but it is prone to spurious feature correlations. Recent work constructs state abstractions, i.e. visual representations containing task-relevant features, from language as a way to perform more generalizable learning. However, these abstractions also depend on a user's preference for what matters in a task, which may be hard to describe or infeasible to exhaustively specify using language alone. How do we construct abstractions to capture these latent preferences? We observe that how humans behave reveals how they see the world. Our key insight is that changes in human behavior inform us that there are differences in preferences for how humans see the world, i.e. their state abstractions. In this work, we propose using language models (LMs) to query for those preferences directly given knowledge that a change in behavior has occurred. In our framework, we use the LM in two ways: first, given a text description of ",
    "link": "https://arxiv.org/abs/2402.03081",
    "context": "Title: Preference-Conditioned Language-Guided Abstraction\nAbstract: Learning from demonstrations is a common way for users to teach robots, but it is prone to spurious feature correlations. Recent work constructs state abstractions, i.e. visual representations containing task-relevant features, from language as a way to perform more generalizable learning. However, these abstractions also depend on a user's preference for what matters in a task, which may be hard to describe or infeasible to exhaustively specify using language alone. How do we construct abstractions to capture these latent preferences? We observe that how humans behave reveals how they see the world. Our key insight is that changes in human behavior inform us that there are differences in preferences for how humans see the world, i.e. their state abstractions. In this work, we propose using language models (LMs) to query for those preferences directly given knowledge that a change in behavior has occurred. In our framework, we use the LM in two ways: first, given a text description of ",
    "path": "papers/24/02/2402.03081.json",
    "total_tokens": 869,
    "translated_title": "基于用户偏好的语言引导的抽象化",
    "translated_abstract": "从示范中学习是人们教导机器人的常用方式，但它容易出现误导性特征相关性。最近的研究使用语言构建状态抽象，即包含任务相关特征的视觉表示，以进行更通用的学习。然而，这些抽象也取决于用户在任务中关注的偏好，这可能很难用语言描述或仅仅通过语言详尽说明。我们如何构建抽象来捕捉这些潜在偏好呢？我们观察到人类行为反映了他们的世界观。我们的关键观点是，人类行为的变化告诉我们人类对世界的偏好存在差异，即他们的状态抽象也不同。在这项工作中，我们提出使用语言模型（LM）直接查询这些偏好，给定已发生行为变化的知识。在我们的框架中，我们使用LM有两种方式：首先，根据文本描述查询用户的偏好。",
    "tldr": "本研究提出了一种基于用户偏好的语言引导的抽象化方法，通过观察人类行为的变化并使用语言模型查询用户的偏好，构建适用于不同用户的状态抽象。",
    "en_tdlr": "This paper proposes a preference-conditioned language-guided abstraction approach, by observing changes in human behavior and using language models to query user preferences, in order to construct task-relevant state abstractions that are applicable to different users."
}