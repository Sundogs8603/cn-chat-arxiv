{
    "title": "Subgraph-level Universal Prompt Tuning",
    "abstract": "arXiv:2402.10380v1 Announce Type: cross  Abstract: In the evolving landscape of machine learning, the adaptation of pre-trained models through prompt tuning has become increasingly prominent. This trend is particularly observable in the graph domain, where diverse pre-training strategies present unique challenges in developing effective prompt-based tuning methods for graph neural networks. Previous approaches have been limited, focusing on specialized prompting functions tailored to models with edge prediction pre-training tasks. These methods, however, suffer from a lack of generalizability across different pre-training strategies. Recently, a simple prompt tuning method has been designed for any pre-training strategy, functioning within the input graph's feature space. This allows it to theoretically emulate any type of prompting function, thereby significantly increasing its versatility for a range of downstream applications. Nevertheless, the capacity of such simple prompts to ful",
    "link": "https://arxiv.org/abs/2402.10380",
    "context": "Title: Subgraph-level Universal Prompt Tuning\nAbstract: arXiv:2402.10380v1 Announce Type: cross  Abstract: In the evolving landscape of machine learning, the adaptation of pre-trained models through prompt tuning has become increasingly prominent. This trend is particularly observable in the graph domain, where diverse pre-training strategies present unique challenges in developing effective prompt-based tuning methods for graph neural networks. Previous approaches have been limited, focusing on specialized prompting functions tailored to models with edge prediction pre-training tasks. These methods, however, suffer from a lack of generalizability across different pre-training strategies. Recently, a simple prompt tuning method has been designed for any pre-training strategy, functioning within the input graph's feature space. This allows it to theoretically emulate any type of prompting function, thereby significantly increasing its versatility for a range of downstream applications. Nevertheless, the capacity of such simple prompts to ful",
    "path": "papers/24/02/2402.10380.json",
    "total_tokens": 785,
    "translated_title": "子图级通用提示调整",
    "translated_abstract": "在不断发展的机器学习领域，通过提示调整来调整预训练模型的适应性变得日益突出。这一趋势在图领域特别明显，不同的预训练策略为为图神经网络开发有效的基于提示的调整方法提供了独特的挑战。之前的方法受到限制，主要针对具有边预测预训练任务的模型定制了专门的提示函数。然而，这些方法在不同预训练策略之间缺乏泛化能力。最近，设计了一种简单的提示调整方法，可适用于任何预训练策略，在输入图的特征空间内发挥作用。这使其从理论上可以模拟任何类型的提示函数，从而显著提高了其在一系列下游应用中的通用性。",
    "tldr": "设计了一种可适用于任何预训练策略的简单提示调整方法，通过位于输入图特征空间内的功能来实现，从而增加了其在各种下游应用中的通用性",
    "en_tdlr": "A simple prompt tuning method has been designed for any pre-training strategy, functioning within the input graph's feature space, thereby significantly increasing its versatility for a range of downstream applications."
}