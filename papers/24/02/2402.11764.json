{
    "title": "ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs",
    "abstract": "arXiv:2402.11764v1 Announce Type: cross  Abstract: Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debias",
    "link": "https://arxiv.org/abs/2402.11764",
    "context": "Title: ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs\nAbstract: arXiv:2402.11764v1 Announce Type: cross  Abstract: Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debias",
    "path": "papers/24/02/2402.11764.json",
    "total_tokens": 893,
    "translated_title": "基于ChatGPT的数据增强技术用于改善LLMs的参数高效去偏见化",
    "translated_abstract": "大语言模型（LLMs）虽然功能强大，但存在有害的社会偏见。由于计算成本、数据约束和可能降低多任务语言能力，去偏见化通常具有挑战性。本文介绍了一种利用ChatGPT生成合成训练数据的新方法，旨在增强LLMs的去偏见化。我们提出了两种策略：目标提示，对已知偏见提供有效的去偏见化，但需要事先指定问题中的偏见; 一般提示，虽然效果稍逊，但能够跨各种类别进行去偏见化。我们利用适配器调整来实现资源高效的LLM去偏见化，并比较了我们的合成数据与现有去偏见化数据集的效果。我们的结果表明：（1）ChatGPT可以高效地生成用于去偏见化其他LLMs的高质量训练数据；（2）通过我们的方法生成的数据超越了现有数据集在去偏见化上的效果。",
    "tldr": "本研究提出了一种利用ChatGPT生成合成训练数据来增强LLMs去偏见化的新方法，能够高效地去除已知偏见并跨越不同类别进行去偏见化。",
    "en_tdlr": "This study proposes a novel approach using ChatGPT to generate synthetic training data to enhance debiasing of LLMs, efficiently removing known biases and achieving debiasing across different categories."
}