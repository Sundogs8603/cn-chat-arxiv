{
    "title": "Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation",
    "abstract": "arXiv:2402.18159v1 Announce Type: new  Abstract: In the realm of reinforcement learning (RL), accounting for risk is crucial for making decisions under uncertainty, particularly in applications where safety and reliability are paramount. In this paper, we introduce a general framework on Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL), with static Lipschitz Risk Measures (LRM) and general function approximation. Our framework covers a broad class of risk-sensitive RL, and facilitates analysis of the impact of estimation functions on the effectiveness of RSRL strategies and evaluation of their sample complexity. We design two innovative meta-algorithms: \\texttt{RS-DisRL-M}, a model-based strategy for model-based function approximation, and \\texttt{RS-DisRL-V}, a model-free approach for general value function approximation. With our novel estimation techniques via Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in distributional RL with augmented Ma",
    "link": "https://arxiv.org/abs/2402.18159",
    "context": "Title: Provable Risk-Sensitive Distributional Reinforcement Learning with General Function Approximation\nAbstract: arXiv:2402.18159v1 Announce Type: new  Abstract: In the realm of reinforcement learning (RL), accounting for risk is crucial for making decisions under uncertainty, particularly in applications where safety and reliability are paramount. In this paper, we introduce a general framework on Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL), with static Lipschitz Risk Measures (LRM) and general function approximation. Our framework covers a broad class of risk-sensitive RL, and facilitates analysis of the impact of estimation functions on the effectiveness of RSRL strategies and evaluation of their sample complexity. We design two innovative meta-algorithms: \\texttt{RS-DisRL-M}, a model-based strategy for model-based function approximation, and \\texttt{RS-DisRL-V}, a model-free approach for general value function approximation. With our novel estimation techniques via Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in distributional RL with augmented Ma",
    "path": "papers/24/02/2402.18159.json",
    "total_tokens": 919,
    "translated_title": "具有一般函数逼近的可证明风险敏感分布式强化学习",
    "translated_abstract": "在强化学习（RL）领域中，考虑风险对于在不确定性下做出决策至关重要，特别是在安全性和可靠性至关重要的应用中。本文介绍了一个关于风险敏感分布式强化学习（RS-DisRL）的一般性框架，其中包含静态利普希茨风险度量（LRM）和一般函数逼近。我们的框架涵盖了广泛的风险敏感RL类别，并有助于分析估计函数对RSRL策略有效性的影响以及评估它们的样本复杂性。我们设计了两种创新的元算法：\\texttt{RS-DisRL-M}，一种用于基于模型的函数逼近的模型化策略，以及\\texttt{RS-DisRL-V}，一种用于一般价值函数逼近的无模型方法。通过最小二乘回归（LSR）和最大似然估计（MLE）的新颖估计技术，我们在分布式RL中进行了增强的Ma",
    "tldr": "介绍了一个关于风险敏感分布式强化学习的通用框架，涵盖静态利普希茨风险度量和一般函数逼近，设计了两种创新的元算法。",
    "en_tdlr": "Introduces a general framework on Risk-Sensitive Distributional Reinforcement Learning, covering static Lipschitz Risk Measures and general function approximation, and designs two innovative meta-algorithms."
}